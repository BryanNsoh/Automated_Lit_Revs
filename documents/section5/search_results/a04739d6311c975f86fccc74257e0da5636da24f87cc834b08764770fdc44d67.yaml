- DOI: https://doi.org/10.1109/access.2021.3135362
  analysis: '>'
  authors:
  - WJM Lakmini Prarthana Jayasinghe
  - Ravinesh C. Deo
  - Afshin Ghahramani
  - Sujan Ghimire
  - Nawin Raj
  citation_count: 17
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account
    Personal Sign In Personal Sign In * Required *Email Address *Password Forgot Password?
    Sign In Don''t have a Personal Account? Create an IEEE Account now. Create Account
    Learn more about personalization features. IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/6514899/09650850.pdf
  publication_year: 2021
  relevance_score1: 0
  relevance_score2: 0
  title: 'Deep Multi-Stage Reference Evapotranspiration Forecasting Model: Multivariate
    Empirical Mode Decomposition Integrated With the Boruta-Random Forest Algorithm'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1038/s41598-017-08235-z
  analysis: '>'
  authors:
  - Doudou Guo
  - J.A. de Juan
  - Liu Chang
  - Zhang Jing-jin
  - Dan Huang
  citation_count: 22
  full_citation: '>'
  full_text: ">\n1\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\n\
    www.nature.com/scientificreports\nDiscrimination of plant root \nzone water status\
    \ in greenhouse \nproduction based on phenotyping \nand machine learning techniques\n\
    Doudou Guo, Jiaxiang Juan, Liying Chang, Jingjin Zhang   & Danfeng Huang\nPlant-based\
    \ sensing on water stress can provide sensitive and direct reference for precision\
    \ irrigation \nsystem in greenhouse. However, plant information acquisition, interpretation,\
    \ and systematical \napplication remain insufficient. This study developed a discrimination\
    \ method for plant root zone water \nstatus in greenhouse by integrating phenotyping\
    \ and machine learning techniques. Pakchoi plants were \nused and treated by three\
    \ root zone moisture levels, 40%, 60%, and 80% relative water content. Three \n\
    classification models, Random Forest (RF), Neural Network (NN), and Support Vector\
    \ Machine (SVM) \nwere developed and validated in different scenarios with overall\
    \ accuracy over 90% for all. SVM model \nhad the highest value, but it required\
    \ the longest training time. All models had accuracy over 85% in all \nscenarios,\
    \ and more stable performance was observed in RF model. Simplified SVM model developed\
    \ \nby the top five most contributing traits had the largest accuracy reduction\
    \ as 29.5%, while simplified RF \nand NN model still maintained approximately\
    \ 80%. For real case application, factors such as operation \ncost, precision\
    \ requirement, and system reaction time should be synthetically considered in\
    \ model \nselection. Our work shows it is promising to discriminate plant root\
    \ zone water status by implementing \nphenotyping and machine learning techniques\
    \ for precision irrigation management.\nOver the past decade, the environmental\
    \ information based methods for precision management have been widely \napplied\
    \ in greenhouse practice. The ambient climate of greenhouse such as temperature,\
    \ humidity, and root zone \nwater content is monitored closely as the reference\
    \ for crop management1. Water and fertilizer are the most \nimportant inputs for\
    \ greenhouse crop because the plant growth is significantly affected by plant\
    \ water and nutri-\ntion status. Root zone water status has a significant influence\
    \ on plant evapotranspiration rate2. Root zone water \nstatus indicates the moisture\
    \ situation of plant root growth environment and highly correlates to plant growing\
    \ \nstatus3. Monitoring of root zone water status could help to establish the\
    \ precision management strategy on irriga-\ntion in crop production. However,\
    \ conventional probe sensors for moisture measurement are limited on detecting\
    \ \nthe actual substrate water status effectively in trays. Several commercialized\
    \ soil moisture sensors such as tensi-\nometer, neutron probes and time domain\
    \ reflectometry probes have been widely used in pot planting but not \napplicable\
    \ for seedlings mainly because the high costs, unsuitable size, and unreliable\
    \ measurements. The volume \nof cell in plug tray is minimal resulting that common\
    \ probe sensors can hardly insert into substrates. In addition, \ndue to the uneven\
    \ water distribution among different plug trays, the growers require continuous\
    \ and real-time \nwater content monitoring for large -scale rather than for single\
    \ point, to prevent the seedlings from water stress \nand growth restriction.\
    \ The relationship between plug tray weight and root zone water content was studied\
    \ to \nprovide a weighting based irrigation control strategy4. However, irrigation\
    \ based on tray weight is labor intensive \nand inaccuracy in practice.\nWith\
    \ the development of sensing technology, real-time monitoring of physiological\
    \ and ecological informa-\ntion of the plant itself provides an approach to indicate\
    \ root zone water status5, 6. Plant physiological traits such \nas tissue water\
    \ status, stomatal conductance, and sap flow are known sensitively responding\
    \ to water stress. The \nstatus of plant water content could be a useful indicator\
    \ for irrigation7. Phenotypic traits such as leaf area, leaf \nangle, and thermal\
    \ temperature show strong correlations with the root zone water status8. However,\
    \ most of the \nSchool of agriculture and biology, Shanghai Jiao Tong University,\
    \ Shanghai, People’s Republic of China. Doudou Guo \nand Jiaxiang Juan contributed\
    \ equally to this work. Correspondence and requests for materials should be addressed\
    \ \nto J.Z. (email: jj.zhang@sjtu.edu.cn) or D.H. (email: hdf@sjtu.edu.cn)\nReceived:\
    \ 21 March 2017\nAccepted: 7 July 2017\nPublished: xx xx xxxx\nOPEN\nwww.nature.com/scientificreports/\n\
    2\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\nplant -based\
    \ irrigation methods are still at research/developing stage and little used yet\
    \ for practice (except for \nthermal sensing in some situations). Systematical\
    \ research on phenotypic traits selection for irrigation has seldom \nbeen reported.\n\
    Rapid and non-destructive plant sensing method is needed for plant-based information\
    \ acquisition. The \napplication of machine vision technology could be the solution.\
    \ Plant phenotypic traits can be quickly and effi-\nciently obtained by imaging-based\
    \ automatic inspection and analysis5. Therefore, machine vision has the poten-\n\
    tial to realize the rapid, reliable, real-time monitoring of plant status with\
    \ low cost, which meets the requirements \nfor precision irrigation scheduling9.\
    \ Machine vision technology has been applied in plant monitoring such as the \n\
    detection of plant abiotic stress through color variation, plant movement monitoring,\
    \ textural features detection, \ncanopy temperature measurement and detection\
    \ of plant biotic disease8, 10–13. With the development of pheno-\ntyping technology,\
    \ various imaging sensors have been used to collect phenotypic parameters which\
    \ are related \nto plant stress in automated or semi-automated high-throughput\
    \ plant phenotyping platforms14, 15. However, \ndata generated by these platforms\
    \ has enormous volume, variety, velocity, and veracity, which must be efficiently\
    \ \nanalyzed. The classical statistical method is difficult to analyze the high-dimensional\
    \ and dynamical image data, \nresulting limited phenotyping interpretation16.\n\
    Machine learning, a type of artificial intelligence has good performance in efficiently\
    \ dealing with large \namount of data in various domains. It is promising in phenotyping\
    \ applications on features extraction and pat-\nterns identification from large\
    \ dataset through holistic approaches. Machine learning algorithm has few input\
    \ \nfactors with high predict accuracy and strong adaptability. Several machine\
    \ learning algorithms, such as support \nvector machine (SVM), artificial neural\
    \ network (ANN) and random forest (RF), have been used for classification \nmodel\
    \ development17, 18.\nIn practice, machine learning method has been applied to\
    \ remote sensing system and irrigation system19, 20. \nMany studies have reported\
    \ plant stress phenotyping using machine learning in identification, classification,\
    \ \nquantification, and prediction of plant water status, but few focused on phenotyping\
    \ root zone water stress10, 21, 22. \nIt is crucial to detect plant water stress\
    \ at the early stage before any damage observed with visible wilting, which \n\
    affect subsequent plant growth and quality. Correlating root zone water status\
    \ with plant phenotype to supervise \nirrigation system could be a practical way\
    \ in precision irrigation system with easy operation and quick response.\nPakchoi\
    \ (Brassica campestris ssp. L. chinensis) is one of typical vegetables in greenhouse\
    \ production, which is \nsensitive to water status. During the past few years,\
    \ commercial production of leafy vegetable has been initiated \nand developed\
    \ both in the greenhouse (solar plant factory) and artificial light plant factory23.\
    \ Growing baby pak-\nchoi with plug trays using substrate in greenhouse is a promising\
    \ production mode because it is suitable for mech-\nanized harvesting and precision\
    \ management, which improves the production efficiency24. The accurate irrigation\
    \ \nscheduling is critical for safe and high quality of greenhouse vegetable,\
    \ as well as improving the resource utiliza-\ntion and production profits. This\
    \ study aims to discriminate plant root zone water status nondestructively, taking\
    \ \npakchoi as the plant material, which is a part of on-going research on developing\
    \ machine vision-based precision \nirrigation system for factory production of\
    \ pakchoi. The objectives of this study are: (1) investigating the rela-\ntionship\
    \ between root zone water status and plant growth; (2) developing a root zone\
    \ water status discrimination \nmodel by applying phenotyping technology and machine\
    \ learning algorithms; (3) evaluating the performance of \ndeveloped models in\
    \ various application scenarios.\nResults and Discussion\nThe effect of root zone\
    \ water status on plant growth. \nThe effect of root zone water status on pakchoi\
    \ \nplant growth was investigated under three relative water content treatments,\
    \ 40% (low), 60% (medium), and \n80% (high). The fresh and dry weight of shoot\
    \ and root for both cultivars were significantly different between \ntreatments\
    \ as shown in Fig. 1 with ANOVA test (p < 0.01). Treatments had different effects\
    \ between cultivars. For \ncultivar “Huawang”, treatment of medium water content\
    \ had the highest values of fresh and dry weight of shoot \nand root (fresh weight\
    \ as 2727.1 mg and 184.4 mg for shoot and root respectively, and dry weight as\
    \ 185.1 mg and \n15.1 mg per plant for shoot and root respectively), while cultivar\
    \ “Kangre 605” had the lowest values under such \ntreatment (fresh weight as 500.3\
    \ mg and 52.0 mg for shoot and root respectively, and dry weight as 45.3 mg and\
    \ \n5.1 mg per plant for shoot and root respectively). The differences of pakchoi\
    \ growth were significant between two \ncultivars under the treatment of medium\
    \ water content (p < 0.01 for all comparisons), but there were no signifi-\ncant\
    \ differences observed between cultivars under the other two treatments. This\
    \ is mainly because “Kangre 605” \nis a heat-resistant cultivar, which has the\
    \ ability of growing well under water stress conditions. “Huawang” prefers \n\
    moderate water condition, and both the excess and deficiency of water supply could\
    \ affect the accumulation of \nbiomass.\nRelationship between root-zone water\
    \ status and phenotypic traits. \nPhenotypic traits extrac-\ntion. Two types of\
    \ images were acquired, visible and near-infrared (NIR) image (Supplementary Fig. S1).\
    \ There \nwere in total 37 phenotypic traits extracted from images for each plant\
    \ (Table 1). These phenotypic traits were \nclassified into three categories for\
    \ easy description, morphological trait, color trait, and NIR trait, including\
    \ 19, 6, \nand 12 traits respectively. Morphological traits mainly indicated the\
    \ shape feature of target plant region, such as \ncircumference and eccentricity.\
    \ Color traits were generated from color images, including the pixel value informa-\n\
    tion of R (red), G (green), and B (blue) components. NIR traits were the pixel\
    \ intensity information of different \nranges in NIR images.\nWater related phenotypic\
    \ traits selection. \nIn order to increase the efficiency of model development,\
    \ \ntraits selection is necessary to identify redundant traits which are irrelevant\
    \ to root zone water status. The signif-\nicance of 37 extracted phenotypic traits\
    \ on classifying root zone water status were investigated by using ANOVA \n(Fig. 2).\
    \ For better illustration, the p value was transformed into -log10 (p). There\
    \ were five traits with p > 0.05 \nwww.nature.com/scientificreports/\n3\nSCienTifiC\
    \ RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\n(i.e. -log10 (p) < 1.301)\
    \ including Normsmallpax, Circumference, Roundness, Bdrycount and Bdrytoarearatio\
    \ \n(numbered as 14, 23, 26, 28, and 29 in Table 1 respectively). Therefore, the\
    \ five traits were excluded from the data-\nsets in the following modeling process.\n\
    Root zone water status discrimination model development and validation. \nModeling\
    \ potentiality \nassessment. The Partial Least Squares - Discriminant Analysis\
    \ (PLS-DA) was conducted to present a visual \nspace distribution of three treatment\
    \ groups for assessing the modeling potentiality using the 32 selected phe-\n\
    notypic traits. Figure 3 shows the PLS-DA model performance and cumulative variance\
    \ explained by the top \nfive components. The goodness of fit was quantified by\
    \ R2 while the predictive ability was indicated by Q2. The \naccuracy, R2, and\
    \ Q2 of PLS-DA model by using all the top five components reached 73%, 0.35, and\
    \ 0.34, respec-\ntively. The value of R2 and Q2 should be greater than 0.5 to\
    \ be taken as an indicator of model acceptability. The \ncumulative explained\
    \ variance ranged from 17.6% to 72.2% by the number of components from one to\
    \ five, with \nthe first three components explaining 66.9% variance. The performance\
    \ of PLS-DA model improved (R2 = 0.67, \nQ2 = 0.66) by excluding the medium root\
    \ zone water group as the difference between medium water group and \nthe other\
    \ two groups was not significant. Overall, PLS-DA shows the potential of root\
    \ zone water status discrim-\nination by phenotypic traits.\nDiscrimination model\
    \ development. The total dataset used for model development was 2120 samples with\
    \ 32 \ntraits of each (2120*32), and the dataset was separated into 80% training\
    \ dataset and 20% testing dataset. The \ntraining data set was used for model\
    \ development. Parameter values for the three modeling algorithms, which are \n\
    Random Forest (RF), Neural Network (NN), and Support Vector Machine (SVM), were\
    \ selected by using 10-fold \ncross validation in three repetitions tuning (Table 2).\
    \ Among the training dataset, 10% of the data were used as \nvalidation in each\
    \ cross validation. Table 2 also lists the package, function, and program training\
    \ time of modeling \nalgorithms. All the R codes ran in Rstudio 1.0.44 on the\
    \ platform of windows 10 × 64 system with an Intel Core \ni7-5500U CPU and 4 GB\
    \ RAM.\nConsidering the dynamics of the data input and training in practice, the\
    \ model training time is an important \nfactor to be considered in the optimization\
    \ process. The training time increases exponentially with the number \nof parameters\
    \ and varies substantially between algorithms. Among these three modeling algorithms,\
    \ NN took \nthe shortest training time as 544.37 seconds, which was approximately\
    \ 76% of RF and 44% of SVM. The training \ntime consumption of SVM should be considered\
    \ in real case of decision support system as this method is more \nsensitive to\
    \ the data size than others, especially when time is limited and data set is large.\
    \ The trait contribution \nanalysis in each model could be a way to simplify the\
    \ model inputs and reduce the training time, which was dis-\ncussed in section\
    \ 3.3.\nThe mean prediction accuracy and Cohen’s Kappa coefficient (k) of all\
    \ the three models were higher than 90% \nand 0.85 respectively, as shown in Fig. 4A,B.\
    \ SVM model had the best performance with higher accuracy and k \nvalue (92.5%\
    \ and 0.89 respectively) than the other two models. The result also showed that\
    \ SVM was slightly more \nstable with less variation than others.\nThe ROC (receiver\
    \ operating characteristic) curve was used to evaluate the performance of modeling\
    \ classi-\nfier. Figure 4C shows the ROC curves of the three models. The x-axis\
    \ represents false positive rate (FPR), and the \ny-axis represents true positive\
    \ rate (TPR). The area under the curve (AUC) indicates classification performance,\
    \ \nwhich is the ability of target model to correctly classify the plants in different\
    \ root zone water status. SVM model \nhad the highest AUC value as 0.6827 among\
    \ the three models, while the differences were not substantial com-\npared to\
    \ 0.6766 and 0.6788 AUC of RF model and NN model respectively.\nFigure 1. Effects\
    \ of different water treatments on the growth of two pakchoi cultivars. (a) Shoot\
    \ dry weight \nper plant under three different root zone water content treatments,\
    \ 40% (low), 60% (medium), and 80% (high) \nrelevant water content, (b) root dry\
    \ weight per plant under three treatments, (c) shoot fresh weight under three\
    \ \ntreatments, and (d) root fresh weight per plant under three treatments. Means\
    \ with the same letter are not \nsignificantly different (P > 0.01). One half-bar\
    \ represents the standard deviation.\nwww.nature.com/scientificreports/\n4\nSCienTifiC\
    \ RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\nDiscrimination model validation.\
    \ Confusion matrix was implemented to validate the three developed models \nwith\
    \ 2120*32 dataset. The prediction accuracy of each model from cross validation\
    \ is the sum of the predic-\ntion accuracy on the diagonal in each sub-table of\
    \ Table 3. All the developed models had good prediction accu-\nracy higher than\
    \ 90%. SVM model had the highest prediction accuracy as 92.5%, but no substantial\
    \ difference \nobserved on accuracy among the three models (91.3% and 91.4% for\
    \ RF model and NN model respectively).\nCompared to the other two treatments,\
    \ the medium root zone water group had the highest percentage of false \npositive\
    \ (4.0%, 3.4%, 3.8% for RF, NN, and SVM model) and false negative (3.2%, 2.9%\
    \ and 3.2% RF, NN, and \nSVM model) among the three models. It could be explained\
    \ as that medium level is adjacent to both high and low \nlevel, which is less\
    \ distinguishable comparing to the other two levels.\nModel evaluation under different\
    \ conditions. To investigate the robustness and applicability of the three models\
    \ \nin different conditions, the test dataset was reorganized into different scenarios,\
    \ different time of a day, growth \nstage (stage 1 to stage 5), weather condition\
    \ (sunny and cloudy), and cultivar (“Huawang” and “Kangre 605”). \nThe dataset\
    \ used for each growth was 120*32, the dataset used for each time in day was 240\
    \ to 270 records with \n32 traits, and the dataset used for each cultivar was\
    \ 1060*32. All the datasets were separated into 80% training \ndataset and 20%\
    \ testing dataset. Figure 5 shows classification accuracy of three models in all\
    \ scenarios were over \n85%. Among the seven time points of a day, the accuracy\
    \ of the three models were relatively lower at 06:00 h than \nNo.\nTrait name\n\
    Description\nCategory\n1\nNIR_area\nArea of near-infrared plant (mm2)\nNIR\n2\n\
    1 A\nRatio of plant pixels with NIR intensity in 170–186\nNIR\n3\n1 R\nNumber\
    \ of plant pixels with NIR intensity in 186–202\nNIR\n4\n2 A\nRatio of plant pixels\
    \ with NIR intensity in 186–202\nNIR\n5\n2 R\nNumber of plant pixels with NIR\
    \ intensity in 202–218\nNIR\n6\n3 A\nRatio of plant pixels with NIR intensity\
    \ in 202–218\nNIR\n7\n3 R\nNumber of plant pixels with NIR intensity in 218–234\n\
    NIR\n8\n4 A\nRatio of plant pixels with NIR intensity in 218–234\nNIR\n9\n4 R\n\
    Number of plant pixels with NIR intensity in 234–250\nNIR\n10\n5 A\nRatio of plant\
    \ pixels with NIR intensity in 234–250\nNIR\n11\n5 R\nAverage intensity of near-infrared\
    \ plant pixels\nNIR\n12\nNIR intensity\nNumber of plant pixels with NIR intensity\
    \ in 170–186\nNIR\n13\nMincirclediam\nMin Enclosing Circle Diameter (mm)\nMorphological\n\
    14\nNormsmallpax\n2nd Moment Principle Axis Small Norm\nMorphological\n15\nNormlargrpax\n\
    2nd Moment Principle Axis Large Norm\nMorphological\n16\nMinrectarea\nMin Area\
    \ Rectangle Area (mm2)\nMorphological\n17\nMindistcenbdy\nCenter Of Mass To Boundary\
    \ Distance (mm)\nMorphological\n18\nVrectsizey\nHeight of the smallest vertical\
    \ rectangle covering the plant (mm)\nMorphological\n19\nVrectsizex\nWidth of the\
    \ smallest vertical rectangle covering the plant (mm)\nMorphological\n20\nCompactness\n\
    Square of the objects perimeter to object area\nMorphological\n21\nReal area\n\
    Area of plant (mm2)\nMorphological\n22\nPaxratio\n2nd Moments Principal Axis Ratio\n\
    Morphological\n23\nCircumference\nPerimeter of plant excluding holes (mm)\nMorphological\n\
    24\nEccentricity\nThe ratio of the distance between the foci to the length of\
    \ the major axis\nMorphological\n25\nMaxdiam\nMaximum distance between two points\
    \ on the plant boundary (mm)\nMorphological\n26\nRoundness\nThe ratio between\
    \ the inscribed and the circumscribed circles\nMorphological\n27\nBdryround\n\
    Boundary Point Roundness\nMorphological\n28\nBdrycount\nBoundary Point Count\n\
    Morphological\n29\nBdrytoarearatio\nBoundary Points To Area Ratio\nMorphological\n\
    30\nConhullcirc\nConvex Hull Circumference (mm)\nMorphological\n31\nConhullarea\n\
    Convex Hull Area (mm2)\nMorphological\n32\nMean Color Blue\nAverage color in Blue\
    \ range of the RGB color space\nColor\n33\nMean Color Blue \nVariance\nThe variance\
    \ of average color in Blue range of the RGB color space\nColor\n34\nMean Color\
    \ \nGreen\nAverage color in Green range of the RGB color space\nColor\n35\nMean\
    \ Color \nGreen Variance\nThe variance of average color in Green range of the\
    \ RGB color space\nColor\n36\nMean Color Red\nAverage color in Red range of the\
    \ RGB color space\nColor\n37\nMean Color Red \nVariance\nThe variance of average\
    \ color in Red range of the RGB color space\nColor\nTable 1. Phenotypic traits\
    \ extracted from images for each plant.\nwww.nature.com/scientificreports/\n5\n\
    SCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\nat other time\
    \ points (Fig. 5a), which might be caused by the rapid decline of the greenhouse\
    \ humidity at sunrise \nresulting in the change of leaf surface moisture status.\
    \ For NN model, its accuracy dropped at 10:00 h and 14:00 h \nwith the highest\
    \ accuracy at noon, while RF and SVM model had high accuracy at 10:00 h as 100.0%.\
    \ For different \ngrowth stage, the accuracy of the three models had the trend\
    \ of increasing (Fig. 5b). The accuracy of RF and SVM \nmodel reached 100.0% in\
    \ the fourth and fifth growth stage which were substantially higher than NN model.\
    \ In the \nearly stage, leaf area was small resulting in poor discrimination performance.\
    \ With leaf expanding, the difference \nof traits between groups increased. In\
    \ addition, as the treatments of root zone water moisture level was applied \n\
    during the entire growing period, it would have accumulative effect on plant phenotype,\
    \ which increased the \nmodel classification accuracy in later stages.\nThe accuracy\
    \ of the three model were slightly higher in cloudy weather condition than in\
    \ sunny weather \ncondition (Fig. 5c), which might be because of less environmental\
    \ fluctuation in cloudy day. SVM model had the \nhighest accuracy in both weather\
    \ conditions (91.1% and 94.1% respectively). Among the three models, NN model\
    \ \nhad the largest difference of accuracy between cultivar (5.8%, Fig. 5d) compared\
    \ to RF and SVM model (1.1% and \n3.2% respectively). Overall, RF model performed\
    \ more stable than the other two considering all the scenarios.\nFigure 2. Significance\
    \ analysis of phenotypic traits with p value threshold as 0.05. The solid horizontal\
    \ line \nrepresents p = 0.05, i.e. -log10(p) = 1.301.\nFigure 3. PLS-DA model\
    \ performance and cumulative variance explained by different number of the top\
    \ five \ncomponents.\nModeling \nalgorithm\nParameter\nParameter \nrange\nParameter\
    \ \nselected\nR package\nFunction\nTraining \ntime (s)\nRF\nmtry\n2, 5, 8\n2\n\
    randomForest\nrandomForest()\n715.38\nntree\n500\n500\nNN\ndecay\n0, 0.1, 0.001\n\
    0.1\nnnet\nnn()\n544.37\nsize\n2, 5, 9\n9\nSVM\nsigma\n10(-3:0)\n0.1\ne1071\n\
    svm()\n1245.94\nc\n10(1:3)\n1000\nTable 2. Parameter selection result for cross\
    \ validation training in three modeling algorithms. Note: RF - \nRandom Forest,\
    \ NN - Neural Network, SVM - Support Vector Machine.\nwww.nature.com/scientificreports/\n\
    6\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\nFigure 4. Evaluation\
    \ result for different models. (A) Box-whisker plot of accuracy, (B) box-whisker\
    \ plot of \nKappa, and (C) the ROC curve of the three developed models. The straight\
    \ dotted line in subplot (C) represents \n0.5 AUC.\nModel: Random \nForest (RF)\n\
    Actual class\nHigh\nMedium\nLow\nPredicted class\nHigh\n30.3%\n1.4%\n0.9%\nMedium\n\
    2.4%\n30.1%\n1.6%\nLow\n0.3%\n2.0%\n30.9%\nAccuracy\n91.3%\nPredicted class\n\
    High\n30.2%\n1.6%\n1.1%\nMedium\n2.3%\n30.3%\n1.5%\nLow\n0.6%\n1.6%\n30.9%\nAccuracy\n\
    91.4%\nPrediction\nHigh\n30.5%\n1.5%\n0.6%\nMedium\n1.8%\n30.4%\n1.1%\nLow\n0.7%\n\
    1.7%\n31.6%\nAccuracy\n92.5%\nTable 3. Confusion matrix of cross validation model\
    \ results.\nwww.nature.com/scientificreports/\n7\nSCienTifiC RePORts | 7: 8303\
    \  | DOI:10.1038/s41598-017-08235-z\nTrait contribution analysis in developed\
    \ models. In order to check the possibility of reducing phenotyping cost \nand\
    \ model training time without significant accuracy reduction, sensitivity analysis\
    \ was conducted to assess the \ncontribution of each trait in developed models.\
    \ The simplified model with less explanatory variables were devel-\noped according\
    \ to the traits importance evaluation of the model implementation in the root\
    \ zone water status \ndetection. Table 4 lists the top five of most contributing\
    \ traits in each model. The explanation of each trait could \nbe referred to Table 1.\
    \ Different model had different preference on the trait type. In the top five\
    \ traits, there was no \nmorphological trait in RF model, no NIR trait in NN model,\
    \ and no color trait in SVM model.\nFigure 5. Classification accuracy of developed\
    \ models in different scenarios. (a) Different time of a day, (b) \ngrowth stage\
    \ from stage 1 to stage 5, (c) weather condition, and (d) cultivar.\nwww.nature.com/scientificreports/\n\
    8\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\nOverall, color\
    \ trait and NIR trait appeared more frequently than morphological trait, which\
    \ shows plant \ncolor and NIR characteristics would be more related to root zone\
    \ water status. This result agrees with several \nstudies in spectroscopy and\
    \ remote sensing research on the relationship between spectral reflectivity and\
    \ water \nrelated index13. The most frequently appearing traits in top five traits\
    \ of developed models were Mean.Color.\nGreen and Mean.Color.Red. There was only\
    \ one morphological trait in NN model and in SVM model, named as \nVrectsizeymm\
    \ and Compactness, respectively. This is likely because plant morphological appearance\
    \ is basically \ndetermined by genotype and substantially influenced by environment\
    \ such as airflow and illumination.\nModel classification ability dropped when\
    \ only the top five traits were used to develop model, named as sim-\nplified\
    \ model. The accuracy of simplified RF model and NN model could still reach 79.7%\
    \ and 77.4% respectively, \nwhile the accuracy of simplified SVM model decreased\
    \ substantially from 92.5% to 63.0% with 29.5% reduction. \nFor SVM model, the\
    \ cumulative contribution of the top five traits was 22.8%, lower than that of\
    \ RF model and \nNN model (32.6% and 29.9%, respectively), which could be the\
    \ main reason for the great decrease on the simpli-\nfied model accuracy. In addition,\
    \ simplified SVM model did not include color trait, while simplified NN model\
    \ \ndid not include NIR trait (Table 4), which showed color related traits would\
    \ better reflect plant root zone water \nstatus than the other two trait types\
    \ in RF and NN models while the SVM model relied more on NIR traits than \nthe\
    \ others.\nConclusion\nThis study developed a root zone water status discrimination\
    \ method during plant growth by integrating phe-\nnotyping and machine learning\
    \ techniques. Pakchoi plants were used. Three types of phenotyping traits such\
    \ \nas morphological trait, color trait, and near-infrared trait were acquired\
    \ as the inputs for classification models. \nThree machine learning models were\
    \ developed, Random Forest (RF), Neural Network (NN), and Support Vector \nMachine\
    \ (SVM), with accuracy above 90% for all. The SVM model had the highest accuracy\
    \ as 92.5% in model \ndevelopment but it took the longest training time as 1245.94\
    \ seconds in this study. In addition, the SVM model \nhad the largest accuracy\
    \ reduction (22.8%) in simplified model developed by the top five most contributing\
    \ trait \nresulting from trait contribution evaluation.\nThe three developed models\
    \ were evaluated by test dataset (20% of entire data pool) consisting of data\
    \ from \ndifferent scenarios, which were different time a day, growth stage, weather\
    \ condition, and pakchoi cultivar. All the \nthree models reached the overall\
    \ accuracy higher than 90%. SVM model had the highest value as 92.5%, but no \n\
    substantial differences were observed among the three. By evaluated in different\
    \ scenarios, the three models had \naccuracy over 85% in all scenarios with some\
    \ fluctuations. The accuracy at the time point of 06:00 h was the lowest \nfor\
    \ all the three models, and the trend of increasing was observed during the growth\
    \ of plant. Better performances \nwere observed in cloudy weather condition than\
    \ in sunny weather condition of the three models. NN model had \nthe largest difference\
    \ on accuracy between cultivars. Overall, SVM model had the highest classification\
    \ accuracy, \nbut more stable performance was observed in RF model considering\
    \ all the scenarios.\nThis study demonstrates the potential of machine learning\
    \ approach on discriminating root zone water status \nbased on complex plant phenotyping\
    \ traits. The developed discrimination method could promote the plant-based \n\
    irrigation decision making and implementation in practice. Developed models could\
    \ be further used in precision \nirrigation system with appropriate modifications\
    \ if environment or crop change. Further study is suggested to \ninvestigate the\
    \ repaid response of plant phenotypic traits to the change of root zone water\
    \ status and the perfor-\nmance of classification models on such response. Therefore\
    \ accumulative effect of water stress on plant growth \nduring the growing period\
    \ should be minimized with the real-time discrimination to control the potential\
    \ yield \nloss. Population phenotyping and water status discrimination are also\
    \ worth further study due to plant canopy \nmutual occlusion and growth competition.\
    \ In addition, phenotyping traits were extracted from images acquired \nin a constant\
    \ light environment. Changes in intensity, quality, and beam angle of illumination\
    \ would have effects \nModel\nTrait name\nContribution \n(%)\nCumulative \ncontribution\
    \ (%)\nAccuracy reduction (%)\nRF\nMean.Color.Green\n9.7\n32.6\n11.6 (from 91.3\
    \ to 79.9)\nMean.Color.Red\n9.5\n1 R\n5.6\n3 R\n4.0\n1 A\n3.8\nNN\nMean.Color.Green\n\
    6.8\n29.9\n14.0 (from 91.4 to 77.4)\nVrectsizeymm\n6.3\nMean.Color.Blue\n6.3\n\
    Mean.Color.Red\n5.8\nMean.Color.Green.Variance\n4.7\nSVM\n4 A\n4.8\n22.8\n29.5\
    \ (from 92.5 to 63.0)\n3 A\n4.8\n3 R\n4.7\n4 R\n4.6\nCompactness\n3.9\nTable 4.\
    \ The top five most contributing traits of each developed model.\nwww.nature.com/scientificreports/\n\
    9\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\non imaging\
    \ process and hence affect the modeling process. Therefore, research on effective\
    \ acquisition of phe-\nnotyping traits in natural light environment would be a\
    \ direction to reduce cost and enhance applicability. Our \npresented work is\
    \ capable of serving as a basis and supporting for intelligent greenhouse management,\
    \ especially \nfor irrigation management.\nMaterials and Methods\nPlant materials\
    \ and growth conditions. \nThe experiment was conducted in a commercial greenhouse\
    \ \nfrom May 1th to 29th in year of 2015, located at northeast of Shanghai, China\
    \ (31°10′ N, 121°36′ E). Two cultivars \nof pakchoi were used, “Huawang” and “Kangre\
    \ 605”, under pot substrate cultivation. Commercial substrate (veg-\netable seedling\
    \ substrate, Zhongnuo Agriculture Technology Co., Ltd, Huaian, China) composed\
    \ of peat moss, \nperlite, and organic matter was used. Plug tray was used at\
    \ seedling stage. On the tenth day after sowing, seedlings \nwere transplanted\
    \ to pots (volume of 1.13 L) with one seedling per pot.\nThe greenhouse air temperature,\
    \ humidity, photosynthetic photon flux, substrate temperature and electrical \n\
    conductivity were monitored every 5 minutes by using an automatic data logging\
    \ system (PM-11 Phytomonitor, \nBio Instruments S.R.L., Chisinau, Moldova) with\
    \ RTH-11 Meter and SMTE sensor. During the whole experimen-\ntal period, the average\
    \ air temperature was 24.3 °C with the highest as 36 °C and lowest as 13.7 °C.\
    \ The relative \nhumidity ranged from 16.4% to 88% with an average of 58%. Mean\
    \ daily accumulative solar radiation in the \ngreenhouse was 2.62 MJ/(m2·d), with\
    \ a maximum of 5.41 MJ/(m2·d) at May 21st.\nTreatments of root zone moisture level\
    \ were applied to each pot after transplanting. Root zone moisture was \nindicated\
    \ by the relative water content, the percent of volumetric water content comparing\
    \ to field water capacity. \nBecause of the dynamic change of root zone moisture\
    \ status, upper limit was applied to define the moisture level \nof root zone.\
    \ In other word, irrigation stopped when root zone moisture reached the upper\
    \ limit. Considering the \nfeasibility of moisture level control and plant growth,\
    \ three root zone moisture levels were applied, 40%, 60%, and \n80% (referred\
    \ to as ‘Low’, ‘Medium’, and ‘High’ hereafter, respectively). In total, five plants\
    \ of each cultivar were \nrandomly assigned to each treatment. Irrigation was\
    \ conducted manually for all plants once a day according to \nthe pot weight.\n\
    Image acquisition and processing. \nA commercial phenotyping system (Scanalyzer3D,\
    \ LemnaTec GmbH, \nWürselen, Germany) was used for image acquisition (Supplementary\
    \ Fig. S2). Images of each plant were taken \ndaily from top view before irrigation\
    \ from May 11th to 29th, 2015. Two types of image were acquired simultane-\nously,\
    \ near-infrared (NIR) and visible (VIS) images. The 20-day growing period was\
    \ divided into five stages with \nfour days of each, referred to as stage 1, 2,\
    \ 3, 4, and 5. Plant growing level especially the number of leaves was dif-\n\
    ferent at different stage (Fig. 6). The reference growth stage was divided mainly\
    \ according to the new leaf emerge \ntime under different treatments and adjusted\
    \ to even distribution for model comparison.\nTwo image acquisition schedules\
    \ were applied with one from 06:00 h to 18:00 h at two hour intervals and the\
    \ \nother one only at 14:00 h. Weather condition was classified into two scenarios,\
    \ sunny and cloudy (referred to as ‘S’ \nand ‘C’ hereafter, respectively) according\
    \ to weather forecast. Efforts were made to have the two image acquisition \n\
    schedules for each weather scenario in each stage (Table 5). At the end of 20-day\
    \ growing period, the fresh and \ndry weight of each plant sample were measured.\
    \ Each sample was dried at 105 °C for 1 hour and then at 60 °C to \nconstant weight.\
    \ The dried sample was weighted to an accuracy of ± 0.1 mg.\nImages acquired by\
    \ the imaging system were organized into LemnaBase which is the central database\
    \ interface \nfor the phenotyping system, and processed through an analysis pipeline\
    \ specifically adjusted for pakchoi by using \nLemnaGrid which is an image analysis\
    \ component of the phenotyping system. Image processing procedure in \nLemnaGrid\
    \ consisted of four main steps shown as the flowchart in Supplementary Fig. S3:\
    \ (1) image preprocess-\ning, extracting target images from LemnaBase; (2) segmentation,\
    \ separating target plant from the background \nin the image; (3) feature extraction,\
    \ analyzing segmentation result and producing phenotypic traits; and (4) \npost-processing,\
    \ summarizing feature extraction results of all target images and exporting as\
    \ “.xls” file.\nRoot zone water status discrimination model development. \nThe\
    \ development of root zone water \nstatus discrimination was consisted of three\
    \ steps: (1) data preprocessing, organizing data for phenotypic analysis \nand\
    \ model development; (2) phenotypic traits screening, removing those phenotypic\
    \ traits with insignificant \nFigure 6. Pakchoi seedling appearance at five growing\
    \ stages. DAT: days after transplanting.\nwww.nature.com/scientificreports/\n\
    10\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\ndifference\
    \ between treatments of root zone moisture level; (3) model development, using\
    \ different modeling algo-\nrithms to classify root zone water status. MetaboAnalyst\
    \ statistical analysis module (http://www.metaboanalyst.\nca) was implemented\
    \ for data preprocessing and phenotypic traits screening, and R language (windows,\
    \ R3.2.5, R \ncore Team, 2016) was implemented for model development.\nData preprocessing.\
    \ \nIn MetaboAnalyst statistical analysis module, data of extracted phenotypic\
    \ traits from \neach image were organized into one datasheet by sample ID (row)\
    \ and phenotypic traits (column). Empty rows \nwere detected and excluded after\
    \ data uploaded. Columns with more than 50% empty records were removed. In \n\
    the case of columns with data missing but less than 50%, the NAs (not available\
    \ values) were replaced by with \na small value (half of the minimum positive\
    \ values in the original data). All the traits value were normalized by \nusing\
    \ auto scaling method (mean-centered and divided by the standard deviation of\
    \ each variable).\nPhenotypic traits screening. \nAmong those phenotypic traits\
    \ extracted by imaging processing, traits which \nare insignificantly relevant\
    \ to treatments may influence the modeling accuracy. In this study, one-way Analysis\
    \ \nof Variance (ANOVA) was applied to screen traits resulting an optimal set\
    \ of explanatory variables for discrim-\nination model development. Traits with\
    \ P > 0.05 (Fisher’s LSD method) were excluded from the traits set. In \naddition,\
    \ the result of ANOVA presented a preliminary overview of the significance of\
    \ each trait to treatments.\nIn order to assess the potentiality of using phenotypic\
    \ traits to develop discrimination model, Partial Least \nSquares Discriminant\
    \ Analysis (PLS-DA) was applied, which is a supervised algorithm that uses multivariate\
    \ \nregression techniques to predict class membership via linear combination of\
    \ original variables. The PLS-DA was \nperformed by using plsr function in pls\
    \ package of R language, and the classification and cross-validation were \nperformed\
    \ by using the corresponding wrapper function in caret package of R language.\
    \ The resulted PLS-DA \nthree-dimensional component score plot, which shows the\
    \ overall data distribution of different treatments, pre-\nsents the preliminary\
    \ classification indicating modeling potentiality.\nDiscrimination model development.\
    \ \nBased on the purpose of multi-classification in this study, three \nclassification\
    \ modeling algorithms, Random Forests (RF), Neural Network (NN), and Support Vector\
    \ Machine \n(SVM) were selected to develop discrimination model and performed\
    \ with the randomForest, nnet, e1071 pack-\nages in R language, respectively.\n\
    RF was used as a supervised learning algorithm suitable for the case of high dimensional\
    \ data analysis. The RF \ncan handle multiple input variables from large databases\
    \ without variable selection and give estimates of impor-\ntant variables in the\
    \ classification25. RF modeling was performed by using the randomForest package.\
    \ Ntree (the \nnumber of trees) and mtry (the number of features) used to find\
    \ the best feature were required in RF. In this study \nthe RF model was trained\
    \ with ntree as 500 and mtry from 2, 5, and 8.\nNeural network have been successful\
    \ as predictive tools in variety domains such as uncovering \ngenotype-phenotype\
    \ interactions26. Researchers reported that properly trained deep neural networks\
    \ could \nDAT\nStage\nWeather condition\nImage acquisition \nschedule\n1\n1\n\
    C\nB\n2\nS\nB\n3\nS\nA\n4\nC\nA\n5\n2\nC\nB\n6\nC\nB\n7\nS\nA\n8\nC\nA\n9\n3\n\
    C\nB\n10\nC\nB\n11\nS\nA\n12\nC\nB\n13\n4\nC\nA\n14\nC\nB\n15\nS\nA\n16\nC\nB\n\
    17\n5\nC\nA\n18\nC\nB\n19\nC\nA\n20\nS\nB\nTable 5. Weather condition and image\
    \ acquisition schedule during growing period. Note: DAT - days after \ntransplanting,\
    \ S - sunny, C - cloudy; A: image acquisition scheduled from 6:00 h to 18:00 h\
    \ at 2 h intervals; B: \nimage acquisition scheduled only at 14:00 h.\nwww.nature.com/scientificreports/\n\
    11\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\ndiscover,\
    \ model, and disentangle latent factors for phenotyping27. For NN model in this\
    \ study, nnet package was \nused to train the single-hidden-layer feed-forward\
    \ neural network. There are two main parameters, decay and \nsize, which can be\
    \ tuned for NN model in R. In this study, the decay were tuned from 0, 0.1, and\
    \ 0.001, and the \nsize were tuned from 2, 5, and 9.\nThe goal of a SVM is to\
    \ create a boundary, called hyperplane, which leads to homogeneous partitions\
    \ of data \non either side. In this study, multiclass SVM model were trained with\
    \ kernel parameter C (from 10, 100, and \n1000) and regularization parameter gamma\
    \ (from 0.001, 0.01, 0.1, and 0). The best combination of C and gamma \nleading\
    \ to the highest prediction accuracy was chosen. For SVM, svm function in e1071\
    \ package was used to \nspecify the kernel function, cost, and the gamma function.\n\
    The 80% of total samples were selected as the training dastaset via stratified\
    \ random sampling method for \nmodel development and optimization and the rest\
    \ 20% as the test dataset. In order to prevent the over fitting \nproblems, all\
    \ three models were trained with the 10-fold cross validation in three repetitions\
    \ by using the caret \npackage in R language. This method randomly splits the\
    \ dataset in 10 subsets, among which 9 instances of the \ndata were used to train\
    \ the model and the other instance was used to validate the training model. By\
    \ performing \nthree repeats of the 10-fold validation, the average accuracy (percentage\
    \ of correctly classified samples out of all \nsamples) was used to assess the\
    \ performance of the model. However, only accuracy is not enough to evaluate the\
    \ \nmodel performance especially for imbalance data or multi class data. Therefore,\
    \ Cohen’s Kappa coefficient (k) and \nROC (Receiver operating characteristic)\
    \ curve were also used to evaluate the model performance. The k is a good \nmeasure\
    \ that can handle multi-class well and calculated as:\n=\n−\n−\np\np\np\nk\n1\n\
    o\ne\ne\nwhere, po is the observed agreement, and pe is the expected agreement.\
    \ The ROC curves of different models were \ncompared according to the area under\
    \ the curve (AUC) by plotting the true positive rate against the false positive\
    \ \nrate. AUC represents the probability that the classifier will assign a higher\
    \ score to a randomly chosen positive \nexample than to negative example28.\n\
    References\n 1. Incrocci, L. et al. Substrate water status and evapotranspiration\
    \ irrigation scheduling in heterogenous container nursery crops. \nAgricultural\
    \ Water Management 131, 30–40, doi:10.1016/j.agwat.2013.09.004 (2014).\n 2. McPherson,\
    \ R. A. A review of vegetation-atmosphere interactions and their influences on\
    \ mesoscale phenomena. Prog Phys Geog \n31, 261–285, doi:10.1177/0309133307079055\
    \ (2007).\n 3. Peterson, B. J. & Graves, W. R. Responses to Root-zone Water Content\
    \ of Shrub Congeners from Eastern North America and \nMediterranean California.\
    \ HortScience 48, 715–719 (2013).\n 4. Li, M., Zhang, K., Yin, Y. & Huang, D.\
    \ Correlation Research on Plug Weight and Substrate Water Content in Pakchoi Cultivation.\
    \ \nJournal of Changjiang Vegetables 12, 003%\\ 2017-2002–2015 2009:2055:2000\
    \ (2013).\n 5. Story, D. & Kacira, M. Design and implementation of a computer\
    \ vision-guided greenhouse crop diagnostics system. Machine Vision \nand Applications\
    \ 26, 495–506, doi:10.1007/s00138-015-0670-5 (2015).\n 6. Nishina, H. Development\
    \ of Speaking Plant Approach Technique for Intelligent Greenhouse. Agric Agric\
    \ Sci Proc 3, 9–13, \ndoi:10.1016/j.aaspro.2015.01.004 (2015).\n 7. Jones, H.\
    \ G. Irrigation scheduling: advantages and pitfalls of plant-based methods. Journal\
    \ of Experimental Botany 55, 2427–2436, \ndoi:10.1093/jxb/erh213 (2004).\n 8.\
    \ Kacira, M., Ling, P. P. & Short, T. H. Machine vision extracted plant movement\
    \ for early detection of plant water stress. Transactions \nof the Asae 45, 1147–1153\
    \ (2002).\n 9. Lea-Cox, J. D. Using wireless sensor networks for precision irrigation\
    \ scheduling (INTECH Open Access Publisher, 2012).\n 10. Raza, S. E. A., Prince,\
    \ G., Clarkson, J. P. & Rajpoot, N. M. Automatic Detection of Diseased Tomato\
    \ Plants Using Thermal and Stereo \nVisible Light Images. Plos One 10, doi:ARTN\
    \ e0123262 10.1371/journal.pone.0123262 (2015).\n 11. Philipp, I. & Rath, T. Improving\
    \ plant discrimination in image processing by use of different colour space transformations.\
    \ \nComputers and Electronics in Agriculture 35, 1–15, doi:10.1016/S0168-1699(02)00050-9\
    \ (2002).\n 12. Hendrawan, Y. & Murase, H. Neural-Intelligent Water Drops algorithm\
    \ to select relevant textural features for developing precision \nirrigation system\
    \ using machine vision. Computers and Electronics in Agriculture 77, 214–228,\
    \ doi:10.1016/j.compag.2011.05.005 \n(2011).\n 13. Alderfasi, A. A. & Nielsen,\
    \ D. C. Use of crop water stress index for monitoring water status and scheduling\
    \ irrigation in wheat. \nAgricultural Water Management 47, 69–75, doi:10.1016/S0378-3774(00)00096-2\
    \ (2001).\n 14. Chen, D. J. et al. Dissecting the Phenotypic Components of Crop\
    \ Plant Growth and Drought Responses Based on High-Throughput \nImage Analysis.\
    \ Plant Cell 26, 4636–4655, doi:10.1105/tpc.114.129601 (2014).\n 15. Li, L., Zhang,\
    \ Q. & Huang, D. F. A Review of Imaging Techniques for Plant Phenotyping. Sensors\
    \ 14, 20078–20111, doi:10.3390/\ns141120078 (2014).\n 16. Navarro, P. J., Perez,\
    \ F., Weiss, J. & Egea-Cortines, M. Machine Learning and Computer Vision System\
    \ for Phenotype Data \nAcquisition and Analysis in Plants. Sensors 16, doi:ARTN\
    \ 641 10.3390/s16050641 (2016).\n 17. Singh, A., Ganapathysubramanian, B., Singh,\
    \ A. K. & Sarkar, S. Machine Learning for High-Throughput Stress Phenotyping in\
    \ \nPlants. Trends in Plant Science 21, 110–124, doi:10.1016/j.tplants.2015.10.015\
    \ (2016).\n 18. Zhao, J. S., Bodner, G. & Rewald, B. Phenotyping: Using Machine\
    \ Learning for Improved Pairwise Genotype Classification Based on \nRoot Traits.\
    \ Frontiers in Plant Science 7, doi:ARTN 1864 10.3389/fpls.2016.01864 (2016).\n\
    \ 19. Umair, S. M. & Usman, R. Automation of irrigation system using ANN based\
    \ controller. International Journal of Electrical & \nComputer Sciences IJECS-IJENS\
    \ 10, 41–47 (2010).\n 20. Duro, D. C., Franklin, S. E. & Dube, M. G. A comparison\
    \ of pixel-based and object-based image analysis with selected machine \nlearning\
    \ algorithms for the classification of agricultural landscapes using SPOT-5 HRG\
    \ imagery. Remote Sensing of Environment \n118, 259–272, doi:10.1016/j.rse.2011.11.020\
    \ (2012).\n 21. Rahaman, M. M., Chen, D. J., Gillani, Z., Klukas, C. & Chen, M.\
    \ Advanced phenotyping and phenotype data analysis for the study \nof plant growth\
    \ and development. Frontiers in Plant Science 6, doi:ARTN 619 10.3389/fpls.2015.00619\
    \ (2015).\n 22. Behmann, J., Schmitter, P., Steinrücken, J. & Plümer, L. Ordinal\
    \ classification for efficient plant stress prediction in hyperspectral \ndata.\
    \ The International Archives of Photogrammetry, Remote Sensing and Spatial Information\
    \ Sciences 40, 29 (2014).\n 23. Goto, E. Plant Production in a Closed Plant Factory\
    \ with Artificial Lighting. Acta Hortic 956, 37–49 (2012).\nwww.nature.com/scientificreports/\n\
    12\nSCienTifiC RePORts | 7: 8303  | DOI:10.1038/s41598-017-08235-z\n 24. Huang,\
    \ D. F., Sun, L. L. & Zhang, K. Primary Study on Pakchoi Factory. International\
    \ Symposium on Soilless Cultivation 1004, 79–84 \n(2013).\n 25. Liaw, A. & Wiener,\
    \ M. Classification and regression by randomForest. R news 2, 18–22 (2002).\n\
    \ 26. Ritchie, M. D., Holzinger, E. R., Li, R. W., Pendergrass, S. A. & Kim, D.\
    \ Methods of integrating data to uncover genotype-phenotype \ninteractions. Nat\
    \ Rev Genet 16, 85–97, doi:10.1038/nrg3868 (2015).\n 27. Che, Z., Kale, D., Li,\
    \ W., Bahadori, M. T. & Liu, Y. Deep computational phenotyping. Proceedings of\
    \ the 21th ACM SIGKDD \nInternational Conference on Knowledge Discovery and Data\
    \ Mining. 507–516 (ACM).\n 28. Hand, D. J. Measuring classifier performance: a\
    \ coherent alternative to the area under the ROC curve. Machine Learning 77, \n\
    103–123, doi:10.1007/s10994-009-5119-5 (2009).\nAcknowledgements\nThis study was\
    \ supported in part by the National Natural Science Foundation of China (NSFC)\
    \ (Project No. \n31601214), National High-tech R&D Program of China (863 Program,\
    \ Project No. 2013AA103006), Science \nand Technology Commission of Shanghai Municipality\
    \ (Project No. 16391901700), and Shanghai Jiao Tong \nUniversity Agri-X Foundation.\
    \ Any opinions, findings, conclusions, or recommendations expressed in this \n\
    publication are those of the authors and do not necessarily reflect the view of\
    \ the National Natural Science \nFoundation of China, Ministry of Science and\
    \ Technology of China, Science and Technology Commission of \nShanghai Municipality,\
    \ or Shanghai Jiao Tong University.\nAuthor Contributions\nD.G., J.J., L.C. and\
    \ D.H. conceived and designed the experiments; D.G. and J.J. performed the experiments\
    \ \nand managed field trials: D.G., J.J. and J.Z. analyzed the data and produced\
    \ the figures. D.G. and J.Z. wrote the \nmanuscript. L.C., D.H. and J.Z. supervised\
    \ and coordinated work. All authors reviewed the manuscript.\nAdditional Information\n\
    Supplementary information accompanies this paper at doi:10.1038/s41598-017-08235-z\n\
    Competing Interests: The authors declare that they have no competing interests.\n\
    Publisher's note: Springer Nature remains neutral with regard to jurisdictional\
    \ claims in published maps and \ninstitutional affiliations.\nOpen Access This\
    \ article is licensed under a Creative Commons Attribution 4.0 International \n\
    License, which permits use, sharing, adaptation, distribution and reproduction\
    \ in any medium or \nformat, as long as you give appropriate credit to the original\
    \ author(s) and the source, provide a link to the Cre-\native Commons license,\
    \ and indicate if changes were made. The images or other third party material\
    \ in this \narticle are included in the article’s Creative Commons license, unless\
    \ indicated otherwise in a credit line to the \nmaterial. If material is not included\
    \ in the article’s Creative Commons license and your intended use is not per-\n\
    mitted by statutory regulation or exceeds the permitted use, you will need to\
    \ obtain permission directly from the \ncopyright holder. To view a copy of this\
    \ license, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s)\
    \ 2017\n"
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  pdf_link: https://www.nature.com/articles/s41598-017-08235-z.pdf
  publication_year: 2017
  relevance_score1: 0
  relevance_score2: 0
  title: Discrimination of plant root zone water status in greenhouse production based
    on phenotyping and machine learning techniques
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/rs14030638
  analysis: '>'
  authors:
  - Khadijeh Alibabaei
  - Pedro Dinis Gaspar
  - Tânia M. Lima
  - Rebeca M. Campos
  - Inês Girão
  - Jorge Monteiro
  - Carlos M. Lopes
  citation_count: 28
  full_citation: '>'
  full_text: ">\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\n\
    Citation: Alibabaei, K.; Gaspar, P.D.;\nLima, T.M.; Campos, R.M.; Girão, I.;\n\
    Monteiro, J.; Lopes, C.M. A Review of\nthe Challenges of Using Deep\nLearning\
    \ Algorithms to Support\nDecision-Making in Agricultural\nActivities. Remote Sens.\
    \ 2022, 14, 638.\nhttps://doi.org/10.3390/rs14030638\nAcademic Editor: Juan Ignacio\n\
    Arribas\nReceived: 21 December 2021\nAccepted: 26 January 2022\nPublished: 28\
    \ January 2022\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional\
    \ claims in\npublished maps and institutional afﬁl-\niations.\nCopyright:\n© 2022\
    \ by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open\
    \ access article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative\
    \ Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nremote sensing  \nReview\nA Review of the Challenges of Using Deep Learning\
    \ Algorithms\nto Support Decision-Making in Agricultural Activities\nKhadijeh\
    \ Alibabaei 1,2\n, Pedro D. Gaspar 1,2,*\n, Tânia M. Lima 1,2\n, Rebeca M. Campos\
    \ 3\n, Inês Girão 4\n,\nJorge Monteiro 1,2,5\nand Carlos M. Lopes 3\n1\nDepartment\
    \ of Electromechanical Engineering, University of Beira Interior, Rua Marquês\
    \ d’Ávila e Bolama,\n6201-001 Covilhã, Portugal; k.alibabaei@ubi.pt (K.A.); tmlima@ubi.pt\
    \ (T.M.L.); jorge@spaceway.pt (J.M.)\n2\nC-MAST Center for Mechanical and Aerospace\
    \ Science and Technologies, University of Beira Interior,\n6201-001 Covilhã, Portugal\n\
    3\nLinking Landscape, Environment, Agriculture and Food (LEAF), Instituto Superior\
    \ de Agronomia,\nUniversidade de Lisboa, 1349-017 Lisboa, Portugal; rmcampos@isa.ulisboa.pt\
    \ (R.M.C.);\ncarlosmlopes@isa.ulisboa.pt (C.M.L.)\n4\nCenter of Geographical Studies,\
    \ Institute of Geography and Spatial Planning, University of Lisbon,\n1600-276\
    \ Lisboa, Portugal; inesgirao@campus.ul.pt\n5\nSpaceWay, R. Pedro Nunes, 3030-199\
    \ Coimbra, Portugal\n*\nCorrespondence: dinis@ubi.pt\nAbstract: Deep Learning\
    \ has been successfully applied to image recognition, speech recognition,\nand\
    \ natural language processing in recent years. Therefore, there has been an incentive\
    \ to apply\nit in other ﬁelds as well. The ﬁeld of agriculture is one of the most\
    \ important ﬁelds in which the\napplication of deep learning still needs to be\
    \ explored, as it has a direct impact on human well-being.\nIn particular, there\
    \ is a need to explore how deep learning models can be used as a tool for optimal\n\
    planting, land use, yield improvement, production/disease/pest control, and other\
    \ activities. The\nvast amount of data received from sensors in smart farms makes\
    \ it possible to use deep learning as a\nmodel for decision-making in this ﬁeld.\
    \ In agriculture, no two environments are exactly alike, which\nmakes testing,\
    \ validating, and successfully implementing such technologies much more complex\n\
    than in most other industries. This paper reviews some recent scientiﬁc developments\
    \ in the ﬁeld of\ndeep learning that have been applied to agriculture, and highlights\
    \ some challenges and potential\nsolutions using deep learning algorithms in agriculture.\
    \ The results in this paper indicate that by\nemploying new methods from deep\
    \ learning, higher performance in terms of accuracy and lower\ninference time\
    \ can be achieved, and the models can be made useful in real-world applications.\
    \ Finally,\nsome opportunities for future research in this area are suggested.\n\
    Keywords: agriculture; deep learning; smart farm; support decision-making algorithms\n\
    1. Introduction\nEuropean landscapes have been transformed primarily by human\
    \ activities, so that\npristine vegetation and real wilderness are currently extremely\
    \ rare in the European Union\n(EU). Agriculture plays a particularly important\
    \ role in the development of European\nlandscapes, as it now accounts for almost\
    \ half of the total area of the EU [1]. On the other\nhand, we are already exceeding\
    \ the Earth’s capacity with the current type of agricultural\nproduction. Climate\
    \ change, degradation of soils, pollution, rising costs of groundwater\nand pump\
    \ irrigation, the transition from a fuel-based to a bio-based economy, the scarcity\n\
    of freshwater as demand increases, and other environmental and economic issues\
    \ will\nmake access to fresh food an ever-greater challenge [2,3].\nMoreover,\
    \ it has already been shown that some of the aforementioned practices such\nas\
    \ climate changes and scarcity of freshwater lead to stagnation and sometimes\
    \ even a\ndecline in production. Thus, the question arises whether transforming\
    \ all agricultural\nsystems into high-intensity farming systems alone is not counterproductive\
    \ in an attempt\nRemote Sens. 2022, 14, 638. https://doi.org/10.3390/rs14030638\n\
    https://www.mdpi.com/journal/remotesensing\nRemote Sens. 2022, 14, 638\n2 of 43\n\
    to solve the world’s food problems. It is understandable, then, that the implementation\
    \ of\npractices adapted to the reality of a given production will have positive\
    \ effects in terms of\nbetter use of resources and even a different approach to\
    \ their use.\nOne way to address this challenge is to use new technologies, supplement\
    \ workers\nwith robots and machines, and better address biodiversity parameters\
    \ and functions\nas well as ecosystem state. Smart farms include farm management\
    \ systems that use\nnew technologies to increase food safety, quality, and quantity\
    \ and reduce environmental\nimpact. Increased control of production leads to better\
    \ cost management and reduced\nwaste [2,4]. These farms use sensors to monitor,\
    \ control, and treat animals and plants [2,4,5].\nWith the help of sensors in\
    \ the ﬁeld, we can collect data such as soil mapping, climate\nchanges, weather\
    \ data, among others [2,4,5]. The combined analysis of this data can provide\n\
    information to make the appropriate management decisions. Artiﬁcial Intelligence\
    \ (AI)\nreceives signiﬁcant attention in the development of decision-making algorithms.\
    \ AI is any\ntechnique that enables machines to learn from experience, adapt to\
    \ new inputs, and mimic\nhuman behavior [6,7].\nMachine Learning (ML) is a sub-area\
    \ of AI that uses computational algorithms to\nconvert raw data from the real\
    \ world into useful models and decision advice. By using\nML models, the system\
    \ can automatically learn from previous experiences and improve\nitself. ML techniques\
    \ include Support Vector Machines (SVM), decision trees, Bayesian\nlearning, K-means\
    \ clustering, association rule learning, regression, neural networks, and\nmany\
    \ others [6]. Liakos et al. [8] presented an overview of the application of the\
    \ ML model\nin various agricultural activities of agriculture.\nDeep learning\
    \ (DL) is a subﬁeld of ML. The algorithms of DL are more complex than\ntraditional\
    \ ML models. In a network, the layers between input and output are called hidden\n\
    layers. A shallow network has one hidden layer, while a deep network has more\
    \ than one.\nWith multiple hidden layers, deep neural networks can learn the features\
    \ of data and solve\nmore complex problems [6,7]. Since the models of DL are faster\
    \ and more efﬁcient compared\nto the shallow algorithms in ML, and unlike them,\
    \ they can automatically extract features\nfrom the input data, they have been\
    \ the most widely used models in recent years [6,7].\nIn 2012, AlexNet [9] won\
    \ the LSVRC competition for classiﬁcation. Sermanet et al. [10]\nshowed that DL\
    \ models could be used for classiﬁcation, recognition, and localization and\n\
    achieved excellent results. These achievements encourage researchers to apply\
    \ DL models\nto other areas of human life, including agriculture.\nLiu et al.\
    \ [11] reviewed advances in combining edge intelligence and remote sensing\nwith\
    \ Unmanned Aerial Vehicles (UAV) in precision agriculture. This paper presented\
    \ a list\nof recent works using UAVs in combination with DL models. Since the\
    \ focus of the paper\nwas on UAV devices, this work was listed based only on the\
    \ application and model used in\nthe paper and did not include reviews and analysis\
    \ of the work.\nRegarding the diversity of environments in agriculture, the result\
    \ of a trained model\non one environment may not be transferable to others [12].\
    \ This makes the application of\ndeep learning models in agriculture more challenging\
    \ and requires more attention. This\npaper aims to review newly published works\
    \ to ﬁnd new techniques, challenges, and\npossible solutions for using deep learning\
    \ in agriculture since 2018. The following are the\nmain objectives of this paper:\n\
    •\nTo provide an overview of recent work and identify challenges related to data\
    \ acquisi-\ntion and preparation for deep learning models;\n•\nTo review of the\
    \ new DL model algorithms used in agriculture;\n•\nHighlight the challenges in\
    \ training the model;\n•\nExamine the challenges associated with applying the\
    \ trained model in the real world;\n•\nReview the new edge devices used to implement\
    \ the trained models.\nRemote Sens. 2022, 14, 638\n3 of 43\n2. Materials and Methods\n\
    The two most reputable databases for scientiﬁc papers, the Web of Science and\
    \ Sci-\nenceDirect, were searched for relevant journal articles and conference\
    \ proceedings. The\nterm “deep learning” was used in the keywords of the screening\
    \ titles with one of the\nfollowing keywords: “agriculture”, “plant diseases”,\
    \ “weed detection”, “yield estima-\ntion”, “fruit detection”, “species identiﬁcation”,\
    \ “crop classiﬁcation”, “soil management”,\n“automation and robotics” and “irrigation”.\n\
    The articles that did not deal with Deep Learning in agriculture were excluded.\
    \ In\nthe end, forty-six articles remained. Of these, ten articles were related\
    \ to plant diseases,\neleven articles were related to fruit detection, ﬁve articles\
    \ were related to weed detection,\nsix articles were related to species detection,\
    \ three articles were related to soil management,\nﬁve articles were related to\
    \ water management, and six articles were related to automation\nin agriculture.\n\
    The following criteria were considered in the analysis of the articles:\n•\nThe\
    \ way the dataset was collected for training the model and the challenges regarding\n\
    the dataset.\n•\nThe DL models/architectures used in the article and the performance\
    \ of the models.\n•\nThe metrics that were used to evaluate the model.\n•\nThe\
    \ inference time of the model (if speciﬁed), as it is a very important critical\
    \ for the\nuse of the model in the real-time application.\n•\nThe analysis of\
    \ the failure predicted by the model.\n•\nWhether the authors used a low-cost\
    \ device to deploy the trained model.\nThese articles base on the above criteria\
    \ are discussed in Section 3.8 and listed in the\nTables 1–9.\n3. An Overview\
    \ of Deep Learning\nDL models were inspired by human neural networks in the brain.\
    \ The word “deep”\nrefers to the number of hidden layers through which the data\
    \ is transformed. Through\ntraining, they send the input through a deep network\
    \ with different layers, where each layer\nhierarchically extracts speciﬁc features\
    \ at different scales or resolutions from the data and\ncombines them into a higher-level\
    \ feature, and uses these features to make predictions [6].\nDL models are divided\
    \ into supervised learning, unsupervised learning, and rein-\nforcement learning\
    \ [6]. Supervised learning is the task of learning a function from labeled\ntraining\
    \ data. In supervised learning, all data in the dataset is a pair consisting of\
    \ an input\nobject and the desired output value. A supervised learning model analyzes\
    \ the training\ndata and produces a function that can be used for prediction [6].\
    \ The most commonly\nused DL models included in this review are Convolutional\
    \ Neural Networks (CNN) and\nRecurrent Neural Networks (RNN). Unsupervised learning\
    \ was used in three papers.\n3.1. Convolutional Neural Network\nThe CNN model\
    \ is a supervised learning model speciﬁcally designed for classiﬁcation,\nrecognition,\
    \ and segmentation. A CNN model consists of a stack of convolutional layers,\n\
    nonlinear functions, pooling layers, and fully connected layers. A convolutional\
    \ layer\ncontains a set of kernels whose parameters must be learned and is used\
    \ to extract features\nfrom the data. Each kernel slides over the height and width\
    \ of the input, and the dot\nproduct between the kernel entries and each position\
    \ of the input is computed (Figure 1).\nThe output of a convolutional layer can\
    \ be calculated by Equation (1):\nyi = X ∗ Wi + bi,\ni = 1, · · · , m\n(1)\nwhere\
    \ X is the input of the layer, yi is the output corresponding to the i-th convolutional\n\
    kernel, Wi is a matrix of the trainable weights of the i-th kernel, bi is the\
    \ i-th bias, and m is\nthe number of kernels.\nRemote Sens. 2022, 14, 638\n4 of\
    \ 43\nAfter each convolutional layer, a nonlinear activation function is used\
    \ to introduce\nnonlinear features into the model. Rectiﬁed Linear Units (ReLU)\
    \ have become state-of-the-\nart and are the most commonly used activation function\
    \ in Deep Learning models [6]. A\npooling layer is usually located between two\
    \ convolutional layers and is used to reduce the\nnumber of parameters to minimize\
    \ overﬁtting [6,7].\nFigure 1. The left side image is the example of the convolutional\
    \ operation, and the right side is the\nexample of a max-pooling operation with\
    \ pooling size (2,2).\nThe ﬁnal layers consist of fully connected layers that\
    \ take the features extracted by the\nprevious layer and generate class probabilities\
    \ or scores. These layers are fully connected to\nall neurons in the previous\
    \ layer. Figure 2 shows the architecture of the well-known CNN\nmodel called VGG19\
    \ and the visualization of the last layer of the VGG19 model in plant\ndisease\
    \ classiﬁcation, respectively.\nconv1\nconv2\nconv3\nconv4\nconv5\nfc6\nfc7\n\
    fc8\n224 x 224 x 64\n112 x 112 x 128\n56 x 56 x 256\n56 x 56 x 256\n28 x 28 x\
    \ 512\n14x 14 x 512\n1 x 1 x 4096\n1 x 1 x 1000\nconvolutional + ReLU\nmax pooling\n\
    fully connected + ReLU\nsoftmax\n7x 7 x 512\nFigure 2. Architecture of the well-known\
    \ CNN model VGG19. The VGG19 network contains\nconvolutional layers, max-pool\
    \ layers, fully connected layers, and a soft-max classiﬁcation layer.\n3.1.1.\
    \ Classiﬁcation/Regression by CNN\nThe CNN model can be used for classiﬁcation\
    \ or regression problems.\nFor the\nclassiﬁcation task, the last layer of the\
    \ model is chosen as a fully connected layer with\nsoftmax activation function\
    \ and for the regression task as a fully connected layer, usu-\nally with linear\
    \ function. The most popular CNN architectures for classiﬁcation include\nAlexNet\
    \ [9], GoogleNet [13], VGG [14], and ResNet [15], as well as the new lightweight\n\
    models such as MobileNet [16], and EfﬁcientNet [17]. The lightweight models enable\n\
    to use mobile and low cost edge devices such as Jetson nano (https://developer.nvidia.\n\
    com/embedded/jetson-nano-developer-kit, accessed on 8 November 2021), Raspberrypi\n\
    (https://www.raspberrypi.com/, accessed on 15 November 2021) and Google TPU Coral\n\
    (https://coral.ai/, accessed on 8 November 2021) to deploy the trained model in\
    \ robots\nand use them in the ﬁeld [11,18] (Figure 3).\nRemote Sens. 2022, 14,\
    \ 638\n5 of 43\nFigure 3. From left to right: Jetson nano, Raspberrypi, and Google\
    \ TPU Coral.\n3.1.2. Detection by CNN\nThe CNN model can also be used for object\
    \ detection. There are two types of modern\nobject detection algorithms, the two-stage\
    \ detectors such as R-CNN [19], Fast R-CNN [20],\nand Faster R-CNN [21] as well\
    \ as single-stage detectors such as You Look Once (YOLO) [22],\nSingle Shot multi-box\
    \ Detector (SSD) [23], and LedNet [24]. In the two-stage detector,\nregions of\
    \ interest are generated in one stage using a Region Proposal Network (RPN). In\n\
    the other stage, the proposed regions are evaluated for object classiﬁcation and\
    \ bounding\nbox regression using convolutional layers. Such models achieve high\
    \ accuracy rates but\nare very slow [23]. Single-stage detectors, on the other\
    \ hand, contain a single feed-forward\nconvolutional network that directly provides\
    \ the bounding boxes and objects classiﬁcation.\nThe single-stage detectors are\
    \ faster in detection but less accurate than two-stage detectors,\nand it depends\
    \ on the application which model should be chosen. Figure 4 shows the\narchitecture\
    \ of Faster R-CNN and SDD, and Figure 5 shows the result of peach detection\n\
    using the Faster R-CNN model [25].\nVGG Conv\nlayers\nRPN\nclassifier\nA. Faster\
    \ R-CNN\nFeature maps\nFully connected\nlayers\nbackbone \nVGG-16\nconv\n1 x 1\
    \ x 128 \n3x 3x 256-S1\n1 x 1 x 128 \n3x 3x 256-S1\n512\n38 \n38 \nconv 4_3\n\
    Detection: 8732 per class\nclassifier:conv 3 x3 x(4x(classes+4))\nclassifier:conv\
    \ 3 x3 x(6x(classes+4))\n 3 x3 x(4 x(classes+4))\nB. SSD model\n1 x 1 x 256 \n\
    3x 3x 512-S2\n1 x 1 x 1024\n 3 x 3 x 1024\n1 x 1 x 128 \n3x 3x 256-S2\nExtra Conv\
    \ layers \nFigure 4. (A) Faster R-CNN model and (B) SSD model with VGG16 as the\
    \ backbone. In the faster\nR-CNN, a series of convolutional layers (Conv layer)\
    \ is used to extract features from the input, and\nthen an RPN algorithm is used\
    \ to ﬁnd the region of interest, but in the SSD model the features and\nbounding\
    \ box are extracted using only convolutional layers.\nRemote Sens. 2022, 14, 638\n\
    6 of 43\nFigure 5. Detection of peaches in the image using Faster R-CNN model.\
    \ Each bounding box contains\npeach and the score above the box shows the conﬁdence\
    \ score for each detected object [25].\n3.1.3. Segmentation by CNN\nThe segmentation\
    \ task can also be performed with CNN models. In segmentation, an\nimage is divided\
    \ into groups of pixels, and each group is assigned a class. Figure 6 shows\n\
    the segmentation of the grape in the image.\nFigure 6. The left side shows the\
    \ original image and the right side shows the task of segmentation for\nand the\
    \ right side shows the task of segmentation for grapes [26]. Reprinted with permission\
    \ from\nref. [26]. Copyright 2020 Elsevier.\nThe fully Convolutional Networks\
    \ (FCN) [27], Mask R-CNN [28], and Deeplab [29] are\nDL models for segmentation.\
    \ Lottes et al. [30] used an encoder-decoder Fully Convolutional\nNetwork (FCN)\
    \ model to identify crops and weeds during ﬁeld operations. Ghiani et al. [31]\n\
    used Mask R-CNN with ResNet101 which was trained with the dataset COCO, as a back-\n\
    bone for detecting grape branches in the tree.\n3.2. Recurrent Neural Network\n\
    RNN is specially designed for handling sequential data. In RNN, the output from\
    \ the\nprevious step is fed into the current step as input. The output of an RNN\
    \ unit is determined\nusing a tanh function by Equation (2):\nht = tanh(Wxxt +\
    \ Whht−1 + bt)\n(2)\nThe main problem with RNN is the vanishing gradient, i.e.,\
    \ the gradient of the loss\nfunction approaches zero [32]. Long Short-Term Memory\
    \ networks (LSTM) [33] is a special\ntype of RNN created to solve the vanishing\
    \ problem in RNN. A standard LSTM unit\nRemote Sens. 2022, 14, 638\n7 of 43\n\
    consists of an input gate (3), forget gate (4), an output gate (7), and a memory\
    \ cell (6).\nFigure 7 shows an LSTM unit and an RNN unit.\nit = σ(Wixxt + Wihht−1\
    \ + bi),\n(3)\nft = σ(Wf xxt + Wf hht−1 + bf ),\n(4)\nzt = tanh(Wzxxt + Wzhht−1\
    \ + bz),\n(5)\nct = ft ∗ ct−1 + it ∗ zt.\n(6)\not = σ(Woxxt + Wohht−1 + bo)\n\
    (7)\nThe weights (W) and biases (b) are trainable parameters that are adjusted\
    \ and opti-\nmized during the training of the model.\nFigure 7. (A) LSTM unit\
    \ diagram, (B) regular RNN unit. In an RNN unit, all input data is used for\n\
    output. However, in an LSTM unit, a set of gates is used to control the information\
    \ in memory and\ndecide how long and how much information may be stored.\n3.3.\
    \ Unsupervised Learning\nUnsupervised learning is a type of learning in which\
    \ hidden structures are found in\nunlabeled data with minimal human supervision\
    \ [6]. The primary method of unsupervised\nlearning is cluster analysis, such\
    \ as K-Means clustering. Cluster analysis groups or segments\ndatasets with common\
    \ attributes. In supervised learning, labeling a large amount of\nagricultural\
    \ data to train the model is expensive and time-consuming. Deep clustering\nmodels\
    \ can be used to avoid labeling and make the model more robust. Kang and Chen\
    \ [34]\nused automatic label generation by a clustering RCNN (C-RCNN) model to\
    \ identify the\napple on the trees. Tang et al. [35] used K-means clustering to\
    \ label the dataset and\nidentify weeds in the ﬁeld. Ferreira et al. [36] used\
    \ DeepCluster and Joint Unsupervised\nLearning from Deep Representations and Image\
    \ Clusters (JULE) to identify weeds in the\nﬁeld. However, many other new deep\
    \ clustering algorithms can be explored to avoid\nlabeling in agriculture [37].\n\
    3.4. Reinforcement Learning\nReinforcement Learning (RL) is concerned with how\
    \ an agent performs appropriate\nactions in an environment to maximize the rewards\
    \ in a given situation. RL agents must\ndetermine the best strategy to maximize\
    \ their expected cumulative rewards. An interactive\nenvironment receives the\
    \ current state and the action chosen by the agent and indicates\nthe next state\
    \ and reward (see Figure 8). This interaction between the DRL agent and the\n\
    training environment is repeated until the DRL agent converges to an optimal strategy\
    \ for\nchoosing an action.\nRemote Sens. 2022, 14, 638\n8 of 43\nFigure 8. Interaction\
    \ of the agent with the environment [38]. The agent chooses an action at in\n\
    the current state st based on a policy. The environment receives the action and\
    \ the current state\nand outputs the next state and the reward. Reprinted with\
    \ permission from ref. [38]. Copyright\n2022 Elsevier.\nDeep reinforcement learning\
    \ (DRL) combines deep learning and reinforcement learn-\ning [39]. As Bu and Wang\
    \ [39] mentioned, DRL is a promising model for building smart\nfarms. Chen et\
    \ al. [40] used Deep Q-Network to build a decision-making system for rice\nirrigation\
    \ based on weather forecasts.\n3.5. Data Acquired and Data Preparation\nAI models\
    \ convert raw data into usable models, so a training dataset is required to\n\
    train a model. The more data available, the better results are obtained. A dataset\
    \ can be\ncollected using advanced technologies such as sensors mounted on ground-based\
    \ platforms,\nsatellites, and unmanned aerial vehicles (UAVs) [41–46].\nA sensor\
    \ node is a specialized device capable of sensing physical parameters and\nprocessing\
    \ the sensed parameters using a microcontroller. In addition to processing, it\
    \ can\nalso transmit the processed data wirelessly to neighboring nodes or router\
    \ nodes using\ntransceiver [47]. Sensors mounted on ground-based platforms are\
    \ divided into hand-held,\nfree-standing in the ﬁeld (Figure 9), and agricultural\
    \ machinery-mounted sensors that can\nmeasure soil, plant, and weather characteristics\
    \ [41,42,48]. The advantages of ground-based\nsensors are that they are less expensive\
    \ than other sensors, they are not sensitive to humid\nconditions, and they are\
    \ more suitable for small farms. The disadvantage of these types of\nsensors is\
    \ that they are very time-consuming and, even when attached to farm machinery,\n\
    the information is very inefﬁcient for larger ﬁelds. Furthermore, some of these\
    \ sensors are\ninﬂexible in terms of the type of spectral data that can be collected\
    \ [49].\nFigure 9. From left to right: sensors mounted on ground-based platforms\
    \ to test vineyard stress\n(https://www.goodfruit.com/using-sensors-to-test-vineyard-stress/,\
    \ accessed on 9 December 2021),\nSentinel2 satellites (https://www.esa.int, accessed\
    \ on 3 November 2021), and unmanned aerial\nvehicles (UAVs: https://precisionagricultu.re/12-potential-uses-for-uas-in-agriculture/,\
    \ accessed\non 9 December 2021).\nSatellite remote sensing provides synoptic,\
    \ objective, and homogeneous data that\ncan be captured geographically and temporally,\
    \ and therefore could be an efﬁcient tool to\nprovide high-quality information\
    \ about agriculture over large areas, even across national\nRemote Sens. 2022,\
    \ 14, 638\n9 of 43\nborders [50]. Depending on the spatial and spectral resolution,\
    \ repetition frequency, and\ncost, the multitude of satellites and sensors orbiting\
    \ the Earth can be used to provide data\nfor many types of applications [48].\
    \ The main disadvantage of satellite imagery is that it is\nexpensive at high\
    \ spatial resolution. Another disadvantage is their ﬁxed schedule, thus the\n\
    data cannot be acquired in a speciﬁc time step. For example, the time step for\
    \ Sentinel2 is\nﬁve days. Another disadvantage of satellite images is that they\
    \ highly depend on climate\nand cannot be post-processed on cloudy days [48].\
    \ For a list of satellites launched since\n1970, we refer to [48,51].\nAn aircraft\
    \ system with no pilot on board that, once programmed, allows no outside\nintervention\
    \ during ﬂight. Originally intended for military use, they are now used in\nagriculture\
    \ to monitor crops and collect images and data from anywhere in the ﬁeld [11,49].\n\
    Unlike satellites, unmanned aerial systems can collect data at a speciﬁc time\
    \ step, and the\nresults of the collected data have a higher resolution than that\
    \ of satellites. Most unmanned\naerial systems can ﬂy in precipitation, but cloud\
    \ cover can affect the data unless a second\nsensor is used to measure the amount\
    \ of sunlight reaching the aircraft. The unmanned\naerial systems can also be\
    \ expensive, but there are models that are inexpensive. The\ndisadvantages of\
    \ unmanned aerial systems are the requirement of a human operator [49],\nlimited\
    \ ﬂight duration, the limited weight of cargo carried, energy consumption [41],\
    \ and\ndependence on weather conditions [41,52]. In addition, the processing of\
    \ data collected by\nUAS is higher than that of satellite imagery [49].\nIn addition\
    \ to using sensors, the dataset can also be collected from free online datasets\n\
    such as ImageNet, Microsoft COCO, and PlantVillage [46] or images can be generated\n\
    synthetically as described in [53] or by using simulation software in agriculture\
    \ such as\nAguaCrop and DSSAT [54]. For DL models, a large dataset is required\
    \ for training to\nextract appropriate features [55]. Collating and labeling a\
    \ large amount of data is time-\nconsuming. One strategy to overcome these challenges\
    \ is to increase the size of the dataset\nusing augmentation techniques [55].\
    \ Augmentation techniques artiﬁcially create different\nversions of a real dataset\
    \ to increase its size. These include rotation, blurring, scaling, and\nﬂipping\
    \ of images in the dataset [55,56].\nAnother strategy is transfer learning and\
    \ ﬁne-tuning the model [57]. In transfer\nlearning, the model is trained with\
    \ a large dataset such as ImageNet or COCO and then\nthe same model is reused\
    \ and re-trained for a similar task. If the dataset does not contain\nenough data\
    \ to train a model from scratch, this strategy can be used. Ghazi et al. [58]\n\
    investigated how ﬁne-tuning pre-trained models affects plant classiﬁcation results.\
    \ For\nthis purpose, the AlexNet, GoogLeNet, and VGGNet models were trained, once\
    \ from\nscratch, and another time using ﬁne-tuning from Caffe Models. The results\
    \ showed that\nﬁne-tuning the models improved their performance. Barbedo [55],\
    \ however, investigated\nhow the size and diversity of the dataset affect the\
    \ performance of DL techniques applied\nto plant diseases. The results showed\
    \ that even with transfer learning and augmentation\ntechniques, CNN might require\
    \ a large number of images to extract useful features from\nthe data. Figure 10\
    \ shows the transfer learning task.\nThe next challenge is how well the trained\
    \ datasets represent the real world. Ramcha-\nran et al. [12] trained the SSD\
    \ object detection model to identify three diseases in cassava.\nAlthough the\
    \ model was trained on the dataset collected in the ﬁeld with a camera, the\n\
    F1-score decreased by 32% on the images from other sources. Therefore, the main\
    \ focus\nwhen collecting datasets should be how well the dataset represents the\
    \ ﬁeld study.\nRemote Sens. 2022, 14, 638\n10 of 43\nA common Large\nDataset\n\
    Model\nLast layer 1\nPrediction1\nCustom Dataset\nHidden layers\nCustom Last layer\n\
    Prediction2\nTask 1\nTask 2\nTransfer  \nweights\nFigure 10. Transfer learning\
    \ task. The weights and biases of a previously trained model are transferred\n\
    to a similar model, the ﬁnal layers are replaced with modiﬁed new layers, and\
    \ then the model is\ntrained with a custom dataset.\n3.6. Training and Learning\n\
    In the training step of a DL model, the dataset is divided into the training set,\
    \ the\nvalidation set, and the test set. The training set is used to train the\
    \ model. The validation\ndataset is a sample of data retained by the training\
    \ model and used for model selection. The\nmodel structure is determined using\
    \ hyperparameters. Hyperparameters are parameters\nwhose values are set before\
    \ the learning process begins, such as the number of hidden\nlayers in the model,\
    \ the number of kernels in the CNN model, and the number of nodes in\neach LSTM\
    \ layer [32]. A test set is used to test a machine learning model after it has\
    \ been\ntrained on an initial training dataset. When the test set is used during\
    \ the training of the\nmodel, it does not provide information about how the network\
    \ will perform on unseen\ndata, and the evaluation of the result has no statistical\
    \ signiﬁcance [32]. Figure 11 shows\nthe general steps from preprocessing the\
    \ data to ﬁnalizing the model for prediction.\nThe error of a model is deﬁned\
    \ as the difference between the actual value the predicted\noutput, and it must\
    \ be minimized in the training step. The function that is used to compute\nthis\
    \ error is known as the loss function. It can be categorized into two types: Classiﬁcation\n\
    and Regression Loss. The loss function used for the classiﬁcation contains Hinge\
    \ loss,\nExpositional loss, cross-entropy, Log loss, and many more. For the regression\
    \ problems,\nthe loss function such as Mean Square Error (MSE), Mean Absolute\
    \ Error (MAE), Huber\nloss, and log cosh loss can be used. Each of these gives\
    \ a different error for the same\nprediction. The choice of a loss function depends\
    \ on the algorithm. Training DL models\nare based on backpropagation [59]. Backpropagation\
    \ uses the chain rule and computes the\nloss function’s gradient concerning the\
    \ network’s weights and biases for a single input–\noutput example, adjusted network\
    \ weights, and biases. The optimization algorithm must\nbe selected to minimize\
    \ the loss function. The choice of optimization algorithm and loss\nfunctions\
    \ are critical. The same as loss function, there are several ways to optimize\
    \ the error,\nsuch as Stochastic Gradient Descent (SGD), SGD with momentum, Root\
    \ Means Square\npropagation (RMSprop), and Adam [6]. Zhang et al. [60] built up\
    \ an experience to compare\nthe performance between SGD and Adam in identifying\
    \ tomato leaf disease. The ResNet\nwith the SGD optimization method obtained the\
    \ highest test accuracy of 96.51%. Figure 12\nshows the learning steps of a DL\
    \ model.\nRemote Sens. 2022, 14, 638\n11 of 43\nCollect, clean, and label data\n\
    \       training set     \ntest set \nvalidation\nset\nTrain the model\nEvaluate\
    \ the model using\nperformance metricsl\nPrediction using\ntrained model\nacceptable\n\
    result?\nYes\nNo\nSelect the model and\nhyperparameters\nChange\nhyperparameters\n\
    Data preprocessing (scaling, normalization, etc.)\nSelect the loss function,\n\
    optimization algorithm, and\nand performance metrics\n1. Dataset preparation\n\
    2. Training process \n3. Prediction \nFigure 11. Flowchart for DL algorithm tasks.\
    \ The test data is obtained by splitting the original\ndataset, which is not used\
    \ for training and validation. The training data is passed to the model to\nperform\
    \ automatic feature extraction and validation to evaluate the model during training\
    \ and tune\nthe hyperparameters.\nInitial weights and\nbiases randomly\nCompute\
    \ the output\nfrom successive layers\nfor an input batch\nCompute the error\n\
    using the loss function\nThe error is\nacceptable?\nNo\nCompute the gradients\
    \ w.r.t\nweights and biases using the\nbackpropagation algorithm\nUpdate weights\
    \ and biases\nw.r.t. gradients using the\noptimization algorithm\nYes\nFinish\
    \ \nStart\nFigure 12. During training, the parameters of the model (weights and\
    \ biases) are modiﬁed using the\nbackpropagation and optimization algorithms to\
    \ minimize loss.\nRemote Sens. 2022, 14, 638\n12 of 43\n3.7. Evaluating the Models\n\
    Once the model is trained, the most crucial question is how good the model is.\
    \ There\nare several metrics for evaluating DL models. The most commonly used\
    \ metrics in this\nreview for the regression problems are:\n•\nMSE: is the average\
    \ of the squared differences between prediction and actual observation.\n•\nRoot\
    \ Mean Square Error (RMSE): is the square root of MSE.\n•\nMAE: is the average\
    \ of the absolute differences between prediction and actual obser-\nvation.\n\
    •\nR2: is a statistical measure calculated from the variance explained by the\
    \ model versus\nthe total variance. The higher R2 indicates, the smaller the differences\
    \ between the\nobserved data and the ﬁtted values.\nThe metrics that can be used\
    \ in evaluating a classiﬁcation or segmentation problem\nare as follows:\n•\n\
    Accuracy (AC) is the ratio of correctly predicted observation of the total observations.\n\
    •\nRecall (R): also known as sensitivity, measures how many actual positives capture\
    \ by\nthe model correctly.\n•\nPrecision (P): also known as a positive predictive\
    \ value, measures how accurate the\nprediction is in classiﬁcation, i.e., the\
    \ correct prediction.\n•\nThe Average Precision (AP) is given by the area under\
    \ the precision–recall curve\nabove. Since Precision and Recall always lie between\
    \ 0 and 1, AP also lies between 0\nand 1. The Mean Average Precision (mAP) is\
    \ calculated by taking the mean AP over\nall classes.\n•\nF1-score: is the weighted\
    \ average of Precision and Recall.\n•\nIntersection Over Union (IOU): is an evaluation\
    \ metric used to measure an object\ndetector’s accuracy on a particular dataset.\n\
    •\nArea Under the ROC Curve (AUC): measures the area underneath the entire ROC\
    \ curve.\n3.8. Brief Review of Papers\n3.8.1. Disease Detection\nPlant diseases\
    \ cause losses in agricultural production and endanger food security. The\nmost\
    \ common practice in pest and disease control is to spray the crop area evenly\
    \ with\npesticides [8]. This method requires signiﬁcant amounts of pesticides,\
    \ which results in\nhigh ﬁnancial and human resources and signiﬁcantly changes\
    \ the environment. The use\nof a deep learning algorithm that detects the exact\
    \ time, location, and affected crops and\nsprays the pesticides only on the affected\
    \ plants can reduce resource consumption and\nenvironmental changes.\nKerkech\
    \ et al. [61] used the SegNet model to segment and detect grapevine diseases\n\
    using images with RGB and infrared ranges. The dataset was acquired using a UAV\n\
    device with two MAPIR Survey2 camera sensors, including a visible light (RGB)\
    \ sensor\nset for automatic illumination and an infrared sensor. The dataset was\
    \ labeled using a\nsemi-automatic method, i.e., a sliding window to identify potentially\
    \ diseased areas. Then,\neach block was classiﬁed by a LeNet5 network for pre-labeling.\
    \ In the end, the labeled\nimages were manually corrected. The SegNet recognizes\
    \ four classes including the shaded\nareas, soil, healthy and symptomatic vines.\
    \ Two models were trained, one for the RGB\nimages and the other for the infrared\
    \ images. The segmentation results of the two models\nwere also fused in two ways.\
    \ The ﬁrst case was called “Fusion AND”, which means that\nthe symptom is considered\
    \ detected if it is present in both the RGB and infrared images.\nThe second case\
    \ is called “fusion by the union” and has the symbol ”fusion OR”, which\nmeans\
    \ that the symptom is considered detected if it is present in either the RGB or\
    \ the\ninfrared image. The model trained with RGB images (AC = 85.13) outperformed\
    \ the model\ntrained with infrared images (78.72). The fusion AND had the best\
    \ performance, and the\nfusion OR had the worst accuracy. The runtime of SegNet\
    \ on a UAV image was reported to\nRemote Sens. 2022, 14, 638\n13 of 43\nbe 140\
    \ s for visible and infrared images. The fusion between the two segmented images\n\
    takes less than 2 s.\nKerkech et al. [62] used a CNN model to detect Esca disease\
    \ in grapevine using UAV\nRGB images. A CNN model was trained with different combinations\
    \ of patch sizes 16 × 16,\n32 × 32 and 64 × 64 with different color spaces including\
    \ RGB, HSV, LAB, and YUV, and\nvegetation indices such as ExG, ExR, ExGR, GRVI,\
    \ NDI, and RGI. All the different color\nspaces and vegetation indices used in\
    \ this study can be calculated from the RGB images.\nThe results show that the\
    \ CNN model trained with the RGB and YUV color spaces has a\nbetter performance\
    \ compared to the models trained with HSV and LAB. It was pointed\nout that the\
    \ lower accuracy of the models trained with HSV could be due to the Hue (H)\n\
    channel in HSV, which combines all color information into a single channel and\
    \ is less\nrelevant for the network to learn the best color features. The LAB\
    \ color space has one\nluminance channel (L) and two chrominance channels, which\
    \ do not reproduce the colors\nof the diseased vineyard well.\nIn the next experiment,\
    \ vegetation indices were added to the RGB and YUV data.\nCombining vegetation\
    \ indices with RGB and YUV improved classiﬁcation results in most\ncases. The\
    \ ﬁnal investigation concerned the models trained by combining the vegetation\n\
    indices alone. The combination of ExR, ExG, and ExGR vegetation indices with a\
    \ size of\n16 × 16 gave the best performance among the other inputs (including\
    \ color space) and sizes\nwith an accuracy of 95.80%. Furthermore, the combination\
    \ of YUV and ExGR vegetation\nindices with sizes of 32 × 32 and 64 × 64 achieved\
    \ similar performance but the run-time\nwas longer.\nFrom the results, the color\
    \ of the images and leaves is very important in the detection\nof grapevine diseases.\
    \ Furthermore, the number of channels in the input does not affect the\nrun-time\
    \ of the model, but the size of the input and the model structure do.\nBarbedo\
    \ [55] investigated how dataset size and diversity affect the performance of\n\
    DL techniques applied to plant diseases. An image dataset of leaves with a small\
    \ number\nof samples for the CNN model was used to investigate the behavior of\
    \ GoogLeNet under\ndifferent conditions. Data augmentation and transfer learning\
    \ methods were used to\ntrain the CNN. The results showed that even with transfer\
    \ learning and augmentation\ntechniques, CNN requires a large number of images\
    \ to extract useful features from the data.\nEven though a large number of images\
    \ can be easily acquired with the new technology,\nlabeling the dataset is time-consuming.\
    \ One option proposed in this paper was to share\nthe dataset, but as mentioned\
    \ earlier, the two environments are not the same in agriculture.\nThe effect of\
    \ removing the background of the images on the accuracy of the model was\nalso\
    \ investigated. The model has trained again with the images without background.\
    \ The\nmodel has trained again with the images without background. The results\
    \ show different\nbehaviors with respect to the accuracy of the model, including\
    \ no signiﬁcant effect, a\nsigniﬁcant improvement in accuracy in some cases, a\
    \ signiﬁcant decrease in accuracy, and\nmixed results (improved accuracy in some\
    \ diseases while the error rate increased in others).\nFrom the signiﬁcant decrease\
    \ in accuracy for some plants, it was inferred that the CNN\nmodel sometimes uses\
    \ the background of the model to classify the images. An attempt that\ncan be\
    \ made here is to train the model with both datasets (with and without background)\n\
    and investigate the accuracy of the model in this case.\nFerentinos (2018) [63]\
    \ used AlexNet, AlexNetOWTBn [64], GoogLeNet, Overfeat [10],\nand VGG models to\
    \ identify 25 plant disease in 58 different classes of (plant, disease) com-\n\
    binations, including some healthy plants. The dataset used was images of healthy\
    \ and in-\nfected leaves of the plants from the Plantvillag database (https://github.com/spMohanty/\n\
    PlantVillage-Dataset, accessed on 17 November 2021). More than 37.3% of the images\
    \ in\nthe dataset were taken under real cultivation conditions in the ﬁeld, and\
    \ the other images\nwere taken under laboratory conditions. In the ﬁrst experiment,\
    \ the number of images\nacquired under laboratory conditions and real conditions\
    \ was kept similar in the training\nand test set, and the models were trained\
    \ using this dataset. The VGG model performed\nbest on the test set with an accuracy\
    \ of 99.53%.\nRemote Sens. 2022, 14, 638\n14 of 43\nThey also investigated the\
    \ signiﬁcance of the presence of ﬁeld-captured images in\nthe training set. From\
    \ the 58 available classes of the form (plant, disease), the 12 that\ncontained\
    \ images of both types were selected. Two experiments were conducted with\nthese\
    \ 12 classes, and two CNN models were developed: one was trained with images\n\
    under laboratory conditions and tested with images under ﬁeld conditions, and\
    \ another\nwas trained with images under ﬁeld conditions and tested with images\
    \ under laboratory\nconditions. Although the number of images acquired under ﬁeld\
    \ conditions was less than\nthe number of images acquired under laboratory conditions,\
    \ the CNN model trained only\nwith images under ﬁeld conditions performed better\
    \ with an accuracy of 68% than the\nCNN model trained only with images under laboratory\
    \ conditions with an accuracy of\n33%. This result shows the importance of the\
    \ presence of the images acquired under ﬁeld\nconditions. From the misclassiﬁcation\
    \ image, they point out some problematic situations,\nincluding images with extensive\
    \ partial shading on the leaves, images with multiple objects\nin addition to\
    \ the image, images where the leaf occupies a very small and non-centric part\n\
    of the frame, and images without leaves.\nJiang et al. [65] implemented the CNN\
    \ models to detect ﬁve common apple leaf\ndiseases using images from the ﬁeld.\
    \ By applying data augmentation, such as rotational\ntransformations, horizontal\
    \ and vertical ﬂips, intensity disturbances, and Gaussian noise,\n26,377 disease\
    \ images were generated. The problem with SDD is that it cannot detect\na small\
    \ object, and also, an object can be detected multiple times. To overcome these\n\
    drawbacks, they developed an SSD model with a VGG-INCEP as the backbone, where\n\
    two GoogLeNet inception layers replace two convolutional layers of the VGG model.\n\
    Moreover, the structure of feature extraction and fusion is designed by applying\
    \ the\nRainbow concatenation method instead of the pyramid (which is used in the\
    \ SSD model) to\nimprove the performance of feature fusion. The results showed\
    \ that the data augmentation\nimproved the accuracy of the model by 6.91% compared\
    \ to the original dataset. Moreover,\nthe proposed model achieved the best performance\
    \ compared to faster R -CNN and SSD\nwith VGG and Rainbow SSD (RSSD) model.\n\
    To investigate the effect of using a deep model as a backbone, ResNet-101 was\
    \ used\nas a feature extractor for SSD. The results show that ResNet-101 does\
    \ not lead to any\nimprovement. In terms of speed, Faster RNN was the slower model\
    \ with more accuracy.\nThe proposed model with 78.80% mAP and 23.13 Frames Per\
    \ Second (FPS) was more\naccurate than SDD and RSSD, but SSD outperformed the\
    \ other models in terms of time\ninference. By examining the misclassiﬁed images,\
    \ it was indicated that similarity between\ndiseases, misclassiﬁed background,\
    \ and light condition were the challenges in classiﬁcation.\nKarthik et al. [66]\
    \ proposed a Residual Attention Network for disease detection in\ntomato leaves.\
    \ The main idea of attention is to focus on the relevant parts of the input to\n\
    produce outputs. The dataset was collected from the open-source website Plantvillage\
    \ and\ncontained one healthy class and three diseases. They implemented three\
    \ models. One was\na traditional CNN model, the second used the three residual\
    \ layers introduced into the\nCNN model as part of the ResNet architecture, and\
    \ the last used an attention mechanism\non top of the residual CNN for effective\
    \ feature learning. The traditional CNN-based\nmethods focus on ordered feature\
    \ learning, starting from basic image-level features such\nas edges, color, etc.,\
    \ to complex texture-based differences [66]. In the deeper layer, some\nimportant\
    \ features extracted in the ﬁrst layer may be lost. The residual layers are designed\n\
    to avoid this problem [66]. They concatenate the extracted features from the earlier\
    \ layers\nwith the deeper layers. In addition, the attention mechanism is used\
    \ to extract the relevant\nparts of the feature maps. The Residual Attention Network\
    \ CNN performed better with an\noverall accuracy of 98% than the Vanilla Residual\
    \ Network with an accuracy of 95% and\nthe traditional CNN model with an accuracy\
    \ of 84%.\nRemote Sens. 2022, 14, 638\n15 of 43\nLiu et al. [67] implemented the\
    \ model Cascade Inception to detect four common apple\nleaf diseases in images\
    \ captured in the ﬁeld. The Cascade Inception was a modiﬁed AlexNet\nmodel with\
    \ inception layers from GoogleNet. Various data enhancement methods such as\n\
    image rotation, mirror symmetry, brightness adjustment, and PCA jittering were\
    \ applied to\nthe training images. Moreover, the fully connected layers were replaced\
    \ by convolutional\nlayers, which results in fewer parameters and avoids overﬁtting.\
    \ The proposed model was\ntrained using the optimization method Nesterov Accelerated\
    \ Gradient (NAG) and achieved\nan accuracy of 97.62%. The performance of the proposed\
    \ model was compared with\nSVM and BP neural networks, standard AlexNet, GoogLeNet,\
    \ ResNet-20, and VGGNet-\n16. Transfer learning method was used to train VGGNet-16\
    \ and achieved 96% accuracy.\nStandard AlexNet, GoogLeNet, and ResNet-20 were\
    \ trained from scratch using SGD-\noptimization and achieved a maximum accuracy\
    \ of 95.69%. The SVM and BP, which\nachieved an accuracy of less than 60%, show\
    \ that the traditional approaches rely heavily\non the expert developed classiﬁcation\
    \ features to improve the recognition accuracy. They\nalso investigated the effect\
    \ of data augmentation methods and optimization algorithms\non accuracy. The model\
    \ with the SGD optimizer achieved 93.32% accuracy, while the\nmodel with NAG achieved\
    \ 97.62% accuracy. The data augmentation methods improved the\nperformance of\
    \ the model by 10.83%. The advantage of the model is that it outperformed\nother\
    \ CNN models in terms of training time and memory required. Furthermore, the\n\
    number of parameters of the model was less than AlexNet and GoogleNet. One point\n\
    that emerges from the paper is that GoogleNet and AlexNet were trained using the\
    \ SGD\noptimizer, and the proposed model was trained using the NAG method, but\
    \ as mentioned\nin the paper, SGD has the “local optimum” problem. In addition,\
    \ the models were not\ncompared based on the inference time.\nRamcharan et al.\
    \ [12] trained the SSD object detection model to identify three diseases,\ntwo\
    \ types of pest damage, and nutrient deﬁciency in cassava at the mild and pronounced\n\
    stages. A dataset was collected from the ﬁeld, which was divided into 80–20 as\
    \ training\nand testing sets. The model achieved 94 ± 5.7% mAP on the test dataset.\
    \ The trained model\nwas used on a mobile phone to investigate the performance\
    \ of the model in the real world.\nOne hundred twenty images of leaves were captured\
    \ using a mobile device of the study\nexperiment, and the model inference was\
    \ run on a desktop and mobile phone to calculate\nthe performance metrics of the\
    \ trained model. The results show that the average precision\nof the model on\
    \ the real dataset decreases by almost 5%, but the average recall decreases\n\
    by almost half, and the F1-score decreases by 32%. Furthermore, the results show\
    \ that the\nmodel performs better on the leaves with pronounced symptoms than\
    \ on the leaves where\nthe symptoms are only mildly pronounced.\nRemote Sens.\
    \ 2022, 14, 638\n16 of 43\nTable 1. Feature descriptions of recent published papers\
    \ in the ﬁeld of “Disease Detection”.\nReferences\nApplication\nData Used\nModel\n\
    Metric Used\nModel Performance\nKerkech et al. [61]\nDetect Esca disease in grapevine\n\
    using UAV RGB images.\nThe images were collected using\na UAV system with an\n\
    RGB sensor.\nA CNN model was trained with\ndifferent combinations of patch\nsizes\
    \ and different color spaces.\nAC\nThe CNN model trained with the RGB\nand YUV\
    \ color spaces has a better\nperformance compared to the models\ntrained with\
    \ HSV and LAB. Moreover,\nThe model was obtained by combining\nYUV and RGB trained\
    \ with vegetation\nindices gave better performances than\nthe models trained with\
    \ YUV and RGB.\nKerkech et al. [62]\nSegmentation of the plant\nsymptomatic area\n\
    A Quadcopter drone with two\ncamera sensors\nSegNet\nP, R, F1, AC\nThe model trained\
    \ with RGB images\noutperformed the model trained with\ninfrared images.\nBarbedo\
    \ [55]\nDisease detection on 12 plants\nwith a variety of diseases.\nThe images\
    \ were taken with a\nvariety of digital cameras and\nmobile devices.\nGoogleNet\n\
    AC\nCNN’s AC using the original image as\nthe dataset was 84% and using the\n\
    removed background was 87%.\nFerentinos (2018) [63]\nIdentiﬁcation of 25 plant\
    \ disease\nin 58 different classes of\n(plant, disease).\nhttps:\n//github.com/spMohanty/\n\
    PlantVillage-Dataset, accessed on\n17 November 2021\nAlexNet, AlexNetOWTBn,\n\
    GoogLeNet, Overfeat, and\nVGG models\nAC\nThe VGG model performed best on the\n\
    test set. Furthermore, the CNN model\ntrained only with images under ﬁeld\nconditions\
    \ performed better than the\nCNN model trained only with images\nunder laboratory\
    \ conditions.\nJiang et al. [65]\nIdentiﬁcation of apple\nleaf diseases.\nImages\
    \ were taken in the ﬁeld,\nand some of them were in\nthe laboratory.\nSSD with\
    \ base VGG-INCEP.\nmAP\nSSD with VGG-INCEP as base achieved\nbetter performance\
    \ with an mAP of\n78.80% compared to FR-CCN and SSD\nwith VGG and ResNet as the\
    \ base.\nRemote Sens. 2022, 14, 638\n17 of 43\nPicon et al. [68] used DL model\
    \ to detect seventeen diseases of ﬁve crops (wheat,\nbarley, corn, rice, and rapeseed)\
    \ in images captured in the ﬁeld. The dataset contained\nseveral challenges, such\
    \ as multiple diseases on the same plant, similar visual symptoms\namong diseases,\
    \ images of early and early-stage diseases, and diseases of leaves, and stems.\n\
    To improve the accuracy of the model, the crop ID was used in the network. The\
    \ crop ID\nwas deﬁned as a categorical vector with K components, where K is the\
    \ number of crops in\nthe model. Several models were trained. The ﬁrst approach\
    \ was to train an independent\nmodel for each crop and the second approach was\
    \ to train a single model for the entire\ndataset. The results show that the multi-crop\
    \ model had a similar performance to splitting\nthe training dataset into the\
    \ different crops (1% increased accuracy). However, the class of\ndiseases with\
    \ fewer images in the training set may beneﬁt from the multi-crop model. The\n\
    second approach was to add the crop ID to the multi-crop model. The results show\
    \ that by\nadding crop id, the model can still beneﬁt from more images for training,\
    \ while crop ID\ninformation helps the network to discriminate between similar\
    \ diseases.\nChen et al. [69] used pre-trained MobileNet V2 networks to identify\
    \ twelve rice\nplant diseases. The dataset was collected from online sources and\
    \ real agricultural ﬁelds.\nAn attention mechanism was added to the model, and\
    \ transfer learning was performed\ntwice during model training to achieve better\
    \ performance. The MobileNet-V2 achieved\n94.12% accuracy on the PlantVillage\
    \ dataset, while the MobileNet-V2 with attention and\ntransfer learning achieved\
    \ 98.26%. The ﬁve CNNs such as MobileNetv1, MobileNet-V2,\nNASNetMobile, EfﬁcientNet-B0,\
    \ and DenseNet121 were selected for comparison with\nthe proposed models. In addition,\
    \ two other networks named MobileNetv2-2stage and\nMobileAtten-alpha were trained.\
    \ In the MobileNetv2-2stage model, transfer learning was\nperformed twice to identify\
    \ the images of plant diseases.\nSimilarly, in MobileAtten-alpha, the attention\
    \ method was used instead of transfer\nlearning twice. The proposed model (using\
    \ attention and transfer learning twice) achieved\nthe the second best accuracy\
    \ of 98.84%. The DenseNet121 with an accuracy of 98.93%\noutperformed other models.\
    \ MobileNetv2-2stage and MobileAtten-alpha achieved an\naccuracy of 98.68% and\
    \ 96.80%, respectively. The proposed model was trained with images\nfrom the ﬁeld\
    \ and achieved an accuracy of 89.78%.\n3.8.2. Fruit Detection and Yield Forecast\n\
    Yield forecasting is one of the most important and popular topics in precision\
    \ agricul-\nture because it is the basis for yield mapping and estimation, supply\
    \ and demand matching,\nand crop and harvest management [8]. Modern approaches\
    \ go far beyond simple forecast-\ning based on historical data but incorporate\
    \ methods from DL to provide a comprehensive\nmultidimensional analysis of crop,\
    \ climatic and economic conditions to maximize yields.\nRemote Sens. 2022, 14,\
    \ 638\n18 of 43\nTable 2. Feature descriptions of recent published papers in the\
    \ ﬁeld of “Disease Detection”.\nReferences\nApplication\nData Used\nModel\nMetric\
    \ Used\nModel Performance\nKarthik et al. [66]\nIdentifying the type of infection\n\
    in tomato leaves.\nPlant Village Dataset.\nResidual CNN with attention.\nAC\n\
    Residual CNN with attention\nwith an AC of 98% achieved\nbetter performance compared\
    \ to\nCNN and residual CNN.\nLiu et al. [67]\nIdentiﬁcation of 4 apple leaves\n\
    diseases.\nImages were taken in ﬁeld using\ndigital color camera.\nCNN (AlexNet+\
    \ +Inception\nlayers from GoogleNet).\nAC\nTheir model with an AC of\n97.62% performed\
    \ better than\nGoogLeNet, ResNet-20,\nVGGNet16, SVM, and BP,\nAlexNet.\nRamcharan\
    \ et al. [12]\nIdentiﬁcation of cassava pests\nand diseases.\nDataset from [70]\n\
    SSD\nF1-score\nThe F1-score decreased by 32%\nwhen moving from the test set to\n\
    real images.\nPicon et al. [68]\nIdentiﬁcation of 17 diseases and\nﬁve crops by\
    \ adding metadata to\nthe model.\nImages were taken by cell phone\nin real ﬁeld\
    \ wild conditions.\nCNN (ResNet50\nAC, R2, Negative and positive\npredictive value\n\
    Adding plants species\ninformation to the model by\nconcatenating plant information\n\
    in the embedding layer improved\nthe performance of the model\n(AC = 0.98)\nChen\
    \ et al. [69]\nIdentify rice plant diseases from\ncollected real-life images.\n\
    Online sources and agricultural\nﬁelds.\nMobileNet-V1, MobileNet-V2,\nDenseNet-121,\
    \ NASNetMobile,\nEfﬁcientNet-B0, proposed model.\nAC, R, P, F1-score\nThe proposed\
    \ model with an\naccuracy of 0.98 out- performed\nthe other models.\nRemote Sens.\
    \ 2022, 14, 638\n19 of 43\nSilver and Monga [71] trained ﬁve CNN models to estimate\
    \ grape yield from RGB\nimages taken in a vineyard on harvest day using the camera\
    \ of a Samsung Galaxy S3. The\nimages of 40 grapevines were split into two parts,\
    \ one for the left cordon and one for the\nright cordon, resulting in a total\
    \ of 80 cordons. A simple CNN was trained by inputting\nthe RGB images of the\
    \ left and right cordons and estimating the grape yield. The second\nmodel was\
    \ the same as the ﬁrst model, but the input to the model was the right cordon\n\
    images and the inversion of the left cordon images to look similar to right cordon\
    \ images.\nThe third model, an autoencoder network, is trained to a high level\
    \ of accuracy and the\nCNN encoder weights are transferred as starting weights\
    \ into the CNN model for the yield\nestimation model. In the fourth model, an\
    \ autoencoder model was trained to output the\ndensity map of the grapes in the\
    \ image. Then, the weights of the encoder are transferred as\ninitial parameters\
    \ into the CNN model for yield estimation. The last model was trained\nwith the\
    \ output of the autoencoder for the density map as input to the CNN model for\
    \ yield\nestimation instead of the RGB images. The CNN models with ﬂipped images\
    \ outperformed\nthe simple CNN model with an MAE % of 15.43. The models of transfer\
    \ learning from\nDensity Map Network Representation with MAE % of 11.79 achieved\
    \ the best performance\namong the other models. The last model with MAE % of 15.99\
    \ did not perform as well as\nthe fourth model because the accuracy of the density\
    \ map estimates was quite low. The\nresults show that transfer learning, when\
    \ used properly, can improve recognition accuracy.\nAguiar et al. [72] trained\
    \ SSD MobilenetV1 and the Inception model to detect Grape\nBunch in Mid Stage\
    \ and early stages and then transferred the trained model to the TPU-\nEdge device\
    \ to investigate the temporal accuracy of the model. The same strategy in [18]\n\
    was used to collect the dataset and it is publicly available (https://doi.org/10.5281/zenodo.\n\
    5114142, accessed on 3 November 2021) with 1929 vineyard images and their annotation.\n\
    Overall, SSD MobileNet-V1 performed better than the Inception model, as it had\
    \ a higher\nF_1 score, AP, and mAP. The early stage was more difﬁcult to detect\
    \ than the middle growth\nstage. The ﬁrst class represented smaller clusters that\
    \ were more similar in color and\ntexture to the surrounding foliage, making them\
    \ more difﬁcult to detect. SSD MobileNet-V1\nshowed an AP of 40.38% in detecting\
    \ clusters at an early growth stage and 49.48% at a\nmedium growth stage. In terms\
    \ of time, SSD MobileNet-V1 was more than four times faster\nthan the Inception\
    \ model on TPU-Edge Device.\nGhiani et al. [31] used Mask R-CNN with ResNet101\
    \ as a backbone which was pre-\ntrained with the dataset COCO (https://cocodataset.org/#home,\
    \ accessed on 18 November\n2021) for detecting grape branches on the tree. An\
    \ open-source dataset GrapeCS-ML [73]\ncontaining more than 2000 images without\
    \ annotation of ﬁfteen grape varieties at different\nstages of development in\
    \ three Australian vineyards was used to train the model. In\naddition to the\
    \ GrapeCS-ML dataset, 400 images were collected from the island of Sardinia\n\
    (Italy). The result obtained by applying the detector to the test samples was\
    \ an mAP value\nof 92.78%. To investigate the generalizability of the proposed\
    \ model, the model trained on\nthe GrapeCS-ML dataset was tested on its internal\
    \ dataset. The dataset contained different\ngrape varieties, vegetation, and colors\
    \ than the GrapeCS-ML dataset. An mAP of 89.90%\nwas obtained with the internal\
    \ dataset, indicating that the model can be used in other ﬁelds.\nTo investigate\
    \ the signiﬁcance of the size of the dataset used for training and the importance\n\
    of the augmentation techniques, the size of the original dataset was reduced to\
    \ 10% of the\ntraining images in one case with augmentation and the other case\
    \ without augmentation of\nthe dataset. The mAP was reduced by up to 5%, especially\
    \ in the experiments performed\nwithout augmentation. It was observed that the\
    \ recognition accuracy decreased for images\nwith overlap between clusters.\n\
    In Milella et al. [74], a system for the automatic estimation of harvest volume\
    \ and for\ndetecting grapes in vineyards using an RGB-D sensor on board an agricultural\
    \ vehicle has\nbeen proposed. An RGB-D sensor is a special type of depth detection\
    \ device that works in\nconjunction with an RGB camera and is able to add depth\
    \ information to the conventional\nimage pixel by pixel. The approach to determine\
    \ the crop volume involved three steps: the\n3D reconstruction of the vine rows,\
    \ the segmentation of the vines using a deep learning\nmethod, and the estimation\
    \ of the volume of each plant. In the ﬁrst step, the depth output\nRemote Sens.\
    \ 2022, 14, 638\n20 of 43\nfrom the camera was not used because the parameters\
    \ of the algorithm are ﬁxed and cannot\nbe conﬁgured. Instead, the semi Global\
    \ Matching (SGM) algorithm was used, which is a\ncomputer vision algorithm for\
    \ estimating a dense disparity map from a rectiﬁed stereo\nimage pair. A box-grid\
    \ ﬁlter is then used to merge the point clouds.\nIn segmentation, a segmentation\
    \ algorithm was ﬁrst used to separate the canopy from\nthe background using the\
    \ green–red vegetation index (GRVI), and then k-means were\nused to identify each\
    \ plant. Based on the results of the clustering algorithm, different\nplants were\
    \ calculated and an estimate of the volume per plant was performed. Finally, the\n\
    pre-trained AlexNet, VGG16 and VGG19, and GoogLeNet were trained to perform grape\n\
    cluster detection in 5 different classes (grapes, vineyard stakes, vine stems,\
    \ cordons, canes,\nleaves, and background). The VGG model performed the best with\
    \ an accuracy of 91.52.\nSantos et al. [26] used deep learning models and computer\
    \ vision models to estimate\ngrape wine yield from RGB images. Images were captured\
    \ with a Canon EOS REBEL T3i DSLR\ncamera and a Motorola Z2 Play smartphone from\
    \ five different grape varieties. The dataset\nnamed Embrapa Wine Grape Instance\
    \ Segmentation Dataset (WGISD) with 300 RGB images\nis publicly available (https://doi.org/10.5281/zenodo3361736,\
    \ accessed on 18 November\n2021). Models from DL such as Mask R-CNN, YOLOv2, and\
    \ YOLOv3 were trained to detect\nand segment grapes in the images. Then, an image\
    \ processing algorithm called Structure-\nfrom-Motion (SfM) was used to perform\
    \ spatial registration, integrating the data generated by\nthe CNN-based step.\
    \ In the final step, the results of the CV model were used to remove the\nclusters\
    \ detected in different images to avoid counting the same clusters in several\
    \ images. The\nMask R-CNN with ResNet101 as the backbone outperformed YOLOv2 and\
    \ YOLOv3 in terms\nof object detection, but the YOLO model outperformed the Mask\
    \ R-CNN in terms of detection\ntime. The worst performance was obtained with YOLOv3.\
    \ To verify the performance of Mask\nR-CNN+SfM, 500 key-frames of a video were\
    \ used, and the result is shown in a video at\nhttps://youtu.be/1Hji3GS4mm4, accessed\
    \ on 17 November 2021.\nPalacios et al. [75] applied the method of deep learning\
    \ to detect the flowering of\nthe vine and used it for the estimation of early\
    \ yield estimation. Images of six grapevine\nvarieties were acquired using a mobile\
    \ platform developed at the University of La Rioja.\nThe RGB camera was a Canon\
    \ EOS 5D Mark IV RGB with a full-frame CMOS sensor. The\nground truth was acquired\
    \ using the algorithm in [76]. This algorithm was developed to\nprocess only images\
    \ with a single inflorescence and a dark background. In the first step,\nSegNet\
    \ was used with VGG (VGG16 and VGG19) as a backbone to segment and extract\nthe\
    \ inflorescences contained in the images. Then, these regions were used to count\
    \ the\nflowers in each inflorescence using three algorithms, including SegNet,\
    \ Watershed Flower\nSegmentation, and a linear model. The SegNet with VGG19 as\
    \ backbone outperformed the\nmodel with VGG16 in terms of IOU and F1-score. For\
    \ flower recognition, the SegNet model\nwith VGG19 was trained to classify a group\
    \ of flowers per image into three classes including\ncontour, center, and background.\
    \ After segmentation, false-positive filtering of flower\nsegmentation was performed.\
    \ Here, the flowers whose center and contour was segmented,\nand whose contour\
    \ surrounded the center above a certain threshold were considered as\ntrue positives.\
    \ The F1-score reached its highest value when contour filtering was set to 50%,\n\
    resulting in an F1-score of 0.729. The best F1-score for the watershed approach\
    \ was 0.708\nand the worst performance in counting flowers was obtained with linear\
    \ regression in the\nform of the Root Mean Square metric. The number of flowers\
    \ counted using SegNet-VGG19\nfor inflorescence extraction and flower detection,\
    \ and flower counting using the algorithm\nof [76] showed a correlation with R2\
    \ close to or above 0.75 for all cultivars.\nRemote Sens. 2022, 14, 638\n21 of\
    \ 43\nTable 3. Feature descriptions of recent published papers in the ﬁeld of\
    \ “Fruit detection and Yield estimation”.\nReferences\nApplication\nData Used\n\
    Model\nMetric Used\nModel Performance\nSilver and Monga [71]\nEstimate grape yield\
    \ from RGB\nimages.\nImages were taken in of a\nvineyard on harvest day using\n\
    the camera of a Samsung Galaxy\nS3.\nFive CNN models from scratch\ntrained with\
    \ ﬁve different\ntechniques for training\nMAE\nThe models of transfer learning\n\
    from Density Map Network\nRepresentation. with MAE% of\n11.79 achieved the best\n\
    performance among the other\nmodels.\nAguiar et al. [72]\nDetect Grape Bunch in\
    \ Mide\nStage and early stages.\nThe images were acquired from\nfour different\
    \ vineyards in\nPortugal.\nSSD Inception-V2, SSD\nMobileNet- V1\nIOU, mAP, R\n\
    The SSD Inception-V2 had higher\nprecision than the SSD\nMobileNet-V1 at all conﬁdence\n\
    values, but lower recall. In terms\nof time, SSD MobileNet-V1 was\nmore than four\
    \ times faster than\nthe Inception\nGhiani et al. [31]\nDetecting grape branches\
    \ in the\ntree\nGrapeCS-ML [73] and 400 images\nwere collected from ﬁeld\nMask\
    \ R-CNN with ResNet101 as\nback- bone\nIOU, mAP\nThe model achieved an mAP\nvalue\
    \ of 92.78%\nIn Milella et al. [74]\nEstimation of harvest volume\nand for detecting\
    \ grapes in\nvine-yards\nImages was collected from ﬁeld\nusing RGB-D sensor\n\
    AlexNet, VGG, GoogleNet\nP, R, AC, TP (true positive)\nThe VGG model performed\
    \ the\nbest with an accuracy of 91.52%.\nSantos et al. [26]\nEstimate grape wine\
    \ yield from\nRGB images.\nImages were captured with a\ncamera from ﬁve different\
    \ grape\nvarieties.\nMask R-CNN and YOLOv2 and\nYOLOv3\nP, R, F1-score\nThe Mask\
    \ R-CNN with ResNet\n101 as the back-bone\noutperformed YOLOv2 and\nYOLOv3 in\
    \ terms of object\ndetection, but the YOLO model\noutperformed the Mask R-CNN\n\
    in terms of detection time.\nPalacios et al. [75]\nDetect the ﬂowering of the\
    \ vine,\nand used it for the estimation of\nearly yield estimation\nImages of\
    \ six grapevine varieties\nwere acquired using a mobile\nplatform\nSegNet model\
    \ with VGG19\nIOU and F1-Score\nThe number of ﬂowers counted\nusing SegNet- VGG19,\
    \ and\nﬂower counting using the\nalgorithm of [76] showed a\ncorrelation with\
    \ R2 close to or\nabove 0.75 for all cultivars.\nRemote Sens. 2022, 14, 638\n\
    22 of 43\nKang and Chen [34] implemented DaSNet-v2, which is an ’Encoder–Decoder\
    \ with\natrous convolution developed in Deeplab-v3+ to detect and segment the\
    \ apple in an\norchard for harvesting by a robot. Atrous convolution is a type\
    \ of convolutional layer that\nallows control of the resolution of the features\
    \ computed by the CNN. The dataset was\ncollected from an apple orchard as RGB-D\
    \ and RGB. The RGB-D was used to visualize the\nenvironment. A lightweight model,\
    \ Resnet 18, was used as the backbone of the DaNet-v2\nto ensure its deployment\
    \ on the Jetson-TX2 with limited computing capacity. In addition,\nthe model was\
    \ trained with the Resnet50 and Resnet101 backbones. The performance of\nthe model\
    \ with the Resnet101 backbone was compared with DaSNet-v1, YOLO-v3, faster-\n\
    RCNN, and the Mask-RCNN. DaSNet-v2 and Mask-RCNN with F1-score of 0.873 and\n\
    0.868, respectively, outperformed the other models. However, DaSNet-v2 outperformed\n\
    mask-RCNN with a computational efﬁciency between 306 and 436 ms with a time of\n\
    1.3 s. The results also show that single-stage detection models such as Yolo have\
    \ better\ncomputational efﬁciency compared to two-stage detectors. The model was\
    \ implemented\non Jetson-TX2, a lightweight backbone of Resnet-18, and the experimental\
    \ results show\nthat DaSNet-v2 with Resnet-18 can achieve similar performance\
    \ in recall and precision of\ndetection compared to YOLO-v3. Environmental factors\
    \ such as strong sunlight reﬂection,\nshadows, and appearance variations of fruits\
    \ in color, shape, occlusion, or viewing angle\ncould lead to false-negative detection\
    \ results. RGB-D images were processed using DaSNet-\nv2 and the PPTK toolbox,\
    \ a Python package for image visualization, to deploy the robot in\nthe orchard.\n\
    Koirala et al. [77] developed a DL model, called Mango-YOLO, based on YOLO-v3\n\
    and YOLO-v2 (tiny) for counting mangoes on trees. YOLOv2 (tiny) has a small architecture\n\
    with only nine convolutional layers, six pooling layers, and one detection layer,\
    \ sacriﬁcing\naccuracy for speed. YOLOv3 [22] is based on Darknet-53 and improves\
    \ upon YOLOv2 [78].\nThe Mango YOLO had 33 layers compared to 106 layers in YOLO-v3\
    \ and 16 layers in YOLO-\nv2 (tiny). The reduction in the number of layers is\
    \ expected to reduce computation time\nand detect mangoes more accurately. The\
    \ model was trained on the dataset collected from\nﬁve orchards. The Mango-YOLO\
    \ achieved better performance with an accuracy of 0.967%\ncompared to YOLO-v2\
    \ (tiny) (0.9% ) and YOLO-v3 (0.951%). In terms of time inference,\nthe Mango\
    \ YOLO with 15 ms was faster than YOLO-v3 (25 ms) and slower than YOLO-v2\n(tiny)\
    \ (10 ms). Moreover, Mango-YOLO was trained once from scratch on the augmented\n\
    dataset, and the second time transfer learning was used using the COCO dataset.\
    \ The\nmodels had the same performance on the test set, and the reason was explained\
    \ by the fact\nthat the COCO dataset does not contain Mango images. The false\
    \ detection over images\ntaken with Canon camera shows resizing of the images\
    \ and results in image distortion\nwith leaves taking a curved shape resembling\
    \ the fruit and overexposed areas on branches,\ntrunks, and leaves.\nLiang et\
    \ al. [79] applied the SSD network to detect mango and almond on tree fruits.\n\
    The dataset in [80] was used to train the model. The SSD model used the original\
    \ and\nsampled a patch such that the minimum Jaccard overlap with the objects\
    \ is 0.1, 0.3, 0.5,\n0.7, or 0.9, and ﬁnally randomly sampled the input image.\
    \ The size of each sampled area\nand the minimum Jaccard overlap are critical\
    \ for object detection. These two parameters\nwere changed to adopt the SSD model\
    \ for small mango detection. Using VGG16 as the\nbasic framework for the SDD outperformed\
    \ the SSD with ZFNet. Furthermore, using the\ninput image size of 400 × 400 and\
    \ 500 × 500 as the input of the model, the model achieved\nbetter performance\
    \ than SSD with an input size of 300 × 300. The model outperformed the\nfaster\
    \ RCNN in terms of speed and accuracy. The challenges related to mango detection\n\
    on the tree were considered in the paper: the size of mango on the whole image,\
    \ blocking\nthe mango with leaves, branches, resizing the original image, making\
    \ mango smaller, and\nsimilarity between mango and background.\nTian et al. [81]\
    \ developed YOLO-V3 with DenseNet as the backbone to detect apples\non trees.\
    \ They used two datasets for training. The ﬁrst one contained images of apples\
    \ at\none growth stage, and the second one contained images taken at different\
    \ growth stages.\nRemote Sens. 2022, 14, 638\n23 of 43\nThe model showed better\
    \ performance for mature apples than for young and growing\napples because the\
    \ color features were more prominent, the individual volume was larger,\nand there\
    \ was less overlap. The results also showed that the F1-score of the model trained\n\
    with the ﬁrst dataset was higher than that of the model trained with the second\
    \ dataset.\nThe performance of the trained model decreased for images with partial\
    \ occlusion of apples\nwith branches and leaves but is still an acceptable result\
    \ (IOU = 0.889 for mature apples ).\nThe model achieved the best performance compared\
    \ to Faster R-CNN, YOLOV2, and\nthe original YOLOv3. In terms of time inference,\
    \ the model was faster than Faster R-CNN\nand similar to YOLOv3. The F1-score\
    \ and IOU of the model without data augmentation\nmethods decreased by 0.033 and\
    \ 0.058, respectively.\nZhou et al. [82] implemented an SSD model with two lightweight\
    \ backbones Mo-\nbileNetV2 and InceptionV3, to develop an Android APP called KiwiDetector\
    \ that detects\nkiwis in the ﬁeld. Quantization is a technique for performing\
    \ computations and storing\ntensors with bit widths smaller than ﬂoating-point\
    \ numbers. When training a neural net-\nwork, 32-bit ﬂoating-point weights and\
    \ activation values are typically used. A quantized\nmodel performs some or all\
    \ operations with tensors using integers instead of ﬂoating-\npoint values. This\
    \ allows the computational complexity to be reduced and the trained\nmodel can\
    \ be used on devices with lower resource requirements. The quantization method\n\
    was used to compress the model size and improve the detection speed by quantizing\n\
    the weight tensor and activation function data of convolutional neural networks\
    \ from 32\nto bit ﬂoating-point numbers to an 8-bit integer. The results showed\
    \ that MobileNetV2,\nquantized MobileNetV2, InceptionV3, and quantized InceptionV3\
    \ achieve a true detec-\ntion rate of 90.8%, 89.7%, 87.6%, and 72.8%, respectively.\
    \ The quantized MobileNetV2 on\nthe Huawei P20 smartphone outperformed the other\
    \ models in terms of detection speed\n(103 ms/frame) and size. Although the SSD\
    \ with MobileNetV2 was more accurate than the\nSSD with quantized MobileNetV2,\
    \ the SSD with quantized MobileNetV2 was 37% faster.\nThe problem with kiwi detection\
    \ was that overlapping kiwis were reported, which the\nmodel counts as one kiwi.\n\
    3.8.3. Weed Detection\nBesides disease, weeds are considered to be a prevalent\
    \ threat to agricultural produc-\ntion. These are plants considered undesirable\
    \ in a particular situation, as they may compete\nwith crops for sunlight and\
    \ water, resulting in crop and economic loss. One signiﬁcant\nproblem in weed\
    \ control is that they are difﬁcult to detect and distinguish from crops\n[8].\
    \ DL algorithms can improve weed detection and discrimination at a lower cost\
    \ with\nreduced environmental problems and side effects. These technologies could\
    \ power robots\nthat detect and remove weeds.\nBah et al. [83] proposed a CNN\
    \ model with unsupervised training dataset annotation\ncollection for weed detection\
    \ in UAV images of bean and spinach ﬁelds. They assumed that\ncrops are grown\
    \ in regular rows and that plants growing between the rows are considered\nweeds.\
    \ The Hough transform was applied to the skeleton to detect the rate of plant\
    \ rows,\nand then a simple linear iterative clustering (SLIC) algorithm was applied\
    \ to create a\nmarker and delineate the plant rows. This algorithm generated superpixels\
    \ based on\nk-mean clustering. After row detection, a blob coloring algorithm\
    \ was used to identify the\nweeds. The unsupervised training dataset was used\
    \ to train ResNet18 for weed detection\nin the images. The supervised learning\
    \ method performed 6% better than the unsupervised\nlearning method in the bean\
    \ ﬁeld and about 1.5% better in the spinach ﬁeld. The low\nnumber of weeds between\
    \ rows may explain the difference in performance in the bean\nﬁeld. The performance\
    \ of the model was compared with SVM and RF. In general, ResNet18\nshows better\
    \ performance in supervised and unsupervised learning methods than SVM\nand RF.\n\
    Remote Sens. 2022, 14, 638\n24 of 43\nTable 4. Feature descriptions of recent\
    \ published papers in the ﬁeld of “Fruit detection and Yield estimation”.\nReferences\n\
    Application\nData Used\nModel\nMetric Used\nModel Performance\nKang and Chen [34]\n\
    Counting apple on tree.\nThe images were taken in the\nﬁeld with a camera.\nC-RCNN+LedNet\
    \ with a\nlight-weight (LW) as backbone,\nresnet-50, resnet-101, darknet-53.\n\
    IOU, P, R, F1-score\nLedNet with resnet-101 achieved\nan AC of 0.864 and LedNetwith\n\
    LW achieved an Ac of 0.853.\nKoirala et al. [77]\nCounting mango on trees.\nThe\
    \ images were taken in the\nﬁeld.\nCNN (Mango-YOLO)\nF1-score\nMango-YOLO with\
    \ an F1-score of\n0.97 achieved better performance\ncompared to faster R-CNN,\
    \ SSD,\nand YOLO.\nLiang et al. [79]\nDetection of mango and almond\nfruits on\
    \ the tree.\nhttps:\n//arxiv.org/abs/1610.03677,\naccessed on 15 November 20221\n\
    SSD with VGG-16 and ZFNet\nIOU, F1-score\nThe model outperformed the\nfaster RCNN\
    \ in terms of speed\nand accuracy.\nTian et al. [81]\nDetection of apple in image\
    \ and\nyield estimation.\nImage capture was performed\nwith a camera in the ﬁeld.\n\
    Improved YOLO-V3 with\nDenseNet as the backbone.\nF1-score\nThe proposed model\
    \ with an\nF1-score of 0.817 had better\nperformance compared to\nYOLO-V2, YOLOV3,\
    \ and Faster\nR-CNN.\nZhou et al. [82]\nDetection of kiwi fruit in the\norchard\
    \ with android\nsmartphones.\nCollected from the orchard.\nMobilNetV2, quantized\n\
    MobileNetV2, InceptionV3,\nquantized InceptionV3.\nIOU, TDR, FDR\nQuantized MobileNetV2\n\
    outperformed the other models\nin terms of accuracy, speed,\nand size.\nRemote\
    \ Sens. 2022, 14, 638\n25 of 43\nFerreira et al. [36] implemented the unsupervised\
    \ learning models JULE and Deep-\nCluster to detect weeds in the ﬁeld. The JULE\
    \ consists of stacked multiple combinations of\nconvolutional layer, batch normalization\
    \ layer, ReLU layer, and pooling layer. AlexNet and\nVGG16 were implemented as\
    \ the basis of the DeepCluster model to extract features, and\nthen K-means is\
    \ used as the clustering algorithm. Two datasets, Grass-Broadleaf [84] and\nDeepWeeds\
    \ [85] were used in this work. The DeepCluster model performed better than\nthe\
    \ JULE model on both datasets with a large number of clusters. The DeepCluster\
    \ with a\nbase of Alexnet and VGG achieved similar performance, with Alexnet performing\
    \ better\non DeepWeed and VGG on the other dataset. To investigate the effect\
    \ of transfer learning\non unsupervised learning, the pre-trained VGG and AlexNet\
    \ were used on ImageNet. The\npre-trained model did not improve the accuracy of\
    \ Grass-Broadleaf, but it did improve the\naccuracy of DeepWeed. The data augmentation\
    \ also improved the accuracy of the unsuper-\nvised learning methods. They also\
    \ used semi-supervised data labeling. Semi-supervised\nlearning is a machine learning\
    \ approach that deals with the use of labeled and unlabeled\ndata. In the semi-supervised\
    \ method, labeled images from the DeepCluster model were\nused to train Inception-V3,\
    \ VGG, and ResNet. Inception-V3 and VGG outperformed ResNet\non the Grass-Broadleaf\
    \ and DeepWeeds dataset, respectively.\nMilioto et al. [86] modiﬁed Encoder–Decoder\
    \ CNN architecture in [87] to distinguish\nweeds from crops and soil. The number\
    \ of convolutional layers was decreased to reduce\ntime inference, and additional\
    \ vegetation indices (14 channels) were included in the input\nfor more accurate\
    \ detection. The dataset of three ﬁelds was used. Three networks were\ntrained\
    \ with different inputs: one with RGB images, another with RGB and near-infrared\n\
    (NIR) images, and the last one was trained with 14 channels such as RGB, Excess\
    \ Green\n(ExG), Excess Red (ExR), color index of Vegetation Extraction (CIVE),\
    \ and Normalized\nDifference Index (NDI). To investigate the generalization ability\
    \ of the proposed model,\nthe model was trained on the images of one ﬁeld and\
    \ tested on images of all ﬁelds. The\nresults indicate that feeding these 14-channels\
    \ into the input can speed up the training\nand improve the performance of the\
    \ model on the unseen dataset compared to the model\ntrained on RGB images and\
    \ RGB+NRI but still, the recall and precision of the model on the\nunseen ﬁeld\
    \ drop sharply (11–50%).\nAnother experience was conducted to investigate the\
    \ generalization capacity of the\nproposed model. The trained model was retrained\
    \ on the unseen dataset with 10, 20, 50, or\n100 images in the training set. The\
    \ performance of the RGB network when retrained with\n100 images is almost the\
    \ same as the performance of the proposed model trained with ten\nimages. The\
    \ inference time of the proposed model on PC and Jetson TX2 platform was also\n\
    found to be 44 ms and 210 ms, respectively, which is slower than the model trained\
    \ with\nRGB images with 31 ms and 190 ms, respectively.\nLottes et al. [30] developed\
    \ an encoder–decoder Fully Convolutional Network (FCN)\nwith the sequential model\
    \ for weed detection in sugar beet ﬁelds. The encoder–decoder\nFCN was used to\
    \ extract features from the input images, and the sequential model processed\n\
    the ﬁve images in a sequence using 3D convolution and output a sequence code that\
    \ was\nused to learn sequential information about the weeds in ﬁve images in a\
    \ sequence. The\nresults showed that the encoder-decoder with a sequential model\
    \ improved the F1-score of\nthe module by almost 11–14% compared to the encoder-decoder\
    \ FCN. The results indicated\nthat the changes in the visual appearance of the\
    \ images in the training and test dataset\ncould lead to a decrease in model performance,\
    \ and adding additional information, such as\nvegetation indices, leads to better\
    \ generalization for other ﬁelds.\nRemote Sens. 2022, 14, 638\n26 of 43\nTable\
    \ 5. Feature descriptions of recent published papers in the ﬁeld of “Weed Detection”.\n\
    References\nApplication\nData Used\nModel\nMetric Used\nModel Performance\nBah\
    \ et al. [83]\nWeed detection in bean and\nspinach ﬁelds.\nThe images were taken\
    \ with a DJI\nPhantom 3 Pro drone.\nSimple linear iterative clustering\n(unsupervised)+\
    \ CNN (Resnet18)\nAC\nCNN trained with unsupervised\nlabeling achieved an AC of\
    \ 88.73\n(spinach) and 94.34 (bean), and with\nsupervised labeling achieved an\
    \ AC\nof 94.84 (spinach) and 95.70 (bean).\nFerreira et al. [36]\nIdentiﬁcation\
    \ of weeds with\nunsupervised clustering.\nThe Grass-Broadleaf Dataset\nin [84]\
    \ and DeepWeed in [85] .\nJULE, DeepCluster\nAC, NMI\nDeepCluster achieved better\n\
    performance than JULE.\nMilioto et al. [86]\nIdentiﬁcation of weeds\n(additional\
    \ vegetation indices\nadded to the input).\nData was taken with a 4-channel\n\
    RGB+NIR camera.\nMask R-CNN.\nIOU, mIOU\nThe model achieved better\nperformance\
    \ by adding an extra\nchannel at the input of the model\n(IOU = 80.8%)\nLottes\
    \ et al. [30]\nCrop and weed detection in the\nsugar ﬁeld.\nDataset was taken\
    \ with robots\nequipped with a 4-channel with\nRGB+NIR camera\nEncoder–Decoder\
    \ FCN\n(FC-DenseNet) sequential model.\nAverage F1-score\nThe proposed model achieved\
    \ better\nperformance with a F1-score of 92.3\ncompared to encoder-decoder FCN\n\
    without a sequential model, random\nforests, and vanilla FCN.\nWang et al. [88]\n\
    Pixel-wise segmentation of ﬁeld\nimages into soil, crop, and weeds.\nTwo datasets\
    \ used: the ﬁrst\nDataset in [89] and oilseed image\nacquired in the ﬁeld\nEncoder-decoder\
    \ CNN with\ndifferent inputs.\nAC, IOU\nThe model with RGB input with an\nAC of\
    \ 96.06 on dataset1 and 96.12 on\ndataset2 achieved the best\nperformance. Image\
    \ enhancement\nimproved the results.\nRemote Sens. 2022, 14, 638\n27 of 43\nWang\
    \ et al. [88] used the encoder–decoder with Atrous Convolution for pixel-wise\n\
    semantic segmentation of crops and weeds. The two datasets for sugar beet and\
    \ oilseed\nincluded in the paper were taken under completely different lighting\
    \ conditions. To mit-\nigate the effects of the different lighting conditions,\
    \ three image enhancement methods\nwere evaluated, including Histogram Equalization\
    \ (HE), Auto Contrast, and Deep Photo\nEnhancer. The models were also trained\
    \ with various inputs, including YCrCb and YCgCb\ncolor spaces and vegetation\
    \ indices such as Normalized Difference Index (NDI), Normal-\nized Difference\
    \ Vegetation Index (NDVI), Excess Green (ExG), Excess Red (ExR), Excess\nGreen\
    \ minus Excess Red (ExGR), Color Index of Vegetation (CIVE), Vegetative Index\
    \ (VEG),\nand Modiﬁed Excess Green Index (MExG), Combined Indices (COM1), and\
    \ COM2. For\nthe sugar beet dataset, the model trained with NIR images enhanced\
    \ by Auto Contrast\noutperformed the other models with a mean IOU of 87.13%. For\
    \ the Oilseeds dataset, the\nmodels were trained with RGB images only, and the\
    \ model trained with images enhanced\nby Deep Photo Enhancer outperformed the\
    \ other models (mIOU = 88.91%). Moreover, the\nauto-contrast method outperforms\
    \ other methods in terms of time inference.\n3.8.4. Species Recognition\nThe classiﬁcation\
    \ of species (e.g., insects, birds, and plants) is another critical aspect of\n\
    agricultural management. The traditional human approach to species classiﬁcation\
    \ requires\nspecialists in the ﬁeld and is time-consuming. Deep learning can provide\
    \ more accurate\nand faster results by analyzing real-world data.\nRußwurm and\
    \ Körner [90] trained LSTM and GRU models for multitemporal classiﬁ-\ncation,\
    \ which achieved high accuracy in crop classiﬁcation tasks for many crops. Images\n\
    were collected from SENTINEL 2A images between 31 December 2015 and 30 September\n\
    2016. For consistency with the LANDSAT series, the blue, green, red, near-infrared,\
    \ and\nshortwave infrared 1 and 2 bands were selected for this assessment. The\
    \ performance of the\nLSTM model was compared to the RNNs, the CNN model, and\
    \ a baseline SVM. The LSTM\nmodel outperformed the other models in terms of accuracy\
    \ in land cover classiﬁcation.\nLee et al. [91] used CNN to classify 44 plants\
    \ based on leaf images. Two datasets were\nprepared. One contained the entire\
    \ image of a leaf, and for the other, each leaf image was\nmanually cropped. The\
    \ accuracy of the CNN model trained with the second dataset was\nhigher than that\
    \ obtained with the ﬁrst dataset. The results showed that CNN can extract\nhigh-level\
    \ features such as structural subdivisions, leaf tip, leaf base, leaf margin,\
    \ etc., and\nis not limited to shape, color, and texture.\nAyhan et al. [92] implement\
    \ DeeplabeV3+, a CNN model developed from scratch\nand a machine learning method\
    \ that uses NDVI (NDVI+ML) to segment vegetation and\nnon-vegetation in the images.\
    \ The dataset of [93] belongs to two studied sites, Vasiliko in\nCyprus and Kimisala\
    \ in Rhodes Island, were used. The images were acquired using a UAV\nand a modiﬁed,\
    \ uncalibrated near-infrared camera. The image resolution of the Vasiliko\nimage\
    \ is 20 cm per pixel and these images were used to train the models. On the other\n\
    hand, the Kimisala dataset was used as a test set. The images in this dataset\
    \ contain two\ndifferent resolutions of 10 cm per pixel and 20 cm per pixel.\n\
    Two DeeplabeV3+ models were trained with RGB images as input and G, B bands\n\
    plus NDVI as the third channel. Although the loss of the DeeplabeV3+ model trained\
    \ from\nscratch with NDVI+GB was decreased, the test result was very poor. However,\
    \ the same\nmodel trained with transfer learning and NDVI+GB channels improved\
    \ the accuracy of the\nmodel compared to the model trained with RGB images (almost\
    \ one percent). The CNN\ndeveloped from scratch was also trained with two different\
    \ channels. The ﬁrst was trained\nwith RGB images and the second with four channels\
    \ of RGB and NIR. The model trained\nwith RGB and NIR outperformed the model trained\
    \ with RGB images with an accuracy of\n80.9% and an accuracy of 76%.\nIn the last\
    \ experience, the NDVI and the Gaussian Mixture Model (GMM) of the\nmachine learning\
    \ model were used to classify the images. This method includes several\nthresholds\
    \ that need to be adjusted by the user. The NDVI+ML method outperformed the\n\
    Remote Sens. 2022, 14, 638\n28 of 43\nmodels from DL with an accuracy of 87%.\
    \ Note that the deep learning method in this work\nwas trained with a dataset\
    \ from one location and tested at another location with a different\nland cover.\
    \ This could be a reason why NDVI+ML outperformed the DL models.\nBhusal et al.\
    \ [94] used the pre-trained MobileNet on the ImageNet to classify bird\npests\
    \ in the image. Video data from a commercial vineyard captured with a GoPro Hero\n\
    4 outdoor camera with 1080P resolution was used as the dataset. In their work,\
    \ a motion\ndetection algorithm was used that is capable of detecting moving objects.\
    \ For each of these\nmoving objects, the abounded rectangle was extracted. These\
    \ moving objects were cropped\nfrom the original RGB image and reduced to 32 ×\
    \ 32. Each of these cropped images was\nreferred to as motion instance images\
    \ (MIIs). More than 5000 MIIs were collected from\ndifferent videos and classiﬁed\
    \ as bird or non-bird. In the next step, a CNN model developed\nin [95] was used\
    \ to improve image resolution. The images were converted from 32 × 32\nto 64 ×\
    \ 64, 96 × 96, and 128 × 128 and denoted as MIIs (e-MIIs), 2e-MIIs, 3e-MIIs, and\n\
    4e-MIIs, respectively. Five MobilNet models were trained with the MIIs, 2e-MIIs,\
    \ 3e-MIIs,\n4e-MIIs, and the entire Dataset. The worth results in terms of accuracy\
    \ were obtained with\na model trained with MIIs, and the best result was obtained\
    \ with the model trained with\nall the datasets.\nMac Aodha et al. [96] used the\
    \ CNN models from scratch to detect bats from audio\nﬁles. Two CNN models called\
    \ CNNFULL with three convolutional layers and 32 ﬁlters and\nCNNFAST consisting\
    \ of two convolutional layers and 16 ﬁlters were trained. The audio\nﬁles were\
    \ converted to a spectrogram and used as input to the CNN model. CNNFULL and\n\
    CNNFAST took 53 and 9.5 s, respectively, to run the entire detection pipeline\
    \ on the 3.2-min\nfull-spectrum test dataset. CNNFAST showed a trade-off between\
    \ speed and accuracy with\nslightly lower performance compared to CNNFULL.\nThe\
    \ performance of the models was compared with three existing commercial closed-\n\
    source detection systems, including SonoBat (version 3.1.7p) (https://sonobat.com/,\
    \ ac-\ncessed on 26 November 2021); SCAN’R (version 1.7.7), and Kaleidoscope (version\
    \ 4.2.0\nalpha4) (https://www.wildlifeacoustics.com/products/kaleidoscope-pro,\
    \ accessed on 26\nNovember 2021), as well as a machine learning method RF. The\
    \ CNN model signiﬁcantly\noutperformed the other algorithms in terms of mAP.\n\
    Ramalingam et al. [97] used Internet of Things (IoT) based architecture for insect\n\
    detection. The Internet is a global system of interconnected computer networks\
    \ that use\nTransmission Control Protocol/Internet Protocol (TCP/IP) to connect\
    \ billions of computers\nworldwide [98]. The IoT, on the other hand, is a global\
    \ network of physical objects equipped\nwith sensors and actuators that connect\
    \ to the Internet in real-time to be identiﬁed, sensed,\nand controlled remotely\
    \ [98]. The IoT architecture used in [97] consists of four layers:\nperception\
    \ layer, transport layer, processing layer, and application layer. In the perception\n\
    layer, a smart wireless camera captures an image of a sticky insect trap. The\
    \ transport\nlayer uses WiFi communication and TCP/IP to send images to the processing\
    \ layer and\ntransmit processed data to the application layer. In the processing\
    \ layer, an Fast RCNN\nwith ResNet50 is used to detect the insect in the images.\
    \ In the ﬁnal stage, smartphones and\nweb interfaces are used to perform the application\
    \ layer tasks. The experimental results\nshow that the trained model achieves\
    \ 96% accuracy in insect detection and outperforms\nYOLO and SSD in terms of accuracy.\n\
    3.8.5. Soil Management\nFor experts in agriculture, soil is a heterogeneous natural\
    \ resource with complex\nprocesses and unclear mechanisms [8]. Its temperature\
    \ alone can provide insight into\nthe impact of climate change on regional yields.\
    \ Deep learning algorithms study the\nprocesses of evaporation, moisture, and\
    \ soil temperature to understand the dynamics of\nthe ecosystem and the implications\
    \ for agriculture.\nRemote Sens. 2022, 14, 638\n29 of 43\nTable 6. Feature descriptions\
    \ of recent published papers in the ﬁeld of “Species recognition”.\nReferences\n\
    Application\nData Used\nModel\nMetric Used\nModel Performance\nRußwurm and Körner\
    \ [90]\nSegmentation of land cover with\nsequential models.\nImages were collected\
    \ from the\nSENTINEL2A satellite.\nLSTM, RNN, CNN, SVM\nF1-score, P, AC, R\nLSTM\
    \ networks and RNN achieved\nbetter results.\nLee et al. [91]\nClassiﬁcation of\
    \ plants based on\nleaf images.\nMalayaKew (MK) Leaf dataset\nwas collected at\
    \ Royal Botanic\nGardens Kew, England.\nCNN+SVM, CNN+MLP\nAC\nCNN+SVM with an\
    \ AC of 0.993\nachieved better performance compared\nto RBF, CNN+MLP.\nAyhan et\
    \ al. [92]\nClassiﬁcation of land into\nvegetation and non-vegetation\nImages\
    \ were collected\nusing UAV.\nDeeplabv3+, CNN model,\nGMM+NDVI\nAC, IOU\nGMM+NDVI\
    \ outperformed DL models.\nThe DeeplabeV3+ outperformed the\nCNN model developed\
    \ from scratched\nBhusal et al. [94]\nClassify bird pests in the image.\nVideo\
    \ data from a commercial\nvineyard captured with cameras.\nMobileNet with different\
    \ input\nAC\nThe best result was obtained with the\nmodel trained with all the\
    \ datasets.\nMac Aodha et al. [96]\nInsects detection\nImages taken with a WiFi\
    \ camera\nF-RCNN with Resnet50\nAC, R, P, F1-score\nThe model achieved an accuracy\
    \ of 96%.\nRamalingam et al. [97]\nInsect detection using Internet of\nThings\
    \ (IoT) base architecture.\nImages from the traps.\nFast RCNN with ResNet 50,\n\
    YOLO, SSD\nAc\nThe trained model achieves 96%\naccuracy in insect detection and\n\
    outperforms YOLO and SSD in terms\nof accuracy. .\nRemote Sens. 2022, 14, 638\n\
    30 of 43\nLi et al. [99] used a bidirectional LSTM model to estimate soil temperature\
    \ at 30 sites\nunder ﬁve different climate types. Soil temperature (ST) measurements\
    \ were obtained\nfrom the U.S. Department of Agriculture’s National Water and\
    \ Climate Center, which\nhas established more than 200 sites across the country\
    \ to collect data on meteorology,\nsoil, and solar radiation. Two models were\
    \ trained with different inputs and outputs.\nThe ﬁrst model received the meteorological\
    \ weather conditions including daily hours,\nminimum and maximum air temperature,\
    \ minimum and maximum relative humidity,\nvapor pressure, average solar radiation,\
    \ and average wind speed, and outputted the soil\ntemperature amplitude obtained\
    \ by subtracting the daily average soil temperature from the\nhourly soil temperature.\
    \ The second model obtained the meteorological weather conditions\nincluding month,\
    \ day of the month, observed air temperature, dew point temperature,\nminimum\
    \ and maximum air temperature, minimum and maximum relative humidity,\nvapor pressure,\
    \ average solar radiation, average wind speed, and outputted the daily\naverage\
    \ ST. To calculate the hourly ST, the output of the ﬁrst model was added with\
    \ the\noutput of the second model and called the integrated BiLSTM model. The\
    \ result of the\nmodel was compared with the BiLSTM model, which directly estimates\
    \ the hourly ST, the\nLSTM model, the deep neural network, random forest, SVR,\
    \ and Linear Regression. The\nintegrated BiLSTM model outperformed the other models\
    \ in terms of MAE, RMSE, and R2.\nThe LSTM model achieved the second-best performance.\n\
    It was also found that the performance of each model is not as good in snowy areas\
    \ as\nin warm or dry areas and that, the accuracy of the other models increases\
    \ with soil depth\n(except for RF). This behavior could be due to a change in\
    \ the standard deviation of the soil\ntemperature at different depths and climate\
    \ types, but this is not investigated in the study.\nYu et al. [100] implement\
    \ CNN (Conv2D, Conv3D), ConvLSTM to estimate the soil\ntemperature. The difference\
    \ between Conv2D and Conv3D is the size of the input channels.\nIn the ConvLSTM\
    \ model [101], there are convolution structures in both the input-to-state\nand\
    \ state-to-state transitions. The model obtains the last ten days of historical\
    \ data from\nspatiotemporal ST and predicts the ST one, three, and ﬁve days in\
    \ advance. Each model\nwas trained with two different input channels. The ﬁrst\
    \ time the raw data from ST was\nfed into the model, the second time, the input\
    \ was processed using the Empirical Mode\nDecomposition (EMD) method, which is\
    \ a proposed method for processing signals. In\nthe EMD method, the number of\
    \ channels was increased from one to ten. To complement\nthe model DL, persistent\
    \ prediction (PF) is used, a simple prediction method that treats\nthe temperature\
    \ of the ﬁrst day as a prediction for the next day. When forecasting ST\nwith\
    \ one-day historical data, PF outperformed the models of DL with raw data input.\n\
    On the other hand, when the models of DL used the EEMD- processed data as input,\
    \ the\nprediction performance was signiﬁcantly improved. Among all the models,\
    \ EEMD-Conv3D\nperformed the best in predicting the spatiotemporal ST. It could\
    \ be noted that ST depends\nnot only on the historical data of ST but also on\
    \ the meteorological weather conditions,\nwhich can be used as input to the model\
    \ to improve the accuracy.\nAlibabaei et al. [102] used Bidirectional LSTM (BLSTM),\
    \ CNN-LSTM, and a simple\nLSTM model to model daily reference evapotranspiration\
    \ and soil–water content. Meteo-\nrological weather data for three sites in Portugal\
    \ were collected from the stations Póvoa\nde Atalaia, Estação Borralheira, and\
    \ Direção Regional de Agricultura e Pescas do Centro,\nPortugal. Evapotranspiration\
    \ was calculated from meteorological data using the FAO\nPenman–Monteith equation\
    \ and soil water content was retrieved from ERA5 land. The\nBLSTM model with an\
    \ MSE of 0.014 to 0.056 outperformed the LSTM, CNN, and CNN-\nLSTM models. The\
    \ performance of BLSTM was also compared with traditional machine\nlearning methods\
    \ such as Random Forest and SVR and outperformed these two models.\nAmong the\
    \ machine learning methods, RF outperformed the SVR model. The model was\nnot\
    \ tested with data measured in the ﬁeld. For use in the ﬁeld, it can be re-trained\
    \ with a\nsmall set of ﬁeld-measured data to be used with more conﬁdence in real-world\
    \ applications.\nRemote Sens. 2022, 14, 638\n31 of 43\nTable 7. Feature descriptions\
    \ of recent published papers in the ﬁeld of “Soil management”.\nReferences\nApplication\n\
    Data Used\nModel\nMetric Used\nModel Performance\nLi et al. [99]\nEstimation of\
    \ soil temperature\nhttps://www.wcc.nrcs.usda.\ngov/scan/, accessed on 20\nNovember\
    \ 2021\nIntegrated BiLSTM, BiLSTM,\nLSTM, RF, SVR, DDN, LR\nRMSE, MAE, R2, MSE\n\
    Integrated BiLSTM model\noutperformed the other models\nYu et al. [100]\nEstimation\
    \ of soil temperature\nhttps:\n//cds.climate.copernicus.eu/,\naccessed on 20 November\
    \ 2021\nCNN (Conv2D, Conv3D),\nConvLSTM\nRMSE, MAE, R2, MSE\nEEMD-Conv3D performed\
    \ the\nbest in predicting the\nspatiotemporal.\nAlibabaei et al. [102]\nEstimation\
    \ of soil water content\nand evapotranspiration\nhttps:\n//cds.climate.copernicus.eu/,\n\
    accessed on 20 May 2020 and\nweather stations in Portugal\nBLSTM, CNNLSTM, simple\n\
    LSTM, CNN, RF, SVR\nMAE, R2, MSE\nBLSTM outperformed\nother models.\nRemote Sens.\
    \ 2022, 14, 638\n32 of 43\n3.8.6. Water Management\nWater management in agriculture\
    \ has implications for hydrological, climatological,\nand agronomic balance [8].\
    \ Five papers on the topic were reviewed, where machine learning\nmodels are employed\
    \ to estimate various values, such as evapotransition, allowing for\nmore effective\
    \ use of irrigation systems, and water table detection, which helps determine\n\
    crop water needs.\nSaggi and Jain [103] used multilayer perceptrons (MLP) to evaluate\
    \ daily evapotranspi-\nration for irrigation scheduling. They developed their\
    \ model from scratch. The developed\nDL model performed well in estimating evapotranspiration\
    \ and outperformed ML models\nsuch as RF, Generalized Linear Model (GLM), and\
    \ Gradient Boosting Models (GBM).\nZhang et al. [104] presented an LSTM-based\
    \ model for water management in agri-\ncultural areas. The LSTM model with a R2\
    \ value of 0.789–0.952 outperformed the fully\nconnected network. As mentioned\
    \ in the paper, the prediction of water table depth can\nhelp engineers and decision-makers\
    \ to develop an optimal irrigation scheduling system\nwhile controlling the effects\
    \ of salinity on intensive irrigation.\nLoggenberg et al. [105] applied hyperspectral\
    \ sensing and machine learning to model\nwater stress in vineyards. Stem water\
    \ potential (SWP) was measured in the ﬁeld by using a\npressure chamber to determine\
    \ vine water stress status. Vines with SWP values between\n−1.0 MPa and −1.8 MPa\
    \ were classiﬁed as water-stressed, while vines with SWP values\n−0.7 MPa were\
    \ classiﬁed as not stressed. Images were acquired using the SIMERA HX MkII\nhyperspectral\
    \ sensor, which detects 340 spectral wavebands in the VIS and NIR. A spectral\n\
    subset consisting of 176 wavebands with a spectral range of 473–708 nm was used\
    \ as\ninput to the models. Two ML models Random Forest (RF) and Extreme Gradient\
    \ Boosting\n(XGBoost) were used for classiﬁcation. In the ﬁrst experience, RF\
    \ and XGBoost models\nused all wavebands (176 bands) as input. RF with an accuracy\
    \ of 83.3% outperformed\nXGBoost with an accuracy of 78%. When using a subset\
    \ of important wavebands (18 bands),\nthe accuracy of RF and XGBoost was improved\
    \ (RF = 93.3% and XGBoost = 90%). The\nresults show that the choice of input to\
    \ the model in ML is crucial and should be carefully\nselected. The effect of\
    \ smoothing the spectral data with the Savitzky–Golay ﬁlter in the data\npreprocessing\
    \ step was also investigated in the paper. The Savitzky–Golay ﬁlter reduces\n\
    the model accuracy ranging between 0.7% and 3.3%.\nChen et al. [40] used a deep\
    \ Q-learning (DQN) model for an irrigation decision\nstrategy based on short-term\
    \ weather forecasts for rice. Daily observed meteorological\ndata of the rice-growing\
    \ period at three stations, including daily minimum and maximum\ntemperature,\
    \ average temperature, average wind speed, sunshine duration, average relative\n\
    humidity, and precipitation were collected. The state-space consisted of Pt is\
    \ the predicted\nprecipitation sequence for the next 7 days on day t , mm; ht\
    \ is the water depth on day t, mm;\nhmin is the lower limit of water depth, mm;\
    \ hmax is the upper limit of water depth, mm;\nHp is the maximum allowable water\
    \ depth after precipitation. In the action space, three\npossible actions were\
    \ included, representing the fraction of the irrigation quota (irrigation\nto\
    \ hmax) to be supplied to the ﬁeld on day t. Action 0 returns 0%, action 1 returns\
    \ 50%, and\naction 2 returns 100%. The reward is based on the rainfall use reward\
    \ and the yield reward.\nThe results of the DQN irrigation strategy compared to\
    \ the results of the conventional\nirrigation strategy showed a signiﬁcant reduction\
    \ in irrigation water volume, irrigation\ntiming, and drainage water without yield\
    \ loss.\nAlibabaei et al. [38] uses the DQN model to schedule irrigation of a\
    \ tomato ﬁeld in\nPortugal using climate Big Data. Historical data are collected\
    \ from various sources and\nprocessed for use as input. Two LSTM models are trained\
    \ on the obtained historical data to\npredict the total soil water in the soil\
    \ proﬁle for the next day and the tomato yield at the\nend of a season, respectively.\n\
    The trained LSTM models were used in the DRL training environment, which takes\n\
    the current state (historical climate data) and action (amount of irrigation)\
    \ and then returns\nthe next state and reward. The reward was calculated as the\
    \ net return and the Q-value\nRemote Sens. 2022, 14, 638\n33 of 43\nwas estimated\
    \ using an LSTM model. The results show that the agent learns to avoid water\n\
    waste at the beginning of the season and water stress at the end of the season.\
    \ Compared\nto the threshold and ﬁxed irrigation method, the DQN agent increases\
    \ productivity by 11%\nand avoids water waste by 20–30%.\n3.8.7. Automation in\
    \ Agriculture\nDeep learning is also being used to control sensors and robots\
    \ that enable automation\nand optimization of agricultural processes. These robots\
    \ can be used for a variety of\npurposes, including automated seeding, pesticide,\
    \ and crop nutrient application, damage\nrepair, irrigation, weed detection and\
    \ removal, harvesting, etc.\nLi et al. [52] used a deep-learning algorithm to\
    \ quickly and accurately detect and\nlocate longan fruit in imagery and pick the\
    \ fruit with a UAV device. An RGB-D camera\non the UAV was used to collect images\
    \ of the fruits. As mentioned in the paper, one of\nthe disadvantages of UAVs\
    \ is that they are easily affected by local circulation and airﬂow\nwhen capturing\
    \ images, resulting in blurred images of the fruits. In the preprocessing of\n\
    the data, a Fuzzy image processing algorithm was used to remove the blurred images\
    \ from\nthe data set.\nFPN, YOLOv3, YOLOv4, MobileNet-SSD, YOLOv4-tiny, and MobileNet-YOLOv4\n\
    models were trained on the images to detect and locate string fruits, simple fruits,\
    \ and fruit\nbranches. In the ﬁnal phase, the detection results are mapped onto\
    \ the optimized depth\nimage to extract the contours and spatial information of\
    \ the three targets. Based on this in-\nformation, the drone can detect and locate\
    \ the fruits and use them in harvesting. In general,\nthe YOLO models had faster\
    \ detection speed and achieved better accuracy than the FPN\nand SSD models. MobileNet-YOLOv4\
    \ achieved the best performance in terms of accuracy\n(mAP = 89.73) and inference\
    \ time (68 s), while FPN achieved the worst performance.\nTo test the model in\
    \ orchards, a picking platform was developed with a UAV, a Jetson\nTX2, an RGB-D\
    \ camera, a set of scissors with clamps, and a support frame. The accuracy\nrate\
    \ of successful harvesting in four cases was reported as 75, 75, 69.23, and 68.42.\n\
    Aghi et al. [106] presents a low-cost, energy-efﬁcient local motion planner for\
    \ au-\ntonomous navigation of robots in vineyards based on RGB-D images, low-range\
    \ hardware\n(a low-cost device with low power and limited computational capabilities),\
    \ and two control\nalgorithms. The ﬁrst algorithm uses the disparity map and its\
    \ depth representation to\ngenerate proportional control for the robot platform.\
    \ The second backup algorithm, based\non a deep learning algorithm, takes control\
    \ of the machine when the ﬁrst block brieﬂy fails\nand generates high-level motion\
    \ primitives.\nAn Intel RealSense Depth Camera D435i RGB-D camera was used to\
    \ capture images\nand compute the depth map on the platform. The video was captured\
    \ in different terrain,\nquality, and time of day. Then, a light depth map-based\
    \ algorithm processes the depth\nmaps to detect the end of the vineyard row and\
    \ then calculates control values for linear and\nangular velocities using a proportional\
    \ controller. Since, as in many outdoor applications,\nsunlight negatively affects\
    \ the quality of the results and interferes with the control provided\nby the\
    \ local navigation algorithm, MobileNet was trained to classify whether the camera\n\
    was pointed at the center of the end of the vineyard row or one of its sides,\
    \ and it was used\nwhen the ﬁrst algorithm failed due to outdoor conditions. The\
    \ CNN model was trained\nwith transfer learning and classiﬁed the images into\
    \ three classes: Middle, Left, and Right.\nFor the middle class, the video was\
    \ taken in rows with the camera pointed at the center,\nand for the other two\
    \ classes, the videos were rotated left and right with the camera at a\n45-degree\
    \ angle to the long axis of the row. The accuracy of the model on the test set\
    \ was\none. The model was trained on a small portion of the data set to investigate\
    \ the signiﬁcance\nof the size of the data set. The accuracy of the model decreased\
    \ by 6% with the small\ndata set.\nRemote Sens. 2022, 14, 638\n34 of 43\nTable\
    \ 8. Feature descriptions of recent published papers in the ﬁeld of “Water Management”.\n\
    References\nApplication\nData Used\nModel\nMetric Used\nModel Performance\nSaggi\
    \ and Jain [103]\nEstimation of evapotranspiration\nfor irrigation scheduling.\n\
    Fourteen years of time-series\ndata were used.\nMulti-layer DL\nRMSE\nMLDL achieved\
    \ better\nperformance compared to RF,\nGLM, and GBM.\nZhang et al. [104]\nPrediction\
    \ of water table depth in\nagriculture areas\nDaily meteorological data of\n31\
    \ years for Hoshiarpur and\n38 years for Patiala was\nconsidered for the study.\n\
    LSTM\nR2, RMSE\nLSTM achieved better\nperformance.\nLoggenberg et al. [105]\n\
    Model water stress in vineyards\nStem water potential (SWP) was\nmeasured in the\
    \ ﬁeld using a\npressure chamber. Images were\nacquired using the SIMERA HX\n\
    MkII hyperspectral Sensor\nRF and XGBoost\nAC\nRF outperformed XGBoost\nChen et\
    \ al. [40]\nIrrigation decision-making for\nrice base on weather forecasts\nhttp://data.cma.gov.cn,\n\
    accessed on 2 November 2021\nDQN, CNN\nThe threat score (TS), missing\nalarm rate\
    \ (MAR) and false alarm\nrate (FAR)\nThe DQN irrigation strategy\ncompared to\
    \ the results of the\nconventional irrigation strategy\nshowed a signiﬁcant reduction\
    \ in\nirrigation water volume.\nAlibabaei et al. [38]\nIrrigation decision-making\
    \ for\ntomato base on weather forecasts\nwww.drapc.gov.pt, accessed on 1\nMay\
    \ 2021\nDQN, LSTM\nR2-score, RMSE\nCompared to the threshold and\nﬁxed irrigation\
    \ method, the DQN\nagent increases productivity by\n11% and avoids water waste\
    \ by\n20–30%.\nRemote Sens. 2022, 14, 638\n35 of 43\nFinally, the CNN model was\
    \ optimized by discarding all redundant operations and\nreducing the ﬂoating-point\
    \ accuracy from 32 to 16 bits. The accuracy of the optimized\nmodel was the same\
    \ as the original and the time inference was improved. The proposed\nmodel was\
    \ implemented on a robot and the tests were performed in a new vineyard\nscenario.\
    \ The ﬁrst algorithm can detect the end of the vineyard regardless of the direction\n\
    of the long axis of the robot. When the ﬁrst algorithm fails, the CNN model jumps\
    \ in and\ndetects the end of the vineyard.\nBadeka et al. [107] trained YOLOv3\
    \ to detect grape crates in the ﬁeld for use on robots\nharvesting grapes. The\
    \ images of the crate were taken under natural ﬁeld conditions. Three\ndata enhancement\
    \ techniques were used, including rotation, noise, processing, and blur\nprocessing.\
    \ The model achieved an accuracy of 99.74% (mAP%) with an inference time\nof 0.26\
    \ s. To use the trained model on robots, it must be deployed on edge devices and\n\
    report the inference time and accuracy of the model on these devices. Another\
    \ interesting\nproblem is that the robot can detect whether the box is full or\
    \ not.\nMajeed et al. [108] combined the DL model with mathematical models to\
    \ detect\ncordons in grape canopies and determine their trajectories. Images were\
    \ taken so that a tree\ntrunk was approximately in the center of the ﬁeld of view\
    \ of a high-resolution camera (Sony\nCyber-shot RX100 IV). A total of 191 images\
    \ of random RGB images were captured from two\ndifferent vineyard blocks with\
    \ different cultivars. Each image in the dataset was classiﬁed\ninto three classes:\
    \ Background, Trunk, and Cordon (http://hdl.handle.net/2376/16939,\naccessed on\
    \ 1 September 2021). Since the background pixels in the labeled images covered\n\
    most of the images, a weight was assigned to each class in the preprocessing of\
    \ the data,\nwhich was calculated using a median frequency class balancing method\
    \ to avoid bias during\ntraining. Data augmentation and transfer learning techniques\
    \ were used in this work.\nA fully convolutional neural network (FCN) and SegNet\
    \ with VGG and AlexNet\nbackbones were trained to segment the cordons in the images.\
    \ The FCN-VGG16 network\nachieved the best performance in terms of mean accuracy\
    \ and F1-score compared to the\nother networks. In the next step, the mathematical\
    \ model including the Fourier model,\nGaussian model, polynomial model, and sine\
    \ sum model was applied to estimate the\ntrajectory of the cordons. The sixth-order\
    \ polynomial equation with an average R2 value of\n0.99 had the best ﬁt for the\
    \ trajectories of the cordons compared to the other mathematical\nmodels tested.\n\
    Pinto de Aguiar et al. [109] trained seven different DL models for detecting grapevine\n\
    stems in two vineyards in Portugal. The trained models included Single Shot Multi-\n\
    box (SSD) MobileNet-V1 (α = 0.75), SSD MobileNet-V1 with different hyperparameters\n\
    (α = 0.75), SSD MobileNet-V2, Pooling Pyramid Network (PPN) MobileNet-V1, SSDLite\n\
    MobileNet-V2, SSD Inception-V2, Tiny YOLO-V3. In this work, Transfer Learning\
    \ was ap-\nplied using the COCO dataset. A dataset was created using a robotic\
    \ platform. This dataset\ncontains images captured in two different vineyards,\
    \ using two cameras each, including\nthe Raspberry Pi cameras and a conventional\
    \ infrared ﬁlter, and the other camera Mako\nG-125C and an infrared ﬁlter (dataset\
    \ is available at http://vcriis01.inesctec.pt/-DS_AG_39,\naccessed on 1 September\
    \ 2021). After training the model, two edge devices, including\nGoogle’s USB Accelerator\
    \ and NVIDIA’s Jetson Nano were used for real-time inference\nof the model in\
    \ a real-world application. The advantage of NVIDIA’s Jetson Nano over\nGoogle’s\
    \ USB Accelerator is that it supports ﬂoating points and more models. However,\n\
    Google’s USB Accelerator outperformed NVIDIA’s Jetson Nano in terms of inference\
    \ time\n(about 49 frames per second), average accuracy, and time to load models.\
    \ The combination\nof edge devices and lightweight deep learning models makes\
    \ the model DL applicable\nin practice.\nRemote Sens. 2022, 14, 638\n36 of 43\n\
    Table 9. Feature descriptions of recent published papers in the ﬁeld of “Automation\
    \ in agriculture”.\nReferences\nApplication\nData Used\nModel\nMetric Used\nModel\
    \ Performance\nLi et al. [52]\nPick longan fruits with a UAV\ndevice\nA RGB-D\
    \ camera on the UAV\ndevice used to capture images\nfrom the ﬁeld.\nFPN, YOLOv3,\
    \ YOLOv4,\nMobileNet-SSD, YOLOv4- tiny\nand MobileNet-YOLOv4\nmAP, AC, R, P\n\
    The accuracy rate of successful\nharvesting using a picking platform\nin four\
    \ cases was reported as 75, 75,\n69.23, and 68.42.\nAghi et al. [106]\nLocal motion\
    \ planner for\nautonomous navigation of robots\nin vineyards\nA camera D435i RGB-D\
    \ camera\nwas used to capture images from\nthe ﬁeld.\nMobileNet\nAC, R, P, F1-score\n\
    The accuracy of the model on the test\nset was given as one and it was\ndecreased\
    \ 6% with the small dataset.\nBadeka et al. [107]\nDetect grape Crates in the\
    \ ﬁeld\nThe images of the crate were\ntaken under natural ﬁeld\nconditions\nYOLOv3\n\
    AC\nThe model achieved an average\ndetection rate of 99.74%.\nMajeed et al. [108]\n\
    Detect cordons in grape canopies\nand then de- determine their\ntrajectories\n\
    Images were taken so that a tree\ntrunk was approximately in the\ncenter of the\
    \ ﬁeld of view of a\nhigh-resolution camera\nFCN and Seg-Net with VGG and\nAlexNet\n\
    R, P, F1-score\nThe FCN-VGG16 network achieved\nthe best performance in terms\
    \ of\nmean accuracy and F_1 score\ncompared to the other networks.\nPinto de Aguiar\
    \ et al. [109]\nDetect grapevine trunk in two\nvineyards in Portugal using TPU\n\
    and NVIDIA’s Jetson Nano\nThis dataset contains images\ncaptured in two different\n\
    vineyards, using two cameras\nFCN and Seg-Net with VGG and\nAlexNet, SSD MobileNet-V2,\n\
    Pooling Pyramid Network,\nMobileNet-V1, SS- DLite\nMobileNet-V2, SSD Inception-V2,\n\
    Tiny YOLO-V3.\nIOU, F1-score\nSSD MobileNet-V2 on the TPU coral\nwith an average\
    \ precision of 52.98%\nand approximately 49 f/s achieved\nthe best\nAguiar et\
    \ al. [18]\nDetect the trunk in a vine\nThe images were acquired from\nfour different\
    \ vineyards in\nPortugal\nSSD MobileNetv1, SSD\nMobileNetv2, SSD Inception\nmAP,\
    \ IOU, R\nThe SSD MobileNetv1 and the SSD\nMobileNetv2 achieved similar\nperformance\
    \ (maximum AP of\n84.16%), but the SSD Inception\nachieved an AP of 75.78.\nRemote\
    \ Sens. 2022, 14, 638\n37 of 43\nAguiar et al. [18] trained the SSD model using\
    \ MobileNetv1 and MobileNetv2 as\nbackbones to detect the stem in a vine. The\
    \ model was deployed on an edge AI mode\n(Google’s USB Accelerator) in order to\
    \ investigate accuracy and penetration. The images\nwere acquired from four different\
    \ vineyards in Portugal and published under the name\nVineSet. They contain RGB\
    \ and thermal images of a single vineyard and include the anno-\ntations for each\
    \ image (http://vcriis01.inesctec.pt/datasets/DataSet/VineSet.zip, accessed\n\
    on 2 September 2021). A robotic platform AgRob V16 with a frontal stereo RGB camera\n\
    and a frontal thermal camera was used to capture a video from which the images\
    \ were\nextracted. Data augmentation methods such as rotation, translation, scaling,\
    \ ﬂipping, hue\nand saturation, Gaussian noise, and random combination were used\
    \ to resize the dataset\nto a suitable size for the seepage learning method. Two\
    \ training datasets were used to\nevaluate the effect of the training dataset\
    \ size on the performance of the detectors. The\noriginal VineSet dataset and\
    \ a small subset of it containing 336 non-augmented images\nwere used. The SSD\
    \ MobileNetv1 and the SSD MobileNetv2 trained with VineSet and\nusing transfer\
    \ learning achieved similar performance, a maximum AP of 84.16%, but the\nSSD\
    \ Inception achieved an AP of 75.78. In terms of inference time, the MobileNets\
    \ on\nthe TPU achieved an average inference time of 21.18 ms and 23.14 ms, which\
    \ was faster\nthan SSD Inception. The performance of the models trained with a\
    \ small dataset dropped\nsigniﬁcantly (with a maximum of AP = 34.42), reinforcing\
    \ the importance of the dataset\nsize for the performance of the deep learning\
    \ model.\nThe SSD MobileNet-V1 trained with VineSet was published on the Google\
    \ Colabora-\ntory platform (https://colab.research.google.com, accessed on 1 September\
    \ 2021) to help\nresearchers automatically label the new dataset and then manually\
    \ correct the labeling.\nLabeling the data is one of the challenges of using the\
    \ DL model. In this way, labeling the\nimages of the trunk becomes less time-consuming.\n\
    4. Discussion\nDeep learning is already being used in various areas of agriculture,\
    \ but it is still far\nfrom being widely applied in agriculture. In this paper,\
    \ we reviewed 46 recent research\npapers to examine the challenges of using deep\
    \ learning in agriculture. Figure 13 shows\nthe pie chart of the researched papers\
    \ based on the application domain.\nDisease Detection\nYield Prediction\nWeed\
    \ Detection\nSpecies Detection\nSoil Management\nWater Management\nAutomation\
    \ \nFigure 13. Pie chart representing the papers by application area.\nThe following\
    \ challenges can be pointed out from the reviewed articles. The most\ncritical\
    \ point concerns the dataset. Training the model with a small dataset can lead\
    \ to\nsigniﬁcant failures [18,72,106] and even with transfer learning and data\
    \ augmentation,\ntraining the model may require a sufﬁcient amount of data [55].\
    \ The presence of the image\nunder ﬁeld conditions in the data set is very important\
    \ [63]. Therefore, collecting authentic\nﬁeld images and datasets under different\
    \ conditions is the ﬁrst and most important step.\nRemote Sens. 2022, 14, 638\n\
    38 of 43\nUsing open-source datasets, sharing the dataset with other researchers,\
    \ data augmentation,\nand transfer learning properly can help to overcome these\
    \ challenges [71,77].\nIn addition, agricultural image datasets are more complex\
    \ due to outdoor conditions,\nthe object of interest usually occupying a very\
    \ small and uncentered part of the image,\nsimilarity between objects and background,\
    \ blocking the object with leaves and branches,\nmultiple objects in one image,\
    \ and many more [31,63,65,68,79,82]. To be applicable in the\nreal world, the\
    \ dataset should fully represent the state of the environment. In some cases,\n\
    such as variable illuminance, data augmentation may also be helpful.\nThe type\
    \ of input affects the performance of the model [55,62,88,92]. Removing\nbackground\
    \ from images [55], using different color spaces and vegetarian indices as in-\n\
    put [62], detecting crop at different growth stages [81], adding vegetarian indices\
    \ to the\ninput [30,86], cropping the input image [91], and data from different\
    \ climate types [99]\nchange the performance of the model. The size of the input\
    \ also affects the runtime and\naccuracy of the model [62,79]. Therefore, it is\
    \ challenging to determine the best input set\nfor the speciﬁc task.\nLabeling\
    \ large amounts of data is expensive and time-consuming [55]. In addition,\nthere\
    \ are some tasks, such as those related to plant diseases, that can only be performed\n\
    by specialists in the ﬁeld. Even when using open-source datasets with already\
    \ annotated\ndata, there is no way to ﬁnd out who actually did the annotation.\
    \ As it was seen in\nseveral papers, data augmentation and transfer learning are\
    \ ways to avoid labeling a\nlarge dataset, but labeling a small dataset is still\
    \ time-consuming [55]. Unsupervised and\nsemi-supervised learning methods can\
    \ be of great help but need further investigation in\nthis area [35,36,61,83].\n\
    When choosing a model for a task, there is a tradeoff between accuracy and inference\n\
    time [26,34,65,67,77,96]. Depending on the application, the model can be chosen.\n\
    Neither environment is similar in the ﬁeld of agriculture, and each environment\
    \ and\nproblem requires its own dataset, and therefore the DL model may not be\
    \ universally\napplicable, i.e., if a model has been trained with a dataset from\
    \ a particular site or a dataset\nfrom an open-source site such as ImageNet, it\
    \ may not be able to be used at another site, or\naccuracy may decrease when applied\
    \ to the dataset collected in the real world [12,69]. The\nchanges in the visual\
    \ appearance of the images in the training and test dataset could lead\nto a decrease\
    \ in model performance [30]. One way to overcome this is to retrain the already\n\
    trained model with a small dataset from the new environment [86].\nThe design\
    \ of deep models is complex and they are referred to as black boxes [7]. One\n\
    of the challenges in training DL models is that a system with very high computational\n\
    power (GPU) is needed [7]. Moreover, the performance of these models depends on\
    \ the\nchoice of hyperparameters, loss functions, and optimization algorithm [60,67].\
    \ Algorithms\nsuch as Bayesian optimization [110] can help to ﬁnd the right hyperparameters\
    \ [102].\nGoogle researchers used neural architecture search (NAS) algorithm [111]\
    \ to ﬁnd state of\nthe art MobilenetV3 [112]. NAS is a method that searches among\
    \ all possible combinations\nof submodules that can be repeatedly assembled to\
    \ obtain the entire model with the best\npossible accuracy.\nThe other challenge\
    \ relates to the real-time applicability of the models. Most deep\nlearning models\
    \ have many parameters that need to be trained, and after training the\nmodel,\
    \ inference of the model is not made in real-time. In some applications, such\
    \ as using\na robot for harvesting, time inference is essential. As we see in\
    \ this work, most of the\nmodels are implemented and tested on the PC, which requires\
    \ power and it is impractical\nin real applications. Moreover, implementation\
    \ on devices such as smartphones still, brings\nvarious challenges that need to\
    \ be considered, such as memory consumption, speed, etc.\nSingle-stage detection\
    \ models such as YOLO and SSD, lightweight classiﬁcation models\nsuch as MobileNet,\
    \ the development of edge devices such as Raspberry Pi and Jetson nano,\nand the\
    \ use of cloud computing have made it possible to deploy Deep Learning models\
    \ in\nreal-world applications and practical ways [34,52,72,86,109]. The quantization\
    \ method can\nbe also used to compress the model size and improve the detection\
    \ speed [82].\nRemote Sens. 2022, 14, 638\n39 of 43\nOn small farms, the use of\
    \ some methods is sometimes impractical because farmers\nmust have some knowledge\
    \ to interpret the results obtained. For example, the model\nthat predicts soil\
    \ moisture for irrigation scheduling requires a human expert to use this\ninformation\
    \ to determine the best time and amount of water for irrigation. As mentioned\n\
    earlier, Deep Reinforcement Learning is showing great success in several areas\
    \ as a promis-\ning model for building smart farms in several areas [39]. The\
    \ model can be trained to\nmake decisions about when and how much to irrigate\
    \ [38,40]. Information generated by\nIoT devices also helps farmers track agricultural\
    \ operations and make better decisions to\nimprove agricultural productivity [97].\n\
    5. Conclusions\nThis paper discussed some recent work on the application of DL\
    \ to agriculture and\nbiodiversity and highlighted some challenges in this area.\
    \ More sustainable agriculture\nand the promotion of biodiversity in agricultural\
    \ systems can be achieved by reducing the\nuse of agrochemicals, low pesticide\
    \ use and organic farming, appropriate crop rotations,\nsmall-scale ﬁelds, and\
    \ the preservation of natural spaces between agroecosystems [113]. As\nmentioned,\
    \ this can be achieved through the new IoT technology in combination with the\n\
    new biodiversity algorithms and Artiﬁcial Intelligence model. They can be used\
    \ to detect\nand control species and also to improve ecosystem state and thus\
    \ productivity having to\nresort to the use of environmentally damaging practices.\n\
    The novelty of the application of DL models and the challenges identiﬁed in agriculture\n\
    demonstrates the need for further research. CNN is the most widely used DL model\
    \ in\nagriculture. The use of new methods, such as attention mechanisms, new lightweight\n\
    models, single-stage detection models, can improve the performance of the model,\
    \ as a\nslight improvement in accuracy and run time can signiﬁcantly improve the\
    \ ﬁnal results.\nIn the future, we expect to develop or improve models that help\
    \ farmers make crop\nmanagement decisions. These models can be integrated into\
    \ decision support tools, and\nthese tools can guide users through precise steps\
    \ and suggest optimal decision paths. It is\nexpected that the development of\
    \ these models will also enable biodiversity monitoring.\nConsequently, the adoption\
    \ of new sustainable practices supported by DL models and\nbiodiversity monitoring\
    \ will help manage the farm with minimal human intervention and\ngreater effectiveness.\n\
    Author Contributions: K.A.: Investigation; Methodology; Writing—Review, and Software.\
    \ P.D.G.:\nSupervision, Writing—Review, and Project Administration and Funding\
    \ acquisition. T.M.L.: Writing—\nReview and Editing. R.M.C.: Writing—Review and\
    \ Editing. I.G.: Writing—Review and Editing. J.M.:\nWriting—Review and Editing.\
    \ C.M.L.: Writing—Review and Editing; All authors have read and\nagreed to the\
    \ published version of the manuscript.\nFunding: This work is supported by the\
    \ R&D Project BioDAgro—Sistema operacional inteligente de\ninformação e suporte\
    \ á decisão em AgroBiodiversidade, project PD20-00011, promoted by Fundação\n\
    La Caixa and Fundação para a Ciência e a Tecnologia, taking place at the C-MAST-Centre\
    \ for\nMechanical and Aerospace Sciences and Technology, Department of Electromechanical\
    \ Engineering\nof the University of Beira Interior, Covilhã, Portugal.\nData Availability\
    \ Statement: Not applicable.\nAcknowledgments: P.D.G. and T.M.L. acknowledge Fundação\
    \ para a Ciência e a Tecnologia (FCT—\nMCTES) for its ﬁnancial support via the\
    \ project UIDB/00151/2020 (C-MAST).\nConﬂicts of Interest: The authors declare\
    \ no conﬂict of interest.\nReferences\n1.\nOppermann, R.; Paracchini, M. HNV Farming–Central\
    \ to European Cultural Landscapes and Biodiversity. High Nature Value Farming\n\
    in Europe: 35 European Countries—Experiences and Perspectives; Verlag Regionalkultur:\
    \ Ubstadt-Weiher, Germany, 2012.\n2.\nSundmaeker, H.; Verdouw, C.N.; Wolfert,\
    \ J.; Freire, L.P. Internet of Food and Farm 2020. In Digitising the Industry;\
    \ Vermesan, O.,\nFriess, P., Eds.; River Publishers: Ljubljana, Slovenia, 2016;\
    \ pp. 129–150.\nRemote Sens. 2022, 14, 638\n40 of 43\n3.\nRosegrant, M.W.; Ringler,\
    \ C.; Zhu, T. Water for Agriculture: Maintaining Food Security under Growing Scarcity.\
    \ Annu. Rev.\nEnviron. Resour. 2009, 34, 205–222. [CrossRef]\n4.\nLohchab, V.;\
    \ Kumar, M.; Suryan, G.; Gautam, V.; Das, R.K. A Review of IoT based Smart Farm\
    \ Monitoring. In Proceedings of the\n2018 Second International Conference on Inventive\
    \ Communication and Computational Technologies (ICICCT), Coimbatore,\nIndia, 20–21\
    \ April 2018 ; pp. 1620–1625. [CrossRef]\n5.\nDoshi, J.; Patel, T.; kumar Bharti,\
    \ S. Smart Farming using IoT, a solution for optimally monitoring farming conditions.\
    \ Procedia\nComput. Sci. 2019, 160, 746–751. [CrossRef]\n6.\nNguyen, G.; Dlugolinsky,\
    \ S.; Bobak, M.; Tran, V.; Garcia, A.L.; Heredia, I.; Malik, P.; Hluchy, L. Machine\
    \ Learning and Deep\nLearning frameworks and libraries for large-scale data mining:\
    \ A survey. Artif. Intell. Rev. 2019, 52, 77–124. [CrossRef]\n7.\nDargan, S.;\
    \ Kumar, M.; Ayyagari, M.R.; Kumar, G. A Survey of Deep Learning and Its Applications:\
    \ A New Paradigm to Machine\nLearning. Arch. Comput. Methods Eng. 2019, 27, 1071–1092.\
    \ [CrossRef]\n8.\nLiakos, K.; Busato, P.B.; Moshou, D.; Pearson, S.; Bochtis,\
    \ D. Machine Learning in Agriculture: A Review. Sensors 2018, 18, 2674.\n[CrossRef]\
    \ [PubMed]\n9.\nKrizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet Classiﬁcation\
    \ with Deep Convolutional Neural Networks. Commun. ACM\n2017, 60, 84–90. [CrossRef]\n\
    10.\nSermanet, P.; Eigen, D.; Zhang, X.; Mathieu, M.; Fergus, R.; LeCun, Y. OverFeat:\
    \ Integrated Recognition, Localization and\nDetection using Convolutional Networks.\
    \ arXiv 2013, arXiv:1312.6229.\n11.\nLiu, J.; Xiang, J.; Jin, Y.; Liu, R.; Yan,\
    \ J.; Wang, L. Boost Precision Agriculture with Unmanned Aerial Vehicle Remote\
    \ Sensing and\nEdge Intelligence: A Survey. Remote Sens. 2021, 13, 4387. [CrossRef]\n\
    12.\nRamcharan, A.; McCloskey, P.; Baranowski, K.; Mbilinyi, N.; Mrisho, L.; Ndalahwa,\
    \ M.; Legg, J.; Hughes, D.P. A Mobile-Based\nDeep Learning Model for Cassava Disease\
    \ Diagnosis. Front. Plant Sci. 2019, 10, 272. [CrossRef]\n13.\nSzegedy, C.; Liu,\
    \ W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.;\
    \ Rabinovich, A. Going deeper with\nconvolutions. In Proceedings of the 2015 IEEE\
    \ Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA,\n\
    USA, 7–12 June 2015; pp. 1–9.\n14.\nSimonyan, K.; Zisserman, A. Very Deep Convolutional\
    \ Networks for Large-Scale Image Recognition. arXiv 2015, arXiv:1409.1556.\n15.\n\
    He, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition.\
    \ In Proceedings of the 2016 IEEE Conference on\nComputer Vision and Pattern Recognition\
    \ (CVPR), Las Vegas, NV, USA, 26 June–1 July 2016; pp. 770–778.\n16.\nHoward,\
    \ A.G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; Weyand, T.; Andreetto,\
    \ M.; Adam, H. MobileNets: Efﬁcient\nConvolutional Neural Networks for Mobile\
    \ Vision Applications. arXiv 2017, arXiv:1704.04861.\n17.\nTan, M.; Le, Q.V. EfﬁcientNet:\
    \ Rethinking Model Scaling for Convolutional Neural Networks. 2020. Available\
    \ online: http:\n//xxx.lanl.gov/abs/1905.11946 (accessed on 1 May 2021) .\n18.\n\
    Aguiar, A.S.; Monteiro, N.N.; Santos, F.N.d.; Solteiro Pires, E.J.; Silva, D.;\
    \ Sousa, A.J.; Boaventura-Cunha, J. Bringing Semantics to\nthe Vineyard: An Approach\
    \ on Deep Learning-Based Vine Trunk Detection. Agriculture 2021, 11, 131. [CrossRef]\n\
    19.\nGirshick, R.; Donahue, J.; Darrell, T.; Malik, J. Rich Feature Hierarchies\
    \ for Accurate Object Detection and Semantic Segmentation.\nIn Proceedings of\
    \ the 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus,\
    \ OH, USA, 23–28 June 2014;\nIEEE Computer Society: Piscataway, NJ, USA; pp. 580–587.\n\
    20.\nGirshick, R. Fast R-CNN. In Proceedings of the 2015 IEEE International Conference\
    \ on Computer Vision (ICCV), Santiago, Chile,\n7–13 December 2015; IEEE Computer\
    \ Society: Piscataway, NJ, USA, 2015; pp. 1440–1448. [CrossRef]\n21.\nRen, S.;\
    \ He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards Real-Time Object Detection\
    \ with Region Proposal Networks. IEEE\nTrans. Pattern Anal. Mach. Intell. 2017,\
    \ 39, 1137–1149. [CrossRef] [PubMed]\n22.\nRedmon, J.; Farhadi, A. YOLOv3: An\
    \ Incremental Improvement. 2018. Available online: http://xxx.lanl.gov/abs/1804.02767\n\
    (accessed on 1 September 2021) .\n23.\nLiu, W.; Anguelov, D.; Erhan, D.; Szegedy,\
    \ C.; Reed, S.; Fu, C.Y.; Berg, A.C. SSD: Single Shot MultiBox Detector. In Computer\n\
    Vision—ECCV 2016; Springer International Publishing: Cham, Switzerland, 2016;\
    \ pp. 21–37.\n24.\nWang, Y.; Zhou, Q.; Liu, J.; Xiong, J.; Gao, G.; Wu, X.; Latecki,\
    \ L.J. Lednet: A Lightweight Encoder-Decoder Network for Real-Time\nSemantic Segmentation.\
    \ In Proceedings of the 2019 IEEE International Conference on Image Processing\
    \ (ICIP), Taipei, Taiwan,\n22–25 September 2019; pp. 1860–1864.\n25.\nAssunção,\
    \ E.T.; Gaspar, P.D.; Mesquita, R.J.M.; Simões, M.P.; Ramos, A.; Proença, H.;\
    \ Inacio, P.R.M. Peaches Detection Using a\nDeep Learning Technique—A Contribution\
    \ to Yield Estimation, Resources Management, and Circular Economy. Climate 2022,\n\
    10, 11. [CrossRef]\n26.\nSantos, T.T.; de Souza, L.L.; dos Santos, A.A.; Avila,\
    \ S. Grape detection, segmentation, and tracking using deep neural networks\n\
    and three-dimensional association. Comput. Electron. Agric. 2020, 170, 105247.\
    \ [CrossRef]\n27.\nLong, J.; Shelhamer, E.; Darrell, T. Fully convolutional networks\
    \ for semantic segmentation. In Proceedings of the 2015 IEEE\nConference on Computer\
    \ Vision and Pattern Recognition (CVPR), Boston, MA, USA, 7–12 June 2015; pp.\
    \ 3431–3440. [CrossRef]\n28.\nHe, K.; Gkioxari, G.; Dollár, P.; Girshick, R. Mask\
    \ R-CNN. arXiv 2017, arXiv:1703.06870.\n29.\nChen, L.C.; Papandreou, G.; Kokkinos,\
    \ I.; Murphy, K.; Yuille, A.L.\nDeepLab: Semantic Image Segmentation with Deep\n\
    Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. 2017. Available\
    \ online: http://xxx.lanl.gov/abs/1606.00915\n(accessed on 1 October 2021 ).\n\
    Remote Sens. 2022, 14, 638\n41 of 43\n30.\nLottes, P.; Behley, J.; Milioto, A.;\
    \ Stachniss, C. Fully Convolutional Networks with Sequential Information for Robust\
    \ Crop and\nWeed Detection in Precision Farming. IEEE Robot. Autom. Lett. 2018,\
    \ 3, 2870–2877. [CrossRef]\n31.\nGhiani, L.; Sassu, A.; Palumbo, F.; Mercenaro,\
    \ L.; Gambella, F. In-Field Automatic Detection of Grape Bunches under a Totally\n\
    Uncontrolled Environment. Sensors 2021, 21, 3908. [CrossRef]\n32.\nPatterson,\
    \ J.; Gibson, A. Deep Learning: A Practitioner’s Approach; O’Reilly: Beijing,\
    \ China, 2017.\n33.\nHochreiter, S.; Schmidhuber, J. Long Short-Term Memory. Neural.\
    \ Comput. 1997, 9, 1735–1780. [CrossRef]\n34.\nKang, H.; Chen, C. Fruit detection,\
    \ segmentation and 3D visualisation of environments in apple orchards. Comput.\
    \ Electron.\nAgric. 2020, 171, 105302. [CrossRef]\n35.\nTang, J.; Wang, D.; Zhang,\
    \ Z.; He, L.; Xin, J.; Xu, Y. Weed identiﬁcation based on K-means feature learning\
    \ combined with\nconvolutional neural network. Comput. Electron. Agric. 2017,\
    \ 135, 63–70. [CrossRef]\n36.\nFerreira, A.D.S.; Freitas, D.M.; da Silva, G.G.;\
    \ Pistori, H.; Folhes, M.T. Unsupervised deep learning and semi-automatic data\n\
    labeling in weed discrimination. Comput. Electron. Agric. 2019, 165, 104963. [CrossRef]\n\
    37.\nMin, E.; Guo, X.; Liu, Q.; Zhang, G.; Cui, J.; Long, J. A Survey of Clustering\
    \ With Deep Learning: From the Perspective of Network\nArchitecture. IEEE Access\
    \ 2018, 6, 39501–39514. [CrossRef]\n38.\nAlibabaei, K.; Gaspar, P.D.; Assunção,\
    \ E.; Alirezazadeh, S.; Lima, T.M. Irrigation optimization with a deep reinforcement\
    \ learning\nmodel: Case study on a site in Portugal. Agric. Water Manag. 2022,\
    \ 263, 107480. [CrossRef]\n39.\nBu, F.; Wang, X. A smart agriculture IoT system\
    \ based on deep reinforcement learning. Future Gener. Comput. Syst. 2019,\n99,\
    \ 500–507. [CrossRef]\n40.\nChen, M.; Cui, Y.; Wang, X.; Xie, H.; Liu, F.; Luo,\
    \ T.; Zheng, S.; Luo, Y. A reinforcement learning approach to irrigation\ndecision-making\
    \ for rice using weather forecasts. Agric. Water Manag. 2021, 250, 106838. [CrossRef]\n\
    41.\nLoukatos, D.; Templalexis, C.; Lentzou, D.; Xanthopoulos, G.; Arvanitis,\
    \ K.G. Enhancing a ﬂexible robotic spraying platform for\ndistant plant inspection\
    \ via high-quality thermal imagery data. Comput. Electron. Agric. 2021, 190, 106462.\
    \ [CrossRef]\n42.\nRey, B.; Aleixos, N.; Cubero, S.; Blasco, J. Xf-Rovim. A Field\
    \ Robot to Detect Olive Trees Infected by Xylella Fastidiosa Using\nProximal Sensing.\
    \ Remote Sens. 2019, 11, 221. [CrossRef]\n43.\nRußwurm, M.; Körner, M. Multi-Temporal\
    \ Land Cover Classiﬁcation with Sequential Recurrent Encoders. ISPRS Int. J. -Geo-Inf.\n\
    2018, 7, 129. [CrossRef]\n44.\nMa, J.W.; Nguyen, C.H.; Lee, K.; Heo, J. Regional-scale\
    \ rice-yield estimation using stacked auto-encoder with climatic and MODIS\ndata:\
    \ a case study of South Korea. Int. J. Remote. Sens. 2019, 40, 51–71. [CrossRef]\n\
    45.\nBargoti, S.; Underwood, J. Deep Fruit Detection in Orchards. In Proceedings\
    \ of the 2017 IEEE International Conference on\nRobotics and Automation (ICRA),\
    \ Singapore, 29 May–3 June 2017; pp. 3626–3633. [CrossRef]\n46.\nBrahimi, M.;\
    \ Boukhalfa, K.; Moussaoui, A. Deep Learning for Tomato Diseases: Classiﬁcation\
    \ and Symptoms Visualization. Appl.\nArtif. Intell. 2017, 31, 299–315. [CrossRef]\n\
    47.\nKumar, P.; Reddy, S.R.N. Wireless sensor networks: a review of motes, wireless\
    \ technologies, routing algorithms and static\ndeployment strategies for agriculture\
    \ applications. CSI Trans. ICT 2020, 8, 331–345. [CrossRef]\n48.\nSishodia, R.P.;\
    \ Ray, R.L.; Singh, S.K. Applications of Remote Sensing in Precision Agriculture:\
    \ A Review. Remote Sens. 2020,\n12. 3136. [CrossRef]\n49.\nRudd, J.; Roberson,\
    \ G.; Classen, J. Application of satellite, Unmanned Aircraft System, and Ground-Based\
    \ Sensor Data for Precision\nAgriculture: A Review; American Society of Agricultural\
    \ and Biological Engineers: St. Joseph, MI, USA, 2017. [CrossRef]\n50.\nTerres,\
    \ J.M.; Delince, J.; van de Steene, M.; Hawkins, A. The use of remote sensing\
    \ and GIS capabilities to support the reform of\nthe common agricultural policy\
    \ of the European community. Remote Sens. Rev. 1995, 12, 53–60. [CrossRef]\n51.\n\
    Ouhami, M.; Haﬁane, A.; Es-Saady, Y.; El Hajji, M.; Canals, R. Computer Vision,\
    \ IoT and Data Fusion for Crop Disease Detection\nUsing Machine Learning: A Survey\
    \ and Ongoing Research. Remote Sens. 2021, 13, 2486. [CrossRef]\n52.\nLi, D.;\
    \ Sun, X.; Elkhouchlaa, H.; Jia, Y.; Yao, Z.; Lin, P.; Li, J.; Lu, H. Fast detection\
    \ and location of longan fruits using UAV images.\nComput. Electron. Agric. 2021,\
    \ 190, 106465. [CrossRef]\n53.\nRahnemoonfar, M.; Sheppard, C. Deep Count: Fruit\
    \ Counting Based on Deep Simulated Learning.\nSensors 2017, 17, 905.\n[CrossRef]\n\
    54.\nAlibabaei, K.; Gaspar, P.D.; Lima, T.M. Crop Yield Estimation Using Deep\
    \ Learning Based on Climate Big Data and Irrigation\nScheduling. Energies 2021,\
    \ 14, 3004. [CrossRef]\n55.\nBarbedo, J.G.A. Impact of dataset size and variety\
    \ on the effectiveness of deep learning and transfer learning for plant disease\n\
    classiﬁcation. Comput. Electron. Agric. 2018, 153, 46–53. [CrossRef]\n56.\nYang,\
    \ Q.; Shi, L.; Han, J.; Zha, Y.; Zhu, P. Deep convolutional neural networks for\
    \ rice grain yield estimation at the ripening stage\nusing UAV-based remotely\
    \ sensed images. Field Crop. Res. 2019, 235, 142–153. [CrossRef]\n57.\nPan, J.S.;\
    \ Yang, Q. A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 2010, 22,\
    \ 1345–1359. [CrossRef]\n58.\nGhazi, M.M.; Yanikoglu, B.; Aptoula, E. Plant identiﬁcation\
    \ using deep neural networks via optimization of transfer learning\nparameters.\
    \ Neurocomputing 2017, 235, 228–235. [CrossRef]\n59.\nDreyfus, S. The numerical\
    \ solution of variational problems. J. Math. Anal. Appl. 1962, 5, 30–45. [CrossRef]\n\
    60.\nZhang, X.; Qiao, Y.; Meng, F.; Fan, C.; Zhang, M. Identiﬁcation of Maize\
    \ Leaf Diseases Using Improved Deep Convolutional\nNeural Networks. IEEE Access\
    \ 2018, 6, 30370–30377. [CrossRef]\nRemote Sens. 2022, 14, 638\n42 of 43\n61.\n\
    Kerkech, M.; Haﬁane, A.; Canals, R. Vine disease detection in UAV multispectral\
    \ images using optimized image registration and\ndeep learning segmentation approach.\
    \ Comput. Electron. Agric. 2020, 174, 105446. [CrossRef]\n62.\nKerkech, M.; Haﬁane,\
    \ A.; Canals, R. Deep leaning approach with colorimetric spaces and vegetation\
    \ indices for vine diseases\ndetection in UAV images. Comput. Electron. Agric.\
    \ 2018, 155, 237–243. [CrossRef]\n63.\nFerentinos, K.P. Deep learning models for\
    \ plant disease detection and diagnosis. Comput. Electron. Agric. 2018, 145, 311–318.\n\
    [CrossRef]\n64.\nKrizhevsky, A. One Weird Trick for Parallelizing Convolutional\
    \ Neural Networks. 2014. Available online: http://xxx.lanl.gov/\nabs/1404.5997\
    \ (accessed on 1 November 2021) .\n65.\nJiang, P.; Chen, Y.; Liu, B.; He, D.;\
    \ Liang, C. Real-Time Detection of Apple Leaf Diseases Using Deep Learning Approach\
    \ Based on\nImproved Convolutional Neural Networks. IEEE Access 2019, 7, 59069–59080.\
    \ [CrossRef]\n66.\nKarthik, R.; Hariharan, M.; Anand, S.; Mathikshara, P.; Johnson,\
    \ A.; Menaka, R. Attention embedded residual CNN for disease\ndetection in tomato\
    \ leaves. Appl. Soft Comput. 2020, 86, 105933.\n67.\nLiu, B.; Zhang, Y.; He, D.;\
    \ Li, Y. Identiﬁcation of Apple Leaf Diseases Based on Deep Convolutional Neural\
    \ Networks. Symmetry\n2018, 10, 11. [CrossRef]\n68.\nPicon, A.; Seitz, M.; Alvarez-Gila,\
    \ A.; Mohnke, P.; Ortiz-Barredo, A.; Echazarra, J. Crop conditional Convolutional\
    \ Neural\nNetworks for massive multi-crop plant disease classiﬁcation over cell\
    \ phone acquired images taken on real ﬁeld conditions.\nComput. Electron. Agric.\
    \ 2019, 167, 105093. [CrossRef]\n69.\nChen, J.; Zhang, D.; Zeb, A.; Nanehkaran,\
    \ Y.A. Identiﬁcation of rice plant diseases using lightweight attention networks.\
    \ Expert\nSyst. Appl. 2021, 169, 114514. [CrossRef]\n70.\nRamcharan, A.; Baranowski,\
    \ K.; McCloskey, P.; Ahmed, B.; Legg, J.; Hughes, D.P. Deep Learning for Image-Based\
    \ Cassava\nDisease Detection. Front. Plant Sci. 2017, 8. 1852. [CrossRef] [PubMed]\n\
    71.\nSilver, D.L.; Monga, T. In Vino Veritas: Estimating Vineyard Grape Yield\
    \ from Images Using Deep Learning. In Advances in\nArtiﬁcial Intelligence; Meurs,\
    \ M.J., Rudzicz, F., Eds.; Springer International Publishing: Cham, Switzerland,\
    \ 2019; pp. 212–224.\n72.\nAguiar, A.S.; Magalhães, S.A.; dos Santos, F.N.; Castro,\
    \ L.; Pinho, T.; Valente, J.; Martins, R.; Boaventura-Cunha, J. Grape Bunch\n\
    Detection at Different Growth Stages Using Deep Learning Quantized Models. Agronomy\
    \ 2021, 11, 1890 . [CrossRef]\n73.\nSeng, K.P.; Ang, L.M.; Schmidtke, L.M.; Rogiers,\
    \ S.Y. Computer Vision and Machine Learning for Viticulture Technology. IEEE\n\
    Access 2018, 6, 67494–67510. [CrossRef]\n74.\nMilella, A.; Marani, R.; Petitti,\
    \ A.; Reina, G. In-ﬁeld high throughput grapevine phenotyping with a consumer-grade\
    \ depth\ncamera. Comput. Electron. Agric. 2019, 156, 293–306. [CrossRef]\n75.\n\
    Palacios, F.; Bueno, G.; Salido, J.; Diago, M.P.; Hernández, I.; Tardaguila, J.\
    \ Automated grapevine ﬂower detection and\nquantiﬁcation method based on computer\
    \ vision and deep learning from on-the-go imaging using a mobile sensing platform\n\
    under ﬁeld conditions. Comput. Electron. Agric. 2020, 178, 105796. [CrossRef]\n\
    76.\nMillán, B.; Aquino, A.; Diago, M.P.; Tardáguila, J. Image analysis-based\
    \ modelling for ﬂower number estimation in grapevine.\nJ. Sci. Food Agric. 2017,\
    \ 97 3, 784–792. [CrossRef]\n77.\nKoirala, A.; Walsh, K.B.; Wang, Z.; McCarthy,\
    \ C. Deep learning for real-time fruit detection and orchard fruit load estimation:\n\
    benchmarking of ‘MangoYOLO’. Precis. Agric. 2019, 20, 1107–1135. [CrossRef]\n\
    78.\nRedmon, J.; Farhadi, A. YOLO9000: Better, Faster, Stronger. In Proceedings\
    \ of the 2017 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR),\
    \ Honolulu, HI, USA, 21–26 July 2017; pp. 6517–6525. [CrossRef]\n79.\nLiang, Q.;\
    \ Zhu, W.; Long, J.; Wang, Y.; Sun, W.; Wu, W. A Real-Time Detection Framework\
    \ for On-Tree Mango Based on SSD\nNetwork. In Intelligent Robotics and Applications;\
    \ Springer International Publishing: Cham, Switzerland, 2018; pp. 423–436.\n80.\n\
    Bargoti, S.; Underwood, J.P. Image Segmentation for Fruit Detection and Yield\
    \ Estimation in Apple Orchards. J. Field Robot. 2017,\n34, 1039–1060. [CrossRef]\n\
    81.\nTian, Y.; Yang, G.; Wang, Z.; Wang, H.; Li, E.; Liang, Z. Apple detection\
    \ during different growth stages in orchards using the\nimproved YOLO-V3 model.\
    \ Comput. Electron. Agric. 2019, 157, 417–426.10.1016/j.compag.2019.01.012. [CrossRef]\n\
    82.\nZhou, Z.; Song, Z.; Fu, L.; Gao, F.; Li, R.; Cui, Y. Real-time kiwifruit\
    \ detection in orchard using deep learning on Android™\nsmartphones for yield\
    \ estimation. Comput. Electron. Agric. 2020, 179, 105856. [CrossRef]\n83.\nBah,\
    \ M.D.; Haﬁane, A.; Canals, R. Deep Learning with Unsupervised Data Labeling for\
    \ Weed Detection in Line Crops in UAV\nImages. Remote Sens. 2018, 10, 1690. [CrossRef]\n\
    84.\nFerreira, A.D.S.; Freitas, D.M.; da Silva, G.G.; Pistori, H.; Folhes, M.T.\
    \ Weed detection in soybean crops using ConvNets. Comput.\nElectron. Agric. 2017,\
    \ 143, 314–324. [CrossRef]\n85.\nOlsen, A.; Konovalov, D.A.; Philippa, B.; Ridd,\
    \ P.; Wood, J.C.; Banks, W.; Girgenti, B.; Kenny, O.; Whinney, J.; Calvert, B.;\
    \ et al.\nDeepWeeds: A Multiclass Weed Species Image Dataset for Deep Learning.\
    \ Sci. Rep. 2019, 9, 2058. [CrossRef]\n86.\nMilioto, A.; Lottes, P.; Stachniss,\
    \ C. Real-time Semantic Segmentation of Crop and Weed for Precision Agriculture\
    \ Robots\nLeveraging Background Knowledge in CNNs.\nIn Proceedings of the 2018\
    \ IEEE International Conference on Robotics and\nAutomation (ICRA), Brisbane,\
    \ Australia, 21–25 May 2018; pp. 2229–2235.\n87.\nBadrinarayanan, V.; Kendall,\
    \ A.; Cipolla, R. SegNet: A Deep Convolutional Encoder-Decoder Architecture for\
    \ Image Segmentation.\nIEEE Trans. Pattern Anal. Mach. Intell. 2017, 39, 2481–2495.\
    \ [CrossRef]\n88.\nWang, A.; Xu, Y.; Wei, X.; Cui, B. Semantic Segmentation of\
    \ Crop and Weed using an Encoder-Decoder Network and Image\nEnhancement Method\
    \ under Uncontrolled Outdoor Illumination. IEEE Access 2020, 8, 81724–81734. [CrossRef]\n\
    Remote Sens. 2022, 14, 638\n43 of 43\n89.\nChebrolu, N.; Lottes, P.; Schaefer,\
    \ A.; Winterhalter, W.; Burgard, W.; Stachniss, C. Agricultural robot dataset\
    \ for plant classiﬁcation,\nlocalization and mapping on sugar beet ﬁelds. Int.\
    \ J. Robot. Res. 2017, 36, 1045–1052. [CrossRef]\n90.\nRußwurm, M.; Körner, M.\
    \ Multi-Temporal Land Cover Classiﬁcation with Long Short-Term Memory Neural Networks.\
    \ ISPRS\n2017, 42W1, 551–558. [CrossRef]\n91.\nLee, S.H.; Chan, C.S.; Mayo, S.J.;\
    \ Remagnino, P. How deep learning extracts and learns leaf features for plant\
    \ classiﬁcation.\nPattern Recognit. 2017, 71, 1–13. [CrossRef]\n92.\nAyhan, B.;\
    \ Kwan, C.; Budavari, B.; Kwan, L.; Lu, Y.; Perez, D.; Li, J.; Skarlatos, D.;\
    \ Vlachos, M. Vegetation Detection Using Deep\nLearning and Conventional Methods.\
    \ Remote Sens. 2020, 12, 2502. [CrossRef]\n93.\nSkarlatos, D.; Vlachos, M. Vegetation\
    \ Removal From Uav Derived Dsms, Using Combination of Rgb and Nir Imagery. ISPRS\n\
    Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2018, IV-2, 255–262. [CrossRef]\n\
    94.\nBhusal, S.; Bhattarai, U.; Karkee, M. Improving Pest Bird Detection in a\
    \ Vineyard Environment using Super-Resolution and Deep\nLearning. IFAC-PapersOnLine\
    \ 2019, 52, 18–23. [CrossRef]\n95.\nYamanaka, J.; Kuwashima, S.; Kurita, T. Fast\
    \ and Accurate Image Super Resolution by Deep CNN with Skip Connection and\nNetwork\
    \ in Network. In Neural Information Processing; Liu, D., Xie, S., Li, Y., Zhao,\
    \ D., El-Alfy, E.S.M., Eds.; Springer International\nPublishing: Cham, Switzerland,\
    \ 2017; pp. 217–225.\n96.\nMac Aodha, O.; Gibb, R.; Barlow, K.E.; Browning, E.;\
    \ Firman, M.; Freeman, R.; Harder, B.; Kinsey, L.; Mead, G.R.; Newson, S.E.;\n\
    et al. Bat detective—Deep learning tools for bat acoustic signal detection. PLoS\
    \ Comput. Biol. 2018, 14, e1005995. [CrossRef]\n[PubMed]\n97.\nRamalingam, B.;\
    \ Mohan, R.E.; Pookkuttath, S.; Gómez, B.F.; Sairam Borusu, C.S.C.; Wee Teng,\
    \ T.; Tamilselvam, Y.K. Remote\nInsects Trap Monitoring System Using Deep Learning\
    \ Framework and IoT. Sensors 2020, 20, 5280. [CrossRef]\n98.\nVerdouw, C.; Wolfert,\
    \ S.; Tekinerdogan, B. Internet of Things in agriculture. CAB Rev. 2016, 11, 1–12.\
    \ SNNR201611035. [CrossRef]\n99.\nLi, C.; Zhang, Y.; Ren, X. Modeling Hourly Soil\
    \ Temperature Using Deep BiLSTM Neural Network. Algorithms 2020, 13, 173.\n[CrossRef]\n\
    100. Yu, F.; Hao, H.; Li, Q. An Ensemble 3D Convolutional Neural Network for Spatiotemporal\
    \ Soil Temperature Forecasting.\nSustainability 2021, 13, 9174. [CrossRef]\n101.\
    \ Shi, X.; Chen, Z.; Wang, H.; Yeung, D.Y.; Wong, W.K.; Woo, W.c. Convolutional\
    \ LSTM Network: A Machine Learning Approach\nfor Precipitation Nowcasting. In\
    \ Advances in Neural Information Processing Systems; Cortes, C., Lawrence, N.,\
    \ Lee, D., Sugiyama,\nM., Garnett, R., Eds.; Curran Associates, Inc.: New York,\
    \ NY, USA, 2015; Volume 28.\n102. Alibabaei, K.; Gaspar, P.D.; Lima, T.M. Modeling\
    \ Soil Water Content and Reference Evapotranspiration from Climate Data Using\n\
    Deep Learning Method. Appl. Sci. 2021, 11, 5029. [CrossRef]\n103. Saggi, M.K.;\
    \ Jain, S. Reference evapotranspiration estimation and modeling of the Punjab\
    \ Northern India using deep learning.\nComput. Electron. Agric. 2019, 156, 387–398.\
    \ [CrossRef]\n104. Zhang, J.; Zhu, Y.; Zhang, X.; Ye, M.; Yang, J. Developing\
    \ a Long Short-Term Memory (LSTM) based model for predicting water\ntable depth\
    \ in agricultural areas. J. Hydrol. 2018, 561, 918–929. [CrossRef]\n105. Loggenberg,\
    \ K.; Strever, A.; Greyling, B.; Poona, N. Modelling Water Stress in a Shiraz\
    \ Vineyard Using Hyperspectral Imaging\nand Machine Learning. Remote Sens. 2018,\
    \ 10, 202. [CrossRef]\n106. Aghi, D.; Mazzia, V.; Chiaberge, M. Local Motion Planner\
    \ for Autonomous Navigation in Vineyards with a RGB-D Camera-Based\nAlgorithm\
    \ and Deep Learning Synergy. Machines 2020, 8, 27. [CrossRef]\n107. Badeka, E.;\
    \ Vrochidou, E.; Papakostas, G.A.; Pachidis, T.; Kaburlasos, V.G. Harvest Crate\
    \ Detection for Grapes Harvesting Robot\nBased on YOLOv3 Model. In Proceedings\
    \ of the 2020 Fourth International Conference On Intelligent Computing in Data\
    \ Sciences\n(ICDS), Fez, Morocco, 21–23 October 2020; pp. 1–5. [CrossRef]\n108.\
    \ Majeed, Y.; Karkee, M.; Zhang, Q.; Fu, L.; Whiting, M.D. Determining grapevine\
    \ cordon shape for automated green shoot\nthinning using semantic segmentation-based\
    \ deep learning networks. Comput. Electron. Agric. 2020, 171, 105308. [CrossRef]\n\
    109. Pinto de Aguiar, A.S.; Neves dos Santos, F.B.; Feliz dos Santos, L.C.; de\
    \ Jesus Filipe, V.M.; Miranda de Sousa, A.J. Vineyard trunk\ndetection using deep\
    \ learning—An experimental device benchmark. Comput. Electron. Agric. 2020, 175,\
    \ 105535. [CrossRef]\n110. Mockus, J. Bayesian approach to global optimization.\
    \ In Mathematics and its Applications (Soviet Series); Kluwer Academic\nPublishers\
    \ Group: Dordrecht, The Netherlands, 1989; Volume 37, p. xiv+254.\n111. Zoph,\
    \ B.; Le, Q.V. Neural Architecture Search with Reinforcement Learning. arXiv 2016,\
    \ arXiv:1611.01578.\n112. Howard, A.; Sandler, M.; Chu, G.; Chen, L.C.; Chen,\
    \ B.; Tan, M.; Wang, W.; Zhu, Y.; Pang, R.; Vasudevan, V.; et al. Searching for\n\
    MobileNetV3. 2019. Available online: http://xxx.lanl.gov/abs/1905.02244 (accessed\
    \ on 13 November 2020) .\n113. Nentwig, W.; Frank, T.; Lethmayer, C. Sown weed\
    \ strips: Artiﬁcial ecological compensation areas as an important tool in\nconservation\
    \ biological control. In Conservation Biological Control; Academic Press: Cambridge,\
    \ MA, USA, 1998; pp. 133–153.\n"
  inline_citation: '>'
  journal: Remote sensing (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2072-4292/14/3/638/pdf?version=1644317191
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: A Review of the Challenges of Using Deep Learning Algorithms to Support Decision-Making
    in Agricultural Activities
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1515/jisys-2022-0046
  analysis: '>'
  authors:
  - S. Premkumar
  - A. N. Sigappi
  citation_count: 5
  full_citation: '>'
  full_text: '>

    Research Article

    S. Premkumar* and AN. Sigappi

    IoT-enabled edge computing model for smart

    irrigation system

    https://doi.org/10.1515/jisys-2022-0046

    received January 10, 2022; accepted March 16, 2022

    Abstract: Precision agriculture is a breakthrough in digital farming technology,
    which facilitates the appli-

    cation of precise and exact amount of input level of water and fertilizer to the
    crop at the required time for

    increasing the yield. Since agriculture relies on direct rainfall than irrigation
    and the prediction of rainfall

    date is easily available from web source, the integration of rainfall prediction
    with precision agriculture

    helps to regulate the water consumption in farms. In this work, an edge computing
    model is developed for

    predicting soil moisture in real time and managing the water usage in accordance
    with rain prediction. A

    soil moisture prediction hybrid algorithm (SMPHA) has been developed that revolves
    around the decision-

    making techniques with live environmental parameters including weather parameters
    for the prediction of

    soil moisture through the impact of precipitation. Numerous algorithms with the
    combination of regression

    + clustering are estimated, and it is inferred that XGBoost + k-means outperforms
    other algorithmic com-

    binations that is deployed in edge model. This model is used as an intermediary
    between the end IoT

    devices and cloud that results in the saving of computationally intensive processing
    performed on cloud

    servers. The servers located on a local edge network perform the developed algorithmic
    computations.

    Avoiding transmission over the cloud results in signiﬁcant latency, response time,
    and computation power

    savings and therefore increases the eﬃciency of data transfer. The proposed edge
    computing model is

    implemented in Raspberry Pi as an edge, Heroku as cloud, and edge nodes as the
    combination of Pi with

    actuators and sensors. The monitored data from Pi are stored in MongoDB webserver
    that is controlled by

    Web dashboard. Finally, the developed model is implemented in cloud and edge where
    the edge server

    implementation performs better in terms of latency, bandwidth, throughput, response
    time, and CPU

    memory usage.

    Keywords: smart irrigation, edge-based irrigation, edge computing, precision agriculture,
    soil moisture

    prediction, irrigation management system, IoT, oﬄoading mechanism

    1 Introduction

    It is evident that agriculture always has a specialized role in the anthrophonic
    evolution and has been

    serving as an important economic factor for the growth of a country [1]. Around
    58% of the population

    depend on agriculture as the chief source of livelihood in India. The quality
    and productivity of agricultural

    products have declined over these years as several factors have inﬂuenced the
    crop productivity both

    directly and indirectly. Some major factors that aﬀect the crop production are
    climatic changes, global

    warming, and water scarcity [2]. The agricultural land’s productivity is aﬀected
    by the direct and indirect

    

    * Corresponding author: S. Premkumar, Department of Computer Science and Engineering,
    Faculty of Engineering and

    Technology, Annamalai University, Chidambaram - 608002, Tamilnadu, India, e-mail:
    premambal@gmail.com

    AN. Sigappi: Department of Computer Science and Engineering, Faculty of Engineering
    and Technology, Annamalai University,

    Chidambaram - 608002, Tamilnadu, India, e-mail: an.sigappi@gmail.com

    Journal of Intelligent Systems 2022; 31: 632–650

    Open Access. © 2022 S. Premkumar and AN. Sigappi, published by De Gruyter.

    This work is licensed under the Creative

    Commons Attribution 4.0 International License.

    changes in climate [3,4]. The crop growth has been already aﬀected by the changes
    in climate incurred by

    global warming. The nutrition quality of soil, ground water level, sea, and ocean
    are aﬀected by the

    modiﬁcations in average temperature, rainfall, and extreme weather conditions
    such as hail storms, dust

    storms, heatwaves, etc. due to global warming [5,6]. Degradation of soil is primarily
    created by various

    methods including 93.7% by water erosion, 9.5% by wind erosion, 5.9% by salinity
    and alkalinity, etc.

    Further changes in climate would inﬂuence adversely the crop production [7]. Since
    water is an indispen-

    sable requirement for plants and cultivation, the high level of soil is eroded
    and thereby the fertility is also

    declined. Due to the ever-changing climate, water scarcity has become a huge problem.
    Drought-like

    conditions is already formed in several areas and thereby the present and conventional
    farming practices

    are not suitable. New and unique environment preserving techniques are the need
    of the hour [8].

    The conventional approaches in agriculture are enhanced by the advent of several
    advancements in

    technology [9]. These new improved methodologies ensure optimized utilization
    of resources, accurate

    forecast of water needs and environmental parameters, reduction of human intervention,
    etc. [10]. Conse-

    quently, the outcomes of crops in terms of yield and quality are higher with cost-eﬀective
    methods. One

    such booming technology is the Internet of things (IoT) [11].

    IoT is the collection of components embedded in the sensor for measuring and transferring
    data via

    network devices as sensed from pumps and tractors to weather stations. Primarily,
    IoT deals with the

    transmission and reception of data related to farms through devices using the
    Internet for prediction

    and providing decisions to the farmers. IoT-based methodologies has brought a
    changeover in agricultural

    patterns and farming approaches [12]. IoT devices can gather information about
    soil moisture, chemical

    properties, dam levels, livestock health, and weather details in real time. The
    information acquired from IoT

    devices facilitates the farmers in tracking farms periodically. Farmers can save
    time and money by

    responding faster to farm conditions. Cloud computing models integrated with on-ﬁeld
    agricultural sensors

    need to be incorporated for tackling the issue of processing huge voluminous data.

    One of the major challenges of IoT is the processing of huge datasets in a sequential
    way. Some of the

    key factors that need to be focused on this process are as follows: information
    about the type and nature of

    data, the way of acquiring the data, etc. The preliminary stage comprises acquiring
    the data and ingesting

    the data to the system. Substantial cognizance of data are achieved as the data
    pass through all the

    gateways where it is cleansed and transformed before entering into the system.
    In the near future, dynamic

    prediction of soil moisture and precipitation techniques are to be developed for
    smart irrigation systems.

    Therefore, a system is developed for eﬃcient and optimal utilization of fresh
    water in irrigation along with

    drip irrigation system. It aids in ﬁnding which one of the plants fails to get
    suﬃcient water. When the water

    supply is provided the next day, this delay should not disturb the system. It
    becomes important for the

    farmers to understand the optimal usage of water and fertilizers to bring out
    sustenance in the agricultural

    industry. Therefore, processing must be done for analyzing the data, so that patterns
    can be analyzed and

    planning can be done for the long term, accordingly. Hence, it gives a broad vision
    in deciding where the

    processing is to be done exactly. Therefore, it is obvious that not all data are
    crucial, and it provides a clear

    view of which data need to be stored, discarded, and retained for both long-term
    and short-term purposes.

    Thus, all these challenging issues require to be addressed and that is where storage
    technologies are

    actually highlighted. The poor quality of Internet access in developing nations
    makes the implementation

    quite challenging. An applicable solution to solve this problem is through edge
    computing where the

    essential data could be oﬄoaded from the cloud over the edge of the cloud, and
    this is the exact point

    where the approach of smart sensing with edge computing gets in.

    With the purpose of broadening the potential of edge computing and using it in
    the agriculture domain,

    a novel approach using machine learning (ML) methods is proposed for analyzing
    the data acquired by the

    IoT devices deployed at the farm. Here, the data acquired from IoT components
    undergo preprocessing and

    ML models on the edge nodes to analyze and assess the appropriate results for
    providing the best instruc-

    tions for controlling the actuators (e.g., light, pumps at diﬀerent locations)
    in the farms.

    This article presents an automated system, as shown in Figure 1, to predict the
    soil moisture using the

    ﬁeld information acquired from the self-designed sensor node deployed at the ﬁeld
    and the forecast

    information of weather via Internet. A unique algorithm has been developed that
    revolves around the

    IoT-enabled edge computing model for smart irrigation system

    

    633

    machine learning techniques for the prediction of soil moisture. Here, many algorithms
    with the combina-

    tion of regression + clustering was estimated, and it is inferred that XGBoost
    + k-means outperforms other

    algorithmic combinations, and therefore, it is deployed for the prediction of
    soil moisture in the proposed

    work. The proposed algorithm makes eﬀective irrigation decisions with optimized
    usage of water in a more

    accurate and reliable manner. The eﬀective decision-making refers to the process
    of predicting the rainfall,

    thereby reducing the water usage in advance by the proposed algorithm in accordance
    with the predicted

    rainy days. Through this automatic decision-making, over watering is avoided by
    saving the soil. The

    server-side software is developed with node-side connectivity using the information
    for visualization and

    decision support features. This proposed algorithm is implemented in edge to prove
    the eﬃciency of the

    edge server handling the automated system better than the cloud control. The performance
    of the decen-

    tralized edge-based architecture has been evaluated for downloading the hybrid
    algorithm from cloud in

    real time execution. The performance can be enhanced by adopting edge computing
    architecture and

    measured with the help of network parameters like latency, bandwidth, and response
    time. Edge computing

    capacity is also estimated using the CPU processor and memory consumption while
    executing the proposed

    algorithm with irrigation scheduling.

    2 Related work

    In ref. [13], a smart irrigation system not aﬀected by communication disconnection
    and delay is developed

    using edge nodes deployed at the farms. Environmental parameters have an intricate
    impact on the plant

    growth. It becomes necessary for evaluating multiple AI models simultaneously
    in an actual cultivation

    environment for comparing AI models under the same conditions. Due to the working
    of existing irrigation

    systems on the cloud, communication is instable in the concurrent evaluation of
    AI models. However, the

    instability does not induce an edge node in its performance.

    The factors such as type of plant, soil, climate, humidity, temperature, and soil
    moisture need to be

    considered for the irrigation system packed with potential smart decisions. The
    nature and type of plant,

    soil, and climate are queried by ontology (branch of metaphysics dealing with
    the nature of being), whereas

    other factors such as temperature, humidity, and soil moisture are sensed by the
    sensor network. The

    trained ML model predicts the watering decisions based on ontology and other factors
    as mentioned earlier.

    Figure 1: Architecture of the proposed system.

    634

    

    S. Premkumar and AN. Sigappi

    Smart irrigation has three modules: (i) sensor network modules that sense the
    parameters impacting the water

    requirement by using sensors DHT22, light sensor BH1750, and HL-69 hygrometer
    for sensing the temperature,

    soil moisture, light, and humidity in air. (ii) Edge and IoT server’s module to
    send and receive data through

    HTTP requests. (iii) Training module in which KNN is applied on the sample dataset
    for training and decision-

    making regarding the water needs. Based on the input values, the trained model
    categorizes the input into ﬁve

    possible classes: highly not needed, not needed, average, needed, and highly needed
    [14].

    A decentralized smart irrigation approach is proposed for strawberry greenhouses
    in contrast to con-

    ventional cloud-based solutions for keeping the agricultural data at the edge
    of the network. A full-scale

    smart irrigation system in an actual strawberry greenhouse environment is developed
    after a small-scale

    smart irrigation networking prototype system and a reference architecture targeting
    edge data distribution

    for strawberry greenhouse applications are framed. A three-step industrial approach
    is formed for

    designing, implementing, and validating a solution for smart strawberry irrigation
    in greenhouses and

    keeping the corresponding data at the edge of the network at the same time: (i)
    A small-scale smart

    irrigation prototype solution with oﬀ-the-shelf hardware and software equipment
    is tested and evaluated

    on various types of plants for gaining useful insights for deployments on a large
    scale. (ii) A reference

    network architecture is designed for targeting smart irrigation and edge data
    distribution speciﬁcally for

    strawberry greenhouses. (iii) A large-scale system in an actual strawberry greenhouse
    environment is

    developed in Greece, incorporating the proposed reference architecture [15].

    Edge computing is proposed for addressing the issues by taking advantage of computing
    resources in

    the edge of the network. The issues such as an edge mobile device make it easier
    to achieve low end-to-end

    latency, high bandwidth, and low jitter to services located on the edge network.
    An edge can enforce the

    privacy policies of its owner prior to the release of the data to the cloud through
    edge analytics. If a cloud

    service becomes unavailable due to network failure, cloud failure, or a denial-of-service
    attack, a fallback

    service on a nearby edge can temporarily mask the failure. Cloud services, partial
    analysis, and control

    functions are extended to the edge nodes from the cloud data center. Edge nodes
    facilitate the timely

    monitoring of sensors in smart farming by the reduced latency and enhanced data
    transmission. Due to

    these factors, edge computing is applied through farming [16,17]. A three-tier
    open-source software plat-

    form we proposed by authors, and the platform enhanced the precision agriculture
    by introducing edge

    computing and fog computing. An network functions virtualization (NFV)-based approach
    is deployed for

    performing the local operational decisions at the edge level for mitigating the
    inﬂuence of network failures

    while using cloud data centers [18]. For control processing in smart farming,
    a platform enabling cost-

    eﬀective sensor/actuator network based on IoT, utilizes edge computing [19,20].

    The authors in ref. [21] predicted the soil moisture using a mathematical model
    that measures the

    values given by a sensor matrix on the ground. Due to the huge interval in measurements
    (10 minutes), the

    model presented estimated error by more than 10%. This methodology has incorporated
    the online

    approach by making the sensors to send data every minute to edge devices without
    time-based interruption.

    The authors of ref. [22] applied a combinative approach of using ﬁeld sensor network’s
    data along with

    weather forecast station’s data for the management of optimality in water conditions
    for the enhanced

    growth of grapes. The generated data are forwarded to a web server, which displays
    graphics without

    statistical analysis of such data. The analysis must be performed a posteriori
    by the user.

    The watering mechanism for a plant via IoT methodology is devised by the proposed
    smart irrigation

    model without acquiring any pre-processed data. A prototype application is developed,
    which gets adapted

    to the parameters needed in irrigation after a couple of human-made irrigations.
    With the usage of various

    ML algorithms, several tests are devised for manual and automated irrigations
    for the performance evalua-

    tion. After the evaluation using four diﬀerent ML algorithms such as logistic
    regression (LR), K-nearest

    neighbors (KNN), Gaussian naive Bayes (GNB), and gradient boosting regression
    trees (GBRT), it is found

    that GBRT outperforms other algorithms. To analyze the overall performance, a
    test bed for the sensor edge,

    mobile client, and the decision service on the cloud is established. Two diﬀerent
    indoor species are selected

    as test items for the prototype, namely, Peace lily and Sardinia. The outcomes
    were quite good, and it is

    inferred that the prototype has learned the patterns of irrigation and making
    decisions automatically with a

    high rate of accuracy [23].

    IoT-enabled edge computing model for smart irrigation system

    

    635

    The authors from ref. [24] adopted the deep learning methodology for detecting
    the type and the

    category of the plant using an automated plant irrigation system. The water necessity
    of the plant is

    determined using the recognition of predeﬁned set of plant images and data set
    acquired from farm. It

    utilizes the database for fetching the irrigation information after the recognition
    process is completed.

    Modeling the training processes are time consuming as voluminous set of images
    needs to be stored.

    The authors in refs [25,26] incorporated ML methods in the irrigation decision
    support model using a

    pre-processed irrigation data set. A model is developed for learning the irrigation
    needs of any plants

    progressively rather than using a readily available dataset. Several ML algorithms
    are evaluated with their

    precision for concluding the irrigation decisions. Manual irrigations are performed
    two times before making

    precise decisions. Due to the dynamicity in model, data processing is done progressively,
    and it can be

    applied to several plants having varying irrigation conditions. There is a need
    for the learning model that

    can be trained by itself using a comparatively lighter learning process using
    environmental parameters that

    do not need larger storage in the system but need higher computation. From the
    aforementioned survey for

    making a precise decision with instant computation locally, edge computing needs
    to be integrated into the

    irrigation system. This article is directed towards presenting a platform that
    implies IoTs and edge com-

    puting in monitoring soil moisture via sensors, data communication between sensors
    and edge devices, and

    an Analytics-as-a-Service cloud. It analyzes the collected data in the form of
    a density map of soil moisture

    for denoting the areas in need of greater or lesser frequency of irrigation. Here,
    density map does not refer to

    the geographical point data by satellite mapping, and it actually denotes the
    point of dry area and watery

    area through soil moisture detection point. This point is averaged among areas
    of irrigation to be done and

    the irrigation process is controlled with prediction of rainfall using the proposed
    system.

    3 The proposed system

    The proposed learning model for irrigation is implemented in a prototype IoT system
    that has four compo-

    nents: (i) Edge node layer – This layer consists of sensors, actuator, and two
    microcontrollers. In this layer,

    edge node acquires the sensor data from the surroundings and controls the actuator
    for actuating water

    pumps to start irrigation. (ii) Edge server layer – This layer consists of Raspberry
    Pi that act as edge server

    and capable of multitask processing. Here, edge server controls the edge nodes
    for sending signal and

    receiving data at regular interval of time. It is also connected to the cloud
    server for receiving developed and

    trained machine learning model to be deployed and make irrigation decision for
    controlling edge nodes.

    (iii) Edge service layer – This layer is deployed in the edge server and it is
    responsible for controlling the

    whole system through a developed web dashboard. The dashboard has live feed data,
    control of edge

    nodes, and cloud services access. This service layer also has the control access
    of the proposed machine

    learning model. (iv) Cloud server layer – This layer composed of cloud services
    and cloud storage where its

    role is to train the machine learning model and store the data in database. It
    sends the trained proposed

    model to the edge server for decision-making regarding irrigation scheduling.
    The comprehensive inter-

    connections in the system are shown in Figure 2. The proposed IoT-based smart
    irrigation system includes

    ﬁve major components: ﬁeld deployed module, Web-based interface, Web API weather
    input, soil moisture

    prediction mechanism, and edge communication model.

    3.1 Field deployed module

    In the ﬁeld requirements, a wireless sensor network of the sensor nodes needs
    to be deployed as shown in

    Figure 3. Here, ﬁeld data collection device accommodates four diﬀerent sensors:
    Capacitive Soil Moisture

    Sensor V2.0, DS18B20 Water Proof Temperature Sensor Probe for soil temperature,
    ultraviolet (UV) Light

    Radiation, DHT11 – Temperature and Humidity Sensor Module, and GYML8511 Analog
    Output Ultra-Violet

    636

    

    S. Premkumar and AN. Sigappi

    Light Sensor Module. An Arduino Mega connected to Raspberry Pi 4 Model-B read,
    the output of these

    sensors where the program is developed in Python for the Pi model to fetch the
    hourly data from sensors

    and store the data in MongoDB [27] database. It is then synchronized with the
    server database using the

    developed web service. A Wi-Fi-enabled Arduino controls the water pump connected
    to a relay switch.

    Figure 2: Components of the proposed system.

    Figure 3: Real-time prototype of the proposed edge model.

    IoT-enabled edge computing model for smart irrigation system

    

    637

    For the real time monitoring, a trigger is made for controlling the web service
    from the responsive web-

    based interface. The irrigation decisions are checked periodically by the proposed
    model performed in the

    server. The water pump is actuated, and irrigation process is started only if
    the server makes any irrigation

    decision. A wireless sensor network (WSN) [28] scenario with ZigBee [29] technology
    can be implemented

    for a large farming area in which several sensor nodes can be aﬃxed in the speciﬁed
    area and every sensor

    node possesses sensors similar to a standalone device. Then, the Arduino Mega
    reads the sensor output

    connected to ZigBee for transferring data to Gateway Node for aggregating the
    received data and storing it

    in MongoDB locally and also for transferring the data via web service to the edge
    server.

    3.2 Web-based interface

    The proposed framework consists of a web-based application to allow farmers visualize
    the growing data

    and interacting with the garden in real time. In addition, users can also be able
    to examine and analyze the

    historical growing data, if needed, through functionalities such as irrigation
    control, motor control predic-

    tion model deployment, and manual data entry implemented in this web application.
    Here, Node.js was

    chosen for developing the web application [30,31], while MongoDB [27] was utilized
    as the database system.

    Data stored in the database, which is deployed in the cloud, will be used for
    further data analysis in the

    future. The web application’s functions are designed following a software design
    pattern called model-

    view-controller (MVC) as shown in Figure 4. In the frontend, ChartJS is used to
    represent data through

    dynamic charts. The web application is also used as an interface to manage all
    the physical devices/

    actuators in the garden. To deploy the web-server to the cloud, a cloud platform
    as a service (PaaS), namely,

    Heroku, had been utilized. Heroku is a cloud platform that provides platform as
    a service (PaaS), facilitates

    the creation of applications and deploying these online rapidly [32,33]. It also
    enhances scalability and

    functionality by integrating several add-on services. The ﬁeld data are sent to
    the server by Raspberry Pi

    using this web service. This web service manages the network outage/ﬂuctuation
    during data synchroniza-

    tion from the ﬁeld device to the server by taking the help of ﬂag settings at
    the database level. The interface

    facilitates the scheduling of irrigation along with visualizing real time sensors
    and predicted soil moisture

    for upcoming days and precipitation information. By using the denoted threshold
    value of soil moisture

    suggested by agronomists, the irrigation can be scheduled by the user. The system
    maintains the threshold

    value depending on the predicted pattern of soil moisture and precipitation information.
    The process of

    irrigation is initiated automatically and stopped after the speciﬁed threshold
    value generated from the

    proposed algorithm of soil moisture when it is reached.

    Figure 4: Web interface for the irrigation system.

    638

    

    S. Premkumar and AN. Sigappi

    3.3 Web API weather input

    The weather prediction data are collected by a web service developed in Python.
    The forecast data such as

    humidity, temperature, ultra violet index, precipitation, and cloudiness of web
    forecasting portals like

    Open Weather API are aggregated by the developed web service [34]. These portals
    provide the forecasted

    information in HTML, XML, or JSON format. The predicted data with JSON format
    are read by the developed

    web and stored in database at the edge server, which is concerned in the prediction
    algorithm. Also, these

    data are utilized as testing dataset in the ML model for predicting the soil moisture.

    3.4 Soil moisture prediction mechanism

    An algorithm for predicting the soil moisture based on data derived from ﬁeld
    sensors and weather fore-

    casting using the combination of supervised and unsupervised machine learning
    techniques has been

    developed underpinned by regression algorithms and k-means clustering for estimating
    the diﬀerence/

    change in soil moisture owing to weather conditions. Many regression algorithms
    are compared against

    each other and infusing each of them with k-means to check the preciseness in
    mean square error (MSE),

    R2, accuracy and mean absolute percentage error (MAPE) for prediction of soil
    moisture of upcoming days

    with the help of sensor data and weather forecasting days. The information about
    soil moisture for the

    upcoming days and suggestions for irrigation in accordance with the prescribed
    levels of soil moisture and

    predicted precipitation values, thereby saving energy and water, is presented
    by the algorithm. The infor-

    mation generated from the device and the predicted values from the algorithm soil
    moisture prediction

    hybrid algorithm (SMPHA) are stored in the server.

    3.5 Edge communication model

    The communication protocols in the proposed framework are ﬂexible and transparent
    in nature for

    accepting both wired and wireless methodologies. For the maximum utilization of
    potentiality in edge

    computing components, the communication among various components in the edge-IoT
    system requires

    intense probing by using the versatility among the devices in network edges. For
    transferring the data

    gathered from pivot sensors, a communication technology such as Zigbee [35] is
    needed for the irrigation

    systems. Therefore, the communication component in the proposed work is classiﬁed
    into three main areas

    as shown in Figure 5. The Message Query Telemetry Transport (MQTT) protocol is
    used for the

    Figure 5: Proposed edge communication model.

    IoT-enabled edge computing model for smart irrigation system

    

    639

    communication in the proposed system. The analysis in ref. [36] presented seven
    IoT messaging protocols

    (MQTT, CoAP, XMPP, AMQP, DDS, REST-HTTP, and WebSocket) as communication protocols
    that play a

    major role in smart farming. The authors have concluded that MQTT proved to be
    the most secure protocol

    after probing all the protocols with respect to latency, energy and bandwidth
    requirements, throughput,

    reliability, and security. Moreover, MQTT is secure in both end-to-end architecture
    and gateway server

    architecture. In an MQTT setup, a MQTT server termed as MQTT broker executes on
    the IoT solution [37].

    Under a common identiﬁer, a “publisher” and a “subscriber” link among themselves
    to this broker. In the

    IoT solution, publishers and subscribers are the IoT devices and IoT hubs or control
    devices, respectively.

    When the publishers have new data for recording, the data are published to the
    broker. The broker then

    ﬂags that it has new publisher data, and the corresponding data are read by the
    subscriber. Then, the

    subscriber analyzes the data and reacts accordingly.

    The ﬁrst level accomplishes with connecting the end users to system with the help
    of mobile or web-

    based applications through the Internet. The next level (cloud computing server)
    deals with the connection

    of web server and MQTT broker for directing the user requests and other components
    at the edge landscape

    or from the farms to the right cloud-based services like displaying the real time
    status of the farm for the

    users, triggering a new deployment of the updated ML model to the corresponding
    edge node. The third

    level (farming area) is directed toward the deployment of sensors and IoT devices
    (actuators) for commu-

    nicating with other components in the entire system.

    4 Deployment of soil moisture prediction hybrid algorithm

    The watering mechanism of the plant has diﬀerent approaches in the proposed model.
    Primarily, the system

    is trained with manual irrigations datasets during the process of learning with
    respect to suggestions

    deﬁned by agronomists. The model is trained to learn the needs of irrigation in
    the ﬁrst level of deployment

    in cloud without the inclusion of pre-processed data. After acquiring the required
    data and training, the

    proposed system is initiated to grasp the plant’s watering needs by undergoing
    plenty of manual irrigations.

    Thereafter, manual irrigation is not required and the system makes automated decisions
    in watering using

    the gathered data and the application of ML methods. The proposed model then decides
    the irrigation

    strategies automatically using ML methods without the need including collected
    datasets in the automatic

    irrigation process. The proposed model can be improved through the learning process
    when the number of

    precise irrigation inputs is provided to the model at each stage of training.

    The decision-making procedure is developed with two modules for irrigation strategies
    according to the

    soil moisture prediction for upcoming days. The ﬁrst module deals with training
    the model in cloud with

    manual irrigation datasets through steps such as data collection, data preprocessing,
    training, and model

    development. The system acquires values of air temperature (TH), soil temperature
    (SMT), soil moisture

    (SM), humidity (HU), and ultraviolet rays (UV) periodically from the physical
    environment in the data

    collection stage, which is essentially required for arriving at the watering decisions.
    Also, the time of

    performing the manual irrigation is recorded in the database. These data are timestamped
    and stored in

    as datasets to aid in making decisions for knowing the time of irrigation. In
    the next step of pre-processing,

    inconsistencies are eliminated and outliers caused by sensor errors are detected
    from the irrigation dataset,

    thereby helping in the removal of broken data. The training stage involves the
    application of supervised

    machine learning (ML) algorithms. Here the regression algorithms such as support
    vector regression (SVR),

    multiple linear regression (MLR), lasso regression (LR), decision tree regressor
    (DTR), random forest

    regressor (RF), and XG-boost regressor (XB) techniques are used for the deployment.
    The regression algo-

    rithms are trained using the collected datasets. Finally, through training, regression
    models are created,

    namely, SVR model, MLR model, LR model, DTR model, RF model, and XB model that
    are been combined

    with the second module for decision-making.

    The second module caters to the prediction of irrigation for upcoming days by
    infusing the weather data

    as an input to the regression trained models. The live datasets from the weather
    API for future prediction of

    640

    

    S. Premkumar and AN. Sigappi

    soil moisture variable are used. The dependent variables from weather forecast
    data like temperature (TH),

    humidity (HU), ultraviolet (UV), and precipitation (PC) are tested in the aforementioned
    model for soil

    moisture prediction. Then, the regression trained model is evaluated and deployed
    using the weather

    testing data for the prediction of soil moisture in accordance with the precipitation.
    After the prediction

    of data for the upcoming days, these developed regression models are combined
    with unsupervised ML

    algorithm named k-means clustering for estimating the changes incurred in soil
    moisture prediction due to

    the impact of weather conditions. Further, each regression models with k-means
    algorithm are evaluated

    for performances in terms of irrigation decision-making process as shown in Table
    1. The combined algo-

    rithms are estimated through MAPE, MSE, R2, execution speed, power consumption,
    and accuracy. The

    estimation and computation of these parameters are detailed by the authors in
    ref. [38].

    XGBoost + k-means (XB+k-means) approach provides more accuracy with less MSE comparatively
    and

    also the R2 with 98% in soil moisture prediction using combined approach is given
    in Table 1. It is evident

    that the proposed combination performs better when compared to other regression
    + k-means-based

    approaches. XB + k-means-based hybrid machine learning algorithm is applied in
    irrigation planning

    module on account of aforementioned performance metrices of ML. Although it performs
    moderately in

    terms of execution time and power usage, it is selected for the deployment in
    edge computing as it has

    better performed in terms of accuracy, R2, MSE, and MAPE metrices. It is observed
    that the prediction of soil

    moisture for the upcoming days from the proposed algorithm (XB+k-means) is nearer
    to the actual value as

    shown in Table 2, and hence, XB+k-means is selected for the implementation of
    SMPHA in edge-based

    irrigation scheduling.

    4.1 Hardware setup

    IoT system is crucial to handle, collect, and transfer the data to the computing
    nodes at the edge or in the

    cloud. These devices are connected to the edge nodes through wireless communication
    protocols like

    ZigBee. It is used in reducing the latency and loss of data. An Arduino micro-control
    unit controls the

    combined IoT sensors and actuators at the same part of a ﬁeld into a cluster,
    each connected to a Raspberry

    Table 2: Comparison of predicted SM value with actual SM value

    Date

    Average SM value

    from sensor

    Average predicted SM

    value (XB+k-means)

    28-09-2021

    35.23

    34.04

    29-09-2021

    36.41

    37.20

    30-09-2021

    31.57

    30.46

    01-10-2021

    34.66

    33.15

    02-10-2021

    36.73

    37.12

    03-10-2021

    32.88

    33.01

    Table 1: Comparison of performance metrices obtained from various ML algorithms

    Algorithms used

    Accuracy

    R2

    MSE

    MAPE (%)

    Execution time

    Power (J)

    SVR + k-means

    0.96

    0.96

    0.25

    1.98

    0.06078

    1164.85

    MLR + k-means

    0.94

    0.88

    0.31

    2.15

    0.02075

    429.30

    LR + k-means

    0.95

    0.94

    0.32

    2.23

    0.02482

    351.35

    DTR + k-means

    0.93

    0.95

    0.29

    1.62

    0.15687

    914.70

    RF + k-means

    0.95

    0.91

    0.27

    1.57

    0.16745

    1475.13

    XB + k-means

    0.97

    0.98

    0.20

    1.08

    0.03547

    537.87

    IoT-enabled edge computing model for smart irrigation system

    

    641

    Pi that acts as an edge node in processing the gathered data and controlling the
    actuators. For example,

    Figure 2 shows an edge architecture with a Raspberry Pi connected to two components:
    Arduino Uno and

    Arduino Mega units via ZigBee connection. The ﬁrst Arduino Mega node is responsible
    for collecting data

    from sensors and the second one is for controlling the actuators in the ﬁeld.
    Depending on the sensor type

    with collecting Arduino unit, the sensors are connected via analog or digital
    PWM pins while controlling

    Arduino uno joins with actuators in the ﬁeld and controls (turn on/oﬀ) them in
    accordance with upper

    layers (from the edge web server). The trained (cloud) and deployed ML model in
    edge nodes provides the

    necessary instructions to the edge nodes.

    4.2 Web layer setup

    The deployment of web server assists the user in planning and managing the irrigation
    system. It visualizes

    the crucial information of factors like temperature of air and soil, UV, humidity,
    and soil moisture in live

    irrigation with real time updates in the form of various charts. In accordance
    with the selected ﬁeld, the web

    application redirects the user to the ﬁeld’s dashboard as shown in Figure 4. The
    dashboard consists of ﬁeld

    parameters as well as control signals for activating all the physical devices/actuators
    at the garden layer.

    These signals are denoted as switch buttons, and each switch controls (turn on/oﬀ)
    a particular kind of

    actuator (for instance, water pump to start and stop the irrigation). The user
    interface facilitates remote

    controlling of the ﬁeld by just clicking on the buttons as shown in Figure 4.

    4.3 Edge layer setup

    The edge node acts as a computing center where incoming data are analyzed and
    fed as the input vector to

    the ML model for processing and to return the control signals for activating or
    deactivating the actuators

    placed at the farm. Edge node processes the physical data (real time) at every
    end device such as the

    collected and processed data via the Raspberry Pi nodes presented in the proposed
    scheme. The prediction

    model is designed using TensorFlow API and trained, tested on Google Colab in
    this work. Amazon Web

    Service (AWS) oﬀers a library named Boto3 having many APIs to upload and download
    objects. After the

    development of model, it is transferred to Amazon S3, a service provided by AWS.
    The edge node utilizes the

    trained model from S3 for analyzing the sensed data acquired from garden’s sensors.
    The decision is

    delivered based on real time data analysis at the edge node and transmitted to
    Arduino nodes in the ﬁelds

    landscape immediately for controlling the actuators. In another ﬂow, the data
    collected from sensors are

    ﬁltered so as to keep only the modiﬁed data at the edge node before being sent
    back for mitigating the

    communication cost to the database in the cloud. These data are used in the updation
    of the ML model to

    enhance its eﬃciency.

    4.4 Analytics setup

    The main goal of this experiment lies in gathering the various physical parameters
    of a farming land via

    sensors and utilizing the fetched data along with weather forecast information
    for developing an algorithm

    using hybrid machine learning approach to infuse higher accuracy in predicting
    the soil moisture for the

    upcoming days. As discussed in Section 4, for the proper planning and provisioning
    of optimal irrigation,

    the algorithm provides a predictable estimate of soil moisture with the assistance
    of various statistical

    measures as shown in Table 1. The measures are adopted for estimating the appropriateness
    and error rate

    of the proposed algorithm. It is inferred from the experiment that, optimal irrigation
    is feasible using a good

    642

    

    S. Premkumar and AN. Sigappi

    estimation (close to the actual value) of the soil moisture (Table 2), with the
    support of ﬁeld data and

    forecast information, thereby utilizing the natural rain eﬃciently.

    The SMPHA ML model is interdependent on dynamic changes in weather environment
    where the

    models deployed on edge nodes need to change the controls accordingly after model
    gets trained con-

    tinuously. For the process of retraining, the trained model needs to be updated.
    The parameters such as TM,

    HU, ST, UV, SM about grown plants are logged for the training purpose, and these
    generated datasets are

    recorded from the already developed manual mode system [39]. The growth of the
    Indian Mundu Chilli [40]

    is taken for the observation from the ﬁrst stage to the last grown stage for 95
    days. While retraining the

    model, the training is carried in cloud without causing eﬀect to the functionalities
    at edge nodes. A signal is

    transferred to the corresponding edge server for triggering the task of updating
    the SMPHA model from the

    web server. At that time, the newly trained model is downloaded to replace the
    existing one at the con-

    sidering edge server. From then, the ML model at the edge server is called to
    be updated with the real-world

    knowledge and is ready for its garden controlling tasks (to apply in the next
    farming season).

    4.5 Work ﬂow

    The ﬂowchart in Figure 6 depicts the working of the proposed system based on the
    decision support system

    that is beneﬁcial for irrigation needed for the growth of vegetables. The chilli
    plant is grown in a growbag

    attached with sensors and Pi as shown in Figure 3 and monitored for 95 days of
    data collection. To bring out

    optimality in the irrigation system, features relating to climate, soil, crop,
    and ﬁeld infrastructure are to be

    considered. To provide several recommendations in the production of vegetables,
    decision support systems

    (DSSs) are designed, which process voluminous information [39]. This proposed
    work is the extension of

    soil moisture diﬀerences (SMD) model [41] developed for soil moisture prediction.
    The threshold values of

    soil moisture are used in the SMD model where the system schedules the irrigation
    date based on the

    predicted soil moisture and weather forecast (precipitation) information automatically
    using SVR+ k-means

    modeling. Therefore, in the extension of the aforementioned work, further more
    number of sensors are used

    to log soil moisture value, which is averaged in the proposed model. This model
    is developed in two

    divisions of ﬂowchart as shown in Figure 7, where both are interconnected. It
    is observed that the prediction

    of XB + k-mean approach provides better results as presented in Table 2.

    The ﬁrst phase of the ﬂowchart describes the hybrid algorithm for the soil moisture
    prediction (SMPHA)

    using the combination of XB + k-means algorithm. During the data collection step,
    the sensor data for the

    parameters, namely, TM, HU, ST, UV, and SM, are collected. During preprocessing,
    null values and outliers

    are removed and the preprocessed data are used to train the XG-Boost model. The
    developed model is then

    trained with variables of live weather features (TM, HU, UV, PC) obtained from
    Weather API for the

    prediction of SM data. These data are given as input to k-means clustering algorithm
    to predict the soil

    moisture, which is deﬁned as SMPHA value to be infused in the next phase of the
    ﬂowchart. The second

    phase of the ﬂowchart deﬁnes the automatic irrigation planning setup. The setup
    starts obtaining the soil

    moisture maximum (SMMax) and soil moisture minimum (SMMin) values in the dashboard
    for setting the

    maximum and minimum level of soil moisture. Then, the current soil moisture (CuSM)
    is sensed and

    compared against the threshold SMMin. If the resulting value is less than SMMin,
    the process proceeds

    with SMPHA. On the contrary, it stops the irrigation process by sending 0 to the
    relay. In SMPHA, the

    nearest precipitation date is selected and it is assigned to the predicted soil
    moisture (PSM). The SMMax is

    decided by ﬁnding the minimum of (PSM + SMMin, SMMax), and the predicted SMMax
    is further checked

    against CuSM with a condition if SMMax is greater the CuSM then it sends 1 to
    the relay as a signal to start

    irrigation. If the condition fails, then it sends 0 to stop irrigation. The process
    of automatic irrigation ends

    by forecasting the irrigation schedule in accordance with the live weather parameters.

    IoT-enabled edge computing model for smart irrigation system

    

    643

    5 Experimental setup and evaluation

    The test bed is developed and deployed, and the data are collected for the analysis
    in irrigation manage-

    ment. Here, Heroku cloud platform is used to deploy the cloud web server. The
    same cloud is also installed

    at a local edge that is at two Raspberry Pi units equipped with Wi-Fi 802.11n
    connections to denote the edge

    nodes. JMeter application is used to get sequential accesses to the web page from
    various users for eval-

    uating the network parameters. The speciﬁcation of these servers is given in Tables
    3 and 4.

    We evaluated the performance of the proposed IoT-based smart farm on two diﬀerent
    platforms,

    namely, in the cloud and on the local computer to show the feasibility and the
    beneﬁt of the edge com-

    puting scheme. Further many parameters are considered for evaluation and discussed
    in the next section to

    show that edge deployment is better than cloud.

    Figure 6: Flow chart of the proposed edge model.

    644

    

    S. Premkumar and AN. Sigappi

    5.1 Evaluation

    A hybrid machine learning methodology is used in evaluating the ﬁrst stage of
    the proposed model. The

    predicted value of the soil moisture is better in terms of their accuracy and
    error rate. From the comparison

    of the other ML algorithms as shown in Table 2, XB + k-means performs better and
    taken further to be

    deployed in edge and cloud to check its eﬃciency with each other. Therefore, for
    analyzing the eﬃciency of

    the edge server in accordance with the proposed hybrid algorithm SMPHA is evaluated
    in terms of the time

    taken to train the ML model in edge and cloud. In this experiment Raspberry Pi
    is used to train the SMPHA

    model with 196,400 rows, that is, input data sample size and takes around 1,710,000
    ms (approximately

    28.5 min). The same model when it is trained in Google Colab cloud environment,
    it takes 204,000 ms

    (approximately 3.4 min) as depicted in Table 5. The main purpose is to run the
    trained model on edge not to

    train the model at edge. So due to the lack of computing capability at the edge,
    it takes more time to train

    the model, but it can be ignored as it does not aﬀect the purpose of the proposed
    model. Here, edge is

    introduced to obtain the task of computing from the cloud (i.e., oﬄoading the
    task) by making the system

    more edge-oriented deployment. It can be accomplished rapidly as it requires only
    14 s to download a

    trained SMPHA model from the cloud to the edge node with a size of 3,101 kb as
    given in Table 5. The time to

    Figure 7: Average response time with 10 test scenarios.

    Table 3: Conﬁguration of raspberry Pi

    CPU

    Broadcom BCM2711, Quad core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5 GHz

    RAM

    8 GB LPDDR4-3200 SDRAM

    Network

    2.4 GHz and 5.0 GHz IEEE 802.11ac wireless, Bluetooth 5.0, BLE, Gigabit Ethernet

    Pinboard

    8 GB LPDDR4-3200 SDRAM

    Operating system, language

    Raspbian , Python 3

    Table 4: Conﬁguration of Heroku cloud

    Country

    United states

    Service

    Amazon web service S3

    Processor

    2.4 GHz Intel Xeon E5-2676 v3 Processor

    CPU Power

    8 GB

    Virtual CPUs

    3–5

    IoT-enabled edge computing model for smart irrigation system

    

    645

    download varies according to the size of the trained model. So, from this process
    it can be inferred that

    downloading the trained model saves time when compared to training the model at
    the edge. Through this

    in real time, deployment of the trained SMPHA model in edge is better compared
    to deployment in cloud

    services. Furthermore, network parameters like latency, throughput, bandwidth,
    and response time are

    adopted to measure the performance improvements in edge computing.

    The performance metrices taken into account are latency, bandwidth, and response
    time [42]. The

    latency of an application is the product of two factors: computing latency and
    transmission latency. The

    time spent on data processing and transmission between end devices to cloud servers
    is termed as com-

    puting latency and transmission latency, respectively. The computational capacity
    of the system decides

    the computing latency as the network servers possess a considerable amount of
    capacity to make the data

    processing faster, whereas the sensors come with limited computing capacity. The
    latency in transmission

    is increased by the end devices and cloud servers. Bandwidth: As large number
    of sensors are deployed in

    IoT, data generated would be huge that consumes an intense range of bandwidth
    and leads to several

    problems such as delay in transmission and loss of packets. It becomes unacceptable
    for the data to be

    transferred directly to cloud servers without applying compression. Therefore,
    data preprocessing and

    aggregation are needed for IoT gateways before redirecting them to remote cloud
    servers. Then, the issue

    to be confronted is to control the traﬃc ﬂow by migrating data processing and
    aggregation tasks optimally

    to decrease the bandwidth needs of the end users while maintaining the data quality.
    Response time: The

    total response time is calculated by adding up transmission and processing time.
    The local deployment of

    the proposed model for controlling IoT-based irrigation are deployed on two modes:
    (i) Cloud mode: The

    developed SMPHA model is implemented in the cloud communicating with IoT sensors
    nodes directly to

    manage the irrigation process. The data are stored and processed at the cloud
    server itself where it uses

    Heroku platform. (ii) Edge mode – Raspberry Pi is deployed as an edge server that
    involves in processing of

    the SMPHA model controlling the IoT sensor nodes. Here, the data are stored and
    processed locally within

    the edge servers. This SMPHA model from both the edge and cloud does the job of
    controlling the actuators

    to initiate and quit the working of water ﬂow motors. Through this deployment
    in both the environments,

    performance of edge server and cloud server can be checked in terms of latency,
    throughput, bandwidth,

    and response time is shown in aforementioned graphs in Figures 6, 8, and 9. This
    performance metrices is

    not feasible to calculate while deploying in real time, so the aforementioned
    scenarios of two modes are

    virtually created by generating many request and response threads between the
    servers. This sampling,

    load test, and distributed testing are conducted through JMeter application [43]
    and also veriﬁed with

    Wireshark [44] in cloud servers. The test scenario is created here by data of
    sending and receiving sampling

    data between cloud to IoT sensors and between Edge to IoT sensors. The sampling
    data considered in this

    work refer to the approximate number of requests generated by Arduino to cloud
    and Arduino to Raspberry

    Pi that are calculated in real time. The test scenario is divided into 10 days
    of sampling data collected for

    each day. The evaluation results are depicted for latency and response times in
    10 days perspective. In

    latency parameter, edge service has decreased by an average of 77.85% time compared
    to the with cloud. In

    the same manner, the response time of edge service is also decreased by 74.09%
    time compared to cloud

    service. In throughput calculation, sampling data are calculated for an hourly
    basis for the 10 hours data in

    a day. From the hourly comparisons of throughput value, edge outperforms with
    67.17% high Mbps usage.

    Through this analysis as shown in Table 6, it is evident that the proposed edge
    computing methodology

    deployed in Raspberry Pi or in local computers outperforms the cloud-oriented
    approach.

    Table 5: Comparison of model training time

    Edge

    Cloud

    Model training time

    28.4 min

    3.4 min

    Downloading time

    Not applicable

    14 s

    646

    

    S. Premkumar and AN. Sigappi

    Finally, to illustrate the eﬃciency of resource management in edge computing,
    CPU and memory

    utilization are considered for the analysis as both factors rely on the service
    execution model and the

    computational needs of the services being ﬁred from oﬀ-loaders. Figure 10 depicts
    the utilization of CPU

    and RAM on the Raspberry Pi acting as an edge node in two cases: with and without
    the deployment of

    SMPHA model on it. As shown in Figure 10, the SMPHA model aﬀects the CPU of the
    Raspberry Pi node

    signiﬁcantly as it consumed around 41.2% of the CPU compared to only 3.5% when
    it does not host the

    Figure 8: Average latency with 10 test scenarios.

    Figure 9: Average throughput value with 10 h test scenarios.

    Table 6: Performance metrices for cloud and edge services

    Performance metrices

    Cloud service

    Edge service

    Throughput (Mbps)

    0.04944

    0.08265

    Latency (ms)

    1415.8

    313.6

    Response time (ms)

    1519.6

    393.8

    Bandwidth (bps)

    86

    1,365

    IoT-enabled edge computing model for smart irrigation system

    

    647

    SMPHA model. However, the memory (RAM) utilization in both the cases (with and
    without deployment of

    an SMPHA model) is nearly the same which is around 31%. Comparatively RAM utilization
    does not have

    much diﬀerence in with and without SMPHA. It is worthwhile to note that, the CPU
    utilization is still much

    lower than the 50% of total CPU capacity in Raspberry Pi. Therefore, it becomes
    feasible for adopting edge

    server implementation in the proposed irrigation system.

    6 Conclusion

    This article proposed a novel approach to edge-based irrigation system to facilitate
    decision-making on

    watering the plants on scheduled time. The proposed approach applying IoT with
    an edge computing

    framework enables the farming system to adapt to the changes in environmental
    conditions automatically

    and eﬃciently. The process of automatic irrigation regulates irrigation according
    to the live weather para-

    meters for forecasting the irrigation process. Soil moisture prediction was performed
    using major regression

    algorithms that are again combined with k-means clustering for estimating the
    changes incurred in soil

    moisture prediction. These techniques were compared through metrics such as MAPE,
    MSE, speed, and

    power consumption from which XB + k-means was found to perform better. The XB
    + k-means algorithm

    was further used for the implementation of decision mechanism on the developed
    edge computing model.

    The proposed edge model saves the data communication cost and reduces the response
    time of IoT services.

    It can be deployed on existing devices on the network edges serving as edge nodes,
    thereby reducing the

    overall implementation cost of a large-scale IoT system. The edge-based approach
    was found to perform

    better than the cloud-based approach in terms of response time, latency, throughput,
    and bandwidth

    usage. Finally, the edge model was analyzed through CPU and memory usage while
    running with and

    without the algorithm. In both cases, the memory utilization is almost lower to
    total available resource of

    the edge device. From this, edge device can allocate its remaining resource for
    other computing services,

    which increases the eﬃciency of edge computing device. The number of end edge
    nodes can be increased

    according to the ﬁeld area and then to check the potency of the system.

    Conﬂict of interest: The authors declare no conﬂict of interest.

    Data availability statement: All data that support the ﬁndings of this study are
    included within the article.

    Figure 10: CPU and memory utilization with and without SMPHA.

    648

    

    S. Premkumar and AN. Sigappi

    References

    [1]

    India: Issues and Priorities for Agriculture, The World Bank, May 17, 2012. https://www.worldbank.org/en/news/feature/

    2012/05/17/india-agriculture-issues-priorities.

    [2]

    India at a glance in Agriculture, FAO in India. https://www.fao.org/india/fao-in-india/india-at-a-glance/en/.

    [3]

    Cavicchioli R, Ripple WJ, Timmis KN, Azam F, Bakken LR, Baylis M, et al. Scientists’
    warning to humanity: Microorganisms

    and climate change. Nature Rev Microbiol. 2019;17(9):569–86. doi: 10.1038/s41579-019-0222-5.

    [4]

    Huong NTL, Bo YS, Fahad S. Economic impact of climate change on agriculture using
    Ricardian approach: A case of

    Northwest Vietnam. J Saudi Society Agricult Sci. 2019;18(4):449–457. doi: 10.1016/j.jssas.2018.02.006.

    [5]

    Fagodiya RK, Pathak H, Bhatia A, Jain N, Kumar A, Malyan SK. Global warming impacts
    of nitrogen use in agriculture: An

    assessment for India since 1960. Carbon Management. 2020;11(3):291–301. doi: 10.1080/17583004.2020.1752061.

    [6]

    Sarkar S, Chatterjee S, Misra S. Assessment of the suitability of fog computing
    in the context of internet of things. IEEE

    Trans Cloud Comput. 2018;6(1):46–59. doi: 10.1109/TCC.2015.2485206.

    [7]

    Porter JR, Xie L, Challinor AJ, Cochrane K, Howden SM, Iqbal MM, et al. Food security
    and food production systems. In: Field

    CB, Barros VR, Dokken DJ, Mach KJ, Mastrandrea MD, Bilir TE, et al., editors.
    Climate Change 2014: Impacts, Adaptation,

    and Vulnerability. Part A: Global and Sectoral Aspects. Contribution of Working
    Group II to the Fifth Assessment Report of

    the Intergovernmental Panel on Climate Change Cambridge, United Kingdom: Cambridge
    University Press and New York,

    NY, USA; 2014. p. 485–533.

    [8]

    Lal R. Adaptation and mitigation of climate change by improving agriculture in
    India. In: S. SherazMahdi (Ed.), Climate

    Change and Agriculture in India: Impact and Adaptation. Cham: Springer International
    Publishing; 2019. p. 217–27.

    [9]

    Saravanan K, Julie G, Robinson H. (Eds.), Handbook of research on implementation
    and deployment of IoT projects in

    smart cities. Hershey: IGI global, 2019.

    [10] Baylis A. Advances in precision farming technologies for crop protection.
    Outlooks Pest Manag. 2017;28(4):158–61.

    [11]

    Mulla D, Khosla R. Historical evolution and recent advances in precision farming.
    Soil-Speciﬁc Farming Precision

    Agriculture. Boca Raton: CRC Press; 2015.

    [12] Dutta L, and Basu TK. Extraction and optimization of leaves images of mango
    tree and classiﬁcation using ANN. IJRAET

    2013;1(3):46–51.

    [13] Kawai T, Mineno H. Evaluation environment using edge computing for artiﬁcial
    intelligence-based irrigation system. 2020

    16th International Conference on Mobility, Sensing and Networking (MSN). Tokyo,
    Japan: IEEE; 2020. p. 214–9.

    [14] Munir MS, Bajwa IS, Ashraf A, Anwar W, Rashid R. Intelligent and smart irrigation
    system using edge computing and IoT.

    Complexity. 2021;2021:1–16.

    [15] Angelopoulos CM, Filios G, Nikoletseas S, Raptis TP. Keeping data at the
    edge of smart irrigation networks: A case study in

    strawberry greenhouses. Comput Netw. 2020;167:107039.

    [16] Satyanarayanan M. The emergence of edge computing. Computer. 2017;50(1):30–9.

    [17] Shi W, Dustdar S. The promise of edge computing. Computer. 2016;49(5):78–81.

    [18] Ramirez Izolan PL, Diniz Rossi F, Hohemberger R, Konzen MP, da Cunha Rodrigues
    G, Saquette LR, et al. Low-cost fog

    computing platform for soil moisture management. In: 2020 International Conference
    on Information Networking (ICOIN).

    Barcelona, Spain: IEEE; 2020. p. 499–504.

    [19] Ferrandez-Pastor F, Garcia-Chamizo, J, Nieto-Hidalgo, M, Mora-Pascual, J,
    Mora-Martínez, J. Developing ubiquitous sensor

    network platform using internet of things: application in precision agriculture.
    Sensors. 2016;16(7):1141.

    [20] Xu X, Liu X, Xu Z, Dai F, Zhang X, Qi L. Trust-oriented IoT service placement
    for smart cities in edge computing. IEEE Internet

    Things J. 2020;7(5):4084–91.

    [21] Wu X, Liu M. In-situ soil moisture sensing: Measurement scheduling and estimation
    using compressive sensing. In: 2012

    ACM/IEEE 11th International Conference on Information Processing in Sensor Networks
    (IPSN). Beijing, China: IEEE; 2012.

    p. 1–11.

    [22] Kameoka T, Nishioka K, Motonaga Y, Kimura Y, Hashimoto A, Watanabe N. Smart
    sensing in a Vineyard for advanced

    viticultural management. In: Proceedings of the 2014 International Workshop on
    Web Intelligence and Smart Sensing.

    Saint Etienne France; 2014. p. 1–4.

    [23] Cagri Serdaroglu K, Onel C, Baydere S. IoT-based smart plant irrigation system
    with enhanced learning. In: 2020 IEEE

    Computing, Communications and IoT Applications (ComComAp.) Beijing, China: IEEE;
    2020. p. 1–6.

    [24] Kwok J, Sun Y. A smart IoT-based irrigation system with automated plant recognition
    using deep learning. In: Proceedings

    of the 10th International Conference on Computer Modeling and Simulation - ICCMS2018.
    Sydney, Australia: ACM Press;

    2018. p. 87–91.

    [25] Goldstein A, Fink L, Meitin A, Bohadana S, Lutenberg O, Ravid G. Applying
    machine learning on sensor data for irrigation

    recommendations: Revealing the agronomist’s tacit knowledge. Precision Agricult.
    2018;19(3):421–44.

    [26] Vij A, Vijendra S, Jain A, Bajaj S, Bassi A, Sharma A. IoT and machine learning
    approaches for automation of farm irrigation

    system. Proc Comput Sci. 2020;167:1250–7.

    [27] Krishnan H, Scholar R. MongoDB – a comparison with NoSQL databases. Int J
    Scientiﬁc Eng Res. 2016;7(5):1035–7.

    IoT-enabled edge computing model for smart irrigation system

    

    649

    [28] Ojha T, Misra S, Raghuwanshi NS. Wireless sensor networks for agriculture:
    The state-of-the-art in practice and future

    challenges. Comput Electr Agricult. 2015;118:66–84.

    [29] Gutierrez J, Villa-Medina JF, Nieto-Garibay A, Porta-Gandara MA. Automated
    irrigation system using a wireless sensor

    network and GPRS module. IEEE Trans Instrument Measurement. 2014;63(1):166–76.

    [30] Chanthakit S, Keeratiwintakorn P, Rattanapoka C. An IoT system design with
    real time stream processing and data ﬂow

    integration. In: 2019 Research, Invention, and Innovation Congress (RI2C.) Bangkok,
    Thailand: IEEE; 2019. p. 1–5.

    [31] Lv H, Wang S. Design and application of IoT microservices based on Seneca.
    USA: DEStech Transactions on Computer

    Science and Engineering, (icte.). 2016.

    [32] Lee B-H, Dewi EK, Wajdi MF. Data security in cloud computing using AES under
    HEROKU cloud. In: 2018 27th Wireless and

    Optical Communication Conference (WOCC). Hualien: IEEE; 2018. p. 1–5.

    [33] Lopez Pena MA, Munoz Fernandez I. SAT-IoT: An architectural model for a high-performance
    fog/edge/cloud IoT platform.

    In: 2019 IEEE 5th world forum on internet of things (WF-IoT.) Limerick, Ireland:
    IEEE; 2019. p. 633–8.

    [34] Weather API. Retrieved from https://openweathermap.org/api.

    [35] Drew Gislason. Zigbee wireless networking, 1st ed. Newnes, London: Elsevier
    Publisher; 2008.

    [36] Tanabe K, Tanabe Y, Hagiya M. Model-based testing for MQTT applications.
    In: Virvou M, Nakagawa H, Jain LC. (Eds.),

    Knowledge-Based Software Engineering: 2020. Cham: Springer International Publishing;
    2020. p. 47–59.

    [37] Babun L, Denney K, Celik ZB, McDaniel P, Uluagac AS. A survey on IoT platforms:
    Communication, security, and privacy

    perspectives. Comput Netw. 2021;192:108040.

    [38] Rastogi K, Lohani D. Edge computing-based internet of things framework for
    indoor occupancy estimation. Int J Ambient

    Comput Intell. 2020;11(4):16–37.

    [39] Premkumar S, Sigappi AN. Functional framework for edge-based agricultural
    system. In: AI, Edge and IoT-based Smart

    Agriculture, 1st ed. USA: Academic Press, Elsevier; 2021. p. 71–100.

    [40] Phani Kumar J, Paramaguru P, Arumugam T, Manikanda Boopathi N, Venkatesan
    K. Genetic divergence among Ramnad

    mundu chilli (Capsicum annuum L.) genotypes for yield and quality. Electr J Plant
    Breeding. 2021;12(1):228–34.

    [41] Goap A, Sharma D, Shukla AK, Rama Krishna C. An IoT-based smart irrigation
    management system using Machine learning

    and open source technologies. Comput Electronic Agricult. 2018;155:41–9.

    [42] Aslanpour MS, Gill SS, Toosi AN. Performance evaluation metrics for cloud,
    fog and edge computing: A review, taxonomy,

    benchmarks and standards for future research. Internet Things. 2020;12:100273.

    [43] Sunardi A, Suharjito MVC architecture: a comparative study between Laravel
    framework and slim framework in freelancer

    project monitoring system web based. Proc Comput Sci. 2019;157:134–41.

    [44] Robert Shimonski. The wireshark ﬁeld guide, 1st ed. New York: Syngress Press,
    Elsevier; 2013.

    650

    

    S. Premkumar and AN. Sigappi

    '
  inline_citation: '>'
  journal: Journal of intelligent systems
  limitations: '>'
  pdf_link: https://www.degruyter.com/document/doi/10.1515/jisys-2022-0046/pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: IoT-enabled edge computing model for smart irrigation system
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/plants11233344
  analysis: '>'
  authors:
  - Narendra Singh Chandel
  - Yogesh Anand Rajwade
  - Kumkum Dubey
  - Abhilash K. Chandel
  - A. Subeesh
  - Mukesh Tiwari
  citation_count: 8
  full_citation: '>'
  full_text: ">\nCitation: Chandel, N.S.; Rajwade,\nY.A.; Dubey, K.; Chandel, A.K.;\n\
    Subeesh, A.; Tiwari, M.K. Water\nStress Identiﬁcation of Winter Wheat\nCrop with\
    \ State-of-the-Art AI\nTechniques and High-Resolution\nThermal-RGB Imagery. Plants\
    \ 2022,\n11, 3344. https://doi.org/10.3390/\nplants11233344\nAcademic Editors:\
    \ John T. Hancock\nand Mikihisa Umehara\nReceived: 8 October 2022\nAccepted: 27\
    \ November 2022\nPublished: 2 December 2022\nPublisher’s Note: MDPI stays neutral\n\
    with regard to jurisdictional claims in\npublished maps and institutional afﬁl-\n\
    iations.\nCopyright:\n© 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\n\
    This article is an open access article\ndistributed\nunder\nthe\nterms\nand\n\
    conditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nplants\nArticle\nWater Stress Identiﬁcation of Winter Wheat Crop with\n\
    State-of-the-Art AI Techniques and High-Resolution\nThermal-RGB Imagery\nNarendra\
    \ S. Chandel 1, Yogesh A. Rajwade 2,*\n, Kumkum Dubey 1, Abhilash K. Chandel 3,4,*\n\
    , A. Subeesh 1\nand Mukesh K. Tiwari 5\n1\nAgricultural Mechanization Division,\
    \ ICAR—Central Institute of Agricultural Engineering,\nBhopal 462038, MP, India\n\
    2\nIrrigation and Drainage Engineering Division, ICAR—Central Institute of Agricultural\
    \ Engineering,\nBhopal 462038, MP, India\n3\nDepartment of Biological Systems\
    \ Engineering, Virginia Tech Tidewater AREC, Suffolk, VA 23437, USA\n4\nCenter\
    \ for Advanced Innovation in Agriculture (CAIA), Virginia Tech, Blacksburg, VA\
    \ 24061, USA\n5\nCollege of Agricultural Engineering and Technology, Anand Agricultural\
    \ University, Godhra 389001, GJ, India\n*\nCorrespondence: yogesh.rajwade@icar.gov.in\
    \ (Y.A.R.); abhilashchandel@vt.edu (A.K.C.)\nAbstract: Timely crop water stress\
    \ detection can help precision irrigation management and minimize\nyield loss.\
    \ A two-year study was conducted on non-invasive winter wheat water stress monitoring\n\
    using state-of-the-art computer vision and thermal-RGB imagery inputs. Field treatment\
    \ plots were\nirrigated using two irrigation systems (ﬂood and sprinkler) at four\
    \ rates (100, 75, 50, and 25% of crop\nevapotranspiration [ETc]). A total of 3200\
    \ images under different treatments were captured at critical\ngrowth stages,\
    \ that is, 20, 35, 70, 95, and 108 days after sowing using a custom-developed\
    \ thermal-\nRGB imaging system. Crop and soil response measurements of canopy\
    \ temperature (Tc), relative\nwater content (RWC), soil moisture content (SMC),\
    \ and relative humidity (RH) were signiﬁcantly\naffected by the irrigation treatments\
    \ showing the lowest Tc (22.5 ± 2 ◦C), and highest RWC (90%)\nand SMC (25.7 ±\
    \ 2.2%) for 100% ETc, and highest Tc (28 ± 3 ◦C), and lowest RWC (74%) and SMC\n\
    (20.5 ± 3.1%) for 25% ETc. The RGB and thermal imagery were then used as inputs\
    \ to feature-\nextraction-based deep learning models (AlexNet, GoogLeNet, Inception\
    \ V3, MobileNet V2, ResNet50)\nwhile, RWC, SMC, Tc, and RH were the inputs to\
    \ function-approximation models (Artiﬁcial Neural\nNetwork (ANN), Kernel Nearest\
    \ Neighbor (KNN), Logistic Regression (LR), Support Vector Machine\n(SVM) and\
    \ Long Short-Term Memory (DL-LSTM)) to classify stressed/non-stressed crops. Among\n\
    the feature extraction-based models, ResNet50 outperformed other models showing\
    \ a discriminant\naccuracy of 96.9% with RGB and 98.4% with thermal imagery inputs.\
    \ Overall, classiﬁcation accuracy\nwas higher for thermal imagery compared to\
    \ RGB imagery inputs. The DL-LSTM had the highest\ndiscriminant accuracy of 96.7%\
    \ and less error among the function approximation-based models for\nclassifying\
    \ stress/non-stress. The study suggests that computer vision coupled with thermal-RGB\n\
    imagery can be instrumental in high-throughput mitigation and management of crop\
    \ water stress.\nKeywords: winter wheat; crop water stress; canopy temperature;\
    \ computer vision; irrigation\nmanagement\n1. Introduction\nWater stress forces\
    \ leaf stomata closure, which reduces transpiration and increases\ncanopy temperature\
    \ (Tc) [1]. Timely estimation of those stressors may not only help preci-\nsion\
    \ irrigation management but also minimize yield losses [2]. The penalty gap between\n\
    actual and potential yield will widen further as a result of climate change that\
    \ projects\na decline in rainfall frequency, and rising ambient temperatures [3].\
    \ Water stress is typ-\nically assessed using xylem water potentials [4], canopy\
    \ thermometry [5], and stomatal\nPlants 2022, 11, 3344. https://doi.org/10.3390/plants11233344\n\
    https://www.mdpi.com/journal/plants\nPlants 2022, 11, 3344\n2 of 21\nconductance\
    \ measurements [6]. However, these methods are often invasive and tend\nto have\
    \ limited sampling accuracy due to low throughput or point data acquisitions [7].\n\
    Non-invasive proximal or remote sensing techniques have emerged as high throughput\n\
    alternatives for monitoring crop water stress through color features, reﬂectance,\
    \ and ther-\nmal emissivity of the vegetable, fruit, and specialty crops [8–10].\
    \ However, monitoring\ncrop water content using visible-range RGB imaging not\
    \ only requires speciﬁc leaf ori-\nentation relative to the camera but also pre-deﬁned\
    \ illumination conditions. This limits\nthe applicability of RGB imaging to determine\
    \ water content in ﬁeld conditions. Using\nscanner-type imaging devices could\
    \ be a cost and time-effective alternative [11]. Unlike\nthermometry, Tc from\
    \ thermal infrared imagery reﬂects upon the entire canopy emissivity\nproﬁle,\
    \ which is directly proportional to the canopy water content [9]. Thermal imagery\n\
    (8000–14,000 nm) also outperforms color RGB imagery (400–700 nm), and reﬂectance\
    \ char-\nacteristics in terms of robustness to characterize crop water stress\
    \ [8,9]. Nonetheless, the\nadaptability of thermal imaging in agricultural production\
    \ management is still at a nascent\nstage and is consistently evolving to maintain\
    \ imaging quality against drastic variations\nin relative humidity and wind speeds.\
    \ Thermal imaging cameras are also relatively more\nexpensive than simple-to-operate\
    \ RGB cameras. It is for these reasons; thermal imaging\nis still limitedly adopted\
    \ as a golden standard for crop stress mapping. Above all, long-\nwave infrared\
    \ wavelengths (thermal imaging) have a higher penetrating capability over\nvisible-range\
    \ wavelengths, making them more reliable and sensitive to crop water content\n\
    variations. Thermal imaging is therefore a better alternative for precision irrigation\
    \ man-\nagement unlike RGB imaging or using standard crop coefﬁcients coupled\
    \ with reference\nevapotranspiration [8,9].\nRGB imagery has been used to assess\
    \ crop water stress using different deep learning\n(DL) and machine learning (ML)\
    \ techniques [10,12,13]. ML techniques derive unique\nfeatures from input and\
    \ output datasets, which could be used for discrimination between\ndifferent object\
    \ types or classes. ML techniques such as Naïve Bayes, artiﬁcial neural\nnetworks\
    \ (ANNs), support vector machine (SVM), and random forests (RFs) have been\nwidely\
    \ used with RGB images for weed detection, biotic and abiotic stress identiﬁcation\n\
    and/or classiﬁcation, yield predictions, and other crop phenotyping applications\
    \ [14,15].\nThermal imagery has also been used with RFs and decision trees for\
    \ crop water status\nmonitoring in the vineyard and automated irrigation scheduling\
    \ [16]. However, there are\nseveral limitations associated with ML techniques.\
    \ The output quality is highly dependent\non input data quality, the presence\
    \ of noise and outliers, and other unaccounted biases\nthat have been reported\
    \ to signiﬁcantly affect the model performance. Furthermore, ML\ntechniques also\
    \ require skilled operators [17] for deﬁning input features that may also often\n\
    affect the model performances through unintentional subjectivity and bias [10,12].\n\
    DL has emerged as an advanced vision-based learning technique that enables au-\n\
    tomated feature extraction without human dependencies unlike ML [18]. Pertinent\
    \ to\nagricultural applications, crop phenological stages have been detected using\
    \ a deep con-\nvolution neural network (DCNN) trained on RGB imagery [19,20].\
    \ Similarly, different DL\ntechniques (AlexNet, GoogLeNet, and Inception V3) have\
    \ also been used to classify non-\nstressed and water-stressed soybean, maize,\
    \ and okra crops with digital RGB images [10].\nLong Short Term Memory (LSTM)\
    \ is a novel DL approach (DL-LSTM) that has been used\nfor different ﬁeld applications\
    \ like time series forecasting of wheat yield and productiv-\nity [21], irrigation\
    \ requirement [22], predicting agricultural product sale volumes based\non seasonal\
    \ and historical data [23], and identiﬁcation and classiﬁcation of weeds [24].\n\
    Most of the image processing studies have used RGB images (or visible range imagery)\
    \ to\nclassify crop water stress [25,26]. Thermal imagery has been reported to\
    \ be more robust\nfor crop water stress characterization compared to RGB or multispectral\
    \ imagery [27,28].\nThis is majorly due to the fact that the canopy emissivity\
    \ can be highly sensitive to water\ncontent [8,9,29,30].\nSo far, crop water stress\
    \ characterization has been carried out through traditional\nand destructive methods\
    \ that often have restricted commercial applicability. Moreover,\nPlants 2022,\
    \ 11, 3344\n3 of 21\nthese techniques have been limitedly explored using robust\
    \ computer-vision techniques\n(ML or DL models) for thermal infrared imagery inputs.\
    \ Small unmanned aerial system\n(UAS)-based thermal and multispectral remote sensing\
    \ is also being explored for high\nthroughput crop water stress phenotyping. However,\
    \ the frequency of data acquisition\nis limited to once or a few times a day and\
    \ atmospheric interferences including weather\nconditions may severely impact\
    \ the quality of thermal imaging. Additionally, onboard\ndata processing potential\
    \ for complex and robust algorithms is still limited for small UASs.\nContrarily,\
    \ proximal thermal imaging is subjected to the least atmospheric interference\n\
    and imaging frequency constraints. These systems can continuously collect data\
    \ at critical\ngrowth stages and also offer ﬂexibility for custom modiﬁcation\
    \ to implement onboard edge\nprocessing algorithms for real-time decision support\
    \ and management actuation. On the\ncost side, thermal and RGB imaging sensors\
    \ and the UASs are still far more expensive than\nthe proximal imaging systems,\
    \ which can be custom-assembled using miniature sensing\nmodules. However, such\
    \ miniature sensing modules neither offer sufﬁcient resolution nor\ndesired image\
    \ quality when integrated with UASs.\nObtaining robust data handling and the analytical\
    \ pipeline is the major obstacle to\nderiving real-time decision support for crop\
    \ management but is achievable using custom-\nassembled edge devices. This study\
    \ is a step toward alleviating those obstacles and\nfocuses on the evaluation\
    \ of non-invasive and cost-effective thermal-RGB imaging with\nrobust ML and DL\
    \ models for stress characterization in winter wheat crops. This could\nbe critical\
    \ from a precision irrigation scheduling and management perspective and may\n\
    potentially have high grower adaptability. Speciﬁc objectives for this two-year\
    \ study were\nto (a) non-invasively assess the crop responses to two irrigation\
    \ systems and four deﬁcit\nirrigation treatments, and (b) identify the water-stressed\
    \ and non-stressed crops by feature\nextraction using thermal-RGB imagery and\
    \ function approximation approaches using crop\nphysiological parameters and ambient\
    \ weather inputs.\n2. Materials and Methods\n2.1. Experiment Design\nWinter wheat\
    \ (Triticum aestivum L., cv. HI 1544) was planted (November to April,\n2019–2020,\
    \ and 2020–2021) in the research farm (77.24◦ E, 23.18◦ N) of the Central Institute\n\
    of Agricultural Engineering (CIAE), ICAR Bhopal, India (Figure 1). The meteorological\n\
    data are being recorded at the institute observatory since 1985. According to\
    \ Koppen’s\nclassiﬁcation (1934), Bhopal is a Mediterranean climatic zone with\
    \ an average annual\nrainfall of about 1127 mm. The soil type is heavy clay (Vertisols)\
    \ with clay content over 50%\nand moderate fertility with negligible salinity.\
    \ Soil structure is sub-angular blocky with a\nﬁeld capacity of 29.5–32% (db)\
    \ and wilting point of 18-19.5% (db). The average inﬁltration\nand percolation\
    \ rates of the soil are 10–12 mm day−1 and 6.3–7.0%, respectively. The plots\n\
    were irrigated using ﬂood and sprinkler systems at four treatment rates: 100,\
    \ 75, 50, and\n25% of full crop evapotranspiration (ETc). Micro sprinklers of\
    \ 120 lph discharge capacity\n(Make: Netaﬁm) were installed at 3.5 m spacing.\
    \ The reference crop evapotranspiration was\ncalculated using weather data with\
    \ the FAO56 Penman–Montieth method and standard\nnon-stressed crop coefﬁcient\
    \ [31]. The seasonal ETc of wheat during the ﬁrst and second\nyears of growth\
    \ was 380 mm and 345 mm, respectively. The application efﬁciencies of\n0.65 for\
    \ ﬂood irrigation and 0.90 for sprinkler irrigation were used as determined from\n\
    the experiment trials on the same site using the measurements of water applied\
    \ and the\nwater retained in the crop root zone. A total of six irrigation cycles\
    \ were implemented for\n100% ETc treatment (non-stressed) at sowing, crown root\
    \ initiation (CRI), tillering, booting,\nﬂowering, and grain ﬁlling stages. One\
    \ to three irrigation cycles were implemented for\ndeﬁcit treatments (75, 50,\
    \ and 25% of ETc) at jointing, booting, and ﬂowering stages.\nPlants 2022, 11,\
    \ 3344\n4 of 21\nPlants 2022, 11, x FOR PEER REVIEW \n4 of 22 \n \n \ncrown root\
    \ initiation (CRI), tillering, booting, flowering, and grain filling stages. One\
    \ to \nthree irrigation cycles were implemented for deficit treatments (75, 50,\
    \ and 25% of ETc) at \njointing, booting, and flowering stages. \n \nFigure 1.\
    \ Experimental layout of the winter wheat crop irrigated at different rates using\
    \ sprinkler \nand flood irrigation systems [32]. R—Replicates. Layout is prepared\
    \ over google map. \n2.2. Data Collection \nRGB and thermal imagery were synchronously\
    \ captured using a multifunctional \ncustom integrated thermal-RGB imaging system.\
    \ The system has a single board com-\nputer (B+, Raspberry Pi foundation), a thermal\
    \ imaging module (8000–14,000 nm, HTPA, \nHeimann, Pixel resolution: 80 × 64,\
    \ Horizontal and vertical FOV: 120° × 90°), an RGB \nimaging module (400–700 nm,\
    \ Raspberry Pi V2, Sony IMX219, Raspberry Pi foundation, \nPixel resolution: 3280\
    \ × 2464, HFOV: 62.2°, VFOV: 48.8°), a GPS receiver module (NEO \n6M V2, Adafruit)\
    \ for image geotagging, a capacitive touchscreen (LCD 800 × 400 mm, \nRobokit),\
    \ a keypad (Robokits), and a power source (20,000 mAh, 5V/2A, MI power bank).\
    \ \nThe computer used the NOOBS operating system with module-pertinent libraries\
    \ for \ndifferent operations. Imagery data were collected at critical crop growth\
    \ stages in the \n2019 and 2020 growing seasons (five times in each season). Ground\
    \ truth plant biophys-\nical and soil parameters were also measured synchronously.\
    \ \n2.2.1. Imagery Data \nThe developed imaging system was placed 1 m from the\
    \ crop and titled at 45° from \nthe horizontal. A total of 3200 images (400 per\
    \ treatment) were acquired (1600 RGB and \n1600 thermal) in two seasons at Crown\
    \ root initiation (20 days after sowing (DAS)), till-\nering (35 DAS) (Figure\
    \ 2), jointing (70 DAS), flowering and milking (95 DAS), and dough \n(108 DAS)\
    \ stages, between 11 am to 1 pm on clear sky days. Sample masked thermal and \n\
    RGB canopy images for model training are shown in Figure 2.  \nFigure 1. Experimental\
    \ layout of the winter wheat crop irrigated at different rates using sprinkler\n\
    and ﬂood irrigation systems [32]. R—Replicates. Layout is prepared over google\
    \ map.\n2.2. Data Collection\nRGB and thermal imagery were synchronously captured\
    \ using a multifunctional cus-\ntom integrated thermal-RGB imaging system. The\
    \ system has a single board computer (B+,\nRaspberry Pi foundation), a thermal\
    \ imaging module (8000–14,000 nm, HTPA, Heimann,\nPixel resolution: 80 × 64, Horizontal\
    \ and vertical FOV: 120◦ × 90◦), an RGB imaging mod-\nule (400–700 nm, Raspberry\
    \ Pi V2, Sony IMX219, Raspberry Pi foundation, Pixel resolution:\n3280 × 2464,\
    \ HFOV: 62.2◦, VFOV: 48.8◦), a GPS receiver module (NEO 6M V2, Adafruit)\nfor\
    \ image geotagging, a capacitive touchscreen (LCD 800 × 400 mm, Robokit), a keypad\n\
    (Robokits), and a power source (20,000 mAh, 5V/2A, MI power bank). The computer\n\
    used the NOOBS operating system with module-pertinent libraries for different\
    \ operations.\nImagery data were collected at critical crop growth stages in the\
    \ 2019 and 2020 growing\nseasons (ﬁve times in each season). Ground truth plant\
    \ biophysical and soil parameters\nwere also measured synchronously.\n2.2.1. Imagery\
    \ Data\nThe developed imaging system was placed 1 m from the crop and titled at\
    \ 45◦ from\nthe horizontal. A total of 3200 images (400 per treatment) were acquired\
    \ (1600 RGB\nand 1600 thermal) in two seasons at Crown root initiation (20 days\
    \ after sowing (DAS)),\ntillering (35 DAS) (Figure 2), jointing (70 DAS), ﬂowering\
    \ and milking (95 DAS), and dough\n(108 DAS) stages, between 11 am to 1 pm on\
    \ clear sky days. Sample masked thermal and\nRGB canopy images for model training\
    \ are shown in Figure 2.\n2.2.2. Weather and Ground Truth Data\nWeather data were\
    \ acquired for the imaging days from a standard station (Indian\nMeteorological\
    \ Department, Pune, India) installed at 300 m from the study site. The\nparameters\
    \ included, pan evaporation (mm/day), rainfall (mm), maximum and minimum\nair\
    \ temperature (◦C), relative humidity (RH, %), and wind velocity (m/s). The ambient\n\
    and soil ground truth data of air temperature (Ta), RH, and soil moisture content\
    \ (SMC)\nwere collected for each treatment plot during the imaging campaigns each\
    \ year. The Ta\nand RH parameters were recorded using the DHT22 module (Adafruit,\
    \ New York, NY,\nPlants 2022, 11, 3344\n5 of 21\nUSA). SMC was monitored in the\
    \ root zone depth (0–150 mm typical to wheat crops\ngrown in the experimental\
    \ site, soil type: vertisols) using a soil moisture meter of 200 mm\nsensing probe\
    \ length (ICT, MPM-160-B, Armidale, Australia). The probe was inserted at\nﬁve\
    \ different locations in each replication for measurement of soil moisture, acquiring\
    \ 15\ndata points of soil moisture content per measurement. The relative water\
    \ content (RWC)\nof leaves was calculated as the crop ground truth data [33].\
    \ For this, 10 matured and fully\nexpanded leaves from each sample plot were collected\
    \ and fresh weight was recorded on\neach sampling date, immediately following\
    \ the imagery acquisition. Collected samples\nwere then oven-dried at 70 ◦C, dry\
    \ weight was recorded, and RWC was calculated. A\ntotal of 30 samples were collected\
    \ per treatment per campaign amounting to a total of\n150 samples per treatment\
    \ in each year for RWC calculations. End-season yield was also\nrecorded from\
    \ 2 × 2 m areas from three plots in each replication, making nine sample\npoints\
    \ (36 m2 area) per treatment to characterize the effects of crop water stress.\n\
    Plants 2022, 11, x FOR PEER REVIEW \n5 of 22 \n \n \nFigure 2. Sample raw and\
    \ canopy masked RGB images ((a) non-stressed; (b) stressed) and thermal \nimages\
    \ ((c) non-stressed, (d) stressed) captured 35 days after sowing. Pseudo-color\
    \ thermal images \nhere are only for presentation and were scaled between 10 °C\
    \ (RGB: [25, 25, 113]) and 80 °C (RGB: \n[235, 246, 255]). \n2.2.2. Weather and\
    \ Ground Truth Data \nWeather data were acquired for the imaging days from a standard\
    \ station (Indian \nMeteorological Department, Pune, India) installed at 300 m\
    \ from the study site. The pa-\nrameters included, pan evaporation (mm/day), rainfall\
    \ (mm), maximum and minimum \nair temperature (°C), relative humidity (RH, %),\
    \ and wind velocity (m/s). The ambient \nand soil ground truth data of air temperature\
    \ (Ta), RH, and soil moisture content (SMC) \nwere collected for each treatment\
    \ plot during the imaging campaigns each year. The Ta \nand RH parameters were\
    \ recorded using the DHT22 module (Adafruit, New York, NY, \nUSA). SMC was monitored\
    \ in the root zone depth (0–150 mm typical to wheat crops \ngrown in the experimental\
    \ site, soil type: vertisols) using a soil moisture meter of 200 mm \nsensing\
    \ probe length (ICT, MPM-160-B, Armidale, Australia). The probe was inserted at\
    \ \nfive different locations in each replication for measurement of soil moisture,\
    \ acquiring 15 \ndata points of soil moisture content per measurement. The relative\
    \ water content (RWC) \nof leaves was calculated as the crop ground truth data\
    \ [33]. For this, 10 matured and fully \nexpanded leaves from each sample plot\
    \ were collected and fresh weight was recorded on \neach sampling date, immediately\
    \ following the imagery acquisition. Collected samples \nwere then oven-dried\
    \ at 70 °C, dry weight was recorded, and RWC was calculated. A \ntotal of 30 samples\
    \ were collected per treatment per campaign amounting to a total of 150 \nsamples\
    \ per treatment in each year for RWC calculations. End-season yield was also \n\
    recorded from 2 × 2 m areas from three plots in each replication, making nine\
    \ sample \npoints (36 m2 area) per treatment to characterize the effects of crop\
    \ water stress. \nFigure 2. Sample raw and canopy masked RGB images ((a) non-stressed;\
    \ (b) stressed) and thermal\nimages ((c) non-stressed, (d) stressed) captured\
    \ 35 days after sowing. Pseudo-color thermal images\nhere are only for presentation\
    \ and were scaled between 10 ◦C (RGB: [25, 25, 113]) and 80 ◦C (RGB:\n[235, 246,\
    \ 255]).\n2.2.3. Statistical Analysis\nThe impact of irrigation type (ﬂood and\
    \ sprinkler), rate (100, 75, 50, and 25% ETc),\nand interaction of both on crop\
    \ biophysical parameters were statistically evaluated using a\none-way analysis\
    \ of variance at a 5% level of signiﬁcance [34].\n2.3. Crop Water Stress Classiﬁcation\n\
    Two different approaches (1) feature extraction-based (DL models: AlexNet, GoogLeNet,\n\
    Inception V3, MobileNet V2, and ResNet50) and (2) function approximation-based\
    \ ML\nmodels (Artiﬁcial neural network (ANN), K-nearest neighbors (KNN), Support\
    \ vector\nmachine (SVM), and Logistic regression (LR)); and a DL model (DL-LSTM)\
    \ were adopted\nfor crop water stress classiﬁcation. Feature extraction-based\
    \ models were trained on thermal\nas well as RGB imagery. Function approximation-based\
    \ models were trained on ambient\nweather and soil parameters, and Tc inputs from\
    \ thermal imagery.\nDeep CNNs typically have complex architecture and some may\
    \ require signiﬁcant\ncomputational resources. All CNN model training and validation\
    \ processes were performed\non a desktop computer (Intel Core I7 Processor with\
    \ base frequency 2.60 GHz, 16 GB RAM,\n6 GB NVIDIA GeForce GTX 1660 Ti GPU) with\
    \ Windows 10 operating system (64 bits).\nPlants 2022, 11, 3344\n6 of 21\nCNN\
    \ models were developed in MATLAB 2019b using the deep learning and machine\n\
    learning toolbox. All the models are detailed in the following sub-sections.\n\
    2.3.1. Feature Extraction-Based Approaches\nFive DL models were selected as the\
    \ feature extraction-based approaches (1) AlexNet;\n(2) GoogLeNet; (3) Inception\
    \ V3; (4) MobileNet V2; and (5) ResNet50. These models were se-\nlected for their\
    \ extraordinary capabilities of automated feature extraction, easy and efﬁcient\n\
    training of the raw images with optimum computation resources, and their transferability\n\
    to edge computation devices [10,17]. The selected models ranged from the simplest\
    \ architec-\nture (AlexNet and MobileNet V2) to the most complex architecture\
    \ (GoogLeNet, Inception\nV3, and ResNet50) in order to evaluate their robustness\
    \ and efﬁciencies for crop water stress\nprediction. Successful application of\
    \ these models has been reported with accuracies up to\n100% for crop abiotic\
    \ and biotic stress classiﬁcation in recent studies [35–37]. Standardized\narchitectures\
    \ were used in the models for performance comparisons (Table 1).\nTable 1. Architecture\
    \ parameters in the feature extraction-based models for crop water stress classification.\n\
    Architecture Parameters\nAlexNet\nGoogLeNet\nInception V3\nMobileNet V2\nResNet50\n\
    Input image size\n227 × 227 × 3\n224 × 224 × 3\n299 × 299 × 3\n224 × 224 × 3\n\
    224 × 224 × 3\nNo. of layers\n25\n141\n316\n154\n177\nRelu layer\n7\n57\n95\n\
    35\n49\nMax Pooling layer\n3\n13\n4\n-\n01\nConvolutional layers\n5\n57\n94\n\
    35\n53\nDropout layer\n2\n1\n-\n-\n-\nFully connected\n3\n1\n1\n1\n1\nFully connected\
    \ layer Function\nFC8\nLoss3 classiﬁer\nPredictions\nLogits\nFC1000\nDepth\n8\n\
    22\n48\n53\n50\nParameters\n61 × 106\n7 × 106\n23.9 × 106\n3.5 × 106\n25.6 × 106\n\
    The DL-based classification includes steps of pre-trained model selection, data\
    \ pre-\nprocessing using morphological operators, data splitting, setting the\
    \ training hyper-parameters,\nmodel training, model tuning, cross-validation,\
    \ evaluation, and model testing (Figure 3).\nDL models were developed in MATLAB\
    \ (version 2019a, Mathworks, Natick, Boston, MA,\nUSA) using libraries of AlexNet,\
    \ GoogLeNet, Inception V3, MobileNet V2, and ResNet50.\nThe convolutional kernels\
    \ in AlexNet were extracted using the cost function optimized by\na stochastic\
    \ gradient descent with momentum (Sgdm) algorithm. While GoogLeNet pro-\ncesses\
    \ and classiﬁes images by alternately factorizing the convolutions and regularization\n\
    layers. To train a GoogLeNet model, the model’s loss 3-classiﬁer, prob, and output\
    \ layers\nwere replaced by fully connected, softmax and output class layers which\
    \ connected with\nother traditional layers. The inception V3 extracted (a) local\
    \ features of the stressed crop by\nusing small convolutions and (b) high abstracted\
    \ features with large convolutions. The last\nthree prediction layers in inception\
    \ V3 were replaced by three new layers; fully connected,\nsoftmax, and classiﬁcation\
    \ output layer. These layers were interconnected with average\npooling and a fully\
    \ connected layer of the pre-trained DL. MobileNet V2 and ResNet50 are\nthe very\
    \ recently proposed DL models for classiﬁcation problems. MobileNet V2 requires\n\
    less computational power compared to conventional CNN. ResNet50 is a deep residual\n\
    network that uses the shortcut connections by reducing the convolutional layers\
    \ and by\nalso solving the vanishing gradient issue typical to CNN. The residual\
    \ modules in ResNet50\nwere used to connect different layers of CNN to improve\
    \ the model performance [38].\nGeneralization can be poor for feature extraction-based\
    \ models when the number of\nepochs and batch sizes are more than the optimum\
    \ [39]. This is because the model can\noverlearn when trained on a speciﬁc dataset\
    \ at large epochs and batch sizes, and may lose\nits performance and generalization\
    \ capability when trained on new datasets. Conversely,\nthe smaller epochs and\
    \ bath sizes may lead to insufﬁcient learning and underﬁtting of the\nmodel and\
    \ hence may not perform as expected with the new datasets [40]. Therefore, to\n\
    Plants 2022, 11, 3344\n7 of 21\nmaximize the model performance and minimize their\
    \ overﬁtting, optimum hyperparameter\ntuning is required. In this study, all the\
    \ selected feature extraction-based models were\nextensively tuned with learning\
    \ rates, solvers, epochs, and batch sizes as detailed in Table 2.\nPlants 2022,\
    \ 11, x FOR PEER REVIEW \n7 of 22 \n \n \nThese layers were interconnected with\
    \ average pooling and a fully connected layer of the \npre-trained DL. MobileNet\
    \ V2 and ResNet50 are the very recently proposed DL models \nfor classification\
    \ problems. MobileNet V2 requires less computational power compared \nto conventional\
    \ CNN. ResNet50 is a deep residual network that uses the shortcut con-\nnections\
    \ by reducing the convolutional layers and by also solving the vanishing gradient\
    \ \nissue typical to CNN. The residual modules in ResNet50 were used to connect\
    \ different \nlayers of CNN to improve the model performance [38].  \n \nFigure\
    \ 3. Data processing pipeline for stress prediction using selected deep learning\
    \ and machine \nlearning models. \nGeneralization can be poor for feature extraction-based\
    \ models when the number of \nepochs and batch sizes are more than the optimum\
    \ [39]. This is because the model can \noverlearn when trained on a specific dataset\
    \ at large epochs and batch sizes, and may \nlose its performance and generalization\
    \ capability when trained on new datasets. Con-\nversely, the smaller epochs and\
    \ bath sizes may lead to insufficient learning and under-\nfitting of the model\
    \ and hence may not perform as expected with the new datasets [40]. \nTherefore,\
    \ to maximize the model performance and minimize their overfitting, optimum \n\
    hyperparameter tuning is required. In this study, all the selected feature extraction-based\
    \ \nmodels were extensively tuned with learning rates, solvers, epochs, and batch\
    \ sizes as \ndetailed in Table 2. \nTable 2. Hyperparameter tuning considerations\
    \ to reduce overfitting and performance enhance-\nment of the feature extraction-based\
    \ models. \nFigure 3. Data processing pipeline for stress prediction using selected\
    \ deep learning and machine\nlearning models.\nCollected thermal and RGB images\
    \ (1600 each) were labeled into stressed and non-\nstressed classes by the domain\
    \ experts based on the values of crop water stress indicators of\nSMC, RWC, and\
    \ Tc (Table 3). After this, 80% of the labeled dataset (separately for thermal\n\
    and RGB images) was used for DL model training based on features of object dimensions,\n\
    pixel intensity, pixel values (Tc), edges, etc. The remaining 20% of the labeled\
    \ dataset was\nused for model validations and testing.\nTable 2. Hyperparameter\
    \ tuning considerations to reduce overﬁtting and performance enhancement\nof the\
    \ feature extraction-based models.\nParameters\nValue\nEpoch\n5, 10, and 20\n\
    Batchsize\n5, 10, 15, and 20\nIterations\n250 and 300\nSolver\nSgdm and Adam\n\
    Learning rate\n1 × 10−4, 2 × 10−4, and 3 × 10−4\nSgdm: stochastic gradient descent\
    \ with momentum; Adam: adaptive moment estimation.\nPlants 2022, 11, 3344\n8 of\
    \ 21\nTable 3. Crop and auxiliary data ranges for stressed and non-stressed labeling.\n\
    Crop Label\nParameter\nOutput\nReferences\nStressed\nCanopy temperature (Tc):\
    \ >23 ◦C &\nRelative water content (RWC): <90% &\nSoil moisture content (SMC):\
    \ <25%\n0\n[41–46]\nNon-Stressed\nneither of the “stressed” conditions\n1\n2.3.2.\
    \ Function Approximation-Based Approaches\nFour ML models; ANN, KNN, LR, and SVM\
    \ and a DL-based LSTM (DL-LSTM) were\nselected as the function approximation approaches\
    \ for crop water stress classiﬁcation. The\nML models were selected as those provide\
    \ an opportunity to analyze numerous features\nsimultaneously unlike traditional\
    \ methods. ANN is effective in learning complex nonlinear\nfunctions and segmenting\
    \ data based on the learned weights. The input layer had four\nvariables to extract\
    \ features from 1600 samples while the output layer had one neuron to\ncalculate\
    \ the probability of each class [47]. KNN classiﬁes a data point based on its\
    \ distance\nfrom the maximum number of training data points in the neighborhood.\
    \ Typically, KNN\nuses Euclidean, Minkowski, Manhattan, or Hamming distances out\
    \ of which Minkowski\ndistance has been reported to be more reliable [48] and\
    \ was therefore selected in the model.\nLR classiﬁes data points into discrete\
    \ classes based on probability using a sigmoid or logistic\nfunction [49]. SVM\
    \ shifts data points to a higher dimension using linear, non-linear, and\nradial\
    \ kernels to achieve linear separability [50] and then identiﬁes a hyperplane\
    \ for the\nhighest possible distance between data points of the two classes. DL-LSTM\
    \ uses a chain of\nrepeated modules comprising memory cells with a backpropagation\
    \ algorithm to solve the\nclassiﬁcation problems. This model solves premature\
    \ overﬁtting and vanishing gradient\nissues by using the previously stored information\
    \ in the memory cell. The information\nis then used to generate the features during\
    \ the training process to predict the output\nclass [51]. ML models automatically\
    \ tuned their hyperparameter values by using Bayesian\noptimization. The optimization\
    \ minimizes the model loss based on the hyperparameter\ncombination and yields\
    \ the best possible set of parameters. Further, the models were\ntrained and validated\
    \ on these tuned hyperparameters. All function-approximation models\nwere deployed\
    \ on crop environment (Ta, RH, and SMC) and temperature (Tc, from thermal\nimages)\
    \ inputs for classiﬁcation into stressed and non-stressed through binary outputs\
    \ (0\nor 1, Table 3). The models (operating parameters in Table 4) were developed\
    \ in Python 3.7\nwith Keras and TensorFlow libraries.\nTable 4. Training parameters\
    \ of function approximation-based classiﬁcation models.\nFunction Approximation\
    \ Model\nParameters\nArtiﬁcial Neural Network (ANN)\nHidden layers: 2\nNeurons:\
    \ 64, 32\nLearning rate (alpha): 0.01\nActivation functions: sigmoid\nBatch size:\
    \ 8\nNumber of epochs: 300\nOptimizer: Adam\nLoss function: binary cross entropy\n\
    Kernel Nearest Neighbour (KNN)\nNumber of Neighbors (K): 8\nDistance Metric: minkowski\n\
    Weights: uniform\nAlgorithm: ball-tree\nLogistic Regression (LR)\nPenalty parameter:\
    \ L1\nInverse of regularization parameter (C): 5\nMaximum iteration: 100\nTolerance:\
    \ 0.0001\nPlants 2022, 11, 3344\n9 of 21\nTable 4. Cont.\nFunction Approximation\
    \ Model\nParameters\nSupport Vector Machine (SVM)\nKernel Type (Kernel): RBF (Radial\
    \ basis Function)\nPenalty parameter (C): 100\nbandwidth parameter (gamma): 0.001\n\
    Degree of the polynomial kernel: 3\nDeep Learning-Long Short Term Memory (DL-LSTM)\n\
    Number of neurons: 180\nEpochs: 200\nBatch size: 10\nOptimizer: Adam\nNumber of\
    \ hidden layers: 2\nLoss activation function: MAE (Mean absolute error)\nAdam:\
    \ adaptive moment estimation.\n2.4. Model Performance Evaluation\nThe performance\
    \ of both feature extraction and function approximation-based models\nwas evaluated\
    \ through accuracy (A), sensitivity (Se), speciﬁcity (Sp), precision (P), and\
    \ F1\nscore parameters (Equations (1)–(5)). Accuracy is the correct prediction\
    \ rate of non-stressed\nand stressed crops, precision is the fraction of true\
    \ positive (TS) or correctly predicted\nstressed crop from an overall prediction\
    \ of the stressed crop (PS), speciﬁcity is the true\nnegative (TN) or correctly\
    \ predicted non-stressed crops from the actual non-stressed crops\n(AN). Sensitivity\
    \ represents a fraction of the correctly predicted stressed crops (TS) from the\n\
    actual stressed crops (AS) and the F1 score is the harmonic mean of precision\
    \ and sensitivity.\nThe F1 score evaluates the accuracy of a binary classiﬁcation\
    \ problem as in this study, which\naims to classify the crops into two classes\
    \ (stressed and non-stressed). Often, the accuracy\nestimate is affected by true\
    \ negatives and therefore F1 score is highly used over accuracy to\nseek a balance\
    \ between the precision and recall (sensitivity) parameters and when there is\n\
    an uneven class distribution (a large number of actual negatives).\nA = TS + TN\n\
    TT\n(1)\nSe = TS\nAS\n(2)\nSp = TN\nAN\n(3)\nP = TS\nPS\n(4)\nF1 =\n2 ∗ TS\nAS\
    \ + PS\n(5)\nwhere TT is the total number of predictions. Stress/non-stress misclassiﬁcation\
    \ was rep-\nresented by type1 (TE1) (false positive) and type2 errors (TE2) (false\
    \ negative). TE1 is the\nnumber of actual stressed crops misclassiﬁed as non-stressed\
    \ (row 1-column 2 of the confu-\nsion matrix) while TE2 is the number of actual\
    \ non-stressed crops misclassiﬁed as stressed\n(row 2-column 1 of the confusion\
    \ matrix).\n3. Results\n3.1. Plant Water Stress Indicators\nThe thermal imagery\
    \ derived canopy temperatures (Tc (◦C)) under sprinkler irrigation\nat 100, 75,\
    \ 50 and 25% of ETc irrigation levels were 22.1 (2.0) (Mean, standard deviation\n\
    (SD)), 25.6 (1.6), 26.4 (2.2), and 27.9 ◦C (3.0 ◦C), respectively (Figure 4).\
    \ While Tc for ﬂood\nirrigation at the above irrigation levels were 23.2 (2.0),\
    \ 25.9 (1.5), 26.8 (2.4), and 28.1 ◦C\n(3.1 ◦C), respectively. Similarly, mean\
    \ RWC (%) at selected sprinkler irrigation rates were\nPlants 2022, 11, 3344\n\
    10 of 21\n90.4 (2.7), 87.7 (4.2), 75.8 (9.4), and 74.2% (8.2%) while at corresponding\
    \ irrigation levels in\nﬂood irrigation were 89.8 (2.7), 87.2 (4.3), 75.0 (9.4)\
    \ and 73.9% (8.3%), respectively. The mean\nSMCs (%) for respective sprinkler\
    \ irrigation were 26.6 (2.3), 26.2 (2.7), 22.5 (3.3), and 21.1%\n(3.0%) while\
    \ those for respective ﬂood irrigation were 24.9 (2.0), 24.4 (2.5), 21.4 (3.0),\
    \ and\n20.4% (3.1%). When analyzed statistically, Tc, RWC, and SMC were signiﬁcantly\
    \ affected\nby the irrigation method (ﬂood and sprinkler), irrigation rate (100,\
    \ 75, 50, and 25% ETc), as\nwell as their interaction (One-way ANOVA, p < 0.001).\
    \ The RWC and SMC decreased with\nthe decrease in irrigation level, while the\
    \ Tc increased. Based on the categories detailed in\nTable 3, the mean Tc for\
    \ the stressed crop was 26.6 ◦C (±2.6), and that for the non-stressed\ncrop was\
    \ 21.2 ◦C (±1.4). The mean RWC for the non-stressed crop was 92.2% (±1.5) and\n\
    for the stressed crop was 78.9% (±9.2) while the mean SMC for the non-stressed\
    \ crop was\n27.1% (1.6) and for the stressed crop was 21.2% (±2.4).\n3.2. Water\
    \ Stress Prediction\n3.2.1. Feature Extraction-Based Approaches\nThe performances\
    \ of AlexNet, GoogLeNet, Inception V3, MobileNet V2, and ResNet50\nmodels for\
    \ RGB and thermal imagery were tested for different combinations of epochs\nand\
    \ batch sizes (Table 5). The model training accuracies increased with the increase\
    \ in\nthe number of epochs from 5 to 10 and over-ﬁtting was observed for all the\
    \ models when\nepochs increased to 20. Over different epochs, accuracy increased\
    \ with the increase in\nbatch size from 5 to 20. For the batch size of 20 and\
    \ 250 iterations, overﬁtting was observed\nin Inception and ResNet50 with RGB\
    \ imagery inputs and in AlexNet, GoogLeNet, and\nResNet50 with thermal imagery\
    \ inputs (Table 5). Extensive hyperparameter tuning was\nperformed with parameters\
    \ listed in Table 2 to minimize overﬁtting and maximize the\nmodel accuracies.\
    \ Post-tuning, the maximum training accuracies of 94.6%, 96.7%, and 95.6%\nwere\
    \ observed for AlexNet, GoogLeNet, and MobileNet V2, respectively with RGB imagery\n\
    inputs at 10 epochs and batch size of 20 (Figure 5). While the Inception V3 and\
    \ ResNet50\nfor RGB imagery inputs converged at 10 epochs, batch size of 15, and\
    \ 300 iterations with\nrespective accuracies of 92.7% and 97.1% (Figure 5c,e).\
    \ For the thermal imagery inputs, the\noptimum hyperparameters were 10 epochs\
    \ and a batch size of 15, which yielded maximum\naccuracies of 96.4%, 97.2%, and\
    \ 98.5% for AlexNet, GoogLeNet, and ResNet50, respectively.\nFurthermore, 10 epochs\
    \ and a batch size of 20 were found optimum for Inception V3\nand MobileNet V2\
    \ models, and pertinent maximum accuracies were 98.0% and 95.3%,\nrespectively\
    \ (Figure 6). During hyperparameter tuning, the model overﬁtting reduced\nsigniﬁcantly\
    \ at 10 epochs without sacriﬁcing accuracy. The training accuracies fell below\n\
    50% for learning rates of 1 × 10−4 and 4 × 10−4 and went over 50% for the learning\
    \ rate of\n3 × 10−4. Moreover, the model overﬁtting was reduced when the solver\
    \ was shifted from\nSgdm to Adam. All the models converged with training accuracies\
    \ > 90% at the learning\nrate of 3 × 10−4 and Adam as the solver (Figures 5 and\
    \ 6).\nTable 5. Training accuracies of feature extraction-based models to characterize\
    \ wheat water stress\nusing RGB and thermal imagery inputs under different epoch\
    \ and batch size combinations.\nAccuracy (%)\nEpochs\nBatch Size\nAlexNet\nGoogLeNet\n\
    Inception V3\nMobileNet V2\nResNet50\nFeature Extraction-Based Approaches with\
    \ RGB Imagery Inputs\n5\n5\n90.4\n95.2\n91.9\n95.1\n96.3\n5\n10\n89.4\n94.3\n\
    90.5\n93.0\n92.7\n5\n15\n92.3\n94.6\n90.4\n92.3\n93.5\n5\n20\n92.6\n95.0\n90.8\n\
    92.7\n94.6\n10\n5\n93.8\n95.5\n92.2\n93.1\n95.8\n10\n10\n92.7\n95.7\n92.4\n94.2\n\
    95.1\n10\n15\n93.4\n95.9\n92.7\n94.4\n97.1\n10\n20\n94.6\n96.7\n93.6\n95.6\n97.2\n\
    20\n5\n95.3\n97.2\n93.8\n94.4\n92.3\nPlants 2022, 11, 3344\n11 of 21\nTable 5.\
    \ Cont.\nAccuracy (%)\nEpochs\nBatch Size\nAlexNet\nGoogLeNet\nInception V3\n\
    MobileNet V2\nResNet50\nFeature Extraction-Based Approaches with RGB Imagery Inputs\n\
    20\n10\n95.6\n97.5\n94.2\n95.5\n97.2\n20\n15\n96.6 *\n98.0\n94.5\n96.7 *\n97.9\
    \ *\n20\n20\n96.2\n98.2 *\n95.0 *\n96.1\n95.8\nFeature extraction-based approaches\
    \ with thermal imagery inputs\n5\n5\n94.4\n96.2\n97.0\n94.8\n95.9\n5\n10\n95.9\n\
    95.8\n96.8\n94.7\n95.9\n5\n15\n96.4\n95.4\n95.9\n93.1\n98.4\n5\n20\n92.7\n96.0\n\
    96.2\n92.7\n97.6\n10\n5\n94.5\n96.5\n97.2\n93.5\n96.7\n10\n10\n96.0\n96.7\n97.4\n\
    94.2\n97.3\n10\n15\n96.4\n97.2\n97.5\n94.7\n98.5\n10\n20\n96.5\n97.2\n98.0\n95.3\n\
    98.7\n20\n5\n97.2\n97.6\n98.2\n97.2\n99.0\n20\n10\n97.4\n98.1\n98.5\n97.5\n99.2\n\
    20\n15\n98.2 *\n98.5 *\n98.7 *\n98.1 *\n99.5 *\n20\n20\n98.0\n98.0\n98.5\n97.9\n\
    99.0\n* Highest accuracy for the epoch and batch size combinations.\nPlants 2022,\
    \ 11, x FOR PEER REVIEW \n11 of 22 \n \n \nFigure 4. Variations in (a) canopy\
    \ temperature; (b) soil moisture content; (c) relative water content; \nand (d)\
    \ grain yield from wheat plots irrigated at different rates. S and F represent\
    \ sprinkler and \nflood irrigations, respectively and the numbers followed by\
    \ these letters denote irrigation rates \nlevels as % of full crop evapotranspiration\
    \ (ETc). \n3.2. Water Stress Prediction \n3.2.1. Feature Extraction-Based Approaches\
    \ \nThe performances of AlexNet, GoogLeNet, Inception V3, MobileNet V2, and Res-\n\
    Net50 models for RGB and thermal imagery were tested for different combinations\
    \ of \nepochs and batch sizes (Table 5) The model training accuracies increased\
    \ with the in\nFigure 4. Variations in (a) canopy temperature; (b) soil moisture\
    \ content; (c) relative water content;\nand (d) grain yield from wheat plots irrigated\
    \ at different rates. S and F represent sprinkler and ﬂood\nirrigations, respectively\
    \ and the numbers followed by these letters denote irrigation rates levels as\
    \ %\nof full crop evapotranspiration (ETc).\nPlants 2022, 11, 3344\n12 of 21\n\
    \ \n \nhigher for the models with thermal imagery inputs compared to those with\
    \ RGB imagery \ninputs. The individual accuracy and errors for all the feature\
    \ extraction-based models \nwith validation datasets are shown in Figure 7. The\
    \ mean errors were higher for the RGB \nimagery compared to the thermal imagery\
    \ irrespective of the selected models.  \n \nFigure 5. Accuracy and loss curves\
    \ for (a) AlexNet; (b) GoogLeNet; (c) Inception V3; (d) MobileNet \nV2; and (e)\
    \ ResNet50 models with RGB imagery inputs for crop water stress identification.\
    \ \nFigure 5. Accuracy and loss curves for (a) AlexNet; (b) GoogLeNet; (c) Inception\
    \ V3; (d) MobileNet\nV2; and (e) ResNet50 models with RGB imagery inputs for crop\
    \ water stress identiﬁcation.\nThe training time elapsed for AlexNet, GoogLeNet,\
    \ Inception V3, MobileNet V2, and\nResNet50 was 76, 92, 609, 149, and 217 min\
    \ with RGB imagery inputs, and 42, 88, 287,\n134, 168 min with thermal imagery\
    \ inputs, respectively. While the classiﬁcation of an\nindependent image using\
    \ trained models into stressed/non-stressed class consumed less\nthan 5 s. The\
    \ overall validation accuracies (combined for stressed and non-stressed classes)\n\
    for AlexNet, GoogLeNet, Inception V3, MobileNet V2, and ResNet50 models were 93.4%,\n\
    95.9%, 92.5%, 94.4%, and 96.9%, respectively with RGB imagery inputs (Figure 7).\
    \ The\nhighest precision (100%) and F1 score (96.6%) were observed for GoogLeNet\
    \ and ResNet50,\nrespectively while maximum sensitivity was achieved for MobileNet\
    \ V2 (Table 6). Pertinent\nto thermal imagery inputs, overall validation accuracies\
    \ (combined for stressed and non-\nstressed classes) with AlexNet, GoogLeNet,\
    \ Inception V3, MobileNet V2, and ResNet50\nmodels were 96.2%, 96.9%, 97.5%, 94.7%,\
    \ and 98.4%, respectively. Alike RGB imagery,\nResNet50 for thermal imagery had\
    \ the highest precision (96.7%), sensitivity (100%), and\nF1 score (98.3%) (Table\
    \ 6). Additionally, the accuracies were higher for the models with\nthermal imagery\
    \ inputs compared to those with RGB imagery inputs. The individual\naccuracy and\
    \ errors for all the feature extraction-based models with validation datasets\n\
    are shown in Figure 7. The mean errors were higher for the RGB imagery compared\
    \ to the\nthermal imagery irrespective of the selected models.\nPlants 2022, 11,\
    \ 3344\n13 of 21\nPlants 2022, 11, x FOR PEER REVIEW \n14 of 22 \n \n \n \nFigure\
    \ 6. Accuracy and loss curves for (a) AlexNet; (b) GoogLeNet; (c) Inception V3;\
    \ (d) MobileNet \nV2; and (e) ResNet50 models with thermal imagery inputs for\
    \ crop water stress identification. \nTable 6. Validation performance of feature\
    \ extraction and function approximation models to \ncharacterize wheat water stress.\
    \ \nModels \nAccuracy (%) \nPrecision (%) \nSensitivity (%) \nF1 Score (%) \n\
    Feature Extraction-Based Approaches with only RGB Imagery Inputs \nAlexNet \n\
    93.4 \n91.4 \n94.5 \n92.2 \nGoogLeNet \n95.9 \n100 \n91.1 \n95.3 \nInception V3\
    \ \n92.5 \n94.4 \n89.4 \n91.8 \nMobileNet V2 \n94.4 \n89.0 \n100.0 \n94.1 \nResNet50\
    \ \n96.9 \n95.9 \n97.3 \n96.6 \nFeature extraction-based approaches with only\
    \ thermal imagery inputs \nAlexNet \n96.2 \n95.9 \n95.9 \n95.9 \nGoogLeNet \n\
    96.9 \n96.6 \n96.6 \n96.6 \nInception V3 \n97.5 \n96.6 \n98.0 \n97.3 \nMobileNet\
    \ V2 \n94.7 \n94.0 \n94.7 \n94.3 \nResNet50 \n98.4 \n96.7 \n100.0 \n98.3 \nFunction\
    \ approximation-based approaches (with RWC, SMC, Tc, and RH inputs) \nANN \n93.5\
    \ \n92.7 \n92.7 \n93.0 \nKNN \n88.1 \n90.2 \n84.1 \n86.9 \nLR \n89.2 \n95.1 \n\
    82.9 \n88.6 \nSVM \n91.4 \n95.1 \n86.7 \n90.8 \nDL-LSTM \n96.7 \n96.0 \n97.9 \n\
    97.0 \nFigure 6. Accuracy and loss curves for (a) AlexNet; (b) GoogLeNet; (c)\
    \ Inception V3; (d) MobileNet\nV2; and (e) ResNet50 models with thermal imagery\
    \ inputs for crop water stress identiﬁcation.\nTable 6. Validation performance\
    \ of feature extraction and function approximation models to charac-\nterize wheat\
    \ water stress.\nModels\nAccuracy (%)\nPrecision (%)\nSensitivity (%)\nF1 Score\
    \ (%)\nFeature Extraction-Based Approaches with only RGB Imagery Inputs\nAlexNet\n\
    93.4\n91.4\n94.5\n92.2\nGoogLeNet\n95.9\n100\n91.1\n95.3\nInception V3\n92.5\n\
    94.4\n89.4\n91.8\nMobileNet V2\n94.4\n89.0\n100.0\n94.1\nResNet50\n96.9\n95.9\n\
    97.3\n96.6\nFeature extraction-based approaches with only thermal imagery inputs\n\
    AlexNet\n96.2\n95.9\n95.9\n95.9\nGoogLeNet\n96.9\n96.6\n96.6\n96.6\nInception\
    \ V3\n97.5\n96.6\n98.0\n97.3\nMobileNet V2\n94.7\n94.0\n94.7\n94.3\nResNet50\n\
    98.4\n96.7\n100.0\n98.3\nFunction approximation-based approaches (with RWC, SMC,\
    \ Tc, and RH inputs)\nANN\n93.5\n92.7\n92.7\n93.0\nKNN\n88.1\n90.2\n84.1\n86.9\n\
    LR\n89.2\n95.1\n82.9\n88.6\nSVM\n91.4\n95.1\n86.7\n90.8\nDL-LSTM\n96.7\n96.0\n\
    97.9\n97.0\nPlants 2022, 11, 3344\n14 of 21\nPlants 2022, 11, x FOR PEER REVIEW\
    \ \n15 of 22 \n \n \n \nFigure 7. Confusion matrices for AlexNet, GoogLeNet, Inception\
    \ V3, MobileNet V2, and ResNet50 \nmodels pertinent validation datasets of RGB\
    \ imagery (a–e) and thermal imagery (f–j). Cell values \n(%) in row 1 column 2\
    \ represent type 1 error (TE1) while those in row 2 column 1 represent type 2\
    \ \nerror (TE2) (details in Section 2.4). Numbers (% and actual counts) in green\
    \ color indicate prediction \naccuracy while those in red color are prediction\
    \ errors for stressed and non-stressed crop classes. \nNumbers in green box represent\
    \ correct prediction and those in red box represent misclassification \nof non-stressed/stressed\
    \ classes. \n3.2.2. Function Approximation-Based Approaches \nAmongst the function\
    \ approximation approaches, the highest prediction accuracy \nwas obtained with\
    \ the DL-LSTM model (96.7%) followed by ANN (93.5%), SVM (91.4%), \nLR (89.2%),\
    \ and KNN models (88.1%) (Table 6). Moreover, the precision, sensitivity, and\
    \ \nF1 score were also highest for the DL-LSTM (96.0, 97.9, and 97.0%, respectively)\
    \ com-\npared to other ML models. The training and validation accuracies with\
    \ DL-LSTM \nshowed early convergence for which the loss on the validation dataset\
    \ reached minima at \n40 epochs (Figure 8). The TE1 for ANN, KNN, LR, SVM, and\
    \ LSTM were 3.2, 4.3, 2.2, 2.2, \nand 2.1%, respectively and TE2 were 3.2, 7.5,\
    \ 8.6, 6.5, and 1.1%, respectively (Figure 9). The \nDL-LSTM outperformed ML models\
    \ with the lowest mean error (Figure 9). \n \nFigure 8. Accuracy and loss corves\
    \ for Long Short Term memory based deep learning model for \ncrop water stress\
    \ identification. \nFigure 7. Confusion matrices for AlexNet, GoogLeNet, Inception\
    \ V3, MobileNet V2, and ResNet50\nmodels pertinent validation datasets of RGB\
    \ imagery (a–e) and thermal imagery (f–j). Cell values (%)\nin row 1 column 2\
    \ represent type 1 error (TE1) while those in row 2 column 1 represent type 2\
    \ error\n(TE2) (details in Section 2.4). Numbers (% and actual counts) in green\
    \ color indicate prediction\naccuracy while those in red color are prediction\
    \ errors for stressed and non-stressed crop classes.\nNumbers in green box represent\
    \ correct prediction and those in red box represent misclassiﬁcation of\nnon-stressed/stressed\
    \ classes.\n3.2.2. Function Approximation-Based Approaches\nAmongst the function\
    \ approximation approaches, the highest prediction accuracy was\nobtained with\
    \ the DL-LSTM model (96.7%) followed by ANN (93.5%), SVM (91.4%), LR\n(89.2%),\
    \ and KNN models (88.1%) (Table 6). Moreover, the precision, sensitivity, and\
    \ F1\nscore were also highest for the DL-LSTM (96.0, 97.9, and 97.0%, respectively)\
    \ compared\nto other ML models. The training and validation accuracies with DL-LSTM\
    \ showed early\nconvergence for which the loss on the validation dataset reached\
    \ minima at 40 epochs\n(Figure 8). The TE1 for ANN, KNN, LR, SVM, and LSTM were\
    \ 3.2, 4.3, 2.2, 2.2, and 2.1%,\nrespectively and TE2 were 3.2, 7.5, 8.6, 6.5,\
    \ and 1.1%, respectively (Figure 9). The DL-LSTM\noutperformed ML models with\
    \ the lowest mean error (Figure 9).\nPlants 2022, 11, x FOR PEER REVIEW \n15 of\
    \ 22 \n \n \n \nFigure 7. Confusion matrices for AlexNet, GoogLeNet, Inception\
    \ V3, MobileNet V2, and ResNet50 \nmodels pertinent validation datasets of RGB\
    \ imagery (a–e) and thermal imagery (f–j). Cell values \n(%) in row 1 column 2\
    \ represent type 1 error (TE1) while those in row 2 column 1 represent type 2\
    \ \nerror (TE2) (details in Section 2.4). Numbers (% and actual counts) in green\
    \ color indicate prediction \naccuracy while those in red color are prediction\
    \ errors for stressed and non-stressed crop classes. \nNumbers in green box represent\
    \ correct prediction and those in red box represent misclassification \nof non-stressed/stressed\
    \ classes. \n3.2.2. Function Approximation-Based Approaches \nAmongst the function\
    \ approximation approaches, the highest prediction accuracy \nwas obtained with\
    \ the DL-LSTM model (96.7%) followed by ANN (93.5%), SVM (91.4%), \nLR (89.2%),\
    \ and KNN models (88.1%) (Table 6). Moreover, the precision, sensitivity, and\
    \ \nF1 score were also highest for the DL-LSTM (96.0, 97.9, and 97.0%, respectively)\
    \ com-\npared to other ML models. The training and validation accuracies with\
    \ DL-LSTM \nshowed early convergence for which the loss on the validation dataset\
    \ reached minima at \n40 epochs (Figure 8). The TE1 for ANN, KNN, LR, SVM, and\
    \ LSTM were 3.2, 4.3, 2.2, 2.2, \nand 2.1%, respectively and TE2 were 3.2, 7.5,\
    \ 8.6, 6.5, and 1.1%, respectively (Figure 9). The \nDL-LSTM outperformed ML models\
    \ with the lowest mean error (Figure 9). \n \nFigure 8. Accuracy and loss corves\
    \ for Long Short Term memory based deep learning model for \ncrop water stress\
    \ identification. \nFigure 8. Accuracy and loss corves for Long Short Term memory\
    \ based deep learning model for crop\nwater stress identiﬁcation.\nPlants 2022,\
    \ 11, 3344\n15 of 21\nPlants 2022, 11, x FOR PEER REVIEW \n16 of 22 \n \n \n \n\
    Figure 9. The confusion matrices for function approximation-based (a) ANN; (b)\
    \ KNN; (c) LR; (d) \nSVM; and (e) DL-LSTM models. Cell values (%) in row 1 column\
    \ 2 represent type 1 error (TE1) while \nthose in row 2 column 1 represent type\
    \ 2 error (TE2) (details in Section 2.4). Numbers (% and actual \ncounts) in green\
    \ color indicate prediction accuracy while those in red color are prediction errors\
    \ for \nstressed and non-stressed crop classes. Numbers in green box represent\
    \ correct prediction and \nthose in red box represent misclassification of non-stressed/stressed\
    \ classes. \n4. Discussion \nSprinkler irrigation applies a predetermined quantity\
    \ of water and wets the entire \ncanopy, unlike traditional flood irrigation.\
    \ This cools down the microclimate and in-\ncreases relative air humidity to reduce\
    \ the microclimate’s water demand [52]. This could \nbe the reason for lower Tc\
    \ in all the sprinkler irrigation treatments compared to the cor-\nresponding\
    \ flood irrigation treatments (Figure 4). Lowered microclimate water demand \n\
    could have also resulted in lower soil moisture depletion from the root zone and\
    \ there-\nfore higher SMC with sprinkler irrigation [53]. In addition to sufficient\
    \ SMC, sprinkler \nirrigation results in lower deep percolation and nutrient leaching\
    \ compared to conven-\ntional flood irrigation [54–56]. This could have resulted\
    \ in a higher average yield for \nsprinkler irrigation treatment plots (5719 kg/ha)\
    \ compared to the flood irrigation treat-\nment plots (4898 kg/ha). With the projected\
    \ future climate change impacts in the form of \nlow rainfall frequencies and\
    \ high ambient temperatures, crop water stresses are further \nexpected to multiply,\
    \ which will multiply the penalties in yield potentials [3]. Therefore, \nstress-tolerant\
    \ crop cultivars need to be developed and planted for uncompromised yield \ngoals.\
    \ As also reported in our prior work based on canopy reflectance [57], water stress\
    \ \nstarted to occur before the CRI stage in both methods of irrigation. RWCs\
    \ were lower at \nlate jointing and flowering stages in case of flood irrigation.\
    \ Water stress at the flowering \nstage can result in significant yield and biomass\
    \ reductions [58] suggesting that it is also \ninfluenced by the phenological\
    \ growth stage. For robust analysis of this aspect, large \ndatasets are being\
    \ collected at each phenological growth stage. Water stress lowers CO2 \navailability\
    \ due to stomatal closure, thereby affecting photosynthesis and ultimately \n\
    growth, yield, and biomass [59,60]. \nCNNs have been increasingly used for plant\
    \ phenotyping applications over the past \ndecade for their capability of modeling\
    \ complicated plant processes by distinguishing \nand extracting regularized data\
    \ patterns [61,62]. It is for this reason; CNN models were \nhighly accurate in\
    \ predicting stressed and non-stressed crops using thermal and RGB \nimagery.\
    \ Chlorophyll is vital for photosynthesis, while carotenoids are critical \nnon-enzymatic\
    \ antioxidants. Water stress reduces chlorophyll and carotenoid contents, \nas\
    \ well as the ratio of chlorophyll ‘a’ to ‘b’, leading to leaf coloration changes.\
    \ This is the \nreason for RGB images also yielding satisfactory accuracy of up\
    \ to 94.6% for tracing leaf \ncolor changes [63]. Compared to RGB imagery, thermal\
    \ imagery is a more detailed and \nbetter presenter of the crop stress that alters\
    \ the emissivity patterns proportionally \n[64,65]. The canopy temperature is\
    \ affected by the microclimate conditions and the \navailable soil moisture [53].\
    \ This is the reason for the relatively lower accuracy of water \nstress detection\
    \ with RGB images (94.6%) than with thermal images (96.7%) irrespective \nFigure\
    \ 9. The confusion matrices for function approximation-based (a) ANN; (b) KNN;\
    \ (c) LR;\n(d) SVM; and (e) DL-LSTM models. Cell values (%) in row 1 column 2\
    \ represent type 1 error (TE1)\nwhile those in row 2 column 1 represent type 2\
    \ error (TE2) (details in Section 2.4). Numbers (% and\nactual counts) in green\
    \ color indicate prediction accuracy while those in red color are prediction\n\
    errors for stressed and non-stressed crop classes. Numbers in green box represent\
    \ correct prediction\nand those in red box represent misclassiﬁcation of non-stressed/stressed\
    \ classes.\n4. Discussion\nSprinkler irrigation applies a predetermined quantity\
    \ of water and wets the entire\ncanopy, unlike traditional ﬂood irrigation. This\
    \ cools down the microclimate and increases\nrelative air humidity to reduce the\
    \ microclimate’s water demand [52]. This could be the\nreason for lower Tc in\
    \ all the sprinkler irrigation treatments compared to the corresponding\nﬂood\
    \ irrigation treatments (Figure 4). Lowered microclimate water demand could have\n\
    also resulted in lower soil moisture depletion from the root zone and therefore\
    \ higher\nSMC with sprinkler irrigation [53]. In addition to sufﬁcient SMC, sprinkler\
    \ irrigation\nresults in lower deep percolation and nutrient leaching compared\
    \ to conventional ﬂood\nirrigation [54–56]. This could have resulted in a higher\
    \ average yield for sprinkler irrigation\ntreatment plots (5719 kg/ha) compared\
    \ to the ﬂood irrigation treatment plots (4898 kg/ha).\nWith the projected future\
    \ climate change impacts in the form of low rainfall frequencies\nand high ambient\
    \ temperatures, crop water stresses are further expected to multiply, which\n\
    will multiply the penalties in yield potentials [3]. Therefore, stress-tolerant\
    \ crop cultivars\nneed to be developed and planted for uncompromised yield goals.\
    \ As also reported in\nour prior work based on canopy reﬂectance [57], water stress\
    \ started to occur before the\nCRI stage in both methods of irrigation. RWCs were\
    \ lower at late jointing and ﬂowering\nstages in case of ﬂood irrigation. Water\
    \ stress at the ﬂowering stage can result in signiﬁcant\nyield and biomass reductions\
    \ [58] suggesting that it is also inﬂuenced by the phenological\ngrowth stage.\
    \ For robust analysis of this aspect, large datasets are being collected at each\n\
    phenological growth stage. Water stress lowers CO2 availability due to stomatal\
    \ closure,\nthereby affecting photosynthesis and ultimately growth, yield, and\
    \ biomass [59,60].\nCNNs have been increasingly used for plant phenotyping applications\
    \ over the past\ndecade for their capability of modeling complicated plant processes\
    \ by distinguishing and\nextracting regularized data patterns [61,62]. It is for\
    \ this reason; CNN models were highly\naccurate in predicting stressed and non-stressed\
    \ crops using thermal and RGB imagery.\nChlorophyll is vital for photosynthesis,\
    \ while carotenoids are critical non-enzymatic an-\ntioxidants. Water stress reduces\
    \ chlorophyll and carotenoid contents, as well as the ratio\nof chlorophyll ‘a’\
    \ to ‘b’, leading to leaf coloration changes. This is the reason for RGB\nimages\
    \ also yielding satisfactory accuracy of up to 94.6% for tracing leaf color changes\
    \ [63].\nCompared to RGB imagery, thermal imagery is a more detailed and better\
    \ presenter of the\ncrop stress that alters the emissivity patterns proportionally\
    \ [64,65]. The canopy tempera-\nture is affected by the microclimate conditions\
    \ and the available soil moisture [53]. This\nis the reason for the relatively\
    \ lower accuracy of water stress detection with RGB images\n(94.6%) than with\
    \ thermal images (96.7%) irrespective of the selected DL models (Table 5).\nA\
    \ similar observation is reported in a prior study [64] where higher accuracy\
    \ was obtained\nPlants 2022, 11, 3344\n16 of 21\nwith thermal imagery (89%) compared\
    \ to RGB imagery (82%) for wheat ear counting using\nDCNN models. Since thermal\
    \ imaging is often affected by the wind or RH factors of the\nenvironment, the\
    \ quality of data will be critical for training the DL models, especially when\n\
    acquired using aerial platforms [30,64]. Therefore, to maintain thermal data quality,\
    \ imag-\ning campaigns were launched when wind velocities were below 5 kmph. The\
    \ ResNet50\nhad relatively the highest accuracy among the feature extraction models.\
    \ Although it\nis the basic version of GoogLeNet and Inception V3, the performance\
    \ would be highly\nimpacted by the quality of input imagery, size, and robustness\
    \ of the dataset, especially\nfor the agricultural environments. ResNet50 addresses\
    \ the neural network degradation\nproblem by introducing identity mapping, which\
    \ results in the disappearance of gradient\nparameters and the non-ideal convergence\
    \ effect on the deeper networks [66,67]. This fea-\nture contributed to the enhanced\
    \ performance of ResNet50 compared to the other models\nthereby suggesting the\
    \ suitability of ResNet50 for agricultural applications for various\ncrop biotic\
    \ and abiotic stress characterizations. CNN models were also applied to thermal\n\
    imagery for water stress classiﬁcation in maize under well-irrigated, moderately\
    \ irrigated,\nand water-stressed treatments, obtaining an overall accuracy of\
    \ 89% [68]. Color and grey\nimages of maize were also used as inputs to the DCNN\
    \ model for water stress identiﬁcation\nwhere stress identiﬁcation and classiﬁcation\
    \ accuracies were 98% and 95%, respectively [26].\nThe inception-ResNet V2 framework\
    \ utilized for water stress identiﬁcation in sugarcane\nyielded an accuracy of\
    \ 83% with available soil water capacity as input [65]. Thus far, most\nof the\
    \ computer vision models have utilized single-dimensional data inputs, unlike\
    \ this\nstudy which advances water-stress identiﬁcation in wheat crops using multidimensional\n\
    data inputs. Multidimensional data modeling enhances the robustness and applicability\
    \ of\ndeveloped approaches across various agroclimatic conditions.\nCrop growth\
    \ or its water stress response is not necessarily linear to the weather or\nsoil\
    \ conditions [69]. Therefore, the linear (LR) and non-linear function approximation\n\
    approaches (ANN, KNN, SVM, and DL-LSTM) were evaluated to predict the stress class\
    \ of\nthe crop. ANN and SVM had a better stress prediction accuracy (Table 6)\
    \ compared to KNN\nand LR possibly due to two reasons (1) KNN or LR either use\
    \ locally linear segments or a\ngeneralized linear approach for making predictions\
    \ [66,69] and (2) KNN and LR models\ntrain on an unsupervised learning approach,\
    \ unlike ANN and SVM, which train on a\nsupervised learning approach [18]. ANN\
    \ and SVM had comparable accuracies for crop\nstress prediction. However, SVM\
    \ suits small datasets; while ANN can process relatively\nlarger datasets. Therefore,\
    \ ANN would have more conﬁdence in prediction classes.\nCrop phenotyping with\
    \ traditional function approximation approaches (ML mod-\nels) is often subjective\
    \ compared to the advanced DL-LSTM approach as those require\nmanual feature selection\
    \ of Tc, Ta, RH, and SMC. This restricts the robustness and accu-\nracy of the\
    \ ML models. Therefore, DL-LSTM outperformed the traditional ML models\n(Figure\
    \ 9) due to its automated and stabilized feature selection advantage [12,70].\
    \ This\nwas supported by minimum model loss compared to other function approximation-based\n\
    ML models. DL-LSTM not only integrates the thermal imagery features employed in\n\
    DL models but also combines the auxiliary soil and weather data inputs, of function\
    \ ap-\nproximation models. This eventually led to its superior performance over\
    \ the other ML\nmodels evaluated in this study as well as in prior studies of\
    \ crop stress and yield phenotyp-\ning [51] or irrigation forecasting [22]. However,\
    \ GoogleNet, Inception V3, and ResNet50\nprovided comparable or higher stress\
    \ prediction accuracy compared to the DL-LSTM model\n(Table 6). Stress/non-stress\
    \ misclassiﬁcation could be minimized through improved data\nsampling, increasing\
    \ training data size, and optimizing hyper-parameters, or by merging\ndifferent\
    \ ML and DL models for crop’s thermal emissivity and environment data inputs.\n\
    Among the feature extraction and function approximation-based approaches, the\
    \ feature\nextraction-based models outperformed all the function approximation-based\
    \ models for\nwater stress classiﬁcation.\nThe CNN models evaluated in this study\
    \ can be adopted for water stress identiﬁ-\ncation in other wheat cultivars while\
    \ for other crops and their cultivars, sufﬁcient data\nPlants 2022, 11, 3344\n\
    17 of 21\nacquisition, model training, and validations would be required. Along\
    \ similar lines, gath-\nering sufﬁcient data at different crop phenological stages\
    \ will enable growth stage-wise\naccuracy evaluation of ML and DL models in future\
    \ studies. The developed algorithms\nrequired below 5 sec to be successfully implemented\
    \ on independent images for classiﬁca-\ntion into stressed/non-stressed classes.\
    \ This is critical from a real-time stress diagnosis and\nmanagement perspective.\
    \ Trained algorithms are therefore transferrable to handheld or\nedge devices\
    \ for real-time stress detection by breeders, researchers, farmers, and students,\n\
    among others. For commercial adoption of the developed and tested approaches,\
    \ capital\ninvestment would be initially required following which high returns\
    \ may be expected\nthrough improvements in crop stress mitigation and management\
    \ at reduced costs [11].\n5. Conclusions\nThe canopy temperatures, relative water\
    \ content, soil moisture content, and grain\nyield for the wheat crop were signiﬁcantly\
    \ affected by the irrigation type and rates. Lower\nTc and higher RWC, SMC, and\
    \ yield were observed for irrigation at 100% of ETc compared\nto deﬁcit irrigation\
    \ (75, 50, and 25% of ETc). Moreover, a comparable or higher yield was\nobserved\
    \ for sprinkler irrigation compared to conventional ﬂood irrigation and amounted\n\
    to 20% of the water savings.\nThermal images resulted in higher crop water stress\
    \ classification accuracy (94.7–98.4%)\ncompared to RGB imagery (92.5–96.9%).\
    \ Moreover, the DL models (including DL-LSTM)\nperformed better than the ML models\
    \ for stressed and non-stressed crop classiﬁcation.\nAmong the function approximation-based\
    \ approaches, DL-LSTM had the highest accuracy\n(96.7%). Among the feature extraction-based\
    \ methods, ResNet50 had the highest accuracy\nof 96.9% and 98.4% with RGB and\
    \ thermal imagery inputs, respectively.\nOverall, DL models with thermal imagery\
    \ inputs could be highly efﬁcient for crop\nwater stress phenotyping. As a future\
    \ scope, feature extraction-based DL models could\nbe implemented on edge-computing\
    \ devices for real-time water stress monitoring and\nactuation of irrigation systems\
    \ through the internet of things.\nAuthor Contributions: Conceptualization, Y.A.R.\
    \ and N.S.C.; methodology, N.S.C. and M.K.T.;\nsoftware, K.D. and A.S.; validation,\
    \ A.S., K.D., N.S.C. and A.K.C.; resources, Y.A.R.; data curation,\nK.D., M.K.T.\
    \ and A.K.C.; writing—original draft preparation, N.S.C., Y.A.R., and A.K.C.;\
    \ writing—\nreview and editing, A.K.C. and M.K.T. All authors have read and agreed\
    \ to the published version of\nthe manuscript.\nFunding: This research was supported\
    \ by the Indian Council of Agricultural Research-Central\nInstitute of Agricultural\
    \ Engineering Bhopal, India, project# 824.\nData Availability Statement: Data\
    \ will be made available on personalized requests due to restrictions\nfrom the\
    \ parent organization.\nAcknowledgments: The authors would like to thank C.R.\
    \ Mehta and P.S. Tiwari from ICAR-CIAE\nBhopal, for their technical support in\
    \ the conduct of this study.\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nAbbreviations\nAbbreviation\nExpanded Form\nAdam\nAdaptive Moment\
    \ Estimation\nAN\nActual Non-Stressed Crop\nANN\nArtiﬁcial Neural Network\nANOVA\n\
    Analysis of Variance\nCIAE\nCentral Institute of Agricultural Engineering\nCNN\n\
    Convolution Neural Network\nDAS\nDay After Sowing\nDCNN\nDeep Convolution Neural\
    \ Network\nDL\nDeep Learning\nPlants 2022, 11, 3344\n18 of 21\nAbbreviation\n\
    Expanded Form\nDL-LSTM\nDeep Learning-Long Short Term Memory\nETc\nEvapotranspiration\n\
    F1\nF1 Score\nICAR\nIndian Council of Agricultural Research\nKNN\nKernel Nearest\
    \ Neighbor\nLR\nLogistic Regression\nLSTM\nLong Short Term Memory\nMAE\nMean Absolute\
    \ Error\nML\nMachine Learning\nP\nPrecision\nPS\nCorrectly Predicted Stressed\
    \ Crop From all the predictions\nRBF\nRadial Basis Function\nRF\nRandom Forest\n\
    RGB\nRed Green Blue\nRH\nRelative Humidity\nRWC\nRelative Water Content\nSD\n\
    Standard Deviation\nSe\nSensitivity\nSgdm\nStochastic Gradient Descent with Momentum\n\
    SMC\nSoil Moisture Content\nS\nSpeciﬁcity\nVM\nSupport Vector Machine\nTa\nAir\
    \ Temperature\nTc\nCanopy Temperature\nTE1\nType 1 Error\nTE2\nType 2 Error\n\
    TN\nTrue Negative\nTS\nTrue Positive\nUAS\nUnmanned Aerial System\nReferences\n\
    1.\nMega, R.; Abe, F.; Kim, J.-S.; Tsuboi, Y.; Tanaka, K.; Kobayashi, H.; Sakata,\
    \ Y.; Hanada, K.; Tsujimoto, H.; Kikuchi, J. Tuning\nWater-Use Efﬁciency and Drought\
    \ Tolerance in Wheat Using Abscisic Acid Receptors. Nat. Plants 2019, 5, 153–159.\
    \ [CrossRef]\n2.\nIhuoma, S.O.; Madramootoo, C.A. Recent Advances in Crop Water\
    \ Stress Detection. Comput. Electron. Agric. 2017, 141, 267–275.\n[CrossRef]\n\
    3.\nSeiﬁkalhor, M.; Niknam, V.; Aliniaeifard, S.; Didaran, F.; Tsaniklidis, G.;\
    \ Fanourakis, D.; Teymoorzadeh, M.; Mousavi, S.H.;\nBosacchi, M.; Li, T. The Regulatory\
    \ Role of γ-Aminobutyric Acid in Chickpea Plants Depends on Drought Tolerance\
    \ and Water\nScarcity Level. Sci. Rep. 2022, 12, 7034. [CrossRef]\n4.\nOletic,\
    \ D.; Bilas, V. How Thirsty the Crops Are: Emerging Instrumentation for Plant-Based\
    \ Field Measurement of Water Stress.\nIEEE Instrum. Meas. Mag. 2020, 23, 37–46.\
    \ [CrossRef]\n5.\nZhang, L.; Niu, Y.; Zhang, H.; Han, W.; Li, G.; Tang, J.; Peng,\
    \ X. Maize Canopy Temperature Extracted from UAV Thermal and\nRGB Imagery and\
    \ Its Application in Water Stress Monitoring. Front. Plant Sci. 2019, 10, 1270.\
    \ [CrossRef]\n6.\nAgam, N.; Cohen, Y.; Berni, J.A.J.; Alchanatis, V.; Kool, D.;\
    \ Dag, A.; Yermiyahu, U.; Ben-Gal, A. An Insight to the Performance of\nCrop Water\
    \ Stress Index for Olive Trees. Agric. Water Manag. 2013, 118, 79–86. [CrossRef]\n\
    7.\nElsayed, S.; Elhoweity, M.; Ibrahim, H.H.; Dewir, Y.H.; Migdadi, H.M.; Schmidhalter,\
    \ U. Thermal Imaging and Passive Reﬂectance\nSensing to Estimate the Water Status\
    \ and Grain Yield of Wheat under Different Irrigation Regimes. Agric. Water Manag.\
    \ 2017, 189,\n98–110. [CrossRef]\n8.\nChandel, A.K.; Khot, L.R.; Osroosh, Y.;\
    \ Peters, T.R. Thermal-RGB Imager Derived in-Field Apple Surface Temperature Estimates\n\
    for Sunburn Management. Agric. For. Meteorol. 2018, 253, 132–140. [CrossRef]\n\
    9.\nChandel, A.K.; Khot, L.R.; Molaei, B.; Peters, R.T.; Stöckle, C.O.; Jacoby,\
    \ P.W. High-Resolution Spatiotemporal Water Use Mapping\nof Surface and Direct-Root-Zone\
    \ Drip-Irrigated Grapevines Using Uas-Based Thermal and Multispectral Remote Sensing.\
    \ Remote\nSens. 2021, 13, 954. [CrossRef]\n10.\nChandel, N.S.; Chakraborty, S.K.;\
    \ Rajwade, Y.A.; Dubey, K.; Tiwari, M.K.; Jat, D. Identifying Crop Water Stress\
    \ Using Deep\nLearning Models. Neural Comput. Appl. 2021, 33, 5353–5367. [CrossRef]\n\
    11.\nTaheri-Garavand, A.; Mumivand, H.; Fanourakis, D.; Fatahi, S.; Taghipour,\
    \ S. An Artiﬁcial Neural Network Approach for\nNon-Invasive Estimation of Essential\
    \ Oil Content and Composition through Considering Drying Processing Factors: A\
    \ Case\nStudy in Mentha Aquatica. Ind. Crops Prod. 2021, 171, 113985. [CrossRef]\n\
    Plants 2022, 11, 3344\n19 of 21\n12.\nSingh, A.K.; Ganapathysubramanian, B.; Sarkar,\
    \ S.; Singh, A. Deep Learning for Plant Stress Phenotyping: Trends and Future\n\
    Perspectives. Trends Plant Sci. 2018, 23, 883–898. [CrossRef]\n13.\nGoldstein,\
    \ A.; Fink, L.; Meitin, A.; Bohadana, S.; Lutenberg, O.; Ravid, G. Applying Machine\
    \ Learning on Sensor Data for\nIrrigation Recommendations: Revealing the Agronomist’s\
    \ Tacit Knowledge. Precis. Agric. 2018, 19, 421–444. [CrossRef]\n14.\nPetrie,\
    \ P.R.; Wang, Y.; Liu, S.; Lam, S.; Whitty, M.A.; Skewes, M.A. The Accuracy and\
    \ Utility of a Low Cost Thermal Camera and\nSmartphone-Based System to Assess\
    \ Grapevine Water Status. Biosyst. Eng. 2019, 179, 126–139. [CrossRef]\n15.\n\
    Subeesh, A.; Bhole, S.; Singh, K.; Chandel, N.S.; Rajwade, Y.A.; Rao, K.V.R.;\
    \ Kumar, S.P.; Jat, D. Deep Convolutional Neural\nNetwork Models for Weed Detection\
    \ in Polyhouse Grown Bell Peppers. Artif. Intell. Agric. 2022, 6, 47–54. [CrossRef]\n\
    16.\nGutiérrez, S.; Diago, M.P.; Fernández-Novales, J.; Tardaguila, J. Vineyard\
    \ Water Status Assessment Using On-the-Go Thermal\nImaging and Machine Learning.\
    \ PLoS ONE 2018, 13, e0192037. [CrossRef]\n17.\nGhosal, S.; Blystone, D.; Singh,\
    \ A.K.; Ganapathysubramanian, B.; Singh, A.; Sarkar, S. An Explainable Deep Machine\
    \ Vision\nFramework for Plant Stress Phenotyping. Proc. Natl. Acad. Sci. USA 2018,\
    \ 115, 4613–4618. [CrossRef]\n18.\nSchmidhuber, J. Deep Learning in Neural Networks:\
    \ An Overview. Neural Netw. 2015, 61, 85–117. [CrossRef]\n19.\nChakraborty, S.K.;\
    \ Chandel, N.S.; Jat, D.; Tiwari, M.K.; Rajwade, Y.A.; Subeesh, A. Deep Learning\
    \ Approaches and Interventions\nfor Futuristic Engineering in Agriculture. Neural\
    \ Comput. Appl. 2022. [CrossRef]\n20.\nYalcin, H. Plant Phenology Recognition\
    \ Using Deep Learning: Deep-Pheno. In Proceedings of the 2017 6th International\n\
    Conference on Agro-Geoinformatics, Fairfax VA, USA, 7–10 August 2017; pp. 1–5.\n\
    21.\nHaider, S.A.; Naqvi, S.R.; Akram, T.; Umar, G.A.; Shahzad, A.; Sial, M.R.;\
    \ Khaliq, S.; Kamran, M. LSTM Neural Network Based\nForecasting Model for Wheat\
    \ Production in Pakistan. Agronomy 2019, 9, 72. [CrossRef]\n22.\nMouatadid, S.;\
    \ Adamowski, J.F.; Tiwari, M.K.; Quilty, J.M. Coupling the Maximum Overlap Discrete\
    \ Wavelet Transform and\nLong Short-Term Memory Networks for Irrigation Flow Forecasting.\
    \ Agric. Water Manag. 2019, 219, 72–85. [CrossRef]\n23.\nYoo, T.-W.; Oh, I.-S.\
    \ Time Series Forecasting of Agricultural Products’ Sales Volumes Based on Seasonal\
    \ Long Short-Term Memory.\nAppl. Sci. 2020, 10, 8169. [CrossRef]\n24.\nArif, S.;\
    \ Kumar, R.; Abbasi, S.; Mohammadani, K.; Dev, K. Weeds Detection and Classiﬁcation\
    \ Using Convolutional Long-Short-Term\nMemory; Research Square: Durham, NC, USA,\
    \ 2021.\n25.\nZhuang, S.; Wang, P.; Jiang, B.; Li, M.; Gong, Z. Early Detection\
    \ of Water Stress in Maize Based on Digital Images. Comput. Electron.\nAgric.\
    \ 2017, 140, 461–468. [CrossRef]\n26.\nAn, J.; Li, W.; Li, M.; Cui, S.; Yue, H.\
    \ Identiﬁcation and Classiﬁcation of Maize Drought Stress Using Deep Convolutional\
    \ Neural\nNetwork. Symmetry 2019, 11, 256. [CrossRef]\n27.\nNiu, Y.; Zhang, H.;\
    \ Han, W.; Zhang, L.; Chen, H. A Fixed-Threshold Method for Estimating Fractional\
    \ Vegetation Cover of Maize\nunder Different Levels of Water Stress. Remote Sens.\
    \ 2021, 13, 1009. [CrossRef]\n28.\nBiju, S.; Fuentes, S.; Gupta, D. The Use of\
    \ Infrared Thermal Imaging as a Non-Destructive Screening Tool for Identifying\n\
    Drought-Tolerant Lentil Genotypes. Plant Physiol. Biochem. 2018, 127, 11–24. [CrossRef]\n\
    29.\nChandel, A.K.; Khot, L.R.; Yu, L.-X. Alfalfa (Medicago Sativa L.) Crop Vigor\
    \ and Yield Characterization Using High-Resolution\nAerial Multispectral and Thermal\
    \ Infrared Imaging Technique. Comput. Electron. Agric. 2021, 182, 105999. [CrossRef]\n\
    30.\nPrashar, A.; Jones, H.G. Infra-Red Thermography as a High-Throughput Tool\
    \ for Field Phenotyping. Agronomy 2014, 4, 397–417.\n[CrossRef]\n31.\nAllen, R.G.;\
    \ Pereira, L.S.; Raes, D.; Smith, M. Crop Evapotranspiration-Guidelines for Computing\
    \ Crop Water Requirements-FAO\nIrrigation and Drainage Paper 56. FAO: Rome, Italy,\
    \ 1998; Volume 300, p. D05109.\n32.\nGoogle Experimental Layout of Winter Wheat\
    \ Crop at Different Rates Using Sprinkler and Flood Irrigation. 2022. Available\n\
    online: https://www.google.com/maps/ (accessed on 8 October 2022).\n33.\nPanigrahi,\
    \ N.; Das, B.S. Canopy Spectral Reﬂectance as a Predictor of Soil Water Potential\
    \ in Rice. Water Resour. Res. 2018, 54,\n2544–2560. [CrossRef]\n34.\nGomez, K.A.;\
    \ Gomez, A.A. Statistical Procedures for Agricultural Research; John Wiley & Sons:\
    \ Hoboken, NJ, USA, 1984; ISBN\n978-0-471-87092-0.\n35.\nTürko˘glu, M.; Hanbay,\
    \ D. Plant Disease and Pest Detection Using Deep Learning-Based Features. Turk.\
    \ J. Electr. Eng. Comput. Sci.\n2019, 27, 1636–1651. [CrossRef]\n36.\nHendrawan,\
    \ Y.; Damayanti, R.; Al Riza, D.F.; Hermanto, M.B. Classiﬁcation of Water Stress\
    \ in Cultured Sunagoke Moss Using\nDeep Learning. TELKOMNIKA (Telecommun. Comput.\
    \ Electron. Control) 2021, 19, 1594–1604. [CrossRef]\n37.\nEsgario, J.G.; Krohling,\
    \ R.A.; Ventura, J.A. Deep Learning for Classiﬁcation and Severity Estimation\
    \ of Coffee Leaf Biotic Stress.\nComput. Electron. Agric. 2020, 169, 105162. [CrossRef]\n\
    38.\nFulton, L.V.; Dolezel, D.; Harrop, J.; Yan, Y.; Fulton, C.P. Classiﬁcation\
    \ of Alzheimer’s Disease with and without Imagery Using\nGradient Boosted Machines\
    \ and ResNet-50. Brain Sci. 2019, 9, 212. [CrossRef]\n39.\nTurkoglu, M.; Hanbay,\
    \ D.; Sengur, A. Multi-Model LSTM-Based Convolutional Neural Networks for Detection\
    \ of Apple Diseases\nand Pests. J Ambient Intell Hum. Comput 2022, 13, 3335–3345.\
    \ [CrossRef]\n40.\nKandel, I.; Castelli, M. The Effect of Batch Size on the Generalizability\
    \ of the Convolutional Neural Networks on a Histopathology\nDataset. ICT Express\
    \ 2020, 6, 312–315. [CrossRef]\n41.\nBlum, A.; Shpiler, L.; Golan, G.; Mayer,\
    \ J. Yield Stability and Canopy Temperature of Wheat Genotypes under Drought-Stress.\n\
    Field Crops Res. 1989, 22, 289–296. [CrossRef]\nPlants 2022, 11, 3344\n20 of 21\n\
    42.\nRashid, A.; Stark, J.C.; Tanveer, A.; Mustafa, T. Use of Canopy Temperature\
    \ Measurements as a Screening Tool for Drought\nTolerance in Spring Wheat. J.\
    \ Agron. Crop Sci. 1999, 182, 231–238. [CrossRef]\n43.\nDeJonge, K.C.; Taghvaeian,\
    \ S.; Trout, T.J.; Comas, L.H. Comparison of Canopy Temperature-Based Water Stress\
    \ Indices for Maize.\nAgric. Water Manag. 2015, 156, 51–62. [CrossRef]\n44.\n\
    Olsovska, K.; Kovar, M.; Brestic, M.; Zivcak, M.; Slamka, P.; Shao, H.B. Genotypically\
    \ Identifying Wheat Mesophyll Conductance\nRegulation under Progressive Drought\
    \ Stress. Front. Plant Sci. 2016, 7, 1111. [CrossRef]\n45.\nLaxa, M.; Liebthal,\
    \ M.; Telman, W.; Chibani, K.; Dietz, K.-J. The Role of the Plant Antioxidant\
    \ System in Drought Tolerance.\nAntioxidants 2019, 8, 94. [CrossRef]\n46.\nWang,\
    \ X.; Vignjevic, M.; Jiang, D.; Jacobsen, S.; Wollenweber, B. Improved Tolerance\
    \ to Drought Stress after Anthesis Due to\nPriming before Anthesis in Wheat (Triticum\
    \ Aestivum L.) Var. Vinjett. J. Exp. Bot. 2014, 65, 6441–6456. [CrossRef]\n47.\n\
    Kukanov, I.; Hautamäki, V.; Lee, K.A. Recurrent Neural Network and Maximal Figure\
    \ of Merit for Acoustic Event Detection.\nIn Proceedings of the Proceedings of\
    \ the Workshop on Detection and Classiﬁcation of Acoustic Scenes and Events, Munich,\n\
    Germany, 16–17 November 2017.\n48.\nCastro-Zunti, R.; Park, E.H.; Choi, Y.; Jin,\
    \ G.Y.; Ko, S. Early Detection of Ankylosing Spondylitis Using Texture Features\
    \ and\nStatistical Machine Learning, and Deep Learning, with Some Patient Age\
    \ Analysis. Comput. Med. Imaging Graph. 2020, 82, 101718.\n[CrossRef]\n49.\nFan,\
    \ Y.; Bai, J.; Lei, X.; Zhang, Y.; Zhang, B.; Li, K.-C.; Tan, G. Privacy Preserving\
    \ Based Logistic Regression on Big Data. J. Netw.\nComput. Appl. 2020, 171, 102769.\
    \ [CrossRef]\n50.\nRehman, T.U.; Mahmud, M.S.; Chang, Y.K.; Jin, J.; Shin, J.\
    \ Current and Future Applications of Statistical Machine Learning\nAlgorithms\
    \ for Agricultural Machine Vision Systems. Comput. Electron. Agric. 2019, 156,\
    \ 585–605. [CrossRef]\n51.\nZhang, J.; Zhu, Y.; Zhang, X.; Ye, M.; Yang, J. Developing\
    \ a Long Short-Term Memory (LSTM) Based Model for Predicting Water\nTable Depth\
    \ in Agricultural Areas. J. Hydrol. 2018, 561, 918–929. [CrossRef]\n52.\nFanourakis,\
    \ D.; Aliniaeifard, S.; Sellin, A.; Giday, H.; Körner, O.; Nejad, A.R.; Delis,\
    \ C.; Bouranis, D.; Koubouris, G.;\nKambourakis, E. Stomatal Behavior Following\
    \ Mid-or Long-Term Exposure to High Relative Air Humidity: A Review. Plant\nPhysiol.\
    \ Biochem. 2020, 153, 92–105. [CrossRef]\n53.\nZhang, W.-Z.; Han, Y.-D.; Du, H.-J.\
    \ Relationship between Canopy Temperature at Flowering Stage and Soil Water Content,\
    \ Yield\nComponents in Rice. Rice Sci. 2007, 14, 67–70. [CrossRef]\n54.\nCavero,\
    \ J.; Medina, E.T.; Puig, M.; Martínez-Cob, A. Sprinkler Irrigation Changes Maize\
    \ Canopy Microclimate and Crop Water\nStatus, Transpiration, and Temperature.\
    \ Agron. J. 2009, 101, 854–864. [CrossRef]\n55.\nHome, P.G.; Panda, R.K.; Kar,\
    \ S. Effect of Method and Scheduling of Irrigation on Water and Nitrogen Use Efﬁciencies\
    \ of Okra\n(Abelmoschus Esculentus). Agric. Water Manag. 2002, 55, 159–170. [CrossRef]\n\
    56.\nWang, P.; Song, X.; Han, D.; Zhang, Y.; Zhang, B. Determination of Evaporation,\
    \ Transpiration and Deep Percolation of Summer\nCorn and Winter Wheat after Irrigation.\
    \ Agric. Water Manag. 2012, 105, 32–37. [CrossRef]\n57.\nChandel, N.S.; Rajwade,\
    \ Y.A.; Golhani, K.; Tiwari, P.S.; Dubey, K.; Jat, D. Canopy Spectral Reﬂectance\
    \ for Crop Water Stress\nAssessment in Wheat (Triticum Aestivum, L.). Irrig. Drain.\
    \ 2021, 70, 321–331. [CrossRef]\n58.\nGupta, N.K.; Gupta, S.; Kumar, A. Effect\
    \ of Water Stress on Physiological Attributes and Their Relationship with Growth\
    \ and\nYield of Wheat Cultivars at Different Stages. J. Agron. Crop Sci. 2001,\
    \ 186, 55–62. [CrossRef]\n59.\nYousefzadeh, K.; Houshmand, S.; Shiran, B.; Mousavi-Fard,\
    \ S.; Zeinali, H.; Nikoloudakis, N.; Gheisari, M.M.; Fanourakis, D.\nJoint Effects\
    \ of Developmental Stage and Water Deﬁcit on Essential Oil Traits (Content, Yield,\
    \ Composition) and Related Gene\nExpression: A Case Study in Two Thymus Species.\
    \ Agronomy 2022, 12, 1008. [CrossRef]\n60.\nOsakabe, Y.; Osakabe, K.; Shinozaki,\
    \ K.; Tran, L.-S. Response of Plants to Water Stress. Front. Plant Sci. 2014,\
    \ 5, 86. [CrossRef]\n61.\nNasiri, A.; Taheri-Garavand, A.; Fanourakis, D.; Zhang,\
    \ Y.-D.; Nikoloudakis, N. Automated Grapevine Cultivar Identiﬁcation via\nLeaf\
    \ Imaging and Deep Convolutional Neural Networks: A Proof-of-Concept Study Employing\
    \ Primary Iranian Varieties. Plants\n2021, 10, 1628. [CrossRef]\n62.\nTaheri-Garavand,\
    \ A.; Rezaei Nejad, A.; Fanourakis, D.; Fatahi, S.; Ahmadi Majd, M. Employment\
    \ of Artiﬁcial Neural Networks\nfor Non-Invasive Estimation of Leaf Water Status\
    \ Using Color Features: A Case Study in Spathiphyllum Wallisii. Acta Physiol.\n\
    Plant. 2021, 43, 78. [CrossRef]\n63.\nZomorrodi, N.; Rezaei Nejad, A.; Mousavi-Fard,\
    \ S.; Feizi, H.; Tsaniklidis, G.; Fanourakis, D. Potency of Titanium Dioxide\n\
    Nanoparticles, Sodium Hydrogen Sulﬁde and Salicylic Acid in Ameliorating the Depressive\
    \ Effects of Water Deﬁcit on Periwinkle\nOrnamental Quality. Horticulturae 2022,\
    \ 8, 675. [CrossRef]\n64.\nGrbovic, Z.; Panic, M.; Marko, O.; Brdar, S.; Crnojevic,\
    \ V. Wheat Ear Detection in RGB and Thermal Images Using Deep Neural\nNetworks.\
    \ Environments 2019, 11, 13.\n65.\nde Melo, L.L.; de Melo, V.G.M.L.; Marques,\
    \ P.A.A.; Frizzone, J.A.; Coelho, R.D.; Romero, R.A.F.; Barros, T.H. da S. Deep\
    \ Learning\nfor Identiﬁcation of Water Deﬁcits in Sugarcane Based on Thermal Images.\
    \ Agric. Water Manag. 2022, 272, 107820. [CrossRef]\n66.\nMhapsekar, M.; Mhapsekar,\
    \ P.; Mhatre, A.; Sawant, V. Implementation of Residual Network (ResNet) for Devanagari\
    \ Handwritten\nCharacter Recognition. In Advanced Computing Technologies and Applications;\
    \ Springer: Berlin/Heidelberg, Germany, 2020;\npp. 137–148.\n67.\nWang, F.; Qiu,\
    \ J.; Wang, Z.; Li, W. Intelligent Recognition of Surface Defects of Parts by\
    \ Resnet. J. Phys. Conf. Ser. 2021, 1883, 012178.\n[CrossRef]\nPlants 2022, 11,\
    \ 3344\n21 of 21\n68.\nZhuang, S.; Wang, P.; Jiang, B.; Li, M. Learned Features\
    \ of Leaf Phenotype to Monitor Maize Water Status in the Fields. Comput.\nElectron.\
    \ Agric. 2020, 172, 105347. [CrossRef]\n69.\nArchontoulis, S.V.; Miguez, F.E.\
    \ Nonlinear Regression Models and Applications in Agricultural Research. Agron.\
    \ J. 2015, 107,\n786–798. [CrossRef]\n70.\nWijaya, D.R.; Sarno, R.; Zulaika, E.\
    \ DWTLSTM for Electronic Nose Signal Processing in Beef Quality Monitoring. Sens.\
    \ Actuators\nB Chem. 2021, 326, 128931. [CrossRef]\n"
  inline_citation: '>'
  journal: Plants
  limitations: '>'
  pdf_link: https://www.mdpi.com/2223-7747/11/23/3344/pdf?version=1669960246
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Water Stress Identification of Winter Wheat Crop with State-of-the-Art AI
    Techniques and High-Resolution Thermal-RGB Imagery
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.20944/preprints202201.0445.v1
  analysis: '>'
  authors:
  - Luis Omar Colombo-Mendoza
  - Mario Andrés Paredes-Valverde
  - María del Pilar Salas-Zárate
  - Rafael Valencia-Garcı́a
  citation_count: 3
  full_citation: '>'
  full_text: '>

    This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details             Deny Allow selection
    Allow all Instructions for Authors Awards About FAQ Submit Log in/Register preprints.org
    > computer science and mathematics > data structures, algorithms and complexity
    > doi: 10.20944/preprints202201.0445.v1 Preprint Article Version 1 Preserved in
    Portico This version is not peer-reviewed Internet of Things-Driven Data Mining
    for Smart Crop Production Prediction in the Peasant Farming Domain Luis Omar Colombo-Mendoza
    * , Mario Andrés Paredes-Valverde , María del Pilar Salas-Zárate , Rafael Valencia-García
    * Version 1 : Received: 27 January 2022 / Approved: 31 January 2022 / Online:
    31 January 2022 (10:58:30 CET) How to cite: Colombo-Mendoza, L.O.; Paredes-Valverde,
    M.A.; Salas-Zárate, M.D.P.; Valencia-García, R. Internet of Things-Driven Data
    Mining for Smart Crop Production Prediction in the Peasant Farming Domain. Preprints
    2022, 2022010445. https://doi.org/10.20944/preprints202201.0445.v1 Copy Abstract
    Internet of Things (IoT) technologies can greatly benefit from machine learning
    techniques and Artificial Neural Networks for data mining and vice versa. In the
    agricultural field, this convergence could result in the development of smart
    farming systems suitable for use as decision support systems by peasant farmers.
    This work presents the design of a smart farming system for crop production, which
    is based on low-cost IoT sensors and popular data storage services and data analytics
    services on the Cloud. Moreover, a new data mining method exploiting climate data
    along with crop production data is proposed for the prediction of production volume
    from heterogeneous data sources. This method was initially validated using traditional
    machine learning techniques and open historical data of the northeast region of
    the state of Puebla, Mexico, which were collected from data sources from the National
    Water Commission and the Agri-food Information Service of the Mexican Government.
    Keywords data mining; predictive analytics; Internet of Things; peasant farming;
    smart farming system; crop production prediction Subject Computer Science and
    Mathematics, Data Structures, Algorithms and Complexity Copyright: This is an
    open access article distributed under the Creative Commons Attribution License
    which permits unrestricted use, distribution, and reproduction in any medium,
    provided the original work is properly cited. Download PDF Comments (0) We encourage
    comments and feedback from a broad range of readers. See criteria for comments
    and our Diversity statement. Leave a public comment Send a private comment to
    the author(s) * All users must log in before leaving a comment Related Articles
    Peer-review Articles CultivData: Application of IoT to the Cultivation of Agricultural
    Data Felipe Lemus-Prieto et al. IoT, 2021 Machine Learning Applications on Agricultural
    Datasets for Smart Farm Enhancement Fabrizio Balducci et al. Machines, 2018 Internet
    of Things Platform for Smart Farming: Experiences and Lessons Learnt Prem Jayaraman
    et al. Sensors, 2016 Intelligent Data Analytics Framework for Precision Farming
    Using IoT and Regressor Machine Learning Algorithms Ashay Rokade et al. Applied
    Sciences, 2022 Cloud Data-Driven Intelligent Monitoring System for Interactive
    Smart Farming Kristina Dineva et al. Sensors, 2022 A Smart Decision System for
    Digital Farming Carlos Cambra Baseca et al. Agronomy, 2019 Smart Agriculture and
    Rural Revitalization and Development Based on the Internet of Things under the
    Background of Big Data Xi Ma Sustainability, 2023 Smart Farming Techniques for
    Climate Change Adaptation in Cyprus George Adamides et al. Atmosphere, 2020 Smart
    Greenhouse Based on ANN and IOT Medhat Tawfeek et al. Processes, 2022 LoRaFarM:
    A LoRaWAN-Based Smart Farming Modular IoT Architecture Gaia Codeluppi et al. Sensors,
    2020 Views 251 Downloads 216 Comments 0 Get PDF Cite Share 0 Bookmark BibSonomy
    Mendeley Reddit Delicious Alerts Notify me about updates to this article or when
    a peer-reviewed version is published. Preprints.org is a free preprint server
    subsidized by MDPI in Basel, Switzerland. Contact us RSS MDPI Initiatives SciProfiles
    Sciforum Encyclopedia MDPI Books Scilit Proceedings JAMS Important links How it
    Works Advisory Board FAQ Friendly Journals Instructions for Authors About Statistics
    Subscribe Choose the area that interest you and we will send you notifications
    of new preprints at your preferred frequency. Subscribe © 2024 MDPI (Basel, Switzerland)
    unless otherwise stated Disclaimer Privacy Policy Terms of Use  Feedback'
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: null
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Internet of Things-Driven Data Mining for Smart Crop Production Prediction
    in the Peasant Farming Domain
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/access.2022.3233301
  analysis: '>'
  authors:
  - Pedro J. Vaz
  - G. Schütz
  - Carlos Guerrero
  - Pedro J. S. Cardoso
  citation_count: 5
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 11 Hybrid
    Neural Network Based Models for Evapotranspiration Prediction Over Limited Weather
    Parameters Publisher: IEEE Cite This PDF Pedro J. Vaz; Gabriela Schütz; Carlos
    Guerrero; Pedro J. S. Cardoso All Authors 5 Cites in Papers 755 Full Text Views
    Open Access Comment(s) Under a Creative Commons License Abstract Document Sections
    I. Introduction II. Reference Evapotranspiration Problematic and Known Solutions
    III. Proposed Models IV. CoAgMET Agricultural Weather Stations V. Conclusion and
    Future Work Show Full Outline Authors Figures References Citations Keywords Metrics
    Abstract: Evapotranspiration can be used to estimate the amount of water required
    by agriculture projects and green spaces, playing a key role in water management
    policies that combat the hydrological drought, which assumes a structural character
    in many countries. In this context, this work presents a study on reference evapotranspiration
    ( E T o ) estimation models, having as input a limited set of meteorological parameters,
    namely: temperature, humidity, and wind. Since solar radiation (SR) is an important
    parameter in the determination of E T o , SR estimation models are also developed.
    These E T o and SR estimation models compare the use of Artificial Neural Networks
    (ANN), Long Short Term Memory (LSTM), Gated Recurrent Unit (GRU), Recurrent Neural
    Network (RNN), and hybrid neural network models such as LSTM-ANN, RNN-ANN, and
    GRU-ANN. Two main approaches were taken for E T o estimation: (i) directly use
    those algorithms to estimate E T o , and (ii) estimate solar radiation first and
    then use that estimation together with other meteorological parameters in a method
    that predicts E T o . For the latter case, two variants were implemented: the
    use of the estimated solar radiation as (ii.1) a feature of the neural network
    regressors, and (ii.2) the use of the Penman-Monteith method (a.k.a. FAO-56PM
    method, adopted by the United Nations Food and Agriculture Organization) to compute
    E T o , which has solar radiation as one of the input parameters. Using experimental
    data collected from a weather station (WS) located in Vale do Lobo (Portugal),
    the later approach achieved the best result with a coefficient of determination
    ( R 2 ) of 0.977. The developed model was then applied to data from eleven stations
    located in Colorado (USA), with very distinct climatic conditions, showing similar
    results to the ones for which the models were initially designed ( R 2 >0.95 ),
    proving a good generalization. As a final notice, the reduce... (Show More) Comparison
    of classical, artificial neural networks and recurrent neural networks to build
    a Hybrid FAO56-PM method to estimate evapotranspiration under constrained weath...View
    more Published in: IEEE Access ( Volume: 11) Page(s): 963 - 976 Date of Publication:
    29 December 2022 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2022.3233301 Publisher:
    IEEE Funding Agency: CCBY - IEEE is not the copyright holder of this material.
    Please follow the instructions via https://creativecommons.org/licenses/by/4.0/
    to obtain full-text articles and stipulations in the API documentation. SECTION
    I. Introduction A known strategy, to make a water efficient irrigation system,
    is to rely on soil humidity sensors and keep the humidity levels between the field
    capacity (FC) and the management allowable depletion (MAD), which is a percentage
    of the available soil water holding capacity [1]. Apart from components, installation,
    and management costs associated to the implementation of such systems on public
    green space, other problems are common such as vandalism or theft, since normally
    there is no comprehensive security or surveillance in such locations. Further,
    the available water holding capacity changes significantly with soil type [2],
    requiring that for each specific soil, in order to determine the soil humidity
    values corresponding to the field capacity and wilting point (WP), samples would
    need to be sent to a laboratory for analyses. Crop evapotranspiration ( E T c
    ), also known as crop water use, is the water that is used by a crop [3]. The
    Food and Agriculture Organization of the United Nations (FAO) recommends using
    the FAO-56 Penman-Monteith (FAO-56PM) formula as a reference method for computing
    reference evapotranspiration ( E T o ) [4], being E T c and E T o related by a
    crop coefficient ( K c ). FAO56-PM formula uses four main meteorological parameters:
    temperature, humidity, wind, and solar radiation (SR). In this context, several
    computational studies show that SR is the main driver of E T o [5], however, its
    measurement requires sensors like pyranometers, which are typically associated
    with expensive WSs, that need to be properly maintained and calibrated [6]. Also,
    solar radiation forecast application programming interfaces (APIs) are not common
    (at least freely) and present a high system cost penalty. As part of a framework
    for the computation of optimal crop water irrigation scheduling requirements (with
    special emphasis to green spaces), this paper presents the computational models
    being prepared to estimate the E T o using machine learning, deep learning, acquired
    intelligence, meteorological data from WSs on the field, as well as meteorological
    data and forecasts from APIs available on the internet. This will optimize water
    and energy expenditure, improve the well-being of the crop, reduce reaction time
    in solving problems, improve anomalies detection methods, and maintain the quality
    of green spaces. Concisely, the framework being developed will be an intelligent
    irrigation solution, technologically differentiated from other platforms on the
    market. The development of the full framework is being done under project GSSIC
    – Green Spaces SMART Irrigation Control. In short, this study compares several
    E T o and SR estimation models that use Artificial Neural Networks (ANN), Long
    Short Term Memory (LSTM), Gated Recurrent Unit (GRU), Recurrent Neural Network
    (RNN), and hybrid neural network models such as LSTM-ANN, RNN-ANN and GRU-ANN.
    Two approaches were taken for E T o ’s estimation: (i) directly use those machine
    learning models to estimate it and (ii) first estimate solar radiation, and then
    use the obtained value and other meteorological parameters in a method that predicts
    E T o . Furthermore, for the latter case, two variants were addressed, namely:
    the use of the estimated SR as (ii.1) a feature of a neural network regressor,
    and (ii.2) the use of FAO-56PM method to compute E T o , which has SR as one of
    the input parameters. Figure 1 schematizes what we have just detailed. FIGURE
    1. Workflow for the developed approaches: E T o estimation models without SR and
    limited set of features, solar radiation estimation models with a limited set
    of features, and E T o computation based on estimated solar radiation as an input
    to another neural network model or to FAO56-PM formula. Show All Using experimental
    data collected from a WS located in Vale do Lobo, south Portugal, approach (ii.2)
    achieved the best result with a coefficient of determination ( R 2 ) of 0.977,
    improving results previously achieved in [7]. The quality and generalization of
    the proposed models was then tested using 11 weather stations of the Colorado
    Agricultural Meteorological Network (CoAgMET), which is composed of automatic
    weather stations distributed across the state of Colorado, USA. The selected weather
    stations have data available starting from August 1992, which comprises almost
    30 years of data. Also, with temperatures ranging from −39.06 to 44.26 degrees
    Celsius (°C), they represent different climatic conditions than the Mediterranean’s
    Vale do Lobo WS. This allows the assessment of model performance under different
    climate conditions, and for a longer period of historical data. The achieved metrics
    values, when comparable with the ones from other authors, showed that our proposal
    has in general a better performance in the analysed metrics, besides presenting
    alternatives to most of the existing models, namely through the use of recurrent
    neural networks and hybrid methods. The latter model is even more unusual in this
    category of studies, being by itself another evolution to the state of the art.
    The paper is structured as follows. The next section presents the problem’s background
    and the methodologies used by others to tackle the problem in study. Section III
    starts by detailing the computational setup and exploring the dataset, followed
    by section III-B where the proposed neural network architectures, hyperparameters,
    and overall training approach is presented. Then, on section III-C the proposed
    models and associated performance analysis is weighed. Section IV presents the
    results for the 11 CoAgMET agricultural weather stations. Finally, the last section
    presents the conclusion and establishes some future work. SECTION II. Reference
    Evapotranspiration Problematic and Known Solutions Reference evapotranspiration,
    E T o , is the evapotranspiration of a reference surface, defined as hypothetical
    grass with a uniform height of 0.12 m, a fixed surface resistance of 70 s m −1
    , and an albedo (reflection coefficient) of 0.23 [4]. Besides, crop evapotranspiration,
    E T c , represents the crop’s water requirements and is proportional to reference
    evapotranspiration by means of the crop coefficient, K c [4]. Therefore, E T o
    prediction plays an important role, making it one of the fundamental parameters
    for smart irrigation scheduling, since it is proportional to the amount of water
    that needs to be restored during the irrigation period [8]. Some of the main characteristics
    that distinguish E T c from E T o are (i) the crop cover density and total leaf
    area, (ii) the resistance of foliage epidermis and soil surface to the flow of
    water vapor, (iii) the aerodynamic roughness of the crop canopy, and (iv) the
    reflectance of the crop and soil surface to short wave radiation [9]. In this
    context, known the value of K c , the E T c value for a specific time period can
    be estimated by E T c = K c E T o . (1) View Source The crop coefficient can be
    simple or have two components, one representing the basal crop coefficient ( K
    cb ) and another representing the soil surface evaporation component ( K e ),
    being computed by K c = K s K cb + K e , (2) View Source where K s ∈[0,1] is used
    to introduce a K c reduction in cases of environmental stresses, such as lack
    of soil water or soil salinity [9]. So, it becomes clear that in order to make
    crop water requirement predictions, accurate estimation of E T o is required.
    Historically, several deterministic methods have been developed to estimate reference
    evapotranspiration using single or limited weather parameters and being generally
    categorized as: temperature, radiation, or combination based. For example, temperature
    based methods include Thorntwait [10], Blaney-Criddle [11], and Hargreaves and
    Samani [12] formulas; radiation methods include Priestley-Taylor [13] and Makkink
    [14] formulas; and combination methods, requiring both temperature and radiation,
    include Penman [15], modified Penman [16], and FAO-56 Penman-Monteith (FAO56-PM)
    [4] formulas. Shahidian et al. [6] give an overview of several of these methods
    and compare their performance under different climate conditions. The authors
    concluded that, when applied to climates different from those on which the methods
    were developed and tested, most of them yield a poor performance and may require
    the adjustment of empirical coefficients to accommodate to local climate conditions,
    which is not ideal. FAO recommends the use of the FAO-56PM formula as a reference
    method for estimating E T o [4]. To give a deeper idea of the involved parameters,
    measured in millimeters per day [ mm/day ], the formula to estimate E T o is given
    by E T o = 0.408Δ( R n −G)+γ 900 T+273 u 2 ( e s − e a ) Δ+γ(1+0.34 u 2 ) , (3)
    View Source where R n is the net radiation at crop surface [ MJ m −2 da y −1 ],
    G is the soil heat flux density [ MJ m −2 da y −1 ], T is the air temperature
    at 2 m height [ o C ], u 2 is the wind speed at 2 m height [ m s −1 ], e s is
    the saturation vapor pressure [ kPa ], e a is the actual vapor pressure [ kPa
    ], e s − e a is the saturation vapor pressure deficit [ kPa ], Δ is the slope
    vapor pressure curve [ kP a o C −1 ], and γ is the psychrometric constant [ kP
    a o C −1 ]. Being based on physical principles, the formula has become widely
    adopted as a standard for E T o computation since it performs well under different
    climate types [6]. However, to compute E T o using FAO56-PM the following main
    meteorological parameters are required: temperature, solar radiation, relative
    humidity, and wind speed. The remaining meteorological parameters are constants
    being derived from latitude and elevation above sea level. Except for solar radiation,
    all parameters (real or estimated) required by the FAO56-PM formula for E T o
    ’s computation, can be freely obtained from common weather forecast APIs. Solar
    radiation forecasting APIs are, currently, not common and the ones available present
    a high-cost penalty. Therefore, a major asset would be to (i) develop alternative
    methods for E T o estimation using limited meteorological parameters, that do
    not require solar radiation and are compatible with the weather parameters obtained
    by freely available weather forecast and historical weather data APIs, and/or
    (ii) to estimate the solar radiation itself and use it as an approximation on
    the solar radiation dependent methods. This is also important since in most situations
    a proper functioning, maintained, and calibrated WS, with solar radiation measurement
    capability, is not close to the area of interest. Recently, as an alternative,
    several authors have used machine and deep learning to estimate E T o , being
    that, as stated by Chia et al. [17], machine learning has proved itself to be
    a promising solution for E T o estimation with the common meteorological data.
    For instance, using data collected in Central Florida, a humid subtropical climate,
    Granata [18] compared three different evapotranspiration models which differ in
    the input variables. In their work, four variants of machine learning algorithms
    were applied to each model, namely: M5P Regression Tree, Bagging, Random Forest,
    and Support Vector Machine (SVM). Their best results are achieved using M5P Regression
    Tree and Bagging with a coefficient of determination of 0.987 and a mean absolute
    error of 0.14 mm/day (see the metrics definition in Sec. III-A). However, among
    other features, all models included as input variable the net solar radiation.
    The same author also studied three recurrent neural network-based models for the
    prediction of short term ahead evapotranspiration [19]. Two variants of each model
    were developed, selecting the employed algorithm between LSTM and nonlinear autoregressive
    network with exogenous inputs (NARX). The prediction models were trained and tested
    using data from two sites with different climates: Cypress Swamp, southern Florida,
    and Kobeh Valley, central Nevada. With reference to the subtropical climatic conditions
    of South Florida, LSTM models proved to be more accurate than NARX models, while
    some exogenous variables such as sensible heat flux and relative humidity did
    not affect the results significantly. Wu and Fan [20] evaluated eight machine
    learning algorithms divided in four classes: neuron based (MLP – Multilayer Perceptron,
    GRNN – General Regression Neural Network, and ANFIS – Adaptive Network-based Fuzzy
    Inference System), kernel-based (SVM, KNEA – Kernel-based Non Linear Extension
    of Arps decline model), tree-based (M5Tree – M5 model tree, Extreme Gradient Boosting
    – XGBoost), and curve based (MARS – Multivariate Adaptive Regression Spline).
    The methods were applied to data collected from 14 WSs in various climatic regions
    of China and used only temperature or temperature and precipitation as input to
    the models. Ferreira et al. [8] used six alternative empirical reduced-set equations,
    such as Hargreaves and Samani [12], and compared the estimated values with the
    ones from an ANN and a SVM model. Data was collected from 203 WSs and used for
    daily E T o estimation for the entirety of Brazil. Temperature or temperature
    and humidity were used as input features. They concluded that, in general, ANN
    was the best performing model when including, as input features, data from up
    to four previous days. With the best algorithms reaching an R 2 median value around
    0.80 considering all stations, results were weighed good given that only temperature
    or temperature and humidity were used as input. Keshtegar et al. [21] used high-order
    response surface method to compute E T o . They included daily weather information
    which include the maximum temperature, maximum humidity, wind speed, solar radiation,
    and vapor pressure deficit, obtained from three observation stations in Burkina
    Faso, West Africa. Ten models were evaluated with the determination coefficient
    ( R 2 ) and root-mean-square error values ranging between 0.2068 and 0.9966 and
    between 0.7237 and 0.9948, respectively. However, one of the used features is
    solar radiation, which is not always easy to obtain for most of the locations.
    Muhammad et al. [22] studied the implementation of evolutionary computing models,
    namely the gene expression programming (GEP), for the simulation of daily E T
    o in different locations of Peninsular Malaysia. The models used various input
    combinations of meteorological variables including air temperature (mean, maximum,
    and minimum), relative humidity, solar radiation, and mean wind speed. Compared
    to other black box artificial intelligence algorithms, the authors’ alleged major
    advantage of GEP is that it provides a set of equations that can be used by practitioners
    for reliable estimation of E T o at the field, with fewer meteorological variables
    and, thus, can have wide applicability in water resources management. As in the
    previous case, the model also requires the use of solar radiation with the already
    mentioned drawback. Khosravi et al. [23] studied nine models, including five data
    mining algorithms and four adaptive neuro-fuzzy inference systems, for their ability
    to predict E T o at meteorological stations in Baghdad and Mosul, Iraq. As parameters
    for the models, they considered wind speed, sunshine hours, rainfall, maximum
    and minimum temperature, and relative humidity. Investigations on the modeling
    accuracy with different input parameter combinations showed that no single input
    combination showed a consistent modeling outcome. Moreover, hybrid models showed
    a higher predictive power than the individual models. A large part of the result
    shown had R 2 value around 0.9, with the best one achieving 0.951 for one station
    and 0.97 for the other. The geographical robustness of various inter-model ensembles
    in estimating daily E T o was assessed in [17]. The study aimed to develop inter-model
    ensembles that consist of ANN, support vector regressors, ANFIS via Bayesian modeling
    approach, and the non-linear neural ensemble, trained for different meteorological
    stations with differential geographical characteristics in the Peninsular Malaysia.
    This work allowed to infer some aspects of the effect of the geographical characteristics
    on the performance of inter-model ensembles and examine the effect of data management
    strategies applied to solve the data-hungry issue (both qualitative and quantitative)
    of inter-model ensembles. Sanikhani et al. [24] explore 6 artificial intelligence
    models for modeling E T o using minimum and maximum temperatures of the air and
    extraterrestrial radiation. The models include MLP, GRNN, radial basis neural
    networks (RBNN), integrated adaptive neuro-fuzzy inference systems with grid partitioning
    and subtractive clustering (ANFIS-GP and ANFIS-SC), and GEP. The Hargreaves–Samani
    equation and its calibrated version were used to perform a verification analysis
    of the established models. In the best cases, results show R 2 values around 0.95.
    In [25], Chen et al. study the estimation of daily reference evapotranspiration
    using three deep learning models (namely, deep neural network – DNN, temporal
    convolution neural network – TCN, and LSTM neural network). The performance of
    the three models was compared with the results of an SVM algorithm, a random forest
    algorithm, and empirical equations like Hargreaves and modified Hargreaves(temperature-based),
    Ritchie, Priestley-Talor, and Makkink (radiation-based), and Romanenko and Schendel
    (humidity-based) empirical models. Several other works can be found in the literature
    [26], [27], [28], [29], [30], [31], [32], [33], which vary the number and type
    of features, estimation models etc. Vaz et al. [7] used data from a Vale do Lobo
    WS, in south Portugal, and explored the use of machine learning for E T o estimation.
    They concluded that instead of directly estimating E T o , the best result was
    obtained by using machine learning for SR estimation, having as input a limited
    set of meteorological features, and then use that result together with temperature,
    humidity and wind speed as input to the FAO56-PM equation, achieving an R 2 of
    0.975, a MAE of 0.18 mm/day , and a MAPE of 5.51 %. The work here presented explores
    and develops deep learning based E T o prediction models, supported on the same
    data, improving the previous results, as it will be detailed in the next sections.
    Furthermore, the proposed models are applied to 11 other WS stations maintaining,
    without further hyperparameter tuning, similar performance in terms of attained
    metrics values. So, based on a set of features generally available in public WS,
    which do not include solar radiation, our solution produces a stable set of results
    for different weather environments presenting a state-of-the-art methodology.
    SECTION III. Proposed Models A. Experimental Setup and Dataset As a general introduction
    to the computational environment, this work was conducted using Python v3.9.7,
    Numpy v1.21.4 [34], Pandas v1.3.4 [35], [36], Tensor Flow v2.6.0 [37], Keras 2.6.0
    [38], Scikit-learn v1.0.1 [39], and PyET v1.1.0 [40]. The Pandas library was used
    for data analysis and manipulation, PyET to compute the reference evapotranspiration
    using the FAO56-PM method, Scikit-learn is a python machine learning framework
    that includes data preprocessing, model selection, and model metrics evaluation
    tools. Keras runs on top of Tensor Flow, and all neural network models here presented
    were developed using it. Finally, all computation was done on a 2020 MacBook Air
    with an Apple M1 SoC chip and 16 GB of RAM, running MacOS Big Sur v11.6.4. Data
    from Vale do Lobo WS, in south Portugal, was collected starting from February
    2019 up to and including September 2021. The WS is composed of sensors from Davis
    Instruments, where the following weather parameters are measured periodically
    throughout the day and stored with a daily resolution: temperature (minimum, maximum,
    and average), dew point (minimum, maximum, and average), relative humidity (minimum,
    maximum, and average), solar radiation (maximum and average), wind speed (minimum,
    maximum, and average), wind direction, atmospheric pressure (minimum, maximum,
    and average), rain intensity, and precipitation. This is the same dataset previously
    used by Vaz et al. [7], where the use of machine learning algorithms was explored
    to create E T o and SR estimation models. A ratio of 75 % to 25 % of train and
    test data was used, respectively, resulting in train data starting from February
    1st, 2019 up to February 3rd, 2021, and test data from February 4th, 2021 up to
    September 30th, 2021. Being a time series, no shuffling was made to the train
    and test data, and the train data was further divided into 10 folders, used to
    implement time series cross validation [41]. Furthermore, a hyperparameter grid
    search strategy was used to tune the proposed machine/deep learning methods, as
    will be presented in the corresponding sections. For model statistical evaluation
    and performance comparison, the mean absolute error (MAE), the mean absolute percentage
    error (MAPE), the mean square error (MSE), the root-mean-square error (RMSE),
    and the coefficient of determination ( R 2 ) were used [42]. Just to recall, considering
    y t the actual value and y t ^ the estimated value at instants t=1,2,…,n , and
    y ¯ ¯ ¯ the mean value of the actual samples, the evaluation metrics are defined
    as MAE= MAPE= MSE= RMSE= 1 n ∑ t=1 n | y t − y t ^ |, 1 n ∑ t=1 n ∣ ∣ ∣ y t −
    y t ^ y t ∣ ∣ ∣ ×100%, 1 n ∑ t=1 n ( y t − y t ^ ) 2 , MSE − − − − − √ , (4) (5)
    (6) (7) View Source and R 2 =1− ∑ n t=1 ( y t − y t ^ ) 2 ∑ n t=1 ( y t − y ¯
    ¯ ¯ ) 2 , (8) View Source In this set of metrics, the MAE measures the average
    of the absolute residuals in the original unit and the MAPE measures the average
    absolute percent error. The MSE represents the average of the squared difference
    between the original and predicted values in the data set, measuring the variance
    of the residuals in the squared unit of the original data, which can be returned
    to the original unit by computing the RMSE (the standard deviation of residuals).
    Finally, R 2 represents the proportion of the variance in the dependent variable,
    which is explained by the linear regression model, being a scale-free score. While
    values should be near to zero for the MAE, MAPE, MSE, and RMSE, they should be
    near to 1 for the R 2 metric. Depending on the regression problem in study ( E
    T o vs. SR), two targets were considered: (i) E T o was computed using the FAO56-PM
    formula, as per Eq. (3), and using as input the data measured by the already referred
    WS, namely temperature, humidity, wind speed and solar radiation (see Sec. III-C1,
    III-C3, and III-C4); and (ii) the average solar radiation measured by the WS was
    used as SR target (see Sec. III-C2). B. Neural Network Models’ Architecture The
    following neural network types were used for the conduction of this work: ANN,
    LSTM [43], GRU [44], and Artificial RNN [45], as well as hybrid models like LSTM-ANN,
    RNN-ANN, and GRU-ANN. Artificial neural networks typically have an input layer,
    one or more hidden layers, and one output layer. The input layer dimension is
    defined by the number of inputs to the model, i.e., the number of features that
    are used on a particular model. Hidden layers are internal layers that can vary
    in quantity and number of neurons. The output layer can have one or more neurons,
    depending on the number of outputs a model has. Artificial neurons are activated
    using activation functions, which have an important role on the performance of
    neural networks, since they provide the non-linearity that is needed to learn
    a complex problem [46]. During the conduction of this work, Rectified Linear Unit
    (ReLU), Tangent Hyperbolic Function (Tanh) and Sigmoid activation functions were
    tested, however ReLU always yielded better results, being therefore fixed [47].
    LSTM, GRU, and RNN algorithms are deep learning methods that, unlike feed-forward
    neural networks, implement feedback connections. These feedback connections provide
    the ability to add memory to the models, making its use justified through the
    fact that the dataset (and target) is a time series, where some patterns might
    be cyclic or information from the previous days might play an important role [43],
    [44], [45]. Furthermore, in the work presented by Vaz et al. [7], for the same
    problem and dataset here in study (using limited weather parameters as input features),
    several machine learning regression models were compared in their performance,
    namely: Ordinary Least Squares, Ridge, Lasso, k -Nearest Neighbors, Support Vector
    Machine, Decision Tree, and Random Forest (RF). Since Random Forest [48], [49]
    gave the best results for E T o and SR estimation models, RF will also be evaluated,
    so that the proposed neural network based models can be directly compared with
    other top performing methods. Systematic testing (varying the number of neurons
    in powers of 2) allowed us to set the non-hybrid models to have two hidden layers:
    the first one consisting of 512 neurons, followed by a second layer that has 32
    neurons, represented as having [512, 32] neurons. This configuration consistently
    gave the best results, being observed that augmenting the number of layers and/or
    neurons of the neural network would not increase model performance, while reducing
    would impact on model’s performance. Following the same arrangement, allowed us
    to decide the hybrid models’ architecture (namely, LSTM-ANN, RNN-ANN, and GRU-ANN).
    In this case, to balance the training computational requirements and the models’
    performance, it was decided to use four hidden layers. E.g., as depicted in Fig.
    2, the LSTM-ANN model has the first two hidden layers consisting of [32, 64] neurons
    of LSTM type, followed by two more layers of [64, 32] neurons of a fully connected
    ANN. The RNN-ANN and GRU-ANN models are similar, being all represented as having
    [32, 64; 64, 32] neurons. Also, as can be seen in Fig. 2, between each layer there
    is the possibility of using dropout regularization. FIGURE 2. LSTM-ANN neural
    network architecture. Show All Solver, loss function, and hyperparameters like
    learning rate, number of epochs, and batch size play an important role on model
    training and performance [50]. For all models, the Adam optimizer [51] was selected
    as a solver, mean squared error (MSE) was used as the loss function, kernel initialization
    was done using the Glorot normal initialization [52], and the number of epochs
    was set to a high value of 1000, however, early stopping was used, with a patience
    value set to 150. Dropout regularization [53] was applied during training and
    its value was selected using grid search with values ranging from 0.0 up to 0.6,
    in 0.1 steps. During initial model development and training, the batch size was
    also selected using a grid search approach, with a range from 64 up to 256, in
    multiples of 32. However, a batch size of 128 always gave a good and consistent
    result, so later it was fixed to 128 for all models, reducing models training
    time. Input features were normalized to values between 0 and 1. Finally, max norm
    weight constraint was applied, and its value was grid searched between 1 and 4,
    in steps of one. It should also be noticed that time lag was introduced on all
    models, i.e., data from the previous days was introduced as an input feature together
    with current day data. In this case, time lags from 1 up to 10 days were tested,
    however it brought no improvement on models performance. In this experimental
    context, attempts of introducing new features through the use of polynomial features
    and features inverse were also made, but did not present any further improvement
    in models’ metrics and, as such, it is not presented here. In appendix, Tables
    5 and 6 summarize the tested and tuned hyperparameters, respectively, for each
    of the proposed models. TABLE 1 Comparison of Several Regression Methods for E
    T o Estimation Using a Limited Set of Features TABLE 2 Comparison of Several Regression
    Methods for Average Solar Radiation Estimation Using a Limited Set of Features
    TABLE 3 Comparison of Several Regression Methods for E T o Estimation Using a
    Limited Set of Features, and the Previously Estimated Solar Radiation TABLE 4
    Comparison of Several Regression Methods for E T o Estimation Using a Limited
    Set of Features. Data With a Span of Almost 30 Years Was Collected From 11 WS
    Spread Across the Colorado State, and Model Performance Metrics From the 11 WS
    Was Averaged for Each of the Developed Models TABLE 5 Sets of Hyperparameters
    Used in the Grid Search Procedure TABLE 6 Tuned Hyperparameters C. Estimation
    Using ANN Methods With a Limited Set of Features Solar radiation is not normally
    available either as a measurement or as a forecast, therefore the need to develop
    models to estimate E T o that do not require it as an input feature. In this context,
    E T o estimation models that have as input a limited set of parameters are explored
    in Sec. III-C1. Furthermore, since solar radiation is the main factor for the
    determination of E T o [4], in Sec. III-C2 solar radiation estimation models that
    use a limited set of features are explored. Finally, two approaches are taken:
    (i) use the previously estimated solar radiation as an input to another neural
    network model (Sec. III-C3) or (ii) use the FAO56-PM formula to compute E T o
    , using as an input feature the estimated solar radiation (Sec. III-C4). Please
    refer to Fig. 1 for scheme of the proposed flow. 1) E T o Estimation Using ANN
    Methods (Excluding Solar Radiation) In this section neural network based models
    are used to directly estimate E T o , using as inputs limited weather parameters.
    The set of features used are Month∈{1,2,…,12} , maximum and minimum temperature
    ( TempMax and TempMin ), average humidity ( HumididtyAvg ), and average wind speed
    ( WindAvg ). Table 1 shows the results obtained by the non-hybrid models and RF
    (included for comparison purposes), being clear that neural network based models
    outperform, in all metrics, the RF model. ANN, LSTM, GRU and RNN give similar
    results, the best one being the GRU based model with an R 2 of 0.962, a MAE of
    0.24 mm/day , and a MAPE of 7.25 %. The plot of the target E T o (blue), of the
    E T o estimated using the ANN model (orange), and of the corresponding absolute
    error (green) in Fig. 3 shows that the estimator follows relatively well the E
    T o target value. In the same figure, the shadowed region corresponds to the test
    data, being visible the expectable slight increase of the absolute error, when
    comparing to the train region (as stated earlier, all presented metrics are calculated
    using only the test data). Table 3 also presents the results obtained using the
    hybrid models, namely, LSTM-ANN, RNN-ANN, GRU-ANN. The results are slightly worse
    than the ones obtained using the GRU, with the LSTM-ANN model attaining an R 2
    of 0.959 against the 0.962 of GRU (MAE, MAPE, MSE, and RMSE are also similar).
    FIGURE 3. Target E T o vs. ANN estimation, where solar radiation was not used
    as feature. Show All 2) Solar Radiation Estimation Using ANN Methods With a Limited
    Set of Features As an alternative to the presented in the previous section, and
    since solar radiation is the main driver of evapotranspiration [4], [5], this
    section studies solar radiation estimation models that have as input a limited
    feature set, compatible with the limited number of features returned by the common
    weather forecast APIs. The estimated result will be later injected as a feature
    in other machine learning model or used in the FAO56-PM formula (Eq. 3). So, in
    this section, the solar radiation measured by the WS was used as target. Initially,
    the features used in the solar radiation models were the same as the ones proposed
    by Vaz et al. [7], namely: Month , Day , maximum and minimum temperature ( TempMax
    and TempMin ), average humidity ( HumididtyAvg ), average wind speed ( WindAvg
    ), dew point, and the polynomial feature Mont h 2 ×Day . However, it was found
    that the model metrics were improved by dropping dew point and the polynomial
    feature Mont h 2 ×Day and, instead, adding the sunset hour angle ( ω s ) and daylight
    hours ( N ). As a note, the sunset hour angle ( ω s ) was defined as [4] ω s =arccos[−tan(φ)×tan(δ)],
    (9) View Source where φ is the latitude of the WS (in radians) and δ=0.409sin(
    2π 365 J−1.39) (10) View Source is the solar declination (also in radians), and
    J is the number of the day in the year. Furthermore, the daylight hours (N) is
    given by N= 24 π ω s . (11) View Source The results obtained are summarized in
    Table 2, where it can be seen that RNN and ANN are the best performing methods,
    with RNN being slightly better with an R 2 of 0.833, a MAE of 18.46 W/ m 2 /day
    , a MAPE of 10.30 %, and a RMSE of 28.817. This result is better than what was
    obtained by Vaz et al. [7], using the same dataset, on which Random Forest gave
    the best results, with an R 2 of 0.814, a MAE of 21.31 W/ m 2 /day , and a MAPE
    of 11.29 %. The inclusion of daylight hours and sunset angle improved performance
    for all neural network based models but, as can be seen on Table 2, it worsened
    Random Forest performance. Fig. 4 depicts the target solar radiation that was
    measured by the solar station (blue), the approximated solar radiation obtained
    with ANN method (orange), and the absolute error curve (green). Shadowed is the
    test set. This solar radiation estimation will be used next to predict the E T
    o values. FIGURE 4. Target solar radiation vs ANN estimation, input features include
    daylight hours and sunset angle as derived features. Show All As in Sec. III-C1,
    the recursive hybrid LSTM-ANN, RNN-ANN and GRU-ANN models’ results, summarized
    also in Table 2, give similar performance to the recursive non-hybrid methods,
    with the advantage of requiring less computation power, since the number of trainable
    parameters is highly reduced due to the smaller network that is used. E.g., the
    LSTM model has 1,128,609 trainable parameters, while the LSTM-ANN model only has
    35,841, requiring approximately 21 % of the training time, and 58 % of the inference
    time. 3) E T o Estimation Using the Approximated Solar Radiation This section
    describes the implementation of the E T o estimation using as features solar radiation
    (approximated using the ANN presented in Sec. III-C2) together with maximum temperature,
    average humidity, and average wind speed. To be more clear, the difference between
    the present models and the ones in Sec. III-C1 is the use of the (estimated) SR,
    which was not previously used. The obtained results are summarized in Table 3.
    The ANN model was the best one with an R 2 of 0.968, a MAE of 0.22 mm/day , a
    MAPE of 6.66 %, and a RMSE of 0.303 (against the R 2 of 0.962, the MAE of 0.24
    mm/day , the MAPE of 7.25%, and the RMSE of 0.331 previously obtained with GRU
    model). Similarly, with the advantages previously stated, the LSTM-ANN model achieved
    an R 2 of 0.967, just one thousandth worse than the ANN model, but improving the
    MAPE to 6.5%. Also, the obtained result is better than the ones presented in Vaz
    et al. [7], using a similar technique but based on ML algorithms, where the best
    performing model was Random Forest with an R 2 of 0.951, a MAE of 0.26 mm/day
    , and a MAPE of 7.44 %. Feature engineering as well as the use of other weather
    limited features was attempted, but no further improvements could be made. Fig.
    5 (top) depicts the target E T o (blue), estimated E T o (orange), and absolute
    error (green) curves for the train and test (shadowed) dataset, allowing to observe
    the model closely follows the reference evapotranspiration target. FIGURE 5. Target
    E T o vs ANN estimation (top) and Target E T o vs FAO56-PM values using estimated
    solar radiation (bottom). Show All 4) Estimation Using FAO56-PM Equation and the
    Approximated Solar Radiation In this section, another type of hybrid approach
    was tested, which uses the previously estimated solar radiation as an input, together
    with temperature, humidity and wind speed to compute E T o using the FAO56-PM
    formula (SR-ANN → FAO56-PM). The result obtained is presented in Table 3, with
    an R 2 of 0.977, MAE of 0.16 mm/day and MAPE of 5.05 %. This result is better
    than the previously obtained ones, and once again is also better than what was
    presented by Vaz et al. [7] which consisted of an R 2 of 0.975, an MAE of 0.18
    mm/day , and an MAPE of 5.51 %. Fig. 5 (bottom) plots E T o target, estimated
    E T o , and absolute error, where in the bottom plot an improvement of the absolute
    error can be seen when compared with the previous plots. Next section will generalize
    the current results to 11 WS ran by the Colorado Agricultural Meteorological Network
    (CoAgMET). SECTION IV. CoAgMET Agricultural Weather Stations The Colorado Agricultural
    Meteorological Network (CoAgMET) is a network of automatic weather stations distributed
    across the state of Colorado. The collected weather data, such as temperature,
    humidity, wind speed and solar radiation, can be accessed via an API at https://www.coagmet.com.
    Data from 11 weather stations was collected, namely the stations having the following
    code names: alt01, avn01, ctz01, hyk02, idl01, ksy01, lcn01, oth01, pkh01, rfd01,
    yjk01. These weather stations were chosen because they are spread across the Colorado
    state and have data available starting from August 1992, up to and including July
    2021 (when the test were run), which comprises almost 30 years of data. Also,
    with temperatures ranging from −39.06 to 44.26 degrees Celsius (°C), and having
    desert zones, the Colorado state poses different climatic conditions than the
    Mediterranean based Vale do Lobo. This allows the assessment of model performance
    under different climate conditions than Vale do Lobo, and for a longer period
    of historical data. For each WS, the ANN, the LSTM-ANN and the RF models were
    trained using the same hyperparameters found for the Vale do Lobo Dataset (see
    Table 6). As previously, a ratio of 75 % to 25 % of train and test data was used,
    meaning that the developed models are making predictions for more than 7 years.
    The results obtained for the 11 WS were averaged and are presented in Table 4.
    In short, the best results were attained by the hybrid method where SR is estimated
    by an ANN and then injected in the FAO56-PM formula (SR-ANN → FAO56-PM), achieving
    an R 2 of 0.984, a RMSE of 0.261 mm/day , a MSE of 0.069 m m 2 /day , a MAE of
    0.16 mm/day , and a MAPE of 4.84 %. Furthermore, R 2 and RMSE variation across
    all stations is small, as can be observed in the boxplots of Fig. 6. Also, for
    all models, R 2 is always above 0.958 and RMSE is below 0.45 mm/day , which can
    be considered a good result. MAE metric variation across all WS is also small,
    as it can be observed in Table 4 and inferred from the boxplots on Fig. 7 which
    shows that, for the 7 years of test data, the maximum absolute error is below
    1 mm/day , apart outliers. As a note, values are regarded as outliers if they
    are more than 1.5 times the interquartile range ( IQR ) below the first quartile
    ( Q 1 ) or more than 1.5×IQR above the third quartile ( Q 3 ), i.e., values outside
    the interval [ Q 1 −1.5×IQR, Q 3 +1.5×IQR] . FIGURE 6. Boxplot graphs of R 2 (top)
    and RMSE (bottom) obtained using the following E T o estimation models: ANN without
    SR, LSTM-ANN without SR, RF without SR, ANN with SR estimated by an ANN (SR-ANN),
    LSTM-ANN with SR-ANN, RF with SR-ANN, and the hybrid method where SR-ANN is used
    as a feature in FAO56-PM formula (SR-ANN → FAO56-PM). The models were trained
    using data collected from 11 WS, that are spread across the Colorado state. Show
    All FIGURE 7. Boxplot graph of the absolute errors for each of the 11 Colorado
    state WS, using the hybrid FAO56-PM method, where SR was estimated using the ANN
    model and then injected into the FAO56-PM formula (SR-ANN → FAO56-PM). Show All
    SECTION V. Conclusion and Future Work Evapotranspiration can be used to estimate
    the amount of water required by agriculture projects and green spaces, playing
    a key role in water management policies that combat the hydrological drought.
    Some equipment can be used to measure E T o , but they are expensive to buy and
    maintain. As an alternative, many use meteorological data to estimate the E T
    o values, but again, in the most well accepted formula (FAO56-PM), solar radiation
    is required, which is not common to have in general meteorological APIs. In this
    work, several neural network based regression models (namely, ANN, LST, GRU, and
    RNN) and hybrid approaches (namely, LSTM-ANN, RNN-ANN, GRU-ANN) for E T o estimation
    were developed with different degrees of success. Since solar radiation is the
    main E T o driver, as stated by several authors, models were also developed for
    estimating solar radiation using input features that are readily available in
    common weather forecast APIs. This allowed both the use of the previously estimated
    solar radiation in neural network regressors, to estimate E T o , but also the
    possibility to use the hybrid approach where solar radiation is previously estimated,
    and then the FAO56-PM method is used to finally compute E T o . For the Vale do
    Lobo WS, the latter yielded the best results, with an R 2 of 0.977, a RMSE of
    0.256 mm/day , a MSE of 0.066 m m 2 /day , a MAE of 0.16 mm/day , and a MAPE of
    5.05 %. The attained results were also compared against the ones in a previous
    work from the authors, were several other machine learning methods were used (namely,
    Ordinary Least Squares, Ridge, Lasso, k -Nearest Neighbors, Support Vector Machine,
    Decision Tree, and Random Forest), demonstrating the overall good result, considering
    the limited weather parameter features that were used. Being the conditions different,
    and impossible to replicate, we can also see that the proposed methods generically
    improve the metric values attained by other authors. An extensive dataset collected
    from 11 WS from the Colorado CoAgMET network was used to assess model performance
    under different climate conditions and longer periods of data. Our results show
    that the evaluated models give similar performance to the ones obtained when using
    the Vale do Lobo dataset, with average R 2 of 0.984, MAE of 0.16 mm/day , MAPE
    of 4.84 %, and a RMSE of 0.261, when using the Hybrid FAO56-PM model. The result
    is particularly relevant when considering the diversity of weather conditions,
    and the long period of data that comprises almost 30 years. Also, our work shows
    that the recurrent hybrid network models (e.g., LSTM-ANN) give similar results
    to the non-hybrid recurrent network models (e.g., LSTM). However, its computational
    cost is lower due to the decrease of trainable parameters, requiring only 21 %
    of training, and 58 % of inference time when compared to the non-hybrid recurrent
    networks. This is an important result when using edge computing, where computational
    power is limited. Future work will include a dataset collected from the existing
    WS infrastructure that is installed in the Algarve region, in south Portugal.
    The objective will be to develop local and pooled models of E T o predictors for
    the Algarve region. Also, since all limited feature models here presented are
    compatible with the freely available weather forecast APIs, the impact of using
    such APIs as input data to the ML models here developed needs to be further assessed.
    ACKNOWLEDGMENT The authors thank the GSSIC Project’s Companies Visualforma - Tecnologias
    de Informação, S. A. and Itelmatis, Lda., and the Portuguese Foundation for Science
    and Technology (FCT) under Project UIDB/50009/2020—LARSyS, Project UIDB/00631/2020–CEOT
    BASE, Project UIDP/00631/2020—CEOT Programático, and Project UIDB/05183/2020–MED.
    Appendix Deep Learning Algorithm Parameters In this section, the hyperparameters
    and value ranges that were used in the grid search procedure are presented in
    Table 5. Tuned hyperparameters, for models presented in Table 1, 2, and 3, are
    presented in Table 6. Please note that, fixed hyperparameter values (e.g., number
    of neurons, and neural network architecture) are described in Sec. III-B. Authors
    Figures References Citations Keywords Metrics More Like This Prediction of global
    solar radiation in UAE using artificial neural networks 2013 International Conference
    on Renewable Energy Research and Applications (ICRERA) Published: 2013 A review
    of solar radiation prediction using artificial neural networks 2017 International
    Conference on Wireless Technologies, Embedded and Intelligent Systems (WITS) Published:
    2017 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: '>'
  journal: IEEE access
  limitations: '>'
  pdf_link: https://ieeexplore.ieee.org/ielx7/6287639/10005208/10003192.pdf
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Hybrid Neural Network Based Models for Evapotranspiration Prediction Over
    Limited Weather Parameters
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/s23041827
  analysis: '>'
  authors:
  - Canh Nguyen
  - Vasit Sagan
  - Sourav Bhadra
  - Stephen P. Moose
  citation_count: 10
  full_citation: '>'
  full_text: ">\nCitation: Nguyen, C.; Sagan, V.;\nBhadra, S.; Moose, S. UAV Multisensory\n\
    Data Fusion and Multi-Task Deep\nLearning for High-Throughput Maize\nPhenotyping.\
    \ Sensors 2023, 23, 1827.\nhttps://doi.org/10.3390/s23041827\nAcademic Editor:\
    \ Wonsuk (Daniel)\nLee\nReceived: 13 November 2022\nRevised: 16 January 2023\n\
    Accepted: 3 February 2023\nPublished: 6 February 2023\nCopyright:\n© 2023 by the\
    \ authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access\
    \ article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative Commons\n\
    Attribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\n\
    sensors\nArticle\nUAV Multisensory Data Fusion and Multi-Task Deep Learning\n\
    for High-Throughput Maize Phenotyping\nCanh Nguyen 1,2,3, Vasit Sagan 1,2,*\n\
    , Sourav Bhadra 1,2\nand Stephen Moose 4\n1\nTaylor Geospatial Institute, St.\
    \ Louis, MO 63108, USA\n2\nDepartment of Earth and Atmospheric Sciences, Saint\
    \ Louis University, St. Louis, MO 63108, USA\n3\nDepartment of Aviation, University\
    \ of Central Missouri, Warrensburg, MO 64093, USA\n4\nDepartment of Crop Science\
    \ and Technology, University of Illinois, Urbana, IL 61801, USA\n*\nCorrespondence:\
    \ vasit.sagan@slu.edu\nAbstract: Recent advances in unmanned aerial vehicles (UAV),\
    \ mini and mobile sensors, and GeoAI\n(a blend of geospatial and artiﬁcial intelligence\
    \ (AI) research) are the main highlights among agri-\ncultural innovations to\
    \ improve crop productivity and thus secure vulnerable food systems. This\nstudy\
    \ investigated the versatility of UAV-borne multisensory data fusion within a\
    \ framework of\nmulti-task deep learning for high-throughput phenotyping in maize.\
    \ UAVs equipped with a set of\nminiaturized sensors including hyperspectral, thermal,\
    \ and LiDAR were collected in an experimental\ncorn ﬁeld in Urbana, IL, USA during\
    \ the growing season. A full suite of eight phenotypes was in\nsitu measured at\
    \ the end of the season for ground truth data, speciﬁcally, dry stalk biomass,\
    \ cob\nbiomass, dry grain yield, harvest index, grain nitrogen utilization efﬁciency\
    \ (Grain NutE), grain\nnitrogen content, total plant nitrogen content, and grain\
    \ density. After being funneled through a\nseries of radiometric calibrations\
    \ and geo-corrections, the aerial data were analytically processed in\nthree primary\
    \ approaches. First, an extended version normalized difference spectral index\
    \ (NDSI)\nserved as a simple arithmetic combination of different data modalities\
    \ to explore the correlation\ndegree with maize phenotypes. The extended NDSI\
    \ analysis revealed the NIR spectra (750–1000 nm)\nalone in a strong relation\
    \ with all of eight maize traits. Second, a fusion of vegetation indices, struc-\n\
    tural indices, and thermal index selectively handcrafted from each data modality\
    \ was fed to classical\nmachine learning regressors, Support Vector Machine (SVM)\
    \ and Random Forest (RF). The prediction\nperformance varied from phenotype to\
    \ phenotype, ranging from R2 = 0.34 for grain density up\nto R2 = 0.85 for both\
    \ grain nitrogen content and total plant nitrogen content. Further, a fusion of\n\
    hyperspectral and LiDAR data completely exceeded limitations of single data modality,\
    \ especially\naddressing the vegetation saturation effect occurring in optical\
    \ remote sensing. Third, a multi-task\ndeep convolutional neural network (CNN)\
    \ was customized to take a raw imagery data fusion of\nhyperspectral, thermal,\
    \ and LiDAR for multi-predictions of maize traits at a time. The multi-task\n\
    deep learning performed predictions comparably, if not better in some traits,\
    \ with the mono-task deep\nlearning and machine learning regressors. Data augmentation\
    \ used for the deep learning models\nboosted the prediction accuracy, which helps\
    \ to alleviate the intrinsic limitation of a small sample size\nand unbalanced\
    \ sample classes in remote sensing research. Theoretical and practical implications\
    \ to\nplant breeders and crop growers were also made explicit during discussions\
    \ in the studies.\nKeywords: UAV; data fusion; multi-task deep learning; high-throughput\
    \ phenotyping; hyperspectral;\nLiDAR; GeoAI\n1. Introduction\nTimely and accurate\
    \ crop estimates prior to harvest have a great impact on national\nfood policy\
    \ [1], food security, and personal living standards [2]. The conventional estima-\n\
    tion, however, has heavily relied on ground-based ﬁeld surveys, which are labor-costly\n\
    Sensors 2023, 23, 1827. https://doi.org/10.3390/s23041827\nhttps://www.mdpi.com/journal/sensors\n\
    Sensors 2023, 23, 1827\n2 of 38\nand prone to poor crop assessment [3]. Therefore,\
    \ developing a low-cost, rapid, and accu-\nrate high-throughput method for phenotyping\
    \ at a ﬁeld scale is acutely desired for crop\nproduction. Recent technological\
    \ advancements in unmanned aerial vehicles (UAV) and\nsensor miniaturization have\
    \ ﬁlled the current explosive demand for precision agriculture\nin general and\
    \ for high-throughput plant phenotyping in particular. With a UAV system,\naerial\
    \ data at very ﬁne and high spectral, spatial, and temporal resolutions can be\
    \ remotely\nacquired over small to medium ﬁelds for crop monitoring in cost-efﬁcient\
    \ and rapid ﬂight\nmissions [4,5]. The choice of UAV is generally not a matter\
    \ when both ﬁxed- and rotary-\nwing can carry automated phenotyping tasks; the\
    \ matter rests in the payload and mounted\nsensors that would dictate the purpose\
    \ of the study.\nCountless previous studies conducted unmanned aerial missions\
    \ to scout various\ncrops: soybean [5], corn [6], sunﬂower [7], rice [8], maize\
    \ [9], cotton [10], but most of them\nexploited crop properties from passive remote\
    \ sensing data recorded on a few to several\nspectral wavelengths such as red–green–blue\
    \ (RGB) and multispectral sensors mounted\non UAV platforms. Fewer studies used\
    \ UAV-based hyperspectral imaging (HSI) in plant\nphenomics including biochemical\
    \ traits: chlorophyll [11,12], nitrogen [13], biophysical\ntraits: biomass [12,14],\
    \ height and leaf area index (LAI) [12]), physiological traits (water sta-\ntus\
    \ [15], stomatal conductance and ﬂuorescence [16]), biotic stress (i.e., disease)\
    \ [17,18], and\ngrain yield [19–22]. Its broad applicability is perhaps because\
    \ that hyperspectral imaging\nincreases the wavebands to hundreds and even thousands\
    \ of contiguous spectra in both\nvisible (VIS) and near-infrared (NIR) regions,\
    \ which provides enriched pertinent spectral\ninformation of objects. As an imagery\
    \ cube, it concurrently offers spatial information\nalong the image height and\
    \ width, as well as continuous spectral information along the\nimage depth.\n\
    To a certain extent, the great abundance of information of hyperspectral cubes\
    \ poses a\nvariety of challenges in processing and interpreting the data. The\
    \ imbalance between the\nhigh dimensionality of the imagery data and the limited\
    \ availability of training samples of-\nten occurs in remote sensing datasets,\
    \ which is also known as the Hughes phenomenon [23].\nAn adoption of dimensionality\
    \ reduction strategies is necessary to alleviate the issue, includ-\ning but not\
    \ limited to selecting a set of wavelengths [24–26], handcrafting representative\n\
    features such as vegetation indices [16,18], orthogonal transformation (e.g.,\
    \ principal com-\nponent analysis) [27], derivative analysis [24], and wavelets\
    \ and correlation plots [28].\nPreserving the great richness of hyperspectral\
    \ images is a strenuous task if one approaches\nthe process on an image-wise basis,\
    \ as the nature of spatial–pectral information varies\namong inter- and intra-objects\
    \ in a scene of view. Machine vision with widely known\ntechniques, convolutional\
    \ neural networks (CNNs) and its variants in 1D-CNNs, 2D-CNNs,\n3D-CNNs, or hybrid,\
    \ could automate the task by sliding kernel patches to obtain both spa-\ntial\
    \ and spectral representations for regression or classiﬁcation prediction. The\
    \ extraction of\ninterrelated spatial–spectral features can be done by two common\
    \ methods. It can process\nthe spatial features separately by 1D-CNNs or 2D-CNN\
    \ [29,30] and then incorporate the\nresulting spatial features with the spectral\
    \ features extracted from the Recurrent Neural\nNetwork (RNN), Long Short-Term\
    \ Memory (LSTM) [30,31] to have a complete fusion. It can\nbe alternatively done\
    \ by leveraging 3D-CNNs [18] with 3-dimensional patches (p × p × b)\nassociated\
    \ with p × p spatial neighborhood pixels and b spectral bands to extract spatial\
    \ in\ntandem with spectral abstracts, which fully exploits important discriminative\
    \ patterns in\nthe hyperspectral data cubes. This is not to mention that the challenges\
    \ are exponentially\nampliﬁed by the working complication of the UAV hyperspectral\
    \ system when a moving\nUAV platform and the maneuvering offsets must be taken\
    \ during the imagery calibration\nprocess [32]. The following sections in this\
    \ study will address these challenges in detail\nfrom various angles.\nUAV thermal\
    \ (TIR) are other passive optical remote sensing data, ranging at 3–14 µm\nin\
    \ the electromagnetic spectrum. The aerial thermal platform is simply and cost-effectively\n\
    operational and thus, has been widely used in monitoring terrestrial vegetation\
    \ via mea-\nsures of canopy temperature and spectral emissivity [33]. The aerial\
    \ thermal imaging has\nSensors 2023, 23, 1827\n3 of 38\nbeen introduced as a very\
    \ versatile tool for various applications: for instance, discerning\ncrop water\
    \ stress status [34–36], irrigation scheduling [37]. In regard to plant phenotyp-\n\
    ing, thermal imaging remained underexploited [38,39] in spite of its potentials.\
    \ Spectral\nattributes from visual (VIS) and near infrared (NIR), or even short-wave\
    \ infrared (SWIR)\nregions are inadequate for capturing polysaccharides components\
    \ such as cellulose and\nleaf surface properties including waxes and hairs, which\
    \ are mainly reﬂected on the TIR\ndomain [40]. This fact suggests that the UAV\
    \ thermal could be a complement to spectral\nsensing and thus deliver more accurate\
    \ phenotype estimations. Only [41] showed the\neffectiveness of a combination\
    \ between thermal and multispectral features in predicting\nnitrogen concentration\
    \ and chlorophyll a (Chl a) content. In our study, the singularity of\nthermal\
    \ imaging and the fusion with spectral imaging will be processed both feature-wise\n\
    and image-wise under the framework of CNNs.\nLight detection and ranging (LiDAR)\
    \ is an active remote sensor that can rapidly and\nprecisely record 3D structural\
    \ characteristics of terrestrial vegetation in a formation of\nbackscattering\
    \ points (a.k.a. point clouds). Unlike optical remote sensing, airborne LiDAR\n\
    sensed information has less relation to photosynthetic scheme of crops, but is\
    \ able to\ndetail canopy closure patterns, canopy height, and leaf angle distribution\
    \ that affect the\nforming of crop traits. The low-altitude airborne sensor has\
    \ been successfully used in many\nagricultural applications, such as canopy height\
    \ [42], tree species classiﬁcation [43,44], land\nuse land cover [45], and crop\
    \ biomass-related traits such as above ground biomass [46,47].\nIt should be noted\
    \ that in addition to height-associated factors, LiDAR also offers point\nintensity,\
    \ which is a measure, collected for every point, of the return strength of the\
    \ laser\npulse that generated the point. It is based, in part, on the reﬂectivity\
    \ of the object struck\nby the laser pulse. Other descriptions for intensity include\
    \ return pulse amplitude and\nbackscattered intensity of reﬂection that is a function\
    \ of the near-infrared wavelength\nused in the LiDAR. Intensity is used as an\
    \ aid in feature detection and extraction, in lidar\npoint classiﬁcation, and\
    \ as a substitute for aerial imagery when none is available. The\ncontribution\
    \ of UAV LiDAR intensity to high-throughput plant breeding is unknown. This\n\
    study was conducted to provide insights about the potential of airborne LiDAR\
    \ sensing\ntowards crop monitoring.\nIt is ideal if those above-discussed data\
    \ sources become intermingled by some means\nof data fusion that then beneﬁt crop\
    \ estimations at a higher accuracy. Several recent studies\nproved this pathway\
    \ at a certain conﬁdence level in classifying forest tree species [48,49],\ndetecting\
    \ pine wilt disease [50], and estimating crops’ traits such as grain yield [5]\
    \ and\nseed compositions [51]. Among these works, very few exploited the full\
    \ potential of deep\nlearning and convolutional neural networks, in particular,\
    \ for aerial multisensory data\nfusion. Adding to the further side of a more accurate\
    \ multimodal fusion model, a multi-\ntask deep learning model consuming multiple\
    \ data modalities to predict multiple crop\nphenotypes simultaneously is strongly\
    \ desired to surpass and has not even existed in\nthe literature.\nTo ﬁll the\
    \ research gap presented above, the overarching objective of this research was\n\
    to explore the possibility of UAV remote sensing being instrumental for high-throughput\n\
    phenotyping in maize by deploying airborne multisensory data fusion with a single\n\
    multi-task deep learning model. To address it, we aimed to achieve the following\
    \ sub-\nobjectives: (1) developing a machine learning model for multisensory data\
    \ fusion of very\nhigh-resolution UAV borne hyperspectral images, thermal images,\
    \ and LiDAR point clouds\nto estimate a full suite of maize phenotypic traits,\
    \ (2) assembling an end-to-end multimodal\nfusion multi-task deep convolutional\
    \ neural network in a phenotyping regression context,\n(3) examining the individual\
    \ and fused contributions of each data multimodality to a\nrange of maize trait\
    \ predictions, and (4) evaluating the impact of data augmentation on the\nmultimodal\
    \ fusion multi-task deep learning regression to address a limited sample size\
    \ in\nremote sensing research.\nSensors 2023, 23, 1827\n4 of 38\n2. Materials\
    \ and Preprocessing\n2.1. Test Site and UAV Data Acquisition\nAn experimental\
    \ corn ﬁeld was set up between early May and late September in 2020\nat the Crop\
    \ Sciences Research and Education Center located near the University of Illinois\n\
    campus in Urbana, IL, USA (40.08 N, 88.22 W) (Figure 1a). The corn ﬁeld has a\
    \ north–south\ndimension of 93 m and 32.6 m in the east–west dimension. The experiment\
    \ was organized\nin three areas: north–south edges, east–west edges, and the center\
    \ ﬁeld. On the north and\nsouth edges, a block of 8 rows with 4 inside rows of\
    \ genotype ILO3 × ILP1 and 4 outside\nrows of commercial hybrids were grown as\
    \ a cross border. The east and west edges were\ngrown with 29 corn inbred genotypes\
    \ in single row plots. The main center ﬁeld, which was\na focal interest of this\
    \ study, was an experiment of a collection of 66 corn hybrid genotypes\nrepresenting\
    \ two populations, diversity and high-nitrogen response. The experimented soil\n\
    type was a Drummer silty clay loam with 6.5 pH that was equivalent to a source\
    \ of 60 kg\nnitrogen per hectare estimated by subsequent soil sampling and measures\
    \ of plant nitrogen\nrecovery. A primary treatment exposed maize blocks with either\
    \ no supplemental nitrogen\n(low N) or nitrogen fertilizer (high N) at a rate\
    \ of 225 kg/ha as granular ammonium sulfate\n(AMS) at the soil surface. The nitrogen\
    \ fertilization was randomized along north–south\nadjacent blocks at a 0.76 m\
    \ alley in early June 2020 when the corns reached a V3 growth\nstage. Maize was\
    \ grown in a split-plot design sized approximately 5.33 m in length and\n0.76\
    \ m in width, which is rounded to 4 m2 a plot. The ﬁeld was controlled from weed\n\
    by a pre-plant application of herbicide atrazine and metolachlor and by hand weeding,\n\
    as needed.\n2.2. Data Acquisition\n2.2.1. Field Data Collection\nA full suite\
    \ of phenotypic metrics of hybrid corns in the center part of the ﬁeld were\n\
    sampled from 369 single row plots at the R6 growing stage when corns had not yet\
    \ senesced\nand the kernel had been fully ﬁlled (Figure 1c). The in situ phenotyping\
    \ process began\nwith cutting ﬁve plants from each plot at the ground level. After\
    \ removing corn ears,\nthe fresh weight of stover comprising stalk, leaves, tassels,\
    \ and husks was recorded. The\nphenotyping crew used a Vermeer wood chipper to\
    \ shred the fresh stover, collected a\nsubsample of stover shreds, weighted it,\
    \ and put it into a tared cloth bag. The stover\nsamples were dried in an oven\
    \ at 65 ◦C for at least three days, and their dried weight was\nobtained for stover\
    \ biomass. A Will mill was used to grind further the sheds to 2 mm\nground powder.\
    \ A combustion analysis with a Fisons EA-1108 N elemental analyzer was\nperformed\
    \ on a 100 mg portion of the powder to estimate total nitrogen concentration.\
    \ The\ncorn ears were oven-dried to a dryness of below 10% moisture at 37 ◦C for\
    \ about one week,\nafter which, the kernels were shelled and weighed separately\
    \ from the cobs. The kernel\ncomposition and actual moisture content was immediately\
    \ measured with a near-infrared\n(NIR) spectroscopy Perten DA7200 analyzer (Perten\
    \ Instruments, Springﬁeld, IL, USA).\nThe actual moisture value was reported at\
    \ around 8% in ambient storage conditions and\nwas used to correct the grain yield\
    \ to a dry basis. A summarized description and calculated\nformula of each metric\
    \ can be found in Table 1.\n2.2.2. UAV Data Acquisition\nAn aerial data collection\
    \ campaign was conducted on 28 August 2020 over the study\nﬁeld to obtain a full\
    \ set of remote sensing data (Figure 1b). The data collection date\ncorresponded\
    \ to the R5 growing stage when corns had reached physiological maturity\nand the\
    \ kernel had been denting near their crowns. We deployed a swarm of the DJI\n\
    Matrice 600 (M600) Pro hexacopter (DJI Technology Co. Ltd., Shenzhen, China) carrying\n\
    various types of aerial sensors outlined in Table 2. The ﬁrst UAV platform was\
    \ integrated\nwith a Headwall Photonics Nano-Hyperspec sensor (Headwall Photonics\
    \ Inc., Fitchburg,\nMA, USA), FLIR Vue Pro R 640 (FLIR Systems, Wilsonville, OR,\
    \ USA) thermal sensor,\nand Applanix APX-15 (Applanix Corporation, OR, Canada)\
    \ global positioning system\nSensors 2023, 23, 1827\n5 of 38\n(GPS)/inertial measurement\
    \ unit (IMU). The stability of the three equipment elements was\nwarranted by\
    \ a DJI Ronin MX three-axis gimbal. The second platform was hard-attached\nwith\
    \ a Velodyne HDL-32 (Phoenix LiDAR Systems, Los Angeles, CA, USA) LiDAR sensor\n\
    and a Sony A7R II (Sony Corporation, Tokyo, Japan) RGB camera. It should be noted\
    \ that\nthe LiDAR sensor operates at a wavelength of 905 nm, categorized as the\
    \ class 1 laser\nthat is human-eye safe and sensitive to the same types of canopy\
    \ elements. The third\nplatform consisted of an ICI 8640 P-series (Infrared Cameras\
    \ Inc., Beaumont, TX, USA)\nthermal camera, Sony (Sony Corporation, Japan) RGB\
    \ RX10 camera, and a Micasense Altum\n(Micasense In., Seattle, WA, USA) multispectral\
    \ camera. A Gremsy T3 (Gremsy, HCMC,\nVietnam) gimbal was connected to the UAV\
    \ system to frame the ICI 8640 thermal (Infrared\nCameras Inc., Beaumont, TX,\
    \ USA) and RGB RX10 camera (Sony Corporation, Tokyo,\nJapan) and adjust movements\
    \ thereof, while the Micasense Altum was individually held\nby a custom payload\
    \ tray 3D-printed using ABS plastic ﬁlament. Speciﬁcations of sensors\nwill be\
    \ discussed in the section UAV data preprocessing. In addition, each M600 Pro\
    \ was\nequipped with a DJI 3A Pro Flight Controller (DJI Corporation, Shenzhen,\
    \ China), inertial\nmeasurement unit (IMU), and real-time kinematics (RTK) Global\
    \ Navigation Satellite\nSystem (GNSS) receivers, which offer a positional accuracy\
    \ of 2 to 3 cm as claimed by\nthe manufacturer.\nSensors 2023, 23, x FOR PEER\
    \ REVIEW \n5 of 41 \n \n \nFigure 1. Experiment site location (red dot) in Urbana,\
    \ Illinois, USA (a), UAV aerial hyperspectral, \nLiDAR, and thermal data collected\
    \ from the field (b), the ortho-mosaic of maize field and samples \nplots (red\
    \ polygons) (c), UAV hyperspectral, LiDAR height, LiDAR intensity, and thermal\
    \ data of \nrandom plots enlarged and visually projected (d). \n2.2. Data Acquisition\
    \ \n2.2.1. Field Data Collection \nA full suite of phenotypic metrics of hybrid\
    \ corns in the center part of the field were \nsampled from 369 single row plots\
    \ at the R6 growing stage when corns had not yet se-\ne\ned a d the ke\nel had\
    \ bee\nfully filled (Fi u e 1 ) The i\nitu\nhe oty i\no e\nFigure 1. Experiment\
    \ site location (red dot) in Urbana, Illinois, USA (a), UAV aerial hyperspectral,\n\
    LiDAR, and thermal data collected from the ﬁeld (b), the ortho-mosaic of maize\
    \ ﬁeld and samples\nplots (red polygons) (c), UAV hyperspectral, LiDAR height,\
    \ LiDAR intensity, and thermal data of\nrandom plots enlarged and visually projected\
    \ (d).\nSensors 2023, 23, 1827\n6 of 38\nTable 1. Descriptions of maize phenotypic\
    \ traits and their measurements.\nPhenotypic Traits\nUnit\nCalculation\nMeasuring\
    \ Description\nCob Biomass\nkg/ha\n[Cob Biomass (g/plant) × Standing\nPlants]/Plot\
    \ Size (hectare)\nAverage of ﬁve plants from the center of\nrow sampled at R6\
    \ growth stage.\nDry Grain Yield\nkg/ha\n[Dry Grain Biomass\n(g/plant) × Standing\
    \ Plants]/Plot\nSize (hectare)\nAverage of ﬁve corn ears from the center of\n\
    row sampled at R6 growth stage.\nNormalized moisture content of dry grain\nbiomass\
    \ was 15.5%.\nDry Stalk Biomass\nkg/ha\n[Stalk Biomass (g/plant) × Standing\n\
    Plants]/Plot Size (hectare)\nAverage of ﬁve plants from the center of\nrow cut\
    \ at ground level at R6 growth stage,\nweighed, shredded, subsample weighed\n\
    fresh and dry.\nHarvest Index\n/\nDry Grain Biomass (g/plant)/[Dry\nStalk Biomass\
    \ (g/plant) + Cob Biomass\n(g/plant) + Dry Grain\nBiomass (g/plant)]\n/\nGrain\
    \ Density\n/\n/\nMeasured with a near-infrared (NIR)\nspectroscopy Perten DA7200\
    \ analyzer\n(Perten Instruments, Springﬁeld, IL, USA)\non kernels sampled ﬁve\
    \ ears each plot.\nGrain Nitrogen Content\nkg/ha\n[Grain Protein (%)/6.25] × Dry\
    \ Grain\nBiomass (g/plant)]/Plot Size (hectare)\n/\nGrain Nitrogen\nUtilization\
    \ Efﬁciency\n(Grain NutE)\n/\nDry Grain Biomass (g/plant)/[Stalk N\n(%) × Stalk\
    \ Biomass (g/plant) + [Grain\nProtein (%)/6.25] × Dry Grain\nBiomass (g/plant)]\n\
    Describe how the plant uses the nitrogen it\nacquires to produce grain. It is\
    \ the ratio\nbetween dry grain biomass over the total\nNitrogen content of the\
    \ plant.\nPlant Nitrogen Content\nkg/ha\n[Stalk N (%) × Stalk Biomass\n(g/plant)\
    \ + [Grain Protein\n(%)/6.25] × Dry Grain Biomass\n(g/plant)]/Plot Size (hectare)\n\
    The amount of nitrogen of all standing\nplants normalized to their plot area.\
    \ The\ntotal amount of nitrogen of each plant was\nthe addition of the amount\
    \ in stalk and in\ngrain. The stalk nitrogen content was\nmeasured by a combustion\
    \ analysis of dry\nstover. Grain protein percent was\ndetermined by a lab-based\
    \ NIR\nspectrometer, which is converted to grain\nnitrogen content at the Jones\
    \ factor of 6.25\nin maize [52].\nPrior to ﬂights, a calibration tarp with a known\
    \ dimension at 3 × 3 m and three reﬂec-\ntive panels at 56, 30, and 11% reﬂectance\
    \ was placed within the data collection window\nunder a UAV ﬂight swath to be\
    \ imaged for correcting geometry and reﬂectance of the\nhyperspectral cubes. Identiﬁable\
    \ ground control points (GCPs) painted with black and\nwhite were distributed\
    \ evenly at the ﬁeld’s corners and alleys to act as reference points\nfor georeferencing\
    \ multiple datasets. All UAV in-ﬂight deployments were programmed\nwith pre-set\
    \ parameters based on the collecting speciﬁcations of a designated sensor to\n\
    automatically operate and collect remotely sensed data without the pilot’s involvement.\n\
    The ﬂight mission for the hyperspectral system was planned by using UgCS v.4.1\
    \ (SPH\nEngineering SIA, Latvia) software. In exchange for 3 cm Ground Sampling\
    \ Distance (GSD)\n(i.e., the projected pixel size on the ground) and with the\
    \ sensor lens settings, the pho-\ntogrammetry tool of the software determined\
    \ the average ﬂight attitude at 48 m. We set a\n40% side overlap between ﬂight\
    \ swaths for ortho-mosaicking multiple cubes. Owing to\nthe line scanning mechanism,\
    \ it is not necessary to have high forward overlap; instead, we\ntook the minimum\
    \ value of 1% and set the frame per cubes at 10,000, which is equivalent\nto the\
    \ maximum 640 × 10,000 pixels for each raw cube. In addition, we created an area\
    \ of\ninterest (AOI) that determines the ﬁeld data collection window, and whenever\
    \ the UAV\nSensors 2023, 23, 1827\n7 of 38\nenters the AOI, the GPS recognizes\
    \ and triggers the sensor to start capturing data or to stop\nif exiting the AOI.\
    \ The optimal ﬂight speed was determined at 3 m/s, which is an output of\nthe\
    \ illumination intensity, the integration time, the focal length of the sensor\
    \ lens, and the\npreset ﬂight attitude. A dark reference of 1000 frames per 1\
    \ cube, which will be used for\nradiometric calibration, was snapped with the\
    \ lens cap covering on the sensor.\nTable 2. A summary of UAV platforms with multiple\
    \ aerial remote sensors and properties.\nUAV Platform\nData Format\nSensor\nStabilizer\n\
    Recorded\nInformation\nSpectral\nProperties\nGSD\nDJI M600 Pro\nhexacopter (DJI\n\
    Corporation, Shenzhen,\nChina),\nHyperspectral\nImagery\nHeadwall\nNano-Hyperspec\n\
    DJI Ronin\nMX gimbal\n270 VNIR spectral\nbands\n400–1000 nm with\nFWHM of 6 nm\n\
    3 cm\nFLIR Thermal\nImagery\nFLIR Vue Pro R 640\n/\n/\n/\nGPS/IMU\nApplanix APX-15\n\
    DJI M600 Pro\nhexacopter (DJI\nCorporation, Shenzhen,\nChina),\nLiDAR point\n\
    cloud\nVelodyne HDL-32\nHard mount\nLiDAR point cloud\nand attributes\n/\n900\
    \ pts/m2\nRGB Imagery\nSony A7R II\nBlue, Green, Red\nbands\n2.4 cm\nDJI M600\
    \ Pro\nhexacopter (DJI\nCorporation, Shenzhen,\nChina),\nICI Thermal\nImagery\n\
    ICI 8640 P-series\nGremsy T3\ngimbal\n1 thermal IR band\n7–14 µm\n8 cm\nRGB Imagery\n\
    Sony RX10\nMultispectral\nImagery\nMicasense Altum\nHard mount\n5 spectral bands:\n\
    Blue, Green, Red,\nRed-edge, NIR\nSimilarly, the ﬂight mission designed for the\
    \ hyperspectral system above was reused\nfor the ICI thermal and multispectral\
    \ data collection system except upscaling the forward\noverlap to 40% between\
    \ captures. For the LiDAR data collection mission, we designed\nthe ﬂight paths\
    \ by using Phoenix LiDAR FlightPlanner (Phoenix LiDAR Systems, Los\nAngeles, CA,\
    \ USA) software which is proprietarily developed by the vendor. This is\namong\
    \ only a few kinds of ﬂight planning software that can harmoniously accommodate\n\
    ﬂight parameters for both photogrammetry (image-based) and LiDAR speciﬁcations.\
    \ The\nvendor reported the locational accuracy (RMSE) of a point at a range of\
    \ 3.5–5.5 cm within\na 50 m ﬂying height, and the point density, which was of\
    \ our most interest, was jointly\ninﬂuenced by ﬂight altitude, forward velocity\
    \ (speed), and lateral (side) overlap. The\nLiDAR point density was estimated\
    \ at 1600 points/m2 on average from the software after\nconsidering a LiDAR ﬁeld\
    \ of view at 90◦, a ﬂying altitude at 50 m, a speed of 3 m/s, and\na side overlap\
    \ of 70%. It is recommended for mapping mission types to design the last\nﬂight\
    \ path perpendicular to the along-track ﬂight paths, thereby enhancing point cloud\n\
    co-registration [53]. The GSD estimate of the RGB camera paired with the LiDAR\
    \ sensor\nwas less than 1 cm. During point cloud colorization processing later,\
    \ the point clouds can\nbe overlaid with the RGB color information from this camera.\n\
    2.3. Post-Collection Hyperspectral Imagery Processing\nThe Headwall Nano-Hyperspec\
    \ is a push-broom scanner that collects reﬂectance\nthrough an image split perpendicular\
    \ to the ﬂight direction. The image split is a linear\narray of pixels (640 spatial\
    \ pixels for the sensor) with each pixel containing full spectral\nwavelengths,\
    \ and the number of image slits increases as the UAV motion occurs. The sensor\n\
    has a 12 mm lens and a horizontal ﬁeld of view (FOV) of 21.1◦, which gathers radiometric\n\
    data in the 400–1000 nm visual and near-infrared (VNIR) region across 270 bands\
    \ at a\nsampling interval of 2.2 nm and a FWHM of 6 nm. In addition to three GNSS\
    \ antennas\nmounted on the upper of the UAV, there is one antenna for high-performance\
    \ GPS/IMU\nAPX-15 paired with the hyperspectral camera to monitor roll, pitch,\
    \ and yaw motions.\nThe GPS/IMU was run through a post-processing kinematics (PPK)\
    \ program to improve\nthe data quality. The accuracy of the inertial measurement\
    \ unit (IMU) data from the\nSensors 2023, 23, 1827\n8 of 38\nPPK is ± 0.025◦ in\
    \ roll and pitch, and 0.08◦ in yaw or heading. The total payload of the\nM600\
    \ was 3.65 kg, which constrains the ﬂight time to approximately 20 min.\nPush-broom\
    \ sensors are known with hardware-induced spatial noise across-track and\nalong-track.\
    \ The across-track noise or vertical striping is small differences among 640 pixels\n\
    in an individual linear array caused by collecting data simultaneously and independently.\n\
    The along-track noise is differences among linear arrays in each hyperspectral\
    \ cube due to\ntemporal variations when collecting sequentially [54]. Spatial\
    \ pixel measurements should\nbe homogeneous for the same feature, and temporal\
    \ variations between the ﬁrst array and\nlast array should be minimal to affect\
    \ the signal signiﬁcantly. To minimize the noise, we\nconducted the ﬂights at\
    \ noon under minimal cloud conditions. Further, [55] indicated that\nif the UAV\
    \ ﬂies within 30 min, the variation increment is insigniﬁcant at less than 2%\
    \ across\nspatial pixels and spectral bands.\nA series of steps were carried out\
    \ to preprocess hyperspectral cubes, including radio-\nmetric calibration, ortho-rectiﬁcation\
    \ (i.e., geometric correction), and ortho-mosaicking.\nDue to the proximity of\
    \ UAV data collection to the ground, the atmospheric correction\nwas assumed to\
    \ be far less inﬂuenced by atmospheric effects [56]. Assisted by Headwall\nSpectralView\
    \ software, radiometric calibration was ﬁrst performed to convert raw data in\n\
    12-bit digital number (DN) format to radiance values. The cube of 1000 frames\
    \ as a dark\nreference collected prior to the ﬂight was subtracted from the raw\
    \ DN imagery, since they\nare a residual current, or more precisely, randomly\
    \ generated electrons, ﬂowing through\nthe photon-sensible lens [55]. We then\
    \ converted the at-sensor radiance to the at-surface\nreﬂectance that is the standard\
    \ unit for a comparison of different datasets collected from\nmultiple areas and\
    \ multiple times. An empirical line method (ELM) was performed on\nall imagery\
    \ cubes based on the near-Lambertian tarp with three known reﬂectance values\n\
    of 56, 32, and 11%. The orthorectiﬁcation step is required to geometrically correct\
    \ data\ncubes by using their frame indices and associated GPS timestamps obtained\
    \ from the high-\nperformance Applanix APX-15 system. The GPS time is used to\
    \ look up and interpolate to\nthe system motions (roll, pitch, yaw, latitude,\
    \ longitude, ﬂight altitude, and digital elevation\nmodel (DEM)) at the time the\
    \ frame was taken. The motion offsets were parameterized via\nPostPac UAV 8.2.1\
    \ (Applanix Corporation, Richmond Hill, ON, Canada) to generate the\npost-processed\
    \ smoothed best estimate of trajectory (SBET) ﬁle. SpectralView software used\n\
    this enhanced GPS to ortho-rectify each pixel frame by replacing them where they\
    \ were at\nthe time of the ﬂight (the accuracy depends on the enhanced GPS claimed\
    \ by Applanix).\nAll the radiometrically and geometrically corrected data cubes\
    \ were stitched together to\ncreate one single orthoimage of the ﬁeld, which is\
    \ known as ortho-mosaicking.\n2.4. Post-Collection LiDAR Point Cloud Processing\n\
    During LiDAR ﬁeld scanning, the Real-Time Kinematic (RTK) operation mode was\n\
    initiated relying on an on-board GPS receiver (tracking x, y, z point coordinates)\
    \ and IMU\n(tracking the sensor motions and orientation). A linear quadratic estimation\
    \ (LQE) operates\nto integrate GPS and IMU signals to produce a statistically\
    \ optimal estimate of the sensor’s\nposition at any point in time. This mode allows\
    \ generation of the LiDAR data in the point-\ncloud format and visualization of\
    \ them in real-time in Phoenix SpatialExplorer software.\nWith RTK, the data can\
    \ be derived in centimeter-level precision, and thus, Post-Processing\nKinematic\
    \ (PPK) is necessary to enhance the data precision. We deployed the PPK on a\n\
    web-based LiDARMill version 2 (Phoenix LiDAR Systems, Los Angeles, CA, USA), which\n\
    consists of a sequence of 2 pipelines: NavLab and Spatial Fuser. The NavLab pipeline\n\
    requires input data from the onboard GNSS/IMU and the base station to correct\
    \ the ﬂight\ntrajectory in forward and reverse directions several times using\
    \ Loosely Coupled (LC) and\nTightly Coupled (TC) solutions [57]. LC is a naïve\
    \ computation to fuse GNSS-derived\nposition and velocity with IMU, which is infeasible\
    \ with fewer than four satellites’ signals\nor in blocked areas, while TC overcomes\
    \ the shortfall of interrupted signals by directly\nusing GNSS static raw observations\
    \ [58]. The Spatial Fuser pipeline in LiDARMill fuses\nthe corrected NavLab trajectory\
    \ data with the raw LiDAR data to generate a point cloud\nSensors 2023, 23, 1827\n\
    9 of 38\nand further colorize the point cloud if the RGB images are inputted.\
    \ LiDARMill eventually\ndelivers a classiﬁed (ground/non-ground) point cloud and\
    \ its attributes such as intensity,\nRGB values, number of returns in the LAS\
    \ (LASer) format.\nThe LAS ﬁle was then used to generate raster data representing\
    \ canopy height and\nintensity. Canopy height is a normalized surface that is\
    \ the difference between the digital\nsurface model (DSM) and digital terrain\
    \ model (DTM). We created the DSM raster by\nﬁrst ﬁltering the points to only\
    \ non-ground and removing outlier points that lie alone in\nlow-density regions\
    \ whose the nearest neighbors are too far away. We voxelized the point\ncloud\
    \ to a bin of small cells (voxels) at a size of 3 cm that was consistent with\
    \ the pixel size\nof the hyperspectral image. The DSM was formed from the highest\
    \ elevation cells, inside\nof which we selected the maximum point. The creation\
    \ of DTM raster began with, ﬁrst,\nﬁltering ground points and then, voxelizing\
    \ of the point cloud. The triangular irregular\nnetworks (TIN) method was performed\
    \ to interpolate voids found on the earth’s surface.\nThe construction of the\
    \ canopy intensity raster was similar to making the DSM except for\nthe data type\
    \ as point intensity.\nTo assure the conﬁdence that our remote sensing data correctly\
    \ captured the crop’s\nfeatures, we correlated the remote sensing data, especially\
    \ LiDAR data, to the actual data\nthat were manually measured by our ﬁeld management\
    \ team. The ground truth height\nrecorded the average of every three plants in\
    \ the middle of each plot in the R6 stage.\nThe remotely sensed LiDAR height was\
    \ extracted from 90 percentile of the plot height\nto preclude aerial dust at\
    \ the very top of the plot canopy. The correlation between the\ntwo showed a very\
    \ strong and statistically signiﬁcant degree at R2 = 0.9, p < 0.001 (Figure 2).\n\
    Sensors 2023, 23, x FOR PEER REVIEW \n10 of 41 \n \nfiltering ground points and\
    \ then, voxelizing of the point cloud. The triangular irregular \nnetworks (TIN)\
    \ method was performed to interpolate voids found on the earth’s surface. \nThe\
    \ construction of the canopy intensity raster was similar to making the DSM except\
    \ for \nthe data type as point intensity. \nTo assure the confidence that our\
    \ remote sensing data correctly captured the crop’s \nfeatures, we correlated\
    \ the remote sensing data, especially LiDAR data, to the actual data \nthat were\
    \ manually measured by our field management team. The ground truth height \nrecorded\
    \ the average of every three plants in the middle of each plot in the R6 stage.\
    \ The \nremotely sensed LiDAR height was extracted from 90 percentile of the plot\
    \ height to pre-\nclude aerial dust at the very top of the plot canopy. The correlation\
    \ between the two \nshowed a very strong and statistically significant degree\
    \ at R2 = 0.9, p < 0.001 (Figure 2). \n \nFigure 2. Scatter plot between LiDAR-derived\
    \ (remotely captured in R5 stage) and ground truth \nheight (measured in R6 stage)\
    \ in meters. \n2.5. Post-Collection Thermal Imagery Processing \nAn ICI thermal\
    \ sensor recorded the data in DN values in a JPG imagery format, and \ntherefore,\
    \ radiometric calibration for the thermal imagery is required to convert the at-\n\
    sensor data type to a physical meaning data type at the surface–canopy temperature\
    \ in \nCelsius degrees. This process was done in a batch through IR-Flash (Infrared\
    \ Cameras \nInc., Beaumont, TX, USA) software with an internally installed factory\
    \ calibration file. Us-\ners are further allowed to optionally adjust environmental\
    \ conditions, thermal emissivity, \ntransmission, and ambient temperature. The\
    \ converting software outputted thermal im-\nages in 32-bit TIFF format with geo-tags.\
    \ The batch of radiometrically corrected images \nwas loaded in a photogrammetric\
    \ software Pix4D mapper (Pix4D SA, Prilly, Switzerland) \nfor ortho-rectifying\
    \ and mosaicking to create a single image of a captured field. A Pix4D \nmapper\
    \ utilizes a suite of photogrammetry and computer vision techniques for extracting\
    \ \nFigure 2. Scatter plot between LiDAR-derived (remotely captured in R5 stage)\
    \ and ground truth\nheight (measured in R6 stage) in meters.\n2.5. Post-Collection\
    \ Thermal Imagery Processing\nAn ICI thermal sensor recorded the data in DN values\
    \ in a JPG imagery format, and\ntherefore, radiometric calibration for the thermal\
    \ imagery is required to convert the at-\nsensor data type to a physical meaning\
    \ data type at the surface–canopy temperature in\nCelsius degrees. This process\
    \ was done in a batch through IR-Flash (Infrared Cameras\nInc., Beaumont, TX,\
    \ USA) software with an internally installed factory calibration ﬁle.\nUsers are\
    \ further allowed to optionally adjust environmental conditions, thermal emissivity,\n\
    transmission, and ambient temperature. The converting software outputted thermal\
    \ images\nin 32-bit TIFF format with geo-tags. The batch of radiometrically corrected\
    \ images was\nSensors 2023, 23, 1827\n10 of 38\nloaded in a photogrammetric software\
    \ Pix4D mapper (Pix4D SA, Prilly, Switzerland) for\northo-rectifying and mosaicking\
    \ to create a single image of a captured ﬁeld. A Pix4D\nmapper utilizes a suite\
    \ of photogrammetry and computer vision techniques for extracting\nimage key points\
    \ in each image, matching the key points, stitching images together, and\nblending\
    \ overlapping areas in the stitched ortho-mosaic.\n2.6. Image Co-Registration\n\
    Image co-registration is the process of geometrically aligning two or more images\n\
    to integrate or fuse corresponding pixels that represent the same objects or locations\
    \ on\nthe ground [59]. Although all hyperspectral images, canopy height and intensity\
    \ images,\nand thermal images were correctly georeferenced at the same projection,\
    \ they were still\nmisaligned, typically at a centimeter level with the UAV scale.\
    \ Co-registration occurred by\nobtaining the geometric relationship between a\
    \ base image and warped images through a\nnumber of tie points. The UAV hyperspectral\
    \ ortho-image served as the base image, and the\nLIDAR canopy height, intensity,\
    \ and thermal images were warped to be closely re-aligned.\nA minimum of 20 tie\
    \ points was manually selected, including GCP reference panels and\ndistinct features\
    \ that were evenly distributed across the ﬁeld. The tie point selection was\n\
    edited on the geometric correction module of ENVI 5.5 software (Harris Geospatial,\
    \ Boulder,\nCO, USA). The software module then required users to choose warping\
    \ and resampling\nvalues. A second-order polynomial was used for the warping transformation,\
    \ whereas\ncubic convolution was used for resampling warped images, especially\
    \ thermal images from\n10 cm to 3 cm.\n3. Methods\nThe methodology was graphically\
    \ illustrated in Figure 3, an overall workﬂow. The\nmethods could be partitioned\
    \ into four main areas: data collection, post-collection process-\ning, feature\
    \ engineering, and modeling. The UAV aerial data collection and post-collection\n\
    data processing were fully discussed in the Section 2 above. The next sections\
    \ described\nfeature engineering, both manually and automatically, and modeling\
    \ methods. The pre-\ndicted results were geo-located on a spatial map for a residual\
    \ randomness testing (i.e.,\nspatial autocorrelation) and eventually, for visualization.\n\
    3.1. Ground-Truth Data Exploration\nTable 3 summarizes descriptive statistics\
    \ of maize phenotypes harvested at the end of\nthe growing season. It is discernible\
    \ that all three phenotypes, dry grain yield (kg/ha), grain\nnitrogen content\
    \ (kg/ha), and plant nitrogen content (kg/ha), rendered a high coefﬁcient\nof\
    \ variation (46, 53.9, and 51,7%). Rather than a bell-shaped curve, the data distribution\n\
    exhibited a bimodal curve, which is a direct response to the nitrogen experiment.\
    \ The low\nvalues were sampled from low N plots (i.e., no supplemental nitrogen),\
    \ and high values\ncorresponded to high N plots (i.e., 225 kg/ha nitrogen fertilizer\
    \ treatment). The other ﬁve\ntarget variables had various levels of extreme instances,\
    \ which skewed their distribution\nand would possibly negate predicting performance.\
    \ This matter becomes the most obvious\nin a left-skewed distribution of grain\
    \ density, in addition to a very narrow data range\n(1.02–1.35 units) and small\
    \ coefﬁcient of variation (3%).\nIt is necessary to understand the correlation\
    \ degree for each pair of phenotypes\ncollected (scatter plot matrices in Appendix\
    \ A). The correlation pairs of dry stalk biomass,\ncob biomass, dry grain yield,\
    \ grain nitrogen content, and plant nitrogen content presented\na linear positive\
    \ relationship, interpreted as, for example, the higher the stalk biomass is,\n\
    the more likely the cob has a higher biomass. It becomes plainly visible between\
    \ grain\nnitrogen and plant nitrogen content. Data points of harvest index and\
    \ grain density were\nfound to be dispersed when cross-plotted with other phenotypes.\
    \ It is worth mentioning\nthat grain nitrogen utilization efﬁciency persistently\
    \ segmented its data into two high- and\nlow-value clusters, which correspond\
    \ to high and low nitrogen treatment. Given such\nnegated features of the phenotype\
    \ variables, it is advisable to implement transforming\nSensors 2023, 23, 1827\n\
    11 of 38\nand standardizing them prior to a formal process. It is also important\
    \ to project all values\ninto a comparable scale for later multi-task deep learning\
    \ and the loss function of the\nmodel, which was discussed in detail in the data\
    \ transformation section below. Figure 4\ntherefore showed the standardized form\
    \ of ground-truth data of eight maize phenotypes\nafter rescaling the distribution\
    \ values.\nSensors 2023, 23, x FOR PEER REVIEW \n12 of 41 \n \n \nFigure 3. Overall\
    \ workflow implemented in the study. There were four main stages: UAV data col-\n\
    lection, remotely sensed data processing, feature engineering, and modelling.\
    \ Because of a small \nsample size and the existence of extreme values in the\
    \ data, the feature engineering and modelling \nphases were iterated 5 times by\
    \ randomly shuffling the datasets (bootstrapping). The best predicted \nphenotypes\
    \ were eventually plotted on spatial maps as final deliverables. \n \n \nFigure\
    \ 3. Overall workﬂow implemented in the study. There were four main stages: UAV\
    \ data\ncollection, remotely sensed data processing, feature engineering, and\
    \ modelling. Because of a small\nsample size and the existence of extreme values\
    \ in the data, the feature engineering and modelling\nphases were iterated 5 times\
    \ by randomly shufﬂing the datasets (bootstrapping). The best predicted\nphenotypes\
    \ were eventually plotted on spatial maps as ﬁnal deliverables.\nSensors 2023,\
    \ 23, 1827\n12 of 38\nTable 3. A summary of descriptive statistics of each maize\
    \ phenotype collected at the end of grow-\ning season.\nPhenotypes\nCount\nMean\n\
    Std *\ncv (%)\n**\nMin\n25%\n50%\n75%\nMax\nDry Stalk Biomass (kg/ha)\n369\n6510.82\n\
    2153.74\n33.1\n1477\n5033\n6315\n7756\n22,035\nCob Biomass (kg/ha)\n369\n1470.71\n\
    498.90\n33.9\n415\n1091\n1432\n1822\n3853\nDry Grain Yield (kg/ha)\n369\n7176.92\n\
    3300.98\n46\n425\n4282\n7038\n9848\n17,450\nHarvest Index\n369\n0.45\n0.09\n19.4\n\
    0.03\n0.40\n0.46\n0.52\n0.75\nGrain NutE\n369\n55.92\n11.10\n19.9\n5\n50\n57\n\
    63\n77\nGrain N (kg/ha)\n369\n91.70\n49.48\n53.9\n9\n44\n90\n136\n218\nTotal Plant\
    \ N (kg/ha)\n369\n135.88\n70.18\n51.7\n26\n68\n141\n198\n314\nGrain Density\n\
    369\n1.27\n0.038\n3\n1.02\n1.25\n1.27\n1.3\n1.35\n* standard of deviation, **\
    \ coefﬁcient of variation.\nSensors 2023, 23, x FOR PEER REVIEW \n \n \nFigure\
    \ 4. Standardized form of ground-truth data of eight maize phenotypes collected\
    \ du\ngrowing season. The means of all phenotypes after rescaling the distribution\
    \ values wer\nstandard deviations were 1. Dry grain yield, grain nitrogen content,\
    \ total plant nitrogen \ndisplayed a binomial data distribution with no extreme\
    \ instances. The other distributions\nnormal but contained extreme values in their\
    \ datasets. \n3.2. Plot-Level Chip Image Segmentation and Feature Scaling \nThe\
    \ remotely sensed data in this study consisted of hyperspectral imagery, \npoint\
    \ cloud, and thermal imagery. Plot level chip images of each data type extracte\n\
    the ortho-mosaic raster of the whole field using vector data of plot boundaries\
    \ (Figu\nIt would matter if the plot images contain not only areas of interest\
    \ (AOIs) maize\nbut also a various degree of confounding objects such as soil,\
    \ residuals, shadow, e\nnon-AOIs were also affirmed by visually crosschecking\
    \ in all data modalities: for ex\nthe non-AOI shadow pixels valued at 0 in the\
    \ LiDAR height and intensity or therm\nimages. To alleviate the matter and elevate\
    \ prediction accuracy, it is suggested to se\nout these confounding pixels. As\
    \ the segmentation task runs on the entire spect\nsimple method such as threshold\
    \ proves to be insufficient to detect AOIs, especially\npixels and shaded canopy\
    \ region, as studied by [60]. Instead, a unsupervised k-m\nclustering [61] was\
    \ chosen as it receives the most popularity in both academia and\ntry because\
    \ of an easy implementation and a high computational efficiency even wi\ndimensional\
    \ data [62]. Only one drawback of the k-means clustering refers to the ar\nuser\
    \ input for an a priori k number of clusters. This was not our case when each\
    \ p\nsentially had two classes, vegetation and non-vegetation (Figure 5). The\
    \ multimod\nalso existed in different measurement scales: reflectance spectra\
    \ were in the range o\n1, LiDAR-derived canopy height in meters, LiDAR intensity\
    \ unitless, and thermal\nsius degrees. After removing non-AOIs pixels, standardization\
    \ (a.k.a. feature scali\ncrucial step to ensure all features on the same scale\
    \ before feeding them into m\nFigure 4. Standardized form of ground-truth data\
    \ of eight maize phenotypes collected during the\ngrowing season. The means of\
    \ all phenotypes after rescaling the distribution values were 0 and\nstandard\
    \ deviations were 1. Dry grain yield, grain nitrogen content, total plant nitrogen\
    \ content\ndisplayed a binomial data distribution with no extreme instances. The\
    \ other distributions looked\nnormal but contained extreme values in their datasets.\n\
    3.2. Plot-Level Chip Image Segmentation and Feature Scaling\nThe remotely sensed\
    \ data in this study consisted of hyperspectral imagery, LiDAR\npoint cloud, and\
    \ thermal imagery. Plot level chip images of each data type extracted from\nthe\
    \ ortho-mosaic raster of the whole ﬁeld using vector data of plot boundaries (Figure\
    \ 1d).\nIt would matter if the plot images contain not only areas of interest\
    \ (AOIs) maize pixels\nbut also a various degree of confounding objects such as\
    \ soil, residuals, shadow, etc. The\nnon-AOIs were also afﬁrmed by visually crosschecking\
    \ in all data modalities: for example,\nthe non-AOI shadow pixels valued at 0\
    \ in the LiDAR height and intensity or thermal chip\nimages. To alleviate the\
    \ matter and elevate prediction accuracy, it is suggested to segment\nout these\
    \ confounding pixels. As the segmentation task runs on the entire spectrum, a\n\
    simple method such as threshold proves to be insufﬁcient to detect AOIs, especially\
    \ mixed\npixels and shaded canopy region, as studied by [60]. Instead, a unsupervised\
    \ k-means++\nclustering [61] was chosen as it receives the most popularity in\
    \ both academia and industry\nbecause of an easy implementation and a high computational\
    \ efﬁciency even with high\ndimensional data [62]. Only one drawback of the k-means\
    \ clustering refers to the arbitrary\nuser input for an a priori k number of clusters.\
    \ This was not our case when each plot\nessentially had two classes, vegetation\
    \ and non-vegetation (Figure 5). The multimodal\ndata also existed in different\
    \ measurement scales: reﬂectance spectra were in the range of 0\nand 1, LiDAR-derived\
    \ canopy height in meters, LiDAR intensity unitless, and thermal in\nCelsius degrees.\
    \ After removing non-AOIs pixels, standardization (a.k.a. feature scaling)\nis\
    \ a crucial step to ensure all features on the same scale before feeding them\
    \ into machine\nlearning algorithms.\nSensors 2023, 23, 1827\n13 of 38\nrs 2023,\
    \ 23, x FOR PEER REVIEW \n \nFigure 5. The segmentation results of 5 random plots\
    \ using k-means clus\netation pixels (Areas of interest, AOIs) from non-vegetation\
    \ pixels (s\nshadow, etc.). The segmentation was done on weed-free sample plots\
    \ in\nimage of the entire field that possibly contained weeds. The hyperspectr\n\
    tion were again verified and affirmed with LiDAR height and intensity, \n3.3.\
    \ An Extended Normalized Difference Spectral Indices (NDSIs) as a \nNormalized\
    \ difference spectral indices (NDSI) involves statis\nspectral bands in hyperspectral\
    \ data that could be best sensitive\nFigure 5. The segmentation results of 5 random\
    \ plots using k-means clustering to extract pure\nvegetation pixels (Areas of\
    \ interest, AOIs) from non-vegetation pixels (soil, crop residuals, heavy\nshadow,\
    \ etc.). The segmentation was done on weed-free sample plots instead of the ortho-mosaic\n\
    image of the entire ﬁeld that possibly contained weeds. The hyperspectral proﬁles\
    \ of pure vegetation\nwere again veriﬁed and afﬁrmed with LiDAR height and intensity,\
    \ and thermal.\nSensors 2023, 23, 1827\n14 of 38\n3.3. An Extended Normalized\
    \ Difference Spectral Indices (NDSIs) as a Simple Fusion\nNormalized difference\
    \ spectral indices (NDSI) involves statistically normalizing two\nspectral bands\
    \ in hyperspectral data that could be best sensitive to plant’s phenotypes. Re-\n\
    cent studies demonstrated that a NDSI correlation map is useful for identifying\
    \ the optimal\nnormalized indices to predict biochemical, biophysical, and structural\
    \ properties [63,64].\nWe extended the conventional NDSI and applied it to other\
    \ types of our data including\nLiDAR canopy height band, LiDAR intensity band,\
    \ and thermal band. The extended NDSI\nserved as a naïve fusion method to combine\
    \ and normalize not only two spectral bands\nbut also each spectral band with\
    \ LiDAR height, intensity, and thermal data by following\nEquation (1):\nNDSI\
    \ (i, j) = Mi − Mj\nMi + Mj\n(1)\nwhere Mi, Mj are plot-wise mean values of raster\
    \ band i and raster band j. All possible\ncombinations (i, j) of 269 available\
    \ spectral bands, 1 canopy height band, 1 canopy intensity\nband, and 1 thermal\
    \ band were used for NDSI calculations for each phenotypic trait.\n3.4. Feature\
    \ Engineering and Traditional Machine Learning\nFeature engineering is an essential\
    \ step that applies hardcoded transformations to raw\ndata, which makes the data\
    \ more amenable to machine learning algorithms. It especially\nmatters if the\
    \ input data are high-dimensional, such as hyperspectral images, wherein the\n\
    number of features is substantially higher than the number of samples. If not\
    \ being properly\nengineered, unrelated pixels in the spatial domain and multicollinear\
    \ spectral bands in the\nspectral domain could possibly add more noise and diminish\
    \ the model generalization.\nEstablishing vegetation indices (VIs) from high-dimensional\
    \ data is a common technique\nin vegetation remote sensing research. A set of\
    \ 34 VIs representing maize phenotypic\nexpressions (biochemical, biophysical,\
    \ physiological, etc.) was extracted from plot-wise\nhyperspectral cubes, conventionally\
    \ used in previous studies [18,25]. A similar index\nformation on LiDAR and thermal\
    \ data [65,66] generated 30 VIs from the height statistics,\n30 VIs from the intensity\
    \ characteristics, and 1 thermal-derived VI. Table 4 enumerated all\nVIs notions\
    \ and their meanings.\nA machine learning pipeline was then constructed with two\
    \ regressors: Support\nvector machine for regression (SVR) [67] and Random forest\
    \ regression (RFR) [68]. Both\nare versatile and widely accepted methods in the\
    \ vegetation remote-sensing community.\nSVR gives a ﬂexibility to deﬁne how much\
    \ error is acceptable through ﬁnding an optimal\nerror tube (the separating hyperplane\
    \ or decision boundary in the classiﬁcation problem).\nTo achieve a small error,\
    \ we experimented on the SVR with a hyperparameter GridSearch\nlibrary. The ﬁrst\
    \ hyperparameter C controls the width of the margin, and when C is large,\nthe\
    \ SVR tends to be overﬁtting, while when C is small, the SVR tends to be underﬁtting.\n\
    Second, the kernel function, which creates nonlinear combinations of the original\
    \ features\nto project them onto a higher-dimensional space via a mapping function,\
    \ where the new\ntransformed data become linear. γ is the third hyperparameter\
    \ to be optimized, controlling\nthe inﬂuence of the similarity distance. The smaller\
    \ the values of γ, the larger the similarity\nradius, whereas with high values\
    \ of γ, the data examples must be closer to be affected.\nRFR is ensemble learning\
    \ that combines several base learners (i.e., decision trees)\ninto a meta-learner\
    \ in order to achieve a better performance than each individual model\nalone.\
    \ A similar hyperparameter tuning was done in a 5-fold inner cross-validation,\
    \ as [69]\nrecommended. The ﬁrst hyperparameter was the number of decision trees\
    \ (k). With fewer k,\nthe model variance tends to increase and the meta-learner\
    \ is prone to overﬁtting, whereas\nthe model bias remains constant. The next hyperparameters\
    \ were the maximum depth\nthat a tree can grow and the minimum number of samples\
    \ at the leaf nodes. RFR also\nmeasures the feature importance of a feature to\
    \ the predicting power toward the target (i.e.,\nmaize traits). It is also known\
    \ as the mean decrease impurity (MDI) and will be used to\nassess the importance\
    \ of each data modality towards the model’s predictive power later in\nSection\
    \ 5 discussion.\nSensors 2023, 23, 1827\n15 of 38\nTable 4. Selected vegetation\
    \ indices (VIs) across data modalities.\nNo.\nVegetation Index\nAcronym\nEquation\n\
    References\nHyperspectral-derived metrics\n1\nAnthocyanin (Gitelson)\nAntGitelson\n\
    AntGitelson = (1/R550 − 1/R700 ) × R780\n[70]\n2\nChlorophyll Index\nCI\nCI =\
    \ (R750 − R 705 )/(R750 + R705)\n[71]\n3\nOptimized Soil-Adjusted\nVegetation\
    \ Index\nOSAVI\nOSAVI = (1 + 0.16) × (R800 –R 670 )/(R800 + R670 + 0.16)\n[72]\n\
    4\nRed Green Index\nRGI\nRGI = R690/R550\n[73]\n5\nStructure Intensive Pigment\
    \ Index\nSIPI\nSIPI = (R800 − R 450 )/(R800 + R650)\n[74]\n6\nTransformed Chlorophyll\
    \ Absorption in\nReﬂectance Index\nTCARI\nTCARI = 3 × ((R700 − R 670)− 0.2 × R700−\
    \ R 550)×\n(R700/R670))\n[75]\n7\nNitrogen Reﬂectance Index (NRI)\nNRI\nNRI =\
    \ (R570 − R670)/(R570 + R670)\n[76]\n8\nModiﬁed Chlorophyll Absorption in\nReﬂectance\
    \ Index\nmCARI\nmCARI = 1.2 × (2.5 × (R761 − R 651 )–1.3 × (R761 − R 581 ))\n\
    [77]\n9\nPhotochemical Reﬂectance Index\nPRI\nPRI = (R531 –R 570 )/(R531 + R570)\n\
    [78]\n10\nRatio Analysis of reﬂectance Spectral\nChlorophyll a\nRARSa\nRARSa =\
    \ R675/R700\n[79]\n11\nRatio Analysis of reﬂectance Spectral\nChlorophyll b\n\
    RARSb\nRARSb = R675/(R700 × R650)\n[79]\n12\nRatio Analysis of reﬂectance Spectral\n\
    RARSc\nRARSc = R760/R500\n[79]\n13\nPigment speciﬁc simple ratio\nPSSR\nPSSR =\
    \ R800/R680\n[80]\n14\nPlant Senescence Reﬂectance Index\nPSRI\nPSRI = (R660 −\
    \ R 510)/R760\n[81]\n15\nNormalized chlorophyll pigment\nratio index\nNCPI\nNCPI\
    \ = (R670 − R 450)/(R670 + R450)\n[74]\n16\nPlant Pigment ratio\nPPR\nPPR = (R550\
    \ − R 450 )/(R550 + R450)\n[82]\n17\nNormalized Difference Vegetation Index\n\
    NDVI\nNDVI = (R860 − R 670 )/(R860 + R670)\n[83]\n18\nGreenness Index\nGI\nGI\
    \ = R554/R677\n[73]\n19\nGreen NDVI\nGNDVI\nGNDVI = (R750 − R 540 + R570)/(R750\
    \ + R540 − R 570 )\n[84]\n20\nSimple Ratio\nSR\nSR = R900/R680\n[85]\n21\nRed-edgeNDVI\n\
    RNDVI\nRNDVI = (R750 − R705)/(R750 + R705)\n[86]\n22\nModiﬁed Triangular Vegetation\
    \ Index\nMTVI\nMTVI = 1.2 × (1.2 × (R800 – R550) − 2.5 × (R670 − R 550 ))\n[77]\n\
    23\nTriangular Vegetation Index\nTVI\nTVI = 0.5 × (120 × (R761 − R 581 ) – 200(R651\
    \ − R 581 ))\n[87]\n24\nFluorescence Ratio Index 1\nFRI1\nFRI1 = R690/R630\n[88]\n\
    25\nFluorescence Ratio Index 2\nFRI2\nFRI2 = R750/R800\n[89]\n26\nFluorescence\
    \ Ratio Index 3\nFRI3\nFRI3 = R690/R600\n[90]\n27\nFluorescence Ratio Index 4\n\
    FRI4\nFRI4 = R740/R800\n[90]\n28\nFluorescence Curvature Index\nFCI\nFCI = R2683/(R675\
    \ × R691)\n[88]\n29\nModiﬁed Red Edge Simple Ratio Index\nmRESR\nmRESR = (R750\
    \ − R 445 )/(R705 + R445)\n[91]\n30\nNormalized Phaeophytinization Index\nNPQI\n\
    NPQI = (R415 − R 435 )/(R415 + R435)\n[92]\n31\nRed-Edge Vegetation Stress Index\
    \ 1\nRVS1\nRVS1 =((R651 + R750)/2) − R733\n[93]\n32\nRed-Edge Vegetation Stress\
    \ Index 2\nRVS2\nRVS2 =((R651 + R750)/2) − R751\n[93]\n33\nWater Index\nWI\nWI\
    \ = R900/R970\n[94]\n34\nWater Stress and Canopy Temperature\nWSCT\nWSCT = (R970\
    \ − R 850 )/(R970 + R850)\n[95]\nLiDAR-derived canopy height metrics\n1\nMaximum\
    \ of canopy height\nHmax\n2\nMinimum of canopy height\nHmin\n3\nMean of canopy\
    \ height\nHmean\n4\nMode of canopy height\nHmode\n5\nStandard deviation of canopy\
    \ height\nHsd\n6\nCoefﬁcient of variation of canopy height\nHcv\n7\nHmad\nHmad\
    \ = 1.4826 × median (|height − Hmedian|)\n8\nHaad\nHaad = mean (|height − Hmean|)\n\
    9–20\nPercentile of canopy height\nHper\nH10, H20, H30, H40, H50, H60, H70, H80,\
    \ H90, H95, H98,\nH99\n21\nThe Interquartile Range (iqr) of\ncanopy height\nHiqr\n\
    Hiqr = H75 − H25\n22\nSkewness of canopy height\nHskn\n23\nKurtosis of canopy\
    \ height\nHkurt\n24–28\nCanopy return density of height\nHcrd\nThe proportion\
    \ of points above the height quantiles (10th,\n30th, 50th, 70th, and 90th) to\
    \ the total number of points:\nHd10, Hd30, Hd50, Hd70, Hd90\n29\nCanopy relief\
    \ ratio of height\nHcrr\n(Hmean-Hmin)/(Hmax−Hmin)\n30\nHcg\nThe ratio of canopy\
    \ returns of height and ground returns of\nheight\nSensors 2023, 23, 1827\n16\
    \ of 38\nTable 4. Cont.\nNo.\nVegetation Index\nAcronym\nEquation\nReferences\n\
    LiDAR-derived canopy intensity metrics\n1\nMaximum of canopy intensity\nImax\n\
    2\nMinimum of canopy intensity\nImin\n3\nMean of canopy intensity\nImean\n4\n\
    Mode of canopy intensity\nImode\n5\nStandard deviation of canopy intensity\nIsd\n\
    6\nCoefﬁcient of variation of\ncanopy intensity\nIcv\n7\nImad\nImad = 1.4826 ×\
    \ median (|intensity − Imedian|)\n8\nIaad\nIaad = mean (|intensity−Imean|)\n9–20\n\
    Percentile of canopy intensity\nIper\nI10, I20, I30, I40, I50, I60, I70, I80,\
    \ I90, I95, I98, I99\n21\nThe Interquartile Range (iqr) of canopy\nintensity\n\
    Iiqr\nIiqr = I75−I25\n22\nSkewness of canopy intensity\nIskn\n23\nKurtosis of\
    \ canopy intensity\nIkurt\n24–28\nCanopy return density of intensity\nIcrd\nThe\
    \ proportion of points above the intensity quantiles (10th,\n30th, 50th, 70th,\
    \ and 90th) to the total number of points:\nId10, Id30, Id50, Id70, Id90\n29\n\
    Canopy relief ratio of intensity\nIcrr\n(Imean–Imin)/(Imax−Imin)\n30\nIcg\nThe\
    \ ratio of canopy returns of intensity and ground returns\nof intensity\nThermal-derived\
    \ metric\n1\nNormalized relative canopy\ntemperature index\nTir\nTir = (Ti–Tmin)/(Ti–Tmax)\n\
    [96]\n3.5. Multimodal Fusion and Multi-Task Deep Learning\n3.5.1. Deep Learning\
    \ and the Need for Data Augmentation\nDeep learning prediction performance could\
    \ generally achieve its potential when\ntraining on a sufﬁciently large dataset.\
    \ This is valid partly due to its nature and capability\nfor searching relevant\
    \ and salient features in the training data without any need for manual\nfeature\
    \ engineering, which can only be done on the availability of a large amount of\
    \ data.\nMany have shown that data augmentation improves the generalization performance\
    \ and\nreduces overﬁtting on a small dataset [97,98]. We attained more data samples\
    \ by iterating\nrandom cropping on each plot boundary via a restricted ‘ﬁeld of\
    \ view’ (FOV). The FOV\nwas the actual plot size at 5.33 m × 0.76 m, equivalently,\
    \ 176 pixels in length and 25 pixels\nin width at 3 cm GSD, whereas the plot boundaries\
    \ were fairly larger because the mature\nplants traverse over the allies. The\
    \ spatial 176 × 25 window randomly and iteratively\nsliced 20 times on each plot\
    \ to cover enough every corner of a plot but with not too\nmuch overlapping among\
    \ cropped images in the dataset. For each cropping iteration, a\nrandom number\
    \ generator was set the same across hyperspectral, LiDAR-derived, thermal\nimages\
    \ to ensure the sliding window was cropping the same region within each plot.\
    \ The\naugmentation procedure was solely applied on the training set.\n3.5.2.\
    \ Convolutional Neural Network for Imagery Representation Learning\nThe convolutional\
    \ neural network (CNN) has gained huge popularity in the applica-\ntion of deep\
    \ learning in the last decade due to its efﬁcient and robust performance toward\n\
    learning salient representations or relevant features of the imagery data format\
    \ [62]. This\nstudy orchestrates a stack of 3D convolutional layers that can automate\
    \ extracting jointly\nspatial and spectral representations of a 3D hyperspectral\
    \ cube, relying on a hypothesis\nthat crops exhibit their properties in both spatial\
    \ and spectral domains. Particularly, we\nassembled four 3D convolutional layers\
    \ equipped with a kernel size of 3 × 3 × 7 (3 × 3 in\nspatial dimension and 7\
    \ in spectral dimension) and stride of 1 pixel at a time. The number\nof convolutional\
    \ ﬁlters started with 8 at the ﬁrst layer, raising a power of 2 to 16, 32, and\n\
    64 ﬁlters. Kernel weights of each convolutional layer were initialized by sampling\
    \ from\nGlorot uniform distribution [99]. The kernel bias was typically conﬁgured\
    \ to initialize with\n0. Rectiﬁed linear unit (ReLU) [100] served as activation\
    \ functions due to its widespread\nSensors 2023, 23, 1827\n17 of 38\npopularity\
    \ in tackling the vanishing gradient problem (gradient terms are close to or equal\n\
    0) as a network adds more layers and becomes deeper. Reducing the tensor volume\
    \ by\nsubsampling layers is a recommended practice. We experimented on two pooling\
    \ forms,\nmax pooling and mean pooling, and found that 3D max pooling layers with\
    \ a size of\n2 × 2 × 6 max pooling worked better because features tend to encode\
    \ the spatial presence\nof some pattern over the different tiles of the feature\
    \ map, and obtaining the maximal\npresence of different features became more informative.\
    \ The second advantage of max\npooling refers to a local invariance that means\
    \ small changes in a local neighborhood do not\nchange the result of max pooling.\
    \ Similar to the 3D convolutional for volumetric learning\nin hyperspectral imagery,\
    \ a 2D convolutional version was constructed in two separate\nnetwork streams\
    \ for LiDAR-derived and thermal imagery learning.\n3.5.3. Multimodal Fusion and\
    \ Multi-Task Prediction Block\nEach of the three convolutional network streams\
    \ ended up with 64 feature maps of\ndifferent tensor shapes that were then funneled\
    \ to global average pooling layers. This\nhelped reduce trainable parameters and\
    \ simplify the model capacity, thereby minimizing\nthe risk of overﬁtting. At\
    \ the fusion node, we fused each of the 64 features together. Lastly,\na prediction\
    \ block consisted of fully connected layers carrying 32 neuron units and ReLU\n\
    activation to map convolutional features to the output targets. Inserted between\
    \ fully\nconnected layers was a dropout regularization technique [101] that involves\
    \ the removal\nof randomly selected neurons from the network’s hidden layers in\
    \ each round of training.\nBy a random dropout, the model does not memorize or\
    \ become over-reliant on certain\nfeatures of the data to reduce overﬁtting and\
    \ generate a good prediction. The whole block\nof multimodal fusion and multi-task\
    \ deep learning was graphically illustrated in Figure 6.\nSensors 2023, 23, x\
    \ FOR PEER REVIEW \n20 of 41 \n \n \nFigure 6. Multimodal fusion and multi-task\
    \ deep learning scheme simultaneously predicting all of \nthe maize phenotypes.\
    \ Each stream of convolutional layers automatically processed a different data\
    \ \nmodality prior to being fused and fetched to multi-task regressors. \n3.5.4.\
    \ Loss Function \nSelection of the proper loss function is critical for training\
    \ an accurate model as it \nmeasures how well the model did at predicting the\
    \ outcome. Two common loss functions \nfor a regression modeling are Mean Squared\
    \ Error (MSE) and Mean Absolute Error \n(MAE), and each has certain properties.\
    \ If outliers are present, the quadratic function of \nMSE weights more largely\
    \ on anomalous errors from outliers and significantly magnifies \nh\nMAE h\nb\
    \ h\ni\nMSE\ni\nli\nh\nb\nl\nl\nh\nFigure 6. Multimodal fusion and multi-task\
    \ deep learning scheme simultaneously predicting all of\nthe maize phenotypes.\
    \ Each stream of convolutional layers automatically processed a different data\n\
    modality prior to being fused and fetched to multi-task regressors.\nSensors 2023,\
    \ 23, 1827\n18 of 38\n3.5.4. Loss Function\nSelection of the proper loss function\
    \ is critical for training an accurate model as it\nmeasures how well the model\
    \ did at predicting the outcome. Two common loss functions\nfor a regression modeling\
    \ are Mean Squared Error (MSE) and Mean Absolute Error (MAE),\nand each has certain\
    \ properties. If outliers are present, the quadratic function of MSE\nweights\
    \ more largely on anomalous errors from outliers and signiﬁcantly magniﬁes the\n\
    errors. MAE, however, behaves opposite to MSE, as it applies the absolute value\
    \ to the\ndifference between the predictions and ground truth, thereby averaging\
    \ it out across the\nentire dataset. This property makes MAE ineffective in caring\
    \ about outlier predictions\nas the huge errors coming from the outliers end up\
    \ being weighted the exact same as\nlower errors. The fact is that extreme cases\
    \ usually occur in plant phenotyping expressions\ndue to mutual interactions between\
    \ internal and external variables such as genotypes and\nenvironmental conditions.\
    \ Huber loss function [102] offers the best of both worlds by\nharmonizing MSE\
    \ and MAE using the following piecewise Equation (2):\nLδ(y, f (x)) =\n(\n1\n\
    2(y − f (x))2\nf or |y − f (x)| ≤ δ\nδ|y − f (x)| − 1\n2δ2\nf or |y − f (x)| >\
    \ δ\n(2)\nwhere y is the actual (true) value of the target data point, f (x) is\
    \ the predicted value of the\ndata point. δ deﬁnes a threshold where the Huber\
    \ loss function transitions from quadratic\nto linear. δ is a hyperparameter to\
    \ be tuned in which the Huber loss approaches MAE when\nδ is asymptotic to 0 and\
    \ MSE when δ becomes larger.\nThe deep learning architecture was implemented using\
    \ TensorFlow (tensorﬂow.org)\nand Keras (keras.io) Python libraries. The splitting\
    \ ratio of 60–20–20% was used in training,\nvalidation, and test samples. To assist\
    \ the model to ﬁnd the global minima and achieve\nthe lowest loss, we adopted\
    \ several widely recommended techniques such as the Adam\n(adaptive moment estimation)\
    \ optimizer with a scheduled learning rate (started at 0.001\nand exponentially\
    \ decreased every 5 epochs).\n3.6. Model Evaluation and Performance Metrics\n\
    To evaluate the performance across prediction models, the coefﬁcients of determination\n\
    (R2), root mean square error (RMSE), and mean absolute errors (MAE) were computed\
    \ and\ncontrasted, which can be expressed as follows:\nR2 = 1 − ∑n\ni=1( ˆyi −\
    \ yi)2\n∑n\ni=1(yi − yi)2\nRMSE =\ns\n∑n\ni=1( ˆyi − yi)2\nn − 1\nMAE = 1\nn\n\
    n\n∑\nj=1\n|yi − ˆyi|\nwhere ˆyi and yi are the measured and the predicted values,\
    \ respectively, y is the mean of\nthe measured value, and n is the total number\
    \ of samples in the testing set.\nFurther, a spatial variability of the prediction\
    \ results was statistically evaluated, partic-\nularly by Global Moran’s I (GMI).\
    \ The GMI measures the spatial autocorrelation contingent\non the maize plot locations\
    \ and the model’s regression errors over the study area [5,103].\nThe errors were\
    \ residuals between the measured and predicted phenotypes of each maize\nplot.\
    \ The GMI’s null hypothesis states that the phenotypes’ predicted errors are complete\n\
    spatial randomness or randomly distributed.\nSensors 2023, 23, 1827\n19 of 38\n\
    4. Results\n4.1. Results of a Naïve Fusion NDSI Method\nThe extended NDSI method\
    \ was a fast and naïve approach for fusing all 269 spectral\nbands, LiDAR canopy\
    \ height and intensity, and thermal data. Figure 7 discloses the\ncorrelation\
    \ degree between the established NDSIs and maize phenotypic traits through\nR2\
    \ heatmaps with a same scale of 0–1 (dark blue to dark red). The ﬁgure glimpsed\
    \ that\nNDSI heatmaps formed solely from spectra (Figure 7a) had regions having\
    \ a higher degree\nof correlations than those in the heatmaps formed from spectra,\
    \ thermal, LiDAR height,\nand intensity (Figure 7b). All eight highest R2 (lime\
    \ cross sign) were found in Figure 5a’s\nheatmaps. Equivalently, dry stalk biomass\
    \ received the highest R2 = 35.7% when correlated\nwith the NDSI[534, 868]. Cob\
    \ biomass correlated with the NDSI[715, 855] at R2 = 38.4%. The\nR2 of dry grain\
    \ yield reached up to 74.6% by the NDSI[715, 917]. Harvest index peaked at\nR2\
    \ = 45.1% by the NDSI[504, 700]. The correlation of grain nitrogen utilization\
    \ efﬁciency\n(Grain NutE) with the NDSI[749, 866] made the highest R2 = 27.1%.\
    \ The R2 for grain nitrogen\ncontent equaled 79.6% at the NDSI[751, 769], and\
    \ the total plant nitrogen content R2 was 80%\nby the NDSI[751, 778]. The R2 of\
    \ grain density achieved 27.6% as the highest value at the\nNDSI[751, 789].\n\
    The common theme running through all heatmaps was the contributory signiﬁcance\n\
    of green bands (530–560 nm) and red-edge bands (700–720 nm) in the spectra. Those\
    \ bands\npairing with NIR bands (750–1000 nm) to create NDSIs correlated best\
    \ with dry grain yield,\ngrain nitrogen content, and total plant nitrogen content.\
    \ It is noted that the simple data\nfusion NDSI by combining and normalizing spectral\
    \ bands, LiDAR canopy height, LiDAR\ncanopy intensity, and thermal features correlated\
    \ with eight maize phenotypic traits at a\nminimal degree. This clues a necessity\
    \ for a complication of extracting explanatory features\nfrom each data source\
    \ and fusing them effectively.\n4.2. Machine Learning and Deep Learning Performance\
    \ on Multisensory Fused Data\nFigure 8 demonstrates the mean and standard deviation\
    \ of coefﬁcient R2 performed\non a 5-time bootstrap using four different regressors\
    \ and a variety of multi-sensory data\nfusions. The following common points can\
    \ be noticed from Figure 6, and more details\n(R2, RMSE, MAE of both train and\
    \ test sets) can be accessed in Appendix B. First, the\nprediction success highly\
    \ varied from phenotype to phenotype, possibly dividing into\na limited (R2 <\
    \ 0.5), moderate (0.5 < R2 < 0.8), and high level (R2 > 0.8). Predictions of\n\
    dry grain yield (R2 = 0.78), grain nitrogen content (R2 = 0.85), and total plant\
    \ nitrogen\ncontent (R2 = 0.85) were reported as the highest degree of success.\
    \ Although different\nstudies employed different methods and data available, this\
    \ study’s results were somewhat\nbetter to recent studies of maize yield prediction\
    \ (R2 varied 0.3–0.8 depending on growing\nstages) [104–106], total Nitrogen content\
    \ (R2 = 0.76) [107].\nPredicting dry stalk biomass (R2 = 0.53), cob biomass (R2\
    \ = 0.47), harvest index\n(R2 = 0.56), and grain NutE (R2 = 0.49) came in second\
    \ at a moderate success. There is no\ndirect comparison, but recent studies of\
    \ maize above ground biomass (AGB) predicted\nmore accurately than our results\
    \ at R2 = 0.86 [108] and R2 = 0.87 [109]. Prediction results\nof grain density\
    \ (R2 = 0.34) showed a limited success. The varying prediction success can\nalso\
    \ be seen through the error bars of each model; for example, models predicting\
    \ dry stalk\nbiomass (Figure 8c) had smaller deviations when shufﬂing the dataset,\
    \ while the deviation\nof model predicting grain density was considerably wider.\
    \ This proved that grain NutE\nand grain density contained extreme values in the\
    \ dataset, and when being shufﬂed and\nrandomly split, the train sets and test\
    \ sets did not warrant an equivalence. The substantial\ndisparity between the\
    \ MAE and RMSE (Appendix B) also suggested the existence of a very\nwide and inconsistent\
    \ data range of the two maize traits. This matter could be typically\ndissolved\
    \ by collecting more samples, which is recommended to future studies.\nSensors\
    \ 2023, 23, 1827\n20 of 38\nSensors 2023, 23, x FOR PEER REVIEW \n22 of 41 \n\
    \ \n \nFigure 7. Extended NDSI correlation heatmaps. Each NDSI was established\
    \ by combining and nor-\nmalizing two 269 singular spectral bands (398 nm–1000\
    \ nm) (a), and fused features from spectra, \nLiDAR canopy height, LiDAR canopy\
    \ intensity, and canopy thermal data (b). The lime-colored cross \nsigns indicated\
    \ the best R2 for each maize trait. In detail, Dry Stalk Biomass achieved a maximum\
    \ R2 \n= 0.357 with NDSI[534, 868]. Cob Biomass optimally gained R2 = 0.384 at\
    \ NDSI[715, 855]. Dry Grain Yield \nhad the highest R2 = 0.746 at NDSI[715, 917].\
    \ Harvest Index received the highest R2 = 0.451 at NDSI[504, \n700]. Grain Nitrogen\
    \ Utilization Efficiency (NutE) attained R2 = 0.271 at NDSI[749, 866]. Grain Nitrogen\
    \ \nContent (Grain N) reached R2 = 0.796 at NDSI[751, 769]. Total Plant Nitrogen\
    \ Content (Total Plant N) \nhad the peak of R2 = 0.80 at NDSI[751, 778]. Grain\
    \ Density ran into R2 = 0.276 at NDSI[751, 789]. \n4.2. Machine Learning and Deep\
    \ Learning Performance on Multisensory Fused Data \nFigure 8 demonstrates the\
    \ mean and standard deviation of coefficient R2 performed \non a 5-time bootstrap\
    \ using four different regressors and a variety of multi-sensory data \nfusions.\
    \ The following common points can be noticed from Figure 6, and more details (R2,\
    \ \nRMSE, MAE of both train and test sets) can be accessed in Appendix B. First,\
    \ the predic-\ntion success highly varied from phenotype to phenotype, possibly\
    \ dividing into a limited \n(R2 < 0.5), moderate (0.5 < R2 < 0.8), and high level\
    \ (R2 > 0.8). Predictions of dry grain yield \n(R2 = 0.78), grain nitrogen content\
    \ (R2 = 0.85), and total plant nitrogen content (R2 = 0.85) \nwere reported as\
    \ the highest degree of success. Although different studies employed dif-\nferent\
    \ methods and data available, this study’s results were somewhat better to recent\
    \ \nFigure 7. Extended NDSI correlation heatmaps. Each NDSI was established by\
    \ combining and\nnormalizing two 269 singular spectral bands (398 nm–1000 nm)\
    \ (a), and fused features from spectra,\nLiDAR canopy height, LiDAR canopy intensity,\
    \ and canopy thermal data (b). The lime-colored cross\nsigns indicated the best\
    \ R2 for each maize trait. In detail, Dry Stalk Biomass achieved a maximum\nR2\
    \ = 0.357 with NDSI[534, 868]. Cob Biomass optimally gained R2 = 0.384 at NDSI[715,\
    \ 855]. Dry Grain\nYield had the highest R2 = 0.746 at NDSI[715, 917]. Harvest\
    \ Index received the highest R2 = 0.451 at\nNDSI[504, 700]. Grain Nitrogen Utilization\
    \ Efﬁciency (NutE) attained R2 = 0.271 at NDSI[749, 866]. Grain\nNitrogen Content\
    \ (Grain N) reached R2 = 0.796 at NDSI[751, 769]. Total Plant Nitrogen Content\
    \ (Total\nPlant N) had the peak of R2 = 0.80 at NDSI[751, 778]. Grain Density\
    \ ran into R2 = 0.276 at NDSI[751, 789].\nSecond, the prediction success highly\
    \ varied from data type to data type. Models\ndeploying with data types of either\
    \ hyperspectral singularity or hyperspectral fusion can\nproduce a sustainably\
    \ better estimate for maize phenotypes in comparison to models using\nthermal\
    \ and LiDAR canopy intensity. On the other hand, models without the inclusion\
    \ of\nhyperspectral data, which are thermal, LiDAR intensity, and LiDAR height,\
    \ presented a\nlimited success in predicting all maize traits. The variation of\
    \ those models on shufﬂed\ndata being represented by the error bars in Figure\
    \ 6a–h was also larger than the variation\nof models with the presence of hyperspectral\
    \ features.\nSensors 2023, 23, 1827\n21 of 38\nstudies of maize yield prediction\
    \ (R2 varied 0.3–0.8 depending on growing stages) [104–\n106], total Nitrogen\
    \ content (R2 = 0.76) [107]. \n \nFigure 8. Prediction performance of eight maize\
    \ phenotypes (a–h) represented by R2 across different\nfeature types and regressors.\
    \ Feature types included thermal = canopy thermal, inten = LiDAR can-\nopy intensity,\
    \ dsm = LiDAR canopy height, hyper = hyperspectral images. Feature fusions included\n\
    hyper + dsm = a fusion of hyperspectral and LiDAR canopy height, hyper + dsm +\
    \ inten = a fusion\nof hyperspectral, LiDAR canopy height, and LiDAR canopy intensity,\
    \ and hyper + dsm + thermal =\na fusion of hyperspectral, LiDAR canopy height,\
    \ LiDAR canopy intensity, and thermal. \nPredicting dry stalk biomass (R2 = 0.53),\
    \ cob biomass (R2 = 0.47), harvest index (R2 =\n0.56), and grain NutE (R2 = 0.49)\
    \ came in second at a moderate success. There is no direct\ncomparison, but recent\
    \ studies of maize above ground biomass (AGB) predicted more\naccurately than\
    \ our results at R2 = 0.86 [108] and R2 = 0.87 [109]. Prediction results of grain\n\
    density (R2 = 0.34) showed a limited success. The varying prediction success can\
    \ also be\nFigure 8. Prediction performance of eight maize phenotypes (a–h) represented\
    \ by R2 across differ-\nent feature types and regressors. Feature types included\
    \ thermal = canopy thermal, inten = LiDAR\ncanopy intensity, dsm = LiDAR canopy\
    \ height, hyper = hyperspectral images. Feature fusions included\nhyper + dsm\
    \ = a fusion of hyperspectral and LiDAR canopy height, hyper + dsm + inten = a\
    \ fusion\nof hyperspectral, LiDAR canopy height, and LiDAR canopy intensity, and\
    \ hyper + dsm + thermal = a\nfusion of hyperspectral, LiDAR canopy height, LiDAR\
    \ canopy intensity, and thermal.\nThird, machine learning and multi-task deep\
    \ learning methods performed the re-\ngression comparably with a little disparity\
    \ of R2, MAE, and RMSE. The RFR regressor\noccasionally proved to be a slightly\
    \ more accurate estimation (higher R2, Figure 8d,e), but\nthe multi-task learning\
    \ method occurred as more stable by a narrower deviation (error bars,\nSensors\
    \ 2023, 23, 1827\n22 of 38\nFigure 8f,g). Noticeably, if considering models with\
    \ only thermal and LiDAR intensity for\nall eight maize traits (Figure 8a–h),\
    \ traditional machine learning can do the task minimally,\nwhile a much higher\
    \ prediction accuracy was observed in deep learning regressors. This\nreﬂected\
    \ that the SVR and RFR heavily relied on handcrafted features in which only a\
    \ single\nthermal index was manually extracted and deployed, while the deep learning\
    \ regressors\nperhaps grasped many informative features from the raw thermal images.\n\
    4.3. Spatial Distribution Maps of Predicted Results\nFigure 9 projected the predicted\
    \ values of dry grain yield and total plant nitrogen\ncontent at a plot level\
    \ on spatial maps. The two maps were results from the multi-task\nlearning model\
    \ performing the prediction on the fusion of hyperspectral and LiDAR\ncanopy height\
    \ imagery data. It is necessary to notice that only these two results were\ngraphically\
    \ displayed on the maps due to a page limit of an article, and interested readers\n\
    are encouraged to contact the authors and request a complete copy of the digital\
    \ maps.\ny p\ng\ny\n(\ng\n,\ng\nmulti-task learning method occurred as more stable\
    \ by a narrower devi\nFigure 8f,g). Noticeably, if considering models with only\
    \ thermal and LiD\nall eight maize traits (Figure 8a–h), traditional machine learning\
    \ can d\nmally, while a much higher prediction accuracy was observed in deep lea\n\
    This reflected that the SVR and RFR heavily relied on handcrafted featur\na single\
    \ thermal index was manually extracted and deployed, while th\nregressors perhaps\
    \ grasped many informative features from the raw the\n4.3. Spatial Distribution\
    \ Maps of Predicted Results \nFigure 9 projected the predicted values of dry grain\
    \ yield and tot\ncontent at a plot level on spatial maps. The two maps were results\
    \ fro\nlearning model performing the prediction on the fusion of hyperspectral\n\
    opy height imagery data. It is necessary to notice that only these two res\nically\
    \ displayed on the maps due to a page limit of an article, and inter\nencouraged\
    \ to contact the authors and request a complete copy of the di\n \nFigure 9. Spatial\
    \ distribution maps of predicted dry grain yield (kg/ha) and to\ncontent (kg/ha)\
    \ which were the results of the multimodal and multi-task deep le\nFrom the maps,\
    \ it is visually detected that the distribution of predi\ntered into plot blocks\
    \ of low and high values of both grain yield and plant\ntraits. These low- and\
    \ high-value blocks were consistently aligned with \ntated with the nitrogen experiment.\
    \ It means that low-value plot blocks\nthe control blocks without nitrogen addition,\
    \ whereas high-value plot bl\nthe experiment blocks with 225 (kg/ha) nitrogen\
    \ fertilizer per each plot. F\nels returned the predicted values spanning within\
    \ a narrower range o\nFigure 9. Spatial distribution maps of predicted dry grain\
    \ yield (kg/ha) and total plant nitrogen\ncontent (kg/ha) which were the results\
    \ of the multimodal and multi-task deep learning model.\nFrom the maps, it is\
    \ visually detected that the distribution of predicted values clustered\ninto\
    \ plot blocks of low and high values of both grain yield and plant nitrogen content\
    \ traits.\nThese low- and high-value blocks were consistently aligned with the\
    \ blocks annotated\nwith the nitrogen experiment. It means that low-value plot\
    \ blocks corresponded to the\ncontrol blocks without nitrogen addition, whereas\
    \ high-value plot blocks paired with the\nexperiment blocks with 225 (kg/ha) nitrogen\
    \ fertilizer per each plot. Further, the models\nreturned the predicted values\
    \ spanning within a narrower range of 2700 to 12,000 (kg/ha)\nfor grain yield\
    \ and 41 to 240 (kg/ha) for plant nitrogen content, compared to the actual\nvalues\
    \ of 425 to 17,450 (kg/ha) and 26 to 314 (kg/ha), respectively (Table 3, statistics\
    \ of\nground truth data). This matter occurred due to the possibility of the Huber\
    \ effect set as\nthe loss of the models. Too extreme values in both ends were\
    \ constrained by the Huber\nloss, as such, making the regression errors smaller.\n\
    5. Discussion\n5.1. Remote Sensing Data for High-Throughput Maize Phenotyping\n\
    The results in the preceding section promoted a varying success of maize phenotype\n\
    predictions with a use of multi-sensors UAV at very low altitude and high resolution.\
    \ It\nSensors 2023, 23, 1827\n23 of 38\nis strongly desired to have an innovative\
    \ tool for high-throughput maize phenotyping\nby estimating all traits at a time;\
    \ however, the fact is that each crop phenotype has its\nown mechanism that dissimilarly\
    \ reacts to the nitrogen experiment, not to mention the\nenvironmental conditions\
    \ at different times in a day [110,111]. Eight maize plant trait in\nthis study\
    \ belongs to different categories: biophysical (stalk biomass, cob biomass, harvest\n\
    index) biochemical (plant nitrogen content), and maize grain traits (grain yield,\
    \ grain\nnitrogen content, and grain density).\nThe signiﬁcance of optical remote\
    \ sensing, especially the NIR spectra (750–1000 nm)\nin all eight maize estimations\
    \ was demonstrated. The wavelengths most important for\npredictions are detailed\
    \ in an ascending order: 749 nm, 751 nm, 769 nm, 778 nm, 789 nm,\n855 nm, 866\
    \ nm, 869 nm, and 917 nm (Figure 7). More concretely, the mean decrease\nimpurity\
    \ (MDI) feature importance analysis (Figure 10) unfolded the two most critical\
    \ VIs\nfor predictions, namely, Fluorescence Ratio Index 2 and 4 FRI2[750, 800]\
    \ and FRI4[740, 800] in\nthe form of NIR wavelengths (740 nm, 750 nm, and 800\
    \ nm). It has become obvious that the\nnear-end NIR simulates molecular motion\
    \ of compounds residing in internal leaves that\ninduces a strong reﬂection of\
    \ downwelling radiance [112–116]. The NIR spectral pattern is\nalso primarily\
    \ inﬂuenced by internal scatterings in the cell structure and air-ﬁlled space,\n\
    and the interaction of irradiance with starch, oil, proteins, and further compartments\
    \ inside\nthe cells, cell walls and membranes [117,118]. It is worth mentioning\
    \ that the water content\nof leaves and plants can be characterized in the far-end\
    \ wavelengths (greater than 900 nm)\nin the NIR region [119]. Being able to remotely\
    \ sense the above-stated compositions from\nthe aerial level greatly beneﬁted\
    \ estimating not only canopy and plant phenotypes but\nalso grain-related traits\
    \ since the elements are transported from stems to the corn ears and\neventually\
    \ ended up at kernels.\nStalk biomass was found to be estimated the most accurately\
    \ by fusing data modalities.\nIn addition to the valuable contribution of the\
    \ NIR spectra discussed above, canopy and\nplant structural descriptions derived\
    \ from LiDAR data such as canopy height and intensity\nserved as critical sources\
    \ of information to predict stalk biomass. More obviously, Figure 8a\ninformed\
    \ the dominance of the crop’s structural features when 8 out of 10 of the most\n\
    important features were descents from the LiDAR canopy height. Many studies came\n\
    to a consensus that vegetation spectra alone are insufﬁcient to access a high\
    \ accuracy of\nstalk biomass prediction due to a vegetation saturation effect.\
    \ [112,120] explained that this\neffect likely occurs when the crops canopy outstretches\
    \ and reaches to a 100% cover in\nthe mid-vegetative period, while most crops’\
    \ biomass continues accumulating under the\ncover. In this context, the absorbed\
    \ and reﬂected amount of downwelling radiation remains\nvirtually unchanged, but\
    \ the stalk biomass is more likely to increase, making it harder to\npredict.\
    \ Our study reinforced that the effect was substantially lessened by taking structural\n\
    descriptions such as LiDAR derivatives into the model.\n5.2. Contribution of Different\
    \ Data Modalities for Phenotyping Predictions\nAt the time, this study utilized\
    \ and encompassed all of the state-of-the-art sensors\ntailored to a small UAV\
    \ for phenotyping scouting. In this section, the potential of each data\ntype\
    \ was explored on the basis of both individual and joint contribution toward a\
    \ variety\nof maize phenotyping predictions. First, relying on the results from\
    \ Figure 8 above, the\nhyperspectral data were the modality, whether existing\
    \ in a form of indices or imagery or in\na type of singularity or fusion, being\
    \ able to substantially boost the regression performance.\nFurther analyses, including\
    \ MDI feature importance (Figure 10) and a sensitivity analysis\nof imagery augmentation\
    \ (Figure 11), disclosed that hyperspectral data in both indices and\nimagery\
    \ format stood up as the most contributory predictor. Many of the previous studies\n\
    have acknowledged the great value and applicability of UAV-borne hyperspectral\
    \ imaging\n(HSI) on the basis of a better performance proﬁling vegetation properties\
    \ and respective\nendmembers by a contiguous spectra record and storage [16,18,60].\n\
    Sensors 2023, 23, 1827\n24 of 38\nSensors 2023, 23, x FOR PEER REVIEW \n26 of\
    \ 41 \n \n \nFigure 10. The 10 most important vegetation indices in a descending\
    \ order of feature importance \nanalysis performed by the mean decrease impurity\
    \ (MDI). \nStalk biomass was found to be estimated the most accurately by fusing\
    \ data modali-\nties. In addition to the valuable contribution of the NIR spectra\
    \ discussed above, canopy \nand plant structural descriptions derived from LiDAR\
    \ data such as canopy height and \nintensity served as critical sources of information\
    \ to predict stalk biomass. More obvi-\nously, Figure 8a informed the dominance\
    \ of the crop’s structural features when 8 out of \n10 of the most important features\
    \ were descents from the LiDAR canopy height. Many \nstudies came to a consensus\
    \ that vegetation spectra alone are insufficient to access a high \naccuracy of\
    \ stalk biomass prediction due to a vegetation saturation effect. [112,120] ex-\n\
    plained that this effect likely occurs when the crops canopy outstretches and\
    \ reaches to a \n100% cover in the mid-vegetative period, while most crops’ biomass\
    \ continues \nFigure 10. The 10 most important vegetation indices in a descending\
    \ order of feature importance\nanalysis performed by the mean decrease impurity\
    \ (MDI).\nIn spite of the proven value of the hyperspectral, there was an exception\
    \ with respect\nto predicting maize stalk biomass when the LiDAR-derived canopy\
    \ height became a more\npredictive power than the hyperspectral (Figure 10a and\
    \ 11a). In consistency with previous\nstudies [112,114], crop canopy height was\
    \ highly correlated with biomass, and the inclusion\nof crop height with spectral\
    \ indices improved the accuracy of the biomass prediction. In\naddition to the\
    \ ﬁnding of LiDAR data aforementioned, this study unfolded the signiﬁcance\nof\
    \ representations of 50 and higher percentiles of the canopy height, as their\
    \ indices were\nall displayed as the most important features, particularly for\
    \ stalk biomass prediction. This\nimplied that the upper half of the canopy structure\
    \ such as stems, leave angle, tassels\ncontains enriched materials essential for\
    \ phenotyping scouting. Stalk biomass was the only\ntrait in this study showcasing\
    \ the value of LiDAR canopy height weighed over the other\ndata types value (Figure\
    \ 11a).\nThe third data modality investigated in this study was LiDAR-derived\
    \ intensity at\nthe canopy level. LiDAR intensity indices noticed a weak signiﬁcance\
    \ in predicting grain-\nafﬁliated traits (Figure 10e,h) with a standout of the\
    \ Imax index (the maximum value of\nLiDAR canopy intensity points). The canopy\
    \ roughness and scattering intensity have little\nSensors 2023, 23, 1827\n25 of\
    \ 38\nquantitative meaning in remote sensing for crop monitoring; instead, LiDAR\
    \ intensity could\nbe used for qualitative analyses of the points [121]. Thermal\
    \ data had the least inﬂuence on\nall predictive models of maize traits in this\
    \ study irrespective of machine learning or deep\nlearning regressors and of singularity\
    \ data or fusion data. Graphically explained by the MDI\nfeature importance analysis\
    \ in Figure 10, the thermal index in machine learning models was\ncompletely irrelevant\
    \ in maize predictions, and similarly, a negligible contribution towards\nthe\
    \ predictive power was also found in deep learning models with thermal imagery\
    \ data\nalone (Figure 11). Previous studies showed that thermal infrared (8000–14,000\
    \ nm) remote\nsensing lends itself to modeling water-induced stress in crops by\
    \ recognizing the plant\nresponses, including stomatal closure, decreased transpiration,\
    \ or simply leaf and canopy\nwater content [39]. It was not the same case in this\
    \ study when water was adequately\nsupplied to all plots in the entire growing\
    \ season.\naccumulating under the cover. In this context, the absorbed and reflected\
    \ amount of \ndownwelling radiation remains virtually unchanged, but the stalk\
    \ biomass is more likely \nto increase, making it harder to predict. Our study\
    \ reinforced that the effect was substan-\ntially lessened by taking structural\
    \ descriptions such as LiDAR derivatives into the model. \n5.2. Contribution of\
    \ Different Data Modalities for Phenotyping Predictions \nAt the time, this study\
    \ utilized and encompassed all of the state-of-the-art sensors \ntailored to a\
    \ small UAV for phenotyping scouting. In this section, the potential of each \n\
    data type was explored on the basis of both individual and joint contribution\
    \ toward a \nvariety of maize phenotyping predictions. First, relying on the results\
    \ from Figure 8 \nabove, the hyperspectral data were the modality, whether existing\
    \ in a form of indices or \nimagery or in a type of singularity or fusion, being\
    \ able to substantially boost the regres-\nsion performance. Further analyses,\
    \ including MDI feature importance (Figure 10) and a \nsensitivity analysis of\
    \ imagery augmentation (Figure 11), disclosed that hyperspectral \ndata in both\
    \ indices and imagery format stood up as the most contributory predictor. \nMany\
    \ of the previous studies have acknowledged the great value and applicability\
    \ of \nUAV-borne hyperspectral imaging (HSI) on the basis of a better performance\
    \ profiling \nvegetation properties and respective endmembers by a contiguous\
    \ spectra record and \nstorage [16,18,60]. \n \nFigure 11. The effect of data\
    \ augmentation on the prediction performance of multi-task deep learn-\ning with\
    \ different data modality and fusions. Data types included thermal = canopy thermal\
    \ images, \nFigure 11. The effect of data augmentation on the prediction performance\
    \ of multi-task deep learning\nwith different data modality and fusions. Data\
    \ types included thermal = canopy thermal images,\ninten = LiDAR canopy intensity\
    \ images, dsm = LiDAR canopy height images, hyper = hyperspectral\nimages. Feature\
    \ fusions included hyper + dsm = a fusion of hyperspectral and LiDAR canopy height\n\
    images, hyper + dsm + inten = a fusion of hyperspectral, LiDAR canopy height,\
    \ and LiDAR canopy\nintensity images, and hyper + dsm + thermal = a fusion of\
    \ hyperspectral, LiDAR canopy height,\nLiDAR canopy intensity, and thermal images.\n\
    Sensors 2023, 23, 1827\n26 of 38\nMultimodal data fusion was the focal interest\
    \ in this study that performed a sounder\nprediction than individual data modality\
    \ models. It is plausible that, in as many char-\nacteristics of maize as being\
    \ sensed, each of these details, itself and jointly, supplements\nto predicting\
    \ the crop’s status. The hyperspectral provides ample information about ni-\n\
    trogen [122], chlorophyll and anthocyanin absorption [123], leaf cellular scattering\
    \ [124],\nsenescence [125]. LiDAR derivatives communicate plant structures and\
    \ metabolism [126],\nwhereas thermal discloses canopy temperature and water content\
    \ ([127]. A 3 to 10% more\naccurate prediction by a fusion of multiple aerial\
    \ data modalities is also found in a few\narticles relative to soybean yield estimation\
    \ [5] and urban tree classiﬁcation [128]. We\nexamined four fusion models in this\
    \ study, and the prediction performed by the fusion\nbetween hyperspectral imagery\
    \ and LiDAR canopy height was most accurate.\n5.3. Feature- and Imagery-Based\
    \ Prediction Comparison\nThe results from Figure 6 illuminated a comparable predictive\
    \ performance between\nfeature-wise and image-wise methods with a few minor exceptions.\
    \ When predicting the\nharvest index and grain nitrogen utilization (Figure 6d,e),\
    \ the RFR performance with hand-\ncrafted features proved to be discernible relying\
    \ largely on the signiﬁcance of PPR[450, 550]\nand FRI2[750, 800]. Comparing the\
    \ CNNs deep learning assembled by multiple ﬁlters and\nslicing 3D kernels (3 ×\
    \ 3 × 6) casts doubt on whether the models experienced undesired\ninformation\
    \ loss of the relationship between the above-indicated bands of 450 nm and\n550\
    \ nm, and between 750 nm and 800 nm for the predictions. It was perhaps due to\
    \ the\n3D kernels neglecting ratios of faraway bands in the spectral dimension\
    \ [129,130]. It could\nhardly be adjusted because the study attempted to construct\
    \ a single model for multiple\noutputs, and future studies may want to ﬁne tune\
    \ these hyperparameters and tailor them\nfor individual predictions of harvest\
    \ index and grain nitrogen utilization.\nImage-wise deep learning models’ performance\
    \ discernibly beat the indices-wise\nmachine learning models in all of the predictions\
    \ if either thermal imagery and LiDAR\ncanopy intensity imagery were inputs (Figure\
    \ 8). In the same ﬁgure, given the 5-time dataset\nshufﬂing and bootstrapping,\
    \ the smaller error bars of deep learning models proceeding\nwith thermal and\
    \ LiDAR canopy intensity concretely showed that image-wise models\nremained more\
    \ stable and steadier than the indices-wise models. This result demonstrated\n\
    the high quality of excellence of CNN family architectures when processing images\
    \ and\nextracting learnable details from them [131,132]. It becomes clear that\
    \ the vegetation indices\ncan only derive a few numbers to a dozen of attributes\
    \ such as the temperature mean of\neach whole plot, but by an operational difference,\
    \ the convolutional layers can slide through\nall pixels of the plots’ thermal\
    \ images to attain enriched and complex attractions for the\npredictions. Furthermore,\
    \ the stability of image-wise deep learning methods could again\nbe observed in\
    \ Appendix B (a summary table of training and testing results) citing no\nclue\
    \ of overﬁtting between training and testing metrics (R2, MAE, RMSE). The overﬁtting\n\
    magnitude of indices-wise machine learning models was substantially higher, particularly\n\
    when looping through shufﬂed datasets.\n5.4. Mono-Task and Multi-Task Learning\
    \ Comparison\nIn a comparison between mono-task and multi-task deep learning models,\
    \ it is neces-\nsary to inform that mono-task models learned and inferred independently\
    \ for each of the\neight maize traits, which makes them different from multi-task\
    \ models that simultaneously\naccomplished eight phenotypic predictions. Given\
    \ the same feature fetching approach\n(i.e., data singularity or fusion), the\
    \ results of mono-task and multi-task methods from\nFigure 8 were identical, but\
    \ to mention that the multi-task slightly outperformed in models\npredicting harvest\
    \ index, grain nitrogen utilization efﬁciency (Grain NutE), and grain\ndensity.\
    \ This ﬁnding was very supportive as it was aligned with the results of [133]\
    \ in that\nthe author articulated that multi-task learning could exploit latent\
    \ relatedness of crop traits\nduring the process of optimizing weights and biases\
    \ of each node in the network.\nSensors 2023, 23, 1827\n27 of 38\nFurther, the\
    \ multi-task models appeared to be noticeable when inputs are an imagery\nfusion\
    \ of hyperspectral, LiDAR, and thermal. The high performance of the multi-task\
    \ even\nsustained throughout all fused models, while the performance of the mono-task\
    \ saturated,\nif not slightly decreased, when adding LiDAR canopy intensity and\
    \ thermal to the fused\nmodels (Figure 8). It is obvious that while LiDAR canopy\
    \ intensity or thermal became\nnoisy and corrupt data for a particular maize trait,\
    \ it could be predictive data for another\ntrait. The sharing protocol can be\
    \ achieved only by the multi-task, where it leveraged the\nconvolutional layers\
    \ to extract shared information from the data fusion and allocate them\nto each\
    \ task, if needed, to minimize the preset loss. The last and most visible advantage\n\
    of the multi-task over the mono-task rested in chipping down required computational\
    \ re-\nsources to a fraction and concurrently accelerating high-throughput phenotyping.\
    \ Because\ncalculating resource savings from multi-task learning was not a focus\
    \ of this study, we did\nnot document these ﬁgures, and interested readers can\
    \ refer to this matter in [134,135].\n5.5. Impacts of Data Augmentation on Deep\
    \ Learning Regression\nWith a limited number of samples collected, it becomes\
    \ difﬁcult for any deep learning\nmethods to be convergent during the training\
    \ process and to infer a reliable result. The\nimbalance effect of small labeled\
    \ samples and the high dimensionality of remotely sensed\ndata is an intrinsic\
    \ limitation in the remote sensing research, which is known as the Hughes\nphenomenon\
    \ [23]. This study is not an exception when there were only 369 ﬁeld plots\nmanually\
    \ measured and annotated for analyses. To address the limitation, we augmented\n\
    imagery data by iterating 20 cycles of randomly slicing a spatial window over\
    \ plots only\non training sets. Figure 10 unveiled a boost in the R2 metric when\
    \ the augmented models\ninferred against the test sets. With respect to the impact\
    \ of the augmentation method on\nthe models with a singularity of data types,\
    \ the R2 metrics steeply ascended after a few\naugmentation iterations, and it\
    \ continued even after 20 iterations. It bears noting that the\nhyperspectral\
    \ images did not beneﬁt from the augmentation cycles as much as the LiDAR\ncanopy\
    \ height when the R2 of LiDAR height-inputted models took off and overshadowed\n\
    the hyperspectral model’s R2 (Figure 10a).\nWith respect to the impact of the\
    \ augmentation on data fusion models, the results also\nsoared up after the ﬁrst\
    \ three iterations and reached saturation in the 20 iterations in the\nmodels\
    \ of predicting cob biomass and total plant nitrogen content. The positive impact\n\
    of data augmentation was credited to slicing a ﬁxed-size spatial window through\
    \ every\npixel of a plot in which details of every plant in that plot were fully\
    \ captured. Adding\nnew augmented images to deep learning models equally meant\
    \ forcing the models to\nlearn all useful details of the crop’s plots, and also\
    \ meant lessening the possibility that\nconvolutional nodes fondly remember and\
    \ heavily rely on certain details, which often leads\nto an overﬁtting effect.\n\
    5.6. Performance of Different Methods over Space\nThe residuals between actual\
    \ and predicted values as results of seven data sources\nand four different regressors\
    \ were evaluated in terms of spatial randomness by GMI\nstatistical test. Figure\
    \ 12 represented Moran’s I coefﬁcient in vertical bars colored by\nfour methods\
    \ followed by the asterisks implying a statistical signiﬁcance (p < 0.001) of\n\
    spatial auto-correlation between data points (plot prediction errors). It became\
    \ obvious\nthat regression residuals resulting from deep learning, especially\
    \ multi-task learning, were\ninsigniﬁcantly spatially correlated and remained\
    \ independent from other residuals in\nsurrounding plots. The spatial randomness\
    \ was more solidly secured in deep learning\nmodels carrying hyperspectral alone\
    \ and data fusion. The small and spatially random\nregression errors suggested\
    \ an impressive prediction capability of multi-task deep learning\nmodels that\
    \ could extensively apprehend complex and underlying nonlinear abstracts of\n\
    imagery data of each crop plot, compared to a handcrafting establishment of vegetation\n\
    indices [103]. The SVR and RFR appeared to be less reliable as their regression\
    \ residuals\nwere spatially statistically insigniﬁcant in some cases but also\
    \ signiﬁcant in predictions\nSensors 2023, 23, 1827\n28 of 38\nof harvest index\
    \ (Figure 12d) and grain nitrogen content (Figure 12f). Additionally, the\nGMI\
    \ test reported a signiﬁcance of the regression errors from cob biomass predictions\n\
    across all models and data sources (Figure 12b). The positive sign of Moran’s\
    \ I coefﬁcients\nnoticed a clustering over the space of cob biomass prediction’s\
    \ residuals. Inspecting these\nresiduals over a map, the clusters of residuals\
    \ originated from maize growing along aisles\nexposed the most to weather conditions.\
    \ It is possible that the UAVs failed to sense certain\nconfounding variables\
    \ that could help to explain the corn cob variation, inclusive of, but\nnot limited\
    \ to, photosynthesis under the inﬂuence of sunlight intensity and metabolism\n\
    with air and soil temperature progressive over time. This suggested future UAV\
    \ remote\nsensing research to survey crops in a temporal dimension and document\
    \ and incorporate\nﬁeld metadata into analyses.\nSensors 2023, 23, x FOR PEER\
    \ REVIEW \n31 of 41 \n \n \nFigure 12. Comparison of Moran’s I values of different\
    \ data types and fusions, and different regres-\nsors. The asterisk ‘*’ on the\
    \ top of bars implies the Moran’s I is statistically significant at 0.001 p-\n\
    value level. Feature types included thermal = canopy thermal, inten = LiDAR canopy\
    \ intensity, dsm \n= LiDAR canopy height, hyper = hyperspectral images. Feature\
    \ fusions included hyper + dsm = a \nfusion of hyperspectral and LiDAR canopy\
    \ height, hyper + dsm + inten = a fusion of hyperspectral, \nLiDAR canopy height,\
    \ and LiDAR canopy intensity, and hyper + dsm + thermal = a fusion of hyper-\n\
    spectral, LiDAR canopy height, LiDAR canopy intensity, and thermal. \n6. Conclusions\
    \ \nWith the proven success of UAV in recent digital agriculture, this study was\
    \ an ex-\ntended investigation of the UAV versatility for high-throughput maize\
    \ phenotyping. The \nFigure 12. Comparison of Moran’s I values of different data\
    \ types and fusions, and different regressors.\nThe asterisk ‘*’ on the top of\
    \ bars implies the Moran’s I is statistically signiﬁcant at 0.001 p-value level.\n\
    Feature types included thermal = canopy thermal, inten = LiDAR canopy intensity,\
    \ dsm = LiDAR\ncanopy height, hyper = hyperspectral images. Feature fusions included\
    \ hyper + dsm = a fusion of\nhyperspectral and LiDAR canopy height, hyper + dsm\
    \ + inten = a fusion of hyperspectral, LiDAR\ncanopy height, and LiDAR canopy\
    \ intensity, and hyper + dsm + thermal = a fusion of hyperspectral,\nLiDAR canopy\
    \ height, LiDAR canopy intensity, and thermal.\nSensors 2023, 23, 1827\n29 of\
    \ 38\n6. Conclusions\nWith the proven success of UAV in recent digital agriculture,\
    \ this study was an\nextended investigation of the UAV versatility for high-throughput\
    \ maize phenotyping. The\nUAV aerial remote sensing was instrumental for scouting\
    \ and estimating a full suite of\neight different phenotypes in a corn ﬁeld by\
    \ blending geospatial and artiﬁcial intelligence\n(AI) competence, which is also\
    \ known as GeoAI. The novelty entitling the study to be\nhighly signiﬁcant in\
    \ both theoretical and practical exercises rested in the deployment of\nUAV airborne\
    \ multisensory data fusion within a single multi-task deep learning model.\nConsidering\
    \ the results and discussions presented in the aforementioned, we concluded\n\
    the following:\n1.\nThe success level of UAV multisensory data for high-throughput\
    \ maize phenotyp-\ning varies from trait to trait because each trait is responsive\
    \ to the experiment and\nenvironmental conditions in different mechanisms. Grain\
    \ density prediction was the\nleast successful (R2 = 0.34) in contrast with very\
    \ high predictable traits: plant total\nnitrogen content and grain nitrogen content\
    \ (R2 = 0.85). The resulting RMSE and MAE\nwere congruent in high R2 models and\
    \ became discrepant in low R2 models, which\nsigniﬁes extreme values in the ground\
    \ dataset. Expanding observations and collecting\nmore data are highly recommended,\
    \ particularly for grain density, grain NutE, and\nharvest index in future research.\n\
    2.\nThere is a varying contribution of each data modality (hyperspectral, thermal,\
    \ LiDAR\ncanopy height, LiDAR canopy intensity) individually and their fusion\
    \ for phenotyping\npredictions. Hyperspectral data were the most primarily contributory\
    \ to virtually all\neight estimations, especially dry grain yield, and nitrogen\
    \ content in plants and grains.\nLiDAR canopy height enjoyed its merit in predicting\
    \ stalk biomass more accurately\nthan any other modality. The superiority of multisensory\
    \ data fusion in all phenotype\npredictions was evident in the study because the\
    \ fusion can help to exceed limitations\nof single data modality, for example,\
    \ the vegetation saturation effect occurring in\noptical remote sensing.\n3.\n\
    Feature- and imagery-based prediction are comparable if the latter is not superior\
    \ to\nthe former. Image-based deep learning within a framework of convolutional\
    \ neural\nnetworks (CNNs) demonstrated an automation of the feature extraction,\
    \ neither\nrelying on human expertise nor being prone to human errors. This is\
    \ concretely\nevidenced by the outperformance of image-based deep learning when\
    \ thermal or\nLiDAR intensity data were funneled to the CNNs across maize trait\
    \ predictions.\nThe image-based deep learning remained stable as indicated by\
    \ a smaller deviation\nthrough dataset shufﬂing.\n4.\nMono-task and multi-task\
    \ learning are comparable if the latter is not superior to the\nformer. Multi-task\
    \ deep learning leverages latent relatedness among maize traits\nduring optimizing\
    \ cycles of weights and biases of each network node. The shar-\ning protocol of\
    \ multi-task models can reach its full potential when interacting with\nmultisensory\
    \ data fusion, which becomes multi-input multi-output models. It is\nalso evident\
    \ that executing multi-task learning models only requires a fraction of\nthe computational\
    \ resources and time needed for mono-task learning models, while\naccelerating\
    \ high throughput phenotyping by simultaneous predictions.\n5.\nData augmentation\
    \ for deep learning in the context of regression succeeds to elevate\nthe intrinsic\
    \ issue of a small sample size in remote sensing research (i.e., the Hughes\n\
    effect). Augmented data also help to build up the rigidity and reliability of\
    \ deep\nlearning models by faster convergence and less overﬁtting.\n6.\nA randomness\
    \ over space of the prediction residuals from the Global Morans’ I\nanalysis implies\
    \ that there were no confounding variables implicitly veering the\npredictive\
    \ performance of maize traits. A small and random regression error also\nreinforces\
    \ the versatility of UAV airborne multisensory data fusion in the framework\n\
    of multi-task deep learning. Cob biomass is the only trait showing a clustering\n\
    Sensors 2023, 23, 1827\n30 of 38\npattern of prediction errors in all models,\
    \ which needs to be investigated further in\nfuture research.\nAuthor Contributions:\
    \ Conceptualization, C.N. and V.S.; data curation, C.N. and V.S.; methodology,\n\
    C.N. and V.S.; software, C.N.; validation, C.N. and V.S.; formal analysis, C.N.\
    \ and V.S.; investigation,\nC.N. and V.S.; resources, V.S. and S.M.; writing—original\
    \ draft preparation, C.N.; writing—review\nand editing, C.N., V.S., S.M. and S.B.;\
    \ visualization, C.N.; supervision, V.S.; project administration, V.S.;\nfunding\
    \ acquisition, V.S. All authors have read and agreed to the published version\
    \ of the manuscript.\nFunding: This work is funded in part by USGS AmericaView\
    \ Grant (G18AP00077). Support for\nthe corn phenotyping at Illinois was partially\
    \ provided by the National Science Foundation Plant\nGenome Research Program,\
    \ under award number IOS-1339362 to S.M.\nInstitutional Review Board Statement:\
    \ Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability\
    \ Statement: Not applicable.\nAcknowledgments: The authors thank the teams of\
    \ ﬁeld management, harvest, and measurement\nfrom the University of Illinois in\
    \ Urbana-Champaign. The appreciation is extended to the UAV data\ncollection team\
    \ from the Remote Sensing Lab at Saint Louis University, who spent long and strenuous\n\
    hours collecting aerial data. The authors would like to thank the editors and\
    \ the anonymous reviewers\nfor their thoughtful review and constructive comments.\n\
    Conﬂicts of Interest: The authors declare that they have no known competing ﬁnancial\
    \ interests or\npersonal relationships that could have appeared to inﬂuence the\
    \ work reported in this paper.\nAppendix A\nSensors 2023, 23, x FOR PEER REVIEW\
    \ \n33 of 41 \n \nAuthor Contributions: Conceptualization, C.N. and V.S.; data\
    \ curation, C.N. and V.S.; methodol-\nogy, C.N. and V.S.; software, C.N.; validation,\
    \ C.N. and V.S.; formal analysis, C.N. and V.S.; inves-\ntigation, C.N. and V.S.;\
    \ resources, V.S. and S.M.; writing—original draft preparation, C.N.; writ-\n\
    ing—review and editing, C.N., V.S., S.M. and S.B.; visualization, C.N.; supervision,\
    \ V.S.; project ad-\nministration, V.S.; funding acquisition, V.S. All authors\
    \ have read and agreed to the published ver-\nsion of the manuscript. \nFunding:\
    \ This work is funded in part by USGS AmericaView Grant (G18AP00077). Support\
    \ for the \ncorn phenotyping at Illinois was partially provided by the National\
    \ Science Foundation Plant Ge-\nnome Research Program, under award number IOS-1339362\
    \ to S.M. \nInstitutional Review Board Statement: Not applicable. \nInformed Consent\
    \ Statement: Not applicable. \nData Availability Statement: Not applicable. \n\
    Acknowledgments: The authors thank the teams of field management, harvest, and\
    \ meas-\nurement from the University of Illinois in Urbana-Champaign. The appreciation\
    \ is ex-\ntended to the UAV data collection team from the Remote Sensing Lab at\
    \ Saint Louis Uni-\nversity, who spent long and strenuous hours collecting aerial\
    \ data. The authors would \nlike to thank the editors and the anonymous reviewers\
    \ for their thoughtful review and \nconstructive comments. \nConflicts of Interest:\
    \ The authors declare that they have no known competing financial in-\nterests\
    \ or personal relationships that could have appeared to influence the work reported\
    \ \nin this paper. \nAppendix A \n \nFigure A1. Histograms of Maize Phenotypes\
    \ (In Diagonal) and Scatter Plots between Pairs of the \nPhenotypes. \nFigure\
    \ A1. Histograms of Maize Phenotypes (In Diagonal) and Scatter Plots between Pairs\
    \ of\nthe Phenotypes.\nSensors 2023, 23, 1827\n31 of 38\nAppendix B\nTable A1.\
    \ Results of maize phenotypic prediction performed by different data sources and\
    \ regressors.\nDatasets\nMetrics\nStalk Biomass (kg/ha)\nCob Biomass (kg/ha)\n\
    Dry Grain Yield (kg/ha)\nHand-Crafted\nFeatures-Based\nImagery-Based\nHand-Crafted\n\
    Features-Based\nImagery-Based\nHand-Crafted\nFeatures-Based\nImagery-Based\nSVR\n\
    RFR\nMono-Task\nMulti-Task\nSVR\nRFR\nMono-Task\nMulti-Task\nSVR\nRFR\nMono-Task\n\
    Multi-Task\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\n\
    Train\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\n\
    Train\nTest\nThermal\nR2\n0.06\n0\n0.1\n0.02\n0.15\n0.12\n0.15\n0.11\n0.05\n0\n\
    0.08\n0\n0.22\n0.22\n0.2\n0.2\n0.07\n0.02\n0.13\n0.05\n0.28\n0.26\n0.28\n0.26\n\
    MAE\n1522\n1518\n1519\n1486\n1419\n1403\n1422\n1404\n377\n401\n377\n393\n336\n\
    335\n339\n337\n2672\n2784\n2603\n2743\n2272\n2351\n2273\n2326\nRMSE\n2120\n1969\n\
    2088\n1943\n2015\n1844\n2016\n1855\n485\n507\n474\n498\n438\n443\n444\n448\n3170\n\
    3252\n3081\n3211\n2786\n2833\n2795\n2830\nLiDAR Intensity\nR2\n0.2\n0.09\n0.16\n\
    0.04\n0.15\n0.16\n0.14\n0.13\n0.3\n0.09\n0.16\n0.03\n0.19\n0.22\n0.18\n0.21\n\
    0.37\n0.04\n0.17\n0\n0.14\n0.17\n0.13\n0.15\nMAE\n1394\n1440\n1463\n1502\n1454\n\
    1355\n1473\n1360\n316\n363\n362\n383\n352\n341\n356\n346\n2108\n2615\n2560\n2807\n\
    2484\n2462\n2510\n2493\nRMSE\n1961\n1867\n2003\n1926\n2019\n1805\n2034\n1824\n\
    416\n474\n454\n492\n446\n443\n449\n446\n2603\n3209\n2993\n3311\n3047\n3008\n3065\n\
    3023\nLiDAR Height\nR2\n0.55\n0.48\n0.58\n0.45\n0.37\n0.38\n0.31\n0.31\n0.37\n\
    0.27\n0.45\n0.33\n0.28\n0.3\n0.29\n0.30\n0.47\n0.33\n0.41\n0.37\n0.25\n0.25\n\
    0.26\n0.26\nMAE\n996\n1040\n1052\n1108\n1254\n1191\n1326\n1266\n276\n307\n274\n\
    300\n325\n313\n323\n313\n1698\n1886\n1925\n1956\n2269\n2249\n2254\n2250\nRMSE\n\
    1472\n1414\n1419\n1464\n1744\n1550\n1823\n1632\n393\n423\n369\n410\n422\n419\n\
    419\n418\n2390\n2668\n2526\n2611\n2857\n2851\n2843\n2840\nHyper\nspectral\nR2\n\
    0.54\n0.5\n0.48\n0.36\n0.43\n0.36\n0.4\n0.32\n0.57\n0.5\n0.56\n0.46\n0.45\n0.37\n\
    0.45\n0.39\n0.81\n0.77\n0.81\n0.78\n0.76\n0.72\n0.76\n0.73\nMAE\n953\n1000\n1076\n\
    1146\n1131\n1167\n1145\n1171\n223\n244\n238\n262\n270\n291\n268\n284\n1061\n1206\n\
    1082\n1175\n1225\n1323\n1218\n1276\nRMSE\n1488\n1392\n1582\n1576\n1651\n1568\n\
    1703\n1613\n324\n353\n329\n368\n369\n398\n368\n390\n1430\n1578\n1438\n1554\n1602\n\
    1722\n1616\n1688\nHyper + LiDAR\nHeight\nR2\n0.6\n0.53\n0.64\n0.47\n0.5\n0.46\n\
    0.4\n0.37\n0.54\n0.43\n0.58\n0.47\n0.47\n0.4\n0.44\n0.41\n0.84\n0.73\n0.81\n0.78\n\
    0.76\n0.73\n0.73\n0.72\nMAE\n911\n1008\n989\n1068\n1069\n1096\n1164\n1156\n221\n\
    265\n233\n259\n264\n281\n271\n279\n914\n1297\n1083\n1176\n1223\n1287\n1274\n1312\n\
    RMSE\n1375\n1343\n1321\n1428\n1545\n1443\n1694\n1549\n335\n376\n322\n364\n363\n\
    387\n372\n384\n1301\n1682\n1437\n1555\n1609\n1691\n1708\n1718\nHyper + LiDAR\n\
    Height + LiDAR\nIntensity\nR2\n0.57\n0.49\n0.64\n0.47\n0.47\n0.47\n0.42\n0.39\n\
    0.48\n0.41\n0.57\n0.47\n0.44\n0.41\n0.45\n0.41\n0.86\n0.72\n0.81\n0.77\n0.73\n\
    0.72\n0.74\n0.72\nMAE\n946\n1028\n990\n1069\n1116\n1088\n1147\n1135\n248\n271\n\
    236\n261\n272\n281\n267\n279\n833\n1380\n1082\n1179\n1282\n1323\n1268\n1315\n\
    RMSE\n1437\n1393\n1321\n1430\n1594\n1430\n1674\n1530\n357\n383\n326\n365\n372\n\
    386\n368\n384\n1197\n1748\n1436\n1558\n1705\n1730\n1692\n1723\nHyper + LiDAR\n\
    Height + LiDAR\nIntensity+ Thermal\nR2\n0.57\n0.49\n0.64\n0.47\n0.47\n0.46\n0.45\n\
    0.43\n0.48\n0.41\n0.57\n0.46\n0.44\n0.40\n0.46\n0.43\n0.85\n0.72\n0.81\n0.77\n\
    0.74\n0.72\n0.73\n0.71\nMAE\n939\n1038\n989\n1070\n1113\n1083\n1107\n1108\n248\n\
    271\n237\n263\n269\n284\n262\n274\n923\n1361\n1082\n1179\n1277\n1338\n1267\n1321\n\
    RMSE\n1432\n1402\n1321\n1430\n1599\n1448\n1623\n1478\n357\n383\n326\n367\n370\n\
    388\n365\n380\n1228\n1734\n1436\n1558\n1696\n1752\n1701\n1747\nSensors 2023, 23,\
    \ 1827\n32 of 38\nTable A2. Results of maize phenotypic prediction performed by\
    \ different data sources and regressors (cont.).\nDatasets\nMetrics\nHarvest Index\n\
    Grain Nitrogen Utilization Efﬁciency (Grain NutE)\nGrain Nitrogen Content (kg/ha)\n\
    Hand-Crafted\nFeatures-Based\nImagery-Based\nHand-Crafted\nFeatures-Based\nImagery-Based\n\
    Hand-Crafted\nFeatures-Based\nImagery-Based\nSVR\nRFR\nMono-Task\nMulti-Task\n\
    SVR\nRFR\nMono-Task\nMulti-Task\nSVR\nRFR\nMono-Task\nMulti-Task\nTrain\nTest\n\
    Train\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\n\
    Train\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nThermal\nR2\n\
    0.02\n0\n0.07\n0\n0.14\n0.13\n0.13\n0.13\n0\n0\n0.04\n0\n0.12\n0.1\n0.11\n0.09\n\
    0.03\n0\n0.12\n0.03\n0.33\n0.3\n0.33\n0.31\nMAE\n0.07\n0.07\n0.07\n0.07\n0.06\n\
    0.06\n0.06\n0.06\n8.3\n7.97\n8.42\n8.17\n7.78\n7.41\n7.78\n7.43\n41.08\n43.28\n\
    40.6\n43.28\n33.41\n34.7\n33.22\n34.66\nRMSE\n0.09\n0.09\n0.09\n0.09\n0.08\n0.08\n\
    0.08\n0.08\n11.14\n10.89\n10.94\n10.76\n10.49\n10.15\n10.53\n10.24\n48.44\n50.67\n\
    46.29\n48.97\n40.25\n41.34\n40.33\n41.30\nLiDAR Intensity\nR2\n0.12\n0\n0.1\n\
    0\n0.04\n0.04\n0.02\n0.02\n0.14\n0.08\n0.17\n0.02\n0.11\n0.11\n0.09\n0.09\n0.38\n\
    0.05\n0.2\n0.02\n0.19\n0.2\n0.17\n0.18\nMAE\n0.06\n0.07\n0.07\n0.07\n0.07\n0.06\n\
    0.07\n0.07\n7.76\n7.58\n7.81\n7.91\n7.89\n7.64\n8.08\n7.66\n31.29\n39.66\n39.14\n\
    43.55\n37.98\n38.0\n38.94\n39.1\nRMSE\n0.08\n0.09\n0.08\n0.09\n0.09\n0.09\n0.09\n\
    0.09\n10.36\n10.27\n10.15\n10.60\n10.52\n10.14\n10.69\n10.22\n38.58\n48.06\n44.09\n\
    49.09\n44.39\n44.26\n44.99\n45.04\nLiDAR Height\nR2\n0.23\n0.02\n0.28\n0.12\n\
    0.05\n0.04\n0.04\n0.03\n0.33\n0.2\n0.35\n0.26\n0.18\n0.17\n0.17\n0.19\n0.54\n\
    0.36\n0.47\n0.42\n0.3\n0.3\n0.3\n0.3\nMAE\n0.05\n0.06\n0.06\n0.06\n0.07\n0.06\n\
    0.07\n0.06\n6.48\n6.96\n6.81\n6.96\n7.52\n7.37\n7.63\n7.41\n23.24\n27.93\n27.35\n\
    28.63\n33.98\n34.33\n34.26\n34.53\nRMSE\n0.08\n0.09\n0.08\n0.08\n0.09\n0.09\n\
    0.09\n0.09\n9.15\n9.59\n9.03\n9.24\n10.12\n9.74\n10.16\n9.65\n33.27\n39.52\n35.74\n\
    37.60\n41.1\n41.55\n41.31\n41.57\nHyper\nspectral\nR2\n0.53\n0.49\n0.53\n0.42\n\
    0.45\n0.41\n0.45\n0.41\n0.39\n0.29\n0.44\n0.3\n0.36\n0.29\n0.33\n0.27\n0.87\n\
    0.82\n0.88\n0.85\n0.85\n0.81\n0.84\n0.81\nMAE\n0.04\n0.04\n0.04\n0.04\n0.04\n\
    0.04\n0.04\n0.04\n5.57\n5.85\n5.86\n6.01\n6.13\n6.05\n6.23\n6.15\n12.79\n15.87\n\
    12.67\n13.97\n14.66\n16.13\n14.69\n16.33\nRMSE\n0.06\n0.06\n0.06\n0.07\n0.07\n\
    0.07\n0.07\n0.07\n8.75\n9.05\n8.35\n8.97\n8.92\n9.04\n9.13\n9.18\n17.66\n20.95\n\
    17.22\n19.24\n19.26\n21.37\n19.63\n21.56\nHyper + LiDAR\nHeight\nR2\n0.55\n0.48\n\
    0.6\n0.56\n0.47\n0.41\n0.42\n0.43\n0.42\n0.32\n0.55\n0.49\n0.38\n0.3\n0.27\n0.25\n\
    0.87\n0.81\n0.88\n0.85\n0.84\n0.81\n0.81\n0.79\nMAE\n0.04\n0.04\n0.04\n0.04\n\
    0.04\n0.04\n0.05\n0.04\n5.36\n5.90\n5.51\n5.55\n6.01\n6.12\n6.74\n6.51\n12.69\n\
    16.62\n12.67\n13.99\n14.63\n16.21\n16.03\n16.77\nRMSE\n0.06\n0.06\n0.06\n0.06\n\
    0.06\n0.07\n0.07\n0.07\n8.48\n8.82\n7.48\n7.62\n8.79\n8.97\n9.54\n9.27\n17.77\n\
    21.76\n17.22\n19.27\n19.58\n21.73\n21.42\n22.43\nHyper + LiDAR\nHeight + LiDAR\n\
    Intensity\nR2\n0.56\n0.47\n0.6\n0.55\n0.46\n0.42\n0.42\n0.43\n0.44\n0.33\n0.56\n\
    0.48\n0.26\n0.24\n0.27\n0.27\n0.85\n0.79\n0.88\n0.85\n0.81\n0.79\n0.81\n0.79\n\
    MAE\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\n5.37\n6.02\n5.55\n5.57\n\
    6.86\n6.67\n6.71\n6.44\n14.4\n16.71\n12.66\n14.00\n15.99\n16.96\n15.86\n16.89\n\
    RMSE\n0.06\n0.06\n0.06\n0.06\n0.07\n0.07\n0.07\n0.07\n8.34\n8.77\n7.43\n7.70\n\
    9.62\n9.37\n9.53\n9.17\n19.33\n22.36\n17.21\n19.27\n21.25\n22.62\n21.22\n22.65\n\
    Hyper + LiDAR\nHeight + LiDAR\nIntensity + Thermal\nR2\n0.55\n0.46\n0.6\n0.55\n\
    0.45\n0.42\n0.44\n0.45\n0.45\n0.31\n0.56\n0.48\n0.29\n0.23\n0.29\n0.29\n0.86\n\
    0.8\n0.88\n0.85\n0.81\n0.78\n0.82\n0.79\nMAE\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\n\
    0.04\n0.04\n5.21\n6.08\n5.55\n5.57\n6.71\n6.65\n6.61\n6.36\n13.47\n16.65\n12.66\n\
    14.00\n16.24\n17.01\n15.51\n16.7\nRMSE\n0.06\n0.06\n0.06\n0.06\n0.07\n0.07\n0.07\n\
    0.07\n8.24\n8.90\n7.43\n7.69\n9.44\n9.39\n9.39\n9.02\n18.57\n22.20\n17.21\n19.28\n\
    21.66\n23.10\n21.15\n22.50\nSensors 2023, 23, 1827\n33 of 38\nTable A3. Results\
    \ of maize phenotypic prediction performed by different data sources and regressors\
    \ (cont.).\nDatasets\nMetrics\nTotal Plant N (kg/ha)\nGrain Density\nHand-Crafted\
    \ Features-Based\nImagery-Based\nHand-Crafted Features-Based\nImagery-Based\n\
    SVR\nRFR\nMono-Task\nMulti-Task\nSVR\nRFR\nMono-Task\nMulti-Task\nTrain\nTest\n\
    Train\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\n\
    Train\nTest\nThermal\nR2\n0.03\n0\n0.13\n0.05\n0.34\n0.32\n0.32\n0.29\n0.01\n\
    0\n0.03\n0\n0.14\n0.09\n0.12\n0.11\nMAE\n58.2\n60.79\n57.97\n60.30\n46.8\n47.91\n\
    48.32\n49.19\n0.03\n0.03\n0.03\n0.03\n0.03\n0.02\n0.03\n0.03\nRMSE\n68.96\n71.64\n\
    65.48\n68.07\n56.93\n57.49\n57.74\n58.52\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\n\
    0.04\n0.03\nLiDAR Intensity\nR2\n0.42\n0.04\n0.21\n0.04\n0.21\n0.23\n0.18\n0.19\n\
    0.11\n0.05\n0.13\n0.03\n0.1\n0.12\n0.1\n0.13\nMAE\n43.12\n54.85\n55.63\n61.01\n\
    52.5\n51.9\n55.19\n54.67\n0.03\n0.03\n0.03\n0.03\n0.03\n0.02\n0.03\n0.03\nRMSE\n\
    53.19\n67.95\n62.47\n68.23\n62.17\n60.97\n63.65\n62.65\n0.04\n0.04\n0.04\n0.04\n\
    0.04\n0.03\n0.04\n0.03\nLiDAR Height\nR2\n0.58\n0.44\n0.52\n0.47\n0.36\n0.36\n\
    0.34\n0.34\n0.29\n0.15\n0.28\n0.19\n0.14\n0.13\n0.15\n0.14\nMAE\n32.49\n37.95\n\
    37.19\n39.17\n46.26\n46.61\n46.77\n47.39\n0.02\n0.03\n0.02\n0.02\n0.03\n0.03\n\
    0.03\n0.03\nRMSE\n45.58\n51.94\n48.53\n50.80\n56.26\n55.94\n56.86\n56.57\n0.03\n\
    0.03\n0.03\n0.03\n0.04\n0.03\n0.04\n0.03\nHyperspectral\nR2\n0.88\n0.85\n0.88\n\
    0.86\n0.87\n0.85\n0.85\n0.83\n0.4\n0.29\n0.38\n0.28\n0.34\n0.32\n0.33\n0.32\n\
    MAE\n17.37\n20.39\n17.31\n18.98\n18.97\n20.25\n20.01\n21.56\n0.02\n0.02\n0.02\n\
    0.02\n0.02\n0.02\n0.02\n0.02\nRMSE\n24.51\n26.91\n24.24\n26.16\n25.45\n27.06\n\
    27.18\n28.74\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\nHyper +\nLiDAR Height\n\
    R2\n0.88\n0.85\n0.88\n0.85\n0.86\n0.84\n0.82\n0.82\n0.47\n0.34\n0.44\n0.34\n0.34\n\
    0.32\n0.31\n0.31\nMAE\n17.27\n20.38\n17.41\n19.16\n19.68\n20.83\n22.25\n22.62\n\
    0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\n0.02\nRMSE\n24.3\n27.22\n24.15\n26.64\n\
    26.25\n27.43\n29.53\n29.40\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\nHyper\
    \ +\nLiDAR Height +\nLiDAR Intensity\nR2\n0.87\n0.83\n0.88\n0.85\n0.84\n0.83\n\
    0.83\n0.82\n0.38\n0.31\n0.46\n0.35\n0.3\n0.28\n0.3\n0.3\nMAE\n17.9\n21.82\n17.39\n\
    19.22\n21.37\n22.24\n21.98\n22.5\n0.02\n0.02\n0.02\n0.02\n0.03\n0.03\n0.03\n0.02\n\
    RMSE\n24.86\n28.31\n24.11\n26.74\n28.4\n28.68\n29.17\n29.37\n0.03\n0.03\n0.03\n\
    0.03\n0.03\n0.03\n0.03\n0.03\nHyper +\nLiDAR Height +\nLiDAR Intensity\n+ Thermal\n\
    R2\n0.88\n0.83\n0.88\n0.85\n0.83\n0.83\n0.83\n0.82\n0.38\n0.32\n0.46\n0.35\n0.29\n\
    0.25\n0.31\n0.29\nMAE\n17.12\n21.93\n17.39\n19.23\n21.63\n21.85\n21.65\n22.65\n\
    0.02\n0.02\n0.02\n0.02\n0.03\n0.03\n0.02\n0.02\nRMSE\n23.95\n28.54\n24.11\n26.75\n\
    28.76\n28.39\n28.94\n29.40\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\nSensors\
    \ 2023, 23, 1827\n34 of 38\nReferences\n1.\nNoureldin, N.; Aboelghar, M.; Saudy,\
    \ H.; Ali, A. Rice yield forecasting models using satellite imagery in Egypt.\
    \ Egypt. J. Remote\nSens. Space Sci. 2013, 16, 125–131. [CrossRef]\n2.\nWang,\
    \ L.; Tian, Y.; Yao, X.; Zhu, Y.; Cao, W. Predicting grain yield and protein content\
    \ in wheat by fusing multi-sensor and\nmulti-temporal remote-sensing images. Field\
    \ Crops Res. 2014, 164, 178–188. [CrossRef]\n3.\nReynolds, C.A.; Yitayew, M.;\
    \ Slack, D.C.; Hutchinson, C.F.; Huete, A.; Petersen, M.S. Estimating crop yields\
    \ and production by\nintegrating the FAO Crop Speciﬁc Water Balance model with\
    \ real-time satellite data and ground-based ancillary data. Int. J.\nRemote Sens.\
    \ 2000, 21, 3487–3508. [CrossRef]\n4.\nSchut, A.G.; Traore, P.C.S.; Blaes, X.;\
    \ Rolf, A. Assessing yield and fertilizer response in heterogeneous smallholder\
    \ ﬁelds with\nUAVs and satellites. Field Crops Res. 2018, 221, 98–107. [CrossRef]\n\
    5.\nMaimaitijiang, M.; Sagan, V.; Sidike, P.; Hartling, S.; Esposito, F.; Fritschi,\
    \ F.B. Soybean yield prediction from UAV using\nmultimodal data fusion and deep\
    \ learning. Remote Sens. Environ. 2020, 237, 111599. [CrossRef]\n6.\nGeipel, J.;\
    \ Link, J.; Claupein, W. Combined spectral and spatial modeling of corn yield\
    \ based on aerial images and crop surface\nmodels acquired with an unmanned aircraft\
    \ system. Remote Sens. 2014, 6, 10335–10355. [CrossRef]\n7.\nVega, F.A.; Ramirez,\
    \ F.C.; Saiz, M.P.; Rosúa, F.O. Multi-temporal imaging using an unmanned aerial\
    \ vehicle for monitoring a\nsunﬂower crop. Biosyst. Eng. 2015, 132, 19–27. [CrossRef]\n\
    8.\nYang, Q.; Shi, L.; Han, J.; Zha, Y.; Zhu, P. Deep convolutional neural networks\
    \ for rice grain yield estimation at the ripening stage\nusing UAV-based remotely\
    \ sensed images. Field Crops Res. 2019, 235, 142–153. [CrossRef]\n9.\nAnderson,\
    \ S.L.; Murray, S.C.; Malambo, L.; Ratcliff, C.; Popescu, S.; Cope, D.; Chang,\
    \ A.; Jung, J.; Thomasson, J.A. Prediction of\nmaize grain yield before maturity\
    \ using improved temporal height estimates of unmanned aerial systems. Plant Phenome\
    \ J. 2019,\n2, 1–15. [CrossRef]\n10.\nBallester, C.; Hornbuckle, J.; Brinkhoff,\
    \ J.; Smith, J.; Quayle, W. Assessment of in-season cotton nitrogen status and\
    \ lint yield\nprediction from unmanned aerial system imagery. Remote Sens. 2017,\
    \ 9, 1149. [CrossRef]\n11.\nUto, K.; Seki, H.; Saito, G.; Kosugi, Y. Characterization\
    \ of rice paddies by a UAV-mounted miniature hyperspectral sensor system.\nIEEE\
    \ J. Sel. Top. Appl. Earth Obs. Remote Sens. 2013, 6, 851–860. [CrossRef]\n12.\n\
    Aasen, H.; Burkart, A.; Bolten, A.; Bareth, G. Generating 3D hyperspectral information\
    \ with lightweight UAV snapshot cameras\nfor vegetation monitoring: From camera\
    \ calibration to quality assurance. ISPRS J. Photogramm. Remote Sens. 2015, 108,\
    \ 245–259.\n[CrossRef]\n13.\nQuemada, M.; Gabriel, J.L.; Zarco-Tejada, P. Airborne\
    \ hyperspectral images and ground-level optical sensors as assessment tools\n\
    for maize nitrogen fertilization. Remote Sens. 2014, 6, 2940–2962. [CrossRef]\n\
    14.\nHonkavaara, E.; Saari, H.; Kaivosoja, J.; Pölönen, I.; Hakala, T.; Litkey,\
    \ P.; Mäkynen, J.; Pesonen, L. Processing and Assessment of\nSpectrometric, Stereoscopic\
    \ Imagery Collected Using a Lightweight UAV Spectral Camera for Precision Agriculture.\
    \ Remote Sens.\n2013, 5, 5006–5039. [CrossRef]\n15.\nZarco-Tejada, P.J.; González-Dugo,\
    \ V.; Berni, J.A. Fluorescence, temperature and narrow-band indices acquired from\
    \ a UAV\nplatform for water stress detection using a micro-hyperspectral imager\
    \ and a thermal camera. Remote Sens. Environ. 2012, 117,\n322–337. [CrossRef]\n\
    16.\nMaimaitiyiming, M.; Sagan, V.; Sidike, P.; Maimaitijiang, M.; Miller, A.J.;\
    \ Kwasniewski, M. Leveraging Very-High Spatial\nResolution Hyperspectral and Thermal\
    \ UAV Imageries for Characterizing Diurnal Indicators of Grapevine Physiology.\
    \ Remote\nSens. 2020, 12, 3216. [CrossRef]\n17.\nKumar, A.; Lee, W.S.; Ehsani,\
    \ R.J.; Albrigo, L.G.; Yang, C.; Mangan, R.L. Citrus greening disease detection\
    \ using aerial hyperspec-\ntral and multispectral imaging techniques. J. Appl.\
    \ Remote Sens. 2012, 6, 063542.\n18.\nNguyen, C.; Sagan, V.; Maimaitiyiming, M.;\
    \ Maimaitijiang, M.; Bhadra, S.; Kwasniewski, M.T. Early Detection of Plant Viral\n\
    Disease Using Hyperspectral Imaging and Deep Learning. Sensors 2021, 21, 742.\
    \ [CrossRef]\n19.\nKanning, M.; Kühling, I.; Trautz, D.; Jarmer, T. High-resolution\
    \ UAV-based hyperspectral imagery for LAI and chlorophyll\nestimations from wheat\
    \ for yield prediction. Remote Sens. 2018, 10, 2000. [CrossRef]\n20.\nFeng, L.;\
    \ Zhang, Z.; Ma, Y.; Du, Q.; Williams, P.; Drewry, J.; Luck, B. Alfalfa Yield\
    \ Prediction Using UAV-Based Hyperspectral\nImagery and Ensemble Learning. Remote\
    \ Sens. 2020, 12, 2028. [CrossRef]\n21.\nZhang, X.; Zhao, J.; Yang, G.; Liu, J.;\
    \ Cao, J.; Li, C.; Zhao, X.; Gai, J. Establishment of Plot-Yield Prediction Models\
    \ in Soybean\nBreeding Programs Using UAV-Based Hyperspectral Remote Sensing.\
    \ Remote Sens. 2019, 11, 2752. [CrossRef]\n22.\nZhang, Y.; Qin, Q.; Ren, H.; Sun,\
    \ Y.; Li, M.; Zhang, T.; Ren, S. Optimal Hyperspectral Characteristics Determination\
    \ for Winter\nWheat Yield Prediction. Remote Sens. 2018, 10, 2015. [CrossRef]\n\
    23.\nHughes, G. On the mean accuracy of statistical pattern recognizers. IEEE\
    \ Trans. Inf. Theory 1968, 14, 55–63. [CrossRef]\n24.\nBhadra, S.; Sagan, V.;\
    \ Maimaitijiang, M.; Maimaitiyiming, M.; Newcomb, M.; Shakoor, N.; Mockler, T.C.\
    \ Quantifying Leaf\nChlorophyll Concentration of Sorghum from Hyperspectral Data\
    \ Using Derivative Calculus and Machine Learning. Remote Sens.\n2020, 12, 2082.\
    \ [CrossRef]\n25.\nMaimaitiyiming, M.; Sagan, V.; Sidike, P.; Kwasniewski, M.T.\
    \ Dual Activation Function-Based Extreme Learning Machine (ELM)\nfor Estimating\
    \ Grapevine Berry Yield and Quality. Remote Sens. 2019, 11, 740. [CrossRef]\n\
    26.\nBravo, C.; Moshou, D.; West, J.; McCartney, A.; Ramon, H. Early disease detection\
    \ in wheat ﬁelds using spectral reﬂectance.\nBiosyst. Eng. 2003, 84, 137–145.\
    \ [CrossRef]\nSensors 2023, 23, 1827\n35 of 38\n27.\nXie, C.; He, Y. Spectrum\
    \ and image texture features analysis for early blight disease detection on eggplant\
    \ leaves. Sensors 2016,\n16, 676. [CrossRef]\n28.\nHuang, L.; Zhang, H.; Ruan,\
    \ C.; Huang, W.; Hu, T.; Zhao, J. Detection of scab in wheat ears using in situ\
    \ hyperspectral data and\nsupport vector machine optimized by genetic algorithm.\
    \ Int. J. Agric. Biol. Eng. 2020, 13, 182–188. [CrossRef]\n29.\nLiu, F.; Xiao,\
    \ Z. Disease Spots Identiﬁcation of Potato Leaves in Hyperspectral Based on Locally\
    \ Adaptive 1D-CNN. In Proceedings\nof the 2020 IEEE International Conference on\
    \ Artiﬁcial Intelligence and Computer Applications (ICAICA), Dalian, China, 27–29\n\
    June 2020; pp. 355–358.\n30.\nJin, X.; Jie, L.; Wang, S.; Qi, H.J.; Li, S.W. Classifying\
    \ wheat hyperspectral pixels of healthy heads and Fusarium head blight\ndisease\
    \ using a deep neural network in the wild ﬁeld. Remote Sens. 2018, 10, 395. [CrossRef]\n\
    31.\nHruška, J.; Adão, T.; Pádua, L.; Marques, P.; Peres, E.; Sousa, A.; Morais,\
    \ R.; Sousa, J.J. Deep Learning-Based Methodological\nApproach for Vineyard Early\
    \ Disease Detection Using Hyperspectral Data. In Proceedings of the IGARSS 2018-2018\
    \ IEEE\nInternational Geoscience and Remote Sensing Symposium, Valencia, Spain,\
    \ 22–37 June 2018; pp. 9063–9066.\n32.\nWu, D.; Sun, D.-W. Advanced applications\
    \ of hyperspectral imaging technology for food quality and safety analysis and\n\
    assessment: A review—Part I: Fundamentals. Innov. Food Sci. Emerg. Technol. 2013,\
    \ 19, 1–14. [CrossRef]\n33.\nMulla, D.J. Twenty ﬁve years of remote sensing in\
    \ precision agriculture: Key advances and remaining knowledge gaps. Biosyst.\n\
    Eng. 2013, 114, 358–371. [CrossRef]\n34.\nGómez-Candón, D.; Virlet, N.; Labbé,\
    \ S.; Jolivot, A.; Regnard, J.-L. Field phenotyping of water stress at tree scale\
    \ by UAV-sensed\nimagery: New insights for thermal acquisition and calibration.\
    \ Precis. Agric. 2016, 17, 786–800. [CrossRef]\n35.\nZúñiga Espinoza, C.; Khot,\
    \ L.R.; Sankaran, S.; Jacoby, P.W. High resolution multispectral and thermal remote\
    \ sensing-based water\nstress assessment in subsurface irrigated grapevines. Remote\
    \ Sens. 2017, 9, 961. [CrossRef]\n36.\nPark, S.; Ryu, D.; Fuentes, S.; Chung,\
    \ H.; Hernández-Montes, E.; O’Connell, M. Adaptive estimation of crop water stress\
    \ in\nnectarine and peach orchards using high-resolution imagery from an unmanned\
    \ aerial vehicle (UAV). Remote Sens. 2017, 9, 828.\n[CrossRef]\n37.\nGonzalez-Dugo,\
    \ V.; Goldhamer, D.; Zarco-Tejada, P.J.; Fereres, E. Improving the precision of\
    \ irrigation in a pistachio farm using\nan unmanned airborne thermal system. Irrig.\
    \ Sci 2015, 33, 43–52. [CrossRef]\n38.\nLudovisi, R.; Tauro, F.; Salvati, R.;\
    \ Khoury, S.; Mugnozza Scarascia, G.; Harfouche, A. UAV-based thermal imaging\
    \ for high-\nthroughput ﬁeld phenotyping of black poplar response to drought.\
    \ Front. Plant Sci. 2017, 8, 1681. [CrossRef]\n39.\nSagan, V.; Maimaitijiang,\
    \ M.; Sidike, P.; Eblimit, K.; Peterson, K.T.; Hartling, S.; Esposito, F.; Khanal,\
    \ K.; Newcomb, M.;\nPauli, D.; et al. UAV-Based High Resolution Thermal Imaging\
    \ for Vegetation Monitoring, and Plant Phenotyping Using ICI 8640\nP, FLIR Vue\
    \ Pro R 640, and thermoMap Cameras. Remote Sens. 2019, 11, 330. [CrossRef]\n40.\n\
    Da Luz, B.R.; Crowley, J.K. Spectral reﬂectance and emissivity features of broad\
    \ leaf plants: Prospects for remote sensing in the\nthermal infrared (8.0–14.0\
    \ µm). Remote Sens. Environ. 2007, 109, 393–405. [CrossRef]\n41.\nMaimaitijiang,\
    \ M.; Ghulam, A.; Sidike, P.; Hartling, S.; Maimaitiyiming, M.; Peterson, K.;\
    \ Shavers, E.; Fishman, J.; Peterson, J.;\nKadam, S. Unmanned Aerial System (UAS)-based\
    \ phenotyping of soybean using multi-sensor data fusion and extreme learning\n\
    machine. ISPRS J. Photogramm. Remote Sens. 2017, 134, 43–58. [CrossRef]\n42.\n\
    García, M.; Saatchi, S.; Ustin, S.; Balzter, H. Modelling forest canopy height\
    \ by integrating airborne LiDAR samples with satellite\nRadar and multispectral\
    \ imagery. Int. J. Appl. Earth Obs. Geoinf. 2018, 66, 159–173. [CrossRef]\n43.\n\
    Shi, Y.; Wang, T.; Skidmore, A.K.; Heurich, M. Improving LiDAR-based tree species\
    \ mapping in Central European mixed forests\nusing multi-temporal digital aerial\
    \ colour-infrared photographs. Int. J. Appl. Earth Obs. Geoinf. 2020, 84, 101970.\
    \ [CrossRef]\n44.\nBlomley, R.; Hovi, A.; Weinmann, M.; Hinz, S.; Korpela, I.;\
    \ Jutzi, B. Tree species classiﬁcation using within crown localization of\nwaveform\
    \ LiDAR attributes. ISPRS J. Photogramm. Remote Sens. 2017, 133, 142–156. [CrossRef]\n\
    45.\nQin, Y.; Li, S.; Vu, T.-T.; Niu, Z.; Ban, Y. Synergistic application of geometric\
    \ and radiometric features of LiDAR data for urban\nland cover mapping. Opt Express\
    \ 2015, 23, 13761–13775. [CrossRef] [PubMed]\n46.\nAndújar, D.; Moreno, H.; Bengochea-Guevara,\
    \ J.M.; de Castro, A.; Ribeiro, A. Aerial imagery or on-ground detection? An\n\
    economic analysis for vineyard crops. Comput. Electron. Agric. 2019, 157, 351–358.\
    \ [CrossRef]\n47.\nWang, D.; Xin, X.; Shao, Q.; Brolly, M.; Zhu, Z.; Chen, J.\
    \ Modeling Aboveground Biomass in Hulunber Grassland Ecosystem by\nUsing Unmanned\
    \ Aerial Vehicle Discrete Lidar. Sensors 2017, 17, 180. [CrossRef]\n48.\nSankey,\
    \ T.; Donager, J.; McVay, J.; Sankey, J.B. UAV lidar and hyperspectral fusion\
    \ for forest monitoring in the southwestern USA.\nRemote Sens. Environ. 2017,\
    \ 195, 30–43. [CrossRef]\n49.\nQin, H.; Zhou, W.; Yao, Y.; Wang, W. Individual\
    \ tree segmentation and tree species classiﬁcation in subtropical broadleaf forests\n\
    using UAV-based LiDAR, hyperspectral, and ultrahigh-resolution RGB data. Remote\
    \ Sens. Environ. 2022, 280, 113143. [CrossRef]\n50.\nYu, R.; Luo, Y.; Zhou, Q.;\
    \ Zhang, X.; Wu, D.; Ren, L. A machine learning algorithm to detect pine wilt\
    \ disease using UAV-based\nhyperspectral imagery and LiDAR data at the tree level.\
    \ Int. J. Appl. Earth Obs. Geoinf. 2021, 101, 102363. [CrossRef]\n51.\nDilmurat,\
    \ K.; Sagan, V.; Maimaitijiang, M.; Moose, S.; Fritschi, F.B. Estimating Crop\
    \ Seed Composition Using Machine Learning\nfrom Multisensory UAV Data. Remote\
    \ Sens. 2022, 14, 4786. [CrossRef]\n52.\nJones, D.B. Factors for Converting Percentages\
    \ of Nitrogen in Foods and Feeds into Percentages of Proteins; US Department of\
    \ Agriculture:\nWashington, DC, USA, 1931.\n53.\nBrede, B.; Lau, A.; Bartholomeus,\
    \ H.M.; Kooistra, L. Comparing RIEGL RiCOPTER UAV LiDAR derived canopy height\
    \ and DBH\nwith terrestrial LiDAR. Sensors 2017, 17, 2371. [CrossRef]\nSensors\
    \ 2023, 23, 1827\n36 of 38\n54.\nGómez-Chova, L.; Alonso, L.; Guanter, L.; Camps-Valls,\
    \ G.; Calpe, J.; Moreno, J. Correction of systematic spatial noise in\npush-broom\
    \ hyperspectral sensors: Application to CHRIS/PROBA images. Appl. Opt. 2008, 47,\
    \ F46–F60. [CrossRef] [PubMed]\n55.\nBarreto, M.A.P.; Johansen, K.; Angel, Y.;\
    \ McCabe, M.F. Radiometric assessment of a UAV-based push-broom hyperspectral\
    \ camera.\nSensors 2019, 19, 4699. [CrossRef] [PubMed]\n56.\nLiu, Y.; Wang, T.;\
    \ Ma, L.; Wang, N. Spectral calibration of hyperspectral data observed from a\
    \ hyperspectrometer loaded on an\nunmanned aerial vehicle platform. IEEE J. Sel.\
    \ Top. Appl. Earth Obs. Remote Sens. 2014, 7, 2630–2638. [CrossRef]\n57.\nFalco,\
    \ G.; Pini, M.; Marucco, G. Loose and tight GNSS/INS integrations: Comparison\
    \ of performance assessed in real urban\nscenarios. Sensors 2017, 17, 255. [CrossRef]\
    \ [PubMed]\n58.\nDong, Y.; Wang, D.; Zhang, L.; Li, Q.; Wu, J. Tightly coupled\
    \ GNSS/INS integration with robust sequential kalman ﬁlter for\naccurate vehicular\
    \ navigation. Sensors 2020, 20, 561. [CrossRef]\n59.\nHan, Y.; Choi, J.; Jung,\
    \ J.; Chang, A.; Oh, S.; Yeom, J. Automated coregistration of multisensor orthophotos\
    \ generated from\nunmanned aerial vehicle platforms. J. Sens. 2019, 2019, 2962734.\
    \ [CrossRef]\n60.\nMaimaitijiang, M.; Sagan, V.; Bhadra, S.; Nguyen, C.; Mockler,\
    \ T.; Shakoor, N. A fully automated and fast approach for canopy\ncover estimation\
    \ using super high-resolution remote sensing imagery. ISPRS Ann. Photogramm. Remote\
    \ Sens. Spat. Inf. Sci. 2021, 5,\n219–226. [CrossRef]\n61.\nVassilvitskii, S.;\
    \ Arthur, D. k-means++: The advantages of careful seeding. In Proceedings of the\
    \ Eighteenth Annual ACM-SIAM\nSymposium on Discrete Algorithms, Miami, FL, USA,\
    \ 22–24 January 2006; pp. 1027–1035.\n62.\nRaschka, S. Python Machine Learning;\
    \ Packt Publishing Ltd.: Birmingham, UK, 2015.\n63.\nGosselin, N.; Sagan, V.;\
    \ Maimaitiyiming, M.; Fishman, J.; Belina, K.; Podleski, A.; Maimaitijiang, M.;\
    \ Bashir, A.; Balakrishna, J.;\nDixon, A. Using Visual Ozone Damage Scores and\
    \ Spectroscopy to Quantify Soybean Responses to Background Ozone. Remote\nSens.\
    \ 2020, 12, 93. [CrossRef]\n64.\nMaimaitiyiming, M.; Ghulam, A.; Bozzolo, A.;\
    \ Wilkins, J.L.; Kwasniewski, M.T. Early Detection of Plant Physiological Responses\n\
    to Different Levels of Water Stress Using Reﬂectance Spectroscopy. Remote Sens.\
    \ 2017, 9, 745. [CrossRef]\n65.\nDilmurat, K.; Sagan, V.; Moose, S. AI-driven\
    \ maize yield forecasting using unmanned aerial vehicle-based hyperspectral and\n\
    lidar data fusion. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. 2022, 5,\
    \ 193–199. [CrossRef]\n66.\nMaimaitijiang, M.; Sagan, V.; Erkbol, H.; Adrian,\
    \ J.; Newcomb, M.; LeBauer, D.; Pauli, D.; Shakoor, N.; Mockler, T.C. UAV-based\n\
    sorghum growth monitoring: A comparative analysis of lidar and photogrammetry.\
    \ ISPRS Ann. Photogramm. Remote Sens. Spat.\nInf. Sci. 2020, 5, 489–496. [CrossRef]\n\
    67.\nCortes, C.; Vapnik, V. Support-vector networks. Mach. Learn. 1995, 20, 273–297.\
    \ [CrossRef]\n68.\nBreiman, L. Random forests. Mach. Learn. 2001, 45, 5–32. [CrossRef]\n\
    69.\nKohavi, R. A study of cross-validation and bootstrap for accuracy estimation\
    \ and model selection. In Proceedings of the IJCAI,\nMontreal, QC, Canada, 20–25\
    \ August 1995; pp. 1137–1145.\n70.\nGitelson, A.A.; Gritz, Y.; Merzlyak, M.N.\
    \ Relationships between leaf chlorophyll content and spectral reﬂectance and algorithms\n\
    for non-destructive chlorophyll assessment in higher plant leaves. J. Plant Physiol.\
    \ 2003, 160, 271–282. [CrossRef] [PubMed]\n71.\nGitelson, A.; Merzlyak, M.N. Quantitative\
    \ estimation of chlorophyll-a using reﬂectance spectra: Experiments with autumn\n\
    chestnut and maple leaves. J. Photochem. Photobiol. B Biol. 1994, 22, 247–252.\
    \ [CrossRef]\n72.\nRondeaux, G.; Steven, M.; Baret, F. Optimization of soil-adjusted\
    \ vegetation indices. Remote Sens. Environ. 1996, 55, 95–107.\n[CrossRef]\n73.\n\
    Zarco-Tejada, P.J.; Berjón, A.; López-Lozano, R.; Miller, J.R.; Martín, P.; Cachorro,\
    \ V.; González, M.; De Frutos, A. Assessing\nvineyard condition with hyperspectral\
    \ indices: Leaf and canopy reﬂectance simulation in a row-structured discontinuous\
    \ canopy.\nRemote Sens. Environ. 2005, 99, 271–287. [CrossRef]\n74.\nPenuelas,\
    \ J.; Baret, F.; Filella, I. Semi-empirical indices to assess carotenoids/chlorophyll\
    \ a ratio from leaf spectral reﬂectance.\nPhotosynthetica 1995, 31, 221–230.\n\
    75.\nHaboudane, D.; Miller, J.R.; Tremblay, N.; Zarco-Tejada, P.J.; Dextraze,\
    \ L. Integrated narrow-band vegetation indices for prediction\nof crop chlorophyll\
    \ content for application to precision agriculture. Remote Sens. Environ. 2002,\
    \ 81, 416–426. [CrossRef]\n76.\nBausch, W.C.; Duke, H.R. Remote Sensing of Plant\
    \ Nitrogen Status in Corn. Trans. ASAE 1996, 39, 1869–1875. [CrossRef]\n77.\n\
    Haboudane, D.; Miller, J.R.; Pattey, E.; Zarco-Tejada, P.J.; Strachan, I.B. Hyperspectral\
    \ vegetation indices and novel algorithms for\npredicting green LAI of crop canopies:\
    \ Modeling and validation in the context of precision agriculture. Remote Sens.\
    \ Environ.\n2004, 90, 337–352. [CrossRef]\n78.\nGamon, J.A.; Peñuelas, J.; Field,\
    \ C.B. A narrow-waveband spectral index that tracks diurnal changes in photosynthetic\
    \ efﬁciency.\nRemote Sens. Environ. 1992, 41, 35–44. [CrossRef]\n79.\nChappelle,\
    \ E.W.; Kim, M.S.; McMurtrey, J.E. Ratio analysis of reﬂectance spectra (RARS):\
    \ An algorithm for the remote estimation\nof the concentrations of chlorophyll\
    \ A, chlorophyll B, and carotenoids in soybean leaves. Remote Sens. Environ. 1992,\
    \ 39, 239–247.\n[CrossRef]\n80.\nBlackburn, G.A. Spectral indices for estimating\
    \ photosynthetic pigment concentrations: A test using senescent tree leaves. Int.\
    \ J.\nRemote Sens. 1998, 19, 657–675. [CrossRef]\n81.\nPeñuelas, J.; Gamon, J.A.;\
    \ Fredeen, A.L.; Merino, J.; Field, C.B. Reﬂectance indices associated with physiological\
    \ changes in\nnitrogen- and water-limited sunﬂower leaves. Remote Sens. Environ.\
    \ 1994, 48, 135–146. [CrossRef]\nSensors 2023, 23, 1827\n37 of 38\n82.\nMetternicht,\
    \ G. Vegetation indices derived from high-resolution airborne videography for\
    \ precision crop management. Int. J.\nRemote Sens. 2003, 24, 2855–2877. [CrossRef]\n\
    83.\nSchell, J.; Deering, D. Monitoring vegetation systems in the Great Plains\
    \ with ERTS. NASA Spec. Publ. 1973, 351, 309.\n84.\nGitelson, A.A.; Merzlyak,\
    \ M.N. Remote estimation of chlorophyll content in higher plant leaves. Int. J.\
    \ Remote Sens. 1997, 18,\n2691–2697. [CrossRef]\n85.\nJordan, C.F. Derivation\
    \ of leaf-area index from quality of light on the forest ﬂoor. Ecology 1969, 50,\
    \ 663–666. [CrossRef]\n86.\nBarnes, E.; Clarke, T.; Richards, S.; Colaizzi, P.;\
    \ Haberland, J.; Kostrzewski, M.; Waller, P.; Choi, C.; Riley, E.; Thompson, T.\
    \ Coincident\ndetection of crop water stress, nitrogen status and canopy density\
    \ using ground based multispectral data. In Proceedings of the\nFifth International\
    \ Conference on Precision Agriculture, Bloomington, MN, USA, 16–19 July 2000.\n\
    87.\nBroge, N.H.; Leblanc, E. Comparing prediction power and stability of broadband\
    \ and hyperspectral vegetation indices for\nestimation of green leaf area index\
    \ and canopy chlorophyll density. Remote Sens. Environ. 2001, 76, 156–172. [CrossRef]\n\
    88.\nZarco-Tejada, P.J.; Miller, J.R.; Mohammed, G.H.; Noland, T.L.; Sampson,\
    \ P.H. Chlorophyll ﬂuorescence effects on vegetation\napparent reﬂectance: II.\
    \ Laboratory and airborne canopy-level measurements with hyperspectral data. Remote\
    \ Sens. Environ.\n2000, 74, 596–608. [CrossRef]\n89.\nZarco-Tejada, P.J.; Miller,\
    \ J.R.; Mohammed, G.H.; Noland, T.L. Chlorophyll ﬂuorescence effects on vegetation\
    \ apparent reﬂectance:\nI. Leaf-level measurements and model simulation. Remote\
    \ Sens. Environ. 2000, 74, 582–595. [CrossRef]\n90.\nDobrowski, S.; Pushnik, J.;\
    \ Zarco-Tejada, P.J.; Ustin, S. Simple reﬂectance indices track heat and water\
    \ stress-induced changes in\nsteady-state chlorophyll ﬂuorescence at the canopy\
    \ scale. Remote Sens. Environ. 2005, 97, 403–414. [CrossRef]\n91.\nSims, D.A.;\
    \ Gamon, J.A. Relationships between leaf pigment content and spectral reﬂectance\
    \ across a wide range of species, leaf\nstructures and developmental stages. Remote\
    \ Sens. Environ. 2002, 81, 337–354. [CrossRef]\n92.\nBarnes, J.D.; Balaguer, L.;\
    \ Manrique, E.; Elvira, S.; Davison, A.W. A reappraisal of the use of DMSO for\
    \ the extraction and\ndetermination of chlorophylls a and b in lichens and higher\
    \ plants. Environ. Exp. Bot. 1992, 32, 85–100. [CrossRef]\n93.\nMerton, R. Monitoring\
    \ community hysteresis using spectral shift analysis and the red-edge vegetation\
    \ stress index. In Proceedings\nof the Seventh Annual JPL Airborne Earth Science\
    \ Workshop, Pasadena, CA, USA, 12–16 January 1998; pp. 12–16.\n94.\nPeñuelas,\
    \ J.; Filella, I.; Biel, C.; Serrano, L.; Save, R. The reﬂectance at the 950–970\
    \ nm region as an indicator of plant water status.\nInt. J. Remote Sens. 1993,\
    \ 14, 1887–1905. [CrossRef]\n95.\nBabar, M.; Reynolds, M.; Van Ginkel, M.; Klatt,\
    \ A.; Raun, W.; Stone, M. Spectral reﬂectance to estimate genetic variation for\n\
    in-season biomass, leaf chlorophyll, and canopy temperature in wheat. Crop Sci.\
    \ 2006, 46, 1046–1057. [CrossRef]\n96.\nElsayed, S.; Rischbeck, P.; Schmidhalter,\
    \ U. Comparing the performance of active and passive reﬂectance sensors to assess\
    \ the\nnormalized relative canopy temperature and grain yield of drought-stressed\
    \ barley cultivars. Field Crops Res. 2015, 177, 148–160.\n[CrossRef]\n97.\nYu,\
    \ X.; Wu, X.; Luo, C.; Ren, P. Deep learning in remote sensing scene classiﬁcation:\
    \ A data augmentation enhanced convolutional\nneural network framework. GISci.\
    \ Remote Sens. 2017, 54, 741–758. [CrossRef]\n98.\nLi, W.; Chen, C.; Zhang, M.;\
    \ Li, H.; Du, Q. Data augmentation for hyperspectral image classiﬁcation with\
    \ deep CNN. IEEE Geosci.\nRemote Sens. Lett. 2018, 16, 593–597. [CrossRef]\n99.\n\
    Glorot, X.; Bengio, Y. Understanding the difﬁculty of training deep feedforward\
    \ neural networks. In Proceedings of the Thirteenth\nInternational Conference\
    \ on Artiﬁcial Intelligence And Statistics, Sardinia, Italy, 13–15 May 2010; pp.\
    \ 249–256.\n100. Nair, V.; Hinton, G.E. Rectiﬁed linear units improve restricted\
    \ boltzmann machines. In Proceedings of the ICML, Haifa, Israel,\n21–24 June 2010.\n\
    101. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov,\
    \ R. Dropout: A simple way to prevent neural networks\nfrom overﬁtting. J. Mach.\
    \ Learn. Res. 2014, 15, 1929–1958.\n102. Huber, P.J. Robust Estimation of a Location\
    \ Parameter. Ann. Math. Stat. 1964, 35, 73–101, 129. [CrossRef]\n103. Sagan, V.;\
    \ Maimaitijiang, M.; Bhadra, S.; Maimaitiyiming, M.; Brown, D.R.; Sidike, P.;\
    \ Fritschi, F.B. Field-scale crop yield prediction\nusing multi-temporal WorldView-3\
    \ and PlanetScope satellite data and deep learning. ISPRS J. Photogramm. Remote\
    \ Sens. 2021,\n174, 265–281. [CrossRef]\n104. Fan, J.; Zhou, J.; Wang, B.; de\
    \ Leon, N.; Kaeppler, S.M.; Lima, D.C.; Zhang, Z. Estimation of Maize Yield and\
    \ Flowering Time\nUsing Multi-Temporal UAV-Based Hyperspectral Data. Remote Sens.\
    \ 2022, 14, 3052. [CrossRef]\n105. van Klompenburg, T.; Kassahun, A.; Catal, C.\
    \ Crop yield prediction using machine learning: A systematic literature review.\n\
    Comput. Electron. Agric. 2020, 177, 105709. [CrossRef]\n106. Maresma, Á.; Ariza,\
    \ M.; Martínez, E.; Lloveras, J.; Martínez-Casasnovas, J.A. Analysis of Vegetation\
    \ Indices to Determine Nitrogen\nApplication and Yield Prediction in Maize (Zea\
    \ mays L.) from a Standard UAV Service. Remote Sens. 2016, 8, 973. [CrossRef]\n\
    107. López-Calderón, M.J.; Estrada-Ávalos, J.; Rodríguez-Moreno, V.M.; Mauricio-Ruvalcaba,\
    \ J.E.; Martínez-Sifuentes, A.R.;\nDelgado-Ramírez, G.; Miguel-Valle, E. Estimation\
    \ of Total Nitrogen Content in Forage Maize (Zea mays L.) Using Spectral Indices:\n\
    Analysis by Random Forest. Agriculture 2020, 10, 451. [CrossRef]\n108. Zhu, Y.;\
    \ Zhao, C.; Yang, H.; Yang, G.; Han, L.; Li, Z.; Feng, H.; Xu, B.; Wu, J.; Lei,\
    \ L. Estimation of maize above-ground biomass\nbased on stem-leaf separation strategy\
    \ integrated with LiDAR and optical remote sensing data. PeerJ 2019, 7, e7593.\
    \ [CrossRef]\n109. Meiyan, S.; Mengyuan, S.; Qizhou, D.; Xiaohong, Y.; Baoguo,\
    \ L.; Yuntao, M. Estimating the maize above-ground biomass by\nconstructing the\
    \ tridimensional concept model based on UAV-based digital and multi-spectral images.\
    \ Field Crops Res. 2022,\n282, 108491. [CrossRef]\nSensors 2023, 23, 1827\n38\
    \ of 38\n110. Thenkabail, P.S.; Lyon, J.G. Hyperspectral Remote Sensing of Vegetation;\
    \ CRC Press: Boca Raton, FL, USA, 2016.\n111. Xue, J.; Su, B. Significant remote\
    \ sensing vegetation indices: A review of developments and applications. J. Sens.\
    \ 2017, 2017, 1353691.\n[CrossRef]\n112. Tilly, N.; Aasen, H.; Bareth, G. Fusion\
    \ of Plant Height and Vegetation Indices for the Estimation of Barley Biomass.\
    \ Remote Sens.\n2015, 7, 11449–11480. [CrossRef]\n113. Ahamed, T.; Tian, L.; Zhang,\
    \ Y.; Ting, K.C. A review of remote sensing methods for biomass feedstock production.\
    \ Biomass\nBioenergy 2011, 35, 2455–2469. [CrossRef]\n114. Freeman, K.W.; Girma,\
    \ K.; Arnall, D.B.; Mullen, R.W.; Martin, K.L.; Teal, R.K.; Raun, W.R. By-plant\
    \ prediction of corn forage\nbiomass and nitrogen uptake at various growth stages\
    \ using remote sensing and plant height. Agron. J. 2007, 99, 530–536.\n[CrossRef]\n\
    115. Li, B.; Xu, X.; Zhang, L.; Han, J.; Bian, C.; Li, G.; Liu, J.; Jin, L. Above-ground\
    \ biomass estimation and yield prediction in potato by\nusing UAV-based RGB and\
    \ hyperspectral imaging. ISPRS J. Photogramm. Remote Sens. 2020, 162, 161–172.\
    \ [CrossRef]\n116. Ciurczak, E.W.; Igne, B.; Workman, J., Jr.; Burns, D.A. Handbook\
    \ of Near-Infrared Analysis; CRC Press: Boca Raton, FL, USA, 2021.\n117. Slaton,\
    \ M.R.; Raymond Hunt Jr, E.; Smith, W.K. Estimating near-infrared leaf reﬂectance\
    \ from leaf structural characteristics. Am. J.\nBot. 2001, 88, 278–284. [CrossRef]\
    \ [PubMed]\n118. Gates, D.M.; Keegan, H.J.; Schleter, J.C.; Weidner, V.R. Spectral\
    \ properties of plants. Appl Opt. 1965, 4, 11–20. [CrossRef]\n119. Curran, P.J.\
    \ Remote sensing of foliar chemistry. Remote Sens. Environ. 1989, 30, 271–278.\
    \ [CrossRef]\n120. Thenkabail, P.S.; Smith, R.B.; De Pauw, E. Hyperspectral vegetation\
    \ indices and their relationships with agricultural crop\ncharacteristics. Remote\
    \ Sens. Environ. 2000, 71, 158–182. [CrossRef]\n121. Höﬂe, B. Radiometric correction\
    \ of terrestrial LiDAR point cloud data for individual maize plant detection.\
    \ IEEE Geosci. Remote\nSens. Lett. 2013, 11, 94–98. [CrossRef]\n122. Wang, L.;\
    \ Chen, S.; Li, D.; Wang, C.; Jiang, H.; Zheng, Q.; Peng, Z. Estimation of paddy\
    \ rice nitrogen content and accumulation\nboth at leaf and plant levels from UAV\
    \ hyperspectral imagery. Remote Sens. 2021, 13, 2956. [CrossRef]\n123. Yin, S.;\
    \ Zhou, K.; Cao, L.; Shen, X. Estimating the Horizontal and Vertical Distributions\
    \ of Pigments in Canopies of Ginkgo\nPlantation Based on UAV-Borne LiDAR, Hyperspectral\
    \ Data by Coupling PROSAIL Model. Remote Sens. 2022, 14, 715. [CrossRef]\n124.\
    \ Xu, J.-L.; Gobrecht, A.; Héran, D.; Gorretta, N.; Coque, M.; Gowen, A.A.; Bendoula,\
    \ R.; Sun, D.-W. A polarized hyperspectral\nimaging system for in vivo detection:\
    \ Multiple applications in sunﬂower leaf analysis. Comput. Electron. Agric. 2019,\
    \ 158, 258–270.\n[CrossRef]\n125. Moudrý, V.; Moudrá, L.; Barták, V.; Bejˇcek,\
    \ V.; Gdulová, K.; Hendrychová, M.; Moravec, D.; Musil, P.; Rocchini, D.; Št’astný,\
    \ K.\nThe role of the vegetation structure, primary productivity and senescence\
    \ derived from airborne LiDAR and hyperspectral data\nfor birds diversity and\
    \ rarity on a restored site. Landsc. Urban Plan. 2021, 210, 104064. [CrossRef]\n\
    126. Neupane, K.; Baysal-Gurel, F. Automatic identiﬁcation and monitoring of plant\
    \ diseases using unmanned aerial vehicles: A\nreview. Remote Sens. 2021, 13, 3841.\
    \ [CrossRef]\n127. Zhang, L.; Niu, Y.; Zhang, H.; Han, W.; Li, G.; Tang, J.; Peng,\
    \ X. Maize canopy temperature extracted from UAV thermal and RGB\nimagery and\
    \ its application in water stress monitoring. Front. Plant Sci. 2019, 10, 1270.\
    \ [CrossRef]\n128. Hartling, S.; Sagan, V.; Maimaitijiang, M. Urban tree species\
    \ classiﬁcation using UAV-based multi-sensor data fusion and machine\nlearning.\
    \ GIScience Remote Sens. 2021, 58, 1250–1275. [CrossRef]\n129. Li, Y.; Zhang,\
    \ H.; Shen, Q. Spectral–spatial classiﬁcation of hyperspectral imagery with 3D\
    \ convolutional neural network. Remote\nSens. 2017, 9, 67. [CrossRef]\n130. He,\
    \ M.; Li, B.; Chen, H. Multi-scale 3D deep convolutional neural network for hyperspectral\
    \ image classiﬁcation. In Proceedings\nof the 2017 IEEE International Conference\
    \ on Image Processing (ICIP), Beijing, China, 17–20 September 2017; pp. 3904–3908.\n\
    131. Carrio, A.; Sampedro, C.; Rodriguez-Ramos, A.; Campoy, P. A review of deep\
    \ learning methods and applications for unmanned\naerial vehicles. J. Sens. 2017,\
    \ 2017, 3296874. [CrossRef]\n132. Osco, L.P.; Junior, J.M.; Ramos, A.P.M.; de\
    \ Castro Jorge, L.A.; Fatholahi, S.N.; de Andrade Silva, J.; Matsubara, E.T.;\
    \ Pistori, H.;\nGonçalves, W.N.; Li, J. A review on deep learning in UAV remote\
    \ sensing. Int. J. Appl. Earth Obs. Geoinf. 2021, 102, 102456.\n[CrossRef]\n133.\
    \ Feng, L.; Zhang, Z.; Ma, Y.; Sun, Y.; Du, Q.; Williams, P.; Drewry, J.; Luck,\
    \ B. Multitask Learning of Alfalfa Nutritive Value From\nUAV-Based Hyperspectral\
    \ Images. IEEE Geosci. Remote Sens. Lett. 2021, 19, 1–5. [CrossRef]\n134. Sun,\
    \ X.; Panda, R.; Feris, R.; Saenko, K. Adashare: Learning what to share for efﬁcient\
    \ deep multi-task learning. Adv. Neural Inf.\nProcess. Syst. 2020, 33, 8728–8740.\n\
    135. Vandenhende, S.; Georgoulis, S.; Van Gansbeke, W.; Proesmans, M.; Dai, D.;\
    \ Van Gool, L. Multi-task learning for dense prediction\ntasks: A survey. IEEE\
    \ Trans. Pattern Anal. Mach. Intell. 2021, 44, 3614–3633. [CrossRef] [PubMed]\n\
    Disclaimer/Publisher’s Note: The statements, opinions and data contained in all\
    \ publications are solely those of the individual\nauthor(s) and contributor(s)\
    \ and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility\
    \ for any injury to\npeople or property resulting from any ideas, methods, instructions\
    \ or products referred to in the content.\n"
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  pdf_link: https://www.mdpi.com/1424-8220/23/4/1827/pdf?version=1676560193
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: UAV Multisensory Data Fusion and Multi-Task Deep Learning for High-Throughput
    Maize Phenotyping
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agriculture12121971
  analysis: '>'
  authors:
  - Edwin Pino-Vargas
  - Edgar Aurelio Taya-Acosta
  - Eusebio Ingol-Blanco
  - Alfonso F. Torres‐Rua
  citation_count: 5
  full_citation: '>'
  full_text: ">\nCitation: Pino-Vargas, E.;\nTaya-Acosta, E.; Ingol-Blanco, E.;\n\
    Torres-Rúa, A. Deep Machine\nLearning for Forecasting Daily\nPotential Evapotranspiration\
    \ in Arid\nRegions, Case: Atacama Desert\nHeader. Agriculture 2022, 12, 1971.\n\
    https://doi.org/10.3390/\nagriculture12121971\nAcademic Editor: Dongwei Gui\n\
    Received: 29 September 2022\nAccepted: 17 November 2022\nPublished: 22 November\
    \ 2022\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional claims\
    \ in\npublished maps and institutional afﬁl-\niations.\nCopyright:\n© 2022 by\
    \ the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access\
    \ article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative Commons\n\
    Attribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\n\
    agriculture\nArticle\nDeep Machine Learning for Forecasting Daily Potential\n\
    Evapotranspiration in Arid Regions, Case: Atacama\nDesert Header\nEdwin Pino-Vargas\
    \ 1,*\n, Edgar Taya-Acosta 2,*\n, Eusebio Ingol-Blanco 3\nand Alfonso Torres-Rúa\
    \ 4\n1\nDepartment of Civil Engineering, Jorge Basadre Grohmann National University,\
    \ Tacna 23000, Peru\n2\nDepartment of Computer Engineering and Systems, Jorge\
    \ Basadre Grohmann National University,\nTacna 23000, Peru\n3\nDepartment of Water\
    \ Resources, National Agrarian University La Molina, Lima 15012, Peru\n4\nUtah\
    \ Water Research Laboratory, Civil and Environmental Department, Utah State University,\n\
    Logan, UT 84322, USA\n*\nCorrespondence: epinov@unjbg.edu.pe (E.P.-V.); etayaa@unjbg.edu.pe\
    \ (E.T.-A.); Tel.: +51-952298638 (E.P.-V.);\n+51-958348747 (E.T.-A.)\nAbstract:\
    \ Accurately estimating and forecasting evapotranspiration is one of the most\
    \ important tasks\nto strengthen water resource management, especially in desert\
    \ areas such as La Yarada, Tacna, Peru,\na region located at the head of the Atacama\
    \ Desert. In this study, we used temperature, humidity,\nwind speed, air pressure,\
    \ and solar radiation from a local weather station to forecast potential\nevapotranspiration\
    \ (ETo) using machine learning. The Feedforward Neural Network (Multi-Layered\n\
    Perceptron) algorithm for prediction was used under two approaches: “direct” and\
    \ “indirect”. In the\nﬁrst one, the ETo is predicted based on historical records,\
    \ and the second one predicts the climate\nvariables upon which the ETo calculation\
    \ depends, for which the Penman-Monteith, Hargreaves-\nSamani, Ritchie, and Turc\
    \ equations were used. The results were evaluated using statistical criteria to\n\
    calculate errors, showing remarkable precision, predicting up to 300 days of ETo.\
    \ Comparing the\nperformance of the approaches and the machine learning used,\
    \ the results obtained indicate that,\ndespite the similar performance of the\
    \ two proposed approaches, the indirect approach provides\nbetter ETo forecasting\
    \ capabilities for longer time intervals than the direct approach, whose values\
    \ of\nthe corresponding metrics are MAE = 0.033, MSE = 0.002, RMSE = 0.043 and\
    \ RAE = 0.016.\nKeywords: evapotranspiration; forecasting; machine learning; deep\
    \ learning; arid zones\n1. Introduction\nNowadays, great efforts are made to achieve\
    \ an efﬁcient use of water in economic\nactivities, especially in agriculture;\
    \ activity that registers the highest water consumption\nin the world. Accurate\
    \ prediction of vegetation water consumption is important in the\nﬁelds of hydrology\
    \ and irrigation engineering [1]. Evapotranspiration (ET) is one of\nthe most\
    \ important components of the hydrological cycle and its accurate estimation is\n\
    very important for the hydrological water balance, design, and management of irrigation\n\
    systems, crop yield, and water resources planning [2–5]. Likewise, it plays an\
    \ essential role\nin simulating the hydrological effects of climate change [6].\n\
    Various methods have been developed over time to estimate potential evapotranspira-\n\
    tion (ETo) from weather data. These methods vary in complexity from models that\
    \ require\nonly basic information, such as the maximum and minimum air temperatures,\
    \ to complex\nmodels that estimate ETo through energy balance models, such as\
    \ the Penman-Monteith\nmethod [7].\nTo date, there have been attempts to estimate\
    \ and predict ETo more accurately. Some\nof them involve numerical and statistical\
    \ approaches that attempt to accurately simulate\nthe random nature of climate\
    \ variables [8]. In parallel, artiﬁcial intelligence techniques\nAgriculture 2022,\
    \ 12, 1971. https://doi.org/10.3390/agriculture12121971\nhttps://www.mdpi.com/journal/agriculture\n\
    Agriculture 2022, 12, 1971\n2 of 15\nhave been developed with the use of tools\
    \ based on statistical learning theory, such as\nartiﬁcial neural networks [9–12].\n\
    Artiﬁcial neural networks (ANNs) are a mathematical approach to the functioning\
    \ of\nthe brain that can be schematically represented for better understanding\
    \ [13]. The use of\nANNs is widespread, although conventional machine learning\
    \ (ML) methods, including\nANN, principal component analysis (PCA), support vector\
    \ machines (SVM), and regression\nanalysis, among others, have been successfully\
    \ used for decades. Recent advances in deep\nlearning have aroused special interest\
    \ in academia by the intelligent monitoring of various\nnatural and artiﬁcial\
    \ processes [14]. The reason for selecting the Multilayer Perceptron\nin this\
    \ research was its ease of implementation. MLP is also known for providing high-\n\
    quality models while keeping the learning time relatively low compared to more\
    \ complex\nmethods [15]. Likewise, time series forecasting is becoming one of\
    \ the most important\nbranches of big data analysis [16]. Hydrology is not unrelated\
    \ to this and especially ETo\nforecasting, which is used in crop irrigation scheduling.\n\
    The reference evapotranspiration (ETr) is usually computed in advance to obtain\
    \ the\nactual evapotranspiration (ETa). For the ETo estimation, artiﬁcial intelligence\
    \ techniques,\nspeciﬁcally ML, are suitable methodologies due to their excellent\
    \ computational efﬁciency\nand less dependence on data [17].\nIn Peru, similar\
    \ works have been developed [12,18], but in high Andean basins that\nalso use\
    \ meteorological information; however, other approaches, such as the one addressed\n\
    by this research, have not been considered. In addition, in the Tacna Region,\
    \ there are no\nsimilar studies.\nThere are proposals for comparing the calculation\
    \ of ETr with different methods.\nFor instance, Yang [19] makes a detailed comparison\
    \ of methods based on temperature\nand radiation using computational techniques\
    \ implemented in MATLAB, with signiﬁcant\nresults. Nowadays, ML is becoming a\
    \ widely used tool in hydrology [20]. Research\nproposals using data science are\
    \ increasing in the calculation of ETr and especially in the\nvalidation of the\
    \ precision of various classical and some recent methods [21]. Speciﬁcally,\n\
    deep learning using neural networks has become a very powerful and interesting\
    \ alternative\nfor the prediction of climate variables based on temperature [9].\n\
    This study aims to test the efﬁciency of two approaches (direct and indirect)\
    \ to predict\ndaily ETo in an arid region located in southern Peru, using machine\
    \ learning with daily cli-\nmate information from an automatic recording station\
    \ scheduled to 30 min. The algorithm\nused corresponds to an MLP (Multi-Layer\
    \ Perceptron) neural network architecture, to de-\ntermine the suitability of\
    \ the proposed forecasting schemes, using the hyperbolic tangent as\nan activation\
    \ method that delivers transformed values between −1 and 1. For the forecast-\n\
    ing effort, we considered ETo values derived from the equations of Penman-Monteith\
    \ [7],\nHargreaves [22], Ritchie [23], and Turc [24].\nThe research goal of this\
    \ study is to devise a predictive model to calculate the evotran-\nspiration of\
    \ our study zone and compare this with other methods. The model may be well\n\
    extended to other arid zones. In this direction, this study attempts to answer\
    \ the following\nresearch questions:\nRQ1: Can time series data be adequately\
    \ transformed into data from supervised\nproblems to apply to neural network models?\n\
    RQ2: Can an acceptable accuracy of evapotranspiration prediction be obtained using\n\
    multilayer perceptron?\nRQ3: Can a deep learning model perform better than calculation\
    \ applying the Penman-\nMonteith, Hargreaves-Simoni, Ritchie, and Turc equations\
    \ in arid zones?\nOur main hypothesis is that using deep learning models we can\
    \ obtain a high degree\nof precision in evapotranspiration prediction in the arid\
    \ zone of Tacna.\n2. Theoretical Foundations of Evapotranspiration\nSeveral authors\
    \ [22–25] have studied evapotranspiration dynamics; in this sense, the\npresent\
    \ study uses these deﬁnitions and formulations to be able to compare the calculations\n\
    Agriculture 2022, 12, 1971\n3 of 15\nof evapotranspiration with the prediction\
    \ based on neural networks. Next, we will make a\nbrief description of the formulations\
    \ used in this research.\nPotential Evapotranspiration\nThe potential evapotranspiration\
    \ (ETo) expresses the evaporative power of the atmo-\nsphere at a speciﬁc place\
    \ and time and does not consider the characteristics of the crop or\nsoil factors.\
    \ The factors that affect ETo are climatic variables, such as temperature, radiation,\n\
    humidity, wind and pressure [17–20]. It also describes the maximum water losses\
    \ that\ncan be achieved by evaporation and transpiration from a ﬁeld covered by\
    \ a reference crop\n(for example, turfgrass or alfalfa) without water restrictions\
    \ [26]. Consequently, ETo is a\nclimatic parameter and can be calculated from\
    \ meteorological data. Among the various\nmethods used to estimate ETo, the FAO\
    \ 56 Penman-Monteith method is recommended\nas the main method to determine ETo;\
    \ it is physically based and explicitly incorporates\nboth physiological and aerodynamic\
    \ parameters [7]. Reference evapotranspiration (ETo)\nmodeling is important in\
    \ reservoir management, regional water resource planning, and the\nassessment\
    \ of drinking water supplies [22–24]. Other ETo equations are still being used\n\
    due to historical previous use and data access restrictions (Table 1).\nTable\
    \ 1. Evapotranspiration equations.\nName\nRef.\nEquation\nPenman-Monteith equation\n\
    [25]\n(1)\nHargreaves-Samani equation\n[22]\n(2)\nRitchie equation\n[23]\n(3)\n\
    Turc equation\n[24]\n(4)\n(5)\nPenman-Monteith equation. From the original Penman-Monteith\
    \ equation (Equa-\ntion (1)), the aerodynamic equations, surface resistance, and\
    \ the FAO Penman-Monteith\nmethod can be derived to estimate the ETo [25].\nET0\
    \ =\n\x14\n∆\n∆ + γ∗ (Rn − G)\n\x1210\nL\n\x13\n+\nγ\n∆ + γ∗\n90\nT + 275u2(es\
    \ − ea)\n\x15\n(1)\nwhere:\nET0 = reference evapotranspiration (mm/day).\nγ* =\
    \ modiﬁed psychometric constant (mbar/◦C).\nes − ea = saturation vapor pressure\
    \ deﬁcit (mb).\nes = saturation vapor pressure (mb).\nu2 = wind speed at 2 m from\
    \ the surface (m/s).\nL = latent heat of vaporization (cal/g).\n∆ = slope of the\
    \ saturation pressure curve.\nγ = psychrometric constant (mbar/◦C).\nRn = net\
    \ radiation on the crop surface (cal/cm2 day).\nT = average temperature (◦C).\n\
    G = density of soil heat ﬂux (cal/cm2).\nHargreaves-Samani equation. In the literature\
    \ review, many equations have been\ndeveloped to calculate evapotranspiration,\
    \ but the main limitation of most methods is\nthe availability of data on climate\
    \ variables (sometimes we have incomplete or inaccurate\ndata) and local calibration\
    \ [22]. This equation mainly needs the maximum and minimum\ntemperature and is\
    \ as follows:\nET0 = 0.0023Rs\n\x12 Tmax + Tmin\n2\n+ 17.8\n\x13p\nTmax + Tmin\n\
    (2)\nAgriculture 2022, 12, 1971\n4 of 15\nwhere:\nET0 = reference evapotranspiration.\n\
    Tmax = maximum temperature ◦C.\nTmin = minimum temperature ◦C.\nRs = solar radiation\
    \ extraterrestrial in (MJ/m2 day).\nRitchie equation. This equation is used when\
    \ the crops are not very mature. The\nequation is as follows:\nET = α1\nh\n3.87\
    \ ∗ 10−3 ∗ SR(0.6 ∗ Tmax + 0.4 ∗ Tmin + 29)\ni\n(3)\nwhere:\nET = reference evapotranspiration.\n\
    SR = solar radiation (MJ/m2 day).\nTmax = maximum temperature ◦C.\nTmin = minimum\
    \ temperature ◦C.\nα1 = is a coefﬁcient that is calculated as follows:\nIf: 5\
    \ ◦C < Tmax < 35 ◦C → α1 = 1.1\nIf: Tmax > 35 ◦C → α1 = 1.1 + 0.05 [Tmax − 35]\n\
    If: Tmax < 5 ◦C → α1 = 0.01 + exp [0.18 (Tmax + 20)]\nThe calculation of α1 depends\
    \ on the maximum air temperature.\nTurc equation. Within the classiﬁcation of\
    \ methods for the calculation of evapotranspi-\nration based on radiation, we\
    \ ﬁnd the proposal made by Turc [24]. The expression of the\nequation is the following:\n\
    RH ≥ 50% → ET = 0.0133\nT\nT + 15(SR + 50)\n(4)\nRH < 50% → ET = 0.0133\nT\nT\
    \ + 15(SR + 50)\n\x12\n1 + 50 + RH\n70\n\x13\n(5)\nwhere:\nET = reference evapotranspiration.\n\
    RH = is the percentage relative humidity.\nT = average temperature ◦C.\nMachine\
    \ Learning. We aim to deepen the use of the statistical theory for constructing\n\
    mathematical models designed to make inferences from sample data (historical),\
    \ and the\nrole played by computer science in machine learning is important. First,\
    \ in the training, we\nneed efﬁcient algorithms to solve the optimization problem,\
    \ as well as to store and process\nthe enormous amount of data that is available;\
    \ in our case, of climate variables. Second,\nonce the learning process of a model\
    \ is complete, its representation and algorithmic solution\nfor inference shall\
    \ also be efﬁcient. In certain applications, the efﬁciency of the learning\nor\
    \ inference algorithm, that is, its spatial and temporal complexity, can be as\
    \ important\nas its predictive accuracy. This technique is widely used in the\
    \ ﬁeld of hydrology and its\napplications are very varied for various problems\
    \ related to water management [25,27].\n3. Theoretical Foundations of Artiﬁcial\
    \ Neural Networks\nIn this section, we will review the main concepts, to understand\
    \ artiﬁcial neural\nnetworks as a computational technique to predict evapotranspiration.\n\
    3.1. Artiﬁcial Neural Networks and Multilayer Perceptron\nAccording to Equation\
    \ (6) [13], in the Feedforward Architecture, the Topology of the\nArrangement\
    \ of Neurons and their Interconnections Makes the Information Flow in a\nAgriculture\
    \ 2022, 12, 1971\n5 of 15\nUnidirectional Way so that it can Never Pass more than\
    \ Once through a Neuron before\nGenerating the Output Response.\nˆy = g\n\x10\n\
    w0 + ∑\nm\ni=1 xiwi\n\x11\n(6)\nwhere:\nˆy = exit.\ng = non-linear activation\
    \ function.\nw0 = bias (weights).\n∑m\ni=1 xiwi = linear combination of inputs.\n\
    3.2. Multi-Layer Perceptron\nThere are limitations when working with a simple\
    \ perceptron. With it, we can only\ndiscriminate patterns that can be separated\
    \ by a hyperplane and a line in the case of two\ninput neurons. One way to overcome\
    \ these limitations is to include hidden layers; thus,\nobtaining a neural network\
    \ called multilayer perceptron 2. The Multi-layer Perceptron\nor MLP is usually\
    \ trained using an error Back Propagation algorithm or BP. The MLP\nconﬁguration\
    \ is integrated with neurons stacked in multiple layers. Each node in each\nlayer\
    \ is connected to all other nodes in the next layer. There is no connection between\
    \ the\nnodes in the same layer. In an MLP, the data moves from input to output\
    \ through the layers\nin one direction (forward). Hence, this architecture is\
    \ also known as a backpropagation\nnetwork [28].\n3.3. Optimization\nThe optimization\
    \ method is a gradient descent, which can be seen as a local optimizer\nin a continuous\
    \ search space. The purpose of the gradient descent is to ﬁnd the smallest\nerror\
    \ made in the cost function. Equation (7) establishes the gradient descent method.\
    \ It\nwill take a random point and go through it in a loop until it ﬁnds the point\
    \ of least loss,\nupdating the weights on each route [29].\nW = W − n∂j(W)\n∂W\n\
    (7)\nwhere:\nW = new position for the parameters that are closest to the minimum.\n\
    n = learning ratio\nIn recent years, different optimization algorithms have been\
    \ developed to improve\nneural network models. These algorithms are responsible\
    \ for reducing losses and provid-\ning more accurate results; making improvements\
    \ to the neural network by optimizing\nparameters such as weight optimizations,\
    \ initial weight, learning rate and bias, number of\nhidden layers, number of\
    \ nodes in hidden layers, and activation functions [30].\n4. Materials and Methods\n\
    4.1. Data Description\nData were taken from an automatic weather station (Davis\
    \ Instruments, Vantage Pro2\nPlus), located in the La Yarada irrigation area,\
    \ which we can see in Figure 1. The data\ncorrespond to the period from June 2005\
    \ to March 2020, that is, around 16 years of recording,\nwith steps of 30 min.\
    \ These data were taken from daily ﬁgures, generating 5294 records,\nstarting\
    \ from the ﬁle obtained from the automatic station and using code in Python pro-\n\
    gramming language, as a pre-treatment task. The climate variables extracted from\
    \ the\nstation record were: maximum, minimum, and average temperature; relative\
    \ humidity;\nwind speed; atmospheric pressure; precipitation; solar radiation;\
    \ and evapotranspiration.\nAgriculture 2022, 12, 1971\n6 of 15\nAgriculture 2022,\
    \ 12, x FOR PEER REVIEW \n6 of 16 \n \n \n \nFigure 1. Weather station Davis Instruments,\
    \ Vantage Pro2 Plus. \n4.2. KDD (Knowledge Discovery from Data) \nThis work should\
    \ be aligned with the computational point of view of the KDD pro-\ncess [31].\
    \ In the cleaning phase, we took the original dataset from the automatic weather\
    \ \nstation in xls format (Microsoft Excel 17.0), with 253,091 records (period\
    \ from June 2005 \nto March 2020) of temperature (°C), humidity (%), wind speed\
    \ (km/h), atmospheric pres-\nsure (hPa), precipitation (mm), radiation (W/m2)\
    \ and evapotranspiration (mm), which \nwere measured and recorded every 30 min.\
    \ In the selection phase, we transformed the \ntime and date variables and addressed\
    \ the Null values. In the transformation phase, we \nperformed average aggregation\
    \ tasks in the case of temperature, humidity, wind speed, \natmospheric pressure,\
    \ radiation, and summation aggregation in the case of precipitation \nand evapotranspiration;\
    \ grouping this especially with the date information, to have the \ndata organized\
    \ at a daily level. \nAs part of the transformation phase, noise cleaning of the\
    \ data was carried out using \nthe normalization technique with the mean and standard\
    \ deviation. In the case of the spe-\ncific climate variable of wind speed, we\
    \ had an original defect, apparently caused by some \n(sensor) instrument calibration\
    \ problem, since a lag was noted in its representation be-\ntween the years 2019\
    \ and 2020. This was solved by applying data correction techniques \nbased on\
    \ related settings and interpolations. Furthermore, a moving average technique\
    \ \nwas used to smooth the data and identify any data out of range. For data exploration,\
    \ we \nused machine learning techniques since they are efficient tools for the\
    \ treatment of large \nvolumes of data [32,33], and especially in hydrology [34,35].\
    \ We also had to convert the \ntemporal format (time series) to a working scheme\
    \ of a supervised problem. For the pre-\ndiction, we used a framework based on\
    \ a neural network algorithm (Multi-Layered Per-\nceptron) widely used in hydrological\
    \ forecasting [34]. \n4.3. The Data Science and Its Application in Hydrology \n\
    There are many previous works where different machine learning techniques have\
    \ \nbeen used for the treatment, prediction, and analysis of different hydrological\
    \ variables. \nIn this way, they may inform design strategies for adequate irrigation\
    \ and, thus, optimize \nthe use of water resources. \nFigure 1. Weather station\
    \ Davis Instruments, Vantage Pro2 Plus.\n4.2. KDD (Knowledge Discovery from Data)\n\
    This work should be aligned with the computational point of view of the KDD pro-\n\
    cess [31]. In the cleaning phase, we took the original dataset from the automatic\
    \ weather\nstation in xls format (Microsoft Excel 17.0), with 253,091 records\
    \ (period from June 2005 to\nMarch 2020) of temperature (◦C), humidity (%), wind\
    \ speed (km/h), atmospheric pressure\n(hPa), precipitation (mm), radiation (W/m2)\
    \ and evapotranspiration (mm), which were\nmeasured and recorded every 30 min.\
    \ In the selection phase, we transformed the time and\ndate variables and addressed\
    \ the Null values. In the transformation phase, we performed\naverage aggregation\
    \ tasks in the case of temperature, humidity, wind speed, atmospheric\npressure,\
    \ radiation, and summation aggregation in the case of precipitation and evapotran-\n\
    spiration; grouping this especially with the date information, to have the data\
    \ organized at\na daily level.\nAs part of the transformation phase, noise cleaning\
    \ of the data was carried out using\nthe normalization technique with the mean\
    \ and standard deviation. In the case of the\nspeciﬁc climate variable of wind\
    \ speed, we had an original defect, apparently caused by\nsome (sensor) instrument\
    \ calibration problem, since a lag was noted in its representation\nbetween the\
    \ years 2019 and 2020. This was solved by applying data correction techniques\n\
    based on related settings and interpolations. Furthermore, a moving average technique\
    \ was\nused to smooth the data and identify any data out of range. For data exploration,\
    \ we used\nmachine learning techniques since they are efﬁcient tools for the treatment\
    \ of large volumes\nof data [32,33], and especially in hydrology [34,35]. We also\
    \ had to convert the temporal\nformat (time series) to a working scheme of a supervised\
    \ problem. For the prediction, we\nused a framework based on a neural network\
    \ algorithm (Multi-Layered Perceptron) widely\nused in hydrological forecasting\
    \ [34].\n4.3. The Data Science and Its Application in Hydrology\nThere are many\
    \ previous works where different machine learning techniques have\nbeen used for\
    \ the treatment, prediction, and analysis of different hydrological variables.\
    \ In\nthis way, they may inform design strategies for adequate irrigation and,\
    \ thus, optimize the\nuse of water resources.\nAgriculture 2022, 12, 1971\n7 of\
    \ 15\n4.4. Study Area\nThe study region is located in the Tacna region (Figure\
    \ 2), located at the head of the\nAtacama Desert. It has a hyper-arid climate\
    \ and is located in the extreme south of Peru and\nnorthern Chile [36,37]. In\
    \ this area, the cultivation of olive trees has been developed mainly\ndue to\
    \ its low water consumption. The predominant irrigation system is by drip and\n\
    the water source comes from groundwater, whose aquifer system presents water quality\n\
    problems due to marine intrusion and a signiﬁcant reduction in groundwater levels,\
    \ since\nthe extraction volumes exceed the recharge volume; adding to this are\
    \ governance and\ngovernability problems [38–40].\nAgriculture 2022, 12, x FOR\
    \ PEER REVIEW \n7 of 16 \n \n \n4.4. Study Area \nThe study region is located\
    \ in the Tacna region (Figure 2), located at the head of the \nAtacama Desert.\
    \ It has a hyper-arid climate and is located in the extreme south of Peru \nand\
    \ northern Chile [36,37]. In this area, the cultivation of olive trees has been\
    \ developed \nmainly due to its low water consumption. The predominant irrigation\
    \ system is by drip \nand the water source comes from groundwater, whose aquifer\
    \ system presents water \nquality problems due to marine intrusion and a significant\
    \ reduction in groundwater lev-\nels, since the extraction volumes exceed the\
    \ recharge volume; adding to this are govern-\nance and governability problems\
    \ [38–40]. \n \nFigure 2. Location of the study area. \n4.5. Used Approaches \n\
    Two approaches were considered to forecast the ETo. The first approach (direct\
    \ ap-\nproach, Figure 3a) involved obtaining the ETo that the automatic station\
    \ software calcu-\nlated using the Penman-Monteith equation and then applying\
    \ the MLP-based deep learn-\ning model described above to directly simulate the\
    \ ETo time series; obtaining the \nFigure 2. Location of the study area.\n4.5.\
    \ Used Approaches\nTwo approaches were considered to forecast the ETo. The ﬁrst\
    \ approach (direct ap-\nproach, Figure 3a) involved obtaining the ETo that the\
    \ automatic station software calculated\nusing the Penman-Monteith equation and\
    \ then applying the MLP-based deep learning\nmodel described above to directly\
    \ simulate the ETo time series; obtaining the predicted\nvalues of ETo, which\
    \ were subsequently contrasted and evaluated in their accuracy. The\nAgriculture\
    \ 2022, 12, 1971\n8 of 15\nsecond approach (indirect approach, Figure 3b) involved\
    \ applying the MLP-based deep\nlearning model to predict each of the variables\
    \ on which the ETo calculation depends, and\nthen applying the Penman-Monteith,\
    \ Hargreaves-Samani, Ritchie, and Turc equations for\ncalculating the ETo; being\
    \ of interest to evaluate its application in arid zones. A notebook\nwas implemented\
    \ in Python, using “keras” and “tensorﬂow” high-level libraries, as the\nexecution\
    \ engine of our deep learning model, both for its training, prediction, visualization,\n\
    and evaluation.\nAgriculture 2022, 12, x FOR PEER REVIEW \n8 of 16 \n \n \npredicted\
    \ values of ETo, which were subsequently contrasted and evaluated in their ac-\n\
    curacy. The second approach (indirect approach, Figure 3b) involved applying the\
    \ MLP-\nbased deep learning model to predict each of the variables on which the\
    \ ETo calculation \ndepends, and then applying the Penman-Monteith, Hargreaves-Samani,\
    \ Ritchie, and Turc \nequations for calculating the ETo; being of interest to\
    \ evaluate its application in arid zones. \nA notebook was implemented in Python,\
    \ using “keras” and “tensorflow” high-level li-\nbraries, as the execution engine\
    \ of our deep learning model, both for its training, predic-\ntion, visualization,\
    \ and evaluation. \n \nFigure 3. ETo forecasting approaches: (a) Direct approach;\
    \ (b) Indirect approach; adapted from \nTorres et al. (2011). \n4.6. Method Applied\
    \ to Avoid Prediction \nAfter all the tests were performed and achieving intermediate\
    \ results, we noticed that \nwe could only predict one day at a time, since we\
    \ were not making a “prediction over \nprediction”, that is, predicting values\
    \ based on data that were not real but the product of \nour prediction using MLP.\
    \ Therefore, we created a technique called the “deception \nmethod”, which involved\
    \ us using the past 7 days to predict the next one, and in the next \niteration\
    \ we replaced the last value t-1 (which was already predicted) with the real value\
    \ \ncorresponding to the day in question; and thus, day to day, we predicted 300\
    \ days, with \nactual data as the input from our MLP. We called this technique\
    \ the “phased and replace-\nment model”, according to Figure 4. \nFigure 3. ETo\
    \ forecasting approaches: (a) Direct approach; (b) Indirect approach; adapted\
    \ from\nTorres et al. (2011) [7].\n4.6. Method Applied to Avoid Prediction\nAfter\
    \ all the tests were performed and achieving intermediate results, we noticed\
    \ that\nwe could only predict one day at a time, since we were not making a “prediction\
    \ over\nprediction”, that is, predicting values based on data that were not real\
    \ but the product of\nour prediction using MLP. Therefore, we created a technique\
    \ called the “deception method”,\nwhich involved us using the past 7 days to predict\
    \ the next one, and in the next iteration we\nreplaced the last value t-1 (which\
    \ was already predicted) with the real value corresponding\nto the day in question;\
    \ and thus, day to day, we predicted 300 days, with actual data as\nthe input\
    \ from our MLP. We called this technique the “phased and replacement model”,\n\
    according to Figure 4.\nAgriculture 2022, 12, x FOR PEER REVIEW \n9 of 16\n \n\
    \ \nFigure 4. Proposal of the phased and replacement model to avoid prediction\
    \ over prediction. \n4.7. Performance Measures of Predict Models \nMean Square\
    \ Error (MSE): Sensitive to extreme values of the residual. \n1 \n\U0001D45B\n\
    Figure 4. Proposal of the phased and replacement model to avoid prediction over\
    \ prediction.\nAgriculture 2022, 12, 1971\n9 of 15\n4.7. Performance Measures\
    \ of Predict Models\nMean Square Error (MSE): Sensitive to extreme values of the\
    \ residual.\nMSE = 1\nn\nn\n∑\ni=1\n(yi − ˆyi)2\nRoot Mean Square Error (RMSE):\
    \ Expressed in the same units as Loss Given Default.\nRMSE =\n√\nMSE\nMean Absolute\
    \ Error (MAE): Also known as Mean Absolute Deviation (MAD).\nMAE = 1\nn\nn\n∑\n\
    i=1\n|yi − ˆyi|\nRelative Absolute Error (RAE): Ratio of MAE of the model and\
    \ MAE of a simple predictor.\nRAE = ∑n\ni=1|yi − ˆyi|\n∑n\ni=1|yi − yi|\nCoefﬁcient\
    \ of determination (R-squared): In an OLS regression model with a constant\nterm,\
    \ R-squared can be interpreted as the proportion of variation in LGD that is explained\n\
    by variation in the regressors.\nR2 = 1 − ∑n\ni=1(yi − ˆyi)2\n∑n\ni=1(yi − yi)2\n\
    Adjusted coefﬁcient of determination (adjusted R-squared): Corrected for the number\n\
    of regressors (k).\nR2 = 1 −\n\x10\n1 − R2\x11\nn − 1\nn − k − 1\n5. Results and\
    \ Discussion\n5.1. Results\n5.1.1. Data Pre-Processing Results\nIn the data preprocessing\
    \ phase, the moving average technique was used [35] to soften\nand display out-of-range\
    \ values, obtaining very good results as shown in Figure 5. The\ndata and the\
    \ Python code are hosted in Mendeley Data as “Data Set for climate values\nof\
    \ Yarada-Tacna (Peru) 7 June 2005 to 6 March 2020 Period” (it includes the Dataset\
    \ and\nSource Code in Python) [41].\nAgriculture 2022, 12, x FOR PEER REVIEW \n\
    10 of 16 \n \n \nFigure 5. Visualization of out-of-range evapotranspiration values\
    \ using moving averages. \nOnce the data were prepared for treatment as a supervised\
    \ problem, both strategies \nsuggested in Figure 4 were applied. First, for the\
    \ direct approach, day-by-day evapotran-\nspiration was predicted according to\
    \ the historic values of the evapotranspiration ob-\ntained from the weather station.\
    \ To avoid prediction according to the data predicted, this \nprediction (of one\
    \ day) was made based on seven historic previous days. As we moved \nforward,\
    \ the corresponding value was replaced by the actual data from Day 1 of the new\
    \ \ndata, obtained from the weather station, and so on, with our proposed phased\
    \ and re-\nplacement model (Figure 5). The data frame that served to predict the\
    \ next value was up-\nFigure 5. Visualization of out-of-range evapotranspiration\
    \ values using moving averages.\nOnce the data were prepared for treatment as\
    \ a supervised problem, both strategies\nsuggested in Figure 4 were applied. First,\
    \ for the direct approach, day-by-day evapo-\ntranspiration was predicted according\
    \ to the historic values of the evapotranspiration\nAgriculture 2022, 12, 1971\n\
    10 of 15\nobtained from the weather station. To avoid prediction according to\
    \ the data predicted,\nthis prediction (of one day) was made based on seven historic\
    \ previous days. As we\nmoved forward, the corresponding value was replaced by\
    \ the actual data from Day 1 of the\nnew data, obtained from the weather station,\
    \ and so on, with our proposed phased and\nreplacement model (Figure 5). The data\
    \ frame that served to predict the next value was\nupdated, according to the model\
    \ of the previously trained neural network.\n5.1.2. Applied Methods Results\n\
    For the indirect approach, we used the daily prediction of other climate variables\n\
    (temperature, pressure, radiation, wind, humidity) to apply the Penman-Monteith\
    \ equation\nas well as the Hargreaves-Samani, Ritchie, and Turc equations. We\
    \ considered it important\nto verify the precision of each one of them for the\
    \ speciﬁc data of our study region. This\nallowed us to predict 300 days, one\
    \ at a time, and under regular conditions; and allowed\nus to predict daily data\
    \ with great precision. Figure 6 shows the prediction using the\nneural network\
    \ contrasted with the actual data; observing a good approximation for the\ndirect\
    \ approach.\n \n \nFigure 5. Visualization of out-of-range evapotranspiration\
    \ values using moving averages. \nOnce the data were prepared for treatment as\
    \ a supervised problem, both strategies \nsuggested in Figure 4 were applied.\
    \ First, for the direct approach, day-by-day evapotran-\nspiration was predicted\
    \ according to the historic values of the evapotranspiration ob-\ntained from\
    \ the weather station. To avoid prediction according to the data predicted, this\
    \ \nprediction (of one day) was made based on seven historic previous days. As\
    \ we moved \nforward, the corresponding value was replaced by the actual data\
    \ from Day 1 of the new \ndata, obtained from the weather station, and so on,\
    \ with our proposed phased and re-\nplacement model (Figure 5). The data frame\
    \ that served to predict the next value was up-\ndated, according to the model\
    \ of the previously trained neural network. \n5.1.2. Applied Methods Results \n\
    For the indirect approach, we used the daily prediction of other climate variables\
    \ \n(temperature, pressure, radiation, wind, humidity) to apply the Penman-Monteith\
    \ equa-\ntion as well as the Hargreaves-Samani, Ritchie, and Turc equations. We\
    \ considered it im-\nportant to verify the precision of each one of them for the\
    \ specific data of our study region. \nThis allowed us to predict 300 days, one\
    \ at a time, and under regular conditions; and al-\nlowed us to predict daily\
    \ data with great precision. Figure 6 shows the prediction using \nthe neural\
    \ network contrasted with the actual data; observing a good approximation for\
    \ \nthe direct approach. \n \nFigure 6. Comparison of the 300-day ETo prediction\
    \ using a neural network with the actual data \n(Direct approach). \nPrecision\
    \ indicators of the model based on the neural network for the direct approach\
    \ \nare: MAE = 0.03312 (Media Absolute Error); MSE = 0.001875 (Media Square Error);\
    \ RMSE \n= 0.043303 (Root of the MSE); and RAE = 0.015553 (Relative Absolute error).\
    \ \nBy having close values between the MAE and RMSE, we can infer that there is\
    \ an \nerror evenly distributed and no significant outliers, which is confirmed\
    \ as we can visual-\nize the comparative curve of the predicted values with the\
    \ actual values. In addition, for \nFigure 6. Comparison of the 300-day ETo prediction\
    \ using a neural network with the actual data\n(Direct approach).\nPrecision indicators\
    \ of the model based on the neural network for the direct ap-\nproach are: MAE\
    \ = 0.03312 (Media Absolute Error); MSE = 0.001875 (Media Square Error);\nRMSE\
    \ = 0.043303 (Root of the MSE); and RAE = 0.015553 (Relative Absolute error).\n\
    By having close values between the MAE and RMSE, we can infer that there is an\n\
    error evenly distributed and no signiﬁcant outliers, which is conﬁrmed as we can\
    \ visualize\nthe comparative curve of the predicted values with the actual values.\
    \ In addition, for the\nmethods based on gradients optimization, it is convenient\
    \ to consider the RMSE to set\nsome parameters in the learning rate, which we\
    \ did reiteratively.\nThe MAE is a low value (MAE = 0.033) and represents an element\
    \ of penalty for\nlarge errors; that is, it is less sensitive to atypical errors,\
    \ which means that there are no\nlarge errors.\nThen we apply scheme (b), in which\
    \ we are in charge of predicting the values of\nthe climate variables using our\
    \ model based on neural networks, and then, based on\nthose predictions, calculate\
    \ the different equations in the literature (Penman-Monteith,\nHargreaves-Samani,\
    \ Ritchie, and Turc) and compare their errors and correlation coefﬁcients\nwith\
    \ the actual data.\nIn the following graphs, we show the prediction of the different\
    \ climate variables\nthat will serve as the input to apply the different previously\
    \ mentioned equations for\ncalculating the ETo, and compare their precision. Figure\
    \ 7 shows the prediction for the\ntemperature variable, minimum and maximum, humidity,\
    \ wind speed, pressure, and\nradiation in 300 days, speciﬁcally using the MLP\
    \ neural networks.\nAgriculture 2022, 12, 1971\n11 of 15\n \nture variable, minimum\
    \ and maximum, humidity, wind speed, pressure, and radiation in \n300 days, specifically\
    \ using the MLP neural networks. \nWith the predicted climate variables, and using\
    \ our proposal for a phased and ex-\nchange model, we calculated a total of 300\
    \ values daily (Figure 7). The evapotranspiration \nwas calculated using different\
    \ equations, established as Penman-Monteith, Hargreaves-\nSamani, Ritchie, and\
    \ Turc, and we compared these results to the actual prediction and \nanalyzed\
    \ its accuracy and performance, as can be seen in Figure 8. \n \nFigure 7. Blue\
    \ color, original data; red color, predicted data. The figure shows the prediction\
    \ for 300 \ndays, using neural networks, of Maximum Temperature (°C), Minimum\
    \ Temperature (°C), Wind \nSpeed (km/hr), Moisture (%), Pressure (hPa), and Evapotranspiration\
    \ (mm). \nFigure 7. Blue color, original data; red color, predicted data. The\
    \ ﬁgure shows the prediction for\n300 days, using neural networks, of Maximum\
    \ Temperature (◦C), Minimum Temperature (◦C), Wind\nSpeed (km/hr), Moisture (%),\
    \ Pressure (hPa), and Evapotranspiration (mm).\nWith the predicted climate variables,\
    \ and using our proposal for a phased and ex-\nchange model, we calculated a total\
    \ of 300 values daily (Figure 7). The evapotranspiration\nwas calculated using\
    \ different equations, established as Penman-Monteith, Hargreaves-\nSamani, Ritchie,\
    \ and Turc, and we compared these results to the actual prediction and\nanalyzed\
    \ its accuracy and performance, as can be seen in Figure 8.\nAgriculture 2022,\
    \ 12, x FOR PEER REVIEW \n12 of 16 \n \n \nFigure 8. ETo calculated using the\
    \ Penman-Monteith, Hargreaves-Samani, Ritchie, and Turc equa-\ntions for 300 days,\
    \ using climate variables predicted with neural networks compared with the actual\
    \ \ndata. \nIn Table 2, we can see the summary of the various indicators of errors\
    \ and accuracy \nfactors under the indirect approach. In this second approach,\
    \ climate variables that serve \nas the basis for calculating the ETo were predicted.\
    \ The precision indicators show good \nconditions in their process, with MAE,\
    \ MSE, RMSE, and RAE values, which guarantee the \nsuitability of the predicted\
    \ variables. \nTable 2. Metrics results. \nIndicator \nNeural  \nNetwork \nPenman-Monteith\
    \ Hargreaves-Samani \nRitchie \nTurc \nFigure 8. ETo calculated using the Penman-Monteith,\
    \ Hargreaves-Samani, Ritchie, and Turc equations\nfor 300 days, using climate\
    \ variables predicted with neural networks compared with the actual data.\nIn\
    \ Table 2, we can see the summary of the various indicators of errors and accuracy\n\
    factors under the indirect approach. In this second approach, climate variables\
    \ that serve\nas the basis for calculating the ETo were predicted. The precision\
    \ indicators show good\nconditions in their process, with MAE, MSE, RMSE, and\
    \ RAE values, which guarantee the\nsuitability of the predicted variables.\nAgriculture\
    \ 2022, 12, 1971\n12 of 15\nTable 2. Metrics results.\nIndicator\nNeural Network\n\
    Penman-Monteith\nHargreaves-Samani\nRitchie\nTurc\nMAE\n0.033\n0.586\n0.467\n\
    0.749\n0.104\nMSE\n0.002\n0.411\n0.285\n0.632\n0.016\nRMSE\n0.043\n0.641\n0.534\n\
    0.795\n0.128\nRAE\n0.016\n0.230\n0.192\n0.285\n0.046\nR-Squared\n0.998\n0.550\n\
    0.689\n0.309\n0.982\nMAE = Mean absolute error; MSE = Mean square error; RMSE\
    \ = root of the MSE; and RAE = Relative absolute error.\nAccording to the MAE,\
    \ being the most robust reference for our purpose as it is less\nsensitive to\
    \ atypical values, we established that the best prediction for evapotranspiration\n\
    is that of Turc, having a MAE of 0.104, as opposed to Hargreaves-Samani with a\
    \ MAE of\n0.467, Ritchie with a MAE of 0.749, and ﬁnally, Penman-Monteith with\
    \ a MAE of 0.586. It\nshould be noted that we do not consider the prediction based\
    \ on neural networks (MLP)\nsince in the application of scheme (b), we only use\
    \ the equations to calculate the value of\nevapotranspiration.\nFigure 9 shows\
    \ a comparative summary of the correlation factors between the\nmethods used.\n\
    Agriculture 2022, 12, x FOR PEER REVIEW \n13 of 16 \n \n \nFigure 9. Comparison\
    \ of the ETo as compared to the application of the Penman-Monteith, Har-\ngreaves-Samani,\
    \ Ritchie, and Turc equations. \n5.2. Discussion \nWe found that deep learning\
    \ using neural network algorithms, such as Multi-Layer \nPerceptron, may be used\
    \ for the prediction of time series as in this case, but it is necessary \nto\
    \ emphasize that one of the main problems in the classification of time series\
    \ is given by \nits structure, generally, when classifying phenomena described\
    \ by attributes. Their order \ndoes not affect the result; however, the time series\
    \ preserve order or temporality, which \ndoes not allow for a change in the position\
    \ of the data. Thus, algorithms that work with \nattributes cannot be applied\
    \ in this type of problem [42]. This was previously solved by \nadapting the time\
    \ series to the format of a supervised problem (Figure 10). \nFigure 9. Comparison\
    \ of the ETo as compared to the application of the Penman-Monteith, Hargreaves-\n\
    Samani, Ritchie, and Turc equations.\n5.2. Discussion\nWe found that deep learning\
    \ using neural network algorithms, such as Multi-Layer\nPerceptron, may be used\
    \ for the prediction of time series as in this case, but it is necessary\nto emphasize\
    \ that one of the main problems in the classiﬁcation of time series is given by\n\
    its structure, generally, when classifying phenomena described by attributes.\
    \ Their order\ndoes not affect the result; however, the time series preserve order\
    \ or temporality, which\ndoes not allow for a change in the position of the data.\
    \ Thus, algorithms that work with\nattributes cannot be applied in this type of\
    \ problem [42]. This was previously solved by\nadapting the time series to the\
    \ format of a supervised problem (Figure 10).\nAgriculture 2022, 12, 1971\n13\
    \ of 15\n \nPerceptron, may be used for the prediction of time series as in this\
    \ case, but it is necessary \nto emphasize that one of the main problems in the\
    \ classification of time series is given by \nits structure, generally, when classifying\
    \ phenomena described by attributes. Their order \ndoes not affect the result;\
    \ however, the time series preserve order or temporality, which \ndoes not allow\
    \ for a change in the position of the data. Thus, algorithms that work with \n\
    attributes cannot be applied in this type of problem [42]. This was previously\
    \ solved by \nadapting the time series to the format of a supervised problem (Figure\
    \ 10). \n \nFigure 10. Transposing data from time format to supervised problem.\
    \ \nThus the “data frame” or data structure with two dimensions that stores the\
    \ data of \nthe variables is defined as shown in Table 3, where “t” represents\
    \ the predicted value. The \nvalues are normalized for the hyperbolic tangent,\
    \ which is the function of the neural net-\nwork activation whose input values\
    \ are between −1 and 1. \nFigure 10. Transposing data from time format to supervised\
    \ problem.\nThus the “data frame” or data structure with two dimensions that stores\
    \ the data\nof the variables is deﬁned as shown in Table 3, where “t” represents\
    \ the predicted value.\nThe values are normalized for the hyperbolic tangent,\
    \ which is the function of the neural\nnetwork activation whose input values are\
    \ between −1 and 1.\nTable 3. Transformation of time series to supervised problem\
    \ format.\nt-7\nt-6\nt-5\nt-4\nt-3\nt-2\nt-1\nt\n−0.32\n−0.32\n−0.33\n0.35\n−0.35\n\
    −0.36\n−0.36\n0.35\n−0.32\n−0.33\n−0.35\n0.35\n−0.36\n−0.36\n−0.35\n0.36\n−0.33\n\
    −0.35\n−0.35\n0.36\n−0.36\n−0.35\n−0.36\n0.36\n−0.35\n−0.35\n−0.36\n0.36\n−0.35\n\
    −0.36\n−0.36\n0.36\n−0.35\n−0.36\n−0.36\n0.35\n−0.36\n−0.36\n−0.36\n0.37\nThe\
    \ results indicate that the prediction of evapotranspiration values using neural\n\
    networks has a lower error than the traditional formulations of Penman-Monteith\
    \ [25],\nHargreaves [22], Ritchie [23] and Turc [24].\n6. Conclusions\nThis work\
    \ demonstrates the feasibility of forecasting the short-term daily ETo, which\n\
    is required for irrigation water management purposes, based on the Penman-Monteith,\n\
    Hargreaves-Samani, Ritchie, and Turc equations. Two approaches were tested using\
    \ the\nFeedforward neural network algorithm (Multi-Layered Perceptron). The ﬁrst\
    \ approach, the\ndirect approach, involves estimating future ETo time series from\
    \ historical data obtained\nfrom the automatic station. The second approach, the\
    \ indirect approach, considers forecast-\ning the weather data necessary for the\
    \ ETo equations, maximum temperature, minimum\ntemperature, wind speed, humidity,\
    \ pressure, and evapotranspiration on a daily level using\nthe aforementioned\
    \ machine learning and, subsequently, using these predicted values to\nestimate\
    \ future ETo values.\nThe model precision indicators based on neural networks\
    \ for the direct approach\nare values that guarantee a good precision in the forecast,\
    \ MAE = 0.033, MSE = 0.002,\nRMSE = 0.043, and RAE = 0.016. Regarding the indirect\
    \ approach, the prediction of the\nclimate variables that were used to calculate\
    \ the ETo was carried out. The precision\nindicators show optimal conditions in\
    \ their process and guarantee the suitability of the\npredicted variables.\nThe\
    \ results indicate that using the approaches proposed in this study makes it possible\n\
    to forecast up to 300 days of daily ETo in advance within a reasonable range of\
    \ time.\nFurthermore, the use of this methodology provides an additional estimate\
    \ of the expected\nvariability values for each forecast day, which ensures a very\
    \ good estimate of ETo. The\nforecast with more than 300 days in advance is affected\
    \ by the relationship of the value\nof the time series with the previous ones.\
    \ Therefore, the accuracy of the predicted ETo\ndecreases over time.\nAgriculture\
    \ 2022, 12, 1971\n14 of 15\nComparing the performance of the approaches and the\
    \ machine learning used, the\nresults obtained indicate that, despite the similar\
    \ performance of the two proposed ap-\nproaches, the indirect approach provides\
    \ better ETo forecasting capabilities for longer time\nintervals than the direct\
    \ approach. This result is because it only uses weather parameters\nrequired for\
    \ ETo equations to model and predict behavior, while for the direct approach,\
    \ the\nmachine learning model is required to forecast the combined effect of the\
    \ climate variables’\ntrend on the resulting ETo. Therefore, the indirect approximation\
    \ may be extended to other\nequations of ETo.\nAuthor Contributions: Conceptualization,\
    \ E.P.-V. and A.T.-R.; software, E.T.-A.; data curation, E.T.-\nA.; validation,\
    \ E.P.-V. and A.T.-R.; formal analysis, E.P.-V., E.T.-A., E.I.-B. and A.T.-R.;\
    \ writing—original\ndraft preparation, E.P.-V.; writing—review and editing, E.P.-V.,\
    \ E.T.-A., E.I.-B. and A.T.-R.; project\nadministration, E.P.-V. All authors have\
    \ read and agreed to the published version of the manuscript.\nFunding: This research\
    \ was funded by Jorge Basadre Grohmann National University and the APC\nwas funded\
    \ by research vice-rectorate.\nData Availability Statement: Data are available\
    \ on https://data.mendeley.com/datasets/df46xjw6\n2v/1 (accessed on 30 April 2021)\
    \ with CC BY 4.0 license.\nAcknowledgments: This work was ﬁnanced by funds from\
    \ the mining royalties, IGIN, VIIN of the\nUNJBG, within the framework of the\
    \ research project “Study of risk and alternatives for the protection\nof the\
    \ population in the area of inﬂuence of the Quebrada del Diablo, Tacna, Peru”.\n\
    Conﬂicts of Interest: The authors declare no conﬂict of interest.\nReferences\n\
    1.\nWang, H.; Yan, H.; Zeng, W.; Lei, G.; Ao, C.; Zha, Y. A Novel Nonlinear Arps\
    \ Decline Model with Salp Swarm Algorithm for\nPredicting Pan Evaporation in the\
    \ Arid and Semi-Arid Regions of China. J. Hydrol. 2020, 582, 124545. [CrossRef]\n\
    2.\nFan, J.; Yue, W.; Wu, L.; Zhang, F.; Cai, H.; Wang, X.; Lu, X.; Xiang, Y.\
    \ Evaluation of SVM, ELM and Four Tree-Based Ensemble\nModels for Predicting Daily\
    \ Reference Evapotranspiration Using Limited Meteorological Data in Different\
    \ Climates of China.\nAgric. For. Meteorol. 2018, 263, 225–241. [CrossRef]\n3.\n\
    Goyal, M.K.; Bharti, B.; Pham, Q.N.; Adamowski, J.; Pandey, A. Modeling of Daily\
    \ Pan Evaporation in Sub Tropical Climates\nUsing ANN, LS-SVR, Fuzzy Logic, and\
    \ ANFIS. Expert Syst. Appl. 2014, 41, 5267–5276. [CrossRef]\n4.\nGuan, Y.; Mohammadi,\
    \ B.; Pham, Q.; Adarsh, S.; Balkhair, K.S.; Khalil, U.R.; Rahman, U.; Nguyen,\
    \ T.T.L.; Linh, T.T.; Tri, Q. A\nNovel Approach for Predicting Daily Pan Evaporation\
    \ in the Coastal Regions of Iran Using Support Vector Regression Coupled\nwith\
    \ Krill Herd Algorithm Model. Theor. Appl. Climatol. 2020, 142, 349–367. [CrossRef]\n\
    5.\nManikumari, N.; Vinodhini, G.; Murugappan, A. Modelling of Reference Evapotransipration\
    \ Using Climatic Parameters for\nIrrigation Scheduling Using Machine Learning.\
    \ ISH J. Hydraul. Eng. 2020, 28, 272–281.\n6.\nRaza, A.; Shoaib, M.; Khan, A.;\
    \ Baig, F.; Faiz, M.A.; Khan, M.M. Application of Non-Conventional Soft Computing\
    \ Approaches for\nEstimation of Reference Evapotranspiration in Various Climatic\
    \ Regions. Theor. Appl. Climatol. 2020, 139, 1459–1477. [CrossRef]\n7.\nTorres,\
    \ A.F.; Walker, W.R.; McKee, M. Forecasting Daily Potential Evapotranspiration\
    \ Using Machine Learning and Limited\nClimatic Data. Agric. Water Manag. 2011,\
    \ 98, 553–562. [CrossRef]\n8.\nSilva, D.; Meza, F.J.; Varas, E. Estimating Reference\
    \ Evapotranspiration (ETo) Using Numerical Weather Forecast Data in Central\n\
    Chile. J. Hydrol. 2010, 382, 64–71. [CrossRef]\n9.\nAdamala, S. Temperature Based\
    \ Generalized Wavelet-Neural Network Models to Estimate Evapotranspiration in\
    \ India. Inf.\nProcess. Agric. 2018, 5, 149–155. [CrossRef]\n10.\nAdeloye, A.J.;\
    \ Rustum, R.; Kariyama, I.D. Neural Computing Modeling of the Reference Crop Evapotranspiration.\
    \ Environ. Model.\nSoftw. 2012, 29, 61–73. [CrossRef]\n11.\nAntonopoulos, V.Z.;\
    \ Antonopoulos, A.V. Daily Reference Evapotranspiration Estimates by Artiﬁcial\
    \ Neural Networks Technique\nand Empirical Equations Using Limited Input Climate\
    \ Variables. Comput. Electron. Agric. 2017, 132, 86–96. [CrossRef]\n12.\nLaqui,\
    \ W.; Zubieta, R.; Rau, P.; Mejía, A.; Lavado, W.; Ingol, E. Can Artiﬁcial Neural\
    \ Networks Estimate Potential Evapotranspira-\ntion in Peruvian Highlands? Model.\
    \ Earth Syst. Environ. 2019, 5, 1911–1924. [CrossRef]\n13.\nDavid Rumelhart Geoffrey\
    \ Hinton, R.W. Learning Representations by Back-Propagating Errors. Nature 1986,\
    \ 323, 533–536.\n14.\nZhang, S.; Zhang, S.; Wang, B.; Habetler, T.G. Deep Learning\
    \ Algorithms for Bearing Fault Diagnosticsx—A Comprehensive\nReview. IEEE Access\
    \ 2020, 8, 29857–29881. [CrossRef]\n15.\nCar, Z.; Šegota, S.B.; An ¯deli´c, N.;\
    \ Lorencin, I.; Mrzljak, V. Modeling the Spread of COVID-19 Infection Using a\
    \ Multilayer\nPerceptron. Comput. Math. Methods Med. 2020, 2020, 5714714. [CrossRef]\
    \ [PubMed]\n16.\nShen, Z.; Zhang, Y.; Lu, J.; Xu, J.; Xiao, G. A Novel Time Series\
    \ Forecasting Model with Deep Learning. Neurocomputing 2020, 396,\n302–313. [CrossRef]\n\
    Agriculture 2022, 12, 1971\n15 of 15\n17.\nKumar, M.; Raghuwanshi, N.S.; Signh,\
    \ R.; Wallender, W.W.; Pruitt, W.O. Estimating of Evapotranspiration Using Artiﬁcial\
    \ Neural\nNetwork. J. Irrig. Drain. Eng. 2002, 128, 224–233. [CrossRef]\n18.\n\
    Machaca-Apaza, L.C. Estimación de la Evapotranspiración de Referencia Utilizando\
    \ Modelos de Redes Neuronales Artiﬁciales\nEn Función de Elementos Climáticos\
    \ en la Cuenca Del Rio Huancané. Bachelor’s Thesis, Universidad Nacional del Altiplano,\n\
    Puno, Peru, 2016.\n19.\nYang, Y.; Chen, R.; Han, C.; Liu, Z. Evaluation of 18\
    \ Models for Calculating Potential Evapotranspiration in Different Climatic\n\
    Zones of China. Agric. Water Manag. 2021, 244, 106545. [CrossRef]\n20.\nNearing,\
    \ G.S.; Kratzert, F.; Sampson, A.K.; Pelissier, C.S.; Klotz, D.; Frame, J.M.;\
    \ Prieto, C.; Gupta, H.V. What Role Does\nHydrological Science Play in the Age\
    \ of Machine Learning? Water Resour. Res. 2020, 57, e2020WR028091. [CrossRef]\n\
    21.\nKaya, Y.Z.; Zelenakova, M.; Üne¸s, F.; Demirci, M.; Hlavata, H.; Mesaros,\
    \ P. Estimation of Daily Evapotranspiration in Košice City\n(Slovakia) Using Several\
    \ Soft Computing Techniques. Theor. Appl. Climatol. 2021, 144, 287–298. [CrossRef]\n\
    22.\nHargreaves, G.H.; Samani, Z.A. Reference Crop Evapotranspiration from Temperature.\
    \ Appl. Eng. Agric. 1985, 1, 96–99. [CrossRef]\n23.\nRitchie, J.T. Model for Predicting\
    \ Evaporation from a Row Crop with Incomplete Cover. Water Resour. Res. 1972,\
    \ 8, 1204–1213.\n[CrossRef]\n24.\nLecarpentier, C. L’évapotranspiration Potentielle\
    \ et Ses Implications Géographiques (Suite). Ann. Georgr. 1975, 84, 385–414.\n\
    [CrossRef]\n25.\nValiantzas, J.D. Simpliﬁed Forms for the Standardized FAO-56\
    \ Penman-Monteith Reference Evapotranspiration Using Limited\nWeather Data. J.\
    \ Hydrol. 2013, 505, 13–23. [CrossRef]\n26.\nBabakos, K.; Papamichail, D.; Tziachris,\
    \ P.; Pisinaras, V.; Demertzi, K.; Aschonitis, V. Assessing the Robustness of\
    \ Pan Evaporation\nModels for Estimating Reference Crop Evapotranspiration during\
    \ Recalibration at Local Conditions. Hydrology 2020, 7, 62.\n[CrossRef]\n27.\n\
    Giles-Hansen, K.; Wei, X.; Hou, Y. Dramatic Increase in Water Use Efﬁciency with\
    \ Cumulative Forest Disturbance at the Large\nForested Watershed Scale. Carbon\
    \ Balance Manag. 2021, 16, 6. [CrossRef] [PubMed]\n28.\nEfron, B.; Hastie, T.\
    \ Computer Age Statistical Inference; Cambridge Univesity Press: Cambridge, UK,\
    \ 2016.\n29.\nRuder, S. An Overview of Gradient Descent Optimization Algorithms.\
    \ arXiv 2016, arXiv:1609.04747.\n30.\nAbdolrasol, M.G.M.; Hussain, S.M.S.; Ustun,\
    \ T.S.; Sarker, M.R.; Hannan, M.A.; Mohamed, R.; Ali, J.A.; Mekhilef, S.; Milad,\
    \ A.\nArtiﬁcial Neural Networks Based Optimization Techniques: A Review. Electronics\
    \ 2021, 10, 2689. [CrossRef]\n31.\nFayyad, U.; Piatetsky-Shapiro, G.; Smyth, P.\
    \ The KDD Process for Extracting Useful Knowledge from Volumes of Data. Commun.\n\
    ACM 1996, 39, 27–34. [CrossRef]\n32.\nHong, W.C. Rainfall Forecasting by Technological\
    \ Machine Learning Models. Appl. Math. Comput. 2008, 200, 41–57. [CrossRef]\n\
    33.\nAhmad, S.; Kalra, A.; Stephen, H. Estimating Soil Moisture Using Remote Sensing\
    \ Data: A Machine Learning Approach. Adv.\nWater Resour. 2010, 33, 69–80. [CrossRef]\n\
    34.\nFeng, L.; Hong, W. On Hydrologic Calculation Using Artiﬁcial Neural Networks.\
    \ Appl. Math. Lett. 2008, 21, 453–458. [CrossRef]\n35.\nBühlmann, P. ELSEVEX Moving-Average\
    \ Representation of Autoregressive Approximations. Stoch. Process. Appl. 1995,\
    \ 60,\n331–342. [CrossRef]\n36.\nPino, E.; Tacora, P.; Steenken, A.; Alfaro, L.;\
    \ Valle, A.; Chávarri, E.; Ascencios, D.; Marcacuzco, J.A.M. Efecto de las Características\n\
    Ambientales y Geológicas Sobre la Calidad del Agua en la Cuenca del Río Caplina,\
    \ Tacna, Perú. Tecnol. Cienc. Agua 2017, 8, 77–99.\n[CrossRef]\n37.\nPino-Vargas,\
    \ E.; Chávarri-Velarde, E.; Ingol-Blanco, E.; Mejía, F.; Cruz, A.; Vera, A. Impacts\
    \ of Climate Change and Variability on\nPrecipitation and Maximum Flows in Devil’s\
    \ Creek, Tacna, Peru. Hydrology 2022, 9, 10. [CrossRef]\n38.\nVera, A.; Pino-Vargas,\
    \ E.; Verma, M.P.; Chucuya, S.; Chávarri, E.; Canales, M.; Torres-Martínez, J.A.;\
    \ Mora, A.; Mahlknecht, J.\nHydrodynamics, Hydrochemistry, and Stable Isotope\
    \ Geochemistry to Assess Temporal Behavior of Seawater Intrusion in the la\nYarada\
    \ Aquifer in the Vicinity of Atacama Desert, Tacna, Peru. Water 2021, 13, 3161.\
    \ [CrossRef]\n39.\nChucuya, S.; Vera, A.; Pino-Vargas, E.; Steenken, A.; Mahlknecht,\
    \ J.; Montalván, I. Hydrogeochemical Characterization and\nIdentiﬁcation of Factors\
    \ Inﬂuencing Groundwater Quality in Coastal Aquifers, Case: La Yarada, Tacna,\
    \ Peru. Int. J. Environ. Res.\nPublic Health 2022, 19, 2815. [CrossRef] [PubMed]\n\
    40.\nNarvaez-Montoya, C.; Torres-Martínez, J.A.; Pino-Vargas, E.; Cabrera-Olivera,\
    \ F.; Loge, F.J.; Mahlknecht, J. Predicting Adverse\nScenarios for a Transboundary\
    \ Coastal Aquifer System in the Atacama Desert (Peru/Chile). Sci. Total Environ.\
    \ 2022, 806, 150386.\n[CrossRef]\n41.\nPino-Vargas, E.; Taya-Acosta, E.; Torres-Rúa,\
    \ A. Data Set for Climate Values of Yarada-Tacna(Perú) 7-6-2005 to 6-3-2020 Period,\n\
    Tacna. 2021. Available online: https://data.mendeley.com/datasets/df46xjw62v/1\
    \ (accessed on 30 April 2021).\n42.\nSantos-Camacho, E.A.; Figueroa-Nazuno, J.G.;\
    \ Eguía, J.C.C. Clasiﬁcador No Supervisado Para Series de Tiempo Unsupervised\n\
    Classiﬁer for Time Series. Res. Comput. Sci. 2015, 105, 21–29. [CrossRef]\n"
  inline_citation: '>'
  journal: Agriculture (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2077-0472/12/12/1971/pdf?version=1669284143
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: 'Deep Machine Learning for Forecasting Daily Potential Evapotranspiration
    in Arid Regions, Case: Atacama Desert Header'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/su15097677
  analysis: '>'
  authors:
  - Mercedeh Taheri
  - H. K. Schreiner
  - Abdolmajid Mohammadian
  - Hamidreza Shirkhani
  - Pierre Payeur
  - Hanifeh Imanian
  - Juan Hiedra Cobo
  citation_count: 3
  full_citation: '>'
  full_text: ">\nCitation: Taheri, M.; Schreiner, H.K.;\nMohammadian, A.; Shirkhani,\
    \ H.;\nPayeur, P.; Imanian, H.; Cobo, J.H. A\nReview of Machine Learning\nApproaches\
    \ to Soil Temperature\nEstimation. Sustainability 2023, 15,\n7677. https://doi.org/10.3390/\n\
    su15097677\nAcademic Editor: Gwanggil Jeon\nReceived: 6 March 2023\nRevised: 1\
    \ May 2023\nAccepted: 4 May 2023\nPublished: 7 May 2023\nCopyright:\n© 2023 by\
    \ the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access\
    \ article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative Commons\n\
    Attribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\n\
    sustainability\nReview\nA Review of Machine Learning Approaches to Soil\nTemperature\
    \ Estimation\nMercedeh Taheri 1,*, Helene Katherine Schreiner 1, Abdolmajid Mohammadian\
    \ 1\n, Hamidreza Shirkhani 2,\nPierre Payeur 3\n, Hanifeh Imanian 1\nand Juan\
    \ Hiedra Cobo 2\n1\nDepartment of Civil Engineering, University of Ottawa, Ottawa,\
    \ ON K1N 6N5, Canada\n2\nNational Research Council Canada, Ottawa, ON K1A 0R6,\
    \ Canada\n3\nSchool of Electrical Engineering and Computer Science, University\
    \ of Ottawa, Ottawa, ON K1N 6N5, Canada\n*\nCorrespondence: mtahe067@uottawa.ca\n\
    Abstract: Soil temperature is an essential factor for agricultural, meteorological,\
    \ and hydrological\napplications. Direct measurement, despite its high accuracy,\
    \ is impractical on a large spatial scale due\nto the expensive and time-consuming\
    \ process. On the other hand, the complex interaction between\nvariables affecting\
    \ soil temperature, such as topography and soil properties, leads to challenging\n\
    estimation processes by empirical methods and physical models. Machine learning\
    \ (ML) approaches\ngained considerable attention due to their ability to address\
    \ the limitations of empirical and physical\nmethods. These approaches are capable\
    \ of estimating the variables of interest using complex nonlinear\nrelationships\
    \ with no assumptions about data distribution. However, their sensitivity to input\
    \ data\nas well as the need for a large amount of training ground truth data limits\
    \ the application of machine\nlearning approaches. The current paper aimed to\
    \ provide a review of ML techniques implemented\nfor soil temperature modeling,\
    \ their challenges, and milestones achieved in this domain.\nKeywords: soil temperature;\
    \ direct measurement; empirical methods; physical models; machine learning\n1.\
    \ Introduction\nSoil temperature is an essential environmental parameter in earth\
    \ sciences, such as me-\nteorology and climate change; soil processes, such as\
    \ evaporation, inﬁltration, and nutrient\ncycling; and agricultural purposes,\
    \ such as precision farming and irrigation scheduling at\nmany developmental stages.\
    \ As an important meteorological parameter, soil temperature\ncontrols the heat\
    \ transfer from the soil to the atmosphere and vice versa. In addition,\nagriculture\
    \ applications including potential evapotranspiration, root development, crop\n\
    growth, and yield of crop are affected by soil temperature variations. Soil temperature\n\
    also plays a remarkable role in determining hydrological parameters, such as evaporation,\n\
    precipitation, and inﬁltration of water through soil.\nAccurate estimation of\
    \ soil temperature is complicated because of the physical, chem-\nical, and biological\
    \ processes governing the soil-plant-atmosphere system [1]. Since soil\ntemperature\
    \ is highly localized and can be difﬁcult to measure directly, indirect methods\
    \ of\nassessing soil temperature using more easily measured variables such as\
    \ air temperature\nand readily available weather data gained much attention.\n\
    Generally, estimation methods for soil temperature are classiﬁed into three groups\n\
    (1) in situ measurements, (2) satellite observations, and (3) model-based estimates.\n\
    The most reliable method is in situ measuring using local instruments such as\
    \ ther-\nmistors, thermocouples, thermocouple wires, and averaging thermocouples,\
    \ which can\nprovide accurate estimates of soil temperature. Although in situ\
    \ measurements, which\nare necessary to validate the two other methods, can provide\
    \ soil temperature with a\nrelatively high degree of accuracy, the low density\
    \ of sensors and ﬁne spatial resolution of\nsampling restrict the use of point\
    \ (or ﬁeld) observations to map the soil temperature over\nSustainability 2023,\
    \ 15, 7677. https://doi.org/10.3390/su15097677\nhttps://www.mdpi.com/journal/sustainability\n\
    Sustainability 2023, 15, 7677\n2 of 26\na wide geographical area because of expensive\
    \ equipment. In other words, continuous\nmonitoring of soil temperature at different\
    \ depths requires a variety of sensors installed at\nshort distances to represent\
    \ the spatial heterogeneity in soil temperature. However, high\ncosts of installation\
    \ and maintenance, equipment requirement, special expertise, and point\nmeasurement\
    \ limit the application of these ﬁeld soil temperature sensors.\nTherefore, satellite\
    \ sensors received signiﬁcant attention over the past few decades. Yet,\nscaling\
    \ issues and measurement depth, despite current advances, are the most considerable\n\
    challenges of remote sensing techniques.\nThe model-based approach, including\
    \ empirical and physical models, can retrieve\nsoil temperature by surface information\
    \ and weather-forcing variables. Empirical models\nuse statistical regression\
    \ techniques and parametric functions to map the soil temperature\nthrough in\
    \ situ measurements. These models are simple to implement without a speciﬁc\n\
    background. Although these models are useful to model soil temperature dynamics\
    \ with\nan acceptable accuracy, the need for suitable reference samples of the\
    \ desired target variable\nfor calibration and validation of results restricts\
    \ the use of the models due to the costly and\ntime-consuming process of in situ\
    \ measurements. Moreover, the estimation accuracy of\nreference samples can be\
    \ affected by errors during the measurement process. The statistical\nregression-based\
    \ relationship between input and output is site dependent; therefore, empir-\n\
    ical relationships, despite their fewer input data requirements, are valid only\
    \ under speciﬁc\noperational conditions in which the samples were collected [2–4].\
    \ Furthermore, varying\nenvironmental conditions in addition to some matters such\
    \ as nonlinearity, outlier data,\nmulticollinearity, and heteroscedasticity can\
    \ decrease the estimation accuracy of statistical\nmodels due to the lack of physical\
    \ structure.\nPhysical models, including agronomical models, hydrological models,\
    \ earth system\nmodels, and land surface models (LSMs), are based on assumptions\
    \ that simplify the\nreal phenomena, which leads to more general use in comparison\
    \ to collections of in situ\nmeasurements. However, these simplifying assumptions\
    \ lead to uncertainty in estimating\nthe soil temperature at the continental or\
    \ global scales. Physical models suffer from high\ncomplexity, intensive input\
    \ data requirement, and high computational cost in estimating\nnear-real-time\
    \ soil temperature, which makes accurate estimation of physical parameters\nchallenging\
    \ [5]. Additionally, errors due to forcing data and parameters potentially lead\
    \ to\nuncertainties in the long-term monitoring of soil temperature. Although\
    \ data assimilation\ntechniques such as the land data assimilation system (LDAS)\
    \ can improve the accuracy of\nestimates, errors due to the physics of these models\
    \ are inevitable [6].\nWith the signiﬁcant progress in computer technology, various\
    \ machine learning (ML)\ntechniques, known as computational artiﬁcial intelligence-based\
    \ (AI) models with a dy-\nnamic input-output mapping approach, were developed\
    \ to address the aforementioned\ndrawbacks. These data-driven models are mathematical\
    \ representations built based on\ntraining data to estimate target variables by\
    \ analyzing data attributes. They are capable\nof approximating complex nonlinear\
    \ relationships, including nonlinear, nonmonotonic,\nand multimodal relationships,\
    \ with limited assumptions about data distribution as well\nas predeﬁned conceptual\
    \ relationships between input and output data because of their\nadvanced learning\
    \ strategies. Due to this property, a wide range of diverse datasets with\nunknown\
    \ probability density functions can be used to estimate variables of interest\
    \ using\nML with better performance than other parametric methods [2] Moreover,\
    \ multi-source\nusage makes ML models suitable for large-scale studies such as\
    \ climate and hydrology\nstudies. Since ML techniques rely on the relationships\
    \ between model inputs and desired\noutputs, they can accurately represent earth\
    \ processes without an explicit structure.\nUnlike empirical models, ML-based\
    \ outputs can be applied as important information\nsources to retrieve soil temperature\
    \ in unsampled areas [7,8]. In addition, these methods\nwith high computational\
    \ speed and high adaptability to different inputs and conﬁgurations\nare superior\
    \ to LSMs and Soil Vegetation Atmosphere Transfer (SVAT) models [9]. However,\n\
    ML methods need a large number of samples with known target labels to capture\
    \ the\nSustainability 2023, 15, 7677\n3 of 26\nvariability of complex systems.\
    \ Other challenges are overﬁtting the models to the training\ndata as well as\
    \ low convergence during the learning process [10].\nIf an ML-based model is sufﬁciently\
    \ trained to represent the physical process, it can\nprovide accurate predictions\
    \ of data not included in the training process, i.e., the general-\nization concept\
    \ [11]. Thus, introducing physical principles and constraints into machine\nlearning\
    \ techniques can improve the estimation accuracy of the model by reducing the\n\
    optimization search space to physically based possibilities [12–14]. Furthermore,\
    \ apply-\ning a penalty to the optimization process because of inconsistent predictions\
    \ can drive\nthe optimization process to reduce error and generate physically\
    \ meaningful results. In\naddition to the training process, ML models are highly\
    \ dependent on input variables that\nare correlated with soil temperature, including\
    \ climatic variables such as air temperature\nand precipitation, soil properties\
    \ such as soil texture and land cover, and vegetation in-\ndices [15,16]. In other\
    \ words, the quality and quantity of training data signiﬁcantly affect\nthe performance\
    \ of ML models. As a result, the ML models are incapable of capturing\nthe relationships\
    \ between data and generalizing appropriate patterns to new datasets for\nsmall\
    \ datasets, resulting in an overﬁtted model. In contrast, noisy data may lead\
    \ to the\nmodel underﬁtting to the training datasets if a relatively simple learning\
    \ structure is used.\nFurthermore, limited datasets representing speciﬁc conditions\
    \ cause the model to fail to\nrecognize true underlying patterns. A data preprocessing\
    \ process, such as bias reduction,\ndata normalization, and data size increment,\
    \ can mitigate these issues, in addition to the\nfeature selection process that\
    \ determines which data are appropriate for input.\nThe advent of ML-based soil\
    \ temperature prediction occurred when [17] used relative\nhumidity, wind speed,\
    \ and air temperature, as inputs in both single-layer and multi-\nlayer neural\
    \ networks to predict soil temperature, thereby providing a foundation for\nmachine\
    \ learning as a method for soil temperature estimation using climate variables.\
    \ Since\nthen, many studies proposed other ML models and input variables for soil\
    \ temperature\nprediction. Given that ML models were successfully used to retrieve\
    \ soil characteristics for\ntwo decades, the current research addresses the application\
    \ of different machine learning\nmethodologies in predicting soil temperature.\
    \ These predictive models along with decision-\nsupport systems can alleviate\
    \ the time and operational costs associated with instrumental\nmeasurements. The\
    \ remainder of this paper is organized as follows. In Section 2, different\nartiﬁcial\
    \ intelligence-based models are classiﬁed into four main categories: (i) artiﬁcial\n\
    neural networks, (ii) deep learning, (iii) kernel models, and (iv) hybrid models,\
    \ in which\nthe studies carried out by these methods are assessed in detail. The\
    \ limitations and future\nperspectives are also discussed in Section 3.\n2. AI-Based\
    \ Models for Soil Temperature Estimation\n2.1. Artiﬁcial Neural Networks\nAn artiﬁcial\
    \ neural network (ANN), which is a universal approximator for nonlinear\nmapping,\
    \ classically comprises three layers including input, hidden, and output layers,\
    \ as\nshown in Figure 1. The model can learn the multivariate non-linear relationships\
    \ between\nthe input and output without physical concepts and generalize them\
    \ to other points [18,19].\nThus, ANNs eliminate the need for an explicit conﬁguration\
    \ of physical relationships by\ndeﬁning the input–output relationship, which prevents\
    \ the models from errors caused by\nincorrect associations. Moreover, these models\
    \ beneﬁt from a relatively low computational\ncost due to the one-time calibration.\n\
    The most widely used variants of ANN include feed-forward neural networks (FFNNs)\n\
    or multilayer perceptrons (MLPs), radial basis neural networks (RBNN), and generalized\n\
    regression neural networks (GRNNs). Despite the different structures of ANN models,\
    \ all\nof them consist of two functional components, i.e., learning and optimization\
    \ processes for\nclassifying inputs to hidden layers and to outputs, respectively.\
    \ The first component, known as\nthe input-hidden component, detects nonlinearity\
    \ in the input data and classifies inputs into\nhidden layers by a learning algorithm\
    \ (supervised or unsupervised). The second functional\ncomponent, known as the\
    \ hidden-output component, includes an optimization process to find\nSustainability\
    \ 2023, 15, 7677\n4 of 26\nthe best match between the classified inputs and targets.\
    \ The classified inputs are mapped to\nthe outputs by linear projection, which\
    \ adjusts the model parameters by a negative gradient\nalgorithm by minimizing\
    \ the mean square error between the ANN output and target data.\nANNs are capable\
    \ of dealing with a large amount of data because of the separate training\nprocess\
    \ of the components. In addition, an adaptive procedure can be used to recursively\n\
    update the parameters when the observational data of interest are sufficient.\n\
    (FFNNs) or multilayer perceptrons (MLPs), radial basis neural networks (RBNN),\
    \ and \ngeneralized regression neural networks (GRNNs). Despite the different\
    \ structures of \nANN models, all of them consist of two functional components,\
    \ i.e., learning and optimi-\nzation processes for classifying inputs to hidden\
    \ layers and to outputs, respectively. The \nfirst component, known as the input-hidden\
    \ component, detects nonlinearity in the input \ndata and classifies inputs into\
    \ hidden layers by a learning algorithm (supervised or unsu-\npervised). The second\
    \ functional component, known as the hidden-output component, \nincludes an optimization\
    \ process to find the best match between the classified inputs and \ntargets.\
    \ The classified inputs are mapped to the outputs by linear projection, which\
    \ adjusts \nthe model parameters by a negative gradient algorithm by minimizing\
    \ the mean square \nerror between the ANN output and target data. ANNs are capable\
    \ of dealing with a large \namount of data because of the separate training process\
    \ of the components. In addition, \nan adaptive procedure can be used to recursively\
    \ update the parameters when the obser-\nvational data of interest are sufficient.\
    \ \n \nFigure 1. Structure of the ANN model. \nANNs are the most popular type\
    \ of AI models for approximating hydrological and \nenvironmental components such\
    \ as soil temperature with a long-term history. For exam-\nple, ref. [20] compared\
    \ the performance of three AI-based models, i.e., MLPs, GRNNs, and \nRBNNs, as\
    \ well as multiple linear regression (MLR) in modeling monthly soil tempera-\n\
    tures at different depths. They assessed the effect of climatic data, including\
    \ relative hu-\nmidity, solar radiation, wind speed, and air temperature, on the\
    \ resulting soil temperature \nby GRNN, among which the air temperature was identified\
    \ as the most effective variable. \nAccording to the results, RBNN performed better\
    \ than MLP and GRNN in estimating soil \ntemperature at depths of 5 and 10 cm,\
    \ while MLR and GRNN models presented the best \naccuracy at 50 and 100 cm, respectively.\
    \ In addition to climatic data, the effect of periodic-\nity on model accuracy\
    \ in the training, validation, and test periods was investigated, lead-\ning to\
    \ a decrease in the root mean square error (RMSE) of GRNN, MLP, RBNN, and MLR\
    \ \nby 19, 15, 19, and 15 %, respectively. \nFigure 1. Structure of the ANN model.\n\
    ANNs are the most popular type of AI models for approximating hydrological and\n\
    environmental components such as soil temperature with a long-term history. For\
    \ example,\nref. [20] compared the performance of three AI-based models, i.e.,\
    \ MLPs, GRNNs, and\nRBNNs, as well as multiple linear regression (MLR) in modeling\
    \ monthly soil temperatures\nat different depths. They assessed the effect of\
    \ climatic data, including relative humidity,\nsolar radiation, wind speed, and\
    \ air temperature, on the resulting soil temperature by\nGRNN, among which the\
    \ air temperature was identiﬁed as the most effective variable.\nAccording to\
    \ the results, RBNN performed better than MLP and GRNN in estimating soil\ntemperature\
    \ at depths of 5 and 10 cm, while MLR and GRNN models presented the best\naccuracy\
    \ at 50 and 100 cm, respectively. In addition to climatic data, the effect of\
    \ periodicity\non model accuracy in the training, validation, and test periods\
    \ was investigated, leading to\na decrease in the root mean square error (RMSE)\
    \ of GRNN, MLP, RBNN, and MLR by 19,\n15, 19, and 15 %, respectively.\nRef. [21]\
    \ used several data-intelligent ML models, i.e., ANN, extreme learning machine\n\
    (ELM), and M5 Model Tree (M5 Tree), to estimate monthly soil temperatures at 5,\
    \ 50, and\n100 cm. The models were trained by meteorological information obtained\
    \ from two stations\nin Turkey, including air temperature, relative humidity,\
    \ wind speed, solar radiation, and\nperiodicity. The ELM model, with the highest\
    \ accuracy and lowest error, was considered\nthe most accurate model at a depth\
    \ of 50 cm.\nEven though climatic data may be unavailable in some regions, most\
    \ studies use\nmeteorological parameters as input data to estimate soil temperature.\
    \ For example, ref. [22]\nemployed geographical information, i.e., latitude, longitude,\
    \ and altitude as well as the cal-\nendar month, to estimate monthly soil temperature\
    \ at depths of 5, 10, 50, and 100 cm using\nSustainability 2023, 15, 7677\n5 of\
    \ 26\nANN, adaptive neuro-fuzzy inference system (ANFIS) and gene expression programming\n\
    (GEP). According to the results, the ANFIS model provided the most accurate estimates,\n\
    followed by the ANN and GEP. Ref. [23] predicted the monthly soil temperature\
    \ at a depth\nof 10 cm by ANN over a large spatial domain with complex terrain\
    \ in southwestern China.\nThe independent variables included geographical information\
    \ obtained from a digital\nelevation model and the Normalized Difference Vegetation\
    \ Index (NDVI) derived from\nsatellite imagery. Compared to multiple linear regressions,\
    \ ANNs improved the RMSE,\nmean absolute error (MAE), and R2 by about 44%, 70%,\
    \ and 18%, respectively.\nUsing Earth observation satellite data, ref. [24] evaluated\
    \ the predictive capability of\nfeed forward back propagation neural network (FFBPNN)\
    \ for land surface temperature by\npast land surface temperature (LST) values\
    \ as well as geographical characteristics. They\nconﬁrmed the ability of ANN to\
    \ learn and predict a non-linear complex dataset.\nThe aforementioned studies\
    \ modeled soil temperature on a monthly time scale while\ndaily or sub-daily resolutions\
    \ are more advantageous for agricultural purposes. Moreover,\nthe applicability\
    \ of these models is limited due to the lack of environmental and atmospheric\n\
    data used for soil temperature estimation. To address these issues, ref. [25]\
    \ employed ANN\nand wavelet neural network (WNN) models to forecast soil temperature\
    \ 1–7 days ahead\nby only surface air temperature data with hourly temporal resolution.\
    \ The results showed\nthat a wavelet transform, which decomposes the inputs into\
    \ low and high-frequency\ncomponents, can improve the prediction accuracy of soil\
    \ temperature.\nThe methods of ELM, GRNN, backpropagation neural networks (BPNN),\
    \ and random\nforests (RF) were exploited to derive half-hourly soil temperatures\
    \ at depths of 2 cm, 5 cm,\n10 cm, and 20 cm from datasets of air temperature,\
    \ relative humidity, solar radiation,\nwind speed, and vapor pressure deﬁcit [26].\
    \ Despite the desirable performance of all the\nmodels, the ELM model with a high\
    \ computational speed showed more accurate results at\ndifferent soil depths.\
    \ Ref. [27] used the MLP model, RF, Gaussian process (GaP), and M5P\nmodels to\
    \ estimate daily soil temperature in arid regions by climate data obtained from\n\
    two stations, which included sunshine, wind speed, relative humidity, and air\
    \ temperature.\nIt was found that the MLP model with RMSE ranging from 3.3 to\
    \ 6.3 ◦C performed\nbetter than the other models in estimating soil temperature\
    \ at a depth of 5 cm. Ref. [28]\nemployed MLP and MLR models to estimate the daily\
    \ soil temperature at 5, 10, 20, 30, 50,\nand 100 cm by daily meteorological data,\
    \ including relative humidity, solar radiation, air\ntemperature, and precipitation.\
    \ The results showed the superiority of MLP to MLR as well\nas the higher effectiveness\
    \ of air temperature and humidity over other variables. Ref. [29]\napplied MLP\
    \ and ANFIS models to estimate the daily soil temperature at two stations\nin\
    \ Illinois, USA. A variety of weather data, including air temperature, relative\
    \ humidity,\ndew point temperature, potential evapotranspiration, wind speed,\
    \ solar radiation, and soil\ntemperature at 10 and 20 cm depths were used to model\
    \ the soil temperature. The input\ndata for air temperature, wind speed, and solar\
    \ radiation were recognized as the best data\ncombination for both models. In\
    \ addition, it was found that the MLP model outperformed\nANFIS for both stations\
    \ at depths of 10 and 20 cm. Ref. [30] exploited ANN, MLR, and\nGEP to estimate\
    \ the multi-depth daily soil temperature at a station with a semiarid climate\n\
    in Iran. To adopt the best input data, they used 12 combinations of meteorological\
    \ data\nincluding relative humidity, wind speed, extraterrestrial radiation, sunshine\
    \ hours, and\nminimum and maximum air temperatures. Although all models were able\
    \ to estimate\nsoil temperature, the ANN model exhibited the best performance.\
    \ Ref. [31] applied ANN,\nWNN, and GEP models to estimate the daily soil temperature\
    \ at different depths in Iran\nbased on single-station measurements. At all depths,\
    \ air temperature, radiation, and\nsunshine hours had the greatest effect on the\
    \ soil temperature prediction. The results\nrevealed that the best performance\
    \ belonged to WNN with the lowest RMSE and highest R;\nhowever, the estimation\
    \ accuracy as well as the effect of climatic factors decreased with\nincreasing\
    \ soil depth.\nSince neural network (NN)-based soil temperature models are location-speciﬁc,\
    \ they\ncannot be extended to other locations because of the unique relationships\
    \ between soil\nSustainability 2023, 15, 7677\n6 of 26\ntemperature and climate\
    \ conditions at the training location. However, it is possible to build\nspatially\
    \ distributed temperature models if sufﬁcient temperature data are initially known.\n\
    In this regard, ref. [32] used an ensemble approach using machine learning techniques\
    \ such\nas ANN, decision tree (DT), and k-nearest neighbors (k-NN) to predict\
    \ spatially distributed\nland surface temperatures from a given dataset of land\
    \ temperature at known locations. The\nstatistical indices showed good performance\
    \ of the proposed model built by DT, variable\nridge regression (VRR), and conditional\
    \ inference tree (CIT).\nSome studies evaluated the efﬁciency of ML models for\
    \ predicting soil temperature\nonly at shallow depths, while the relationship\
    \ between effective parameters and soil\ntemperature changes with the depth of\
    \ the soil temperature sample, leading to a need for\nmulti-depth evaluation.\
    \ In this regard, ref. [33] used an ANN and a co-active neuro-fuzzy\ninference\
    \ system (CANFIS) to predict the soil temperature at two locations in Iran at\
    \ depths\nfrom 5 to 100 cm using only air temperature data. The resulting models\
    \ performed better\nfor shallower sample depths, and stations in dry climates.\
    \ The authors hypothesized that\nmoisture in humid climates may act as a thermal\
    \ insulator, interfering with the relationship\nbetween the air and soil temperatures.\
    \ Ref. [34] modeled soil temperature at depths from\n5 to 100 cm at Mersin station,\
    \ Turkey, by using ELM, ANN, classiﬁcation and regression\ntrees (CRT), and the\
    \ group method of data handling (GMDH). They also investigated\nthe models’ sensitivity\
    \ to the input variables, including air temperature, solar radiation,\nwind speed,\
    \ relative humidity, and soil temperature. According to the results, the ELM\n\
    performed best among all the models. The models for shallow soil temperature performed\n\
    best and were found to rely only on the climate input of air temperature; while\
    \ at deeper\ndepths, more input variables (namely wind speed and solar radiation)\
    \ were required.\nThe aforementioned studies used input data that associate concurrent\
    \ climate pa-\nrameters, and each individual set of measurements stands on its\
    \ own, with no inherent\nconnection in time to other data points. This data structure\
    \ makes it necessary to introduce\nother techniques to account for time-related\
    \ behavior. The simplest technique to account\nfor seasonal cycles in soil temperature\
    \ is to include the month of the year and, optionally,\nthe day as well, as another\
    \ input parameter. With this approach, ref. [35] investigated soil\ntemperatures\
    \ in Adana, Turkey, at ﬁve depths from 5 to 100 cm. The soil temperature was\n\
    modeled using linear regression (LR) and nonlinear regression (NLR) as well as\
    \ a feedfor-\nward neural network using different input sets depending on the\
    \ season: air temperature,\ndepth, and month were used year-round, and in the\
    \ hot season atmospheric pressure,\nand solar radiation inputs were added. The\
    \ results revealed the superior performance\nof FFNN over linear and nonlinear\
    \ models. In addition, the results did not show any\nchange in the model performance\
    \ with sample depth for the neural network. Ref. [36]\nmodeled soil temperature\
    \ data from across Turkey using ANN, ANFIS, and MLR models.\nThe study found that\
    \ monthly soil temperature could be predicted using monthly mini-\nmum and maximum\
    \ air temperatures, soil depth, calendar month number, and monthly\nprecipitation.\
    \ The models were built using all soil temperature data from different soil\n\
    depths and climate regions, which would implicitly have different relationships\
    \ with the\nclimate input variables. The success of the models, especially the\
    \ ANFIS model, indicates\nthat these differences could be accounted for using\
    \ the inputs of climate variables and\nsoil depth, allowing for a model that was\
    \ not intrinsically tied to a single location or soil\ndepth. [37] used ANN models\
    \ to predict soil temperature at locations across Turkey using\nthe input climate\
    \ parameters of solar radiation, sunshine duration, and air temperature;\nlocation\
    \ parameters of altitude, latitude, and longitude; and time parameters of month\n\
    and year. The best performing model was for the 20 cm depth, and the 100 cm depth\n\
    model performed marginally worse. The inclusion of the year in the input is an\
    \ interesting\nchoice: it limits the models’ applicability to make predictions\
    \ for the future, where the\nyear will, by nature, be outside the range used for\
    \ training the model. However, within\nthe range of historical data, using year\
    \ as an input variable allows the model to account\nfor changes in seasonal patterns\
    \ from year to year. Including only the month and day in\nthe input variables\
    \ inherently assumes that the seasonal patterns do not vary from year to\nSustainability\
    \ 2023, 15, 7677\n7 of 26\nyear; however, natural variation in weather patterns\
    \ as well as climate change contradict\nthis assumption.\nAnother way of accounting\
    \ for time without assuming identical yearly patterns is to\nuse a sliding window\
    \ approach. With this method, the soil temperature at a given time\nis modeled\
    \ with an input of climate variables from the same time and from one or more\n\
    previous time steps. Ref. [38] modeled the soil temperature at two study sites\
    \ in Iran\nusing MLPs and support vector machines (SVMs) with and without optimization\
    \ via the\nﬁreﬂy algorithm (FFA). The input climate variables (air temperature,\
    \ wind speed, relative\nhumidity, and sunshine hours) from the current day, the\
    \ previous day, and, in some models,\nthe day prior to that were all used as model\
    \ inputs. The study concluded that using two\ndays of previous data led to the\
    \ best results, and the models that used FFA outperformed\nthe standalone models\
    \ without optimization. Ref. [39] investigated two stations in different\nclimate\
    \ regions of Iran, with sampling depths ranging from 5 to 100 cm. They used seasonal\n\
    autoregressive integrated moving average (SARIMA), a linear stochastic approach,\
    \ with\nseasonal standardization and spectral analysis in preprocessing. This\
    \ model was compared\nto three machine learning models, i.e., ELM, self-adaptive\
    \ evolutionary ELM (SaE-ELM),\nand ANFIS, and was shown to outperform them. The\
    \ SARIMA model showed no loss of\nquality for deeper soil temperatures and performed\
    \ moderately better at deeper depths.\nOne difﬁculty in the sliding window approach\
    \ is that it makes assumptions about the\nlag time between the causal climate\
    \ variable and the effect on soil temperature (namely, lag\ntimes equal to or\
    \ less than the window length). The actual lag times may be long or follow\ncomplex\
    \ patterns. Along with building several machine learning models to predict hourly\n\
    soil temperatures at their sites in Iran, ref. [40] used wavelet decomposition\
    \ to assess the\ncoherence between various climate parameters and soil temperature.\
    \ They observed that\nair temperature and soil temperature were highly coherent\
    \ at a periodicity of 4 to 8 h for\na soil depth of 5 cm, and 8 to 16 h at depths\
    \ of 10 and 30 cm. Relative humidity and soil\ntemperature were coherent at a\
    \ periodicity of 16 to 64 h for 5 cm depth, and 4 to 8 h for\n10 and 30 cm depths.\
    \ While most studies examine soil temperature over larger time scales\n(i.e.,\
    \ daily, not hourly), the different periodicities between different climate variables\
    \ and\nsoil temperatures imply that the best sliding window models of soil temperature\
    \ would\nneed to include data from many different previous time steps. Furthermore,\
    \ the fact that\nthe periodicity of different climate variables changed in opposite\
    \ directions upon changing\nthe soil depth indicates that the sliding window would\
    \ have to be different for models\npredicting temperatures at different depths.\n\
    Compared with other artiﬁcial intelligence models, ANNs can better extract the\
    \ hidden\nfeatures of big data. However, they are usually based on single data\
    \ feature extraction\nwithout accounting for the spatiotemporal patterns of the\
    \ data, which leads to a decrease\nin the prediction accuracy of soil temperature.\
    \ In this regard, increasing the neural network\ndepth can improve the optimization\
    \ rate and model expression such as in deep learning\n(DL) models [41], which\
    \ are discussed in the next section. A list of studies related to soil\ntemperature\
    \ estimation by ANN models is given in Table 1.\nTable 1. A list of studies carried\
    \ out to estimate soil temperature by ANNs.\nResearch\nModels\nOutput\nInput\n\
    Soil Depth\nPerformance\nCriteria\nBest Model(s)\n[20]\nMLP, GRNN,\nRBNN, MLR\n\
    Monthly soil\ntemperature\nRelative humidity, solar\nradiation, wind speed, air\n\
    temperature, soil\ntemperature\n5, 10, 50, 100 cm\nRMSE, MAE, R2\nRBNN at depths\n\
    of 5 and 10 cm,\nMLR at depth of\n50 cm,\nGRNN at depth\nof 100 cm\n[21]\nANN,\
    \ ELM, M5\nTree\nMonthly soil\ntemperature\nAir temperature, relative\nhumidity,\
    \ wind speed,\nsolar radiation,\nperiodicity\n5, 50, 100 cm\nR, RMSE, MAE,\nWI,\
    \ NSE,\nLMI\nELM\nSustainability 2023, 15, 7677\n8 of 26\nTable 1. Cont.\nResearch\n\
    Models\nOutput\nInput\nSoil Depth\nPerformance\nCriteria\nBest Model(s)\n[22]\n\
    ANN, ANFIS,\nGEP\nMonthly soil\ntemperature\nLatitude, longitude,\naltitude, number\
    \ of\nmonths\n5, 10, 50, 100 cm\nR2, RMSE, MAE\nANFIS\n[23]\nANN\nMonthly soil\n\
    temperature\nLatitude, longitude,\nelevation, topographic\nwetness index, NDVI\n\
    10 cm\nRMSE, MAPE, R2\n[24]\nFFBPNN\nLand surface\ntemperature at 14\nyears’ interval\n\
    A sequence of past LST\nvalues, latitude, longitude\n-\nR, MSE\n[25]\nANN, WNN\n\
    Next 1 to 7 day\nsoil temperature\nSurface air temperature\n5, 10, 20, 30 cm\n\
    RMSE\nWNN\n[26]\nELM, GRNN,\nBPNN, RF\nHalf-hourly soil\ntemperature\nAir temperature,\n\
    wind speed, relative\nhumidity, solar radiation,\nand vapor pressure deﬁcit\n\
    2, 5, 10, 20 cm\nRMSE, MAE,\nNSE, R\nELM\n[27]\nMLP, RF, GP, M5P\nDaily soil\n\
    temperature\nSunshine hours, wind\nspeed, relative humidity,\nair temperature\n\
    5 cm\nMAE, RMSE, R\nMLP\n[28]\nMLP, MLR\nDaily soil\ntemperature\nAir temperature,\
    \ solar\nradiation, relative\nhumidity, precipitation\n5, 10, 20, 30, 50,\n100\
    \ cm\nR, RMSE, MAE\nMLP\n[29]\nMLP, ANFIS\nDaily soil\ntemperature\nAir temperature,\
    \ relative\nhumidity, dew point\ntemperature, potential\nevapotranspiration, wind\n\
    speed, solar radiation, soil\ntemperature\n10, 20 cm\nNSE, RMSE,\nMAE, APE\nMLP\n\
    [30]\nGEP, ANN, MLR\nDaily soil\ntemperature\nRelative humidity, wind\nspeed,\
    \ extraterrestrial\nradiation, sunshine hours,\nminimum and maximum\nair temperature\n\
    5, 10, 20, 30, 50,\n100 cm\nR2, RMSE\nANN\n[31]\nANN, WNN,\nGEP\nDaily soil\n\
    temperature\nAir temperature, solar\nradiation, pressure, soil\ndepth, periodicity\n\
    10, 20, 30, 50,\n100 cm\nR, MAE, RMSE,\nAIC, Taylor\ndiagrams\nWNN\n[32]\nAn ensemble\n\
    approach based\non ANN, MARS,\nCIT, DT, ICR,\nk-NN, LAR,\nNNLS, NCPQR,\nPCA, Lasso,\
    \ VRR,\nPPR\nLand Surface\nTemperature\nLatitude, longitude,\ntemperature\nTotal\n\
    computation time,\nRMSE, R2\nModel built by\nDT, VRR, and\nCIT.\n[33]\nANN, CANFIS\n\
    Daily soil\ntemperature\nAir temperature\n5, 10, 20, 30, 50,\n100 cm\nRMSE, R\n\
    ANN\n[34]\nELM, ANN, CRT,\nGMDH\nMonthly soil\ntemperature\nAir temperature, relative\n\
    humidity, solar radiation,\nwind speed\n5, 10, 50, 100 cm\nRMSE, R2\nELM\n[35]\n\
    FFBPNN, LR,\nNLR\nMonthly soil\ntemperature\nAir temperature,\natmospheric pressure,\n\
    solar radiation, depth,\nmonth\n5, 10, 20, 50,\n100 cm\nMAPE, R\nFFBPNN\n[36]\n\
    ANN, ANFIS,\nMLR\nMonthly soil\ntemperature\nMinimum and maximum\nair temperature,\
    \ calendar\nmonth number, depth of\nsoil, precipitation\n5, 10, 20, 50,\n100 cm\n\
    RMSE, MAE, R2\nANFIS\nSustainability 2023, 15, 7677\n9 of 26\nTable 1. Cont.\n\
    Research\nModels\nOutput\nInput\nSoil Depth\nPerformance\nCriteria\nBest Model(s)\n\
    [37]\nFFNN\nMonthly mean\nsoil temperature\nAltitude, latitude,\nlongitude, month,\
    \ year,\nsolar radiation, sunshine\nduration, air temperature\n5, 10, 20, 50,\n\
    100 cm\nRMSE, R\n[38]\nSVM, MLP,\nSVM-FFA,\nMLP-FFA\nSoil temperature\nair temperature,\
    \ relative\nhumidity, sunshine hours,\nwind speed\n5, 10, 20 cm\nRMSE, MAE, R\n\
    MLP-FFA\n[39]\nSARIMA, ELM,\nSaE-ELM, ANFIS\nDaily soil\ntemperature\nSoil temperature\n\
    5, 10, 20, 30, 50,\n100 cm\nRMSE, MAPE, R2\nSARIMA\n[40]\nANFIS, SVM,\nRBNN, MLP\n\
    optimized by the\nFFA, SFO, SSA,\nand PSO\nalgorithms\nHourly soil\ntemperature\n\
    Air temperature, relative\nhumidity, solar radiation,\nwind speed\n5, 10, 30 cm\n\
    NSE, RMSE,\nMAE, R2, PBIAS\nANFIS-SFO\nAbbreviation: Willmott Index of Agreement\
    \ (WI), Legates and McCabe index (LMI), Mean Squared Error (MSE),\nAbsolute Percentage\
    \ Error (APE), Akaike Information Criterion (AIC), Multivariate Adaptive Regression\
    \ Spline\n(MARS), Independent Component Regression (ICR), Least Angle Regression\
    \ (LAR), Non Negative Least Square\n(NNLS), Non Convex Penalized Quantile Regression\
    \ (NCPQR), Principal Component Analysis (PCA), Projection\nPursuit Regression\
    \ (PPR), SunFlower Optimization (SFO), Salp Swarm Algorithm (SSA), Particle Swarm\
    \ Optimization\n(PSO), Nash–Sutcliffe Efficiency (NSE), Percent Bias (PBIAS),\
    \ Mean Absolute Percentage Error (MAPE).\n2.2. Deep Learning\nTraditional neural\
    \ network models are incapable of processing big data and have\nlimited generalization\
    \ ability and scalability. Moreover, their slow training process and\neasy falling\
    \ into local optima are other weaknesses of standard neural networks. There-\n\
    fore, ref. [42] developed deep learning (DL) models to deal with large multi-feature\
    \ data\nthrough a multiple hidden layer structure. Compared to traditional neural\
    \ networks, DL\nmodels, which are known as large-sized neural networks, as shown\
    \ in Figure 2, have\nstrong computing power which can automatically extract high\
    \ level features from low level\ninput data by intermediate layers with learning\
    \ complex nonlinear functions in different\nﬁelds such as search engines and image\
    \ recognition. Since soil has extremely complex and\nnonlinear structural characteristics,\
    \ DL models are more effective in analyzing soil particle\nsize and soil texture\
    \ than traditional models [16,43]. In addition, these methods simulate\nland-atmosphere\
    \ interactions without complex knowledge governing physical models.\nThese models\
    \ can efﬁciently process future data with a high computational speed and\nintegrate\
    \ more data to improve prediction accuracy through additional learning. However,\n\
    an inappropriate sample size of the input data may cause overﬁtting, thereby decreasing\n\
    the prediction accuracy.\nThe most common DL models for environmental prediction\
    \ include spatial models\n(Convolutional Neural Networks, CNNs by [44]), temporal\
    \ models (Long Short-Term\nMemory networks, LSTMs by [45]) and spatial–temporal\
    \ models (Convolutional LSTM,\nConvLSTM by [46]).\nThe CNN, as a classical data-driven\
    \ DL model, provides a powerful feature extraction\ntool to improve the predictive\
    \ process by identifying the spatial features of data in multiple\narrays. However,\
    \ the CNN model, with decreasing number of layers, performs poorly in\nfeature\
    \ extraction. To address this issue, ref. [47] developed the residual network\
    \ (ResNet)\nby introducing the residual block to construct deep neural networks.\n\
    LSTM models, which are a type of recurrent neural networks (RNN), are applied\n\
    to deal with sequential data in order to predict the time series of target variables\
    \ by\nlearning from past observations. Therefore, these models, which are able\
    \ to simulate\nnonlinear dynamic systems with minimal input data, retain the advantageous\
    \ information\nof long-term time series data across multiple time steps. Although\
    \ RNN is well suited for\nsimulating time series data because it preserves the\
    \ information of previous time steps\nSustainability 2023, 15, 7677\n10 of 26\n\
    through internal self-looped cells, the problem of gradient disappearance occurring\
    \ in the\nlong-range memory of the original RNN restricts its applicability in\
    \ predicting targets.\nTo overcome this problem, ref. [45] used a gate mechanism\
    \ to obtain information in an\nRNN-based LSTM network model. However, LSTM neglects\
    \ the learning of backward\nfeatures, which leads to the development of the Bidirectional\
    \ LSTM (BiLSTM) network\nmodel by combining backward and forward LSTMs.\neasy\
    \ falling into local optima are other weaknesses of standard neural networks.\
    \ There-\nfore, ref. [42] developed deep learning (DL) models to deal with large\
    \ multi-feature data \nthrough a multiple hidden layer structure. Compared to\
    \ traditional neural networks, DL \nmodels, which are known as large-sized neural\
    \ networks, as shown in Figure 2, have \nstrong computing power which can automatically\
    \ extract high level features from low \nlevel input data by intermediate layers\
    \ with learning complex nonlinear functions in dif-\nferent fields such as search\
    \ engines and image recognition. Since soil has extremely com-\nplex and nonlinear\
    \ structural characteristics, DL models are more effective in analyzing \nsoil\
    \ particle size and soil texture than traditional models [16,43]. In addition,\
    \ these meth-\nods simulate land-atmosphere interactions without complex knowledge\
    \ governing phys-\nical models. These models can efficiently process future data\
    \ with a high computational \nspeed and integrate more data to improve prediction\
    \ accuracy through additional learn-\ning. However, an inappropriate sample size\
    \ of the input data may cause overfitting, \nthereby decreasing the prediction\
    \ accuracy. \n \nFigure 2. Structure of the DL model. \nThe most common DL models\
    \ for environmental prediction include spatial models \n(Convolutional Neural\
    \ Networks, CNNs by [44]), temporal models (Long Short-Term \nMemory networks,\
    \ LSTMs by [45]) and spatial–temporal models (Convolutional LSTM, \nConvLSTM by\
    \ [46]). \nFigure 2. Structure of the DL model.\nThe CNN and LSTM models only\
    \ capture spatial and temporal variations, while the\ntarget variables have a\
    \ correlation between space and time. This problem was solved via\nthe ConvLSTM\
    \ model, which is a combination of the CNN and LSTM models. Unlike the\nCNN and\
    \ LSTM models, ConvLSTM using the output of the previous layer as the input for\n\
    the next layer is capable of extracting both temporal and spatial features by\
    \ a convolution\nlayer at the same time without decreasing the map size.\nSome\
    \ researchers exploited CNN, LSTM, and ConvLSTM models to estimate and pre-\n\
    dict the soil temperature. For example, ref. [48] modeled soil temperature using\
    \ persistence\nforecast (PF), BPNN, LSTM, and CNN trained with data decomposed\
    \ using ensemble em-\npirical mode decomposition (EEMD). They investigated three\
    \ sites in Switzerland, at depths\nbetween 5 and 30 cm. The EEMD-based models\
    \ provided the best results, and no model\nperformance differences were noted\
    \ between depths. Ref. [49] built on this work, focusing on\nthe proposed EEMD-Conv3d\
    \ model and extending it to predict spatiotemporally distributed\nsoil temperature\
    \ by air temperature data. The model showed the best performance among the\nConv\
    \ (2d and 3d), ConvLSTM, EEMD-Conv2D, and EEMD-ConvLSTM models.\nRef. [50] predicted\
    \ the soil temperature in Ottawa, Canada, at depths between 0 and 7 cm.\nThey\
    \ used a wide variety of models and input variables in order to form a comprehensive\
    \ view\non the best possible models for soil temperature prediction. Their modeling\
    \ methods included\nLRs, k-NNs, DT-based models, SVMs, stacking methods, MLP,\
    \ DL, and ANFIS models. The\ninput variables were the air temperature, precipitation,\
    \ surface pressure, evaporation, wind\nspeed, dew point temperature, solar radiation,\
    \ and thermal radiation. The three best-performing\nmodels were the DL model,\
    \ MLP, and stacking model; the choice of the best model among the\nthree depended\
    \ on the size of the dataset and the computational requirements. These three\n\
    models were used in a sensitivity analysis to determine the most relevant input\
    \ parameters,\nwhich were identified as air temperature and solar radiation.\n\
    Ref. [51] used 30 measurement stations across the US, representing different climates.\n\
    BiLSTM was used to model soil temperatures at these locations, which trains models\
    \ with\ndata points from both previous and future time steps. BiLSTM outperformed\
    \ the other\nSustainability 2023, 15, 7677\n11 of 26\nmodels at all but the shallowest\
    \ depth, demonstrating that using climate variables and time\ndata together eliminate\
    \ the decreased effectiveness of models at deep depths. The study\nalso explicitly\
    \ shows the correlation of different climate variables with soil temperatures\n\
    at different depths, all of which decrease with increasing depth except for month\
    \ and day.\nThis is in keeping with the result from ANNs that using current climate\
    \ variables without\ntime data leads to less accurate models at greater depths.\n\
    Ref. [52] proposed an embedded network prediction model based on the gated recurrent\n\
    unit (GRU), a modified type of LSTM, to learn the features of the historical soil\
    \ temperature\nat local and global scales. By comparing the model with ANN, ELM,\
    \ and LSTM, it was\nconcluded that the proposed model outperformed the other models\
    \ in predicting multi-depth\nsoil temperature at different time points, including\
    \ 6 h, 12 h, and 24 h.\nRef. [53] compared the performance of ML-based models,\
    \ including the DL model\nand RBNN, with conventional approaches for soil temperature\
    \ interpolation in southeast\nCanada. According to the results, the spline deterministic\
    \ spatial interpolation method, even\nwith increasing the spline nonlinearity,\
    \ failed to provide the spatial distribution for the soil\ntemperature and soil\
    \ water content. On the other side, the ML models could capture the\nspatial variability\
    \ of soil temperature even in sharp transition areas. It should be noted that\n\
    deep learning, with a normalized RMSE of 9.0% and R2 of 89.2%, outperformed the\
    \ RBNN. A\nlist of studies related to soil temperature estimation by DL models\
    \ is given in Table 2.\nTable 2. Studies conducted to model soil temperature using\
    \ DL techniques.\nResearch\nModels\nOutput\nInput\nSoil Depth\nPerformance\nCriteria\n\
    Best Model(s)\n[48]\nEEMD-CNN,\nPF, BPNN,\nLSTM,\nEEMD-LSTM\nNext 1, 3, 5\nday’s\
    \ soil\ntemperature\nSoil temperature at\ndifferent depths\nand areas\n5, 10,\
    \ 30 cm\nMSE, RMSE,\nMAE, R2\nEEMD-CNN,\nEEMD-LSTM\n[49]\nEEMD-Conv3d,\nConv2D,\n\
    Conv3D,\nConvLSTM,\nEEMD-\nConv2D,\nEEMD-\nConvLSTM\nNext 1, 3, 5\nday’s soil\n\
    temperature\nSoil temperature\n7 cm\nMSE, RMSE,\nMAE, R2,\nMAPE\nEEMD-Conv3d\n\
    [50]\nLR, ridge\nregression,\nLasso, ENet,\nDT, RF, k-NN,\nXGBoost, SVM,\ngradient\n\
    boosting,\nstacking\nmethods, MLP,\nDL, ANFIS\nHourly soil\ntemperature\nAir temperature,\n\
    precipitation,\nsurface pressure,\nevaporation, wind\nspeed, dew point\ntemperature,\
    \ solar\nradiation, thermal\nradiation\n7 cm\nMAE, MSE,\nRMSE, R2,\nMAPE\nDL,\
    \ MLP,\nstacking model\n[51]\nBiLSTM, LSTM,\nDNN, RF, SVR,\nLR\nHourly soil\n\
    temperature\nMaximum and\nminimum air\ntemperature, wind\nspeed, solar\nradiation,\n\
    maximum and\nminimum relative\nhumidity, vapor\npressure, dew point\ntemperature\n\
    5, 10, 20, 50,\n100 cm\nRMSE, MAE,\nR2\nBiLSTM\nSustainability 2023, 15, 7677\n\
    12 of 26\nTable 2. Cont.\nResearch\nModels\nOutput\nInput\nSoil Depth\nPerformance\n\
    Criteria\nBest Model(s)\n[52]\nGRU-based\nmodel, ANN,\nELM, LSTM\nSoil\ntemperature\
    \ at\ndifferent time\npoints (6 h, 12 h,\n24 h)\nHistorical soil\ntemperature\n\
    5, 10, 15 cm\nRMSE, MAE,\nMSE, R2\nGRU-based\nmodel\n[53]\nRBFN, DL,\nspline\n\
    deterministic\nspatial\ninterpolation\nmethod\nSoil\ntemperature\nSoil temperature,\n\
    soil moisture,\nclimate data\n10 cm\nRMSE, NRMSE,\nSI, MAPE, Bias,\nR2, MAE, NSE,\n\
    VAF, AIC, MSE,\nMaxE\nDL\nAbbreviation: Deep Neural Network (DNN), Support Vector\
    \ Regression (SVR), Normalized RMSE (NRMSE),\nScatter Index (SI), Variance Accounted\
    \ For (VAF), Maximum Residual Error (MaxE).\n2.3. Kernel Models\nKernel functions\
    \ play a signiﬁcant role in machine learning because of their simplicity\nand\
    \ generality. Various kernel function embedded models were developed to capture\n\
    non-linear relationships among which the support vector machine (SVM) is a well-known\n\
    kernel model. This model was originally developed for classiﬁcation purposes in\
    \ which a\nclassiﬁcation hyperplane is obtained by maximizing the margin between\
    \ different classes\nas shown in Equation (1) [54].\nwx + b = 0\n(1)\nwhere w\
    \ is a weighting vector, x is the independent input vector, and b indicates the\
    \ bias.\nTo distinguish negative and positive samples, the hyperplane provides\
    \ two dashed lines\nw·x + b = −1 and w·x + b = 1 with a maximum distance of\n\
    2\n∥w∥. The optimization function\nis also expressed as follows:\n\x1A\nmin 1\n\
    2∥ w ∥2\nyi(wxi + b) ≥ 1, i = 1, 2, . . . l\n(2)\nwhere y is the dependent output\
    \ vector and l is the number of samples.\nSVMs models, which are based on the\
    \ statistical learning theory, can solve inverse\nproblems by past data to obtain\
    \ variables of interest forward in time. To predict the variables\nof interest,\
    \ these models formulate quadratic optimization by structural risk minimization\n\
    (SRM) instead of empirical risk minimization (ERM), which is their outstanding\
    \ charac-\nteristic, to ensure a global optimum [55]. This ML-based approach,\
    \ considered a sparse\nmethod without being affected by dimensionality, utilizes\
    \ the generation error bound and\nintensive loss function, leading to precise\
    \ predictions. Furthermore, SVMs can resist noises\nand generalize concepts under\
    \ limited data conditions [56]. However, kernel function\nselection is a challenging\
    \ stage of their learning process. In addition, the scale and speed of\nthe learning\
    \ process can be other limiting factors of SVM, especially for real-time data.\n\
    SVM models, because of their high processing speed and the need for fewer training\n\
    data besides appropriate performance, recently gained attention for extracting\
    \ geo-/bio-\nphysical parameters. For example, ref. [57] developed an SVM-based\
    \ model comprising an\nannual average soil temperature prediction model and a\
    \ daily soil temperature amplitude\nprediction model to retrieve the daily soil\
    \ temperature. The annual average of humidity,\nwind speed, radiation, soil, and\
    \ air temperature are employed to train the annual average\nsoil temperature prediction\
    \ model and the daily soil temperature amplitude prediction.\nSustainability 2023,\
    \ 15, 7677\n13 of 26\nDespite the signiﬁcant role of soil temperature in meteorological,\
    \ ecological, and hy-\ndrological processes, studies did not address soil temperature\
    \ evaluation with ﬁne temporal\nresolution in data-scarce regions. Furthermore,\
    \ most studies employed air temperature\nas an input training dataset to estimate\
    \ soil temperature while a signiﬁcant correlation\nwas observed between soil temperature\
    \ drop and soil moisture rise. To this end, ref. [58]\nexploited the SVM model\
    \ along with an extreme gradient boosting system (XGBoost),\nRF, and MLP to predict\
    \ the spatiotemporal variability of soil temperature in the Indian\nHimalayan\
    \ Region (IHR) by hourly and half-hourly time series data, including rainfall,\n\
    soil moisture, soil temperature, air temperature, relative humidity, vapor pressure\
    \ deﬁcit,\nand solar radiation. Different input data combinations were utilized\
    \ to estimate the soil\ntemperature: Meteorological parameters, Meteorological\
    \ parameters + rainfall, Meteorolog-\nical parameters + soil moisture, and Meteorological\
    \ parameters + rainfall + soil moisture.\nAmong the models, XGBoost showed the\
    \ best performance, followed by RF, MLP, and SVM.\nMoreover, the addition of soil\
    \ moisture, unlike rainfall, to meteorological data improved\nthe estimation accuracy\
    \ signiﬁcantly.\nIn addition to classiﬁcation, SVM can be used for regression\
    \ purposes, which is\nknown as support vector regression (SVR). The basis of SVR\
    \ is the iterative process of the\nsequential minimal optimization (SMO) algorithm\
    \ [59]. Similar to SVM, SVR, which can\ncapture nonlinear relationships between\
    \ input and output variables, is used to estimate soil\ntemperature due to its\
    \ robustness to noise and generalization ability under conditions with\nlimited\
    \ reference samples [60]. Ref. [61] investigated soil temperature at depths from\
    \ 10 to\n100 cm at ﬁve different locations across Iran, ranging from hyper-arid\
    \ to hyper-humid. SVR\nmodels were built with and without an input corresponding\
    \ to the month of the year, and\nthe models containing the month parameter were\
    \ found to be more accurate, especially in\ndeeper layers. Furthermore, the results\
    \ showed that SVR models were better in deep layers\nthan in shallow ones while\
    \ MLR models could predict daily soil temperature at surface\nlayers with relatively\
    \ good accuracy.\nRef. [62] assessed the accuracy of SVR and Elman neural network\
    \ (ENN) as well as\nSVR and ENN coupled with the ﬁreﬂy algorithm (SVR-FFA and\
    \ ENN-FFA) and the krill\nherd algorithm (SVR-KHA and ENN-KHA) in modeling soil\
    \ temperature at depths of 5, 10,\n20, 30, 50, and 100 cm. Various meteorological\
    \ data combinations were evaluated under ﬁve\nscenarios, which included (air temperature),\
    \ (air temperature, sunshine), (air temperature,\nsunshine, relative humidity),\
    \ (air temperature, sunshine, relative humidity, wind speed),\nand (air temperature,\
    \ sunshine, relative humidity, wind speed, pressure deﬁcit). With the\nbest performance\
    \ at 10 cm soil depth, all models showed a decrease in error from 5 to\n10 cm\
    \ and an increase from 10 to 100 cm. Additionally, optimization algorithms provided\n\
    relatively satisfactory results in modeling soil temperature, especially at lower\
    \ depths.\nRef. [63] estimated the daily soil temperature at depths of 5, 10,\
    \ 20, and 50 cm for\ntwo stations with semiarid and humid climates in Turkey by\
    \ Bayesian Tuned Support\nVector Regression (BT-SVR), Bayesian Tuned Gaussian\
    \ Process Regression (BT-GPR), and\nLSTM models. A variety of daily meteorological\
    \ indicators, including cloudiness, air\ntemperature, relative humidity, precipitation,\
    \ and pressure, were used as input variables,\nand soil temperature at different\
    \ depths was the output variable. To increase the accuracy,\nthe study used a\
    \ novel scheme based on the Bayesian optimization method to optimize the\nparameters\
    \ of kernel functions of the GPR and SVR models. A number of kernel functions\n\
    were used for GPR, such as constant, linear, polynomial, squared exponential,\
    \ rational\nquadratic, power, and Matern-3, as well as linear, polynomial, Gaussian,\
    \ and sigmoid\nfunctions for SVR. Among the models, the BT-GPR model exhibited\
    \ the highest accuracy\nfor both stations at depth of 5 cm. Table 3 presents a\
    \ list of studies carried out to model soil\ntemperature using kernel function\
    \ embedded models.\nSustainability 2023, 15, 7677\n14 of 26\nTable 3. Studies\
    \ conducted to model soil temperature using kernel functions.\nResearch\nModels\n\
    Output\nInput\nSoil Depth\nPerformance\nCriteria\nBest Model(s)\n[57]\nSVM\nDaily\
    \ soil\ntemperature\nHumidity, wind\nspeed, radiation,\nsoil temperature,\nair\
    \ temperature,\ntime of year\n5, 10, 20, 50,\n100 cm\nRMSE, MAE,\nR2\n[58]\nXGBoost,\
    \ SVM,\nRF, MLP\nHourly and\nhalf-hourly soil\ntemperature\nRainfall, soil\nmoisture,\
    \ soil\ntemperature, air\ntemperature,\nrelative humidity,\nvapor pressure\ndeﬁcit,\
    \ solar\nradiation\n15 cm\nR2, MAE\nXGBoost\n[61]\nSVR, MLR\nDaily soil\ntemperature\n\
    Minimum and\nmaximum air\ntemperature, solar\nradiation, relative\nhumidity, dew\n\
    point temperature,\natmospheric\npressure\n10, 30, 100 cm\nNRMSE, MBE,\nNSE, R2,\
    \ WR2\nSVR\n[62]\nSVR, ENN,\nSVR-FFA,\nENN-FFA,\nSVR-KHA,\nENN-KHA\nDaily soil\n\
    temperature\nAir temperature,\nsunshine hours,\nrelative humidity,\nwind speed,\n\
    pressure deﬁcit\n5, 10, 20, 30, 50,\n100 cm\nRMSE, MARE,\nR2\nSVR-KHA\n[63]\n\
    LSTM, BT-SVR,\nBT-GPR\nDaily soil\ntemperature\nCloudiness, air\ntemperature,\n\
    relative humidity,\nprecipitation,\npressure\n5, 10, 20, 50 cm\nR2, RMSE,\nMAE\n\
    BT-GPR\nAbbreviation: Mean Bias Error (MBE), Weighted Coefﬁcient of Determination\
    \ (WR2).\n2.4. Hybrid Models\nDespite considerable progress achieved in dealing\
    \ with dynamic, non-stationary, and\nnon-linear data by AI methods, the single\
    \ models suffer from insufﬁcient accuracy in some\ncases of environmental simulation.\
    \ To overcome the limitations of any single method,\ncoupled prediction models\
    \ were developed to improve the accuracy in both the prediction\nand optimization\
    \ stages by incorporating the superior features of individual models.\nAdaptive\
    \ neuro-fuzzy inference systems (ANFIS), which are a combination of ANN\nand fuzzy\
    \ logic, comprise an important category of hybrid models (Figure 3). These models\n\
    map input variables to output targets through an ensemble of membership functions\
    \ and\nif-then rules. Although ANFIS can predict target variable dynamics with\
    \ limited input\nrequirements and acceptable prediction accuracy, the long-term\
    \ generalization capability\nof NN is restricted by the ad hoc nature of fuzzy\
    \ logic rules. Moreover, additional pre-\nprocessing requires extensive time and\
    \ frequency domain computations. However, ANFIS\nis one of the most popular models\
    \ for simulating and predicting environmental compo-\nnents such as soil temperature.\
    \ Ref. [64] built several ANFIS models for soil temperature\nprediction to explore\
    \ different optimization schemes. Using only air temperature as an\ninput variable,\
    \ the models predicted soil temperature at locations in North Dakota, U.S., at\n\
    10 cm depth. The optimization schemes were the PSO, SSA, grey wolf optimizer (GWO),\n\
    genetic algorithm (GA), dragonﬂy algorithm (DA), grasshopper optimization algorithm\n\
    Sustainability 2023, 15, 7677\n15 of 26\n(GOA), and a proposed hybrid SSA-GOA\
    \ algorithm including a mutation phase (mSG). The\nproposed optimization scheme\
    \ resulted in a model with improved predictive power over\nother ANFIS models,\
    \ though the convergence time was longer than for most optimization\nschemes except\
    \ GOA.\nSustainability 2023, 15, x FOR PEER REVIEW \n17 of 27 \n \n \nFigure 3.\
    \ Structure of the ANFIS model. Input and output layers (pink), Fuzzification\
    \ and defuzzi-\nfication layers (yellow), product and normalization layers (green).\
    \ \nIn addition to ANFIS, researchers developed other hybrid models by combining\
    \ \nANN, SVM, DL, etc. For example, two ML-based models, FFBPNN and GEP, a time-se-\n\
    ries-based model, fractionally autoregressive integrated moving average (FARIMA),\
    \ and \ntwo hybrid models, FFBPNN-FARIMA and GEP-FARIMA, were used to estimate\
    \ the \ndaily soil temperature at depths of 5, 10, 50, and 100 cm [71]. To generalize\
    \ the models to \ndifferent climate classes, the stations were selected in arid,\
    \ semi-arid, and very humid \nclimates. At all depths, the ML-based models outperformed\
    \ the time-series-based model, \nand the hybrid models were superior to classical\
    \ models. \nIn order to overcome the generalization issues of classical standalone\
    \ models, opti-\nmization algorithms, such as GA, GWO, and PSO were integrated\
    \ into the models to de-\nvelop strong knowledge-based predictive systems. In\
    \ this regard, some researchers ad-\ndressed the lack of optimization algorithms\
    \ by developing optimized AI techniques. Ref. \n[72] evaluated the performance\
    \ of three hybrid models including SVM, MLP, and ANFIS \nboosted by the slime\
    \ mold algorithm (SMA), PSO, and spotted-hyena optimizer (SHO) in \nforecasting\
    \ daily soil temperature at 5, 15, and 30 cm in a semi-arid region of Punjab,\
    \ India. \nSince the performance of ML models highly depends on the input variables,\
    \ a gamma test \n(GT) was performed to determine the optimal input dataset for\
    \ ML models, resulting in \nan optimal combination of relative humidity, wind\
    \ speed, solar radiation, and air temper-\nature. Among the models, SVM-SMA provided\
    \ the best accuracy in predicting the soil \ntemperature at depths of 5, 15, and\
    \ 30 cm, respectively. To improve the ELM performance \nin estimating the daily\
    \ soil temperature at depths of 5, 10, 20, 30, 50, and 100 cm, ref. [73] \nused\
    \ the SaE algorithm to optimize the parameters of ELM’s hidden node. The input\
    \ var-\niables included the daily minimum, maximum, and average air temperatures\
    \ because of \ntheir high correlations with soil temperature data at all depths.\
    \ Despite the desirable be-\nhavior of both ELM and SaE-ELM against ANN and genetic\
    \ programming (GP), the hy-\nbrid model showed slightly better performance with\
    \ a lower mean absolute bias error \n(MABE) and a higher correlation coefficient\
    \ (R). Ref. [74] employed a hybrid multi-layer \nperceptron algorithm integrated\
    \ with the firefly optimizer algorithm (MLP-FFA) to pre-\ndict soil temperature\
    \ at multiple depths, including 5, 10, 20, 50, and 100 cm, using soil \ndepth,\
    \ periodicity (or the respective month), atmospheric pressure, air temperature,\
    \ and \nsolar radiation as model predictors. They compared the results with MLP\
    \ model and \nfound MLP-FFA model had better performance than the standalone MLP\
    \ model. Ref. [75] \nemployed ENN, which is a type of dynamic recurrent neural\
    \ network, in combination \nwith the gravitational search algorithm (GSA) and\
    \ ant colony optimization (ACO) to im-\nprove the estimation accuracy of daily\
    \ soil temperature. The hybrid models, i.e., ENN-\nFigure 3. Structure of the\
    \ ANFIS model. Input and output layers (pink), Fuzziﬁcation and defuzziﬁ-\ncation\
    \ layers (yellow), product and normalization layers (green).\nRef. [65] extended\
    \ a historical record of 1827 days of soil temperature data from two\nsites in\
    \ Illinois, US, each with data at 10 and 20 cm depths. The data were extended\
    \ using\nMLP and ANFIS with and without PSO as well as auto-regressive moving\
    \ average (ARMA).\nTo account for seasonal variation, some models were built using\
    \ data preprocessed with\nspectral analysis or wavelet decomposition to remove\
    \ the seasonal periodic component.\nThe ARMA model combined with spectral analysis\
    \ outperformed the AI techniques for\nboth depths.\nRef. [66] applied ANFIS, dynamic\
    \ evolving neurofuzzy inference system (DENFIS),\nWNN, and multivariate adaptive\
    \ regression spline (MARS) to predict land surface temper-\nature using NDVI,\
    \ normalized difference built-up index (NDBI), normalized difference\nbareness\
    \ index (NDBaI), normalized difference water index (NDWI), urban index (UI), and\n\
    elevation. According to the results, ANFIS showed the best performance in training\
    \ and\ntest periods with RMSE of 0.78 ◦C and 0.36 ◦C, respectively.\nIn [67],\
    \ the performance of ANFIS model against a time-series-based model, i.e., bi-\n\
    linear model, was evaluated for daily soil temperature retrieval at different\
    \ soil depths\n(5, 10, 50, and 100 cm) by daily recorded soil temperature. Furthermore,\
    \ two hybrid\nmodels built by combining ANFIS with bi-linear and wavelet analyses\
    \ were developed to\nimprove the retrieval accuracy of soil temperature. Based\
    \ on the soil temperature results,\nthe ANFIS model demonstrated better performance\
    \ than the bi-linear model as well as\nthe hybrid models were superior to classical\
    \ models. In addition to local evaluation, an\nexternal analysis was implemented\
    \ to assess the performance of ANFIS in modeling soil\ntemperature at 5 and 50\
    \ cm depths by soil temperature data at 10 and 100 cm, respectively,\nand vice\
    \ versa, denoting the positive effect of soil temperature data at another depth\
    \ in\nretrieving soil temperature at target depth.\nRef. [68] evaluated the applicability\
    \ of ANN, ANFIS, and GP in simulating monthly\nsoil temperature at different depths\
    \ by using weather data at two stations in Turkey. The\nclimatic data used to\
    \ model soil temperature were air temperature, relative humidity, solar\nradiation,\
    \ wind speed, and soil temperature at different depths (10, 20, and 100 cm). The\n\
    results illustrated the superior performance of GP in comparison with ANN and\
    \ ANFIS\nin estimating the soil temperature at 10, 50, and 100 cm. By including\
    \ periodicity (month\nSustainability 2023, 15, 7677\n16 of 26\nof the year) as\
    \ another input variable, considerable improvement was achieved in the\naccuracy\
    \ of the models.\nRef. [69] used ANN, CANFIS, WNN, and wavelet transformation\
    \ combined with\nCANFIS (WCANFIS) to retrieve soil temperatures at 5, 10, 20,\
    \ 30, and 100 cm by air\ntemperature data. The results indicated the suitable\
    \ performance of WCANFIS followed by\nANN, CANFIS, and WNN. In addition, the minimum\
    \ and maximum errors were obtained\nfor the depth of 20 and 100 cm, respectively.\
    \ Long-term soil temperature data are required\nto model soil temperature, which\
    \ restricts the prediction models in most situations due\nto insufﬁcient stations\
    \ recording soil temperature [35]. To address this gap, ref. [70] used\nshort-time\
    \ soil temperature data recorded by thermal sensors along with air temperature,\n\
    environmental parameters, and soil properties to model soil temperature at 5 and\
    \ 10 cm by\nANN, ANFIS, and MLR. The air temperature was found to be the most\
    \ effective parameter\nin soil temperature modeling. In addition, the best performance\
    \ belonged to ANFIS,\nfollowed by ANN and MLR, respectively.\nIn addition to ANFIS,\
    \ researchers developed other hybrid models by combining ANN,\nSVM, DL, etc. For\
    \ example, two ML-based models, FFBPNN and GEP, a time-series-\nbased model, fractionally\
    \ autoregressive integrated moving average (FARIMA), and two\nhybrid models, FFBPNN-FARIMA\
    \ and GEP-FARIMA, were used to estimate the daily soil\ntemperature at depths\
    \ of 5, 10, 50, and 100 cm [71]. To generalize the models to different\nclimate\
    \ classes, the stations were selected in arid, semi-arid, and very humid climates.\n\
    At all depths, the ML-based models outperformed the time-series-based model, and\
    \ the\nhybrid models were superior to classical models.\nIn order to overcome\
    \ the generalization issues of classical standalone models, op-\ntimization algorithms,\
    \ such as GA, GWO, and PSO were integrated into the models to\ndevelop strong\
    \ knowledge-based predictive systems. In this regard, some researchers\naddressed\
    \ the lack of optimization algorithms by developing optimized AI techniques.\n\
    Ref. [72] evaluated the performance of three hybrid models including SVM, MLP,\
    \ and\nANFIS boosted by the slime mold algorithm (SMA), PSO, and spotted-hyena\
    \ optimizer\n(SHO) in forecasting daily soil temperature at 5, 15, and 30 cm in\
    \ a semi-arid region of\nPunjab, India. Since the performance of ML models highly\
    \ depends on the input variables,\na gamma test (GT) was performed to determine\
    \ the optimal input dataset for ML models,\nresulting in an optimal combination\
    \ of relative humidity, wind speed, solar radiation, and\nair temperature. Among\
    \ the models, SVM-SMA provided the best accuracy in predicting\nthe soil temperature\
    \ at depths of 5, 15, and 30 cm, respectively. To improve the ELM\nperformance\
    \ in estimating the daily soil temperature at depths of 5, 10, 20, 30, 50, and\n\
    100 cm, ref. [73] used the SaE algorithm to optimize the parameters of ELM’s hidden\
    \ node.\nThe input variables included the daily minimum, maximum, and average\
    \ air temperatures\nbecause of their high correlations with soil temperature data\
    \ at all depths. Despite the\ndesirable behavior of both ELM and SaE-ELM against\
    \ ANN and genetic programming\n(GP), the hybrid model showed slightly better performance\
    \ with a lower mean absolute\nbias error (MABE) and a higher correlation coefﬁcient\
    \ (R). Ref. [74] employed a hybrid\nmulti-layer perceptron algorithm integrated\
    \ with the ﬁreﬂy optimizer algorithm (MLP-FFA)\nto predict soil temperature at\
    \ multiple depths, including 5, 10, 20, 50, and 100 cm, using\nsoil depth, periodicity\
    \ (or the respective month), atmospheric pressure, air temperature,\nand solar\
    \ radiation as model predictors. They compared the results with MLP model and\n\
    found MLP-FFA model had better performance than the standalone MLP model. Ref.\
    \ [75]\nemployed ENN, which is a type of dynamic recurrent neural network, in\
    \ combination with\nthe gravitational search algorithm (GSA) and ant colony optimization\
    \ (ACO) to improve\nthe estimation accuracy of daily soil temperature. The hybrid\
    \ models, i.e., ENN-GSA and\nENN-ACO, exploited optimization algorithms to train\
    \ the ENN’s parameters. The training\ndata were mean temperature, maximum temperature,\
    \ minimum temperature, dew point\ntemperature, wind speed, relative humidity,\
    \ precipitation, sunshine, and soil temperature\nat depths of 5, 10, 50, and 100\
    \ cm, which were used under different scenarios, including\ntemperature-based,\
    \ other meteorological parameters-based, and combined-based scenarios.\nSustainability\
    \ 2023, 15, 7677\n17 of 26\nAlthough hybrid models outperformed the classical\
    \ ENN, the ENN-GSA was identiﬁed as\nthe best-performing model at all depths.\
    \ In addition, the highest accuracy was achieved at\nthe full-input pattern for\
    \ both standalone and hybrid models.\nIn spite of the fact that ANNs are suitable\
    \ models for soil temperature prediction,\nthere are a number of problems related\
    \ to their application, including the scaling problem,\nthe intensive computational\
    \ effort, and the local minimum. To address these issues, Gill\nand [76] developed\
    \ an ANN-based model boosted by genetic algorithm to predict daily\nsoil temperature\
    \ at depths of 5, 10, and 30 cm by air temperature and rainfall as well as\npast\
    \ soil temperature data. The results showed that the developed model was successful\n\
    in predicting soil temperature. Table 4 presents a list of studies conducted to\
    \ estimate soil\ntemperature using hybrid models.\nTable 4. Studies conducted\
    \ to estimate soil temperature using hybrid models.\nResearch\nModels\nOutput\n\
    Input\nSoil Depth\nPerformance\nCriteria\nBest Model(s)\n[64]\nANFIS,\nANFIS-SSA,\n\
    ANFIS-GOA,\nANFIS-mSG,\nANFIS-GWO,\nANFIS-PSO,\nANFIS-GA,\nANFIS-DA\nDaily soil\n\
    temperature\nmaximum, average,\nand minimum air\ntemperature\n10 cm\nRMSE, STD,\
    \ MAE,\nRMSRE, AAPRE,\nR2, NSE\nANFIS-mSG\n[65]\nMLP, ANFIS,\nMLP-PSO,\nANFIS-PSO,\n\
    ARMA\nDaily soil\ntemperature\nAverage, minimum,\nmaximum, median,\nstandard deviation,\n\
    coefﬁcient of\nvariation,\nskewness, kurtosis,\nﬁrst quarter, and\nthird quarter\n\
    10, 20 cm\nR2, MAE, RMSE,\nMAPE\nARMA\n[66]\nMARS, WNN,\nANFIS, DENFIS\nLand surface\n\
    temperature\nNDVI, NDBI,\nNDWI, NDBaI, UI,\nelevation\n-\nR2, RMSE, MAE\nANFIS\n\
    [67]\nANFIS,\nbi-linear model,\nhybrid models\nbased on\nANFIS,\nbi-linear, and\n\
    wavelet\nanalysis\nDaily soil\ntemperature\nSoil temperature\n5, 10, 50,\n100\
    \ cm\nRMSE, MAE, KGE\nANFIS model\ncombined with\nbi-linear and\nwavelet\nanalysis\n\
    [68]\nANN, ANFIS,\nGP\nMonthly soil\ntemperature\nAir temperature,\nrelative humidity,\n\
    solar radiation,\nwind speed, month\nof year, soil\ntemperature at\ndifferent\
    \ depths\n10, 20,\n100 cm\nRMSE, MARE, R2,\nNSE\nGP\nSustainability 2023, 15,\
    \ 7677\n18 of 26\nTable 4. Cont.\nResearch\nModels\nOutput\nInput\nSoil Depth\n\
    Performance\nCriteria\nBest Model(s)\n[69]\nANN, CANFIS,\nWNN,\nWCANFIS\nSoil\n\
    temperature\nAir temperature\n5,10,20,30,\n100 cm\nNSE, RMSE, CRM\nWCANFIS\n[70]\n\
    ANN, ANFIS,\nMLR\nAir temperature,\nsoil temperature,\nenvironmental\nparameters,\
    \ soil\nproperties\n5, 10 cm\nR2, MAPE\nANFIS\n[71]\nFARIMA,\nFFBPNN, GEP\nGEP-FARIMA,\n\
    FFBPNN-\nFARIMA\nDaily soil\ntemperature\nHistorical records\nof soil temperature\n\
    data\n5, 10, 50,\n100 cm\nRMSE, MAE,\nRRMSE\nGEP-FARIMA\n[72]\nSVM, MLP, and\n\
    ANFIS\nhybridized\nwith SMA, PSO,\nand SHO\nDaily soil\ntemperature\nRelative\
    \ humidity,\nwind speed, solar\nradiation, air\ntemperature\n5, 15, 30 cm\nMAE,\
    \ RMSE, IS,\nNSE, R, WIA, radar\nchart, scatter plots,\nbox-whisker plot,\nTaylor\
    \ diagram\nSVM-SMA\n[73]\nELM, SaE-ELM,\nANN, GP\nDaily soil\ntemperature\nMinimum,\n\
    maximum, and\naverage air\ntemperature\n5, 10, 20, 30,\n50, 100 cm\nMAPE, RMSE,\
    \ R\nSaE-ELM\n[74]\nMLP, MLP-FFA\nMonthly soil\ntemperature\nSoil depth,\nperiodicity\
    \ (or\nthe respective\nmonth), air\ntemperature,\natmospheric\npressure, solar\n\
    radiation\n5,10,20,50,\n100 cm\nRMSE, MAE,\nMAPE, MBE,\nTaylor diagram\nMLP-FFA\n\
    [75]\nENN-GSA,\nENN-ACO\nDaily soil\ntemperature.\nMean temperature,\nmaximum\n\
    temperature,\nminimum\ntemperature, dew\npoint temperature,\nwind speed,\nrelative\
    \ humidity,\nprecipitation,\nsunshine hours,\nsoil temperature\n5, 10, 50,\n100\
    \ cm\nRMSE, RRMSE, R2,\na-20 index\nENN-GSA\n[76]\nANN- based\nmodel boosted\n\
    by genetic\nalgorithm\nDaily soil\ntemperature\nAir temperature,\nrainfall, past\
    \ soil\ntemperature data\n5, 10, 30 cm\nError value\nAbbreviation: Standard Deviation\
    \ (STD), Root Mean Squared Relative Error (RMSRE), Average Absolute Percent\n\
    Relative Error (AAPRE), Kling-Gupta Efﬁciency (KGE), Coefﬁcient of Residual Mass\
    \ (CRM), Relative RMSE\n(RRMSE), Mean Bias Error (MBE).\nUltimately, Table 5 summarizes\
    \ the weaknesses and strengths of different ML-based\nmodels to make a comparison\
    \ as follows:\nSustainability 2023, 15, 7677\n19 of 26\nTable 5. Strengths and\
    \ weaknesses of different ML-based models [42,77,78].\nModel\nStrength\nWeakness\n\
    ANN\n•\nEasy to implement\n•\nFew parameters\n•\nRelatively low\ncomputational\
    \ cost\n•\nAble to learn the multivariate\nnon-linear relationships\n•\nSensitive\
    \ to dimensionality\nof data\n•\nNeed for preliminary setting of\nneurons and\
    \ functions\n•\neasy falling into local optima\n•\nLack of considering\nspatiotemporal\
    \ patterns of data\n•\nscaling problem\nDL\n•\nHighly effective for\ncomplex applications\n\
    •\nAble to learn complex\nunderlying patterns in data\n•\nFlexible to data variations\
    \ over\ntime and space\n•\nStrong computing power\n•\nHigh computational speed\n\
    •\nDifﬁcult to interpret\n•\nHigh computational cost\n•\nLarge dataset requirement\n\
    Kernel-based\n•\nHighly effective for\ndata classiﬁcation\n•\nHandling non-linear\
    \ relationships\n•\nHandling high dimensional data\n•\nHigh accuracy\n•\nPreventing\
    \ overﬁtting\n•\nSensitive to kernel function\n•\nParameter tuning requirement\n\
    •\nComputationally expensive for\nlarge datasets\n•\nHigh memory usage\nHybrid\n\
    •\nImproving retrieval accuracy\n•\nAble to handle complex systems\nwith large\
    \ dataset requirement\n•\nFlexibility\n•\nRobustness\n•\nHigh computational\n\
    requirements\n•\nIncreased complexity\n•\nSelecting the suitable algorithms\n\
    3. Input Dataset\nThree general categories of input variables are used to train\
    \ ML-based models, includ-\ning climate data, time series of soil temperature,\
    \ and a combination of soil temperature\ntime series and climatic data. Climate\
    \ variables applied to ML models are relative humidity\n(average, maximum, and\
    \ minimum), solar radiation, wind speed, air temperature (average,\nmaximum, and\
    \ minimum), vapor pressure deﬁcit, sunshine hours, precipitation, dew point\n\
    temperature, potential evapotranspiration, evaporation, atmospheric pressure,\
    \ and cloudi-\nness. According to the studies, air temperature was identiﬁed as\
    \ the most effective variable\nin predicting soil temperature [20,28,31,50,70,73].\
    \ The importance of climate variables such\nas solar radiation, relative humidity,\
    \ precipitation, and soil moisture is disputed between\ndifferent studies; this\
    \ may be because the effect of moisture on soil temperature varies with\nthe soil\
    \ depth and the type of climate. However, most studies demonstrated that both\n\
    relative humidity and solar radiation are more effective than other variables\
    \ [28,31,40,50].\nMoreover, [58] showed a signiﬁcant correlation between soil\
    \ temperature and soil moisture\nthan soil temperature and precipitation.\nThe\
    \ models using climate parameters are less effective at greater depths, where\
    \ above-\nground climate has a decreased impact and the effect of previous temperatures\
    \ is corre-\nspondingly increased. It can be due to the relationships between\
    \ climate parameters and\nsoil temperature change with the soil depth. Therefore,\
    \ the inclusion of periodicity (number\nof days, months, and years) as another\
    \ input variable led to a considerable improvement\nin estimation accuracy, especially\
    \ at deeper soil depths [20,61,68]. In addition, combining\ncurrent climate conditions\
    \ with temporal information is recommended for the best results\nfor deep soil\
    \ temperature predictions [24,51].\nSustainability 2023, 15, 7677\n20 of 26\n\
    4. Conclusions and Future Outlook\nSoil temperature is a key factor affecting\
    \ the physical, chemical, and biological prop-\nerties of soil. In addition to\
    \ meteorological variables, soil temperature depends on to-\npographic conditions\
    \ and soil characteristics. The complex interactions between these\nvariables\
    \ make the estimation of soil temperature challenging.\nDirect measurement of\
    \ soil temperature by instruments such as thermistors, thermo-\ncouples, and thermocouple\
    \ wires on a large spatial scale is impractical because it is an\nexpensive and\
    \ time-consuming process. Therefore, soil temperature is mainly estimated\nusing\
    \ physically and statistically based models with a tradeoff between resolution,\
    \ accuracy,\nand computational efﬁciency.\nPhysically based models, despite their\
    \ wide use in soil temperature estimation, are\ncomplex with intense data requirement.\
    \ On the other hand, the empirical approach is\na simple method that requires\
    \ less input data; however, the statistical regression-based\nrelationship between\
    \ input and output is site dependent. The limitations of the above\nmethods led\
    \ to the development of data-driven ML techniques, such as ANN, DL, and\nSVM,\
    \ to estimate the soil temperature at multiple depths.\nANNs, as the most widely\
    \ used data-driven models, are capable of capturing nonlinear\ndata trends but\
    \ these models are used to extract single data features without learning\nspatiotemporal\
    \ patterns. DL techniques can address this issue by increasing the depth\nof neural\
    \ networks. However, the prediction accuracy may decrease as a result of an\n\
    inadequate sample size of the input data. Although the use of kernel function\
    \ embedded\nmodels such as SVMs ensures a global optimum, the selection of kernel\
    \ functions restricts\ntheir application. These traditional ML models have some\
    \ drawbacks, including ANN’s\nlow generalization performance, ANFIS’s need for\
    \ accurate weighting of the membership\nfunction, ELM’s large training datasets\
    \ requirement, and SVM’s high sensitivity to hyper-\nparameter selection, which\
    \ limit their applications.\nThe best ML technique for soil temperature retrieval\
    \ generally depends on training\ndatasets, model structure, and target level of\
    \ accuracy. Based on studies carried out\nto estimate soil temperature, DL models,\
    \ such as BiLSTM and LSTM, showed better\nperformance than other models. After\
    \ that, ANFIS exhibited the best performance followed\nby SVM, WNN, ELN, and ANN.\
    \ On the other hand, most studies found that hybrid\nmodels, which are mostly\
    \ standalone models boosted by optimization algorithms, were\nsuperior to classical\
    \ models. In addition to hybrid models with optimization schemes,\na new generation\
    \ of standalone AI-based models has the potential to offer promising\nalternatives\
    \ to traditional models because of their ﬂexibility.\nIn addition, combining current\
    \ climate conditions with time information is necessary\nfor the best results,\
    \ especially for deep soil temperature predictions. In this regard, im-\nproving\
    \ both machine learning and statistical methods to account for periodicity increases\n\
    the accuracy of the soil temperature prediction. If modeling for deeper soil depths\
    \ can\nbe brought up to the standard currently achieved by models for shallow\
    \ depths, it may\nbe possible to reduce the number of climate variables needed\
    \ as input. This can simplify\nthe process of soil temperature prediction and\
    \ make the models more generalizable to\ndifferent climates. For this purpose,\
    \ there are several approaches. The simplest approach\nis to add the month to\
    \ the set of input climate variables. This method should be easy to\nimplement\
    \ for any model using the current climate conditions for prediction. It relies\n\
    on seasons being relatively consistent in terms of temperature and timing from\
    \ year to\nyear. Another approach is the sliding window approach, which uses climate\
    \ variables\nfrom previous time steps as the additional input variables. The current\
    \ conditions are,\nthus, situated in a broader context in time without any assumptions\
    \ about the season. The\nsliding window approach relies on choosing a window that\
    \ is appropriate for the lag time\nbetween climate conditions and their effect\
    \ on soil temperatures. In complex models, lag\ntimes between climate variables\
    \ and soil temperatures may not be the same, nor may lag\ntimes between a variable\
    \ and soil temperatures at different depths. Future studies should\naddress these\
    \ issues. Moreover, remote sensing observations of soil properties such as\nSustainability\
    \ 2023, 15, 7677\n21 of 26\nmoisture content and vegetation cover can be combined\
    \ with machine learning models\nas a future perspective to achieve more accurate\
    \ soil temperature estimation. In addition,\nthe transferability of models over\
    \ different regions and climates can be more explored to\ndetermine the effective\
    \ factors on the model’s performance in soil temperature retrieval\nacross various\
    \ soil types and climates.\nAuthor Contributions: Conceptualization, A.M., H.S.,\
    \ P.P., H.I. and J.H.C.; data curation, M.T. and\nH.K.S.; formal analysis, A.M.,\
    \ M.T., H.K.S. and H.I.; funding acquisition, H.S. and J.H.C.; investigation,\n\
    A.M., H.S., P.P., H.I. and J.H.C.; methodology, A.M., M.T. and H.K.S.; project\
    \ administration, A.M.,\nH.S., P.P., H.I. and J.H.C.; resources, M.T. and H.K.S.;\
    \ supervision, A.M., H.S., P.P., H.I. and J.H.C.;\nvisualization, M.T.; writing—original\
    \ draft, M.T. and H.K.S.; writing—review and editing, A.M., P.P.\nand H.I. All\
    \ authors have read and agreed to the published version of the manuscript.\nFunding:\
    \ This research was funded by the National Research Council Canada through the\
    \ Artiﬁcial\nIntelligence for Logistics Supercluster Support Program, grant number\
    \ AI4L-120.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent\
    \ Statement: Not applicable.\nData Availability Statement: Not applicable.\nConﬂicts\
    \ of Interest: The authors declare no conﬂict of interest.\nAbbreviations\nAAPRE\n\
    Average Absolute Percent Relative Error\nACO\nAnt Colony Optimization\nAI\nArtiﬁcial\
    \ Intelligence\nAIC\nAkaike Information Criterion\nANFIS\nAdaptive Neuro-Fuzzy\
    \ Inference System\nANN\nArtiﬁcial Neural Network\nAPE\nAbsolute Percentage Error\n\
    ARMA\nAuto-Regressive Moving Average\nBiLSTM\nBi-directional LSTM\nBPNN\nBackpropagation\
    \ neural network\nBT-GPR\nBayesian Tuned Gaussian Process Regression\nBT-SVR\n\
    Bayesian Tuned Support Vector Regression\nCANFIS\nCo-Active Neuro-Fuzzy Inference\
    \ System\nCIT\nConditional Inference Tree\nCNN\nConvolutional Neural Network\n\
    ConvLSTM\nConvolutional LSTM\nCRM\nCoefﬁcient of Residual Mass\nCRT\nClassiﬁcation\
    \ and Regression Tree\nDA\nDragonﬂy Algorithm\nDL\nDeep Learning\nDNN\nDeep Neural\
    \ Network\nDT\nDecision Tree\nEEMD\nEnsemble Empirical Mode Decomposition\nEEMD-Conv2d\n\
    Ensemble Empirical Mode Decomposition- Convolutional 2 dimension\nEEMD-Conv3d\n\
    Ensemble Empirical Mode Decomposition- Convolutional 3 dimension\nELM\nExtreme\
    \ Learning Machine\nENN\nElman Neural Network\nERM\nEmpirical Risk Minimization\n\
    FARIMA\nFractionally Autoregressive Integrated Moving Average\nSustainability\
    \ 2023, 15, 7677\n22 of 26\nFFA\nFireFly Algorithm\nFFBPNN\nFeed Forward Back\
    \ Propagation Neural Network\nFFNN\nFeed-Forward Neural Network\nGA\nGenetic Algorithm\n\
    GaP\nGaussian Process\nGEP\nGene Expression Programming\nGMDH\nGroup Method of\
    \ Data Handling\nGOA\nGrasshopper Optimization Algorithm\nGP\nGenetic Programming\n\
    GRNN\nGeneralized Regression Neural Network\nGRU\nGated Recurrent Unit\nGSA\n\
    Gravitational Search Algorithm\nGT\nGamma Test\nGWO\nGrey Wolf Optimizer\nICR\n\
    Independent Component Regression\nIHR\nHimalayan Region\nKGE\nKling-Gupta Efﬁciency\n\
    KHA\nKrill Herd Algorithm\nk-NN\nk-Nearest Neighbors\nLAR\nLeast Angle Regression\n\
    LDAS\nLand Data Assimilation System\nLMI\nLegates and McCabe Index\nLR\nLinear\
    \ Regression\nLSM\nLand Surface Model\nLST\nLand Surface Temperature\nLSTM\nLong\
    \ Short-Term Memory network\nMABE\nMean Absolute Bias Error\nMAPE\nMean Absolute\
    \ Percentage Error\nMARS\nMultivariate Adaptive Regression Spline\nMaxE\nMaximum\
    \ residual Error\nMBE\nMean Bias Error\nMLP\nMultilayer Perceptron\nMLR\nMultiple\
    \ Linear Regression\nMSE\nMean Squared Error\nmSG\nA hybrid SSA-GOA algorithm\
    \ including a mutation phase\nM5 Tree\nM5 Model Tree\nNCPQR\nNon Convex Penalized\
    \ Quantile Regression\nNDBaI\nNormalized Difference Bareness Index\nNDBI\nNormalized\
    \ Difference Built-up Index\nNDVI\nNormalized Difference Vegetation Index\nNDWI\n\
    Normalized Difference Water Index\nNLR\nNon-Linear Regression\nNN\nNeural Network\n\
    NNLS\nNon Negative Least Square\nNRMSE\nNormalized RMSE\nNSE\nNash–Sutcliffe Efﬁciency\n\
    PBIAS\nPercent Bias\nPCA\nPrincipal Component Analysis\nPF\nPersistence Forecast\n\
    PPR\nProjection Pursuit Regression\nPSO\nParticle Swarm Optimization\nRBNN\nRadial\
    \ Basis Neural Network\nResNet\nResidual Network\nRF\nRandom Forest\nSustainability\
    \ 2023, 15, 7677\n23 of 26\nRMSE\nRoot Mean Square Error\nRMSRE\nRoot Mean Squared\
    \ Relative Error\nRNN\nRecurrent Neural Networks\nRRMSE\nRelative RMSE\nSaE-ELM\n\
    Self-Adaptive Evolutionary ELM\nSARIMA\nSeasonal Auto-Regressive Integrated Moving\
    \ Average\nSFO\nSunﬂower Optimization\nSHO\nSpotted Hyena Optimizer\nSI\nScatter\
    \ Index\nSMA\nSlime Mold Algorithm\nSMO\nSequential Minimal Optimization\nSRM\n\
    Structural Risk Minimization\nSSA\nSalp Swarm Algorithm\nSTD\nStandard Deviation\n\
    SVAT\nSoil Vegetation Atmosphere Transfer\nSVM\nSupport Vector Machine\nSVR\n\
    Support Vector Regression\nUI\nUrban Index\nVAF\nVariance Accounted For\nVRR\n\
    Variable Ridge Regression\nWCANFIS\nWavelet transformation combined with CANFIS\n\
    WI\nWillmott Index of Agreement\nWNN\nWavelet Neural Network\nWR2\nWeighted Coefﬁcient\
    \ of Determination\nXGBoost\nExtreme Gradient Boosting System\nReferences\n1.\n\
    Verma, P.; Yeates, J.; Daly, E. A stochastic model describing the impact of daily\
    \ rainfall depth distribution on the soil water balance.\nAdv. Water Resour. 2011,\
    \ 34, 1039–1048. [CrossRef]\n2.\nAli, I.; Greifeneder, F.; Stamenkovic, J.; Neumann,\
    \ M.; Notarnicola, C. Review of machine learning approaches for biomass and\n\
    soil moisture retrievals from remote sensing data. Remote Sens. 2015, 7, 16398–16421.\
    \ [CrossRef]\n3.\nColombo, R.; Bellingeri, D.; Fasolini, D.; Marino, C.M. Retrieval\
    \ of leaf area index in different vegetation types using high\nresolution satellite\
    \ data. Remote Sens. Environ. 2003, 86, 120–131. [CrossRef]\n4.\nMeroni, M.; Colombo,\
    \ R.; Panigada, C. Inversion of a radiative transfer model with hyperspectral\
    \ observations for LAI mapping\nin poplar plantations. Remote Sens. Environ. 2004,\
    \ 92, 195–206. [CrossRef]\n5.\nZhang, D.; Zhou, G. Estimation of soil moisture\
    \ from optical and thermal remote sensing: A review. Sensors 2016, 16, 1308.\n\
    [CrossRef] [PubMed]\n6.\nMonsivais-Huertero, A.; Graham, W.D.; Judge, J.; Agrawal,\
    \ D. Effect of simultaneous state–parameter estimation and forcing\nuncertainties\
    \ on root-zone soil moisture for dynamic vegetation using EnKF. Adv. Water Resour.\
    \ 2010, 33, 468–484. [CrossRef]\n7.\nKhanal, S.; Fulton, J.; Shearer, S. An overview\
    \ of current and potential applications of thermal remote sensing in precision\n\
    agriculture. Comput. Electron. Agric. 2017, 139, 22–32. [CrossRef]\n8.\nLakhankar,\
    \ T.; Jones, A.S.; Combs, C.L.; Sengupta, M.; Vonder Haar, T.H.; Khanbilvardi,\
    \ R. Analysis of large scale spatial\nvariability of soil moisture using a geostatistical\
    \ method. Sensors 2010, 10, 913–932. [CrossRef]\n9.\nGhedira, H.; Lakhankar, T.;\
    \ Jahan, N.; Khanbilvardi, R. Combination of passive and active microwave data\
    \ for soil moisture\nestimates. In Proceedings of the IGARSS 2004. 2004 IEEE International\
    \ Geoscience and Remote Sensing Symposium, Anchorage,\nAK, USA, 20–24 September\
    \ 2004; IEEE: Piscataway, NJ, USA, 2004.\n10.\nLi, P.; Zha, Y.; Shi, L.; Tso,\
    \ C.-H.M.; Zhang, Y.; Zeng, W. Comparison of the use of a physical-based model\
    \ with data assimilation\nand machine learning methods for simulating soil water\
    \ dynamics. J. Hydrol. 2020, 584, 124692. [CrossRef]\n11.\nBreen, K.H.; James,\
    \ S.C.; White, J.D.; Allen, P.M.; Arnold, J.G. A hybrid artiﬁcial neural network\
    \ to estimate soil moisture using\nswat+ and SMAP data. Mach. Learn. Knowl. Extr.\
    \ 2020, 2, 16. [CrossRef]\n12.\nKarpatne, A.; Watkins, W.; Read, J.; Kumar, V.\
    \ Physics-guided neural networks (pgnn): An application in lake temperature\n\
    modeling. arXiv 2017, arXiv:1710.11431.\n13.\nBergen, K.J.; Johnson, P.A.; de\
    \ Hoop, M.V.; Beroza, G.C. Machine learning for data-driven discovery in solid\
    \ Earth geoscience.\nScience 2019, 363, eaau0323. [CrossRef]\n14.\nNoé, F.; Olsson,\
    \ S.; Köhler, J.; Wu, H. Boltzmann generators: Sampling equilibrium states of\
    \ many-body systems with deep\nlearning. Science 2019, 365, eaaw1147. [CrossRef]\
    \ [PubMed]\n15.\nHassan-Esfahani, L.; Torres-Rua, A.; Jensen, A.; McKee, M. Assessment\
    \ of surface soil moisture using high-resolution multi-\nspectral imagery and\
    \ artiﬁcial neural networks. Remote Sens. 2015, 7, 2627–2646. [CrossRef]\nSustainability\
    \ 2023, 15, 7677\n24 of 26\n16.\nJiang, H.; Cotton, W.R. Soil moisture estimation\
    \ using an artiﬁcial neural network: A feasibility study. Can. J. Remote Sens.\
    \ 2004,\n30, 827–839. [CrossRef]\n17.\nGeorge, R.K. Prediction of soil temperature\
    \ by using artiﬁcial neural networks algorithms. Nonlinear Anal. Theory Methods\
    \ Appl.\n2001, 47, 1737–1748. [CrossRef]\n18.\nBasheer, I.A.; Hajmeer, M. Artiﬁcial\
    \ neural networks: Fundamentals, computing, design, and application. J. Microbiol.\
    \ Methods\n2000, 43, 3–31. [CrossRef]\n19.\nNugroho, A.S. Information Analysis\
    \ Using Softcomputing: The Applications to Character Recognition, Meteorological\
    \ Prediction,\nand Bioinformatics Problems. Ph.D. Thesis, Nagoya Institute of\
    \ Technology, Nagoya, Japan, 2003.\n20.\nKisi, O.; Tombul, M.; Kermani, M.Z. Modeling\
    \ soil temperatures at different depths by using three different neural computing\n\
    techniques. Theor. Appl. Climatol. 2015, 121, 377–387. [CrossRef]\n21.\nSanikhani,\
    \ H.; Deo, R.C.; Yaseen, Z.M.; Eray, O.; Kisi, O. Non-tuned data intelligent model\
    \ for soil temperature estimation: A new\napproach. Geoderma 2018, 330, 52–64.\
    \ [CrossRef]\n22.\nMehdizadeh, S.; Behmanesh, J.; Khalili, K. Evaluating the performance\
    \ of artiﬁcial intelligence methods for estimation of monthly\nmean soil temperature\
    \ without using meteorological data. Environ. Earth Sci. 2017, 76, 325. [CrossRef]\n\
    23.\nWu, W.; Tang, X.-P.; Guo, N.-J.; Yang, C.; Liu, H.-B.; Shang, Y.-F. Spatiotemporal\
    \ modeling of monthly soil temperature using\nartiﬁcial neural networks. Theor.\
    \ Appl. Climatol. 2013, 113, 481–494. [CrossRef]\n24.\nIkechukwu, M.; Ebinne,\
    \ E.; Yun, Z.; Patrick, B. Prediction of Land Surface Temperature (LST) Changes\
    \ within Ikon City in Nigeria\nUsing Artiﬁcial Neural Network (ANN). Int. J. Remote\
    \ Sens. Appl. 2016, 6, 96–107.\n25.\nAraghi, A.; Mousavi-Baygi, M.; Adamowski,\
    \ J.; Martinez, C.; van der Ploeg, M. Forecasting soil temperature based on surface\
    \ air\ntemperature using a wavelet artiﬁcial neural network. Meteorol. Appl. 2017,\
    \ 24, 603–611. [CrossRef]\n26.\nFeng, Y.; Cui, N.; Hao, W.; Gao, L.; Gong, D.\
    \ Estimation of soil temperature from meteorological data using different machine\n\
    learning models. Geoderma 2019, 338, 67–77. [CrossRef]\n27.\nSihag, P.; Esmaeilbeiki,\
    \ F.; Singh, B.; Pandhiani, S.M. Model-based soil temperature estimation using\
    \ climatic parameters: The\ncase of Azerbaijan Province, Iran. Geol. Ecol. Landsc.\
    \ 2020, 4, 203–215. [CrossRef]\n28.\nTabari, H.; Sabziparvar, A.-A.; Ahmadi, M.\
    \ Comparison of artiﬁcial neural network and multivariate linear regression methods\n\
    for estimation of daily soil temperature in an arid region. Meteorol. Atmos. Phys.\
    \ 2011, 110, 135–142. [CrossRef]\n29.\nKim, S.; Singh, V.P. Modeling daily soil\
    \ temperature using data-driven models and spatial distribution. Theor. Appl.\
    \ Climatol.\n2014, 118, 465–479. [CrossRef]\n30.\nBehmanesh, J.; Mehdizadeh, S.\
    \ Estimation of soil temperature using gene expression programming and artiﬁcial\
    \ neural networks\nin a semiarid region. Environ. Earth Sci. 2017, 76, 76. [CrossRef]\n\
    31.\nSamadianfard, S.; Asadi, E.; Jarhan, S.; Kazemi, H.; Kheshtgar, S.; Kisi,\
    \ O.; Sajjadi, S.; Manaf, A.A. Wavelet neural networks and\ngene expression programming\
    \ models to predict short-term soil temperature at different depths. Soil Tillage\
    \ Res. 2018, 175, 37–50.\n[CrossRef]\n32.\nKaur, S.; Randhawa, S. Global land\
    \ temperature prediction by machine learning combo approach. In Proceedings of\
    \ the 2018 9th\nInternational Conference on Computing, Communication and Networking\
    \ Technologies (ICCCNT), Bengaluru, India, 10–12 July\n2018; IEEE: Piscataway,\
    \ NJ, USA, 2018.\n33.\nZare Abyaneh, H.; Bayat Varkeshi, M.; Golmohammadi, G.;\
    \ Mohammadi, K. Soil temperature estimation using an artiﬁcial neural\nnetwork\
    \ and co-active neuro-fuzzy inference system in two different climates. Arab.\
    \ J. Geosci. 2016, 9, 377. [CrossRef]\n34.\nAlizamir, M.; Kisi, O.; Ahmed, A.N.;\
    \ Mert, C.; Fai, C.M.; Kim, S.; Kim, N.W.; El-Shaﬁe, A. Advanced machine learning\
    \ model for\nbetter prediction accuracy of soil temperature at different depths.\
    \ PLoS ONE 2020, 15, e0231055. [CrossRef]\n35.\nBilgili, M. Prediction of soil\
    \ temperature using regression and artiﬁcial neural network models. Meteorol.\
    \ Atmos. Phys. 2010,\n110, 59–70. [CrossRef]\n36.\nCitakoglu, H. Comparison of\
    \ artiﬁcial intelligence techniques for prediction of soil temperatures in Turkey.\
    \ Theor. Appl. Climatol.\n2017, 130, 545–556. [CrossRef]\n37.\nOzturk, M.; Salman,\
    \ O.; Koc, M. Artiﬁcial neural network model for estimating the soil temperature.\
    \ Can. J. Soil Sci. 2011,\n91, 551–562. [CrossRef]\n38.\nShamshirband, S.; Esmaeilbeiki,\
    \ F.; Zarehaghi, D.; Neyshabouri, M.; Samadianfard, S.; Ghorbani, M.A.; Mosavi,\
    \ A.; Nabipour,\nN.; Chau, K.-W. Comparative analysis of hybrid models of ﬁreﬂy\
    \ optimization algorithm with support vector machines and\nmultilayer perceptron\
    \ for predicting soil temperature at different depths. Eng. Appl. Comput. Fluid\
    \ Mech. 2020, 14, 939–953.\n[CrossRef]\n39.\nZeynoddin, M.; Ebtehaj, I.; Bonakdari,\
    \ H. Development of a linear based stochastic model for daily soil temperature\
    \ prediction:\nOne step forward to sustainable agriculture. Comput. Electron.\
    \ Agric. 2020, 176, 105636. [CrossRef]\n40.\nSeiﬁ, A.; Ehteram, M.; Nayebloei,\
    \ F.; Soroush, F.; Gharabaghi, B.; Torabi Haghighi, A. GLUE uncertainty analysis\
    \ of hybrid models\nfor predicting hourly soil temperature and application wavelet\
    \ coherence analysis for correlation with meteorological variables.\nSoft Comput.\
    \ 2021, 25, 10723–10748. [CrossRef]\n41.\nPaloscia, S.; Pampaloni, P.; Pettinato,\
    \ S.; Santi, E. A comparison of algorithms for retrieving soil moisture from ENVISAT/ASAR\n\
    images. IEEE Trans. Geosci. Remote Sens. 2008, 46, 3274–3284. [CrossRef]\n42.\n\
    Hinton, G.E.; Osindero, S.; Teh, Y.-W. A fast learning algorithm for deep belief\
    \ nets. Neural Comput. 2006, 18, 1527–1554. [CrossRef]\nSustainability 2023, 15,\
    \ 7677\n25 of 26\n43.\nLiu, Y.; Mei, L.; Ooi, S.K. Prediction of soil moisture\
    \ based on extreme learning machine for an apple orchard. In Proceedings of\n\
    the 2014 IEEE 3rd International Conference on Cloud Computing and Intelligence\
    \ Systems, Shenzhen, China, 27–29 November\n2014; IEEE: Piscataway, NJ, USA, 2014.\n\
    44.\nLeCun, Y.; Bengio, Y. Convolutional networks for images, speech, and time\
    \ series. Handb. Brain Theory Neural Netw. 1995,\n3361, 1995.\n45.\nHochreiter,\
    \ S.; Schmidhuber, J. Long short-term memory. Neural Comput. 1997, 9, 1735–1780.\
    \ [CrossRef] [PubMed]\n46.\nShi, X.; Chen, Z.; Wang, H.; Yeung, D.-Y.; Wong, W.-K.;\
    \ Woo, W.-C. Convolutional LSTM network: A machine learning approach\nfor precipitation\
    \ nowcasting. Adv. Neural Inf. Process. Syst. 2015, 28, 802–810.\n47.\nHe, K.;\
    \ Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In\
    \ Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition,\
    \ Las Vegas Valley, NV, USA, 26 June–1 July 2016; IEEE: Piscataway, NJ, USA, 2016.\n\
    48.\nHao, H.; Yu, F.; Li, Q. Soil temperature prediction using convolutional neural\
    \ network based on ensemble empirical mode\ndecomposition. IEEE Access 2020, 9,\
    \ 4084–4096. [CrossRef]\n49.\nYu, F.; Hao, H.; Li, Q. An Ensemble 3D convolutional\
    \ neural network for spatiotemporal soil temperature forecasting. Sustainability\n\
    2021, 13, 9174. [CrossRef]\n50.\nImanian, H.; Hiedra Cobo, J.; Payeur, P.; Shirkhani,\
    \ H.; Mohammadian, A. A Comprehensive Study of Artificial Intelligence\nApplications\
    \ for Soil Temperature Prediction in Ordinary Climate Conditions and Extremely\
    \ Hot Events. Sustainability 2022, 14, 8065.\n[CrossRef]\n51.\nLi, C.; Zhang,\
    \ Y.; Ren, X. Modeling hourly soil temperature using deep BiLSTM neural network.\
    \ Algorithms 2020, 13, 173.\n[CrossRef]\n52.\nWang, X.; Li, W.; Li, Q. A new embedded\
    \ estimation model for soil temperature prediction. Sci. Program. 2021, 2021,\
    \ 5881018.\n[CrossRef]\n53.\nImanian, H.; Shirkhani, H.; Mohammadian, A.; Hiedra\
    \ Cobo, J.; Payeur, P. Spatial Interpolation of Soil Temperature and Water\nContent\
    \ in the Land-Water Interface Using Artiﬁcial Intelligence. Water 2023, 15, 473.\
    \ [CrossRef]\n54.\nVapnik, V. The Nature of Statistical Learning Theory; Springer\
    \ Science & Business Media: Berlin/Heidelberg, Germany, 1999.\n55.\nPasolli, L.;\
    \ Notarnicola, C.; Bruzzone, L. Estimating soil moisture with the support vector\
    \ regression technique. IEEE Geosci.\nRemote Sens. Lett. 2011, 8, 1080–1084. [CrossRef]\n\
    56.\nAdeyemi, O.; Grove, I.; Peets, S.; Domun, Y.; Norton, T. Dynamic neural network\
    \ modelling of soil moisture content for predictive\nirrigation scheduling. Sensors\
    \ 2018, 18, 3408. [CrossRef]\n57.\nXing, L.; Li, L.; Gong, J.; Ren, C.; Liu, J.;\
    \ Chen, H. Daily soil temperatures predictions for various climates in United\
    \ States using\ndata-driven model. Energy 2018, 160, 430–440. [CrossRef]\n58.\n\
    Nanda, A.; Sen, S.; Sharma, A.N.; Sudheer, K. Soil temperature dynamics at hillslope\
    \ scale—Field observation and machine\nlearning-based approach. Water 2020, 12,\
    \ 713. [CrossRef]\n59.\nHong, Z. A Data-Driven Approach to Soil Moisture Collection\
    \ and Prediction Using a Wireless Sensor Network and Machine\nLearning Techniques.\
    \ Master’s Thesis, University of Illinois at Urbana-Champaign, Urbana Champaign,\
    \ IL, USA, 2015.\n60.\nOkujeni, A.; Van der Linden, S.; Jakimow, B.; Rabe, A.;\
    \ Verrelst, J.; Hostert, P. A comparison of advanced regression algorithms for\n\
    quantifying urban land cover. Remote Sens. 2014, 6, 6324–6346. [CrossRef]\n61.\n\
    Delbari, M.; Sharifazari, S.; Mohammadi, E. Modeling daily soil temperature over\
    \ diverse climate conditions in Iran—A\ncomparison of multiple linear regression\
    \ and support vector regression techniques. Theor. Appl. Climatol. 2019, 135,\
    \ 991–1001.\n[CrossRef]\n62.\nMoazenzadeh, R.; Mohammadi, B. Assessment of bio-inspired\
    \ metaheuristic optimisation algorithms for estimating soil tempera-\nture. Geoderma\
    \ 2019, 353, 152–171. [CrossRef]\n63.\nGuleryuz, D. Estimation of soil temperatures\
    \ with machine learning algorithms—Giresun and Bayburt stations in Turkey. Theor.\n\
    Appl. Climatol. 2022, 147, 109–125. [CrossRef]\n64.\nPenghui, L.; Ewees, A.A.;\
    \ Beyaztas, B.H.; Qi, C.; Salih, S.Q.; Al-Ansari, N.; Bhagat, S.K.; Yaseen, Z.M.;\
    \ Singh, V.P. Metaheuristic\noptimization algorithms hybridized with artiﬁcial\
    \ intelligence model for soil temperature prediction: Novel model. IEEE Access\n\
    2020, 8, 51884–51904. [CrossRef]\n65.\nBonakdari, H.; Moeeni, H.; Ebtehaj, I.;\
    \ Zeynoddin, M.; Mahoammadian, A.; Gharabaghi, B. New insights into soil temperature\n\
    time series modeling: Linear or nonlinear? Theor. Appl. Climatol. 2019, 135, 1157–1177.\
    \ [CrossRef]\n66.\nMustafa, E.K.; Co, Y.; Liu, G.; Kaloop, M.R.; Beshr, A.A.;\
    \ Zarzoura, F.; Sadek, M. Study for predicting land surface temperature\n(LST)\
    \ using landsat data: A comparison of four algorithms. Adv. Civ. Eng. 2020, 2020,\
    \ 1–16. [CrossRef]\n67.\nMehdizadeh, S.; Ahmadi, F.; Kozekalani Sales, A. Modelling\
    \ daily soil temperature at different depths via the classical and hybrid\nmodels.\
    \ Meteorol. Appl. 2020, 27, e1941. [CrossRef]\n68.\nKisi, O.; Sanikhani, H.; Cobaner,\
    \ M. Soil temperature modeling at different depths using neuro-fuzzy, neural network,\
    \ and genetic\nprogramming techniques. Theor. Appl. Climatol. 2017, 129, 833–848.\
    \ [CrossRef]\n69.\nBayatvarkeshi, M.; Bhagat, S.K.; Mohammadi, K.; Kisi, O.; Farahani,\
    \ M.; Hasani, A.; Deo, R.; Yaseen, Z.M. Modeling soil\ntemperature using air temperature\
    \ features in diverse climatic conditions with complementary machine learning\
    \ models. Comput.\nElectron. Agric. 2021, 185, 106158. [CrossRef]\n70.\nFathololoumi,\
    \ S.; Vaezi, A.; Alavipanah, S.; Montzka, C.; Ghorbani, A.; Biswas, A. Soil temperature\
    \ modeling using machine\nlearning techniques. Desert (2008–0875) 2020, 25, 185–199.\n\
    Sustainability 2023, 15, 7677\n26 of 26\n71.\nMehdizadeh, S.; Fathian, F.; Safari,\
    \ M.J.S.; Khosravi, A. Developing novel hybrid models for estimation of daily\
    \ soil temperature\nat various depths. Soil Tillage Res. 2020, 197, 104513. [CrossRef]\n\
    72.\nMalik, A.; Tikhamarine, Y.; Sihag, P.; Shahid, S.; Jamei, M.; Karbasi, M.\
    \ Predicting daily soil temperature at multiple depths\nusing hybrid machine learning\
    \ models for a semi-arid region in Punjab, India. Environ. Sci. Pollut. Res. 2022,\
    \ 29, 71270–71289.\n[CrossRef] [PubMed]\n73.\nNahvi, B.; Habibi, J.; Mohammadi,\
    \ K.; Shamshirband, S.; Al Razgan, O.S. Using self-adaptive evolutionary algorithm\
    \ to improve\nthe performance of an extreme learning machine for estimating soil\
    \ temperature. Comput. Electron. Agric. 2016, 124, 150–160.\n[CrossRef]\n74.\n\
    Samadianfard, S.; Ghorbani, M.A.; Mohammadi, B. Forecasting soil temperature at\
    \ multiple-depth with a hybrid artiﬁcial neural\nnetwork model coupled-hybrid\
    \ ﬁreﬂy optimizer algorithm. Inf. Process. Agric. 2018, 5, 465–476. [CrossRef]\n\
    75.\nMehdizadeh, S.; Mohammadi, B.; Pham, Q.B.; Khoi, D.N.; Linh, N.T.T. Implementing\
    \ novel hybrid models to improve indirect\nmeasurement of the daily soil temperature:\
    \ Elman neural network coupled with gravitational search algorithm and ant colony\n\
    optimization. Measurement 2020, 165, 108127. [CrossRef]\n76.\nGill, J.; Singh,\
    \ S. An efﬁcient neural networks based genetic algorithm model for soil temperature\
    \ prediction. Int. J. Emerg. Technol.\nEng. Res. (IJETER) 2015, 3, 1–5.\n77.\n\
    Le, V.T.; Quan, N.H.; Loc, H.H.; Duyen, N.T.T.; Dung, T.D.; Nguyen, H.D.; Do,\
    \ Q.H. A multidisciplinary approach for evaluating\nspatial and temporal variations\
    \ in water quality. Water 2019, 11, 853. [CrossRef]\n78.\nAlmomani, A.; Wan, T.;\
    \ Manasrah, A.; Altaher, A.; Almomani, E. A survey of learning based techniques\
    \ of phishing email ﬁltering.\nInt. J. Digit. Content Technol. Its Appl. 2012,\
    \ 6, 119.\nDisclaimer/Publisher’s Note: The statements, opinions and data contained\
    \ in all publications are solely those of the individual\nauthor(s) and contributor(s)\
    \ and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility\
    \ for any injury to\npeople or property resulting from any ideas, methods, instructions\
    \ or products referred to in the content.\n"
  inline_citation: '>'
  journal: Sustainability
  limitations: '>'
  pdf_link: https://www.mdpi.com/2071-1050/15/9/7677/pdf?version=1683454526
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: A Review of Machine Learning Approaches to Soil Temperature Estimation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/su15129567
  analysis: '>'
  authors:
  - Mohammad Zeynoddin
  - Hossein Bonakdari
  - Silvio José Gumière
  - Alain N. Rousseau
  citation_count: 1
  full_citation: '>'
  full_text: ">\nCitation: Zeynoddin, M.; Bonakdari,\nH.; Gumiere, S.J.; Rousseau,\
    \ A.N.\nMulti-Tempo Forecasting of Soil\nTemperature Data; Application over\n\
    Quebec, Canada. Sustainability 2023,\n15, 9567. https://doi.org/10.3390/\nsu15129567\n\
    Academic Editor: Jeroen Meersmans\nReceived: 30 April 2023\nRevised: 31 May 2023\n\
    Accepted: 12 June 2023\nPublished: 14 June 2023\nCopyright:\n© 2023 by the authors.\n\
    Licensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed\n\
    under\nthe\nterms\nand\nconditions of the Creative Commons\nAttribution (CC BY)\
    \ license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nsustainability\n\
    Article\nMulti-Tempo Forecasting of Soil Temperature Data; Application\nover Quebec,\
    \ Canada\nMohammad Zeynoddin 1\n, Hossein Bonakdari 2,*\n, Silvio José Gumiere\
    \ 1\nand Alain N. Rousseau 3\n1\nDepartment of Soils and Agri-Food Engineering,\
    \ Université Laval, Quebec, QC G1V 0A6, Canada;\nmohammad.zeynoddin.1@ulaval.ca\
    \ (M.Z.); silvio-jose.gumiere@fsaa.ulaval.ca (S.J.G.)\n2\nDepartment of Civil\
    \ Engineering, University of Ottawa, Ottawa, ON K1N 6N5, Canada\n3\nInstitut National\
    \ de la Recherche Scientiﬁque-Eau Terre Environnement INRS-ETE,\nQuebec, QC G1K\
    \ 9A9, Canada; alain.rousseau@inrs.ca\n*\nCorrespondence: hossein.bonakdari@uottawa.ca;\
    \ Tel.: +1-6135625800 (ext. 6016)\nAbstract: The profound impact of soil temperature\
    \ (TS) on crucial environmental processes, includ-\ning water inﬁltration, subsurface\
    \ movement, plant growth, and its inﬂuence on land–atmosphere\ndynamics, cannot\
    \ be undermined. While satellite and land surface model-based data are valuable\
    \ in\ndata-sparse areas, they necessitate innovative solutions to bridge gaps\
    \ and overcome temporal delays\narising from their dependence on atmospheric and\
    \ hydro–meteorological factors. This research\nintroduces a viable technique to\
    \ address the lag in the Famine Early Warning Network Land Data\nAssimilation\
    \ System (FLDAS). Notably, this approach exhibits versatility, proving highly\
    \ effective in\nanalyzing datasets characterized by signiﬁcant seasonal trends,\
    \ and its application holds immense\nvalue in watershed-scaled hydrological research.\
    \ Leveraging the enhanced state-space (SS) method\nfor forecasting in the FLDAS,\
    \ this technique harnesses TS datasets collected over time at various\ndepths\
    \ (0–10 cm, 10–40 cm, and 40–100 cm), employing a multiplicative SS model for\
    \ modeling\npurposes. By employing the 1-step, 6-step, and 12-step-ahead models\
    \ at different depths and 2 lo-\ncations in Quebec, Canada, the outcomes showcased\
    \ a performance with an average coefﬁcient of\ndetermination (R2) of 0.88 and\
    \ root mean squared error (RMSE) of 2.073 ◦C for the dynamic model,\nR2 of 0.834\
    \ and RMSE of 2.979 ◦C for the 6-step-ahead model, and R2 of 0.921 and RMSE of\
    \ 1.865 ◦C\nfor the 12-step-ahead model. The results revealed that as the prediction\
    \ horizon expands and the\nlength of the input data increases, the accuracy of\
    \ predictions progressively improves, indicating that\nthis model becomes increasingly\
    \ accurate over time.\nKeywords: prediction; FLDAS; soil science; soil temperature;\
    \ satellite; lag\n1. Introduction\nSoil temperature (TS) is one of the essential\
    \ environmental factors used in various\ndisciplines such as engineering and hydraulic\
    \ structures design, geotechnics, hydrological\nand meteorological studies, agriculture,\
    \ forestry, and many other ﬁelds. The majority of\nthe chemical and physical characteristics\
    \ of soil are temperature-dependent. For instance,\nﬂuctuations in TS directly\
    \ affect water inﬁltration; thus, variations in TS and inﬁltration\nrates follow\
    \ similar patterns [1–3]. Soil temperature may affect other processes, such as\n\
    groundwater discharge, land–atmosphere ﬂuxes such as evapotranspiration, gas exchange,\n\
    and surface runoff patterns [4–6]. Fluxes of greenhouse gases such as carbon dioxide\
    \ (CO2)\nand nitrous oxide (N2O) directly depend on TS [7,8], which relates to\
    \ global warming on a\nlarge scale. Any alterations to atmospheric constituents\
    \ mutually affect the soil temperature.\nBecause the thermal energy balance between\
    \ the soil and atmosphere is preserved, the soil\ncan either act as a heat source\
    \ or sink [9–12]. Similarly, the impact of TS on the agriculture\nsection cannot\
    \ be ignored, e.g., the dependency of crops’ growth on TS, irrigation planning,\n\
    and hydrological drought monitoring [13,14].\nSustainability 2023, 15, 9567. https://doi.org/10.3390/su15129567\n\
    https://www.mdpi.com/journal/sustainability\nSustainability 2023, 15, 9567\n2\
    \ of 21\nEstimating the TS time series by using artiﬁcial intelligence (AI) models\
    \ and exoge-\nnous variables is common. For instance, multilayer perceptron (MLP),\
    \ an artiﬁcial neural\nnetwork (ANN) model and a multivariate linear regression\
    \ with air temperature (Ta), solar\nradiation (RS), relative humidity (RH), and\
    \ precipitation inputs as inputs [15], MLP and\nan adaptive neuro–fuzzy inference\
    \ system, genetic programming and an ANN with Ta,\nRS, RH, wind speed, and dew\
    \ points inputs [16–18], a self-adaptive evolutionary extreme\nlearning machine\
    \ with Rs, Ta, and pressure [19], an emotional neural network, and a\nleast square\
    \ support vector machine [20] are some of the AI methods used in this ﬁeld.\n\
    Though these modeling methods are powerful in creating estimations, they are prone\
    \ to\nmodel parameter tuning and input selection uncertainties [21,22]. To solve\
    \ this problem,\nresearchers developed remotely sensed data.\nFurther developments\
    \ of remote sensing tools and land surface methods with extended\nhistorical temporal\
    \ coverage have improved the quality and quantity of environmental data,\nparticularly\
    \ in regions with limited available data. However, land coverage, upwelling\n\
    atmospheric emissions [23], cloud coverage [24], satellite machine failure, and\
    \ consequent\nerrors in the inputs of land surface models are most often responsible\
    \ for missing values in\nremotely sensed data. Therefore, many methods have been\
    \ proposed to ﬁll in the dataset’s\ngaps [25].\nVarious methods have been proposed\
    \ to ﬁll in gaps in remotely sensed data, including\ncombining temporal steps\
    \ to create a composite set. Weiss et al. [26] categorized data\nrecovery methods\
    \ into three classes: (a) spatial, (b) temporal sequences of images, and\n(c)\
    \ spatio–temporal ﬁlling methods. Inverse distance weighting (IDW), kriging, cokriging,\n\
    and their variants are widely recognized methods in spatial class [27,28]. IDW\
    \ and its\nvariants, such as harmony search, genetic algorithm (ga), and particle\
    \ swarm–IDW, are\ncommonly used and reliable approaches. A comparative study by\
    \ Bărbulescu et al. [29]\nshowed that the ga–IDW method outperformed other methods\
    \ in 70% of the cases studied.\nThe principal component analysis, stochastic models,\
    \ and k nearest neighbor (kNN) are\nother popular statistical approaches for estimating\
    \ missing values [30,31]. Weiss et al. used\nthe spatio–temporal method to ﬁll\
    \ gaps in the land surface temperature and vegetation\nindex, achieving an accuracy\
    \ of R2 = 0.87, however could not evaluate embedded uncer-\ntainties or account\
    \ for temperature values impacted by cloudy days. This conclusion adds\nto the\
    \ inherent uncertainty of their model.\nIntegrating remotely-sensed data with\
    \ modeling backed by historical data decreases\nmeasurement limitations and variable-based\
    \ method deﬁciencies, namely, input variable\nuncertainties and error accumulation\
    \ [18,22], where both non-linear and linear models are\napplicable. However, using\
    \ a method with a derivable explicit expression fast enough to\nbuild real-time\
    \ estimations and reasonably accurate information is prioritized over other\n\
    methods. The advanced state-space (SS) method in hydrology is a relatively less\
    \ utilized\nlinear method for modeling time series with intense periodic ﬂuctuations.\
    \ This model\nuses multiple in-line pre-processing methods to produce white noise\
    \ data. It additionally\ncan be used to remove time series anomalies [32] and\
    \ forecast future steps using the\nout-of-sample forecasting ability of the model.\
    \ Studies on the SS model as a forecasting\nmethod are limited. Reservoir inﬂow\
    \ modeling, climatic data, catchment rainfalls, and\nland hotspots are the parameters\
    \ modeled and forecasted by the SS method [33–36]. For\nexample, Vijayakumar and\
    \ Vennila [33] compared the autoregressive integrated moving\naverage (ARIMA)\
    \ model with the SS method. The model input included reservoir inﬂows,\nand the\
    \ results indicated that the SS method could overcome the deﬁciencies of the ARIMA\n\
    model. They stated that the ARIMA model did not successfully deal with the time\
    \ series\ncomponents observed in their data. For the forecast procedure, the trend\
    \ component\nwas removed before autocorrelation modeling. Additionally, they mentioned\
    \ that the\nmodel could not forecast the “turning points”. These deﬁciencies,\
    \ alongside the linear\nnature of stochastic models and requisite preprocessing\
    \ steps before modeling, represent\nother limitations.\nSustainability 2023, 15,\
    \ 9567\n3 of 21\nIn a recent study by Zeynoddin et al. [37], a comprehensive exploration\
    \ of techniques\nfor predicting time series data was undertaken, encompassing\
    \ variable-based models, AI\nmodeling approaches, and linear methods. They proposed\
    \ a linear methodology that pre-\nprocesses TS data using several methods and\
    \ models the data dynamically via stochastic\nmodels. Comparative analysis with\
    \ ML techniques [19] revealed that combining the SS\nmethod with the stochastic\
    \ model yielded signiﬁcantly more precise results. Building upon\nthese ﬁndings,\
    \ Zeynoddin and Bonakdari [38] introduced a structurally optimized deep\nlearning\
    \ method that integrated the SS method for long-term soil moisture forecasting.\
    \ Re-\nmarkably, their study demonstrated that the SS method excelled at regenerating\
    \ long-term\ntime series features and substantially enhanced forecast accuracy\
    \ compared to other meth-\nods. Moreover, their solution effectively mitigated\
    \ the data size dependency problem often\nencountered in data-driven modeling\
    \ approaches. While this proposed structure has limita-\ntions, such as its ﬁnite\
    \ domain of model variables, exclusivity to seasonally structured data,\nand the\
    \ complexity of the deep learning method, the authors hypothesized that the compre-\n\
    hensive and generalizable structure of the SS method for seasonal data modeling\
    \ could be\nleveraged to ﬁll the temporal gap in FLDAS datasets. These datasets,\
    \ generated at a monthly\nresolution [39] with a two-month availability lag, as\
    \ indicated on the GEE catalog website\n(Earth Engine Data Catalog: NASA_FLDAS_NOAH01_C_GL_M_V001#description),\
    \ are\ninﬂuenced by the annual or monthly trends exhibited by the TS time series\
    \ [10], which\ntemporally and spatially vary due to atmospheric and subsoil processes\
    \ affecting ther-\nmal equilibrium.\nThe objectives of this paper are (a) to introduce\
    \ an extraction method for the collection\nof monthly mean TS data for two agricultural\
    \ areas in Quebec, Canada and (b) to apply\nthe powerful SS method for modeling.\
    \ Following these are (c) a spatial validation analysis\nwill be carried out to\
    \ analyze the model’s accuracy alternation and pattern evaluation in\ncase a time\
    \ series of different locations are used, and (d) a trend and long-term alternations\n\
    analysis of the TS variable will be carried out based on the model results and\
    \ forecasts.\nFinally, (e) the results of the proposed FLDAS–state-space method\
    \ will be comprehensively\ndiscussed in the following sections. Furthermore, this\
    \ paper will outline the practical\napplication of these results, including their\
    \ integration into future TS forecasts, thereby\nmaking a signiﬁcant contribution\
    \ to the ﬁeld.\n2. Time Series and Pre-Processing Review\nSince this study uses\
    \ a time series-based approach, a review of necessary methods for\ntime series\
    \ preparation is presented. Time series can have jumps, periods, and seasonal\n\
    and non-seasonal trends deemed deterministic. They can additionally have a stochastic\n\
    component, constituted of residuals of the deterministic characteristics subtracted\
    \ from\nthe time series’ values. The deterministic component is the predictable\
    \ part of the time\nseries’ pattern and can be detected and extracted. Detecting\
    \ the mentioned patterns and\nanalyzing TS data entails employing several tests.\
    \ These tests not only help to detect\nmeaningful patterns, however they can additionally\
    \ help to extract statistics and interpret\nthe time series. Following pre-processing\
    \ methods, models are applied to prepare data for\nforecasting [40].\nThe ﬁrst\
    \ attribute of a time series to be checked is stationarity. It means the absence\n\
    of meaningful changes in mean, variance, and other statistics over time. Therefore,\
    \ the\nKwiatkowski–Phillips–Schmidt–Shin (KPSS) test is applied to TS for assessing\
    \ station-\narity [41]. In the KPSS test, a regression equation is ﬁtted to the\
    \ data. The TS series is\nstationary if the variance of the independent variables\
    \ of that equation is null. Alternatively,\nit is stationary in trend if the deterministic\
    \ term of the trend is not null and stationary in\nlevel if the deterministic\
    \ term of the trend is equal to zero. Seasonal and non-seasonal trends\nin the\
    \ TS time series as one of the non-stationarity factors is checked by the Mann–Kendal\n\
    and seasonal Mann–Kendal trend (MK and SMK) tests [42,43].\nJumps appear in time\
    \ series as sudden increases or decreases and can be both induced\nby human and\
    \ natural sources. To evaluate this factor, the non-parametric Mann–Whitney\n\
    Sustainability 2023, 15, 9567\n4 of 21\n(MW) test can be performed [42]. More\
    \ details about pre-processing steps can be found in\nZeynoddin et al. [22]. The\
    \ autocorrelation function (ACF) is a tool to evaluate a dataset’s\nrandomness\
    \ or assess whether a non-linear or linear model would be appropriate. This\n\
    way, it can be found whether an observation is related to adjacent observations\
    \ or not.\nAdditionally, speciﬁc patterns in time series, such as sinusoidal ﬂuctuations\
    \ interpreted\nas periodicity, can be detected. Therefore, we can judge the time\
    \ series’ condition and\ninterpret it. An ACF plot is an intuitive method to assess\
    \ the data more conveniently.\nThe ACF demonstrates the autocorrelation of data\
    \ separated by a lag distance. Both\npositive and negative ACF values are considered\
    \ as the correlation between lags. Still,\nfor several lags, if the positive values\
    \ of an ACF consecutively exceed speciﬁc values,\nin this case, the 95% signiﬁcance\
    \ levels, they are interpreted as signiﬁcantly correlated\nlags. These signiﬁcant\
    \ correlations should be reduced or removed from the time series\nto obtain independent\
    \ variables and reduce the impacts of the deterministic components\nin the modeling\
    \ procedure. As mentioned earlier, the deterministic components are the\npredictable\
    \ part of the series and are responsible for dependency in the time series. This\n\
    part of the time series’ structure can be separated before modeling and restored\
    \ to modeled\ndata. The stochastic component is the mere important part for modeling\
    \ purposes. The\nsigniﬁcance intervals are deﬁned based on the cumulative distribution\
    \ function of the\nstandard normal distribution and the 95% signiﬁcance level.\n\
    3. Materials and Methods\n3.1. Soil Temperature Collection and Analyses\n3.1.1.\
    \ Famine Early Warning Network Land Data Assimilation System\nThe Famine Early\
    \ Warning Network Land Data Assimilation System (FLDAS) program\nimproves the\
    \ quality of measured values by combining hydroclimatic data and observations\n\
    to develop rainfall, soil moisture, evapotranspiration, and soil temperature products\
    \ at a\nmonthly resolution. These products are hosted by Google Earth Engine and\
    \ can be merged\nwith variables related to food security assessment in data-sparse\
    \ countries. The program\nis developed by the U.S. Geological Survey (USGS), NASA\
    \ GSFC, Goddard Space Flight\nCenter (GSFC), and the University of California\
    \ Santa Barbara (UCSB). It can provide\nhelpful information to water resources\
    \ and agricultural stakeholders. The output of the\nFLDAS can be used to compare\
    \ current and past hydrological conditions and can be merged\nwith other land\
    \ information systems to produce an ensemble of hydroclimatic variables.\nThe\
    \ FLDAS utilizes the Noah version 3.6.1 land surface model (LSM), the Climate\n\
    Hazards Group Infrared Precipitation with Station (CHIRPS) 6-hourly rainfall,\
    \ and the\nModern-Era Retrospective Analysis for Research and Applications version\
    \ 2 (MERRA-2).\nMoreover, the system uses the African Rainfall Estimation Algorithm\
    \ Version 2 (RFE 2.0)\nand the CHIRPS for low latency. Using the Land Data Tool\
    \ Kit (LDTK), the FLDAS provides\nand preprocesses the land cover parameters needed\
    \ for the Noah model. The LDTK contains\nthe products of the NASA Soil Moisture\
    \ Active Passive (SMAP), the Moderate Resolution\nImaging Spectroradiometer (MODIS),\
    \ the Advanced Very High-Resolution Radiometer\n(AVHRR) from NOAA 19 satellites,\
    \ the CHIRPS quasi-global rainfall data set obtained\nfrom high-resolution satellite\
    \ imagery, MERRA2, and the Global Data Assimilation System\n(GDAS), which uses\
    \ multiple sources of information such as balloon, aircraft, radar, and\nsatellite\
    \ data. This tool kit allows the FLDAS to use rich sources of high-resolution\
    \ datasets,\nwith near real-time observations. In return, the data are fed to\
    \ the Noah LSM to produce\nhigh-accuracy outputs.\nAll datasets are available\
    \ from 1982 to the present at a 0.1 arc degree and a monthly\ntemporal resolution.\
    \ A ﬂexibility in using different products represents the advantage\nof the FLDAS,\
    \ products such as FEWS NET operational rainfall. Using multiple surface\nmodels,\
    \ normal assessment, and a fast and appropriate data distribution represent the\n\
    other advantages of this system. The FLDAS datasets can be obtained from the Google\n\
    Earth Engine (GEE), which contains a vast catalog of satellite and geospatial\
    \ datasets. In\nthis application programming interface (API), both Python and\
    \ JavaScript languages can\nSustainability 2023, 15, 9567\n5 of 21\nbe used to\
    \ obtain and process the required data. Using the GEE API, various datasets\n\
    for different ﬁelds of study can be accessed, such as global surface water changes,\
    \ crop\nproduction, and drought monitoring [44–46]. Figure 1 shows the data collection\
    \ and\nmodeling procedural steps. Aside from the GEE API, many data transmission\
    \ mechanisms\nare presented in this ﬁgure. In the GEE code editor, the following\
    \ pseudocode and the\ncorresponding commands in the Java language can be used\
    \ to obtain the desired datasets\n(Appendix A Algorithm A1), or users can download\
    \ the dataset from the developed\napplication, SOILPARAM APP, by [47].\nSustainability\
    \ 2023, 15, x FOR PEER REVIEW \n5 of 22 \n \nEngine (GEE), which contains a vast\
    \ catalog of satellite and geospatial datasets. In this \napplication programming\
    \ interface (API), both Python and JavaScript languages can be \nused to obtain\
    \ and process the required data. Using the GEE API, various datasets for \ndiﬀerent\
    \ ﬁelds of study can be accessed, such as global surface water changes, crop pro-\n\
    duction, and drought monitoring [44–46]. Figure 1 shows the data collection and\
    \ model-\ning procedural steps. Aside from the GEE API, many data transmission\
    \ mechanisms are \npresented in this ﬁgure. In the GEE code editor, the following\
    \ pseudocode and the corre-\nsponding commands in the Java language can be used\
    \ to obtain the desired datasets (Ap-\npendix A Algorithm A1), or users can download\
    \ the dataset from the developed applica-\ntion, SOILPARAM APP, by [47]. \n \n\
    Figure 1. Data collection and modeling ﬂowchart using FLDAS; ωi is the i-th period,\
    \ and Si, li, and \nbi are seasons, level, and trend initials, respectively, at\
    \ i-th time. TS is the values of observed data \nand mean data, respectively.\
    \ \n3.1.2. Case Study \nTwo locations in Quebec, Canada, were chosen for this\
    \ study. The ﬁrst location is \nsituated in a suburb of Quebec City (St. Augustin),\
    \ with a latitude of 46.73° and longitude \nof −71.50°. The second is in a suburb\
    \ of Montreal (St. Mirabel), with a latitude of 45.67° \nand longitude of −74.03°.\
    \ According to the Köppen classiﬁcation, Quebec City’s climate is \n“humid continental”.\
    \ This means that summers are hot, and winters are cold and snowy. \nFall and\
    \ spring temperatures are, additionally, moderate. The atmospheric temperature\
    \ \nin this region varies on average between −18 °C and 25 °C. Montreal has a\
    \ “warm-summer \nhumid continental climate”, based on the same classiﬁcation.\
    \ This classiﬁcation means \nFigure 1. Data collection and modeling ﬂowchart using\
    \ FLDAS; ωi is the i-th period, and Si, li, and bi\nare seasons, level, and trend\
    \ initials, respectively, at i-th time. TS is the values of observed data and\n\
    mean data, respectively.\n3.1.2. Case Study\nTwo locations in Quebec, Canada,\
    \ were chosen for this study. The ﬁrst location is\nsituated in a suburb of Quebec\
    \ City (St. Augustin), with a latitude of 46.73◦ and longitude\nof −71.50◦. The\
    \ second is in a suburb of Montreal (St. Mirabel), with a latitude of 45.67◦\n\
    and longitude of −74.03◦. According to the Köppen classiﬁcation, Quebec City’s\
    \ climate is\n“humid continental”. This means that summers are hot, and winters\
    \ are cold and snowy.\nFall and spring temperatures are, additionally, moderate.\
    \ The atmospheric temperature in\nthis region varies on average between −18 ◦C\
    \ and 25 ◦C. Montreal has a “warm-summer\nhumid continental climate”, based on\
    \ the same classiﬁcation. This classiﬁcation means\nsummers are hot and humid,\
    \ and winters are icy and snowy. The other two seasons are\nSustainability 2023,\
    \ 15, 9567\n6 of 21\nmoderate. The air temperature ranges between −30 ◦C and 27\
    \ ◦C in some regions of the\nstudy areas [48]. Figure 2 shows the locations on\
    \ a map.\nTo validate the results of the proposed methodology, the soil temperatures\
    \ at two\nother locations were obtained, using the same method as the GEE API.\
    \ These locations\nwere selected based on the mapping of the land use of the St.\
    \ Lawrence Lowlands report,\npublished by Environment and Climate Change Canada\
    \ in 2018. The Mauricie (MA)\nvalidation point is located between the ﬁrst two\
    \ study points (QC and MC), and the second\nvalidation point Lennoxville (LX)\
    \ is located east of these points. These locations are two\ncritical agricultural\
    \ areas in Quebec, Canada. They are located between and southeast of\nthe previous\
    \ ones (Figure 2).\nSustainability 2023, 15, x FOR PEER REVIEW \n6 of 22 \n \n\
    summers are hot and humid, and winters are icy and snowy. The other two seasons\
    \ are \nmoderate. The air temperature ranges between −30 °C and 27 °C in some\
    \ regions of the \nstudy areas [48]. Figure 2 shows the locations on a map. \n\
    To validate the results of the proposed methodology, the soil temperatures at\
    \ two \nother locations were obtained, using the same method as the GEE API. These\
    \ locations \nwere selected based on the mapping of the land use of the St. Lawrence\
    \ Lowlands report, \npublished by Environment and Climate Change Canada in 2018.\
    \ The Mauricie (MA) vali-\ndation point is located between the ﬁrst two study\
    \ points (QC and MC), and the second \nvalidation point Lennoxville (LX) is located\
    \ east of these points. These locations are two \ncritical agricultural areas\
    \ in Quebec, Canada. They are located between and southeast of \nthe previous\
    \ ones (Figure 2). \n \nFigure 2. The chosen locations in Quebec, Canada; QC:\
    \ Quebec City; MC: Montreal; MA: Mauricie; \nand LX: Lennoxville. \nThe time series\
    \ plots for each location are displayed in Figures 3 and 4. The time \nseries\
    \ length was divided into training and testing intervals, with a ~75:25% ratio\
    \ for mod-\neling purposes. Hence, from a total of 445 months of data, 336 months\
    \ are considered for \nmodel training and the rest, 109 months, for testing. In\
    \ addition, two other sites were se-\nlected to obtain additional TS data for\
    \ further validation of the models. These locations \nwere chosen based on the\
    \ 2018 land cover mapping of the St. Lawrence Lowlands, pub-\nlished by “Environnement\
    \ et Changement Climatique Canada”. The ﬁrst site is in Mau-\nricie (MA), with\
    \ a latitude of 46.216° and longitude of −73.056°, and the second one is in \n\
    Lennoxville (LX), with a latitude of 45.37° and longitude of −71.82°. The diﬀerences\
    \ be-\ntween the study sites and validation sites are illustrated in Figure 5.\
    \ The size of the da-\ntasets of the validation sites is the same as that of the\
    \ study sites. \nFigure 2. The chosen locations in Quebec, Canada; QC: Quebec\
    \ City; MC: Montreal; MA: Mauricie;\nand LX: Lennoxville.\nThe time series plots\
    \ for each location are displayed in Figures 3 and 4. The time series\nlength\
    \ was divided into training and testing intervals, with a ~75:25% ratio for modeling\n\
    purposes. Hence, from a total of 445 months of data, 336 months are considered\
    \ for model\ntraining and the rest, 109 months, for testing. In addition, two\
    \ other sites were selected\nto obtain additional TS data for further validation\
    \ of the models. These locations were\nchosen based on the 2018 land cover mapping\
    \ of the St. Lawrence Lowlands, published by\n“Environnement et Changement Climatique\
    \ Canada”. The ﬁrst site is in Mauricie (MA),\nwith a latitude of 46.216◦ and\
    \ longitude of −73.056◦, and the second one is in Lennoxville\n(LX), with a latitude\
    \ of 45.37◦ and longitude of −71.82◦. The differences between the study\nsites\
    \ and validation sites are illustrated in Figure 5. The size of the datasets of\
    \ the validation\nsites is the same as that of the study sites.\nSustainability\
    \ 2023, 15, 9567\n7 of 21\nSustainability 2023, 15, x FOR PEER REVIEW \n7 of 22\
    \ \n \n \nFigure 3. Soil temperature (TS) time series for all depths for the Quebec\
    \ City (QC) study suburbs. \n \nFigure 4. Soil temperature (TS) time series for\
    \ all depths for Montreal (MC) study suburbs. \n \nFigure 5. Statistical features\
    \ of soil temperature datasets in studied locations; QC: Quebec City; MC: \nMontreal;\
    \ MA: Mauricie; LX: Lennoxville; *V: validation; and *S: study points. \nFigure\
    \ 3. Soil temperature (TS) time series for all depths for the Quebec City (QC)\
    \ study suburbs.\nSustainability 2023, 15, x FOR PEER REVIEW \n7 of 22 \n \n \n\
    Figure 3. Soil temperature (TS) time series for all depths for the Quebec City\
    \ (QC) study suburbs. \n \nFigure 4. Soil temperature (TS) time series for all\
    \ depths for Montreal (MC) study suburbs. \n \nFigure 5. Statistical features\
    \ of soil temperature datasets in studied locations; QC: Quebec City; MC: \nMontreal;\
    \ MA: Mauricie; LX: Lennoxville; *V: validation; and *S: study points. \nFigure\
    \ 4. Soil temperature (TS) time series for all depths for Montreal (MC) study\
    \ suburbs.\nSustainability 2023, 15, x FOR PEER REVIEW \n7 of 22 \n \n \nFigure\
    \ 3. Soil temperature (TS) time series for all depths for the Quebec City (QC)\
    \ study suburbs. \n \nFigure 4. Soil temperature (TS) time series for all depths\
    \ for Montreal (MC) study suburbs. \n \nFigure 5. Statistical features of soil\
    \ temperature datasets in studied locations; QC: Quebec City; MC: \nMontreal;\
    \ MA: Mauricie; LX: Lennoxville; *V: validation; and *S: study points. \nFigure\
    \ 5. Statistical features of soil temperature datasets in studied locations; QC:\
    \ Quebec City; MC:\nMontreal; MA: Mauricie; LX: Lennoxville; *V: validation; and\
    \ *S: study points.\nSustainability 2023, 15, 9567\n8 of 21\n3.2. Modeling, Calibration,\
    \ and Validation\n3.2.1. State-Space Method\nSmoothing methods are among the data\
    \ processing approaches to stationarize time\nseries and eliminate the deterministic\
    \ components. Using speciﬁc algorithms, they can\nremove ﬂuctuations and predictable\
    \ patterns to enable predicting trends, seasonality, and\nfuture values. Among\
    \ them are included moving average, simple exponential, linear expo-\nnential,\
    \ seasonal exponential smoothing, and damped trend exponential smoothing [49–51].\n\
    The state-space (SS) method is a renowned, advanced, and robust triple exponential\n\
    smoothing method used to remove trend, seasonality, and level in highly seasonal\
    \ series\nand has additionally been used for forecasting data [52]. The SS method\
    \ is divided into an\nadditive form for cases where seasonality is roughly constant\
    \ and a multiplicative form\nfor series with seasonal trends. Each one has three\
    \ smoothing parameters that need to be\noptimized using any optimization method,\
    \ or trial and error. The multiplicative form of\nthe method is as follows [53]:\n\
    lt = α(Ts/(St−ω)) + (1 − α)(lt−1 + bt−1)\n(1)\nbt = β(lt − lt−1) + (1 − β)bt−1\n\
    (2)\nSt = γ(Ts/(lt−1 + bt−1)) + (1 − γ)bt−1\n(3)\nSSt+m = (lt + mbt)St−ω+m\n(4)\n\
    In the above equations, lt is level smoothing, bt is trend, and St is seasonal\
    \ components\nsmoothing; A, β, and γ are additional smoothing parameters; Ω is\
    \ seasonality, and m\ndenotes forecasting time steps; SSt+m is the forecasted\
    \ data by the state-space method\nfor m future lags; and T – ω + m assures the\
    \ use of the latest historical data. Using\nreal data to update forecasts, the\
    \ model adapts itself to the new data, and the error is\nsubsequently reduced.\
    \ Therefore, the longer the training set is, the fewer errors will be\ngenerated.\
    \ This feature can be considered a drawback too. In short time series, the model\n\
    cannot adapt to data ﬂuctuations, and therefore, accuracy may be reduced in multi-step\n\
    forecasts. Compared to stochastic models, another advantage of this model is that\
    \ it does\nnot require normalization. The SS parameters can be obtained through\
    \ either trial and error\nor optimization algorithms such as nonlinear generalized\
    \ reduced gradient (GRG), which\nis applied to the three components of the SS\
    \ method in this study.\n3.2.2. Performance Metrics\nThe ﬁrst 75% of the data\
    \ is used in this study to calibrate the model parameters, and\nthe last 25% is\
    \ used for model validation by using the following indices: Coefﬁcients of\ndetermination\
    \ (R2), root mean squared error (RMSE), and mean absolute error (MAE) are\nused\
    \ to evaluate the model accuracy of the time series obtained from pre-processing\
    \ of\nthe original TS data. To assess the modeling power, the Nash–Sutcliffe model\
    \ efﬁciency\n(EN-S) is additionally used [54]. The closer EN-S is to a value of\
    \ one, the more powerful the\nmodel is.\nR2 =\n\n\n\0∑n\ni=1\n\0TS,obsi − TS,obsi\n\
    \x01\0TS,Pi − TS,Pi\n\x01\x01\nq\n∑n\ni=1\n\0TS,obsi − TS,obsi\n\x012 ∑n\ni=1\n\
    \0TS,Pi − TS,Pi\n\x012\n\n\n2\n(5)\nRMSE =\ns\n(\nn\n∑\ni=1\n\x10\nTSobs,i −\
    \ TSP,i\n\x112\n)/n\n(6)\nMAE = 1\nn\nn\n∑\ni=1\n\f\f\fTSobs,i − TSP,i\n\f\f\f\
    \n(7)\nSustainability 2023, 15, 9567\n9 of 21\nEN−S = 1 −\n\" \nN\n∑\ni=1\n\x10\
    \nTSobs,i − TSP,i\n\x112\n!\n/\n \nN\n∑\ni=1\n\x10\nTSobs,i − TSobs,i\n\x112\n\
    !#\n(8)\nwhere TSobs,i, TSP,i and TS are the ith value of observed data, predicted\
    \ soil temperature, and\nmean of data, respectively; n is the number of months.\n\
    4. Results Analysis\n4.1. TS Time Series\nIn order to retrieve the soil temperature\
    \ data from the FLDAS, a set of commands was\ncoded using the GEE API. The extracted\
    \ time series were tested for stationarity, and the\nresults are presented in\
    \ Table 1 for Quebec City and Montreal, respectively. Since the focus\nwas on\
    \ the study points, and the validation points followed the same pattern, the results\
    \ of\nthese points are not presented. The SMK test results show signiﬁcant seasonal\
    \ trends in\nboth TS time series. For each soil layer (there are 3 layers with\
    \ the following upper–lower\ndepths of 0–10, 10–40, and 40–100 cm), the probabilities\
    \ corresponding to the MK and MW\ntest results are higher than the signiﬁcance\
    \ level of 5%. Thus, the presence of non-seasonal\ntrends and jumps in the datasets\
    \ are not signiﬁcant. Figure 6 shows the ACF plots of all\nthe series. The seasonal\
    \ correlations can be seen as sinusoidal patterns, with peaks at lags\n12, 24,\
    \ 36, 48, 60, 72, and 84, equivalent to 7 seasonal lags with a repetition of 12\
    \ months.\nIn these lags, the ACF values are higher than the 95% conﬁdence interval,\
    \ and therefore,\nthese correlations are considered signiﬁcant. They are found\
    \ at all soil depths. Considering\nthe signiﬁcant periodicity that could be observed\
    \ and the presence of a seasonal trend, the\nKPSS test failed to detect the impact\
    \ of these non-stationary factors properly. All probability\nvalues for this test\
    \ are higher than a signiﬁcance level of 5%. It additionally indicates the\nimportance\
    \ of utilizing several tests for stationarity detection. In other words, by using\n\
    the MK and SMK tests, the signiﬁcance of the gradual changes over time was assessed.\n\
    According to these results, these incremental changes were only signiﬁcant seasonally.\
    \ As\nthe MW test demonstrated, no sudden changes were detected in the datasets.\n\
    On the other hand, the ACF plots showed intensive periodicity. Consequently, as\n\
    the seasonal trend and periodicity are both prerequisites of the state-space multiplicative\n\
    method, this method is deemed suitable for forecasting. On the other hand, if\
    \ the series\nwere trendless, then the additive method would have been appropriate.\n\
    Table 1. Stationarity survey for soil temperature time series of three soil layers\
    \ (upper and lower\ndepths of 0–10, 10–40, and 40–100 cm).\nTests\nJump\nQuebec\
    \ City (QC)\nMontreal (MC)\nTrend\nStationarity\nJump\nTrend\nStationarity\ndepth\
    \ cm\nPMW\nPMK\nPSMK\nPKPSS\nPMW\nPMK\nPSMK\nPKPSS\n0–10\n29.69\n19.12\n0.01\n\
    90.05\n48.37\n28.13\n0.06\n95.14\n10–40\n16.74\n12.71\n0.01\n74.74\n36.53\n29.18\n\
    0.32\n92.19\n40–100\n9.97\n13.05\n0.01\n70.65\n32.37\n32.24\n0.17\n90.99\nP =\
    \ p-value > 5% = acceptable.\nSustainability 2023, 15, 9567\n10 of 21\nSustainability\
    \ 2023, 15, x FOR PEER REVIEW \n1\n \n \nFigure 6. Autocorrelation function (ACF)\
    \ plots of TS data. \n4.2. State-Space Modeling Results \nIn this study, as mentioned\
    \ in the methodology section, the state-space (SS) m\nis employed to forecast\
    \ the TS of Montreal and Quebec City. This method require\nsmoothing parameters\
    \ to eliminate deterministic terms in the time series. Moreover\nvalues are needed\
    \ to obtain the SS formula’s level, trend, and seasonal components. \nfore, the\
    \ ﬁrst period of the TS time series was used to initiate the modeling, which \n\
    case, was the ﬁrst 12 months or the ﬁrst seasonal lag. In addition, optional initial\
    \ \nwere chosen for the smoothing parameters α, β, and γ, and they were optimized\
    \ \nGRG solving method to achieve the most accurate results. \nThree types of\
    \ modeling were conducted to examine the SS method’s model\npability. The ﬁrst\
    \ was dynamic modeling, where the model was updated after eac\ncast. Next, a 6-step-ahead\
    \ forecast (fs6) was performed, followed by a 12-step-ahea\ncast (fs12) that covered\
    \ the entire period. The results of the 1-step-ahead forecast (fs1\nels are provided\
    \ in Table 2. As seen, the SS model could forecast the test period \ncoeﬃcient\
    \ of determination higher than 0.90 and with low and relatively close erro\ncept\
    \ for the 40–100 cm depth in Quebec City. Additionally, a decreasing trend in\
    \ \ncuracy by increasing the depths was observed. \nTable 2. State-space dynamic\
    \ modeling (1-step-ahead) for both QC and MC locations. \nStation \nDepth cm \n\
    (α, β, γ) \nR2 \nRMSE °C \nMAE °C \nQuebec City \n0–10 \n(0.164, 0.006, 0.028)\
    \ \n0.970 \n1.633 \n1.340 \n10–40 \n(0.348, 0.014, 0.008) \n0.921 \n1.979 \n1.537\
    \ \n40–100 \n(0.947, 0.010, 0.021) \n0.632 \n2.826 \n2.074 \nMontreal \n0–10 \n\
    (0.168, 0.028, 0.101) \n0.952 \n2.131 \n1.808 \n10–40 \n(0.331, 0.014, 0.008)\
    \ \n0.902 \n2.119 \n1.682 \n40–100 \n(0.000, 0.013, 0.907) \n0.906 \n1.749 \n\
    1.319 \nTables 3 and 4 present the modeling results for the fs6 and fs12 modeling\
    \ me\nAs the soil depth increased, a general decline in accuracy could be seen,\
    \ except for th\nsoil layer (10–40 cm) for both fs6 and fs12. At this depth, the\
    \ accuracy was lower t\nthe other two depths. According to the results, there\
    \ are some variations in the mag\nof the errors with a maximum value of ~3 °C.\
    \ For all depths, the average diﬀeren\ntween forecasts and observations was ~2\
    \ °C except for Montreal (10–40 cm). For th\nbec City modeled time series, all\
    \ sudden drops in accuracy were observed deeper\nFigure 6. Autocorrelation function\
    \ (ACF) plots of TS data.\n4.2. State-Space Modeling Results\nIn this study, as\
    \ mentioned in the methodology section, the state-space (SS) method\nis employed\
    \ to forecast the TS of Montreal and Quebec City. This method requires three\n\
    smoothing parameters to eliminate deterministic terms in the time series. Moreover,\n\
    initial values are needed to obtain the SS formula’s level, trend, and seasonal\
    \ components.\nTherefore, the ﬁrst period of the TS time series was used to initiate\
    \ the modeling, which\nin this case, was the ﬁrst 12 months or the ﬁrst seasonal\
    \ lag. In addition, optional initial\nvalues were chosen for the smoothing parameters\
    \ α, β, and γ, and they were optimized by\nthe GRG solving method to achieve the\
    \ most accurate results.\nThree types of modeling were conducted to examine the\
    \ SS method’s modeling\ncapability. The ﬁrst was dynamic modeling, where the model\
    \ was updated after each\nforecast. Next, a 6-step-ahead forecast (fs6) was performed,\
    \ followed by a 12-step-ahead\nforecast (fs12) that covered the entire period.\
    \ The results of the 1-step-ahead forecast (fs1)\nmodels are provided in Table\
    \ 2. As seen, the SS model could forecast the test period with\na coefﬁcient of\
    \ determination higher than 0.90 and with low and relatively close errors,\nexcept\
    \ for the 40–100 cm depth in Quebec City. Additionally, a decreasing trend in\
    \ the\naccuracy by increasing the depths was observed.\nTable 2. State-space dynamic\
    \ modeling (1-step-ahead) for both QC and MC locations.\nStation\nDepth cm\n(α,\
    \ β, γ)\nR2\nRMSE ◦C\nMAE ◦C\nENS\nQuebec City\n0–10\n(0.164, 0.006, 0.028)\n\
    0.970\n1.633\n1.340\n0.966\n10–40\n(0.348, 0.014, 0.008)\n0.921\n1.979\n1.537\n\
    0.859\n40–100\n(0.947, 0.010, 0.021)\n0.632\n2.826\n2.074\n0.568\nMontreal\n0–10\n\
    (0.168, 0.028, 0.101)\n0.952\n2.131\n1.808\n0.952\n10–40\n(0.331, 0.014, 0.008)\n\
    0.902\n2.119\n1.682\n0.894\n40–100\n(0.000, 0.013, 0.907)\n0.906\n1.749\n1.319\n\
    0.896\nTables 3 and 4 present the modeling results for the fs6 and fs12 modeling\
    \ methods. As\nthe soil depth increased, a general decline in accuracy could be\
    \ seen, except for the ﬁrst soil\nlayer (10–40 cm) for both fs6 and fs12. At this\
    \ depth, the accuracy was lower than in the\nother two depths. According to the\
    \ results, there are some variations in the magnitude of\nthe errors with a maximum\
    \ value of ~3 ◦C. For all depths, the average difference between\nforecasts and\
    \ observations was ~2 ◦C except for Montreal (10–40 cm). For the Quebec City\n\
    modeled time series, all sudden drops in accuracy were observed deeper in the\
    \ soil proﬁle\n(40–100 cm). A decrease in model accuracy was additionally observed\
    \ in the Montreal\nmodel results. However, this reduction in accuracy of the ﬁrst\
    \ soil layer (10–40 cm) is\nSustainability 2023, 15, 9567\n11 of 21\nmore signiﬁcant\
    \ than that of the second (40–100 cm), in contrast to the Quebec City model\n\
    results, which had a uniform trend in accuracy reduction. The Nash–Sutcliffe coefﬁcient\n\
    additionally demonstrates the power and efﬁciency of the generated model in forecasting\n\
    shallow depths. This sudden decline is due to extreme changes in the level component\
    \ of\nthe SS model, and the model could not adequately adapt over that period.\
    \ Furthermore,\nthe parameters are not optimized properly. The TS datasets are\
    \ highly periodic, and the\nSS seasonal parameter was expected to have a greater\
    \ weight in the equation. However,\non the contrary, for the middle soil layer\
    \ in Montreal (10–40 cm), the involvement of the\nseasonal parameter in the model\
    \ equation is less than the other 2 parameters.\nAlongside assessing the models\
    \ using indices such as R2, RMSE, etc., the statistical\nfeatures of the forecasts\
    \ should be evaluated. Appropriate models should additionally\nreproduce the statistical\
    \ characteristics of the original data [55]. To investigate the reason\nfor drops\
    \ in accuracy and evaluate the statistical aspects, box plots of forecasts vs.\
    \ observed\ndata were drawn (Figure 7). Some outliers were observed in QCTSs (40–100\
    \ cm) and TS\nTs (10–40 cm). These outliers were the reason for poor results at\
    \ these depths and were\ncreated because of some extreme values in the level component\
    \ during the training period.\nThese extreme values were multiplied by the SS\
    \ relationship’s seasonal factor and seasonal\ncoefﬁcients. The interquartile\
    \ areas and means of the TS data in the Montreal time series\nwere almost perfectly\
    \ reproduced. However, these features for the Quebec City time series\nﬂuctuated\
    \ by a few degrees centigrade.\nTable 3. State-space modeling (6-steps-ahead)\
    \ for both QC and MC locations.\nStation\nDepth cm\n(α, β, γ)\nR2\nRMSE ◦C\nMAE\
    \ ◦C\nENS\nQuebec City\n0–10\n(0.164, 0.006, 0.028)\n0.974\n1.506\n1.215\n0.971\n\
    10–40\n(0.223, 0.001, 0.015)\n0.889\n2.338\n1.870\n0.803\n40–100\n(0.219, 0.003,\
    \ 0.013)\n0.638\n2.956\n1.948\n0.527\nMontreal\n0–10\n(0.168, 0.028, 0.101)\n\
    0.927\n2.735\n2.175\n0.920\n10–40\n(0.331, 0.014, 0.008)\n0.670\n6.591\n4.255\n\
    0.025\n40–100\n(0.000, 0.013, 0.907)\n0.906\n1.749\n1.319\n0.896\nTable 4. State-space\
    \ modeling (12-steps-ahead) for both QC and MC locations.\nStation\nDepth cm\n\
    (α, β, γ)\nR2\nRMSE ◦C\nMAE ◦C\nENS\nQuebec City\n0–10\n(0.164, 0.006, 0.028)\n\
    0.976\n1.385\n1.116\n0.975\n10–40\n(0.348, 0.014, 0.008)\n0.929\n2.097\n1.745\n\
    0.842\n40–100\n(0.947, 0.010, 0.021)\n0.840\n2.128\n1.813\n0.755\nMontreal\n0–10\n\
    (0.168, 0.028, 0.101)\n0.969\n1.710\n1.378\n0.969\n10–40\n(0.331, 0.014, 0.008)\n\
    0.902\n2.123\n1.562\n0.894\n40–100\n(0.000, 0.013, 0.907)\n0.906\n1.749\n1.319\n\
    0.896\nFigures 8 and 9 display both locations’ TS forecasts and scatter plots.\
    \ The 0–10 cm\nand 40–100 cm soil layers are presented as samples of the forecasting\
    \ capability of the SS\nmethod. These ﬁgures show a good correlation between the\
    \ forecasts and observed data,\nand most of the estimates are located within the\
    \ 10% intervals of the scatter plots. For each\ntime step, the dispersion in the\
    \ forecasts of QC datasets increases with soil depth; however,\nthe forecasts\
    \ were more correlated for the fs12 predictions than in the other time steps.\n\
    The forecasts were more dispersed for the fs6 forecasts of the QC 10–40 cm and\
    \ 40–100 cm\nsoil layers, as the fs6 forecast of the QC 10–40 cm soil layer had\
    \ the most dispersion. The\nsame pattern was observed for the MC forecast datasets,\
    \ albeit the fs6 forecasts of the\nMC 10–40 cm soil layer had the most dispersion.\
    \ According to the time series plots of the\nQC datasets, the SS method generally\
    \ overestimated the highs and lows for all time steps\nof the 10–40 cm and 40–100\
    \ cm soil layers and underestimated the peaks of the 0–10 cm soil\nlayer data.\
    \ On the other hand, the peaks of the MC datasets were generally overestimated\n\
    Sustainability 2023, 15, 9567\n12 of 21\nby the SS method, whereas the lows were\
    \ underestimated. The middle soil layer (10–40 cm)\nof both locations was poorly\
    \ forecasted, compared to the other 2 layers.\nSustainability 2023, 15, x FOR\
    \ PEER REVIEW \n12 of 22 \n \n \nFigure 7. The box plot of the TS data (T) vs.\
    \ forecasts of diﬀerent soil layer depths with diﬀerent \nforecast steps (fs).\
    \ \nFigures 8 and 9 display both locations’ TS forecasts and scatter plots. The\
    \ 0–10 cm and \n40–100 cm soil layers are presented as samples of the forecasting\
    \ capability of the SS \nmethod. These ﬁgures show a good correlation between\
    \ the forecasts and observed data, \nand most of the estimates are located within\
    \ the 10% intervals of the scatter plots. For each \ntime step, the dispersion\
    \ in the forecasts of QC datasets increases with soil depth; how-\never, the forecasts\
    \ were more correlated for the fs12 predictions than in the other time \nsteps.\
    \ The forecasts were more dispersed for the fs6 forecasts of the QC 10–40 cm and\
    \ 40–\n100 cm soil layers, as the fs6 forecast of the QC 10–40 cm soil layer had\
    \ the most dispersion. \nThe same pattern was observed for the MC forecast datasets,\
    \ albeit the fs6 forecasts of the \nMC 10–40 cm soil layer had the most dispersion.\
    \ According to the time series plots of the \nQC datasets, the SS method generally\
    \ overestimated the highs and lows for all time steps \nof the 10–40 cm and 40–100\
    \ cm soil layers and underestimated the peaks of the 0–10 cm \nsoil layer data.\
    \ On the other hand, the peaks of the MC datasets were generally overesti-\nmated\
    \ by the SS method, whereas the lows were underestimated. The middle soil layer\
    \ \n(10–40 cm) of both locations was poorly forecasted, compared to the other\
    \ 2 layers. \nThe magnitude of the outliers seen in the forecasts’ box plots are\
    \ less than a few de-\ngrees, and as seen in the time series plots (Figures 8\
    \ and 9), they can be neglected. More-\nover, these outliers were seen in the\
    \ QC second soil layer (40–100 cm) fs1 and fs6 and MC \nﬁrst soil layer (10–40\
    \ cm) fs6 while they were removed for the fs12 forecasts. As mentioned \nearlier,\
    \ the reasons for the formation of outliers were changes in level components and\
    \ \nsmoothing parameters. However, while the model was updated with the new real\
    \ data in \nfs12 forecasts, and the weight of the seasonal parameters increased,\
    \ the level component \nwas able to correct itself and dampen the errors; therefore,\
    \ no outliers were produced. The \nR2 additionally increased from 0.63 to 0.84.\
    \ The self-correction feature of the SS model \nthrough time represents a positive\
    \ feature; however, as it requires long time series, it can \nadditionally be\
    \ viewed as its weakness. \nFigure 7. The box plot of the TS data (T) vs. forecasts\
    \ of different soil layer depths with different\nforecast steps (fs).\nSustainability\
    \ 2023, 15, x FOR PEER REVIEW \n13 of 22 \n \n \nFigure 8. Forecasts vs. original\
    \ TS data plots for Quebec City (QC), based on the trained model from \n1 October\
    \ 2010 to 1 October 2019. \nFigure 8. Forecasts vs. original TS data plots for\
    \ Quebec City (QC), based on the trained model from\n1 October 2010 to 1 October\
    \ 2019.\nSustainability 2023, 15, 9567\n13 of 21\n \nFigure 8. Forecasts vs. original\
    \ TS data plots for Quebec City (QC), based on the trained model from \n1 October\
    \ 2010 to 1 October 2019. \n \nFigure 9. Forecasts vs. original TS data plots\
    \ for Montreal (MC), based on the trained model from 1 \nOctober 2010 to 1 October\
    \ 2019. \nFigure 9. Forecasts vs. original TS data plots for Montreal (MC), based\
    \ on the trained model from 1\nOctober 2010 to 1 October 2019.\nThe magnitude\
    \ of the outliers seen in the forecasts’ box plots are less than a few degrees,\n\
    and as seen in the time series plots (Figures 8 and 9), they can be neglected.\
    \ Moreover,\nthese outliers were seen in the QC second soil layer (40–100 cm)\
    \ fs1 and fs6 and MC ﬁrst\nsoil layer (10–40 cm) fs6 while they were removed for\
    \ the fs12 forecasts. As mentioned\nearlier, the reasons for the formation of\
    \ outliers were changes in level components and\nsmoothing parameters. However,\
    \ while the model was updated with the new real data in\nfs12 forecasts, and the\
    \ weight of the seasonal parameters increased, the level component\nwas able to\
    \ correct itself and dampen the errors; therefore, no outliers were produced.\
    \ The\nR2 additionally increased from 0.63 to 0.84. The self-correction feature\
    \ of the SS model\nthrough time represents a positive feature; however, as it\
    \ requires long time series, it can\nadditionally be viewed as its weakness.\n\
    The SS method could not produce equally precise results for the last soil layer\n\
    (40–100 cm). This performance is additionally valid for fs6 periods, when compared\
    \ to the\ndifferent soil layer depths and forecasts’ steps. The overall performance\
    \ is acceptable for\ntemporal lag compensation and future state forecasts. This\
    \ method can produce reliable\nresults, compared to black box AI methods and even\
    \ other linear methods such as stochastic\nmodels. Since it has a simple structure,\
    \ it is one of the most generalizable methods. The SS\nmethod was used to model\
    \ data from October 1982 to October 2019. Given the fact that\nthe FLDAS data\
    \ were updated to October 2020, the estimated parameters were used to\nmodel data\
    \ from November 2019 to October 2020 and perform an out-of-sample forecast\nto\
    \ estimate TS values from November 2020 to October 2021. The results are introduced\n\
    in Figure 10. The shaded areas are the out-of-sample forecasts. As seen in this\
    \ ﬁgure,\nthe ~2–3 ◦C error remained in the forecasts of the 2020 data. Considering\
    \ this error, an\nuncertainty bound equal to 3 ◦C can be expected for the 2021\
    \ values.\nSustainability 2023, 15, 9567\n14 of 21\nthat the FLDAS data were updated\
    \ to October 2020, the estimated parameters were used \nto model data from November\
    \ 2019 to October 2020 and perform an out-of-sample fore-\ncast to estimate TS\
    \ values from November 2020 to October 2021. The results are introduced \nin Figure\
    \ 10. The shaded areas are the out-of-sample forecasts. As seen in this ﬁgure,\
    \ the \n~2–3 °C error remained in the forecasts of the 2020 data. Considering\
    \ this error, an uncer-\ntainty bound equal to 3 °C can be expected for the 2021\
    \ values. \n \nFigure 10. Future forecasts are based on the trained model parameters;\
    \ shaded areas are the out-of-\nsample forecasts (12-step-ahead (fs12)) from 1\
    \ November 2020 to 1 October 2021. \n5. Discussion \nIt was mentioned that a signiﬁcant\
    \ seasonal trend exists in these time series. The \nmonthly values of diﬀerent\
    \ years were separated from the series to exhibit this seasonal \ntrend and assess\
    \ its signiﬁcance. The MK test was then applied again. The test results in-\n\
    dicated a trend in each month of the TS time series, some of which are signiﬁcant.\
    \ In Que-\nbec City, the months of August, September, October, and November and\
    \ in Montreal, Sep-\ntember, October, and November had a signiﬁcant positive trend\
    \ for diﬀerent soil layer \ndepths. Each location had a positive trend for seven\
    \ months of the year. This means that \nthe soil is generally getting warmer.\
    \ Figure 11 presents the time series of the ﬁrst soil layer \n(0–10 cm) for August,\
    \ September, October, and November in Quebec City, along their lin-\near trend\
    \ lines, as examples of the progressive warming in the TS time series. \nFigure\
    \ 10. Future forecasts are based on the trained model parameters; shaded areas\
    \ are the out-of-\nsample forecasts (12-step-ahead (fs12)) from 1 November 2020\
    \ to 1 October 2021.\n5. Discussion\nIt was mentioned that a signiﬁcant seasonal\
    \ trend exists in these time series. The\nmonthly values of different years were\
    \ separated from the series to exhibit this seasonal\ntrend and assess its signiﬁcance.\
    \ The MK test was then applied again. The test results\nindicated a trend in each\
    \ month of the TS time series, some of which are signiﬁcant. In\nQuebec City,\
    \ the months of August, September, October, and November and in Montreal,\nSeptember,\
    \ October, and November had a signiﬁcant positive trend for different soil layer\n\
    depths. Each location had a positive trend for seven months of the year. This\
    \ means that\nthe soil is generally getting warmer. Figure 11 presents the time\
    \ series of the ﬁrst soil layer\n(0–10 cm) for August, September, October, and\
    \ November in Quebec City, along their linear\ntrend lines, as examples of the\
    \ progressive warming in the TS time series.\nSustainability 2023, 15, x FOR PEER\
    \ REVIEW \n15 of 22\n \n \nFigure 11. Quebec City (0–10 cm) monthly TS time series,\
    \ from October 1982 to October 2021; Tr.:\ntrend line (linear). \nQian et al.\
    \ [56] performed extensive research on the TS trend and its association with\n\
    the trend in climatical variable changes in Canada. They investigated the TS trend\
    \ in 30\nstations across Canada, from 1958 to 2008. Their research showed a positive\
    \ trend in TS in\n60% of stations, signiﬁcantly in spring and summer means. They\
    \ additionally reported a\npositive correlation between TS trend and changes trend\
    \ in snow cover thickness, air tem-\nperature, rain and snowfall, and total precipitation.\
    \ The correlation between these param-\neters was additionally surveyed in other\
    \ studies [10,57]. Later, Assani et al. [58] performed\nFigure 11. Quebec City\
    \ (0–10 cm) monthly TS time series, from October 1982 to October 2021;\nTr.: trend\
    \ line (linear).\nQian et al. [56] performed extensive research on the TS trend\
    \ and its association with\nthe trend in climatical variable changes in Canada.\
    \ They investigated the TS trend in 30 sta-\ntions across Canada, from 1958 to\
    \ 2008. Their research showed a positive trend in TS in 60%\nof stations, signiﬁcantly\
    \ in spring and summer means. They additionally reported a positive\nSustainability\
    \ 2023, 15, 9567\n15 of 21\ncorrelation between TS trend and changes trend in\
    \ snow cover thickness, air temperature,\nrain and snowfall, and total precipitation.\
    \ The correlation between these parameters was\nadditionally surveyed in other\
    \ studies [10,57]. Later, Assani et al. [58] performed a similar\ninvestigation\
    \ on the maximum and minimum air temperature and rainfall, exclusively to\nQuebec\
    \ province. They reported signiﬁcant trends in these variables during summer,\
    \ indi-\ncating that summer warming in southern Quebec is more pervasive than\
    \ winter warming.\nConsidering these ﬁndings, the progressive warming in soil\
    \ temperature can be justiﬁed\nby reduced rainfall during the warm season and\
    \ decreased soil moisture, especially in the\nmentioned months. Consequently,\
    \ water exploitation typically increases during this season.\nThe increase in\
    \ air temperature and, at a large scale, the impact of global warming have\ncaused\
    \ changes in the overall trend of soil temperature. Some researchers even reported\n\
    a stark trend in soil temperature compared to air temperature [59,60], which means\
    \ the\ntrend in the former is more noticeable than that in the latter. Monitoring\
    \ and controlling\nsoil moisture can be a short-term solution to controlling the\
    \ TS upward trend. However,\nunmanaged water exploitation can signiﬁcantly reduce\
    \ groundwater and decrease soil\nmoisture. This factor is essential as it directly\
    \ impacts the soil’s thermal conductivity and\ncan lower it to one-eighth [61].\
    \ Moreover, the heat-retaining feature of soil is negatively\naffected since the\
    \ speciﬁc heat of pure water (4.2 J/g◦C) is required compared to that of clay\n\
    minerals (0.76 J/g◦C), a dominant component of the soil [10]. To understand the\
    \ magnitude\nof the impact, it should be mentioned that an increase of 1 ◦C of\
    \ 1 gram of dry clay soil\nneeds 1.1 J whereas 1 gram of wet clay soil needs 2.21\
    \ J (average) of energy under laboratory\nconditions with 25% of water content\
    \ and 1300 kg/m3 density. However, this value varies\nfor different conditions\
    \ and depends on soil texture, water content, and soil density [62].\nSoil moisture\
    \ and temperature are the two essential parameters that affect soil res-\npiration.\
    \ The relationship between these parameters and respiration can be estimated\n\
    using several models, e.g., quadratic and exponential models. As mentioned earlier,\
    \ these\nparameters have a strong correlation. Several studies have been carried\
    \ out to distinguish\nthem from one another. In this case, the estimation of soil\
    \ moisture given the available soil\ntemperature at multiple or limited depths\
    \ has been debated by many scholars, and several\nmethods have been proposed in\
    \ this regard [63–66]. These parameters are essential in the\nlong-term prediction\
    \ of drought and food monitoring [39,67,68]. The other applications in-\nclude\
    \ irrigation planning, water resources management, ﬂow forecasting, and hydrological\n\
    land–atmospheric studies.\nHigher temperatures are linked to climate change [69].\
    \ Crop water demand and evap-\notranspiration increase as the temperature rises.\
    \ Consequently, soil moisture decreases,\nand soil droughts result from this phenomenon\
    \ [70]. As the TS level increases over time at\nboth Quebec City and Montreal,\
    \ soil moisture reduction disrupts irrigation scheduling in\nterms of amount and\
    \ time intervals. More water pumping is needed to compensate for the\nreduced\
    \ soil moisture, and the additional supply of water leads to more energy consump-\n\
    tion [71]. However, the interactions between crop water requirement, soil temperature\
    \ and\nmoisture, evapotranspiration, and other hydro–climatic factors are complex.\
    \ Access to the\nparameter records can additionally be limited. Thus, additional\
    \ studies about these factors,\ntheir interactions, and the use of new data collection\
    \ methods are required. Methods such\nas the FLDAS, development of land surface\
    \ models, integration of deep learning methods\nin the processing and simulating\
    \ of different hydro–climatic factors, as well as access to\nsatellite data (i.e.,\
    \ data-driven Earth system science) are needed [72–74].\n6. Validation of the\
    \ SS Method\nAccess to high-quality ﬁeld data has always been problematic, due\
    \ to ﬁnancial and\noperational resource constraints. Therefore, remotely sensed\
    \ data and model-based simula-\ntions have been used for the past couple of decades.\
    \ On the other hand, evaluating and\nvalidating these methods have been equally\
    \ crucial for scholars and users. Hence, different\nvalidation studies are always\
    \ carried out. In our case, the Noah LSM, the central part of the\nFLDAS, has\
    \ been the center of attention for scholarly work with ﬁeld-data alternatives.\
    \ For\nSustainability 2023, 15, 9567\n16 of 21\ninstance, Xia et al. [75] validated\
    \ Noah soil temperature products by comparing them with\nﬁeld datasets from 137\
    \ stations in the United States over different time scales (e.g., daily,\nmonthly,\
    \ and annually) and different soil layer depths, from 0 to 200 cm. These authors\n\
    declared that model products generally emulated observational ﬁeld data. Although\
    \ they\nfound some bias for greater soil layer depths and a decreased accuracy\
    \ of model results,\nthey stated that Noah modeling “Kills” is acceptable. Other\
    \ articles additionally validated\nthe Noah products for different soil conditions\
    \ and locations [76,77]. The FLDAS LDTK\ninputs are additionally monitored and\
    \ evaluated by The National Aeronautics and Space\nAdministration (NASA) and many\
    \ other scholars. Therefore, in this paper, we concentrated\non validating the\
    \ results of our model at different locations. Following the assessments of\n\
    the obtained time series characteristics for these two locations, the proposed\
    \ methodology\nmodeled the datasets. The results are provided in Tables 5 and\
    \ 6 for the dynamic and\n12-step-ahead forecasts.\nTable 5. State-space modeling\
    \ (1-step-ahead) for both validation locations (MA and LX).\nStation\nDepth cm\n\
    (α, β, γ)\nR2\nRMSE ◦C\nMAE ◦C\nENS\nMauricie\n0–10\n(0.171, 0.028, 0.101)\n0.962\n\
    1.883\n1.560\n0.961\n10–40\n(0.160, 0.000, 0.000)\n0.896\n2.471\n1.953\n0.847\n\
    40–100\n(0.000, 0.013, 0.936)\n0.853\n2.095\n1.536\n0.839\nLennoxville\n0–10\n\
    (0.184, 0.030, 0.040)\n0.962\n1.750\n1.453\n0.959\n10–40\n(0.160, 0.000, 0.000)\n\
    0.867\n2.067\n1.468\n0.845\n40–100\n(0.001, 0.010, 0.927)\n0.931\n1.154\n0.879\n\
    0.931\nTable 6. State-space modeling (12-steps-ahead) for both validation locations\
    \ (MA and LX).\nStation\nDepth cm\n(α, β, γ)\nR2\nRMSE ◦C\nMAE ◦C\nENS\nMauricie\n\
    0–10\n(0.171, 0.028, 0.101)\n0.972\n1.629\n1.291\n0.971\n10–40\n(0.160, 0.000,\
    \ 0.000)\n0.925\n3.784\n2.913\n0.641\n40–100\n(0.000, 0.013, 0.936)\n0.853\n2.095\n\
    1.536\n0.839\nLennoxville\n0–10\n(0.184, 0.030, 0.040)\n0.975\n1.372\n1.093\n\
    0.975\n10–40\n(0.160, 0.000, 0.000)\n0.929\n1.556\n1.195\n0.912\n40–100\n(0.001,\
    \ 0.010, 0.927)\n0.932\n1.153\n0.879\n0.931\nThe same prediction pattern as QC\
    \ and MC is seen in these tables. The reduction\nin accuracy with an increase\
    \ in soil layer depth is obvious. An intercomparison of the\nmodels reveals that\
    \ the smoothing parameters related to trend (β) and seasonal factor\n(γ) are decreased\
    \ with an increase in the soil layer, as they are almost equal to 0 or very\n\
    small for the 10–40 cm and 40–100 cm layers. For instance, in Tables 3 and 4,\
    \ the seasonal\nparameter for soil layer 10–40 cm is equal to 0.008, and the trend\
    \ factor is almost equal to\n0.014 for the MC datasets. Nevertheless, after raising\
    \ the seasonal parameter to 0.907 and\nincreasing its involvement in the modeling\
    \ equation, the correlation of the models was\nenhanced. In the QC results, this\
    \ pattern can be seen for both β and γ parameters; however,\nit is not as obvious.\
    \ After modeling the datasets of the validation points, the same problem\nis seen\
    \ in the models. This means that these important factors are not adequately involved\n\
    in the modeling procedure, which can be due to the nature of the data or the disability\n\
    of the optimization method in ﬁnding the appropriate parameters. The reason for\
    \ this\nproblem can be investigated by using other datasets of a different nature\
    \ (e.g., soil moisture)\nand/or changing the optimization method and using metaheuristic\
    \ optimization methods\nto ﬁnd the best-suited parameters (e.g., utilizing a genetic\
    \ algorithm, particle swarm, or\nteacher–learner optimization methods) [38]. In\
    \ the end, as with previous datasets, the\nfuture forecasts based on the trained\
    \ model parameters were conducted, and the results\nare displayed in Figure 12.\
    \ Shaded areas in this ﬁgure are the out-of-sample forecasts\n(12-step-ahead (fs12))\
    \ from 1 November 2020 to 1 October 2021.\nSustainability 2023, 15, 9567\n17 of\
    \ 21\ng\ng\np\ng\np\nthe best-suited parameters (e.g., utilizing a genetic algorithm,\
    \ particle swarm, or te\nlearner optimization methods) [38]. In the end, as with\
    \ previous datasets, the futur\ncasts based on the trained model parameters were\
    \ conducted, and the results a\nplayed in Figure 12. Shaded areas in this ﬁgure\
    \ are the out-of-sample forecasts (1\nahead (fs12)) from 1 November 2020 to 1\
    \ October 2021. \n \nFigure 12. The future forecasts are based on the trained\
    \ model parameters; shaded areas are t\nof-sample forecasts (12-step-ahead (fs12))\
    \ from 1 November 2020 to 1 October 2021. \n7. Conclusions \nSoil temperature\
    \ (TS) is crucial in environmental processes. However, obtaining\nquality ﬁeld\
    \ data is challenging due to resource constraints. Remotely sensed da\nFigure\
    \ 12. The future forecasts are based on the trained model parameters; shaded areas\
    \ are the\nout-of-sample forecasts (12-step-ahead (fs12)) from 1 November 2020\
    \ to 1 October 2021.\n7. Conclusions\nSoil temperature (TS) is crucial in environmental\
    \ processes. However, obtaining high-\nquality ﬁeld data is challenging due to\
    \ resource constraints. Remotely sensed data and\nmodel-based simulations have\
    \ been used as alternatives; however, they have limitations\nsuch as missing data\
    \ and temporal lags. The FLDAS project addresses this issue by\nproviding reliable\
    \ data collection for regions without soil temperature measurements,\nalthough\
    \ it still has a temporal lag. This study focused on TS at two locations (QC and\
    \ MC)\nacross different soil layers. An SS method was developed to model the time\
    \ series, and\nthe proposed techniques were validated through 1-, 6-, and 12-step-ahead\
    \ forecasts of TS\ndata. The results showed that the proposed SS model could ﬁll\
    \ in the gap with an average\naccuracy of R2 of 0.88 and RMSE of 2.073 ◦C for\
    \ the dynamic model, R2 of 0.834 and RMSE\nof 2.979 ◦C for 6-steps-ahead, and\
    \ R2 of 0.921 and RMSE of 1.865 ◦C for different depths\nat the QC and MC locations,\
    \ respectively. The SS method can adjust its parameters as the\nmodel progresses,\
    \ resulting in increased accuracy with more forecasting steps. However,\nthe model’s\
    \ level component is sensitive to extremes, and the SS seasonal parameter is not\n\
    adequately incorporated, leading to over- and underestimations. Future studies\
    \ should\nexplore alternative optimization methods and different types of time\
    \ series to address this\nissue. Additionally, a trend assessment revealed a signiﬁcant\
    \ warming trend in the datasets\nof both locations. It emphasizes the need for\
    \ a close monitoring of TS in these areas and\nconsideration of inﬂuencing factors\
    \ such as groundwater level, surface soil moisture, air\ntemperature, and evapotranspiration.\
    \ The proposed methodology does not require data\npreprocessing, as the SS model\
    \ performs multiple preprocessing steps simultaneously, and\nit can be applied\
    \ to any study location using the FLDAS. However, a major drawback is its\nreliance\
    \ on historical data, and the FLDAS only provides monthly datasets starting from\n\
    1982. It is recommended to additionally model and compare the FLDAS data with\
    \ other\nlinear and machine learning models.\nSustainability 2023, 15, 9567\n\
    18 of 21\nAuthor Contributions: Conceptualization, H.B.; methodology, M.Z.; software,\
    \ M.Z.; validation,\nH.B. and M.Z.; formal analysis, M.Z.; investigation, M.Z.;\
    \ resources, H.B.; data curation, M.Z.;\nwriting—original draft preparation, M.Z.,\
    \ H.B., S.J.G. and A.N.R.; writing—review and editing,\nM.Z., H.B., S.J.G. and\
    \ A.N.R.; visualization, M.Z.; supervision, H.B.; project administration, H.B.;\n\
    funding acquisition, H.B. and M.Z. All authors have read and agreed to the published\
    \ version of\nthe manuscript.\nFunding: This research was funded by Fonds de recherche\
    \ du Québec–Nature et technologies\n(FRQNT) (#316369) and the Natural Sciences\
    \ and Engineering Research Council of Canada (NCERC)\nDiscovery Grant (#RGPIN-2020-04583)\
    \ to perform the current research.\nInstitutional Review Board Statement: Not\
    \ applicable.\nInformed Consent Statement: Not applicable.\nData Availability\
    \ Statement: The readers can ﬁnd the dataset by the following GEE app [SOIL-\n\
    PARAM] developed by [47]: Link to app: https://zemoh.users.Earthengine.app/view/soilparam\n\
    (accessed on 11 June 2023).\nAcknowledgments: The authors acknowledge the ﬁnancial\
    \ support provided by Fonds de recherche\ndu Québec–Nature et technologies (FRQNT)\
    \ (#316369) and the Natural Sciences and Engineering\nResearch Council of Canada\
    \ (NCERT) Discovery Grant (#RGPIN-2020-04583) to perform this current\nresearch.\n\
    Conﬂicts of Interest: The authors declare no conﬂict of interest. The funders\
    \ had no role in the design\nof the study; in the collection, analyses, or interpretation\
    \ of data; in the writing of the manuscript; or\nin the decision to publish the\
    \ results.\nAppendix A\nAlgorithm A1. GEE TS extraction pseudocode.\n1\nDeﬁne\
    \ the point (ee.Geometry.Point(lng, lat))\n2\nSet start time (ee.Date(‘1982-01-01’))\n\
    3\nSet end time (ee.Date(‘any date))\n4\nCalculate the number of months to process\
    \ (ee.Number(endDate-startDate)\n5\nDeﬁne the path by using Earth engine snippet\
    \ (ee.ImageCollection\n(‘NASA/FLDAS/NOAH01/C/GL/M/V001’))\n6\nUse each one of\
    \ the following band names to access the desired data:\n‘SoilTemp00_10cm_tavg’\n\
    ‘SoilTemp10_40cm_tavg’\n‘SoilTemp40_100cm_tavg’\n7\nGenerate a sequence of numbers\
    \ from start to end by (ee.List.sequence(start, end,\nstep, count))\n8\nMap an\
    \ algorithm over a collection by (map(algorithm))\nThe algorithm:\n9\nCalculate\
    \ the offset from start date (.advance(n,’month’))\n10\nAdvance just one month\
    \ (.advance(1,’month’))\n11\nFilter the collection by a date range (.ﬁlterDate(start,end))\n\
    12\nPrint the results\nReferences\n1.\nIwata, Y.; Nemoto, M.; Hasegawa, S.; Yanai,\
    \ Y.; Kuwao, K.; Hirota, T. Inﬂuence of rain, air temperature, and snow cover\
    \ on\nsubsequent spring-snowmelt inﬁltration into thin frozen soil layer in northern\
    \ Japan. J. Hydrol. 2011, 401, 165–176. [CrossRef]\n2.\nPanahi, M.; Khosravi,\
    \ K.; Ahmad, S.; Panahi, S.; Heddam, S.; Melesse, A.M.; Omidvar, E.; Lee, C.-W.\
    \ Cumulative inﬁltration and\ninﬁltration rate prediction using optimized deep\
    \ learning algorithms: A study in Western Iran. J. Hydrol. Reg. Stud. 2021, 35,\n\
    100825. [CrossRef]\n3.\nJaynes, D.B. Temperature Variations Effect on Field-Measured\
    \ Inﬁltration. Soil Sci. Soc. Am. J. 1990, 54, 305–312. [CrossRef]\n4.\nGenxu,\
    \ W.; Tianxu, M.; Juan, C.; Chunlin, S.; Kewei, H. Processes of runoff generation\
    \ operating during the spring and autumn\nseasons in a permafrost catchment on\
    \ semi-arid plateaus. J. Hydrol. 2017, 550, 307–317. [CrossRef]\nSustainability\
    \ 2023, 15, 9567\n19 of 21\n5.\nMittelbach, H.; Lehner, I.; Seneviratne, S.I.\
    \ Comparison of four soil moisture sensor types under ﬁeld conditions in Switzerland.\
    \ J.\nHydrol. 2012, 430–431, 39–49. [CrossRef]\n6.\nNanda, A.; Sen, S.; Sharma,\
    \ A.N.; Sudheer, K.P. Soil Temperature Dynamics at Hillslope Scale—Field Observation\
    \ and Machine\nLearning-Based Approach. Water 2020, 12, 713. [CrossRef]\n7.\n\
    Schindlbacher, A. Effects of soil moisture and temperature on NO, NO2, and N2O\
    \ emissions from European forest soils. J. Geophys.\nRes. 2004, 109, 1137. [CrossRef]\n\
    8.\nWei, S.; Zhang, X.; McLaughlin, N.B.; Liang, A.; Jia, S.; Chen, X.; Chen,\
    \ X. Effect of soil temperature and soil moisture on CO2 ﬂux\nfrom eroded landscape\
    \ positions on black soil in Northeast China. Soil Tillage Res. 2014, 144, 119–125.\
    \ [CrossRef]\n9.\nSauer, T.J.; Horton, R. Soil Heat Flux. In Micrometeorology\
    \ in Agricultural Systems; Hatﬁeld, J.L., Baker, J.M., Eds.; American Society\n\
    of Agronomy, Crop Science Society of America, and Soil Science Society of America:\
    \ Madison, WI, USA, 2005; pp. 131–154.\nISBN 9780891182689.\n10.\nShirvani, A.;\
    \ Moradi-Choghamarani, F.; Zand-Parsa, S.; Moosavi, A.A. Analysis of long-term\
    \ trends in air and soil temperature in\na semi-arid region in Iran. Environ.\
    \ Earth Sci. 2018, 77, 173. [CrossRef]\n11.\nHu, Q.; Feng, S. A Daily Soil Temperature\
    \ Dataset and Soil Temperature Climatology of the Contiguous United States. J.\
    \ Appl.\nMeteorol. 2003, 42, 1139–1156. [CrossRef]\n12.\nBilgili, M. Prediction\
    \ of soil temperature using regression and artiﬁcial neural network models. Meteorol.\
    \ Atmos. Phys. 2010, 110,\n59–70. [CrossRef]\n13.\nHuang, P.M.; Li, Y.; Sumner,\
    \ M.E. Handbook of Soil Sciences: Properties and Processes; CRC Press: Boca Raton,\
    \ FL, USA, 2011;\nISBN 1439803064.\n14.\nDong, X.; Xu, W.; Zhang, Y.; Leskovar,\
    \ D. Effect of Irrigation Timing on Root Zone Soil Temperature, Root Growth and\
    \ Grain\nYield and Chemical Composition in Corn. Agronomy 2016, 6, 34. [CrossRef]\n\
    15.\nTabari, H.; Sabziparvar, A.-A.; Ahmadi, M. Comparison of artiﬁcial neural\
    \ network and multivariate linear regression methods\nfor estimation of daily\
    \ soil temperature in an arid region. Meteorol. Atmos. Phys. 2011, 110, 135–142.\
    \ [CrossRef]\n16.\nKim, S.; Singh, V.P. Modeling daily soil temperature using\
    \ data-driven models and spatial distribution. Theor. Appl. Clim. 2014,\n118,\
    \ 465–479. [CrossRef]\n17.\nKisi, O.; Sanikhani, H.; Cobaner, M. Soil temperature\
    \ modeling at different depths using neuro-fuzzy, neural network, and genetic\n\
    programming techniques. Theor. Appl. Clim. 2017, 129, 833–848. [CrossRef]\n18.\n\
    Mehdizadeh, S.; Behmanesh, J.; Khalili, K. Evaluating the performance of artiﬁcial\
    \ intelligence methods for estimation of monthly\nmean soil temperature without\
    \ using meteorological data. Environ. Earth Sci. 2017, 76, 59. [CrossRef]\n19.\n\
    Nahvi, B.; Habibi, J.; Mohammadi, K.; Shamshirband, S.; Al Razgan, O.S. Using\
    \ self-adaptive evolutionary algorithm to improve\nthe performance of an extreme\
    \ learning machine for estimating soil temperature. Comput. Electron. Agric. 2016,\
    \ 124, 150–160.\n[CrossRef]\n20.\nEbtehaj, I.; Bonakdari, H.; Samui, P.; Gharabaghi,\
    \ B. Multi-depth daily soil temperature modeling: Meteorological variables or\n\
    time series? Theor. Appl. Clim. 2023, 151, 989–1012. [CrossRef]\n21.\nPost, J.;\
    \ Hattermann, F.F.; Krysanova, V.; Suckow, F. Parameter and input data uncertainty\
    \ estimation for the assessment of\nlong-term soil organic carbon dynamics. Environ.\
    \ Model. Softw. 2008, 23, 125–138. [CrossRef]\n22.\nZeynoddin, M.; Bonakdari,\
    \ H.; Ebtehaj, I.; Esmaeilbeiki, F.; Gharabaghi, B.; Haghi, D.Z. A reliable linear\
    \ stochastic daily soil\ntemperature forecast model. Soil Tillage Res. 2019, 189,\
    \ 73–87. [CrossRef]\n23.\nO’Neill, P.; Bindlish, R.; Chan, S.; Njoku, E.; Jackson,\
    \ T. Algorithm Theoretical Basis Document. Level 2 & 3 Soil Moisture (Passive)\n\
    Data Products 2018. Available online: https://smap.jpl.nasa.gov/system/internal_resources/details/original/316_L2_SM_P_\n\
    ATBD_v7_Sep2015.pdf (accessed on 1 January 2023).\n24.\nHe, M.; Hu, Y.; Chen,\
    \ N.; Wang, D.; Huang, J.; Stamnes, K. High cloud coverage over melted areas dominates\
    \ the impact of clouds\non the albedo feedback in the Arctic. Sci. Rep. 2019,\
    \ 9, 9529. [CrossRef] [PubMed]\n25.\nSarafanov, M.; Kazakov, E.; Nikitin, N.O.;\
    \ Kalyuzhnaya, A.V. A Machine Learning Approach for Remote Sensing Data Gap-Filling\n\
    with Open-Source Implementation: An Example Regarding Land Surface Temperature,\
    \ Surface Albedo and NDVI. Remote Sens.\n2020, 12, 3865. [CrossRef]\n26.\nWeiss,\
    \ D.J.; Atkinson, P.M.; Bhatt, S.; Mappin, B.; Hay, S.I.; Gething, P.W. An effective\
    \ approach for gap-ﬁlling continental scale\nremotely sensed time-series. ISPRS\
    \ J. Photogramm. Remote Sens. 2014, 98, 106–118. [CrossRef] [PubMed]\n27.\nEskelson,\
    \ B.N.I.; Temesgen, H.; Lemay, V.; Barrett, T.M.; Crookston, N.L.; Hudak, A.T.\
    \ The roles of nearest neighbor methods in\nimputing missing data in forest inventory\
    \ and monitoring databases. Scand. J. For. Res. 2009, 24, 235–246. [CrossRef]\n\
    28.\nBhattacharjee, S.; Mitra, P.; Ghosh, S.K. Spatial Interpolation to Predict\
    \ Missing Attributes in GIS Using Semantic Kriging. IEEE\nTrans. Geosci. Remote\
    \ Sens. 2014, 52, 4771–4780. [CrossRef]\n29.\nBărbulescu, A.; S, erban, C.; Indrecan,\
    \ M.-L. Computing the Beta Parameter in IDW Interpolation by Using a Genetic Algorithm.\n\
    Water 2021, 13, 863. [CrossRef]\n30.\nLopes Martins, L.; Martins, W.A.; Rodrigues,\
    \ I.C.D.A.; Freitas Xavier, A.C.; Moraes, J.F.L.D.; Blain, G.C. Gap-ﬁlling of\
    \ daily\nprecipitation and streamﬂow time series: A method comparison at random\
    \ and sequential gaps. Hydrol. Sci. J. 2023, 68, 148–160.\n[CrossRef]\n31.\nChen,\
    \ Y.-Y.; Chu, C.-R.; Li, M.-H. A gap-ﬁlling model for eddy covariance latent heat\
    \ ﬂux: Estimating evapotranspiration of a\nsubtropical seasonal evergreen broad-leaved\
    \ forest as an example. J. Hydrol. 2012, 468–469, 101–110. [CrossRef]\nSustainability\
    \ 2023, 15, 9567\n20 of 21\n32.\nCugueró-Escofet, M.À.; García, D.; Quevedo, J.;\
    \ Puig, V.; Espin, S.; Roquet, J. A methodology and a software tool for sensor\
    \ data\nvalidation/reconstruction: Application to the Catalonia regional water\
    \ network. Control Eng. Pract. 2016, 49, 159–172. [CrossRef]\n33.\nVijayakumar,\
    \ N.; Vennila, S. A comparative analysis of forecasting reservoir inﬂow using\
    \ ARMA model and Holt winters\nexponential smoothening technique. Int. J. Innov.\
    \ Sci. Math. 2016, 4, 85–90.\n34.\nHeydari, M.; Benisi Ghadim, H.; Rashidi, M.;\
    \ Noori, M. Application of Holt-Winters Time Series Models for Predicting Climatic\n\
    Parameters (Case Study: Robat Garah-Bil Station, Iran). Pol. J. Environ. Stud.\
    \ 2020, 29, 617–627. [CrossRef]\n35.\nPuah, Y.J.; Huang, Y.F.; Chua, K.C.; Lee,\
    \ T.S. River catchment rainfall series analysis using additive Holt–Winters method.\
    \ J. Earth\nSyst. Sci. 2016, 125, 269–283. [CrossRef]\n36.\nWahyuningsih, S.;\
    \ Goejantoro, R.; Rizki, N.A. Forecasting hotspots in East Kutai, Kutai Kartanegara,\
    \ and West Kutai as early\nwarning information. IOP Conf. Ser. Earth Environ.\
    \ Sci. 2018, 144, 12022. [CrossRef]\n37.\nZeynoddin, M.; Ebtehaj, I.; Bonakdari,\
    \ H. Development of a linear based stochastic model for daily soil temperature\
    \ prediction:\nOne step forward to sustainable agriculture. Comput. Electron.\
    \ Agric. 2020, 176, 105636. [CrossRef]\n38.\nZeynoddin, M.; Bonakdari, H. Structural-optimized\
    \ sequential deep learning methods for surface soil moisture forecasting, case\n\
    study Quebec, Canada. Neural Comput. Appl. 2022, 34, 19895–19921. [CrossRef]\n\
    39.\nMcNally, A.; Arsenault, K.; Kumar, S.; Shukla, S.; Peterson, P.; Wang, S.;\
    \ Funk, C.; Peters-Lidard, C.D.; Verdin, J.P. A land data\nassimilation system\
    \ for sub-Saharan Africa food and water security applications. Sci. Data 2017,\
    \ 4, 170012. [CrossRef]\n40.\nHamilton, J. Time Series Econometrics; Princeton\
    \ University Press: Princeton, NJ, USA, 1994.\n41.\nKwiatkowski, D.; Phillips,\
    \ P.C.; Schmidt, P.; Shin, Y. Testing the null hypothesis of stationarity against\
    \ the alternative of a unit\nroot. J. Econom. 1992, 54, 159–178. [CrossRef]\n\
    42.\nYue, S.; Pilon, P.; Phinney, B.; Cavadias, G. The inﬂuence of autocorrelation\
    \ on the ability to detect trend in hydrological series.\nHydrol. Process. 2002,\
    \ 16, 1807–1829. [CrossRef]\n43.\nTekleab, S.G.; Kassew, A. Hydrologic responses\
    \ to land use/Land cover change in the Kesem Watershed, Awash basin, Ethiopia.\n\
    J. Spat. Hydrol. 2019, 15, 1–31.\n44.\nPekel, J.-F.; Cottam, A.; Gorelick, N.;\
    \ Belward, A.S. High-resolution mapping of global surface water and its long-term\
    \ changes.\nNature 2016, 540, 418–422. [CrossRef]\n45.\nKlisch, A.; Atzberger,\
    \ C. Operational Drought Monitoring in Kenya Using MODIS NDVI Time Series. Remote\
    \ Sens. 2016, 8, 267.\n[CrossRef]\n46.\nLobell, D.B.; Thau, D.; Seifert, C.; Engle,\
    \ E.; Little, B. A scalable satellite-based crop yield mapper. Remote Sens. Environ.\
    \ 2015, 164,\n324–333. [CrossRef]\n47.\nZeynoddin, M.; Bonakdari, H.; Gumiere,\
    \ S.J.; Caron, J.; Rousseau, A.N. SOILPARAM 1.0: A Global-Scaled Enhanced Remote\
    \ Sens-\ning Application for Soil Characteristics Data Retrieval–Google Engine\
    \ Environment, An Open-Source Treasure. In Proceedings of\nthe 39th IAHR World\
    \ Congress from Snow to Sea, Granada, Spain, 18–23 June 2022; pp. 5309–5319, ISBN\
    \ 978-90-832612-1-8.\n48.\nBeck, H.E.; Zimmermann, N.E.; McVicar, T.R.; Vergopolan,\
    \ N.; Berg, A.; Wood, E.F. Present and future Köppen-Geiger climate\nclassiﬁcation\
    \ maps at 1-km resolution. Sci. Data 2018, 5, 180214. [CrossRef] [PubMed]\n49.\n\
    Taylor, J.W. Exponential smoothing with a damped multiplicative trend. Int. J.\
    \ Forecast. 2003, 19, 715–725. [CrossRef]\n50.\nKalekar, P.S. Time series forecasting\
    \ using holt-winters exponential smoothing. Kanwal Rekhi Sch. Inf. Technol. 2004,\
    \ 4329008, 1–13.\n51.\nArroyo, J.; San Roque, A.M.; Maté, C.; Sarabia, A. Exponential\
    \ smoothing methods for interval time series. In Proceedings of the\n1st European\
    \ Symposium on Time Series Prediction, Helsinki, Finland, 7–9 February 2007; pp.\
    \ 231–240.\n52.\nGoodwin, P. The holt-winters approach to exponential smoothing:\
    \ 50 years old and going strong. Foresight 2010, 19, 30–33.\n53.\nHyndman, R.J.;\
    \ Athanasopoulos, G. Forecasting: Principles and Practice; Otexts: Melbourne,\
    \ Australia, 2013.\n54.\nNash, J.E.; Sutcliffe, J.V. River ﬂow forecasting through\
    \ conceptual models part I—A discussion of principles. J. Hydrol. 1970, 10,\n\
    282–290. [CrossRef]\n55.\nSingh, V.P.; Frevert, D.K. Mathematical Models of Small\
    \ Watershed Hydrology and Applications; Water Resources Publication: Littleton,\n\
    CO, USA, 2002; ISBN 1887201351.\n56.\nQian, B.; Gregorich, E.G.; Gameda, S.; Hopkins,\
    \ D.W.; Wang, X.L. Observed soil temperature trends associated with climate\n\
    change in Canada. J. Geophys. Res. 2011, 116, 1–16. [CrossRef]\n57.\nChudinova,\
    \ S.M.; Frauenfeld, O.W.; Barry, R.G.; Zhang, T.; Sorokovikov, V.A. Relationship\
    \ between air and soil temperature\ntrends and periodicities in the permafrost\
    \ regions of Russia. J. Geophys. Res. 2006, 111, 1–15. [CrossRef]\n58.\nAssani,\
    \ A.A.; Maloney-Dumont, V.; Pothier-Champagne, A.; Kinnard, C.; Quéssy, J.-F.\
    \ Comparison of the temporal variability\nof summer temperature and rainfall as\
    \ it relates to climate indices in southern Quebec (Canada). Theor. Appl. Clim.\
    \ 2019, 137,\n2425–2435. [CrossRef]\n59.\nDu, J.; Li, C.; Liao, J.; Lhak, P.;\
    \ Lu, H. Soil temperature change at shallow layer in Lhasa from 1961 to 2005.\
    \ Arid Land Geogr. 2007,\n6, 826–831.\n60.\nZhang, T.; Barry, R.G.; Gilichinsky,\
    \ D.; Bykhovets, S.S.; Sorokovikov, V.A.; Ye, J. An ampliﬁed signal of climatic\
    \ change in soil\ntemperatures during the last century at Irkutsk, Russia. Clim.\
    \ Chang. 2001, 49, 41–76. [CrossRef]\n61.\nAl Nakshabandi, G.; Kohnke, H. Thermal\
    \ conductivity and diffusivity of soils as related to moisture tension and other\
    \ physical\nproperties. Agric. Meteorol. 1965, 2, 271–279. [CrossRef]\n62.\nAbu-Hamdeh,\
    \ N.H. Thermal Properties of Soils as affected by Density and Water Content. Biosyst.\
    \ Eng. 2003, 86, 97–102.\n[CrossRef]\nSustainability 2023, 15, 9567\n21 of 21\n\
    63.\nLu, S.; Ju, Z.; Ren, T.; Horton, R. A general approach to estimate soil water\
    \ content from thermal inertia. Agric. For. Meteorol. 2009,\n149, 1693–1698. [CrossRef]\n\
    64.\nSteele-Dunne, S.C.; Rutten, M.M.; Krzeminska, D.M.; Hausner, M.; Tyler, S.W.;\
    \ Selker, J.; Bogaard, T.A.; van de Giesen, N.C.\nFeasibility of soil moisture\
    \ estimation using passive distributed temperature sensing. Water Resour. Res.\
    \ 2010, 46, 234. [CrossRef]\n65.\nDong, J.; Steele-Dunne, S.C.; Ochsner, T.E.;\
    \ van de Giesen, N. Determining soil moisture by assimilating soil temperature\n\
    measurements using the Ensemble Kalman Filter. Adv. Water Resour. 2015, 86, 340–353.\
    \ [CrossRef]\n66.\nZhang, S.-W.; Qiu, C.-J.; Xu, Q. Estimating Soil Water Contents\
    \ from Soil Temperature Measurements by Using an Adaptive\nKalman Filter. J. Appl.\
    \ Meteorol. 2004, 43, 379–389. [CrossRef]\n67.\nSvoboda, M.; LeComte, D.; Hayes,\
    \ M.; Heim, R.; Gleason, K.; Angel, J.; Rippey, B.; Tinker, R.; Palecki, M.; Stooksbury,\
    \ D.; et al.\nThe Drought Monitor. Bull. Am. Meteorol. Soc. 2002, 83, 1181–1190.\
    \ [CrossRef]\n68.\nFan, Y.; van den Dool, H. Climate Prediction Center global\
    \ monthly soil moisture data set at 0.5◦ resolution for 1948 to present. J.\n\
    Geophys. Res. 2004, 109, 549. [CrossRef]\n69.\nTurral, H.; Burke, J.; Faurès,\
    \ J.-M. Climate Change, Water and Food Security; Food and Agriculture Organization\
    \ of the United\nNations (FAO): Rome, Italy, 2011; ISBN 9251067953.\n70.\nZhang,\
    \ Z.; Pan, Z.; Pan, F.; Zhang, J.; Han, G.; Huang, N.; Wang, J.; Pan, Y.; Wang,\
    \ Z.; Peng, R. The Change Characteristics and\nInteractions of Soil Moisture and\
    \ Temperature in the Farmland in Wuchuan County, Inner Mongolia, China. Atmosphere\
    \ 2020, 11,\n503. [CrossRef]\n71.\nPasha, M.F.K.; Srinivasamurthy, N.; Singh,\
    \ D.; Yeasmin, D.; Valenzuela, G. An Artiﬁcial Intelligence Model to Predict Crop\n\
    Water Requirement Using Weather, Soil Moisture, and Plant Health Monitoring Data.\
    \ In World Environmental and Water Resources\nCongress 2020: Water Resources Planning\
    \ and Management and Irrigation and Drainage; American Society of Civil Engineers:\
    \ Reston,\nVA, USA, 2020; pp. 9–14.\n72.\nReichstein, M.; Camps-Valls, G.; Stevens,\
    \ B.; Jung, M.; Denzler, J.; Carvalhais, N.; Prabhat. Deep learning and process\
    \ understand-\ning for data-driven Earth system science. Nature 2019, 566, 195–204.\
    \ [CrossRef]\n73.\nSu, Y.-S.; Ni, C.-F.; Li, W.-C.; Lee, I.-H.; Lin, C.-P. Applying\
    \ deep learning algorithms to enhance simulations of large-scale\ngroundwater\
    \ ﬂow in IoTs. Appl. Soft Comput. 2020, 92, 106298. [CrossRef]\n74.\nHu, C.; Wu,\
    \ Q.; Li, H.; Jian, S.; Li, N.; Lou, Z. Deep Learning with a Long Short-Term Memory\
    \ Networks Approach for\nRainfall-Runoff Simulation. Water 2018, 10, 1543. [CrossRef]\n\
    75.\nXia, Y.; Ek, M.; Shefﬁeld, J.; Livneh, B.; Huang, M.; Wei, H.; Feng, S.;\
    \ Luo, L.; Meng, J.; Wood, E. Validation of Noah-Simulated\nSoil Temperature in\
    \ the North American Land Data Assimilation System Phase 2. J. Appl. Meteorol.\
    \ Climatol. 2013, 52, 455–471.\n[CrossRef]\n76.\nZheng, D.; van der Velde, R.;\
    \ Su, Z.; Wen, J.; Wang, X.; Yang, K. Evaluation of Noah Frozen Soil Parameterization\
    \ for Application\nto a Tibetan Meadow Ecosystem. J. Hydrometeor. 2017, 18, 1749–1763.\
    \ [CrossRef]\n77.\nWang, L.; Li, X.; Chen, Y.; Yang, K.; Chen, D.; Zhou, J.; Liu,\
    \ W.; Qi, J.; Huang, J. Validation of the global land data assimilation\nsystem\
    \ based on measurements of soil temperature proﬁles. Agric. For. Meteorol. 2016,\
    \ 218–219, 288–297. [CrossRef]\nDisclaimer/Publisher’s Note: The statements, opinions\
    \ and data contained in all publications are solely those of the individual\n\
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or\
    \ the editor(s) disclaim responsibility for any injury to\npeople or property\
    \ resulting from any ideas, methods, instructions or products referred to in the\
    \ content.\n"
  inline_citation: '>'
  journal: Sustainability
  limitations: '>'
  pdf_link: https://www.mdpi.com/2071-1050/15/12/9567/pdf?version=1686738488
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Multi-Tempo Forecasting of Soil Temperature Data; Application over Quebec,
    Canada
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.21203/rs.3.rs-3343374/v1
  analysis: '>'
  authors:
  - Babita Majhi
  - R. Baloji Naik
  - Sujata Dash
  - Saurav Mallik
  - Amal Al‐Rasheed
  - Mohamed Abbas
  - Ben Othman Soufiene
  citation_count: 0
  full_citation: '>'
  full_text: ">\nForecasting of Daily Pan Evaporation Rate using\nDeep Learning Techniques\
    \ for Three Different Agro-\nClimatic Regions of Chhattisgarh State\nBabita Majhi \n\
    Central University\nRupesh Naik \nCentral University\nSujata Dash \nNagaland University\n\
    Saurav Mallik  (  sauravmtech2@gmail.com )\nHarvard T H Chan School of Public\
    \ Health\nAmal Al-Rasheed \nPrincess Nourah bint Abdulrahman University\nMohamed\
    \ Abbas \nKing Khalid University\nBen Othman Sou¦ene \nUniversity of Sousse\n\
    Research Article\nKeywords: Pan evaporation, Deep learning, Deep Neural Network,\
    \ Recurrent Neural Network, Long Short-\nTerm Memory, Bidirectional LSTM\nPosted\
    \ Date: September 21st, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3343374/v1\n\
    License:   This work is licensed under a Creative Commons Attribution 4.0 International\
    \ License.  \nRead Full License\nAdditional Declarations: No competing interests\
    \ reported.\n1 \n \nForecasting of Daily Pan Evaporation Rate using Deep \nLearning\
    \ Techniques for Three Different Agro-Climatic \nRegions of Chhattisgarh State\
    \ \nBabita Majhi1,*, Rupesh Naik1, Sujata Dash2, Saurav Mallik3,4*Amal Al-Rasheed5,*,\
    \ \nMohamed Abbas6, Ben Othman Soufiene7 \n1Department of CSIT, Guru Ghasidas\
    \ Vishwavidyalaya, Central University, Bilaspur, \nChhattisgarh, India, 495009\
    \ email: babita.majhi@gmail.com, rupeshnaik493@gmail.com \n2Department of Information\
    \ Technology, School of Engineering and Technology, Nagaland \nUniversity, \n\
    Dimapur \nCampus, \nNagaland, \nIndia \n– \n797112, \nemail: \nsujata@nagalanduniversity.ac.in\
    \ \n3Department of Environmental Health, Harvard T H Chan School of Public Health,\
    \ Boston, \nMA-02115, USA, sauravmtech2@gmail.com, smallik@hsph.harvard.edu \n\
    4Department of Pharmacology & Toxicology, University of Arizona, Tucson, AZ 85711,\
    \ \nUSA, sauravmtech2@gmail.com, smallik@arizona.edu \n5Department of Information\
    \ Systems, College of Computer and Information Sciences, \nPrincess Nourah bint\
    \ Abdulrahman University, P.O. Box 84428, Riyadh 11671, Saudi \nArabia; aaalrasheed@pnu.edu.sa\
    \ \n6Electrical Engineering Department, College of Engineering, King Khalid University,\
    \ Abha \n61421, Saudi Arabia; mabas@kku.edu.sa \n7PRINCE Laboratory Research,\
    \ ISITcom, Hammam Sousse, University of Sousse, Sousse, \nTunisia; ben_oth_soufiene@yahoo.fr\
    \ \n \n*Correspondence: Amal Al-Rasheed (aaalrasheed@pnu.edu.sa), Saurav Mallik\
    \ \n(sauravmtech2@gmail.com, smallik@hsph.harvard.edu) and Babita Majhi \n(babita.majhi@gmail.com)\
    \  \n \nAbstract \nAccurate measurement or computation of evaporation loss is\
    \ crucial for developing and successfully \nimplementing water resource management\
    \ strategies, irrigation planning, reservoir management, \nhydropower generation,\
    \ drought and flood mitigation, urban planning and increasing agricultural \n\
    productivity, especially in drought-prone areas. Evaporation can be measured directly\
    \ using \n2 \n \nevaporimeters or forecasted using empirical models based on climatic\
    \ variables such as temperature, \nhumidity, wind speed, sunlight, and solar radiation,\
    \ that influence the evaporation process. Modeling \nevaporation using climatic\
    \ factors is difficult, especially when accounting for the wide range of agro-\n\
    climatic conditions as it is an exceedingly nonlinear process. This paper uses\
    \ different machine \nlearning (ML) and deep learning algorithms to estimate pan\
    \ evaporation(\U0001D438\U0001D443) for three distinct agro-\nclimatic zones in\
    \ the Indian state of Chhattisgarh.  In this research, the performance of three\
    \ machine \nlearning models (Support Vector Machine, AdaBoost, and XGBoost) and\
    \ four deep learning models \n(Deep Neural Network, Recurrent Neural Network,\
    \ Long Short-Term Memory, and Bidirectional \nLong Short Term Memory) are evaluated\
    \ and outcomes from each location are compared. Simulation \nresults demonstrated\
    \ that across all three regions, deep-Learning models outperform machine-learning\
    \ \nand conventional models. Out of all deep learning models DRNN perform the\
    \ best. As the results \nexhibit that the \U0001D438\U0001D443 loss per day is\
    \ less than 1 mm, the proposed model can be used for irrigation \nscheduling,\
    \ water resource management which is very important for agriculture and its related\
    \ \nactivities. \nKeywords: Pan evaporation, Deep learning, Deep Neural Network,\
    \ Recurrent Neural \nNetwork, Long Short-Term Memory, Bidirectional LSTM. \n1.\
    \ Introduction \nAgriculture is a field that has an immense effect on the economy\
    \ of India. However, as years \nroll on, the ratio of people to resources becomes\
    \ unsustainable. With each passing year, the \nmost efficient use of resources\
    \ becomes increasingly important. The weather has become \nincreasingly unpredictable\
    \ as time has passed. Because of this, the normal pattern of the \nseasons has\
    \ been changed. This affects the amount of water required by crops. The crop gets\
    \ \nthe water it needs from rain and irrigation mostly. As a result of evaporation,\
    \ a hundred \npercent of the available water is not used by the crop. Hence evaporation\
    \ rate prediction has \nemerged as a pivotal component of plant development and\
    \ maintenance. The rate of pan \nevaporation get affected by the different climatic\
    \ factors like temperature, rainfall, relative \n3 \n \nhumidity, wind speed and\
    \ solar radiation. Many authors [1]–[7] have previously \nrecommended traditional\
    \ approaches for estimating pan evaporation, but these methods are \ndifficult\
    \ and time-consuming. Therefore, different machine learning (ML) algorithms are\
    \ \nwidely used for this. This study [8] explores the regional and temporal trends\
    \ of Pan \nevaporation (\U0001D438\U0001D443)and assess its regulating elements\
    \ using daily meteorological records from \n2,018 sites throughout China from\
    \ 1965 to 2018. This model optimizes in shorter time for \nclimatic data of the\
    \ Bakersfield station (California, USA). The evaporation at the Jabalpur \nstation\
    \ in the Kymore Plateau and Satpura Hills Agro-climatic Zone of Madhya Pradesh\
    \ in \nCentral India is estimated using the traditional statistical Linear Regression\
    \ (LR) approach, \nconventional empirical methods of Linacre and Christianson,\
    \ and a Multi-layer Perception \n(MLP) and Radial Basis Function (RBF) based neural\
    \ network [9]. Input of all the chosen \nmeteorological factors into MLP and RBF\
    \ based models yields more accurate evaporation \nestimates than LR and empirical\
    \ methods. To better understand the meteorological and \nenvironmental changes\
    \ in the Qinghai-Tibet Plateau (QTP) and China, \U0001D438\U0001D443 study is\
    \ performed \nin [10]. This research aimed to predict monthly and yearly grid\
    \ pan evaporation using the Pen \nPan model. Random Forest (RF) and two deep learning\
    \ approaches, Convolution neural \nnetwork (CNN) and deep neural network (DNN),\
    \ are utilized in [11], to properly forecast \nmonthly pan evaporation rates.\
    \ To effectively estimate the pan evaporation two metaheuristic \nintegrated predictors\
    \ are employed by [12]. Shuffled complex evolution (SCE) and \nelectromagnetic\
    \ field optimization (EFO) are two of the fastest metaheuristic algorithms that\
    \ \nare synthesized with artificial neural network (ANN).For the purpose of improving\
    \ the results \nof estimating \U0001D438\U0001D443 on a monthly scale of pan evaporation\
    \ in four semi-arid meteorological \nstations (Ardabil, Khalkhal, Manjil (from\
    \ Iran), and Grand Island (from the United States)), \n[13] has proposed a hybrid\
    \ method called multiple model-support vector machine (MM-\nSVM). In [14], the\
    \ authors use data from five meteorological stations across Iran's humid, \n4\
    \ \n \nsub-humid, arid, and hyper-arid climate zones to evaluate the efficacy\
    \ of novel meta-heuristic \noptimization algorithms like Genetic Algorithm (GA),\
    \ Grey Wolf Optimization (GWO), and \nWhale Optimization Algorithm (WOA) in tandem\
    \ with ANNs for estimating daily \U0001D438\U0001D443 and \noptimizing explicit\
    \ predictive. Daily pan evaporation rate is estimated using a functional link\
    \ \nartificial neural network (FLANN) in [15]. Using the identical input data\
    \ and characteristics, \nthe FLANN model's output is compared to that of a multi-layer\
    \ artificial neural network \n(MLANN) and two empirical approaches. In [16] the\
    \ influence of data preprocessing on the \nANN-based prediction intervals generation\
    \ of the evaporation process in several climatic \nzones in Iran is noted. \n\
    \ \nMotivated by the review of relevant literature an attempt has been made to\
    \ develop \nsuitable models for predicting daily pan evaporation at three stations\
    \ covering the state of \nChhattisgarh in central India using deep learning models.\
    \ It is also noted that very few work \nhas been done in this field for the state\
    \ of Chhattisgarh whose important living hood is \nagriculture mostly and the\
    \ state is also known as rice bowl of India.  The contribution of the \npaper\
    \ is as follows: \n1. Development of Deep learning models, DNN, DRNN, DLSTM and\
    \ DBILSTM for \n\U0001D438\U0001D443 prediction. \n2. Implementation of three\
    \ ML models such as Adaboost, XGBoost, SVM. \n3. Comparison of deep learning models\
    \ with ML models. \nThe methodology section describes in detail the research locations,\
    \ data sets, architecture, \nand implementation of all the models explored in\
    \ this paper. The simulation study and the \nfindings collected are discussed\
    \ in the next section. Finally, the study's contribution is \ndescribed in the\
    \ final section. \n \n5 \n \n2. Materials and methods \n2.1 Study location \n\
    This research makes use of data from three meteorological stations in the state\
    \ of \nChhattisgarh in the heart of India. The three stations: Raipur, Jagdalpur,\
    \ and Ambikapur, \nwhich correspond to the Chhattisgarh Plains, the Bastar Plateau,\
    \ and the Northern Hills, \nrespectively. In Ambikapur, the rainy season is stifling\
    \ and foggy, whereas the dry season is \noften clear and hot. Temperatures seldom\
    \ drop below 43 degrees Fahrenheit or rise over 111 \ndegrees Fahrenheit throughout\
    \ the year, often hovering between 49 and 105 degrees. \nSummers in Raipur are\
    \ short but very hot, while winters are short but frigid; the city has a \nlargely\
    \ dry and clear climate year-round. Throughout the year, temperature typically\
    \ ranges \nfrom 51 to 104 degrees Fahrenheit, with occasional dips to 46 degrees\
    \ and spikes to 109 \ndegrees. Whereas Jagdalpur's wet season is muggy and cloudy,\
    \ the dry season is consistently \nclear and hot throughout the year. Average\
    \ highs for the year are about 102 degrees \nFahrenheit, while lows seldom drop\
    \ below 48 degrees or rise beyond 108 degrees. The \nlocations of these stations\
    \ are shown in Figure 1. \n2.2 Metrological data used in study \nData for three\
    \ Chhattisgarh stations—Ambikapur, Jagdalpur, and Raipur from IMD-certified \n\
    repositories are collected to accurately estimate the pan evaporation value. The\
    \ time period \ncovered by the datasets is given in Table 1. Data from Raipur\
    \ and Jagdalpur are complete, \nwhile statistics from Ambikapur is lacking certain\
    \ values. The KNN imputer [17] is used to \ndeal with the missing values at the\
    \ Ambikapur station using the algorithm as given below. \nMinimum temperature\
    \ (\U0001D440\U0001D456\U0001D45B\U0001D447), maximum temperature (\U0001D440\U0001D44E\
    \U0001D465\U0001D447), relative humidity 1 (\U0001D445\U0001D43B1), \nrelative\
    \ humidity 2 (\U0001D445\U0001D43B2), bright sunlight hours (\U0001D435\U0001D446\
    \U0001D446), and wind speed (\U0001D44A\U0001D446) are the six \nclimatic factors\
    \ that have been taken into consideration in this study. Other environmental \n\
    6 \n \nconditions, such as radiation, also influence the rate at which water evaporates\
    \ from a pan, \nbut no statistics are available for this variable. Because of\
    \ this, accurate estimates of pan \nevaporation are more difficult to achieve.\
    \ Since the majority of days will not experience \nprecipitation, this information\
    \ is disregarded during simulation. Then data are normalized and \nscaled between\
    \ 0 and 1using min-max normalization.  \nAlgorithm: Algorithm for KNN Imputer[17]\
    \ \n1. Start \n2. Locate the missing values in dataset. \n3. Determine the distance\
    \ for the value that is missing. \n4. Find the top K distances by sorting the\
    \ distances in ascending order.  \n5. Impute the missing value. \n6. For each\
    \ missing value, repeat steps 3 through 5. \n7. Stop \n \n \n \n \n \n \n \n \n\
    \ \n7 \n \nTable 1: Details of Datasets \nName of Station Total Period of Data\
    \ Training Period Testing Period \nAmbikapur \n1/1/96-31/12/17 \n(22 Years) \n\
    1996-2013 \n(18 Years) \n2014-2017 \n(04 Years) \nJagdalpur \n1/1/93-31/12/17\
    \ \n(25 Years) \n1993-2012 \n(20 Years) \n2013-2017 \n(05 Years) \nRaipur \n1/1/81-31/12/17\
    \ \n(37 Years) \n1981-2010 \n(30 Years) \n2011-2017 \n(07 Years) \n8 \n \n \n\
    Figure 1: Study Locations. \nTables 2 through 4 provide the statistics of minimum\
    \ temperature, maximum temperature, \nsunlight, wind speed, relative humidity\
    \ 1 and 2 for each of the three locations. Raipur is \ngeographically significant\
    \ distinct then both Jagdalpur and Ambikapur. Raipur is at the \ngeographic center\
    \ of the vast plain, Ambikapur is at an elevation of about 2043.96 feet (623 \n\
    meters), and Jagdalpur is located in an area with an elevation of about 1,820\
    \ feet (555 meters) \n9 \n \nand is located in the southern section of the Indravati\
    \ River. Ambikapur has an average of \n69.4 wet days per year[18], whereas Jagdalpur\
    \ has an average of 76.3 rainy days per year \n[19] and Raipur has an average\
    \ of 56.8 rainy days per year [20], all with an average rainfall \nof 1193.3 millimeters.\
    \ The relative humidity in Raipur is 49, while it is 58 in Jagdalpur and \n54\
    \ in Ambikapur. Standard deviations (STD) vary from a high of 6.4 to a low of\
    \ 4.2 across \nall three locations. \U0001D435\U0001D460\U0001D460, \U0001D44A\
    \U0001D446and \U0001D438\U0001D443, STD values are substantially lower in Jagdalpur\
    \ and \nAmbikapur, while they are much higher in Raipur. The STD values for \U0001D445\
    \U0001D43B1 and \U0001D445\U0001D43B2 range \nfrom 11.60 to 24.039, depending\
    \ on location. Raipur has a greater mean pan evaporation rate \n(5.488 mm/day)\
    \ than Jagdalpur (4.233 mm/day) and Ambikapur (4.719 mm/day). Raipur has \nthe\
    \ largest pan evaporation during the peak, reaching 21 mm per day, whereas Jagdalpur\
    \ and \nAmbikapur only have pan evaporation of roughly 15 mm per day. \nTable\
    \ 2: Statistics of daily climatic variables of Ambikapur \nAmbikapur \n  \n\U0001D474\
    \U0001D482\U0001D499\U0001D47B \n\U0001D474\U0001D48A\U0001D48F\U0001D47B \n\U0001D479\
    \U0001D46F\U0001D7CF \n\U0001D479\U0001D46F\U0001D7D0 \n\U0001D47E\U0001D47A \n\
    \U0001D469\U0001D47A\U0001D47A \n\U0001D46C\U0001D477 \nmean 30.31068 17.82836\
    \ 46.39663 3.601618 3.60606 7.02905 4.71926 \nstd \n5.343748 \n6.47393 \n23.88168\
    \ 12.20784 2.28358 3.30709 \n2.6076 \nmin \n15.6 \n0.4 \n5 \n0 \n0 \n0 \n0 \n\
    max \n45.6 \n31.9 \n100 \n216.8 \n17.1 \n13.6 \n16 \n \n \n \n10 \n \nTable 3:\
    \ Statistics of daily climatic variables of Jagdalpur  \nJagdalpur \n \n\U0001D474\
    \U0001D482\U0001D499\U0001D47B \n\U0001D474\U0001D48A\U0001D48F\U0001D47B \n\U0001D479\
    \U0001D46F\U0001D7CF \n\U0001D479\U0001D46F\U0001D7D0 \n\U0001D47E\U0001D47A \n\
    \U0001D469\U0001D47A\U0001D47A \n\U0001D46C\U0001D477 \nmean \n30.9361 \n18.30571\
    \ 86.32329 49.48166 4.77159 \n6.3639 \n4.23327 \nstd \n4.279691 5.836634 11.60308\
    \ 23.08981 2.72566 3.42607 2.35232 \nmin \n18 \n1 \n21 \n3 \n0.1 \n0 \n0 \nmax\
    \ \n44.4 \n32.5 \n100 \n100 \n22.3 \n12 \n15.2 \n \nTable 4: Statistics of daily\
    \ climatic variables of Raipur \nRaipur \n \n\U0001D474\U0001D482\U0001D499\U0001D47B\
    \ \n\U0001D474\U0001D48A\U0001D48F\U0001D47B \n\U0001D479\U0001D46F\U0001D7CF\
    \ \n\U0001D479\U0001D46F\U0001D7D0 \n\U0001D47E\U0001D47A \n\U0001D469\U0001D47A\
    \U0001D47A \n\U0001D46C\U0001D477 \nmean \n32.72019 19.87247 79.32448 44.03387\
    \ 5.43246 14.07719 5.48814 \nstd \n5.258151 6.016144 18.81431 24.03913 3.86046\
    \ 6.576404 3.52726 \nmin \n15.4 \n3.6 \n9 \n3 \n0 \n0 \n0.1 \nmax \n47.6 \n34.1\
    \ \n100 \n100 \n31.4 \n29.9 \n21.5 \n \n11 \n \nThe correlation matrix is calculated\
    \ to establish the degree of association between the \nfeatures of the dataset\
    \ and shown in Figures 2-4. In the Ambikapur station, \U0001D440\U0001D44E\U0001D465\
    \U0001D447,\U0001D440\U0001D456\U0001D45B\U0001D447,\U0001D44A\U0001D446, \nand\
    \ \U0001D435\U0001D446\U0001D446are positively linked with the desired value \U0001D438\
    \U0001D443, but \U0001D440\U0001D44E\U0001D465\U0001D447has the most influence\
    \ \nwith a correlation value of 0.76. \U0001D440\U0001D456\U0001D45B\U0001D447\
    has a correlation value of 0.45, \U0001D44A\U0001D446has a correlation \nvalue\
    \ of 0.37, and \U0001D435\U0001D446\U0001D446has a correlation value of 0.3. Both\
    \ \U0001D445\U0001D43B1 and \U0001D445\U0001D43B2had negative \ncorrelations with\
    \ the \U0001D438\U0001D443, with values of -0.48 and -0.18, respectively. \nFigure\
    \ 2: Correlation matrix of Ambikapur dataset. \n\U0001D440\U0001D44E\U0001D465\
    \U0001D447,\U0001D440\U0001D456\U0001D45B\U0001D447is positively correlated with\
    \ \U0001D438\U0001D443, as shown by the Jagdalpur matrices. When \ncomparing \U0001D440\
    \U0001D44E\U0001D465\U0001D447and \U0001D438\U0001D443, the correlation is 0.8477,\
    \ while when comparing \U0001D440\U0001D456\U0001D45B\U0001D447and\U0001D438\U0001D443\
    , it is \nonly 0.3123. Humidity is one of the most essential characteristics,\
    \ as evidenced by correlation \nmatrix values of -0.744132 and -0.538410 for RH1\
    \ and RH2, respectively. A negative \nrelationship exists between humidity and\
    \ \U0001D438\U0001D443. In this station, the effect of \U0001D435\U0001D446\U0001D446\
    is higher than \nAmbikapur, with a value of 0.4350. But \U0001D44A\U0001D446is\
    \ less influential than Ambikapur, with an \nimpact of 0.2424. \n12 \n \n \nFigure\
    \ 3: Correlation matrix of Jagdalpur dataset. \nThe Raipur data set demonstrates\
    \ a strong positive correlation between \U0001D438\U0001D443 and the \n\U0001D440\
    \U0001D44E\U0001D465\U0001D447attribute (0.901539). There is a positive correlation\
    \ between the \U0001D438\U0001D443 and \U0001D440\U0001D456\U0001D45B\U0001D447\
    \ \n(0.488252). The \U0001D445\U0001D43B1 has a stronger negative correlation\
    \ with \U0001D438\U0001D443(-0.855531) than \U0001D445\U0001D43B2(-\n0.529035).\
    \  \nFigure 4: Correlation matrix of Raipur dataset. \n13 \n \nThere is a negative\
    \ relationship between \U0001D435\U0001D446\U0001D446and the \U0001D438\U0001D443\
    (-0.252655), while the effect of \n\U0001D44A\U0001D446 is small and positive\
    \ (0.361943). At the Raipur station, the \U0001D435\U0001D446\U0001D446variable\
    \ is the only one \nwith a negative correlation, at all other stations, correlations\
    \ are positive. When considering \n\U0001D438\U0001D443, temperature is a major\
    \ factor. Evaporation rates would likely decrease if there was less \nsunlight.\
    \ It's possible that elevated levels of humidity would slow the rate at which\
    \ water \nevaporates. An increase in wind speed often triggers evaporation in\
    \ hot, dry conditions. \nIncreasing humidity has a dampening effect on pan evaporation.\
    \ Evaporation rates are \nprimarily affected by the wind speed and relative humidity.\
    \ Even in the silted area, the \namount of evaporation is influenced by wind speed\
    \ and relative humidity. \n \n2.3 Methods \nThe pan evaporation estimation is\
    \ formulated an error optimization problem. The complete \nmethodology for it\
    \ is displayed in Figure 5. The schematic diagram of \U0001D438\U0001D443 prediction\
    \ model is \nshown in Figure 6. The inputs to the model are \U0001D440\U0001D44E\
    \U0001D465\U0001D461, \U0001D440\U0001D456\U0001D45B\U0001D447, \U0001D435\U0001D446\
    \U0001D446, \U0001D44A\U0001D446, \U0001D445\U0001D43B1, \U0001D445\U0001D43B\
    2. The \ncorresponding \U0001D438\U0001D443 value is the target value for the\
    \ model during training. The details of the \ntime period of the dataset is given\
    \ in Table 1. After collection of data, it is checked for any \nmissing value.\
    \ Only in case of Ambikapur few missing values are there, which is filled by \n\
    using KNN imputation method. Then normalization of the data is done using min-max\
    \ \nnormalization. The model receives the first input pattern and calculates the\
    \ predicted output \nusing its own process. The error is calculated by comparing\
    \ the predicted output with the \ncorresponding target value. The update rules\
    \ of the particular approach are then implemented \nto change the model's parameters\
    \ using the error value. This procedure will be continued until \nall 80% of the\
    \ inputs have been exhausted. The first experiment is now completed. The \nexperiment\
    \ is repeated until the value of the Root mean squared error (RMSE) is as low\
    \ as \n14 \n \npossible. Once the error is minimized, the model's parameters are\
    \ frozen and utilized for \ntesting the model with the testing set. The error\
    \ value for each trial is saved and shown to \ndemonstrate the model's convergence\
    \ characteristics. During model testing, the testing \npatterns are given to the\
    \ model and different performance measure values are computed to \nassess the\
    \ model's performance.  \nPython is used to simulate the models using the modules\
    \ Pandas, Scikit-Learn, NumPy, \nMatplotlib and etc. \n \n15 \n \nStart\nData\
    \ \ncollection\nNormalization\nTrain - Test Split \nof data\nTraining of \nModel\
    \ \nTesting and \nPerformance \nEvaluation\nFinal Trained \nModel\nEnd\nIs error\
    \ \nreduced\nNO\nYES\nFinal \nOutput\n \nFigure 5: Methodology for prediction\
    \ of pan evaporation. \n16 \n \nmaxt\nmint\nrh1\nrh2\nws\nbss\nep\nMin Max \n\
    Normalization\nTrain Set\n Test Set\nML/DL Models\nƩ \nEstimated \nOutput \n(-)\n\
    (+)\nTarget \nOutput \nUpdate Algorithm\nFinal Output\nI  N  P  U  T  S\n \nFigure\
    \ 6: Schematic diagram of \U0001D438\U0001D443 Prediction Model. \nThe Below is\
    \ a quick summary of the different models that are employed in this study: \n\
    2.3.1 Support Vector Regression: \nThe Support Vector Machine (SVM) [18] is utilized\
    \ for classification and regression \nassignments. The goal is to locate a hyperplane\
    \ in an N-dimensional space. In an \nSVM, the maximal marginal distance is used\
    \ to select hyperplanes. Support vectors \nare the coordinates that pass through\
    \ the adjacent positive and negative point. The \nclassification problem is divisible\
    \ into linear separable problems and nonlinear \nseparable problems. By drawing\
    \ a single line in vector space, it can categorize \nproblems that are linearly\
    \ separable. In order to accomplish classification in a \nnonlinear separable\
    \ problem, the low dimension must be converted into the high \ndimension. SVM\
    \ are also useful for fixing regression problems; this method is known \nas support\
    \ vector regression (SVR)[19]. SVR is an approach to supervised learning \nfor\
    \ forecasting discrete values. SVM and SVR share the same basic principle. SVR\
    \ is \nmore difficult to model than quadratic regression. In classification and\
    \ regression, the \nfunction of the kernel is crucial. Kernel is a method for\
    \ analyzing a dataset's patterns. \nThey are extremely beneficial for solving\
    \ nonlinear problems. Using the kernel \n17 \n \nfunction, input data can be transformed\
    \ into the format required for processing. The \n\"kernel\" is a collection of\
    \ mathematical operations used in SVM as a window through \nwhich data may be\
    \ modified. The Kernel function modifies the training data so that a \nnon-linear\
    \ decision surface can be converted into a linear equation in spaces with \nadditional\
    \ dimensions. This paper uses the linear, rbf and polynomial kernels. \n2.3.2\
    \ AdaBoost \nAdaBoost [20] is a common strategy for boosting. The primary purpose\
    \ of the \nboosting technique is to transform all weak learners into strong learners.\
    \ Boosting \ntechniques are predicated on the concept of first constructing a\
    \ model on the training \ndataset, and then constructing a second model to fix\
    \ the flaws of the first model. This \nprocess is repeated until the errors are\
    \ minimum and the predicted class or value is \nprecise. Boosting is a technique\
    \ for minimizing bias error, which occurs when models \nfail to identify relevant\
    \ data trends. This is determined by comparing the actual and \nexpected values.\
    \ Similar to the AdaBoost classifier, the AdaBoost regressor processes \ndata.\
    \ AdaBoost is a meta-estimator that begins by fitting a regressor to the original\
    \ \ndataset, and then repeatedly fits the same regressor to the same dataset with\
    \ different \nweights of instances depending on the error of the current prediction.\
    \ In this study the \nbase estimator is Decision Tree Regressor initialized with\
    \ max_depth value three and \nloss function usedis linear. \n2.3.3 XGBoost \n\
    Extreme Gradient Boosting is abbreviated as XGBoost [21]. XGBoost, a gradient-\n\
    boosted decision tree implementation, is created in order to maximize efficiency.\
    \ The \nGradient Boosting framework is used to implement machine learning algorithms.\
    \ The \nparallel tree boosting of XGBoost enables the efficient and exact resolution\
    \ of a broad \nrange of data science tasks. The XGBoost Regressor operates by\
    \ training a series of \n18 \n \ndecision trees iteratively, with each tree being\
    \ trained to remedy the defects of the \npreceding tree. The algorithm optimizes\
    \ the loss function, which measures the \ndifference between the predicted and\
    \ actual values, using gradient boosting. The \n\"squared error\" loss function\
    \ is employed in the current study, and the warm_start \nparameter is set to false.\
    \ \n2.3.4 Artificial Neural Network \nArtificial Neural Networks (ANN) [22] are\
    \ computer networks that mimic the \nbehavior of biological neural networks. The\
    \ mathematical representation, that is based \nprimarily on the biological neuron\
    \ system in the human brain, is known as an artificial \nneural network. Deep\
    \ neural networks (DNN) are a kind of artificial neural networks \n(ANN) as shown\
    \ in the Figure 7. The term \"deep\" refers to the fact that it often \ncontains\
    \ two or more than two hidden layers, allowing for a more complicated \nrepresentation\
    \ and feature extraction from the input data. A DNN neuron receives \ninput from\
    \ the previous layer and generates output using an activation function. The \n\
    outputs of one layer are fed into the next layer, and so on until the final layer\
    \ produces \nthe desired output. The intermediate layers, also known as hidden\
    \ layers, allow the \nnetwork to learn complex representations of the input data\
    \ via a hierarchical \ntechnique. In this study a DNN is trained in which input\
    \ layer has 6 nodes for \n\U0001D440\U0001D456\U0001D45B\U0001D447, \U0001D440\
    \U0001D44E\U0001D465\U0001D447, \U0001D445\U0001D43B1, \U0001D445\U0001D43B2,\
    \ \U0001D435\U0001D446\U0001D446, \U0001D44A\U0001D446. For the different stations\
    \ DNN has trained using \nthe different number of hidden layers and each layers\
    \ have different number of \nneurons. The details of the number of hidden node\
    \ and neurons is mentioned in Table \n10 to 12. However, the “Relu” activation\
    \ function is used by all the neuron in every \nlayers. Output layer has only\
    \ one node with linear activation function for the target \nvariable \U0001D438\
    \U0001D443. \nANN calculates the output using the following equation: \n19 \n\
    \ \n\U0001D445 = \U0001D44F + \U0001D4641\U0001D44E1 + \U0001D4642\U0001D44E2\
    \ + ⋯ \U0001D464\U0001D45B\U0001D44E\U0001D45B                   ………….(1) \nwhere,\
    \ \n \n\U0001D44F = Bias \n \n\U0001D464 = Weights \n \n\U0001D44E = Input \n\
    \ \n\U0001D445 = Output \nMint\nMaxt\nRH1\nRH2\nBSS\nWS\nInput \nLayer \nHidden\
    \ Layer\nOutput \nLayer\n \nFigure 7: Structure of Deep Neural Network.   \n2.3.5\
    \ Recurrent Neural Network \nA recurrent neural network (RNN)[23]is a form of\
    \ artificial neural network that is designed to \noperate with time series data\
    \ or data that contains sequences. The structure of RNN is given in \nFigure 8.\
    \ A deep feedforward model may need specific parameters for each member of the\
    \ \nsequence. It may also be limited in its ability to generalize to variable-length\
    \ sequences. The \nHidden state, which retains particular information about a\
    \ sequence, is the most significant \ncomponent of RNN. RNNs feature a memory\
    \ that retains all computation information. A \nRNN is trained in this research\
    \ with a 6-node input layer that takes variables such \n20 \n \nas \U0001D440\U0001D456\
    \U0001D45B\U0001D447, \U0001D440\U0001D44E\U0001D465\U0001D447, \U0001D435\U0001D446\
    \U0001D446, \U0001D44A\U0001D446, \U0001D445\U0001D43B1 and \U0001D445\U0001D43B\
    2. The RNN is trained for each of the three stations \nwith a unique combination\
    \ of hidden layer depth and neuron count. Tables 10 to 12 contain \nmore details\
    \ of the number of hidden nodes and neurons. \nFormula for calculating the output\
    \ is as follows: \n\U0001D445\U0001D461 = \U0001D44Aℎ\U0001D456ℎ\U0001D461 \n\
    \ \n \n…... (2) \nwhere \n\U0001D445\U0001D461 is Output \n\U0001D44Aℎ\U0001D456\
    weight at output layer. \nℎ\U0001D461 hidden state \n \nMinT\nMaxT\nRH1\nRH2\n\
    BSS\nWS\nEP\nInput \nLayer \nOutput \nLayer\nHidden \nlayers\n \nFigure 8: Structure\
    \ of Recurrent Neural Network. \n \n21 \n \n2.3.6 Long Short-Term Memory \nLong\
    \ Short-Term Memory (LSTM) [24] is a RNN architecture developed to overcome the\
    \ \nvanishing gradient issue in regular RNNs. The structure of a LSTM neuron is\
    \ exhibited in \nFigure 9. LSTM networks may learn long-term relationships in\
    \ sequential data and are \nextensively used for natural language processing,\
    \ speech recognition, and time-series \nprediction. The addition of a memory cell\
    \ in the network enables it to selectively forget or \nretain information over\
    \ time, making it well-suited for simulating sequential data with long-\nterm\
    \ dependencies. Multiple LSTM layers are piled on top of each other in a deep\
    \ LSTM \nnetwork, allowing the network to learn more abstract and complicated\
    \ representations of \nsequential input. Each layer of the network is made up\
    \ of LSTM cells, which analyze the \ninput sequence and keep an internal memory\
    \ state. In this study, a LSTM is trained with a 6-\nnode in input layer that\
    \ accepts \U0001D440\U0001D456\U0001D45B\U0001D447, \U0001D440\U0001D44E\U0001D465\
    \U0001D447, \U0001D445\U0001D43B1, \U0001D445\U0001D43B2, \U0001D435\U0001D446\
    \U0001D446, and \U0001D44A\U0001D446 as inputs. For each of \nthe three stations,\
    \ the LSTM is trained with a different hidden layer level and number of \nneurons\
    \ in them. Details regarding the number of hidden nodes and neurons are provided\
    \ in \nTables 10 to 12. \nFigure 9: Structure of a Long Short Term Memory Neuron.\
    \ \nThe function of an LSTM neuron architecture is depicted by the following equations:\
    \ \nEquation of Forget gate,  \n \n \n\U0001D453\U0001D45C\U0001D454 = \U0001D70E\
    \U0001D44E(\U0001D44A\U0001D453\U0001D45C\U0001D454 × \U0001D465\U0001D461 + \U0001D448\
    \U0001D453\U0001D45C\U0001D454 × ℎ(\U0001D461 − 1) + \U0001D44F\U0001D453\U0001D45C\
    \U0001D454) \n...... (3) \nEquation of Input gate, \nTanh  \n(h) \nfog \nh(t-1)\
    \ \nC(t) \nh(t) \n() \n() \nTanh  \n(g) \n() \nC(t-1) \ning \noug \ninput \n\
    output \n22 \n \n \n \n\U0001D456\U0001D45B\U0001D454 = \U0001D70E\U0001D44E(\U0001D44A\
    \U0001D456\U0001D45B\U0001D454 × \U0001D465\U0001D461 + \U0001D448\U0001D456\U0001D45B\
    \U0001D454 × ℎ(\U0001D461 − 1) + \U0001D44F\U0001D456\U0001D45B\U0001D454) ......\
    \ (4) \nEquation of Output gate, \n \n \n\U0001D45C\U0001D462\U0001D454 = \U0001D70E\
    \U0001D44E(\U0001D44A\U0001D45C\U0001D462\U0001D454 × \U0001D465\U0001D461 + \U0001D448\
    \U0001D45C\U0001D462\U0001D454 × ℎ(\U0001D461 − 1) + \U0001D44F\U0001D45C\U0001D462\
    \U0001D454) \n...... (5) \n \n \n\U0001D436′(\U0001D461) = \U0001D70Eℎ(\U0001D44A\
    \U0001D450\U0001D461 × \U0001D465\U0001D461 + \U0001D448\U0001D450\U0001D461 ×\
    \ ℎ(\U0001D461 − 1) + \U0001D44F\U0001D450\U0001D461) ...... (6) \nEquation of\
    \ Cell state, \n \n \n\U0001D436(\U0001D461) = \U0001D453\U0001D45C\U0001D454\
    . \U0001D436(\U0001D461 − 1) + \U0001D456\U0001D45C\U0001D454. \U0001D436′(\U0001D461\
    ) \n \n \n...... (7) \nEquation of Hidden state, \n \n \nℎ(\U0001D461) = \U0001D45C\
    \U0001D462\U0001D454. \U0001D70Eℎ(\U0001D436(\U0001D461))  \n \n \n \n...... (8)\
    \ \nwhere,  \n\U0001D453\U0001D45C\U0001D454 = forgate gate,  \n\U0001D456\U0001D45B\
    \U0001D454 = input gate,  \n\U0001D45C\U0001D462\U0001D454 = output gate,  \n\U0001D436\
    ′(\U0001D461) = used to generate \U0001D436(\U0001D461) and ℎ(\U0001D461) \n\U0001D436\
    (\U0001D461) = cell state,  \nℎ(\U0001D461) = hidden state,  \n\U0001D44A\U0001D453\
    \U0001D45C\U0001D454, \U0001D44A\U0001D456\U0001D45B\U0001D454, \U0001D44A\U0001D45C\
    \U0001D462\U0001D454, \U0001D448\U0001D453\U0001D45C\U0001D454, \U0001D448\U0001D456\
    \U0001D45B\U0001D454, \U0001D448\U0001D45C\U0001D462\U0001D454 = weight matrices\
    \ \n\U0001D44F\U0001D453\U0001D45C\U0001D454, \U0001D44F\U0001D456\U0001D45B\U0001D454\
    , \U0001D44F\U0001D45C\U0001D462\U0001D454, \U0001D44F\U0001D450\U0001D461 = bias\
    \ \n\U0001D465\U0001D461 = input \n2.3.7 Bi-Directional LSTM \nBidirectional LSTM\
    \ (Bi-LSTM) [25] is a kind of LSTM that improves model performance on \nsequence-based\
    \ challenges. Bi-LSTM is the technique of allowing any neural network to store\
    \ \nsequence information in both directions, forward and backward (future to past\
    \ and past to \nfuture). Bi - LSTMs train two instead of one LSTM on the input\
    \ sequence in cases where all \n23 \n \ntime steps of the input sequences are\
    \ known. The first on the original input sequence and the \nsecond on a reversed\
    \ replica of the original input sequence. This may offer more context to \nthe\
    \ network, resulting in quicker and more complete issue learning. In a deep Bi-LSTM\
    \ \narchitecture, many layers of Bi-LSTM networks are stacked on top of each other\
    \ as shown in \nFigure \n10. \nIn \nthis \nstudy, \na \n6-node \ninput \nlayer\
    \ \nBi-LSTM \nis \ntrained \nwith \n\U0001D440\U0001D456\U0001D45B\U0001D447,\
    \ \U0001D440\U0001D44E\U0001D465\U0001D447, \U0001D445\U0001D43B1, \U0001D445\U0001D43B\
    2, \U0001D435\U0001D446\U0001D446, and \U0001D44A\U0001D446 as inputs. Bi-LSTM\
    \ is trained with a distinct hidden \nlayer depth and neuron. The output node\
    \ has a single node for the \U0001D438\U0001D443 value. Tables 10 to 12 \nprovide\
    \ information regarding the number of hidden nodes and neurons. \n \n \n \n \n\
    \ \n \n \nFigure 10. Structure of Bi-LSTM \nThe following key equations are used\
    \ in Bi-LSTM : \nForward hidden layer, \nℎ\U0001D453 = tanh (\U0001D44Aℎ\U0001D453\
    \U0001D44E\U0001D461 + \U0001D44Aℎ\U0001D453ℎ\U0001D453 + \U0001D44F\U0001D453\
    ) \n \n...... (9) \nBackward hidden layer, \nℎ\U0001D44F = tanh (\U0001D44Aℎ\U0001D44F\
    \U0001D44E\U0001D461 + \U0001D44Aℎ\U0001D44Fℎ\U0001D44F + \U0001D44F\U0001D44F\
    ) \n \n...... (10) \nOutput, \n\U0001D442\U0001D456 = \U0001D44Aℎ\U0001D453. ℎ\U0001D453\
    \ + \U0001D44Aℎ\U0001D44Fℎ\U0001D44F + \U0001D44F\U0001D466 \n \n \n...... (11)\
    \ \nLSTM \nLSTM \nLSTM \nLSTM \nLSTM \nLSTM \nBackward Layer \nForward Layer \n\
    z1 \nz2 \nzn \nOutput Layer \na1 \nInput Layer \na2 \nan \n24 \n \nWhere,  \n\
    \ \nℎ\U0001D453 = hidden layer (forward) \n \nℎ\U0001D44F = hidden layer (backward)\
    \ \n \n\U0001D442\U0001D456 = output of both the hidden layer ℎ\U0001D453 and\
    \ ℎ\U0001D44F \n2.3.8 \nEmpirical methods for pan evaporation estimation: \n2.3.8.1\
    \ Linacre method: \nThe Linacre [26] describes how to use temperature information\
    \ to determine pan evaporation. \nHere, evaporation may be calculated more easily\
    \ than using the Penman Formula. The \nproposed equation by Linacre is as follow.\
    \ \n\U0001D438\U0001D45D =\n700\U0001D447\U0001D45A\n(100−\U0001D437)+15(\U0001D447\
    −\U0001D447\U0001D451)\n(80−\U0001D447)\n \n \n \n..…(12) \nwhere \n(\U0001D447\
    \ − \U0001D447\U0001D451) = 0.0023\U0001D462 + 0.37\U0001D447 + 0.53\U0001D445\
    \ + 0.35 \U0001D445\U0001D44E\U0001D45B\U0001D45B − 10.9 \n\U0001D447\U0001D45A\
    \ = \U0001D447 + 0.006ℎ,  \n\U0001D447 = (\U0001D447_\U0001D45A\U0001D44E\U0001D465\
    \ + \U0001D447_\U0001D45A\U0001D456\U0001D45B)/2 =  \U0001D45A\U0001D452\U0001D44E\
    \U0001D45B \U0001D461\U0001D452\U0001D45A\U0001D45D\U0001D452\U0001D45F\U0001D44E\
    \U0001D461\U0001D462\U0001D45F\U0001D452 °\U0001D436, \n\U0001D462 =  ℎ\U0001D452\
    \U0001D456\U0001D454ℎ\U0001D461 \U0001D456\U0001D45B \U0001D45A\U0001D452\U0001D461\
    \U0001D452\U0001D45F, \n\U0001D437 =  \U0001D45A\U0001D452\U0001D44E\U0001D460\
    \U0001D462\U0001D45F\U0001D456\U0001D45B\U0001D454 \U0001D451\U0001D452\U0001D454\
    \U0001D45F\U0001D452\U0001D452\U0001D460 \U0001D45C\U0001D453 \U0001D459\U0001D44E\
    \U0001D461\U0001D456\U0001D461\U0001D462\U0001D451\U0001D452, \n\U0001D445 = \
    \ \U0001D451\U0001D44E\U0001D456\U0001D459\U0001D466 \U0001D45A\U0001D452\U0001D44E\
    \U0001D45B \U0001D461\U0001D452\U0001D45A\U0001D45D\U0001D452\U0001D45F\U0001D44E\
    \U0001D461\U0001D462\U0001D45F\U0001D452 \U0001D463\U0001D44E\U0001D45F\U0001D456\
    \U0001D44E\U0001D461\U0001D456\U0001D45C\U0001D45B \U0001D456\U0001D45B °\U0001D436\
    \ \n\U0001D445\U0001D44E\U0001D45B\U0001D45B =  \U0001D45A\U0001D452\U0001D44E\
    \U0001D45B \U0001D461\U0001D452\U0001D45A\U0001D45D\U0001D452\U0001D45F\U0001D44E\
    \U0001D461\U0001D462\U0001D45F\U0001D452 \U0001D451\U0001D456\U0001D453\U0001D453\
    \U0001D452\U0001D45F\U0001D452\U0001D45B\U0001D450\U0001D452 \U0001D44F\U0001D452\
    \U0001D461\U0001D464\U0001D452\U0001D452\U0001D45B ℎ\U0001D45C\U0001D461\U0001D461\
    \U0001D452\U0001D460\U0001D461 \U0001D44E\U0001D45B\U0001D451 \U0001D450\U0001D45C\
    \U0001D459\U0001D451\U0001D452\U0001D460\U0001D461 \U0001D45A\U0001D45C\U0001D45B\
    \U0001D461ℎ\U0001D460 \U0001D456\U0001D45B °\U0001D436. \n2.3.8.2 Christiansen\
    \ method: \nChristiansen [26] has developed the equation utilizing the multiple\
    \ correlation method in order \nto determine Pan-Evaporation. As coefficients\
    \ in this equation, temperature, humidity, wind \nspeed, and sunshine data are\
    \ taken into consideration. The equation is as follow- \n\U0001D438\U0001D45D\
    \ = 0.473\U0001D445\U0001D44E\U0001D436\U0001D447\U0001D436\U0001D464\U0001D436\
    \U0001D43B\U0001D436\U0001D460\U0001D436\U0001D438\U0001D436\U0001D45A  \n \n\
    …..(13) \n25 \n \nwhere,  \n\U0001D445\U0001D44E= extra-terrestrial radiation\
    \ (\U0001D45A\U0001D45A\U0001D451−1 ) \n\U0001D436\U0001D447 =  0.393 +  0.5592\
    \ \U0001D447 20 +  0.04756 (\n\U0001D447\n20) + 0.04756 (\n\U0001D447\n20)\n2\n\
    , \n\U0001D447 \U0001D456\U0001D460 \U0001D45A\U0001D452\U0001D44E\U0001D45B \U0001D44E\
    \U0001D456\U0001D45F \U0001D461\U0001D452\U0001D45A\U0001D45D\U0001D452\U0001D45F\
    \U0001D44E\U0001D461\U0001D462\U0001D45F\U0001D452  °\U0001D436 \n\U0001D436\U0001D44A\
    \ =  0.708 +  0.3276 (\n\U0001D464\n96⋅6) − 0.036 (\n\U0001D464\n20)\n2\n, \U0001D44A\
    \ \U0001D456\U0001D460 \U0001D45A\U0001D452\U0001D44E\U0001D45B \U0001D451\U0001D44E\
    \U0001D456\U0001D459\U0001D466 \U0001D464\U0001D456\U0001D45B\U0001D451 \U0001D460\
    \U0001D45D\U0001D452\U0001D452\U0001D451 (\U0001D458\U0001D45A\U0001D451 − 1)\
    \ \n\U0001D436\U0001D43B =  1.250 −  0.212 (\nℎ\n57.4) −  0.038 (\n\U0001D43B\n\
    57⋅4)\n2\n, \U0001D43B \U0001D456\U0001D460 \U0001D45A\U0001D452\U0001D44E\U0001D45B\
    \ \U0001D451\U0001D44E\U0001D456\U0001D459\U0001D466 \U0001D45F\U0001D452\U0001D459\
    \U0001D44E\U0001D461\U0001D456\U0001D463\U0001D452 ℎ\U0001D462\U0001D45A\U0001D456\
    \U0001D451\U0001D456\U0001D461\U0001D456\U0001D461\U0001D466 (%) \n\U0001D436\U0001D446\
    \ =  0.542 +  0.64 (\n\U0001D460\n80) −  0.4992 (\n\U0001D460\n80)\n2\n+  0.3174\
    \ (\n\U0001D460\n80)\n3\n, \U0001D446 \U0001D456\U0001D460 \U0001D460\U0001D462\
    \U0001D45B\U0001D460h\U0001D456\U0001D45B\U0001D452 (%)  \n\U0001D436\U0001D438\
    \ =  0.970 + 0.030 ( \U0001D438\n305), \U0001D438 \U0001D456\U0001D460 \U0001D452\
    \U0001D459\U0001D452\U0001D463\U0001D44E\U0001D461\U0001D456\U0001D45C\U0001D45B\
    \ (\U0001D45A) \n\U0001D447 = (\U0001D447\U0001D45A\U0001D44E\U0001D465 + \U0001D447\
    \U0001D45A\U0001D456\U0001D45B)\n2\n\U0001D44E\U0001D45B\U0001D451 \U0001D43B\
    \ = (\U0001D445\U0001D43B\U0001D43C +  \U0001D445\U0001D43B\U0001D43C\U0001D43C\
    )\n2\n \n2.4 Performance Metrics \nUsing the following two efficiency metrics,\
    \ the results of the various models are evaluated. \n2.4.1 \nRoot Mean Square\
    \ Error  \nRoot Mean Square Error (RMSE) [27] is a statistical measure that calculates\
    \ the average \nmagnitude of the discrepancies in a dataset between predicted\
    \ and actual values. The RMSE \nvalue represents the standard deviation of the\
    \ residuals, which are the differences between \nexpected and actual values. It\
    \ measures the distribution of prediction errors to determine the \noverall accuracy\
    \ of a predictive model. Lower RMSE values indicate better predictive \nperformance,\
    \ meaning that the model's predictions are more accurate on average. In contrast,\
    \ \nhigher RMSE values indicate more prediction errors and fewer correct predictions.\
    \ \n\U0001D445\U0001D440\U0001D446\U0001D438 = √1\n\U0001D45B ∑\n(\U0001D466\U0001D456\
    \ − \U0001D466̂\U0001D456)2\n\U0001D45B\n\U0001D456=1\n \n \n \n………….(14) \nwhere,\
    \  \n26 \n \n\U0001D466\U0001D456: Real value corresponding to the ith observation\
    \ \n\U0001D466̂\U0001D456: The ith observation's estimated value \n\U0001D45B\
    : Number of testing patterns \n2.4.2 \nR-Square \nR-Square, commonly known as\
    \ the coefficient of determination, is a statistical measure used \nto assess\
    \ the model's fit [28]. It determines how much of the variation in the dependent\
    \ \nvariable can be explained by the model's independent variables. R-Squared\
    \ measures how \nwell the regression model matches the observed data. The R-Squared\
    \ value is between 0 and \n1. A value of 0 indicates that the independent factors\
    \ account for no variability in the \ndependent variable, while a value of 1 show\
    \ that the independent variables account for all \nvariability in the dependent\
    \ variable. \n\U0001D4452 =\n(∑\n(\U0001D466\U0001D456−\U0001D466\U0001D456̅ )(\U0001D466\
    \ \U0001D456−\U0001D466̂\U0001D456̅ ))\n\U0001D45B\n\U0001D456=1\n2\n∑\n(\U0001D466\
    \U0001D456−\U0001D466\U0001D456̅ )2\n\U0001D45B\n\U0001D456=1\n∑\n(\U0001D466\
    \ \U0001D456−\U0001D466̂\U0001D456\U0001D456\n̅̅̅̅)2\n\U0001D45B\n\U0001D456=1\n\
    \              ……………(15) \nwhere, \n\U0001D466\U0001D456: Real value corresponding\
    \ to the ith observation \n\U0001D466̂\U0001D456: The ith observation's estimated\
    \ value \n\U0001D45B: Number of testing patterns \n3. Simulation study \nTo estimate\
    \ the \U0001D438\U0001D443 rate, simulation is done using python following the\
    \ diagram displayed in \nFigure 6. The simulation utilizes the capabilities of\
    \ the libraries such as pandas, sklearn, tensorflow, \nmatplotlib, keras, and\
    \ seaborn. Simulation uses machine learning models like SVR, AdaBoost, and \n\
    XGBoost along with deep learning methods like DNN, RNN, LSTM, and Bi-LSTM. The\
    \ available \ndaily climate data are utilized for the simulation. Table 1 contains\
    \ information regarding the \nsimulation data. To establish the relationship between\
    \ the target and training variables, the correlation \n27 \n \ncoefficient between\
    \ the feature variables and the target variable is calculated.  Each dataset is\
    \ \nstandardized between 0 and 1 before being used for training and testing, and\
    \ it is then renormalized to \nits original unit for the final comparison of actual\
    \ and projected values. Almost 80% of the available \ndata are used to train the\
    \ suggested models, and the rest of the data are used to test the models. \nThe\
    \ training data is fed into the machine learning models to train it. Using the\
    \ six variables as \ninput, SVR seeks the ideal hyperplane that best fits the\
    \ data points while reducing error. This process \ninvolves solving a quadratic\
    \ programming problem to determine the support vectors and their \ncorresponding\
    \ weights. Using the test data, the trained SVR model's performance is evaluated.\
    \ After \nthat experiments are carried out with different hyperparameters such\
    \ as, kernel, epsilon, max_iter to \nset the best reults. Similarly, the Adaboost\
    \ and XGBoost models are trained using their respective \nmethodologies. \nTo\
    \ train the DNN model, the initial pattern is passed into the neural network,\
    \ and the final \noutput is acquired at the output node following the forward\
    \ pass and estimation procedure. The \nnumber of hidden layers in DNN varies from\
    \ 1 to 4. All of the training patterns are applied \nsequentially. The method\
    \ is repeated until all of the remaining input patterns are exhausted. The error\
    \ \nis obtained by comparing the outputs for each input pattern with the desired\
    \ output. Using the \nbackpropagation learning algorithm, the change in weights\
    \ along each path is established. For each \ninput, the change in weight of each\
    \ path of the model is saved and the average change in weight is \nthen computed.\
    \ The weights are then updated by adding the weight changes of each path to the\
    \ old \nvalue. This completes one iteration. The same process is repeated 1000\
    \ times. The RMSE and the R-\nSquare values are calculated as performance measures.\
    \ At the point when the MSE value is as low as \nit can go and the error plot\
    \ converged, training process is completed. After the training has been done,\
    \ \nthe weights and biases at each layer of the neural network are set to the\
    \ values as obtained at the final \niteration. Then test patterns are fed to the\
    \ model one after the other to see how well it predicts. For \neach test pattern,\
    \ the expected output is found and compared to the desired values. Similarly,\
    \ the \nRNN, LSTM, and Bi-LSTM models are also trained and evaluated using their\
    \ methodologies for all \nthree locations. \n28 \n \nTest performance based on\
    \ the evaluation criteria is given in Tables 5 to 8. Table 5 displays \nthe results\
    \ of deep learning models. Tables 6–8 illustrate the outcomes of machine learning\
    \ models. \nTable 5 contains the hyperparameter values for deep learning models,\
    \ and Table 9 shows the values \nfor machine learning models.  \n29 \n \nTable\
    \ 5: Hyperparameters used and Results of deep learning models \nStation \nModel\
    \ \nEpoc\nhs \nNum\nber of \nHidd\nen \nlayers \nBat\nch \nSize \nHidden Layer\
    \ 1 \nHidden Layer 2 \nHidden Layer 3 \nHidden Layer 4 \nRMSE \nR-\nSquare \n\
    Ra\nnk \nNeur\nons \nActivat\nion \nFunctio\nn \nDro\nup \nout \nNeur\nons \n\
    Activat\nion \nFunctio\nn \nDro\nup \nout \nNeur\nons \nActivat\nion \nFunctio\n\
    n \nDro\nup \nout \nNeu\nrons \nActiva\ntion \nFuncti\non \nDro\nup \nOut \nAmbikapur\
    \ \nBidirecti\nonal \nLSTM \n200 \n4 \n64 \n60 \nRelu \n0.02 \n45 \nRelu \n0.02\
    \ \n55 \nRelu \n0.02 \n50 \nRelu \n0.02 \n0.9911 \n0.7578 \n4 \nLSTM \n150 \n\
    4 \n64 \n32 \nRelu \n0.02 \n20 \nRelu \n0.02 \n25 \nRelu \n0.02 \n15 \nRelu \n\
    0.02 \n0.9894 \n0.7586 \n3 \nRNN \n1000 \n4 \n64 \n60 \nRelu \n0.02 \n45 \nRelu\
    \ \n0.02 \n55 \nRelu \n0.02 \n50 \nRelu \n0.02 \n0.7463 \n0.8627 \n1 \nDNN \n\
    200 \n4 \n  \n60 \nRelu \n  \n45 \nRelu \n  \n55 \nRelu \n  \n50 \nRelu \n  \n\
    0.9810 \n0.7627 \n2 \nJagdalpur \nBidirecti\nonal \nLSTM \n150 \n4 \n64 \n32 \n\
    Relu \n0.02 \n20 \nRelu \n0.02 \n25 \nRelu \n0.02 \n15 \nRelu \n0.02 \n0.9498\
    \ \n0.7796 \n3 \nLSTM \n200 \n4 \n64 \n60 \nRelu \n0.02 \n45 \nRelu \n0.02 \n\
    25 \nRelu \n0.02 \n15 \nRelu \n0.02 \n0.9606 \n0.7746 \n4 \nRNN \n700 \n4 \n64\
    \ \n60 \nRelu \n0.02 \n45 \nRelu \n0.02 \n55 \nRelu \n0.02 \n25 \nRelu \n0.02\
    \ \n0.7193 \n0.8736 \n1 \nDNN \n200 \n4 \n  \n60 \nRelu \n  \n45 \nRelu \n  \n\
    55 \nRelu \n  \n50 \nRelu \n  \n0.9321 \n0.7878 \n2 \nRaipur \nBidirecti\nonal\
    \ \nLSTM \n200 \n4 \n64 \n32 \nRelu \n0.02 \n20 \nRelu \n0.02 \n25 \nRelu \n0.02\
    \ \n15 \nRelu \n0.02 \n0.9671 \n0.9071 \n3 \nLSTM \n200 \n4 \n64 \n32 \nRelu \n\
    0.02 \n20 \nRelu \n0.02 \n25 \nRelu \n0.02 \n15 \nRelu \n0.02 \n0.9987 \n0.9009\
    \ \n4 \nRNN \n1000 \n4 \n64 \n60 \nRelu \n0.02 \n45 \nRelu \n0.02 \n55 \nRelu\
    \ \n0.02 \n50 \nRelu \n0.02 \n0.7072 \n0.9503 \n1 \nDNN \n200 \n4 \n64 \n60 \n\
    Relu \n  \n45 \nRelu \n  \n55 \nRelu \n  \n50 \nRelu \n  \n0.9531 \n0.9098 \n\
    2 \n \n \n \n30 \n \nTable 6: Results of Machine Learning models for Raipur station\
    \ \nModel \nKernel \nRMSE \nR-\nSquare \nRank \nSVR \nrbf \n0.985 \n0.859 \n4\
    \ \nPoly \n1.066 \n0.905 \n2 \nLinear \n1.204 \n0.879 \n3 \nAdaBoost \n  \n1.314\
    \ \n0.856 \n5 \nXGBoost \n  \n1.028 \n0.912 \n1 \n \nTable 7: Results of Machine\
    \ Learning models for Jagdalpur station \nModel \nKernel \nRMSE \nR-\nSquare \n\
    Rank \nSVR \nrbf \n1.033 \n0.7621 \n1 \nPoly \n1.0172 \n0.7471 \n3 \nLinear \n\
    1.0104 \n0.7504 \n2 \nAdaBoost \n  \n1.0832 \n0.7132 \n5 \nXGBoost \n  \n1.0466\
    \ \n0.7322 \n4 \n \nTable 8: Results of Machine Learning models for Jagdalpur\
    \ station \nModel \nKernel \nRMSE \nR-\nSquare \nRank \nSVR \nrbf \n1.4647 \n\
    0.4528 \n4 \nLinear \n1.5257 \n0.6950 \n1 \nLinear \n1.5275 \n0.6949 \n2 \nAdaBoost\
    \ \n  \n1.5709 \n0.3705 \n5 \nXGBoost \n  \n1.4581 \n0.6420 \n3 \n \n31 \n \n\
    Table 9: Hyperparameters of Machine Learning Models used in simulation \nModel\
    \ Name \nKernel \nHyperparameter \nSVR \nRBF \n'epsilon': 0.1, 'kernel': 'RBF',\
    \ max_iter': 1000, 'tol': 0.001, \nLinear \n'epsilon': 0.1, 'kernel': 'linear',\
    \ 'max_iter': 1000, 'tol': 0.001, \nPoly \n'epsilon': 0.1, 'kernel': 'poly', 'max_iter':\
    \ 1000, 'tol': 0.001 \nAdaBoost \n \n'learning_rate': 1.0, 'loss': 'linear', 'n_estimators':\
    \ 100, 'random_state': 0 \nXGBoost \n \nobjective': 'reg:linear', 'booster': 'gbtree',\
    \ 'gamma': 0, 'learning_rate': \n0.300000012, 'n_estimators': 10, 'random_state':\
    \ 123, 'validate_parameters': 1, \n \n3.1 Sensitivity analysis \nSensitivity study\
    \ has been conducted to precisely estimate the pan evaporation and to decide the\
    \ optimized \nvalue of hyperparameters. Sensitivity analysis examines the influence\
    \ of changing important parameters on the \nrobustness of the findings. Tables\
    \ 5 and 9 provide the hyperparameters utilized in this investigation and as well\
    \ \nas the results. The number of hidden layers in all deep learning models ranges\
    \ from 1 to 4, along with one input \nlayer having 6 nodes and one output layer\
    \ with one node. All hidden neurons employ the \"Relu\" activation \nfunction,\
    \ whereas the output neurons use the linear activation function. Simulation is\
    \ carried out for 10, 20, 50, \n100, 150, 200, 500, 700, and 1000 epochs. The\
    \ number of neurons in the hidden layer is adjusted to 15, 20, 25, \n32, 45, 50,\
    \ 55, 64, 245, 260, 280, and 300 in various combinations. The batch sizes are\
    \ 64 and 512. Overfitting \nof the models is prevented with the help of the dropout.\
    \ For DRNN, DLSTM, and DBi-LSTM neurons, the \ndropout is set to 0.02. The performance\
    \ are measured by the RMSE and R-Square values. Tables 10–12 provide \nthe results\
    \ of the sensitivity analysis. It has been observed that as the number of epochs\
    \ increases, the error plot \ngoes down, and after 1000 epochs, there is very\
    \ little change. Less number of neurons is beneficial in terms of \ncomputation\
    \ time but performance wise it is not satisfactory, while a large number of neurons\
    \ also do not \nprovide expected outcomes and increases the complexity. Neurons\
    \ in between high and low numbers are giving \ngood results with less computation\
    \ time. The deep RNN is giving the best result for all the three stations. \n\
    \ \n32 \n \nTable 10: Results of Sensitivity analysis for the Raipur station.\
    \ \nModel \nEpochs \nTraining \nShuffle \nBatch \nSize \n  \nHidden Layer 1 \n\
    Hidden Layer 2 \nHidden Layer 3 \nHidden Layer 4 \nRMSE \nR-Square \nNeurons \n\
    Droup \nout \nNeurons \nDroup \nout \nNeurons \nDroup \nout \nNeurons \nDroup\
    \ \nOut \nBidirectional \nLSTM \n50 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n\
    0.02 \n15 \n0.02 \n0.9976 \n0.9011 \nLSTM \n50 \nTRUE \n64 \n32 \n0.02 \n20 \n\
    0.02 \n25 \n0.02 \n15 \n0.02 \n1.0860 \n0.8829 \nRNN \n50 \nTRUE \n64 \n32 \n\
    0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.0246 \n0.8957 \nDNN \n50 \nTRUE \n\
    \  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n0.9994 \n0.9008 \nBidirectional \nLSTM\
    \ \n100 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.0067 \n\
    0.8993 \nLSTM \n100 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02\
    \ \n1.0148 \n0.8977 \nRNN \n100 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02\
    \ \n15 \n0.02 \n0.9869 \n0.9033 \nDNN \n100 \nTRUE \n  \n32 \n  \n20 \n  \n25\
    \ \n  \n15 \n  \n0.9930 \n0.9021 \nBidirectional \nLSTM \n150 \nTRUE \n64 \n32\
    \ \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9924 \n0.9022 \nLSTM \n150 \n\
    TRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.0138 \n0.8979 \n\
    RNN \n150 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9762\
    \ \n0.9053 \nDNN \n150 \nTRUE \n  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n0.9894\
    \ \n0.9028 \nBidirectional \nLSTM \n200 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n\
    25 \n0.02 \n15 \n0.02 \n0.9671 \n0.9071 \nLSTM \n200 \nTRUE \n64 \n32 \n0.02 \n\
    20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9987 \n0.9009 \nRNN \n200 \nTRUE \n64 \n\
    32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9334 \n0.9135 \nDNN \n200 \n\
    TRUE \n  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n0.9824 \n0.9041 \nBidirectional\
    \ \nLSTM \n50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9998\
    \ \n0.9007 \nLSTM \n50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n\
    0.02 \n1.0070 \n0.8993 \n33 \n \nRNN \n50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02\
    \ \n55 \n0.02 \n50 \n0.02 \n0.9773 \n0.9051 \nDNN \n50 \nTRUE \n  \n60 \n  \n\
    45 \n  \n55 \n  \n50 \n  \n0.9877 \n0.9031 \nBidirectional \nLSTM \n100 \nTRUE\
    \ \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9973 \n0.9012 \nLSTM\
    \ \n100 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n1.0397 \n\
    0.8926 \nRNN \n100 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02\
    \ \n0.9686 \n0.9068 \nDNN \n100 \nTRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n\
    \  \n1.0211 \n0.8964 \nBidirectional \nLSTM \n150 \nTRUE \n64 \n60 \n0.02 \n45\
    \ \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9824 \n0.9041 \nLSTM \n150 \nTRUE \n64 \n\
    60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n1.0250 \n0.8956 \nRNN \n150 \n\
    TRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9482 \n0.9107 \n\
    DNN \n150 \nTRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n  \n1.0012 \n0.9004 \n\
    Bidirectional \nLSTM \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n\
    50 \n0.02 \n0.9865 \n0.9033 \nLSTM \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n\
    55 \n0.02 \n50 \n0.02 \n1.0030 \n0.9001 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n\
    \  \n  \n  \n  \n  \n  \n0.9601 \n0.9084 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n\
    45 \n  \n  \n  \n  \n  \n0.9342 \n0.9133 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n\
    45 \n0.02 \n55 \n0.02 \n  \n  \n0.9354 \n0.9131 \nRNN \n200 \nTRUE \n64 \n60 \n\
    0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.8995 \n0.9196 \nRNN \n500 \nTRUE\
    \ \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.8029 \n0.9360 \nRNN\
    \ \n700 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.7975 \n\
    0.9368 \nRNN \n1000 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02\
    \ \n0.7072 \n0.9503 \nDNN \n200 \nTRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n\
    \  \n0.9531 \n0.9098 \nBidirectional \nLSTM \n10 \nTRUE \n512 \n300 \n0.02 \n\
    245 \n0.02 \n280 \n0.02 \n260 \n0.02 \n1.1502 \n0.8686 \nBidirectional \nLSTM\
    \ \n20 \nTRUE \n512 \n300 \n0.02 \n245 \n0.02 \n280 \n0.02 \n260 \n0.02 \n1.1594\
    \ \n0.8665 \n34 \n \nBidirectional \nLSTM \n50 \nTRUE \n512 \n300 \n0.02 \n245\
    \ \n0.02 \n280 \n0.02 \n260 \n0.02 \n1.0594 \n0.8885 \n \nTable 11: Results of\
    \ the Sensitivity Analysis for the Jagdalpur station. \nModel \nEpochs \nTraining\
    \ \nShuffle \nBatch \nSize \nHidden Layer 1 \nHidden Layer 2 \nHidden Layer 3\
    \ \nHidden Layer 4 \nRMSE \nR-\nSquare \nNeurons \nDroup \nout \nNeurons \nDroup\
    \ \nout \nNeurons \nDroup \nout \nNeurons \nDroup \nOut \nBidirectional \nLSTM\
    \ \n50 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9665 \n\
    0.7718 \nLSTM \n50 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02\
    \ \n1.0012 \n0.7551 \nRNN \n50 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02\
    \ \n15 \n0.02 \n0.9532 \n0.7781 \nDNN \n50 \nTRUE \n  \n32 \n  \n20 \n  \n25 \n\
    \  \n15 \n  \n0.9565 \n0.7765 \nBidirectional \nLSTM \n100 \nTRUE \n64 \n32 \n\
    0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9613 \n0.7742 \nLSTM \n100 \nTRUE\
    \ \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9710 \n0.7697 \nRNN\
    \ \n100 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9318 \n\
    0.7879 \nDNN \n100 \nTRUE \n  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n0.9609 \n\
    0.7744 \nBidirectional \nLSTM \n150 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n\
    0.02 \n15 \n0.02 \n0.9498 \n0.7796 \nLSTM \n150 \nTRUE \n64 \n32 \n0.02 \n20 \n\
    0.02 \n25 \n0.02 \n15 \n0.02 \n0.9703 \n0.7700 \nRNN \n150 \nTRUE \n64 \n32 \n\
    0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9377 \n0.7852 \nDNN \n150 \nTRUE\
    \ \n  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n0.9584 \n0.7756 \nBidirectional \n\
    LSTM \n200 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9564\
    \ \n0.7766 \nLSTM \n200 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n\
    0.02 \n0.9644 \n0.7728 \nRNN \n200 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n\
    0.02 \n15 \n0.02 \n0.9362 \n0.7859 \nDNN \n200 \nTRUE \n  \n32 \n  \n20 \n  \n\
    25 \n  \n15 \n  \n0.9460 \n0.7814 \n35 \n \nBidirectional \nLSTM \n50 \nTRUE \n\
    64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9874 \n0.7618 \nLSTM \n\
    50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9895 \n0.7608\
    \ \nRNN \n50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9884\
    \ \n0.7614 \nDNN \n50 \nTRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n  \n0.9650\
    \ \n0.7725 \nBidirectional \nLSTM \n100 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n\
    55 \n0.02 \n50 \n0.02 \n1.0283 \n0.7417 \nLSTM \n100 \nTRUE \n64 \n60 \n0.02 \n\
    45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9814 \n0.7647 \nRNN \n100 \nTRUE \n64 \n\
    60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9326 \n0.7876 \nDNN \n100 \n\
    TRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n  \n0.9473 \n0.7808 \nBidirectional\
    \ \nLSTM \n150 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n\
    0.9571 \n0.7762 \nLSTM \n150 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n\
    50 \n0.02 \n0.9666 \n0.7718 \nRNN \n150 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n\
    55 \n0.02 \n50 \n0.02 \n0.8824 \n0.8098 \nDNN \n150 \nTRUE \n  \n60 \n  \n45 \n\
    \  \n55 \n  \n50 \n  \n0.9569 \n0.7763 \nBidirectional \nLSTM \n200 \nTRUE \n\
    64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9783 \n0.7662 \nLSTM \n\
    200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9606 \n0.7746\
    \ \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n  \n  \n  \n  \n  \n  \n0.9297 \n0.7888\
    \ \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n  \n  \n  \n  \n0.8964 \n\
    0.8037 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n  \n  \n\
    0.8614 \n0.8187 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n\
    50 \n0.02 \n0.8416 \n0.8270 \nRNN \n500 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n\
    55 \n0.02 \n50 \n0.02 \n0.7377 \n0.8670 \nRNN \n700 \nTRUE \n64 \n60 \n0.02 \n\
    45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.7193 \n0.8736 \nDNN \n200 \nTRUE \n  \n\
    60 \n  \n45 \n  \n55 \n  \n50 \n  \n0.9321 \n0.7878 \nBidirectional \nLSTM \n\
    10 \nTRUE \n512 \n300 \n0.02 \n245 \n0.02 \n280 \n0.02 \n260 \n0.02 \n1.0248 \n\
    0.7435 \n36 \n \nBidirectional \nLSTM \n20 \nTRUE \n512 \n300 \n0.02 \n245 \n\
    0.02 \n280 \n0.02 \n260 \n0.02 \n1.0119 \n0.7499 \nBidirectional \nLSTM \n50 \n\
    TRUE \n512 \n300 \n0.02 \n245 \n0.02 \n280 \n0.02 \n260 \n0.02 \n0.9707 \n0.7698\
    \ \n \nTable 12: Results of Sensitivity Analysis for the Ambikapur station \n\
    Model \nEpochs \nTraining \nShuffle \nBatch \nSize \nHidden Layer 1 \nHidden Layer\
    \ 2 \nHidden Layer 3 \nHidden Layer 4 \nRMSE \nR-\nSquare \n  \n  \n  \n  \nNeurons\
    \ \nDroup \nout \nNeurons \nDroup \nout \nNeurons \nDroup \nout \nNeurons \nDroup\
    \ \nOut \nBidirectional \nLSTM \n50 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n\
    0.02 \n15 \n0.02 \n1.1632 \n0.6664 \nLSTM \n50 \nTRUE \n64 \n32 \n0.02 \n20 \n\
    0.02 \n25 \n0.02 \n15 \n0.02 \n1.1287 \n0.6859 \nRNN \n50 \nTRUE \n64 \n32 \n\
    0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.1400 \n0.6796 \nDNN \n50 \nTRUE \n\
    \  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n1.0040 \n0.7514 \nBidirectional \nLSTM\
    \ \n100 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.0587 \n\
    0.7236 \nLSTM \n100 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02\
    \ \n0.9971 \n0.7548 \nRNN \n100 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02\
    \ \n15 \n0.02 \n1.0580 \n0.7240 \nDNN \n100 \nTRUE \n  \n32 \n  \n20 \n  \n25\
    \ \n  \n15 \n  \n1.0575 \n0.7242 \nBidirectional \nLSTM \n150 \nTRUE \n64 \n32\
    \ \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.0258 \n0.7405 \nLSTM \n150 \n\
    TRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n0.9894 \n0.7586 \n\
    RNN \n150 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.0806\
    \ \n0.7121 \nDNN \n150 \nTRUE \n  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n1.0331\
    \ \n0.7368 \nBidirectional \nLSTM \n200 \nTRUE \n64 \n32 \n0.02 \n20 \n0.02 \n\
    25 \n0.02 \n15 \n0.02 \n1.0052 \n0.7509 \nLSTM \n200 \nTRUE \n64 \n32 \n0.02 \n\
    20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.1056 \n0.6986 \n37 \n \nRNN \n200 \nTRUE\
    \ \n64 \n32 \n0.02 \n20 \n0.02 \n25 \n0.02 \n15 \n0.02 \n1.0010 \n0.7529 \nDNN\
    \ \n200 \nTRUE \n  \n32 \n  \n20 \n  \n25 \n  \n15 \n  \n1.0000 \n0.7534 \nBidirectional\
    \ \nLSTM \n50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n1.0959\
    \ \n0.7038 \nLSTM \n50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n\
    0.02 \n1.3561 \n0.5466 \nRNN \n50 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n\
    0.02 \n50 \n0.02 \n1.0012 \n0.7528 \nDNN \n50 \nTRUE \n  \n60 \n  \n45 \n  \n\
    55 \n  \n50 \n  \n1.4103 \n0.4531 \nBidirectional \nLSTM \n100 \nTRUE \n64 \n\
    60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n1.0717 \n0.7168 \nLSTM \n100 \n\
    TRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n1.0612 \n0.7223 \n\
    RNN \n100 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n1.0713\
    \ \n0.7170 \nDNN \n100 \nTRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n  \n1.4317\
    \ \n0.4364 \nBidirectional \nLSTM \n150 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n\
    55 \n0.02 \n50 \n0.02 \n1.0044 \n0.7512 \nLSTM \n150 \nTRUE \n64 \n60 \n0.02 \n\
    45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n1.0583 \n0.7238 \nRNN \n150 \nTRUE \n64 \n\
    60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9924 \n0.7571 \nDNN \n150 \n\
    TRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n  \n1.2836 \n0.5364 \nBidirectional\
    \ \nLSTM \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n\
    0.9911 \n0.7578 \nLSTM \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n\
    50 \n0.02 \n1.1048 \n0.6990 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n  \n  \n  \n\
    \  \n  \n  \n1.0566 \n0.7247 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n\
    \  \n  \n  \n  \n0.9421 \n0.7812 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02\
    \ \n55 \n0.02 \n  \n  \n0.9532 \n0.7760 \nRNN \n200 \nTRUE \n64 \n60 \n0.02 \n\
    45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.9108 \n0.7954 \nRNN \n500 \nTRUE \n64 \n\
    60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.8682 \n0.8141 \nRNN \n700 \n\
    TRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.7597 \n0.8577 \n\
    RNN \n1000 \nTRUE \n64 \n60 \n0.02 \n45 \n0.02 \n55 \n0.02 \n50 \n0.02 \n0.7463\
    \ \n0.8659 \n38 \n \nDNN \n200 \nTRUE \n  \n60 \n  \n45 \n  \n55 \n  \n50 \n \
    \ \n0.9810 \n0.7627 \n \n3.2 Result discussion \nThe SVM, XGB, AdaBoost, DNN,\
    \ DRNN, DLSTM, and DBI-LSTM models are used to train data from all \nthree stations.\
    \ To train and assess the models, different batch sizes, activation functions,\
    \ epoch, and layer \ncounts are utilized. The findings exhibit that deep learning\
    \ models are more effective for \U0001D438\U0001D443 estimation for \neach of\
    \ the three stations. The RMSE and R-Square values with 1000 epochs and 64 batch\
    \ sizes are 0.7072 and \n0.9503 for Raipur station respectively, and 0.7463 and\
    \ 0.8626 for Ambikapur station respectively. The same \nmodel with similar condition\
    \ provide RMSE value of 0.7193 and R-Squared value of 0.8736 for the Jagdalpur\
    \ \nstation in 700 epochs. The largest real value in the testing section is 17.6,\
    \  for Raipur while the maximum \nforecasted value is 16.08, and the lowest actual\
    \ value is 0.4, but the forecasted minimum value is 0.19.  The \nactual highest\
    \ result in the Jagdalpur testing section is 12 while the forecasted maximum value\
    \ is 11.33. The \nactual minimum value for the same station is 0.19, whereas the\
    \ projected minimum is 0. At Ambikapur, the \nactual highest value is 11.8 while\
    \ the forecasted maximum value is 12.67. The actual minimum value is 0.1, \nhowever\
    \ the forecasted minimum value is -1.00. The \"Relu\" activation function is used\
    \ throughout all the \nfindings. Tables 6-8, 10-12 display all of the findings.\
    \ Line and scatter plots of the actual data versus the \nprojected data are displayed\
    \ in Figures 11–13. The figures show that the models predict results quite close\
    \ to \nthe real ones. Table 13 provides a comparison of the results with that\
    \ of the literature. The table makes it \nevident that the proposed DRNN for all\
    \ three stations outperforms the models available in the literature and \ncan\
    \ be used for practical practices.  \nTable 13: Comparison of results with existing\
    \ literature \nStation Name \nModel Name \nRMSE \nR-Square \nRank \nRaipur \n\
    Linacre [15] \n2.190 \n0.830 \n5 \nChristiansen [15] \n1.310 \n0.850 \n4 \nFlann-4[15]\
    \ \n1.060 \n0.890 \n3 \nDeep-LSTM-6[28] \n0.98 \n0.915 \n2 \n39 \n \nProposed\
    \ DRNN \n0.707 \n0.950 \n1 \nJagdalpur \nLinacre [15] \n1.430 \n0.680 \n5 \nChristiansen\
    \ [15] \n1.190 \n0.690 \n4 \nFlann-4[15] \n0.880 \n0.810 \n2 \nDeep-LSTM-6[28]\
    \ \n0.970 \n0.769 \n3 \nProposed DRNN \n0.719 \n0.873 \n1 \nAmbikapur \nLinacre\
    \ [15] \n1.070 \n0.670 \n4 \nChristiansen [15] \n1.210 \n0.640 \n5 \nFlann-4[15]\
    \ \n0.850 \n0.750 \n2 \nDeep-LSTM-6[28] \n0.930 \n0.716 \n3 \nProposed DRNN \n\
    0.746 \n0.862 \n1 \n \n \n(a) Line Plot of Actual and Predicted values. \n \n\
    \ \n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n1\n71\n141\n211\n281\n351\n421\n491\n\
    561\n631\n701\n771\n841\n911\n981\n1051\n1121\n1191\n1261\n1331\n1401\n1471\n\
    1541\n1611\n1681\n1751\n1821\n1891\n1961\n2031\n2101\n2171\n2241\n2311\n2381\n\
    2451\n2521\nEp Value\nNo. of Testing Patterns\nActual vs Predicted for Raipur\
    \ Station using RNN\nActual\nPredicted\n40 \n \ny = 0.9327x + 0.2236\nR² = 0.9514\n\
    0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n0\n5\n10\n15\n20\nPREDICTED VALUE\nACTUAL VALUE\n\
    ACTUAL VS PREDICTED FOR RAIPUR \nSTATION\n \n \n \n \n \n \n \n \n \n(b)  Line\
    \ Plot of Actual and Predicted values. \nFigure 11: Plotting of Actual and Predicted\
    \ values in Ambikapur station. \n \n \n \n(a) Line Plot of Actual and Predicted\
    \ values. \n \n0\n2\n4\n6\n8\n10\n12\n14\n0\n50\n100\n150\n200\n250\n300\n350\n\
    400\n450\n500\n550\n600\n650\n700\n750\n800\n850\n900\n950\n1000\n1050\n1100\n\
    1150\n1200\n1250\n1300\n1350\n1400\n1450\n1500\n1550\n1600\n1650\n1700\n1750\n\
    1800\nEp value\nNo. of Testing Patterns\nActaul Vs Predicted for Jagdalpur station\
    \ using RNN\nActual\nPredicted\n41 \n \n \n(b) Scatter Plotof Actual and Predicted\
    \ values. \nFigure 12: Plotting of Actual and Predicted values in Jagdalpur station.\
    \ \n \n \n(a) 1 Line plot of Actual and Predicted values. \n \ny = 0.908x + 0.3605\n\
    R² = 0.8805\n0\n2\n4\n6\n8\n10\n12\n0\n2\n4\n6\n8\n10\n12\n14\nPREDICTED VALUES\n\
    ACTUAL VALUES\nACTUAL VS PREDICTED VALUE FOR JAGDALPUR \nSTATION USING RNN\n-2\n\
    0\n2\n4\n6\n8\n10\n12\n14\n1\n39\n77\n115\n153\n191\n229\n267\n305\n343\n381\n\
    419\n457\n495\n533\n571\n609\n647\n685\n723\n761\n799\n837\n875\n913\n951\n989\n\
    1027\n1065\n1103\n1141\n1179\n1217\n1255\n1293\n1331\n1369\n1407\n1445\nActual\
    \ Vs Predicted for Ambikapur using RNN\nActual\nPredicted\n42 \n \n \n(b) Scatter\
    \ plot of Actual and Predicted values. \nFigure 13: Plotting of Actual and Predicted\
    \ values in Ambikapur station. \n4. Conclusion \nThis study is carried out to\
    \ assess the potentiality of Deep Learning models for estimation of daily \U0001D438\
    \U0001D443 losses \nunder different agro-climatic situations with the help of\
    \ climatic data influencing the evaporation process. \nInitially the data has\
    \ been collected and preprocessing steps, such as (i) missing value handling and\
    \ (ii) min-\nmax normalization has been applied on the dataset. After that data\
    \ is divided into train and test parts in the ratio \nof 80% - 20%. The models\
    \ are trained using the training dataset and performance has been evaluated using\
    \ the \ntesting dataset. The RMSE value obtained during testing using DRNN is\
    \ 0.707, 0.719 and 0.746 for Raipur, \nJagdalpur and Ambikapur stations respectively.\
    \ The investigation has led to the following conclusion: \n• \nDeep learning models\
    \ outperform machine learning models across all three stations. \n• \nDRNN is\
    \ the best deep learning model for all three stations among the four deep learning\
    \ models under \nstudy. \n• \nThe proposed model can be used for irrigation scheduling,\
    \ water resource management which is very \nimportant for agriculture and its\
    \ related activities. \nThe limitations of the study are: \ny = 0.9265x + 0.3041\n\
    R² = 0.8659\n-2\n0\n2\n4\n6\n8\n10\n12\n14\n0\n2\n4\n6\n8\n10\n12\n14\nPREDICTED\
    \ VALUE\nACTUAL VALUE\nACTUAL VS PREDICTED FOR \nAMBIKAPUR STATION\n43 \n \n1.\
    \ The data after 2017 is not available till date. \n2. Solar radiation is one\
    \ of the important factors and plays a vital role in prediction of the \U0001D438\
    \U0001D443 value but it \nis not available for all three stations. \nAcknowledgments\
    \ \nWe acknowledge our sincere thanks to Dr. Diwakar Naidu of IACR Raipur for\
    \ sharing the data with us for \nresearch purpose. \n \nDeclaration \nEthics approval\
    \ and consent to participate \nNot applicable. \nConsent for publication \nNot\
    \ applicable. \nAvailability of data and materials \nThe datasets used during\
    \ the current study are available from the corresponding author on reasonable\
    \ \nrequest. \n \nCompeting interests \nThe authors declare no competing interests.\
    \ \nFunding \nThis research was funded by Princess Nourah bint Abdulrahman University\
    \ Researchers Supporting \nProject number (PNURSP2023R235), Princess Nourah bint\
    \ Abdulrahman University, Riyadh, Saudi \nArabia. The authors extend their appreciation\
    \ to the Deanship of Scientific Research at King Khalid \nUniversity (KKU) for\
    \ funding this work through the Research Group Program Under the Grant \nNumber:\
    \ (R.G.P.2/382/44).  \n \n44 \n \nAuthors' contributions \nBabita Majhi: Supervision,\
    \ Software, Conceptualization, Methodology, Writing- Reviewing and Editing. \n\
    Rupesh \nNaik: \nSoftware, \nData \ncuration, \nWriting- \nOriginal \ndraft \n\
    preparation, \nVisualization, \nInvestigation. Sujata Dash: Supervision, Software,\
    \ Validation, Writing- Reviewing and Editing. Saurav \nMallik: Methodology, Validation,\
    \ Writing- Reviewing and Editing. Amal Al-Rasheed: Writing- Original draft \n\
    preparation, \nVisualization, \nMohamed \nAbbas: \nSoftware, \nValidation, \n\
    Ben \nOthman \nSoufiene: \nValidation, Writing- Reviewing and Editing. All authors\
    \ have read and agreed to the published version of the \nmanuscript.  \nReference:\
    \ \n[1] \nC. H. B. Priestley, “On the Assessment of Surface Heat Flux and Evaporation\
    \ Using Large-\nScale Parameters.” \n[2] \nJ. E. Christiansen, “Pan evaporation\
    \ and evapotranspiration from climatic data,” J. Irrig. Drain. \nDiv., vol. 94,\
    \ pp. 243–266, 1968. \n[3] \nJ. F. Griffiths, “ANOTHER EVAPORATION FORMULA.” \n\
    [4] \nE. H. Stephens, John C and Stewart, “A comparison of procedures for computing\
    \ evaporation \nand evapotranspiration},” Fort Lauderdle, vol. 62, pp. 123–133,\
    \ 1963. \n[5] \nF. W. Reichelderfer et al., “No. 18 *Normal Mean Virtual Temperatures\
    \ and Weights of the Air \nColumn Between Sea Level and 10,000 Feet. Staff, Extended\
    \ Forecast Section,” 1943. \n[6] \nH. Ey et al., “N a tu ra l evaporation from\
    \ open w ater, bare soil a n d grass,” 1946. [Online]. \nAvailable: https://royalsocietypublishing.org/\
    \ \n[7] \nS. Kim, J. Shiri, V. P. Singh, O. Kisi, and G. Landeras, “Predicting\
    \ daily pan evaporation by \nsoft computing models with limited climatic data,”\
    \ \nhttps://doi.org/10.1080/02626667.2014.945937, vol. 60, no. 6, pp. 1120–1136,\
    \ Jun. 2015, doi: \n10.1080/02626667.2014.945937. \n45 \n \n[8] \nY. Jin et al.,\
    \ “Decreasing relative humidity dominates a reversal of decreasing pan evaporation\
    \ \nin mainland China after 1989,” J. Hydrol., vol. 608, 2022, doi: 10.1016/j.jhydrol.2022.127641.\
    \ \n[9] \nA. K. Srivasatava, D. Naidu, M. Bhan, and L. M. Bal, “Neural network\
    \ based predictors for \nevaporation estimation at jabalpur in central india,”\
    \ J. Sci. Ind. Res. (India)., vol. 81, no. 3, pp. \n319–328, 2022, doi: 10.56042/jsir.v81i03.58166.\
    \ \n[10] T. Yao, H. Lu, Q. Yu, W. Feng, and Y. Xue, “Change and attribution of\
    \ pan evaporation \nthroughout the Qinghai-Tibet Plateau during 1979–2017 using\
    \ China meteorological forcing \ndataset,” Int. J. Climatol., vol. 42, no. 3,\
    \ pp. 1445–1459, 2022, doi: 10.1002/joc.7312. \n[11] M. Abed, M. A. Imteaz, A.\
    \ N. Ahmed, and Y. F. Huang, “Modelling monthly pan evaporation \nutilising Random\
    \ Forest and deep learning algorithms,” Sci. Rep., vol. 12, no. 1, p. 13132, \n\
    2022, doi: 10.1038/s41598-022-17263-3. \n[12] H. Moayedi, S. Ghareh, and L. K.\
    \ Foong, “Quick integrative optimizers for minimizing the \nerror of neural computing\
    \ in pan evaporation modeling,” Eng. Comput., vol. 38, no. \n0123456789, pp. 1331–1347,\
    \ 2022, doi: 10.1007/s00366-020-01277-4. \n[13] M. A. Ghorbani, M. A. Jabehdar,\
    \ Z. M. Yaseen, and S. Inyurt, “Solving the pan evaporation \nprocess complexity\
    \ using the development of multiple mode of neurocomputing models,” \nTheor. Appl.\
    \ Climatol., vol. 145, no. 3–4, pp. 1521–1539, 2021, doi: 10.1007/s00704-021-\n\
    03724-8. \n[14] A. Seifi and F. Soroush, “Pan evaporation estimation and derivation\
    \ of explicit optimized \nequations by novel hybrid meta-heuristic ANN based methods\
    \ in different climates of Iran,” \nComput. Electron. Agric., vol. 173, no. February,\
    \ 2020, doi: 10.1016/j.compag.2020.105418. \n[15] B. Majhi and D. Naidu, “Pan\
    \ evaporation modeling in different agroclimatic zones using \nfunctional link\
    \ artificial neural network,” Inf. Process. Agric., vol. 8, no. 1, pp. 134–147,\
    \ 2021, \ndoi: 10.1016/j.inpa.2020.02.007. \n46 \n \n[16] V. Nourani, M. Sayyah-Fard,\
    \ M. T. Alami, and E. Sharghi, “Data pre-processing effect on \nANN-based prediction\
    \ intervals construction of the evaporation process at different climate \nregions\
    \ in Iran,” J. Hydrol., vol. 588, p. 125078, 2020, doi: 10.1016/j.jhydrol.2020.125078.\
    \ \n[17] S. Zhang, “The Journal of Systems and Software Nearest neighbor selection\
    \ for iteratively k \nNN imputation,” J. Syst. Softw., vol. 85, no. 11, pp. 2541–2552,\
    \ 2012, [Online]. Available: \nhttp://dx.doi.org/10.1016/j.jss.2012.05.073 \n\
    [18] L. Dong et al., “Estimating the pan evaporation in northwest china by coupling\
    \ catboost with \nbat algorithm,” Water (Switzerland), vol. 13, no. 3, 2021, doi:\
    \ 10.3390/w13030256. \n[19] Ö. Baydaroǧlu and K. Koçak, “SVR-based prediction\
    \ of evaporation combined with chaotic \napproach,” J. Hydrol., vol. 508, pp.\
    \ 356–363, 2014, doi: 10.1016/j.jhydrol.2013.11.008. \n[20] X. Zhu, P. Zhang,\
    \ and M. Xie, “A Joint Long Short-Term Memory and AdaBoost regression \napproach\
    \ with application to remaining useful life estimation,” Measurement, vol. 170,\
    \ p. \n108707, Jan. 2021, doi: 10.1016/J.MEASUREMENT.2020.108707. \n[21] C. Ji,\
    \ X. Zou, Y. Hu, S. Liu, L. Lyu, and X. Zheng, “XG-SF: An XGBoost Classifier Based\
    \ on \nShapelet Features for Time Series Classification,” Procedia Comput. Sci.,\
    \ vol. 147, pp. 24–28, \nJan. 2019, doi: 10.1016/J.PROCS.2019.01.179. \n[22] B.\
    \ Majhi, “A Modified Artificial Neural Network (ANN)-Based Time Series Prediction\
    \ of \nCOVID-19 Cases from Multi-Country Data,” J. Inst. Eng. Ser. B, no. 0123456789,\
    \ 2023, doi: \n10.1007/s40031-022-00849-w. \n[23] M. Ibrahim and R. Elhafiz, “Modeling\
    \ an intrusion detection using recurrent neural networks,” \nJ. Eng. Res., vol.\
    \ 11, no. 1, p. 100013, 2023, doi: 10.1016/j.jer.2023.100013. \n[24] A. Niknam,\
    \ H. K. Zare, H. Hosseininasab, and A. Mostafaeipour, “Developing an LSTM model\
    \ \nto forecast the monthly water consumption according to the effects of the\
    \ climatic factors in \nYazd, Iran,” J. Eng. Res., vol. 11, no. 1, p. 100028,\
    \ 2023, doi: 10.1016/j.jer.2023.100028. \n47 \n \n[25] J. Xiang, Z. Qiu, Q. Hao,\
    \ and H. Cao, “Multi-time scale wind speed prediction based on WT-bi-\nLSTM,”\
    \ MATEC Web Conf., vol. 309, p. 05011, 2020, doi: 10.1051/matecconf/202030905011.\
    \ \n[26] E. T. Linacre, “A simple formula for estimating evaporation rates in\
    \ various climates, using \ntemperature data alone,” Agric. Meteorol., vol. 18,\
    \ no. 6, pp. 409–424, 1977, doi: \n10.1016/0002-1571(77)90007-3. \n[27] A. Abdou,\
    \ N. Ramadan, A. Sayed Abdou, and N. Ramadan Darwish, “Early Prediction of \n\
    Software Defect using Ensemble Learning: A Comparative Study Software Project\
    \ \nManagement View project Early Prediction of Software Defect using Ensemble\
    \ Learning: A \nComparative Study,” Artic. Int. J. Comput. Appl., vol. 179, no.\
    \ 46, pp. 975–8887, 2018, \nAccessed: Nov. 03, 2022. [Online]. Available: \nhttps://www.researchgate.net/publication/325859147\
    \ \n[28] B. Majhi, D. Naidu, A. P. Mishra, and S. C. Satapathy, “Improved prediction\
    \ of daily pan \nevaporation using Deep-LSTM model,” Neural Comput. Appl., vol.\
    \ 32, no. 12, pp. 7823–7838, \n2020, doi: 10.1007/s00521-019-04127-7. \n \n"
  inline_citation: '>'
  journal: Research Square (Research Square)
  limitations: '>'
  pdf_link: https://www.researchsquare.com/article/rs-3343374/latest.pdf
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Forecasting of Daily Pan Evaporation Rate using Deep Learning Techniques
    for Three Different Agro-Climatic Regions of Chhattisgarh State
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.21203/rs.3.rs-3137542/v1
  analysis: '>'
  authors:
  - C. Pabitha
  - S Benila
  - A. Suresh
  citation_count: 0
  full_citation: '>'
  full_text: '>

    Page 1/20

    A digital footprint in enhancing agricultural practices with improved

    production using machine learning

    Pabitha C  (  pabithac.cse@srmvalliammai.ac.in )

    SRM Valliammai Engineering College

    Benila S 

    SRM Valliammai Engineering College

    Suresh A 

    SRM Institute of Science and Technology: SRM Institute of Science and Technology
    (Deemed to be University)

    Research Article

    Keywords: agriculture, digital footprint, arti¦cial intelligence, machine learning,
    random forest algorithm

    Posted Date: July 31st, 2023

    DOI: https://doi.org/10.21203/rs.3.rs-3137542/v1

    License:   This work is licensed under a Creative Commons Attribution 4.0 International
    License.   Read Full License

    Page 2/20

    Abstract

    Agriculture shows an important role in the enlargement of an economy. The proposed
    system explores how utilizing digital footprints might help improve

    agricultural practices and production. Agricultural data is increasingly available
    in digital form because of the development of modern technology and the

    spread of connected devices. These data can be utilized to produce digital footprints
    that document the whole lifecycle of agricultural production, from

    planting to harvesting. Farmers may then decide when to sow, water, fertilize,
    and harvest their crops by using machine learning algorithms trained on these

    digital footprints to spot patterns and forecast results. The study examines various
    machine learning methods that can be used to examine digital footprints

    and shows the advantages of doing so, including higher yields, better resource
    management, and lower costs. The timing of crop planting, fertilization, and

    harvest has long been determined by the agriculture sector using conventional
    techniques and experience. However, with the expansion of agricultural data

    available, there is now a chance to use machine learning algorithms to recognize
    trends and forecast outcomes based on this data. This study explores the

    potential of machine learning methods, particularly Random Forest optimization,
    to improve agricultural practices and boost output. The study uses Random

    Forest optimization, a potent machine learning method that can examine massive
    datasets, to forecast crop yield based on many variables like weather

    patterns, soil quality, and fertilizer application. The study''s ¦ndings show
    how well the Random Forest optimization technique predicts crop productivity with

    an accuracy of 99.97%. By giving farmers useful information about forecasting
    crop yields, they may increase their overall productivity by using their

    resources wisely. The report also emphasizes the necessity of additional study
    and funding for the creation of machine learning algorithms that are

    speci¦cally customized to the peculiarities of the agricultural sector, as well
    as the signi¦cance of data quality and privacy in the collecting and analysis
    of

    agricultural data. A combination of hardware and software tools, including data
    collection sensors, data processing equipment, graphics processing units

    (GPUs), cloud computing services, and mobile applications, will be needed for
    the hardware implementation of increasing agricultural practices in production

    using machine learning with Random Forest optimization techniques.

    I. INTRODUCTION

    The practice of farming involves cultivating land, keeping animals, and creating
    food, ¦ber, and other goods that support human life. As a source of food and

    raw materials for businesses that produce goods and services, it is a crucial
    sector of the economy. In many developing nations, since it is frequently the
    sole

    source of income for rural communities, agriculture plays a signi¦cant role in
    the economies of those nations. Agriculture has a signi¦cant role in the nation''s

    GDP and export revenues in addition to producing food and raw materials. It advances
    the construction of rural infrastructure, provides cash, and offers

    employment opportunities. The sustainability of the environment and rural development
    are also intimately related to agriculture. To address more general

    economic and social problems like food security, poverty alleviation, and environmental
    sustainability, the agricultural sector is also crucial. Furthermore,

    because higher productivity and e¨ciency can result in higher earnings and investments
    in other industries, agriculture may be a signi¦cant contributor to

    economic growth. In summary, agriculture is an important economic sector that
    has an impact on society and the environment in addition to its economic

    contribution [1]. Agriculture has been practiced for thousands of years, and over
    that period, it has undergone many changes. Scienti¦c techniques and

    cutting-edge technology are used in modern agriculture to boost output and e¨ciency
    while reducing its negative effects on the environment. Agroforestry,

    crop cultivation, and animal husbandry are some of the main agricultural activities.
    Food security, economic growth, and environmental sustainability all

    suffer from the effects of agriculture. Millions of people all around the world,
    particularly in poor nations, depend on it greatly for their daily survival.

    Sustainable agricultural practices are being created to make sure that agricultural
    activities meet the demands of the present without compromising the

    capacity of future generations to meet their own needs [2]. The use of synthetic
    inputs like fertilizers, insecticides, and herbicides to boost crop yields is

    referred to as conventional agricultural practices. These methods are based on
    monoculture farming, which is the practice of growing the same crop over and

    over in a single ¦eld. Large pieces of equipment, irrigation infrastructure, and
    high-input agricultural methods de¦ne conventional agriculture. Even though

    crop yields and food production have increased signi¦cantly thanks to conventional
    agriculture, the environment is still negatively impacted. When synthetic

    inputs are used excessively, they can cause soil erosion, water pollution, and
    biodiversity loss. In addition, monoculture farming''s reliance on crops makes

    them more susceptible to pests and disease, which can cost farmers a lot of money
    [3]. With the help of recent advancements in technology and scienti¦c

    knowledge, modern agricultural practices have undergone tremendous change. Farmers
    can now produce more food with fewer resources and less negative

    environmental impact thanks to these practices. However, the sector faces several
    challenges that can have signi¦cant impacts on its productivity and

    sustainability. Some of the challenges in agriculture include:

    Climate Change: Climate change is a signi¦cant challenge facing agriculture, as
    it can affect crop yields and increase the incidence of pests and

    diseases. Extreme weather events, such as droughts and §oods, can also impact
    agriculture and reduce crop productivity.

    Limited Resources: Agriculture is dependent on several resources, including water,
    land,

    and energy. However, these resources are becoming increasingly scarce due to factors such as population growth and urbanization. This can lead to comp

    Soil Degradation: Soil degradation is a signi¦cant challenge facing agriculture, as it can reduce the productivity
    of farmland and affect crop yields. Soil

    degradation can result from factors such as erosion, overuse of pesticides and
    fertilizers, and deforestation.

    Pest and Disease Management: Pests and diseases can have a signi¦cant impact on
    crop yields, leading to reduced productivity and higher costs for

    farmers. However, managing pests and diseases can be challenging, as it requires a comprehensive understanding of the pests and diseases and the

    appropriate management strategies.

    Access to Markets: Access to markets can be a signi¦cant challenge for farmers, particularly those in developing countries. Farmers may lack access to m

    and transportation systems, which can limit their ability to sell their products
    and receive fair prices.

    Labor Shortages: Labor shortages can be a signi¦cant challenge for farmers, particularly
    in developed countries. Labor shortages can lead to higher labor

    costs and di¨culties in ¦nding workers to perform critical tasks such as planting and
    harvesting.

    Financial Constraints: Farmers often face ¦nancial constraints, including limited
    access to

    credit and high costs for inputs such as seeds, fertilizers, and machinery. These constraints can limit their ability to invest
    in their farms and increase their

    Page 3/20

    productivity.

    Precision agriculture is one of the biggest innovations in contemporary agriculture.
    Data about soil conditions, crop development, and weather patterns are

    collected using cutting-edge technologies like GPS, sensors, and drones. With
    the help of this knowledge, decisions about pest management,

    fertilizer, irrigation, and planting may be made that will maximize yields and
    the e¨cient use of resources [4]. Genetically modi¦ed organisms (GMOs) are

    another signi¦cant development in agriculture. To increase their resistance to
    pests, diseases, and environmental stressors, these crops have undergone

    genetic engineering. As a result, farmers are now able to grow more food with
    fewer inputs like pesticides and herbicides.

    The importance of sustainable agriculture is likewise rising as global concern over climate change and
    environmental degradation rises. Reduced soil erosion,

    water conservation, and soil health are all bene¦ts of sustainable farming techniques
    like cover crops, crop rotation, and conservation tillage.

    By storing carbon in the soil, they also aid in lowering greenhouse gas emissions. The mechanization of farms has
    also aided modern agriculture. Many

    chores can now be automated by farmers, which lowers labor costs. Examples of
    advanced machinery include tractors, planters, and harvesters. Farmers can

    now manage greater land areas, which has increased farming e¨ciency.

    In summary, contemporary agricultural methods are crucial for feeding a growing
    world population while minimizing the negative effects on the environment.

    The "World Population Prospects," makes predictions on population growth. The
    most recent edition of this report, which spans the years 1950 to 2100, was

    released in 2019. According to the report, there were 7.7 billion people on the
    planet in 2019 and 9.7 billion are expected to live there by the year 2050 [5].
    The

    research also points out that although population growth rates have been falling
    since the 1960s, the overall number of people added to the world''s population

    every year has been rising. Farmers may continue to grow their food production
    while using fewer resources by utilizing technology, science, and sustainable

    farming methods, ensuring the industry''s viability. This is implemented using
    arti¦cial intelligence techniques as shown in ¦gure 1.

    Arti¦cial intelligence (AI) is the period used to designate the capacity of machines
    to carry out tasks that would ordinarily require human intelligence to

    complete, such as comprehending natural language, recognizing patterns, making
    judgment calls, and picking up knowledge from experience. The subject of

    arti¦cial intelligence is expanding quickly, revolutionizing several industries
    and perhaps changing how we work and live [6]. Numerous applications, including

    voice assistants, picture and speech recognition, natural language processing,
    and predictive analytics, currently make use of AI technology. It is also

    employed in industries including manufacturing, transportation, banking, and healthcare,
    where it helps to increase productivity, lower costs, and increase

    safety. However, the quick development of AI also prompts signi¦cant moral and
    societal issues. Concerns have been raised regarding how AI will affect

    employment, privacy, and security, as well as the possibility that these technologies
    would help to reinforce prejudice and discrimination. It will be crucial to

    address these concerns as AI technology develops and to make sure that it is created
    and applied in a way that bene¦ts society as a whole. Arti¦cial

    intelligence (AI) is taking on a bigger and bigger role in agriculture by assisting
    farmers and other stakeholders in making better decisions and maximizing

    their operations [7]. Crop observing, yield estimating, soil investigation, and
    pest controller are just a few of the applications of AI.

    Agronomists may spot possible issues early and take action before they worsen
    by using crop monitoring, where AI analyses images captured by drones or

    sensors to ¦nd changes in plant health or growth patterns. Based on data inputs
    like weather patterns, soil conditions, and previous yield data, AI- powered

    yield prediction can assist farmers in estimating crop yields. Additionally, AI
    can identify and forecast the spread of pest outbreaks to assist in pest

    management, analyze soil samples to evaluate nutrient content and provide fertilization
    suggestions.

    In general, using AI in agriculture helps farmers increase their yields, cut costs,
    and encourage sustainable farming methods. It can transform how we cultivate

    food and tackle some of the biggest issues confronting agriculture, such as food
    security and climate change. They are done using machine learning with

    optimization algorithms. It is de¦ned as the branch of arti¦cial intelligence
    called "machine learning" which uses algorithms and statistical models to let

    computers learn from data without having to be explicitly programmed. It is predicated
    on the premise that computers can discover patterns and reach

    conclusions by examining massive amounts of data [8]. With the introduction of
    technology, especially the use of machine learning techniques, agriculture

    has recently seen a substantial revolution. Since it can automate complicated
    activities and make predictions based on data, machine learning has grown in

    popularity. Some of the causes for utilizing machine learning are as follows:

    Automation: By automating time-consuming and repetitive processes, machine learning
    frees up human resources to work on tasks that are more

    complicated and creative.

    Forecasting and prediction: Machine learning models can be trained on historical
    data to create forecasts and predictions about future outcomes,

    including consumer behavior, sales trends, and market prices.

    Personalization: Machine learning can be used to customize user experiences, such
    as news feeds or product recommendations on e-commerce websites.

    Fraud detection: By learning from trends in historical data, machine learning
    can be used to identify fraudulent transactions or activities.

    Natural language processing: Chatbots, voice assistants, and other language-based
    applications can be made possible through machine learning when it

    comes to analyzing and comprehending natural language.

    Image and speech recognition: By using machine learning to identify patterns in
    images and audio, applications like facial and object identi¦cation and

    voice-based user interfaces are made possible.

    This study examines how digital footprints can improve farming practices and increase
    output. The process of transforming information from an analog to a

    digital representation is referred to as digitalization. Then, utilizing electrical
    equipment and computer networks, this digital format may be stored, processed,

    and sent. From the way we interact and consume information to the way we work
    and access healthcare, digitalization has drastically changed many facets

    of our lives [9]. The term "digital footprint" describes the online tracks that
    a person or business leaves behind on the internet. The importance of having a

    digital footprint in agriculture is rising as machine learning-enhanced production
    practices improve farming practices.

    Page 4/20

    Here are a few explanations:

    Data Gathering: Digital footprints can be used to gather information on agricultural
    practices, including the usage of pesticides, irrigation systems, and

    fertilizers. By using this information, it is possible to develop digital pro¦les
    of farms and produce insights that can be used to maximize agricultural

    output. Then, by analyzing this data, machine learning algorithms may provide
    farmers with insights and suggestions for enhancing productivity.

    Predictive Analytics: Machine learning algorithms can create predictions about
    crop yields, insect outbreaks, and weather trends using data from digital

    footprints. This can assist farmers in choosing planting, irrigation, and pest
    management strategies with more knowledge, which can increase yield. With

    precision agriculture, it is possible to construct computerized maps of farms
    that show regions with a particular need, such as nutrient shortages or pest

    infestations. Then, using machine learning algorithms, production can be improved
    by reducing waste and using targeted applications of pesticides or

    fertilizer, for example.

    Crop Monitoring: Sensors, drones, or satellites can employ digital footprints
    to monitor crops in real time. The data can then be analyzed using machine

    learning algorithms to give farmers information on the health and growth trends
    of their crops. This can assist farmers in identifying and addressing

    problems like nutrient de¦cits or pest outbreaks early on, which can boost crop
    yields and lower losses.

    The capacity to store and analyze enormous volumes of data rapidly and effectively
    is one of the main advantages of digitization. As a result, cutting-edge

    ¦elds like machine learning and arti¦cial intelligence have emerged, which generate
    predictions and judgments using vast volumes of data. It is now simpler

    for people to work together across distances and time zones thanks to new kinds
    of communication and collaboration made possible by digitalization. Online

    collaboration tools and remote work have grown in popularity over the past several
    years as a result of this. Agriculture and food production are being

    revolutionized by digitalization [10]. It has the potential to boost productivity,
    cut expenses, and boost yields, ultimately resulting in more lucrative and

    sustainable agriculture.

    In the case of cash crops, digitalization has allowed farmers to more effectively
    track and manage their harvests from planting to harvest. Farmers can

    monitor soil moisture, nitrogen levels, and other crucial factors thanks to technologies
    like precision farming and remote sensing, which improve crop yields

    and cut expenses. Digitalization is assisting in addressing problems with food
    crops such as food waste and ine¨cient supply chains [11]. For instance, food

    goods can be tracked using blockchain technology from farm to table, lowering
    the danger of contamination and enabling more effective recalls.

    Agriculture and food production are being transformed overall by digitalization,
    becoming more effective, sustainable, and pro¦table for both farmers and

    customers. We examine the application of machine learning algorithms in agriculture
    and how they might increase crop output and support farmers'' decision-

    making. We also look at the opportunities and problems associated with integrating
    digital footprints into agriculture and how that might result in sustainable

    practices. The digital footprints that people leave behind when interacting with
    technology are referred to as digital footprints. Digital footprints in agriculture

    can be used to gather information on farming methods, crop output, weather trends,
    and soil health. Machine learning algorithms can be created using this

    data to enhance agricultural practices. For instance, information on weather patterns
    can be utilized to forecast crop output and determine the ideal time for

    sowing. To utilize fertilizer and pesticides most effectively, data on soil health
    can be employed. Machine learning algorithms come in a variety of forms, such

    as reinforcement learning, unsupervised learning, and supervised learning [12].
    Unsupervised learning includes spotting patterns in unlabeled data, whereas

    supervised learning uses labeled data to train a machine learning model. Due to
    its capacity to collect and analyze vast amounts of data to produce insights

    that might assist farmers in making more informed decisions, machine learning
    is becoming more and more widely used in agricultural practices. The usage

    of resources can be optimized, waste can be decreased, and crop yields can now
    be increased thanks to machine learning. Precision farming is one of the

    most often used uses of machine learning in agriculture. Applying the appropriate
    inputs, such as fertilizer, herbicides, and water, at the appropriate time and

    in the appropriate amount includes precision agriculture, which involves analyzing
    data to identify variability within a ¦eld. To assist farmers in making more

    accurate judgments about how to manage their crops, machine learning algorithms
    can analyze data from sensors, satellite photos, and weather forecasts.

    Predicting crop yields is another way that machine learning is used in agriculture.
    Machine learning algorithms can create predictive models that assist

    farmers in estimating their yields for the current season by analyzing historical
    data on crop yields and weather trends. Making decisions about when to

    harvest, when to sell, and how much to grow can be done using this information.

    Plant disease management and identi¦cation can both be done using machine learning.
    Machine learning algorithms can detect illnesses and pests early on

    by examining photographs of plants, allowing farmers to take preventative measures
    before serious damage is done. By doing this, you can increase crop

    quality while using less pesticides. Machine learning can be used to enhance not
    only crop management but also animal management [13]. Machine learning

    algorithms can aid farmers in identifying potential health issues and optimizing
    feeding and breeding schedules by analyzing data on animal behavior, such

    as feeding patterns and mobility. Despite the potential advantages of integrating
    digital footprints in agriculture, there are still several issues that need to
    be

    resolved. Data privacy and security are one of the di¨culties. Farmers'' concerns
    about privacy and security may make them hesitant to share their data. The

    absence of technology and data connectivity in rural places is another problem.
    The technology required to gather and analyze data may not be available to

    farmers in remote locations.

    This is implemented through Information and Communication Technology (ICT). By
    boosting output, boosting e¨ciency, and lowering environmental impact,

    ICT has also signi¦cantly contributed to the transformation of agriculture. Using
    ICT in agriculture entails putting different digital technologies to use to help

    agricultural practices such as precision agriculture, smart farming, and farm
    management systems. With the help of ICT tools like GPS, sensors, and drones,

    farmers can monitor and analyze data on their crops and soil to make educated
    decisions about planting, fertilizing, and irrigation. Precision agriculture is
    a

    growing ¦eld. In doing so, crop yields are improved while the consumption of resources
    like water and fertilizer is reduced. Automating various farming tasks,

    such as planting, harvesting, and livestock monitoring, through the use of ICT
    tools like Internet of Things (IoT) sensors and machine learning algorithms is

    known as smart farming. This might lower labor costs, boost productivity, and
    enhance farming techniques'' precision.

    Page 5/20

    To handle several areas of farming, including inventory management, ¦nancial management,
    and supply chain management, farm management systems use

    ICT tools like cloud-based software applications. This can facilitate operational
    simpli¦cation and enhance decision-making for farmers. In general, the

    application of ICT in agriculture has the potential to revolutionize the sector
    by enabling more productive, e¨cient, and sustainable farming methods. The

    major contributions of the proposed work include the following:

    Improved Agricultural Practices: By introducing the use of machine learning algorithms
    in analyzing data gathered from various sources, such as weather

    data, soil samples, and crop yields, the study may contribute to improving ag
    practices. Improved crop yields may arise from farmers making better

    decisions about when to sow, irrigate, fertilize, and harvest their crops as a
    result.

    Increased Crop productivity: By ¦nding ideal circumstances for crop development,
    recognizing and forecasting probable crop illnesses, and suggesting

    the optimum fertilizers or pesticides employ, the implementation of machine learning
    techniques in agriculture may boost agricultural productivity.

    Environmental Sustainability: By using fewer pesticides and fertilizers, improving
    irrigation techniques, and lowering crop waste, the paper may also help

    to advance environmental sustainability.

    Information Transfer: By presenting a framework for how machine learning might
    be used in the agriculture industry, the study may help spread

    information and best practices. This might facilitate the understanding of the
    potential advantages of this technology by other researchers, practitioners,

    and policymakers and promote its implementation.

    In conclusion, the use of machine learning in agricultural practices is growing.
    Farmers can make better decisions, enhance yields, and save waste thanks to

    its capacity to handle massive amounts of data and produce predictive models.
    Machine learning will likely play a bigger part in the future of agriculture as
    a

    result of the ongoing improvement of technology.

    II. SYSTEMATIC REVIEW

    The proposed system paved way for smart farming. Using cutting-edge technology
    and data analytic methods, smart farming, sometimes referred to as

    precision agriculture, aims to enhance farming practices, boost agricultural production,
    and lessen waste and environmental effects. Optimizing farming

    operations like irrigation, fertilization, pest control, and crop management,
    entails the integration of numerous technologies like sensors, drones, GPS, and

    arti¦cial intelligence (AI). To meet the increasing demand for food in a world
    that is changing quickly, smart farming aims to make agriculture more effective,

    sustainable, and lucrative. Machine learning is one of these technologies; it
    uses algorithms to ¦nd patterns and insights in huge datasets. Machine learning

    in agriculture offers the potential to increase agricultural productivity and
    support farmers'' decision-making. This research examines how digital footprints
    can

    be used to improve agricultural practices and increase output. Smart agriculture
    practices use a variety of technology to streamline farming processes,

    increase yields, and cut waste. Among the most important technologies used in
    smart agriculture are:

    Precision farming: To maximize crop productivity and reduce resource wastage,
    such as water, fertilizer, and pesticides, precision farming is a

    sophisticated agricultural management method. Making educated judgments regarding
    planting, harvesting, and resource allocation entails gathering

    and analyzing data about the state of the soil, weather patterns, crop growth
    patterns, and other pertinent elements. The GPS is one of the most important

    technologies used in precision farming. (GPS). With the aid of GPS technology,
    farmers can follow their machinery in real-time

    and keep an eye on the location and crop productivity. Making informed judgments
    about where to sow crops and how much fertilizer and water to use can be

    done using this data. Additionally, GPS can be used to build ¦eld maps, which
    can be utilized to spot problem regions such as those with poor soil quality or

    low crop yield. Another increasingly popular precision farming method is Variable
    Rate Technology (VRT). Farmers are now able to apply the exact amount of

    pesticides, water, and fertilizer required for each ¦eld area thanks to VRT technology.
    By doing so, crop yields increase, waste is reduced, and money is saved

    [14]. As an illustration, if a farmer uses VRT to apply fertilizer, the system
    can modify the amount applied following the crop''s needs and the soil''s

    circumstances. The quality and production of the crops can be enhanced by making
    sure that the proper amount of fertilizer is applied to each section of the

    ¦eld. Remote sensing is a crucial tool for precision farming. The health of plants
    and the amount of moisture in the soil can be precisely determined using

    remote sensing technology like satellite photography and drones. Utilizing this
    information will allow you to keep an eye on crop development and spot ¦eld

    issues. The farmer can inspect the region and establish whether there is a pest
    infestation or a nutrient shortage, for instance, if a drone spots a patch of

    fading plants. Precision farming also includes the use of precision irrigation
    systems. These systems optimize water use by using sensors and software to

    make sure crops get the proper amount of water at the right time. Water waste
    is decreased, and crop quality is raised. For instance, a precision irrigation

    system can automatically change the amount of water sprayed to a ¦eld''s dry patches
    if it notices that the area is dry. In conclusion, precision farming

    techniques are revolutionizing the agricultural sector by utilizing cutting-edge
    technology and data analytics to maximize crop output, improve crop quality, cut

    down on resource waste, and boost e¨ciency and pro¦tability for farmers. Farmers
    may decide wisely about planting, harvesting, and resource allocation by

    gathering and analyzing data on soil quality, weather patterns, and crop growth
    patterns. This can result in better outcomes for farmers as well as the

    environment.

    Internet of Things (IoT): The Internet of Things (IoT) is playing a progressively
    signi¦cant part in agriculture, allowing farmers to enhance their resources

    and improve their yields while reducing costs and increasing e¨ciency. Here are
    some of how IoT is revolutionizing agriculture:

    1. IoT sensors can be utilized in agriculture to gather information on soil moisture,
    temperature, and nutrient levels as well as weather, plant development, and

    pest infestations. This information can be used to identify ¦elds that need attention,
    improve irrigation and fertilization schedules, and track crop health and

    growth rates.

    2. Livestock management: IoT sensors can be used to track a livestock''s movement,
    observe its eating and drinking routines, and spot any irregularities that

    would point to an ailment or injury. By using this information, feeding, and care
    schedules can be improved, disease outbreaks can be stopped, and overall

    Page 6/20

    animal health and production can be increased.

    3. Supply chain management: From the farm to the consumer, IoT sensors may be
    utilized to track the movement of crops and livestock. Farmers and

    distributors can bene¦t from this by maximizing logistics, cutting waste, and
    ensuring that goods are delivered to markets on schedule and in good shape.

    4. Monitoring environmental conditions: IoT sensors can be used to keep an eye
    on things like air and water quality, weather patterns, and climate change.

    This can assist farmers in selecting crops, timing plantings, and other agricultural
    practices that are in§uenced by environmental conditions.

    5. IoT technology can be utilized to create autonomous farming systems, where
    robots and drones can take care of chores like planting, watering, and

    harvesting crops. By doing so, the danger of human mistakes can be decreased while
    labor expenses can be decreased and productivity increased.

    Big Data Analytics: Big data analytics in agriculture have the potential to change
    how farmers and agricultural enterprises conduct their operations.

    Farmers may make more educated decisions that can result in higher yields, more
    e¨ciency, and lower costs by gathering, analyzing, and interpreting

    vast amounts of data. Predictive insights are one of the main advantages of big
    data analytics in agriculture. Farmers can estimate future yields with

    greater accuracy by looking back at historical data on weather patterns, soil
    characteristics, and crop performance. This can assist them in maximizing

    planting schedules, selecting the crops that are most suited for a speci¦c site,
    and modifying their agricultural techniques to increase productivity.

    Precision agriculture is another ¦eld where big data analytics can have a signi¦cant
    effect. Farmers can obtain comprehensive information about soil

    moisture, nitrogen levels, and other environmental aspects that in§uence crop
    growth by employing sensors and other data-collection equipment. Making

    decisions concerning irrigation, fertilizer application, and other farming-related
    issues will thus be easier with the aid of this knowledge [15]. Farmers may

    use big data analytics to spot possible issues like pest infestations or crop
    diseases early on. Farmers can spot warning signs early on and take action

    before the issue gets worse by analyzing data from sensors and other sources.
    Big data analytics in agriculture has the potential to greatly boost

    productivity, lower costs, and raise yields. However, it necessitates a large
    investment in technology, data analytic skills, and a readiness to adopt novel

    farming practices.

    Robotics: Precision farming, harvesting, monitoring, and maintenance are just
    a few of the agricultural applications where robots are becoming more and

    more crucial. Increased productivity is one of the key advantages of deploying
    robots in agriculture because they can operate around the clock and

    reliably perform repetitive jobs. Robots can also lower labor costs, boost productivity,
    and enhance sustainability. Robots are having a big impact on

    several industries, including precision farming. Robots can make accurate choices
    on planting, watering, fertilizing, and harvesting by gathering and

    analyzing data on crops, soil, weather, and other aspects. Thus, waste is decreased,
    environmental sustainability is increased, and crop yields are

    improved. Robots used for soil analysis, sensor-equipped drones, and self-driving
    tractors are a few examples of precision farming robots. Harvesting is

    another industry where robots are having a big impact. Robots are being developed
    to pick fruits and vegetables, which can be a labor-intensive operation,

    more effectively and precisely. Additionally, according to quality and other criteria,
    automated systems may sort and grade collected products. Due to their

    ability to operate continuously, harvesting robots can speed up the process of
    getting crops to market. The usage of robots in agriculture also extends to

    monitoring and maintenance. Robots can be employed to keep an eye on crop development
    and spot any issues like pests or illness. With the use of

    sensors and other cutting- edge technology, this can be done, allowing farmers
    to act immediately to avoid crop loss. To eliminate the need for manual

    labor, robots can also complete activities like irrigation, weeding, and trimming.
    In agriculture, autonomous vehicles are also becoming more prevalent

    [16]. It is possible to carry crops and supplies as well as plant and harvest
    crops using self-driving tractors and other autonomous vehicles. This lowers

    the demand for human labor and boosts productivity. Overall, the use of robots
    has the potential to revolutionize agriculture by increasing its productivity,

    sustainability, and e¨ciency. Robots can assist farmers in meeting the rising
    need for food while minimizing the impact of farming on the environment by

    lowering labor costs, boosting production, and improving sustainability. We may
    anticipate seeing even more cutting-edge agricultural robots produced as

    technology develops.

    Cloud Computing: Agriculture is seeing an increase in the importance of cloud
    computing, which bene¦ts farmers, academics, and other sector

    participants in a variety of ways. Cloud computing is assisting in enhancing e¨ciency,
    production, and sustainability in agriculture through data

    management and predictive analytics. Improved data management is one of the main
    advantages of cloud computing for agriculture. It can be di¨cult to

    store and analyse the vast volumes of data

    that farmers produce on their crops, soil, weather, and other variables [17].
    By providing a safe and expandable platform for data processing and storing, cloud

    computing offers a remedy. Farmers who use cloud-based solutions can access and
    analyse their data on any device, at any time, from anywhere. This

    enables them to decide on planting, fertilizing, and harvesting more wisely, which
    increase agricultural yields and decrease can waste. Additionally, the use of

    cloud computing improves communication between scientists and agronomists. Researchers
    can collaborate globally with co-workers and collaborators by

    putting study data on the cloud and making it accessible to them. This might hasten
    agricultural research and development and result in fresh ideas for

    managing crops, controlling pests, and other things. Predictive analytics is a
    bene¦t of cloud computing for agriculture as well. Farmers can utilize cloud-

    based technologies to forecast agricultural yields by examining data on crop growth,
    weather patterns, and other variables. They can then modify their

    planting and harvesting schedules as necessary. The risk of crop loss due to weather
    or other reasons can be reduced and resources may be used more

    effectively as a result. In terms of sustainability, cloud computing also has
    advantages. Farmers can use cloud-based solutions to manage data and cut down

    on the amount of paper and other physical resources required for data storage
    and analysis [18]. Farmers may also cut waste and make better use of

    resources by employing predictive analytics to optimize agricultural yields. Overall,
    cloud computing is playing a signi¦cant role in agriculture, providing

    advantages in terms of sustainability, cooperation, data management, and predictive
    analytics. We may anticipate seeing even more cutting-edge cloud-based

    technologies created for use in agriculture as technology develops, assisting
    farmers to increase output, decrease waste, and satisfy the rising need for food.

    Blockchain technology: To increase transparency, e¨ciency, and sustainability
    throughout the supply chain, blockchain technology is rapidly being

    applied in agriculture. Blockchain technology can revolutionize business and provide
    farmers, consumers, and other stakeholders with a wide range of

    advantages, from tracking food safety to expediting transactions. Improved traceability
    is one of the blockchain''s main advantages in agriculture.

    Farmers may follow their products from farm to table using blockchain-based technologies,
    giving consumers more details about the food''s origins and

    production processes [19]. Permitting quicker and more precise recalls in the
    event of contamination or another problem can assist boost consumer and

    Page 7/20

    farmer trust while also enhancing food safety. E¨ciency gains are another advantage
    of blockchain in agriculture. Farmers may cut down on the need for

    middlemen and streamline the buying and selling of products by adopting blockchain-based
    technologies to coordinate transactions. Better outcomes for

    both farmers and customers may result from this as it can lower costs and improve
    supply chain transparency. Blockchain can also aid increase

    agricultural sustainability. Blockchain-based platforms can encourage more sustainable
    practices and decrease waste by enabling farmers to track the

    environmental impact of their products. Blockchain can also encourage more sustainable
    production methods and lessen agriculture''s impact on the

    environment by giving consumers more information about the sustainability of the
    items they purchase. Smart contracts are one application of

    blockchain in agriculture. The details of the agreement between the buyer and
    seller are directly encoded into lines of code in smart contracts, which are

    self-executing contracts. This reduces the need for middlemen and can lower the
    possibility of fraud or disagreements. Smart contracts can be used to

    automate transactions between farmers and purchasers in the agricultural industry,
    guaranteeing that both sides receive fair compensation and lowering

    the possibility of payment delays or disagreements. By enhancing transparency,
    e¨ciency, and sustainability throughout the supply chain, blockchain

    technology has the potential to fundamentally change the agricultural industry
    [20]. We may anticipate more e¨cient and sustainable agricultural

    practices, which will bene¦t farmers, consumers, and the environment as more producers,
    retailers, and farmers implement blockchain-based systems.

    III. OVERVIEW OF MACHINE LEARNING

    Machine learning (ML) is increasingly being used to improve agricultural practices,
    by enabling farmers and agriculture professionals to optimize operations,

    increase productivity, and reduce waste. The use of ML in agriculture can include
    tasks such as crop yield prediction, pest and disease detection, precision

    agriculture, and livestock management. By leveraging advanced algorithms and data
    analytics, ML is helping to revolutionize the agricultural sector, improving

    the e¨ciency and sustainability of food production. This is implemented using
    an optimization algorithm. Random Forest is a machine learning algorithm

    that can be used to enhance productivity in agriculture by analyzing large amounts
    of data and providing valuable insights into crop production. The

    algorithm works by creating multiple decision trees, each of which independently
    analyzes a subset of the data [21]. The results of these trees are then

    combined to make a more accurate prediction. Certainly, the following ¦ve categories
    are employed in agriculture to boost production using machine learning

    as shown below in Fig. 2:

    A brief overview of machine learning:

    The development of models to mimic human learning and decision-making can be credited
    with giving rise to machine learning in the 1940s and 1950s. The

    creation of the perceptron algorithm by Frank Rosenblatt in 1957 is one of the
    early instances of machine learning. A particular kind of arti¦cial neural network

    known as a perceptron was capable of learning to divide inputs into two categories
    using a set of weights and biases. The development of decision trees and

    rule-based systems in the 1960s and 1970s contributed to the advancement of machine
    learning research [22]. These techniques were used to create expert

    systems that can decide based on a set of guidelines and information. Machine
    learning began concentrating on statistical techniques and algorithms in the

    1980s and 1990s, like logistic regression and linear regression. Classi¦cation,
    regression, and clustering challenges all used these techniques. Machine

    learning regained prominence in the 2000s with the rise of big data and improvements
    in computing power [23]. Numerous new methods, including deep

    learning and reinforcement learning, have been created and used in a variety of
    ¦elds, such as autonomous cars, natural language processing, and picture

    identi¦cation. Machine learning is still a discipline that is quickly developing
    today, with new methods and programs being created and used in sectors

    including healthcare, ¦nance, and cyber security.

    Following are the steps that machine learning uses to operate:

    1. Data Gathering: Gathering pertinent data from diverse sources is the initial
    stage in machine learning. Depending on the issue at hand and the desired

    solution, this data may be presented in either structured or unstructured form.

    2. Data processing: After the data has been gathered, it must be prepared for
    analysis. To prepare the data for use by machine learning algorithms, the data

    must be cleaned, missing values must be removed, and any outliers must be removed.

    3. Feature extraction: The process of selecting and engineering relevant features
    from the data aims to provide new features that may be more advantageous

    for machine learning algorithms.

    4. Model Selection: After the data is ready, the type of problem being tackled
    is taken into consideration and a suitable machine learning technique is selected.

    supervised learning, unsupervised learning, and reinforcement learning are some
    examples of this.

    5. Training the Model: Using the prepared data, the selected machine learning
    method is then practiced. To reduce the error between the projected output and

    the actual output, the algorithm makes adjustments to its parameters as it learns
    from the data during training.

    6. Model Evaluation: After the model has been trained, its generalizability to
    fresh, unexplored data is assessed using a variety of performance criteria.

    7. Model tuning: Depending on the results of the evaluation, the performance of
    the model may need to be improved by modifying its hyper parameters or by

    employing a new algorithm.

    8. Model deployment: The model can be deployed in a production environment to
    make predictions on new data after it has been trained and tweaked.

    9. Monitoring and Maintenance: To ensure that machine learning models continue
    to perform well over time, it is necessary to monitor and maintain them.

    This entails keeping an eye out for data changes, frequently retraining the model,
    and making adjustments as necessary.

    Machine learning is an iterative process, thus when the model is created and applied,
    some steps might need to be repeated or improved. Depending on the

    type of problem being handled, numerous metrics can be used to assess how well
    machine learning models perform. The main objective of assessing a

    machine learning model''s performance is to ¦nd out how effectively it generalizes
    to new, untried data. Random Forest is a machine learning algorithm that

    can be used in agriculture to predict crop yield. It works by creating an ensemble
    of decision trees, where each tree is trained on a random subset of the data

    and a random subset of the features [24]. In the context of agriculture crop yielding,
    the algorithm can be used to predict the yield of a particular crop based on

    Page 8/20

    a variety of input variables such as weather patterns, soil quality, seed varieties,
    and management practices. To use the algorithm for crop yield prediction,

    historical data on crop yields and the corresponding input variables are used
    to train the model. Once the model is trained, it can be used to predict the yield
    of

    a new crop based on the input variables provided. The random forest algorithm
    is particularly useful for agriculture because it can handle a large number of

    input variables and can also capture complex interactions between the variables
    that may be di¨cult to identify using traditional statistical methods [25].

    Additionally, the algorithm is robust to noisy data and missing values, making
    it suitable for real-world agricultural data. Overall, the random forest algorithm

    can be a powerful

    tool for predicting crop yields in agriculture and can help farmers make informed
    decisions about crop management and resource allocation. Here are a few

    typical metrics for assessing machine learning models:

    Accuracy: This gauges the percentage of instances that were correctly classi¦ed
    out of all the instances. For classi¦cation issues, it is frequently

    employed.

    Precision is the percentage of all accurately identi¦ed positive instances (also
    known as true positives) among all positive predictions. It is bene¦cial in

    situations when the expense of false positives is large, like in medical diagnostics.

    Recall: In case you forgot, this quanti¦es the percentage of real positives among
    all positive results. It helps identify fraudulent transactions when the cost

    of false negatives is signi¦cant.

    F1 Score: When false positives and false negatives are equally essential, this
    score—which is the harmonic mean of precision and recall—is helpful.

    Mean Squared Error (MSE): For regression issues, this statistic is frequently
    utilized. The average squared difference between the expected and actual

    values is what is measured.

    Since it is expressed in the same units as the target variable, the Root Mean
    Squared Error (RMSE), which is the square root of the MSE, is a more

    understandable statistic.

    R-squared (R2): This indicator shows how much of the target variable''s variance
    the model can account for. A better ¦t is indicated by higher values,

    which range from 0 to 1.

    Depending on the problem being solved, it is crucial to utilize the right evaluation
    criteria when assessing a machine learning model''s performance. To make

    sure the model can generalize to new data, it''s also critical to assess the model''s
    performance on a different test set. The trade-off between model complexity

    and performance is also crucial to take into account because more complex models
    may over¦t training data while underperforming on fresh data [26].

    In general, machine learning models can be divided into three groups based on
    the kind of learning that they employ:

    Supervised Learning: In supervised learning, the target variable is known and
    the computer is trained on a labelled dataset. To make precise predictions

    on fresh, unforeseen data, the model must learn the link between the input features
    and the target variable.

    Unsupervised Learning: In unsupervised learning, the goal variable is unknown
    and the machine is trained on an unlabelled dataset. The model''s objective

    is to identify structures, correlations, and patterns in the data without receiving
    any direct instruction or feedback.

    Reinforcement Learning: In reinforcement learning, a machine learns by doing things
    incorrectly, interacting with its environment, and receiving feedback

    in the form of rewards or punishments. The ideal policy, which maximizes the cumulative
    reward over time, is what the model seeks to learn. In robots,

    video games, and control systems, this kind of learning is frequently employed.

    The following categories are used to enhance productivity in agricultural sectors
    evolving machine learning techniques:

    Crop prediction: Choosing which crops to grow, when to plant them, and how to
    maximize their production all depend on crop prediction, which is a crucial

    component of agriculture. To ¦nd trends and create precise predictions, machine
    learning (ML) approaches have shown considerable promise in the ¦eld of

    crop forecasting. The gathering and pre-processing of data is the ¦rst stage in
    crop prediction using ML. This entails compiling information on many

    elements, such as weather patterns, soil quality, and the type of crop planted,
    that may have an impact on crop development [27]. The data is then pre-

    processed to clean out any errors or noise and to make sure that it is in a format
    that ML algorithms can use. The pre-processed data is then subjected to ML

    algorithms to ¦nd patterns and generate predictions. Several ML methods, such
    as decision trees, random forests, support vector machines (SVM), and

    arti¦cial neural networks, can be applied to crop prediction. (ANN). Each algorithm
    has its advantages and disadvantages, and the best one to use depends on

    the particulars of the problem at hand. The ML algorithm can be used to generate
    predictions on new data after being trained on existing data. For instance, a

    farmer can input information on the weather, soil quality, and other parameters
    to predict the yield of a speci¦c crop in a given season, and the ML algorithm

    will then do so [28]. The ability to maximize crop yield and minimize waste is
    one of the main advantages of utilizing ML for crop prediction. ML algorithms

    can detect the ideal circumstances for crop growth and suggest adjustments to
    planting plans, irrigation schedules, and other practices to maximize yield by

    analysing data on the elements that affect crop growth. The ability to lower the
    risk of crop failure for farmers is another advantage of utilizing ML for crop

    prediction. Farmers can take proactive steps to minimize or lessen the impact
    of crop failure by utilizing pest-resistant crops or modifying planting dates
    by

    forecasting the possibility of crop failure based on factors such as weather conditions,
    soil quality, and disease outbreaks. In conclusion, crop prediction using

    machine learning has the potential to completely transform the agricultural industry
    by empowering farmers to make data-driven decisions that maximize

    crop output and minimize risk [29]. To accurately anticipate crop production and
    assist farmers in choosing planting techniques, irrigation schedules, and

    other practices that in§uence crop growth, machine learning (ML) systems analyse
    vast volumes of data and look for trends. It is expected that ML technology

    will become more and more signi¦cant as it develops in agriculture and other sectors
    of the economy.

    Plant disease detection: Farmers and the agriculture sector are very concerned
    about plant diseases. They can lower crop quality and result in considerable

    yield losses, which might result in ¦nancial losses. To reduce these losses, early
    plant disease detection and prevention are essential. To forecast plant

    Page 9/20

    diseases and provide prompt intervention to stop their spread, machine learning
    approaches can be helpful. Data collection is the ¦rst step in the machine

    learning process used to predict plant diseases. This entails gathering information
    on the plant species, its growth stage, the type of disease, and climatic

    factors including temperature, humidity, and rainfall. Sensors, drones, satellite
    photography, and human observation can all be used to gather data [30]. Once

    the data is collected, it is pre-processed to remove noise, handle missing values,
    and normalize the data. Feature engineering is then performed to extract

    relevant features from the data that can be used to train the machine learning
    model. The extracted features may include plant morphology, texture, color, and

    shape, as well as environmental factors such as temperature and humidity. Various
    machine learning algorithms can be used for plant disease prediction,

    including decision trees, random forests, support vector machines, and deep learning
    models such as convolutional neural networks. The choice of algorithm

    depends on the size and complexity of the dataset and the accuracy required for
    prediction. The trained model is then tested on a separate dataset to evaluate

    its accuracy and performance. This is done by splitting the data into training
    and testing sets, with the majority of the data used for training the model and
    a

    smaller subset used for testing. A map displaying the location and severity of
    the disease or an alert system notifying farmers when the sickness is found are

    just two examples of how the model''s results can be displayed to the user. Using
    this knowledge, preventive steps can then be taken, such as applying

    fungicides or removing affected plants [31]. Machine learning- based plant disease
    prediction has several advantages over conventional disease detection

    techniques. By enabling early illness identi¦cation, it can lessen the need for
    expensive and time-consuming manual checks. Additionally, it can assist farmers

    in deciding when and where to apply fungicides and other

    treatments, minimizing the amount of chemicals needed and the negative effects
    on the environment. In conclusion, predicting plant diseases using machine

    learning is a potential strategy that can help to lower crop losses and improve
    agricultural practices. The right machine learning algorithms must be chosen,

    together with careful data gathering, pre-processing, and feature engineering.
    Machine learning can deliver precise and timely predictions of plant illnesses

    with the correct methods and equipment, allowing farmers to take preventative
    measures to save their crops [32]. In conclusion, predicting plant diseases

    using machine learning is a potential strategy that can help to lower crop losses
    and improve agricultural practices. The right machine learning algorithms

    must be chosen, together with careful data gathering, pre- processing, and feature
    engineering. Utilizing the appropriate methods and resources, machine

    learning.

    Weed detection: Weed detection using machine learning is a rapidly developing
    ¦eld that has the potential to revolutionize the way we manage agriculture. In

    traditional agriculture, weed management is time- consuming and labour-intensive,
    often requiring the use of herbicides, which can have negative

    environmental impacts. Machine learning-based weed detection offers an alternative
    solution, using computer vision and machine learning algorithms to

    identify and map weeds in real time. One of the key advantages of machine learning-based
    weed detection is its ability to process large amounts of data

    quickly and accurately. This is achieved by training machine learning algorithms
    on large datasets of images of crops and weeds [33]. These datasets are

    typically labelled to identify which pixels correspond to crops and which correspond
    to weeds, allowing the machine learning algorithm to learn to distinguish

    between the two. The training data is fed into a machine learning model, which
    then uses this data to create a model that can recognize patterns in new

    images. When new images of crops are input into the model, the machine learning
    algorithm can quickly identify which pixels correspond to weeds and which

    correspond to crops. This information can then be used to create a map of weed
    distribution across a ¦eld. Several challenges must be addressed when

    developing a machine learning-based weed detection system. One of the key challenges
    is ensuring that the system is robust to changes in lighting conditions

    and other environmental factors. This can be achieved by using a combination of
    image processing techniques and machine learning algorithms to pre-

    process images before they are input into the weed detection model [34]. Another
    challenge is ensuring that the system can generalize to new images that it

    has not seen before. This can be achieved by using a combination of data augmentation
    techniques and transfer learning, which involves using a pre-trained

    model as a starting point for training a new model on a smaller dataset. Several
    different types of machine learning algorithms can be used for weed

    detection, including convolutional neural networks (CNNs), support vector machines
    (SVMs), and decision trees. Each of these algorithms has its strengths

    and weaknesses, and the choice of algorithm will depend on the speci¦c requirements
    of the application [35]. In conclusion, weed detection using machine

    learning has the potential to revolutionize agriculture by providing a fast, accurate,
    and environmentally friendly alternative to traditional weed management

    techniques. While there are still several challenges that must be addressed, advances
    in computer vision and machine learning are making this technology

    increasingly accessible to farmers and researchers alike.

    Crop yield prediction: Crop yield prediction is a crucial task in agriculture
    as it helps farmers and agricultural experts to make informed decisions about
    crop

    production, resource management, and market supply. Traditionally, crop yield
    prediction has been done using various statistical models and methods, but

    with the recent advancements in machine learning, it has become possible to predict
    crop yields more accurately using data-driven approaches. Machine

    learning has proven to be a valuable tool in agriculture and has the potential
    to revolutionize the industry by making it more e¨cient and sustainable. The

    main objective of crop yield prediction using machine learning is to develop models
    that can accurately estimate the yield of a particular crop in a speci¦c

    area based on various factors such as weather conditions, soil properties, and
    agricultural practices. Machine learning models for crop yield prediction

    typically use large datasets of historical weather and crop data to train the
    model and make predictions for the future. One of the most common approaches

    for crop yield prediction is supervised learning, where the model is trained using
    labelled data, which includes both input features (such as temperature,

    rainfall, and soil type) and

    the corresponding crop yield. Supervised learning algorithms such as regression,
    decision trees, and neural networks are commonly used for crop yield

    prediction [36]. These algorithms can learn the relationship between the input
    features and the output variable (crop yield) and make accurate predictions for

    new data. Another approach for crop yield prediction is unsupervised learning,
    where the model is trained on unlabelled data to identify patterns and

    relationships in the data. Clustering algorithms such as k-means clustering and
    hierarchical clustering are commonly used in unsupervised learning for crop

    yield prediction. These algorithms can group similar data points based on their
    characteristics and help identify areas with similar crop yield patterns. In recent

    years, deep learning algorithms such as convolutional neural networks (CNNs) and
    recurrent neural networks (RNNs) have been applied to crop yield

    prediction. CNNs are particularly useful for analysing spatial data such as satellite
    images to identify crop types, growth stages, and yield potential [37]. RNNs

    are well-suited for time series data such as historical weather data, which can
    be used to predict future weather patterns and their impact on crop yields.

    Page 10/20

    One of the key challenges in crop yield prediction using machine learning is the
    availability and quality of data. Historical weather and crop data may not be

    readily available or may be incomplete, making it di¨cult to train accurate models.
    Moreover, the data may be biased or outdated, leading to inaccurate

    predictions. Another challenge is the interpretation of the machine learning models,
    which can be complex and di¨cult to understand for non-experts.

    Therefore, it is important to have a clear understanding of the data and the machine
    learning models used for crop yield prediction. In conclusion, crop yield

    prediction using machine learning is a promising approach to improving the e¨ciency
    and sustainability of agriculture. With the right data and machine

    learning models, it is possible to accurately predict crop yields and make informed
    decisions about crop production and resource management. However, there

    are challenges in data availability, model interpretation, and practical implementation
    that need to be addressed to fully realize the potential of machine

    learning in agriculture.

    Farm management: Farm management is a complex process that involves numerous tasks,
    such as planting, harvesting, pest control, and irrigation, among

    others. With the advent of machine learning and arti¦cial intelligence, farmers
    can use technology to optimize their operations and improve their yield.

    Machine learning is a subset of arti¦cial intelligence that enables computer systems
    to learn from data and make predictions or decisions based on that data.

    In the context of farm management, machine learning can be used to analyze various
    data sources, including weather patterns, soil conditions, and crop

    growth, among others. One way in which machine learning can be used in farm management
    is by analysing satellite imagery to monitor crop health and

    growth. By analyzing changes in vegetation density and color, machine learning
    algorithms can predict crop yields, identify areas of the farm that require

    attention, and provide recommendations for irrigation and fertilization [38].
    Another application of machine learning in farm management is in predicting

    weather patterns and their impact on crop growth. By analyzing historical weather
    data and current weather patterns, machine learning algorithms can predict

    future weather patterns and provide farmers with recommendations on how to adjust
    their operations to minimize the impact of adverse weather conditions.

    In addition, machine learning can be used to optimize irrigation and fertilization
    schedules by analyzing soil data and plant growth patterns. By analyzing the

    nutrient content of the soil and the growth rate of the plants, machine learning
    algorithms can provide farmers with recommendations on the optimal amount

    and timing of irrigation and fertilization. Machine learning can also be used
    to predict pest and disease outbreaks and provide farmers with recommendations

    on how to prevent or mitigate them [39]. By analyzing historical data on pest
    and disease outbreaks, as well as current weather and soil conditions, machine

    learning algorithms can predict the likelihood of an outbreak and provide farmers
    with recommendations on how to prevent or mitigate it. In conclusion,

    machine learning has the potential to revolutionize farm management by providing
    farmers with valuable insights and recommendations on how to optimize

    their operations and improve their yield. By analyzing various data sources, including
    weather patterns, soil conditions, and crop growth, machine learning

    algorithms can provide farmers with

    recommendations on irrigation, fertilization, pest control, and other key aspects
    of farm management, enabling them to make more informed decisions and

    improve their bottom line [40]. There are several advantages of using machine
    learning in agriculture, some of which include:

    Increased E¨ciency: Machine learning algorithms can help farmers and other agriculture
    stakeholders optimize their operations by reducing the time and

    effort required to perform tasks such as crop monitoring, disease detection, and
    yield forecasting. This can help to increase overall e¨ciency and reduce

    costs.

    Improved Crop Yields: Machine learning algorithms can help farmers predict crop
    yields by analyzing a wide range of data, including weather patterns, soil

    moisture, and historical crop yields. By predicting yields more accurately, farmers
    can make better decisions about when to plant and harvest crops, leading to

    increased yields.

    Enhanced Resource Management: Machine learning algorithms can help farmers manage
    resources such as water, fertilizer, and pesticides more e¨ciently.

    By analyzing data on crop growth and soil conditions, these algorithms can optimize
    resource usage, reducing waste and maximizing productivity.

    Early Disease Detection: Machine learning algorithms can analyze data from sensors
    and other sources to detect signs of plant diseases early. This allows

    farmers to take proactive measures to prevent the spread of disease, minimizing
    the impact on crop yields.

    Precision Agriculture: Machine learning can help farmers to target speci¦c areas
    of their ¦elds that require more attention, allowing for more precise

    application of resources such as fertilizer and pesticides. This can reduce waste
    and improve overall crop health.

    Overall, the use of machine learning in agriculture has the potential to revolutionize
    the way that crops are grown and managed, leading to more e¨cient,

    sustainable, and productive farming practices.

    IV. PERFORMANCE ANALYSIS OF THE PROPOSED SYSTEM

    The proposed system involves machine learning with random forest optimization
    to detect various parameters such as crop yield, disease prediction, weed

    detection, etc. The proposed system is shown in ¦gure 3.

    1. Data gathering: Data gathering is an essential step in machine learning, and
    it is particularly crucial when building a model to detect crop yield using the

    random forest algorithm. The quality and quantity of data used to train the model
    can signi¦cantly affect its accuracy and ability to make accurate

    predictions. To gather data for crop yield detection using random forests, one
    must collect data on various environmental factors that affect crop growth and

    yield, such as soil type, temperature, rainfall, sunlight, and fertilizers used.
    The data must be collected from different farms, regions, and seasons to ensure
    a

    diverse dataset. Once the data is collected, it must be preprocessed, cleaned,
    and prepared for training the random forest model. The dataset should be split

    into training, validation, and testing sets to prevent over¦tting and ensure the
    model''s generalizability. The random forest algorithm is a powerful tool for
    crop

    yield detection as it can handle high- dimensional datasets with non-linear relationships
    between variables. It works by creating multiple decision trees and

    combining them to make predictions. Each tree is trained on a random subset of
    the features and a subset of the training data to reduce the risk of over¦tting

    Page 11/20

    [41]. Overall, data gathering is a critical step in machine learning using the
    random forest algorithm to detect crop yield. By collecting high-quality and diverse

    data, preprocessing it, and training the model, we can accurately predict crop
    yield and optimize farming practices for improved crop production. The predicted

    data set is observed from a village to detect the crop yield as shown in ¦gure
    4. At the settlement, the information is taken into account as latitude varies
    from

    place to place. To operate the system, information on the crop, the climate in
    a speci¦c village, and the region are three necessary elements.

    1. Data processing: In machine learning, data processing is a critical stage that
    involves transforming raw data into a format that can be easily analysed by

    algorithms. The data processing stage typically includes several steps, such as
    data cleaning, data transformation, feature extraction, and data splitting. Data

    cleaning involves removing any irrelevant or duplicate data, correcting errors,
    and handling missing data. Data transformation involves scaling, normalizing,
    or

    encoding the data to ensure that it is in a standardized format. Feature extraction
    involves selecting the most relevant features that have a strong correlation

    with the target variable [42]. After completing these steps, the data is usually
    divided into training, validation, and testing sets. The training set is used
    to train

    the machine learning model, while the validation set is used to evaluate the model''s
    performance during training. Finally, the testing set is used to evaluate the

    model''s performance on unseen data. Overall, the data processing stage is essential
    for ensuring the quality of the data and the accuracy of the machine

    learning model [43]. Proper data processing can lead to better model performance,
    faster training times, and more accurate predictions. The testing and

    training phase is implemented in Matlab as shown below:

    2. Feature extraction: A crucial stage in machine learning is feature extraction,
    which entails choosing and manipulating pertinent information from raw data

    to produce instructive features for predictive models. Features like soil type,
    weather patterns, and crop health can all be identi¦ed and studied using feature

    extraction techniques to increase production in agriculture [44]. Some common
    techniques for feature extraction in agriculture include:

    Remote sensing: Using satellites or drones to capture images of crop ¦elds, which
    can then be analysed to extract features such as vegetation index,

    crop growth stage, and soil moisture.

    Sensor data: Collect data from sensors such as soil moisture sensors, temperature
    sensors, and weather stations to extract features such as

    temperature, humidity, rainfall, and soil nutrients.

    Data fusion: Combining data from multiple sources, such as remote sensing and
    sensor data, to create more informative features for predictive modelling.

    Data pre-processing: Transforming raw data into more useful features, such as
    scaling, normalization, and dimensionality reduction techniques. By using

    feature extraction techniques in agriculture, machine learning models can be trained
    to predict crop yields, identify areas for improvement, and optimize

    resource allocation, leading to improved productivity and pro¦tability for farmers.

    3. Model selection: Model selection is an important step in machine learning in
    agriculture to improve productivity. It involves choosing the most appropriate

    machine learning algorithm and model for a speci¦c agricultural problem. Here
    are some key considerations for model selection in agriculture:

    Data quality: The quality and quantity of data available are critical for selecting
    an

    appropriate machine learning algorithm and model. Ensure that the data is representative, unbiased, and of high quality.

    Complexity: Agricultural systems can be complex, with many variables and interactions. Consider the
    complexity of the problem when selecting a model.

    Simple models such as linear regression may work for simple problems, while more
    complex models such as neural networks or decision trees may be needed

    for more complex problems. Interpretability: Interpretability is important in agriculture, as it allows for the understanding of the model''s decision-

    making process. More interpretable models, such as decision trees, may be preferred
    over less interpretable models such as neural networks.

    Performance: The performance of the model is a key consideration. Evaluate the model''s performance
    using appropriate metrics such as accuracy, precision,

    recall, and F1 score. Consider the trade-off between model performance and interpretability.

    Scalability: Consider the scalability of the model, as agricultural systems often
    involve large amounts of data. Models that can handle large volumes of data,

    such as deep learning models, may be preferred. Overall, selecting the right machine
    learning model can help improve agricultural productivity by enabling

    better decision-making, predicting crop yields, detecting disease outbreaks, and
    optimizing resource allocation. Here random forest algorithm is initiated to

    enhance productivity.

    4. Optimization algorithm: Random Forest is a machine-learning algorithm that
    can be used for classi¦cation and regression tasks. While it can be applied to

    various ¦elds, including agriculture, it is not speci¦cally designed to extract
    maximum yield in agriculture. That being said, Random Forests can be used in

    agriculture to make predictions based on input variables such as soil type, weather
    conditions, and crop variety. For example, a Random Forest model could be

    Page 12/20

    trained to predict crop yield based on factors such as temperature, rainfall,
    soil pH, and fertilizer application rates. The model could then be used to optimize

    crop management decisions, such as adjusting irrigation schedules, choosing optimal
    planting dates, and selecting the most effective fertilizers and

    pesticides. To train a Random Forest model for agricultural applications, data
    must ¦rst be collected and pre-processed [45]. This might involve gathering data

    on crop yield, weather conditions, soil properties, and other relevant factors
    from past growing seasons. Once the data is collected, it can be split into training

    and testing sets to evaluate the performance of the model. The Random Forest algorithm
    works by creating a multitude of decision trees, each trained on a

    subset of the data. The algorithm randomly selects a subset of the available features
    at each split point, which helps to reduce over¦tting and improve

    generalization. During training, the algorithm measures the accuracy of each decision
    tree on the training set and uses this information to build a consensus

    model that combines the predictions of all the trees as shown below:

    In this code, we ¦rst load the dataset and split it into training and testing
    sets using the divider and function. The optimized hyperparameters (x_opt) are
    used

    to train the ¦nal random forest model using the TreeBagger function. The model
    is then used to predict the test set and the accuracy of the model is calculated

    as the RMSE. Note that this code assumes that the crop_yield_data.mat ¦le contains
    a matrix X of features and a vector y of corresponding yields. The

    hyperparameters being optimized are the minimum leaf size (x(1)), the number of
    variables to sample (x(2)), and the maximum number of splits (x(3)). Once

    the model is trained, it can be used to predict crop yield based on input variables
    such as weather data and soil properties [46]. The accuracy of the model can

    be evaluated using the testing set, and adjustments can be made to improve its
    performance. In summary, Random Forests can be used in agriculture to

    predict crop yield and optimize management decisions. To train a Random Forest
    model for agricultural applications, data must be collected and pre-

    processed, and the model must be evaluated using a testing set as represented
    in ¦gure 5.

    5. Model evaluation: The model evaluation process in machine learning (ML) involves
    assessing the performance of a trained model on a set of test data to

    determine its accuracy, precision, recall, and other evaluation metrics. In the
    context of agriculture, one popular ML algorithm is the Random Forest algorithm,

    which is commonly used for tasks such as crop yield prediction, pest detection,
    and soil quality assessment. Here is a step-by-step process for evaluating a

    Random Forest model in agriculture:

    Collect and preprocess data: Collect data related to the agricultural problem
    at hand, such as soil data, weather data, crop growth data, etc. Preprocess

    the data by removing missing values, scaling features, and converting categorical variables into
    numeric form.

    Split the data: Split the preprocessed data into two parts: training data and
    test data. The training data is used to train the Random Forest model, while the
    test

    data is used to evaluate the model''s performance.

    Train the model: Train the Random Forest model on the training data. The model
    should be tuned to obtain the best results.

    Test the model: Test the model on the test data. The test data should be completely
    separate from the training data to avoid bias.

    Evaluate the model: Evaluate the performance of the model using various evaluation
    metrics such as accuracy, precision, recall, F1 score, confusion matrix,

    etc. These metrics will provide insight into how well the model is performing
    and whether it is suitable for the given agricultural problem. Adjust the model: 

    Based on the evaluation results, adjust the model parameters or the preprocessing
    steps to improve the model''s performance.

    Finalize the model: After obtaining satisfactory results, ¦nalize the model and
    deploy it for practical use. Overall, the model evaluation process in MI using

    Random Forest in agriculture involves collecting and preprocessing data, splitting
    it into training and test data, training and testing the model, evaluating its

    performance using various metrics, and adjusting the model as needed to achieve
    optimal results.

    Page 13/20

    6. Performance analysis and outcome: The proposed system is applied in Matlab
    Simulink to assess the optimum results as demonstrated in ¦gure 6.

    The confusion matrix is a performance evaluation tool that is widely used in machine
    learning to measure the accuracy of a model''s predictions. In the context

    of a random forest algorithm for agricultural yield enhancement, a confusion matrix
    can be used to evaluate the accuracy of the model in predicting crop yield

    based on a set of input variables such as soil type, weather conditions, and crop
    variety. 

    The confusion matrix is a table that summarizes the performance of a classi¦cation
    model by comparing the actual and predicted values [47]. 

    It consists of four different components: true positive (TP), true negative (TN),
    false positive (FP), and false negative (FN1).

    In the context of agricultural yield enhancement, these components can be de¦ned
    as follows:

    True positive (TP): The number of times the model correctly predicted a high-yield crop when the
    actual yield was high.

    True negative (TN): The number of times the model correctly predicted a low-yield crop when the
    actual yield was low.

    False positive (FP): The number of times the model incorrectly predicted a high-yield crop when the
    actual yield was low.

    False negative (FN): The number of times the model incorrectly predicted a low-yield crop when the
    actual yield was high.

    The confusion matrix can be used to calculate several performance metrics for
    the model, such as accuracy, precision, recall, and F1-score. These metrics can

    provide insights into how well the model is performing and can help identify areas
    for improvement.

    V. COMPARATIVE ANALYSIS

    TABLE I: Machine learning algorithm

    Machine

    learning

    algorithm

    Algorithm description

    CNN

    The Convolutional Neural Network (CNN) algorithm has shown great potential in
    various ¦elds, including agriculture crop yield prediction.

    CNNs can learn complex patterns and relationships in high- dimensional data, making
    them suitable for analyzing crop images and

    predicting yield. In agriculture crop yield prediction, CNNs are used to analyze
    crop images captured by drones or other aerial vehicles. The

    images are pre-processed and then fed into the CNN to extract features and learn
    patterns. The CNN then makes predictions on the yield

    based on the features extracted from the images [48]. CNNs have been used to predict
    crop yields for various crops, including maize, wheat,

    and rice. The predictions made by CNNs are accurate and reliable and can help
    farmers make informed decisions about crop management.

    Overall, CNNs have the potential to revolutionize agriculture by providing accurate
    crop yield predictions, which can help farmers optimize

    crop management practices and increase crop yields.

    SVM

    Support Vector Machines (SVM) is a machine learning algorithm that can be used
    in agriculture to predict crop yield. SVM is a supervised

    learning technique that is used for classi¦cation and regression analysis. It
    works by ¦nding the best decision boundary that separates the

    data points into different classes or predicts the value of the target variable.
    In agriculture, SVM can be used to predict crop yield based on

    various input features such as soil quality, weather conditions, and farming practices.
    The algorithm can analyze these features and create

    a model that can predict the crop yield for a given set of input data [49]. The
    use of SVM in agriculture can help farmers make informed

    decisions about planting and harvesting crops. By analyzing historical data, SVM
    can predict the optimal time to plant a crop, the best

    fertilizers to use, and the ideal harvesting time. This can help farmers increase
    their crop yields and reduce the risk of crop failure.

    RNN

    Recurrent Neural Network (RNN) is a type of deep learning algorithm that can be
    used in agriculture to predict crop yields. RNNs are

    particularly useful for time-series data, where the data points are collected
    at regular intervals, such as daily or monthly. In agriculture, RNNs

    can be used to analyze a variety of factors that affect crop yields, including
    weather patterns, soil moisture, and fertilizer usage. By

    analyzing this data over time, RNNs can identify patterns and make predictions
    about future crop yields.

    RNNs work by processing inputs sequentially, with each new input being in§uenced
    by the previous inputs [50]. This allows the model to

    capture the temporal dependencies between the data points, which is important
    in time-series analysis. The output of an RNN is a prediction

    of the next value in the sequence. Overall, the use of RNN algorithms in agriculture
    can help farmers make more informed decisions about

    when to plant and harvest their crops, as well as how much fertilizer to apply
    and how often to water their crops. By predicting crop yields

    more accurately, farmers can increase their productivity and pro¦tability, while
    also reducing waste and environmental impact.

    Regression

    algorithm

    To forecast crop yield, regression methods can be employed in agriculture. These
    algorithms can examine several variables that affect

    agricultural output, including weather patterns, soil quality, and fertilizer
    application rates. The program analyses past crop yield data as

    well as data on the variables that affect crop output to forecast crop yield in
    the future. Farmers may enhance crop yields and decrease

    waste by using this data to drive their decisions about when to plant, how much
    fertilizer to use, and when to harvest. Regression algorithms

    are useful for forecasting crop yields, and modern agriculture is using them more
    frequently.

    kNN

    The k-Nearest Neighbors (kNN) algorithm is a machine learning technique used to
    classify data based on similarity measures. In agriculture,

    kNN can be used to predict crop yields by analyzing historical data on crop yields
    and environmental factors such as temperature, rainfall,

    and soil nutrient levels. To use the kNN algorithm in agriculture crop yield prediction,
    the ¦rst step is to gather relevant data on past crop

    yields and environmental factors for a given region. This data can be used to
    train the algorithm and create a model that can predict future

    crop yields based on new environmental data. Once the model is trained, it can
    be used to predict crop yields for a speci¦c location by

    comparing its environmental data to the historical data in the training set. The
    kNN algorithm identi¦es the k-nearest neighbors to the new

    data point and assigns a yield prediction based on the average yield of those
    neighbors. Using the kNN algorithm in agriculture crop yield

    prediction can help farmers optimize their crop production by providing accurate
    predictions of future crop yields. This information can be

    used to adjust crop management practices and make informed decisions about crop
    selection and planting schedules.

    Table I denotes the classi¦cation of the machine learning algorithm.

    TABLE II: Crop yield prediction

    Page 14/20

    Crop

    Input data

    Algorithm model

    Optimum output

    Maize

    Weatherandsatellite

    spectral data

    Randomforest algorithm, CNN, SVM,

    RNN, kNN, regression algorithm

    (1) Corn yield: RF (R2 = 7.73); (2) CNN: NN (R2 = 0.54); (3) SVM: NN (R2 = 

    0.67); (4) RNN (R2 = 0.651); (5) kNN (R2 = 0.722);

    (6)Regression algorithm (R2 = 0.38)

    Cotton

    Multispectralaerial

    images

    Randomforest algorithm, CNN, SVM,

    RNN, kNN, regression algorithm

    Yield: RF (R2 = 9.73);

    (2) CNN: NN (R2 = 0.54); (3) SVM: NN (R2 = 1.67); (4) RNN (R2 = 2..651); (5)

    kNN (R2 = 3..432); (6) Regression algorithm (R2 = 0.38)

    Tomato

    RGB images

    Randomforest algorithm, CNN, SVM,

    RNN, kNN, regression

    algorithm

    NN: RMSE = 0.656 tha − 1, R2 = 0.76; (2) PLSR: RMSE = 0.83 tha − 1, R2 = 

    0.76

    Potato

    RGB images

    Randomforest algorithm, CNN, SVM,

    RNN, kNN, regression algorithm

    Reduced dataset: LB: MAE = 7.95%, R2 =

    0.59; (2) No feature selection: SVM: MAE = 9.64%, R2 = 0.63; (3) 1–2 months

    before harvest: RF: MAE = 8.71%, R2 = 0.89

    Turmeric

    RGB images

    Randomforest algorithm, CNN, SVM,

    RNN, kNN, regression algorithm

    Acc values: ANN: 90%, MLR: 28%

    Soya

    bean

    Multispectralaerial

    images

    Randomforest algorithm, CNN, SVM,

    RNN, kNN, regression algorithm

    R2 values:(1)SVM: 0.64; (2) RF: 0.98; (3)

    ANN: 0.88

    DNN (1) Corn: 28–31%

    moreaccurate(2)

    Soybean: 20–22% more accurate

    Rice

    Weatherandsatellite

    spectral data

    Randomforest

    algorithm, CNN, SVM,

    RF: RMSE = 0.985,

    MAE = 0.03, R = 0.87

     

     

    RNN, kNN, regression

    algorithm

     

    Table II determines the crop yield prediction using various machine learning algorithms
    to obtain the optimum results. This shows that the random forest

    algorithm helps to obtain the optimum results. Numerous challenges in agriculture
    have been successfully solved using random forest algorithms. They are

    favored over many other algorithms for a variety of reasons.

    First of all, farmers can make better decisions because of random forest algorithms''
    high accuracy, which is crucial in agriculture. Farmers can properly

    forecast crop yields, spot disease outbreaks, and make better- informed decisions
    about irrigation and fertilizer use by employing random forest algorithms.

    Secondly, random forest algorithms are highly robust to outliers, noise, and missing
    data. This is particularly important in agriculture, where data can be noisy

    and incomplete due to various factors like weather conditions, soil quality, and
    pest attacks.

    Thirdly, random forest algorithms provide feature importance, which means that
    they can identify the most important variables that in§uence the outcome of

    a particular problem. This is helpful in agriculture, where farmers can use this
    information to focus their efforts on the most critical variables.

    Finally, random forest algorithms are scalable and can handle large datasets,
    making them suitable for agriculture, where large amounts of data are generated

    from various sources.

    Overall, the high accuracy, robustness, feature importance, and scalability of
    random forest algorithms make them an excellent choice for solving problems in

    agriculture.

    TABLE III: Comparative Performance Matrix

    Pot

    Accuracy

    Precision

    Recall

    F1 score

    CNN

    0.51

    0.43

    0.37

    0.5

    SVM

    0.46

    0.42

    0.42

    0.3

    Regression algorithm

    0.62

    0.21

    0.54

    0.7

    Random forest algorithm

    0.998

    0.77

    0.8

    0.9

    kNN

    0.36

    0.41

    0.5

    0.7

    Table II denotes the comparative performance analysis using various optimization
    algorithms. This shows that the random forest algorithm provides the

    optimum output.

    Page 15/20

    Future trends of AI in agriculture:

    The future of agriculture is being shaped by arti¦cial intelligence (AI) and machine
    learning (ML) technologies. These technologies have the potential to

    revolutionize the industry by improving crop yields, reducing waste, and making
    farming more e¨cient and sustainable One trend that is expected to gain

    traction is precision farming. By using AI and ML algorithms, farmers can monitor
    crop health, water usage, and soil quality in real-time, enabling them to

    make data-driven decisions and optimize resource allocation. This can lead to
    higher yields, lower costs, and reduced environmental impact. Another trend is

    the development of autonomous farming equipment. Self-driving tractors and other
    machines equipped with AI and ML algorithms can navigate ¦elds, detect

    and remove weeds, and plant crops with precision, reducing the need for human
    labor and increasing e¨ciency. AI-powered predictive analytics is also

    expected to play a signi¦cant role in agriculture. By analyzing historical data
    and real-time weather and soil conditions, farmers can predict crop yields and

    identify potential issues before they become signi¦cant problems. Overall, AI
    and ML technologies have the potential to transform agriculture, making it more

    sustainable, e¨cient, and productive. As these technologies continue to develop
    and become more accessible, they will help farmers address the challenges of

    feeding a growing global population while protecting the environment.

    VI. CONCLUSION

    Using machine learning in agriculture with optimization aims to raise crop yields,
    decrease resource waste, and boost farmer pro¦tability. For the best planting,

    irrigation, fertilization, and pest management practices, machine learning models
    can analyze a signi¦cant quantity of data on crop development, soil

    characteristics, weather patterns, and other factors. The agricultural process
    can be further enhanced by introducing optimization algorithms into the machine

    learning models to discover the optimum solutions to issues like crop rotation,
    water management, and harvesting schedules. This may result in less waste,

    more effective use of resources, and eventually higher yields and more pro¦tability
    for farmers. Overall, using machine learning and optimization in agriculture

    can help farmers overcome some of the di¨culties they encounter, including the
    need to produce more food with fewer resources and the rising need for

    sustainable farming methods.

    Declarations

    Author contributions All the authors have participated in writing the manuscript
    and have revised the ¦nal version. All authors read and approved the ¦nal

    manuscript.

    Funding There is no funding for this study.

    Compliance with Ethical Standards

    Con§ict of interest The Authors declare that they have no con§ict of interest.

    Ethical approval This article does not contain any studies with human participants
    and/or animals performed by any of the authors.

    Informed consent  There is no informed consent for this study.

    References

    1. Zhang J, Zhou L, Chen Z (June 2019) IEEE Trans Industr Inf 15:3389–3397. 10.1109/TII.2018.2883057.
    "Precision Agriculture Based on Big Data and

    Machine Learning,"

    2. Sun C, Liu X, Li S (2020) "A Decision Support System for Precision Agriculture
    Based on Internet of Things and Machine Learning," in IEEE Access, vol. 8,

    pp. 24337–24345, doi: 10.1109/ACCESS.2020.2977065

    3. Liu X, Sun C, Huang W (2020) "Agricultural Crop Yield Prediction Based on Machine
    Learning and Internet of Things," in IEEE Access, vol. 8, pp. 145045–

    145054, doi: 10.1109/ACCESS.2020.3017665

    4. Li H et al (2019) An Intelligent Agricultural System Based on Internet of Things
    and Machine Learning. IEEE Access 7:74911–74919.

    10.1109/ACCESS.2019.2921574

    5. Wu X et al (2018) "A Machine Learning Approach for Crop Yield Prediction Based
    on Remotely Sensed Data," in IEEE Transactions on Geoscience and

    Remote Sensing, vol. 56, no. 10, pp. 5747–5758, doi: 10.1109/TGRS.2018.2833874

    ½. Liu Y, Zhang Z, Zhang X (2020) "Smart Agriculture with IoT, Big Data and Arti¦cial
    Intelligence," in IEEE Access, vol. 8, pp. 69535–69544, doi:

    10.1109/ACCESS.2020.2982109

    7. Yu X et al (May 2019) Deep Learning for Crop Yield Prediction Based on Multi-source
    Data. IEEE Trans Industr Inf 15(5):2636–2644.

    10.1109/TII.2018.2867070

    ¾. Rahman A et al (2019) "A Smart Irrigation System Based on IoT and Machine Learning,"
    in IEEE Access, vol. 7, pp. 64197–64207, doi:

    10.1109/ACCESS.2019.2911565

    9. Tao D et al (2021) "A Deep Learning Approach to Rice Growth Stage Recognition
    Based on Multiple Types of Sensor Data," in IEEE Transactions on

    Industrial Informatics, vol. 17, no. 2, pp. 1076–1084, doi: 10.1109/TII.2020.2992874

    10. Wang H, Zhang X, Liu Y (2020) Smart Greenhouse Monitoring System Based on
    IoT and Machine Learning. IEEE Access 8:34844–34851.

    10.1109/ACCESS.2020.2973531

    Page 16/20

    11. Yang K, Li Y, Cai Z, Huang W (2019) "Agricultural Expert System Based on Random
    Forest Algorithm," IEEE International Conference on Smart Cloud

    (SmartCloud), 2019, pp. 289–294, doi: 10.1109/SmartCloud.2019.00062

    12. Bao L, Li X, Xie L, "Research on Agricultural Drought Monitoring Based on
    Random Forest Algorithm," (2020) IEEE International Conference on Smart Grid

    and Clean Energy Technologies (ICSGCE), 2020, pp. 409–412, doi: 10.1109/ICSGCE50895.2020.9370904

    13. Zhang Y, Shen X, Wang X, Yang H, "Prediction of Crop Yields Using a Random
    Forest Algorithm," 2019 IEEE International Conference on Computational

    Science and Engineering (CSE) and IEEE International Conference on Embedded and,
    Computing U (2019) (EUC), pp. 353–357, doi:

    10.1109/CSE/EUC.2019.00062

    14. Gao J, Zhang H, Xu S, Wang Z, Guo X (2019) "Identi¦cation of Maize Growth
    Stages Based on Random Forest Algorithm," 2019 IEEE 5th International

    Conference on Computer and Communications (ICCC), pp. 2165–2169, doi: 10.1109/CompComm.2019.8936046

    15. Zhang J, Chen W, Li Z (2019) "Prediction of Wheat Yield Based on Random Forest
    Algorithm," 2019 IEEE 4th International Conference on Advanced

    Robotics and Mechatronics (ICARM), pp. 528–533, doi: 10.1109/ICARM.2019.8834104

    1½. Li J, Yang Y, Li Z, Gao J, "Prediction of Rice Yield Based on Random Forest
    Algorithm," 2019 IEEE International Conference on Arti¦cial Intelligence and,

    Applications C (2019) (ICAICA), pp. 68–72, doi: 10.1109/ICAICA47620.2019.00020

    17. Wu C, Zhang X, Cao Y, Chen Y (2019) "Detection of Tomato Diseases Using a
    Random Forest Algorithm," 2019 IEEE International Conference on Robotics

    and Automation Sciences (ICRAS), pp. 901–906, doi: 10.1109/ICRAS.2019.8932379

    1¾. Liu Y, Zhang L, He Z, "Research on the Prediction of Tea Yield Based on Random
    Forest Algorithm," (2019) IEEE 6th International Conference on Energy

    Smart Systems (ESS), 2019, pp. 13–17, doi: 10.1109/ESS.2019.8888863

    19. Li H, Wang Y, Li S, Huang J, "Research on Soil Nutrient Prediction Based on
    Random Forest Algorithm, (2019) " 2019 IEEE 5th International Conference on

    Big Data Analytics (ICBDA), pp. 104–108, doi: 10.1109/ICBDA.2019.8754318

    20. Yang Y, Chen J, Zhang M, Zhao Z, "Research on Fruit Classi¦cation Based on
    Random Forest Algorithm," 2020 IEEE International Conference on Arti¦cial

    Intelligence and, Engineering C (2020) (ICAICE), pp. 134–137, doi: 10.1109/

    21. Singh M, Singh P, Singh A (March 2014) Application of genetic algorithm in
    agriculture: a review. Int J Comput Appl 90(5):1–7

    22. Ramesh MV, Suresh S (June 2011) Optimization of irrigation scheduling using
    genetic algorithm. IEEE Trans Instrum Meas 60(6):2021–2027

    23. Das SS, Dash SK, Barik RN "Optimization of fertilizer application using genetic
    algorithm," in Proceedings of the 2014 IEEE International Conference on

    Computer and Information Technology (CIT), pp. 598–603

    24. Luu TV, Ain MF, Zulki§i Z (2016) "Application of genetic algorithm in crop
    yield optimization," in Proceedings of the IEEE Conference on Open Systems

    (ICOS), pp. 50–54

    25. Benos L, Tagarakis AC, Dolias G, Berruto R, Kateris D, Bochtis D (2021) Machine
    Learning in Agriculture: A Comprehensive Updated Review. Sensors

    21:3758. https://doi.org/10.3390/s21113758

    2½. Patil SB, Kulkarni RV (April 2015) Genetic algorithm-based decision support
    system for crop yield prediction. IEEE Trans Industr Inf 11(2):400–408

    27. Zahraei SH, Jafari MJ (2017) "Optimization of greenhouse cultivation parameters
    using genetic algorithm," in Proceedings of the IEEE International

    Conference on Industrial Engineering and Engineering Management (IEEM), pp. 916–920

    2¾. Song HH, Lee JH (2015) "Design and implementation of a genetic algorithm-based
    greenhouse environment control system," in Proceedings of the IEEE

    International Conference on Consumer Electronics (ICCE), pp. 139–140

    29. Lu T, Wang Y "Optimization of aquaponic systems using genetic algorithm,"
    in Proceedings of the 2016 IEEE International Conference on Intelligent

    Transportation, Big Data & Smart City (ICITBS), pp. 224–227

    30. Chen L, Han C, Chen Y "Design of an intelligent irrigation system based on
    genetic algorithm," in Proceedings of the 2014 IEEE International Conference on

    Mechatronics and Automation (ICMA), pp. 635–640

    31. Li J, Liang J, Zhang S, Li Y (2019) Optimizing drip irrigation system design
    using a modi¦ed particle swarm optimization algorithm. IEEE Access

    7:72758–72766

    32. Jayasree SN, Subramanian P (2019) "Optimal design of greenhouse based on particle
    swarm optimization," in 2019 International Conference on

    Electrical, Electronics and Communication Engineering (ICEECE), Chennai, India,
    pp. 1–4

    33. Liu Y, Shao Q, Xu X, Zhang Z (2020) "Optimizing agricultural water allocation
    with particle swarm optimization algorithm," in 2020 3rd International

    Conference on Intelligent Transportation, Big Data & Smart City (ICITBS), Shenzhen,
    China, pp. 386–389

    34. Zhao J, Wang J, Sun H (2018) "Optimization of irrigation scheduling based
    on particle swarm optimization," in 2018 IEEE 3rd Advanced Information

    Technology, Electronic and Automation Control Conference (IAEAC), Chongqing, China,
    pp. 496–500

    35. Lin HY, Shen TM (2018) "Optimization of nutrient management in agriculture
    using particle swarm optimization," in 2018 4th International Conference on

    Green Energy and Applications (ICGEA), Singapore, pp. 246–250

    3½. Xu J, Wang D, Zhang J (2020) "A particle swarm optimization algorithm for
    optimizing irrigation scheduling in precision agriculture," in 2020 IEEE 5th

    Information Technology and Mechatronics Engineering Conference (ITOEC), Chongqing,
    China, pp. 527–530

    37. Li X, Li Z, Wang J (2020) "Optimizing irrigation management in farmland based
    on particle swarm optimization," in 2020 5th International Conference on

    Electrical and Information Technologies (ICEIT), Xi''an, China, pp. 498–502

    3¾. Zhao Y, Wang J, Hu Y (2020) "Optimizing the allocation of fertilizer in farmland
    based on particle swarm optimization," in 2020 IEEE 4th Information

    Technology and Mechatronics Engineering Conference (ITOEC), Chongqing, China,
    pp. 360–363

    Page 17/20

    39. Zhang Y, Yang L, Liu H (2020) "Optimization of crop planting layout based
    on particle swarm optimization algorithm," in 2020 12th International

    Conference on Computational Intelligence and Security (CIS), Chongqing, China,
    pp. 39–42

    40. Kulkarni KS, Pawar PG, Patil PG (2015) "Precision farming using k-means clustering
    algorithm," 2015 International Conference on Pervasive Computing

    (ICPC), Pune, India, pp.1–4

    41. Al-Qahtani AR, Nawi NM, Yaakob SS (2016) Cluster analysis of soil fertility
    parameters using k-means algorithm. 2016 IEEE 12th International Colloquium

    on Signal Processing & Its Applications (CSPA). Melaka, Malaysia, pp 245–250

    42. Rahman MSS, Hasan MM, Mottaleb MA (2019) "K-means clustering for rice yield
    prediction in Bangladesh," 2019 International Conference on Electrical,

    Computer and Communication Engineering (ECCE), Cox''s Bazar, Bangladesh, pp. 1–6

    43. Abdulghani MA, Abduljabbar NA, Al-Sawa¦ RA, International Conference on Intelligent
    Computing and, Applications I (2018) (ICICA), Penang, Malaysia,

    2018, pp. 88–93

    44. Singh SK, Singh K, Singh SP (2016) "Application of k-means clustering algorithm
    for crop yield prediction," International Conference on Advances in

    Computing, Communication, & Automation (ICACCA), Dehradun, India, 2016, pp. 1–5

    45. Liu J, Wang Y, Li J (2017) "Agricultural crop classi¦cation based on k-means
    clustering algorithm," IEEE International Conference on Computational

    Intelligence and Virtual Environments for Measurement Systems and Applications
    (CIVEMSA), Annecy, France, 2017, pp. 1–5

    4½. Hossain SE, Hossain MA, "K-means clustering algorithm for improved agricultural
    productivity in Bangladesh," 2018 3rd International Conference on

    Electrical and, Engineering E (2018) (ICEEE), Dhaka, Bangladesh, pp. 1–6

    47. Paul BK, Islam MA (2017) "Application of k-means clustering algorithm for
    precision agriculture," International Conference on Electrical, Computer and

    Communication Engineering (ECCE), Cox''s Bazar, Bangladesh, 2017, pp. 431–436

    4¾. Al-Qahtani AR, Nawi NM, Yaakob SS, IEEE International Conference on Innovative
    Research and, Development (2016) (ICIRD), Manila, Philippines, 2016,

    pp. 1–5

    49. Sutar SR, Bhowate SS, Deshmukh SD (2015) "Agricultural decision making using
    k-means clustering algorithm," 2015 International Conference on

    Electrical, Electronics, Signals, Communication and Optimization (EESCO), Visakhapatnam,
    India, pp. 1–5

    50. Fan C, Liu Q, Chen J (2018) "Optimization of farmland irrigation scheduling
    based on particle swarm optimization algorithm," in 2018 15th International

    Conference on Control, Automation, Robotics and Vision (ICARCV), Singapore, pp.
    1–6

    Figures

    Figure 1

    Role of AI in agriculture

    Page 18/20

    Figure 2

    Factors to enhance productivity

    Figure 3

    Proposed system

    Page 19/20

    Figure 4

    Crop production in a particular village (dataset)

    Figure 5

    Random forest algorithm

    Page 20/20

    Figure 6

    Matlab Implementation

    '
  inline_citation: '>'
  journal: Research Square (Research Square)
  limitations: '>'
  pdf_link: https://www.researchsquare.com/article/rs-3137542/latest.pdf
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: A digital footprint in enhancing agricultural practices with improved production
    using machine learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.18687/laccei2023.1.1.965
  analysis: '>'
  authors:
  - Francisco José Poyato López
  - Andrés Fernando Jiménez López
  - Juan Sebastian Castellanos Patiño
  citation_count: 0
  full_citation: '>'
  full_text: ">\n21st LACCEI International Multi-Conference for Engineering, Education,\
    \ and Technology: “Leadership in Education and Innovation in Engineering in the\
    \ Framework of Global \nTransformations: Integration and Alliances for Integral\
    \ Development”, Hybrid Event, Buenos Aires - ARGENTINA, July 17 - 21, 2023. \n\
    1 \nForecasting irrigation scheduling based on deep \nlearning models using IoT\
    \ \n \nFabián-R  Jiménez-López, MSc1, Juan-S Castellanos-Patiño, Eng.1, and Andrés-F\
    \ Jiménez-López, PhD2 \n1Universidad Pedagógica y Tecnológica de Colombia, Colombia,\
    \ fabian.jimenez02@uptc.edu.co, juan.castellanos02@uptc.edu.co \n2Universidad\
    \ de los Llanos, Colombia, a.jimenez@unillanos.edu.co \n \nAbstract– Potatoes\
    \ are one of the staple foods in Boyacá and \nplay an important role in family\
    \ nutrition and food security in \nColombia. Therefore, timely and accurate information\
    \ on the \nirrigation of this crop is relevant in agricultural decision-making,\
    \ \nthe sustainable development of its production and the reduction of \nunnecessary\
    \ water consumption. This study estimated the \nirrigation prescription of a potato\
    \ crop from crop meteorological \ninformation with the support of IoT technologies\
    \ to solve the \nproblem of inefficient water dosing in the crop. Two deep learning\
    \ \nmodels were developed, a One-Dimensional Convolutional Neural \nNetwork (1D-CNN)\
    \ and a short-term long-term memory neural \nnetwork (LSTM). Training data was\
    \ collected daily from a potato \ncrop from 2018 to 2020 using two weather stations\
    \ located in the \nUsoChicamocha \nirrigation \ndistrict. \nTo \npredict \nirrigation\
    \ \nprescriptions, deep learning architectures were trained using \nPython® by\
    \ selecting input climatic variables measured with a \nsubsystem of sensors installed\
    \ in the crop and an actuation \nsubsystem with control of Latch-type solenoid\
    \ valves, both \nremotely controlled wirelessly.  The algorithms were validated\
    \ by \ncalculating precision metrics such as MSE and coefficient of \ndetermination.\
    \ The results showed that the LSTM model surpassed \nthe 1D-CNN model, obtaining\
    \ training and validation errors less \nthan 0.096 and presenting greater precision\
    \ in the estimation of \ncrop irrigation, giving a coefficient of determination\
    \ R2 between \n0.881 and 0.919. Irrigation prediction algorithms using deep \n\
    learning techniques achieved promising results and serve as a \ndecision support\
    \ tool for farmers to automatically decide when and \nhow much water to irrigate.\
    \ \nKeywords-- \nDeep \nLearning, \nCNN, \nLSTM, \nIrrigation \nPrescription,\
    \ IoT, Smart Farming. \n \nI.  INTRODUCTION \nFor the agricultural sector of the\
    \ department of Boyacá in \nColombia, it is necessary to develop irrigation methods\
    \ based \non precision agriculture. Specifically, potato crops require the \n\
    use of irrigation forecast estimation technologies that \ncontribute to increasing\
    \ economic and productive yields, \nminimizing the environmental impact that these\
    \ tasks can \nproduce [1].   \nPrecision farming is a strategy based on the use\
    \ of new \ntechnologies, which allows the productivity of the land to be \nmanaged\
    \ more efficiently, maximizes income and minimizes \nenvironmental impact [2].\
    \ With the advancement of new \ntechnologies in telemetry, remote sensing, communications,\
    \ \nsignal processing, internet connectivity and scientific-technical \nknowledge\
    \ in these areas, it is possible to create crop \necosystems based on the Internet\
    \ of Things (IoT) [3].  \nIoT allows the connection and monitoring of objects\
    \ \nlocated over long distances using WSN technologies and \ncentralized data\
    \ collection systems connected to the network \n[4, 5]. For precision irrigation\
    \ systems, based on IoT, humidity \nsensors located in the crop are used, weather\
    \ stations close to \nthe planted area and actuator systems on solenoid valves\
    \ that \nallow irrigation supported by technical approaches, which take \ninto\
    \ account the dynamic behavior of the crop [6, 7]. The \nirrigation prescription\
    \ and application methods that are now \navailable can be divided into five types:\
    \ \n- Through soil variables: where soil sensors are used to \nmeasure and process\
    \ data on the Volumetric Water Content \n(VWC) or the Potential of the Soil Matrix\
    \ (PCM) [8]. \n- Through climatological variables: which are based on \ncalculations\
    \ of missing water, through formulas of water \nbalance \nand \nmeteorological\
    \ \ndata, \nto \ncalculate \nthe \nEvapotranspiration (ET) [9]. \n- Using plant\
    \ parameters: in this method, remote sensing \ntools are used (satellites, drones\
    \ and Geographic Information \nSystems (GIS) for the management of irrigation\
    \ in crops) and \nsensors connected directly to the plants [10]. \n- Using crop\
    \ models: several models endorsed by \ninternational organizations such as FAO\
    \ are used, which allow \nestimating the hydric status of the soil and the particular\
    \ \nrequirements in each cultivation stage [11]. \n- With the farmer's experience:\
    \ it is based entirely on \nempirical data acquired by each farmer in his field\
    \ and on his \nown experiences. \nAll of these methods can be integrated and automated\
    \ \nthrough IoT-based technologies, cloud computing, and the use \nof machine\
    \ learning algorithms. Additionally, Deep Learning \n(DL) techniques have been\
    \ used in applications for agriculture, \nsince agricultural processes are characterized\
    \ by the \nbiodiversity of products and the variability of crop conditions. \n\
    DL extends its application in the estimation of sowing \nprocesses, crop yield,\
    \ prediction of irrigation prescription, \napplication of nutrients and fertilizers\
    \ in the plots, collection of \nproducts, identification of diseases, climate\
    \ and soil mapping, \namong others, processing information automatically [12-18].\
    \  \nThis work explains the implementation and development \nof an irrigation\
    \ prescription forecasting system in a potato crop \nusing IoT technologies for\
    \ the acquisition and transmission of \ncrop data measured by a sensor network.\
    \ The treatment of the \ncultivation variables is transmitted remotely, to enter\
    \ an \nDigital Object Identifier: (only for full papers, inserted by LACCEI).\
    \ \nISSN, ISBN: (to be inserted by LACCEI). \nDO NOT REMOVE \nISBN: 978-628-95207-4-3.\
    \ ISSN: 2414-6390. Digital Object Identifier: https://dx.doi.org/10.18687/LACCEI2023.1.1.965\n\
    21st LACCEI International Multi-Conference for Engineering, Education, and Technology:\
    \ “Leadership in Education and Innovation in Engineering in the Framework of Global\
    \ \nTransformations: Integration and Alliances for Integral Development”, Hybrid\
    \ Event, Buenos Aires - ARGENTINA, July 17 - 21, 2023. \n2 \nirrigation prescription\
    \ estimation system based on deep \nlearning. \nCrop variables such as maximum,\
    \ minimum and average \ntemperature, soil moisture and rainfall were measured\
    \ daily for \nthree years. The Evapotranspiration of the crop was calculated,\
    \ \nand together with the climatic and soil variables were entered \nas input\
    \ data to the deep learning algorithms to make reactive \nand proactive decisions\
    \ about the prescription of irrigation in \nthe potato crop through an infrastructure\
    \ of drip irrigation.  \nII. MATERIALS AND METHODS \nA. Studio Site Selection\
    \ \nThis study was developed in potato crops from Finca San \nCarlos located in\
    \ the municipality of Tibasosa, Boyacá, \nColombia. The land belongs to large-scale\
    \ Irrigation and \nDrainage District of Chicamocha (UsoChicamocha). The \nUsoChicamocha\
    \ Irrigation Station has eleven irrigation units \nwith one pumping station each\
    \ as illustrated in Fig. 1.  \n \n \nFig. 1. Location of the study field in the\
    \ UsoChicamocha Irrigation District \nand Irrigation Unit (9) of Tibasosa. \n\
    \ \nThe water source for crop irrigation is the La Copa dam, \nand the water is\
    \ distributed along the Chicamocha riverbed. \nThe potato crop is located in the\
    \ Irrigation Unit of Tibasosa (9 \nin Fig. 1). The Tibasosa irrigation unit supplies\
    \ water from the \npotato crops on the San Carlos farm through a pumping \nstation.\
    \ Four lots were used, with an approximate total area of \n100 m2, which are located\
    \ with central coordinates at 5.76534 \n° N, - 72.98283 ° W. \nCultivation lots\
    \ have an average elevation of 2575 m.a.s.l. \nand it has a dry-cold climate.\
    \ The lots have a bimodal regime \nand a water deficit during most of the year,\
    \ the winters are \nshort, cool and humid with a concentration of rains between\
    \ \nApril and May in the first semester and between October and \nNovember in\
    \ the second semester, according to with data \ncollected over 10 years [19].\
    \ Temperature does not drop \nbelow 7 °C or rise above 21 °C, where the average\
    \ annual \ntemperature is 14.05 °C. The average annual brightness in the \nIrrigation\
    \ unit Tibasosa is 1853, with a high qualification for \nthe sowing of agricultural\
    \ species [20].   \n \nB. IoT Based Architecture \nEach intelligent system houses\
    \ the trained deep learning \nmodel implemented on a RaspBerry Pi platform that\
    \ is \nconnected to a Davis Vantage Pro2 Series weather station, \nallowing it\
    \ to access data on ambient temperature, soil \nmoisture, precipitation, and evapotranspiration\
    \ [21]. \nPotato crop was divided into four lots of 25 m2. Each \ncrop is supervised\
    \ and managed by four intelligent systems that \nhave a sensor subsystem in the\
    \ center of the crop and an \nactuation subsystem with control of a Latch-type\
    \ solenoid \nelectrovalve at the entrance to the irrigation supply. A water \n\
    supply with constant outlet pressure is guaranteed for each \ncrop, so that there\
    \ is a linear relationship between the opening \ntime of the solenoid valve and\
    \ the amount of resource applied. \nAccess to meteorological data allows the Smart\
    \ system to \nmake irrigation decisions according to the water balance \nequation\
    \ of the crop expressed in (1).  \n \n)\n(\n1\nmm\nDP\nET\nR\nI\nWC\nWC\nC\nAIN\n\
    RR\nt\nt\n−\n−\n+\n+\n=\n−\n          (1) \n \nWhere WCt is the water content\
    \ in the soil in the current \nday (mm), WCt-1 is the water content in the soil\
    \ of the previous \nday (mm), IRR and RAIN are the irrigation applied and the\
    \ \nrainfall that fell last day (mm) respectively, ETC is the \naccumulated evapotranspiration\
    \ of the previous day (mm) and \nDP the deep percolation (mm) [22]. With the data\
    \ acquired in \nthe field and from the meteorological station strategically \n\
    located near the crop, the amount of irrigation to be applied \nwas determined\
    \ by calculating the difference in the volumetric \nwater content and through\
    \ changes that the farmer can make \nfrom the mobile application, the schedules\
    \ of when to carry out \nthe watering process. \nThe IoT based architecture showed\
    \ in Fig. 2 has been \nproposed to collect, transmit and process the climatic\
    \ and soil \nvariables of the potato crop (air temperature, soil moisture, and\
    \ \nprecipitation) of farming land along with the weather forecast \ninformation\
    \ to manage the irrigation efficiently. \n \nFig. 2. IoT based arquitecture for\
    \ Prescription Irrication Prediction using Deep \nLearning (DL). \nThe Smart Multiple\
    \ Sensors Array (SMSA) is the \nMeasurement Station that allows the acquisition\
    \ of field \ninformation. Central Station is considered the brain of the \nprescription\
    \ irrigation system where the crop information is \nstored and analyzed, as well\
    \ as being the direct connection to \nthe database located in the Firebase computer\
    \ services. \nCENTRAL \nSTATION \n21st LACCEI International Multi-Conference for\
    \ Engineering, Education, and Technology: “Leadership in Education and Innovation\
    \ in Engineering in the Framework of Global \nTransformations: Integration and\
    \ Alliances for Integral Development”, Hybrid Event, Buenos Aires - ARGENTINA,\
    \ July 17 - 21, 2023. \n3 \nIrrigation Station is the system in charge of acting\
    \ on the \nsolenoid valves of the system. Weather Station is responsible \nfor\
    \ acquiring data from the environment around the crop and a \ncommunication system\
    \ with the farmer based on a mobile user \napplication (MUA) for management irrigation\
    \ system.  \n \nC. Dataset Collection \nThe set of daily climatic data for the\
    \ potato crop were \ncollected through one weather station located in the field\
    \ El \nClan that belongs to the Tibasosa irrigation unit (9 in Fig. 1), \nduring\
    \ the period of time from June 2018 to June 2021. This \ndataset provided selected\
    \ climatic variables based on daily \nhistorical data that include rain, maximum,\
    \ minimum and mean \ntemperature, reference crop evapotranspiration and water\
    \ \ncontent in soil (Table I). \nTABLE I \nWEATHER AND SOIL INFORMATION FOR TRAINING.\
    \ \nData \nSource \nFeature \nName \nUnit \nDescription \nClimatic \nData \nTAVG\
    \ \nTMIN \nTMAX \nRAIN \nWC1 \nWC2 \nRN \n°C \n°C \n°C \nmm / day \n% \n% \nH\
    \ \nDaily Average Air Temperature. \nDaily Minimum Air Temperature. \nDaily Maximum\
    \ Air Temperature. \nDaily Cumulative Rainfall \nWater Content in Soil at 20 cm.\
    \  \nWater Content in Soil at 40 cm. \nDaily Hours of Solar Radiation  \n \nDeep\
    \ learning models require a large volume of data to \ntrain. Furthermore, recording\
    \ crop yields based on different \nirrigation schedules is too slow and sometimes\
    \ impossible. \nEvapotranspiration (ET) is the amount of water that \nevaporates\
    \ from the soil and the soil water content (SWC) is \nthe volume of water per\
    \ unit volume of soil. ETC was \ncalculated and used as input for the water balance\
    \ equation. \nETC was estimated from meteorological data using the FAO-56 \nPenman-Monteith\
    \ Equation (2) [9]: \n \n(\n)\n)\n(\n.0 34\n1\n271\n900\n)\n(\nmm\nWS\nWS p\n\
    T\nG\nR\nET\nAVG\nN\nC\n⋅\n+\n+\n∆\n⋅\n⋅\n\n\n\n\n\n+\n+\n−\n⋅\n∆\n=\n\
    γ\nγ\n          (2) \n \nWhere RN is the net solar radiation, G is the heat flux\
    \ from \nthe ground, p is the pressure deficit, γ is the psychometric \nconstant,\
    \ WS is the wind speed and TAVG is the mean daily \ntemperature. With the amount\
    \ of rainfall measured, ET was \ncalculated in an hourly time interval over a\
    \ 24-hour period and \nsummarized in a daily time interval. The advantage of using\
    \ \ndeep learning models is that it does not require manual \nadjustments once\
    \ the model is trained and can be used \nautomatically. These models can be used\
    \ to create a decision \nsupport system for irrigation scheduling. \nD. Data Pre-Processing\
    \ \nIn the preprocessing phase, the data set was prepared so \nthat the characteristic\
    \ extraction phase was more efficient. The \nclimatic data set for the period\
    \ from June 2018 to February \n2020 (60%) was reserved for model training, the\
    \ data set from \nMarch 2020 to October 2020 (20%) was used for validation \n\
    and the dataset for the period November 2021 to June 2021 \n(20%) were used to\
    \ test the models, respectively. In time series \nmodeling, the estimation becomes\
    \ less accurate gradually, so it \nis more advantageous to train the models with\
    \ real data when \navailable. \nDuring the training phase, the dataset was used\
    \ to update \nthe parameters of the networks. The validation dataset was \nused\
    \ during the training phase to monitor the process and \ndetect overfitting [23].\
    \ Finally, the trained models were tested \nwith the test dataset characterized\
    \ as new and unexamined \n(Fig. 3). \n \nFig. 3. Scheme of the Data Preprocessing\
    \ for predict the irrigation \nprescription. \nThe training, validation and test\
    \ data consisted of time \nseries spaced every hour. The variables were normalized\
    \ to \nensure that they remain on the same scale. This preprocessing \nguarantees\
    \ a stable convergence of parameters in the model. \nThe standardization formula\
    \ used in the studied data set was a \nMin-Max normalization model. For each variable\
    \ in the data \nset, the minimum value of that variable is converted to 0, the\
    \ \nmaximum value is converted to 1, and all other values are \nconverted to a\
    \ decimal number between 0 and 1 using (3): \n)\nmin(\n)\nmax(\n)\nmin(\nj\nj\n\
    j\nj\ni\nij\nx\nx\nx\nx\nx\nnew\n−\n−\n=\n                                   \
    \   (3) \nWhere xi represents the data in the i-th of variable j, min(xj) \nis\
    \ the minimum value of x in variable j and max(xj) is the \nmaximum value of x\
    \ in variable j, for j = 1, 2, ..., 6. \nE. Developed Models \n1) Architecture\
    \ and parameters of the One-Dimensional \nConvolutional Neural Network (1-DCNN):\
    \ Convolutional \nNeural Networks (CNN) have been successful in machine \nvision\
    \ applications to process crop images mainly in \nclassification, \nrecognition,\
    \ \ndetection \nand \nsegmentation \nprocesses [24-26].  \nConvolution and Max\
    \ Pooling Filter principles used in \nimages can be simplified to work with one-dimensional\
    \ data in \nthe form of time series. CNN models make predictions based \non current\
    \ input data and do not use past observations to make \nfuture decisions. The\
    \ 1-DCNN model proposed reads the data \nfrom delayed observations and extracts\
    \ useful features to \ngenerate future predictions.  \nThe architecture of a CNN\
    \ is made up of a stack of \nconnected hidden layers that are reduced in width\
    \ from input \nIdentify applicable funding agency here. If none, delete this text\
    \ box. \nIdentify applicable funding agency here. If none, delete this text box.\
    \ \n21st LACCEI International Multi-Conference for Engineering, Education, and\
    \ Technology: “Leadership in Education and Innovation in Engineering in the Framework\
    \ of Global \nTransformations: Integration and Alliances for Integral Development”,\
    \ Hybrid Event, Buenos Aires - ARGENTINA, July 17 - 21, 2023. \n4 \nto output.\
    \ CNN thickening reduction is done to ensure \ncondensation of information into\
    \ more abstract concepts in \ndeeper layers. Each shrinking stage generally consists\
    \ of one or \nmore convolution layers and several pooling layers connected \n\
    to each other. \nFirst a set of convolutional layers is organized using a \nlocal\
    \ layer-shared filter that generates the response of the \nfilter's convolution\
    \ with its inputs in the output. The outputs of \nthe convolutional layers usually\
    \ have the same dimension as the \ninputs. Second, the outputs of the convolutional\
    \ layers are the \nactivations that go into the pooling layers, which use a \n\
    dimension reduction operator (for example, max) to thin the \nlayers.  \nAs a\
    \ result, a hidden neural unit has a smaller field of view \nthan the entire input\
    \ layer, that is, a neuron located in the \nupper layers has a field of view indicated\
    \ by dashed black lines \nas shown in Fig. 4. This hierarchical design helps to\
    \ capture the \nmost prevalent local training characteristics and extracts the\
    \ \nmost essential parameters on a larger scale in deeper layers.  \n \nFig. 4.\
    \ Structure of the One-Dimensional Convolutional Neural Network used \nin this\
    \ study. \nThe parameters learned by the convolutional layers are \nmade thanks\
    \ to the application of the layer-shared filters. All \nneurons in the convolutional\
    \ layers share a single filter that \nreduces the number of parameters compared\
    \ to a fully \nconnected layers, making CNN training easier. In this work, \n\
    several tests were performed using the 1-DCNN architecture. \nThe model begins\
    \ with the 1D convolutional layer composed \nof 16 convolutional filters, a kernel\
    \ size of 3 and a Rectified \nLinear Unit (ReLU) as activation function. Added\
    \ a max \npooling layers to recover the input characteristics, into more \nmeaningful\
    \ and useful inputs. The inputs are flattened with the \nnext flatten layer which\
    \ is used to split convolutional layers and \nfully connected layers and get the\
    \ first prediction step. \n2) Architecture and parameters of the Long Short-Term\
    \ \nMemory model (LSTM): Another deep learning architecture \nuses Recurrent Neural\
    \ Networks (RNN) to process sequential \ndata efficiently. RNN integrate feedback\
    \ loops, allowing \nthrough them that the information persists during some training\
    \ \nperiods (epochs), by means connections from the outputs of \nthe layers, which\
    \ embedding their results on the input data.  \nBasically, the RNN remember previous\
    \ states and use this \ninformation to predict which one will be next. The connections\
    \ \nbetween nodes form a graph directed along a time sequence, so \nthey are applied\
    \ in lists to handle time series. RNNs have been \nintroduced for sequential learning,\
    \ as they are capable of \nstoring and relating prior information. \nLong Short\
    \ Term Memory (LSTM) are a special type of \nRNN networks that have computational\
    \ units with the ability \nto dominate or suppress input characteristics, allowing\
    \ them to \nstore only characteristic weights more important. While \nstandard\
    \ RNN can model short-term dependencies (that is, \nclose relationships in the\
    \ time series), LSTMs can learn long-\nterm dependencies and determine the optimal\
    \ time delay for \ntime series problems. In this sense, LSTMs operate dependent\
    \ \non experience over time and can learn when to forget and how \nlong to keep\
    \ state information. LSTMs outperform RNNs by \nholding the value of the previous\
    \ output for a short period. \nGiven this characteristic, LSTMs are used mainly\
    \ for \nprocessing and estimating complex variables and combinational \ninputs.\
    \ LSTM have been used in crops such as tomato, \nsoybean and corn, where climate\
    \ data and environmental \nparameters are mapped. These crop variables are periodically\
    \ \nmonitored and data are generated in the form of time series \nthat are analyzed\
    \ and processed to provide diagnosis and \nestimation of irrigation prescriptions\
    \ in crops with very good \nefficiency [27-29]. \nThe solution to the vanishing\
    \ gradient problem is the \nLSTM model that contain specially designed units called\
    \ gates \nand memory cells. The gates are simply neurons with weights \nor gains\
    \ that have the ability to learn: The gates surround the \nmemory cell Ct to control\
    \ the information flow. After training, \nthe input gate it controls which entries\
    \ are significant enough \nto remember. The forget gate ft decides how long and\
    \ what \npast state memory should be retained. The output gate ot \ndetermines\
    \ the amount of memory that is used to produce the \noutput. The integrated operation\
    \ of the gates allows the \nnetwork to remember information from the past and\
    \ discard \nnon-essential information. \n \n \nFig. 5. Structure of the LSTM Neural\
    \ Network used in this study. \n21st LACCEI International Multi-Conference for\
    \ Engineering, Education, and Technology: “Leadership in Education and Innovation\
    \ in Engineering in the Framework of Global \nTransformations: Integration and\
    \ Alliances for Integral Development”, Hybrid Event, Buenos Aires - ARGENTINA,\
    \ July 17 - 21, 2023. \n5 \nThe LSTM architecture proposed and implemented in\
    \ this \nwork uses a model known as “many to one”, which means that \nmultiple\
    \ data inputs can be predicted in the form of time series \nto predict a single\
    \ value [30]. Fig. 5 shows the LSTM \narchitecture developed where a single input\
    \ layer was arranged \nthat receives the input data xt, a single hidden recurring\
    \ layer \nthat stores the information in a hidden state ht in the memory \ncell\
    \ of the LSTM and a single layer output ot. The hidden layer \ncontains the memory\
    \ units of the LSTM that manage and \ncontrol the input, output and storage of\
    \ data. Tanh or sigmoid \ntransformation operators are provided in each LSTM memory\
    \ \ncell that scale the data to facilitate the information flow. \nEach memory\
    \ cell contains one or more memory cells and \nfour multiplicative gates: input\
    \ it, forget f t, cell gt and output ot \n[31], to overcome the short-term memory\
    \ limitation of the \nRNN. The memory cells memorize the value of the temporary\
    \ \nstate in arbitrary time intervals and the four gates control the \nopening\
    \ or closing of the flow from the new input xt to the next \ncell since it enters\
    \ through the input gate it.  \nThe learning process discriminates what information\
    \ is \nimportant and what is not, as time passes while the assigned \nweights\
    \ are updated. The relationships that define the \noperation and structure of\
    \ the model of each LSTM memory \ncell are defined in (4) to (9) with initial\
    \ values of the state of \ncell C0 = 0 and of the recurring output h0 = 0: \n\
    )\n(\n−1\n+\n+\n=\nt\ni\ni\nt\ni\nt\nU h\nb\nW x\ni\nσ\n                     \
    \              (4) \n)\n(\n−1\n+\n+\n=\nt\nf\nf\nt\nf\nt\nU h\nb\nW x\nf\nσ\n\
    \                                 (5) \n)\ntanh (\n−1\n+\n+\n=\nt\ng\ng\nt\ng\n\
    t\nU h\nb\nW x\ng\n                               (6) \nt\nt\nt\nt\nt\ng\ni\n\
    C\nf\nC\n⊕\n+\n⊕\n=\n−1\n                                      (7) \n)\n(\n−1\n\
    +\n+\n=\nt\no\no\nt\no\nt\nU h\nb\nW x\no\nσ\n                               \
    \    (8) \n)\ntanh(\nt\nt\nt\nC\no\nh\n⊕\n=\n                                \
    \          (9) \nWhere x t is the input at time t, Ct is the current cell state\
    \ \nand ht the output layer hidden at time t. Furthermore, σ is the \nsigmoid\
    \ function, tanh is the hyperbolic tangent function. \nVectors U define the weights\
    \ of each gate in the hidden output \nlayer, vectors W are the weights of each\
    \ gate in the input layer, \nand b are the additive training bias vectors for\
    \ each gate. The \noperator ⊕ is the Hadamard multiplier that performs the \n\
    vector product to generate outputs of the same dimensions as \nthe input. \nIn\
    \ the LSTM model, an epoch corresponds to one pass of \nall the training examples.\
    \ For each epoch a loss function is \nused to evaluate how well the specific algorithm\
    \ models the \ndataset. If the predictions deviate too much from the actual \n\
    results, the loss function will produce a large value. The \nnumber of LSTM cell\
    \ units proposed in the model was 32. \nDropout is a technique used during training\
    \ to avoid \noverfitting.  \nF. Hyperparameter tuning and reproducibility \nHyperparameters\
    \ determine the structure of the network \nmodel and how it is trained. The results\
    \ of multiple simulations \nof the same algorithm with different hyperparameters\
    \ varied in \nthis work. For the training of the irrigation prediction models\
    \ \nin the potato crop, several hyperparameters were selected and \niteratively\
    \ tested to determine the best performances of the \ndeep learning models such\
    \ as the number of neurons or nodes \nper layer, batch size, number of layers,\
    \ dropout values, number \nof epochs, activation functions and optimizers. Table\
    \ II \ndescribes the hyperparameters selected to train the deep \nlearning models\
    \ that presented the best results. \nTABLE II \nHYPERPARAMETERS SETTED TO TRAIN\
    \ DEEP LEARNING MODELS. \nNeural Network \nModel \nHyperparameters \n1-DCNN \n\
    \ \nLSTM \nNodes per Layer \nEpochs \nBatch Size \nActivation Function \nNeurons\
    \ \nOptimizer \nDropout Size \n64, 128, 256 \n64, 128 \n16, 32, 64 \nReLU \n64,\
    \ 128, 256 \nAdam \n0, 0.1, 0.2 \n \nModels with 16 and 32 nodes performed very\
    \ poorly and \nwere excluded from the experimental results. Finally, the \nnetwork\
    \ architectures were tested with 64, 128, and 256 nodes \nper layer. The activation\
    \ function ReLU was used to capture \nthe non-linear relationship between input\
    \ and output, as it \nlooks for positivity in the input arguments. If the input\
    \ value is \npositive, it is returned the value; otherwise, if the value is less\
    \ \nthan zero, value zero (0) is returned as the final output. \nThe optimization\
    \ algorithm selected to minimize the loss \nfunction was Adam optimizer. The Adaptive\
    \ Moment \nEstimation Algorithm (Adam) allows to calculate an adaptive \nlearning\
    \ rate for each of the parameters. Adam also maintains \nan exponentially decreasing\
    \ sum of the past gradients. This \noptimizer works very well in practice as it\
    \ converges faster, \nand the overall learning speed of the model is also quite\
    \ fast \nand efficient, compared to other optimizers. \nG. Implementation Details\
    \ \nTensorFlow library for numerical applications was used \ntogether with Keras\
    \ Deep Learning library. Keras allows the \nconfiguration and training of deep\
    \ neural networks developed \non \nthe \nPython® \nprogramming \nplatform. \n\
    For \nmatrix \ncalculations, model evaluation and graph display, Numpy, \nScikitlearn\
    \ and Matplotlib libraries were used in Python®. The \ndeep neural network models\
    \ were trained using a computer \nequipment with the next specifications: CPU:\
    \ 2.5 GHz, Intel \nCore I7, and 12 GB of RAM.  \nH. Model Evaluation Metrics \n\
    To \nevaluate \nthe \nprediction \nperformance \nof \nthe \nimplemented deep learning\
    \ models, two evaluation metrics \nwere used. To measure the losses of the deep\
    \ learning models, \nthe Mean Square Error (MSE) function was applied, which \n\
    calculates the variance presented by the model, defined in (10):  \n∑\n=\n\n\
    \n\n\n\n\n−\n=\nN\ni\ni\ni\ni\ny\ny\ny\nN\nMSE\n1\n2\nˆ\n1\n           \
    \                           (10) \nTo evaluate the precision of the irrigation\
    \ prescription \nforecast, the Coefficient of Determination R2 was used. R2 is\
    \ a \n21st LACCEI International Multi-Conference for Engineering, Education, and\
    \ Technology: “Leadership in Education and Innovation in Engineering in the Framework\
    \ of Global \nTransformations: Integration and Alliances for Integral Development”,\
    \ Hybrid Event, Buenos Aires - ARGENTINA, July 17 - 21, 2023. \n6 \nstatistical\
    \ measure that calculates the variance explained by the \nmodel on the total variance\
    \ showed in (11):  \n(\n)\n(\n)\n(\n)\n∑\n∑\n∑\n=\n=\n=\n−\n−\n−\n−\n=\nN\ni\n\
    i\nN\ni\ni\ni\nN\ni\ni\ny\ny\ny\ny\ny\ny\nR\n1\n2\n1\n2\n1\n2\n2\nˆ\n        \
    \                (11) \nWhere N defines the number of total observations, yi is\
    \ the \ntrue value at the i-th time,  is the estimated value by the model \nat\
    \ the i-th time, and  is the average of the true values and i \nvaries from 1\
    \ to N. The higher the R2 score, the smaller the \ndifferences between the observed\
    \ data and the adjusted values. \nIII. ANALYSIS AND DISCUSSION \nA. IoT System\
    \ \nA \nnetwork \nof \nfour \ncyber-physical \nsystems \nwas \nimplemented for\
    \ the prescription and application of irrigation \nin an experimental potato crop\
    \ (Fig. 6). It was proposed that \nthe networks of intelligent systems for the\
    \ application of \nirrigation have reactive powers for the prescription and \n\
    application of irrigation, since they depend on the perception \nthey have of\
    \ their environment and the irrigation conditions \nprogrammed by the user. \n\
    \ \nFig. 6. Experimetal Potato Crop used in this study. \n \nFig. 7. Mobile Application\
    \ for Irrigation Management and Monitoring. \nThe availability of WiFi connectivity\
    \ in the study area was \nnecessary for the display, visualization and interpretation\
    \ of the \ninformation in the crop. A mobile application was developed to \ncontrol\
    \ and visualize the status of growing agents, connected \nto devices in the field,\
    \ through a cloud computing platform \n(Cloud-FireBase), hence the importance\
    \ of having a connection \nto Internet Fig. 7. \nB. Training and Testing Performance\
    \ \nThe efficiency of deep learning algorithms can be \ndetermined by evaluating\
    \ the models with the application at \nruntime or by calculating evaluation metrics.\
    \ The overfitting \nand underfitting effects of machine learning models can be\
    \ \nevaluated by comparing losses in the training and testing stages \nof the\
    \ prediction algorithms. In the training of the 1-DCNN \nmodel for the weather\
    \ station, the results showed that the \nexperiments were able to predict the\
    \ irrigation prescription \nwith an MSE lower than 0,105 and validation values\
    \ around \n0,157. The losses in the prediction of the irrigation prescription\
    \ \nfor the implemented LSTM model were lower compared to the \n1-DCNN model,\
    \ where the MSE obtained was 0,082 and \n0,096 for training and validation data\
    \ respectively. \nThe lower training and validation losses for the LSTM \nmodel\
    \ are evident due to increased functionality in the \ncomputational units of the\
    \ LSTM cells. This trend of training \nand validation losses data is listed in\
    \ Table III. \nTABLE III \nPERFORMANCE PRECISION METRICS FOR THE BEST ITERATION\
    \ OF THE DEEP \nLEARNING MODELS \nModel \nTraining Dataset \nValidation Dataset\
    \ \nMSE \nR2 \nMSE \nR2 \n1-DCNN \n0,105 \n0,812 \n0,157 \n0,867 \nLSTM \n0,082\
    \ \n0,881 \n0,096 \n0,919 \n \nFig. 8. shows the comparison of the loss of precision\
    \ of \nthe deep learning models during the training and validation of \nthe dataset\
    \ for the irrigation prescription of the potato crop in \nterms of the number\
    \ of epochs. \n \n  \n \na)                                                  b)\
    \ \nFig. 8. Training and Validation loss curves for the best training performance\
    \ \nin predicting irrigation prescription. a) Loss for 1-DCNN. b). Loss for LSTM.\
    \ \n \nThe number and size of the hidden layers were important \nfor the design\
    \ and evaluation of the deep learning models. The \nhighest R2 value and the minimum\
    \ MSE = 0,157 for the 1-\nDCNN model were obtained when 32 epochs were configured,\
    \ \na dropout of 0, batch size of 32 and 32 epochs with score of R2 \n= 0,867.\
    \ For the LSTM model with 64 cells, dropout of 0.2, \nbatch size of 64 and 128\
    \ epochs, the best performance was \nobtained with an MSE = 0,096 and R2 = 0,919.\
    \  \nThese two models had a favorable statistical performance \nand therefore\
    \ the selection of one of these models over the \nother depends on the available\
    \ data. \n \n21st LACCEI International Multi-Conference for Engineering, Education,\
    \ and Technology: “Leadership in Education and Innovation in Engineering in the\
    \ Framework of Global \nTransformations: Integration and Alliances for Integral\
    \ Development”, Hybrid Event, Buenos Aires - ARGENTINA, July 17 - 21, 2023. \n\
    7 \n \na) \n \nb) \nFig. 9. Irrigation prescription forecast evaluation for trained\
    \ and validated \ndeep learning models and their correlation score. a) 1-DCNN\
    \ Model, b). \nLSTM Model. \nIV. CONCLUSIONS \nPotato crop in the Department of\
    \ Boyacá plays an \nimportant role in the economy and food security of the region\
    \ \nand the country, since a large part of the population depends \non it for\
    \ their livelihood. \nIn this work, a precision irrigation prescription prediction\
    \ \nsystem was developed in an experimental potato crop that \nintegrates remote\
    \ sensing, IoT and machine learning \ntechnologies. \nToday, IoT has expanded\
    \ its capabilities to support smart \nand sustainable agriculture. This type of\
    \ technological \napplications for agriculture allow the farmer to efficiently\
    \ \nmanage the crop, increasing productivity and product quality, \nimproving\
    \ crop yield and optimizing the use of water \nresources. \nAn irrigation prescription\
    \ prediction system was designed \nand implemented using IoT technologies that\
    \ automatically \ncommunicate the central processing station with the \nmeasurement\
    \ station and the weather station to make decisions \nabout the application of\
    \ precise irrigation in the crop. The \ncentral system receives weather data from\
    \ the environment that \nwas entered by two verified deep learning algorithms,\
    \ stored \non RaspBerry Pi platforms. \nThe estimation of the irrigation prescription\
    \ in a potato \ncrop was developed using two deep learning models: 1-DCNN \nand\
    \ LSTM. The models were trained using as inputs climatic \nvariables such as the\
    \ maximum temperature, the average \ntemperature, the minimum temperature, the\
    \ amount of water in \nthe soil at two depths, the wind speed and the calculation\
    \ of \nthe Evapotranspiration of the crop.  \nWhen performing the training and\
    \ validation processes \nwith the deep learning algorithms, the LSTM model \n\
    outperformed the estimation of irrigation prediction when \ncompared with the\
    \ results obtained by the 1-DCNN model. \nThe lowest training and test losses\
    \ for the hybrid LSTM model \nwere 0,082 and 0,096. The LSTM performance algorithm\
    \ had \nthe highest determination coefficient, obtaining a coefficient of \ndetermination\
    \ R2 = 0,919, compared to the 1-DCNN model \nscore, which obtained a score of\
    \ R2 = 0,86. \nAlthough the LSTM model presented better prediction \nresults in\
    \ the irrigation prescription in the potato crop with \nrespect to the results\
    \ of the 1-DCNN model, the differences in \nthe irrigation prescription estimates\
    \ of the deep learning \nmodels were successful. Validation of these deep learning\
    \ \nmodels allowed farmers to properly schedule potato crop \nirrigation according\
    \ to water supply predictions.  \nACKNOWLEDGMENT \nA.F. Jiménez express his gratitude\
    \ to Universidad de los \nLlanos for supporting in the development of this work\
    \ within \nthe framework of the research project “Sistema Inteligente de \nadquisición\
    \ de datos y prescripción de riego en cultivos \nagrícolas mediante redes de sensores\
    \ inalámbricos, IoT e \ninteligencia artificial”.  \nF.R. Jiménez is grateful\
    \ for the support of the I2E \nResearch Group of the School of Electronic Engineering\
    \ of the \nUPTC to develop the First Call Project entitled “Evaluación de \ntécnicas\
    \ de aprendizaje profundo para la prescripción de \nriego en cultivos agrícolas”.\
    \   \nREFERENCES \n[1] \nC.R. Betancourt, T. Tartabull, & Y. Labaut, “Integrated\
    \ management of \nwater in agriculture: need of implementation and linked aspects,”\
    \ Revista \nCientifica Agorsistemas, vol. 5, no. 2, pp. 40-54, Dec. 2017 \n[2]\
    \ \nA.  Goap, D. Sharma, A.K.Shukla, & C.R. Krishna, “An IoT based smart \nirrigation\
    \ management system using Machine learning and open source \ntechnologies,” Computers\
    \ and Electronics in Agriculture, vol. 155, pp. s \n41-49, Dec. 2018. \n[3] \n\
    L. García, L. Parra, J.M. Jimenez, J. Lloret, & P. Lorenz, “IoT-Based \nSmart\
    \ Irrigation Systems: An Overview on the Recent Trends on Sensors \nand IoT Systems\
    \ for Irrigation in Precision Agriculture,” Sensors, vol. 20, \nno. 4, Feb. 2020.\
    \ \n[4] \nM. Keerthana,M. Pooja, M. Raksha, T. Vijaykumar, & M. Smitha, “IoT \n\
    baed Smart irrigation System,” International Research Journal of \nEngineering\
    \ and Technology – IRJET, vol. 08, Issue: 07, pp. 2464-2469, \nJul. 2021. \n[5]\
    \ \nA. Abdelmoamen, A. Suhib, O. Ripendra, A. Fares, & M. Chouikha, “A \ndistributed\
    \ system for supporting smart irrigation using Internet of \nThings technology,”\
    \ Wiley Engineering Reports, pp. 1-13, Dec. 2020. \n[6] \nJ. C. Zhao, J. F. Zhang,\
    \ Y. Feng, & J. X. Guo, “The study and \napplication of the IOT technology in\
    \ agriculture,” Proc. in 2010 3rd \nInternational Conference on Computer Science\
    \ and Information \nTechnology, vol. 2, pp. 462-465, Jul. 2010. \n[7] \nM. Kumari,\
    \ & A. Kumar, “IoT Enabled Smart Irrigation System, \nMonitoring and Water Harvesting\
    \ in Different Soils,” International \nJournal of Engineering Research & Technology\
    \ – IJERT, vol. 10, Issue: \n03, pp. 545-551, Mar. 2021. \n[8] \nC. Shock, T.\
    \ Welch, F. Wang, R. Flock, E. Feibert, A. Shock, & A. \nPereira, “El control\
    \ del riego mediante la tensión matricial del suelo,” \nTecnicas para agricultura\
    \ sostenible, pp. 1-10, Mar.2013. \n[9] \nR. G. Allen, L.S. Pereira, & S. D. Raes,\
    \ “Crop eVapotranspiration—\nGuidelines for Computing Crop Water Requirements\
    \ FAO Irrigation and \nDrainage Paper 56,” FAO—Food and Agriculture Organization\
    \ of the \nUnited Nations: Rome, Italy, 1998. \n[10] E. Chitu, & C. Paltineanu,\
    \ “Relationships between MDS, soil, and \nweather variables for topaz apple tree\
    \ cultivated in coarse-textured soils,” \n21st LACCEI International Multi-Conference\
    \ for Engineering, Education, and Technology: “Leadership in Education and Innovation\
    \ in Engineering in the Framework of Global \nTransformations: Integration and\
    \ Alliances for Integral Development”, Hybrid Event, Buenos Aires - ARGENTINA,\
    \ July 17 - 21, 2023. \n8 \nJournal of Irrigation and Drainage Engineering, vol.\
    \ 145, issue: 2, Feb. \n2019. \n[11] X. Jin, Z. Li, H. Feng, Z. Ren, & S. Li,\
    \ “Estimation of maize yield by \nassimilating biomass and canopy cover derived\
    \ from hyperspectral data \ninto the AquaCrop model,” Agricultural Water Management,\
    \ vol. 227, \nIssue: 105846, Jan. 2021. \n[12] A. Kamilaris, & F. X. Prenafeta-Boldú,\
    \ “Deep learning in agriculture: A \nsurvey,” Comp. and Electron. in Agricult.,\
    \ vol. 147, pp. 70-90, Apr. \n2018. \n[13] C. Shen, “A Transdisciplinary Review\
    \ of Deep Learning Research and Its \nRelevance for Water Resources Scientists,”\
    \ Water Resources Research, \nvol. 54, pp. 8558–8593, Aug. 2018. \n[14] L. Boukhris,\
    \ J. B. Abderrazak, & H. Besbes, “Tailored Deep Learning \nbased Architecture\
    \ for Smart Agriculture,” in Proc. 2020 Internat. \nWireless Communic. and Mobile\
    \ Comput. (IWCMC), pp. 964-969, \n2020. \n[15] X. Cheng, Y. Zhang, Y. Chen, Y.\
    \ Wu, & Y. Yue, “Pest identification via \ndeep residual learning in complex background,”\
    \ Computers and \nElectronics in Agriculture, vol. 141, pp. 351–356, Sept. 2017.\
    \ \n[16] J. G. Barbedo, “Factors influencing the use of deep learning for plant\
    \ \ndisease recognition,” Biosystems Eng., vol. 172, pp. 84–91, Aug. 2018. \n\
    [17] J. Padarian, B. Minasny, & A. B. McBratney, “Using deep learning for \ndigital\
    \ soil mapping,” SOIL, vol.5, pp. 79–89, Feb. 2019. \n[18] O. Manoj & J. P. Ananth,\
    \ “MapReduce and Optimized Deep Network \nfor Rainfall Prediction in Agriculture,”\
    \ The Computer Journal, vol. 63, \nissue 1, pp. 900-912, Jan. 2020. \n[19] C.\
    \ A. Ardila, & N. I. Gomez, “Estado del arte del distrito de riego alto \nChicamocha,”\
    \ Monografía documental como requisito del Diplomado \nprofundización en Hidrotecnia\
    \ para optar al título de Ingeniería Civil, \nUniversidad la GranColombia, Bogotá,\
    \ 2017. \n[20] Weather Spark, “Average Weather in Nobsa, Colombia States,” 2021.\
    \ \nhttps://weatherspark.com/y/25267/Average-Weather-in-Nobsa-\nColombia-Year-Round.\
    \ (last accessed 712 05/12/2021). \n[21] S. Tenzin, S. Siyang, T. Pobkrut, & T.\
    \ Kerdcharoen, “Low cost weather \nstation for climate-smart agriculture,” In\
    \ Proc. 2017 9th international \nconference on knowledge and smart technology\
    \ – KST. vol. 1, pp. 172-\n177, Feb. 2017. \n[22] K. Djaman, M. O’Neill, C. Owen,\
    \ D. Smeal, K. Koudahe, M. West, S. \nAllen, K. Lombard, & S. Irmak, “Crop evapotranspiration,\
    \ irrigation \nwater requirement and water productivity of maize from meteorological\
    \ \ndata under semiarid climate,” Water, vol. 10, Issue: 4, pp. 1-17, Mar. \n\
    2018. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n[23] B. Alhnaity,\
    \ S. Pearson, G. Leontidis, & S. Kollias, “Using deep learning \nto predict plant\
    \ growth and yield in greenhouse environments,” Acta \nHorticulturae, vol. 1296,\
    \ pp. 425-432, Jul. 2019. \n[24] X. Wang, J. Huang, Q. Feng, & D. Yin, “Winter\
    \ wheat yield prediction \nat county level and uncertainty analysis in main wheat-producing\
    \ regions \nof china with deep learning approaches,” Remote Sensing, vol. 12,\
    \ no. \n11, issue: 1744, pp. 1-20, May 2020. \n[25] P. Nevavuori, N. Narra, and\
    \ T. Lipping,. “Crop yield prediction with \ndeep convolutional neural networks,”\
    \ Computers and Electronics in \nAgriculture, vol. 163, no. 11, Aug. 2019. \n\
    [26] S. Khaki, L. Wang, and S. V. Archontoulis, “A CNN-RNN framework \nfor crop\
    \ yield prediction,” Front. Plant Sci., vol. 10, 1750, Jan. 2020. \n[27] H. C.\
    \ Castro, O. A. Carvalho, O. L. Ferreira, P. Pozzobon, R. Dos \nSantos, A. O.\
    \ Albuquerque, C. Rosa, P. H. Guimarães, R. F. Guimarães, \nand R. A. Trancoso,\
    \ “Rice Crop Detection Using LSTM, Bi-LSTM, and \nMachine Learning Models from\
    \ Sentinel-1 Time Series,” Remote \nSensing, vol. 12, no. 16, issue: 2655, pp.\
    \ 1-25, Aug. 2020. \n[28] A. F. Jimenez, B. V. Ortíz, L. Bondesan, G. Morata,\
    \ and D. Damianidis, \n“Long Short‑Term Memory Neural Network for irrigation management:\
    \ a \ncase study from Southern Alabama, USA,” Precision Agriculture, vol. \n22,\
    \  no.2 pp. 475-492, 2021. \n[29] A. Wunsch, T. Liesch, and S. Broda, “Groundwater\
    \ Level Forecasting \nwith Artificial Neural Networks: A Comparison of LSTM, CNN\
    \ an \nNARX,” Hidrology and Earth Systems Sciences Discussions, HESS, vol. \n\
    25, issue: 3, pp. 1671–1687, Apr. 2021. \n[30] M. K. Dharani, R. Thamilselvan,\
    \ P. Natesan, P. C. Kalaivaani, and S. \nKumar, “Review on Crop Prediction Using\
    \ Deep Learning Techniques,” \nJournal of Physics: Conference Series 1767,  012026,\
    \ pp. 1-10, Feb. \n2021. \n[31] X. Wang, Y. Liu, S. Chengjie, B. Wang, & X. Wang,\
    \ “Predicting \npolarities of tweets by composing word embeddings with long short-term\
    \ \nmemory,” in Proc. 53rd annual meeting of the association of computing \nlinguistics\
    \ and 7th int. joint conference on natural language process, \nvol. 1, pp. 1343–1353,\
    \ Jul. 2015. \n"
  inline_citation: '>'
  journal: ''
  limitations: '>'
  pdf_link: https://laccei.org/LACCEI2023-BuenosAires/papers/Contribution_965_a.pdf
  publication_year: 2023
  relevance_score1: 0
  relevance_score2: 0
  title: Forecasting irrigation scheduling based on deep learning models using IoT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1155/2022/5614974
  analysis: '>'
  authors:
  - Qiusi Zhang
  - Bo Li
  - Yong Zhang
  - Shufeng Wang
  citation_count: 0
  full_citation: '>'
  full_text: ">\nResearch Article\nSuitability Evaluation of Crop Variety via Graph\
    \ Neural Network\nQiusi Zhang\n,1,2 Bo Li\n,3 Yong Zhang\n,3 and Shufeng Wang\n\
    1\n1Information Technology Research Center, Beijing Academy of Agriculture and\
    \ Forestry Sciences, Beijing 100097, China\n2National Engineering Research Center\
    \ for Agroecological Big Data Analysis & Application,\nSchool of Electronics and\
    \ Information Engineering, Anhui University, Hefei 230601, China\n3Beijing Key\
    \ Laboratory of Multimedia and Intelligent Software Technology, Beijing Institute\
    \ of Artifcial Intelligence,\nDepartment of Information Science, Beijing University\
    \ of Technology, Beijing 100124, China\nCorrespondence should be addressed to\
    \ Shufeng Wang; wangsf@nercita.org.cn\nReceived 26 May 2022; Revised 25 June 2022;\
    \ Accepted 29 June 2022; Published 9 August 2022\nAcademic Editor: Dalin Zhang\n\
    Copyright © 2022 Qiusi Zhang et al. Tis is an open access article distributed\
    \ under the Creative Commons Attribution License,\nwhich permits unrestricted\
    \ use, distribution, and reproduction in any medium, provided the original work\
    \ is properly cited.\nWith the continuous growth of the global population, insufcient\
    \ food production has become an urgent problem to be solved in\nmost countries.\
    \ At present, using artifcial intelligence technology to improve suitability between\
    \ land and crop varieties to\nincrease crop yields has become a consensus among\
    \ agricultural researchers. However, there are still many problems in existing\n\
    works, such as limited crop phenotypic data and the poor performance of artifcial\
    \ intelligence models. In this regard, we take\nmaize as an example to collect\
    \ a large amount of environmental climate and crop phenotypic traits data at multiple\
    \ experimental\nsites and construct an extensive dataset. Ten, we introduce a\
    \ graph neural network model to learn crop suitability evaluation and\nfnally\
    \ achieve a good evaluation efect. Te evaluation results of the model can not\
    \ only provide a reference for expert evaluation\nbut also judge the suitability\
    \ of the variety to other test trial sites according to the data of the current\
    \ one, so as to guide future\nbreeding experiments.\n1. Introduction\nCrop variety\
    \ suitability evaluation refers to the suitability of crop\nvariety growth for\
    \ corresponding planting land. Soil conditions\nand climatic environments vary\
    \ signifcantlyfrom place to place,\nand the suitability of diferent crop varieties\
    \ difers greatly. Select\nsuitable varieties for planting, and then maximize the\
    \ use of\nlimited land resources to produce more food. Afected by many\nfactors\
    \ such as the outbreak of new coronavirus pneumonia,\nclimate change, and frequent\
    \ natural disasters, the world food\nsecurity situation has become more severe\
    \ in recent years, which\nmay lead to a further increase in the global hunger\
    \ population. In\nthis regard, the world food security situation has become more\n\
    severe in recent years, leading to a further increase in the global\nhunger population,\
    \ so that future crop varieties can be accurately\nplanted on suitable land, to\
    \ improve food production.\nClimate change will continue to afect the whole period\n\
    of crop growth, which has a great impact on the suitability\nevaluation of crop\
    \ varieties. Long-term climate change leads\nto large-scale reallocation of freshwater\
    \ resources resulting\nin changes in crop breeding [1, 2]. Literature [3] points\
    \ out\nthat, due to climate change in the next few years, the total\noutput of\
    \ crops will decline, which is in great contradiction\nwith the growing food demand\
    \ of the global population. To\nalleviate this contradiction, we need to actively\
    \ explore the\nrelationship between climate change and crop variety\nadaptability\
    \ and optimize the utilization of land resources.\nCrop phenotypic traits are\
    \ the intuitive expression of the\nsuitability between crop growth and current\
    \ land, and the\nresult of the interaction between environmental factors such\n\
    as soil and climate and crop varieties. Crop variety selection\nbased on crop\
    \ phenotype was relatively systematic long\nbefore technologies such as DNA and\
    \ molecular markers\nemerged. Even the same crops and genes will produce\ndiferent\
    \ phenotypes in diferent environments. Ultimately,\ncrop harvest is phenotypic\
    \ data, not genome. Terefore,\ndirect research and analysis of crop phenotype\
    \ are the most\nnatural and efective method. However, the biggest problem\nis\
    \ that phenotypic data is not enough to support extensive\ndata analysis.\nHindawi\n\
    Computational Intelligence and Neuroscience\nVolume 2022, Article ID 5614974,\
    \ 10 pages\nhttps://doi.org/10.1155/2022/5614974\nCrop suitability evaluation\
    \ has always been a major\nproblem in agricultural production, but the currently\
    \ used\nevaluation and analysis methods are outdated and have low\nevaluation\
    \ accuracy. Most of the existing methods are based\non traditional machine learning\
    \ methods. Tis method\ntreats each piece of data as an independent sample and\
    \ lacks\nthe exploration of the relationship between the data. Te\nlater introduction\
    \ of deep learning made the model more\npowerful in nonlinear ﬁtting but still\
    \ failed to model higher-\norder correlations between data.\nGiven the the lack\
    \ of variety suitability evaluation\ndataset, we collected crop variety trait\
    \ data and environ-\nmental-climate data from multiple breeding sites in the past\n\
    ﬁve years (2017–2021), with a total of 10,000 records. Each\nrecord includes 15\
    \ of trait data and 24 of climate data, and\nexperts are invited to conduct corresponding\
    \ suitability\nevaluation, and experts are invited to conduct corresponding\n\
    suitability evaluations. Considering the high-order complex\ncorrelation between\
    \ crop phenotypic traits and climate data\n[4–6], we incorporate climate data\
    \ into the learning suit-\nability assessment. Ten, we use the graph neural network\
    \ to\nlearn the association representation between the data, and\nﬁnally achieve\
    \ better evaluation accuracy. Overall, this paper\nmainly includes the following\
    \ three contributions:\n(1) We have collected a large amount of data related to\n\
    cultivar adaptability, alleviating the diﬃculty of the\nscarcity of datasets in\
    \ the current ﬁeld.\n(2) Te graph neural network model is introduced into\nthe\
    \ variety suitability evaluation, and good evalua-\ntion results were obtained.\n\
    (3) Te results of the experiments can provide a refer-\nence for future breeding\
    \ programs and improve\nbreeding eﬃciency.\n2. Related Works\nVariety suitability\
    \ evaluation is a long-term problem, and\nmany works in this ﬁeld have guiding\
    \ signiﬁcance for ag-\nricultural production. Below we brieﬂy introduce some\n\
    representative works.\n2.1. Relevant Works of Variety Suitability Evaluation.\
    \ Te\nauthors of [7] believe that environmental climate and genetic\nfactors jointly\
    \ aﬀect the ﬁnal yield of crops, so the authors\naim to understand the impact\
    \ of climate on agriculture\nthrough methods similar to quantitative genetics,\
    \ and to\nimprove crop yield through selection, manipulation, and\nediting of\
    \ genetic variations. Traditional empirical land\nassessment and soil surveys\
    \ rely on expert explanations.\nTey cannot answer future land use issues, such\
    \ as future\nclimate change, including the availability of water resources,\n\
    and the introduction of new crop hybrids. In this regard, [8]\nexplores the eﬀect\
    \ of limited water availability on the growth\nof various maize hybrids under\
    \ future climatic conditions.\nLiterature [9] is committed to developing an eﬃcient\
    \ ﬁeld\nhigh-throughput phenotypic analysis platform to make\ncrop-related data\
    \ collection more comprehensive and\naccurate. Literature [10] focuses on the\
    \ current and long-\nterm needs of society. Te authors believe that the future\n\
    breeding data will integrate genetic, statistical, and gene-\nphenotypic traits\
    \ to promote our understanding of func-\ntional germplasm diversity and gene-phenotypic-trait\
    \ rela-\ntionships in local and transgenic crops. Literature [11] is\ncommitted\
    \ to exploring ﬁeld climate intelligent crops, using\na large amount of data from\
    \ phenotypic and genomic\ndatasets. Te authors integrate genome and crop phenotypic\n\
    information into speciﬁc databases and intelligent platforms\nand then select\
    \ the appropriate climate environment to\nmake crops adapt to the environment\
    \ and ultimately im-\nprove crop yield.\n2.2. Deep Learning in Agriculture. Agriculture\
    \ is closely\nrelated to people’s daily life, and its importance at the na-\n\
    tional level is self-evident. Given the amazing learning ability\nof deep learning\
    \ and the rapid accumulation of agricultural\ndata, many researchers have begun\
    \ to explore how to use the\ntechnology to guide agricultural production. Below\
    \ we\nbrieﬂy introduce some recent works using deep learning for\nagricultural\
    \ production and then introduce the application\nof graph neural networks in agriculture.\
    \ Te impact of\nweather data on sustainable agricultural production is\nenormous,\
    \ but the complex nonlinear relationship between\ndata makes weather data unpredictable.\
    \ In response,[12]\nproposes a deep learning predictor with a continuous two-\n\
    level decomposition structure, which continuously de-\ncomposes weather data into\
    \ four components and then\ntrains a Gated Recurrent Unit (GRU) network as a sub-\n\
    predictor for each component. Literature [13] is dedicated to\nsolving crop management\
    \ problems in agricultural auto-\nmation. Te authors use convolutional neural\
    \ network\ntechnology to identify weeds in the early stages of crop\ngrowth and\
    \ control the side eﬀects of weeds on crop growth,\nthereby improving yields.\
    \ Tey propose AgroAVNET, a\nhybrid model based on AlexNet and VGGNET, with a\n\
    extensive performance improvement compared to existing\nmethods. Literature [14]\
    \ is dedicated to using past agri-\ncultural production data to predict future\
    \ agricultural\nproduction. Te authors propose a deep learning model\nAGR-DL based\
    \ on CNN and RNN. Te experimental results\nshow that the prediction accuracy of\
    \ the model is better than\nthat of classical algorithms such as SVM, MLP, and\
    \ AdaBoost.\nFaced with limited water resources and arable land resources,\nhow\
    \ to maximize the utilization has become the common goal\nof researchers. In this\
    \ regard, [15] proposes an IoT precision\nagriculture intelligent irrigation system\
    \ based on deep\nlearning neural network. It can make arable land smarter by\n\
    using a long short-term memory network to predict the\nprevious day’s volumetric\
    \ soil moisture content and irrigation\ncycle. Te combination of Industry 4.0\
    \ and smart agriculture\nis the future development direction, but IoT devices\
    \ have\nalways faced the potential risk of being attacked. In this\nregard, [16]\
    \ proposes a DDoS attack intrusion detection\nnetwork based on convolutional neural\
    \ network, deep neural\nnetwork, and recurrent neural network, which ensures the\n\
    security of thousands of IoT-based smart devices.\n2\nComputational Intelligence\
    \ and Neuroscience\nLiterature [17] uses graph convolutional neural net-\nworks\
    \ to encode knowledge implicit in the GO hierarchy.\nTe authors propose a DeepGOA\
    \ model to predict protein\nannotations, achieving superior performance to deep\n\
    learning. Literature [18] is dedicated to exploring the eﬀects\nof soil composition\
    \ on vegetation growth, and ultimately to\nrational irrigation scheduling and\
    \ optimization of water\nuse tools. Te authors construct an end-to-end framework,\n\
    using graph neural network to learn time graph structure\nand soil moisture. Literature\
    \ [19] uses a graph-based re-\ncurrent neural network to predict crop yield. Te\
    \ authors\nfurther improve the prediction ability of the model by\nreasonably\
    \ utilizing the knowledge of geography and time,\nwhich is superior to the most\
    \ advanced methods. Literature\n[20] is committed to graph neural networks to\
    \ classify the\nmaturity of avocado. Te authors create a set of alligator\nimage\
    \ data and then use the node classiﬁcation method of\ngraph neural network to\
    \ classify them.\nTe above works have improved the suitability between\ncrops\
    \ and planting sites. However, there are still many\nunsolved problems. For example,\
    \ the dataset collected by [7]\nis small, and the most important crop phenotypic\
    \ data in\nsuitability evaluation is only 6 kinds, which is seriously\ninsuﬃcient.\
    \ In addition, the methods used in most suitability\nevaluation works are outdated,\
    \ and there is much room for\nimprovement.\n3. Data Collection\nAccording to the\
    \ Bureau of Statistics and China Institute of\nCommerce and Industry, corn is\
    \ one of the essential food\ncrops in China, and its crop yield exceeds that of\
    \ rice and\nwheat. In 2021, the national grain ﬁeld was 6.3275 million\ntons,\
    \ 1.6 million tons more than the previous year, an in-\ncrease of 2.6%. Of these,\
    \ rice production was 21.285 million\ntons, up 100,000 tons or 0.5% of the prior\
    \ years; wheat\nproduction was 13.695 million tons, up 270,000 tons or 2.0%\n\
    of the prior years; and and corn production was 27.255\nmillion tons, up 1.64\
    \ million tons or 4.6% of the prior year.\nAs of December 2021, China’s grain\
    \ yield was 5805 kg/ha,\nunchanged from the previous year. Among grain crops,\
    \ rice\nyield was the highest at 7,113.4 kg/ha, while corn and wheat\nyields were\
    \ 6,291 and 5,863 kg/ha, respectively. Our phe-\nnotypic data and climatic data\
    \ used in this paper are from 14\ntest trial sites in mainland China, including\
    \ Beijing-Tianjin-\nHebei, Northeast, North China, Huang-Huai-Hai, North-\nwest,\
    \ and Southwest. Assessing the suitability of target va-\nrieties and planting\
    \ sites requires large amounts of\nexperimental data, and the corresponding costs\
    \ are often\nenormous [21].\n3.1. Data Introduction. Trough the collection and\
    \ collation\nof crop experimental data in the past ﬁve years, we have\n10,000\
    \ tabular datasets, each of which describes in detail the\nmultiple traits of\
    \ a certain maize variety at a certain ex-\nperimental point, including leaf blight,\
    \ lodging rate, in-\nversion rate, grey speck disease, plant height, ear height,\n\
    empty stalk rate, duration period, ear rot, hundred-grain\nweight, ear length,\
    \ bald tip length, fresh ear ﬁeld, acre yield,\nand relative change of yield.\
    \ Next, we will detail what each\ntrait dataset means and its possible eﬀect on\
    \ the crop.\n3.1.1. Leaf Blight (LB). Te disease is caused by Corynespora\numbilicus.\
    \ It mainly damages leaves, and in severe cases, it\nalso damages leaf sheaths\
    \ and bracts. It generally starts at the\nbottom leaf and gradually expands upwards.\
    \ Te disease is\nwidely distributed in all maize-growing regions in the world\n\
    and generally reduces maize production by 15–20%, and in\nsevere cases, it reduces\
    \ production by more than 50%. Te\noccurrence and prevalence of the disease are\
    \ comprehen-\nsively aﬀected by many factors such as disease resistance of\ninbred\
    \ lines, crop rotation system, climatic conditions, and\ncultivation measures.\n\
    3.1.2. Lodging Rate (LR). Lodging refers to the phenomenon\nthat crops that grow\
    \ upright are skewed due to excessive\ngrowth or even fall to the ground. Lodging\
    \ rate refers to the\npercentage of plants with a slope greater than 45 degrees\
    \ to\nthe total number of plants. It reﬂects the tilt or landing of\nmaize plants\
    \ due to wind and rain or improper management\nin the growth process of maize.\
    \ Te main reason for corn\nlodging is the weather, mainly rainy days in the jointing\n\
    period and storms in the grain-ﬁlling period.\n3.1.3. Inversion Rate (IR). It\
    \ refers to the percentage of plants\nbroken below the ear in the total number\
    \ of plants after\ntasseling. Tis phenomenon generally occurs about ten days\n\
    before the corn tassel stage, when the corn stalks are easily\nbroken by strong\
    \ winds. Tis situation is related to the heredity\nof varieties and the climatic\
    \ environment (such as wind speed)\nof planting sites.\n3.1.4. Grey Speck Disease\
    \ (GSD). Grey speck disease is one of\nthe most devastating corn diseases in northern\
    \ China,\nmainly aﬀecting the leaves. It is mainly harmful to leaves. In\nthe\
    \ early stages, rounded gray spots without distinct edges\nform on the surface\
    \ of the leaves, later turning brown. In\nsevere cases, most of the leaves turn\
    \ yellow and scorch, the\nears droop, the grains are loose and dry, and the 100-grain\n\
    weight decreases, which seriously aﬀects the yield and\nquality. Te disease is\
    \ obviously aﬀected by the climate, and\nit is easy to occur in weather conditions\
    \ with many rainy\ndays, high air humidity, and poor light.\n3.1.5. Plant Height\
    \ (PH). Plant height refers to the height of\nthe corn plant. Tis index has a\
    \ great inﬂuence on the yield\nand lodging rate of varieties. If the corn plant\
    \ is too high, it\nwill be more aﬀected by natural disasters such as strong wind\n\
    and heavy rain during the critical period of corn production.\nTe plant height\
    \ of corn is greatly aﬀected by fertilization.\nFor example, excessive nitrogen\
    \ fertilizer but lack of po-\ntassium fertilizer will cause the plant to grow\
    \ too vigorously,\nand the plant will be too high but the yield will decrease.\n\
    Computational Intelligence and Neuroscience\n3\n3.1.6. Ear Height (EH). It is\
    \ the length from the root of the\ncorn to the bottom of the ear of the corn.\
    \ Te lower the ear\nposition of corn is, the stronger the lodging rate is, and\
    \ on\nthe contrary, lodging occurs easily. Terefore, people prefer\nthe varieties\
    \ with low ear position and sometimes artiﬁcially\nsuppress the ear position.\
    \ Te ear height is mainly deter-\nmined by the variety but also has a certain\
    \ relationship with\nthe environment.\n3.1.7. Empty Stalk Rate (ESR). Empty stalk\
    \ generally refers\nto corn without ears, and the empty stalk rate generally refers\n\
    to the percentage of the total number of corn plants without\nears or ears without\
    \ seeds after the corn matures. Empty\nstalk rate is a common phenomenon in corn\
    \ production, and\nthe empty bar rate directly aﬀects the level of corn yield.\
    \ If\ncorn encounters rainy weather during the ﬂowering period,\nthe empty stalk\
    \ rate of some corn varieties may be as high as\n50% to 60%, resulting in a sharp\
    \ drop in corn yield.\n3.1.8. Duration Period (DP). It refers to the number of\
    \ days it\ntakes corn to mature from sowing to new seeds. Diﬀerent\nvarieties\
    \ of corn have diﬀerent duration periods, and climatic\nconditions will also lead\
    \ to changes in corn duration periods,\nsuch as north-south diﬀerences. According\
    \ to the length of\nthe duration period, corn varieties are also divided into\
    \ early-\nmaturing and late-maturing. Terefore, diﬀerent regions and\ndiﬀerent\
    \ varieties of corn have diﬀerent duration periods.\n3.1.9. Ear Rot (ER). Corn\
    \ ear rot is a disease caused by a\nvariety of pathogens, mainly caused by more\
    \ than 20 kinds\nof molds such as Fusarium graminearum, Penicillium, and\nAspergillus.\
    \ Te disease occurs in all corn-producing regions\nin China, especially in the\
    \ rainy and humid southwest. Some\npathogenic bacteria that cause this disease,\
    \ such as Asper-\ngillus ﬂavus, can produce toxic metabolites such as aﬂa-\ntoxins,\
    \ which cause serious harm to the health of humans,\nlivestock, and poultry. Te\
    \ disease is mainly related to the\nvariety, and the humid environment also has\
    \ a certain\ninﬂuence.\n3.1.10.\nHundred-Grain\nWeight\n(HGW). Hundred-grain\n\
    weight refers to the weight of 100 seeds, expressed in grams,\nand is an indicator\
    \ of seed size and plumpness. Te weight of\n100 grains of corn is generally around\
    \ 26–28 grams. If the\nvariety is good and the planting level is high, it can\
    \ generally\nexceed 30 grams. If you want to increase the grain weight, the\n\
    sowing date can be determined according to the local annual\ntemperature to meet\
    \ the accumulated temperature demand\nof the corn, so that the grains are within\
    \ the suitable grain-\nﬁlling temperature range. Tis index is aﬀected by corn\
    \ size\nand moisture content and varies by cultivar and growing\ntechnique.\n\
    3.1.11. Ear Length (EL). Ear length refers to the length of the\nwhiskers on the\
    \ tip of the corn cob. It is mainly determined\nby cultivar genes.\n3.1.12. Bald\
    \ Tip Length (BTL). Bald tip length refers to the\nlength of the tip and top of\
    \ the cob when corn is harvested\nwithout small kernels. Fresh ear ﬁeld is determined\
    \ by\nvarious factors such as the quality of corn varieties, soil\nmoisture, soil\
    \ fertility, pests and diseases, planting density,\nand planting technology.\n\
    3.1.13. Fresh Ear Field (FEF). Fresh ear ﬁeld refers to the\nweight of the mature\
    \ ear of fresh corn, which has a strong\ncorrelation with the yield per mu.\n\
    3.1.14. Corn Acre Yield (CAY). Corn acre yield refers to the\nweight of dry corn\
    \ kernels harvested on an acre of land.\nDiﬀerences in geographical environment,\
    \ varieties, man-\nagement techniques, etc. may lead to diﬀerent corn yields.\n\
    3.1.15. Relative Change of Yield (RCY). Relative change of\nyield refers to the\
    \ change of corn yield at the planting ex-\nperimental point relative to the reference\
    \ group. Tis index\nreﬂects the yield gap between the current experimental\nvariety\
    \ and the control group and is an important basis for\nour suitability evaluation.\n\
    Considering the impact of environmental and climatic\nfactors on the growth of\
    \ crops, we also collected daily en-\nvironmental and climatic data of each experimental\
    \ point,\nincluding temperature, air pressure, and humidity. Ten, the\nclimate\
    \ data of each variety growth cycle were preprocessed:\nthe mean and variance\
    \ of climate from sowing to maturity of\nmaize varieties were taken, including\
    \ the maximum tem-\nperature (MaxT), average temperature (AT), minimum\ntemperature\
    \ (MinT), temperature diﬀerence (TD), ground\npressure (GP), relative humidity\
    \ (RH), precipitation (P),\nmaximum wind speed (MWS), average wind speed (AWS),\n\
    wind direction angle (WDA), sunshine time (ST), and wind\nlevel (WL). Finally,\
    \ the above 15 crop phenotypic traits\ndatasets and the climate data of 24 test\
    \ trial sites were in-\ntegrated into the variety suitability evaluation data.\n\
    3.2. Data Preprocess. We further process the above data so\nthat it can be used\
    \ for model training. Data processing can be\nsimply divided into two steps: outliers\
    \ processing and data\nstandardization. Due to environmental diﬀerences in dif-\n\
    ferent test trial sites, some of the traits are not collected or\nrecorded correctly,\
    \ resulting in some outliers or missing\nvalues in the data. We ﬁrst manually\
    \ ﬁlter out possible\noutliers from the data and then ﬁll the average of these\n\
    feature data. Data standardization is mainly to solve the\nproblem of diﬀerent\
    \ dimensions of current data indexes.\nDiﬀerent evaluation indexes often have\
    \ diﬀerent dimensions\nand dimension units, and the direct addition cannot cor-\n\
    rectly reﬂect the comprehensive results of diﬀerent index. In\norder to eliminate\
    \ the dimensional impact between indexes,\ndata standardization is needed to achieve\
    \ comparability\nbetween datasets. Te visualization of data distribution\nbefore\
    \ and after standardization is shown in Figure 1.\nIn addition, we also carried\
    \ out data normalization\nexperiments, detailed in Tables 1and 2. Te experimental\n\
    4\nComputational Intelligence and Neuroscience\nresults show that, compared with\
    \ standardization, nor-\nmalization reduces the accuracy of the model. We infer\
    \ that\nthe reason is that the diﬀerence between the maximum value\nand the minimum\
    \ value in the data of various traits is large,\nand after normalizing it, the\
    \ boundaries between many\ndatasets are more blurred, and the model is diﬃcult\
    \ to\nidentify, so the accuracy of the model decreases.\n4. Data Correlation Analysis\n\
    Tis chapter is devoted to exploring the relationship between\nvariety suitability\
    \ and crop traits and the environmental\nclimate data of the test site. To further\
    \ understand the\ncomplex correlations between the datasets, we used the\nPearson\
    \ correlation coeﬃcient to analyze the correlations\nbetween the datasets.\nTere\
    \ are 39 types of experimental data, including 24\nkinds of climate data and 15\
    \ kinds of crop traits data. We\nﬁrst analyze the correlation between the datasets,\
    \ that is, the\nrelationship between the 39 types of data and the proposed\nlabel.\
    \ Te recommended variety labels fall into two cate-\ngories: termination test\
    \ and continuing test. Te former\nindicates that the crop is unsuitable for the\
    \ test trial site and\nshould be abandoned. Te latter indicates the variety has\n\
    good performance in the test trial site and could be further\ntested or planted\
    \ in large areas. Pearson correlation coef-\nﬁcient is used to measure the correlation\
    \ between recom-\nmended labels and climate and trait data, deﬁned as the\nquotient\
    \ of covariance and standard deviation between two\nvariables, as shown in Formula\
    \ (1). Finally, the relevant\nconclusions are shown in Table 3. For ease of viewing,\
    \ we\nroughen up the data that is more relevant.\nc \x88\n\U0010FF50n\ni\x881\
    \ Xi − X\n\0\U0010FF01 Yi − Y\n\0\U0010FF01\n\x81\x81\x81\x81\x81\x81\x81\x81\x81\
    \x81\x81\x81\x81\n\U0010FF50n\ni\x881 Xi − X\n\0\U0010FF01\n2\n\U0010FF71\n\x81\
    \x81\x81\x81\x81\x81\x81\x81\x81\x81\x81\x81\n\U0010FF50n\ni\x881 Yi − Y\n\0\U0010FF01\
    \n2\n\U0010FF71\n.\n(1)\nIt can be seen from Table 3 that the most relevant data\
    \ on\nthe recommended label of crop varieties is the relative\nchange of yield,\
    \ which represents the relative relationship\nbetween the current crop yield and\
    \ the reference group. In\naddition, the relative humidity, sunshine time, and\
    \ mini-\nmum temperature of the current test trial site environment\nalso have\
    \ a great impact on variety proposed label.\nAmong the experts’ evaluation criteria\
    \ of variety\nadaptability, relative change of yield is the most important\nreference\
    \ index, which also conforms to the variety suit-\nability judgment in most cases;\
    \ that is, yield increase means\nbetter adaptability. In other words, the goal\
    \ of variety\nsuitability can be attributed to increasing crop yield to some\n\
    extent. It is worth mentioning that, in Section 6.2 of this\narticle, we also\
    \ conducted experiments that do not use the\nrelative change of yield index to\
    \ determine the suitability of\nvarieties. Secondly, relative humidity directly\
    \ reﬂects the soil\nmoisture status. Relative humidity can increase maize leaf\n\
    area and yield to some extent [22, 23]. Ten, sunshine time\ndirectly determines\
    \ the time of crop photosynthesis, af-\nfecting the various stages of crop growth.\
    \ Maize is a short-\nday crop, and the whole growth period requires strong light,\n\
    so sunshine time has a greater impact on crops [24, 25].\nFinally, because maize\
    \ is a light-loving crop, it needs higher\ntemperature during the whole growth\
    \ period, so the eﬀect of\nminimum temperature on maize growth is more obvious.\
    \ If\nthe temperature of corn seedling stage is too low, it will lead\nto delayed\
    \ emergence and increased chance of infection.\nLow temperature during the growth\
    \ period of maize will lead\nto dwarﬁng of plants and poor growth and leaf development.\n\
    Low temperatures during the ripening period will delay the\ntime for corn to ripen.\
    \ Literature [26] reaches similar\nconclusions on the relationship between the\
    \ minimum\ntemperature and crop growth.\n5. Graph Neural Network Model for\nSuitability\
    \ Evaluation\nWe treat breed suitability evaluation as a classiﬁcation task.\n\
    Unlike previous methods based on machine learning and\nmultilayer perceptual networks,\
    \ graph neural networks can\nexploit the correlation between graph datasets to\
    \ inform\nsuitability evaluation. Te task of variety suitability evalu-\nation\
    \ is to judge the suitability of crops and test trial sites\nthrough phenotypic\
    \ data of crops and climate and envi-\nronmental data of test trial sites. Te\
    \ input to the model is\ntabular data, and the ﬁnal classiﬁcation result is output.\n\
    Machine learning or multilayer perceptron methods are\ngenerally not suitable\
    \ for tabular data, and they cannot ﬁnd\noptimal solutions to tabular decision\
    \ manifolds due to lack\nof proper inductive bias. Second, NLP-based methods are\n\
    diﬃcult to apply due to the lack of strong semantic asso-\nciations between columns.\
    \ In contrast, graph neural net-\nworks can model correlations between datasets,\
    \ using\nassociations to classify tabular data. Furthermore, consid-\nering the\
    \ large diﬀerences in the distribution of climate and\nsoil conditions among our\
    \ test trial sites, the introduction of\ngraph neural networks can also eﬀectively\
    \ exploit the geo-\ngraphic relationship between test trial sites. When the model\n\
    is predicting one of the test trial sites, the characteristics of\nthe adjacent\
    \ test trial sites can be combined with its own\ncharacteristics to improve the\
    \ prediction ability. Next, we\nbrieﬂy introduce the development process of graph\
    \ neural\nnetwork, then describe the construction method of graph,\nand ﬁnally\
    \ compare and analyze the experimental results of\nthe model.\nGraph neural network\
    \ is a new type of neural network.\nTe neural network adopts the idea of bionics\
    \ to realize\nmodeling by simulating the structure and function of the\nbiological\
    \ neural network. It can be regarded as a black box\nwhere we input speciﬁc data\
    \ features and obtain speciﬁc\noutput. Neural network can often learn the mapping\
    \ rela-\ntionship between input and output through internal itera-\ntions to meet\
    \ our task requirements. Speciﬁcally, classical\nneural network can be divided\
    \ into input layer, intermediate\nlayer (also known as hidden layer), and input\
    \ layer. Te\nnumber of nodes in the input layer and output layer is often\nﬁxed,\
    \ and the middle layer can be freely speciﬁed to hide any\nnumber of nodes. Experience\
    \ shows that the two-layer\nneural network can approximate any continuous function\n\
    and has very good data ﬁtting ability.\nComputational Intelligence and Neuroscience\n\
    5\nTable 3: Data correlation between 39 types of data and proposed label. Te trait\
    \ data are replaced by abbreviations. Climate data are preﬁxed\nwith A and V,\
    \ which represent the mean and variance, respectively.\nLB-0.041\nLR-0.059\nIR-0.052\n\
    GSD-0.015\nPH 0.014\nEH 0.016\nESR-0.062\nDP-0.017\nER 0.013\nHGW 0.047\nEL 0.038\n\
    BTL-0.011\nFEF 0.033\nAY 0.045\nRCY 0.346\nLabel 1.000\nAMaxT 0.046\nVMaxA 0.044\n\
    AAT-0.053\nVAT-0.041\nAMinT 0.071\nVMinT-0.046\nATD-0.054\nVAT -0.058\nAGP-0.023\n\
    VGP -0.018\nARH-0.079\nVRH-0.087\nAP 0.025\nVP 0.010\nAMWS-0.046\nVMVS -0.062\n\
    AAWS-0.012\nVAWS-0.012\nADWA 0.048\nVWDA 0.024\nAST-0.073\nVST-0.042\nAWR-0.048\n\
    VWL -0.027\nFrequency\nValue\n1600.00\n1400.00\n1200.00\n1000.00\n800.00\n600.00\n\
    400.00\n200.00\n0.00\n90.00\n102.16\n114.32\n126.48\n138.64\n150.80\n162.96\n\
    175.12\n187.28\n199.44\n211.6\n223.76\n235.92\n248.08\n260.24\n272.40\n284.56\n\
    296.72\n308.88\n321.04\n333.20\n345.36\n357.52\n369.68\n381.84\n394.00\n406.16\n\
    Frequency\n3500.00\n3000.00\n2500.00\n2000.00\n1500.00\n1000.00\n500.00\n0.00\n\
    Frequency\n-95.00\n-88.12\n-81.23\n-74.35\n-67.46\n-60.58\n-53.70\n- 46.81\n-39.93\n\
    -33.04\n-26.16\n-19.28\n-12.39\n-5.51\n1.38\n8.26\n15.14\n22.03\n28.91\n35.80\n\
    42.68\n49.56\n56.45\n63.33\n70.22\n77.10\nValue\nFrequency\n3500.00\n3000.00\n\
    2500.00\n2000.00\n1500.00\n1000.00\n500.00\n0.00\nFrequency\nValue\n-11.00\n-10.26\n\
    -9.51\n-8.77\n-8.02\n-7.28\n-6.54\n-5.79\n-5.05\n-4.30\n-3.56\n-2.81\n-2.07\n\
    -1.33\n-0.58\n0.16\n0.91\n1.65\n2.39\n3.14\n3.88\n4.63\n5.37\n6.11\n6.86\n7.60\n\
    8.35\n9.09\nFrequency\nValue\nFrequency\n1600.00\n1400.00\n1200.00\n1000.00\n\
    800.00\n600.00\n400.00\n200.00\n0.00\n-5.62\n-5.23\n-4.85\n-4.47\n-4.08\n-3.70\n\
    -3.32\n-2.93\n-2.55\n-2.17\n-1.79\n-1.40\n-1.02\n-0.64\n-0.25\n0.13\n0.51\n0.90\n\
    1.28\n1.66\n2.05\n2.43\n2.81\n3.20\n3.58\n3.96\n-6.00\nFrequency\nFigure 1: Visualization\
    \ of numerical distribution of relative change of yield (up) and plant height\
    \ (down); the left column is the original\ndata, and the right column is after\
    \ normalization.\nTable 1: Accuracy comparison of the following networks with\
    \ diﬀerent numbers of training samples.\nNumber of training samples\n50\n100\n\
    400\n700\n1000\n2000\nRaw data\n0.651\n0.660\n0.666\n0.674\n0.673\n0.675\nStandardized\
    \ data\n0.655\n0.693\n0.731\n0.743\n0.748\n0.758\nNormalized data\n0.551\n0.556\n\
    0.562\n0.555\n0.564\n0.576\nTable 2: Te performance comparison of traditional\
    \ machine learning methods and neural network under two data initialization situations.\n\
    Important data are mar—d in bold.\nKNN\nLR\nSVM\nNB\nRF\nDT\nMLP\nRBFNN\nGAT\n\
    GCN\nAccuracy\n0.647\n0.676\n0.671\n0.567\n0.670\n0.553\n0.684\n0.681\n0.731\n\
    0.748\nPrecision score\n0.628\n0.651\n0.644\n0.562\n0.662\n0.563\n0.670\n0.657\n\
    0.743\n0.687\nRecall score\n0.870\n0.880\n0.892\n0.946\n0.812\n0.583\n0.834\n\
    0.872\n0.708\n0.911\nF1-score\n0.729\n0.748\n0.748\n0.705\n0.730\n0.545\n0.749\n\
    0.750\n0.725\n0.783\nAUC score\n0.623\n0.654\n0.647\n0.526\n0.655\n0.483\n0.673\n\
    0.661\n0.732\n0.748\n6\nComputational Intelligence and Neuroscience\nGraph neural\
    \ network (GNN) refers to the use of neural\nnetwork to learn graph structure\
    \ data and extract and ex-\nplore the characteristics and patterns in graph structure\
    \ data.\nGNN formulates certain strategies for nodes and edges in the\ngraph,\
    \ converts the graph structure data into standardized\nrepresentation, and inputs\
    \ them into various neural net-\nworks for node classiﬁcation, edge information\
    \ dissemina-\ntion, graph clustering, and other tasks. Literature [27]\nproposes\
    \ to apply convolution operation to graph and\nproposes graph convolution network\
    \ (GCN) by clever\ntransformation of convolution operator. Te core idea of\ngraph\
    \ convolution is to learn a function f to generate the\nrepresentation of node\
    \ Vi by aggregating its own feature Xi\nand neighbor feature Xj, where j ∈ N(Vi),\
    \ and N(Vi)\nrepresents the neighboring nodes near Vi. A general graph\nconvolution\
    \ structure can be represented as shown in\nFormula (2), which consists of 2 basic\
    \ operations, aggre-\ngation and update, and corresponding weights.\nX(l+1) \x88\
    \ Update Aggregate Xl, Wagg\nl\n\U0010FF10\n\U0010FF11, Wupdate\nl\n\U0010FF10\
    \n\U0010FF11.\n(2)\nTe ﬁrst step in using a graph neural network is to build the\n\
    graph structure. Firstly, we input all the data with dimension\n[10000, 39] into\
    \ the graph structure. Each dataset is regarded as a\nnode, and the distance between\
    \ nodes is regarded as an edge of\nthe graph. More speciﬁcally, we take the chord\
    \ distance of node\ncharacteristics as the edge of the graph network and construct\n\
    the graph according to the corresponding source node and target\nnode. Secondly,\
    \ we use a certain number of nodes as losses to\ntrain graph networks to meet\
    \ our performance requirements.\nFinally, the model was used to assist experts\
    \ to determine the\nsuitability of varieties and test trial sites. Te whole project\n\
    process is shown in Figure 2.\nTe architecture diagram of the graph neural network\n\
    model is shown in Figure 3. Te network loss adopts neg-\native log likelihood\
    \ loss, which inputs 2 tensors, the pre-\ndiction tensor and the label. Te output\
    \ of the network\nobtains the logarithmic probability in the neural network\n\
    through the log softmax layer, namely, the prediction tensor\nof the network,\
    \ and then uses the data label to calculate the\nloss. In addition, the network\
    \ uses Adam optimizer [28] to\noptimize network parameters.\n6. Experiments\n\
    6.1. Experimental Results and Analysis. Diﬀerent from the\ntraditional neural\
    \ network, the graph network needs to input\nthe entire dataset into the graph\
    \ at one time and then specify\na node as a loss to update the network parameters.\
    \ Terefore,\nfor a total of 10000 nodes, we choose 50, 100, 400, 700, 1000,\n\
    and 2000 nodes as losses to update the network, and the\nresults are shown in\
    \ Table 1.\nIt can be seen from Table 1 that the prediction perfor-\nmance of\
    \ the model after data standardization is the best,\nwhether it is the graph convolution\
    \ network or the tradi-\ntional machine learning method; that is, the data stan-\n\
    dardization operation is conducive to improving the\nprediction accuracy of the\
    \ model. Ten, for the graph neural\nnetwork, the more the training data are, the\
    \ more ﬁtting the\ndistribution of the entire data is. In other words, with the\n\
    increase of the number of training samples, the accuracy of\nthe model is gradually\
    \ improved.\nTo verify the performance of the graph neural network\nmodel, we\
    \ conduct comparative experiments using tradi-\ntional machine learning and neural\
    \ network methods. We\nﬁrst divide the dataset with data dimension [10000, 39]\
    \ into\ntraining set and test set according to the ratio of 4 :1, training\nset:\
    \ test set \x88 8000 : 2000. Ten, we use traditional neural\nnetworks and various\
    \ machine learning methods for\ntraining, including KNN (K-Nearest Neighbor (N\
    \ \x88 15)), LR\n(logistic regression), SVM (Support Vector Machine), NB\n(Naive\
    \ Bayes classiﬁer), DT (decision tree), RF (Random\nForest), MLP (multilayer perceptron),\
    \ RBFNN (Radial Basis\nFunction Neural Network [29]). Furthermore, we also used\
    \ a\nGAT (graph attention neural network [30]) model for\ncomparison.\nFor a relatively\
    \ fair comparison, we align the hidden\nlayers of the traditional neural network\
    \ with the graph\nneural network. First, we design a six-layer neural network\n\
    with four hidden layers, the six-layer perceptron. Te input\nfeature dimension\
    \ is 39 and the output feature dimension is\n2. Cross entropy is used as loss,\
    \ probability distribution p is\nexpected output, probability distribution q is\
    \ actual output,\nand cross entropy can be expressed as in Formula (3). For\n\
    RBFNN and GAT, due to the large diﬀerence in network\nstructure, it is diﬃcult\
    \ to align with GCN, so we choose\ncommon network settings. Te number of input\
    \ nodes of\nGAT is 39, the hidden layer nodes is 64, and the attention\nhead is\
    \ 2.\nH(p, q) \x88 − \U0010FF58\nx\n(p(x)logq(x)) +(1 − p(x))log(1 − q(x)).\n\
    (3)\nTe results obtained by using the above machine\nlearning model for training\
    \ are shown in Table 2; the higher\nperformance among them is marked in bold.\
    \ In order to\nshow the performance of the model more comprehensively,\nwe use\
    \ ﬁve indicators for evaluation: accuracy rate, precision\nrate, recall rate,\
    \ F1-score, and AUC, and we ﬁnally take the\naverage of 20 repeated experiments\
    \ as the experimental\nresult. Accuracy refers to the ratio of the number of correctly\n\
    classiﬁed samples to the total number of samples, which\nmost directly reﬂects\
    \ the performance of the model but is\neasily aﬀected by class imbalance. Te precision\
    \ rate is the\nratio of the number of correctly classiﬁed positive examples\n\
    to the number of classiﬁed positive examples, which mea-\nsures the precision\
    \ rate of the model. Recall is the ratio of the\nnumber of correctly classiﬁed\
    \ positive examples to the actual\nnumber of positive examples and measures the\
    \ recall rate of\nthe model. Te F1 score can be regarded as the harmonic\naverage\
    \ of the model’s accuracy and recall, and the calcu-\nlation formula is as shown\
    \ in formula (4).\nAUC (Area under Curve) is deﬁned as the area enclosed\nby the\
    \ coordinate axis under the ROC curve. Te closer the\nAUC to 1.0, the higher the\
    \ authenticity of the detection\nmethod; when it is equal to 0.5, the authenticity\
    \ is the lowest\nand has no application value.\nComputational Intelligence and\
    \ Neuroscience\n7\nF1 \x88 2 ∗ Precision ∗ Recall\nPrecision + Recall.\n(4)\n\
    Among those machine learning methods, random forest,\nSupport Vector Machine,\
    \ and logistic regression perform the\nbest, while decision tree and na¨ıve Bayesian\
    \ model perform\nthe worst. Compared with the decision tree, the random\nforest\
    \ adopts the integrated algorithm, which is equivalent to\nintegrating multiple\
    \ decision tree models, and determines\nthe result by voting or averaging each\
    \ tree, so the accuracy is\nbetter than that of the decision tree. In addition,na¨ıveNaive\n\
    Bayesian model has two basic assumptions. Te independent\nvariables are independent\
    \ of each other, and the continuous\nindependent variables are subject to normal\
    \ distribution\nrelative to the dependent variables. Combined with the\nvisualization\
    \ analysis of the numerical distribution of the\ndata in Chapter 3, the independent\
    \ variable does not fully\nconform to the normal distribution relative to the\
    \ dependent\nvariable but ﬂuctuates within a certain range. We believe that\n\
    this is the main reason for the decline in the accurana¨ıve the\nNaive Bayesian\
    \ model.\nWe use the 1000 nodes of the GCN model as the training\nloss accuracy\
    \ for comparison, which is 74.8%. Compared\nwith traditional machine learning\
    \ (67.6%), MLP (68.4%),\nand RBFNN (68.1%), graph neural network achieves higher\n\
    variety suitability evaluation accuracy with fewer training\nsamples. Furthermore,\
    \ compared with GAT (73.1%), the\nGCN model is better in accuracy, but the accuracy\
    \ is not as\ngood as GAT. Moreover, the GCN model also has a good\nrecall rate,\
    \ F1, and AUC scores, further verifying the su-\nperiority of the model performance.\n\
    For the traditional neural network and machine learning\nalgorithms, each variety\
    \ suitability evaluation dataset is\nconsidered as a point feature information,\
    \ and the algorithm\nlearns the complex mapping relationship between features\n\
    and labels. In contrast, the graph neural network can\ntransmit information through\
    \ the graph structure, update\nthe state of hidden nodes through the sum of the\
    \ weights of\nadjacent nodes, and eﬀectively utilize the association be-\ntween\
    \ feature nodes. For tabular data, diﬀerent data come\nfrom diﬀerent experimental\
    \ points, and there are obvious\ncorrelations (such as climate factors) between\
    \ adjacent test\ntrial sites. Terefore, the method of node aggregation can not\n\
    only mine the similarity between features but also make\ngood use of the association\
    \ between geographic locations.\nGAT is generally considered to be an upgrade\
    \ of GCN.\nWhen GAT updates the features of nodes, it ﬁrst calculates\nthe attention\
    \ scores of all neighbor nodes and then\nInput\nReLU\nOutput\nND128\nND256\nND32\n\
    ND8\nND64\nFigure 3: Graph neural network framework. Te network consists of an\
    \ input layer, 4 hidden layers, and an output layer, and the ReLU\nactivation\
    \ function is used in the middle to increase the nonlinear ﬁtting ability.\nData\
    \ analysis\nData \nsource\nClimate\nPhenotype\nAnalysis\nCorrelation analysis\n\
    Potential association\nGNN construction\nGraph\n…\nLB\n-0.041\nLR\n-0.059\nIR\n\
    -0.052\nGSD\n-0.015\nPH\n0.014\nEH\n0.016\nESR\n-0.062\nDP\n-0.017\nER\n0.013\n\
    HGW\n0.047\nEL\n0.038\nBTL\n-0.011\nFEF\n0.033\nAY\n0.045\nRCY\n0.346\nlabel \n\
    1.000\nAMaxT\n0.046\nVMaxA\n0.044\nAAT\n-0.053\nVAT\n-0.041\nAMinT\n0.071\nVMinT\n\
    -0.046\nATD\n-0.054\nVAT\n-0.058\nAGP\n-0.023\nVGP\n-0.018\nARH\n-0.079\nVRH\n\
    -0.087\nAP\n0.025\nVP\n0.010\nAMWS\n-0.046\nVMVS\n-0.062\nAAWS\n-0.012\nVAWS\n\
    -0.012\nADWA\n0.048\nVWDA\n0.024\nAST\n-0.073\nVST\n-0.042\nAWR\n-0.048\nVWL\n\
    -0.027\nFigure 2: Overall ﬂowchart of the project. Te whole project ﬂowchart can\
    \ be divided into 3 parts: data analysis, correlation analysis, and\nconstruction\
    \ of graph structure. Te data analysis part shows the source and numerical distribution\
    \ of the data; the correlation analysis part\ngives the relationship between the\
    \ suitability evaluation indicators and the climate, environment, and crop phenotype\
    \ data; the graph\nconstruction part uses each piece of data as a node to construct\
    \ a graph and input it into GNN.\n8\nComputational Intelligence and Neuroscience\n\
    aggregates the corresponding neighbor features according to\nthe attention scores\
    \ to better utilize the correlation between\nfeatures. However, GAT (73.1%) does\
    \ not perform as well as\nGCN (74.8%) on our applicability evaluation task. We\
    \ infer\nthat the reason is that the GATdoes not fully utilize the edge\ninformation\
    \ and the network does not learn the connection\nweights between nodes well.\n\
    6.2. Further Research. It can be seen from the data corre-\nlation in Table 3\
    \ that the correlation between the relative\nchange of ﬁeld index and the suitability\
    \ evaluation label is\nmuch larger than that of other types of data. Terefore,\
    \ we\ndoubt whether the accuracy of the model is too much af-\nfected by the index,\
    \ resulting in a sharp decline in the\nperformance of the model that is indeed\
    \ the index, thereby\nreducing the actual availability of the model. Terefore,\
    \ we\nconduct feature data ablation experiments in a targeted\nmanner.\nFirstly,\
    \ the relative changes of yield traits in the overall\ndata were removed, and\
    \ the other data remained unchanged.\nTen, 20 groups of experiments were carried\
    \ out, and the\naverage value was taken as shown in Table 4. Te accuracy of\n\
    the graph neural network model is reduced by about 4%. In\ncontrast, the traditional\
    \ machine learning and neural net-\nwork methods decrease greatly, which to some\
    \ extent shows\nthat the graph neural network learns more data high-order\ncorrelation\
    \ and the model is more robust. In summary, in\nthe absence of relative change\
    \ of yield index, we can think\nthat the overall performance of the model is within\
    \ an ac-\nceptable range.\n7. Conclusion and Future Work\nWith the continuous\
    \ growth of the world population and the\ndeterioration of the political and commercial\
    \ situation, food\nproduction has become the focus of attention. Te use of\nartiﬁcial\
    \ intelligence technology to improve land suitability\nand variety adaptability,\
    \ thereby increasing the yield of food\ncrops, has become the consensus of agricultural\
    \ researchers.\nWe collected traits and local climate data of 10,000 maize\nlines\
    \ in multiple test trial sites, artiﬁcial intelligence tech-\nnology to learn\
    \ and explore the suitability between maize\nvarieties and test trial sites. Among\
    \ all artiﬁcial intelligence\nmethods, graph neural network has generally achieved\
    \ good\napplicability evaluation results, and only 1/10 training\nsamples are\
    \ used to achieve 75% accuracy.\nIn the future, we will introduce more factors\
    \ related to\nsuitability evaluation, such as the genetic sequence of va-\nrieties\
    \ and soil components, and improve the current in-\ntelligent technology, so that\
    \ artiﬁcial intelligence can\nessentially replace expert evaluation. Furthermore,\
    \ after\nmastering the data of a variety in a test trial site, the\nsuitability\
    \ of the variety for other test trial sites can be judged\naccording to the trait\
    \ data of the variety and the current\nenvironmental data. Tis can eliminate a\
    \ large number of\nschemes considered unsuitable by artiﬁcial intelligence, thus\n\
    greatly reducing the cost of trial and error between varieties\nand test trial\
    \ sites, accelerating the identiﬁcation of varieties\nmost suitable for current\
    \ test trial sites, and ultimately in-\ncreasing the yield of food crops.\nData\
    \ Availability\nTe data are available from the corresponding author upon\nrequest.\n\
    Conflicts of Interest\nTe authors declare that they have no conﬂicts of interest.\n\
    Acknowledgments\nTe current work was supported by National Key Research\nand Development\
    \ Program of China: Integration and\ndemonstration of cloud platform for the scientiﬁc\
    \ and\ntechnological information and achievement transformation\nof\nnational\n\
    agriculture\nand\nrural\nareas\n(no.\n2020YFD1100605).\nReferences\n[1] D. K.\
    \ Ray, P. C. West, and M. Clark, “Climate change has\nlikely already aﬀected global\
    \ food production,” PLoS One,\nvol. 14, no. 5, Article ID e0217148, 2019.\n[2]\
    \ A. Raza, A. Razzaq, and S. S. Mehmood, “Impact of climate\nchange on crops adaptation\
    \ and strategies to tackle its out-\ncome: a review,” Plants, vol. 8, no. 2, 34\
    \ pages, 2019.\n[3] A. J. Challinor, J. Watson, D. B. Lobell, S. M. Howden,\n\
    D. R. Smith, and N. Chhetri, “A meta-analysis of crop yield\nunder climate change\
    \ and adaptation,” Nature Climate\nChange, vol. 4, no. 4, pp. 287–291, 2014.\n\
    [4] A. Ren, D. Jiang, and M. Kang, “Development of artiﬁcial\nintelligence climate\
    \ chamber and experimental study on crop\nphenotype Acquisition,” Research Square,\
    \ 2022.\n[5] Z. Attia, C. S. Pogoda, S. Reinert, N. C. Kane, and B. S. Hulke,\n\
    “Breeding for sustainable oilseed crop yield and quality in a\nchanging climate,”\
    \ TAG. Teoretical and applied genetics.\nTeoretische und angewandte Genetik, vol.\
    \ 134, no. 6,\npp. 1817–1827, 2021.\n[6] A. Langstroﬀ, M. C. Heuermann, and A.\
    \ Stahl, “Opportunities\nand limits of controlled-environment plant phenotyping\
    \ for\nclimate response traits,” Teoretical and Applied Genetics,\nvol. 135, pp.\
    \ 1–16, 2021.\n[7] X. Li, T. Guo, and G. Bai, “Genetics-inspired data-driven\n\
    approaches explain and predict crop performance ﬂuctua-\ntions attributed to changing\
    \ climatic conditions,” Molecular\nPlant, vol. 15, 2022.\nTable 4: Comparison\
    \ of the original performance of the model and the performance after removing\
    \ the relative change of yield (RCY) index.\nKNN\nLR\nSVM\nNB\nRF\nDT\nMLP\nRBFNN\n\
    GAT\nGCN\nAll data\n0.647\n0.676\n0.671\n0.567\n0.658\n0.553\n0.684\n0.681\n0.731\n\
    0.748\nNo RCY\n0.588\n0.571\n0.580\n0.552\n0.580\n0.551\n0.615\n0.581\n0.698\n\
    0.709\nComputational Intelligence and Neuroscience\n9\n[8] A. Bonfante and J.\
    \ Bouma, “Te role of soil series in quan-\ntitative land evaluation when expressing\
    \ eﬀects of climate\nchange and crop breeding on future land use,” Geoderma,\n\
    vol. 259-260, pp. 187–195, 2015.\n[9] J. L. Araus and J. E. Cairns, “Field high-throughput\
    \ pheno-\ntyping: the new crop breeding Frontier,” Trends in Plant\nScience, vol.\
    \ 19, no. 1, pp. 52–61, 2014.\n[10] M. Cooper, C. D. Messina, D. Podlich et al.,\
    \ “Predicting the\nfuture of plant breeding: complementing empirical evaluation\n\
    with genetic prediction,” Crop & Pasture Science, vol. 65,\nno. 4, pp. 311–336,\
    \ 2014.\n[11] J. I. Marsh, H. Hu, M. Gill, J. Batley, and D. Edwards, “Crop\n\
    breeding for a changing climate: integrating phenomics and\ngenomics with bioinformatics,”\
    \ TAG. Teoretical and applied\ngenetics. Teoretische und angewandte Genetik, vol.\
    \ 134, no. 6,\npp. 1677–1690, 2021.\n[12] X. B. Jin, X. H. Yu, X. Y. Wang, Y.\
    \ T. Bai, T. L. Su, and\nJ. L. Kong, “Deep learning predictor for sustainable\
    \ precision\nagriculture based on internet of things system,” Sustainability,\n\
    vol. 12, no. 4, 1433 pages, 2020.\n[13] T. R. Chavan and A. V. Nandedkar, “AgroAVNET\
    \ for crops\nand weeds classiﬁcation: a step forward in automatic farm-\ning,”\
    \ Computers and Electronics in Agriculture, vol. 154,\npp. 361–372, 2018.\n[14]\
    \ T. Khan, H. H. R. Sherazi, M. Ali, S. Letchmunan, and\nU. M. Butt, “Deep learning-based\
    \ growth prediction system: a\nuse case of China agriculture,” Agronomy, vol.\
    \ 11, no. 8,\n1551 pages, 2021.\n[15] P. K. Kashyap, S. Kumar, A. Jaiswal, M.\
    \ Prasad, and\nA. H. Gandomi, “Towards precision agriculture: IoT-enabled\nintelligent\
    \ irrigation systems using deep learning neural\nnetwork,” IEEE Sensors Journal,\
    \ vol. 21, no. 16, Article ID\n17479, 2021.\n[16] M. A. Ferrag, L. Shu, H. Djallel,\
    \ and K. K. R. Choo, “Deep\nlearning-based intrusion detection for distributed\
    \ denial of\nservice attack in agriculture 4.0,” Electronics, vol. 10, no. 11,\n\
    1257 pages, 2021.\n[17] G. Zhou, J. Wang, and X. Zhang, “Predicting functions\
    \ of\nmaize proteins using graph convolutional network,” BMC\nBioinformatics,\
    \ vol. 21, no. 16, pp. 1–16, 2020.\n[18] A. Vyas and S. Bandyopadhyay, Dynamic\
    \ Structure Learning\nthrough Graph Neural Network for Forecasting Soil Moisture\n\
    in Precision Agriculture, 2020.\n[19] J. Fan, J. Bai, and Z. Li, “A GNN-RNN approach\
    \ for har-\nnessing geospatial and temporal information: application to\ncrop\
    \ yield prediction,” 2021, https://arxiv.org/pdf/2111.08900.\n[20] C. D. D. Yu\
    \ and J. F. Villaverde, “Avocado ripeness classi-\nﬁcation using graph neural\
    \ network,” IEEE, in Proceedings of\nthe 2022 14th International Conference on\
    \ Computer and\nAutomation Engineering (ICCAE), pp. 74–79, Brisbane,\nAustralia,\
    \ March 2022.\n[21] D. Reynolds, F. Baret, C. Welcker et al., “What is cost-eﬃcient\n\
    phenotyping? Optimizing costs for diﬀerent scenarios,” Plant\nScience, vol. 282,\
    \ pp. 14–22, 2019.\n[22] E. M. Mateo, J. V. G´omez, D. Romera et al., “Environmental\n\
    temperature and relative humidity, two Key factors in maize\ntechnology aﬀecting\
    \ ochratoxin a production and growth of\nochratoxigenic species,” ETP International\
    \ Journal of Food\nEngineering, vol. 4, no. 1, pp. 51–57, 2018.\n[23] S. K. A.\
    \ Alshariﬁ, N. Shtewy, and S. A. I. Alaamer, “Aﬀecting\nmechanical on some growth\
    \ properties for corn, MAHA\ncultivar,” in Proceedings of the IOP Conference Series:\
    \ Earth\nand Environmental Science, vol. 735, no. 1, Article ID 012009,\n2021.\n\
    [24] A. Kalisz, “Growth and earliness of Chinese cabbage (Brassica\nrapa var.\
    \ chinensis) as a function of time and weather con-\nditions,” Folia Horticulturae,\
    \ vol. 23, no. 2, pp. 131–138, 2011.\n[25] M. Maitah, K. Malec, Y. Ge et al.,\
    \ “Assessment and prediction\nof maize production considering climate change by\
    \ extreme\nlearning machine in Czechia,” Agronomy, vol. 11, no. 11,\n2344 pages,\
    \ 2021.\n[26] Y. Lu, J. Jin, and L. M. Kueppers, “Crop growth and irrigation\n\
    interact to inﬂuence surface ﬂuxes in a regional climate-\ncropland model (WRF3.3-CLM4crop),”\
    \ Climate Dynamics,\nvol. 45, no. 11-12, pp. 3347–3363, 2015.\n[27] T. N. Kipf\
    \ and M. Welling, “Semi-supervised classiﬁcation\nwith graph convolutional networks,”\
    \ 2016, https://arxiv.org/\nabs/1609.02907.\n[28] D. P. Kingma and J. Ba, “Adam:\
    \ a method for stochastic\noptimization,” 2014, https://arxiv.org/abs/1412.6980.\n\
    [29] S. Chen, C. F. N. Cowan, and P. M. Grant, “Orthogonal least\nsquares learning\
    \ algorithm for radial basis function net-\nworks,” IEEE Transactions on Neural\
    \ Networks, vol. 2, no. 2,\npp. 302–309, 1991.\n[30] P. Velickovic, G. Cucurull,\
    \ and A. Casanova, “Graph attention\nnetworks,” Stat, vol. 1050, 20 pages, 2017.\n\
    10\nComputational Intelligence and Neuroscience\n"
  inline_citation: '>'
  journal: Computational Intelligence and Neuroscience
  limitations: '>'
  pdf_link: https://downloads.hindawi.com/journals/cin/2022/5614974.pdf
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Suitability Evaluation of Crop Variety via Graph Neural Network
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.35537/10915/139373
  analysis: '>'
  authors: []
  citation_count: 0
  full_citation: '>'
  full_text: ">\n    Home Search Upload Material Institutional Frequentes questions\
    \ Contact Search resources Search among the 155445 resources available in the\
    \ repository Upload resources Upload your works to SEDICI to increase its visibility\
    \ and improve its impact       Eventos → Jornadas de Cloud Computing → X Jornadas\
    \ de Cloud Computing, Big Data & Emerging Topics Short papers of the 10th Conference\
    \ on Cloud Computing, Big Data & Emerging Topics Compilers: Naiouf, Marcelo |\
    \ De Giusti, Armando Eduardo | Chichizola, Franco | De Giusti, Laura Cristina\
    \ | Rucci, Enzo 2022           Document type: Libro Abstract Compilación de los\
    \ short papers presentados en las 10mas Jornadas de Cloud Computing, Big Data\
    \ & Emerging Topics (JCC-BD&ET2022), llevadas a cabo en modalidad híbrida durante\
    \ junio de 2021 y organizadas por el Instituto de Investigación en Informática\
    \ LIDI (III-LIDI) y la Secretaría de Posgrado de la Facultad de Informática de\
    \ la UNLP, en colaboración con universidades de Argentina y del exterior. General\
    \ information Issue date: 11 de julio de 2022 Document language: Spanish Publisher:\
    \ Facultad de Informática (UNLP) Origin: Facultad de Informática ISBN: 978-950-34-2126-0\
    \ xmlui.dri2xhtml.METS-1.0.item-dc-subject: cloud computing ; big data ; high-performance\
    \ computing ; Inteligencia artificial ; emerging tech Subjects: Informática Download\
    \ Files Documento completo Download file (4.691Mb) - PDF BASE GoogleScholar Created:\
    \ 12 de julio de 2022 Available since: 12 de julio de 2022 Please, use one of\
    \ this identificators(URI) for cite o linking this item: http://sedici.unlp.edu.ar/handle/10915/139373\
    \ https://doi.org/10.35537/10915/139373 Show full item record This item appears\
    \ in the following Collection(s) Jornadas de Cloud Computing → X Jornadas de Cloud\
    \ Computing, Big Data & Emerging Topics Except where otherwise noted, this item's\
    \ license is described as Creative Commons Attribution-NonCommercial-ShareAlike\
    \ 4.0 International (CC BY-NC-SA 4.0) Login ?   PREBI - SEDICI © 2003-2024 National\
    \ University of La Plata Rights reserved contemplated by 11.723 law Supported\
    \ by Dspace 49 and 115 St - Ex Liceo Apt. Floor 1 La Plata, Buenos Aires (C.P.\
    \ 1900) Phone: +54 0221 6447282 Tel: +54 0221 6696/6677 (int. 141)"
  inline_citation: '>'
  journal: Facultad de Informática (UNLP) eBooks
  limitations: '>'
  pdf_link: null
  publication_year: 2022
  relevance_score1: 0
  relevance_score2: 0
  title: Short papers of the 10th Conference on Cloud Computing, Big Data &amp; Emerging
    Topics
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
