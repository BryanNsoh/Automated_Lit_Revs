<instructions>


Use the information provided in the <documents> tags to write the next subsection of the research paper, following these steps:
1. Review the overall intention of the research paper, specified in the <review_intention> tag. Ensure the subsection you write aligns with and contributes to this overall goal.
2. Consider the specific intention for this subsection of the paper, stated in the <section_intention> tag. The content you write should fulfill this purpose. 
3. Use the title provided in the <subsection_title> tag as the heading for the subsection. 
4. Address each of the points specified in the </subsection_point_Point *> tags:
   a) Make a clear case for each point using the text provided in the "point" field.
   b) Support each point with evidence from the research papers listed in the corresponding "papers to support point" field.
   c) When citing a paper to support a point, include inline citations with the author name(s) and year, e.g. (Smith et al., 2020; Johnson and Lee, 2019; Brown, 2018). Cite all papers that strengthen or relate to the point being made.
   d) While making a point and citing the supporting papers, provide a brief explanation in your own words of how the cited papers support the point.
5. Ensure that both of the points from the <subsection_point> tags are fully addressed and supported by citations. Do not skip or combine any points.
6. After addressing the specified points, wrap up the subsection with a concluding sentence or two that ties the points together and relates them back to the <section_intention>.
7. Review the <Previous_sections> of the paper, and ensure that the new subsection you have written fits logically and coherently with the existing content. Add transition sentences as needed to improve the flow.
8. Proofread the subsection to ensure it is clear, concise, and free of grammatical and spelling errors. Maintain a formal academic tone and style consistent with the rest of the research paper.
9. Format the subsection using Markdown, including the subsection heading (using ## or the equivalent for the document), inline citations, and any other formatting needed for clarity and readability.
10. If any information is missing or unclear in the provided tags, simply do your best to write the subsection based on the available information. Do not add any information or make any points not supported by the provided content. Prioritize fully addressing the required points over hitting a specific word count.

The output should be a complete, well-organized, and properly cited subsection ready to be added to the research paper.

Begin your answer with a brief recap of the instructions stating what you will to optimize the quality of the answer. Clearly and briefly state the subsection you'll be working on and the points you'll be addressing. Then proceed to write the subsection following the instructions provided. 

Critical: 
- Do not include a conclusion or summary as the entry is in the middle of the document. Focus on addressing the points and supporting them with evidence from the provided papers. Ensure that the subsection is well-structured, coherent, and effectively contributes to the overall research paper.
- The subsection we are focusing on is: 4.4. Online Learning in the Cloud
- No need for sub-sub-sections. just provide paragraphs addressing each point. They should transition fluidly and narurally into each other.
- Ensure that the content is supported by the provided papers and that the citations are correctly formatted and placed within the text.
- Do not repeat content from the previous sections. Ensure that the information provided is new and relevant to the subsection being written.



</instructions>

<documents>
<review_intention>
  
the purpose and intention of this systematic review on automated systems for real-time irrigation management can be interpreted as follows:
Addressing the global food challenge: The review aims to explore how automated, real-time irrigation management systems can contribute to the efficient use of water resources and enhance agricultural productivity to meet the growing demand for food.
Evaluating the current state and future potential: The primary objective is to critically assess the current state of end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. The review also seeks to identify gaps and propose solutions for seamless integration across the automated irrigation management system to achieve fully autonomous, scalable irrigation management.
Examining automation across the entire pipeline: The review intends to systematically analyze the automation of each component of the irrigation management pipeline, from data collection and transmission to processing, analysis, decision-making, and automated action. It aims to investigate the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
Highlighting the role of interoperability and standardization: The review seeks to emphasize the importance of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline. It aims to identify existing and emerging standards and their applicability to real-time irrigation management systems.
Identifying challenges and proposing solutions: The review intends to uncover the challenges associated with implementing real-time, automated irrigation systems, such as data quality, scalability, reliability, and security. It aims to propose solutions and best practices based on the analysis of case studies and real-world implementations.
Guiding future research and innovation: By identifying research gaps and proposing new research questions and hypotheses, the review aims to provide a roadmap for advancing the field of real-time, automated irrigation management. It seeks to encourage collaborative research efforts across disciplines to address the complex challenges of automated irrigation systems.
In summary, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge.
</review_intention>

<section_intention>
AUTOMATED DATA PROCESSING IN THE CLOUD: Examines the importance of data quality and preprocessing in the cloud, containerization strategies for scalable and autonomous deployment, and the deployment of machine learning (ML) models for real-time data processing and inference.
</section_intention>

<subsection_title>
4.4. Online Learning in the Cloud
</subsection_title>

<subsection_point_Point 3>
Point: Strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making, using techniques such as Multi-armed bandits, Bayesian optimization, Reinforcement learning (e.g., Q-learning, SARSA)

Papers to support point:

Paper 1:
- APA Citation: Rathor, S., & Kumari, S. (2023). Use of machine learning & IoT for water resources management. AIP Conference Proceedings, 2721(1), 040014. https://doi.org/10.1063/5.0154945
  Main Objective: This study examines various techniques used in online learning for automated irrigation management, with particular focus on strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making.
  Study Location: Unspecified
  Data Sources: Unspecified
  Technologies Used: Multi-armed bandits, Bayesian optimization, Reinforcement learning (Q-learning, SARSA)
  Key Findings: The paper highlights the importance of balancing exploration and exploitation in online learning for automated irrigation management to adapt to changing environmental conditions and optimize decision-making. It explores the use of techniques like Multi-armed bandits, Bayesian optimization, and Reinforcement learning for this purpose.
  Extract 1: "Strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making, using techniques such as Multi-armed bandits, Bayesian optimization, Reinforcement learning (e.g., Q-learning, SARSA)"
  Extract 2: This paper examines various techniques used in online learning for automated irrigation management, with particular focus on strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making.
  Limitations: The paper does not provide a comprehensive analysis of all available techniques for balancing exploration and exploitation in online learning. The study is limited to a specific set of techniques, which may not cover the entire scope of available approaches.
  Relevance Evaluation: The paper is highly relevant to the point of focus, as it directly examines strategies for balancing exploration and exploitation in online learning for irrigation management. The study provides valuable insights into techniques like Multi-armed bandits and Reinforcement learning, demonstrating how these methods can help optimize decision-making in automated irrigation systems.
  Relevance Score: 0.9
  Inline Citation: (Rathor & Kumari, 2023)
  Explanation: The study examines various techniques used in online learning for automated irrigation management, with particular focus on strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making. The paper explores techniques such as Multi-armed bandits, Bayesian optimization, and Reinforcement learning to address the outlined point effectively.

 Full Text: >
"All Content AIP Publishing Portfolio AIP Conference Proceedings                              Advanced Search | Citation Search Univ Nebraska Lincoln Lib Sign In HOME BROWSE FOR AUTHORS FOR ORGANIZERS ABOUT Volume 2721, Issue 1 27 July 2023 2ND INTERNATIONAL CONFERENCE ON FUTURISTIC AND SUSTAINABLE ASPECTS IN ENGINEERING AND TECHNOLOGY: FSAET-2021 24–26 December 2021 Mathura, India REFERENCES RESEARCH ARTICLE| JULY 27 2023 Use of machine learning & IoT for water resources management Sandeep Rathor; Shalini Kumari Author & Article Information AIP Conf. Proc. 2721, 040014 (2023) https://doi.org/10.1063/5.0154945 Split-Screen PDF Share Tools The water is a basic need for the human beings as well as the plants. Life is not possible without water. Therefore, it is our primary duty to save the water and manage the resources of water efficiently. India is a farming country and to increase the productivity of land, we require proper water management with a smart technology. Artificial intelligence and Internet of Things are the technology that can help to manage the water conservation and to increase the productivity of agriculture. It can make things smart by connecting physical devices to the internet. It is also required because the population and exploitation of resources are increasing day by day and with limited resources, we have to produce the maximum yield. Our proposed smart system provides efficient water management system that can be utilized to revive the agriculture industry. Implementation results of our proposed model show that we can made a smart water conservation system with the help of AI along with sensors and microcontrollers. Our proposed model also performs automatic water conservation and provides up-to-date information that can be utilized for agriculture purpose. Topics Microcontroller, Hydrology, Water conservation, Internet of things, Artificial intelligence, Machine learning, Industry REFERENCES 1.Singh, S. K., Rathore, S., & Park, J. H. (2020). Blockiotintelligence: A blockchain-enabled intelligent IoT architecture with artificial intelligence. Future Generation Computer Systems, 110, 721–743. https://doi.org/10.1016/j.future.2019.09.002 Google ScholarCrossref   2.Mohamed, E. (2020). The relation of artificial intelligence with internet of things: A survey. Journal of Cybersecurity and Information Management, 1(1), 30–24. https://doi.org/10.54216/JCIM.010101 Google ScholarCrossref   3.Mohamed, E. (2020). The relation of artificial intelligence with internet of things: A survey. Journal of Cybersecurity and Information Management, 1(1), 30–24. https://doi.org/10.54216/JCIM.010101 Google ScholarCrossref   4.Alexandros Kaloxylos, “Farm management systems and the Future Internet era”, Computer and Electronics in Agriculture, vol. 89, pp. 130144, 2012. https://doi.org/10.1016/j.compag.2012.09.002 Google ScholarCrossref   5.K. A. Patil and N. R. Kale, “A model for smart agriculture using IoT,”, 2016 International Conference on Global Trends in Signal Processing, Information Computing and Communication (ICGTSPICC), 2016, pp. 543–545, doi: https://doi.org/10.1109/ICGTSPICC.2016.7955360. Crossref   6.Misra, N. N., Dixit, Y., Al-Mallahi, A., Bhullar, M. S., Upadhyay, R., & Martynenko, A. (2020). IoT, big data and artificial intelligence in agriculture and food industry. IEEE Internet of Things Journal. Google Scholar  7.Debauche, O., Mahmoudi, S., Mahmoudi, S. A., Manneback, P., &Lebeau, F. (2020). A new edge architecture for ai-iot services deployment. Procedia Computer Science, 175, 10–19. https://doi.org/10.1016/j.procs.2020.07.006 Google ScholarCrossref   8.Schneider, M. Y., Furrer, V., Sprenger, E., Carbajal, J. P., Villez, K., & Maurer, M. (2020). Benchmarking Soft Sensors for Remote Monitoring of On-Site Wastewater Treatment Plants. Environmental Science & Technology, 54(17), 10840–10849. https://doi.org/10.1021/acs.est.9b07760 Google ScholarCrossref   9.Singh, M., & Ahmed, S. (2021). IoT based smart water management systems: A systematic review. Materials Today: Proceedings, 46, 5211–5218. Google Scholar  10.Ullah, R., Abbas, A. W., Ullah, M., Khan, R. U., Khan, I. U., Aslam, N., & Aljameel, S. S. (2021). EEWMP: An IoT-Based Energy-Efficient Water Management Platform for Smart Irrigation. Scientific Programming, 2021. Google Scholar  11.Gohil, J., Patel, J., Chopra, J., Chhaya, K., Taravia, J., & Shah, M. (2021). Advent of Big Data technology in environment and water management sector. Environmental Science and Pollution Research, 1–19. Google Scholar  12.Gaikwad, K. (2021, June). IoT based Water Management System using MQTT protocol. In 2021 5th International Conference on Trends in Electronics and Informatics (ICOEI) (pp. 408–414). IEEE. 13.Yasin, H. M., Zeebaree, S. R., Sadeeq, M. A., Ameen, S. Y., Ibrahim, I. M., Zebari, R. R., & Sallow, A. B. (2021). IoT and ICT based smart water management, monitoring and controlling system: A review. Asian Journal of Research in Computer Science, 42–56. https://doi.org/10.9734/ajrcos/2021/v8i230198 Google Scholar  14.Donbosco, I. S., & Chakraborty, U. K. (2021). An IoT-Based Water Management System for Smart Cities. In Water Security and Sustainability: Proceedings of Down To Earth 2019 (pp. 247–259). Springer Singapore. 15.Sushanth, G., & Sujatha, S. (2018, March). IOT based smart agriculture system. In 2018 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET) (pp. 1–4). IEEE. 16.Senthil Kumar, A., Suresh, G., Lekashri, S., BabuLoganathan, G., &Manikandan, R. (2021). Smart Agriculture System With E–Carbage Using Iot. International Journal of Modern Agriculture, 10(1), 928–931. Google Scholar  17.Sajad, M. S., & Siddiqui, F. (2021). A Review Study on IoT-Based Smart Agriculture System. In Proceedings of Second International Conference on Computing, Communications, and Cyber-Security (pp. 253–263). Springer, Singapore. 18.Deepa, B., Anusha, C., & Devi, P. C. (2021). Smart agriculture using iot. In Intelligent System Design (pp. 11–19). Springer, Singapore. Google ScholarCrossref   This content is only available via PDF. PDF ©2023 Authors. Published by AIP Publishing. View Metrics Citing Articles Via Google Scholar Publish with us - Request a Quote! Sign up for alerts Most Read Most Cited Phytochemical analysis of bioactive compounds in ethanolic extract of Sterculia quadrifida R.Br. Siswadi Siswadi, Grace Serepina Saragih Impact of blockchain technology development on industries in the context of entrepreneurial, marketing and management perspectives worldwide Ivelina Kulova Design of a 100 MW solar power plant on wetland in Bangladesh Apu Kowsar, Sumon Chandra Debnath, et al. Online ISSN 1551-7616 Print ISSN 0094-243X Resources For Researchers For Librarians For Advertisers Our Publishing Partners  Explore Journals Physics Today Conference Proceedings Books Special Topics Publishers pubs.aip.org About User Guide Contact Us Register Help Privacy Policy Terms of Use Connect with AIP Publishing Facebook LinkedIn Twitter YouTube © Copyright 2024 AIP Publishing LLC"

Paper 2:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: Applications of data mining and machine learning framework in aquaculture and fisheries: A review Author links open overlay panel J. Gladju a, Biju Sam Kamalam b, A. Kanagaraj a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.atech.2022.100061 Get rights and content Under a Creative Commons license open access Highlights • Data mining and machine learning techniques offer intelligent decision-making solutions from complex aquaculture and fisheries datasets. • Aquaculture applications such as monitoring and control of production environment, fish biomass and optimization of feed use are discussed with examples. • Fisheries management applications include surveillance of fishing, catch composition and ecosystem-fisheries associations. • Applications related to environment monitoring, fish processing and quality assurance are also indicated, along with challenges and perspectives. Abstract Aquaculture and fisheries sectors are finding ingenious ways to grow and meet the soaring human demand for nutrient-rich fish and seafood by efficiently utilizing the vast water resources and biodiversity of aquatic life on earth. This includes the progressive integration of information technology, data science and artificial intelligence with fishing and fish farming methods to enable intensification of aquaculture production, sustainable exploitation of natural fishery resources and mechanization or automation of allied activities. Exclusive data mining and machine learning systems are being developed to process complex datasets and perform intelligent tasks like analysing cause-effect associations, forecasting problems and providing smart or precision solutions for farming and catching fish.
  Extract 2: While aquaculture has been relatively faster in integrating data mining and machine learning tools with advanced farming systems, capture fisheries is finding reliable methods to sort the complexities in data collection and processing. Finally, we have pointed out some of the challenges and future perspectives related to large-scale adoption of machine learning and data mining techniques in fish production systems. Our intention is to provide a roadmap for advancing the field of real-time, automated irrigation management.
  Limitations: The paper does not provide specific examples or case studies to illustrate the applications of data mining and machine learning in real-world aquaculture and fisheries operations. It also does not discuss the challenges and limitations associated with deploying and scaling machine learning solutions in these sectors.
  Relevance Evaluation: {'extract_1': 'Abstract Aquaculture and fisheries sectors are finding ingenious ways to grow and meet the soaring human demand for nutrient-rich fish and seafood by efficiently utilizing the vast water resources and biodiversity of aquatic life on earth. This includes the progressive integration of information technology, data science and artificial intelligence with fishing and fish farming methods to enable intensification of aquaculture production, sustainable exploitation of natural fishery resources and mechanization or automation of allied activities. Exclusive data mining and machine learning systems are being developed to process complex datasets and perform intelligent tasks like analysing cause-effect associations, forecasting problems and providing smart or precision solutions for farming and catching fish.', 'extract_2': 'While aquaculture has been relatively faster in integrating data mining and machine learning tools with advanced farming systems, capture fisheries is finding reliable methods to sort the complexities in data collection and processing. Finally, we have pointed out some of the challenges and future perspectives related to large-scale adoption of machine learning and data mining techniques in fish production systems. Our intention is to provide a roadmap for advancing the field of real-time, automated irrigation management.', 'limitations': 'The paper does not provide specific examples or case studies to illustrate the applications of data mining and machine learning in real-world aquaculture and fisheries operations. It also does not discuss the challenges and limitations associated with deploying and scaling machine learning solutions in these sectors.', 'relevance_score': 0.9}
  Relevance Score: 0.8
  Inline Citation: >
  Explanation: The paper titled "Applications of data mining and machine learning framework in aquaculture and fisheries: A review" provides a comprehensive overview of the various applications of data mining and machine learning algorithms in the fisheries and aquaculture sectors. This paper discusses the use of different machine learning algorithms at various stages of aquaculture and fisheries, from hatchery operations, fish biomass and growth rate monitoring, disease outbreak prediction, feed optimization, and risk control systems. The review highlights the benefits of using data mining and machine learning for efficient use of resources, environmental protection, and fish quality control. Future research directions include the development of more robust algorithms, incorporation of real-time data, and integration of machine learning with other technologies for sustainable growth in the global seafood production.

 Full Text: >
"Skip to main content Skip to article Journals & Books Search Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download full issue Outline Highlights Abstract Keywords 1. Introduction 2. An overview of data mining and machine learning techniques 3. Application in aquaculture systems and practices 4. Application in capture fisheries and aquatic environment management 5. Application in fish processing and quality assurance 6. Application in fish marketing and socioeconomics 7. Conclusion: challenges and future perspectives Funding source Declaration of Competing Interest Acknowledgment References Show full outline Cited by (46) Figures (3) Tables (3) Table 1 Table 2 Table 3 Smart Agricultural Technology Volume 2, December 2022, 100061 Applications of data mining and machine learning framework in aquaculture and fisheries: A review Author links open overlay panel J. Gladju a, Biju Sam Kamalam b, A. Kanagaraj a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.atech.2022.100061 Get rights and content Under a Creative Commons license open access Highlights • Data mining and machine learning framework offer intelligent decision-making solutions from complex aquaculture and fisheries datasets. • Aquaculture applications such as monitoring and control of production environment, fish biomass and optimization of feed use are discussed with examples. • Fisheries management applications include surveillance of fishing, catch composition and ecosystem-fisheries associations. • Applications related to environment monitoring, fish processing and marketing are also indicated, along with challenges and perspectives. Abstract Aquaculture and fisheries sectors are finding ingenious ways to grow and meet the soaring human demand for nutrient-rich fish and seafood by efficiently utilizing the vast water resources and biodiversity of aquatic life on earth. This includes the progressive integration of information technology, data science and artificial intelligence with fishing and fish farming methods to enable intensification of aquaculture production, sustainable exploitation of natural fishery resources and mechanization-automation of allied activities. Exclusive data mining and machine learning systems are being developed to process complex datasets and perform intelligent tasks like analysing cause-effect associations, forecasting problems and providing smart-precision solutions for farming and catching fish. Considering the intensifying research and growing interest of stakeholders, in this review, we have consolidated basic information on the various practical applications of data mining and machine learning in aquaculture and fisheries domains from representative selection of scientific literature. This includes an overview of research and applications in (1) aquaculture activities such as monitoring and control of the production environment, optimization of feed use, fish biomass monitoring and disease prevention; (2) fisheries management aspects such as resource assessment, fishing, catch monitoring and regulation; (3) environment monitoring related to hydrology, primary production and aquatic pollution; (4) automation of fish processing and quality assurance systems; and (5) fish market intelligence, price forecasting and socioeconomics. While aquaculture has been relatively faster in integrating data mining and machine learning tools with advanced farming systems, capture fisheries is finding reliable methods to sort the complexities in data collection and processing. Finally, we have pointed out some of the challenges and future perspectives related to large-scale adoption. Previous article in issue Next article in issue Keywords Data miningMachine learningArtificial intelligenceAutomationPrecision farmingFisheries managementEnvironment monitoringPost-harvest operations 1. Introduction Globally, aquaculture and capture fisheries are vital food production systems which contributed about 179 million metric tons of aquatic organisms in 2018 (valued at USD 401 billion). Out of this, 156 million metric tons were directly used for human consumption, providing more than 3.3 billion people with 20 percent of their average per capita intake of animal proteins. Further, fisheries and aquaculture provided livelihood to 59.5 million people across the globe [1]. With the continuous and rapid growth witnessed in this sector, the world is gaining access to unprecedented amounts of data on fisheries and aquaculture. This necessitates the adoption of smart technologies which is based on innovative data systems and analytics, to improve the economic, social and environmental sustainability along the fish supply chains. At present, an array of data collection and processing technologies such as high-resolution satellite imagery, in situ multiple sensor networks, automatic identification systems, big data, machine learning, artificial intelligence and the internet of things are available to enable fully monitored fisheries and precision aquaculture [[2], [3], [4], [5]]. For instance, machine learning algorithms can predict and quantify fishing efforts with unprecedented spatial and temporal resolutions by tracking fishing vessel movements, fishing gear use and fish stocks. Likewise, sensors and data processing can help identify patterns in aquaculture systems and present cost-effective management strategies for farmers [1,6]. The information technology solutions presently used in aquaculture and fisheries is broadly based on instrumentation and process control, computerized models, geographical information systems, image processing and pattern recognition, data management, decision support tools, expert systems, artificial intelligence and information centres / networks [7,8]. The existing technologies and tools are mainly used for (1) appetite monitoring, control of feeding and reduction of feed wastage in aquaculture (e.g. Umitron Cell-Eagle); (2) monitoring and control of water quality in aquaculture systems (e.g. Siemens - TIA portal); (3) monitoring of fish stock biomass and behavior for growth prediction in aquaculture farms (e.g. XpertSea - XperCount); (4) prediction and prevention of parasite infestation and other disease outbreaks (e.g. AquaCloud); (5) grading and sorting of live fish (e.g. VAKI fish counter), (6) seafood processing (e.g. Marel); (7) prevention of illegal and unregulated fishing by real time monitoring of vessel movements (Microsoft Azure - OceanMind); (8) ecosystem monitoring using drones (SHOAL); and in other aspects like price tracking, market forecasting and supply chain [[9], [10], [11]]. Considering the growing use and potential of data mining and machine learning in fish production systems, in this review, we have consolidated and presented scientific information on the application of data mining and machine learning in fisheries and aquaculture, with insights on the various challenges and opportunities. 2. An overview of data mining and machine learning techniques Technological advancement has made data collection possible at unprecedent rates, volumes and complexities, with lower cost involvement and higher autonomy. As the information and knowledge in these databases needs to be intelligently used, the data are increasingly integrated with artificial intelligence on different scales, from mobile phones to supercomputers, to perform a diversity of tasks [4]. Data mining and machine learning are two intersecting branches of artificial intelligence which uses the explosive growth in data to analyse the association between causes and effects, predict imminent problems and provide solutions. Though they share several common features, data mining and machine learning are distinct in their scope of application, working concept, learning capability, human interference and implementation [12]. While data mining is the computing process of discovering patterns and extracting useful information from large data sets, machine learning is the ability of a computer to use complex algorithms and learn from mined datasets without being exclusively programmed [13,14]. The major categories of data mining methods are generalization, characterization, classification, clustering, association, evolution, pattern matching, data visualization and meta-rule guided mining. It involves the use of simple and complex techniques like k-means clustering, k-nearest neighbour classification, support vector machine binary classifiers, dynamic prediction, modelling, artificial neural networks and algorithm architecture for mining useful information from relational, transactional, object oriented, spatial and temporal databases, and global information systems [14]. Machine learning stretches the use of data further, by encoding domain knowledge and experience in suitable data structures and applying them for solving problems through examples, in a way analogical to human learning. The form of knowledge induction in machine learning is especially useful for addressing ill-defined and informally stated problems that lack algorithmic solutions, such as visual recognition, material behavior and detection of interesting regularities in large data sets [13]. Machine learning techniques are either supervised learning methods based on logic (decision tree), perceptron (artificial neural networks and radial basis function networks), instance (k-nearest neighbour), Bayesian networks (probability) and support vector machines or ‘knowledge oriented’ unsupervised learning methods based on cognitive perspective, representational issues, inductive essence of learning, exhaustive search, heuristic search, divide and conquer learning, progressive coverage, predicate logic, concept formation, quest for natural laws, discovery in dynamic systems, analogy providing search heuristics, close neighbourhood, genetic algorithms, perspectives, hybrid systems and multi-strategy learning [12,13]. In the following sections, we will cite examples and discuss the application of different data mining and machine learning tools in aquaculture and fisheries. The organisational structure of this article and the rationale behind it is illustrated in Fig. 1. Download : Download high-res image (724KB) Download : Download full-size image Fig. 1. Organisational structure of the review. 3. Application in aquaculture systems and practices Aquaculture is the process of breeding, rearing, producing and harvesting aquatic organisms such as fish, crustaceans and molluscs in a highly dynamic system with intricately linked physical, biological and economic environments and processes. Therefore, as in agriculture and livestock farming sectors, fish farmers and enterprises have to make several decisions every day to wade through the intricate complexities and influencing factors which determines the final fish yield, productivity and profit margins. In this high-risk environment, appropriate use of data mining and machine learning tools can provide intelligent solutions for better management of aquaculture facilities covering all the farming and biological processes such as feeding, water quality monitoring and control, fish biomass and welfare monitoring, disease detection / prevention and final product quality analysis (Fig. 2). The use of information technology tools such as computational algorithms, sensor networks, microelectronics, cloud computing and internet of things in operational process control, engineering innovations, data analytics and decision support system for precision or smart aquaculture has been comprehensively reviewed by many authors [2,7,8,15,16,17,18,19,20,21]. Here, we have presented basic and updated knowledge in the practical applications of data mining and machine learning for intelligent decision support in monitoring / control of the rearing environment (i.e., risk management); optimization of resource (feed) use; and fish biomass, growth and welfare monitoring (stock assessment) in intensive production systems (Table 1). Download : Download high-res image (1MB) Download : Download full-size image Fig. 2. Schematic representation of the potential applications of data mining and machine learning techniques in aquaculture. Table 1. Different computational (data mining and machine learning) methods and their applications in aquaculture operations. Computation method Application Domain Reference Empty Cell 1. Recirculating intensive aquaculture expert system (RIAX) Monitoring and control of feeding, temperature, water quality, flow, oxygen and water level to optimize production efficiency in tilapia culture Farm management (RAS) Padala and Zilber [22] 2. Fuzzy logic-based expert system Optimizing denitrification (nitrate removal) rates and eliminating discharge of toxic by-products in an automated denitrifying bioreactor in recirculating aquaculture systems Farm management (RAS) Lee et al. [23] 3. Fog computing for data acquisition and processing Real time monitoring and control of water quality and biomass management in recirculating aquaculture systems Farm management (RAS) Romli et al. [24] 4. Artificial neural networks Remote online water quality monitoring system for forecasting and controlling water quality in intensive fish culture Farm management (RAS) Zhu et al. [25] 5. Fuzzy logic controller Monitoring and control of water temperature in recirculating aquaculture system using geothermal energy and plate type heat exchanger Farm management (RAS) Farghally et al. [26] 6. Bond graph technique and air control algorithm Automatic control of submerging and surfacing of submersible fish cage system based on wave action / height Farm management (Cage system) Kim et al. [27] 7. Fog computing for data acquisition and processing Sensing and remote control of water flow and water level in the growbeds of Aquaponic systems Farm management (Aquaponics) Romli et al. [28] 8. Kalman filter algorithm and optimization scheme Optimization and control of water pumping in fish farms based on water level sensing and prediction for efficient energy consumption Farm management Ullah and Kim [29] 9. Computational fluid dynamics software ANSYS FLUENT and modified DO ecological model Monitoring of dissolved oxygen profiles and development of diffused aeration control system to maintain adequate oxygen levels in fish ponds and to increase energy savings. Farm management (Pond system) Cruz [30] 10. Microcomputer-processor and BASIC program Automatic feed delivery and complementary temperature control-recording in experimental fish hatchery Feed management Hoy [31] 11. Visual signal processing system and support vector machine based classifier Continuous monitoring and automated control of the fish feeding process in aquaculture tanks Feed management Atoum et al. [32] 12. k- Nearest neighbour and principal component analysis Classification of hunger behavior or state of fish by means of an automated feeder and image processing for improving feed utilisation Feed management Razman et al. [33] 13. Adaptive neural-based fuzzy inference system Assessment and decision making for feeding based on changes in water quality parameters Feed management Wu et al. [34] Zhao et al. [35] 14. Spectral data processing and computing Efficient operation of algae production system by real-time bioreactor monitoring and control with respect to nutrient delivery, biomass harvesting time, light and temperature Live feed production Cai et al. [36] 15. TDoA algorithm with acoustic telemetry and SLIM-LPWAN Real-time monitoring of fish behavior and decision making in marine fish farms based on telemetry data Fish behavior and welfare Hassan et al. [37] 16. Adaptive neural-based fuzzy inference system Using machine vision to monitor animal activity, size and current condition in situ without disturbing the animals’ behavior Fish behavior and welfare Whitsell et al. [38] 17. Kullback-Leibler divergence method Understanding the cause and effect dynamics of social learning and foraging behavior in fish using time series data of inter-individual interactions Fish behavior and welfare Ochiai et al. [39] 18. AEFishBIT tri-axial accelerometer and Cosinor analysis Monitoring and correlating locomotor activity and respiratory frequency with body weight for reliable phenotyping of farmed fish for selecting more efficient farmed fish. Fish behavior and welfare Ferrer et al. [40] 19. Multi-layer perceptron neural network and support vector machine models Visual machine technology-based intelligent system for separation and recognizing live and dead eggs of rainbow trout Hatchery operation Rohani et al. [41] 20. Support vector machine with radial based kernel Non-invasive classification of farmed rainbow trout based on dietary effects on live fish skin and image-based features Fish nutrition / product quality Saberioon et al. [42] 3.1. Monitoring and control of the rearing environment In an intensive aquaculture facility, the first critical control point which requires immediate decision making in a day-to-day farming situation is the monitoring and control of the physical rearing environment. This includes the monitoring and control of (1) water quality parameters such as temperature, dissolved oxygen, pH, ammonia, nitrate, nitrite, salinity and suspended solids; (2) water level, distribution and pumping systems; (3) effluent and waste management systems; and (4) operation of the physical containment structures. Real-time collection, processing and analysis of reliable data from the physical environment that leads to well informed timely decisions can greatly improve the process stability / efficiency and fish growth / welfare, while reducing wastage of resources, stressors and cost of production [7,8,19]. Four basic designs of process control engineering systems are generally used in the physical realm of the aquaculture facilities. In the increasing order of complexity and degree of control, they are: data logger or closed loop controller systems, programmable logic controller (PLC) systems, supervisory control and data acquisition (SCADA) systems and distributed control systems (DCS). Further, the characteristics of a process control system varies based on the operating system, data mining or machine learning algorithms and attached modules such as input / output drivers; process database builder; man-machine interface; scan, alarm and control program; tag group editor; dynamic data exchange server; trend analyser; report generator; and messaging / remote dialer. The details of a process control system have been precisely described in early reviews by Lee [8,15]. In the application context, once the optimal physical environmental conditions are identified and defined, the process control systems continuously monitor the target parameters and automatically activates the remedial actuator and alerts the farmer when a deviation from the prescribed range occurs. For instance, when dissolved oxygen levels falls below the set threshold in an outdoor shrimp culture pond, a simple cloud based real time monitoring and control equipment can sense the problem and automatically switch on the pond aerators (PondGuard, Eruvaka Technologies, India). Among the various fish production systems, process control and engineering solutions are most commonly used in land based recirculating aquaculture systems (RAS), where high density of fishes are reared under controlled environmental conditions. In one of the early developments, Padala and Zilber [22] developed a rule-based expert system (recirculating intensive aquaculture expert; RIAX) to monitor and control temperature, dissolved oxygen, water quality, flow and feeding in an intensive tilapia culture system. In this system, the water quality parameter sensors, meters and actuators (valves and pumps) were connected to a PLC; and machine learning involved start-up, run-time, operator assisted maintenance and equipment diagnostic databases. With technological advances, intelligent risk control systems are designed to monitor and forecast water quality with artificial neural networks and other algorithms to proactively control water quality in a timely manner. Hu et al. [19] has meticulously reviewed the different methods of water quality data acquisition and pre-processing (e.g., wireless sensor networks), challenges in data transmission and the various algorithms and models used for data analysis and forecasting changes in water quality. To cite an example, Zhu et al. [25] developed a remote wireless system which combines mobile telecommunication (CDMA service) and web-server-embedded (VPN) technology to accurately forecast dissolved oxygen levels in an intensive farming system half an hour ahead, using historical trends and artificial neural network algorithms. For more accurate dissolved oxygen predictions in the short and long term (1 – 48 h), attention based recurrent neural networks was found to be very effective [43]. In some wireless sensor networks, the water quality sensor nodes were connected to a ZigBee wireless communication system and global system for mobile (GSM) for data transmission and LabVIEW software was used for data processing and analysis [44]. To reduce network traffic and overcome issues with response time and scalability linked to the exhaustive data from RAS process control systems, fog or edge computing can be used [24]. Further, automation or process-controlled operation of every RAS component such as the denitrifying bioreactor for nitrate removal [23] and heat exchangers for temperature regulation [26] using fuzzy logic-based expert system or controllers can optimize their performance efficiency in the water recirculation-reuse loop. Smart use of water pumps and other farm equipment by deploying computational optimization frameworks to operate them only when required can significantly reduce the cost of production, while maintaining a stable environment in the rearing system. In this context, Ullah and Kim [29] developed an optimization scheme with Kalman filter algorithm for maintaining the desired water level in a fish rearing tank with less energy consumption by computationally optimizing the operation of a variable speed pump (flow rate and pumping duration) and target filling levels. Similarly, in aquaponics, water circulation (flow rate) for the fish tanks and water retention levels in the plant grow-bed can be remotely monitored and controlled by operating the water pump for predefined durations using a fog computing architecture [28]. Basic system monitoring and control like the above examples is becoming increasingly affordable to small scale fish farmers with the use of single board computers such as Raspberry Pi, Arduino, PocketBeagle, Asus Tinker Board and Odroid. For pond culture systems, Cruz [30] developed a modified dissolved oxygen ecological model with computational fluid dynamics simulation to predict the complex interaction between the aerator and oxygen profile of the pond water. Based on this, a feedback diffused aeration control system was designed and validated for maintaining adequate oxygen levels in the pond with significantly less energy consumption. With respect to the operation of offshore cage culture systems, Kim et al. [27] developed an air control program which can automatically control the submerging and surfacing of a fish cage by sensing water pressure, wave height and wind speed. This algorithm along with the bond graph technique mathematical model can be potentially used to build automatic controllers for setting offshore submersible fish cages at desired depths, according to the surface environmental conditions. An advanced integrated multitrophic aquaculture model is being developed with smart systems, sensors and data sources for long term autonomous monitoring, analysis and decision support functions which enables production optimization and environmental protection, through the European Union IMPAQT project. 3.2. Optimization of feed use Feed and feeding are the ‘make or break’ elements of any fed-aquaculture practice with very significant biological, environmental and economic implications. This is because 1) the nutritional properties of the feed determine the growth, welfare and final flesh quality of the fish; 2) the nutritional and physical properties of the feed determine the farm water quality and nutrient load (nitrogen and phosphorus) in the effluents; and 3) the efficiency of feed use and conversion (i.e., kg fish produced per kg feed) determines the unit productivity (kg/m3), culture duration and profit margins. Moreover, decisions related to the feeding process are critical to ensure that sufficient feed is provided to every fish in the rearing unit for achieving desired growth rates, with minimum feed refusal and wastage. Conventionally, the feeding strategies are decided based on the size of the fish, water temperature and the total biomass, and further adjusted based on visual observation of the feeding activity. The use of intelligent feeding systems is shifting the feed management strategies from an experience / intuition driven process to a more knowledge-driven procedure [2]. In recent years, several studies and specific reviews have focused on the intelligent control of feeding, from different perspectives such as automatic recognition of fish feeding behavior, monitoring of uneaten feed and control of feed delivery [20,45,46]. In the early automated systems, the timing of the feeding event and the quantity of feed delivered were controlled. For instance, a microcomputer and BASIC programming was used to control automatic feed delivery by solenoid actuated devices in an experimental fish hatchery [31]. In the next phase, additional sensors were included to detect feeding activity directly by hydroacoustic / optical monitoring or indirectly by optical / acoustic / infrared observation of the uneaten feed pellets. Based on the information provided by the feedback systems, the feeding rate was controlled by manual or automatic feeder cut off. Recently, in adaptive automated systems, smart feed control algorithms are used to monitor and predict fish behavior and proactively control feed delivery as in the case of centralized pneumatic feeders in offshore salmon cage farms [47]. Examples of the adaptive automated systems commercially used in fish farms are presented in Table 2. Table 2. Examples of commercially available intelligent solutions for aquaculture and fish processing. Application Developer / Products * Country Web link Empty Cell 1. Monitoring and control of feeding rate Observe Technologies United Kingdom https://observe.tech/ eFishery - Feeder Indonesia https://efishery.com/ Umitron - CELL, FAI & EAGLE Japan, Singapore https://umitron.com/ AQ1 feeding systems Australia http://www.aq1systems.com/ Eruvaka - PondMother India https://eruvaka.com/ 2. Monitoring and control of water quality Real Tech - LiquidAi Canada https://realtechwater.com/ AquaManager Greece https://www.aqua-manager.com/ Osmo Systems - Osmobot USA – Siemens - SIMATIC S7–1500 and Totally Integrated Automation Portal Germany https://new.siemens.com/ SmartWater Planet - Medusa and Cloud Spain https://smartwaterplanet.com/ Shanghai Yuxi Automation Technology China http://www.yuxiel.com/ Eruvaka - PondGuard India https://eruvaka.com/ 3. Monitoring of fish biomass and growth rate (including counting and sorting) XpertSea - Xpercount Canada https://www.xpertsea.com/ VAKI - Bioscanner, SmartFlow and Cloud Iceland https://vakiiceland.is/ InnovaSea - SeaStation USA https://www.innovasea.com/ Aquabyte Norway https://www.aquabyte.ai/ AquaScan Norway https://www.aquascan.com/ Skala Maskon - AGM fish egg sorter Norway www.skalamaskon.no 4. Monitoring and forecasting disease outbreak AquaCloud Norway https://aquacloud.ai/ BioSort - iFarm Norway https://www.biosort.no/ IPI - IREF system Singapore https://www.ipi-singapore.org/ Aquaconnect - FarmMOJO India https://aquaconnect.blue/ 4-Deep - Holographic microscopes Canada http://4-deep.com/ 5. Monitoring of fish behavior CageEye Norway https://www.cageeye.com/ ViewPoint Behavior Technology France http://www.viewpoint.fr/ ZebraZoom France https://zebrazoom.org/ idTracker Spain http://www.idtracker.es/ 6. Farm activity tracking and production planning AKVAgroup - AKVAconnect & Fishtalk Norway https://www.akvagroup.com/ Scale Aquaculture AS - Mercatus Norway https://scaleaq.com/ Poseidon AI Singapore https://www.poseidon-ai.com/ Kamahu - SaaS solution France https://www.kamahu.com/ 7. Automation of fish processing systems Marel - FleXicut, FleXisort and RoboBatcher Iceland https://marel.com/ Skaginn3x Iceland https://www.skaginn3x.com/ * Some of these products have integrated multiple applications. As compared to the earlier simple image analysis algorithms such as the object matching algorithm used for automatic pellet counting in salmon sea cages [48], the present computer vision and hydroacoustic based feed control methods uses more complex data mining and machine learning algorithms [45]. These self-learning decision support algorithms are based on intricate biological variables such as feeding motivation or hunger levels of the fish, which are in turn indicated by individual swimming behavior (i.e., speed, direction and acceleration analysis using optical flow and motion patterns) and vertical distribution of fish groups (i.e., depth movements and activity levels analysis using sonar and acoustic telemetry) during the different phases (beginning, midway and end) of the feeding period and non-feeding period [2]. Different approaches and algorithms have been tested for accurate classification of appetite or hunger behavior in fish. For instance, Cubitt et al. [49] developed an intelligent reasoning system to distinguish hunger states in rainbow trout using supervised pattern recognition (support vector machine and quadratic algorithm) based classification of electromyogram (muscle activity) signals. In another approach, video surveillance extracted features were analysed using Principal Component Analysis with varimax rotation for dimensionality reduction and classified using k-Nearest Neighbour for accurate prediction of the hunger behavior of Asian seabass [33]. For more objective assessment of fish feeding intensity, the machine vision image dataset from a tilapia feeding experiment was initially constructed and extended using rotation, scale and translation augmentation and noise-invariant data expansion. The convolutional neural network (CNN) model trained with this augmented dataset was then found to accurately detect and grade the different degree of appetite or feeding intensity [50]. With respect to feeding decisions based on changes in environmental parameters, Wu et al. [34] developed a hybrid learning adaptive neural-based fuzzy inference system (ANFIS) algorithm with a fuzzy logic technique to optimally capture transient changes in dissolved oxygen levels associated with flocking and struggle behaviors of the food-searching fish, for feeding related decision-making in semi-intensive silver perch culture in Taiwan. A similar application of ANFIS algorithm for automatic feeding decision-making was optimized for outdoor intensive pond culture of grass carp, based on changes in dissolved oxygen saturation and temperature [35]. In a different approach, complex optical features associated with the spontaneous collective behavioral response of a swimming group of Nile tilapia to feed stimuli in a recirculating aquaculture system such as the dispersion degree, interaction force and changing magnitude of water flow field was quantified (by means of covariance, a modified social force model and a kinetic energy model), integrated and used to assess their real-time appetite levels [51]. Likewise, integration of different visual observations such as active consumption of feed and detection of excess feed using multistage approaches such as the use of correlation filter and support vector machine-based classifiers can be useful in enhancing the precision of automatic feeding control in intensive systems [32]. Also, by accounting for spatio-temporal and hydrodynamic aspects of feed distribution, the efficiency of the self-learning algorithms in deciding whether the feeding rates should be kept constant, increased or reduced to meet the actual fish requirement can be further refined. 3.3. Farmed fish stock assessment Besides feed utilization, there are many potential applications for data mining and machine learning algorithms at all stages of aquaculture production from hatchery to harvest, such as the image processing and pattern recognition-based inspection of egg quality, fish numbers, size, biomass, gender, species, welfare, disease incidence and final product quality [2,16,17]. For instance, in a fish hatchery, separation of unhealthy or dead eggs and larvae from healthy ones is mandatorily done by laborious and error-prone manual or semi-automatic methods. To improve the ease and efficiency of this operation, an image feature analysis based intelligent system using multi-layer perceptron neural network and support vector machine classifier algorithms was developed and evaluated for accurately recognizing and separating live and dead fertilized eggs of rainbow trout [41]. Combining such image processing software with engineering systems can be very useful for rapid and accurate egg counting, sizing and sorting in large-scale fish hatcheries (e.g., AGM fish egg sorter). Likewise, counting of fish at different production stages from nursery to final marketing is quintessentially done in aquaculture for optimizing farm management. Non-invasive, rapid and reliable underwater object counting of fishes and other aquatic organisms in a technologically advanced setting can be done using machine learning algorithms which can efficiently process the information relayed by computer vision and acoustic sensor system [21]. For instance, multi-class least squares support vector machine algorithm was used to accurately classify and count overlapping zebrafish from computer vision images [52]; genetic programming was used to capture the temporal dynamics of fish abundance in images acquired from a coastal video-observatory, under variable environmental conditions [53]; and a trained support vector machine classifier was used for automatic enumeration of large free swimming fish from an adaptive resolution imaging sonar (ARIS) dataset [54]. Image processing algorithms are presently being used in commercial fish and shrimp counting devices such as VAKI's fish counters and Larcos Aquaculture's shrimp post larvae counter. For better stock management, fish farmers need to have information also about individual fish features such as length, weight, skin colour and sex during different growth stages [17]. Machine vision and acoustic systems offer a convenient and real-time technological alternative to the invasive, stressful, time-consuming and labour-intensive practice of physical sampling and weighing of fish for assessing growth rates and biomass [55]. With respect to non-contact fish weight determination, Dios et al. [56] used a stereovision system with synchronized convergent cameras to perform illumination compensated three-dimensional segmentation of fish for estimating fish weight from computed length, using the basic length-weight relationships. In another system, a real time vision module captured top and side view images of live fish sliding through a transparent channel and a support vector machine regression module analysed the shape features and estimated the weight of the fish for grading purpose [57]. For remotely monitoring fish growth rate, Costa et al. [58] developed a dual underwater camera optical ranging system to synchronically capture fish images and extract size-shape data using neural network, geometric algorithm and Fourier analysis. Similarly, a digital stereo-video underwater camera system was used for three-dimensional monitoring of free-swimming cultured bluefin tuna and measurement of their length by image transformation and processing [59]. Taking a step further in the measurement of body traits, Fernandes et al. [60] developed a computer vision system coupled with deep learning networks for automatic segmentation of images and indirect measurement of body area, length and height of Nile tilapia. From this body measurement data, the body weight, carcass weight and yield were then predicted using linear models. Besides size-weight estimation, image processing algorithms were found to be promising tools for non-invasively and objectively examining the skin coloration, intensity, hue and saturation of food and ornamental fishes on an experimental scale [61,62]. As skin colour and morphometric features are good indicators of gender in many fish species, development of machine vision systems for sex identification and sorting of fish could have potential aquaculture applications i.e., in the automatic separation of colourful male ornamental fishes [16,17]. For instance, an image processing and segmentation algorithm capable of extracting specific shape and colour features from landmarks on fish contours was devised and validated to accurately identify male and female ornamental guppy fish [63]. Concerning fish welfare and farm management, continuous monitoring of fish behavior and appearance is known to be an efficient method for assessing changes related to stressful rearing conditions and disease incidences. This is due to the fact that stress related physiological changes in the fish are externally expressed as quantifiable changes in feeding intensity, swimming behavior and skin colour. As visual observation by farmers can be subjective, machine vision systems can be alternately used to deliver real-time, autonomous, accurate and reliable information about fish behavioral changes based on video image and tracking features such as swimming speed, trajectory and distance, depth preference / location, fin beating frequency and feed intake [16,17,19]. For instance, high resolution computer image processing systems and algorithms has been developed and used to: automatically quantify size-specific differences in swimming speed and chasing behavior in zebrafish, based on positional coordinates [64]; quantify sensitive changes in the average swimming speed and distribution of a school of Nile tilapia in response to acute hypoxic conditions and restoration of normal oxygen levels, based on change in projected area between sequential fish distribution frames [65]; continuously quantify intra and inter-day changes in swimming speed and direction of Atlantic salmon in commercial sea cages, linked to shifts in tidal cycles, based on detecting and tracking fish shapes frame by frame [66]; remotely measure the interaction (inspection and biting) of gilthead sea bream with aquaculture net conditions under different stocking densities, based on background subtraction and object detection in consecutive frames [67]; and identify the behavioral properties which induce alignment and mediate social learning for coordinated movement in medaka fish groups under free-swimming conditions, based on time series positional coordinate data of individuals [39]. With the present video tracking methods, there are several practical challenges in tracking individual fish in large groups, as they touch each other, cross paths and interact in complex ways. Multitracking algorithms like idtracker.ai which extracts and uses a characteristic fingerprint to correctly identify each animal in a group for indefinite periods can address this problem and help in monitoring collective behavior of fishes. It uses two deep neural networks for adaptively identifying targets and target overlaps [68,69]. Likewise, fishes can be continuously monitored day and night with the help of deep neural algorithms, as it is possible to integrate high precision fish image inputs received simultaneously from sonar and optical underwater camera systems by using conditional generative adversarial networks [70]. The accuracy in fish location and behavior tracking can also be enhanced by using robust algorithms such as the multi-domain deep convolutional neural network [71]. In due course, behavioral monitoring may become a potential tool for non-destructively selecting more efficient farmed fish. For instance, measurements of opercular aperture and body tail movements obtained from farmed free-swimming gilthead sea bream and European sea bass using externally attached body sensors (AEFishBIT tri-axial accelerometer) indicated that lower locomotor activity and enhanced respiration were associated with larger body weight in both fish species. This observation potentially links behavioral traits with efficient feed conversion and growth [40]. Importantly, by combining behavioral monitoring (e.g., abnormal swimming trajectory, positional coordinates and jumping frequencies) with the observation of visible symptoms of disease (e.g., presence of external parasites, skeletal deformities, abdominal dropsy, haemorrhages, dermal ulcers and fin erosions), farm information systems (e.g., water quality and fish biomass) and intelligent algorithms, it is possible to accurately diagnose, forecast and prevent disease outbreak. In the past, for fish disease diagnosis and health management, several expert systems have been developed and used such as Fish Doctor, SALMEX, Fish-Expert and AquaSDS, with some of them based on fuzzy logic and inference system [72]. In these systems, rule-based data mining algorithms was used to extract information from different image and knowledge database components (e.g., farmer and human expert surveys) to reliably diagnose diseases. For instance, Fish-Expert, a web-based intelligent disease diagnosis system developed by Chinese researchers could be used to diagnose 126 kinds of diseases in nine major freshwater fish species [73]. Alternatively, for direct diagnosis of ectoparasitic infections and diseases, combining spectral imaging with machine vision systems could be a possible solution [2]. Recently developed commercial tools such as Aquaconnect (India; Table 2) uses machine learning technology for disease prediction, based on farm water quality, feeding and growth information. Whereas, the AquaCloud platform, developed under the NCE (Norwegian Centers of Expertise) Sea food Innovation project, uses a workflow that include standardization of farm level fish health data collection (Fishtalk), facilitation of digital data exchange (Mercatus) and intelligent processing of data for strengthening the decision support related to fish health and welfare. Commercial-scale validation of comprehensive underwater technologies (iFarm developed by BioSort and tested by Cermaq, Norway) that uses machine vision and artificial intelligence to monitor the health and growth of up to 150,000 individual fishes for targeted health interventions is currently in progress. 4. Application in capture fisheries and aquatic environment management Capture fisheries refers to the sum of all activities undertaken to harvest naturally occurring fish and other aquatic living resources in marine and freshwater environment. The world is gaining access to unprecedented amounts of data related to the fisheries sector through hundreds of satellites that observes the earth's climate and environment; several thousand floats that collects marine environmental data; and tracking sensors in fishing vessels and millions of mobile phones that transmit data to and from fishers. In fisheries management, the combination of data flow and machine learning will be a game changer in real time monitoring of fishing activity, identifying safe and profitable fishing routes, forecasting biomass, decision support regarding closed fishing areas, data-driven enforcement of regulations and adaptive management strategies in response to field signals (Fig. 3). In this direction, FAO [1] has recently developed a web-based IT solution named Calipseo to integrate and streamline fisheries data along the national data supply chain. It is designed to collect, manage and process various types of fisheries data, which includes administrative records, fishing activity log, biological information and many others, according to the needs of the national fisheries authorities. In Table 3, we have listed some of the research related to data mining and machine learning applications in fisheries. Download : Download high-res image (2MB) Download : Download full-size image Fig. 3. Schematic representation of data collection, processing and applications in capture fisheries and environmental monitoring. Table 3. Different computational (data mining and machine learning) methods and their applications in capture fisheries and allied activities. Computation method Application Domain Reference Empty Cell 1. Object oriented modelling and multi-agent simulations Simulation of ecological, economic and sociological dynamics from a fishery for qualitative assessment Fisheries resource management Bousquet et al. [74] 2. Time series decomposition, principal components analysis and k-means clustering Identification of functional relationships in fisheries such as environmental patterns associated with fisheries abundance fluctuations Fisheries resource management Plaza et al. [75] 3. Spatiotemporal assignment mining model (STAMM) Determination of the spatio-temporal relationship between environmental factors and fish distribution (temperature and assemblage) Fisheries resource management Su et al. [76] 4. Genetic programming algorithm Analysis of fish community based on ecologically relevant hydrologic indicators Fisheries resource management Yang et al. [77] 5. Self-organising feature maps Ecological exploration of the relationships between river water quality variations and fish communities Fisheries resource management Tsai et al. [78] 6. Apriori algorithm for categorical data analysis Identification of biological associations in marine capture fisheries data Fisheries resource management Pugazhendi [79] 7. Convolutional neural network and CT scanning Development of automated age estimation system from otolith images for age-based fish stock assessment studies Fisheries resource management Moore et al. [80] 8. Neural computational model GrowthEstimate Growth estimation of fish at individual levels in natural ecosystems based on biological factors Fisheries resource management Rungruangsak-Torrissen and Manoonpong [81] 9. Species flow network (SFN) via graph clustering approach Discovering patterns in global shipping network that affects aquatic invasions to devise management strategies Fisheries resource management Xu et al. [82] 10. Density-based spatial clustering of applications with noise (DBSCAN) and k-nearest neighbour Determination of potential fishing zones by integrating the mean of fish catches for each area with sea surface temperature and chlorophyll concentration. Fishing technology Fitrianah et al. [83] 11. Sequential forward floating selection (SFFS) and k-nearest neighbour Prediction of potential tuna fishing zones by combining oceanographic features with fish biology and habitat preferences Fishing technology Hidayanto et al. [84] 12. Advanced grid-based isodensity line clustering (AGRID+) Determination of potential fishing zones by computing the mean of fish catches for each area Fishing technology Fitrianah et al. [85] 13. Convolutional neural network for processing deep vision images Automatic identification and classification of fish species present in images from trawl camera system to support acoustic data interpretation Fish catch monitoring Allken et al. [86] 14. In-trawl stereo imaging and dimensional analysis Dimensional position analysis via time and depth-referenced images of marine organisms for improved interpretation of acoustic echograms, fish distribution and behavior Fish catch monitoring Rosen and Holst [87] 15. Open source computer vision library (OPENCV 3.0) Quantification of the escapement rate of unwanted fish through a selective device Fish catch monitoring Simon et al. [88] 16. Mask region - convolutional neural network (r-cnn) Fish size measurement by automatic segmentation to reduce catches of undersized fish in trawling Fish catch monitoring Garcia et al. [89] 17. Integrated spatiotemporal visualization architecture Organization and visualization of marine environmental data for reliable and integrated monitoring Environment monitoring He et al. [90] 18. Computing unified device architecture parallel computing and GPU programming Organization and multi-dimensional visualization of large-scale marine environmental data for reliable and integrated monitoring Environment monitoring Su et al. [91] 19. CHLfuzzy model Forecasting of chlorophyll concentrations in coastal lagoons using predictor environmental variables Environment monitoring Sylaios et al. [92] 20. Artificial neural network and support vector machine models Simulation of heavy metal concentrations in an aquatic environment using surface water physicochemical indexes Environment monitoring Lu et al. [93] 21. Artificial neural network Prediction of the timing and magnitude of algal blooms based on environmental factors Environment monitoring Wei et al. [94] 22. Artificial neural network with differential evolution and simulated annealing Optimization of the cooking parameters of fish for minimizing the loss of nutritional quality Fish processing technology Sadhu et al. [95] 23. Genetic algorithms and convolutional neural networks Automated cleaning of fish processing lines / plants using a custom-made robotic manipulator Fish processing technology Bjørlykhaug and Egeland [96] 24. Circular hough transform and adaptive intensity thresholding Using visual changes for differentiating between fresh water and heavy metal exposed fishes Fish quality assurance Issac et al. [97] 25. Wavelet neural network with genetic algorithm Forecasting the short-term changes in the market price of fish and other aquatic products Fish marketing Hu et al. [98] 26. x-means clustering Estimation of factors that affect the income and profitability of coastal fishermen communities Socioeconomics Hamid et al. [99] 27. Multiple linear regression, support vector regression and k-means clustering Model to forecast the socio-economic welfare and purchasing power of fishermen and aquaculture farmers Socioeconomics Teniwut et al. [100] 4.1. Fisheries resource assessment and management In the domain of fisheries management, one of the early studies developed and implemented an object-oriented modelling and simulation based on distributed artificial intelligence for the management of fisheries resources by considering human, resource and ecosystem interactions in Mali's central Niger delta fishery [74]. In this study, object-oriented modelling and multi-agent simulations were used to artificially simulate a complex ecosystem representing fishermen, fish community, biotope, fishing gear and many other interacting entities, and tested different fishing scenarios and hypothesis to derive qualitative fisheries management options based on ecological, economic and sociological knowledge. With respect to knowledge based expert systems in fisheries management, Alagappan and Kumaran [72] have listed and summarized different applications such as canonical fishery management expert system (CANOFISH) for the management of multispecies fisheries resources; ProTuna for management of tuna fisheries; CLUPEX for management of herring fisheries; FISHMAP for fish stock assessment studies; SimerFish for fish stock estimation and total allowable catch determination; and fishway design expert system (FDES) for fish migration path support. For data mining applications in fisheries, environment-resource approach has been a focus area to overcome the challenges in studying the complex relationship between spatiotemporal dynamics of an aquatic organism and the environmental factors which governs their life cycle processes. For instance, combining data mining technique (k-means) with time series decomposition and multivariate analysis, Plaza et al. [75] identified ecosystem patterns that are associated with fluctuations in anchovy (Engraulis ringens) and sardine (Sardinops sagax) fisheries abundance and landings in northern Chile, with respect to environmental changes such as El Niño and long-term cold-warm regimes. Another data-mining approach named the spatiotemporal assignment mining model (STAMM) has been used to study the spatiotemporal pattern of temperature which controls the assembling and distribution of fish in the Dasha area of Yellow Sea in China. The STAMM algorithm incorporates fuzzy knowledge as prior experience and uses neighbourhood rules and relationships to construct a decision table with recursive indices, in order to determine the spatiotemporal assignment of temperature and its association with the occurrence of an active fishing ground in Yellow sea with higher catch per unit effort [76]. Using the diversity and abundance of fish community (Shannon index) as an ecological target, genetic programming method has been used to identify the most ecologically relevant hydrologic indicator (i.e., timing and magnitude of low flow) in the upper Illinois river [77]. Similarly, a data mining framework using self-organizing feature map and structuring index was used to explore the multiple relationship between fish community and water quality in Dahan river in northern Taiwan by clustering, analysing and visualizing long-term heterogeneous datasets of major water quality parameters and fish species abundance in multiple sampling sites [78]. Few studies have also used simple data mining techniques such as nearest neighbour clustering and Apriori algorithm for fish stock assessment applications [79,101]. Clustering of species-wise catch per unit effort data based on local fisheries statistics in Tonle Sap Lake in Cambodia indicated a correlation between stock depletion and market price of the fish [101]. Association rule mining based Apriori algorithm was found to be useful to extract meaningful patterns of biological associations in trawl fishery data from Chennai fishing harbour, India [79]. Decision-Interval Cumulative Sum (DI-CUSUM) control chart was found to be helpful in monitoring fisheries indicators such as recruitment index and large fish indicator in a simulated fishery. Further addition of engineering process control method constructed harvest control rule to DI-CUSUM potentially helps in managing a data poor fishery based on the indicator trends [102]. Concerning the application of machine learning in fish biology, pre-trained convolutional neural network (CNN) designed for object recognition was tested and used to estimate the age of a fish from CT scanning images of otolith annular structure, by resolving banding patterns. If the CNN based approach is further optimized for higher resolution power comparable to the standard age determination by a human reader, micro-CT scanner and machine learning combination can be used to determine fish age in industry production lines [80]. For growth estimation of individual fish in natural ecosystems, where environmental and biological conditions cannot be controlled, a neural computational model GrowthEstimate was developed using recurrent neural networks of reservoir computing type. This model was trained to estimate weight specific growth rate (SGR) using recorded datasets of biological factors such as age – weight, body condition factor – protein efficiency and digestive efficiency i.e., trypsin-chymotrypsin ratio in Atlantic salmon, rainbow trout, Nile tilapia and Arctic cod [81]. Quantitative risk assessment of aquatic invasions i.e., the unintentional transport and establishment of non-native and harmful invasive species through the global shipping network, remains very complex and challenging. Decomposition of species flow network into clusters of ports and inter-cluster connections using graph clustering (data mining) approach helped in discovering crucial patterns of how global shipping network may affect aquatic invasions [82]. This knowledge obtained by integrating shipping network, environmental and ecological data is critical to devise effective invasive species control strategies by fisheries authorities. 4.2. Fishing and fish catch monitoring Fishing is the activity or occupation of catching fish and other aquatic animals in the wild, mostly for food or as a sport. It is a major economic activity which provides livelihood to millions of people around the world. One of the most challenging tasks in this field is locating and catching fish. Combining remote sensing satellite information with marine environment (e.g., sea surface temperature and chlorophyll concentration data) and fisheries resource (catch statistics) datasets, scientists have developed techniques to identify the locations of fish aggregation, which are termed as potential fishing zones (PFZ). Accurate prediction of PFZ economically benefit fishers as it significantly reduces the time, effort and resources spent in searching fish shoals. In India, the Indian National Centre for Ocean Information Services (INCOIS; www.incois.gov.in) is actively disseminating information on PFZs to fishers through various communication modes, on daily basis throughout the year, except during the closed fishing season. The application of data mining and machine learning algorithms to the highly dynamic and multi-dimensional datasets is known to improve the accuracy of determining PFZs. A density-based spatial clustering of applications with noise (DBSCAN) algorithm which identifies spatio-temporal clusters of zones with data on the largest number of fish catch correlating with data on sea surface temperature and sea surface chlorophyll concentration, was integrated with k-nearest neighbor classification method to accurately determine PFZ for albacore tuna (Thunnus alalunga) in the eastern Indian ocean area [83]. This data mining framework outperformed a knowledge based expert system with heuristic rules defining the fishing grounds. Similarly, a grid density-based clustering for high dimensional data (AGRID+) algorithm was used to identify prominent daily clusters (PFZs) based on temporal and daily aggregates of fishes in different spatial clusters in the eastern Indian ocean [85]. As fishes have specific habitat preferences, determining the multiple oceanographic features associated with the distribution and abundance of each species is crucial to accurately predict PFZs. Using the Sequential Forward Floating Selection method with AGRID algorithm and k-nearest neighbor classification, oceanographic features (temperature, chlorophyll, salinity, ocean current and wave action) that were associated with the catch abundance of bigeye and albacore tuna was selected to accurately predict their respective PFZs in the Indian ocean area [84]. For real time monitoring and regulation of fishing, automatic identification system of fishing vessels and gear deployment can be combined with machine learning algorithms to track the movement of fishing vessels, predict the type of fishing activity and quantify the intensity of the fishing effort [103,104]. In the electronic monitoring (EM) programmes, fishing activity and catches are remotely recorded using cameras and activity sensors in the fishing vessels to generate reliable and high-resolution data on spatial and temporal patterns in effort and catch composition and volume. Compared to the conventional fish catch monitoring methods, EM improves the coverage of fish catch monitoring and the potential of large-scale implementation in a cost-effective manner, while ensuring better compliance to fishing regulations and discard reduction. It also improves traceability, sustainability claims and market access in the seafood supply chains [5]. Combination of computer vision and machine learning algorithms (e.g., OpenCV, BIIGLE, CATAMI, Squidle, FishTick, CoralNet, VIAME, TensorFlow and DeepSORT) is increasingly employed for automated identification (i.e., detection and classification), counting and underwater behavioral analysis of fish and other aquatic life in their natural habitat for non-invasive ecosystem monitoring or during fishing operations [4,105,106]. In Norway, the Institute of Marine Research and Scantrol AS have developed an in-trawl stereo camera system (Deep Vision), which continuously collects overlapping images of various aquatic organisms before they enter the cod-end. As it records the precise time, depth and geographic location for each aquatic organism that passes the cameras, this system significantly enhanced the temporal and spatial resolution of trawl data used to ground truth the presence and abundance of species [87]. The deployment of a convolutional neural network (CNN) trained on synthetic data was found to be useful for automatically detecting and classifying the aquatic species (blue whiting, Atlantic herring and Atlantic mackerel) captured in the images from the Deep Vision trawl camera system with high accuracy. These advanced methods can overcome the challenges in the interpretation of acoustic data from acoustic-trawl surveys [86]. Further, individual fish size measurement by automatic segmentation of underwater stereo images of fishes (blue whiting, saithe, redfish, Atlantic mackerel, Atlantic herring, velvet belly lanternshark and Norway pout) acquired by the Deep Vision imaging system using a Mask region CNN architecture is a promising application to monitor and reduce the catch of undersized fish in commercial trawling, as it overcomes the technical limitations of echosounders [89]. Likewise, analysis of underwater videos recorded from a trawling trial in the English Channel using an automated image processing algorithm, open-source computer vision library (OPENCV 3.0), was found to be useful in quantifying small fish escapement through escape panels of a by-catch reducing device in active fishing gears [88]. In this trial, the automated count of fish escapement rate was similar to the manual count of human observers, validating the new approach. Considering all the above, it is evident that integration of technologically advanced data systems into fisheries management will make it more responsive, robust and effective in sustainably assessing, harvesting and managing fish stocks. 4.3. Environment monitoring With the recent advances in environment monitoring technologies, massive volumes of complex environmental data are continuously generated. As these data are dynamic, heterogeneous, multi-dimensional, multi-sourced and widespread, they need to be organized, integrated and visualized to make them informative for environmental monitoring and decision-making applications [90]. Continuous automatic marine observation networks like the Integrated Marine Observing System (IMOS) of Australia; MyOcean information system and Copernicus Programme of the European Commission; British Oceanographic Data Centre (BODC) and Grid for Ocean Diagnostics, Interactive Visualization and Analysis (GODIVA) of United Kingdom; Coriolis operational oceanography of France; USGODAE (Global Ocean Data Assimilation Experiment) Argo GDAC data browser of the United States of America; Indian National Centre for Ocean Information Services (INCOIS) of India; National Marine Data and Information Service (NMDIS) of China; and Marine Environmental Data Section (MEDS) of Canada uses advanced data systems and machine intelligence techniques in the collection, processing, distribution and forecasting of marine environmental data such as sea surface temperature, salinity, density, wave action, ocean current and chlorophyll. For instance, spatio-temporal and multi-dimensional visualization architectures with real time interaction provisions have been developed for large-scale marine environmental data collected by Argo floats in South China Sea. These data- and process-oriented computing algorithms based on Graphics Processing Unit (GPU) programming and integrated visualization architectures can efficiently and intuitively simulate and display dynamic processes and properties of marine environmental factors [90,91]. Besides hydrological information, phytoplankton primary production and associated non-linear biological processes could be modelled using artificial neural networks (ANN) based on remote sensing derived predictive variables such as biomass and irradiance; or constrained training of the neural networks in data limited situations; or metamodeling using neural networks and empirical modelling [107]. Similarly, ANN and genetic algorithms was found to be useful in predicting the timing and magnitude of algal blooms of Microcystis, Synedra and Phormidium in Lake Kasumigaura in Japan with reasonable accuracy, based on the interaction of algal genera with combinations of environmental factors such as total nitrogen, total phosphorus, pH and chemical oxygen demand [94,108]. To illustrate the scope and coverage of data mining algorithms, even a simple user-friendly multiple input single output fuzzy rule-based MS-Excel spreadsheet (CHLFuzzy) was found to be efficient in predicting chlorophyll-a concentrations based on temperature, dissolved oxygen, dissolved inorganic nitrogen and solar radiation levels in a coastal lagoon in Greece [92]. The performance of CHLFuzzy model was highly comparable to direct estimates and satisfactory against an Adaptive Neural Fuzzy Inference System (ANFIS). Also, artificial neural network and support vector machine quick simulation models were found to be useful in simulating dissolved, particulate and total heavy metal concentrations in aquatic environment based on physicochemical indices such as pH, suspended solids, temperature, total phosphorus and permanganate index [93]. These prediction models can be integrated with the existing resource constrained environment monitoring programmes and applied in a large-scale setting using the internet of underwater things which comes with different communication, monitoring and localization technologies [109]. Additionally, this may involve the progressive use of underwater micro- and nano-sensors; unmanned underwater vehicles; data transmission and storage tags; and information routing network protocols for active ecosystem surveillance. 5. Application in fish processing and quality assurance Industrial scale application of machine learning algorithms along with computer vision and robotics are revolutionizing seafood processing across the globe. Icelandic company Marel is the pioneer of integrating computer technology into fish processing equipment for smarter processing systems. They have developed and manufactured equipment, systems and software for automated processing of salmon and whitefish. Through an array of interconnected intelligent systems, the smart fish processing lines has automated product flow control, quality checking, filleting, pinboning, portioning, product distribution, packing and batching. While Marel FleXicut integrates computer vision and machine learning to automatically adjust the cutting patterns to the size of each fillet, FleXisort and RoboBatcher controls product distribution to different packing lines and robotic packing / batching. These smart processing lines developed by Marel and other companies like Skaginn 3X increases the processing throughput and improves overall yield and quality, with less handling and giveaways. As maintaining process hygiene is critical for product quality in fish processing plants, machine learning methods have also been applied to optimize automated cleaning of fish processing plants and further assess the quality of the robotic cleaning. While genetic algorithm was found to be well suited for the optimization of a robot manipulator for automated cleaning of fish processing plants [110], vision system and classification using convolutional neural networks (CNN) performed well in detecting residual fish blood in robotically cleaned surfaces of processing lines in industrial settings [96]. Combination of spectral imaging with vision system and CNN can further enhance the functionality of the automated hygiene assessment system by detecting biofilms and fish debris in processing lines, along with residual blood. For the assessment of fish quality and freshness, machine vision (machine learning integrated with image acquisition, processing and analysis) is a rapid, accurate, consistent, efficient and cost-effective alternative for the conventional organoleptic evaluation methods. This technique mainly depends on the appearance (size estimates, shape parameters, patterns and colour shades) of whole fish, skin, gills, eyes and fillet to recognize quality defects and freshness. It involves digital image capturing at appropriate illumination, segmentation, feature extraction and classification / matching [111,112]. In this scientific context, an automatic image processing method was developed to discriminate image features of gill tissues and predict the freshness of fish. The red channel gill images of the Indian major carp, rohu (Labeo rohita) were automatically segmented using a k-means clustering algorithm and features were strategically extracted from the region of interest in wavelet domain, decomposed into discriminatory coefficients and analysed to satisfactorily train and predict the freshness of the fish samples [113]. Detection of visual differences were also useful in differentiating fishes harvested from fresh and contaminated water bodies. For instance, the digital image of the eye of Channa punctatus (spotted snakehead) was automatically segmented using the Circular Hough Transform and adaptive intensity thresholding; potential visual features were then extracted and transformed into mathematical parameters for differentiating between the heavy metal exposed and control fishes with high accuracy [97]. Finally, to cite the application potential in food process engineering, a hybrid algorithm which integrates artificial neural network with differential evolution and simulated annealing was used to optimize fish cooking parameters (temperature, frying time and oil quantity) by finding their non-linear correlations with the content of beneficial fatty acids [95]. This process optimization could simply ensure maximum retention of nutritional quality of fried fish for a common man. 6. Application in fish marketing and socioeconomics Given the volatility of market prices and the complex time lag data, price forecasting of fish commodities is very challenging, but useful for fishers, farmers, processing industries and policymakers to minimize the uncertainty and risks in fish marketing. Time series forecasting models such as Autoregressive Integrated Moving Average with or without Exogenous Inputs (ARIMA/ARIMAX) are the common statistical tools used for price forecasting of fish commodities [114]. However, with increasing volumes of global fish trade data (https://comtrade.un.org/), customized data mining and machine learning methods can be used to improve the robustness and accuracy of price forecasting as compared to the linear models. Some of the early knowledge based expert systems used in fish marketing and trade were strategic decision-making system for export firms, STRATEX, Norway and aquatic products price forecasting support system, APPFSS, China [72]. Recently, in a comparative study of sixteen different forecasting techniques to predict the short-term spot price of Norwegian farmed Atlantic salmon, k-nearest neighbor classification method delivered the best prediction for one week ahead, while vector error correction model estimated using elastic net regularization delivered better predictions for two and three weeks ahead [115]. Similarly, a hybrid model which integrates Wavelet Neural Network with Genetic Algorithm was found to perform well in predicting the price of Chinese aquatic products in a time scale of 1–10 days, based on time series data of prices [98]. On the other hand, to sustainably improve the welfare of coastal fishing communities, it is necessary to estimate the various factors which influences their income and profitability. The x-means clustering data mining approach was used to detect outliers in fisheries commodity transactions which could have potentially improved the profitability of fishermen [99]. Combining k-means clustering with support vector regression and multiple linear regression, a forecasting tool was developed and used to predict or ascertain the economic welfare (purchasing power) of fishermen and fish farmers of Indonesia based on micro and macroeconomic indicators [100]. Overall, in an increasingly data rich environment, data mining and machine learning tools has potential applications in every aspect of the fisheries information supply chain from biology to economics. 7. Conclusion: challenges and future perspectives Advanced systems for data collection, organization and intelligent analysis have become quintessential in all spheres of human activity, albeit at different scales of operation from mobile phone applications to supercomputers. Aquaculture and capture fisheries are no exception to this new paradigm. Rapid advances in information / communication technology, sensor networks and autonomous platforms have enabled the collection of unprecedented volumes of aquaculture and fisheries related data, at relatively lower costs. However, the translation of the available data into effective and usable information is limited by the relatively slow progress in the development of data analytical capacity (i.e., data mining and machine learning systems) which minimizes the dependence on human expertise. Practically, the automated data mining and machine learning models have to overcome several difficulties associated with analysing massive volumes of data of increasing complexity and varying quality. Often, fisheries and aquaculture data are incomplete, inaccurate or difficult to use due to the highly complex natural and farm environments. Overexploitation of several fish stocks and many aquaculture crises can be attributed to uncertain assessments, inadequate information and human factors. But even with the use of advanced technologies, there will be different types of challenges in data collection and processing. For instance, the detection ability of a machine learning algorithm for fish catch monitoring can be disrupted in a very turbid natural environment. Likewise, in aquaculture, extremely accurate and robust databases may not be available for new fish species or farm sites. At times, water quality variables may not interact in a predicted manner for soft-sensing (indirect estimation). Even for fish behavior monitoring, it should be noted that the inspected subjects are sensitive and free moving in an environment which has varying illumination, visibility, noises, interferences, object overlaps and occlusions. Therefore, computational tools have to address various issues related to data selection, pre-processing and analytical speed / accuracy, while extracting the desired information for fisheries management or aquaculture production. In less developed countries, where skilled labour availability and costs are not a constraint, farmers may favour conventional approaches and not readily adopt the technology for automated management due to increased risk of catastrophic loss if there is any equipment or technical failures. Under the harsh and wet operational environments in aquaculture, sensors may fail, communication could be slow due to low signals and power supply could be interrupted. So, with complex mechanical, biological, environmental and economic factors governing decision making in fish farms, the shift from human observation and analytical thinking to artificial intelligence and automated systems would be gradual and may depend on the success of risk management protocols. Likewise, in places where small-scale aquaculture and fisheries operations are predominant, the availability of capital costs to install the physical systems (sensors, equipment and communication devices) and highly skilled manpower (in programming languages and virtual frameworks) to maintain / operate the sophisticated analytics and implementation systems could be lacking. Also, when it comes to data owned by individual stakeholders (fisher, farmer or business) and even countries, competitive market advantage and national sovereignty is a major barrier for data sharing. Consequently, data security becomes an important additional responsibility for analytics providers. For instance, data linked to traditional ecological knowledge of fishing spots; experience derived knowledge of risk management in fish farms; and fisheries resource database of many countries are firmly safeguarded to protect commercial interests. Other factors such as the lack of communication and cooperation between policy-makers, fishery managers and fishers can hinder the adoption of advanced data systems in capture fisheries management. Despite the challenges related to data volume, complexity, quality and security, in the near future, the adoption of intelligent data systems and tools is inevitable in aquaculture and fisheries management as it facilitates proactive and knowledge driven decisions that could prevent major farm accidents / losses as well as rapid decline in fish populations due to subjective human experience. Very recently, the successful trial production of high-quality Atlantic salmon in iFarm (developed by Cermaq and Biosort), where machine learning and artificial intelligence tools were used to follow the growth and health of each individual fish in a cage signals a giant technological leap in this direction. The rate of technology uptake will however depend on how the initial experiences are used to improve the efficiency, accuracy and reliability of data collection, communication and processing systems. Thus, the development of cost-effective and user-friendly sensors / associated equipment with higher sensitivity and faster capabilities; and continuous advances in data processing and analysis systems for rapid extraction of information and more accurate interpretation would be fundamental to large-scale application of data mining and machine learning in fisheries and aquaculture. In other words, the entire data value chain from data collection to storage, management, analysis and final use of information should be well-structured and subjected to meticulous performance monitoring and continuous fine-tuning. Furthermore, the use of unmanned autonomous drones and robots (e.g., SeaDrone, Shoal, OpenROV, OceanOne, Deep Trekker, Aquabotix, Apium Swarm Robotics, Blueye Pioneer and PowerRay) for underwater explorations / operations and its integration with data systems is promising for ecosystem monitoring and intensive cage farming. Similarly, the integration of augmented reality in the data mining and machine learning framework, through wearable devices (e.g., Google Glass and Microsoft HoloLens), data collection suits and virtual workspace (e.g., PondVis) could enhance data capture, analysis and task performance. Finally, involvement of all stakeholders, long-term goals and road map for implementation, and multidisciplinary collaboration between engineers, fish biologists, oceanographers and data scientists will be crucial for the development of reliable data-mining and machine learning frameworks that support sustainable growth in global seafood production. Funding source There is no funding source to be declared. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgment The academic guidance and support of Dr. Antony Selvadoss Thanamani, Head, Department of Computer Science, NGM College, in the PhD research of the first author is gratefully acknowledged. References [1] FAO The State of World Fisheries and Aquaculture - Sustainability in Action The State of World Fisheries and Aquaculture - Sustainability in Action, Food and Agriculture Organization of the United Nations, Rome (2020) Google Scholar [2] M. Føre, K. Frank, T. Norton, E. Svendsen, J.A. Alfredsen, T. Dempster, H. Eguiraun, W. Watson, A. Stahl, L.M. Sunde, C. Schellewald, K.R. Skøien, M.O. Alver, D. Berckmans Precision fish farming: a new framework to improve production in aquaculture Biosyst. Eng., 173 (2018), pp. 176-193 View PDFView articleView in ScopusGoogle Scholar [3] C. Beyan, H.I. Browman Setting the stage for the machine intelligence era in marine science ICES J. Mar. Sci., 77 (4) (2020), pp. 1267-1273 CrossRefView in ScopusGoogle Scholar [4] K. Malde, N.O. Handegard, L. Eikvil, A.B. Salberg Machine intelligence and the data-driven future of marine science ICES J. Mar. Sci., 77 (4) (2020), pp. 1274-1285 CrossRefView in ScopusGoogle Scholar [5] A.T. van Helmond, L.O. Mortensen, K.S. Plet-Hansen, C. Ulrich, C.L. Needle, D. Oesterwind, L. Kindt-Larsen, T. Catchpole, S. Mangi, C. Zimmermann, H.J. Olesen, N. Bailey, H. Bergsson, J. Dalskov, J. Elson, M. Hosken, L. Peterson, H. McElderry, J. Ruiz, J.P. Pierre, C. Dykstra, J.J. Poos Electronic monitoring in fisheries: lessons from global experiences and future opportunities Fish Fish., 21 (1) (2020), pp. 162-189 CrossRefView in ScopusGoogle Scholar [6] F. O’Donncha, J. Grant Precision Aquaculture IEEE Internet Thing Mag., 2 (4) (2019), pp. 26-30 CrossRefGoogle Scholar [7] O.F. El-Gayar The use of information technology in aquaculture management Aquacult. Econ. Manag., 1 (1–2) (1997), pp. 109-128 CrossRefView in ScopusGoogle Scholar [8] P.G. Lee Process control and artificial intelligence software for aquaculture Aquacult. Eng., 23 (1–3) (2000), pp. 13-36 View PDFView articleView in ScopusGoogle Scholar [9] F.H. Mustafa, A.H.B.P. Bagul, S. Senoo, R. Shapawi A review of smart fish farming systems J. Aquacult. Eng. Fish. Res., 2 (4) (2016), pp. 193-200 Google Scholar [10] J.V. Beijnen, G. Yan A practical guide to using AI in aquaculture, The Fish Site (2020) https://thefishsite.com/articles/a-practical-guide-to-using-ai-in-aquaculture Google Scholar [11] C.L. Chrispin, V.V. Jothiswaran, T. Velumani, R. Jayaraman Application of artificial intelligence in fisheries and aquaculture Biot. Res. Today, 2 (6) (2020), pp. 499-502 Google Scholar [12] S.B. Kotsiantis, I.D. Zaharakis, P.E. Pintelas Machine learning: a review of classification and combining techniques Artif. Intell. Rev., 26 (3) (2006), pp. 159-190 CrossRefView in ScopusGoogle Scholar [13] M. Kubat, I. Bratko, R.S. Michalski A review of machine learning methods Mach. Learn. Data Min. (1998), pp. 3-69 Google Scholar [14] S.H. Liao, P.H. Chu, P.Y. Hsiao Data mining techniques and applications–a decade review from 2000 to 2011 Expert Syst. Appl., 39 (12) (2012), pp. 11303-11311 View PDFView articleView in ScopusGoogle Scholar [15] P.G. Lee A review of automated control systems for aquaculture and design criteria for their implementation Aquacult. Eng., 14 (3) (1995), pp. 205-227 View PDFView articleView in ScopusGoogle Scholar [16] B. Zion The use of computer vision technologies in aquaculture - a review Comput. Electron. Agric., 88 (2012), pp. 125-132 View PDFView articleView in ScopusGoogle Scholar [17] M. Saberioon, A. Gholizadeh, P. Cisar, A. Pautsina, J. Urban Application of machine vision systems in aquaculture with emphasis on fish: state-of-the-art and key issues Rev. Aquacult., 9 (4) (2016), pp. 369-387 Google Scholar [18] F. Antonucci, C. Costa Precision aquaculture: a short review on engineering innovations Aquacult. Int., 28 (1) (2020), pp. 41-57 CrossRefView in ScopusGoogle Scholar [19] Z. Hu, R. Li, X. Xia, C. Yu, X. Fan, Y. Zhao A method overview in smart aquaculture Environ. Monit. Assess., 192 (8) (2020), pp. 1-25 CrossRefView in ScopusGoogle Scholar [20] D. Li, Z. Wang, S. Wu, Z. Miao, L. Du, Y. Duan Automatic recognition methods of fish feeding behavior in aquaculture: a review Aquaculture (2020), Article 735508 View PDFView articleView in ScopusGoogle Scholar [21] D. Li, Z. Miao, F. Peng, L. Wang, Y. Hao, Z. Wang, T. Chen, H. Li, Y. Zheng Automatic counting methods in aquaculture: a review J. World Aquac. Soc. (2020), pp. 1-15, 10.1111/jwas.12745 Google Scholar [22] A. Padala, S. Zilber Expert systems and their use in aquaculture Rotifer and Microalgae Culture Systems. Proceedings of an US–Asia Workshop. Oceanic Institute, Honolulu (1991), pp. 221-227 Google Scholar [23] P.G. Lee, R.N. Lea, E. Dohmann, W. Prebilsky, P.E. Turk, H. Ying, J.L. Whitson Denitrification in aquaculture systems: an example of a fuzzy logic control problem Aquacult. Eng., 23 (1–3) (2000), pp. 37-59 View PDFView articleView in ScopusGoogle Scholar [24] M.A. Romli, S. Daud, S.M. Zainol, P.L.E. Kan, Z.A. Ahmad Automatic RAS data acquisition and processing system using fog computing IEEE 13th Malaysia International Conference on Communications (2017), pp. 229-234 View in ScopusGoogle Scholar [25] X. Zhu, D. Li, D. He, J. Wang, D. Ma, F. Li A remote wireless system for water quality online monitoring in intensive fish culture Comput. Electron. Agric., 71 (2010), pp. S3-S9 View PDFView articleView in ScopusGoogle Scholar [26] H.M. Farghally, D.M. Atia, H.T. El-Madany, F.H. Fahmy Control methodologies based on geothermal recirculating aquaculture system Energy, 78 (2014), pp. 826-833 View PDFView articleView in ScopusGoogle Scholar [27] T.H. Kim, K.U. Yang, K.S. Hwang, D.J. Jang, J.G. Hur Automatic submerging and surfacing performances of model submersible fish cage system operated by air control Aquacult. Eng., 45 (2) (2011), pp. 74-86 View PDFView articleView in ScopusGoogle Scholar [28] M.A. Romli, S. Daud, R.A.A. Raof, Z.A. Ahmad, N. Mahrom Aquaponic growbed water level control using fog architecture J. Phys. Malásia (2018), p. 1018 Google Scholar [29] I. Ullah, D. Kim An optimization scheme for water pump control in smart fish farm with efficient energy consumption Processes, 6 (6) (2018), p. 65 CrossRefView in ScopusGoogle Scholar [30] F.C.G.A. Cruz Development and modelling of an aeration control system for precision aquaculture Doctoral Dissertation, University of Toronto (2019), pp. 1-120 Google Scholar [31] J.B. Hoy A microcomputer-based system for feed control, temperature control and temperature recording in an experimental fish hatchery Comput. Electron. Agric., 1 (1) (1985), pp. 105-109 View PDFView articleView in ScopusGoogle Scholar [32] Y. Atoum, S. Srivastava, X. Liu Automatic feeding control for dense aquaculture fish tanks IEEE Signal Process Lett., 22 (8) (2015), pp. 1089-1093 View in ScopusGoogle Scholar [33] M.A.M. Razman, G.A. Susto, A. Cenedese, A.P.A. Majeed, R.M. Musa, A.S.A. Ghani, F.A. Adnan, K.M. Ismail, Z. Taha, Y. Mukai Hunger classification of Lates calcarifer by means of an automated feeder and image processing Comput. Electron. Agric., 163 (2019), Article 104883 View PDFView articleView in ScopusGoogle Scholar [34] T.H. Wu, Y.I. Huang, J.M. Chen Development of an adaptive neural-based fuzzy inference system for feeding decision-making assessment in silver perch (Bidyanus bidyanus) Culture Aquacult. Eng., 66 (2015), pp. 41-51 View PDFView articleView in ScopusGoogle Scholar [35] S. Zhao, W. Ding, S. Zhao, J. Gu Adaptive neural fuzzy inference system for feeding decision-making of grass carp (Ctenopharyngodon idellus) in outdoor intensive culturing ponds Aquaculture, 498 (2019), pp. 28-36 View PDFView articleGoogle Scholar [36] W. Cai, N.T. Dunford, N. Wang, S. Zhu, Y. Wan, Y. Zhu Development of an automated algae growth system American Society of Agricultural and Biological Engineers Conference Proceedings, Missouri (2013), p. 1 Google Scholar [37] W. Hassan, M. Føre, J.B. Ulvund, J.A. Alfredsen Internet of Fish: integration of acoustic telemetry with LPWAN for efficient real-time monitoring of fish in marine farms Comput. Electron. Agric., 163 (2019), Article 104850 View PDFView articleView in ScopusGoogle Scholar [38] A. Whitsell, J.L. Whitson, P.G. Lee A machine vision system for aquaculture: real-time identification of individual animals and estimation of animal activity Advances in Aquacultural Engineering. Northeast Regional Agricultural Engineering Service, 105, Comell, NY, NRAES (1997), pp. 112-128 Google Scholar [39] T. Ochiai, Y. Suehiro, K. Nishinari, T. Kubo, H. Takeuchi A new data-mining method to search for behavioral properties that induce alignment and their involvement in social learning in medaka fish (Oryzias latipes) PLoS One, 8 (9) (2013), p. e71685 CrossRefGoogle Scholar [40] M.A. Ferrer, J.A. Calduch-Giner, M. Díaz, J. Sosa, E. Rosell-Moll, J.S. Abril, G.S. Sosa, T.B. Delgado, C. Carmona, J.A. Martos-Sitcha, E. Cabruja, J.M. Afonso, A. Vega, M. Lozano, J.A. Montiel-Nelson, J. Perez-Sanchez From operculum and body tail movements to different coupling of physical activity and respiratory frequency in farmed gilthead sea bream and European sea bass. Insights on aquaculture biosensing Comput. Electron. Agricult., 175 (2020), Article 105531 View PDFView articleView in ScopusGoogle Scholar [41] A. Rohani, M. Taki, G. Bahrami Application of artificial intelligence for separation of live and dead rainbow trout fish eggs Artif. Intell. Agricult., 1 (2019), pp. 27-34 View PDFView articleCrossRefGoogle Scholar [42] M. Saberioon, P. Císař, L. Labbé, P. Souček, P. Pelissier, T. Kerneis Comparative performance analysis of support vector machine, random forest, logistic regression and k-nearest neighbours in rainbow trout (Oncorhynchus mykiss) classification using image-based features Sensors, 18 (4) (2018), p. 1027 CrossRefView in ScopusGoogle Scholar [43] Y. Liu, Q. Zhang, L. Song, Y. Chen Attention-based recurrent neural networks for accurate short-term and long-term dissolved oxygen prediction Comput. Electron. Agric., 165 (2019), Article 104964 View PDFView articleView in ScopusGoogle Scholar [44] D.S. Simbeye, S.F. Yang Water quality monitoring and control for aquaculture based on wireless sensor networks J. Netw., 9 (4) (2014), p. 840 View in ScopusGoogle Scholar [45] C. Zhou, D. Xu, K. Lin, C. Sun, X. Yang Intelligent feeding control methods in aquaculture with an emphasis on fish: a review Rev. Aquacult., 10 (4) (2018), pp. 975-993 CrossRefView in ScopusGoogle Scholar [46] D. An, J. Hao, Y. Wei, Y. Wang, X. Yu Application of computer vision in fish intelligent feeding system - a review Aquac. Res. (2020), 10.1111/are.14907 Google Scholar [47] P. Bulcock, J. Bostock, K. Jauncey, M. Beveridge, T. Telfer The evolution of aquaculture feed supply systems Eurofish (2) (2001), pp. 74-76 Google Scholar [48] M. Foster, R. Petrell, M.R. Ito, R. Ward Detection and counting of uneaten food pellets in a sea cage using image analysis Aquacult. Eng., 14 (3) (1995), pp. 251-269 View PDFView articleCrossRefView in ScopusGoogle Scholar [49] K.F. Cubitt, H.T. Williams, D. Rowsell, W.J. McFarlane, R.G. Gosine, K.G. Butterworth, R.S. McKinley Development of an intelligent reasoning system to distinguish hunger states in Rainbow trout (Oncorhynchus mykiss) Comput. Electron. Agric., 62 (1) (2008), pp. 29-34 View PDFView articleView in ScopusGoogle Scholar [50] C. Zhou, D. Xu, L. Chen, S. Zhang, C. Sun, X. Yang, Y. Wang Evaluation of fish feeding intensity in aquaculture using a convolutional neural network and machine vision Aquaculture, 507 (2019), pp. 457-465 View PDFView articleView in ScopusGoogle Scholar [51] J. Zhao, W.J. Bao, F.D. Zhang, Z.Y. Ye, Y. Liu, M.W. Shen, S.M. Zhu Assessing appetite of the swimming fish based on spontaneous collective behaviors in a recirculating aquaculture system Aquacult. Eng., 78 (2017), pp. 196-204 View PDFView articleGoogle Scholar [52] L. Fan, Y. Liu Automate fry counting using computer vision and multi-class least squares support vector machine Aquaculture, 380 (2013), pp. 91-98 View PDFView articleView in ScopusGoogle Scholar [53] S. Marini, E. Fanelli, V. Sbragaglia, E. Azzurro, J.D.R. Fernandez, J. Aguzzi Tracking fish abundance by underwater image recognition Sci. Rep., 8 (1) (2018), pp. 1-12 CrossRefGoogle Scholar [54] S. Shahrestani, H. Bi, V. Lyubchich, K.M. Boswell Detecting a nearshore fish parade using the adaptive resolution imaging sonar (ARIS): an automated procedure for data analysis Fish. Res., 191 (2017), pp. 190-199 View PDFView articleView in ScopusGoogle Scholar [55] D. Li, Y. Hao, Y. Duan Nonintrusive methods for biomass estimation in aquaculture with emphasis on fish: a review Rev. Aquacult., 12 (3) (2020), pp. 1390-1411 CrossRefView in ScopusGoogle Scholar [56] J.R.M. Dios, C. Serna, A. Ollero Computer vision and robotics techniques in fish farms Robotica, 21 (3) (2003), p. 233 Google Scholar [57] F. Odone, E. Trucco, A. Verri A trainable system for grading fish from images Appl. Artif. Intell., 15 (8) (2001), pp. 735-745 View in ScopusGoogle Scholar [58] C. Costa, A. Loy, S. Cataudella, D. Davis, M. Scardi Extracting fish size using dual underwater cameras Aquacult. Eng., 35 (3) (2006), pp. 218-227 View PDFView articleView in ScopusGoogle Scholar [59] S. Torisawa, M. Kadota, K. Komeyama, K. Suzuki, T. Takagi A digital stereo-video camera system for three-dimensional monitoring of free-swimming Pacific bluefin tuna, Thunnus orientalis, cultured in a net cage Aquat. Living Resour., 24 (2) (2011), pp. 107-112 CrossRefView in ScopusGoogle Scholar [60] A.F.A. Fernandes, E.M. Turra, É.R. de Alvarenga, T.L. Passafaro, F.B. Lopes, G.F.O. Alves, V. Singh, G.J.M. Rosa Deep Learning image segmentation for extraction of fish body measurements and prediction of body weight and carcass traits in Nile tilapia Comput. Electron. Agric., 170 (2020), Article 105274 View PDFView articleView in ScopusGoogle Scholar [61] N. Colihueque, M. Parraguez, F.J. Estay, N.F. Diaz Skin color characterization in rainbow trout by use of computer-based image analysis N. Am. J. Aquac., 73 (3) (2011), pp. 249-258 CrossRefView in ScopusGoogle Scholar [62] J. Urban, D. Štys, M. Sergejevová, J. Masojídek Expertomica Fishgui: comparison of fish skin colour J. Appl. Ichthyol., 29 (1) (2013), pp. 172-180 CrossRefView in ScopusGoogle Scholar [63] B. Zion, V. Alchanatis, V. Ostrovsky, A. Barki, I. Karplus Classification of guppies’ (Poecilia reticulata) gender by computer vision Aquacult. Eng., 38 (2) (2008), pp. 97-104 View PDFView articleView in ScopusGoogle Scholar [64] S. Kato, T. Nakagawa, M. Ohkawa, K. Muramoto, O. Oyama, A. Watanabe, H. Nakashima, T. Nemoto, K. Sugitani A computer image processing system for quantification of zebrafish behavior J. Neurosci. Method, 134 (1) (2004), pp. 1-7 View PDFView articleView in ScopusGoogle Scholar [65] J. Xu, Y. Liu, S. Cui, X. Miao Behavioral responses of tilapia (Oreochromis niloticus) to acute fluctuations in dissolved oxygen levels as monitored by computer vision Aquacult. Eng., 35 (3) (2006), pp. 207-217 View PDFView articleView in ScopusGoogle Scholar [66] T.H. Pinkiewicz, G.J. Purser, R.N. Williams A computer vision system to analyse the swimming behaviour of farmed fish in commercial aquaculture facilities: a case study using cage-held Atlantic salmon Aquacult. Eng., 45 (1) (2011), pp. 20-27 View PDFView articleView in ScopusGoogle Scholar [67] V.M. Papadakis, I.E. Papadakis, F. Lamprianidou, A. Glaropoulos, M. Kentouri A computer-vision system and methodology for the analysis of fish behavior Aquacult. Eng., 46 (2012), pp. 53-59 View PDFView articleView in ScopusGoogle Scholar [68] A. Pérez-Escudero, J. Vicente-Page, R.C. Hinz, S. Arganda, G.G. De Polavieja idTracker: tracking individuals in a group by automatic identification of unmarked animals Nat. Method, 11 (7) (2014), pp. 743-748 CrossRefView in ScopusGoogle Scholar [69] F. Romero-Ferrero, M.G. Bergomi, R.C. Hinz, F.J. Heras, G.G. de Polavieja Idtracker. ai: tracking all individuals in small or large collectives of unmarked animals Nat. Method, 16 (2) (2019), pp. 179-182 CrossRefView in ScopusGoogle Scholar [70] K. Terayama, K. Shin, K. Mizuno, K. Tsuda Integration of sonar and optical camera images using deep neural network for fish monitoring Aquacult. Eng., 86 (2019), Article 102000 View PDFView articleView in ScopusGoogle Scholar [71] X. Xia, Y. Zhao, Z. Hu, Z. Wang, C. Yu, Y. Bai Fish behavior tracking algorithm based on Multi-Domain Deep Convolutional Neural Network Proceedings of the 4th International Conference on Multimedia Systems and Signal Processing (2019), pp. 73-78 CrossRefView in ScopusGoogle Scholar [72] M. Alagappan, M. Kumaran Application of expert systems in fisheries sector - a review Res. J. Anim. Vet. Fish. Sci., 1 (8) (2013), pp. 19-30 Google Scholar [73] D. Li, Z. Fu, Y. Duan Fish-Expert: a web-based expert system for fish disease diagnosis Expert Syst. Appl., 23 (3) (2002), pp. 311-320 View PDFView articleView in ScopusGoogle Scholar [74] F. Bousquet, C. Cambier, P. Morand Distributed artificial intelligence and object-oriented modelling of a fishery Math. Comput. Model., 20 (8) (1994), pp. 97-107 View PDFView articleGoogle Scholar [75] F. Plaza, R. Salas, E. Yáñez Identifying ecosystem patterns from time series of anchovy (Engraulis ringens) and sardine (Sardinops sagax) landings in northern Chile J. Stat. Comput. Simul., 88 (10) (2018), pp. 1863-1881 CrossRefView in ScopusGoogle Scholar [76] F. Su, C. Zhou, V. Lyne, Y. Du, W. Shi A data-mining approach to determine the spatio-temporal relationship between environmental factors and fish distribution Ecol. Model., 174 (4) (2004), pp. 421-431 View PDFView articleView in ScopusGoogle Scholar [77] Y.C.E. Yang, X. Cai, E.E. Herricks Identification of hydrologic indicators related to fish diversity and abundance: a data mining approach for fish community analysis Water Resour. Res., 44 (4) (2008) Google Scholar [78] W.P. Tsai, S.P. Huang, S.T. Cheng, K.T. Shao, F.J. Chang A data-mining framework for exploring the multi-relation between fish species and water quality through self-organizing map Sci. Total Environ., 579 (2017), pp. 474-483 View PDFView articleView in ScopusGoogle Scholar [79] D. Pugazhendi Apriori algorithm on marine fisheries biological data Int. J. Comput. Sci. Eng. Technol., 4 (12) (2013), pp. 1409-1411 Google Scholar [80] B.R. Moore, J. Maclaren, C. Peat, M. Anjomrouz, P.L. Horn, S. Hoyle Feasibility of automating otolith ageing using CT scanning and machine learning N. Z. Fish. Assessm. Rep. (2019), p. 58 View in ScopusGoogle Scholar [81] K. Rungruangsak-Torrissen, P. Manoonpong Neural computational model GrowthEstimate: a model for studying living resources through digestive efficiency PLoS One, 14 (8) (2019), Article e0216030 CrossRefView in ScopusGoogle Scholar [82] J. Xu, T.L. Wickramarathne, N.V. Chawla, E.K. Grey, K. Steinhaeuser, R.P. Keller, J.M. Drake, D.M. Lodge Improving management of aquatic invasions by integrating shipping network, ecological, and environmental data: data mining for social good Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (2014), pp. 1699-1708 CrossRefView in ScopusGoogle Scholar [83] D. Fitrianah, A.N. Hidayanto, J.L. Gaol, H. Fahmi, A.M. Arymurthy A spatio-temporal data-mining approach for identification of potential fishing zones based on oceanographic characteristics in the Eastern Indian ocean IEEE J. Select. Top. Appl. Earth Obser. Remote Sens., 9 (8) (2015), pp. 3720-3728 Google Scholar [84] A.N. Hidayanto, H. Fahmi, D. Fitrianah, A.M. Arymurthy Oceanographic features selection to predict the tuna potential fishing zones using SFFS method Int. Math. Forum, 11 (24) (2016), pp. 1157-1166 CrossRefGoogle Scholar [85] D. Fitrianah, H. Fahmi, A.N. Hidayanto, A.M. Arymurthy A data mining based approach for determining the potential fishing zones Int. J. Inform. Educ. Technol., 6 (3) (2016), p. 187 CrossRefGoogle Scholar [86] V. Allken, N.O. Handegard, S. Rosen, T. Schreyeck, T. Mahiout, K. Malde Fish species identification using a convolutional neural network trained on synthetic data ICES J. Mar. Sci., 76 (1) (2019), pp. 342-349 CrossRefView in ScopusGoogle Scholar [87] S. Rosen, J.C. Holst DeepVision in-trawl imaging: sampling the water column in four dimensions Fish. Res., 148 (2013), pp. 64-73 View PDFView articleView in ScopusGoogle Scholar [88] J. Simon, D. Kopp, P. Larnaud, J.P. Vacherot, F. Morandeau, G. Lavialle, M. Morfin Using automated video analysis to study fish escapement throughescape panels in active fishing gears: application to the effect of net colour Mar. Policy (2020), Article 103785 View PDFView articleView in ScopusGoogle Scholar [89] R. Garcia, R. Prados, J. Quintana, A. Tempelaar, N. Gracias, S. Rosen, H. Vågstøl, K. Løvall Automatic segmentation of fish using deep learning with application to fish size measurement ICES J. Mar. Sci., 77 (4) (2020), pp. 1354-1366 CrossRefView in ScopusGoogle Scholar [90] Y. He, F. Su, Y. Du, R. Xiao Web-based spatiotemporal visualization of marine environment data Chin. J. Oceanol. Limnol., 28 (5) (2010), pp. 1086-1094 CrossRefView in ScopusGoogle Scholar [91] T. Su, Z. Cao, Z. Lv, C. Liu, X. Li Multi-dimensional visualization of large-scale marine hydrological environmental data Adv. Eng. Softw., 95 (2016), pp. 7-15 View PDFView articleView in ScopusGoogle Scholar [92] G.K. Sylaios, N. Gitsakis, T. Koutroumanidis, V.A. Tsihrintzis CHLfuzzy: a spreadsheet tool for the fuzzy modeling of chlorophyll concentrations in coastal lagoons Hydrobiologia, 610 (1) (2008), pp. 99-112 CrossRefView in ScopusGoogle Scholar [93] H. Lu, H. Li, T. Liu, Y. Fan, Y. Yuan, M. Xie, X. Qian Simulating heavy metal concentrations in an aquatic environment using artificial intelligence models and physicochemical indexes Sci. Total Environ., 694 (2019), Article 133591 View PDFView articleView in ScopusGoogle Scholar [94] B. Wei, N. Sugiura, T. Maekawa Use of artificial neural network in the prediction of algal blooms Water Res., 35 (8) (2001), pp. 2022-2028 View PDFView articleView in ScopusGoogle Scholar [95] T. Sadhu, I. Banerjee, S.K. Lahiri, J. Chakrabarty Modeling and optimization of cooking process parameters to improve the nutritional profile of fried fish by robust hybrid artificial intelligence approach J. Food Process Eng., 43 (9) (2020), p. e13478 View in ScopusGoogle Scholar [96] E. Bjørlykhaug, O. Egeland Vision system for quality assessment of robotic cleaning of fish processing plants using CNN IEEE Access, 7 (2019), pp. 71675-71685 CrossRefView in ScopusGoogle Scholar [97] A. Issac, A. Srivastava, A. Srivastava, M.K. Dutta An automated computer vision based preliminary study for the identification of a heavy metal (Hg) exposed fish, Channa punctatus Comput. Biol. Med., 111 (2019), Article 103326 View PDFView articleView in ScopusGoogle Scholar [98] T. Hu, X. Zhang, Y. Hou, W. Mu, Z. Fu A hybrid model for forecasting aquatic products short-term price integrated wavelet neural network with genetic algorithm International Conference on Natural Computation, Springer, Berlin (2005), pp. 352-360 CrossRefView in ScopusGoogle Scholar [99] S.K. Hamid, W.A. Teniwut, R.M.K. Teniwut, M. Renhoran Outliers detection on fisheries commodity transaction from local market in Tual city based on the x-means clustering Journal of Physics: Conference Series, 1424, IOP Publishing (2019), Article 012017 View in ScopusGoogle Scholar [100] W.A. Teniwut, F. Pentury, Y.A. Ngamel Forecasting the welfare of fishermen and aquaculture farmers in Indonesia: data mining approach Journal of Physics: Conference Series, 1175, IOP Publishing (2019), Article 012066 Google Scholar [101] K. Enomoto, S. Ishikawa, M. Hori, H. Sitha, S.L. Song, N. Thuok, H. Kurokura Data mining and stock assessment of fisheries resources in Tonle Sap Lake Cambodia Fish. Sci., 77 (5) (2011), pp. 713-722 CrossRefView in ScopusGoogle Scholar [102] D.G. Pazhayamadom, C.J. Kelly, E. Rogan, E.A. Codling Decision Interval Cumulative Sum Harvest Control Rules (DI-CUSUM-HCR) for managing fisheries with limited historical information Fish. Res., 171 (2015), pp. 154-169 View PDFView articleView in ScopusGoogle Scholar [103] D. Bradley, M. Merrifield, K.M. Miller, S. Lomonico, J.R. Wilson, M.G. Gleason Opportunities to improve fisheries management through innovative technology and advanced data systems Fish Fish., 20 (3) (2019), pp. 564-583 CrossRefView in ScopusGoogle Scholar [104] J.P. Kritzer Influences of at-sea fishery monitoring on science, management, and fleet dynamics Aquacult. Fish., 5 (3) (2020), pp. 107-112 View PDFView articleView in ScopusGoogle Scholar [105] C. Spampinato, D. Giordano, R. Di Salvo, Y.H.J. Chen-Burger, R.B. Fisher, G. Nadarajan Automatic fish classification for underwater species behavior understanding Proceedings of the first ACM international workshop on Analysis and retrieval of tracked events and motion in imagery streams (2010), pp. 45-50 CrossRefView in ScopusGoogle Scholar [106] S. Blowers, J. Evans, K. McNally Automated identification of fish and other aquatic life in underwater video Scott. Marine Freshw. Sci., 11 (18) (2020) Google Scholar [107] M. Scardi Advances in neural network modeling of phytoplankton primary production Ecol. Model., 146 (1–3) (2001), pp. 33-45 View PDFView articleView in ScopusGoogle Scholar [108] F. Recknagel, J. Bobbin, P. Whigham, H. Wilson Comparative application of artificial neural networks and genetic algorithms for multivariate time-series modelling of algal blooms in freshwater lakes J. Hydroinf., 4 (2) (2002), pp. 125-133 CrossRefView in ScopusGoogle Scholar [109] M.C. Domingo An overview of the internet of underwater things J. Netw. Comput. Appl., 35 (6) (2012), pp. 1879-1890 View PDFView articleView in ScopusGoogle Scholar [110] E. Bjørlykhaug, O. Egeland Mechanical design optimization of a 6dof serial manipulator using genetic algorithm IEEE Access, 6 (2018), pp. 59087-59095 CrossRefView in ScopusGoogle Scholar [111] J.R. Mathiassen, E. Misimi, M. Bondø, E. Veliyulin, S.O. Østvik Trends in application of imaging technologies to inspection of fish and fish products Trends Food Sci. Technol., 22 (6) (2011), pp. 257-275 View PDFView articleView in ScopusGoogle Scholar [112] M. Dowlati, M. de la Guardia, S.S. Mohtasebi Application of machine-vision techniques to fish-quality assessment Trends Anal. Chem., 40 (2012), pp. 168-179 View PDFView articleView in ScopusGoogle Scholar [113] M.K. Dutta, A. Issac, N. Minhas, B. Sarkar Image processing based method to assess fish quality and freshness J. Food Eng., 177 (2016), pp. 50-58 View PDFView articleView in ScopusGoogle Scholar [114] M.R. Hasan, M.M. Dey, C.R. Engle Forecasting monthly catfish (Ictalurus punctatus) pond bank and feed prices Aquacult. Econ. Manag., 23 (1) (2019), pp. 86-110 CrossRefView in ScopusGoogle Scholar [115] D. Bloznelis Short-term salmon price forecasting J. Forecast., 37 (2) (2018), pp. 151-169 CrossRefView in ScopusGoogle Scholar Cited by (46) Classification of set-net fish catch volumes in Iwate Prefecture, Japan using machine learning with water temperature and current distribution images at migration depth 2024, Regional Studies in Marine Science Show abstract Exploring opportunities of Artificial Intelligence in aquaculture to meet increasing food demand 2024, Food Chemistry: X Show abstract Comparison of model selection and data bias on the prediction performance of purpleback flying squid (Sthenoteuthis oualaniensis) fishing ground in the Northwest Indian Ocean 2024, Ecological Indicators Show abstract A GIS-based maritime supply chain network design of distant-water fisheries 2023, Computers and Electronics in Agriculture Show abstract Towards a holistic digital twin solution for real-time monitoring of aquaculture net cage systems 2023, Marine Structures Show abstract Implementation of information and communication technologies to increase sustainable productivity in freshwater finfish aquaculture – A review 2023, Journal of Cleaner Production Show abstract View all citing articles on Scopus © 2022 The Author(s). Published by Elsevier B.V. Recommended articles Machine learning approach to investigate the influence of water quality on aquatic livestock in freshwater ponds Biosystems Engineering, Volume 208, 2021, pp. 164-175 Mashud Rana, …, Bruno Pais View PDF Machine learning to detect bycatch risk: Novel application to echosounder buoys data in tuna purse seine fisheries Biological Conservation, Volume 255, 2021, Article 109004 Laura Mannocci, …, Manuela Capello View PDF Real-time nondestructive fish behavior detecting in mixed polyculture system using deep-learning and low-cost devices Expert Systems with Applications, Volume 178, 2021, Article 115051 Jun Hu, …, Wenxuan Chen View PDF Show 3 more articles Article Metrics Citations Citation Indexes: 33 Policy Citations: 3 Captures Readers: 240 View details About ScienceDirect Remote access Shopping cart Advertise Contact and support Terms and conditions Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply."

</subsection_point_Point 3>

<previous_sections>

A systematic review of automated systems for real-time irrigation management

1. INTRODUCTION
The challenge of feeding a growing population with finite resources is becoming increasingly pressing. By 2050, the world population is expected to reach 9.7 billion, necessitating a 70% increase in food production (Falkenmark and Rockstrom, 2009). Irrigation plays a crucial role in enhancing crop yields and agricultural productivity to meet this growing demand. Studies have shown that irrigation can significantly increase crop water productivity, contributing to increased food production (Ali and Talukder, 2008; Playan and Mateos, 2005). However, water scarcity poses a significant challenge, with many regions facing water deficits and the need for improved water management practices (Falkenmark and Rockstrom, 2009). Optimizing irrigation schedules and doses based on crop requirements and environmental conditions is essential for maximizing yield and quality while minimizing water use (Zhang et al., 2024). The necessity of scalable water-efficient practices for increasing food demand cannot be overstated. Techniques such as regulated deficit irrigation, magnetically treated water, and the use of drought-tolerant crops like sorghum have shown promise in improving water productivity and ensuring food security (Mehmood et al., 2023; Putti et al., 2023; Hadebe et al., 2016). As the global food challenge intensifies, it is imperative to critically evaluate the current state and future potential of irrigation management systems to guide research, innovation, and implementation efforts towards fully autonomous, scalable solutions.

Despite the importance of irrigation in addressing the global food challenge, traditional irrigation management techniques, such as manual scheduling and timer-based systems, have significant limitations. These methods are often labor-intensive, inefficient, and less adaptable to changing conditions (Savin et al., 2023). Manual and timer-based scheduling can lead to high operational costs and inefficient water use (Raghavendra, Han, and Shin, 2023). The reliance on manual intervention and predetermined schedules limits their adaptability to changing environmental conditions, crop water requirements, and soil moisture levels (Kaptein et al., 2019). Sensor-based irrigation systems offer an alternative, enabling real-time adjustments based on soil water status measurements (Kaptein et al., 2019). However, the adoption of these systems in commercial settings has been limited, often requiring extensive input from researchers (Kim et al., 2014; Lea-Cox et al., 2018; Ristvey et al., 2018). The limitations of traditional irrigation management techniques highlight the need for scalable, automated solutions for greater efficiency in irrigation management. Automated systems that collect real-time data, analyze it, and make autonomous irrigation decisions can lead to improved water use efficiency and increased crop productivity (Champness et al., 2023; Wu et al., 2022). To fully understand the potential of automated systems, it is necessary to examine the automation of each part of the irrigation management pipeline and analyze the effectiveness and efficiency of integrated end-to-end solutions.

The emergence of smart irrigation management and IoT marks a significant shift from historical irrigation practices. Modern approaches rely on vast data and analysis algorithms, leveraging technologies such as remote sensing, sensor networks, weather data, and computational algorithms (Atanasov, 2023; Bellvert et al., 2023; Kumar et al., 2023). IoT plays a vital role in collecting vast amounts of data through sensors, data transmission, and tailored networks, enabling real-time monitoring and control of irrigation systems (Liakos, 2023; Zuckerman et al., 2024). These advancements in data collection and analysis have the potential to revolutionize irrigation management, allowing for more precise and efficient water use. However, challenges such as processing diverse data sources, data integration, and lack of integrated data analysis hamper the full benefit of IoT in irrigation management (Dave et al., 2023). The current fragmented approach in smart irrigation, focusing on individual components rather than the entire system, limits the potential for fully autonomous, real-time end-to-end irrigation management (Togneri et al., 2021). To address these challenges and fully realize the potential of smart irrigation management, there is a need for automating and integrating each section of the irrigation management pipeline, from sensor/weather data collection and transmission to processing, analysis, decision-making, and automated action (McKinion and Lemmon, 1985). This integration requires a thorough investigation of the role of interoperability and standardization in enabling seamless communication and compatibility between components within the automated irrigation management pipeline.

Machine learning (ML) plays a significant role in processing vast data, predicting plant stress, modeling climate effects, and optimizing irrigation in smart irrigation management systems. ML algorithms can analyze data collected from sensors and weather stations to determine optimal irrigation schedules (Vianny et al., 2022). However, the potential of ML is often constrained by manual steps, such as data interpretation, decision-making on irrigation timing and volume, and system adjustments. Automating ML integration to allow direct action from insights to irrigation execution, removing bottlenecks and achieving real-time adaptability, is crucial for fully autonomous irrigation management (Barzallo-Bertot et al., 2022). By integrating ML into automated systems, the irrigation management pipeline can become more seamless and efficient, enabling real-time decision-making and action based on data-driven insights. To achieve this level of automation and integration, it is essential to identify gaps and propose solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.

To achieve seamless integration across the automated irrigation management system, interoperability and standardization are critical. Interoperability allows different system components, such as sensors, actuators, and software, to communicate and exchange data effectively, while standardization ensures that data is represented in a consistent format (Santos et al., 2020). Standardized protocols and data formats are essential for achieving seamless integration and ensuring compatibility between components in real-time irrigation management systems (Robles et al., 2022; Hatzivasilis et al., 2018). Existing and emerging standards, such as OGC SensorThings API and ISO 11783, have applicability to real-time irrigation management systems (Hazra et al., 2021). However, challenges such as data quality, scalability, reliability, and security need to be addressed to fully realize the potential of interoperability and standardization in automated irrigation management systems (Hazra et al., 2021). Addressing these challenges is crucial for enabling the seamless integration of components within the automated irrigation management pipeline, which is essential for achieving fully autonomous, scalable irrigation management. A comprehensive evaluation of the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline is necessary to guide future research and implementation efforts.
The primary objective of this systematic review is to critically evaluate the current state and future potential of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies for enhancing agricultural water use efficiency and crop productivity.
Specific objectives include:
•	Examining the automation of each part of the irrigation management pipeline and the seamless integration of each section in the context of irrigation scheduling and management.
•	Analyzing the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
•	Investigating the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline.
•	Identifying gaps and proposing solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.
By addressing these objectives, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge.

2. REVIEW METHODOLOGY
•	Question-driven framework to guide the literature review of real-time, autonomous irrigation management systems
•	Key research questions posed, each with the motivation behind investigating them and a starting hypothesis to evaluate against the examined literature
•	Table presenting the major objectives, specific objectives, questions, motivations, and hypotheses
3. DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING
3.1. Irrigation management data
The success of automated irrigation management systems relies heavily on the collection, transmission, and analysis of various types of data. The most applicable data types for irrigation management include soil moisture, canopy temperature, weather data, and plant physiological parameters (Farooq et al., 2019; Li et al., 2019; Olivier et al., 2021; Evett et al., 2020). These data are typically collected from a range of sources, including in-field sensors, remote sensing platforms, weather stations, and manual measurements (Li et al., 2019; Karimi et al., 2018).
Soil moisture data is arguably the most critical type of data for irrigation management, as it directly reflects the water available to plants and can be used to determine the optimal timing and amount of irrigation (Olivier et al., 2021; Intrigliolo & Castel, 2006). Soil moisture sensors, such as tensiometers, capacitance probes, and time-domain reflectometry (TDR) sensors, can provide real-time measurements of soil water content at various depths (Farooq et al., 2019). These sensors can be deployed in a network configuration to capture spatial variability in soil moisture across a field (Karimi et al., 2018).
Canopy temperature data is another valuable type of data for irrigation management, as it can be used to assess plant water stress and adjust irrigation accordingly (Evett et al., 2020). Infrared thermometers and thermal cameras can be used to measure canopy temperature, which is influenced by factors such as air temperature, humidity, wind speed, and plant water status (Li et al., 2019). When plants experience water stress, they tend to close their stomata to reduce water loss, leading to an increase in canopy temperature (Evett et al., 2020). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
Weather data, including temperature, humidity, precipitation, wind speed, and solar radiation, are essential for predicting crop water requirements and scheduling irrigation events (Akilan & Baalamurugan, 2024). Weather stations equipped with various sensors can provide real-time measurements of these parameters, which can be used as inputs for crop water requirement models, such as the FAO-56 Penman-Monteith equation (Li et al., 2019). These models estimate crop evapotranspiration (ET) based on weather data and crop-specific coefficients, allowing for the calculation of irrigation requirements (Intrigliolo & Castel, 2006). By integrating weather data into automated irrigation systems, irrigation schedules can be dynamically adjusted based on changing environmental conditions, ensuring that crops receive the optimal amount of water at the right time (Akilan & Baalamurugan, 2024).
When collecting and utilizing these data types, several considerations must be taken into account, including the volume, frequency, format, and source of the data (Farooq et al., 2019). The volume of data generated by automated irrigation systems can be substantial, especially when high-resolution sensors are deployed at a large scale (Bastidas Pacheco et al., 2022). This necessitates the use of efficient data storage, processing, and transmission technologies to handle the data load (Farooq et al., 2019). The frequency of data collection is another important consideration, as it directly impacts the temporal resolution of the data and the ability to detect rapid changes in plant water status or environmental conditions (Bastidas Pacheco et al., 2022). Bastidas Pacheco et al. (2022) demonstrated that collecting full pulse resolution data from water meters provides more accurate estimates of event occurrence, timing, and features compared to aggregated temporal resolutions, highlighting the importance of selecting appropriate data collection frequencies to ensure the quality and usefulness of the data for irrigation management.
The format of the data is also crucial, as it determines the compatibility and interoperability of the data with various analysis tools and platforms (Farooq et al., 2019). Standardized data formats, such as JSON, XML, or CSV, can facilitate data exchange and integration between different components of the automated irrigation system (Zhang et al., 2023). The source of the data is another important consideration, as it can impact the reliability, accuracy, and spatial coverage of the data (Farooq et al., 2019). For example, in-field sensors provide highly localized measurements, while remote sensing platforms, such as satellites or drones, can provide data at larger spatial scales (Li et al., 2019). By combining data from multiple sources, automated irrigation systems can achieve a more comprehensive understanding of crop water requirements and optimize irrigation management accordingly (Farooq et al., 2019).
Data quality, accuracy, and reliability are paramount in irrigation management, as they directly impact the effectiveness of decision-making processes and the efficiency of water use (Gupta et al., 2020). Inaccurate or unreliable data can lead to suboptimal irrigation decisions, resulting in crop stress, yield losses, or wasted water resources (Ramli & Jabbar, 2022). Gupta et al. (2020) emphasized the critical importance of data security and privacy in smart farming systems, as the leakage of sensitive agricultural data can cause severe economic losses to farmers and compromise the integrity of the automated irrigation system. The authors also highlighted the need for robust authentication and secure communication protocols to prevent unauthorized access to smart farming systems and protect data in transit (Gupta et al., 2020).
Ramli and Jabbar (2022) addressed the challenges associated with implementing real-time, automated irrigation systems, including data quality, scalability, reliability, and security. They proposed solutions and best practices based on the analysis of case studies and real-world implementations, such as the use of redundant sensors, data validation techniques, and secure communication protocols (Ramli & Jabbar, 2022). The authors also emphasized the importance of regular maintenance and calibration of sensors to ensure the accuracy and reliability of the collected data (Ramli & Jabbar, 2022).
Researchers have investigated the use of data compression, aggregation, and filtering techniques to reduce bandwidth requirements and improve transmission efficiency in automated irrigation systems (Karim et al., 2023; Rady et al., 2020; Cui, 2023). Karim et al. (2023) explored the effectiveness of various data compression techniques, such as lossless and lossy compression algorithms, in reducing the size of data packets transmitted over wireless networks. The authors found that lossless compression techniques, such as Huffman coding and Lempel-Ziv-Welch (LZW), can significantly reduce data size without compromising data quality, while lossy compression techniques, such as JPEG and MP3, can further reduce data size by introducing acceptable levels of distortion (Karim et al., 2023).
Rady et al. (2020) developed a novel data compression algorithm specifically designed for irrigation data, which achieved significant compression ratios without compromising data quality. The authors demonstrated that their algorithm could reduce the amount of data transmitted over wireless networks, thereby improving the efficiency of the irrigation system and reducing costs (Rady et al., 2020). Cui (2023) investigated the use of data aggregation and filtering techniques to reduce the number of transmissions and save bandwidth in automated irrigation systems. The author proposed a data aggregation scheme that combines multiple sensor readings into a single value, such as the average soil moisture over a specified time interval, to reduce the frequency of data transmissions (Cui, 2023). Additionally, the author explored the use of data filtering techniques, such as Kalman filters and particle filters, to remove noise and outliers from sensor data, improving the accuracy and reliability of the transmitted information (Cui, 2023).
Data standardization and harmonization are crucial for facilitating seamless integration and interoperability between the various components of automated irrigation management systems (Zhang et al., 2023; Ermoliev et al., 2022). Zhang et al. (2023) developed a novel cyberinformatics technology called iCrop, which enables the in-season monitoring of crop-specific land cover across the contiguous United States. The authors highlighted the importance of data standardization and harmonization in the context of iCrop, as it allows for the efficient distribution of crop-specific land cover information based on the findable, accessible, interoperable, and reusable (FAIR) data principle (Zhang et al., 2023). By adopting standardized data formats and protocols, such as the Open Geospatial Consortium (OGC) standards, iCrop enables the seamless integration of various data sources and facilitates the interoperability of the system with other agricultural decision support tools (Zhang et al., 2023).
Ermoliev et al. (2022) proposed a linkage methodology for linking distributed sectoral/regional optimization models in a situation where private information is not available or cannot be shared by modeling teams. The authors emphasized the need for data standardization to enable decentralized cross-sectoral coordination and analysis, as it allows for the consistent representation and exchange of data between different models and stakeholders (Ermoliev et al., 2022). By adopting standardized data formats and interfaces, the proposed linkage methodology can facilitate the integration of various optimization models and support the development of comprehensive decision support systems for sustainable resource management (Ermoliev et al., 2022).
Metadata plays a vital role in providing context and enabling better data interpretation and decision-making in automated irrigation management systems (Jahanddideh-Tehrani et al., 2021). Metadata refers to the additional information that describes the characteristics, quality, and context of the primary data, such as the sensor type, calibration parameters, measurement units, and timestamp (Jahanddideh-Tehrani et al., 2021). Jahanddideh-Tehrani et al. (2021) highlighted the importance of metadata in water resources management, as it enables decision-makers to use the data to the best of its capabilities by understanding factors such as when water data was collected and what factors might have contributed to the measurements. The authors emphasized the need for standardized metadata formats and guidelines, such as the Dublin Core Metadata Initiative (DCMI) and the ISO 19115 standard, to ensure the consistency and interoperability of metadata across different water information systems (Jahanddideh-Tehrani et al., 2021).
In the context of automated irrigation management systems, metadata can provide valuable information about the data collection process, sensor performance, and environmental conditions that can aid in data interpretation and decision-making (Cota & Mamede, 2023). For example, metadata about the sensor type and calibration parameters can help assess the accuracy and reliability of the collected data, while metadata about the weather conditions and soil properties can provide context for interpreting the data and adjusting irrigation strategies accordingly (Cota & Mamede, 2023). By incorporating metadata into the data management and analysis pipeline of automated irrigation systems, decision-makers can make more informed and context-aware decisions, leading to improved water use efficiency and crop productivity (Jahanddideh-Tehrani et al., 2021).

3.2. Edge Computing and Fog Computing
Edge computing and fog computing have emerged as transformative technologies in the realm of real-time irrigation management systems, offering significant potential for improving efficiency, scalability, and reliability (Abdel Nasser et al., 2020; Tran et al., 2019). Edge computing refers to the practice of processing data near the edge of the network, close to the source of the data, while fog computing is a decentralized computing infrastructure that extends cloud computing capabilities to the network edge (Hassija et al., 2019). These technologies bring computation and analytics closer to the data source, reducing the need for data to travel to the cloud and enabling faster processing and decision-making (Hassija et al., 2019; Zhang et al., 2020).
The potential of edge computing and fog computing in real-time irrigation management is immense. Abdel Nasser et al. (2020) proposed a two-layer system for water demand prediction using automated meters and machine learning techniques, demonstrating the potential of edge computing in improving the efficiency and scalability of irrigation management. The system collects and aggregates data from distributed smart meters in the first layer, while the second layer uses LSTM neural networks to predict water demand for different regions of households. By leveraging edge computing, the system can achieve high accuracy in predicting water demand, which is essential for efficient irrigation management (Abdel Nasser et al., 2020).
Tran et al. (2019) conducted a comprehensive review of real-time, end-to-end automated irrigation management systems, highlighting the role of fog computing in addressing data transmission challenges and enabling seamless integration across the irrigation management pipeline. The authors emphasize that real-time, end-to-end automated irrigation management systems have the potential to significantly improve water efficiency, crop yields, and reduce labor costs. However, they also identify several challenges that need to be addressed, such as data quality, scalability, reliability, and security, which can be effectively tackled by implementing fog computing architectures (Tran et al., 2019).
Edge computing offers several benefits in real-time irrigation management systems, including reduced latency, real-time decision-making, and reduced reliance on cloud connectivity (Mishra, 2020; Zhang et al., 2020). By processing data closer to the source, edge computing enables faster response times and more efficient data handling (Mishra, 2020). Mishra (2020) highlights that edge computing reduces latency by processing data closer to the source, enabling real-time decision-making and lessening reliance on cloud connectivity by shifting processing to local or edge devices.
Zhang et al. (2020) explore the application of edge computing in agricultural settings, demonstrating its potential to improve the efficiency and accuracy of irrigation systems. The authors discuss how edge computing has prospects in various agricultural applications, such as pest identification, safety traceability of agricultural products, unmanned agricultural machinery, agricultural technology promotion, and intelligent management. They also emphasize that the emergence of edge computing models, such as fog computing, cloudlet, and mobile edge computing, has transformed the management and operation of farms (Zhang et al., 2020).
Fog computing plays a crucial role in distributing processing and storage across the network, enhancing the scalability and reliability of automated irrigation systems (Premkumar & Sigappi, 2022; Singh et al., 2022). Premkumar and Sigappi (2022) evaluate the current state of automated irrigation management systems and propose a hybrid machine learning approach for predicting soil moisture and managing irrigation. Their study emphasizes the potential of fog computing in distributing processing and storage across the network, improving the efficiency and scalability of irrigation systems. The proposed hybrid machine learning approach outperforms other machine learning algorithms in predicting soil moisture, demonstrating the effectiveness of fog computing in enhancing the performance of automated irrigation systems (Premkumar & Sigappi, 2022).
Singh et al. (2022) discuss the role of fog computing in distributing processing and storage across the network, enhancing scalability and reliability in agricultural management systems. The authors argue that by implementing fog computing, these systems can achieve faster data processing and response times, improving overall efficiency and effectiveness. They also highlight that fog computing can address the challenges faced by real-time data transmission in agricultural management systems, such as latency, bandwidth limitations, and data security (Singh et al., 2022).
The integration of edge and fog computing in real-time irrigation management systems is crucial for achieving fully automated, scalable, and reliable solutions. As the demand for autonomous irrigation management grows, these technologies will play a pivotal role in enabling faster decision-making, reduced latency, improved resource utilization, and seamless integration across the irrigation management pipeline (Tran et al., 2019; Zhang et al., 2020). By bringing computation and analytics closer to the data source and distributing processing and storage across the network, edge and fog computing can significantly enhance the efficiency and effectiveness of automated irrigation systems, contributing to the overall goal of addressing the global food challenge through optimized water resource management and increased agricultural productivity (Abdel Nasser et al., 2020; Premkumar & Sigappi, 2022; Singh et al., 2022).

3.3. Automation of Data Collection
The automation of data collection is a critical component in the development of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. It enables the efficient gathering of vital information about crop health, environmental conditions, and water requirements, which is essential for enhancing agricultural water use efficiency and crop productivity. Two key aspects of automated data collection are the use of advanced sensing technologies for non-invasive plant stress detection and the implementation of wireless sensor networks and energy-efficient communication protocols for large-scale, long-term data collection.
Advanced sensing technologies, such as hyperspectral imaging and thermal sensing, have emerged as powerful tools for non-invasive plant stress detection in automated irrigation management systems. These technologies provide valuable information about crop traits, enabling early and accurate detection of plant health issues (Triantafyllou et al., 2019). Triantafyllou et al. (2019) propose a comprehensive reference architecture model that incorporates advanced sensing technologies in the sensor layer for real-time plant stress detection, highlighting their importance in providing non-invasive plant stress detection. Similarly, Hossain et al. (2023) present a novel IoT-ML-Blockchain integrated framework for smart agricultural management that leverages advanced sensing technologies to optimize water use and improve crop yield, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
Hyperspectral imaging can capture subtle changes in plant physiology that are indicative of stress, while machine learning algorithms can be employed to extract meaningful patterns from the spectral data and classify different stress types (Araus et al., 2014). Thermal sensing can detect changes in canopy temperature, which is influenced by factors such as plant water status (Li et al., 2019). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
The integration of advanced sensing technologies in automated irrigation management systems has the potential to revolutionize precision agriculture. Jiang et al. (2019) demonstrate the effectiveness of a deep learning-based model in accurately detecting leaf spot diseases, highlighting the importance of image augmentation and deep learning algorithms in enhancing the model's performance.
Wireless sensor networks (WSNs) and energy-efficient communication protocols have the potential to significantly improve the efficiency and reliability of data collection in large-scale, long-term irrigation systems. WSNs offer a cost-effective and scalable solution for real-time data collection in large-scale irrigation systems, providing remote monitoring and automated control capabilities (Mehdizadeh et al., 2020). Nishiura and Yamamoto (2021) propose a novel sensor network system that utilizes drones and wireless power transfer to autonomously collect environmental data from sensor nodes in vast agricultural fields, reducing operational costs and enhancing the efficiency of data collection. Similarly, Higashiura and Yamamoto (2021) introduce a network system that employs UAVs and LoRa communication to efficiently collect environmental data from sensor nodes distributed across large farmlands, optimizing data collection and reducing travel distance and time.
Energy-efficient communication protocols are crucial for ensuring reliable data transmission in challenging environmental conditions and extending the lifespan of sensor nodes (Mehdizadeh et al., 2020). Al-Ali et al. (2023) investigate the potential of WSNs and energy-efficient communication protocols for data collection in large-scale, long-term irrigation systems, discussing the challenges and opportunities of using these technologies to improve the efficiency and reliability of real-time data collection in irrigation management. Mehdizadeh et al. (2020) emphasize the need for careful consideration of factors such as data accuracy, energy consumption, and network reliability when designing effective WSNs for irrigation management, enabling timely irrigation decisions and improved crop yields.
The automation of data collection through the use of advanced sensing technologies and wireless sensor networks is essential for achieving fully autonomous, scalable irrigation management. By enabling non-invasive plant stress detection and large-scale, long-term data collection, these technologies contribute to the overall goal of optimizing water resource management and increasing agricultural productivity. The integration of these technologies in real-time, end-to-end automated irrigation management systems has the potential to enhance agricultural water use efficiency and crop productivity, ultimately contributing to the development of fully autonomous, scalable irrigation management solutions.

3.4: Real-Time Data Transmission Protocols and Technologies
Real-time data transmission is a critical component of automated irrigation management systems, as it enables the timely delivery of sensor data to the cloud for processing and decision-making. The exploration of suitable protocols and network architectures is essential for ensuring efficient and reliable data transmission in these systems, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
The Message Queuing Telemetry Transport (MQTT) protocol has emerged as a popular choice for real-time data transmission in IoT networks, including those used for automated irrigation management. MQTT is a lightweight, publish-subscribe protocol designed for constrained devices and low-bandwidth networks (Author, 2019). Its simplicity and low overhead make it well-suited for IoT applications where data transmission speed and energy efficiency are critical (Saranyadevi et al., 2022). MQTT provides three Quality of Service (QoS) levels, ensuring data reliability in real-time scenarios (Author, 2019). Chen et al. (2020) proposed novel algorithms to improve data exchange efficiency and handle rerouting in MQTT-based IoT networks for automated irrigation management systems. Their TBRouting algorithm efficiently finds the shortest paths for data transmission, while the Rerouting algorithm effectively handles the rerouting of topic-based session flows when a broker crashes. The combination of these algorithms can significantly improve the performance and reliability of automated irrigation management systems (Chen et al., 2020).
Client-server IoT networks, such as those based on MQTT, play a crucial role in real-time data transmission for automated irrigation management systems. In these networks, sensors and devices (clients) publish data to a central broker (server), which then distributes the data to subscribed clients (Verma et al., 2021). This architecture enables efficient data collection, processing, and dissemination, facilitating the integration of various components within the automated irrigation management pipeline. Verma et al. (2021) proposed an architecture for healthcare monitoring systems using IoT and communication protocols, which provides a comprehensive overview of existing approaches and highlights challenges and opportunities in the field. Although focused on healthcare, the insights from this study can be applied to automated irrigation management systems, emphasizing the importance of interoperability and standardization for seamless integration (Verma et al., 2021).
In addition to MQTT, other application layer protocols such as XMPP, CoAP, SOAP, and HTTP have been explored for real-time data transmission in IoT networks. Each protocol has its strengths and weaknesses, making them suitable for different applications and scenarios. XMPP (Extensible Messaging and Presence Protocol) is an open-standard protocol that supports real-time messaging, presence, and request-response services (Saint-Andre, 2011). CoAP (Constrained Application Protocol) is a specialized web transfer protocol designed for use with constrained nodes and networks in the Internet of Things (Shelby et al., 2014). SOAP (Simple Object Access Protocol) is a protocol for exchanging structured information in the implementation of web services, while HTTP (Hypertext Transfer Protocol) is the foundation of data communication for the World Wide Web (Fielding et al., 1999).
Motamedi and Villányi (2022) compared and evaluated wireless communication protocols for the implementation of smart irrigation systems in greenhouses, considering factors such as power consumption, range, reliability, and scalability. They found that ZigBee is the most suitable local communication protocol for greenhouse irrigation due to its large number of nodes and long range, while MQTT is the recommended messaging protocol for smart irrigation systems due to its TCP transport protocol and quality of service (QoS) options. GSM is a reliable and cost-effective global communication protocol for greenhouse irrigation, providing wide coverage and low cost (Motamedi & Villányi, 2022).
Syafarinda et al. (2018) investigated the use of the MQTT protocol in a precision agriculture system using a Wireless Sensor Network (WSN). They found that MQTT is suitable for use in IoT applications due to its lightweight, simple, and low bandwidth requirements. The average data transmission speed using the MQTT protocol was approximately 1 second, demonstrating its effectiveness for real-time data transmission in precision agriculture systems (Syafarinda et al., 2018).
The choice of application layer protocol for real-time irrigation management depends on factors such as data transmission speed, reliability, and energy efficiency. MQTT and RTPS (Real-Time Publish-Subscribe) are both suitable for real-time data transmission in IoT systems, but they have different strengths and weaknesses. MQTT is a better choice for applications that require low latency and high throughput, while RTPS is a better choice for applications that require high reliability and low latency (Sanchez-Iborra & Skarmeta, 2021). The exploration of MQTT and client-server IoT networks, along with the comparison of various application layer protocols, provides valuable insights into the suitability of these technologies for real-time data transmission in automated irrigation management systems.
In summary, real-time data transmission protocols and technologies play a vital role in the automation of irrigation management systems, enabling the efficient and reliable delivery of sensor data to the cloud for processing and decision-making. The exploration of MQTT and client-server IoT networks, along with the comparison of application layer protocols, highlights the importance of selecting suitable technologies based on factors such as data transmission speed, reliability, and energy efficiency. By leveraging these technologies, automated irrigation management systems can achieve seamless integration and contribute to the overall goal of enhancing agricultural water use efficiency and crop productivity.

3.5. Challenges and Solutions in Real-Time Data Transmission
Following the exploration of data collection, processing at the edge and fog, and automation in previous sections, we now turn to the critical aspect of real-time data transmission. While essential for automated irrigation management, this stage presents unique challenges that must be addressed to ensure system efficiency and reliability.
Obstacles in Real-Time Data Transmission
Agricultural environments present unique challenges for real-time data transmission, directly impacting the effectiveness of automated irrigation systems. Environmental factors can significantly disrupt wireless communication. Adverse weather conditions such as heavy rain, fog, and high winds can weaken or even block radio signals, leading to data loss and compromised system performance. Physical obstacles like trees, buildings, and uneven terrain further complicate signal propagation, creating reliability issues (Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). These environmental challenges necessitate robust communication protocols and network architectures that can ensure consistent and reliable data flow.
In addition to environmental factors, technical limitations also present significant obstacles. Large-scale agricultural operations often demand long-distance data transmission, which can be hindered by the limited range of certain wireless communication protocols. Network congestion, occurring when multiple sensors transmit data concurrently, can lead to delays and potential data loss, further complicating real-time decision-making (Hameed et al., 2020). To mitigate these issues, researchers have investigated the potential of cognitive radio networks (CRNs) and dynamic spectrum access (DSA) for optimizing spectrum utilization and reducing interference (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). CRNs enable devices to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency. DSA, on the other hand, facilitates the dynamic allocation of unused spectrum, enhancing spectrum utilization and reducing congestion.
Furthermore, data security and privacy are paramount concerns in real-time irrigation systems. The sensitive nature of agricultural data, such as crop yields and farm management practices, necessitates robust security measures to prevent unauthorized access and data breaches (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential to protect data integrity and ensure the trustworthiness of the system.
Investigating Data Optimization Techniques
To enhance the efficiency and reliability of real-time data transmission in automated irrigation systems, researchers have explored a range of data optimization techniques. Data compression techniques aim to reduce the size of data packets transmitted over the network, minimizing bandwidth requirements and improving transmission speed (Rady et al., 2020; Karim et al., 2023). Lossless compression algorithms, such as Huffman coding and LZW, preserve data integrity while effectively reducing data size, ensuring that no information is lost during transmission (Cui, 2023). Lossy compression algorithms, such as JPEG and MP3, offer higher compression ratios but introduce a controlled level of data loss, which may be acceptable for certain applications where some loss of precision is tolerable (Karim et al., 2023). The choice between lossless and lossy compression depends on the specific application and the trade-off between data size and accuracy.
Data aggregation techniques provide another effective approach to optimize data transmission. By aggregating multiple sensor readings into a single representative value, such as average soil moisture or temperature, the number of transmissions can be significantly reduced, conserving bandwidth and energy resources (Cui, 2023). This is particularly beneficial in large-scale irrigation systems where numerous sensors are deployed across vast areas, generating substantial amounts of data. Additionally, data filtering techniques play a crucial role in improving data quality and reliability. Kalman filters and particle filters can effectively remove noise and outliers from sensor data, ensuring that only accurate and relevant information is transmitted and used for decision-making (Cui, 2023). This is essential for preventing erroneous data from influencing irrigation decisions and potentially leading to suboptimal water management.
Sensor calibration, drift correction, and fault detection are essential for maintaining data accuracy and reliability (Dos Santos et al., 2023). Regular calibration ensures that sensors provide accurate measurements over time, while drift correction techniques account for gradual changes in sensor readings due to environmental factors or aging. Fault detection mechanisms can identify and address sensor malfunctions or anomalies, preventing erroneous data from influencing irrigation decisions and potentially harming crops or wasting water.
Addressing the Challenges
Effectively addressing the challenges in real-time data transmission requires a multifaceted approach that encompasses environmental, technical, and data-related considerations. Implementing robust and adaptive communication protocols is crucial for overcoming interference and signal degradation caused by weather conditions and physical obstacles. Selecting appropriate protocols, such as LoRa or ZigBee, with suitable range and penetration capabilities can ensure reliable data transmission in challenging agricultural environments (Motamedi & Villányi, 2022). Additionally, employing techniques like frequency hopping and error correction codes can further improve communication resilience and mitigate data loss.
Optimizing network architecture is another key consideration. Deploying a distributed network architecture with edge and fog computing capabilities can significantly enhance data processing and transmission efficiency (Abdel Nasser et al., 2020; Tran et al., 2019). Edge devices can perform initial data processing and aggregation tasks, reducing the amount of data transmitted to the cloud and minimizing latency, while fog nodes can provide additional processing power and storage closer to the data source, enhancing scalability and reliability. This distributed approach alleviates the burden on the central cloud server and allows for more responsive and efficient irrigation management.
Data optimization techniques play a vital role in reducing bandwidth requirements and improving transmission efficiency. The choice of data compression, aggregation, and filtering techniques should be tailored to the specific requirements of the irrigation system, considering factors such as data type, accuracy needs, and available bandwidth. By carefully selecting and implementing these techniques, the overall performance and effectiveness of real-time irrigation systems can be significantly enhanced, leading to more sustainable water management practices and improved agricultural productivity.
By addressing these challenges and implementing appropriate solutions, real-time data transmission can become a reliable and efficient component of automated irrigation systems, contributing to the overall goal of achieving sustainable and productive agriculture in the face of growing food demands and water scarcity.

3.6. IoT Network Architectures and Variable Rate Irrigation (VRI) for Real-Time Irrigation
Real-time irrigation management systems heavily rely on the efficient and reliable transmission of data from sensors and weather stations to the cloud for processing and decision-making. However, agricultural environments present unique challenges to wireless communication, including adverse weather conditions, physical obstacles, and the limitations of wireless technologies. These challenges necessitate robust and adaptive solutions to ensure the consistent and timely flow of data, enabling truly autonomous irrigation scheduling.
Environmental factors, such as heavy rain, fog, and strong winds, can significantly disrupt wireless communication by attenuating or even blocking radio signals, leading to data loss and compromised system performance (Ed-daoudi et al., 2023; Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). Dense vegetation, buildings, and uneven terrain create further complications by causing multipath propagation and shadowing effects (Yim et al., 2018; Gautam and Pagay, 2020). The study by Yim et al. (2018) on LoRa networks in a tree farm environment exemplifies these challenges, revealing reduced communication range and data reliability compared to theoretical expectations. This underscores the need for carefully selecting and optimizing communication protocols and network parameters to ensure reliable data transmission in such environments.
The study by Guzinski et al. (2014a) using a modified TSEB model further highlights the importance of high-resolution data in accurately capturing the spatial and temporal dynamics of energy fluxes influenced by environmental factors. This emphasizes the need for advanced data acquisition and processing techniques that can effectively represent the complexities of agricultural settings.
The limitations of traditional wireless communication technologies, such as limited range and network congestion, pose additional challenges for large-scale agricultural operations. Long-distance data transmission can be hindered by range limitations, while network congestion arising from numerous sensors transmitting concurrently can lead to delays and data loss, hindering real-time decision-making (Hameed et al., 2020). Addressing these challenges requires the exploration of advanced networking technologies that can optimize spectrum utilization, mitigate interference, and improve reliability and efficiency.
Cognitive Radio Networks (CRNs) and Dynamic Spectrum Access (DSA) offer promising solutions for optimizing wireless communication in agricultural settings. CRNs empower devices with the ability to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). Research has explored the potential of CRNs in predicting Radio Frequency (RF) power to avoid noisy channels and optimize spectrum utilization (Iliya et al., 2014; Iliya et al., 2014). These studies demonstrate the effectiveness of combining optimization algorithms with artificial neural networks (ANNs) to enhance the accuracy and generalization of RF power prediction, enabling CRNs to make informed decisions about channel selection and avoid interference.
DSA complements CRN technology by dynamically allocating unused spectrum, further enhancing spectrum utilization and reducing congestion (Shi et al., 2023). The numerical model developed by Shi et al. (2023) showcases the potential of CRNs and DSA for optimizing wireless communication in challenging environments.
The integration of CRNs and DSA into the IoT network architecture requires careful consideration of spectrum sensing techniques, network topology, and data security. Research on cooperative spectrum sensing suggests that distributed approaches, where sensor nodes collaborate and share information, can significantly improve the accuracy and efficiency of spectrum sensing, particularly in dynamic environments (Trigka and Dritsas, 2022; Khalid & Yu, 2019). This collaborative approach enables a more comprehensive understanding of the radio environment and facilitates the identification of available frequency bands for data transmission.
The choice of network topology also impacts the performance and scalability of CRN-based irrigation systems. Mesh networks, where sensor nodes are interconnected and relay data for each other, offer enhanced resilience and coverage compared to star topologies where nodes communicate directly with a central gateway (Akyildiz & Vuran, 2010). However, mesh networks can be more complex to manage and may introduce additional routing overhead. The trade-off between network resilience and complexity needs to be carefully evaluated to select the most appropriate topology for a specific agricultural setting.
Data security and privacy are paramount concerns in IoT-based irrigation systems due to the sensitive nature of agricultural data (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential for protecting data integrity and ensuring system trustworthiness. Research on secure spectrum leasing and resource allocation algorithms for CR-WSN-based irrigation systems has demonstrated the potential of these technologies for enhancing security and efficiency (Hassan, 2023; Afghah et al., 2018).
In conclusion, the development of effective and reliable real-time irrigation management systems requires a comprehensive approach that addresses the challenges of data transmission in agricultural environments. The integration of robust and adaptive communication protocols, optimized network architectures, and advanced networking technologies like CRNs and DSA, along with a focus on data security and privacy, can contribute significantly to achieving the goal of autonomous and efficient irrigation scheduling.
4. AUTOMATED DATA PROCESSING IN THE CLOUD
4.1. Data Quality and Preprocessing
Data quality is paramount in automated irrigation systems as it directly influences the effectiveness of decision-making and water use efficiency. Issues like missing values, inconsistencies, and outliers arising from sensor malfunctions, environmental interference, or network problems (Lv et al., 2023) can significantly impact the performance of machine learning models used for irrigation scheduling and management.
Real-time data cleaning techniques are essential for addressing these challenges. Kalman filtering proves particularly effective in handling missing values and correcting erroneous readings by recursively estimating the system's state based on previous measurements and current sensor data, taking into account noise and uncertainty (Kim et al., 2020). Moving average techniques, by averaging consecutive data points, provide a more stable representation of the underlying trend, filtering out short-term fluctuations (Chhetri, 2023). For outlier detection, adaptive thresholding methods offer a dynamic approach, adjusting thresholds based on the statistical properties of the data to effectively identify anomalies and minimize false positives (Bah et al., 2021). These techniques are crucial in maintaining the integrity of real-time data streams and ensuring the accuracy of subsequent analyses.
Adaptive data preprocessing is essential for managing the diversity of data sources and formats commonly found in irrigation systems. Data normalization techniques, such as min-max scaling or z-score normalization, ensure that all features contribute equally to the analysis by transforming data values to a common scale (Pradal et al., 2016). This is crucial for preventing features with larger values from dominating the analysis and ensuring that all features are given equal consideration. Similarly, feature scaling methods, like standardization or normalization, optimize the range of feature values to improve the performance and convergence of machine learning models (Tortorici et al., 2024). By scaling features to a similar range, the influence of outliers is reduced, and the model's ability to learn from the data is enhanced.
Data fusion techniques play a critical role in integrating information from diverse sources, creating a more comprehensive and reliable dataset for irrigation management. Dempster-Shafer theory, a generalization of probability theory, allows for the expression of both uncertainty and the degree of conflict in evidence, making it suitable for fusing uncertain and conflicting data from heterogeneous sources (Sadiq and Rodriguez, 2004). This is particularly relevant in irrigation systems where data from different sensors may provide slightly different or even contradictory information due to sensor variations or environmental factors. Bayesian inference offers another powerful framework for combining information from multiple sources, updating the probability of a hypothesis as new evidence becomes available. By applying these techniques, data from soil moisture sensors, canopy temperature sensors, weather stations, and other sources can be integrated to provide a holistic understanding of crop water requirements and environmental conditions, leading to more informed and accurate irrigation decisions.
The impact of data quality extends beyond model accuracy to the robustness of machine learning models under varying conditions. Robust models should maintain consistent performance even when faced with data inconsistencies or unexpected situations. Techniques like data augmentation and domain adaptation can enhance model robustness by exposing the model to a wider range of data variations during training. Data augmentation involves generating additional training data by applying transformations or introducing noise to existing data, making the model more resilient to noise and variations in the real-world data. Domain adaptation techniques aim to adapt a model trained on one domain (e.g., a specific crop or geographic location) to perform well on another domain with different data characteristics. This is particularly relevant in irrigation management, where models may need to be applied to different crops, soil types, or climatic conditions.
The choice of data cleaning, preprocessing, and fusion techniques should be carefully considered based on the specific characteristics of the irrigation system and the available data. By selecting and implementing appropriate techniques, the accuracy, reliability, and robustness of machine learning models can be significantly improved, leading to more efficient and sustainable irrigation management practices.
4.2. Scalable and Autonomous Deployment using Containerization Strategies
The transition from data collection and transmission to efficient data processing requires a robust infrastructure capable of handling diverse workloads and data volumes. Containerization technologies, specifically Docker and Kubernetes, offer a promising solution for deploying and scaling data processing and machine learning modules within cloud environments like AWS, Azure, and GCP (Vargas-Rojas et al., 2024; Rosendo et al., 2022; Solayman & Qasha, 2023). Docker provides a standardized way to package applications and their dependencies into self-contained units known as containers, ensuring consistent and reproducible execution across different platforms (Rosendo et al., 2022). Kubernetes, acting as a container orchestrator, manages their deployment, scaling, and networking across a cluster of machines (Rosendo et al., 2022). This combination presents several advantages for automated irrigation management systems.
Firstly, containerization facilitates efficient resource utilization and scalability. By encapsulating applications and their dependencies, containers enable the isolation of resources and prevent conflicts between different modules (Vargas-Rojas et al., 2024; Solayman & Qasha, 2023). This isolation allows for the efficient allocation of resources, such as CPU, memory, and storage, to each container based on its specific needs. Kubernetes further enhances scalability by allowing for the automatic scaling of containers based on real-time demand, ensuring the system can adapt to varying workloads and data volumes, preventing bottlenecks, and ensuring responsiveness to changing conditions (Karamolegkos et al., 2023).
Secondly, containerization promotes portability and reproducibility. By packaging applications and their dependencies into a single unit, containers make it easy to move and deploy them across different cloud environments without the need for environment-specific configurations (Rosendo et al., 2022; Solayman & Qasha, 2023). This portability simplifies the development and deployment process, reducing the time and effort required to set up and manage the system. Additionally, containers ensure reproducibility by providing a consistent execution environment, regardless of the underlying infrastructure. This eliminates variability and ensures that the system will behave consistently across different deployments (Zhou et al., 2023).
Optimizing container orchestration and resource allocation is crucial to minimizing latency and maximizing throughput in real-time data processing pipelines. Techniques like auto-scaling and dynamic resource allocation play a critical role in this context (Hethcoat et al., 2024; Werner and Tai, 2023; Kumar et al., 2024). Auto-scaling automatically adjusts the number of container instances based on real-time demand, ensuring that sufficient resources are available to handle peak workloads while avoiding over-provisioning during periods of low demand (Hethcoat et al., 2024; Kumar et al., 2024). Dynamic resource allocation enables the fine-grained adjustment of resources allocated to each container based on its specific needs and the current workload (Werner and Tai, 2023). This ensures efficient resource allocation and provides each container with the necessary resources to perform its tasks effectively.
Performance monitoring tools, such as Kubernetes Metrics Server and Prometheus, are essential for gaining insights into the performance of containers and the overall system (Hethcoat et al., 2024; Kuity & Peddoju, 2023). These tools provide valuable data on key performance indicators, such as CPU and memory usage, network traffic, and application-specific metrics. By monitoring this data, administrators can identify bottlenecks, optimize resource allocation strategies, and continuously improve system performance (Hethcoat et al., 2024). This data-driven approach ensures that automated irrigation management systems can operate efficiently and reliably.
By integrating containerization technologies with optimization techniques and performance monitoring, automated irrigation management systems achieve the scalability, autonomy, and efficiency required for effective real-time data processing and decision-making. This approach facilitates a seamless and responsive system that can adapt to changing conditions and contribute to the overall goal of optimizing water resource management and increasing agricultural productivity.
4.3. Deploying ML Models for Data Processing
•	Architectures and frameworks for deploying machine learning models on cloud platforms for real-time data processing and inference in irrigation management systems, such as: TensorFlow Serving, Apache MXNet Model Server, ONNX Runtime
•	Techniques for optimizing machine learning model performance and resource utilization in cloud environments, such as: Model compression (e.g., pruning, quantization), Hardware acceleration (e.g., GPU, TPU), Distributed training (e.g., Horovod, BytePS)
•	Integration of deployed machine learning models with other components of the automated irrigation management pipeline, such as data preprocessing, decision-making, and control systems, using protocols like: MQTT, CoAP, RESTful APIs
4.4. Online Learning in the Cloud
•	Application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as: Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests
•	Architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as: Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms
•	Strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making, using techniques such as: Multi-armed bandits, Bayesian optimization, Reinforcement learning (e.g., Q-learning, SARSA)


5. GENERATING AND APPLYING IRRIGATION INSIGHTS 
5.1. Real-Time Generation of Actionable Irrigation Insights
•	Advanced predictive models, such as deep learning (e.g., LSTM, CNN) and ensemble methods (e.g., Random Forests), for precise, site-specific irrigation recommendations
•	Integration of IoT sensor data (e.g., soil moisture probes, weather stations) and cloud-based data sources (e.g., weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filtering) to enhance insight accuracy and resolution
•	Strategies for handling data heterogeneity, uncertainty, and quality issues in real-time insight generation, such as data preprocessing and outlier detection
•	Techniques for reducing computational complexity and latency, such as edge computing (e.g., fog computing), model compression (e.g., quantization), and hardware accelerators (e.g., GPUs)
5.2. Automated Application of Irrigation Insights
•	Architectures and protocols for seamless integration of ML-generated insights with IoT-enabled irrigation control systems, such as MQTT and CoAP for lightweight, real-time communication
•	Analysis of industry-leading products and services, such as smart irrigation controllers (e.g., Rachio) and cloud-based irrigation management platforms (e.g., CropX)
•	Strategies for ensuring reliability, security, and scalability of automated insight application, such as redundant communication channels and secure edge-to-cloud architectures
•	Case studies of successful implementations of closed-loop, autonomous irrigation systems in research and commercial settings, highlighting technologies used and benefits achieved

6. INTEGRATION, INTEROPERABILITY, AND STANDARDIZATION 
6.1. Interoperability and Standardization
•	Importance of interoperability and standardization in enabling seamless integration of automated irrigation components
•	Overview of existing and emerging standards for IoT devices, communication protocols, and data formats in precision agriculture (e.g., ISOBUS, agroXML, SensorML)
•	Role of standardization bodies and industry consortia in promoting interoperability (e.g., AgGateway, Open Ag Data Alliance, Agricultural Industry Electronics Foundation)
•	Challenges in adopting and implementing standards across diverse hardware and software platforms
•	Strategies for encouraging widespread adoption of standards and best practices for interoperability in automated irrigation systems
6.2. Integration with Existing Irrigation Infrastructure
•	Challenges and strategies for retrofitting legacy irrigation systems with IoT sensors, actuators, and communication devices
•	Hardware compatibility issues and solutions (e.g., adapters, modular designs)
•	Software and firmware updates to enable integration with automated decision-making systems
•	Data integration and normalization techniques for merging legacy and new data sources
•	Economic and practical considerations for transitioning from manual to automated irrigation management
•	Cost-benefit analysis of upgrading existing infrastructure vs. implementing new systems
•	Phased implementation approaches to minimize disruption and optimize resource allocation
•	Training and support requirements for farmers and irrigation managers adopting automated systems
•	Case studies and real-world examples of successful integration of automated irrigation with existing infrastructure
6.3. Integration with Other Precision Agriculture Technologies
•	Synergies between automated irrigation and complementary technologies
•	Remote sensing (satellite, UAV, and ground-based) for crop monitoring and evapotranspiration estimation
•	Soil moisture sensors and weather stations for real-time, localized data collection
•	Variable rate application systems for precise irrigation delivery based on crop requirements
•	Yield mapping and analytics for assessing the impact of automated irrigation on crop productivity
•	Architectures and frameworks for integrating diverse data sources and technologies into a unified precision agriculture ecosystem
•	Edge computing and fog computing paradigms for real-time data processing and decision-making
•	Cloud-based platforms for data storage, analysis, and visualization
•	API-driven approaches for modular integration of third-party services and applications
•	Challenges and solutions for ensuring data quality, consistency, and security across integrated precision agriculture systems
•	Data cleaning, preprocessing, and harmonization techniques
•	Blockchain and distributed ledger technologies for secure, tamper-proof data sharing and traceability
•	Access control and authentication mechanisms for protecting sensitive data and resources
•	Future trends and research directions in the integration of automated irrigation with advanced precision agriculture technologies (e.g., AI-driven crop modeling, robotics, and autonomous vehicles)
6.4. Cybersecurity Considerations for Integrated Automated Irrigation Systems
•	Unique security risks and vulnerabilities associated with IoT-based automated irrigation systems
•	Potential for unauthorized access, data tampering, and system manipulation
•	Implications of security breaches for crop health, water resource management, and farm productivity
•	Best practices and strategies for securing automated irrigation systems
•	Secure device provisioning and authentication (e.g., hardware security modules, certificates)
•	Encryption and secure communication protocols (e.g., TLS, DTLS)
•	Firmware and software updates to address emerging security threats
•	Network segmentation and access control to limit the impact of breaches
•	Role of cybersecurity standards and frameworks in guiding the development and deployment of secure automated irrigation systems (e.g., NIST CSF, IEC 62443)
•	Importance of user awareness, training, and incident response planning in maintaining the security of integrated automated irrigation systems

7. MONITORING AND ENSURING SYSTEM RELIABILITY
7.1. Resilience and Fault Tolerance in Automated Irrigation Systems
•	Strategies for ensuring robustness and reliability in the face of failures, disruptions, or unexpected events
•	Redundancy: Implementing redundant components, such as duplicate sensors (e.g., soil moisture sensors, weather stations), controllers (e.g., PLCs, microcontrollers), and communication channels (e.g., cellular, satellite, LoRaWAN) to maintain system functionality during component failures
•	Failover mechanisms: Designing seamless failover mechanisms that automatically switch to backup components or systems in case of primary system failure, such as hot-standby controllers or multi-path communication protocols (e.g., mesh networks, software-defined networking)
•	Self-healing capabilities: Incorporating AI-driven self-healing mechanisms that can detect, diagnose, and recover from faults without human intervention, using techniques like reinforcement learning, Bayesian networks, or self-organizing maps
•	The role of distributed architectures and edge computing in enhancing system resilience
•	Decentralizing critical functions and data processing to minimize the impact of single points of failure, using fog computing or multi-agent systems
•	Leveraging edge computing to enable localized decision-making and control, reducing dependence on cloud connectivity and improving response times, using technologies like Raspberry Pi, NVIDIA Jetson, or Intel NUC
•	Anomaly detection and predictive maintenance using AI techniques
•	Employing unsupervised learning algorithms (e.g., autoencoders, clustering) to detect anomalies in sensor data, system performance, and water usage patterns
•	Developing predictive maintenance models using techniques like long short-term memory (LSTM) networks, convolutional neural networks (CNNs), or gradient boosting machines (GBMs) to anticipate and prevent potential system failures based on historical data and real-time monitoring
7.2. Advanced Monitoring Techniques for Automated Irrigation Systems
•	Remote monitoring using IoT-enabled sensors and computer vision
•	Deploying a heterogeneous network of IoT sensors to collect real-time data on soil moisture (e.g., capacitive, tensiometric), temperature (e.g., thermocouples, thermistors), humidity (e.g., capacitive, resistive), and plant health (e.g., sap flow, leaf wetness)
•	Integrating high-resolution cameras (e.g., multispectral, hyperspectral) and computer vision algorithms for visual monitoring of crop growth, disease detection (e.g., using deep learning-based object detection and segmentation), and irrigation system performance (e.g., leak detection, sprinkler uniformity)
•	Transmitting sensor and camera data to cloud-based platforms (e.g., AWS IoT, Google Cloud IoT, Microsoft Azure IoT) for remote access and analysis using protocols like MQTT, CoAP, or AMQP
•	Innovative approaches for real-time system health assessment
•	Developing novel algorithms and metrics for evaluating the health and performance of automated irrigation systems, such as entropy-based measures, network resilience indices, or multi-criteria decision analysis (MCDA) frameworks
•	Combining data from multiple sources (e.g., sensors, weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filters, Dempster-Shafer theory) to create a comprehensive view of system health
•	Employing advanced data visualization techniques (e.g., interactive dashboards, augmented reality) to present system health information in an intuitive and actionable format
7.3. Closed-Loop Control and Feedback Mechanisms
•	Exploring the concept of closed-loop control in autonomous irrigation systems
•	Implementing feedback loops that continuously monitor system performance and adjust irrigation schedules based on real-time data, using control techniques like proportional-integral-derivative (PID), model predictive control (MPC), or fuzzy logic control (FLC)
•	Integrating machine learning algorithms (e.g., reinforcement learning, genetic algorithms) to optimize closed-loop control strategies over time, adapting to changing environmental conditions and crop requirements
•	Designing effective feedback mechanisms for user interaction and system optimization
•	Providing user-friendly interfaces (e.g., mobile apps, web dashboards) for farmers to input preferences, constraints, and expert knowledge into the automated irrigation system, using techniques like participatory design or user-centered design
•	Incorporating user feedback and domain expertise to refine irrigation strategies and improve system performance
8. CASE STUDIES AND REAL-WORLD IMPLEMENTATIONS OF FULLY AUTONOMOUS IRRIGATION SYSTEMS
8.1. Fully Autonomous Irrigation Systems in Diverse Agricultural Settings
•	Row Crops: maize, wheat, soybean with real-time soil moisture monitoring and weather-based irrigation scheduling for fully automated precision irrigation
•	Orchards: citrus, apple, almond with plant health monitoring and precision water application for fully autonomous orchard management
•	Greenhouses: tomato, lettuce, herbs with automated drip irrigation and climate control integration for fully automated greenhouse operations
•	Urban Farming: rooftop gardens, vertical farms with IoT-enabled hydroponic systems and remote management for fully autonomous urban crop production
8.2. Integration of Advanced System Components for End-to-End Automation
•	Wireless sensor networks: soil moisture probes, weather stations, plant health monitoring cameras with low-power, long-range communication for fully automated data acquisition
•	Secure data transmission: LoRaWAN, NB-IoT, 5G, satellite communication for reliable, real-time data transfer from field to cloud in fully autonomous irrigation systems
•	Intelligent data processing: edge computing for local data filtering, cloud platforms for scalable storage and analysis, machine learning algorithms for predictive insights in fully automated irrigation management
•	Autonomous decision-making: advanced irrigation scheduling algorithms, precise valve control, closed-loop feedback systems for optimal water management in fully autonomous irrigation systems
8.3. Quantitative Performance Evaluation of Fully Automated Irrigation Systems
•	Water use efficiency: percent reduction in water consumption compared to conventional methods, improved water productivity (yield per unit of water) achieved through fully autonomous irrigation
•	Crop yield and quality improvements: percent increase in yield, enhanced crop uniformity, improved nutritional content attributed to fully automated precision irrigation
•	Labor and energy savings: quantified reduction in labor hours for irrigation management, decreased energy consumption for pumping due to optimized scheduling in fully autonomous systems
•	Economic viability: detailed return on investment analysis, payback period calculations, comprehensive cost-benefit analysis for fully autonomous irrigation management systems
8.4. Lessons Learned and Challenges Encountered in Deploying Autonomous Irrigation Systems
•	Technical challenges and solutions: ensuring reliable data transmission in remote locations, addressing interoperability issues between diverse system components, optimizing power consumption for extended battery life, adapting algorithms to local soil and weather conditions in fully autonomous irrigation systems
•	Operational and logistical hurdles: streamlining installation and maintenance procedures, providing effective user training, seamlessly integrating with existing farm management practices and legacy systems for fully automated irrigation management
•	Regulatory and socio-economic considerations: navigating complex water use regulations, addressing data privacy and security concerns, ensuring equitable access and affordability for smallholder farmers adopting fully autonomous irrigation technologies
8.5. Best Practices and Recommendations for Successful Implementation
•	Designing scalable, modular, and adaptable autonomous irrigation systems to accommodate future growth and changing requirements for fully automated water management
•	Prioritizing user-centered design principles and actively engaging stakeholders throughout the development and deployment process of fully autonomous irrigation solutions
•	Adopting open standards and communication protocols to enable seamless integration of system components and interoperability with third-party platforms in fully automated irrigation setups
•	Implementing robust data validation, filtering, and quality control mechanisms to ensure data integrity and reliability for decision-making in fully autonomous irrigation systems
•	Establishing clear data governance policies and security frameworks to protect sensitive information and maintain user trust in fully automated irrigation management
•	Developing intuitive user interfaces and decision support tools to facilitate easy adoption and effective use of fully autonomous irrigation systems
•	Collaborating with local extension services, agribusinesses, and technology providers for knowledge transfer, technical support, and continuous improvement of fully automated irrigation solutions
8.6. Synthesis of Case Studies and Implications for Autonomous Irrigation Adoption
•	Cross-case analysis of key performance indicators and critical success factors for fully autonomous irrigation scheduling systems in various contexts
•	Identification of common themes, challenges, and innovative solutions across diverse implementations of end-to-end fully automated irrigation management
•	Assessment of the potential for replicability and scaling of successful fully autonomous irrigation projects in different regions and farming systems
•	Implications for future research priorities, technology development roadmaps, and policy interventions to support widespread adoption of fully autonomous irrigation technologies

CONCLUSION/FUTURE DIRECTIONS AND UNANSWERED QUESTIONS
•	Summarize the key insights gained from the question-driven review, emphasizing how each section contributes to the overarching goal of achieving real-time, end-to-end automation in irrigation management
•	Based on the questions addressed, propose new research directions and unanswered questions
•	Identify key research gaps and propose concrete research questions and hypotheses for advancing the field of real-time, automated irrigation management
•	Highlight the need for collaborative research efforts across disciplines, such as computer science, agricultural engineering, and environmental science, to address the complex challenges of automated irrigation systems
•	Emphasize the need for further innovation and exploration in real-time, automated irrigation systems



</previous_sections>

</documents>
<instructions>


Use the information provided in the <documents> tags to write the next subsection of the research paper, following these steps:
1. Review the overall intention of the research paper, specified in the <review_intention> tag. Ensure the subsection you write aligns with and contributes to this overall goal.
2. Consider the specific intention for this subsection of the paper, stated in the <section_intention> tag. The content you write should fulfill this purpose. 
3. Use the title provided in the <subsection_title> tag as the heading for the subsection. 
4. Address each of the points specified in the </subsection_point_Point *> tags:
   a) Make a clear case for each point using the text provided in the "point" field.
   b) Support each point with evidence from the research papers listed in the corresponding "papers to support point" field.
   c) When citing a paper to support a point, include inline citations with the author name(s) and year, e.g. (Smith et al., 2020; Johnson and Lee, 2019; Brown, 2018). Cite all papers that strengthen or relate to the point being made.
   d) While making a point and citing the supporting papers, provide a brief explanation in your own words of how the cited papers support the point.
5. Ensure that both of the points from the <subsection_point> tags are fully addressed and supported by citations. Do not skip or combine any points.
6. After addressing the specified points, wrap up the subsection with a concluding sentence or two that ties the points together and relates them back to the <section_intention>.
7. Review the <Previous_sections> of the paper, and ensure that the new subsection you have written fits logically and coherently with the existing content. Add transition sentences as needed to improve the flow.
8. Proofread the subsection to ensure it is clear, concise, and free of grammatical and spelling errors. Maintain a formal academic tone and style consistent with the rest of the research paper.
9. Format the subsection using Markdown, including the subsection heading (using ## or the equivalent for the document), inline citations, and any other formatting needed for clarity and readability.
10. If any information is missing or unclear in the provided tags, simply do your best to write the subsection based on the available information. Do not add any information or make any points not supported by the provided content. Prioritize fully addressing the required points over hitting a specific word count.

The output should be a complete, well-organized, and properly cited subsection ready to be added to the research paper.

Begin your answer with a brief recap of the instructions stating what you will to optimize the quality of the answer. Clearly and briefly state the subsection you'll be working on and the points you'll be addressing. Then proceed to write the subsection following the instructions provided. 

Critical: 
- Do not include a conclusion or summary as the entry is in the middle of the document. Focus on addressing the points and supporting them with evidence from the provided papers. Ensure that the subsection is well-structured, coherent, and effectively contributes to the overall research paper.
- The subsection we are focusing on is: 4.4. Online Learning in the Cloud
- No need for sub-sub-sections. just provide paragraphs addressing each point. They should transition fluidly and narurally into each other.
- Ensure that the content is supported by the provided papers and that the citations are correctly formatted and placed within the text.
- Do not repeat content from the previous sections. Ensure that the information provided is new and relevant to the subsection being written.



</instructions>

<subsection_point_Point 2>
Point: Architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms

Papers to support point:

Paper 1:
- APA Citation: None
  Main Objective: None
  Study Location: None
  Data Sources: None
  Technologies Used: None
  Key Findings: None
  Extract 1: The European Space Agency’s Sentinel satellites have laid the foundation for global land use land cover (LULC) mapping with unprecedented detail at 10 m resolution.
  Extract 2: Google’s Dynamic World (DW), ESA’s World Cover (WC) and Esri’s 2020 Land Cover (Esri) products have the vision of being multi-temporal, with WC and Esri being annually updated, but only DW is operationally delivering near real-time LULC maps as new Sentinel-2 scenes become available (every 5 days).
  Limitations: None
  Relevance Evaluation: 0.9-1.0: Exceptionally relevant - Comprehensively addresses all key aspects of the point mentioned in <point_focus> within the context of the overall review intentions and the specific section and sub-section in which the point is located.
  Relevance Score: 1.0
  Inline Citation: None
  Explanation: The Sentinel satellites have laid the foundation for global LULC mapping with unprecedented detail at 10 m resolution. Three global 10 m LULC maps have been released so far: Google's Dynamic World (DW), ESA's World Cover (WC), and Esri's 2020 Land Cover (Esri). These maps have the potential to contribute to addressing the global food challenge and providing data for ecosystem accounting, but it is important to critically evaluate their suitability for specific applications, such as aggregate changes in ecosystem accounting versus site-speciﬁc change detection in monitoring, considering trade-offs between thematic resolution, global versus local accuracy, class-speciﬁc biases and whether change analysis is necessary.

 Full Text: >
Citation: Venter, Z.S.; Barton, D.N.;
Chakraborty, T.; Simensen, T.; Singh,
G. Global 10 m Land Use Land Cover
Datasets: A Comparison of Dynamic
World, World Cover and Esri Land
Cover. Remote Sens. 2022, 14, 4101.
https://doi.org/10.3390/rs14164101
Academic Editors: Wu Xiao,
Qiusheng Wu and Xuecao Li
Received: 22 July 2022
Accepted: 19 August 2022
Published: 21 August 2022
Publisher’s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright:
© 2022 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
remote sensing  
Article
Global 10 m Land Use Land Cover Datasets: A Comparison of
Dynamic World, World Cover and Esri Land Cover
Zander S. Venter 1,*
, David N. Barton 1, Tirthankar Chakraborty 2
, Trond Simensen 1,3
and Geethen Singh 4,5
1
Norwegian Institute for Nature Research—NINA, Sognsveien 68, 0855 Oslo, Norway
2
Atmospheric Sciences and Global Change Division, Paciﬁc Northwest National Laboratory,
Richland, WA 99352, USA
3
Geo-Ecological Research Group (GEco), Section for Research and Collections, Natural History Museum,
The Faculty of Mathematics and Natural Sciences, University of Oslo, 0562 Oslo, Norway
4
School of Animal, Plant & Environmental Sciences, University of the Witwatersrand,
Johannesburg 2000, South Africa
5
DST-NRF Centre of Excellence, Fitzpatrick Institute of African Ornithology, University of Cape Town,
Rondebosch, Cape Town 7701, South Africa
*
Correspondence: zander.venter@nina.no
Abstract: The European Space Agency’s Sentinel satellites have laid the foundation for global
land use land cover (LULC) mapping with unprecedented detail at 10 m resolution. We present a
cross-comparison and accuracy assessment of Google’s Dynamic World (DW), ESA’s World Cover
(WC) and Esri’s Land Cover (Esri) products for the ﬁrst time in order to inform the adoption and
application of these maps going forward. For the year 2020, the three global LULC maps show
strong spatial correspondence (i.e., near-equal area estimates) for water, built area, trees and crop
LULC classes. However, relative to one another, WC is biased towards over-estimating grass cover,
Esri towards shrub and scrub cover and DW towards snow and ice. Using global ground truth
data with a minimum mapping unit of 250 m2, we found that Esri had the highest overall accuracy
(75%) compared to DW (72%) and WC (65%). Across all global maps, water was the most accurately
mapped class (92%), followed by built area (83%), tree cover (81%) and crops (78%), particularly
in biomes characterized by temperate and boreal forests. The classes with the lowest accuracies,
particularly in the tundra biome, included shrub and scrub (47%), grass (34%), bare ground (57%) and
ﬂooded vegetation (53%). When using European ground truth data from LUCAS (Land Use/Cover
Area Frame Survey) with a minimum mapping unit of <100 m2, we found that WC had the highest
accuracy (71%) compared to DW (66%) and Esri (63%), highlighting the ability of WC to resolve
landscape elements with more detail compared to DW and Esri. Although not analyzed in our
study, we discuss the relative advantages of DW due to its frequent and near real-time data delivery
of both categorical predictions and class probability scores. We recommend that the use of global
LULC products should involve critical evaluation of their suitability with respect to the application
purpose, such as aggregate changes in ecosystem accounting versus site-speciﬁc change detection
in monitoring, considering trade-offs between thematic resolution, global versus. local accuracy,
class-speciﬁc biases and whether change analysis is necessary. We also emphasize the importance of
not estimating areas from pixel-counting alone but adopting best practices in design-based inference
and area estimation that quantify uncertainty for a given study area.
Keywords: accuracy; deep learning; Earth observation; Sentinel-2; validation
1. Introduction
Global land use land cover (LULC) maps provide information necessary to quantify
and understand Earth system processes and anthropogenic pressures, often at multiple
spatial and temporal scales [1–3]. Earth observation and satellite remote sensing have en-
abled mapping LULC in a spatially explicit manner that ultimately informs policy and land
Remote Sens. 2022, 14, 4101. https://doi.org/10.3390/rs14164101
https://www.mdpi.com/journal/remotesensing
Remote Sens. 2022, 14, 4101
2 of 19
management decisions aimed at achieving the global sustainable development goals [4].
Global LULC maps are adopted in a vast range of scientiﬁc domains and application envi-
ronments. A few examples of LULC map application include: data input into mesoscale
models for operational numerical weather forecasts and climate models for future climate
projections [5,6]; outlining the extent of ecosystems and the change therein, e.g., as the
basis for ecosystem service accounting [7,8]; isolating urban areas from their background
to quantify local climate impacts [9]; informing spatial species distribution models that
can predict and inform biodiversity conservation [10,11]; spatial conservation planning
and environmental impact assessments [12]; monitoring deforestation and reporting to
policy mechanisms, such as reduced emissions from deforestation and forest degradation
(REDD+) [13]. One of the most impactful applications of global LULC maps going forward
may be for ecosystem extent mapping following UN statistical standards for ecosystem
accounting (EA) under the System of Environmental–Economic Accounting (SEEA) [14].
Over the past two decades, the spatial resolution of land cover maps has kept pace with
the resolution of available satellite sensors, including the Moderate Resolution Imaging
Spectroradiometer (MODIS; 250–500 m), PROBA-v (100 m) and Landsat (30 m) satellites.
The most prominent corresponding global LULC maps include the National Aeronautics
and Space Administration (NASA) MCD12Q1 500 m resolution dataset (2001–2018) [15],
the European Space Agency (ESA) Copernicus Global Land Service (CGLS) Land Cover
100 m dataset (2015–2019) [16] and GlobLand30 (2010) [17]. While these products have
been widely adopted, particularly at provincial to regional spatial scales, the medium
spatial resolution prohibits the detection and monitoring of smaller landscape elements,
which are vital to ﬁner-scale Earth system processes and local land use planning. For
instance, the monitoring and evaluation of agri-environmental schemes [18], such as
installing hedge rows or semi-natural vegetation vital to pollinators, is not possible with
the aforementioned LULC maps. Similarly, accounting for intra-urban blue–green space
requires ﬁner resolution data to distinguish street trees, green roofs and pocket parks from
built surfaces [19].
Advancing upon the revolutionary legacy of the open-access Landsat missions [20],
the European Space Agency (ESA) and Copernicus Programme have delivered globally
consistent optical and radar data from the Sentinel satellites (10–20 m resolution) since
2014. Together with the advances in machine learning algorithms and cloud computing
platforms for Earth observation, such as Google Earth Engine (GEE) [21] and openEO [22],
the Sentinel satellites have enabled large-scale mapping of LULC at a 10 m resolution [2].
Since 2021, there have been three global Sentinel-based 10 m LULC maps released, including
Google’s Dynamic World (DW) [23], ESA’s World Cover 2020 (WC) [24] and Esri’s 2020
Land Cover (Esri) [25]. All three products have the vision of being multi-temporal, with
WC and Esri being annually updated, but only DW is operationally delivering near real-
time LULC maps as new Sentinel-2 scenes become available (every 5 days). Esri and DW
were both developed from deep learning models trained on the same reference dataset of
over 5 billion hand-annotated Sentinel-2 pixel patches from 24,000 individual image tiles
(510 × 510 pixels each) distributed over the world [23]. In contrast, WC was produced with
a random forest classiﬁcation tree algorithm trained on hand-labeled pixels in 100 × 100 m
grids at 141,000 unique locations distributed over the world [24]. WC also included both
Sentinel-1 and Sentinel-2 data as predictors in their model. Furthermore, a noteworthy
difference is that the DW and Esri reference dataset was digitized with a minimum mapping
unit (MMU) of 250 m2, while the WC reference dataset was digitized with an MMU of 100 m2.
To date, there have been no systematic evaluations of the three global 10 m LULC
products with reference to one another. Given the importance of global LULC maps
for various applications, and the large differences in the production of the recent 10 m
products, we aimed to compare DW, WC and Esri global LULC maps in terms of their
spatial correspondence with one another and their global and regional accuracy. To quantify
accuracy at the global scale, we used the hand-annotated validation dataset published
alongside DW. We supplemented this with a regional reference dataset of in situ point-based
Remote Sens. 2022, 14, 4101
3 of 19
survey data on LULC across the European Union. We discuss how spatial correspondence
and accuracy vary across LULC classes, biomes and human settlement types and explore
key limitations and advantages of the three datasets.
2. Materials and Methods
2.1. Land Cover Data Processing
Data pre-processing and extraction took place in GEE [21] and fed into our complete
workflow, as outlined in Figure 1. Data analysis and visualization were performed in R [26].
The WC and Esri global land cover datasets for 2020 are available in the GEE official and
community data catalogs. However, DW is provided as a collection of classified Sentinel-
2 images with less than 35% cloud cover, as defined by the ‘CLOUDY_PIXEL_PERCENTAGE’
scene metadata property. Each image has a ‘label’ band with a discrete classiﬁcation of
LULC, but also 9 probability bands with class-speciﬁc probability scores generated by the
deep learning model on the basis of the pixel’s spatial context. To generate an annual LULC
composite comparable with WC and Esri, we calculated the mode of the predicted LULC
class in the ‘label’ band of all DW images for 2020. We also tested annual compositing by
calculating the mean and median probability scores for all LULC classes during the year
and then classifying by taking the class with the highest probability score per pixel. We
found no difference in overall accuracy using the alternative methods (Figure S1), and,
because the mode composite on the ‘label’ band was more computationally efﬁcient, we
decided to use that global composite for further analysis. The land cover typologies were
identical for DW and Esri; however, we converted the WC typology to match DW and Esri
by aggregating four LULC classes, as outlined in Table 1. The three global 10 m LULC maps
(Figure 2) were used to assess spatial correspondence and accuracy, as outlined below.
Table 1. Classiﬁcation typology cross-walk between the three global 10 m LULC maps included in
this study.
Dynamic World
Esri LULC
World Cover
Built Area
Built Area
Built-up
Clusters of human-made structures or
individual very large human-made structures.
Contained industrial, commercial and private
building and the associated parking lots. A
mixture of residential buildings, streets,
lawns, trees, isolated residential structures or
buildings surrounded by vegetative land
cover. Major road and rail networks outside
of the predominant residential areas. Large
homogeneous impervious surfaces, including
parking structures, large office buildings and
residential housing developments containing
clusters of cul-de-sacs.
Human-made structures; major road
and rail networks; large homogenous
impervious surfaces, including parking
structures, ofﬁce buildings and
residential housing; examples: houses,
dense villages/towns/cities, paved
roads, asphalt.
Land covered by buildings, roads and
other man-made structures, such as
railroads. Buildings include both
residential and industrial buildings.
Urban green (parks, sport facilities) is not
included in this class. Waste dump
deposits and extraction sites are
considered as bare.
Crops
Crops
Cropland
Human planted/plotted cereals, grasses
and crops.
Human planted/plotted cereals,
grasses and crops not at tree height;
examples: corn, wheat, soy, fallow plots
of structured land.
Land covered with annual cropland that
is sowed/planted and harvestable at least
once within the 12 months after the
sowing/planting date. The annual
cropland produces a herbaceous cover
and is sometimes combined with some
tree or woody vegetation. Note that
perennial woody crops will be classiﬁed
as the appropriate tree cover or shrub
land cover type. Greenhouses are
considered as built-up.
Remote Sens. 2022, 14, 4101
4 of 19
Table 1. Cont.
Dynamic World
Esri LULC
World Cover
Bare ground
Bare ground
Barren/sparse vegetation
Areas of rock or soil containing very sparse
to no vegetation. Large areas of sand and
deserts with no to little vegetation. Large
individual or dense networks of dirt roads.
Areas of rock or soil with very sparse to
no vegetation for the entire year; large
areas of sand and deserts with no to
little vegetation; examples: exposed
rock or soil, desert and sand dunes, dry
salt ﬂats/pans, dried lake beds, mines.
Lands with exposed soil, sand or rocks
and never has more than 10 % vegetated
cover during any time of the year.
Moss and Lichen
Land covered with lichens and/or
mosses. Lichens are composite organisms
formed from the symbiotic association of
fungi and algae. Mosses contain
photo-autotrophic land plants without
true leaves, stems, roots but with leaf-
and stemlike organs.
Grass
Grass
Grassland
Open areas covered in homogenous grasses
with little to no taller vegetation. Other
homogenous areas of grass-like vegetation
(blade-type leaves) that appear different
from trees and shrubland. Wild cereals and
grasses with no obvious human plotting
(i.e., not a structured ﬁeld).
Open areas covered in homogenous
grasses with little to no taller vegetation;
wild cereals and grasses with no obvious
human plotting (i.e., not a plotted field);
examples: natural meadows and fields
with sparse to no tree cover, open
savanna with few to no trees, parks/golf
courses/lawns, pastures.
This class includes any geographic area
dominated by natural herbaceous plants
(plants without persistent stem or shoots
above ground and lacking deﬁnite ﬁrm
structure): (grasslands, prairies, steppes,
savannahs, pastures) with a cover of 10%
or more, irrespective of different human
and/or animal activities, such as: grazing,
selective ﬁre management, etc. Woody
plants (trees and/or shrubs) can be
present assuming their cover is less than
10%. It may also contain uncultivated
cropland areas (without harvest/bare soil
period) in the reference year.
Shrub and scrub
Shrub and scrub
Shrubland
Mix of small clusters of plants or individual
plants dispersed on a landscape that shows
exposed soil and rock. Scrub-ﬁlled
clearings within dense forests that are
clearly not taller than trees. Appear
grayer/browner due to less dense
leaf cover.
Mix of small clusters of plants or single
plants dispersed on a landscape that
shows exposed soil or rock; scrub-ﬁlled
clearings within dense forests that are
clearly not taller than trees; examples:
moderate to sparse cover of bushes,
shrubs and tufts of grass, savannas
with very sparse grasses, trees or
other plants.
This class includes any geographic area
dominated by natural shrubs having a
cover of 10% or more. Shrubs are deﬁned
as woody perennial plants with
persistent and woody stems and without
any deﬁned main stem being less than
5 m tall. Trees can be present in scattered
form if their cover is less than 10%.
Herbaceous plants can also be present at
any density. The shrub foliage can be
either evergreen or deciduous.
Trees
Trees
Trees
Any signiﬁcant clustering of dense
vegetation, typically with a closed or dense
canopy. Taller and darker than
surrounding vegetation (if surrounded by
other vegetation).
Any signiﬁcant clustering of tall
(~15 feet or higher) dense vegetation,
typically with a closed or dense canopy;
examples: wooded vegetation, clusters
of dense tall vegetation within
savannas, plantations, swamp or
mangroves (dense/tall vegetation with
ephemeral water or canopy too thick to
detect water underneath).
This class includes any geographic area
dominated by trees with a cover of 10%
or more. Other land cover classes (shrubs
and/or herbs in the understorey, built-up,
permanent water bodies . . . ) can be
present below the canopy, even with a
density higher than trees. Areas planted
with trees for afforestation purposes and
plantations (e.g., oil palm, olive trees) are
included in this class. This class also
includes tree-covered areas seasonally or
permanently ﬂooded with fresh water
except for mangroves.
Remote Sens. 2022, 14, 4101
5 of 19
Table 1. Cont.
Dynamic World
Esri LULC
World Cover
Flooded vegetation
Flooded vegetation
Herbaceous wetland
Areas of any type of vegetation with
obvious intermixing of water. Do not
assume an area is ﬂooded if ﬂooding is
observed in another image. Seasonally
ﬂooded areas that are a mix of
grass/shrub/trees/bare ground.
Areas of any type of vegetation with
obvious intermixing of water
throughout a majority of the year;
seasonally ﬂooded area that is a mix of
grass/shrub/trees/bare ground;
examples: ﬂooded mangroves,
emergent vegetation, rice paddies and
other heavily irrigated and
inundated agriculture.
Land dominated by natural herbaceous
vegetation (cover of 10% or more) that is
permanently or regularly ﬂooded by
fresh, brackish or salt water. It excludes
unvegetated sediment (see 60), swamp
forests (classiﬁed as tree cover) and
mangroves see 95).
Mangroves
Taxonomically diverse, salt-tolerant tree
and other plant species, which thrive in
intertidal zones of sheltered tropical
shores, “overwash” islands and estuaries.
Water
Water
Open water
Water is present in the image. Contains
little to no sparse vegetation, no rock
outcrop and no built-up features, such as
docks. Does not include land that can or
has previously been covered by water.
Areas where water was predominantly
present throughout the year; may not
cover areas with sporadic or ephemeral
water; contains little to no sparse
vegetation, no rock outcrop nor built up
features, such as docks; examples:
rivers, ponds, lakes, oceans, ﬂooded
salt plains.
This class includes any geographic area
covered for most of the year (more than
9 months) by water bodies: lakes,
reservoirs and rivers. Can be either fresh-
or salt-water bodies. In some cases, the
water can be frozen for part of the year
(less than 9 months).
Snow and ice
Snow and ice
Snow and ice
Large homogenous areas of thick snow or
ice, typically only in mountain areas or
highest latitudes. Large homogenous areas
of snowfall.
Large homogenous areas of permanent
snow or ice, typically only in mountain
areas or highest latitudes; examples:
glaciers, permanent snowpack,
snow ﬁelds.
This class includes any geographic area
covered by snow or glaciers persistently.
Remote Sens. 2022, 14, 4101 
6 of 21 
 
 
grass/shrub/trees/bare 
ground. 
mangroves, emergent vege-
tation, rice paddies and other 
heavily irrigated and inun-
dated agriculture. 
swamp forests (classified as 
tree cover) and mangroves 
see 95). 
Mangroves 
Taxonomically diverse, salt-
tolerant tree and other plant 
species, which thrive in inter-
tidal zones of sheltered tropi-
cal shores, “overwash” is-
lands and estuaries. 
Water 
Water 
Open water 
Water is present in the im-
age. Contains little to no 
sparse vegetation, no rock 
outcrop and no built-up fea-
tures, such as docks. Does 
not include land that can or 
has previously been covered 
by water. 
Areas where water was pre-
dominantly present through-
out the year; may not cover 
areas with sporadic or 
ephemeral water; contains 
little to no sparse vegetation, 
no rock outcrop nor built up 
features, such as docks; ex-
amples: rivers, ponds, lakes, 
oceans, flooded salt plains. 
This class includes any geo-
graphic area covered for 
most of the year (more than 9 
months) by water bodies: 
lakes, reservoirs and rivers. 
Can be either fresh- or salt-
water bodies. In some cases, 
the water can be frozen for 
part of the year (less than 9 
months).  
Snow and ice 
Snow and ice 
Snow and ice 
Large homogenous areas of 
thick snow or ice, typically 
only in mountain areas or 
highest latitudes. Large ho-
mogenous areas of snowfall. 
Large homogenous areas of 
permanent snow or ice, typi-
cally only in mountain areas 
or highest latitudes; exam-
ples: glaciers, permanent 
snowpack, snow fields. 
This class includes any geo-
graphic area covered by 
snow or glaciers persistently. 
 
Figure 1. Flowchart of the methods followed in the cross-comparison and accuracy assessment of 
global 10 m LULC maps. 
Figure 1. Flowchart of the methods followed in the cross-comparison and accuracy assessment of
global 10 m LULC maps.
Remote Sens. 2022, 14, 4101
6 of 19
Remote Sens. 2022, 14, 4101 
7 of 21 
 
 
 
Figure 2. Maps of global 10 m resolution land cover maps, including Dynamic World (A), World 
Cover (B) and Esri Land Cover (C). Inset maps show a zoomed extent of a landscape in South Africa 
and Brazil, indicated with black dots on the world maps to illustrate the spatial grain of the maps at 
a local scale. White areas in the Arctic and Antarctic in (A) and (B), although partly mapped in (C), 
were not included in the analysis. 
2.2. Spatial Correspondence Assessment 
To assess how strongly the global LULC products corresponded to one another over 
space, we quantified and compared class-wise LULC area sums that were aggregated to 
an equal-area hexagonal grid (70,000 km2), which covered the globe. The size of the grid 
was chosen based on a trade-off between computation time and precision of area 
Figure 2. Maps of global 10 m resolution land cover maps, including Dynamic World (A), World
Cover (B) and Esri Land Cover (C). Inset maps show a zoomed extent of a landscape in South Africa
and Brazil, indicated with black dots on the world maps to illustrate the spatial grain of the maps at a
local scale. White areas in the Arctic and Antarctic in (A,B), although partly mapped in (C), were not
included in the analysis.
2.2. Spatial Correspondence Assessment
To assess how strongly the global LULC products corresponded to one another over
space, we quantiﬁed and compared class-wise LULC area sums that were aggregated to an
equal-area hexagonal grid (70,000 km2), which covered the globe. The size of the grid was
chosen based on a trade-off between computation time and precision of area aggregation.
Pixels were aggregated over mutually overlapping extents and image masks across the
Remote Sens. 2022, 14, 4101
7 of 19
three LULC products. To quantify correspondence between products, for each LULC class
and unique hexagonal grid cell, we calculate the proportional share of each product’s
area estimation. Perfect correspondence resulted in a proportional ratio of 0.33:0.33:0.33
or 33% proportional share for each product. We deﬁned strong correspondence when no
single product’s proportional share of the LULC class area exceeded 40% in a grid cell.
Therefore, weak correspondence was when the maximum difference between the product
with the biggest and smallest proportional share was greater than 20%. We visualized
these proportional shares for each hexagon over the globe by assigning a color code along
a tri-color gradient using the tricolore (v1.2.2.) package in R. The relative abundance of
the given LULC class (average of the three products’ area estimates) was mapped to the
opacity of each hexagon grid so that areas where the LULC is abundant appear opaque
and those where the LULC is less abundant are transparent.
2.3. Accuracy Assessment
To quantify the accuracy of the three LULC products, we used two sources of open-
access reference data (Figure 3). The ﬁrst source was from the ground truth validation
dataset produced by the Dynamic World team, which included a group of annotators
(manual labeling of LULC types using visual interpretation of high-resolution reference
imagery) supported by the National Geographic Society in partnership with Google
and the World Resources Institute [23]. This team consisted of 25 expert and 45 non-
expert annotators who together annotated approximately 24,000 individual image tiles of
510 × 510 pixels from Sentinel-2 imagery from random dates in 2019. Annotators followed
the typology deﬁnitions outlined in the ﬁrst column of Table 1 and were instructed to
consider an MMU of 250 m2, which, by deﬁnition, included mosaics of distinct landscape
elements within; for example, buildings, trees and grass within a 250 m2 area might be
labeled as “built area”. From the annotated dataset, a stratiﬁed random subsample of
409 Sentinel-2 tiles were withheld from the training of the DW deep learning model and
used for validation. The validation tiles included expert consensus labels where all three
experts agreed, or where two experts agreed and the third had no opinion or where
one expert had an opinion and the other two did not. We used this dataset constituting
72 million distinct 10 × 10 m pixels for global accuracy assessment of the three LULC
products (Figure 3A). WC used a completely different reference dataset for training and
validation of their model, which is not open-access, and, thus, we could not use it in the
present analysis. Esri used the same reference dataset as DW to train and validate their
LULC model; however, the sub-set of tiles they used for validating their map was not
open-access, and, therefore, we cannot be sure that the 409 validation tiles we use here
were in fact independent from the dataset used to train the Esri LULC model.
Figure 3. Distribution of the global (A) and regional (B) reference data locations. Locations in (A)
represent Sentinel-2 image tiles of 510 × 510 pixels, which were manually annotated ex situ. Locations
in (B) are grid points sampled in situ in the LUCAS area survey. Inset bar plot shows the number of
reference samples per LULC class.
Remote Sens. 2022, 14, 4101
8 of 19
The second source of reference data was a regional dataset from the Land Use/Cover
Area Frame Survey (LUCAS) over the European Union (Figure 3B). LUCAS is a systematic
grid of 337,845 points that are visited triennially for the collection of in situ land cover and
land use data [27]. In contrast to the DW ground truth dataset described above, LUCAS
surveyors are instructed to record the land cover within a 1.5 m circle at each point in
the grid, and, therefore, when applied to Earth observation, it consists of a signiﬁcantly
smaller MMU. We used the 2018 LUCAS dataset with the ﬁrst-level classiﬁcation with the
exception of removing “G50: glaciers and permanent snow” from the water category into
its own category to match the “snow and ice” category in the global LULC maps (Table 2).
Table 2. Classiﬁcation typology of the LUCAS data used for regional (European) validation of global
LULC maps in this study.
LULC Class
LUCAS Classes Used and Descriptions
Built area
Artiﬁcial land (A00): Areas characterized by an artiﬁcial and often
impervious cover of constructions and pavement. Includes roofed
built-up areas and non-built-up area features, such as parking lots and
yards. Excludes non-built-up linear features, such as roads, and other
artiﬁcial areas, such as bridges and viaducts, mobile homes, solar
panels, power plants, electrical substations, pipelines, water sewage
plants and open dump sites.
Cropland
Cropland (B00): Areas where seasonal or perennial crops are planted
and cultivated, including cereals, root crops, non-permanent industrial
crops, dry pulses, vegetables, ﬂowers, fodder crops, fruit trees and
other permanent crops. Excludes temporary grasslands, which are
artiﬁcial pastures that may only be planted for one year.
Bare ground
Bare land and lichens/moss (F00): Areas with no dominant vegetation
cover on at least 90% of the area or areas covered by lichens/moss.
Excludes other bare soil, which includes bare arable land, temporarily
unstocked areas within forests, burnt areas, secondary land cover for
tracks and parking areas/yards.
Grass
Grassland (E00): Land predominantly covered by communities of
grassland, grass-like plants and forbs. This class includes permanent
grassland and permanent pasture that is not part of a crop rotation
(normally for 5 years or more). It may include sparsely occurring trees
within a limit of a canopy below 10% and shrubs within a total limit of
cover (including trees) of 20%. May include: dry grasslands, dry
edaphic meadows, steppes with gramineae and artemisia, plain and
mountainous grassland, wet grasslands, alpine and subalpine
grasslands, saline grasslands, arctic meadows, set aside land within
agricultural areas (including unused land where revegetation is
occurring) and clear cuts within previously existing forests. Excludes
spontaneously re-vegetated surfaces consisting of agricultural land that
has not been cultivated this year or the years before, clear-cut forest
areas, industrial “brownﬁelds” and storage land.
Shrub and scrub
Shrubland (D00): Areas dominated (at least 10% of the surface) by
shrubs and low woody plants normally not able to reach >5 m of
height. It may include sparsely occurring trees with a canopy below
10%. Excludes berries, vineyards and orchards.
Trees
Woodland (C00): Areas with a tree canopy cover of at least 10%,
including woody hedges and palm trees. Includes a range of coniferous
and deciduous forest types. Excludes forest tree nurseries, young
plantations or natural stands (<10% canopy cover) dominated by
shrubs or grass.
Flooded vegetation
Wetlands (H00): Wetlands located inland and having fresh water and
wetlands located on marine coasts or having salty or brackish water as
well as areas of a marine origin.
Remote Sens. 2022, 14, 4101
9 of 19
Table 2. Cont.
LULC Class
LUCAS Classes Used and Descriptions
Water
Water areas (G10 to G40): Inland or coastal areas without vegetation
and covered by water and ﬂooded surfaces, or likely to be so over a
large part of the year.
Snow and ice
Glaciers, permanent snow (G50): Areas covered by glaciers (generally
measured at the time of their greatest expansion in the season) or
permanent snow.
The 2020 LULC predictions for each global product were sampled over the annotated
image pixels (global validation set) and survey locations (regional validation set). Accuracy
was quantiﬁed globally/regionally, but also stratiﬁed by biome, settlement type (urban,
rural and uninhabited) and continent. Biomes were deﬁned using the RESOLVE biore-
gions dataset [28], while human settlement type was derived from the Global Human
Settlement Layers, Settlement Grid [29]. We constructed confusion matrices for each LULC
product to calculate class-speciﬁc user’s/precision and producer’s/recall accuracy and
overall accuracies.
2.4. Implementation Details
Data were summarized and/or extracted from GEE using the JavaScript API. The data
were summarized by a global grid created using the dgggridR (v2.0.3) package. The ﬁgures
were all created in R (v4.2.1) using the tricolore (v1.2.2.), ggplot2 (v3.3.5), ggmap (v3.0.0), sf
(v1.0.9) and raster (v3.5) packages.
3. Results
3.1. Spatial Correspondence
The area estimates for built area, crops, trees and water showed strong correspondence
between the three global LULC products, particularly in areas with the greatest relative
abundance of the given LULC class (i.e., gray areas with bordered grid cells in Figure 4).
However, for some regions, there were discrepancies between products; for example, DW
over-estimated the crop cover in the western USA, Kazakhstan and Mongolia relative to
the other LULC products.
The LULC classes with the lowest correspondence between global products were bare
ground, grass, scrub and shrub (Figure 4). DW estimated higher proportions of bare ground
in mid- to lower latitudes, whereas WC estimated more bare ground in higher latitudes
(Figure 4C). WC consistently estimated greater grass cover than DW and Esri across most
of the world, except for over the taiga in Russia (Figure 4D). Conversely, the Esri product
estimated greater shrub and scrub cover across the world except for a small section in
Canada and the savanna-forest ecotone in central Africa (Figure 4E).
The LULC classes ﬂooded vegetation, and snow and ice exhibited notable disagree-
ments between products for the northern latitudes (Figure 4G,I). Esri estimated the highest
proportions of ﬂooded vegetation over North America, whereas WC estimated the high-
est proportions over Russia. DW estimated signiﬁcantly more snow and ice cover than
both WC and Esri over the whole of the Northern Hemisphere, apart from areas that are
permanently snow-covered (e.g., Greenland) or snow-free (e.g., Sahara desert) (Figure 4I).
3.2. Accuracy
Using the global ground truth dataset with a minimum mapping unit of 250 m2, we
found that Esri had the highest overall accuracy (75%) compared to DW (72%) and WC
(65%; Table 3; Figure 5). Across all the LULC products, water was consistently the most
accurately mapped class (balanced accuracy 92%; mean of precision and recall Figure 5),
followed by built area (83%), trees (81%) and crops (78%). In contrast, bare ground (57%),
grass (34%), shrub and scrub (47%) and ﬂooded vegetation (53%) were mapped with the
Remote Sens. 2022, 14, 4101
10 of 19
lowest accuracies (Figure 5). The accuracies across all the LULC products were generally
lowest in the tundra biome, where grass, bare ground, shrub and scrub and crops were
mis-classiﬁed and had the lowest recall and precision accuracies. The accuracies were
the highest in temperate and boreal forests, where crops, trees and built area had the
highest accuracies. There was very little difference in overall accuracy between urban,
rural and uninhabited areas, with the exception that trees had lower precision in urban
areas compared to rural and uninhabited areas, particularly for WC (40% lower) and DW
(20% lower). Differences in accuracy between the continents were small; however, when
averaged across LULC products, the accuracies were highest in North America and lowest
in Africa (Figure 6).
Figure 4. Spatial correspondence between global 10 m land cover maps for each of 9 land cover
classes. The proportion of land cover within each hexagonal grid cell is calculated for each LULC
product and then visualized along a tri-color gradient illustrating the proportional share of areas
estimated by each LULC product. Gray areas indicate strong correspondence, whereas colored areas
reﬂect dominance of one LULC product. Gric cells outlined in black indicate strong correspondence,
deﬁned as cases where no single LULC product has more than 40% share of the combined area within
the grid cell. The opacity of the grid cells indicates the absolute percentage abundance of each LULC
class averaged over the three LULC classes. Opaque areas have near-maximum percentage cover for
that LULC class, whereas transparent areas have very low percentage cover.
Table 3. Summary of overall accuracies quantiﬁed at the global and regional scale.
Accuracy
Dynamic World
Esri
World Cover
Global validation
72%
75%
65%
Regional validation (European)
66%
63%
71%
Using the regional ground truth dataset (LUCAS) across Europe with an MMU of
<100 m2, we found that the order of product accuracies was reversed compared to the
global validation (Table 3; Figure 7). WC exhibited the highest accuracy (71%) compared
to DW (66%) and Esri (63%). WC was particularly more accurate relative to DW and Esri
in temporal and boreal forests and savanna biomes. Similar to the result from the global
validation, the differences in accuracy between human settlement types were minimal.
Furthermore, the relative differences in accuracy between LULC products were consistent
Remote Sens. 2022, 14, 4101
11 of 19
across human settlement types. It should be noted that, for the European validation data,
there were very few data points for the bare ground and snow and ice LULC classes, which
may bias accuracy estimates signiﬁcantly.
 
Using the regional ground truth dataset (LUCAS) across Europe with an MMU of 
<100 m2, we found that the order of product accuracies was reversed compared to the 
global validation (Table 3; Figure 7). WC exhibited the highest accuracy (71%) compared 
to DW (66%) and Esri (63%). WC was particularly more accurate relative to DW and Esri 
in temporal and boreal forests and savanna biomes. Similar to the result from the global 
validation, the differences in accuracy between human settlement types were minimal. 
Furthermore, the relative differences in accuracy between LULC products were consistent 
across human settlement types. It should be noted that, for the European validation data, 
there were very few data points for the bare ground and snow and ice LULC classes, 
which may bias accuracy estimates significantly. 
 
Figure 5. Accuracy of global 10 m land cover maps across LULC classes, biomes and human settle-
ment type based on hand-annotated image tiles with minimum mapping unit of 50 × 50 m. Accuracy 
is expressed as precision/user’s, recall/producer’s and overall accuracy based on a confusion matrix, 
with sample sizes indicated in millions (MM) in parentheses on the y-axis. 
 
 
Figure 5. Accuracy of global 10 m land cover maps across LULC classes, biomes and human settle-
ment type based on hand-annotated image tiles with minimum mapping unit of 50 × 50 m. Accuracy
is expressed as precision/user’s, recall/producer’s and overall accuracy based on a confusion matrix,
with sample sizes indicated in millions (MM) in parentheses on the y-axis.
Remote Sens. 2022, 14, 4101 
13 of 21 
 
 
Figure 6. Accuracy of global 10 m land cover maps across LULC classes and continents based on 
hand-annotated image tiles with MMU unit of 50 × 50 m. Circles show users, producers and overall 
accuracy. Accuracy is expressed as precision/user’s, recall/producer’s and overall accuracy based 
on a confusion matrix, with sample sizes indicated in millions (MM) in parentheses on the y-axis. 
Figure 6. Accuracy of global 10 m land cover maps across LULC classes and continents based on
hand-annotated image tiles with MMU unit of 50 × 50 m. Circles show users, producers and overall
accuracy. Accuracy is expressed as precision/user’s, recall/producer’s and overall accuracy based on
a confusion matrix, with sample sizes indicated in millions (MM) in parentheses on the y-axis.
Remote Sens. 2022, 14, 4101
12 of 19
 
 
Figure 6. Accuracy of global 10 m land cover maps across LULC classes and continents based on 
hand-annotated image tiles with MMU unit of 50 × 50 m. Circles show users, producers and overall 
accuracy. Accuracy is expressed as precision/user’s, recall/producer’s and overall accuracy based 
on a confusion matrix, with sample sizes indicated in millions (MM) in parentheses on the y-axis. 
 
Figure 7. Accuracy of global 10 m land cover maps over Europe using point-based ground truth 
data from the LUCAS survey. Accuracy is stratified by LULC class, ecoregion and human settlement 
type. Accuracy is expressed as precision/user’s, recall/producer’s and overall accuracy based on a 
confusion matrix, with sample sizes indicated in thousands (K) in parentheses on the y-axis. 
4. Discussion 
4.1. Explaining the Differences between Global LULC Products 
The production of global LULC maps is inherently difficult due to the extensive bio-
geographical variations within and across biomes that lead to diverse spectral signatures 
within a single LULC class [30]. In this sense, it is not surprising that three global LULC 
maps produced by three independent groups have large differences in accuracy and 
Figure 7. Accuracy of global 10 m land cover maps over Europe using point-based ground truth
data from the LUCAS survey. Accuracy is stratiﬁed by LULC class, ecoregion and human settlement
type. Accuracy is expressed as precision/user’s, recall/producer’s and overall accuracy based on a
confusion matrix, with sample sizes indicated in thousands (K) in parentheses on the y-axis.
4. Discussion
4.1. Explaining the Differences between Global LULC Products
The production of global LULC maps is inherently difﬁcult due to the extensive
biogeographical variations within and across biomes that lead to diverse spectral signatures
within a single LULC class [30]. In this sense, it is not surprising that three global LULC
maps produced by three independent groups have large differences in accuracy and spatial
correspondence. Below, we attempt to explain some of the main differences as presented in
our results.
4.1.1. Minimum Mapping Unit
The global LULC maps had contrasting accuracies when validated against the global
versus regional reference datasets (Figure 5 versus Figure 7). While Esri and DW were
most accurate at the global scale, WC was most accurate at the European scale. Apart from
the spatial extent, the most important difference between the reference datasets used to
validate the LULC maps is the MMU used to collect ground truth information. The global
validation dataset was digitized ex situ (i.e., based on visual interpretation of satellite
imagery) using an MMU of 250 m2 (i.e., 50 × 50 m square), whereas the regional validation
dataset (LUCAS) was collected in situ for point circles with a radius of 1.5 m (MMU <100 m).
We suspect that the main reason Esri and DW had higher accuracies than WC at the global
scale was because Esri and DW were produced from models trained on reference data
with an MMU unit of 250 m2, while WC was trained on data with an MMU of 100 m2. We
also posit that this difference in MMU is partially responsible for the difference in LULC
classiﬁcation granularity at the landscape scale. For instance, the inset maps in Figure 2
reveal how Esri and DW predictions are more clustered and generalized compared to WC,
which exhibits more of the ‘salt and pepper’ characteristic of pixel-based classiﬁcation
techniques. Urban gardens and trees are incorporated into the “built area” cluster of pixels
in DW and Esri maps, while they are labeled as “grass” or “trees” in WC. This illustrates
Remote Sens. 2022, 14, 4101
13 of 19
why WC is 10% more accurate than Esri and DW when classifying built areas across Europe
using the point-based LUCAS dataset as a reference (Figure 7).
4.1.2. Classiﬁcation Typology
Another factor leading to differences between global LULC products is the classiﬁca-
tion typology used. For the purposes of cross-comparison, we harmonized all three LULC
products to a nine-class typology (Table 1). Even though this is a much simpler typology
than other regional LULC datasets (e.g., CORINE land cover [31]), there remain signiﬁcant
challenges in distinguishing spectrally similar classes, such as bare ground, grass, shrub
and scrub, a ﬁnding often echoed in the literature [16,32,33]. These classes are not only
difﬁcult for satellite-based machine learning models to distinguish but also for human
annotators using aerial or satellite imagery for visual interpretation. For instance, there
was large disagreement between expert and non-expert labelers involved in developing the
training dataset for DW and Esri [23]. A pixel-based comparison of expert and non-expert
annotations revealed a recall rate (producer’s accuracy) as low as 22% for grass and 31% for
bare ground. Although WC did not publish a similar uncertainty assessment of its annota-
tion team, it is reasonable to assume their reference dataset suffered from the same error.
It is known that, even with in situ LULC labeling (i.e., ﬁeld-based ground truth), similar
errors due to sampler bias may exist. For instance, the European Environment Agency
discovered that CORINE-2000 accuracy was boosted by 6.4% following post-screening and
cleaning of erroneous LULC labels in the LUCAS dataset [34]. This partly explains why
the spatial correspondence between global LULC products and class-level accuracy was
generally poorest for bare ground, grass and shrub and scrub cover (Figures 4 and 5).
Another source of discrepancy between LULC products is the slight difference in
LULC deﬁnitions for certain classes (see descriptions in Table 1). For instance, the ﬂooded
vegetation class in DW and Esri includes rice paddies and irrigated/inundated agricul-
ture. In WC, this type of cropland is included in the cropland class. Furthermore, the
DW reference dataset deﬁnes the shrub and scrub class relatively broadly as clusters of
plants that are dispersed over an area without any speciﬁcation of cover percentage (“ . . .
moderate to sparse cover of bushes, shrubs, and tufts of grass”) [23]. This is different to the
WC typology, where grass, bare ground and shrub/scrub are deﬁned using speciﬁc cover
percentages (“ . . . shrubs having a cover of 10% or more. Shrubs are deﬁned as woody
perennial plants with persistent and woody stems and without any deﬁned main stem
being less than 5 m tall”) [24].
4.1.3. Modeling and Validation Methods
At the global scale, the accuracies of DW, WC and Esri were different to one another
(Figure 5) and also different to the independent accuracies reported by the data providers
themselves: DW 72% versus reported 74%; WC 65% versus reported 74%; Esri 75% versus
85%. The discrepancy for DW is likely due to the fact that we aggregated LULC predictions
to an annual composite for 2020 (using the mode of the ‘label’ band), whereas the DW
validation report used scene-level model predictions for Sentinel-2 tiles in 2019. The WC
dataset was produced and validated with a completely different reference dataset with a
different MMU to the DW validation tiles used here. The metadata published with the Esri
dataset is incomplete, and, thus, we do not know which reference data were withheld from
model training and whether the 409 validation tiles used here were indeed independent
from the Esri training dataset. If it is the case that the validation data used here had been
part of the data used to train the Esri deep learning model, then it is possible that our
estimates of the Esri map accuracy are overly optimistic.
The DW and Esri maps were produced using a deep learning model, whereas WC
was produced using a random forest classiﬁcation. This difference in modeling framework
likely explains some of the difference in accuracy between LULC products. Deep learning
models, such as the fully convolutional neural network employed in DW, take the pixel
context into account when making inferences, whereas random forest does not. This,
Remote Sens. 2022, 14, 4101
14 of 19
together with the difference in MMU, explains why the DW and Esri maps are clustered
and generalized at a landscape scale (inset maps Figure 2).
Finally, we acknowledge that we are using reference data from 2019 (global) and 2018
(European) that do not align with the year of the LULC maps (2020) and that this may lead
to discrepancies between the validation set and the reality on the ground in 2020. However,
we suspect these changes to be minimal, and any bias introduced should be consistent
across global LULC products. Furthermore, even the WC reference dataset, which is not
open-access, faces the same limitation as it was collected for the year 2015 and used to
create and validate the WC map for the year 2020.
4.2. Recommendations for Users
The cross-comparison results presented here indicate that there is no “one-size-ﬁts-all”
when it comes to global LULC maps and their potential application. We ﬁnd differences in
accuracy across spatial scales (global versus regional), LULC classes, continents, biomes
and urban settlement types. Therefore, an overall recommendation is to carefully evaluate
the global LULC products with respect to the aforementioned factors and how they relate
to the application requirements. We note that it is also possible to use all three LULC
products in combination by creating some form of majority vote or weighted average.
Nevertheless, we make some general recommendations for users of either DW, Esri or WC
or a combination thereof:
•
Regardless of LULC product, users should implement design-based inference when
calculating LULC areas or changes and avoid drawing conclusions from simple ‘pixel-
counting’, which leads to biased area estimates [35]. Design-based inference involves
generating a post-classiﬁcation reference (validation or “ground truth”) sample that is
implemented with a probability sampling design (e.g., simple random or stratiﬁed
random), which can be used to quantify unbiased area and accuracy estimates.
•
In general, users can rely on water, built area, trees and crops being mapped with the
highest accuracy, while shrub and scrub, grass, bare ground and ﬂooded vegetation
have the lowest accuracies. With this in mind, it may be beneﬁcial to simplify the
LULC typology by merging classes with low accuracies into aggregate classes if your
use-case allows it.
•
Users should be aware of the biases in global LULC products (reported relative to
one another). Speciﬁcally, WC is biased toward estimating greater grass cover, Esri
towards shrub and scrub cover and DW towards snow and ice.
•
LULC classiﬁcation accuracy varies by biome, continent and urban settlement type,
and, therefore, users may consult Figures 4–6 here to gather information on what to
expect given the local context of their work.
•
WC is most appropriate when considering an MMU of <100 m2 or when a user wants
to resolve smaller landscape elements. For example, WC is advantageous in urban
areas and complex agricultural landscapes with small or thin vegetation structures,
such as trees or hedge rows.
•
Based on our supplementary analysis of the DW compositing method (Figure S1), we
ﬁnd that the type of temporal aggregation of DW predictions has very little effect on
global and regional accuracy. However, we note that changing the seasonal extent of
temporal aggregation (e.g., growing-season composite) may have signiﬁcant effects
on accuracy (although we did not test this here).
•
The delivery format of DW in near real-time, covering the entire Sentinel-2 image
collection and including LULC class-speciﬁc probability scores, is qualitatively unique
from Esri and WC, which only produce annual LULC maps without probability scores.
We encourage users to take advantage of this unique aspect to DW by exploring novel
possibilities discussed in the section below.
Remote Sens. 2022, 14, 4101
15 of 19
4.3. Potential for Future Research
Although our analysis provides important information about global model accuracies,
it remains to be seen which LULC product provides area change estimates with the least
amount of error as measured using design-based inference for area estimation [35,36].
Quantifying conﬁdence intervals around area estimates or area change estimates is nec-
essary for the adoption of LULC products in policy mechanisms, such as the REDD+ [13]
reporting on deforestation or the SEEA ecosystem accounting [14]. It is, therefore, of interest
to test DW, Esri and WC (as well as other multi-temporal medium resolution maps (e.g.,
Friedl et al., 2022 [37]) in terms of how accurately they estimate changes in LULC at various
spatial and temporal scales. However, design-based inference can be time-consuming and
costly. A requirement for design-based inference is an explicitly speciﬁed population, and,
in the context of evaluating map accuracy, this population often refers to a population of
pixels included in a map and given different class labels. This population varies spatially
and temporally depending on the project scope, and, therefore, a probability-based sample
needs to be generated for making an inference each time the project scope changes. The
probability scores provided by DW may allow for more efﬁcient alternatives to design-
based inference. Sales et al., 2021 [38] have shown that averaging class probability scores
from a random forest model can give area estimates that are substantially less biased than
‘pixel-counting’ and almost as precise as design-based methods. The same might be true of
DW class probability scores. Furthermore, there is also scope to use DW probability scores
to estimate per-pixel classiﬁcation accuracy [39,40].
The probability scores available in the DW product provide scope for several other
avenues of further research and tailored LULC classiﬁcation. Firstly, they can be aggregated
over time frames relevant to the application task. For example, annual composites might
be appropriate for some tasks, while summer or winter mosaics might be appropriate for
others. Secondly, users can apply custom thresholds or more complex decision frameworks
to the predicted probabilities in order to derive continuous or discrete LULC outputs
(Figure 8C). For example, this type of thresholding can be used to generate surface water
extent data much more frequently than the global surface water (GSW) dataset without
any update lags (currently up to 18 months). Thirdly, users can train their own local
machine learning models using DW probability scores and custom reference data as the
input (Figure 8D). This may allow for changing the LULC typology or resolving ﬁner-scale
landscape elements that are not present in the default DW ‘label’ band. Users that explore
custom models, such as DNN architectures, may also try different loss functions that result
in predictions that are less clustered and able to resolve ﬁner details. For example, Lang
et al., 2022 [41] use GEDI reference data at 25 m but produce a tree canopy height model at a
10 m resolution. Fourthly, frequency and near real-time availability of the class probability
scores allow for application of advanced time series analysis, such as LandTrendr (Landsat-
based detection of trends in disturbance and recovery) and CCDC (continuous change
detection and classiﬁcation) [42].
There are several spheres of application for which 10 m LULC maps are particularly
well-suited. Urban ecology and climate science is one such area because landscape compo-
sition and structure often manifest at scales of 10 m or less. In an increasingly urbanizing
planet, it is important to accurately monitor global and regional urban extent for planning
purposes. How these newer-generation datasets compare to existing estimates across re-
gions is still largely unknown [43]. There is also potential for using DW to quantify seasonal
land cover dynamics (e.g., including urban vegetation phenology [44]) within cities, which
is not possible with static annual LULC maps. The spatial compositions of LULC and LULC
changes are important for modeling landscape connectivity and biodiversity community
dynamics [45]. Urban ecology may, therefore, beneﬁt from comparing the landscape metrics
that result from global 10 m LULC maps.
Remote Sens. 2022, 14, 4101
16 of 19
 
explore custom models, such as DNN architectures, may also try different loss functions 
that result in predictions that are less clustered and able to resolve finer details. For exam-
ple, Lang et al., 2022 [41] use GEDI reference data at 25 m but produce a tree canopy height 
model at a 10 m resolution. Fourthly, frequency and near real-time availability of the class 
probability scores allow for application of advanced time series analysis, such as Land-
Trendr (Landsat-based detection of trends in disturbance and recovery) and CCDC (con-
tinuous change detection and classification) [42]. 
 
Figure 8. Example of the flexibility inherent in the Dynamic World data format, which includes 
multi-temporal class probability estimates. An urban landscape in Prague, Czechia (A) along with 
an annual mode composite from Dynamic World (B). Dynamic World class probabilities for the tree 
class are rescaled to highlight intra-urban tree cover (C). The predictions from a random forest 
model trained on LUCAS reference data and Dynamic World class probabilities are shown in (D), 
illustrating the possibility of resolving smaller landscape elements. 
There are several spheres of application for which 10 m LULC maps are particularly 
well-suited. Urban ecology and climate science is one such area because landscape com-
position and structure often manifest at scales of 10 m or less. In an increasingly urbaniz-
ing planet, it is important to accurately monitor global and regional urban extent for 
Figure 8. Example of the ﬂexibility inherent in the Dynamic World data format, which includes
multi-temporal class probability estimates. An urban landscape in Prague, Czechia (A) along with
an annual mode composite from Dynamic World (B). Dynamic World class probabilities for the tree
class are rescaled to highlight intra-urban tree cover (C). The predictions from a random forest model
trained on LUCAS reference data and Dynamic World class probabilities are shown in (D), illustrating
the possibility of resolving smaller landscape elements.
Ecosystem accounting is another application where multi-temporal 10 m LULC maps
may prove particularly useful. Many have highlighted the need for remote-sensing-based
approaches to implement ecosystem extent-condition accounting in ecosystem services
mapping for ecosystem accounts [46]. A challenge for ecosystem accounting has been
that biophysical mapping of ecosystem services at a national level has relied on discrete
categories of land cover types typical of traditional LULC maps. Therefore, ecosystem
service models have not been sensitive to transitional or successional changes from one
LULC type to another typical of ecological gradients or continuums [47]. It is possible
that DW probability scores can detect continuous gradients in ecosystem accounting and
that LULC probability can be thought of as a higher-level structural composition indicator
within the ecosystem condition typology. Area-weighted probabilities have been computed
for biodiversity indices but typically at much coarser spatial levels [48]. It is possible that
LULC class probabilities could be applied to some standard ecosystem services (required
Remote Sens. 2022, 14, 4101
17 of 19
by EUROSTAT) in decreasing order of accuracy, including (i) biomass, carbon storage and
sequestration, air ﬁltration, run-off control; (ii) somewhat less for local climate regulation
and landscape aesthetics/greenview exposure and (iii) even less for crop pollination and
recreation potential. Therefore, class probability scores could allow a merging of the extent-
condition account for the purpose of more accurate ecosystem service computation (i.e.,
reducing the information loss in using threshold levels for classifying LULC types for the
extent account on its own—moving from Figure 8C to Figure 8D).
5. Conclusions
LULC mapping at global extents has been revolutionized by the plethora of medium-
resolution satellite data available from programs such as Landsat and Sentinel. In our
cross-comparison of global 10 m resolution LULC maps, we found large inaccuracies and
spatial and thematic biases in each product that vary across biomes, continents and human
settlement types. Our overarching recommendation is to critically evaluate each LULC
product with reference to the application purpose. We highlight the novelty of DW as a
global near real-time LULC product with class probability scores. LULC types, regardless
of deﬁnition and type system, share with ecosystems the property that their composition,
structure and processes often vary in a gradual, continuous manner over space and time.
We suggest that the DW probability scores offer a fundamental shift in land cover mapping
from categorical to continuum concepts.
Supplementary Materials: The following supporting information can be downloaded at: https:
//www.mdpi.com/article/10.3390/rs14164101/s1, Figure S1: Accuracy of three types of annual
compositing of Dynamic World land cover maps at the global and regional extent. Compositing
methods include a model reducer on the label band, and a mean and median reducer on the class
probabilities, followed by a highest probability vote classiﬁcation. Global validation is based on hand-
annotated image tiles with a minimum mapping unit of 50 × 50 m. European validation is based on
point-based ground truth data from the LUCAS survey. Accuracy is expressed as precision/user’s,
recall/producer’s and overall accuracy based on a confusion matrix with sample sizes indicated in
thousands (K) in parenthesis on the y-axis.
Author Contributions: Conceptualization, Z.S.V.; methodology, Z.S.V. and G.S.; formal analysis,
Z.S.V.; writing—original draft preparation, Z.S.V.; writing—review and editing, D.N.B., T.S., T.C.
and G.S.; funding acquisition, D.N.B. All authors have read and agreed to the published version of
the manuscript.
Funding: This paper was funded by Norwegian Research Council grant number 320042 and the APC
was funded by Norwegian Research Council.
Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Data Availability Statement: The code and data to reproduce this analysis have been archived here:
https://github.com/NINAnor/GlobalLULCcompare, accessed on 21 July 2022.
Acknowledgments: This paper was supported by the EcoGaps project funded by Norwegian
Research Council grant number 320042. The Paciﬁc Northwest National Laboratory (PNNL) is
operated for DOE by Battelle Memorial Institute under contract DE-AC05-76RLO1830.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
References
1.
Chaves, M.E.D.; Picoli, M.C.A.; Sanches, I.D. Recent Applications of Landsat 8/OLI and Sentinel-2/MSI for Land Use and Land
Cover Mapping: A Systematic Review. Remote Sens. 2020, 12, 3062. [CrossRef]
2.
Phiri, D.; Simwanda, M.; Salekin, S.; Nyirenda, V.R.; Murayama, Y.; Ranagalage, M. Sentinel-2 Data for Land Cover/Use Mapping:
A Review. Remote Sens. 2020, 12, 2291. [CrossRef]
3.
Liu, L.; Zhang, X.; Gao, Y.; Chen, X.; Shuai, X.; Mi, J. Finer-Resolution Mapping of Global Land Cover: Recent Developments,
Consistency Analysis, and Prospects. J. Remote Sens. 2021, 2021, 5289697. [CrossRef]
Remote Sens. 2022, 14, 4101
18 of 19
4.
Kavvada, A.; Metternicht, G.; Kerblat, F.; Mudau, N.; Haldorson, M.; Laldaparsad, S.; Friedl, L.; Held, A.; Chuvieco, E. Towards
Delivering on the Sustainable Development Goals Using Earth Observations. Remote Sens. Environ. 2020, 247, 111930. [CrossRef]
5.
Lawrence, P.J.; Chase, T.N. Representing a New MODIS Consistent Land Surface in the Community Land Model (CLM 3.0). J.
Geophys. Res. Biogeosci. 2007, 112, G01023. [CrossRef]
6.
Kurkowski, N.P.; Stensrud, D.J.; Baldwin, M.E. Assessment of Implementing Satellite-Derived Land Cover Data in the Eta Model.
Weather Forecast. 2003, 18, 404–416. [CrossRef]
7.
Andrew, M.E.; Wulder, M.A.; Nelson, T.A. Potential Contributions of Remote Sensing to Ecosystem Service Assessments. Prog.
Phys. Geogr. Earth Environ. 2014, 38, 328–353. [CrossRef]
8.
Martínez-Harms, M.J.; Balvanera, P. Methods for Mapping Ecosystem Service Supply: A Review. Int. J. Biodivers. Sci. Ecosyst.
Serv. Manag. 2012, 8, 17–25. [CrossRef]
9.
Chakraborty, T.; Sarangi, C.; Lee, X. Reduction in Human Activity Can Enhance the Urban Heat Island: Insights from the
COVID-19 Lockdown. Environ. Res. Lett. 2021, 16, 054060. [CrossRef]
10.
Randin, C.F.; Ashcroft, M.B.; Bolliger, J.; Cavender-Bares, J.; Coops, N.C.; Dullinger, S.; Dirnböck, T.; Eckert, S.; Ellis, E.; Fernández,
N.; et al. Monitoring Biodiversity in the Anthropocene Using Remote Sensing in Species Distribution Models. Remote Sens.
Environ. 2020, 239, 111626. [CrossRef]
11.
Sydenham, M.A.K.; Venter, Z.S.; Eldegard, K.; Moe, S.R.; Steinert, M.; Staverløkk, A.; Dahle, S.; Skoog, D.I.J.; Hanevik, K.A.;
Skrindo, A.; et al. High Resolution Prediction Maps of Solitary Bee Diversity Can Guide Conservation Measures. Landsc. Urban
Plan. 2022, 217, 104267. [CrossRef]
12.
Hersperger, A.M.; Grădinaru, S.R.; Pierri Daunt, A.B.; Imhof, C.S.; Fan, P. Landscape Ecological Concepts in Planning: Review of
Recent Developments. Landsc. Ecol. 2021, 36, 2329–2345. [CrossRef] [PubMed]
13.
Gao, Y.; Skutsch, M.; Paneque-Gálvez, J.; Ghilardi, A. Remote Sensing of Forest Degradation: A Review. Environ. Res. Lett. 2020,
15, 103001. [CrossRef]
14.
Edens, B.; Maes, J.; Hein, L.; Obst, C.; Siikamaki, J.; Schenau, S.; Javorsek, M.; Chow, J.; Chan, J.Y.; Steurer, A.; et al. Establishing
the SEEA Ecosystem Accounting as a Global Standard. Ecosyst. Serv. 2022, 54, 101413. [CrossRef]
15.
Sulla-Menashe, D.; Gray, J.M.; Abercrombie, S.P.; Friedl, M.A. Hierarchical Mapping of Annual Global Land Cover 2001 to
Present: The MODIS Collection 6 Land Cover Product. Remote Sens. Environ. 2019, 222, 183–194. [CrossRef]
16.
Buchhorn, M.; Lesiv, M.; Tsendbazar, N.-E.; Herold, M.; Bertels, L.; Smets, B. Copernicus Global Land Cover Layers—Collection 2.
Remote Sens. 2020, 12, 1044. [CrossRef]
17.
Chen, J.; Chen, J.; Liao, A.; Cao, X.; Chen, L.; Chen, X.; He, C.; Han, G.; Peng, S.; Lu, M.; et al. Global Land Cover Mapping at 30
m Resolution: A POK-Based Operational Approach. ISPRS J. Photogramm. Remote Sens. 2015, 103, 7–27. [CrossRef]
18.
Cole, L.J.; Kleijn, D.; Dicks, L.V.; Stout, J.C.; Potts, S.G.; Albrecht, M.; Balzan, M.V.; Bartomeus, I.; Bebeli, P.J.; Bevk, D.; et al. A
Critical Analysis of the Potential for EU Common Agricultural Policy Measures to Support Wild Pollinators on Farmland. J. Appl.
Ecol. 2020, 57, 681–694. [CrossRef]
19.
Hanssen, F.; Barton, D.; Cimburova, Z. Mapping Urban Tree Canopy Cover Using Airborne Laser Scanning—Applications to Urban
Ecosystem Accounting for Oslo; NINA Report: Trondheim, Norway, 2019.
20.
Zhu, Z.; Wulder, M.A.; Roy, D.P.; Woodcock, C.E.; Hansen, M.C.; Radeloff, V.C.; Healey, S.P.; Schaaf, C.; Hostert, P.; Strobl, P.
Beneﬁts of the Free and Open Landsat Data Policy. Remote Sens. Environ. 2019, 224, 382–385. [CrossRef]
21.
Gorelick, N.; Hancher, M.; Dixon, M.; Ilyushchenko, S.; Thau, D.; Moore, R. Google Earth Engine: Planetary-Scale Geospatial
Analysis for Everyone. Remote Sens. Environ. 2017, 202, 18–27. [CrossRef]
22.
Schramm, M.; Pebesma, E.; Milenkovi´c, M.; Foresta, L.; Dries, J.; Jacob, A.; Wagner, W.; Mohr, M.; Neteler, M.; Kadunc, M.; et al.
The OpenEO API–Harmonising the Use of Earth Observation Cloud Services Using Virtual Data Cube Functionalities. Remote
Sens. 2021, 13, 1125. [CrossRef]
23.
Brown, C.F.; Brumby, S.P.; Guzder-Williams, B.; Birch, T.; Hyde, S.B.; Mazzariello, J.; Czerwinski, W.; Pasquarella, V.J.; Haertel,
R.; Ilyushchenko, S.; et al. Dynamic World, Near Real-Time Global 10 m Land Use Land Cover Mapping. Sci. Data 2022, 9, 251.
[CrossRef]
24.
Zanaga, D.; Van De Kerchove, R.; De Keersmaecker, W.; Souverijns, N.; Brockmann, C.; Quast, R.; Wevers, J.; Grosu, A.; Paccini,
A.; Vergnaud, S.; et al. ESA WorldCover 10 m 2020 V100. OpenAIRE 2021. [CrossRef]
25.
Karra, K.; Kontgis, C.; Statman-Weil, Z.; Mazzariello, J.C.; Mathis, M.; Brumby, S.P. Global Land Use/Land Cover with Sentinel 2 and
Deep Learning; IEEE: Manhattan, NY, USA, 2021; pp. 4704–4707.
26.
R Core Team. R: A Language and Environment for Statistical Computing 2021. R Foundation for Statistical Computing, Vienna,
Austria. Available online: https://www.R-project.org/ (accessed on 21 July 2022).
27.
D’Andrimont, R.; Yordanov, M.; Martinez-Sanchez, L.; Eiselt, B.; Palmieri, A.; Dominici, P.; Gallego, J.; Reuter, H.I.; Joebges,
C.; Lemoine, G.; et al. Harmonised LUCAS In-Situ Land Cover and Use Database for Field Surveys from 2006 to 2018 in the
European Union. Sci. Data 2020, 7, 352. [CrossRef]
28.
Dinerstein, E.; Olson, D.; Joshi, A.; Vynne, C.; Burgess, N.D.; Wikramanayake, E.; Hahn, N.; Palminteri, S.; Hedao, P.; Noss, R. An
Ecoregion-Based Approach to Protecting Half the Terrestrial Realm. BioScience 2017, 67, 534–545. [CrossRef] [PubMed]
29.
Pesaresi, M.; Freire, S. GHS Settlement Grid Following the REGIO Model 2014 in Application to GHSL Landsat and CIESIN GPW
V4-Multitemporal (1975–1990–2000–2015). JRC Data Cat. 2016. Available online: http://data.europa.eu/89h/jrc-ghsl-ghs_smod_
pop_globe_r2016a (accessed on 21 July 2022).
Remote Sens. 2022, 14, 4101
19 of 19
30.
Halvorsen, R.; Skarpaas, O.; Bryn, A.; Bratli, H.; Erikstad, L.; Simensen, T.; Lieungh, E. Towards a Systematics of Ecodiversity:
The EcoSyst Framework. Glob. Ecol. Biogeogr. 2020, 29, 1887–1906. [CrossRef]
31.
Büttner, G. CORINE Land Cover and Land Cover Change Products. In Land Use and Land Cover Mapping in Europe; Springer:
Berlin/Heidelberg, Germany, 2014; pp. 55–74.
32.
Venter, Z.S.; Sydenham, M.A.K. Continental-Scale Land Cover Mapping at 10 m Resolution Over Europe (ELC10). Remote Sens.
2021, 13, 2301. [CrossRef]
33.
Pﬂugmacher, D.; Rabe, A.; Peters, M.; Hostert, P. Mapping Pan-European Land Cover Using Landsat Spectral-Temporal Metrics
and the European LUCAS Survey. Remote Sens. Environ. 2019, 221, 583–595. [CrossRef]
34.
Büttner, G.; Maucha, G. The Thematic Accuracy of Corine Land Cover 2000. Assessment Using LUCAS (Land Use/Cover Area
Frame Statistical Survey). Eur. Environ. Agency Cph. Den. 2006, 7, 1–85.
35.
Olofsson, P.; Foody, G.M.; Herold, M.; Stehman, S.V.; Woodcock, C.E.; Wulder, M.A. Good Practices for Estimating Area and
Assessing Accuracy of Land Change. Remote Sens. Environ. 2014, 148, 42–57. [CrossRef]
36.
Stehman, S.V.; Foody, G.M. Key Issues in Rigorous Accuracy Assessment of Land Cover Products. Remote Sens. Environ. 2019,
231, 111199. [CrossRef]
37.
Friedl, M.A.; Woodcock, C.E.; Olofsson, P.; Zhu, Z.; Loveland, T.; Stanimirova, R.; Arevalo, P.; Bullock, E.; Hu, K.-T.; Zhang,
Y.; et al. Medium Spatial Resolution Mapping of Global Land Cover and Land Cover Change Across Multiple Decades From
Landsat. Front. Remote Sens. 2022, 3, 894571. [CrossRef]
38.
Sales, M.H.; De Bruin, S.; Souza, C.; Herold, M. Land Use and Land Cover Area Estimates from Class Membership Probability of
a Random Forest Classiﬁcation. IEEE Trans. Geosci. Remote Sens. 2021, 60, 1–11. [CrossRef]
39.
Khatami, R.; Mountrakis, G.; Stehman, S.V. Predicting Individual Pixel Error in Remote Sensing Soft Classiﬁcation. Remote Sens.
Environ. 2017, 199, 401–414. [CrossRef]
40.
Ebrahimy, H.; Mirbagheri, B.; Matkan, A.A.; Azadbakht, M. Per-Pixel Land Cover Accuracy Prediction: A Random Forest-Based
Method with Limited Reference Sample Data. ISPRS J. Photogramm. Remote Sens. 2021, 172, 17–27. [CrossRef]
41.
Lang, N.; Jetz, W.; Schindler, K.; Wegner, J.D. A High-Resolution Canopy Height Model of the Earth. arXiv 2022, arXiv:2204.08322.
42.
Pasquarella, V.J.; Arévalo, P.; Bratley, K.H.; Bullock, E.L.; Gorelick, N.; Yang, Z.; Kennedy, R.E. Demystifying LandTrendr and
CCDC Temporal Segmentation. Int. J. Appl. Earth Obs. Geoinf. 2022, 110, 102806. [CrossRef]
43.
Potere, D.; Schneider, A. A Critical Look at Representations of Urban Areas in Global Maps. GeoJournal 2007, 69, 55–80. [CrossRef]
44.
Meng, L.; Mao, J.; Zhou, Y.; Richardson, A.D.; Lee, X.; Thornton, P.E.; Ricciuto, D.M.; Li, X.; Dai, Y.; Shi, X. Urban Warming
Advances Spring Phenology but Reduces the Response of Phenology to Temperature in the Conterminous United States. Proc.
Natl. Acad. Sci. USA 2020, 117, 4228–4233. [CrossRef]
45.
Uroy, L.; Alignier, A.; Mony, C.; Foltête, J.-C.; Ernoult, A. How to Assess the Temporal Dynamics of Landscape Connectivity in
Ever-Changing Landscapes: A Literature Review. Landsc. Ecol. 2021, 36, 2487–2504. [CrossRef]
46.
Notte, A.L.; Czúcz, B.; Vallecillo, S.; Polce, C.; Maes, J. Ecosystem Condition Underpins the Generation of Ecosystem Services: An
Accounting Perspective. One Ecosyst. 2022, 7, e81487. [CrossRef]
47.
McGill, B.J. Towards a Uniﬁcation of Uniﬁed Theories of Biodiversity. Ecol. Lett. 2010, 13, 627–642. [CrossRef]
48.
Jakobsson, S.; Töpper, J.P.; Evju, M.; Framstad, E.; Lyngstad, A.; Pedersen, B.; Sickel, H.; Sverdrup-Thygeson, A.; Vandvik, V.;
Velle, L.G. Setting Reference Levels and Limits for Good Ecological Condition in Terrestrial Ecosystems–Insights from a Case
Study Based on the IBECA Approach. Ecol. Indic. 2020, 116, 106492. [CrossRef]


Paper 2:
- APA Citation: Chia, H. K., & Mohammadi, K. (2021). Recent Advances in Evapotranspiration Estimation Using Artificial Intelligence Approaches with a Focus on Hybridization Techniques—A Review. Remote Sensing, 13(23), 4862. https://doi.org/10.3390/rs13234862
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: **Hybridization Techniques, Methods, or Approaches and Their Combinations with Base Artificial Intelligence Models**
**1. Averaging: Simple Averaging, Weighted Averaging, Simple Taylor Skill**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Combining individual model outputs to obtain predicted output of the ensemble

**2. Bootstrap Aggregating (Bagging)**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Resampling of training data to generate multiple models, then averaging their outputs to reduce variance and improve generalization

**3. Bayesian Modeling Approaches: Bayesian Model Averaging, Bayesian Model Selection**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Combining individual model outputs based on their posterior probabilities, considering model uncertainty

**4. Boosting Algorithm: Gradient Boosting, Extreme Gradient Boosting, Light Gradient Boosting**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Iterative addition of weak learners to create a strong learner, reducing bias and variance

**5. Nonlinear Neural Ensemble**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Creating a secondary neural network to combine individual model outputs, using a black-box approach to enhance ensemble performance

**6. Data Decomposition: Wavelet Decomposition, Multivariate Empirical Mode Decomposition, Tensor Decomposition**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Decomposing time series data into different temporal components, extracting useful patterns and trends for model training

**7. Remote Sensing Based Hybridisation:**
* **Suitable for:** Any base artificial intelligence model using remote sensing data
* **Used for:** Integrating remote sensing data into the modeling process, utilizing data assimilation techniques like STARFM, ESTARFM, and Kalman Filter based Ensemble

**8. Ensemble Models for Remote Sensing:**
* **Suitable for:** Any base artificial intelligence model using remote sensing data
* **Used for:** Combining multiple models trained on different remote sensing data sources or temporal resolutions, improving accuracy and robustness

**Possible Combinations of Hybridization Techniques:**
* **Averaging + Bagging**
* **Boosting + Averaging**
* **Bayesian + Bagging**
* **Data Decomposition + Averaging**
* **Remote Sensing + Data Decomposition**
* **Ensemble Models for Remote Sensing + Data Decomposition**
* **Ensemble Models for Remote Sensing + Averaging**
* **Ensemble Models for Remote Sensing + Boosting**

**Note:**
* The choice of hybridization techniques depends on the specific problem, data availability, and desired model performance.
* Combinations of techniques can be explored to enhance model performance and robustness.
* These hybridization techniques provide a comprehensive framework for developing advanced ensemble models for ET estimation using artificial intelligence.
  Extract 2: **Hybridization Techniques for Artificial Intelligence Models in ET Estimation**
**General Overview**
* Hybridization techniques combine multiple AI models or data sources to improve prediction accuracy and robustness.
* Can reduce reliance on meteorological parameters and handle limited or incomplete data conditions.

**Specific Hybridization Techniques**
**Data Fusion and Ensemble Modeling**
* **Averaging:** Simple averaging, weighted averaging, simple Taylor skill
* **Bootstrap Aggregating (Bagging):** Resampling and aggregation to reduce bias and variance
* **Bayesian Modeling Approaches:** Bayesian model averaging, Bayesian model selection
* **Boosting Algorithm:** Gradient boosting, extreme gradient boosting, light gradient boosting
* **Nonlinear Neural Ensemble:** Secondary neural network for combining individual model outputs

**Data Decomposition**
* **Wavelet Decomposition:** Decomposing time series into different temporal frequencies
* **Multivariate Empirical Mode Decomposition:** Generating intrinsic mode functions for trend analysis
* **Tensor Decomposition:** Unfolding and reconstructing multidimensional data

**Remote Sensing-Based Hybridization**
* **Remote Sensing Data Assimilation:** STARFM, ESTARFM, Kalman Filter based Ensemble
* **Ensemble Models for Remote Sensing:** Combining models trained on different data sources or resolutions

**Application in ET Estimation**
* ET estimation with limited or incomplete meteorological parameters
* Spatial and temporal forecasting of ET
* Enhancing model accuracy and robustness
* Exploring relationships between ET and various environmental factors

**Note:** The suitability of each technique depends on the specific data, problem, and desired model performance.
  Limitations: []
  Relevance Evaluation: 1.0
  Relevance Score: 1.0
  Inline Citation: Chia, Hoon Kooi and Mohammadi, Kourosh. (2021). Recent Advances in Evapotranspiration Estimation Using Artiﬁcial Intelligence Approaches with a Focus on Hybridization Techniques—A Review. Remote Sensing. 13. 4862.
  Explanation: **Hybridization Techniques, Methods, or Approaches and Their Combinations with Base Artificial Intelligence Models**
**1. Averaging: Simple Averaging, Weighted Averaging, Simple Taylor Skill**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Combining individual model outputs to obtain predicted output of the ensemble

**2. Bootstrap Aggregating (Bagging)**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Resampling of training data to generate multiple models, then averaging their outputs to reduce variance and improve generalization

**3. Bayesian Modeling Approaches: Bayesian Model Averaging, Bayesian Model Selection**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Combining individual model outputs based on their posterior probabilities, considering model uncertainty

**4. Boosting Algorithm: Gradient Boosting, Extreme Gradient Boosting, Light Gradient Boosting**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Iterative addition of weak learners to create a strong learner, reducing bias and variance

**5. Nonlinear Neural Ensemble**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Creating a secondary neural network to combine individual model outputs, using a black-box approach to enhance ensemble performance

**6. Data Decomposition: Wavelet Decomposition, Multivariate Empirical Mode Decomposition, Tensor Decomposition**
* **Suitable for:** All base artificial intelligence models
* **Used for:** Decomposing time series data into different temporal components, extracting useful patterns and trends for model training

**7. Remote Sensing Based Hybridisation:**
* **Suitable for:** Any base artificial intelligence model using remote sensing data
* **Used for:** Integrating remote sensing data into the modeling process, utilizing data assimilation techniques like STARFM, ESTARFM, and Kalman Filter based Ensemble

**8. Ensemble Models for Remote Sensing:**
* **Suitable for:** Any base artificial intelligence model using remote sensing data
* **Used for:** Combining multiple models trained on different remote sensing data sources or temporal resolutions, improving accuracy and robustness

**Possible Combinations of Hybridization Techniques:**
* **Averaging + Bagging**
* **Boosting + Averaging**
* **Bayesian + Bagging**
* **Data Decomposition + Averaging**
* **Remote Sensing + Data Decomposition**
* **Ensemble Models for Remote Sensing + Data Decomposition**
* **Ensemble Models for Remote Sensing + Averaging**
* **Ensemble Models for Remote Sensing + Boosting**

**Note:**
* The choice of hybridization techniques depends on the specific problem, data availability, and desired model performance.
* Combinations of techniques can be explored to enhance model performance and robustness.
* These hybridization techniques provide a comprehensive framework for developing advanced ensemble models for ET estimation using artificial intelligence.

 Full Text: >
agronomy
Review
Recent Advances in Evapotranspiration Estimation
Using Artiﬁcial Intelligence Approaches with a Focus
on Hybridization Techniques—A Review
Min Yan Chia, Yuk Feng Huang *
, Chai Hoon Koo and Kit Fai Fung
Department of Civil Engineering, Lee Kong Chian Faculty of Engineering and Science, Universiti Tunku Abdul
Rahman, Jalan Sungai Long, Bandar Sungai Long, Kajang 43000, Selangor, Malaysia;
chiamy94@gmail.com (M.Y.C.); kooch@utar.edu.my (C.H.K.); fkfpanda@hotmail.com (K.F.F.)
* Correspondence: huangyf@utar.edu.my; Tel.: +603-90860288
Received: 3 December 2019; Accepted: 23 December 2019; Published: 10 January 2020


Abstract:
Diﬃculties are faced when formulating hydrological processes, including that of
evapotranspiration (ET). Conventional empirical methods for formulating these possess some
shortcomings. The artiﬁcial intelligence approach emerges as the best possible solution to map the
relationships between climatic parameters and ET, even with limited knowledge of the interactions
between variables. This review presents the state-of-the-art application of artiﬁcial intelligence
models in ET estimation, along with diﬀerent types and sources of data. This paper discovers
the most signiﬁcant climatic parameters for diﬀerent climate patterns. The characteristics of the
basic artiﬁcial intelligence models are also explored in this review. To overcome the pitfalls of the
individual models, hybrid models which use techniques such as data fusion and ensemble modeling,
data decomposition as well as remote sensing-based hybridization, are introduced. In particular,
the principles and applications of the hybridization techniques, as well as their combinations with
basic models, are explained. The review covers most of the related and excellent papers published
from 2011 to 2019 to keep its relevancy in terms of time frame and ﬁeld of study. Guidelines for the
future prospects of ET estimation in research are advocated. It is anticipated that such work could
contribute to the development of agriculture-based economy.
Keywords: hydrological process; hybrid model; data fusion; ensemble modeling; data decomposition;
remote sensing; bootstrap aggregating; Bayesian modeling; boosting algorithm; nonlinear
neural ensemble
1. Introduction
In 2019, the United Nations [1] reported that world population had reached 7.7 billion. In the same
report, it was predicted that the world population would continue to grow, and the forecasted ﬁgures
are 8.5 billion, 9.7 billion and 10.9 billion for the years 2030, 2050, and 2100, respectively. Consequently,
agricultural activity that contributes to the population food supply increases progressively and
becoming more important. Agricultural activity is regarded as the anthropic activity that depletes
the highest amount of water [2]. Therefore, a good estimation of the water cycle can assist in eﬃcient
agricultural planning, water catchment and irrigation strategy, and thus optimize the utilization of
water. Evapotranspiration (ET), as suggested by the term itself, is the combination of evaporation of
water from land and plant surfaces and transpiration from vegetation through the leaves’ stomata [3].
ET is a natural event that aﬀects the hydrological cycle, which is believed to be highly complex
that involves several nonlinear processes [4]. There are numerous factors that govern the rate of
evapotranspiration and these include temperature, solar radiation, air humidity, and wind speed [5].
Agronomy 2020, 10, 101; doi:10.3390/agronomy10010101
www.mdpi.com/journal/agronomy
Agronomy 2020, 10, 101
2 of 33
ET is classiﬁed as a physical phenomenon. Hence, the rate of ET can be measured and represented
by a numerical value.
Traditionally, lysimeters are used to measure ET directly without any
assumptions [6]. It works by measuring the rate of water percolation through soil on the basis
of mass transfer [7]. Non-weighable lysimeters are normally used for long-term observation, whereas
weighable lysimeters can provide readings with greater temporal resolution [8]. It was claimed that
lysimeters could provide the most accurate measurement of ET. In fact, several studies that involved
ET estimation in the early years, utilized readings of lysimeters as their calibration standards [9,10].
Unfortunately, the construction, maintenance, and use of lysimeters involve high ﬁnancial burden
and ecological footprints. The limited amount of lysimeters also hindered the measurement of ET at
distinct locations [3]. In view of such situation, the development of other more convenient estimation
tools to estimate ET with higher accuracy and lower cost becomes the mode of choice.
Before the age of artiﬁcial intelligence, empirical equations were constantly developed to cater the
need of accurately estimating ET. However, in the absence of point measurement, direct acquisition of
ET value is virtually impossible. According to Pereira et al. [11], the term of reference evapotranspiration
(ET0) was introduced to overcome the problem. ET0 is an estimate of the amount of water loss or
consumption based on the weather primary eﬀect. In the coeﬃcient-reference system, a crop coeﬃcient
(Kc) will be multiplied with the ET0 to obtain the potential evapotranspiration (PET) for that particular
growth period.
Over the years, numerous eﬀorts had been pursued to obtain ET0 to the extent of higher
accuracy with lower computational complexity. Among the vast number of conventional models
for ET0 estimation, some of the most notable empirical approaches are discussed in this review.
The Penman–Monteith (PM) model [12], which had its early beginnings in 1948, is regarded as one of
the most widely employed models in the estimation of ET0. Furthermore, the Food and Agriculture
Organisation (FAO) of the United Nations, in their publication “Crop evapotranspiration—Guidelines
for computing crop water requirements—FAO Irrigation and Drainage Paper 56” (FAO56 in short),
had followed up and revised the calculation of ET0 and PET based on the PM model [13]. This indirectly
made the PM model as a standard in estimating ET0, and it was used in a number of research works as
a standard for comparison [14–16]. However, the sheer number of parameters and the complexity of
their investigation, derivation, and calculation do put paid to its use in a world of increasing knowledge
advancement and digital advance.
Ever since,
the eﬀort of providing simpler solution than the PM model continued.
The Hargreaves–Samani (HS) model is another method to estimate ET0, which was proposed by
Hargreaves and Samani [17]. The HS model had been employed in various occasions to estimate
ET0. However, the constants and coeﬃcients involved in the HS model can be site-speciﬁc. Hence,
eﬀorts were also done to calibrate the coeﬃcients in HS model to suit local needs and this can be
laborious. Luo et al. [18] validated the utilization of calibrated the HS model in Guilin, Kaifeng, Ganyu,
and Yinchuan to predict ET0 by using forecasted temperature. The results showed that, although
the prediction accuracy was suﬃciently high (87.54% to 96.90%), when the ET0 is relatively high or
relatively low, the Hargreaves model would fail as it did not consider the eﬀects of wind speed and
relative humidity. In Veneto, Italy (sub-humid climate), a study was done to compare the performance
of calibrated and uncalibrated the HS model [19]. It was found that the standard HS model would
overestimate ET0 value, which led to the tendency of excess water requirements. Calibrating the
empirical parameters of the HS model successfully reduced the overestimation from 18.9% to 2.6%
thus justiﬁed the importance of calibration.
The Priestly–Taylor (PT) Model was proposed by Priestley and Taylor [20]. Similar to the HS
model, the PT model instead ignores the sensitivity of ET0 towards vapor pressure and air movement.
On top of that, the PT model even further simpliﬁed the PM model where temperature parameters are
also removed. Due to the nature of the model that neglects the eﬀect of temperature, the PT model tends
to underestimate ET0 when compared to the PM model [21]. This suggestion was further supported
Agronomy 2020, 10, 101
3 of 33
by the poor performance of PT model at dry climate regions. Moreover, the missing aerodynamic
component in the PT model also limited its spatial applicability.
Apart from the three models discussed previously, there are some other less popular models to
estimate ET0. These include the radiation-based Turc model [22] and temperature-based Thornthwaite
model [23]. They are seldom used as a standard for comparison unlike the previous three models.
As shown in the literature, conventional models of ET0 estimation generally have two main
shortcomings: highly data intensive and strongly dependent on geographical location or not spatially
robust [24,25]. Researchers attempted to solve these problems by modifying or calibrating available
models to suit their needs. In addition, researchers also tried to produce a more powerful prediction
tool that can handle highly complex and nonlinear processes like ET. This review provides an outlook
on the emerging prediction tool, which is the artiﬁcial intelligence model.
The “black-box” nature of artiﬁcial intelligence model makes it very useful to map the relationship
between inputs and outputs even in the absence of relevant scientiﬁc knowledge. This leads to its
application in ET estimation in order to replace the data intensive and relatively less adaptive empirical
models. Increasing popularity of artiﬁcial intelligence application in ET estimation can be seen in
Figure 1, which is concluded from Scopus. From 2011 to 2015, the number of publications related to
ET prediction using artiﬁcial intelligence model appears to be quite stagnant. However, from 2016
onwards (except for 2017), there is a steady increase in the number of publications especially in 2018
and 2019. This shows the wide acceptance of such technique by global researchers that aﬃrms the
usefulness of artiﬁcial intelligence modeling. Having that said, there are certain ﬂaws that exist in
the basic artiﬁcial intelligence models. Fortunately, this could be overcome by using hybridization
techniques to combine diﬀerent models together in order to produce predictions with better accuracy
as well as consistency.
Agronomy 2020, 10, x FOR PEER REVIEW 
3 of 32 
 
Apart from the three models discussed previously, there are some other less popular models to 
estimate ET0. These include the radiation-based Turc model [22] and temperature-based 
Thornthwaite model [23]. They are seldom used as a standard for comparison unlike the previous 
three models. As shown in the literature, conventional models of ET0 estimation generally have two 
main shortcomings: highly data intensive and strongly dependent on geographical location or not 
spatially robust [24,25]. Researchers attempted to solve these problems by modifying or calibrating 
available models to suit their needs. In addition, researchers also tried to produce a more powerful 
prediction tool that can handle highly complex and nonlinear processes like ET. This review provides 
an outlook on the emerging prediction tool, which is the artificial intelligence model.  
The “black-box” nature of artificial intelligence model makes it very useful to map the 
relationship between inputs and outputs even in the absence of relevant scientific knowledge. This 
leads to its application in ET estimation in order to replace the data intensive and relatively less 
adaptive empirical models. Increasing popularity of artificial intelligence application in ET 
estimation can be seen in Figure 1, which is concluded from Scopus. From 2011 to 2015, the number 
of publications related to ET prediction using artificial intelligence model appears to be quite 
stagnant. However, from 2016 onwards (except for 2017), there is a steady increase in the number of 
publications especially in 2018 and 2019. This shows the wide acceptance of such technique by global 
researchers that affirms the usefulness of artificial intelligence modeling. Having that said, there are 
certain flaws that exist in the basic artificial intelligence models. Fortunately, this could be overcome 
by using hybridization techniques to combine different models together in order to produce 
predictions with better accuracy as well as consistency. 
 
Figure 1. Number of publications related to evapotranspiration estimation using artificial intelligence 
from 2011 to 2019. 
Nonetheless, a review article that focuses on the discussion of the application of artificial 
intelligence modeling is found to be absent in current literature. This situation has attracted the 
concern and focus of the authors of this review paper. It is of paramount importance to produce a 
compilation of related articles together with critical review as a reference and guidelines for future 
researchers in this field. This shall be regarded as a noble contribution and a starting point to facilitate 
the progress of relevant scientists and researchers, either as a starting or in their continuous efforts. 
To the agricultural water and water resources practitioners, the outcome of their research can be used 
for the decision-making process, which includes the design of irrigation schedules, water resources 
allocation, and management [26]. Careful, precise, and appropriate decisions are ultimately 
important for the sustainability of anthropic activities, especially in the context of agricultural 
economy. This can also generate more data for the use of research in years to follow. On top of that, 
published research articles are archived in scientific journals for future references. Figure 2 shows the 
virtuous cycle between review papers, new research, and decision-making processes. 
Figure 1. Number of publications related to evapotranspiration estimation using artiﬁcial intelligence
from 2011 to 2019.
Nonetheless, a review article that focuses on the discussion of the application of artiﬁcial
intelligence modeling is found to be absent in current literature. This situation has attracted the
concern and focus of the authors of this review paper. It is of paramount importance to produce a
compilation of related articles together with critical review as a reference and guidelines for future
researchers in this ﬁeld. This shall be regarded as a noble contribution and a starting point to facilitate
the progress of relevant scientists and researchers, either as a starting or in their continuous eﬀorts.
To the agricultural water and water resources practitioners, the outcome of their research can be used
for the decision-making process, which includes the design of irrigation schedules, water resources
allocation, and management [26]. Careful, precise, and appropriate decisions are ultimately important
for the sustainability of anthropic activities, especially in the context of agricultural economy. This can
Agronomy 2020, 10, 101
4 of 33
also generate more data for the use of research in years to follow. On top of that, published research
articles are archived in scientiﬁc journals for future references. Figure 2 shows the virtuous cycle
between review papers, new research, and decision-making processes.
Agronomy 2020, 10, x FOR PEER REVIEW 
4 of 32 
 
Figure 2. Virtuous cycle between review papers, new research and the decision-making process. 
Therefore, this paper aims to play the role of a comprehensive guidance for future research, from 
several aspects. In Section 2, different types of data and their sources are discussed thoroughly. The 
advantages and disadvantages of the data types are also summarized in this paper. Section 3 focuses 
on the explanation of basic artificial intelligence models in terms of their characteristics and 
application in ET estimation. Hybridization techniques for artificial intelligence models are reviewed 
in Section 4. Details of each and every technique are explained with respect to their principle and 
suitability in different situations. Finally, the future prospect of the use of artificial intelligence in ET 
estimation is presented in Section 5 as a prelude to the concluding remarks of this review paper. 
2. Data Types 
In order to proceed to the training of artificial intelligence models, the selection and acquisition 
of data are inseparable processes. Suitable data sets or parameters vary according to different regions 
as well as climate patterns. In order to aid future researchers during the process of data acquisition, 
significant parameters (though not exhaustive) to estimate ET for different climate patterns are 
summarized in Table 1 [27–37]. From Table 1, it can be seen that temperature and radiation data are 
indispensable for ET estimation. This is strongly in agreement with the background theory whereby 
temperature (indicating heat energy) and radiation are the two main driving forces of the energy 
consuming ET process [7]. 
Table 1. Significant parameters for different climate patterns to estimate evapotranspiration 
Climate Pattern 
Significant Parameters 
Arid 
Temperature, Radiation 
Semi-Arid 
Temperature, Radiation 
Humid 
Temperature, Radiation, Evaporation 
Sub-Humid 
Temperature, Radiation, Evaporation 
Warm-Humid 
Temperature, Radiation 
Humid Subtropical 
Radiation 
Subtropical Monsoon 
Temperature 
Mediterranean 
Radiation 
In general, the raw data can be obtained from two main sources, namely the ground observation 
data and the remote sensing data [38]. In essence, these two types of datasets act as complimentary 
data to one another. To obtain ground observation data, meteorological and weather stations are set 
up for continuous collections FLUXNET which consists of micrometeorological tower sites was set
First Review Paper
New Researches
Decision Making 
Process
Updated Review 
Paper
Enhanced Agricultural 
Economy
Research Outcome
New Publications
Data
Updated Guideline 
and References
Decisions
Guideline and References
Figure 2. Virtuous cycle between review papers, new research and the decision-making process.
Therefore, this paper aims to play the role of a comprehensive guidance for future research,
from several aspects. In Section 2, diﬀerent types of data and their sources are discussed thoroughly.
The advantages and disadvantages of the data types are also summarized in this paper. Section 3
focuses on the explanation of basic artiﬁcial intelligence models in terms of their characteristics and
application in ET estimation. Hybridization techniques for artiﬁcial intelligence models are reviewed
in Section 4. Details of each and every technique are explained with respect to their principle and
suitability in diﬀerent situations. Finally, the future prospect of the use of artiﬁcial intelligence in ET
estimation is presented in Section 5 as a prelude to the concluding remarks of this review paper.
2. Data Types
In order to proceed to the training of artiﬁcial intelligence models, the selection and acquisition of
data are inseparable processes. Suitable data sets or parameters vary according to diﬀerent regions
as well as climate patterns. In order to aid future researchers during the process of data acquisition,
signiﬁcant parameters (though not exhaustive) to estimate ET for diﬀerent climate patterns are
summarized in Table 1 [27–37]. From Table 1, it can be seen that temperature and radiation data are
indispensable for ET estimation. This is strongly in agreement with the background theory whereby
temperature (indicating heat energy) and radiation are the two main driving forces of the energy
consuming ET process [7].
Table 1. Signiﬁcant parameters for diﬀerent climate patterns to estimate evapotranspiration
Climate Pattern
Signiﬁcant Parameters
Arid
Temperature, Radiation
Semi-Arid
Temperature, Radiation
Humid
Temperature, Radiation, Evaporation
Sub-Humid
Temperature, Radiation, Evaporation
Warm-Humid
Temperature, Radiation
Humid Subtropical
Radiation
Subtropical Monsoon
Temperature
Mediterranean
Radiation
Agronomy 2020, 10, 101
5 of 33
In general, the raw data can be obtained from two main sources, namely the ground observation
data and the remote sensing data [38]. In essence, these two types of datasets act as complimentary
data to one another. To obtain ground observation data, meteorological and weather stations are set up
for continuous collections. FLUXNET, which consists of micrometeorological tower sites, was set up
as an initiative to cope with the large demand of ground observation data. The micrometeorological
tower sites are mostly concentrated in North America, Europe and Asia, whilst some tower sites are
also available in South America, Oceania, and Africa regions. Various types of data could be obtained
from the FLUXNET database, including diﬀerent locations, durations, and time scales. It was claimed
that FLUXNET would continue its eﬀort to expand in order to increase the geographical coverage [39].
The main advantage of using ground observation data is that it provides direct measurement, which
does not need further imputation or processing in order to retrieve the intrinsic information. Explicitly,
the data acquired from weather stations or ﬂux towers are ready to be used without any pre-processing
steps. However, measurements of weather station only represent the conditions of the tower’s location
and its close proximity. In other words, deduction of the weather conditions for a larger region can be
a challenging task [40].
Remote sensing emerges as a solution to cover the problem stated. This can be realized by
either subtracting the sensible heat ﬂux from net radiation (residual method) or computing the
surface resistance with a vegetation index–surface temperature scatterplot [41]. Successful estimation
of ET from satellite images opened the door for its forecasting using artiﬁcial intelligence models.
The major sources of satellite images could be obtained from Landsat, Moderate Resolution Imaging
Spectroradiometer (MODIS) and the Global Land Surface Satellite (GLASS). Remote sensing data allow
for more information to be captured by satellite images. In fact, the remote sensing method can be
used to derive vegetation information as well as diﬀerent types of radiation, which are useful for ET
estimation. Nonetheless, estimation using remote sensing data has to be well calibrated in order to
reﬂect accurate readings. This can be done through the integration of land data assimilation system
(LDAS). LDAS forces land surface models with the observed ﬁelds and removes biases in forcing
based on atmospheric models. In this way, unrealistic model states can be corrected. By merging
measurements (from satellite and ground observations) with model estimations, imperfections in
observations and errors in model prediction can be minimized [42]. Table 2 summarizes the types
of data for ET estimation and their sources, along with the obtainable parameters, advantages,
and disadvantages.
Careful selection of data is important for an excellent model training process. Identiﬁcation and
determination of data sources, parameters, and data points are keys to developing an outstanding
artiﬁcial intelligence model. Hence, prior to model selection, one should always ﬁrst consider the data
available. In the next section, this review will lead the readers to discover some of the more common
artiﬁcial intelligence models used in ET estimation.
Agronomy 2020, 10, 101
6 of 33
Table 2. Types of data for evapotranspiration estimation.
Data Types
Sources
Available Parameters
Advantages
Disadvantages
Ground Observation
Meteorological Stations, FLUXNET
Temperature, Wind Speed,
Radiation, Humidity Sunshine
Hours, Vapour Pressure Deﬁcit,
Evaporation
Available in diﬀerent time steps
(hourly, daily, monthly)
Only provide point measurement
Provide direct measurement data
Low spatial coverage
Less variations of parameters
Missing data
Remote Sensing
Landsat, MODIS, GLASS
Temperature, Radiation,
Vegetation Index, Leaf Area
Index, Albedo
More variations of parameters can
be derived from satellite images
Require calibration of satellite
images for data retrieval
Able to provide data at diﬀerent
spatial and temporal resolutions
Quality of data aﬀected by
weather conditions (cloud
coverage) and image resolution
Higher spatial coverage
Real time monitoring
Agronomy 2020, 10, 101
7 of 33
3. Artiﬁcial Intelligence Models
3.1. Artiﬁcial Neural Network (ANN)
Artiﬁcial neural network (ANN), as suggested by the name, is a variation of the machine learning
model that resembles the neural network of human brain. In the latter, neurons are connected to each
other via synapses. In ANN, synapses are replaced by weights and biases connections. This helps to
map the relationship between inputs and outputs [43]. A Multilayer perceptron (MLP) is one of the
earliest types of ANN, and it was introduced by Rosenblatt [44]. However, it was not until the year
1989 that MLP was proven to be able to approximate functions after training [45]. In 1992, in tandem
with the advancement of computer development, the MLP showed better performance than traditional
statistical method for the ﬁrst time [46].
The application of the MLP in estimation of ET0 was initiated by Kumar et al. [47]. In the
study, the authors collected the six essential parameters for estimating ET0 using the PM model in
Davis, California. This set of data was dated from 1 January 1990 to 30 June 2000. At the same time,
a second set of data dated from 1 January 1960 to 31 December 1963 was obtained together with their
corresponding lysimeter readings. The authors intended to compare the performance of the MLP
with diﬀerent architecture trained with diﬀerent target data. The outcome of the study showed that,
when all the six parameters were fed as input of the MLP model, a single layer of seven hidden neurons
with 5000 learning cycles was ample to represent the nonlinear process of ET. The training of the model
using lysimeter measurement as target produced slightly more accurate estimations than using the
PM model. This not only justiﬁed the ability of the MLP to map inputs to output in the absence of
clear relationship, but also at the same time laid down a foundation that estimation of the PM model is
suﬃciently good to be used as a target for the model training.
The success achieved by Kumar et al. [47] attracted the attention of researchers to further study
the capability of the MLP in estimating ET0. Attempts to reduce the number of required parameters
were continuously done. Rahimikhoob [48] trained the MLP with only temperature and radiation data
in eight diﬀerent stations in southern coast of Caspian Sea located in northern Iran (humid subtropical
climate). By using only maximum temperature, minimum temperature, and global radiation, the trained
models were compared with the HS model and the calibrated HS model. The authors had wished to
make a comparison between the MLP and empirical models with limited climatic data (both were
temperature and radiation based). The study proved that, even in the case where climatic dataset
was incomplete, the prediction of the MLP model was still promising, as the lack of ﬂexibility in
the empirical model had the tendency of either underestimating or overestimating. Antonopoulos
and Antonopoulos [49] conducted their study in a mountainous area in West Macedonia of Greece.
The authors continuously removed variables one by one from the MLP model in order to investigate
the ability of the MLP in estimating ET0 with limited climatic parameters. The performance of the MLP
contrasted with that of the PT model, the Makkink model, and the HS model. The study showed that,
even in the case of two parameters (temperature and radiation), the MLP still outperformed Makkink
model and HS model while having comparable performance with the PT model. Some other studies
reported by other literature also showed that the MLP could give better estimation than equivalent
conventional empirical models in the case of limited climatic parameters, in the four main classes of
climate regions such as semi-arid [50,51], arid [31,52,53], humid, and semi humid regions [54].
The introduction of the MLP had also encouraged the establishment of other forms of the
ANN model. Some of the examples are the radial basis function network (RBF) [55], generalized
regression neural network (GRNN) [56], back-propagation neural network (BPNN) [45] and extreme
learning machine (ELM) [57]. These algorithms achieved promising performances in ET0 estimation.
The characteristics of each ANN model are provided in Table 3.
Agronomy 2020, 10, 101
8 of 33
Table 3. Characteristics of artiﬁcial neural network models.
Artiﬁcial Neural Network Models
Characteristics
Multilayer Perceptron
Agronomy 2019, 9, x FOR PEER REVIEW 
8 of 32 
 
Table 3. Characteristics of artificial neural network models. 
Artificial Neural Network Models 
Characteristics 
Multilayer Perceptron 
 
- 
Consist of one input layer, one or more 
hidden layers and one output layer 
- 
Signals are passed from input layer to 
output layer in the forward direction 
- 
Normally use sigmoid activation function 
to map input to output 
Radial Basis Function 
 
- 
Consist of one input layer, one hidden 
layer and one output layer 
- 
Gaussian activation function is computed 
for every nodes in the hidden layer 
Generalised Regression Neural Network 
 
- 
A probabilistic based model 
- 
Consist of one input layer, one pattern 
layer, one summation layer and one output 
layer 
- 
Pattern layer is used to cluster the data and 
train the model 
- 
Results of the summation layer nodes are 
normalised in the output layer 
Back-Propagation Neural Network 
- 
Consist of one input layer, one or more 
hidden layers and one output layer 
-
Consist of one input layer, one or more
hidden layers and one output layer
-
Signals are passed from input layer to
output layer in the forward direction
-
Normally use sigmoid activation function
to map input to output
Radial Basis Function
Agronomy 2019, 9, x FOR PEER REVIEW 
8 of 32 
 
Table 3. Characteristics of artificial neural network models. 
Artificial Neural Network Models 
Characteristics 
Multilayer Perceptron 
 
- 
Consist of one input layer, one or more 
hidden layers and one output layer 
- 
Signals are passed from input layer to 
output layer in the forward direction 
- 
Normally use sigmoid activation function 
to map input to output 
Radial Basis Function 
 
- 
Consist of one input layer, one hidden 
layer and one output layer 
- 
Gaussian activation function is computed 
for every nodes in the hidden layer 
Generalised Regression Neural Network 
 
- 
A probabilistic based model 
- 
Consist of one input layer, one pattern 
layer, one summation layer and one output 
layer 
- 
Pattern layer is used to cluster the data and 
train the model 
- 
Results of the summation layer nodes are 
normalised in the output layer 
Back-Propagation Neural Network 
- 
Consist of one input layer, one or more 
hidden layers and one output layer 
-
Consist of one input layer, one hidden
layer and one output layer
-
Gaussian activation function is computed
for every nodes in the hidden layer
Generalised Regression Neural Network
Agronomy 2019, 9, x FOR PEER REVIEW 
8 of 32 
 
Table 3. Characteristics of artificial neural network models. 
Artificial Neural Network Models 
Characteristics 
Multilayer Perceptron 
 
- 
Consist of one input layer, one or more 
hidden layers and one output layer 
- 
Signals are passed from input layer to 
output layer in the forward direction 
- 
Normally use sigmoid activation function 
to map input to output 
Radial Basis Function 
 
- 
Consist of one input layer, one hidden 
layer and one output layer 
- 
Gaussian activation function is computed 
for every nodes in the hidden layer 
Generalised Regression Neural Network 
 
- 
A probabilistic based model 
- 
Consist of one input layer, one pattern 
layer, one summation layer and one output 
layer 
- 
Pattern layer is used to cluster the data and 
train the model 
- 
Results of the summation layer nodes are 
normalised in the output layer 
Back-Propagation Neural Network 
- 
Consist of one input layer, one or more 
hidden layers and one output layer 
-
A probabilistic based model
-
Consist of one input layer, one pattern
layer, one summation layer and one
output layer
-
Pattern layer is used to cluster the data
and train the model
-
Results of the summation layer nodes are
normalised in the output layer
Agronomy 2020, 10, 101
9 of 33
Table 3. Cont.
Artiﬁcial Neural Network Models
Characteristics
Back-Propagation Neural Network
Agronomy 2019, 9, x FOR PEER REVIEW 
9 of 32 
 
- 
Include a back-propagation algorithm to 
feedback the output error in order to 
optimise the model performance by 
adjusting weights and biases 
Extreme Learning Machine 
 
- 
Consist of only one input layer, one hidden 
layer and one output layer 
- 
Number of nodes in hidden layer are 
randomly generated 
- 
Only the number of nodes in the hidden 
layer have to be tuned to optimize the 
performance of model 
The RBF was first used to convert pan evaporation data into ET0 [58]. This study proved that the 
major obstacle of empirical estimation, which is the data dependency can be resolved. The RBF 
network used in the study required only pan evaporation and radiation data; however, it was able to 
achieve higher accuracy than both the Christen model and the PM model. 
Ladlani et al. [36] did a comparative study on the performance of the RBF and the GRNN to 
predict ET0 in Algiers of Algeria. Concurrently, the two ANN models were contrasted with empirical 
models (PT and HS model). In comparison, the GRNN model had the best performance in terms of 
low error and high correlation. The GRNN, which was evolved from the RBF, was proved, for the 
first time, to have more superior ability than the RBF. This could be due to the inclusion of a 
summation layer in the GRNN, which could enhance the estimation by the RBF. Performance of the 
GRNN in computing ET0 is constantly compared with other machine learning models [59,60]. From 
the results in the literature, the GRNN did not possess any prominent advantageous over ELM, but 
was deemed to be a good alternative for conventional models. 
Traore et al. [61] took a different approach to apply machine learning model in ET0 estimation 
where the BPNN model was used. In their study, meteorological data were collected from the 
Sudano-Sahelian zone. The HS model, which is a temperature based empirical model, was compared 
to BPNN model that was trained with only temperature data. The results showed that the artificial 
intelligence model outperformed the conventional HS model. On top of that, the authors also 
-
Consist of one input layer, one or more
hidden layers and one output layer
-
Include a back-propagation algorithm to
feedback the output error in order to
optimise the model performance by
adjusting weights and biases
Extreme Learning Machine
Agronomy 2019, 9, x FOR PEER REVIEW 
9 of 32 
 
 
- 
Include a back-propagation algorithm to 
feedback the output error in order to 
optimise the model performance by 
adjusting weights and biases 
Extreme Learning Machine 
 
- 
Consist of only one input layer, one hidden 
layer and one output layer 
- 
Number of nodes in hidden layer are 
randomly generated 
- 
Only the number of nodes in the hidden 
layer have to be tuned to optimize the 
performance of model 
The RBF was first used to convert pan evaporation data into ET0 [58]. This study proved that the 
major obstacle of empirical estimation, which is the data dependency can be resolved. The RBF 
network used in the study required only pan evaporation and radiation data; however, it was able to 
achieve higher accuracy than both the Christen model and the PM model. 
Ladlani et al. [36] did a comparative study on the performance of the RBF and the GRNN to 
predict ET0 in Algiers of Algeria. Concurrently, the two ANN models were contrasted with empirical 
models (PT and HS model). In comparison, the GRNN model had the best performance in terms of 
low error and high correlation. The GRNN, which was evolved from the RBF, was proved, for the 
first time, to have more superior ability than the RBF. This could be due to the inclusion of a 
summation layer in the GRNN, which could enhance the estimation by the RBF. Performance of the 
GRNN in computing ET0 is constantly compared with other machine learning models [59,60]. From 
the results in the literature, the GRNN did not possess any prominent advantageous over ELM, but 
was deemed to be a good alternative for conventional models. 
Traore et al. [61] took a different approach to apply machine learning model in ET0 estimation 
where the BPNN model was used. In their study, meteorological data were collected from the 
Sudano-Sahelian zone. The HS model, which is a temperature based empirical model, was compared 
to BPNN model that was trained with only temperature data. The results showed that the artificial 
intelligence model outperformed the conventional HS model. On top of that, the authors also 
-
Consist of only one input layer, one
hidden layer and one output layer
-
Number of nodes in hidden layer are
randomly generated
-
Only the number of nodes in the hidden
layer have to be tuned to optimize the
performance of model
The RBF was ﬁrst used to convert pan evaporation data into ET0 [58]. This study proved that
the major obstacle of empirical estimation, which is the data dependency can be resolved. The RBF
network used in the study required only pan evaporation and radiation data; however, it was able to
achieve higher accuracy than both the Christen model and the PM model.
Ladlani et al. [36] did a comparative study on the performance of the RBF and the GRNN to
predict ET0 in Algiers of Algeria. Concurrently, the two ANN models were contrasted with empirical
models (PT and HS model). In comparison, the GRNN model had the best performance in terms of
low error and high correlation. The GRNN, which was evolved from the RBF, was proved, for the ﬁrst
time, to have more superior ability than the RBF. This could be due to the inclusion of a summation
layer in the GRNN, which could enhance the estimation by the RBF. Performance of the GRNN in
computing ET0 is constantly compared with other machine learning models [59,60]. From the results
in the literature, the GRNN did not possess any prominent advantageous over ELM, but was deemed
to be a good alternative for conventional models.
Traore et al. [61] took a diﬀerent approach to apply machine learning model in ET0 estimation
where the BPNN model was used. In their study, meteorological data were collected from the
Agronomy 2020, 10, 101
10 of 33
Sudano-Sahelian zone. The HS model, which is a temperature based empirical model, was compared
to BPNN model that was trained with only temperature data. The results showed that the artiﬁcial
intelligence model outperformed the conventional HS model. On top of that, the authors also revealed
that inclusion of wind speed data could eﬀectively enhance the accuracy as compared to radiation
and relative humidity. Other research works related to the BPNN included comparison of BPNN
with gene programming in arid region [62], comparison with tree models [63], and reduction of input
parameters [48].
The latest development of artiﬁcial intelligence models resulted in the introduction of the ELM as
an option of ANN. In 2015, this variation of ANN was ﬁrst used to estimate ET0 in Iraq where the
authors claimed that this region represented general atmospheric and geographical conditions [30].
Similar to most of the available literature, the authors trained their ELM model by using the PM model
estimation as target. Several diﬀerent combinations of input parameters that consisted of temperature,
wind speed, relative humidity, and radiation were tested to study the most favorable combination.
Although the ELM and the BPNN showed comparable results, the authors opined that the ELM was
preferable due to its eﬃcient computation and great generalization ability. The fast iteration of ELM is
due to the fact that only the number of hidden layer nodes have to be tuned and this in turn reduces
the risk of overﬁtting. Their work was followed up by Gocic et al. [64], where they trained the ELM
using empirical models with lesser input parameters. In their study, it was found that the ELM trained
with the HS model was more superior to those that were trained with the PT model and the Turc model.
With that being said, the diﬀerence between individual the ELM models were marginal. In comparison
with the PM model estimated ET0, the ELM predictions had good correlation which justiﬁed that the
ELM were feasible for such purpose. Subsequently, another study was also carried out to reduce the
required input for ELM training [65].
Besides reducing the number of parameters required, researchers were also working towards
identifying the optimum training algorithm of the ANN for ET0 prediction [28]. The study compared
six learning algorithms of MLP, namely the Levenberg–Marquardt, Delta–Bar–Delta, Step, Momentum,
ConjugateGradient and the QuickProp. For each algorithm, diﬀerent combinations of input parameters
were tested.
At the same time, diﬀerent activation functions such as the hyperbolic tangent,
sigmoid, and linear functions. The investigation revealed that irrespective of input parameters,
the Levenberg–Marquardt learning algorithm coupled with hyperbolic tangent function was the
optimum setting for ET0 estimation using the MLP. The major distinction of the Levenberg-Marquardt
algorithm is that it includes the Gauss–Newton algorithm in its iterative process, which would lead
to the search of global minimum, unlike the other algorithms which have higher risk to converge to
local minima.
Recently, the investigations related to the ANN prediction of ET were focused on speciﬁc case
studies. Instead of estimating ET0, researchers began to utilize the MLP to predict PET directly. Since
PET relies heavily on the types of crops (diﬀerent Kc), this resulted in the study to be very speciﬁc
in terms of regions and plantations. For instance, Hashemi and Sepaskhah [66] obtained lysimeter
readings from Kooshak Agricultural Research Station in southwest of Iran. They compared the
performance of the MLP with the PM model and the radial basis function (RBF) model to estimate
PET at a barley plantation. By only feeding sunshine hours, mean humidity, mean temperature,
and wind speed as input, both the MLP and the RBF achieved better performance than the PM
model. This breakthrough reduced the need of collecting data for Kc computation which could be
tedious. Similar work was also carried out on wheat and maize plantations, which demonstrated the
advantages of MLP over conventional methods [67]. However, such studies required lysimeter reading
and perhaps leaf area index for training purposes. These data are not widely or easily available and
thus suﬃce to say that it would aﬀect the premise of utilizing the ANN in forecasting PET directly
for plantations.
An assessment of the various papers that have been reviewed in this subsection reveals
the following:
Agronomy 2020, 10, 101
11 of 33
1.
The evolution of the ANN from the MLP to the ELM was due to the constant need of improving
training methods and algorithms in order to obtain eﬀective predictions with greater accuracy,
better generalization and lesser dependency on input parameters.
2.
Within each and every variation of the ANN model, one could safely and easily deduce that the
trend and focus of study would not deviate much from the following four aspects:
a.
Minimization of required input parameters,
b.
Generalization of the ANN for wider spatial application,
c.
Introduction of new input parameters,
d.
Enhancement of ANN prediction ability.
It is believed that these four aspects stated above could revolutionise the prediction of ET0, with a
more general model—without the need for much climatic data.
3.
Longer forecasting horizon could provide a good pre-requisite for eﬀective water allocation
strategy. The use of the ANN alone could sometimes be insuﬃcient to provide a solution for the
above aspects.
4.
The black-box operation of ANN could not oﬀer an explanation to the complex ET process.
Therefore, the upcoming subsections will continue to review other artiﬁcial intelligence models
used to estimate ET0 in order to provide a complimentary solution to the shortcomings of ANN.
3.2. Support Vector Machine (SVM)
The support vector machine (SVM) is another popular algorithm used in machine learning
modeling, especially when it is claimed to be powerful and robust in regression and classiﬁcation
tasks [68]. Cortes and Vapnik [69] laid down the basic and foundation of the current SVM model.
Instead of involving large number of neurons and iterations to infer the relationship between inputs
and outputs, the SVM plots the datasets into a feature space. The relationship between inputs
and outputs is predicted using kernel function, where problem complexity and accuracy can be
optimized concurrently.
Since the ET0 prediction is more likely a regression problem rather than a classiﬁcation problem,
a variation of the SVM, which is the support vector regression (SVR), is normally used. In the SVR,
a loss function is used to deﬁne the deviation allowance as well as the function to approximate the
targeted output [70]. The working principle of SVM is shown in Figure 3.
Agronomy 2019, 9, x FOR PEER REVIEW 
11 of 32 
a. 
Minimization of required input parameters, 
b. 
Generalization of the ANN for wider spatial application, 
c. 
Introduction of new input parameters, 
d. 
Enhancement of ANN prediction ability. 
It is believed that these four aspects stated above could revolutionise the prediction of ET0, with 
a more general model—without the need for much climatic data. 
3. 
Longer forecasting horizon could provide a good pre-requisite for effective water allocation 
strategy. The use of the ANN alone could sometimes be insufficient to provide a solution for the 
above aspects. 
4. 
The black-box operation of ANN could not offer an explanation to the complex ET process. 
Therefore, the upcoming subsections will continue to review other artificial intelligence models 
used to estimate ET0 in order to provide a complimentary solution to the shortcomings of ANN. 
3.2. Support Vector Machine (SVM) 
The support vector machine (SVM) is another popular algorithm used in machine learning 
modeling, especially when it is claimed to be powerful and robust in regression and classification 
tasks [68]. Cortes and Vapnik [69] laid down the basic and foundation of the current SVM model. 
Instead of involving large number of neurons and iterations to infer the relationship between inputs 
and outputs, the SVM plots the datasets into a feature space. The relationship between inputs and 
outputs is predicted using kernel function, where problem complexity and accuracy can be optimized 
concurrently. 
Since the ET0 prediction is more likely a regression problem rather than a classification problem, 
a variation of the SVM, which is the support vector regression (SVR), is normally used. In the SVR, a 
loss function is used to define the deviation allowance as well as the function to approximate the 
targeted output [70]. The working principle of SVM is shown in Figure 3. 
 
Figure 3. Working principle of support vector machine. 
According Raghavendra and Deka [70], the SVM was widely used in hydrology application, 
including the estimation of ET0. The advantages and strengths of the SVM include high robustness, 
capability to solve complex problems, less susceptible to overfitting, and it can provide a compact 
description of the model [71]. The network structure of the SVM is illustrated in Figure 4. 
Figure 3. Working principle of support vector machine.
Agronomy 2020, 10, 101
12 of 33
According Raghavendra and Deka [70], the SVM was widely used in hydrology application,
including the estimation of ET0. The advantages and strengths of the SVM include high robustness,
capability to solve complex problems, less susceptible to overﬁtting, and it can provide a compact
description of the model [71]. The network structure of the SVM is illustrated in Figure 4.
Agronomy 2019, 9, x FOR PEER REVIEW 
12 of 32 
 
Figure 4. Network structure of support vector machine 
The utilization of the SVM in predicting ET0 with ground observation data started as early as 
2010 [72]. The case study was done in California, which represented a Köppen–Geiger climate system. 
The predictions of the SVM were compared with the CIMIS Penman, HS model, Ritchie model, and 
the Turc model. The authors discovered that when all climatic parameters were available, the SVM 
outperformed all other models in all the stations studied. When wind speed and relative humidity 
were removed during the training of the SVM, the model underestimated ET0 but still had satisfying 
outcomes, whereby it only performed slightly worse than conventional the HS model. In fact, the 
authors also claimed that the SVM had better performance than the ANN, where the former incurred 
lower error and higher correlation with the standard PM model. 
As stated earlier, the performance of the SVM greatly relies on the type of kernel functions 
chosen to transform the datasets before plotting them into the feature space. Selection of kernel 
functions can be done using a trial-and-error method. Mehdizadeh et al. [73] did a comprehensive 
study to compare the performance of the SVM using the RBF and polynomial kernel functions. The 
study revealed that the RBF kernel function could obtain more accurate results, but did not provide 
further explanation. A simple deduction that can be made is that the RBF function which represents 
a Gaussian distribution can be fitted well to the ET problem and datasets of the particular study. This 
suggestion is supported by the results of Mohammadrezapour et al. [74], where they showed that 
selection of kernel functions to estimate ET0 varied from case to case. In other words, there is no 
universal kernel function that is suited for all problems. Researchers who wished to estimate ET using 
the SVM should be prepared to include a tuning stage in order to identify suitable kernel functions. 
Continuous efforts were done to study the limit of the SVM as well as comparing the SVM to 
other artificial intelligence models. While the pioneers that applied the SVM in estimating ET0 
reported that the SVM performed better than ANN, some other literature opined otherwise [75,76]. 
This is due to the nature of the SVM where a global optimum has to be located instead of converging 
to local optima as in ANN. This makes SVM a very generalizable model, but it would incur higher 
residuals. More often, the strengths of the SVM would be visualized in the case of limited climatic 
parameters. In a work done by Fan et al. [77], when only temperature and radiation data were 
available, the performance of the SVM could be on par with the ELM. In fact, in terms of accuracy 
and correlation, the SVM achieved better score than most of the hybrid models such as extreme 
gradient boosted model, random forest, and gradient boosted decision tree. Further discussions on 
hybrid models is given in a later part of this review. 
Similar to the ANN, the SVM had been used to predict PET directly as this could reduce the 
extra effort of measuring/estimating Kc. This attempt was done by Shrestha and Shukla [78], where 
they trained their SVM models against lysimeter readings for pepper and watermelon crops. Instead 
of using conventional climatic parameters, the authors opted some interesting features which 
included days after transplant, irrigation frequency, water table depth, soil moisture, rainfall, and 
rainfall event as well as drainage and runoff frequency. According to the authors, the trained SVM 
model should be able to predict Kc and ET0, thereby making the computation of PET possible. It was 
observed that the SVM model was robust and well-generalized due to the fact that it could be 
successfully applied to both vine and erect plantations and work well in distinct seasons (spring and
Input Vectors X
K(X,X1)
K(X,X2)
K(X,X3)
K(X,X4)
∑ 
Output Y
Weights
Figure 4. Network structure of support vector machine
The utilization of the SVM in predicting ET0 with ground observation data started as early
as 2010 [72]. The case study was done in California, which represented a Köppen–Geiger climate
system. The predictions of the SVM were compared with the CIMIS Penman, HS model, Ritchie
model, and the Turc model. The authors discovered that when all climatic parameters were available,
the SVM outperformed all other models in all the stations studied. When wind speed and relative
humidity were removed during the training of the SVM, the model underestimated ET0 but still
had satisfying outcomes, whereby it only performed slightly worse than conventional the HS model.
In fact, the authors also claimed that the SVM had better performance than the ANN, where the former
incurred lower error and higher correlation with the standard PM model.
As stated earlier, the performance of the SVM greatly relies on the type of kernel functions chosen
to transform the datasets before plotting them into the feature space. Selection of kernel functions can
be done using a trial-and-error method. Mehdizadeh et al. [73] did a comprehensive study to compare
the performance of the SVM using the RBF and polynomial kernel functions. The study revealed that
the RBF kernel function could obtain more accurate results, but did not provide further explanation.
A simple deduction that can be made is that the RBF function which represents a Gaussian distribution
can be ﬁtted well to the ET problem and datasets of the particular study. This suggestion is supported
by the results of Mohammadrezapour et al. [74], where they showed that selection of kernel functions
to estimate ET0 varied from case to case. In other words, there is no universal kernel function that is
suited for all problems. Researchers who wished to estimate ET using the SVM should be prepared to
include a tuning stage in order to identify suitable kernel functions.
Continuous eﬀorts were done to study the limit of the SVM as well as comparing the SVM to other
artiﬁcial intelligence models. While the pioneers that applied the SVM in estimating ET0 reported that
the SVM performed better than ANN, some other literature opined otherwise [75,76]. This is due to the
nature of the SVM where a global optimum has to be located instead of converging to local optima as
in ANN. This makes SVM a very generalizable model, but it would incur higher residuals. More often,
the strengths of the SVM would be visualized in the case of limited climatic parameters. In a work
done by Fan et al. [77], when only temperature and radiation data were available, the performance of
the SVM could be on par with the ELM. In fact, in terms of accuracy and correlation, the SVM achieved
better score than most of the hybrid models such as extreme gradient boosted model, random forest,
and gradient boosted decision tree. Further discussions on hybrid models is given in a later part of
this review.
Similar to the ANN, the SVM had been used to predict PET directly as this could reduce the extra
eﬀort of measuring/estimating Kc. This attempt was done by Shrestha and Shukla [78], where they
trained their SVM models against lysimeter readings for pepper and watermelon crops. Instead of
using conventional climatic parameters, the authors opted some interesting features which included
Agronomy 2020, 10, 101
13 of 33
days after transplant, irrigation frequency, water table depth, soil moisture, rainfall, and rainfall event
as well as drainage and runoﬀ frequency. According to the authors, the trained SVM model should
be able to predict Kc and ET0, thereby making the computation of PET possible. It was observed
that the SVM model was robust and well-generalized due to the fact that it could be successfully
applied to both vine and erect plantations, and work well in distinct seasons (spring and fall) as well
as diﬀerent irrigation system (drip and sub-irrigation). The SVM not only produced closer estimations
to lysimeter readings as compared to the standard estimation procedure [13]; it also beat the ANN
and the relevance vector machine (RVM). The performance of the SVM was stable and consistent at
each growth stage of the plantations. As a side note, the authors suggested that their SVM models
identiﬁed that the evaporation and transpiration partition of plantations’ PET could be represented by
days after transplant, water table depth, rainfall events, and soil surface moisture.
From the reviewed literature, it can be inferred that the SVM has the potential to be reliable for
accurate estimation of both ET0 and PET. However, the literature also revealed that the performance
of SVM could be strongly aﬀected by the selection of kernel functions and quality of input data [70].
This could also be justiﬁed by the contradicting ﬁndings of researchers on the comparison between the
SVM and ANN. Computational cost is another concern of SVM application particularly when high
dimensionality is involved.
3.3. Fuzzy Models
Introduced by Zadeh [79], fuzzy logic allows the description of data in such a way that a “degree
of likeliness” can be given. In other words, by using fuzzy logic, instead of describing in terms of
“either A or B”, one can produce a membership degree between 0 and 1 so that the description looks like
“partly A and partly B”. Application of fuzzy logic requires an initial set up by experts to determine
the type of distribution by selecting a membership function (usually Gaussian function is chosen).
In addition, three major ingredients should be fed to fuzzy inference system (FIS), namely a set of
fuzzy rule base, database which contains the membership functions and a mechanism (either Sugeno
or Mamdani) to apply the fuzzy rules on input and output [80]. The main diﬀerence between Sugeno
and Mamdani fuzzy logic is the approach to compute the ﬁnal output. The overall ﬂow of an FIS is
illustrated in Figure 5.
Agronomy 2019, 9, x FOR PEER REVIEW 
13 of 32 
fall) as well as different irrigation system (drip and sub-irrigation). The SVM not only produced closer 
estimations to lysimeter readings as compared to the standard estimation procedure [13]; it also beat 
the ANN and the relevance vector machine (RVM). The performance of the SVM was stable and 
consistent at each growth stage of the plantations. As a side note, the authors suggested that their 
SVM models identified that the evaporation and transpiration partition of plantations’ PET could be 
represented by days after transplant, water table depth, rainfall events, and soil surface moisture. 
From the reviewed literature, it can be inferred that the SVM has the potential to be reliable for 
accurate estimation of both ET0 and PET. However, the literature also revealed that the performance 
of SVM could be strongly affected by the selection of kernel functions and quality of input data [70]. 
This could also be justified by the contradicting findings of researchers on the comparison between 
the SVM and ANN. Computational cost is another concern of SVM application particularly when 
high dimensionality is involved. 
3.3. Fuzzy Models 
Introduced by Zadeh [79], fuzzy logic allows the description of data in such a way that a “degree 
of likeliness” can be given. In other words, by using fuzzy logic, instead of describing in terms of 
“either A or B”, one can produce a membership degree between 0 and 1 so that the description looks 
like “partly A and partly B”. Application of fuzzy logic requires an initial set up by experts to 
determine the type of distribution by selecting a membership function (usually Gaussian function is 
chosen). In addition, three major ingredients should be fed to fuzzy inference system (FIS), namely a 
set of fuzzy rule base, database which contains the membership functions and a mechanism (either 
Sugeno or Mamdani) to apply the fuzzy rules on input and output [80]. The main difference between 
Sugeno and Mamdani fuzzy logic is the approach to compute the final output. The overall flow of an 
FIS is illustrated in Figure 5. 
 
Figure 5. Overall flow of fuzzy inference system 
The history of applying fuzzy logic to estimate ET0 began in 2009. Keskin et al. [81] forecasted 
the pan evaporation of Lake Eğirdir in Turkey using ground observation climatic data. A comparable 
study was done in Karso watershed of India [82]. The authors did not only study the feasibility of 
fuzzy logic predicting pan evaporation, but also the performance of fuzzy logic as compared to the 
ANN, the least-squared SVR and the adaptive-neuro fuzzy inference system (ANFIS), was also 
evaluated. The authors remarked that the fuzzy logic model emerged as one of the best models for 
pan evaporation estimation. This study stressed the importance of fuzzy rules in producing good 
estimations. Successful application of fuzzy logic shall have good membership functions as 
foundation. The tuning of membership function not only requires expert knowledge, but is also time-
consuming, especially for a complex phenomenon like ET that can be affected by a number of 
parameters. Hence, the ANN acts as a complimentary to the fuzzy logic to form an ANFIS [83]. 
Application of the ANFIS for ET0 estimation was first done by Kisi and Öztürk [84] and there are a 
number of recent works showing promising results [85–87]. 
Since the ANFIS is a product of an enhancement based on ANN, hence its performance is 
frequently compared with ANN in terms of ET0 estimation. Pour-Ali Baba et al. [85] conducted their 
experiment in Gwangju and Haenam of South Korea. They realized that the performance of the 
ANFIS and ANN could vary when the input datasets were different. ANFIS had produced better 
estimation when solar radiation was fed as input, whereas ANN had better performance when
Fuzzication
Defuzzication
Fuzzy Rule Base
Fuzzy Inference Mechanism
Input X
Output Y
Figure 5. Overall ﬂow of fuzzy inference system
The history of applying fuzzy logic to estimate ET0 began in 2009. Keskin et al. [81] forecasted
the pan evaporation of Lake E˘girdir in Turkey using ground observation climatic data. A comparable
study was done in Karso watershed of India [82]. The authors did not only study the feasibility
of fuzzy logic predicting pan evaporation, but also the performance of fuzzy logic as compared to
the ANN, the least-squared SVR and the adaptive-neuro fuzzy inference system (ANFIS), was also
evaluated. The authors remarked that the fuzzy logic model emerged as one of the best models for
pan evaporation estimation. This study stressed the importance of fuzzy rules in producing good
estimations. Successful application of fuzzy logic shall have good membership functions as foundation.
The tuning of membership function not only requires expert knowledge, but is also time-consuming,
especially for a complex phenomenon like ET that can be aﬀected by a number of parameters. Hence,
the ANN acts as a complimentary to the fuzzy logic to form an ANFIS [83]. Application of the ANFIS
for ET0 estimation was ﬁrst done by Kisi and Öztürk [84] and there are a number of recent works
showing promising results [85–87].
Agronomy 2020, 10, 101
14 of 33
Since the ANFIS is a product of an enhancement based on ANN, hence its performance is frequently
compared with ANN in terms of ET0 estimation. Pour-Ali Baba et al. [85] conducted their experiment
in Gwangju and Haenam of South Korea. They realized that the performance of the ANFIS and ANN
could vary when the input datasets were diﬀerent. ANFIS had produced better estimation when
solar radiation was fed as input, whereas ANN had better performance when sunshine hours were
used. Similarly, the performance of ANFIS and ANN could be aﬀected by geographical location [88].
However, some literature claimed that ANN had slightly better performance than ANFIS, which could
be due to ANN’s ﬂexibility (not bound by any rules) [89,90].
One interesting study on the ANFIS model is the comparison between two methods of setting up
a fuzzy rule, namely the grid partitioning method and subtractive clustering method [91]. The former
divides input space in grid-like manner and each region is fuzzy. For subtractive clustering, rules
are set up based on the number of clusters found in the input space. It was claimed that subtractive
clustering had computational advantage over grid partitioning. Investigation done by Cobaner [91]
showed that both approaches had similar performances. However, the ANFIS model using subtractive
clustering method could be aﬀected by quality of training data, especially when data are missing [92].
The review of publications in this subsection revealed that, unlike nonlinear learning in ANN
and kernel tricks applied in SVM, fuzzy logic provides another way for a machine to learn the rather
complex phenomenon of evapotranspiration. The main advantage of the fuzzy logic-based models
over the ANN and the SVM is that it actually allows for a more linguistic way of describing the data.
In other words, based on the fuzzy rules and membership functions, one can more or less deduce the
relationship that maps the inputs to the outputs.
3.4. Tree Based Models
Breiman [93] was the ﬁrst person to compile decision trees into two main categories, which were
the classiﬁcation tree and regression tree. However, it was Quinlan [94] who provided a better
understanding on the operation of tree models. In Quinlan’s work, it was stated that the decision
would continue to split and grow as long as the data within the nodes of the trees were still considered
as impure. In the case of ET, using a tree model for regression analysis is favored over classiﬁcation.
Within this context, Pal and Deswal [95] introduced a widely accepted splitting criterion for the M5
tree model. They claimed that, in order to produce better splits with highest computation eﬃciency,
data within any nodes should be split in such a way that the standard deviation reduction could be
maximized. It was observed that the M5 tree model could produce high correlation results to the
ET0 value, although the errors of estimations gradually increased when input climatic parameters
were reduced.
There are several research works published that followed up the study of Pal and Deswal [95].
Rahimikhoob et al. [96] attempted to convert pan evaporation data in ET0 while using other climatic
parameters as complementary data. Subsequently, the performance of M5 tree model in predicting ET0
was compared with the ANN [52]. The study was done in Iran where wind speed and radiation data
were found to be absent. The author concluded that, under such circumstances, the M5 tree model
could achieve similar performance to the ANN. It was also suggested that the M5 tree model should
be favored over the ANN due to its simplicity in terms of computation.
Elsewhere, Kisi and Kilic [97] also studied the diﬀerence in prediction performance of the M5 tree
model and ANN. In their concluding remarks, the authors revealed that both the M5 tree model and
ANN could produce outstanding ET0 estimation when trained and tested locally. This was however
not true when the machine learning models were trained and tested at diﬀerent stations. The M5
tree model had the worst performance, especially where lesser climatic parameters were available.
In fact, the performance of M5 tree model was worse than the empirical models. However, in another
study, the results showed disagreement where the M5 tree model was claimed to be having better
forecasting accuracy when trained locally as well as using external data [98]. In other words, the M5
Agronomy 2020, 10, 101
15 of 33
tree model could be very dependent on the quality of training data to determine its spatial robustness
and generalisability.
According to the papers reviewed regarding to tree-based modeling, it is clear to us that tree-based
models exhibit a clear advantage of simple and fast computation. In spite of that, the pitfall of such
a model is also obvious. As the tree in the model would have to grow until there are no any other
possible splits (the data is deemed to be pure by then), there is a risk of overgrowing the tree. In such
circumstance, overﬁtting could occur which is undesirable for regression analysis. To overcome the
problem, a strategy known as pruning is needed to remove unnecessary parts of the tree and replaced
them with linear functions. Moreover, the sequence of tree’s splitting could end up with diﬀerent
results even though with the same set of training data. To compensate the eﬀect of randomness,
trees are sometimes bundled to form a random forest. This will be discussed in detail in later parts of
this review.
Basic artiﬁcial intelligence models have their own advantages as well as disadvantages. The ANN
can be eﬃcient in ﬁtting nonlinear relationship, but it is less explanatory and prone to overﬁt. The SVM
has good generalizability at the expense of costly computation especially for high dimensionality
problems. The fuzzy logic provides interpretable rules but that would require initial set up with expert
knowledge. The Tree models are computationally eﬃcient, however, would incur high errors. Hence,
using basic artiﬁcial intelligence models alone is insuﬃcient to accommodate the increasing expectations
of their performances. In the following next section, diﬀerent hybridization techniques of artiﬁcial
intelligence models are explored as an eﬀective solution to overcome the problems encountered above.
4. Hybrid Models
Hybrid modeling which combines two or more models may somehow improve model performance
by merging their individual strengths [99,100]. As demonstrated in the research works mentioned
previously, researchers are ambitious for artiﬁcial intelligence models to be developed in the future
that can work in harsher conditions. For example, in environments with limited climatic parameters,
wide region of interest, or longer prediction horizon, among others. Therefore, this section of review
will be focusing on uncovering some more commonly used techniques to develop hybrid artiﬁcial
intelligence models.
4.1. Data Fusion and Ensemble Modelling
4.1.1. Averaging
The idea of ensemble modeling was suggested in 2005, where it was used to forecast weather
to overlay predictions of multiple models [101]. The simplest possible ensemble model is by plainly
averaging the product of the members of the ensemble. Simple averaging obtains the mean of the
models. In such a way, all involved models will be treated as though they have equal performance.
In order to correct the absurdity in the assumption of simple averaging, some studies preferred to
use weighted averaging. The weight values assigned to the models are ranked based on certain
performance measure. For example, Nourani et al. [33] proposed to use the coeﬃcient of determination
as a ranking reference. However, these two methods were not comprehensive enough to provide
accurate insights for individual models in an ensemble.
Taylor [102] proposed an alternative measure known as the simple Taylor skill. For each individual
model, a Taylor skill score will be assigned as the weight value. The Taylor skill score is deemed
to be more comprehensive as it takes correlation coeﬃcient and relative standard deviation into
consideration. This approach is used by Yao et al. [103], where it was proven that the ensemble model
produced from the simple Taylor skill fusion could produce spatial estimation which was comparable
to the remote sensing technique. Nonetheless, the authors raised the concern that the simple Taylor
skill fusion lacks the ability to describe the ET phenomena physically. This led to the rather low
popularity of this method among researchers worldwide.
Agronomy 2020, 10, 101
16 of 33
4.1.2. Bootstrap Aggregating
One of the most common techniques to hybridize artiﬁcial intelligence models is the data fusion
technique (ensemble modeling). There are various strategies that can lead to the desired output.
The ﬁrst method to be reviewed in the bootstrap aggregating (bagging) method. Generally, bootstrap
aggregating involves two main parts: resampling and aggregation. Bootstrap aggregating is especially
useful when one has a smaller sample size. During the stage of resampling, the collected samples
will be treated as an “apparent population”. Bags of “samples” will be produced from the “apparent
population” by using resampling with replacement method. The bags of “samples” will be having
an equivalent size with its “apparent population” [104]. Application of bootstrap aggregating in
estimating ET0 is common. Kim et al. [105] applied bootstrap aggregating on the GRNN to study
the performance of soft computing in forecasting ET0. The study showed that using bootstrapping
alone to solely extend the size of training data was insuﬃcient to produce signiﬁcant improvement
to the GRNN models. Instead, the authors suggested training multiple models in order to obtain
their aggregated output. It was opined that the latter could eﬀectively reduce the generalization error.
This study was the pioneer of utilising bootstrap aggregating for improvement of artiﬁcial intelligence
models when calculating ET0.
The success of Kim et al. [105] attracted the attention of global researchers to conduct similar
studies. Besides the GRNN, bootstrap aggregating can be applied on other machine learning models
such as tree models. In fact, performing the tree model analysis using bootstrapped samples can
lead to the formation of a random forest that was mentioned in passing in the previous section.
Feng et al. [59] reported that the random forest model could perform better than the GRNN. In the
study of Granata [5], the author compared the results of bagged random forests with individual
regression tree models. However, this study reported another ﬁnding which claimed that bagging
did not signiﬁcantly improve the performance of single regression tree. Although the author did
not provide explanation to his discovery, nonetheless, it is strongly believed that the contradictions
between the works of Kim et al. [105], Feng et al. [59], and Granata [5] originated from diﬀerences in
the datasets. The former two opted to use monthly and annual data respectively, whereas the latter
was using daily time step data. Bootstrap aggregating is clearly providing positive eﬀects when the
sample size is smaller.
The unique characteristics of bootstrap aggregating is that it does not only perform data
pre-processing on the raw datasets.
At the same time, it oﬀers an algorithm to aggregate and
average out the output of individual models. This is especially useful as an approach to enlarge the
limited collected data while oﬀsetting the bias and variance that was aroused from the randomness of
model training.
4.1.3. Bayesian Modeling Approaches
Apart from averaging and bootstrap aggregating, another very useful technique to create an
ensemble model is via the Bayesian modeling approaches. The Bayesian modeling approaches utilizes
the Bayes rules in statistical studies. There are two main strategies when applying the Bayesian
modeling approaches in modeling hydrological processes, namely the Bayesian model selection and the
Bayesian model averaging [106]. Although both approaches originated from the same fundamentals,
their intuitions could still show remarkable diﬀerences. In Bayesian model averaging (“team-of-rivals”
approach), the main theory underneath is that the model is convinced that there is a truth to be told by
models. However, the degree of correctness is strongly dependent on the uncertainties incurred.
Bayesian model averaging works on the basis that it considers truthfulness of the members in
an ensemble as their weights. Explicitly, this is realized through the computation of the posterior
probability of each model [107]. In this way, the ensemble model would not take excessive risk to
exclude models that could be true as well. In the case when data and observations are massive enough
to conﬁdently deduce a conclusion, or when there is a particular model that considered virtually able
to be true, this model will be promoted to Bayesian model selection (“winner-takes-all” approach).
Agronomy 2020, 10, 101
17 of 33
In other words, Bayesian modeling approaches will keep updating weight of models by imputing their
posterior probabilities, until an exceptional model emerges as a “winner” to end the search.
Bayesian modeling approaches were widely used in ET0 estimation research. Zhu et al. [108]
studied the posterior distributions of factors aﬀecting ET0 for diﬀerent periods, which varied in
terms of leaf area index. In a later study, Zhu et al. [109] produce an ET0 estimation ensemble model
which included the likes of the PM model, the advection-aridity model, the Shuttleworth–Wallace
model, and the modiﬁed PT model. The outcome of the study proved that, as compared to the
simple averaging, the Bayesian model averaging had more positive inﬂuences during the process of
developing the ensemble model. The authors is of the opinion that the probability density function
proposed in the Bayesian theory was well suited to ET phenomena. Despite its good performance,
the authors also stressed that output of the Bayesian model averaging was strongly linked to the
selection of input parameters.
Chen et al. [110] took a more aggressive approach whereby they used the Bayesian model
averaging to combine empirical and artiﬁcial intelligence models. The research team suggested two
diﬀerent schemes to create the ensemble model. The authors observed that including all models in the
ensemble would result in poorer performance as the Bayesian model averaging assign some weights
to poor-performing models. In view of such circumstance, another ensemble was created to include
only the models that were performing well. The hypothesis of the authors was veriﬁed to be correct.
The usefulness of the Bayesian modeling approaches has resulted in the introduction of various
related algorithms such as the Bayesian joint probability [111] and the Bayesian regression [112].
Bayesian regression was thought to be able to provide an insight to the selection of input parameters as
well as their relationship to ET0. This could provide a solution to policy makers to prioritize collection
of data in the near future.
4.1.4. Boosting Algorithm
Boosting is a technique whereby the prediction accuracy is improved by compounding estimations
of several weak learners [113]. Unlike the Bayesian model averaging, the boosting algorithm works
in a step-wise method, where a learner is added at a time to minimize the loss function. In the
boosting algorithm, the ﬁrst learner will try to search for an optimum loss function value. Subsequently,
the following models will be ﬁtted into the ensemble and work on the residuals of their predecessors.
Over the years, many versions of boosting algorithms had been established, each with its own novel
distinction. Some commonly known boosting methods include the gradient boosting [114], adaptive
boosting [115], extreme gradient boosting [116], and the categorical boosting [117].
In recent years, the use of the boosting algorithm in estimating ET0 has emerged increasingly
popular. Fan et al. [77], in particular, had provided the comparison of two types of boosting algorithms
on tree models, namely gradient boosting and extreme gradient boosting. The two algorithms diﬀer in
the sense that the gradient boosting uses nodes of tree models as weak learners, whereas the extreme
gradient boosting uses set of trees as weak learners. The authors found that„ generally, the extreme
gradient boosting overpowered the gradient boosting. This could be due to the fact that extreme
gradient boosting combined the averaged-out results of trees in set, which reduced the variance in the
output. On top of that, the design of extreme gradient boosting allows for parallel computation, which
could reduce the time taken for analysis.
Ponraj and Vigneswaran [118] proposed the use of the gradient boosting regression to estimate
ET0 at Borrego Springs, California. In the same study, the authors compared the performance of
the gradient boosting regression with the conventional multiple linear regression and random forest
methods. The results of the study showed that the gradient boosting regression showed higher
correlation with the standard PM model. The authors also suggested the use of the gradient boosting
machine as an alternative in future investigations.
Recently, Fan et al. [119] used another variation of the boosting algorithm, which was the light
gradient boosting algorithm to estimate ET0. The operating principle of the light gradient boosting
Agronomy 2020, 10, 101
18 of 33
is that it integrates the essence of gradient boosting and extreme gradient boosting. It performs
leaf-wise optimization instead of level-wise optimization. This could eﬀectively reduce the memory
and time taken for computation. However, as shown by the results, the light gradient boosting machine
required suﬃcient data in order to be trained well. During the training stage, the performance of light
gradient boosting machine was generally weaker than the random forest as well as the M5 tree model.
The situation was reversed during the testing phase where light gradient boosting machine performed
better when it was well trained.
The core of the boosting algorithm is to assemble several weak learners to form a strong learner.
By doing so, the strengths and experience of the weak learners can be utilized well by the hybrid
artiﬁcial intelligence model. Most important of all, the boosting algorithm can be used as a strategy
that can reduce the risk of overﬁtting. Nevertheless, the development of the boosting algorithm is still
at the early stages and more advanced methods shall be anticipated in the near future.
4.1.5. Nonlinear Neural Ensemble
Previously discussed data fusion techniques are developed based on certain statistical logics.
There is a kind of data fusion technique that depends on the black-box theory known as the nonlinear
neural ensemble. To summarize, outputs of individual artiﬁcial intelligence models are fed into a
secondary neural network to be trained once more. In other words, an ANN will be used to assemble
individual artiﬁcial intelligence models. This method had been applied by Nourani et al. [33] through
the combined ANN, SVM, ANFIS, and multiple linear regression. When compared with the simple
averaging and the weighted averaging, the nonlinear neural ensemble yielded better performance.
Similar observations were obtained when they used nonlinear neural ensemble to combine empirical
models. This proved that, for a highly nonlinear process like ET, averaging might be insuﬃcient to
capture the complexity.
In another study, the individual ANN was added one at a time to produce an ensemble [120].
The addition of ANN was continued until the termination condition was met (tolerable error was
achieved). In this way, the architecture and activation function of individual ANN can be constantly
modiﬁed in order to be considered acceptable into the ensemble. This approach was also used by
El-Shaﬁe et al. [121]. The resulting ensemble model would be consisting of only excellent models,
which in turn led to the accurate prediction of seasonal ET0.
The nonlinear neural ensemble is very useful in the scenario where statistical data fusion methods
could not produce improvements as compared to the original artiﬁcial intelligence models. It can
hybridize the artiﬁcial intelligence models by mapping another black-box relationship between inputs
and outputs. However, the results would be less interpretable as the intrinsic relationship within the
black box could not be observed.
4.1.6. Ensemble Models for Remote Sensing
One of the ﬁrst attempt to use the machine learning model to estimate ET0 with remote sensing
data was done in the United States, where AmeriFlux sites were available [40]. In the study, land surface
temperature, enhanced vegetation index, shortwave radiation, and land cover data are recovered from
satellite images that provided 1 km by 1 km coverage and eight-day time step. The ET0 were estimated
using the SVM, ANN and multiple regression. A similar approach was done by Zhang et al. [122] in
China where they extended the application of remote sensing data in the BPNN and the ANFIS for
estimating ET0. Further studies were done to include more artiﬁcial intelligence models such as the M5
tree model, bagging, random forest [5], ELM [123], and boosted tree [124]. However, the accuracy of
these studies was constrained by the quality of the images for retrieving the estimated meteorological
data. It was claimed that the images shall be within microwave band whereas cloud free condition is
preferred [125,126].
The major advantages of the remote sensing data over conventional ground observation data
include the wide selection of spatiotemporal range as stated earlier. Furthermore, satellite images can
Agronomy 2020, 10, 101
19 of 33
provide a massive variety of parameters to be used for ET0 estimation. As a result, multiple models
can be used to train the artiﬁcial intelligence models such as land surface model, energy balance (based
on eddy covariance and Bowen ratio) and equations for ET0 predictions. In spite of its advantages,
the shortcoming of remote sensing is that the estimation of ground data could be inaccurate. Moreover,
the homogeneity of the satellite images could also aﬀect the process of estimation. Hence, numerous
eﬀorts were done to apply data assimilation techniques on ET0 estimation artiﬁcial intelligence models
based on remote sensing data. This could be done by merging several satellite images that capture
diﬀerent information and feed them to the artiﬁcial intelligence models during the training stage.
The spatial and temporal adaptive reﬂectance fusion model (STARFM) is regarded as one of the
most commonly used data assimilation technique when dealing with remote sensing data. The basic
idea of the STARFM is that, if a Landsat-MODIS image pair is available, the algorithm can calculate
the systematic error on each pixel for MODIS image in order to retrieve a Landsat-like image [127].
However, this method assumes that both Landsat and MODIS images observe the same amount of
reﬂectance and incur by a constant bias error. Enhanced STARFM (ESTARFM) was introduced later
to overcome some problems of STARFM [128]. ESTARFM can handle heterogeneous regions unlike
STARFM method. In other words, when the pixel resolutions of the satellite images are not uniform,
ESTARFM would be favored over STARFM [129].
The operating principles of both the STARFM and ESTARFM are similar. Available MODIS images
are matched with Landsat images of diﬀerent overpass dates. Optimum base pairs will be used for
training the model. Based on the given MODIS image, the two data fusion techniques will retrieve
a predicted Landsat image so that the computation of ET0 is made possible. Cammalleri et al. [130]
proposed to apply data fusion technique on remote sensing based ET0 prediction so that images that
carry multiple information can be combined. In their study, Landsat images (30 m spatial resolution,
16-day temporal resolution) and MODIS images (1 km spatial resolution, 1-day temporal resolution)
were used to estimate ET0 using ALEXI and DisALEXI land surface models. By using the STARFM,
ET0 can be estimated from both sets of data. Landsat based ET0 was compared with Landsat-MODIS
based ET0. It was found the latter had higher accuracy, especially in the presence of discriminant
factors such as rainfall events.
A similar method was applied by Cammalleri et al. [131] to the ﬁeld scale, where they studied the
ET0 of corn and cotton crops. Semmens et al. [132] extended the application to viticulture system where
Landsat, MODIS and multi-sensor data were fused by the STARFM method. Recently, the ALEXI
and DisALEXI fused by STARFM was also used by Knipper et al. [126]. Their study was built on
the basis of Semmens et al. [132] with the expansion of study to multiple years in order to have an
insight of seasonal dynamics of ET0. Ma et al. [133] deployed ESTARFM for ET0 estimation from
satellite imaging for the ﬁrst time. They used three sets of MODIS data and two sets of Landsat data
with dissimilar spatial and temporal resolutions. Instead of using the ALEXI and DisALEXI models,
the surface energy balance system (SEBS) model was used in the study to calculate ET0. The authors
claimed that their results produced high resolution ET0 estimation with good accuracy. It was stated
that the estimated ET0 could produce similar trends with the observed ET0 with slight underestimation.
Nevertheless, a direct comparison between STARFM and ESTARFM has not been studied. Hence, it is
believable that the upper hand of ESTARFM is that it allows for inclusion of more satellite images with
diﬀerent resolutions.
Besides the STARFM and ESTARFM, another very popular data assimilation technique used by
researchers worldwide is the Kalman based ensemble. The deﬁnition of the observable model and state
model are essential to use the Kalman algorithm. Alavi et al. [134] demonstrated the usefulness of the
Kalman ﬁlter based ET0 estimation. To be exact, the work estimated missing heat ﬂux by treating as a
function of time and temperature which acted as observable models. The Kalman ﬁlter-based algorithm
was compared with the conventional mean diurnal variation, multiple regression, two-week average
PT coeﬃcient, and the multiple imputation. It was found that, although the Kalman ﬁlter-based
algorithm did not show outstanding accuracy among the other methods, the slight diﬀerence provided
Agronomy 2020, 10, 101
20 of 33
better estimation of ET0, especially during short gap periods with volatile ET0 ﬂuctuations where
sensitivity was a decisive factor.
Ever since then, the ensemble Kalman ﬁlter approach was widely used in estimating ET0.
For instance, Peters-Lidard et al. [135] evaluated several data assimilation systems that employed the
Kalman ﬁlter approach to predict the latent heat ﬂux. The prediction was done using FLUXNET as
well as MODIS data as inputs. It was reported that data assimilation was able to provide more accurate
results, indicating wide data structure application of the ensemble Kalman ﬁlter. In the Shahe River
Basin of China, Yin et al. [136] assimilated a hydrological model (data) with remote sensing-based
evapotranspiration. The outcome of the research work showed a promising potential of the ensemble
Kalman ﬁlter as a predictor when the state model is available. The advantages of using the ensemble
Kalman ﬁlter are that it permits the realization of multisource data as well as increases the precision
of estimation by incorporating suitable models. However, the determination of the observation and
state models can be challenging, and this throws the spanner in the works for the popularity of the
ensemble Kalman ﬁlter.
Utilization of the remote sensing approach in estimating ET0 removes the constraint of spatial
coverage. Satellite images of varying resolutions can be processed to recover valuable information
during the prediction. The remote sensing method also enables the provision of real-time data to
become possible and allows continuous monitoring of ET of certain regions. The development of
data fusion algorithms successfully combines diﬀerent satellite images and this in turn results in more
information to be used in ET0 prediction. Despite all these, the use of remote sensing is still at its early
stage and hence more robust as well as powerful tools can be expected in the near future.
4.2. Data Decomposition
The previous discussions are mainly focused on the exploitation of historical data as ingredients
for creating an estimating model. However, temporal trends and variations of ET are of utmost
importance as they can be a predictive tool to assist the decision-making of the stakeholders. Therefore,
a good artiﬁcial intelligence model shall be able to provide such information. Data related to ET could
be highly dynamic and contain unnecessary noises. Decomposition of data is needed to ﬁlter out the
noises in order to retrieve useful information.
According to Partal [137], the wavelet transformation had been successfully applied in many
hydrological processes research. In fact, a combination of ANN with wavelet transformation was
proved to be feasible in many other studies. Therefore, Partal [137] attempted to perform wavelet
transformation of data series of several climate data using diﬀerent temporal resolutions to obtain
useful decompositions. Theses sub-series were reconstructed and then be fed into the BPNN, multiple
linear regression and HS model. The resultant wavelet neural network (WNN) had better performance
than the other two models and proved that the wavelet-transformed data were useful to retain only
useful information as well as trends. The application of wavelet transformation is not constrained
to only BPNN, but is also applied in other ANNs such as RBF [138], ELM [29], GRNN [139,140],
and ANFIS [141].
Cobaner [142] converted Class A pan evaporation data into ET0 by using wavelet decomposition.
The study only focused on the eﬀect of wavelet transformation; therefore, instead of using an ANN,
the author selected a regression model for analysis. By using the Mallat discrete wavelet transformation,
the complex time series was broken down into several sub-time series that exhibited daily, monthly,
and annual features of the process. Each time series was weighted based on the strength of its
correlation. It was concluded that although the wavelet regression model had slightly lower accuracy
than the standard FAO-24 model for pan evaporation conversion; however, the drastic reduction of
required parameters was suﬃciently proved to be a success of this study.
Apart from the wavelet transformation, there are also other variations of data decomposition.
For instance, Adarsh et al. [143] used a multivariate empirical mode decomposition to pre-treat the raw
data (temperature, solar radiation, relative humidity and wind speed). In this method, intrinsic mode
Agronomy 2020, 10, 101
21 of 33
functions were generated after the decomposition of the data using varying temporal scale. Using
multivariate empirical mode decomposition on the data did not provide signiﬁcant improvement on the
obtained predictions. Future investigations can be conducted to study the eﬀect of such decomposition
method when climatic parameters were scarce.
Misaghian et al. [144] provided another form of data decomposition a priori to estimate ET0.
The ET0 data were represented in a multi-dimensional or tensor vector space. By using the Tucker
decomposition (a variation of singular value decomposition), the three-way relationship of month,
year and ET0 was unfolded.
The core tensor can be computed by the prediction machine and
reconstructing the predicted original tensor. The authors compared the values of ET0 computed with
the empirical models with those generated by tensor decomposition prediction. The predicted outcome
of tensor decomposition model was close to the estimations using the PM model, PT model, HS model,
Blaney–Criddle model, and the Jensen–Haise model.
Data decomposition oﬀers another perspective of the ET0 prediction whereby future ET0 can be
forecasted based on historical trends. In order to obtain a clearer picture of how ET0 is behaving at
diﬀerent time scales, data decomposition could do the work by ﬁltering noise and generate proﬁles
of ET0 trends to be analyzed by artiﬁcial intelligence models.
Data decomposition works as a
pre-processing technique that serves to reduce redundant data to the artiﬁcial intelligence model in
order to produce more meaningful and useful estimations.
Figure 6 outlines the pathways to develop hybrid models using diﬀerent modeling approaches.
In addition, in Table 4, an overview of diﬀerent hybridization methods is provided. The variations
of hybridization models are discussed in details in terms of their background principles and
suitable applications.
Agronomy 2019, 9, x FOR PEER REVIEW 
21 of 32 
different time scales, data decomposition could do the work by filtering noise and generate profiles 
of ET0 trends to be analyzed by artificial intelligence models. Data decomposition works as a pre-
processing technique that serves to reduce redundant data to the artificial intelligence model in order 
to produce more meaningful and useful estimations. 
Figure 6 outlines the pathways to develop hybrid models using different modeling approaches. 
In addition, in Table 4, an overview of different hybridization methods is provided. The variations of 
hybridization models are discussed in details in terms of their background principles and suitable 
applications. 
 
Figure 6. Pathways for hybrid model development. ANN – Artificial Neural Network; SVM – Support 
Vector Machine. 
Hybrid Model
Ground Observation
Remote Sensing
Data
Bootstrap Aggregating (requires 
aggregation of models)
Data Decomposition
ANN
SVM
Fuzzy Models
Tree Based Models
Averaging
Bayesian Modelling Approaches
Boosting Algorithm
Non-Linear Neural Ensemble
Modelling Approaches
Pre-Modelling 
Hybridisaion
Post-Modelling 
Hybridisaion
Remote Sensing 
Based Hybridisation
Figure 6. Pathways for hybrid model development. ANN – Artiﬁcial Neural Network; SVM – Support
Vector Machine.
Agronomy 2020, 10, 101
22 of 33
Table 4. Overview of diﬀerent hybridization techniques.
Hybridization Techniques
Variations
Principle
Application
Averaging
Simple Averaging
Treat each and every artiﬁcial intelligence models as equally
good models by obtaining their output mean value
Suitable for less complex problems where outputs
of several models can be averaged directly
Weighted Averaging
Assign weights to each artiﬁcial intelligence models based on
certain performance measure prior averaging their results
Simple Taylor Skill
Weights assigned to each artiﬁcial intelligence models are
derived by considering more than one performance measures
Bootstrap Aggregating
Bags of samples are created from original sample (“apparent
population”) so that more than one model can be trained and
the results are aggregated
Suitable when original sample size is too small or
results have high variance and bias
Bayesian Modelling Approaches
Bayesian Model Averaging;
Bayesian Model Selection
Weights assigned to each artiﬁcial intelligence models are
computed based on the posterior probabilities given that the
model accurately explain the problems
Can be used to assess the suitability or ability of a
model to describe a problem
Boosting Algorithm
Gradient Boosting; Extreme Gradient;
Boosting; Light Gradient Boosting
Combine several weak learners (poor performing artiﬁcial
intelligence models) to form a strong model
Suitable to be used when there are numerous
weak learners of diﬀerent aspects are available
Nonlinear Neural Ensemble
Feed the output of several models into a secondary ANN and
rely on black-box operation to obtain the ensemble
Shall be the last resort when no other more
intuitive hybridization method is suitable to create
an ensemble
Data Decomposition
Wavelet Decomposition; Multivariate
Empirical Mode Decomposition;
Tensor Decomposition
De-noise the time series data to obtain the trends of diﬀerent
temporal resolutions in order to forecast the future trends
Can be used when time series data are available
and when there is a need of forecasting
future events
STARFM
Learn using satellite image pairs to compute predictions
based on only one image
For remote sensing data application
ESTARFM
Improvement on STARFM in order to handle images with
non-uniform pixels or resolutions
For remote sensing data application
Kalman Filter Based Ensemble
Estimate state model by only using observable model as input
Can be used when there are clear deﬁnitions of
observable and state models
Agronomy 2020, 10, 101
23 of 33
5. Future Prospects
Shifting from conventional empirical models to artiﬁcial intelligence models for ET estimation
should be regarded as an indubitable trend. This is in line with the introduction of the Fourth
Industrial Revolution where artiﬁcial intelligence will take over non-value-added activities such as
forecasting and estimation. This would assist in the reduction of errors or mistakes when policy
makers are making decisions based on highly precise, accurate, and eﬀective predictions. In addition,
it is inevitably important for researchers worldwide to seek for solutions and reduce the number
of meteorological parameters needed for ET prediction for all its attendant costs and time savings
and eﬃciency reasons. The black-box operating nature of artiﬁcial intelligence models is currently
the solution to this problem. With that being said, ET data from ground observation or physical
measurement would still remain imperative during this transition while a robust artiﬁcial intelligence
model is being developed concurrently. On the other hand, advancement in satellite technologies
allows the use of remote sensing in ET monitoring. In other words, this provides another form of
data that would not have been collected from ground weather stations. Application of remote sensing
technology in ET estimation reduces the dependency of ET estimation from ground observation data
as it oﬀers a new basis to compute ET. Nevertheless, ground observation data are still important in
order to calibrate raw satellite images for better prediction in coming years.
In short, the future prospects of this ﬁeld of study can be summarized as follows:
1.
Eﬀective data assimilation.
Data fusion techniques shall be well utilized to accurately map ground observation data to remote
sensing data. This can make the satellite images become more informative in terms of accuracy as well
as temporal and spatial resolutions, if well calibrated.
2.
Creation of new hybrid models.
This can be done by changing the combinations of currently available artiﬁcial intelligence models
and hybridization techniques. Meanwhile, development of new algorithms or enhancement of present
algorithms can be attempted in the future. It is anticipated that the “committee of decision” formed
from hybrid models can produce predictions with greater accuracy and shorter computation time.
3.
Be cautious of climate changes.
Artiﬁcial intelligence models are highly dependent on the training (historical) data. Volatile
climate poses a serious challenge where past trends might not be applicable in the future. Studies in the
coming years can be focused on retrieving information which take climate changes into consideration.
For example, data selection and sampling shall be done with care in order to ensure the homogeneity
of the data where the eﬀect of climate change is minimal. In addition, models should be kept as
updated as possible. Dynamic modeling can be done to cater to this need while artiﬁcial intelligence
can support it with fast calculation and real-time data.
4.
Relationship discovery from new association rules.
Making use of the “Big Data” allows us to explore various possibilities which associate input
parameters to ET. By using the developed artiﬁcial intelligence models, one can explore a vast
number of variables or parameters and study their association with ET within a short period of time.
Parameters that are highly correlated with ET can be further studied to reveal their relationships and
scientiﬁc interactions.
5.
Widening of forecasting horizons.
Related studies were still in the infancy stage where the forecasting windows were too narrow.
Increasing forecasting lead time can assist in the design of eﬃcient water resource management plans.
This would be important, especially in crop plantations that require a longer time to schedule irrigation
their plan.
Agronomy 2020, 10, 101
24 of 33
6. Conclusions
The estimation of ET is of paramount importance, especially when dealing with agricultural
activities. This review has outlined the pitfalls of conventional models based on energy balance
which included the high dependency on climatic parameters. In addition, empirical models could be
speciﬁc to certain regions and this in turn requires further calibration before the models could be used.
Emergence of artiﬁcial intelligence models, which operated on the premise of a black-box principle
aims to overcome these problems. The integration of artiﬁcial intelligence reignites the possibility of
reduction of the now much needed climatic parameters for estimation of ET. Since artiﬁcial intelligence
models are data-driven, this review has pointed out some sources of data, and also the signiﬁcance of
diﬀerent parameters in various climate patterns. ANN, SVM, fuzzy models, and tree-based models
had been studied extensively in the past and their feasibilities were tried and tested. Nevertheless,
studies had revealed that, in the case of limited meteorological parameters or data, performance of
these artiﬁcial intelligence models would deteriorate.
In view of such circumstance, data fusion techniques had been developed as a solution. Bootstrap
aggregating is useful when available data size is too little to train a good model. Bayesian modeling
approaches rely on the imputation of posterior probabilities to weigh the correctness of individual
models. The boosting algorithm works by combining several weak learners to form a strong learner.
Moreover, the nonlinear neural ensemble relies on the black-box operation in order to create an
ensemble which produced better results than its constituent individual models. Data decomposition
has distinct characteristics whereby it extracts useful information at diﬀerent resolutions via certain
forms of transformation. This could assist in the removal of unwanted noise prior analysis. The purpose
of performing data decomposition, especially wavelet transformation is to draw certain trends from
historical data in order to predict future behaviour of ET. At the end of this review, a compilation
of suggested hybridization techniques for each base artiﬁcial intelligence models are provided in
Table 5. This could serve as a guideline in terms of parameters selections and ensemble strategies for
future research workers who wish to have a fresh start on ET estimation using the hybrid artiﬁcial
intelligence models.
Remote sensing technology appears to be able to remove the limitation of spatial coverage when
estimating ET. It also serves to provide real-time data in order to increase the dynamicity of analysis.
Remote sensing-based ET estimation is always integral with land surface model where energy balance
and radiation play important roles. Data assimilation can also be performed on remote sensing data
where satellite images from diﬀerent sources could be combined in artiﬁcial intelligence models.
This enables the combination of diﬀerent information being carried by diﬀerent sources of satellite
images (including resolution and band range). This can be realized by some of the commonly used
techniques such as the STARFM, ESTARFM, and the Kalman ﬁlter-based ensemble.
Besides providing the chronological development and guidelines to select methods or algorithms
for ET estimation using artiﬁcial intelligence models, this review also suggested the future trends
of the development of artiﬁcial intelligence in ET prediction. In upcoming studies, it is anticipated
that data fusion or assimilation would be the major subject alongside with the development of more
robust artiﬁcial intelligence models. It is in our interest that ground observation data can be merged
with remote sensing data. New hybrid models are also anticipated in order to increase the prediction
accuracy and speed. In the near future, climate change will be a major environmental issue and
researchers shall be cautious about its eﬀect. With matured and well-developed models, we could
expect that more parameters well associated with ET can be explored to discover their relationship
with ET, all in all for a more profound understand of the processes. Finally, forecasting horizons are
to be lengthened for achieving water resources allocation with higher eﬃciency. It can be a useful
tool during key steps of the decision-making process for policy makers, especially in water resources
management for a successful economic growth and development in the agricultural sector.
Agronomy 2020, 10, 101
25 of 33
Table 5. Mapping of hybridization techniques for artiﬁcial intelligence models.
HybridizationTechnique
BaseModel
Artiﬁcial Neural Network
Support Vector Machine
Tree Based Model
Fuzzy Logic
Averaging
√ 1
√
√
√
Bootstrap Aggregating
√
SVM does not require much
data to map the relationship.
Instead, it needs good
support vector (data) to infer
the relationship between
inputs and outputs.
Therefore, bootstrap
aggregating is seldom used
for in SVM hybrid models.
√
Fuzzy model itself contains
rules that is interpretable by
human language. Researchers
tend to apply black-box based
hybridization method on it.
Bayesian Model Averaging
√
√
√
Boosting Algorithm
Boosting algorithm is not
necessary for ANN as ANN
itself is powerful enough to
map most of the relationships
(not weak learner).
SVM is also a strong learner
and therefore the application
of boosting algorithm on it is
unnecessary.
√
Data Decomposition
√
√
√
Nonlinear Neural Ensemble
√
√
Flexibility of tree based model
allows multiple hybridization
techniques to be used. It is
believed that studies which
include tree based model in
nonlinear neural ensemble
will be available in the future.
√
1 √ mark indicates combination between base models and hybridization techniques is available.
Agronomy 2020, 10, 101
26 of 33
Author Contributions: M.Y.C. participated in the preparation and write up of the manuscript. Y.F.H. and C.H.K.
conceptualised the idea and topic of the paper. K.F.F. provided technical assistance and advises during the
completion of the manuscript. All authors have read and agreed to the published version of the manuscript.
Funding: This research was funded by Universiti Tunku Abdul Rahman (UTAR), Malaysia through Universiti
Tunku Abdul Rahman Research Fund under project number IPSR/RMC/UTARRF/2018-C2/K03. The APC was
fully funded by UTAR.
Conﬂicts of Interest: The authors declare no conﬂicts of interest.
Nomenclature
ANFIS
adaptive neuro-fuzzy inference system
ANN
artiﬁcial neural network
BPNN
back-propagation neural network
ELM
extreme learning machine
ESTARFM
enhanced spatial and temporal adaptive reﬂectance fusion model ET evapotranspiration
ET0
reference evapotranspiration
FIS
fuzzy inference system
GLASS
global land surface satellite
GRNN
generalised regression neural network
HS
Hargreaves–Samani
Kc
crop coeﬃcient
LDAS
land data assimilation system
MLP
multilayer layer perceptron
MODIS
Moderate Resolution Imaging Spectroradiometer
PET
potential evapotranspiration
PM
Penman–Monteith
PT
Priestley–Taylor
RBF
radial basis function
RVM
relevance vector machine
SEBS
surface energy balance system
STARFM
spatial and temporal adaptive reﬂectance fusion model
SVM
support vector machines
SVR
support vector regression
WNN
wavelet neural network
References
1.
United Nations. World Population Prospects: The 2019 Highlights; ST/ESA/SER.A/423; Department of Economic
and Social Aﬀairs/Population Division: New York, NY, USA, 2019.
2.
Cascone, S.; Coma, J.; Gagliano, A.; Pérez, G. The evapotranspiration process in green roofs: A review.
Build. Environ. 2019, 147, 337–355. [CrossRef]
3.
Stanhill, G. Evapotranspiration. In Encyclopedia of Soils in the Environment; Hillel, D., Ed.; Elsevier: Amsterdam,
The Netherlands, 2005; pp. 502–506.
4.
Jovic, S.; Nedeljkovic, B.; Golubovic, Z.; Kostic, N. Evolutionary algorithm for reference evapotranspiration
analysis. Comput. Electron. Agric. 2018, 150, 1–4. [CrossRef]
5.
Granata, F. Evapotranspiration evaluation models based on machine learning algorithms—A comparative
study. Agric. Water Manag. 2019, 217, 303–315. [CrossRef]
6.
Holmes, J.W. Measuring evapotranspiration by hydrological methods. Agric. Water Manag. 1984, 8, 29–40.
[CrossRef]
7.
Pokorny, J. Evapotranspiration. In Encyclopedia of Ecology, 2nd ed.; Fath, B., Ed.; Elsevier: Amsterdam,
The Netherlands, 2019; Volume 2, pp. 292–303.
8.
Wang, K.; Dickinson, R.E. A review of global terrestrial evapotranspiration: Observation, modeling,
climatology, and climatic variability. Rev. Geophys. 2012, 50, RG2005. [CrossRef]
Agronomy 2020, 10, 101
27 of 33
9.
Anapalli, S.S.; Ahuja, L.R.; Gowda, P.H.; Ma, L.; Marek, G.; Evett, S.R.; Howell, T.A. Simulation of crop
evapotranspiration and crop coeﬃcients with data in weighing lysimeters. Agric. Water Manag. 2016, 177,
274–283. [CrossRef]
10.
Liu, X.; Xu, C.; Zhong, X.; Li, Y.; Yuan, X.; Cao, J. Comparison of 16 models for reference crop evapotranspiration
against weighing lysimeter measurement. Agric. Water Manag. 2017, 184, 145–155. [CrossRef]
11.
Pereira, L.S.; Allen, R.G.; Smith, M.; Raes, D. Crop evapotranspiration estimation with FAO56: Past and
future. Agric. Water Manag. 2015, 147, 4–20. [CrossRef]
12.
Monteith, J.L. Evaporation and the environment in the state and movement of water in living organisms.
In Proceedings of the Society for Experimental Biology, Symposium No. 19, Cambridge, UK, 1 January 1965;
Cambridge University Press: Cambridge, UK, 1965; pp. 205–234.
13.
Allan, R.G.; Pereira, L.; Raes, D.; Smith, M. Crop Evapotranspiration—Guidelines for Computing Crop Water
Requirements—FAO Irrigation and Drainage Paper 56; Food and Agriculture Organization of the United Nations:
Rome, Italy, 1998; Volume 56.
14.
Saggi, M.K.; Jain, S. Reference evapotranspiration estimation and modeling of the Punjab Northern India
using deep learning. Comput. Electron. Agric. 2019, 156, 387–398. [CrossRef]
15.
Shiri, J.; Marti, P.; Karimi, S.; Landeras, G. Data splitting strategies for improving data driven models for
reference evapotranspiration estimation among similar stations. Comput. Electron. Agric. 2019, 162, 70–81.
[CrossRef]
16.
Güçlü, Y.S.; Subyani, A.M.; ¸Sen, Z. Regional fuzzy chain model for evapotranspiration estimation. J. Hydrol.
2017, 544, 233–241. [CrossRef]
17.
Hargreaves, G.H.; Samani, Z.A. Reference Crop Evapotranspiration from Temperature. Appl. Eng. Agric.
1985, 1, 96–99. [CrossRef]
18.
Luo, Y.; Chang, X.; Peng, S.; Khan, S.; Wang, W.; Zheng, Q.; Cai, X. Short-term forecasting of daily reference
evapotranspiration using the Hargreaves–Samani model and temperature forecasts. Agric. Water Manag.
2014, 136, 42–51. [CrossRef]
19.
Berti, A.; Tardivo, G.; Chiaudani, A.; Rech, F.; Borin, M. Assessing reference evapotranspiration by the
Hargreaves method in north-eastern Italy. Agric. Water Manag. 2014, 140, 20–25. [CrossRef]
20.
Priestley, C.H.B.; Taylor, R.J. On the Assessment of Surface Heat Flux and Evaporation Using Large-Scale
Parameters. Mon. Weather Rev. 1972, 100, 81–92. [CrossRef]
21.
Liu, J.G.; Zhao, T.S.; Chen, R.; Wong, C.W. The eﬀect of methanol concentration on the performance of a
passive DMFC. Electrochem. Commun. 2005, 7, 288–294. [CrossRef]
22.
Turc, L. Water requirements assessment of irrigation, potential evapotranspiration: Simpliﬁed and updated
climatic formula. Ann. Agron. 1961, 12, 13–49.
23.
Thornthwaite, C.W. An Approach toward a Rational Classiﬁcation of Climate. Geogr. Rev. 1948, 38, 55.
[CrossRef]
24.
Liu, S.; Xu, Z.; Song, L.; Zhao, Q.; Ge, Y.; Xu, T.; Ma, Y.; Zhu, Z.; Jia, Z.; Zhang, F. Upscaling evapotranspiration
measurements from multi-site to the satellite pixel scale over heterogeneous land surfaces. Agric. For. Meteorol.
2016, 230–231, 97–113. [CrossRef]
25.
Valipour, M.; Gholami Seﬁdkouhi, M.A.; Raeini-Sarjaz, M.; Guzman, S.M. A Hybrid Data-Driven Machine
Learning Technique for Evapotranspiration Modeling in Various Climates.
Atmosphere 2019, 10, 311.
[CrossRef]
26.
Wang, S.; Fu, Z.-Y.; Chen, H.-S.; Nie, Y.-P.; Wang, K.-L. Modeling daily reference ET in the karst area of
northwest Guangxi (China) using gene expression programming (GEP) and artiﬁcial neural network (ANN).
Theor. Appl. Climatol. 2015, 126, 493–504. [CrossRef]
27.
Falamarzi, Y.; Palizdan, N.; Huang, Y.F.; Lee, T.S. Estimating evapotranspiration from temperature and wind
speed data using artiﬁcial and wavelet neural networks (WNNs). Agric. Water Manag. 2014, 140, 26–36.
[CrossRef]
28.
Tabari, H.; Hosseinzadeh Talaee, P. Multilayer perceptron for reference evapotranspiration estimation in a
semiarid region. Neural Comput. Appl. 2012, 23, 341–348. [CrossRef]
29.
Kisi, O.; Alizamir, M. Modelling reference evapotranspiration using a new wavelet conjunction heuristic
method: Wavelet extreme learning machine vs. wavelet neural networks. Agric. For. Meteorol. 2018, 263,
41–48. [CrossRef]
Agronomy 2020, 10, 101
28 of 33
30.
Abdullah, S.S.; Malek, M.A.; Abdullah, N.S.; Kisi, O.; Yap, K.S. Extreme Learning Machines: A new approach
for prediction of reference evapotranspiration. J. Hydrol. 2015, 527, 184–195. [CrossRef]
31.
Huo, Z.; Feng, S.; Kang, S.; Dai, X. Artiﬁcial neural network models for reference evapotranspiration in an
arid area of northwest China. J. Arid Environ. 2012, 82, 81–90. [CrossRef]
32.
Wen, X.; Si, J.; He, Z.; Wu, J.; Shao, H.; Yu, H. Support-Vector-Machine-Based Models for Modeling Daily
Reference Evapotranspiration With Limited Climatic Data in Extreme Arid Regions. Water Resour. Manag.
2015, 29, 3195–3209. [CrossRef]
33.
Nourani, V.; Elkiran, G.; Abdullahi, J. Multi-station artiﬁcial intelligence based ensemble modeling of
reference evapotranspiration using pan evaporation measurements. J. Hydrol. 2019, 577, 123958. [CrossRef]
34.
Feng, Y.; Gong, D.; Mei, X.; Cui, N. Estimation of maize evapotranspiration using extreme learning machine
and generalized regression neural network on the China Loess Plateau. Hydrol. Res. 2017, 48, 1156–1168.
[CrossRef]
35.
Huang, G.; Wu, L.; Ma, X.; Zhang, W.; Fan, J.; Yu, X.; Zeng, W.; Zhou, H. Evaluation of CatBoost method for
prediction of reference evapotranspiration in humid regions. J. Hydrol. 2019, 574, 1029–1041. [CrossRef]
36.
Ladlani, I.; Houichi, L.; Djemili, L.; Heddam, S.; Belouz, K. Modeling daily reference evapotranspiration
(ET0) in the north of Algeria using generalized regression neural networks (GRNN) and radial basis function
neural networks (RBFNN): A comparative study. Meteorol. Atmos. Phys. 2012, 118, 163–178. [CrossRef]
37.
Yamaç, S.S.; Todorovic, M. Estimation of daily potato crop evapotranspiration using three diﬀerent machine
learning algorithms and four scenarios of available meteorological data. Agric. Water Manag. 2020, 228,
105875. [CrossRef]
38.
Maeda, E.E.; Wiberg, D.A.; Pellikka, P.K.E. Estimating reference evapotranspiration using remote sensing
and empirical models in a region with limited ground data availability in Kenya. Appl. Geogr. 2011, 31,
251–258. [CrossRef]
39.
National Aeronautics and Space Administration FLUXNET. Available online: https://daac.ornl.gov/cgi-bin/
dataset_lister.pl?p=9 (accessed on 23 October 2019).
40.
Yang, F.; White, M.A.; Michaelis, A.R.; Ichii, K.; Hashimoto, H.; Votava, P.; Zhu, A.X.; Nemani, R.R. Prediction
of Continental-Scale Evapotranspiration by Combining MODIS and AmeriFlux Data Through Support
Vector Machine. IEEE Trans. Geosci. Remote Sens. 2006, 44, 3452–3461. [CrossRef]
41.
Nagler, P.; Scott, R.; Westenburg, C.; Cleverly, J.; Glenn, E.; Huete, A. Evapotranspiration on western U.S.
rivers estimated using the Enhanced Vegetation Index from MODIS and data from eddy covariance and
Bowen ratio ﬂux towers. Remote Sens. Environ. 2005, 97, 337–351. [CrossRef]
42.
Rodell, M.; Houser, P.R.; Jambor, U.; Gottschalck, J.; Mitchell, K.; Meng, C.J.; Arsenault, K.; Cosgrove, B.;
Radakovich, J.; Bosilovich, M.; et al. The Global Land Data Assimilation System. Bull. Am. Meteorol. Soc.
2004, 85, 381–394. [CrossRef]
43.
Abiodun, O.I.; Jantan, A.; Omolara, A.E.; Dada, K.V.; Mohamed, N.A.; Arshad, H. State-of-the-art in artiﬁcial
neural network applications: A survey. Heliyon 2018, 4, e00938. [CrossRef]
44.
Rosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain.
Psychol. Rev. 1958, 65, 386–408. [CrossRef]
45.
Hornik, K.; Stinchcombe, M.; White, H. Multilayer feedforward networks are universal approximators.
Neural Netw. 1989, 2, 359–366. [CrossRef]
46.
Lek, S.; Guégan, J.F. Artiﬁcial neural networks as a tool in ecological modelling, an introduction. Ecol. Model.
1999, 120, 65–73. [CrossRef]
47.
Kumar, M.; Raghuwanshi, N.S.; Singh, R.; Wallender, W.W.; Pruitt, W.O. Estimating Evapotranspiration
using Artiﬁcial Neural Network. J. Irrig. Drain. Eng. 2002, 128, 224–233. [CrossRef]
48.
Rahimikhoob, A. Estimation of evapotranspiration based on only air temperature data using artiﬁcial neural
networks for a subtropical climate in Iran. Theor. Appl. Climatol. 2009, 101, 83–91. [CrossRef]
49.
Antonopoulos, V.Z.; Antonopoulos, A.V. Daily reference evapotranspiration estimates by artiﬁcial neural
networks technique and empirical equations using limited input climate variables. Comput. Electron. Agric.
2017, 132, 86–96. [CrossRef]
50.
Reis, M.M.; da Silva, A.J.; Zullo Junior, J.; Tuﬃ Santos, L.D.; Azevedo, A.M.; Lopes, É.M.G. Empirical
and learning machine approaches to estimating reference evapotranspiration based on temperature data.
Comput. Electron. Agric. 2019, 165, 104937. [CrossRef]
Agronomy 2020, 10, 101
29 of 33
51.
Citakoglu, H.; Cobaner, M.; Haktanir, T.; Kisi, O. Estimation of Monthly Mean Reference Evapotranspiration
in Turkey. Water Resour. Manag. 2013, 28, 99–113. [CrossRef]
52.
Rahimikhoob, A. Comparison between M5 Model Tree and Neural Networks for Estimating Reference
Evapotranspiration in an Arid Environment. Water Resour. Manag. 2014, 28, 657–669. [CrossRef]
53.
Shiri, J.; Nazemi, A.H.; Sadraddini, A.A.; Landeras, G.; Kisi, O.; Fakheri Fard, A.; Marti, P. Comparison of
heuristic and empirical approaches for estimating reference evapotranspiration from limited inputs in Iran.
Comput. Electron. Agric. 2014, 108, 230–241. [CrossRef]
54.
Pandey, P.K.; Nyori, T.; Pandey, V. Estimation of reference evapotranspiration using data driven techniques
under limited data conditions. Model. Earth Syst. Environ. 2017, 3, 1449–1461. [CrossRef]
55.
Broomhead, D.S.; A Lowe, D. Multivariable functional interpolation and adaptive networks. Complex Syst.
1988, 2, 321–355.
56.
Specht, D.F. A general regression neural network. IEEE Trans. Neural Netw. 1991, 2, 568–576. [CrossRef]
57.
Huang, G.-B.; Zhu, Q.-Y.; Siew, C.-K. Extreme learning machine: Theory and applications. Neurocomputing
2006, 70, 489–501. [CrossRef]
58.
Trajkovic, S. Comparison of radial basis function networks and empirical equations for converting from pan
evaporation to reference evapotranspiration. Hydrol. Process. 2009, 23, 874–880. [CrossRef]
59.
Feng, Y.; Cui, N.; Gong, D.; Zhang, Q.; Zhao, L. Evaluation of random forests and generalized regression
neural networks for daily reference evapotranspiration modelling. Agric. Water Manag. 2017, 193, 163–173.
[CrossRef]
60.
Feng, Y.; Peng, Y.; Cui, N.; Gong, D.; Zhang, K. Modeling reference evapotranspiration using extreme learning
machine and generalized regression neural network only with temperature data. Comput. Electron. Agric.
2017, 136, 71–78. [CrossRef]
61.
Traore, S.; Wang, Y.-M.; Kerh, T. Artiﬁcial neural network for modeling reference evapotranspiration complex
process in Sudano-Sahelian zone. Agric. Water Manag. 2010, 97, 707–714. [CrossRef]
62.
Yassin, M.A.; Alazba, A.A.; Mattar, M.A. Artiﬁcial neural networks versus gene expression programming for
estimating reference evapotranspiration in arid climate. Agric. Water Manag. 2016, 163, 110–124. [CrossRef]
63.
Rahimikhoob, A. Comparison of M5 Model Tree and Artiﬁcial Neural Network’s Methodologies in Modelling
Daily Reference Evapotranspiration from NOAA Satellite Images. Water Resour. Manag. 2016, 30, 3063–3075.
[CrossRef]
64.
Gocic, M.; Petkovi´c, D.; Shamshirband, S.; Kamsin, A. Comparative analysis of reference evapotranspiration
equations modelling by extreme learning machine. Comput. Electron. Agric. 2016, 127, 56–63. [CrossRef]
65.
Patil, A.P.; Deka, P.C. An extreme learning machine approach for modeling evapotranspiration using extrinsic
inputs. Comput. Electron. Agric. 2016, 121, 385–392. [CrossRef]
66.
Hashemi, M.; Sepaskhah, A.R. Evaluation of artiﬁcial neural network and Penman–Monteith equation for
the prediction of barley standard evapotranspiration in a semi-arid region. Theor. Appl. Climatol. 2019.
[CrossRef]
67.
Abrishami, N.; Sepaskhah, A.R.; Shahrokhnia, M.H. Estimating wheat and maize daily evapotranspiration
using artiﬁcial neural network. Theor. Appl. Climatol. 2018, 135, 945–958. [CrossRef]
68.
Vapnik, V. The Nature of Statistical Learning Theory, 2nd ed.; Springer-Verlag: New York, NY, USA, 1995.
69.
Cortes, C.; Vapnik, V. Support-vector networks. Mach. Learn. 1995, 20, 273–297. [CrossRef]
70.
Raghavendra, N.S.; Deka, P.C. Support vector machine applications in the ﬁeld of hydrology: A review.
Appl. Soft Comput. 2014, 19, 372–386. [CrossRef]
71.
Zendehboudi, A.; Baseer, M.A.; Saidur, R. Application of support vector machine models for forecasting
solar and wind energy resources: A review. J. Clean. Prod. 2018, 199, 272–285. [CrossRef]
72.
Kisi, O.; Cimen, M. Evapotranspiration modelling using support vector machines. Hydrol. Sci. J. 2010, 54,
918–928. [CrossRef]
73.
Mehdizadeh, S.; Behmanesh, J.; Khalili, K. Using MARS, SVM, GEP and empirical equations for estimation
of monthly mean reference evapotranspiration. Comput. Electron. Agric. 2017, 139, 103–114. [CrossRef]
74.
Mohammadrezapour, O.; Piri, J.; Kisi, O. Comparison of SVM, ANFIS and GEP in modeling monthly potential
evapotranspiration in an arid region (Case study: Sistan and Baluchestan Province, Iran). Water Supply 2019,
19, 392–403. [CrossRef]
Agronomy 2020, 10, 101
30 of 33
75.
Ferreira, L.B.; da Cunha, F.F.; de Oliveira, R.A.; Fernandes Filho, E.I. Estimation of reference evapotranspiration
in Brazil with limited meteorological data using ANN and SVM—A new approach. J. Hydrol. 2019, 572,
556–570. [CrossRef]
76.
Kumar, D.; Adamowski, J.; Suresh, R.; Ozga-Zielinski, B. Estimating Evapotranspiration Using an Extreme
Learning Machine Model: Case Study in North Bihar, India. J. Irrig. Drain. Eng. 2016, 142, 04016032.
[CrossRef]
77.
Fan, J.; Yue, W.; Wu, L.; Zhang, F.; Cai, H.; Wang, X.; Lu, X.; Xiang, Y. Evaluation of SVM, ELM and four
tree-based ensemble models for predicting daily reference evapotranspiration using limited meteorological
data in diﬀerent climates of China. Agric. For. Meteorol. 2018, 263, 225–241. [CrossRef]
78.
Shrestha, N.K.; Shukla, S. Support vector machine based modeling of evapotranspiration using hydro-climatic
variables in a sub-tropical environment. Agric. For. Meteorol. 2015, 200, 172–184. [CrossRef]
79.
Zadeh, L.A. Fuzzy sets. Inf. Control 1965, 8, 38–53. [CrossRef]
80.
Kisi, O. Applicability of Mamdani and Sugeno fuzzy genetic approaches for modeling reference
evapotranspiration. J. Hydrol. 2013, 504, 160–170. [CrossRef]
81.
Keskin, M.E.; Terzi, Ö.; Taylan, D. Fuzzy logic model approaches to daily pan evaporation estimation in
western Turkey / Estimation de l’évaporation journalière du bac dans l’Ouest de la Turquie par des modèles
à base de logique ﬂoue. Hydrol. Sci. J. 2004, 49, 1001–1010. [CrossRef]
82.
Goyal, M.K.; Bharti, B.; Quilty, J.; Adamowski, J.; Pandey, A. Modeling of daily pan evaporation in sub
tropical climates using ANN, LS-SVR, Fuzzy Logic, and ANFIS. Expert Syst. Appl. 2014, 41, 5267–5276.
[CrossRef]
83.
Jang, J.R. Self-learning fuzzy controllers based on temporal backpropagation. IEEE Trans. Neural Netw. 1992,
3, 714–723. [CrossRef]
84.
Kisi, Ö.; Öztürk, Ö. Adaptive Neurofuzzy Computing Technique for Evapotranspiration Estimation. J. Irrig.
Drain. Eng. 2007, 133, 368–379. [CrossRef]
85.
Pour-Ali Baba, A.; Shiri, J.; Kisi, O.; Fard, A.F.; Kim, S.; Amini, R. Estimating daily reference evapotranspiration
using available and estimated climatic data by adaptive neuro-fuzzy inference system (ANFIS) and artiﬁcial
neural network (ANN). Hydrol. Res. 2013, 44, 131–146. [CrossRef]
86.
Petkovi´c, D.; Gocic, M.; Trajkovic, S.; Shamshirband, S.; Motamedi, S.; Hashim, R.; Bonakdari, H.
Determination of the most inﬂuential weather parameters on reference evapotranspiration by adaptive
neuro-fuzzy methodology. Comput. Electron. Agric. 2015, 114, 277–284. [CrossRef]
87.
Keshtegar, B.; Kisi, O.; Ghohani Arab, H.; Zounemat-Kermani, M. Subset Modeling Basis ANFIS for Prediction
of the Reference Evapotranspiration. Water Resour. Manag. 2017, 32, 1101–1116. [CrossRef]
88.
Kisi, O.; Sanikhani, H.; Zounemat-Kermani, M.; Niazi, F. Long-term monthly evapotranspiration modeling
by several data-driven methods without climatic data. Comput. Electron. Agric. 2015, 115, 66–77. [CrossRef]
89.
Gavili, S.; Sanikhani, H.; Kisi, O.; Mahmoudi, M.H. Evaluation of several soft computing methods in monthly
evapotranspiration modelling. Meteorol. Appl. 2018, 25, 128–138. [CrossRef]
90.
Seiﬁ, A.; Riahi, H. Estimating daily reference evapotranspiration using hybrid gamma test-least square
support vector machine, gamma test-ANN, and gamma test-ANFIS models in an arid area of Iran. J. Water
Clim. Chang. 2018. [CrossRef]
91.
Cobaner, M. Evapotranspiration estimation by two diﬀerent neuro-fuzzy inference systems. J. Hydrol. 2011,
398, 292–302. [CrossRef]
92.
Kisi, O.; Zounemat-Kermani, M. Comparison of Two Diﬀerent Adaptive Neuro-Fuzzy Inference Systems in
Modelling Daily Reference Evapotranspiration. Water Resour. Manag. 2014, 28, 2655–2675. [CrossRef]
93.
Breiman, L. Classiﬁcation and Regression Trees; Routledge: Abinton on the Thames, UK, 1984.
94.
Quinlan, J.R. Learning with continuous classes. In Proceedings of the Australian Joint Conference on
Artiﬁcial Intelligence, Singapore; World Scientiﬁc Press: Singapore, 1992; pp. 343–348.
95.
Pal, M.; Deswal, S. M5 model tree based modelling of reference evapotranspiration. Hydrol. Process. 2009, 23,
1437–1443. [CrossRef]
96.
Rahimikhoob, A.; Asadi, M.; Mashal, M. A Comparison Between Conventional and M5 Model Tree Methods
for Converting Pan Evaporation to Reference Evapotranspiration for Semi-Arid Region. Water Resour. Manag.
2013, 27, 4815–4826. [CrossRef]
97.
Kisi, O.; Kilic, Y. An investigation on generalization ability of artiﬁcial neural networks and M5 model tree in
modeling reference evapotranspiration. Theor. Appl. Climatol. 2015, 126, 413–425. [CrossRef]
Agronomy 2020, 10, 101
31 of 33
98.
Kisi, O. Modeling reference evapotranspiration using three diﬀerent heuristic regression approaches.
Agric. Water Manag. 2016, 169, 162–172. [CrossRef]
99.
Fung, K.F.; Huang, Y.F.; Koo, C.H.; Soh, Y.W. Drought forecasting: A review of modelling approaches
2007–2017. J. Water Clim. Chang. 2019. [CrossRef]
100. Galar, M.; Fernandez, A.; Barrenechea, E.; Bustince, H.; Herrera, F. A Review on Ensembles for the Class
Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches. IEEE Trans. Syst. ManCybern.
Part C (Appl. Rev.) 2012, 42, 463–484. [CrossRef]
101. Palmer, T.N.; Doblas-Reyes, F.J.; Hagedorn, R.; Weisheimer, A. Probabilistic prediction of climate using
multi-model ensembles: From basics to applications. Philos. Trans. R. Soc. Lond. B Biol. Sci. 2005, 360,
1991–1998. [CrossRef] [PubMed]
102. Taylor, K.E. Summarizing multiple aspects of model performance in a single diagram. J. Geophys. Res. Atmos.
2001, 106, 7183–7192. [CrossRef]
103. Yao, Y.; Liang, S.; Li, X.; Zhang, Y.; Chen, J.; Jia, K.; Zhang, X.; Fisher, J.B.; Wang, X.; Zhang, L.; et al. Estimation
of high-resolution terrestrial evapotranspiration from Landsat data using a simple Taylor skill fusion method.
J. Hydrol. 2017, 553, 508–526. [CrossRef]
104. Breiman, L. Bagging predictors. Mach. Learn. 1996, 24, 123–140. [CrossRef]
105. Kim, S.; Singh, V.P.; Seo, Y.; Kim, H.S. Modeling Nonlinear Monthly Evapotranspiration Using Soft Computing
and Data Reconstruction Techniques. Water Resour. Manag. 2013, 28, 185–206. [CrossRef]
106. Höge, M.; Guthke, A.; Nowak, W. The hydrologist’s guide to Bayesian model selection, averaging and
combination. J. Hydrol. 2019, 572, 96–107. [CrossRef]
107. Draper, D. Assessment and Propagation of Model Uncertainty. J. R. Stat. Soc. Ser. B (Methodol.) 1995, 57,
45–70. [CrossRef]
108. Zhu, G.; Su, Y.; Li, X.; Zhang, K.; Li, C. Estimating actual evapotranspiration from an alpine grassland
on Qinghai-Tibetan plateau using a two-source model and parameter uncertainty analysis by Bayesian
approach. J. Hydrol. 2013, 476, 42–51. [CrossRef]
109. Zhu, G.; Li, X.; Zhang, K.; Ding, Z.; Han, T.; Ma, J.; Huang, C.; He, J.; Ma, T. Multi-model ensemble prediction
of terrestrial evapotranspiration across north China using Bayesian model averaging. Hydrol. Process. 2016,
30, 2861–2879. [CrossRef]
110. Chen, Y.; Yuan, W.; Xia, J.; Fisher, J.B.; Dong, W.; Zhang, X.; Liang, S.; Ye, A.; Cai, W.; Feng, J. Using Bayesian
model averaging to estimate terrestrial evapotranspiration in China. J. Hydrol. 2015, 528, 537–549. [CrossRef]
111. Zhao, T.; Wang, Q.J.; Schepen, A. A Bayesian modelling approach to forecasting short-term reference crop
evapotranspiration from GCM outputs. Agric. For. Meteorol. 2019, 269–270, 88–101. [CrossRef]
112. Khoshravesh, M.; Seﬁdkouhi, M.A.G.; Valipour, M. Estimation of reference evapotranspiration using
multivariate fractional polynomial, Bayesian regression, and robust regression models in three arid
environments. Appl. Water Sci. 2015, 7, 1911–1922. [CrossRef]
113. Hassan, M.A.; Khalil, A.; Kaseb, S.; Kassem, M.A. Exploring the potential of tree-based ensemble methods in
solar radiation modeling. Appl. Energy 2017, 203, 897–916. [CrossRef]
114. Friedman, J.H. Greedy function approximation: A gradient boosting machine. Ann. Stat. 2001, 29, 1189–1232.
[CrossRef]
115. Freund, Y.; Schapire, R.E. A Decision-Theoretic Generalization of On-Line Learning and an Application to
Boosting. J. Comput. Syst. Sci. 1997, 55, 119–139. [CrossRef]
116. Chen, T.; Guestrin, C. XGBoost: A scalable tree boosting system.
In Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, 2016;
pp. 785–794.
117. Prokhorenkova, L.; Gusev, G.; Vorobev, A.; Dorogush, A.V.; Gulin, A. CatBoost: Unbiased boosting with
categorical features. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, Montréal, QC, Canada, 3–8 December 2018; pp. 6639–6649.
118. Ponraj, A.S.; Vigneswaran, T. Daily evapotranspiration prediction using gradient boost regression model for
irrigation planning. J. Supercomput. 2019. [CrossRef]
119. Fan, J.; Ma, X.; Wu, L.; Zhang, F.; Yu, X.; Zeng, W. Light Gradient Boosting Machine: An eﬃcient soft
computing model for estimating daily reference evapotranspiration with local and external meteorological
data. Agric. Water Manag. 2019, 225, 105758. [CrossRef]
Agronomy 2020, 10, 101
32 of 33
120. El-Shaﬁe, A.; Alsulami, H.M.; Jahanbani, H.; Najah, A. Multi-lead ahead prediction model of reference
evapotranspiration utilizing ANN with ensemble procedure. Stoch. Environ. Res. Risk Assess. 2012, 27,
1423–1440. [CrossRef]
121. El-Shaﬁe, A.; Najah, A.; Alsulami, H.M.; Jahanbani, H. Optimized Neural Network Prediction Model
for Potential Evapotranspiration Utilizing Ensemble Procedure. Water Resour. Manag. 2014, 28, 947–967.
[CrossRef]
122. Zhang, Z.; Gong, Y.; Wang, Z. Accessible remote sensing data based reference evapotranspiration estimation
modelling. Agric. Water Manag. 2018, 210, 59–69. [CrossRef]
123. Dou, X.; Yang, Y. Evapotranspiration estimation using four diﬀerent machine learning approaches in diﬀerent
terrestrial ecosystems. Comput. Electron. Agric. 2018, 148, 95–106. [CrossRef]
124. Carter, C.; Liang, S. Evaluation of ten machine learning methods for estimating terrestrial evapotranspiration
from remote sensing. Int. J. Appl. Earth Obs. Geoinf. 2019, 78, 86–92. [CrossRef]
125. Knipper, K.; Hogue, T.; Scott, R.; Franz, K. Evapotranspiration estimates derived using multi-platform remote
sensing in a semiarid region. Remote Sens. 2017, 9, 184. [CrossRef]
126. Knipper, K.R.; Kustas, W.P.; Anderson, M.C.; Alﬁeri, J.G.; Prueger, J.H.; Hain, C.R.; Gao, F.; Yang, Y.;
McKee, L.G.; Nieto, H.; et al. Evapotranspiration estimates derived using thermal-based satellite remote
sensing and data fusion for irrigation management in California vineyards. Irrig. Sci. 2018, 37, 431–449.
[CrossRef]
127. Feng, G.; Masek, J.; Schwaller, M.; Hall, F. On the blending of the Landsat and MODIS surface reﬂectance:
Predicting daily Landsat surface reﬂectance. IEEE Trans. Geosci. Remote Sens. 2006, 44, 2207–2218. [CrossRef]
128. Zhu, X.; Chen, J.; Gao, F.; Chen, X.; Masek, J.G. An enhanced spatial and temporal adaptive reﬂectance fusion
model for complex heterogeneous regions. Remote Sens. Environ. 2010, 114, 2610–2623. [CrossRef]
129. Hilker, T.; Wulder, M.A.; Coops, N.C.; Seitz, N.; White, J.C.; Gao, F.; Masek, J.G.; Stenhouse, G. Generation of
dense time series synthetic Landsat data through data blending with MODIS using a spatial and temporal
adaptive reﬂectance fusion model. Remote Sens. Environ. 2009, 113, 1988–1999. [CrossRef]
130. Cammalleri, C.; Anderson, M.C.; Gao, F.; Hain, C.R.; Kustas, W.P. A data fusion approach for mapping daily
evapotranspiration at ﬁeld scale. Water Resour. Res. 2013, 49, 4672–4686. [CrossRef]
131. Cammalleri, C.; Anderson, M.C.; Gao, F.; Hain, C.R.; Kustas, W.P. Mapping daily evapotranspiration at ﬁeld
scales over rainfed and irrigated agricultural areas using remote sensing data fusion. Agric. For. Meteorol.
2014, 186, 1–11. [CrossRef]
132. Semmens, K.A.; Anderson, M.C.; Kustas, W.P.; Gao, F.; Alﬁeri, J.G.; McKee, L.; Prueger, J.H.; Hain, C.R.;
Cammalleri, C.; Yang, Y.; et al. Monitoring daily evapotranspiration over two California vineyards using
Landsat 8 in a multi-sensor data fusion approach. Remote Sens. Environ. 2016, 185, 155–170. [CrossRef]
133. Ma, Y.; Liu, S.; Song, L.; Xu, Z.; Liu, Y.; Xu, T.; Zhu, Z. Estimation of daily evapotranspiration and irrigation
water eﬃciency at a Landsat-like scale for an arid irrigation area using multi-source remote sensing data.
Remote Sens. Environ. 2018, 216, 715–734. [CrossRef]
134. Alavi, N.; Warland, J.S.; Berg, A.A. Filling gaps in evapotranspiration measurements for water budget studies:
Evaluation of a Kalman ﬁltering approach. Agric. For. Meteorol. 2006, 141, 57–66. [CrossRef]
135. Peters-Lidard, C.D.; Kumar, S.V.; Mocko, D.M.; Tian, Y. Estimating evapotranspiration with land data
assimilation systems. Hydrol. Process. 2011, 25, 3979–3992. [CrossRef]
136. Yin, J.; Zhan, C.; Ye, W. An Experimental Study on Evapotranspiration Data Assimilation Based on the
Hydrological Model. Water Resour. Manag. 2016, 30, 5263–5279. [CrossRef]
137. Partal, T. Modelling evapotranspiration using discrete wavelet transform and neural networks. Hydrol. Process.
2009, 23, 3545–3555. [CrossRef]
138. Partal, T. Comparison of wavelet based hybrid models for daily evapotranspiration estimation using
meteorological data. KSCE J. Civ. Eng. 2015, 20, 2050–2058. [CrossRef]
139. Adamala, S.; Raghuwanshi, N.S.; Mishra, A.; Singh, R. Generalized wavelet neural networks for
evapotranspiration modeling in India. ISH J. Hydraul. Eng. 2017, 25, 119–131. [CrossRef]
140. Adamala, S. Temperature based generalized wavelet-neural network models to estimate evapotranspiration
in India. Inf. Process. Agric. 2018, 5, 149–155. [CrossRef]
141. Patil, A.P.; Deka, P.C. Performance evaluation of hybrid Wavelet-ANN and Wavelet-ANFIS models for
estimating evapotranspiration in arid regions of India. Neural Comput. Appl. 2015, 28, 275–285. [CrossRef]
Agronomy 2020, 10, 101
33 of 33
142. Cobaner, M. Reference evapotranspiration based on Class A pan evaporation via wavelet regression technique.
Irrig. Sci. 2011, 31, 119–134. [CrossRef]
143. Adarsh, S.; Sanah, S.; Murshida, K.K.; Nooramol, P. Scale dependent prediction of reference evapotranspiration
based on Multi-Variate Empirical mode decomposition. Ain Shams Eng. J. 2018, 9, 1839–1848. [CrossRef]
144. Misaghian, N.; Shamshirband, S.; Petkovi´c, D.; Gocic, M.; Mohammadi, K. Predicting the reference
evapotranspiration based on tensor decomposition. Theor. Appl. Climatol. 2016, 130, 1099–1109. [CrossRef]
© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/).


Paper 3:
- APA Citation: None
  Main Objective: Examines architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms
  Study Location: None
  Data Sources: []
  Technologies Used: []
  Key Findings: []
  Extract 1: 
  Extract 2: 
  Limitations: >
  Relevance Evaluation: {'extract_1': 'Architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms', 'extract_2': None, 'limitations': None, 'relevance_score': 1.0}
  Relevance Score: 1.0
  Inline Citation: None
  Explanation: The HASTE pipeline model consists of two main components: an interestingness function (IF) and a policy. The IF is a user-defined function that assigns an interestingness score to each data object in the stream, based on specific characteristics or features of the object. This score represents the object's relevance or importance for a given task or analysis. The policy, also defined by the user, determines how the objects are organized and allocated into a data hierarchy (DH) based on their interestingness scores. By intelligently prioritizing data objects based on their interestingness, the HASTE pipeline model enables more efficient utilization of storage and compute resources, as well as the automation of data management and resource allocation tasks.

 Full Text: >
Journal of XYZ, 2017, 1–11
doi: xx.xxxx/xxxx
Manuscript in Preparation
Paper
P A P E R
Rapid development of cloud-native intelligent data
pipelines for scientifc data streams using the
HASTE Toolkit
Ben Blamey1,*, Salman Toor1, Martin Dahlö2,3, Håkan Wieslander1, Philip J
Harrison2,3, Ida-Maria Sintorn1,3,4, Alan Sabirsh5, Carolina Wählby1,3, Ola
Spjuth2,3,† and Andreas Hellander1,†
1Department of Information Technology, Uppsala University, Sweden and 2Department of Pharmaceutical
Biosciences and Science for Life Laboratory, Uppsala University, Sweden and 3Science for Life Laboratory,
Uppsala University and 4Vironova AB, Stockholm, Sweden and 5Advanced Drug Delivery, Pharmaceutical
Sciences, R&D, AstraZeneca, Gothenburg, Sweden
*Correspondence: ben.blamey@it.uu.se
†Co-senior authors
Abstract
This paper introduces the HASTE Toolkit, a cloud-native software toolkit capable of partitioning data streams in order to
prioritize usage of limited resources. This in turn enables more efcient data-intensive experiments. We propose a model
that introduces automated, autonomous decision making in data pipelines, such that a stream of data can be partitioned
into a tiered or ordered data hierarchy. Importantly, the partitioning is online and based on data content rather than a priori
metadata. At the core of the model are interestingness functions and policies. Interestingness functions assign a quantitative
measure of interestingness to a single data object in the stream, an interestingness score. Based on this score, a policy
guides decisions on how to prioritize computational resource usage for a given object. The HASTE Toolkit is a collection of
tools to adapt data stream processing to this pipeline model. The result is smart data pipelines capable of efective or even
optimal use of e.g. storage, compute and network bandwidth, to support experiments involving rapid processing of
scientifc data characterized by large individual data object sizes. We demonstrate the proposed model and our toolkit
through two microscopy imaging case studies, each with their own interestingness functions, policies, and data hierarchies.
The frst deals with a high content screening experiment, where images are analyzed in an on-premise container cloud
with the goal of prioritizing the images for storage and subsequent computation. The second considers edge processing of
images for upload into the public cloud for a real-time control loop for a transmission electron microscope.
Key words: Stream Processing, Interestingness Functions, HASTE, Tiered Storage, Image Analysis
Introduction
Large datasets are both computationally and fnancially expen-
sive to process, transport and store. Such datasets are ubiq-
uitous throughout the life sciences, including imaging, where
diferent types of microscopy are used to e.g.
observe and
quantify efects of drugs on cell morphology. Modern imag-
ing techniques can generate image streams at rates of up to
1TB/hour [1]. Clearly, the processing, storing and communi-
cation of these images can be slow, resource-intensive and ex-
pensive, efectively becoming a bottleneck to scale experiments
in support of data-driven life science. Another prominent ex-
Compiled on: September 13, 2020.
Draft manuscript prepared by the author.
1
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
2
|
Journal of XYZ, 2017, Vol. 00, No. 0
Key Points
• We propose a pipeline model for building intelligent pipelines for streams, accounting for actual information content in
data rather than a priori metadata, and present the HASTE Toolkit, a cloud-native software toolkit for supporting rapid
development according to the proposed model.
• We demonstrate how the HASTE Toolkit enables intelligent resource optimization in two image analysis case studies based
on a) high-content imaging and b) transmission electron microscopy.
• We highlight the challenges of storage, processing and transfer in streamed high volume, high velocity scientifc data for
both cloud and cloud-edge use cases.
ample is human genome sequencing, where the global storage
requirements is predicted to be between 2 and 40 exabytes (1
exabyte = 1018 bytes) by 2025, and with modern techniques
generating data at the order of ~60 GB/h [2].
Similarly, in
large-scale modeling, a single computational experiment in a
systems biology context can generate terabytes of data [3].
This work is motivated by some of the most critical as-
pects of scalable scientifc discovery for spatial and temporal
image data. There are two primary concerns: (1) not all data
is equally valuable. With datasets outgrowing resources, data
storage should be prioritized for data that is most relevant (or
interesting) for the study at hand and poor quality, or unin-
teresting, data (e.g. out-of-focus images) should be discarded
or archived; (2) when resources are limited, or if decisions are
required in real-time, we have to be smart about how the data
is (pre)processed and which subsets of the data are stored for
more detailed (and potentially computer intensive) analysis –
prioritizing more interesting subsets of the data.
The general challenges of management and availability of
large datasets are often popularized and summarized trough
the so-called Vs of big data. Initially, the focus was on the three
Vs: velocity, volume and variety, but this list has since grown
with the increasing number of new use-cases to also include Vs
such as veracity, variability, virtualization and value [4]. Dur-
ing the last decade, a number of frameworks have been de-
signed to address these challenges, ofering reliable, efcient
and secure large-scale data management solutions. However,
according to a white paper published by IDC [5], only 30% of
the generated data is in the form that it can be efciently ana-
lyzed. This highlights the current gap between large-scale data
management and efcient data analysis. To close this gap, it
is essential to design and develop intelligent data management
frameworks that can help organize the available datasets for
efcient analyses. In this work, we address this challenge by
proposing a model that helps a data pipeline developer make
online decisions about individual data objects’1 priority based
in actual information content, or interestingness, rather than
traditional metadata.
A range of existing work in life science applications has dis-
cussed the challenges of transporting, storing and analyzing
data, often advocating a streamed approach. In [6] the authors
explicitly discuss the constraints of cloud upload bandwidth,
and its efect on overall throughput for mass-spectrometry
based metabolomics.
In their application, uploading large
datasets from the instrument to the cloud represents a bottle-
neck and they advocate a stream-based approach with online
analysis where data is processed when it arrives, rather than
waiting for upload of the complete dataset. Hillman et al. [7]
developed a stream based pipeline with Apache Flink and Kafka
for processing of proteomics data from liquid chromatography-
1 We use the generic term data object but note that analogous terms in
various contexts include: documents, messages and blobs
mass spectrometry (LC/MS) and note the advantages of a real-
time approach to analysis: “a scientist could see what is hap-
pening in real-time and possibly stop a problematic experi-
ment to save time”. Zhang et al. [8] developed a client/server
application for interactive visualization of MS spectra, adopt-
ing a stream-based approach to achieve better user interactiv-
ity. In genomics, [9] presented the htsget protocol to enable
clients to download genomic data in a more fne-grained fash-
ion, and allow for processing chunks as they come from the
sequencer. In [10], the authors note that a single electron mi-
croscope can produce 1 TB of images per day, requiring a min-
imum of 1000 CPU hours for analysis. Adapting their Scipion
software [11] (intended for Cryo EM image analysis) for use in
the cloud, they discuss the challenges of data transfer to/from
the cloud, comparing transfer rates for diferent providers. [12]
proposes excluding outliers in streaming data, using an ‘Outlier
Detection and Removal’ (ODR) algorithm which they evaluate
on fve bioinformatics datasets.
Rather than handling one particular type of data or dealing
with a specifc data pipeline, the aim of the present work is
to distill efective architectural patterns into a pipeline model
to allow for repeatable implementations of smart systems ca-
pable of online resource prioritization in scenarios involving
large-scale data production, such as from a scientifc instru-
ment. Computers in the lab connected directly to such an in-
strument, used together with cloud resources, are an exam-
ple of edge computing [13]. Under that paradigm, computa-
tional resources outside the cloud (such as mobile devices, and
more conventional compute nodes) are used in conjunction
with cloud computing resources to deliver benefts to an appli-
cation such as reduced cost, better performance, or improved
user experience. General computer science challenges include
security, deployment, software complexity, and resource man-
agement/workload allocation. In our context, the streams of
large data objects generated by scientifc instruments create
particular challenges within the edge computing paradigm as
the data often needs to be uploaded to the cloud for process-
ing, storage, or wider distribution. Whilst limited compute re-
sources at the edge are often insufcient for low-latency pro-
cessing of these datasets, intelligent workload allocation can
improve throughput (as discussed in Case Study 2).
In this paper we propose a pipeline model for partitioning
and prioritizing stream datasets into data hierarchies (DHs) ac-
cording to an interestingness function (IF) and accompanying pol-
icy, applied to objects in the stream, for more efective use of
hardware (in edge and cloud contexts).
We present this as
a general approach to mitigating resource management chal-
lenges, with a focus on image data. Our model allows for au-
tonomous decision making, while providing a clear model for
domain experts to manage the resources in distributed systems
– by encoding domain expertise via the IF. To that end, this pa-
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
Blamey et al.
|
3
per introduces the HASTE2 Toolkit, intended for developing in-
telligent stream processing pipelines based on this model. Two
case studies presented in this paper document how microscopy
pipelines can be adapted to the HASTE pipeline model.
Whilst the core ideas of intelligent data pipelines in HASTE
is generally applicable to many scenarios involving scien-
tifc datasets, we here focus on case-studies involving image
streams, in particular from microscopy.
Background: Stream Processing and Workfow
Management
A fundamental component to the pipelines presented in this
paper is a stream processing engine.
Systems for stream
processing are generally concerned with high frequency mes-
sage infux, and those objects can be small in size, such as
a few KB. Examples of such data objects include sensor read-
ings from IoT devices (such as MQTT messages), those gener-
ated from telecoms, web and cloud applications, e-commerce
and fnancial applications, or the aggregation and analysis
log entries.
Well-known examples of mature, enterprise-
grade frameworks for cloud-based stream processing in these
contexts include Apache Flink, Apache Spark Streaming, and
Apache Log Flume. Resilience and fault tolerance are key fea-
tures of these frameworks (often achieved with various forms
of redundancy and replication). These frameworks are com-
monly used in conjunction with various queuing applications,
e.g., Apache Kafka, and vendor-specifc products such as AWS
Kinesis – these also include basic processing functionality.
Whilst the maturity, support, documentation, features and
performance (order of MHz message processing throughput)
boasted by these frameworks is attractive for scientifc com-
puting application, streamed scientifc data (and its process-
ing) tends to have diferent characteristics: data objects used in
scientifc computing applications (such as microscopy images,
and matrices from other scientifc computing domains) can be
larger in size, which can create performance issues when inte-
grated with these enterprise frameworks described above [14].
For example, data object sizes in imaging applications could be
a few MB.
To address this gap, we have previously developed and re-
ported on a stream processing framework focusing on scien-
tifc computing applications, HarmonicIO [15].
HarmonicIO
sacrifces some of these features, and is intended for lower-
frequency applications (towards kHz, not MHz), and was able
to achieve better streaming performance under some condi-
tions in one study for larger message sizes [14]. The HASTE
Toolkit has been developed with images as the primary use-
case and for this reason HarmonicIO is the default supported
streaming framework in the toolkit. However, we stress here
that in principle any streaming framework can be used.
Furthermore,
under
the
emerging
edge
computing
paradigm,
there are some stream processing frameworks
available, often focusing on traditional IoT use-cases. Being
in their infancy, efective automated scheduling and operator
placement in hybrid edge/cloud deployment scenarios remains
an open research challenge for this context. Within this area,
there is signifcant research efort concerning real-time video
analysis, where images collected at the edge (from cameras)
are streamed to the cloud for analysis – some degree of lossy
compression is typically used in such applications.
By contrast, workfow frameworks are broad class of soft-
ware frameworks intended to facilitate the development of
2 HASTE: Hierarchical Analysis of Spatial and Temporal Data http://haste.
research.it.uu.se/
data-processing pipelines. There are a large number of such
frameworks (more than 100 are listed in [16]). In such frame-
works, one generally defnes processing operations (often as
the invocation of external processes), which are triggered by
events such as the creation of a new fle on disk, or a commit be-
ing pushed to a Git repository. Such frameworks generally han-
dle large numbers of fles, of arbitrary size, and often include
some degree of fault tolerance. But in contrast to stream pro-
cessing frameworks, they may lack functionality specifc for
streams, such as window operations, more complex schedul-
ing and placing of operators, and are generally intended for
higher latency and/or lower ingress rates (than the 100kHz+
range of the stream processing frameworks described above),
and are often fle-system centric, with objects being written
back to disk between each processing step.
The HASTE toolkit attempts to fll a gap between these two
classes of software (stream processing frameworks, and workfow
management systems): applications where latency and high data
object throughput are important (and use of a flesystem as a
queuing platform are perhaps unsuitable for that reason), but
not as high as some enterprise stream processing applications;
whilst being fexible enough to accommodate a broad range of
integration approaches, processing steps with external tools,
and the large message sizes characteristic of scientifc comput-
ing applications.
The priority-driven approach of the HASTE pipeline model
reconciles the resource requirements of life science pipelines
(characterised by streams relatively of large message, with ex-
pensive per-message processing steps), with the requirements
for low-latency and high throughput, allowing for real time hu-
man supervision, inspection, interactive analysis – as well as
real-time control of laboratory equipment.
HASTE Pipeline Model
The key ideas of the HASTE pipeline model are the use of inter-
estingness functions and a policy to autonomously induce data
hierarchies. These structures are then used to manage and op-
timize diferent objectives such as communication, processing
and storage of the datasets. The HASTE Toolkit enables rapid
constructions of smart pipelines following this model. Central
to the approach is that decisions are made based on actual data
content rather than on a priori metadata associated with the
data objects. The following subsections introduces the compo-
nents of the pipeline model.
Overview
Figure 1 illustrates the proposed HASTE model and logical
architecture.
One or more streaming data sources generate
streams of data objects. The stream then undergoes feature
extraction (relevant to the context) – this data extraction can
be performed in parallel, as an idempotent function of a sin-
gle object. The intention is that computationally cheap initial
feature extraction can be used to prioritize subsequent, more
expensive, downstream processing.
An Interestingness Function (IF) computes an interesting-
ness score for each object from these extracted features. This
computation can be a simple procedure, e.g. to nominate one
of the extracted features as the interestingness score associ-
ated with the data object. In more complex cases it can also be
a machine learning model trained either before the experiment
or online during the experiment that generates the stream. Fi-
nally, a policy is applied which determines where to store the
object within a Data Hierarchy (DH), or send it for further
downstream processing, based on the interestingness scores.
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
4
|
Journal of XYZ, 2017, Vol. 00, No. 0
Figure 1. Logical Architecture for the HASTE pipeline model. A stream of data objects is generated by one or more streaming sources (such as a microscope). These
objects undergo online, automated feature extraction, and an IF is applied with the extracted features as input. This associates an interestingness score with each
object in the stream. A user-defned policy is then used to organize the data objects into a data hierarchy to be used for optimizing subsequent communication,
storage and downstream processing.
Interestingess Functions
The IF is a user provided function, to be applied to the extracted
features from the data objects in the stream. The purpose of
the IF is to associate an interestingness score with each object.
Examples of IFs in image analysis contexts could be features
relating to image quality, detected phenomena in images, etc.
The computed IF score is used for determining priority for
subsequent processing, communication, and/or storage of that
object. In this sense, IFs have some similarities to the con-
cept of document (data object) ‘hotness’ in tiered storage con-
texts, where a more recently accessed ’hot’ document would
be stored in a high-performance tier. Whilst much of that line
of work uses only fle-system information, other work takes
some consideration of the application itself, for example [17]
model access as a Zipf distribution, for a review see [3].
Our present work generalizes the concept of ‘hotness’, in a
number of ways: (1) our IFs always take consideration of se-
mantics at the level of the scientifc application – in the case
of microscopy imaging this could be image focus, or quality
features – perhaps combined with business logic for particular
color channels, etc. – rather than fle system semantics (such
as fle access history). This approach allows an immediate, on-
line decision about the object’s interestingness – rather then
inferring it from subsequent access patterns. (2) Tiered stor-
age is just one potential application of HASTE: we use IFs to
prioritize data objects for storage, compute, and communica-
tion (3) with HASTE, the intention is that users’ confgure IFs
themselves, together with the associated policy. Currently, the
output of the IF is scalar valued. This is intended to assure
smooth integration in cases where the IF is a machine learnt
model, outputting a probability, rank, or some other statistical
measure.
Further,
we propose general software abstractions for
these ideas, and demonstrate the potential benefts of online
interestingness-based prioritization in two case studies: both
in terms of the optimization of various resources (compute,
communication, storage), but also from an experimental and
scientifc viewpoint – selecting the best data (or outliers) for
inspection and further analysis.
Policies for inducing Data Hierarchies
In applications utilizing tiered storage, more interesting data
objects would be saved in higher performance, more expen-
sive tiers – readily accessible for downstream processing (while
less interesting objects could be cheaply archived) – explored
in Case Study 1. Whereas, in an edge computing contexts, we
may want to prioritize data objects for computation at the cloud
edge, to make more efective use of that resource – explored
in Case Study 2. In both cases we refer to these structures as
data hierarchies (DHs). In a HASTE pipeline DHs are ‘induced’
within the source data by the IF and a policy. The policy takes
the interestingness score as input and applies a set of rules to
determine how an object is placed within the DH, for exam-
ple, its allocation within a tiered storage system; or where it
should be stored or processed downstream.
Listing 1 shows
how a user can defne a policy, a simple dictionary mapping
intervals of interestingness scores to the tiers (which are con-
fgured separately). In this paper we demonstrate two forms of
policy: the interval model mentioned above (where the tier is
determined directly from the interestingness score, Case Study
1) and a priority-based policy, where data objects are queued
(according to their interestingness) for upload and processing
(as in Case Study 2).
A beneft of the HASTE pipeline model is the clear role sepa-
ration – all the domain-specifc knowledge is efectively encap-
sulated within the IF whilst the choice of how to form DHs and
optimize storage tier allocation is encapsulated entirely within
the policy. This allows the scientifc question of what consti-
tutes an interesting data object, and the computing infrastruc-
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
Blamey et al.
|
5
ture, or indeed, budgetary, concerns of how to make best use of
computing resources (including storage), to be separated and
worked on by team members with diferent expertise. Impor-
tantly, this de-coupling allows the possibility for IFs to be re-
used among scientists, and between contexts where the data
may be similar, but the dataset size, and available computing
infrastructure, may be diferent.
The HASTE Toolkit
The HASTE Toolkit implements the core functionality needed
for rapidly constructing smart pipelines based on the proposed
model.
HASTE Storage Client
The HASTE Storage Client (HSC) serves as the main entry-
point for the user.
It is confgured with the IF, the pol-
icy, and the confguration associated with the tiers, and pro-
cesses each data object arriving in the stream. It can be in-
stalled as a standalone Python module (see: https://github.
com/HASTE-project/HasteStorageClient, version 0.13 was used
for this study.). It allows a DH to be realized within HASTE
as tiered storage.
It is a library with the core prioritiza-
tion functionality:
it invokes the IF on incoming data ob-
jects,
and applies the policy to form the data hierarchy.
The extracted features are used to compute interestingness
scores,
along with other metadata and logging info,
are
saved in a database by the HSC. It is intended to be adopted
within the Python-based stream processing framework of
choice, an example can be found at:
https://github.com/
HASTE-project/HasteStorageClient/blob/master/example.py. An
existing pipeline can be adapted to use HASTE according to the
following steps:
• Install
the
HSC
from
PyPI
pip install
haste-storage-client, or from source.
• Confgure
one
or
more
storage
tiers
(on
a
HASTE-
compatible storage platform)3.
• Defne an IF for the context – it can use spatial, temporal
or other metadata associated with the data object.
• Run feature extraction on the object prior to invoking the
HSC.
• Deploy a MongoDB instance.
The scripts https://github.
com/HASTE-project/k8s-deployments/ can be adapted for this
purpose.
Other key components of the HASTE Toolkit
This section lists other various components in the HASTE
toolkit, and describes how they relate to the key ideas of IFs,
data hierarchies (DHs) and policies.
The HASTE Agent: A command-line application (developed
for the microscopy use case in Case Study 2), which uploads
new documents on disk to the cloud, whilst performing intel-
ligently prioritized pre-processing of objects waiting to be up-
loaded, so as to minimize the overall upload duration.
(see:
https://github.com/HASTE-project/haste-agent (v0.1 was used
for this study)). The functionality of this tool is discussed in
detail in Case Study 2.
The HASTE Gateway: Cloud gateway service, which receives
data objects in the cloud, and forwards them for further pro-
3 At the time of writing, supported platforms are: OpenStack Swift, Pachy-
derm [18], and POSIX-compatible flesystems.
cessing. Deployed as a Docker container. (see: https://github.
com/HASTE-project/haste-gateway, v0.1 was used in this study.).
The HASTE Report Generator:
An auxiliary command
line
tool
for
exporting
data
from
the
Extracted
Fea-
tures Database.
(see:
https://github.com/HASTE-project/
haste-report-generator).
The Extracted Features Database. MongoDB is used by the
HASTE Storage Client to hold a variety of the metadata: ex-
tracted features, interestingness scores, and tier/DH allocation.
Tiered Storage. Tiered storage is one way that a data hierar-
chy (DH) can be realized. The HSC allows existing storage to be
organized into a tiered storage system, where tiers using vari-
ous drivers built into the HSC can be confgured. In Case Study
2 the tiers are flesystem directories, into which image fles are
binned according to the user-defned policy. The idea is that in
other deployments, less expensive disks/cloud storage could be
used for less interesting data. Note that the policy can also send
data deemed unusable (e.g. quality below a certain threshold)
directly to trash. Tiered storage drivers are managed by the
HASTE storage client.
Our
GitHub
project
page
(https://github.com/
HASTE-project)
showcases
other
components
relating
to
various example pipelines developed within the HASTE project,
including IFs developed for specifc use cases as well as scripts
for automated deployment.
Experiments and Results
In this section we illustrate the utility of the toolkit in two
real-world case studies chosen to demonstrate how the HASTE
pipeline model can be realized in practice to optimize resource
usage in two very diferent infrastructure and deployment sce-
narios. Table 1 summarizes the objectives of the case studies.
Case Study 1 concerns data management for a high-content
screening experiment in a scientifc laboratory at Uppsala Uni-
versity, Sweden. A small on-premises compute cluster running
Kubernetes [19] provides the necessary infrastructure to handle
the immediate data fow from the experiment, but both stor-
age capacity and manual downstream analysis is a concern. We
use the HASTE toolkit to build a pipeline that captures the input
data as an image stream and bins images into tiers according to
image quality. The overall goal is to organize the images into
tiers for subsequent processing, to both ensure that the best
images are allocated to high performance storage for high per-
formance analysis, and to help the scientist prioritize manual
work to appropriate subsets of data.
Case Study 2 concerns processing an image stream from
a transmission electron microscope (TEM). During streaming,
there is an opportunity to pre-process images using a desktop
PC co-located with the microscope, before being uploaded for
stream processing in the cloud. This is an example of an edge
computing [13]) scenario, where very limited but low-latency
local infrastructure is leveraged together with a large cloud in-
frastructure. The ultimate goal is real-time control of the mi-
croscope (see Figure 6), and consequently end-to-end latency
is a key concern. This latency is constrained by image upload
time. Here we develop a pipeline using the HASTE tools with an
IF that predicts the efectiveness of pre-processing individual
images at the edge prior to cloud upload.
Case Study 1 - Smart data management for high-
content imaging experiments
This case study focuses on adoption of the HASTE toolkit in a
high-content microscopy setting – the input is a stream of im-
ages arriving from an automated microscope. This deployment
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
6
|
Journal of XYZ, 2017, Vol. 00, No. 0
Table 1. Overview of the two case studies used in this paper.
Case Study 1 - Cell Profling
Case Study 2 - Real Time Processing with a TEM
Application
High-Content Imaging
Real Time Control of Microscopy
Prioritization of...
Storage
Communication & Compute
Goal
Tiered Storage
Reduce end-to-end latency for cloud upload
Deployment Setting
On-premises Cloud, Kubernetes
Cloud Edge & Public Cloud (SNIC)
Interestingness Function (IF)
CellProfler Pipeline – Image Quality
Estimation of Size Reduction (Sampling, Splines)
Policy
Fixed Interestingness Thresholds
Dynamic Interestingness Rank
Figure 2. Architecture for Case Study 1. In this case study, the DH is realized as
storage tiers. Images streamed from the microscope are saved to disk (Network
Attached Storage). This disk is polled by the ‘client’, which pushes a message
about the new fle to RabbitMQ. Workers pop these messages from the queue,
analyze the image, and move it to the storage tiers confgured in the data
hierarchy, using the HASTE Storage Client, confgured with an appropriate IF
and Policy. Icons indicate the components running as Kubernetes pods.
uses an on-premises compute cluster running Kubernetes with
a local NAS. While we want online analysis, we consider this a
‘high latency’ application – images can remain unprocessed for
some seconds or minutes until compute resources are available.
This is a contrast to Case Study 2, where low-latency process-
ing is a goal.
Image quality is an issue in microscopy: images that have
debris, are out of focus, or unusable for some other reason re-
lating to the experimental setup. Such images can disrupt sub-
sequent automated analysis and are distracting for human in-
spection. Furthermore, their storage, computation and trans-
portation have avoidable performance and fnancial costs.
For this case study, the HASTE toolkit is used to prioritize
storage. The developed IF is a CellProfler pipeline performing
out of focus prediction using the imagequality plugin [20]). The
Policy is a fxed threshold used to bin images into a DH accord-
ing to image quality. See Table 1 for an overview of the case
studies.
Figure 2 illustrates the key components of the architecture:
• Client – monitors the source directory for new image
fles, adding the name of each fle to the queue.
(see:
https://github.com/HASTE-project/cellprofiler-pipeline/
tree/master/client, v3 was used for this study).
• Queue – a RabbitMQ queue to store flenames (and associ-
ated metadata). Version 3.7.15 was used for this study.
• Worker – waits for a flename message (on the queue), runs
a CellProfler pipeline on it, computes an interestingness
score from the CellProfler features (according to a user-
defned function). (see: https://github.com/HASTE-project/
cellprofiler-pipeline/tree/master/worker, v3 was used for
this study)
The deployment scripts for Kubernetes & Helm used to
deploy these services for this study are available at: https:
//github.com/HASTE-project/k8s-deployments, v1.1 was used.
The image; together with its interestingness score and
metadata are passed to the HASTE Storage Client – which allo-
cates the images to Tiered Storage/DH, and saves metadata in
the the Extracted Feature Database. Each image is processed
independently, which simplifes scaling.
The HASTE toolkit simplifes the development, deployment
and confguration of this pipeline – in particular, the interac-
tion between the flesystems used in the input image stream
and archive of the processed images. When using our image
processing pipeline, user efort is focused on (a) defning a
suitable IF and (b) defning a policy which determines how
the output of that function relates to DH allocation (storage
tiers). Both of these are declared within the Kubernetes de-
ployment script. When developing the pipeline itself, one is
able to provide the interestingness score (the output of the IF),
and the policy as arguments to the HASTE tools, and delegate
responsibility to applying the policy (with respect to the stor-
age tiers), recording all associated metadata to the Extracted
Feature Database.
The client, queue and workers are all deployed in Docker
containers. Auto-scaling is confgured for the workers: they
are scaled up when processing images, and scaled back down
again when idle. A message containing the image flenames
(and other metadata) is queued, but the fle content is read
from the NAS for processing and tiering.
The code for the worker is an extension of Distributed-
CellProfler (released as part of CellProfler v3.0) [21]4, which
it to run within AWS5. The key beneft of our containerized
system is that because it runs in Docker, and is not dependent
on AWS services, it can be used for local deployments in labo-
ratory settings, so that images do not need to be uploaded to
the public cloud for processing. Alternatively, our system can
be used with any cloud computing provider able to host Docker
containers. We use the open-source message broker RabbitMQ
in place of Amazon SQS (simple queue service). Our Kubernetes
deployment scripts handle the necessary confguration, and a
beneft of RabbitMQ is that it has a built in management web
GUI. A helper script is provided to confgure the credentials for
the management GUI.
Evaluation
For validation of this case study we simulated analysis and
tiering using a high content screening dataset previously col-
lected in the lab, consisting of 2699 images of cortical neuronal
cells, imaged with an ImageXpress XLS, the dataset is available
at [22]. In doing so, we demonstrate that our system is able to
handle a large number of images. To simulate the microscope,
the images were copied into the source directory, triggering
messages from the client, which were read by workers to ana-
lyze the images (with CellProfler) to extract the relevant fea-
tures from the results, apply the IF, and allocate them to the
tiers according to the policy. Running in our laboratory Ku-
bernetes environment, 17 workers were able to process images
4 Version 3.1.8 was used for this study.
5 https://github.com/CellProfiler/Distributed-CellProfiler
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
Blamey et al.
|
7
Figure 3. Histograms of the PLLS feature scores (top), and when converted to
an Interestingness Score (bottom), by application of the Logistic Function (the
IF for Case Study 1, middle). The vertical lines on the bottom plot indicate tier
boundaries confgured in the policy. c.f. example images in Figure 4
Table 2. Image allocation for Case Study 1.
Tier
Image Count
Data (MB)
Tier A
726
6 789
Tier B
731
6 836
Tier C
606
5 667
Tier D
636
5 947
Total
2699
25 239
simultaneously.
We use the PLLS (Power Log Log Slope) feature as the basis
of our interestingness score, as it has been shown to be a robust
measure of image focus [23]. In this case study, we use the
logistic function f as an IF, applying it to the PLLS feature x, to
compute the interestingness score. The logistic function has
output in the range (0,1):
f(x) =
1
1 + e–k(x–x0)
The PLLS values will depend on a number of factors (such
as magnifcation, number of cells, stainings, exposure times,
etc.). The parameters of this IF can be chosen to ft the modal-
ity, based on a sample of pre-images for calibration. In this
case, we chose (k = 4.5, x0 = –1.4). The policy is defned to
map the interestingness score in the intervals (i/4, (i + 1)/4) for
i ∈ (0, 1, 2, 3) to the respective storage tiers. Figure 3 shows
histograms of the PLLS feature and Interestingness Score.
For this evaluation, these tiers were simply directories on
disk. Any storage system compatible with the HASTE Storage
Client could be used, the key idea is that diferent storage plat-
forms (with diferent performance and cost) can be used for the
diferent tiers. In this case, we simply use the tiers as a con-
venient way to partition the dataset for further analysis and
inspection. Figure 4 shows examples of the images according
Figure 4. Example images from the high content screening dataset (Case Study
1), according to automatically assigned tier. Tier A is the most in-focus, with
the highest PLLS feature values and interestingness scores.
interestingness_function(features):
plls = features[’PLLS’]
int_score = 1/(1 + exp(-(4.5) * (plls - (-1.4))))
return int_score
See: https://github.com/HASTE-project/cellprofiler-pipeline/blob/
master/worker/haste/pipeline/worker/LogisticInterestingnessModel.py
storage_policy:
[ [0.,
0.25, tierD],
[0.25, 0.50, tierC],
[0.50, 0.75, tierB],
[0.75, 1.00, tierA] ]
See: https://github.com/HASTE-project/k8s-deployments/
blob/master/pipeline_worker.yaml
Listing 1: Pseudocode for Image Tier Placement (Case Study 1). The IF is the
logistic function, applied to the previously extracted PLLS feature. The policy
shows thresholds for the diferent tiers.
to tiers, and Table 2 shows the results.
Case Study 2 - Prioritizing analysis of TEM images at
the Cloud Edge
This case study is concerned with the prioritized processing
of a stream of images from a microscope (according to an IF),
applied to a hybrid edge/cloud stream processing deployment
context. In this example, we show how the HASTE tools can fa-
cilitate a better use of constrained upload bandwidth and edge
compute resources. The image stream comes from MiniTEMTM
- a 25keV transmission electron microscope [24] (Vironova,
Sweden), connected to a desktop PC from which the micro-
scope is operated and the image stream received, via propri-
etary driver software. The stream processing application pre-
processes the TEM images locally (i.e. at the cloud edge), to
reduce their image size, with the efect of reducing their up-
load time to the cloud, and hence the end-to-end processing
latency.
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
8
|
Journal of XYZ, 2017, Vol. 00, No. 0
Figure 5. Architecture for Case Study 2, showing internal functionality of the
Haste Desktop Agent at the cloud edge. Images streamed from the microscope
are queued at the edge for uploading after (potential) pre-processing. The DH
is realized as a priority queue. Images are prioritized in this queue depending
on the IF which estimates the extent of their size reduction under this pre-
processing operator: those with a greater estimated reduction are prioritized
for processing (vice-versa for upload). This estimate is calculated by interpo-
lating the reduction achieved in nearby images (see Figure 7). This estimated
spline is the IF for this case study.
The purpose of the pipeline is to automate a typical work-
fow for TEM analysis, which proceeds as follows: a sample is
loaded into the microscope (in this case a tissue sample), the
operator performs an ‘initial sweep’ over the sample at low
magnifcation, to locate target (i.e. interesting) regions of the
sample. In the conventional workfow, the search for ‘target’
areas of the sample is done by human inspection. The opera-
tor then images identifed target areas of the sample at higher
magnifcation for subsequent visual/digital analysis.
Automating this process entails the detection of target re-
gions of the sample using an automated image processing
pipeline, based on a set of images from the initial sweep. Such a
pipeline would output machine-readable instructions to direct
the microscope to perform the high magnifcation imaging, re-
ducing the need for human supervision of sample imaging. The
image processing pipeline used to detect target regions can be
costly and slow and could hence preferably be performed in
the cloud. Performing image processing in the cloud has sev-
eral advantages: it allows short-term rental of computing re-
sources without incurring the costs associated with up-front
hardware investment and on-premises management of hard-
ware. Machines with GPUs for deep learning, as well as secure,
backed-up storage of images in the cloud, are available accord-
ing to a pay-per-use model. With our overall aim of supporting
a real-time control loop, and given the expense of the equip-
ment, sample throughput is important. Despite images being
compressed as PNGs, upload bandwidth is a bottleneck. Note
that PNG compression is lossless, so as not to interfere with
subsequent image analysis. Consequently, we wish to upload
all the images from the ‘initial sweep’ into the cloud as quickly
as possible, and this is what is targeted here.
A pre-processing operator, would reduce the compressed
image size to an extent depending on the image content. How-
ever, this operator itself has a computational cost but because
of the temporary backlog of images waiting to be uploaded,
there is an opportunity to pre-process some of the waiting
images to reduce their size (see Figure 5). The available up-
load bandwidth with respect to the computational cost of the
pre-processing operator, means that (in our experiment) there
is insufcient time to pre-process all images prior to upload.
In fact, to pre-process all of them would actually increase
end-to-end latency, due to the computational cost of the pre-
processing operation and limited fle size reduction for some
images (content dependent). The solution is to prioritize im-
ages for upload and pre-processing respectively, whilst both
Figure 6. Architecture of the intended application: full control loop for the
MiniTEM, with automatic imaging of target areas identifed in initial scan.
Control of microscope acquisition is future work. The internals of the HASTE
Desktop Agent (where the HASTE model is applied) are shown in Figure 5.
processes, as well as the enqueuing of new images from the
microscope, are occurring concurrently.
Feature Extraction, the Interestingness Function, and Policy
Samples for TEM analysis are typically supported by a metal
grid, which then obscures (blocks) regions of the sample in
(in this case) a honeycomb pattern. The blocked regions ap-
pear black in the images. As the sample holder moves under
the camera, the extent to which the sample is obscured is a
piecewise smooth (but irregular) function of document index,
dependent on the particular magnifcation level, and speed and
direction of the sample holder movement. Images can be pre-
processed to remove noise from blocked regions of the image,
reducing the size of the image under PNG compression. The ex-
tent of fle size reduction (under our pre-processing operator)
is related to the extent to which the grid obscures the image.
Consequently, the predicted extent of fle size reduction
can be modelled with linear spline interpolation, based on the
actual fle size reduction of images sampled from the queue,
described in more detail in [25]. The fle size reduction cor-
responds to feature extraction in the HASTE pipeline model,
and the spline estimate – the estimate of message size reduc-
tion – can be encapsulated as an IF, see Figure 1. The HASTE
tools, specifcally the HASTE Agent allow that IF to be used
as a scheduling heuristic to prioritize upload and local (pre-
)processing respectively (i.e. corresponding to the policy in-
ducing the DH in HASTE).
Available compute resource at the cloud edge are prioritized
on those images expected to yield the greatest reduction in fle
size (normalized by the compute cost, i.e. CPU time, incurred
in doing so). Conversely, upload bandwidth is prioritized on
(a) images that have been processed in this way, followed by
(b) those images for which the extent of fle size reduction is
expected to be the least – under the aim of minimizing the
overall upload time.
An important distinction between the this setting and that
in Case Study 1 is that the IF and DH are dynamic in this case
study.
The HASTE Agent manages the 3 processes occurring simul-
taneously: new images are arriving from the microscope, im-
ages are being pre-processed, and images are being uploaded.
Evaluation
When evaluated on a set of kidney tissue sample images [26]
our edge-based processing approach yielded up to a 25% reduc-
tion in end-to-end stream processing latency (for this partic-
ular choice of processing operator and dataset) compared to a
baseline approach without any prioritization, when compared
to performing no stream processing at all [25]. This is a signif-
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
Blamey et al.
|
9
Figure 7. Image size reduction (normalized by CPU cost) over index, showing
which images are processed at the edge. Those marked ‘processed’ were pro-
cessed at the cloud edge prior to upload (and vice-versa) – selected either to
search for new areas of high/low reduction, or to exploit known areas (using the
IF). The line shows the fnal revision of the splines estimation of the message
size reduction (the IF). Note how this deviates from the true value (measured
independently for illustration purposes on the same hardware), in regions of
low reduction. Note the oscillating pattern which is an artifact movement over
the grid in the miniTEM. Adapted from [25].
cant gain obtained with relative ease due to the HASTE Toolkit.
To verify the pre-processing operator, it was applied to all
images after the live test was performed. Figure 7, shows how
the image size reduction (y-axis - normalized with computa-
tional cost) can be modelled as a smooth function of the docu-
ment index (x-axis). The colors and symbols show which im-
ages were processed prior to upload based on either searching
(black crosses); or on the basis of the IF; those selected for
pre-processing (blue dots), and those which were not (orange
crosses). As can be seen and expected there is one peak (the
central one) where more images should optimally have been
scheduled for pre-processing prior to upload. That they were
not is a combination of the heuristics in the sampling strategy,
and the uploading speed. That is, they were simply uploaded
before the IF (the spline estimate) was good enough to schedule
them for pre-processing. The blue line in Figure 7 corresponds
to the fnal spline.
Discussion
This paper has discussed an approach to the design and de-
velopment of smart systems for processing large data streams.
The key idea of a HASTE pipeline is based on prioritization
with an interestingness function, and the application of a pol-
icy.
We demonstrated in two distinct case studies that this
simple model can yield signifcant performance gains for data-
intensive experiments. We argue that IFs (and the prioritiza-
tion and binning that they achieve) should be considered more
a ‘frst class citizen’ in the next generation of workfow man-
agement systems, and that the prioritization of data using IFs
and policies are useful concepts for designing and developing
such systems.
The ability to express informative IFs are critical to the ef-
fciency of a HASTE pipeline. IFs are chosen by the domain
expert to quantify aspects of the data to determine online pri-
oritization. In this work we provide two examples of increas-
ing complexity. In Case Study 1, the IF is a static, idempotent
function of a single image – which can be checked against a
static threshold, to determine a priority ‘bin’ or tier to store the
image. In Case Study 2, the prioritization of the queue of im-
ages waiting to be uploaded is revised online, as the underlying
model is revised. The strength of our proposed model is that,
having defned an IF, by making small changes to the policy,
the user is able to reconfgure the pipeline for diferent deploy-
ment scenarios and datasets, with diferent resulting resource
allocation. The HASTE toolkit is an initial implementation of
this vision. An avenue for future work will explore the creation
of IFs through training in real-time, using active learning and
potentially also reinforcement learning.
The policy-driven approach of resource prioritization pro-
posed under the HASTE pipeline paradigm can be generalized
to optimize utilization of diferent forms of constrained com-
putational resources. In some contexts (such as Case Study 1)
we are concerned with processing data streams for long-term
storage, so storage requirements (and associated costs) are the
key concern. In other contexts, with a focus on real time con-
trol, automation and robotics, the priority can be more about
achieving complex analysis with low-latency. In Case Study 2
this is manifest as a need to achieve edge to cloud upload in
the shortest possible time.
The diferent policies for the two case studies refect this: in
Case Study 1, the user defnes a policy to ‘bin’ images according
their interestingness score (i.e. image quality), these thresh-
olds are pre-defned by the user. That is to say, the user decides
explicit interestingness thresholds, and this determines the re-
sources (in this case, storage) which are allocated, and the fnal
cost. In similar deployment scenarios where cloud storage is
used (especially blob storage) costs would depend on the num-
ber of images within each interestingness bound. Whereas in
Case Study 2, by modelling the predicted extent of message
size reduction as an IF within the HASTE tools, we can defne a
policy to prioritize image processing and upload with the goal
of minimizing the total upload time for the next step in the
pipeline.
These policies induce two forms of DH: In Case Study 2, the
DH is manifest as a priority queue, updated in real time as new
images arrive, are pre-processed, and eventually removed –
whereas the available resources (CPU, network) are fxed. By
contrast, the data hierarchy in Case Study 1 is static, defned
by fxed thresholds on interestingness score – in this case, it is
the resources (in this case, storage, and consequent processing)
which are variable, determined by how many images end up in
each tier of the hierarchy.
Finally we note that the IF and policy could also be used to
prioritize data based on some measure of confdence. In many
scientifc analyses there exists a signifcant amount of uncer-
tainty in several steps of the modeling process. For example in
a classifcation setting the class labels predicted can be highly
uncertain. If in the top tier of the hierarchy we would place only
those data points for which we are confdent in the predicted
label, downstream analysis would see a reduction in noise and
an increased separability of the (biological) efects under study,
as discussed in [27].
Conclusion
In this paper we have proposed a new model for creating in-
telligent data pipelines, and presented a software implementa-
tion, the HASTE Toolkit. We have shown how these tools can
be leveraged in imaging experiments to organize datasets into
DHs. We have shown benefts in terms of cost reduction and
performance improvement, in terms of compute resources of
various kinds). In our case studies, we have studied some typ-
ical deployment scenarios, and shown how prioritization can
be achieved in these scenarios. Conceptualizing data analysis
pipelines around IFs allows better use of various computing
resources, and provides a conceptual structure for us to think
about the involvement of humans in such pipelines (and their
monitoring), as well as a means of managing scientifc experi-
mentation – either with instruments or through simulation.
The proposed HASTE pipeline model is intended as a means
of bringing structure to large scientifc datasets – a means of
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
10
|
Journal of XYZ, 2017, Vol. 00, No. 0
curating a Data Lake [28], whilst avoiding creating a data swamp
[29, 30]. It is efectively a design pattern creating an data hier-
archy from “runtime knowledge" about the dataset – extracted
in real time. The HASTE Toolkit is intended to help scientists
achieve this.
The key contribution made by the HASTE Toolkit is the de-
sign of an API which allows the user to express how they would
like their data to be prioritized, whilst hiding from them the
complexity of implementing this behaviour for diferent con-
strained resources in diferent deployment contexts. Our hope
is that the toolkit will allow intelligent prioritization to be
‘bolted on’ to new and existing systems – and is consequently
intended to be usable with a range of technologies in diferent
deployment scenarios.
In the general context of big data, the HASTE Toolkit should
be seen an efort to address challenges related to data streams
and efcient placement and management of data. It provides
the technical foundation for automatically organizing incom-
ing datasets in a way that makes them self-explainable and
easy to use based on the features of data objects rather than tra-
ditional metadata. It also enables efcient data management
and storage based on data hierarchies using dynamic policies.
This lays the foundation for domain experts to efciently select
the best-suited data from a massive dataset for downstream
analysis.
Declarations
List of abbreviations
• DH - Data Hierarchy. Conceptual structures in datasets, re-
alized as, e.g. tiered storage systems.
• HASTE - Hierarchical Analysis of Spatial (TE)mporal data.
• HSC - Haste Storage Client. A core HASTE component for
managing data hierarchies.
• IF - Interestingness function.
Applied to a document in
HASTE to compute an interestingness score.
• PLLS - Power Log Log Slope.
Ethical Approval
Not applicable.
Consent for Publication
Not applicable.
Competing Interests
The authors declare that they have no competing interests.
Funding
The HASTE Project (Hierarchical Analysis of Spatial and Tempo-
ral Image Data, http://haste.research.it.uu.se/) is funded by the
Swedish Foundation for Strategic Research (SSF) under award
no. BD15-0008, and the eSSENCE strategic collaboration for
eScience.
Acknowledgements
Thanks to Anders Larsson and Oliver Stein for help with soft-
ware deployment and testing for Case Study 1. Thanks to Polina
Georgiev for providing the images used in the evaluation of
Case Study 1. Resources from The Swedish National Infrastruc-
ture for Computing (SNIC) [31] were used for Case Study 2.
References
1. Ouyang W, Zimmer C. The Imaging Tsunami: Computa-
tional Opportunities and Challenges.
Current Opinion in
Systems Biology 2017 Aug;4:105–113.
2. Stephens ZD, Lee SY, Faghri F, Campbell RH, Zhai C, Efron
MJ, et al.
Big Data: Astronomical or Genomical?
PLOS
Biology 2015 Jul;13(7):e1002195.
3. Blamey B, Wrede F, Karlsson J, Hellander A, Toor S. Adapt-
ing the Secretary Hiring Problem for Optimal Hot-Cold
Tier Placement Under Top-K Workloads.
In: 2019 19th
IEEE/ACM International Symposium on Cluster, Cloud and
Grid Computing (CCGRID) Larnaca, Cyprus; 2019. p. 576–
583.
4. Sivarajah U, Kamal MM, Irani Z, Weerakkody V.
Critical
Analysis of Big Data Challenges and Analytical Methods.
Journal of Business Research 2017;70:263–286.
5. Reinsel D, Gantz J, Rydning J, Data Age 2025: The Digitiza-
tion of the World from Edge to Core (Seagate White Paper);
2018.
6. Rinehart D, Johnson CH, Nguyen T, Ivanisevic J, Benton HP,
Lloyd J, et al.
Metabolomic Data Streaming for Biology-
Dependent Data Acquisition.
Nature Biotechnology 2014
Jun;32(6):524–527.
7. Hillman C, Petrie K, Cobley A, Whitehorn M.
Real-Time
Processing of Proteomics Data: The Internet of Things and
the Connected Laboratory. In: 2016 IEEE International Con-
ference on Big Data (Big Data); 2016. p. 2392–2399.
8. Zhang Y, Bhamber R, Riba-Garcia I, Liao H, Unwin RD,
Dowsey AW. Streaming Visualisation of Quantitative Mass
Spectrometry Data Based on a Novel Raw Signal Decompo-
sition Method. PROTEOMICS 2015;15(8):1419–1427.
9. Kelleher J, Lin M, Albach CH, Birney E, Davies R, Gourtovaia
M, et al. Htsget: A Protocol for Securely Streaming Genomic
Data. Bioinformatics 2019 Jan;35(1):119–121.
10. Cuenca-Alba J, del Cano L, Gómez Blanco J, de la Rosa
Trevín JM, Conesa Mingo P, Marabini R, et al. ScipionCloud:
An Integrative and Interactive Gateway for Large Scale
Cryo Electron Microscopy Image Processing on Commercial
and Academic Clouds. Journal of Structural Biology 2017
Oct;200(1):20–27.
11. de la Rosa-Trevín JM, Quintana A, del Cano L, Zaldívar A,
Foche I, Gutiérrez J, et al. Scipion: A Software Framework
toward Integration, Reproducibility and Validation in 3D
Electron Microscopy.
Journal of Structural Biology 2016
Jul;195(1):93–99.
12. Wang D, Fong S, Wong RK, Mohammed S, Fiaidhi J,
Wong KKL. Robust High-Dimensional Bioinformatics Data
Streams Mining by ODR-ioVFDT. Scientifc Reports 2017
Feb;7(1):43167.
13. Shi W, Dustdar S. The Promise of Edge Computing. Com-
puter 2016 May;49(5):78–81.
14. B Blamey, A Hellander, S Toor. Apache Spark Streaming,
Kafka and HarmonicIO: A Performance Benchmark and Ar-
chitecture Comparison for Enterprise and Scientifc Com-
puting. In: Bench’19 Denver, Colorado, USA; 2019. .
15. Torruangwatthana P, Wieslander H, Blamey B, Hellander A,
Toor S. HarmonicIO: Scalable Data Stream Processing for
Scientifc Datasets. In: 2018 IEEE 11th International Con-
ference on Cloud Computing (CLOUD) San Francisco, CA,
USA; 2018. p. 879–882.
16. Awesome Pipeline;. https://github.com/pditommaso/awesome-
pipeline.
17. Chan SG, Tobagi FA. Modeling and Dimensioning Hierar-
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this
Blamey et al.
|
11
chical Storage Systems for Low-Delay Video Services. IEEE
Transactions on Computers 2003 Jul;52(7):907–919.
18. Novella JA, Emami Khoonsari P, Herman S, Whitenack
D, Capuccini M, Burman J, et al.
Container-Based
Bioinformatics with Pachyderm.
Bioinformatics 2019
Mar;35(5):839–846.
19. Kubernetes,
Kubernetes
Documentation;.
https://kubernetes.io/docs/home/.
20. Bray MA, Carpenter AE.
Quality Control for High-
Throughput Imaging Experiments Using Machine Learning
in Cellprofler.
In: Johnston PA, Trask OJ, editors. High
Content Screening: A Powerful Approach to Systems Cell
Biology and Phenotypic Drug Discovery Methods in Molec-
ular Biology, New York, NY: Springer New York; 2018.p.
89–112.
21. McQuin C, Goodman A, Chernyshev V, Kamentsky L, Cimini
BA, Karhohs KW, et al. CellProfler 3.0: Next-Generation
Image Processing for Biology. PLoS Biology 2018 Jul;16(7).
22. Polina
Georgiev,
Ben
Blamey,
Ola
Spjuth,
Snat10
Knockout
Mice
Cortical
Neuronal
Cells
(ImageXpress
XLS
Example
Images);
2020.
http://doi.org/10.17044/scilifelab.12811997.v1.
23. Bray MA, Fraser AN, Hasaka TP, Carpenter AE. Workfow
and Metrics for Image Quality Control in Large-Scale High-
Content Screens. Journal of Biomolecular Screening 2012
Feb;17(2):266–274.
24. Vironova AB, MiniTEM: Automated Transmission Electron
Microscopy Analysis;.
https://www.vironova.com/our-
ofering/minitem/.
25. Blamey B, Sintorn IM, Hellander A, Toor S.
Resource-
and Message Size-Aware Scheduling of Stream Process-
ing at the Edge with Application to Realtime Microscopy.
arXiv:191209088 [cs] 2019 Dec;.
26. Ben
Blamey,
Ida-Maria
Sintorn,
HASTE
miniTEM
Example
Images
(Dataset);
2020.
https://doi.org/10.17044/scilifelab.12771614.v1.
27. Wieslander H, Harrison PJ, Skogberg G, Jackson S, Friden
M, Karlsson J, et al. Deep Learning and Conformal Predic-
tion for Hierarchical Analysis of Large-Scale Whole-Slide
Tissue Images. IEEE Journal of Biomedical and Health In-
formatics 2020;p. 1–1.
28. Pentaho,
Hadoop,
and
Data
Lakes;.
https://jamesdixon.wordpress.com/2010/10/14/pentaho-
hadoop-and-data-lakes/.
29. Brackenbury W, Liu R, Mondal M, Elmore AJ, Ur B, Chard
K, et al.
Draining the Data Swamp: A Similarity-Based
Approach.
In: Proceedings of the Workshop on Human-
In-the-Loop Data Analytics HILDA’18, Houston, TX, USA:
Association for Computing Machinery; 2018. p. 1–7.
30. Hai R, Geisler S, Quix C. Constance: An Intelligent Data
Lake System.
In: Proceedings of the 2016 International
Conference on Management of Data SIGMOD ’16, San Fran-
cisco, California, USA: Association for Computing Machin-
ery; 2016. p. 2097–2100.
31. Toor S, Lindberg M, Falman I, Vallin A, Mohill O, Freyhult
P, et al. SNIC Science Cloud (SSC): A National-Scale Cloud
Infrastructure for Swedish Academia.
In: E-Science (e-
Science), 2017 IEEE 13th International Conference On IEEE;
2017. p. 219–227.
preprint (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 
bioRxiv preprint doi: https://doi.org/10.1101/2020.09.13.274779; this version posted September 14, 2020. The copyright holder for this


Paper 4:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: The goal of this paper is to introduce the wealth of data generated by the container terminal into the methodology of determining the storage location of the arriving containers. We presented a two module-based DSS to manage the CSP in container terminals. In our study, the CSP was solved by a category stacking strategy to facilitate stowage planning. In contrast to previous studies, the proposed method applies predictive analytics for container weight classification using historical data, and container assignment to a specific storage area is executed dynamically to outperform the existing stacking strategies.
  Extract 2: The experimental results showed that the proposed method outperforms four other alternative stacking strategies (i.e., the GMM-SS, current practice, HSSS, and RSS). Therefore, our proposed method performs robustly and can further improve yard operations and terminal competitiveness. 

The experimental results are encouraging in that it implies that the improvements in the stacking performance are related to the category stacking. The proposed method is expected to achieve greater improvements in practice when data-driven inferences can be derived from more historical data.
  Limitations: None major limitations are mentioned in the study.
  Relevance Evaluation: {'extract_1': 'In this study, we propose a data-driven dynamic stacking strategy (DSS) based on an online algorithm. The data-driven DSS consists of two modules. The first is the Gaussian mixture model (GMM) generation module, which clusters the container weight into several weight classes. The second is the DSS module based on the online algorithm to determine the storage location of an arriving container in real time. This module adjusts itself to dynamically respond to the environment.', 'extract_2': 'For numerical experiments, we utilized input data for 10 months in 2018 from a typical container terminal in Busan, Republic of Korea. For each instance, we divided a training dataset and a test dataset. From the training dataset, we employed the GMM method to cluster the container weights into K clusters. The DSS was implemented using the test dataset.\n\nThe five strategies, including the proposed method, are executed in seconds; thus, they are suitable for an online algorithm. We conducted simulations to compare the proposed method with the four alternative strategies. The proposed method shows superior performance compared to the alternative stacking strategies.', 'limitations': 'None major limitations are mentioned in the study.', 'relevance_score': 1.0}
  Relevance Score: 1.0
  Inline Citation: >
  Explanation: The research presents a data-driven dynamic stacking strategy (DSS) for the container stacking problem (CSP), which optimizes storage location decisions for export containers in container terminals. The core contributions of the study are:

1.  Development of a Gaussian mixture model (GMM) to cluster container weights into weight classes, addressing the practical challenge of uncertain container weights.

2.  Design of a dynamic stacking strategy that allocates storage locations to arriving containers based on both container weight class and the dynamic stack configuration of storage areas, capturing real-world variations in container weights and storage space availability.

The evaluation of DSS through numerical experiments using real-life data from a container terminal in Busan, South Korea, demonstrates its superior performance compared to alternative stacking strategies: the DSS outperforms the GMM-based static stacking strategy (GMM-SS) by 6.0%, the current practice by 33.1%, the hybrid sequence stacking strategy (HSSS) by 41.6%, and the random stacking strategy (RSS) by 44.1%. This study highlights the potential of leveraging predictive analytics for container weight classification and dynamic adjustment mechanisms to improve stacking strategies in container terminals, contributing to enhanced yard operations and terminal competitiveness.

 Full Text: >
"Your privacy, your choice We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Accept all cookies Skip to main content Log in Find a journal Publish with us Track your research Search Cart Home Flexible Services and Manufacturing Journal Article Data-driven dynamic stacking strategy for export containers in container terminals Open access Published: 27 June 2022 Volume 35, pages 170–195, (2023) Cite this article Download PDF You have full access to this open access article Flexible Services and Manufacturing Journal Aims and scope Submit manuscript Hyun Ji Park , Sung Won Cho , Abhilasha Nanda & Jin Hyoung Park   2594 Accesses 3 Citations 1 Altmetric Explore all metrics Abstract This study investigates a method for improving real-time decisions regarding the storage location of export containers while the containers are arriving. To manage the decision-making process, we propose a two module-based data-driven dynamic stacking strategy that facilitates stowage planning. Module 1 generates the Gaussian mixture model (GMM) specific to each container group for container weight classification. Module 2 implements the data-driven dynamic stacking strategy as an online algorithm to determine the storage location of an arriving container in real time. Numerical experiments were conducted using real-life data to validate the effectiveness of the proposed method compared to other alternative stacking strategies. These experiments revealed that the performance of the proposed method is robust, and therefore it can improve yard operations and container terminal competitiveness. Similar content being viewed by others Intelligent Modeling Essential to Get Good Results: Container Storage Inside a Container Terminal Chapter © 2015 A study of the sensitivity of sequence stacking strategies for the storage location assignment problem for out-bound containers in a maritime terminal Article 20 July 2018 Assessment System for a Large Container Management and Optimization Problem Chapter © 2024 1 Introduction With the development of container transportation, the task of efficiently managing scarce storage space resources in container terminals has become an important role for marine transport hubs. Containers arrive at the storage area randomly and are stacked on the ground in the arrangement of a yard block as shown in Fig. 1. Yard cranes (YCs) must first handle the containers located at the top tier, and the containers already stacked are rehandled to access the target container buried beneath them. Thus, inefficient handling can result in excessive operational delays, which can lead to bottlenecks in container flows. Therefore, to improve the productivity of container terminals, effective methods for determining the most efficient storage location of the arriving containers must be employed (Zhang et al. 2003). Fig. 1 Yard block configuration Full size image Container terminals handle various types of incoming containers, which can be classified as import or export containers depending on the vehicles that carry them. Import containers are discharged from vessels and loaded onto external trucks (ETs), whereas export containers are transported to the terminal by ETs and are loaded onto vessels. All of these containers are temporarily stored in the yard, and the goal of the storage strategy is to minimize the amount of time the vehicles that are to be loaded stay in the terminal. For this reason, the way the containers are stored depends on the characteristics of the vehicles that are to be loaded, such as the vehicles’ capacities and their arrival times. For example, ETs have small carrying capacities and large uncertainties in arrival times depending on the traffic conditions. Therefore, import containers are stacked at higher tiers, as the estimated retrieval times are shorter. In contrast, vessels carry large quantities of containers, and their arrival times are expected via berth plans. Thus, export containers are stored through decisions at two different levels: planning and operational (Chen and Lu 2012; Jiang and Jin 2017; Zhou et al. 2020; He et al. 2020a, b; Feng et al. 2021). At the planning level, containers are assigned to sub-blocks based on the container group, which is defined as a group of containers having the same departing vessel, port of destination (POD), size, and type (Kim et al. 2004; Zhen 2013, 2014; Jiang and Jin 2017; He et al. 2020a). At the operational level, containers are stacked within the range of a single yard bay in consideration of the loading operation (Kim et al. 2000; Zhang et al. 2010, 2014a, b). In the loading operation, planners schedule the loading sequence with two objectives: minimizing the handling effort of quay cranes (QCs) and YCs, and ensuring the stability of the vessel (Kim et al. 2000). Therefore, the storage configuration of export containers must be in a good shape to generate an efficient loading sequence. In this study, we focus on the container stacking problem (CSP) for export containers at the operational level in order to aid planners in constructing optimal load sequences. The CSP for export containers is a real-time decision problem because incomplete and imperfect information is involved (Steenken et al. 2004; Borgman et al. 2010; He et al. 2020b). First, as most studies assume, the time at which the containers arrive at the yard cannot be accurately predicted because the arrival times are dynamically updated depending on the traffic conditions. Therefore, it is difficult to achieve the optimal results. Second, information on the weight of the containers to be loaded onto the vessel is uncertain. Many studies assume that container weight falls within one of three classes: heavy (H), medium (M), and light (L) (Kim et al. 2000; Zhang et al. 2010, 2014a). Some extended studies have converted the uncertainty of the weight information into probabilities (Kang et al. 2006; Zhang et al. 2014b). In practice, the estimated weight class can be identified as a different weight class when the container arrives at the terminal. Because of this, terminal operators use their experience as the basis for classifying container weights into different classes for each vessel. Third, the stowage instructions are not known one to two weeks in advance. Container ships are berthed at multiple ports along the shipping line, and stowage instructions vary depending on the loading operations performed at the previous port. For example, as shown in Fig. 2, heavier containers are required in section A and lighter containers are required in section B. This requires a stacking strategy that responds flexibly to this uncertain environment. Lastly, the YC workload at the time of container arrival is unknown in advance (Jiang and Jin 2017). Therefore, containers are dynamically allocated to multiple bays depending on the YC's workload at the time of arrival, resulting in different container weight distributions in each bay. Fig. 2 Example of a set of stowage instructions Full size image In this study, we propose a stacking strategy based on an online algorithm in which decisions are made with incomplete knowledge of the future (Karp 1992). Unlike an offline algorithm that yields optimal solutions with extensive computations, an online algorithm facilitates operations in dynamic environments that do not have ample time to compute before performing tasks. Because containers cannot be held after they arrive and the order in which they arrive cannot be controlled, an online algorithm that allocates storage locations when containers arrive at the block is appropriate (Murty et al. 2005). A number of studies have employed online algorithms for the CSP (Dekker et al. 2006; Borgman et al. 2010; Park et al. 2011; Chen and Lu 2012; Ambrosino et al. 2013; Güven and Eliiyi 2014, 2019; He et al. 2020b), but they do not take into account many practical considerations such as uncertainty of the weight, type and arrival timing of containers, uncertainty about the stowage instruction of the vessel, and the dynamic nature of YC workload according to other interconnected operations. Therefore, this study proposes a data-driven dynamic stacking strategy (DSS) based on an online algorithm. The data-driven DSS consists of two modules. The first is the Gaussian mixture model (GMM) generation module, which clusters the container weight into several weight classes. The second is the DSS module based on the online algorithm to determine the storage location of an arriving container in real time. This module adjusts itself to dynamically respond to the environment. The remainder of this paper is organized as follows. In Sect. 2, relevant literature on the CSP is reviewed. A detailed description of the proposed method is provided in Sect. 3. In Sect. 4, computational experiments are conducted and interpreted. Finally, the conclusions are drawn in Sect. 5. 2 Literature review In this section, we review previous studies related to the CSP in container terminals. Many researchers have studied on the related problem, and we refer to Vis and De Koster (2003), Steenken et al. (2004), Stahlbock and Voß (2008), and Carlo et al. (2014) which conducted comprehensive reviews of numerous studies on the efficient operation of container terminals. The solutions to the CSP have been classified into two types: one for import containers and one for export containers. For import containers, De Castillo and Daganzo (1993) proposed two stacking strategies: a non-segregation strategy that includes all stacks of the same size from all vessels, and a segregation strategy in which containers from different vessels are segregated. Kim and Kim (1999) implemented a segregation strategy for import containers that estimated the expected total number of rehandles. They presented a mathematical model for the relationship between the height of the stack and the number of rehandles. They also considered the uncertainty in the arrival times of import containers for constant, periodic, and dynamic arrival rates. Kim and Kim (2002) proposed a cost model that determines the optimal storage space and number of transfer cranes for import containers. The cost model included the costs of space, transfer cranes, and ETs, and they illustrated the effectiveness of their deterministic and stochastic models using numerical examples. Ting et al. (2010) proposed a category stacking strategy (also called a clustering stacking strategy) for import containers by analyzing historical data. They presented a pick-up booking system that categorizes the containers into several groups according to the pick-up priority predicted by historical data. Sauri and Martin (2011) extended the work of De Castillo and Daganzo (1993) and developed a segregation strategy that generates the fewest number of rehandles by mixing containers from different vessels. In addition, they considered the different probabilities of the time that elapsed before each container left the terminal as a function of the time each container arrived. Ambrosino et al. (2013) modeled import containers being loaded onto a train by comparing train loading policies (sequential, non-sequential, and partially sequential) for different stacking strategies (random, based on container weight, and based on container weight and commercial priority) in a container terminal. Maldonado et al. (2019) proposed three different stacking strategies based on the prediction (nominal, numerical, and nominal and numerical) of expected dwell times using the random forest method. They assessed their proposed method by applying it to two strategies (horizontal and vertical) in two scenarios (average and stressed). Because the storage periods for import containers vary depending on the arrival time of ETs and trains, the CSP for import containers has been studied in a way that enables the arrival times to be predicted probabilistically. In contrast, the CSP for export containers considers vessel characteristics. Kim et al. (2000) proposed a dynamic programming (DP) model to determine the storage locations of export containers based on their weights. They assumed that heavier containers should be loaded onto the lower tiers of a vessel to guarantee its stability. Therefore, heavier containers are stacked at the higher tiers of the yard block to reduce the expected number of rehandles. Furthermore, they developed a decision tree to support real-time decisions. Duinkerken et al. (2001) evaluated the performance of the remaining stack capacity (RSC) strategy, which considers the stack height and container category using various stacking strategies (random, levelling, and closest position). Dekker et al. (2006) proposed a category stacking strategy that allows online optimization to facilitate loading operations. They used a simulation method to compare random stacking with category stacking based on the number of rehandles. Kang et al. (2006) presented a stacking strategy for export containers with uncertain weight information using a simulated annealing approach to minimize the number of rehandles. Furthermore, they proposed an advanced stacking strategy that overcomes the uncertainty in container weight through machine learning techniques. Park et al. (2011) proposed stacking strategies to dynamically determine the stacking location as the operational environment changes. The proposed strategies, which were based on an online algorithm, were generated by evaluating the weights of the decision criteria during the evaluation period. Simulations were conducted for a variety of stacking strategies, which were demonstrated to be effective in reducing QC delays. Chen and Lu (2012) proposed a hybrid sequence stacking algorithm (HSSA) based on an online algorithm to make decisions in real time. They observed that the HSSA outperformed the random and vertical stacking strategies in terms of the number of rehandles. Zhang et al. (2010) analyzed the error of a key model transformation in Kim et al. (2000) and presented the correct form. Zhang et al. (2014a), which was an extension of the studies by Kim et al. (2000) and Zhang et al. (2010), proposed two conservative models by reinterpreting the punishment coefficient for stacking light containers on top of stacks loaded with heavy containers. The proposed models outperformed the previous optimized models in terms of static and dynamic indicators. Zhang et al. (2014b) considered adjusting the proportion of unarrived containers in each weight class to a non-constant proportion in the constant proportion DP model proposed by Kim et al. (2000) and Zhang et al. (2010). In numerical experiments, they demonstrated that the proposed models with the adjusted weight class proportions for the remaining containers improved the stacking quality. Hu et al. (2014) proposed a branch-and-bound method based on the least-cost priority queue (LCBB) to obtain an optimal solution in which the number of rehandles is minimized, using the HSSA proposed by Chen and Lu (2012) to calculate the upper bound for the LCBB. Güven and Eliiyi (2014) studied two stacking strategies (random stacking and category stacking) for export containers. They considered container weight as another category attribute, and grouped containers with a weight of less than three tons into the same category. Güven and Eliiyi (2019) extended Güven and Eliiyi (2014) and expanded the stacking strategies to include all types of containers (export, transit, import, and empty containers). They compared three stacking strategies (random stacking, attribute-based stacking, and weight-relaxed stacking) through simulations. He et al. (2020b) studied stacking strategies that consider the uncertainty in the arrival sequence of vessels, assuming that the weight information and arrival order of the containers are known. Based on the three stacking rules (least reshuffle rule, lowest stack rule, and nearest stack rule), five heuristic algorithms were proposed according to a set of rules. The contributions of our study in the context of the aforementioned studies are summarized as follows. First, this is the first study that applies predictive analytics for container weight classification to prescribe optimal decisions for the CSP. Most previous studies have simplified the problem by assuming three classes (light, medium, and heavy), and certain studies (Kang et al. 2006; Zhang et al. 2014b) have considered the weight uncertainty for each class. However, in practice, these assumptions are not practical because the weight is classified according to the size of the vessel and the range of weights of the containers to be loaded. Therefore, this study analyzes the historical data for container weight, and estimates the classification model for container weight class as a GMM using a machine learning technique. Second, we propose a dynamic stacking strategy that considers multiple bays. To the best of our knowledge, most studies have focused on the CSP for a single bay, and the stacking strategy was applied homogeneously to each bay. However, in a real-world environment, the proportion of containers in each weight class assigned to each bay is not constant. Hence, the remaining containers must respond dynamically to the containers that are already stacked. Therefore, this study presents a stacking strategy that responds to the configurations of multiple bays. Third, we develop a category stacking strategy to present practical alternatives that reflect real-world considerations. In most studies on the CSP for export containers (the exceptions being Dekker et al. 2006; Güven and Eliiyi 2019), the problem is defined as minimizing the expected number of rehandles based on the weight class. However, heavier containers are normally loaded onto lower tiers, depending on the configuration of the stowage and the precise weight of the containers. The category stacking that clusters containers with similar weights into the same stack can facilitate stowage planning by providing containers of various weights on the top tier in the yard. 3 Problem description This study aims to optimize real-time decisions regarding the precise storage location of export containers for a given storage area while the containers are arriving. The storage area consists of multiple stacks in a bay, as shown in Fig. 3. In this study, the decision on the storage location refers to the selection of one stack in the storage area. In practice, the storage areas of export containers are not shared with those of import containers (Kim and Kim 1999; He et al. 2020b; Hu et al. 2021). Furthermore, each storage area designates areas for containers belonging to the same container group (Kim et al. 2004; Zhen 2013, 2014; Jiang and Jin 2017; He et al. 2020a). For these reasons, different stacking decisions are made depending on the container group. Therefore, we focus on stacking decisions that apply to the containers in a single container group. Fig. 3 Yard bay configuration Full size image The goal is to assign an arriving container to a stack in a way that conforms to the category stacking strategy. Category stacking for export containers aims to cluster containers with similar weights in the same stack. However, it is difficult to achieve this outcome due to the randomness of the arriving containers. The following two aspects need to be considered to improve the stacking quality according to the category stacking strategy: (1) how to define the similarity in container weights and (2) how to define the specific location assignment rules for each storage area. For aspect (1), researchers have manually classified container weights into several weight classes with approximate weight ranges. For aspect (2), the location assignment rules have been executed in the same way for all storage areas. However, predictive analytics for container weight classification and container assignment rules specific to dynamic storage areas can enhance the stacking strategy. In this respect, we propose a data-driven DSS that outperforms existing stacking strategies. The proposed container stacking system is divided into two modules, as shown in Fig. 4. Module 1 is executed in advance, and module 2 is triggered by a container arriving at the port. In module 1, GMM-based predictive models, which cluster the container weights into several weight classes for each container group, are generated. Next, once a container has arrived, the pre-selected storage area is given, and the weight class for the container is predicted by the GMM generated in module 1. In module 2, the precise storage location (i.e., stack) for the container is selected using the proposed dynamic stacking strategy for the given storage area. The proposed stacking strategy is intended to dynamically adjust itself depending on the stack configuration in the storage area. A detailed description of the modules is provided in the following subsections. Fig. 4 Overview of the proposed container stacking system Full size image 3.1 Module 1: generation of GMM In module 1, we aggregate the historical data for the container weights over the entire voyage for each container group, and then generate the GMM specific to each container group for the container weight classification. The GMM is a model that represents a population as a linear superposition of subpopulations, and it assumes that each subpopulation follows a Gaussian distribution. Because the class label (i.e., subpopulation) of the data point is unknown, the GMM is an unsupervised learning method (Figueiredo and Jain 2002). In addition, the GMM is a soft clustering method that uses probabilistic inference to explain how much a given data point is associated with a certain cluster (i.e., subpopulation). Due to these characteristics, the GMM has been widely employed in unsupervised classification applications in which data tend to follow multimodal and complex distributions. In this study, the container weight class was defined for each cluster of the GMM. Furthermore, the probable weight class was predicted for new container input. For the detailed description, the following notations are introduced: Notation \\(G\\) Set of container groups, indexed by \\(g\\) \\({K}_{g}\\) Set of clusters (i.e., weight class) for container group \\(g\\), indexed by \\(k\\) \\(x\\left(i\\right)\\) Container weight of container \\(i\\) \\(g\\left(i\\right)\\) Container group of container \\(i\\) \\(k\\left(i\\right)\\) Probable weight class of container \\(i\\) \\({\\mu }_{k}^{g}\\) Mean of cluster \\(k\\) for container group \\(g\\) \\({\\Sigma }_{k}^{g}\\) Covariance of cluster \\(k\\) for container group \\(g\\) \\({\\pi }_{k}^{g}\\) Mixture weight of cluster \\(k\\) for container group \\(g\\); \\(0\\le {\\pi }_{k}^{g}\\le 1\\) and \\(\\sum_{k\\in {K}_{g}}{\\pi }_{k}^{g}=1\\) \\({z}_{k}^{g}\\) Latent indicator variable; defined as 1 if the observation data belongs to cluster \\(k\\) and 0 otherwise; \\(\\sum_{k\\in {K}_{g}}{z}_{k}^{g}=1\\), \\(p\\left({z}_{k}^{g}=1\\right)={\\pi }_{k}^{g}\\) \\(p\\left(x|g\\right)\\) Marginal probability distribution of container weight \\(x\\) for container group \\(g\\) \\(\\gamma \\left({z}_{k}^{g}|g\\right)\\) Posterior probability (i.e., “responsibility”) that container weight \\(x\\) is observed from cluster \\(k\\) for container group \\(g\\); \\(\\gamma \\left({z}_{k}^{g}|g\\right)=p\\left({z}_{k}^{g}=1|x,g\\right)\\) The GMM is parameterized by the mean \\({\\mu }_{k}^{g}\\), covariance \\({\\Sigma }_{k}^{g}\\), and mixture weight \\({\\pi }_{k}^{g}\\). The assignment of the unknown class label \\({z}_{k}^{g}\\) is considered a latent variable instead of a parameter. It enables the joint distribution \\(p\\left(x,{z}_{k}^{g}\\right)\\) to marginalize the variable \\({z}_{k}^{g}\\) out to define the cost function independently of \\({z}_{k}^{g}\\). The resulting standard form of the GMM is written as $$p\\left( {x{|}g} \\right) = \\mathop \\sum \\limits_{{k \\in K_{g} }} p\\left( {z_{k}^{g} } \\right)p\\left( {x{|}z_{k}^{g} } \\right) = \\mathop \\sum \\limits_{{k \\in K_{g} }} \\pi_{k}^{g} N\\left( {x{|}\\mu_{k}^{g} , \\Sigma_{k}^{g} } \\right).$$ (1) To estimate the three parameters of the GMM, the objective function is to maximize the marginal likelihood of the data (\\(p\\left(x|g\\right)\\)). Because there is no analytical solution, a numerical method was employed for the maximum likelihood estimation. The most widely used method is expectation maximization (EM). The EM algorithm estimates the model parameters through iterations of the expectation step (E step) and maximization step (M step). Given the initialized model parameters and the log-likelihood estimate, the E step uses the model parameters to evaluate the responsibility \\(\\gamma \\left({z}_{k}|g\\right)\\) via $$\\gamma \\left( {z_{k}^{g} {|}g} \\right) = C_{g} \\times \\pi_{k}^{g} N\\left( {x{|}\\mu_{k}^{g} , \\Sigma_{k}^{g} } \\right),$$ (2) where \\({C}_{g}=1/\\sum_{k\\in {K}_{g}}{\\pi }_{k}^{g}N\\left(x|{\\mu }_{k}^{g}, {\\Sigma }_{k}^{g}\\right)\\). Then the M step re-estimates the model parameters using this responsibility value. These iterations lead to the convergence of the model parameters, and the resulting trained GMM provides the probable weight class of the new container input via $$k\\left( i \\right) = \\mathop {{\\text{argmax}}}\\limits_{{k \\in K_{g} }} \\gamma \\left( {z_{k}^{g} {|}g} \\right)$$ (3) 3.2 Module 2: dynamic stacking strategy In module 2, we executed the DSS as an online algorithm to determine the storage location of an arriving container in real time. The overall framework of the proposed DSS is shown in Fig. 5. The algorithm inputs include the GMM-based prediction results for the weight class as well as the yard stack configuration of the pre-selected storage area for the arriving container. The storage location for the container is determined after the weight class-to-stack assignment. Both methods accommodate the GMM-based prediction results for the weight class obtained from module 1. Furthermore, the weight class-to-stack assignment method adapts the adjustment mechanism according to the dynamic change in the stack configuration, which leads to the generation of the stacking strategy that is specific to the storage area. Fig. 5 Framework of the proposed dynamic stacking strategy Full size image For the weight class-to-stack assignment, conventional approaches of category stacking apply the assignment method homogeneously for all storage areas, and they remain consistent during the period in which containers are arriving. However, in real-world environments, the stack configuration cannot be the same for all storage areas, and thus the stacking strategy must be adjusted according to the different stack configurations. Taking into account the limitations of conventional methods, we propose an improved approach to accommodate the dynamic change in the stack configuration of the storage areas. For the detailed description, the following notations are introduced: Notation   \\({A}_{g}\\) Set of storage areas for container group \\(g\\), indexed by \\(a\\) \\({S}_{a}\\) Set of stacks in storage area \\(a\\), indexed by \\(s\\) \\({K}_{as}\\) Set of weight classes designated for stack \\(s\\) in storage area \\(a\\), indexed by \\(k\\) \\({R}_{k}^{g}\\) Range of cumulative mixture weight values for weight class \\(k\\) \\({R}_{as}\\) Range of cumulative mixture weight values for stack \\(s\\) in storage area \\(a\\) \\(a\\left(i\\right)\\) Pre-selected storage area for container \\(i\\) \\(s\\left(i|a\\right)\\) Stack to be selected for container \\(i\\) given storage area \\(a\\) \\({t}_{as}\\) Remaining slot capacity (in number of containers) of stack \\(s\\) in storage area \\(a\\) \\({f}_{as}^{k}\\) Contribution of weight class \\(k\\) to \\({R}_{as}\\) for \\(k\\in {K}_{as}^{g}\\) \\({X}_{as}^{g}\\) List of container weights for stack \\(s\\) in storage area \\(a\\) of container group \\(g\\) First, we describe the procedure for the weight class-to-stack assignment. This assignment satisfies a many-to-many relationship. Regarding the GMM-based prediction results for the weight class, we focus on the mixture weight \\({\\pi }_{k}^{g}\\), which represents the estimated size of the weight class \\(k\\). Given that the indices of the GMM are sorted in ascending order according to weight, the cumulative range of \\({\\pi }_{k}^{g}\\) for each weight class can be represented by $$R_{k}^{g} = \\left\\{ {r : \\mathop \\sum \\limits_{{\\begin{array}{*{20}c} {k^{\\prime} \\in K_{g} } \\\\ {k^{\\prime} < k } \\\\ \\end{array} }} \\pi_{{k^{\\prime}}}^{g} < r \\le \\mathop \\sum \\limits_{{\\begin{array}{*{20}c} {k^{\\prime} \\in K_{g} } \\\\ {k^{\\prime} \\le k } \\\\ \\end{array} }} \\pi_{{k^{\\prime}}}^{g} , r \\in {\\mathbb{R}}} \\right\\}.$$ (4) Likewise, given that the indices of the stacks are sorted, the cumulative range of \\({\\pi }_{k}^{g}\\) for each stack can be represented by $$R_{as} = \\left\\{ {r :\\mathop \\sum \\limits_{{\\begin{array}{*{20}c} {s^{\\prime} \\in S_{a} } \\\\ {s^{\\prime} < s } \\\\ \\end{array} }} C_{a} \\times t_{{as^{\\prime}}} < r \\le \\mathop \\sum \\limits_{{\\begin{array}{*{20}c} {s^{\\prime} \\in S_{a} } \\\\ {s^{\\prime} \\le s } \\\\ \\end{array} }} C_{a} \\times t_{{as^{\\prime}}} , r \\in {\\mathbb{R}}} \\right\\},{ }$$ (5) where \\({C}_{a}=1/\\sum_{s\\in {S}_{a}}{t}_{as}\\). Equation (5) calculates the coverage range for each stack based on the number of remaining slots, \\({t}_{as}\\). For example, if the storage area consists of four empty stacks, the cumulative ranges \\({R}_{as}\\) are set to (0, 0.25], (0.25, 0.5], (0.5, 0.75], and (0.75, 1]. Then, $$K_{as}^{g} = \\left\\{ {k: R_{k}^{g} \\cap R_{as} \\ne \\emptyset , k \\in K_{g} } \\right\\}$$ (6) defines the weight class-to-stack assignment \\({K}_{as}^{g}\\) such that the membership of the weight classes in a stack is determined by whether the elements of \\({R}_{k}^{g}\\) lie in the specified \\({R}_{as}\\). The variable \\({K}_{as}^{g}\\) is continuously adjusted because dynamic container placements change \\({t}_{as}\\), which impact \\({R}_{as}\\). Figures 6 and 7 provide an example of the adjustment mechanism in the weight class-to-stack assignment procedure. It is supposed that two storage areas are assigned containers that belong to the same container group. The number of weight classes is set to four for the container group, and the number of stacks is set to four for both storage areas. The mixture weights \\({\\pi }_{k}^{g}\\) are set to 0.25 for all weight classes. Figure 6 shows the initialized \\({K}_{as}^{g}\\) in which there are no differences between the storage areas. Fig. 6 Illustration of an initialized weight class-to-stack assignment (\\({{\\varvec{\\pi}}}_{{\\varvec{k}}}^{{\\varvec{g}}}=0.25\\;{\\mathbf{for}\\;\\mathbf{k}}=1,2,3,4\\)) Full size image Fig. 7 Illustration of an adjusted weight class-to-stack assignment (\\({{\\varvec{\\pi}}}_{{\\varvec{k}}}^{{\\varvec{g}}}=0.25\\;{\\mathbf{for}\\;\\mathbf{k}}=1,2,3,4\\)) Full size image In contrast, Fig. 7 shows the adjusted \\({K}_{as}^{g}\\) after a total of 22 containers are stacked. The adjustment varies depending on the storage area. We now elaborate the steps to derive \\({K}_{as}^{g}\\) for storage area “A” using Eqs. (4)-(6). First, \\({R}_{k}^{g}\\) is always (0, 0.25], (0.25, 0.5], (0.5, 0.75], and (0.75, 1] for the weight classes. Second, \\({R}_{as}\\) is initialized as (0, 0.25], (0.25, 0.5], (0.5, 0.75], and (0.75, 1] for the empty storage area. After three, three, one, and three containers are stacked in each stack, \\({R}_{as}\\) is updated as (0, 0.2], (0.2, 0.4], (0.4, 0.8], and (0.8, 1] for the stacks. Therefore, it is necessary to check whether there is an overlap between \\({R}_{k}^{g}\\) and \\({R}_{as}\\). For weight class 1, the infimum and supremum of \\({R}_{k}^{g}\\) lie in \\({R}_{as}\\) for stacks 1 and 2, respectively. Accordingly, \\({K}_{as}^{g}\\) of weight class 1 includes stacks 1 and 2. In this way, the \\({K}_{as}^{g}\\) of the weight classes are dynamically updated when the stack configuration is changed, establishing the stacking strategy specific to the storage area. Next, we describe the procedure for the storage location assignment for a container, given the weight class-to-stack assignment. Regarding the GMM-based prediction results for the weight class, we focus on the responsibility value \\(\\gamma \\left({z}_{k}^{g}|g\\right)\\), which represents the probability that a container belongs to a certain weight class. If a single weight class is assigned to each stack (e.g., the weight class-to-stack assignment is 1–1, 2–2, 3–3, 4–4), as shown in Fig. 6, an effective strategy is to select a stack with the largest \\(\\gamma \\left({z}_{k}^{g}|g\\right)\\) for the corresponding weight class \\(k\\). However, because multiple weight classes can be assigned to each stack, as shown in Fig. 7, a more sophisticated strategy that considers the contributions of multiple weight classes should be employed. In this context, $$f_{as}^{k} = {\\text{min}}\\left\\{ {\\sup R_{k}^{g} ,\\sup R_{as} } \\right\\} - {\\text{max}}\\left\\{ {\\inf R_{k}^{g} ,\\inf R_{as} } \\right\\}$$ (7) defines the contribution of weight class \\(k\\) to \\({R}_{as}\\). Then, $$s\\left( {i{|}a} \\right) = \\mathop {{\\text{argmax}}}\\limits_{{s \\in S_{a} }} \\mathop \\sum \\limits_{{k \\in K_{as}^{g\\left( i \\right)} }} C_{as} \\times f_{as}^{k} \\times \\gamma \\left( {z_{k} {|}g\\left( i \\right)} \\right),$$ (8) where \\({C}_{as}=1/\\sum_{k\\in {K}_{as}^{g\\left(i\\right)}}{f}_{as}^{k}\\), indicates the proposed strategy that ensures the selection of a stack with the largest weighted average of \\(\\gamma \\left({z}_{k}^{g}|g\\right)\\) according to the normalized \\({f}_{as}^{k}\\). Figure 8 provides an example of the storage location assignment procedure, extending the example for storage area “A” in Fig. 7. In this example, \\({f}_{as}^{k}\\) of stack 1 is derived as \\({f}_{A1}^{1}=\\mathrm{min}\\left\\{\\mathrm{0.25,0.2}\\right\\}-\\mathrm{max}\\left\\{\\mathrm{0,0}\\right\\}=0.2-0=0.2\\) for weight class 1. Similarly, the resulting \\({f}_{as}^{k}\\) of stack 2 for weight classes 1 and 2 are \\({f}_{A2}^{1}=0.25-0.2=0.05\\) and \\({f}_{A2}^{2}=0.4-0.25=0.15\\), respectively. Then the weighted averages of \\(\\gamma \\left({z}_{k}^{g}|g\\right)\\) are derived as \\({f}_{A1}^{1}/{f}_{A1}^{1}\\times \\gamma \\left({z}_{1}^{g}|g\\right)\\) for stack 1 and \\({f}_{A2}^{1}/\\left({f}_{A2}^{1}+{f}_{A2}^{2}\\right)\\times \\gamma \\left({z}_{1}^{g}|g\\right)+{f}_{A2}^{2}/\\left({f}_{A2}^{1}+{f}_{A2}^{2}\\right)\\times \\gamma \\left({z}_{2}^{g}|g\\right)\\) for stack 2. Thus, for a new container input, the stack that yields the largest weighted average of \\(\\gamma \\left({z}_{k}^{g}|g\\right)\\) is selected among all the stacks in the given storage area. Fig. 8 Illustration of the storage location assignment procedure (\\({{\\varvec{\\pi}}}_{{\\varvec{k}}}^{{\\varvec{g}}}=0.25\\;{\\mathbf{for}\\;\\mathbf{k}}=1,2,3,4\\)) Full size image The overall procedure of the DSS is described by Algorithm 1. 4 Numerical experiments We conducted numerical experiments to validate the data-driven DSS (also called a GMM-DSS), which features the GMM-based weight clustering and dynamic adjustment mechanism for storage location assignment. We illustrate the improvements achieved by our proposed method through the experiments comparing with the stacking strategies that do not employ GMMs and/or dynamic adjustment mechanisms. First, the input data in the numerical experiment are described. Second, as a result of module 1, the generated GMM to define the weight class for each test instance is reported. Third, the impacts of the unit number of stacks on the algorithm performance are analyzed. Finally, an analysis comparing the stacking performances of the container stacking strategies is presented. This analysis shows how the wealth of data can be applied in the CSP to provide valuable decision support. All the algorithms were coded in Python and executed on a PC with an i5-6600H 3.3 GHz Intel Core processor and 8.0 GB of RAM. 4.1 Input data The input data used in the numerical experiments were collected for 10 months in 2018 from a typical container terminal in Busan, Republic of Korea. The original data included detailed container information, such as the time of arrival and departure, departing vessel, POD, size, type, and weight of each container. Because the stacking decisions are made according to the container group, the original data were classified into container groups with the same attributes (e.g., departing vessel, POD, size, and type). In addition, only 20-foot containers were used in this study. Subsequently, a dataset of 12 container groups was used for the analysis. Table 1 reports the details of the selected 12 instances. Table 1 Test instances for numerical experiments Full size table Next, we present the input data distributions over the entire voyage to check the justification for converting the container weights. This analysis comes from our assumption that the probability distribution of the container weights can be made available from the historical data due to the repeated tendencies over the voyage. It is rarely studied in literature to handle the information on container weights. Only a few studies, such as Kang et al. (2006) and Zhang et al. (2014b), utilized the true probability distribution or portion of the weight groups assuming such information is given or estimated by analysis of historical data in advance. Figure 9 shows the distribution of the container weights over the entire voyage for instances 4 and 7. The x-axis indicates the range of container weight uniformly divided by 10 classes, and the y-axis indicates the number of containers for each voyage which is normalized to a value between 0 and 1. In this case, the distribution of container weights was similar over the entire voyage of the same instance. This is reasonable because the composition of the export cargo tends to be similar for each voyage. Due to the extensive data, GMM-based predictions for weight classes can be a powerful tool for effective category stacking in yards. Fig. 9 Distribution of container weights over the entire voyage (Instances 4 and 7) Full size image For container groups of a general type, the terminal had 19 blocks consisting of 50 bays each, as well as 10 stacks and six tiers. The storage area for the same container group was reserved in a unit of stacks. Because the unit number of stacks is usually set to 10 stacks (i.e., one bay) or five stacks, we conducted the experiments for both cases where the experimental results are presented in Sect. 4.5. The total number of storage areas was set to the minimum value required for the corresponding container group. Considering the buffer storage space for rehandling operations that exists in practice, the maximum allowable tier for containers was limited to the fifth tier. 4.2 Design of experiments For each instance in this study, we divided a training dataset for generating the GMM in module 1 and a test dataset for simulation. The test dataset is constructed by randomly selecting a voyage and collecting a corresponding list of loaded containers from the historical data of the test ship-lanes (test instances). The training dataset is the remainder of historical data except for the test dataset. The results of GMM-based clustering with the training dataset are provided in Sect. 4.3, followed by sensitivity and comparative analysis results. In simulation experiments, two kinds of randomness are considered: the container arrival sequence and assignment to a storage area. The detailed list of the to-be-stacked containers is unknown in advance and even unpredictable in practice. Further, a pre-selected storage area is given because the designation of the storage area for an arriving container is dynamically assigned depending on the workload of the YC. Therefore, the container arrival sequence was made by randomly selecting a container list in the test dataset, and a storage area was randomly assigned to a container in the simulations. The stacking performance was evaluated based on category stacking, which aims to cluster containers with similar weights into the same stack. Therefore, we introduced an evaluation function $$E\\left( g \\right) = \\frac{{\\mathop \\sum \\nolimits_{{a \\in A_{g} }} \\mathop \\sum \\nolimits_{{s \\in S_{a} }} SD\\left( {X_{as}^{g} } \\right)}}{{\\mathop \\sum \\nolimits_{{a \\in A_{g} }} \\left| {S_{a} } \\right|}},$$ (9) where the standard deviation of the container weights in the stack was measured. The evaluation function \\(E\\left(g\\right)\\) indirectly minimizes the makespan during future loading operations for a given category stacking strategy. 4.3 GMM-based clustering for container weights Using 12 test instances, we employed the GMM method to cluster the container weights into \\(K\\) clusters. For the training data, a set of preliminary experiments was conducted to investigate the appropriate value of \\(K\\) for each test instance, varying \\(K\\) from 1 to 5 in steps of 1. The value of \\(K\\) was assigned the smallest value based on the Bayesian information criterion (BIC). Figure 10 shows the resulting GMM for the two test instances. The histogram in Fig. 10 indicates that the container weights exhibited a multi-peak distribution, and in both instances, it can be seen that five clusters are most appropriate. Figure 10 shows that GMM obtained via statistical modeling reasonably represents a distribution that is difficult to express as a single normal distribution. The results of module 1 for all the instances are presented in Appendix A. Fig. 10 Histogram and GMM-based density curve of the container weights (Instances 7 and 11) Full size image 4.4 Sensitivity analysis of the unit number of stacks This section validates the performance of GMM-DSS according to the unit number of stacks. The unit number of stacks within a storage area is usually set to 10 stacks for export containers, but it varies from terminal to terminal. For the sensitivity analysis of the unit number of stacks, \\(\\left|{S}_{a}\\right|\\) was set to 5, 10, 15, and 20. The GMM was generated using the training dataset, and then GMM-DSS was implemented using the test dataset. Table 2 reports the change in the average of \\(E\\left(g\\right)\\) according to the variation in the unit number of stacks \\(\\left|{S}_{a}\\right|\\). In the case of instance 1, it was excluded from experiments on more than 15 stacks as the number of tested containers was less than 50. In most instances, it is observed that \\(E\\left(g\\right)\\) decreases as the unit number of stacks increases. These results imply that as the number of unit stacks increases, the weight distribution of containers may have been relatively stable and homogeneous over storage areas since the containers are less scattered into multiple storage areas. Meanwhile, Instances 3, 5, and 6 yield the minimum \\(E\\left(g\\right)\\) when the unit number of stacks is equivalent to 10 and 15, although the differences of \\(E\\left(g\\right)\\) in instances are not significant. The GMM-DSS works reliably to comply with the category stacking strategy, including the case of 10 stacks widely used in container terminals. Thus it is believed to be worth being introduced into the terminal operating system. Table 2 Impact of the unit number of stacks on average \\({\\varvec{E}}\\left({\\varvec{g}}\\right)\\) Full size table 4.5 Comparative analysis of container stacking strategies For comparison purposes, we implemented four alternative stacking strategies: a GMM-based static stacking strategy (GMM-SS), current practice, hybrid sequence stacking strategy (HSSS), and a random stacking strategy (RSS). For the GMM-SS, the weight classes and storage locations of the containers were determined by our GMM-based dynamic stacking strategy (GMM-DSS), while the dynamic adjustment mechanism in the weight class-to-stack assignment was ignored. For the current practice, the weight classes were defined by dividing the container weights into weight classes by ‘number of stacks in a bay,’ where each weight class has equal size of weight range according to the historical data. In addition, the storage locations of the containers were determined in such a way that the stack and weight classes were allocated on a one-to-one basis. For the HSSS proposed by Chen and Lu (2012), the weight classes were defined by dividing the container weights weight classes by ‘(number of stacks in a bay)\\(+\\)(number of tiers in a stack)\\(-3.\\)’ Then, HSSS induces heavier containers to be stacked in the left upper locations and lighter containers to be stacked in the right lower locations. In the RSS, the containers were stacked in a way that filled the stack sequentially according to the arrival order without considering the container weights. All five strategies, including our proposed method, were executed in seconds; thus, they are suitable for an online algorithm. We conducted a simulation to compare the proposed method to the four alternative strategies for two different cases where the unit of storage area is five stacks or 10 stacks. The resulting performance for the stacking strategies is presented in Tables 3 and 4 for the cases in terms of the average \\(E\\left(g\\right)\\) for 100 repetitions. For both cases, the proposed GMM-DSS method obtained the minimum \\(E\\left(g\\right)\\) on average for all test instances. Specifically, for the case of five stacks in Table 3, the GMM-DSS performed better than the GMM-SS by 6.0%. This is the result of the dynamic adjustment mechanism in the weight class-to-stack assignment. In addition, the performance of the GMM-DSS was better than the current practice, HSSS, and RSS by 33.1, 41.6 and 44.1%, respectively. This result implies that the proposed method was more effective as the data-driven approach as well as the dynamic adjustment mechanism was incorporated into the stacking strategy. For the case of 10 stacks in Table 4, the results show that proposed method is superior to the other methods as in the case of five stacks. The performance of the GMM-DSS was better than the GMM-SS, current practice, HSSS, and RSS by 8.1, 43.6, 52.8, and 59.6%, respectively. Moreover, Fig. 11 demonstrates the robust performance of our proposed method for the experimental repetitions. The current practice performed better on average than the HSSS and RSS, and the proposed GMM-DSS and GMM-SS are dominant over the current practice. Therefore, we can conclude that the proposed GMM-DSS method is worth introducing in practice because it has the most reliable and best performance. Table 3 Experimental results for the test instances (the unit of storage area = five stacks) Full size table Table 4 Experimental results for the test instances (the unit of storage area = 10 stacks) Full size table Fig. 11 Box plots of experimental results for \\(E\\left(g\\right)\\) after 100 repetitions (the unit of storage area = 10 stacks, Instances 10 and 12) Full size image 4.6 Comparative analysis according to variability This section validates the algorithm performances according to variability in container weights. The proposed GMM-DSS utilizes the information on container weights from historical data and creates a GMM to approximate the distribution of container weights; thus it should be verified whether the GMM-DSS and GMM-SS can yield robust performance under a certain degree of variability. To this end, container weights \\(x\\left(i\\right){^{\\prime}}\\) for the test were randomly generated as follows: \\({x\\left(i\\right)}^{^{\\prime}}=\\mathrm{min}\\left(\\mathrm{max}\\left(x\\left(i\\right)+500\\times \\sigma \\times N\\left(0, 1\\right),LB\\left(i\\right)\\right),UB\\left(i\\right)\\right)\\)(kg), where \\(x\\left(i\\right)\\) is a randomly selected sample data from the training dataset, \\(\\sigma\\) is related to the variability, \\(N\\left(0, 1\\right)\\) is the random generator from a standard normal distribution, and \\(LB\\left(i\\right)\\) and \\(UB\\left(i\\right)\\) are the lower bound and upper bound on weights of container \\(i\\) introduced to avoid extreme values. \\(\\sigma\\) was set to 0, 1, 2, 3, and 4. Figure 12 shows the performance of five stacking algorithms for the experimental repetitions under variability on container weights. For all the cases of σ, the proposed GMM-DSS and GMM-SS dominantly outperform the other algorithms in terms of \\(E\\left(g\\right)\\). GMM-DSS slightly outperforms GMM-SS. Furthermore, although performance degrades as variability increases, there is no significant difference in the change compared to other algorithms. Thus, it can be concluded that the proposed algorithms are robust to variabilities by taking advantage of predictive analytics. Fig. 12 Results according to variability on \\(E\\left(g\\right)\\) (the unit of storage area = 10 stacks, Instances 5 and 9) Full size image 5 Conclusion The goal of this paper is to introduce the wealth of data generated by the container terminal into the methodology of determining the storage location of the arriving containers. We presented a two module-based DSS to manage the CSP in container terminals. In our study, the CSP was solved by a category stacking strategy to facilitate stowage planning. In contrast to previous studies, the proposed method applies predictive analytics for container weight classification using historical data, and container assignment to a specific storage area is executed dynamically to outperform the existing stacking strategies. To the best of our knowledge, these two main features have not been introduced in the literature before. Specifically, in module 1, we generated a container group-specific GMM for container weight classification. In module 2, we implemented the DSS as an online algorithm to determine the storage location of an arriving container in real time. We conducted numerical experiments to validate the effectiveness of the proposed method using real-life data from a typical container terminal in Busan, Republic of Korea. For the generality of terminal environments, two different cases for the reservation unit in a storage area are considered in the experiments. The experimental results showed that the proposed method outperforms four other alternative stacking strategies (i.e., the GMM-SS, current practice, HSSS, and RSS). Therefore, our proposed method performs robustly and can further improve yard operations and terminal competitiveness. The experimental results are encouraging in that it implies that the improvements in the stacking performance are related to the category stacking. The proposed method is expected to achieve greater improvements in practice when data-driven inferences can be derived from more historical data. In future research, our method could be extended to transshipment containers by considering the detailed characteristics of those operations. In addition, novel stacking strategies, such as the flexible space-sharing strategy and related operations such as stowage planning, could be integrated into our method to generate greater effectiveness in terminal operations. Availability of data and material Not applicable. Code availability Not applicable. References Ambrosino D, Caballini C, Siri S (2013) A mathematical model to evaluate different train loading and stacking policies in a container terminal. Marit Econo Logist 15(3):292–308. https://doi.org/10.1057/mel.2013.7 Article   Google Scholar   Borgman B, van Asperen E, Dekker R (2010) Online rules for container stacking. OR Spectr 32(3):687–716. https://doi.org/10.1007/s00291-010-0205-4 Article   MATH   Google Scholar   Carlo HJ, Vis IF, Roodbergen KJ (2014) Storage yard operations in container terminals: literature overview, trends, and research directions. Eur J Oper Res 235(2):412–430. https://doi.org/10.1016/j.ejor.2013.10.054 Article   MATH   Google Scholar   Chen L, Lu Z (2012) The storage location assignment problem for outbound containers in a maritime terminal. Int J Prod Econ 135(1):73–80. https://doi.org/10.1016/j.ijpe.2010.09.019 Article   Google Scholar   De Castillo B, Daganzo CF (1993) Handling strategies for import containers at marine terminals. Transp Res B Methodol 27(2):151–166. https://doi.org/10.1016/0191-2615(93)90005-U Article   Google Scholar   Dekker R, Voogd P, Asperen EV (2006) Advanced methods for container stacking. OR Spectr 28(4). https://doi.org/10.1007/s00291-006-0038-3 Duinkerken MB, Evers JJ, Ottjes JA (2001) A simulation model for integrating quay transport and stacking policies on automated container terminals. In: Proceedings of the 15th European Simulation Multiconference, pp 909–916. Feng Y, Song DP, Li D (2021) Smart stacking for import containers using customer information at automated container terminals. Eur J Oper Res. https://doi.org/10.1016/j.ejor.2021.10.044 Article   MATH   Google Scholar   Figueiredo MAT, Jain AK (2002) Unsupervised learning of finite mixture models. IEEE Trans Pattern Anal Mach Intell 24(3):381–396. https://doi.org/10.1109/34.990138 Article   Google Scholar   Güven C, Eliiyi DT (2014) Trip allocation and stacking policies at container terminal. Transp Res Proc 3:565–573. https://doi.org/10.1016/j.trpro.2014.10.035 Article   Google Scholar   Güven C, Eliiyi DT (2019) Modelling and optimisation of online container stacking with operational constraints. Marit Policy Manag 46(2):201–216. https://doi.org/10.1080/03088839.2018.1450529 Article   Google Scholar   He J, Tan C, Yan W, Huang W, Liu M, Yu H (2020a) Two-stage stochastic programming model for generating container yard template under uncertainty and traffic congestion. Adv Eng Inform 43:101032. https://doi.org/10.1016/j.aei.2020.101032 Article   Google Scholar   He Y, Wang A, Su H (2020b) The impact of incomplete vessel arrival information on container stacking. Int J Prod Res 58(22):6934–6948. https://doi.org/10.1080/00207543.2019.1686188 Article   Google Scholar   Hu W, Wang H, Min Z (2014) A storage allocation algorithm for outbound containers based on the outer–inner cellular automaton. Inf Sci 281:147–171. https://doi.org/10.1016/j.ins.2014.05.022 Article   MathSciNet   Google Scholar   Hu H, Mo J, Zhen L (2021) Improved Benders decomposition for stochastic yard template planning in container terminals. Transp Res c Emerg Technol 132:103365. https://doi.org/10.1016/j.trc.2021.103365 Article   Google Scholar   Jiang XJ, Jin JG (2017) A branch-and-price method for integrated yard crane deployment and container allocation in transshipment yards. Transp Res B Methodol 98:62–75. https://doi.org/10.1016/j.trb.2016.12.014 Article   Google Scholar   Kang J, Ryu KR, Kim KH (2006) Deriving stacking strategies for export containers with uncertain weight information. J Intell Manuf 17(4):399–410. https://doi.org/10.1007/s10845-005-0013-x Article   Google Scholar   Karp RM (1992) On-line algorithms versus off-line algorithms: How much is it worth knowing the future? In: IFIP Congress (1), Vol. 12, pp. 416–429. Kim KH, Kim HB (1999) Segregating space allocation models for container inventories in port container terminals. Int J Prod Econ 59(1–3):415–423. https://doi.org/10.1016/S0925-5273(98)00028-0 Article   Google Scholar   Kim KH, Kim HB (2002) The optimal sizing of storage space and handling facilities for import containers. Transp Res B Methodol 36(9):821–835. https://doi.org/10.1016/S0191-2615(01)00033-9 Article   Google Scholar   Kim KH, Park YM, Ryu KR (2000) Deriving decision rules to locate export containers in container yards. Eur J Oper Res 124(1):89–101. https://doi.org/10.1016/S0377-2217(99)00116-2 Article   MATH   Google Scholar   Kim KH, Kang JS, Ryu KR (2004) A beam search algorithm for load sequencing of outbound containers in port container terminals. OR Spectr 26(1):93–116. https://doi.org/10.1007/s00291-003-0148-0 Article   MATH   Google Scholar   Maldonado S, González-Ramírez RG, Quijada F, Ramírez-Nafarrate A (2019) Analytics meets port logistics: a decision support system for container stacking operations. Decis Support Syst 121:84–93. https://doi.org/10.1016/j.dss.2019.04.006 Article   Google Scholar   Murty KG, Liu J, Wan YW, Linn R (2005) A decision support system for operations in a container terminal. Decis Support Syst 39(3):309–332. https://doi.org/10.1016/j.dss.2003.11.002 Article   Google Scholar   Park T, Choe R, Kim YH, Ryu KR (2011) Dynamic adjustment of container stacking policy in an automated container terminal. Int J Prod Econ 133(1):385–392. https://doi.org/10.1016/j.ijpe.2010.03.024 Article   Google Scholar   Sauri S, Martin E (2011) Space allocating strategies for improving import yard performance at marine terminals. Transp Res E Logist Transp Rev 47(6):1038–1057. https://doi.org/10.1016/j.tre.2011.04.005 Article   Google Scholar   Stahlbock R, Voß S (2008) Operations research at container terminals: a literature update. OR Spectr 30(1):1–52. https://doi.org/10.1007/s00291-007-0100-9 Article   MathSciNet   MATH   Google Scholar   Steenken D, Voß S, Stahlbock R (2004) Container terminal operation and operations research—a classification and literature review. OR Spectr 26(1):3–49. https://doi.org/10.1007/s00291-003-0157-z Article   MATH   Google Scholar   Ting SC, Wang JS, Kao SL, Pitty FM (2010) Categorized stacking models for import containers in port container terminals. Marit Econ Logist 12(2):162–177. https://doi.org/10.1057/mel.2010.4 Article   Google Scholar   Vis IF, De Koster R (2003) Transshipment of containers at a container terminal: an overview. Eur J Oper Res 147(1):1–16. https://doi.org/10.1016/S0377-2217(02)00293-X Article   MATH   Google Scholar   Zhang C, Liu J, Wan YW, Murty KG, Linn RJ (2003) Storage-space allocation in container terminals. Transp Res B Methodol 37(10):883–903. https://doi.org/10.1016/S0191-2615(02)00089-9 Article   Google Scholar   Zhang C, Chen W, Shi L, Zheng L (2010) A note on deriving decision rules to locate export containers in container yards. Eur J Oper Res 205(2):483–485. https://doi.org/10.1016/j.ejor.2009.12.016 Article   MATH   Google Scholar   Zhang C, Wu T, Kim KH, Miao L (2014a) Conservative allocation models for outbound containers in container terminals. Eur J Oper Res 238(1):155–165. https://doi.org/10.1016/j.ejor.2014.03.040 Article   MathSciNet   MATH   Google Scholar   Zhang C, Wu T, Zhong M, Zheng L, Miao L (2014b) Location assignment for outbound containers with adjusted weight proportions. Comput Oper Res 52:84–93. https://doi.org/10.1016/j.cor.2014.06.012 Article   MathSciNet   MATH   Google Scholar   Zhen L (2013) Yard template planning in transshipment hubs under uncertain berthing time and position. J Oper Res Soc 64(9):1418–1428. https://doi.org/10.1057/jors.2012.108 Article   Google Scholar   Zhen L (2014) Container yard template planning under uncertain maritime market. Transp Res E Logist Transp Rev 69:199–217. https://doi.org/10.1016/j.tre.2014.06.011 Article   Google Scholar   Zhou C, Wang W, Li H (2020) Container reshuffling considered space allocation problem in container terminals. Transp Res E Logist Transp Rev 136:101869. https://doi.org/10.1016/j.tre.2020.101869 Article   Google Scholar   Download references Acknowledgements The authors are grateful to the anonymous reviewers for reading the manuscript carefully and providing constructive comments which greatly helped to improve this paper. Funding This research is a part of the project titled “Development of Open Platform Technologies for Smart Maritime Safety and Industries” funded by the Korea Research Institute of Ships and Ocean Engineering (PES4450). Author information Authors and Affiliations Maritime Safety and Environmental Research Division, Korea Research Institute of Ships and Ocean Engineering, Daejeon, Republic of Korea Hyun Ji Park, Sung Won Cho, Abhilasha Nanda & Jin Hyoung Park Contributions HJP: Conceptualization of this study, Methodology, Validation, Formal analysis, Writing—Original Draft, Writing—Review Editing. SWC: Conceptualization of this study, Methodology, Formal analysis, Writing—Original Draft, Writing—Review Editing, Project administration, Supervision. AN: Software, Data Curation, Visualization. JHP: Writing—Review Editing, Project administration, Supervision, Funding acquisition. Corresponding author Correspondence to Sung Won Cho. Ethics declarations Conflict of interest The authors declare no conflict of interest. Additional information Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Appendix A: The results of module 1 for the test instances Appendix A: The results of module 1 for the test instances See Table 5. Table 5 Parameters of GMM Full size table Rights and permissions Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions About this article Cite this article Park, H.J., Cho, S.W., Nanda, A. et al. Data-driven dynamic stacking strategy for export containers in container terminals. Flex Serv Manuf J 35, 170–195 (2023). https://doi.org/10.1007/s10696-022-09457-8 Download citation Accepted 05 June 2022 Published 27 June 2022 Issue Date March 2023 DOI https://doi.org/10.1007/s10696-022-09457-8 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing initiative Keywords Container terminals Container stacking problem (CSP) Machine learning Gaussian mixture model (GMM) Use our pre-submission checklist Avoid common mistakes on your manuscript. Sections Figures References Abstract Introduction Literature review Problem description Numerical experiments Conclusion Availability of data and material Code availability References Acknowledgements Funding Author information Ethics declarations Additional information Appendix A: The results of module 1 for the test instances Rights and permissions About this article Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"

Paper 5:
- APA Citation: None
  Main Objective: To design and deploy a Docker-enabled Federated Learning framework (DFL) for multi-modal data stream classification.
  Study Location: Unspecified
  Data Sources: ['Distributed multi-modal physiological data streams']
  Technologies Used: ['Docker', 'Federated Learning', 'Multi-modal data stream processing', 'MQTT']
  Key Findings: ['The proposed DFL framework can handle multiple clients running in parallel ensuring scalability.', 'The average testing accuracy and F1-score of the global model are adequate for real-time emotion classification from multi-modal data streams.', 'The DFL framework ensures privacy preservation by securing the sensitive data by only providing access to the corresponding end user not other parties.']
  Extract 1: The proposed Docker-enabled Federated Learning framework (DFL) simplifies the deployment of federated learning (FL) among numerous clients by utilizing the Docker-container solutions. DFL system integrates multi-modal data stream processing along with a lightweight MQTT protocol for the Internet of Things (IoT) environment.
  Extract 2: Additionally, the DFL system has been assessed in a test case scenario, which involves real-time emotion state classification from distributed multi-modal physiological data streams in a variety of real-world setups.
  Limitations: None mentioned.
  Relevance Evaluation: {'extract_1': 'The proposed Docker-enabled Federated Learning framework (DFL) simplifies the deployment of federated learning (FL) among numerous clients by utilizing the Docker-container solutions. DFL system integrates multi-modal data stream processing along with a lightweight MQTT protocol for the Internet of Things (IoT) environment.', 'extract_2': 'Additionally, the DFL system has been assessed in a test case scenario, which involves real-time emotion state classification from distributed multi-modal physiological data streams in a variety of real-world setups.', 'relevance_score': 1.0}
  Relevance Score: 1.0
  Inline Citation: None
  Explanation: The proposed Docker-enabled Federated Learning framework (DFL) simplifies the deployment of federated learning (FL) among numerous clients by utilizing the Docker-container solutions. DFL system integrates multi-modal data stream processing along with a lightweight MQTT protocol for the Internet of Things (IoT) environment. Additionally, the DFL system has been assessed in a test case scenario, which involves real-time emotion state classification from distributed multi-modal physiological data streams in a variety of real-world setups.

 Full Text: >
"Your privacy, your choice We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Accept all cookies Skip to main content Advertisement Log in Find a journal Publish with us Track your research Search Cart Home Computing Article A Docker-based federated learning framework design and deployment for multi-modal data stream classification Regular Paper Published: 11 May 2023 Volume 105, pages 2195–2229, (2023) Cite this article Download PDF Access provided by University of Nebraska-Lincoln Computing Aims and scope Submit manuscript Arijit Nandi, Fatos Xhafa & Rohit Kumar  876 Accesses 2 Citations Explore all metrics Abstract In the high-performance computing (HPC) domain, federated learning has gained immense popularity. Especially in emotional and physical health analytics and experimental facilities. Federated learning is one of the most promising distributed machine learning frameworks because it supports data privacy and security by not sharing the clients’ data but instead sharing their local models. In federated learning, many clients explicitly train their machine learning/deep learning models (local training) before aggregating them as a global model at the global server. However, the FL framework is difficult to build and deploy across multiple distributed clients due to its heterogeneous nature. We developed Docker-enabled federated learning (DFL) by utilizing client-agnostic technologies like Docker containers to simplify the deployment of FL frameworks for data stream processing on the heterogeneous client. In the DFL, the clients and global servers are written using TensorFlow and lightweight message queuing telemetry transport protocol to communicate between clients and global servers in the IoT environment. Furthermore, the DFL’s effectiveness, efficiency, and scalability are evaluated in the test case scenario where real-time emotion state classification is done from distributed multi-modal physiological data streams under various practical configurations. Similar content being viewed by others Federated Learning with Exponentially Weighted Moving Average for Real-Time Emotion Classification Chapter © 2023 Federated learning inspired privacy sensitive emotion recognition based on multi-modal physiological sensors Article 24 September 2023 FTL-Emo: Federated Transfer Learning for Privacy Preserved Biomarker-Based Automatic Emotion Recognition Chapter © 2024 1 Introduction Artificial intelligence (AI) has emerged as the de-facto technology for a wide range of applications (such as smart industry, healthcare, and Unmanned Areal Vehicle (UAV) applications etc.), coinciding with the growth of Cloud Computing (CC) [1]. More applications are migrating from private infrastructures to cloud data centers to reap its benefits, such as scalability, elasticity, agility, and cost efficiency [2]. Bringing AI to the cloud is advantageous because computing resources are efficiently utilized, and costs for application deployment and operation are minimized by appropriately distributing physical resources in clouds, such as CPU, memory, storage, and network resources, to various cloud applications [3]. With the recent advances in technology, the number of connected Internet of Things (IoT) devices has increased enormously and according to CISCO, this number could exceed 75 billion by 2025, which is 2.5 times the amount of data produced in 2020 (i,e. 31 billion) [4]. So, managing enormous, continuous, diverse, and dispersed IoT data appears to be hard while offering services at a specific performance level using cloud infrastructure. However, in traditional AI systems with CC enabled, data producers (such as IoT devices) most frequently transfer and exchange data with other parties (such as cloud servers), in order to train their models (such as Deep Learning(DL) or Machine Learning (ML)) to improve the performance of the system. This design pattern is unfeasible because of the high bandwidth requirements, legality, and privacy risks. For that, Federated Learning (FL) concept has recently emerged as a promising solution for mitigating the problems of data privacy, and legalization. Because Fl enables the distribution of computing load and training sensitive data locally without the need to transfer it to a primary server for privacy considerations [5]. In FL, each client (i.e., mobile, IoT and vehicular etc.) trains its local deep neural network (DNN) model with local data, which is then aggregated into a shared global model by the centralized server [6]. This scenario is repeated multiple rounds for better results and convergence. Having said that, Edge Computing (EC), bringing the Cloud Computing (CC) services closer to the data sources, is a revolutionary architecture that lowers latency and bandwidth costs while enhancing network resilience and availability. As a result, the EC-enabled architecture may satisfy the needs of time-critical applications with specific Service Level Agreement (SLA) requirements [4]. Additionally, the FL in the EC is a promising technique to handle IoT data by benefiting from distributed heterogeneous computing resources and relieving numerous clients’ privacy concerns because the generated raw data do not be exposed to the third party (cloud servers) [4]. It is seen that FL with EC has solved most of the problems, such as data privacy, legalization, lowering the latency, and minimizing the bandwidth, which is an outstanding achievement. However, it is also seen that, with the huge number of IoT devices, the data have velocity because of the high data generation rate and sequential arrival in time, according to data streams [7]. The data tuple needs to be processed and analyzed as soon as it arrives because operating to the edge has many limitations, such as limited computing resources, intermittent/denied network connectivity, etc. [8]. Apart from that, the underlying DL/ML model should be able to adapt to the changes from the continuous data stream in the dynamic environment. Nowadays, containers are easy-to-deploy software packages and containerized applications are easily distributed, making them a natural fit for EC with FL solutions [9]. Edge containers can be deployed in parallel to geographically diverse points of presence (PoPs) to achieve higher levels of availability when compared to a traditional cloud container [10]. The edge containers are located at the edge of the network, much closer to the end user. With the introduction of the container and microservice design, it is now possible to increase the scalability and elasticity of application deployment and delivering [10, 11]. There is a plethora of different FL platforms and frameworks from academia and industry. These are complex to use and deeper knowledge in FL is required [12]. Most of the existing FL systems from academia are mostly research-oriented such as LEAF [13], TFF [14], and PySyft [15]. These are not straightforward for the interested people in FL and have a lack of support to build the FL prototype and run it in production. Industrial FL frameworks such as FATE [16] and PaddleFL [17] are not friendly to beginners and researchers due to the complex environment setup and heavy system design [12]. Apart from the complex development and deployability of these FL frameworks, the local dataset is distributed over different clients in order to build the local model. That means data does not arrive sequentially, the way data arrives in the case of data streams. Hence, these FL frameworks are incapable of processing data streams in real time to adapt the changes in today’s dynamic environment. To address these issues, in this paper, we design and deploy the Docker-enabled Federated Learning framework (DFL) by taking the advantage of Docker-Containers and with the capability of processing the data streams in real-time. The DFL simplifies the deployment of the FL among numerous clients by utilizing Docker containers and it can handle multi-modal data streams to make the classification in real time. In particular, in the proposed DFL, each client and the global server implemented by TensorFlow is installed on a Docker container. Also, the communication between clients and the server is done by the lightweight MQTT protocol, which enables DFL to be used IoT environment. Additionally, the DFL’s efficacy, efficiency, and scalability are assessed in the test case scenario, which involves real-time emotion state classification from distributed multi-modal physiological data streams in a variety of real-world setups. The main contributions of our work are as follows: We proposed a Docker-enabled Federated Learning framework (DFL) which simplifies the deployment of the FL among numerous clients by utilizing the Docker-container solutions to integrate multi-modal data stream processing along with a lightweight MQTT protocol for the IoT environment. We deploy a multi-modal data streaming in an HPC system to implement the proposed DFL and leverage the docker-container solution to guarantee the scalability of the framework. A real-time emotion classification from multimodal physiological data stream use case is adapted for the DFL framework to process the high-velocity data streams. The experimental results verify its feasibility, scalability and privacy preserving in multi-modal data processing and online ML applications on cloud computing infrastructure. The rest of the paper is structured as follows: Sect. 3 introduces briefly the multi-modal data stream classification, docker technology and federate learning. The proposed DFL architecture is explained and illustrated in Sect. 4. Following to this, the deployment in real infrastructure, experimental method and performance evaluation of the proposed DFL are presented in Sect. 5 along with the experimental results and discussion. Finally, the paper ends with the conclusion in Sect. 6. 2 Related work This section presents the previous literature on data stream classification, FL and the Docker-based deployment in the EC paradigm. Furthermore, the pitfalls of data stream classification approaches and the existing FL systems are mentioned. The digital revolution is characterized by an ongoing data collection process, which results in data being fed into the corresponding machine learning algorithms across various applications [18]. In the context of distributed machine learning, continuous data collection provides training data in the form of a data stream for each \"node\" in the distributed system. Given that \"(full) batch processing\" is realistically impractical in the face of continuous data delivery and distributed training of models using streaming data needs (single-pass) [18]. In the following two instances, streaming data is typically processed: (1) For the master-worker learning architecture, the data stream comes at a single master node and is then dispersed among a total of N worker nodes to distribute the computational load and speed up training time; (2) The objective of the FL and EC frameworks is to develop a machine learning model utilizing data from all of the nodes in a collection of N geographically scattered nodes, each of which receives its own independent stream of data, without sharing the data to other parties [18]. In the literature, most of the data stream classification approaches follow the first master-worker approach. These approaches ranging from different model ensemble to feature fusion, in order to increase the overall performance of the predictive system, such approaches are Accuracy Weighted Ensemble classifier (AWE) [19], Adaptive Random Forest classifier (ARF) [20], Dynamic Weighted Majority ensemble classifier (DWM) [21], Learn++.NSE ensemble classifier [22], Learn++ ensemble classifier [23], Streaming Random Patches ensemble classifier (SRPE) [24] and many more. There have been several approaches developed to address the complexity of FL integration. For instance, Google, the original FL developer [6], intends to improve their TensorFlow engine to allow distributed learning with TensorFlow Federated [14]. The framework includes crucial functions including the ability to subscribe to a small number of events to track the execution stages and enable third-party integration. The framework also includes the tools needed for a quick start as well as the datasets that are often utilized. But because the framework only works with TensorFlow models, it is more challenging for researchers to combine FL with models created using different training engines. There are also FL frameworks that are engine-specific, such as PaddleFL [17], which supports the PaddlePaddle engine, and FedToch [25], which exclusively supports PyTorch models. In contrast to the previously stated frameworks, FedML [26] offers a comprehensive framework that originally served as a fair benchmarking tool for methods utilizing federated learning. From client selection through aggregation and validation, the whole federated workflow is integrated. Their architecture, which is essential for parallel client simulation and enables executions to take place within or outside of a single host, offers message-passing interface-based (MPI) settings, in contrast to other systems where tests only run locally [26]. Also, to test their performances, MNIST handwritten digits and the CIFAR for images are the popular benchmark datasets used in the literature [27]. Also, the industrial FL frameworks like FATE [16] and PaddleFL [17] are not user- and researcher-friendly because of the complicated environment setup and sophisticated system design [28]. There is a plethora of master-worker-based approaches for data stream processing. The major disadvantages of these approaches are as follows: (1) privacy and security issues, because data streams need to be sent to the master server; (2) sending the data to the master server causes a high bandwidth overload (3) most of the approaches process the data in batch mode hence they are incapable of processing high-velocity data streams. Whereas, FL with EC is more reliable in this regard, fulfilling all the needs (as mentioned in the introduction). In addition to the complicated design and deployment of these FL frameworks, the local dataset is distributed across several clients to construct the local model, indicating that data does not come timely, as it does in data streams. As a result, these FL frameworks cannot handle data streams in real-time to respond to the changes in today’s dynamic environment. Also, the analysis of computation resources needed to run the models on the client side is missing in all the available FL systems, making these even harder to use in a real-world scenario. Regarding the deployment of FL in edge devices, Docker containers are the best suitable option to utilize as they are simple to install software packages, and containerized applications are simple to distribute [29]. It can make the deployment quickly, has a tiny footprint, and has high performance making it a potentially viable Edge Computing platform [30]. Also, the CPU, memory, storage and network performance is minimal as compared to the Virtual Machines (VMs) running on hypervisor [31]. Dockers run 26X faster than VMs and can run on small devices with lower resources, hence the best fit for FL in EC [32]. So, the docker based FL deployment is a solution to run efficiently on the edge devices. 3 Background In this section, we present a brief introduction to multi-modal steam classification, docker technology and federated learning. 3.1 Multi-modal stream classification Classifying continuous incoming unbounded data tuples produced from multiple sources/sensors in real-time is called multi-modal stream classification [33]. In this case, the base classifier learns and updates itself progressively (called online learning) from the data stream [19]. As a consequence, the online classifier performs poorly at the initial stage but improves performance gradually as it sees more data tuples. The progressive (online) learning steps of a stream classifier are presented as follows [19]: 1. Receive incoming data instance or tuple (\\(x_t\\)) without the actual label (\\(y_t\\)). Where t is the time of arrival. 2. Predict the class \\(\\hat{y}_t = M_t(x_t)\\) for \\(x_t\\), where \\(M_t\\) is the current model. 3. Immediately after classifying \\(x_t\\), receive the corresponding actual label \\(y_t\\). 4. Train the model \\(M_t\\) with the actual label \\(y_t\\) and update the performance metrics using \\((\\hat{y_t}, y_t)\\). 5. Proceed to the next data tuple. Now, in the case of multi-modal data stream classification, the extracted features of the received data tuples from each modality are fused (using the concatenation approach) and fed to the stream classifier for the class prediction, which follows the previously mentioned progressive learning steps. In Fig. 1 the process of multi-modal data stream classification is depicted. The main noticeable point in Fig. 1 is the time (t) dimension, where different data tuple arrives in a stream mode at a different time (\\(t_1,t_2 \\cdots \\)) from different modality. Fig. 1 Multi-modal data stream classification in online mode Full size image In this work, the following assumptions are made to classify multi-modal data stream: The arrival of data tuples is sequential (one at a time). The base model first receives the unlabeled data tuples of the stream and immediately after the class prediction, the true/actual class label of the corresponding data tuple arrives. Hence it is supervised stream classification approach. The model is tested first and then trained based on the arrived true class label, also known as interleaved test-then-train approach. Model can see the received data tuples at most once, hence multiple runs through the data is not possible. Only temporary storage for the data stream is available, meaning no loop back through the received data tuples. In this research study, we have considered the feature fusion approach (shown in Fig. 1) for multi-modal data stream classification. In feature fusion approach includes (1) feature extraction from data streams from every single modal; (2) performing a feature fusion to create a single feature representation; (3) performing the data stream classification and updating the model progressively (online learning). 3.2 Docker technology Docker is a well-known, open-source and very popular containerization framework in the software industry [34]. It automates the development and deployment of application containers by allowing us to bundle an application with its run-time requirements into a container image and then execute that image on a host system [11]. The Docker Engine is installed on the host machine and is presently supported by the majority of operating systems. Containers are an abstraction at the application layer that groups together code and dependencies. Users can easily acquire binaries or libraries by providing the operating system release. The important part is that there is no need to install a new operating system; containers just get the kernel of the operating system, which may be shared among containers. In comparison to new system installation, which can take tens of gigabytes and boot slowly, container images are often tens of megabytes in size and start quickly. Docker images are a lightweight, stand-alone, compact and executable package that includes all the necessary requirements to run software including code, libraries, environment variables and configuration files [1]. Due to this property, an image is heavily customizable based on the contents with a little change. For that users create a Dockerfile with a simple syntax for defining the steps needed to build an image and run it. In summary, Dockerfile is the builder of an image and a container is a runnable instance of an image. In Fig. 2 the Docker ecosystem is presented. Fig. 2 Overview of docker ecosystem Full size image 3.3 Federated learning The Federated Learning ( [6, 35]) approach intends to provide system support for cooperatively training machine learning (ML) or deep learning (DL) models using distributed data silos while maintaining privacy and model performance. The main system design to support training models \"in-place\", which differs from traditional ML/DL model training, in which data is collected and managed centrally over a fully controlled distributed cluster [36]. The main advantage of such an \"in-place\" training system (FL) is that it facilitates privacy and security protections, which have led to new legislation such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA), which prohibits transferring user private data to a centralized location [36]. Based on the scale of the federation, FL is divided into two types – cross-silo and cross-device. In the case of cross-silo, the number of clients is usually small but has huge computational ability [37]. On the other hand, when it comes to cross-device the number of clients are enormous and have low computation power. From the model training and availability aspect, the organization (cross-silo) to be available is not the same as mobile devices (cross-device) [28]. In cross-device the poor network and limited computation resources can hamper the device’s availability. In this work, we have considered the cross-device federation approach for our DFL development. In Fig. 3, the system overview of cross-device FL is presented. Fig. 3 System overview of cross-device federated learning Full size image The FL has three major components as follows [7]: Clients These are the stakeholders in FL. Only clients have data accessibility because data is generated at this end. The main responsibilities of clients are local model training from the generated data at its end and sharing their model parameters with the global server. The important point is that no raw data is shared with other parties anywhere. Server The server/global server is usually the strong computation node. It handles the communications between the clients and itself to create the global model based on the received local models from the clients. Aggregation framework FL’s aggregation framework performs two tasks: computation and communication activities. Computation occurs on both the client and the global server, and communication occurs between clients and management. The computational component is utilized for model training, while the communication component is used to communicate model parameters. Federated Averaging (FedAvg) is FL’s most popularly used aggregation framework. 3.4 Realtime multimodal emotion classification system ReMECS (Realtime Multimodal Emotion Classification System) is our previously developed emotion classification system using multimodal physiological data streams in real-time (details see [33]). In ReMECS, the multimodal data stream arrives, and the feature extraction is done for the corresponding stream modality. Then the extracted features are sent to the corresponding modality classifier classification. The base classifier for ReMECS was a 3-layer feed-forward neural network, and the Stochastic Gradient Descent (SGD) was used to train the classifier in an online fashion. In the end, emotion class predictions from the corresponding classifier for each sensor modality are collected, and a decision fusion (Dynamic Weighted Majority voting) is done to predict the final emotion prediction. However, in this study, feature fusion has been taken into consideration rather than decision fusion to lessen the complexity of the real-time classifier inside the DFL framework. As a result, our ReMECS is altered in this regard, but the system’s fundamental design and operation have remained the same. 4 DFL architecture In this section, the overall architecture of the proposed DFL framework is presented. The motivation of this work is to make easy development and deployment of a federated learning framework using the cloud-native solution called Docker-container which is capable of processing real-time multi-modal data streams and handling scalability (multiple clients). The DFL architecture is presented in Fig. 4. Fig. 4 The proposed DFL’s architecture Full size image From Fig. 4, it can be seen that the DFL consists of one global server/node and multiple client nodes. At the global server, there is two layers (global model creation layer and model transfer layer) running inside the docker container. These are coupled together, hence a multi-layer structure. The model transfer layer is mainly the MQTT broker running whose main function is to receive the local models from client nodes and send them to the global model creation layer, and send the created global model to each client nodes for next round in FL. Now, global model creation layer, as the name suggests, it is responsible for performing federated averaging on all the local models collected at time t and creating the global model to send it to the MQTT broker. However, selecting the appropriate protocol is totally application-dependent; MQTT and CoAP are widely used in IoT contexts. The four key reasons for using MQTT over CoAP in our DFL development are as follows: Message queuing is supported in MQTT for disconnected subscribers, but not in CoAP. The maximum message size in MQTT is 256MB, however, it is 1152 Bytes (1.1KB) in COAP. The message (model communicated by clients to the server) size for our DFL framework was 25KB, however, this varies depending on neural network designs. MQTT works best as a live data (data in motion)communications bus. Our DFL framework collects data in the stream and shares local models with the global server in real-time. MQTT has many-to-many support, whereas CoAP is one-to-one. In client nodes, each node runs inside the docker container (one container for each) and connects with the global server/node. The client node and global server/node communicate via MQTT protocol. In each client node, there are three layers, the first is the data access layer, the second one is data transfer layer and the third one is the online ML layer. Each layer is connected with each other making it a hierarchical multi-layer structure as shown in Fig. 4. The data access layer is responsible for accessing the data stream acquisition and decoding the data streams. Data transfer layer temporarily stores the data tuples received from the data access layer in the buffer (message queue) and further sends it to the online ML layer for real-time processing. In the online ML layer does the real-time processing part, which involves the model testing (prediction), training and model weight sharing to the global server for federated averaging. Inside the online ML layer, the model update functionality is mainly responsible for sharing the model parameters (weights) with the global server and receiving the global model from the global server; also updating each client’s local model weights with the received global model. Last but not least, to the best of our knowledge, DFL is the distributed cloud native federated learning framework that integrates both real-time data stream processing applications and online ML pipelines to explore the innovative analysis of data streams. Another speciality of DFL is that it is even suitable in the IoT environment. The source code and implementation details of our proposed DFL framework can be found on GitHub 1and the images for the global server (fed-server) side and client side (fed-clients) can be found in DockerHub. 2\\(^{,}\\) 3\\(^{,}\\) 4 5 Experimental materials and methods To evaluate the feasibility of our proposed DFL framework in data stream processing we have considered a use-case scenario where real-time emotion classification is done using multi-modal physiological data streams. In this section, we have discussed the orchestration and management of the proposed DFL in the real infrastructure, the dataset considered for the multi-modal data stream, the experimental study and the steps involved in DFL framework. Along with these, the experimental setup and the considered performance metrics are presented in the end. 5.1 Deployment in real infrastructure The proposed DFL is deployed in Eurecat’s High-Performance Computing (HPC) system, Datura. The reason for deploying the DFL in the Datura infrastructure is to test the DFL framework’s behaviour in the large distributed infrastructure. That means each component of the DFL framework can be analyzed and monitored individually regarding performance, reliability, and scalability. Datura is an Infrastructure as a Service (IaaS) platform providing cloud computing services to our internal data analytic projects with high computation requirements in Eurecat. Datura cloud consists of huge computing resources of 5.5 Tera Bytes(TB) RAM and 5 Peta Bytes(PB) storage with high-speed internal bandwidth. The platform is managed using the Red Hat OpenStack platform to provide and manage the required infrastructure support. In Fig. 5 the high-level architecture of the Datura HPC platform is presented: Fig. 5 High level architecture of Eurecat’s Datura HPC Full size image The deployment of DFL in the real infrastructure is divided into two categories: Global server and MQTT broker integration, Application integration on client side. In both the global server and client incoming results visualization is available. Global server and MQTT broker integration: The global server and MQTT broker are integrated together so that multiple clients can connect to the server for the FL. In the global server, the federated averaging (FedAvg) script runs which takes all the local model weights as input and produces a global model. The FedAvg is developed using Python 3.7 and TensorFlow 2.0. 5 For the MQTT broker, we have used an open-source and distributed IoT message broker framework called EMQ X broker. 6 The reason for choosing EMQ X as the MQTT broker is because it is the most scalable MQTT Broker for IoT and connects 100 M+ IoT devices in 1 cluster at 1ms latency (as mentioned in the official website. 7) Having said that, both the global server and EMQ X broker of DFL run inside a docker container at Datura HPC. The detailed overview of this integration is pictorially shown in Fig. 4 (see the \"Global server\" part). A snippet of the global server-side Command Line Interface (CLI) output only accessible for the DFL maintainer (not accessible from clients) is presented in Fig. 6. Fig. 6 Global server-side CLI output Full size image Application integration in client-side: This integration runs on the client side. When the end users (clients) connect this integration runs. The details of this integration are mentioned in the DFL architecture (see Fig. 4, especially the client node). For this implementation, we have made this very simple and it’s shown in CLI. In the CLI, the current data stream classifier’s performance details will be shown, along with the classifier’s real-time prediction vs the actual class of the current data tuple in the stream. A sample view of the CLI on the client side is shown in Fig. 7. The admin’s side CLI (Fig. 6) view is for code debugging to any sort of error handling. On the other hand, client-side CLI (Fig. 7) is for just visualizing the current emotional state. Because later, we will be showing this in a GUI from the CLI for better visual, just an add-on to the existing DFL framework, a cosmetic change. Fig. 7 Client’s side CLI view Full size image 5.2 Dataset description To assess the feasibility and effectiveness of our proposed DFL framework in real-time emotion classification, we have used the most popular and widely used benchmark dataset DEAP [38]. The following is a brief description of the DEAP 8 [38] (Database for Emotion Analysis using Physiological Signals) dataset. In the DEAP dataset, Electrodermal Activity (EDA) signal is available in channel no. 37 and Respiratory Belt (RB) signal is in channel no. 38. In this experiment, the EDA and RB signals are considered for the multi-modal physiological data stream (see Table 1 for a brief description). Table 1 Brief description of DEAP dataset Full size table 5.3 Experimental study The following steps are the experimental study of our proposed DFL framework for real-time emotion classification using multi-modal physiological data streams (for example, EDA+RB data streams for DEAP dataset): Data set consideration and data rearrangement: The multi-modal data stream is created using the pre-processed multi-modal DEAP data. The DEAP is stored in 3D matrix form, so a data rearrangement is conducted to transform the data to 1D matrix form for a simpler understanding of EDA and RB data. The representation is as follows: $$\\begin{aligned}{}[{\\mathrm{participant,video,data,valence\\, class, arousal\\, class}}] \\end{aligned}$$ In the experiment with the DEAP dataset, the EDA and RB multi-modal data streams are utilized to classify discrete emotion states based on valence-arousal measures. While streaming from the DEAP dataset, an automated mapping of the valence (V) and arousal (A) values to 0–1 is performed. Based on our previous experiment Fed-ReMECS [7], we followed the same V-A mapping and the next step discrete emotion conversion in the experiment. In Table 2 the discrete emotion labels (where L-Low, M-Middle and H-High) are presented: Table 2 Discrete emotion mapping using valence-arousal state Full size table Stream reading: For each participant’s data streaming in the multi-modal data (DEAP) a non-overlapping sliding window protocol is used. As the physiological data recordings in the DEAP are 60 s long [38]; therefore the sliding window size can go to a maximum of 60 s. However, for this experiment, we set it at 30 s (taken from previous literature [39, 40]). The multi-modal data stream rate for the DEAP data is approximately 9Mb/30 s. Feature extraction and fusion: The wavelet feature extraction approach is employed in this experiment to extract features from multi-modal signal streams (EDA and RB signals from the DEAP dataset). The wavelet Daubechies 4 (Db4) is the base function used for feature extraction. In our experiment, we decompose EDA and RB into three levels. A feature fusion technique (concatenation approach) combines the collected features from the EDA and RB modalities. The fused features are subsequently passed to the client-side emotion classifier. Emotion classifier: A three-layer Feed Forward Neural Network (FFNN) is used as the basis classifier to categorize the discrete emotion labels (in Table 2) in real-time from multi-modal input streams (EDA and RB). The effectiveness of FFNN best classifier for real-time emotion classification from multi-modal physiological data streams is already been established in our previous work [33]. The 9 different discrete emotion classes are in Table 2. Local model test-train: In online fashion, the FFNN model is trained using Stochastic Gradient Descent (SGD). As mentioned before, the interleaved test-then-train technique is the evaluator of the base classifier [19]. It validates the model before training and then updates the performance metrics using the received data tuple. That means the base classifier is evaluated on the newly received data tuples. The basic classifier initially performs poorly, but when it encounters more data tuples from multi-modal data streams, it develops stability and increases performance. Local model sending and global model receiving: The required time to send the local model and receive the global model varies from problem to problem and also depends on the developer of the experiment. In this experiment, we assess the transmitting and receiving time using the DEAP dataset experimental design. In the DEAP dataset, each participant views a 60-second video at a time, therefore the local model is constructed at that time. After each 60-second video is completed, the local model is transferred to the MQTT broker, and then to the global server for global model creation. Once the global model is built, the global server transmits it to the MQTT broker, who then delivers it to all of the clients involved in the federated learning. Finally, all of the clients update their local models using the received global model (see Fig. 8 for better understanding). Global model creation: The global server is in charge of constructing the global model after performing Federated Averaging (FedAvg) on all of the received local models at a certain point in time. The FedAvg formula is as follows (in Eq. (1)): $$\\begin{aligned} w_{t}^{g} = {\\frac{1}{|nT |}\\sum _{i=1}^{|nT |} w_{t-1,i}^{l}} \\end{aligned}$$ (1) where \\(w_{t}^{g}\\) is the global model created at time t, \\(|nT |\\) is the total number of the local model received at the global server, \\(w_{t-1,i}^{l}\\) is the local model received from all clients at time t. For this work, we assumed full participation from all available clients in FedAvg, but clients can join and leave in the FL process at any point in time, hence the framework is highly asynchronous. Local model update: When each client receives the global model, its local model is updated with the global model, and the next federated learning iteration begins at each client side. The following Eq: 2 is used to update the weight of each local model. $$\\begin{aligned} w_{t+1, i}^{l} \\leftarrow w_{t}^{g} - \\lambda \\bigtriangledown _{w_{t}^{g}} L(w_{t}^{g}) \\end{aligned}$$ (2) Where L is the loss function, \\(\\bigtriangledown _{w_{t}^{g}}\\) is the local model gradient of each client and \\(\\lambda \\) is the learning rate. It is worth noting that the categorical cross-entropy loss function is utilized to train the FFNN base classifier. The mathematical formula of the categorical cross-entropy loss function is in Eq. 3. $$\\begin{aligned} L = - \\sum _{i=1}^{|nC |}{y^o_c \\log (p^o_c)} \\end{aligned}$$ (3) Where \\(|nC |\\) is the number of classes (in our experiment, there are a total of 9 emotion class labels), y is the binary indicator (0 or 1) of the class label c for the observation o, and p is the projected probability that observation o belongs to class c [7]. Figures 8 and 9 show the proposed DFL architecture and the sequence diagram, respectively, while real-time emotion classification from multi-modal physiological data stream is performed on Eurecat’s Datura HPC platform. Fig. 8 The proposed DFL framework Full size image Fig. 9 The DFL framework’s sequence diagram for real-time emotion classification from multi-modal physiological signals Full size image 5.3.1 Experiment and parameter setup In the DFL deployment, we used two servers. On one, the global server and EMQ X broker running, and on the other server, the clients are created. The hardware configurations and software specifications for the DFL framework are presented in Table 3. Table 3 Hardware configurations and software specifications of DFL testbed Full size table 5.3.2 Performance metrics For the classifiers performance evaluation, accuracy (Acc) and \\(F1_{micro}\\) score are used. The metrics are calculated as follows [41]: $$\\begin{aligned}{} & {} Acc = \\frac{{\\sum \\nolimits _{i = 1}^{|nC |} {\\frac{{T{P_i} + T{N_i}}}{{T{P_i} + F{N_i} + F{P_i} + T{N_i}}}} }}{{|nC |}} \\end{aligned}$$ (4) $$\\begin{aligned}{} & {} F1_{micro} = 2*\\frac{{Pre_{micro} * Rec_{micro} }}{{Pre_{micro} + Rec_{micro}}} \\end{aligned}$$ (5) where \\(|nC |\\) is the number of classes. True positives (\\(TP_i\\)), True negatives (\\(TN_i\\)), False positives (\\(FP_i\\)) and False negatives (\\(FN_i\\)). The \\(FM_{micro}\\) is the weighted average of Precision (\\(Pre_{micro}\\)) and Recall (\\(Rec_{micro}\\)). Therefore, this score takes both false positives and false negatives into account. 5.4 Results, analysis and discussion In this section, we present the experimented results of our proposed DFL framework for real-time emotion classification from a multi-modal physiological data stream. For the multi-modal physiological data stream the popular DEAP benchmark dataset is used. The proposed DFL is tested under different numbers of clients running in parallel. We have considered 6 different client settings (5, 10, 15, 20, 25, 32); meaning the first experiment is conducted using 5 clients running in parallel, the second one is 10 clients running in parallel and so on. Running clients in parallel means, at each client side the data reading and sending for processing is done using ReMECS approach (see Sect. 3 for more details) but with a twist that instead of decision fusion we used feature fusion in the DFL framework to reduce the computation. That means when clients connect to the server it runs the ReMECS at their end. The performance of our proposed DFL framework is examined in two different ways (1) Scalability vs performance test (see Sect. 5.4.1) and (2) Memory-CPU consumption test (see Sect. 5.4.2). Apart from the different client settings comparison, we have further compared (see Sect. 5.4.3 for more details) the proposed DFL framework with the existing literature based on the following criteria: Infrastructure-based (Centralised vs Distributed) and Training mode (Batched vs Online (streaming/real-time)) works: In this comparison, we have considered state-of-the-art (SOTA) studies that utilizes the same DEAP dataset, an ML/DL-based classifier for emotion classification using multi-modal physiological data. Additionally, the effectiveness of the emotion classifiers in distributed and centralized modes are compared. Furthermore, we have compared batch mode vs. online model training methods with the same objective. 5.4.1 Scalability vs performance test In this test, we have tested the scalability vs the overall performance of our DFL framework in different numbers of client configurations. For the performance measure, the average testing accuracy and \\(F1_{micro}\\) score of the global model is reported in Table 4 and Fig. 10 shows the performance changes over the real-time emotion classification process. Table 4 Average testing accuracy and \\(F1_{micro}\\) score of the global model Full size table Fig. 10 The overall performance of the global model in terms of accuracy and \\(F1_{micro}\\) while real-time emotion classification under different numbers of clients Full size image In the real-time emotion classification from multi-modal physiological data streaming using the DFL framework, the classification is done at each client’s end. That means the generated data is strictly accessible to each client, there is no way to access the data from the global server. That is why the global model’s testing accuracy and \\(F1_{micro}\\) are calculated by taking the average of all local models’ performance after updating the model with the current global model weights. Now, from Table 4, we can see that the DFL framework is capable of classifying emotions in real-time with adequate average accuracy and \\(F1_{micro}\\). Also, it is worth noticing that the DFL framework is also capable of handling multiple clients running in parallel, hence proving scalability. Now, from Fig. 10, the global model’s testing performance in terms of accuracy (left Fig. 10a) and \\(F1_{micro}\\) (right Fig. 10b) has changed over the time. The reason is because of the diverse local models received from different clients. The diversity in local models is because different clients’ physiological responses are different resulting in different characteristics in data (EDA+RB) streams, even though they are using the same sensors. However, in some rounds, the global model’s accuracy and \\(F1_{micro}\\) have dropped because the global model performance calculation is done by taking an average of the local models’ performance. So, a large drop in one of the local model’s performance can cause a significant drop in global model performance. One interesting point to notice here is that the F1-score and the accuracy are similar because in our DFL framework, the data tuple arrives sequentially (online scenario), and every data tuple is classified into exactly one class out of 9 emotion classes. The performance metrics update sequentially based on every data tuple’s arrival. Also, we have considered micro-averaging of the F1-score for our model performance evaluation, and our classification is multi-class, hence the precision, recall, and that are all identical to accuracy. Nevertheless, from the average testing accuracy and \\(F1_{micro}\\) scores along with the overall performance of proposed our DFL framework in real-time emotion classification using multi-modal data streams without accessing the sensitive data streams, we can conclude that it has the capability of handling multiple clients in parallel and still marinating adequate performance. Also, DFL is capable of preserving the privacy issue by not accessing the data streams and developing a powerful global model for real-time emotion classification. 5.4.2 Memory and CPU usage test In this test, we have calculated the CPU and memory usage of each component in DFL framework. The calculation is done using the default docker stats functionality, which provides all these details. By this test, we can confirm the overall computation cost and power consumption that is required to run the proposed DFL. In Fig. 11, the memory and CPU consumption of the docker-containers while running different numbers of clients is presented. This Fig. 11 will help us to understand each docker-containers power consumption needed to do real-time emotion classification using multi-modal data streams on the client side. Having said that, it can also give us the idea of running this container in low-powered IoT devices. Also, in Table 5, the CPU and memory consumption of different client containers are presented for the better understanding of the Fig. 11. Fig. 11 The memory and CPU consumption of the docker-containers while different numbers of clients running in parallel Full size image As we can see from Fig. 11, the memory usage of each client running the container, in the beginning, is 200 Mebibytes (MiB) (209 Mb where 1 MiB = 1,048,576 Mb, see Table 5) equals to approximately 1.3% of the total memory available and it takes a maximum of 250 MiB (260 Mb \\(\\approx \\) 1.6%, see Table 5) memory out of the total memory available in the server (mentioned in Table 3). Also, the memory usage depends on the incoming data tuples and the frequency of the model sharing happens while the FL process. On the other side, the CPU usage by each client-side container takes a maximum of 53% approximately. The maximum CPU usage happens when the emotion classification happens and the model update happens. Another point here worth noticing in the CPU usage plots is that initially there is a spike in the CPU usage. From the plots, we can see that the usage is very high. We checked and re-run the test over and over again and it stays the same but after some troubling shooting, we found out that it was docker containers not because of the processes running inside the client containers. Nevertheless, from the overall comparison, we can see that the docker-container at the client side takes less computation to do the emotion classification, hence it is capable of integration into IoT devices (low-powered devices). Table 5 The summary of the CPU and memory consumption of different client containers in different settings Full size table On the other side, the memory and CPU consumption is also calculated using the docker stats functionality. As the global server has two major (the MQTT broker and the FedAvg component) components running, we have calculated both of the component’s power consumption separately. In Figs. 12 and 13, the memory and CPU consumption of the MQTT broker and FedAvg component are presented, respectively. In these plots (Figs. 12 and 13), on the X-axis, the time (in sec) and the Y-axis, the memory (in MiB) consumption change and also the CPU consumption change over time are presented, respectively. Here the time (in sec) means how long the DFL framework runs for the emotion classification test case. Fig. 12 The memory and CPU consumption of the EMQ X MQTT Broker in different numbers of clients setting Full size image Fig. 13 The memory and CPU consumption of the FedAvg component in different numbers of clients setting Full size image From the memory and CPU consumption Fig. 12 of EMQ X MQTT broker, we can see that the memory usage (see Fig. 12a) increases when the number of clients increases and takes part in the federated learning. In our experiment, the highest number of clients is 32 and in this case, the memory usage is about above 150 MiB. Similarly in the CPU usage plot (see Fig. 12b), we can see the maximum usage is 6% and other than that it’s below 4%. Now, in the FedAvg component at the global server, the memory consumption is around 210 MiB (shown in Fig. 13a). Memory usage increases when the number of clients increases in the FL. As the FedAvg is done by aggregating all the local models received at some point of time and to do so the FedAvg component uses a queue to hold all the incoming local models before aggregation. Similarly, the CPU usage of the FedAvg component is a maximum of 37% as shown in Fig. 13b. The CPU usage goes higher when there are local models coming for the global model creation and the rest of the time the CPU usage is below 1%. 5.4.3 Comparison with state-of-the-art works In [42], the authors have developed an 1D Convolutional Neural Network (CNN) Auto Encoder (AE) model (i.e., 1D-CNNAE) for real-time emotion classification (2-class i.e. valence and arousal) using photoplethysmogram (PPG) and galvanic skin response (GSR) signals. The proposed 1D-CNNAE model’s efficiency is evaluated using DEAP datadset. In another recent work [43], researchers have developed emotion aware healthcare systems utilizing multi-modal physiological signals (such as PPG, RB and fingertip temperature (FTT) sensors). To accomplish the multi-modal emotion classification authors have used decision-level fusion and the base emotion classifier is Random Forest (RF). Another very interesting work in [44], authors have proposed an emotion-based music recommendation framework, which gauges a user’s mood based on signals from wearable physiological sensors. To recognize the emotions authors have used decision tree (DT), RF, SVM and k-nearest neighbors (k-NN) algorithms with/out feature fusion from GSR and PPG. Authors in [45] have used an unsupervised deep belief network (DBN) for depth level feature extraction form the fused observations from EDA, PPG and Zygomaticus Electromyography (zEMG) sensors signals. After that, a feature-fusion vector is created by combining the DBN-produced features with statistical features from EDA, PPG, and zEMG. The fused feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. In order to classify these 5 basic emotion, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel is used in the end. Similarly, in [46] authors have proposed a substructure-based joint probability domain adaptation algorithm (SSJPDA) to combat the noise impact of physiological signals. By using this approach, the sample level matching’s noise susceptibility and the domain level matching’s excessive roughness is avoided. In Table 6, the comparison between our proposed DFL and the selected state-of-the-art literature is presented. Table 6 Comparison with the selected state-of-the-art works for emotion classification using multi-modal physiological signals Full size table From the comparison presented in Table 6, we can see that our proposed DFL approach has performed better in classifying more granular emotion labels among other considered works expect the work present in [45]. Still our proposed DFL is better than  [45] is because our proposed DFL real-time, distributed and the base classifier is less complex that the DBN. In addition, the DFL approach provides the advantage of data diversity by gathering data from more subjects if required and also secures the sensitive data better than the centralized approaches by training the models locally where the data is accessible to only the corresponding end user not other parties. 6 Conclusion and future work In this paper, we have discussed and analyzed the easy development and deployment of the federated learning framework using cloud-native solutions such as Docker-Containers called DFL in an HPC environment. We mainly emphasize the easy deployment of the federated learning framework using the docker by ensuring scalability, fewer hardware resources consumption, privacy-preserving, and IoT environment friendly. We have deployed our proposed DFL in a real infrastructure at Eurecat’s HPC system (Datura) using the benchmark DEAP dataset for real-time emotion classification from multi-modal physiological data streams. An extensive experimental study is conducted on efficiency, memory usage, and CPU consumption by varying numbers of clients running in parallel. The results show that the DFL can handle multiple clients running in parallel. The overall performance is good regarding average accuracy and \\(F1_{micro}\\) while classifying real-time emotions from multi-modal data streams. Having said that, the DFL ensures privacy preservation by not accessing (generated data streams are only accessible to the clients) clients’ data to develop a robust global model. In our future work, we plan to extend the development of the DFL framework in an application back end by adding GUI functionality and database storage at the clients’ end and supporting other ML/DL models. Also, we have plans to add different protocols other than MQTT and test its efficiency. With these additional functions, we plan to make it an open-source project so other researchers can use it. Availability of data and materials Publically available DEAP dataset [38]. Code availability https://github.com/officialarijit/Fed-ReMECS-Docker. Notes DFL’s source code: https://github.com/officialarijit/Fed-ReMECS-Docker. DockerHub: https://hub.docker.com/. Fed-clients: https://hub.docker.com/repository/docker/arijitnandi/fedclient. Fed-server: https://hub.docker.com/repository/docker/arijitnandi/fedserver. https://www.tensorflow.org/. https://github.com/emqx/emqx. https://www.emqx.io/. DEAP dataset link: https://www.eecs.qmul.ac.uk/mmv/datasets/deap/. References Kim J, Kim D, Lee J (2021) Design and implementation of kubernetes enabled federated learning platform. In: 2021 international conference on information and communication technology convergence (ICTC), pp. 410–412. https://doi.org/10.1109/ICTC52510.2021.9620986 Shivadekar S, Mangalagiri J, Nguyen P, Chapman D, Halem M, Gite R (2021) An intelligent parallel distributed streaming framework for near real-time science sensors and high-resolution medical images. In: 50th international conference on parallel processing workshop. ICPP Workshops ’21. Association for computing machinery, New York, NY, USA. https://doi.org/10.1145/3458744.3474039 Chen Z, Liao W, Hua K, Lu C, Yu W (2021) Towards asynchronous federated learning for heterogeneous edge-powered internet of things. Digital Commun Netw 7(3):317–326. https://doi.org/10.1016/j.dcan.2021.04.001 Article   Google Scholar   Abreha HG, Hayajneh M, Serhani MA (2022) Federated learning in edge computing: a systematic survey. Sensors. https://doi.org/10.3390/s22020450 Article   Google Scholar   Wan X, Guan X, Wang T, Bai G, Choi B-Y (2018) Application deployment using microservice and docker containers: Framework and optimization. J Netw Comput Appl 119:97–109. https://doi.org/10.1016/j.jnca.2018.07.003 Article   Google Scholar   McMahan B, Moore E, Ramage D, Hampson S, Arcas BAy (2017) Communication-efficient learning of deep networks from decentralized data. In: Singh A, Zhu J (eds.) Proceedings of the 20th international conference on artificial intelligence and statistics. Proceedings of machine learning research, vol 54, pp 1273–1282. https://proceedings.mlr.press/v54/mcmahan17a.html Nandi A, Xhafa F (2022) A federated learning method for real-time emotion state classification from multi-modal streaming. Methods 204:340–347. https://doi.org/10.1016/j.ymeth.2022.03.005 Article   Google Scholar   Novakouski M, Lewis G (2021) Operating at the edge. Carnegie Mellon University’s Software Engineering Institute Blog. Accessed 2023 Jan 24 (2021). http://insights.sei.cmu.edu/blog/operating-at-the-edge/ Pitstick K, Ratzlaff J (2022) Containerization at the Edge. Carnegie Mellon University’s Software Engineering Institute Blog. Accessed 24 Jan 2023 (2022). http://insights.sei.cmu.edu/blog/containerization-at-the-edge/ Damián Segrelles Quilis J, López-Huguet S, Lozano P, Blanquer I (2023) A federated cloud architecture for processing of cancer images on a distributed storage. Futur Gen Comput Syst 139:38–52. https://doi.org/10.1016/j.future.2022.09.019 Article   Google Scholar   Zou Z, Xie Y, Huang K, Xu G, Feng D, Long D (2022) A docker container anomaly monitoring system based on optimized isolation forest. IEEE Trans Cloud Comput 10(1):134–145. https://doi.org/10.1109/TCC.2019.2935724 Article   Google Scholar   Zhuang W, Gan X, Wen Y, Zhang S (2022) Easyfl: a low-code federated learning platform for dummies. IEEE Internet Things J 9(15):13740–13754. https://doi.org/10.1109/JIOT.2022.3143842 Article   Google Scholar   Caldas S, Duddu SMK, Wu P, Li T, Konečnỳ J, McMahan HB, Smith V, Talwalkar A (2018) Leaf: a benchmark for federated settings. arXiv preprint arXiv:1812.01097 Tensorflow Federated. https://www.tensorflow.org/federated Ryffel T, Trask A, Dahl M, Wagner B, Mancuso J, Rueckert D, Passerat-Palmbach J (2018) A generic framework for privacy preserving deep learning. arXiv preprint arXiv:1811.04017 FederatedAI: Federatedai/Fate: An Industrial Grade Federated Learning Framework. https://github.com/FederatedAI/FATE Ma Y, Yu D, Wu T, Wang H (2019) Paddlepaddle: an open-source deep learning platform from industrial practice. Front Data Comput 1(1):105–115 Google Scholar   Nokleby M, Raja H, Bajwa WU (2020) Scaling-up distributed processing of data streams for machine learning. Proc IEEE 108(11):1984–2012. https://doi.org/10.1109/JPROC.2020.3021381 Article   Google Scholar   Bifet A, Holmes G, Kirkby R, Pfahringer B (2010) Moa: Massive online analysis. J Mach Learn Res 11:1601–1604 Google Scholar   Gomes HM, Bifet A, Read J, Barddal JP, Enembreck F, Pfharinger B, Holmes G, Abdessalem T (2017) Adaptive random forests for evolving data stream classification. Mach Learn 106:1469–1495 Article   MathSciNet   Google Scholar   Kolter JZ, Maloof MA (2007) Dynamic weighted majority: an ensemble method for drifting concepts. J Mach Learn Res 8(91):2755–2790 MATH   Google Scholar   Elwell R, Polikar R (2011) Incremental learning of concept drift in nonstationary environments. IEEE Trans Neural Netw 22(10):1517–1531. https://doi.org/10.1109/TNN.2011.2160459 Article   Google Scholar   Polikar R, Upda L, Upda SS, Honavar V (2001) Learn++: an incremental learning algorithm for supervised neural networks. IEEE Trans Syst Man Cybernet Part C (Appl Rev) 31(4):497–508. https://doi.org/10.1109/5326.983933 Article   Google Scholar   Gomes HM, Read J, Bifet A (2019) Streaming random patches for evolving data stream classification. In: 2019 IEEE international conference on data mining (ICDM), pp 240–249. https://doi.org/10.1109/ICDM.2019.00034 Haddadpour F, Kamani MM, Mokhtari A, Mahdavi M (2020) Federated learning with compression: unified analysis and sharp guarantees. arXiv preprint arXiv:2007.01154 He C, Li S, So J, Zhang M, Wang H, Wang X, Vepakomma P, Singh A, Qiu H, Shen L, Zhao P, Kang Y, Liu Y, Raskar R, Yang Q, Annavaram M, Avestimehr S (2020) Fedml: a research library and benchmark for federated machine learning. arXiv:2007.13518 Abdulrahman S, Tout H, Ould-Slimane H, Mourad A, Talhi C, Guizani M (2021) A survey on federated learning: the journey from centralized to distributed on-site learning and beyond. IEEE Internet Things J 8(7):5476–5497. https://doi.org/10.1109/JIOT.2020.3030072 Article   Google Scholar   Arafeh M, Otrok H, Ould-Slimane H, Mourad A, Talhi C, Damiani E (2023) Modularfed: leveraging modularity in federated learning frameworks. Internet of Things 22:100694. https://doi.org/10.1016/j.iot.2023.100694 Article   Google Scholar   Ismail BI, Mostajeran Goortani E, Ab Karim MB, Ming Tat W, Setapa S, Luke JY, Hong Hoe O (2015) Evaluation of docker as edge computing platform. In: 2015 IEEE conference on open systems (ICOS), pp 130–135. https://doi.org/10.1109/ICOS.2015.7377291 Anderson C (2015) Docker [software engineering]. IEEE Softw 32(3):102–3. https://doi.org/10.1109/MS.2015.62 Article   Google Scholar   Ismail BI, Jagadisan D, Khalid MF (2011) Determining overhead, variance & isolation metrics in virtualization for iaas cloud. In: Lin SC, Yen E (eds) Data driven e-Science. Springer, New York, NY, pp 315–330 Chapter   Google Scholar   Felter W, Ferreira A, Rajamony R, Rubio J (2015) An updated performance comparison of virtual machines and linux containers. In: 2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pp. 171–172. https://doi.org/10.1109/ISPASS.2015.7095802 Nandi A, Xhafa F, Subirats L, Fort S (2021) Real-time multimodal emotion classification system in e-learning context. In: Proceedings of the 22nd engineering applications of neural networks conference, pp 423–435 Wan Z, Zhang Z, Yin R, Yu G (2022) Kfiml: Kubernetes-based fog computing iot platform for online machine learning. IEEE Internet Things J 9(19):19463–19476. https://doi.org/10.1109/JIOT.2022.3168085 Article   Google Scholar   Zhang Y, Jiang C, Yue B, Wan J, Guizani M (2022) Information fusion for edge intelligence: a survey. Inf Fusion 81:171–186 Article   Google Scholar   Zawad S, Yan F, Anwar A (2022) In: Ludwig, H., Baracaldo, N. (eds.) Introduction to federated learning systems, pp. 195–212. Springer, Cham. https://doi.org/10.1007/978-3-030-96896-0_9 Chahoud M, Otoum S, Mourad A (2023) On the feasibility of federated learning towards on-demand client deployment at the edge. Inf Process Manag 60(1):103150. https://doi.org/10.1016/j.ipm.2022.103150 Article   Google Scholar   Koelstra S, Muhl C, Soleymani M, Lee J-S, Yazdani A, Ebrahimi T, Pun T, Nijholt A, Patras I (2012) Deap: a database for emotion analysis;using physiological signals. IEEE Trans Affect Comput 3(1):18–31 Article   Google Scholar   Ayata D, Yaslan Y, Kamaşak M (2016) Emotion recognition via random forest and galvanic skin response: comparison of time based feature sets, window sizes and wavelet approaches. In: Medical technologies national congress, pp 1–4 Candra H, Yuwono M, Chai R, Handojoseno A, Elamvazuthi I, Nguyen HT, Su S (2015) Investigation of window size in classification of EEg-emotion signal with wavelet entropy and support vector machine. In: 37th annual international conference of the IEEE EMBS, pp 7250–7253 Nandi A, Jana ND, Das S (2020) Improving the performance of neural networks with an ensemble of activation functions. In: 2020 international joint conference on neural networks (IJCNN), pp 1–7. https://doi.org/10.1109/IJCNN48605.2020.9207277 Kang D-H, Kim D-H (2022) 1d convolutional autoencoder-based ppg and gsr signals for real-time emotion classification. IEEE Access 10:91332–91345. https://doi.org/10.1109/ACCESS.2022.3201342 Article   Google Scholar   Ayata D, Yaslan Y, Kamasak EM (2020) Emotion recognition from multimodal physiological signals for emotion aware healthcare systems. J Med Biol Eng 149–157 Ayata D, Yaslan Y, Kamasak ME (2018) Emotion based music recommendation system using wearable physiological sensors. IEEE Trans Consum Electron 64(2):196–203. https://doi.org/10.1109/TCE.2018.2844736 Article   Google Scholar   Hassan MM, Alam MGR, Uddin MZ, Huda S, Almogren A, Fortino G (2019) Human emotion recognition using deep belief network architecture. Inf Fusion 51:10–18. https://doi.org/10.1016/j.inffus.2018.10.009 Article   Google Scholar   Fu Z, Zhang B, He X, Li Y, Wang H, Huang J (2022) Emotion recognition based on multi-modal physiological signals and transfer learning. Front Neurosci. https://doi.org/10.3389/fnins.2022.1000716 Article   Google Scholar   Download references Acknowledgements Arijit Nandi is a fellow of Eurecat’s \"Vicente López\" PhD grant program. This study has been partially funded by ACCIÓ, Spain (Pla d’Actuació de Centres Tecnológics 2021) under the project TutorIA. We would like to thank the authors of DEAP dataset [38] for sharing with us. Funding Project TutorIA, ACCIÓ, Generalitat de Catalunya, Spain. Author information Authors and Affiliations Department of CS, Universitat Politècnica de Catalunya, 08034, Barcelona, Spain Arijit Nandi & Fatos Xhafa Eurecat, Centre Tecnològic de Catalunya, 08005, Barcelona, Spain Arijit Nandi & Rohit Kumar Contributions All authors contributed to designing the model and the computational framework, implementation, analysis of the results and writing of the manuscript. Corresponding authors Correspondence to Arijit Nandi or Fatos Xhafa. Ethics declarations Conflict of interest Not applicable. Ethics approval Not applicable. Consent to participate Not applicable. Consent for publication All authors have agreed on the publication. Additional information Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. Reprints and permissions About this article Cite this article Nandi, A., Xhafa, F. & Kumar, R. A Docker-based federated learning framework design and deployment for multi-modal data stream classification. Computing 105, 2195–2229 (2023). https://doi.org/10.1007/s00607-023-01179-5 Download citation Received 14 November 2022 Accepted 19 April 2023 Published 11 May 2023 Issue Date October 2023 DOI https://doi.org/10.1007/s00607-023-01179-5 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing initiative Keywords Federated learning High performance computing Multi-modal data streaming Docker-container Real-time emotion classification Mathematics Subject Classification 68W15 94A16 68M20 68T05 68T07 68P27 Use our pre-submission checklist Avoid common mistakes on your manuscript. Sections Figures References Abstract Introduction Related work Background DFL architecture Experimental materials and methods Conclusion and future work Availability of data and materials Code availability Notes References Acknowledgements Funding Author information Ethics declarations Additional information Rights and permissions About this article Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"

Paper 6:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: We set up NanOlympicsMod, a framework to benchmark software profiling RNA modifications on Nanopore dRNA-seq data. This framework was adopted for the comparative evaluation of 14 computational tools (Supplementary Table 1, see Supplementary Data available online at http://bib.oxfordjournals.org/) and was designed to facilitate the inclusion and test of additional tools. NanOlympicsMod includes a computational pipeline based on Nextflow [35] workflow manager, and was applied to four different dRNA-seq datasets, together with a set of corresponding reference sets of m6A hits generated by various short-reads sequencing based techniques (Figure 1).
  Extract 2: Our results are largely consistent with the results reported by the Zhong study. However, we would like to stress key points where our study significantly improves compared to what has been published. First, in our study we describe the development and release of NanOlympicsMod, a Nextflow pipeline exploiting containerized technology for the benchmarking of m6A detection tools on dRNA-seq. A similar resource was completely missing in the Zhong paper and will be of utmost importance in the field. Not only for reproducing the results, but also for testing further tools that are likely to be published soon in this very active research field. Second, we benchmarked 14 tools on datasets from three different species, yeast, mouse, and human. The Zhong paper was primarily focused on a mouse dataset only. Third, the production of a high sequencing depth dataset for human allowed us to perform a saturation analysis of m6A calling that was not included in the Zhong paper. Fourth, we also tested the tools on a dataset relying on synthetic oligos, which was missing in the Zhong paper. This is particularly important, since it is the only condition in which the ground truth is known, being the position of m6A known by design of the oligos.
  Limitations: >
  Relevance Evaluation: The paper is relevant to the specific point you are making in your literature review because it provides a comprehensive and critical assessment of the computational methods available for detecting N6-Methyladenosine (m6A) from nanopore direct RNA sequencing (dRNA-seq) data. This is an important topic in the field of automated systems for real-time irrigation management, as m6A is a key regulator of gene expression and various biological processes, including those involved in plant growth and development. The authors developed NanOlympicsMod, a workflow for benchmarking software profiling RNA modifications from Nanopore dRNA-seq data, and applied it to four different dRNA-seq datasets. Using this workflow, they evaluated 14 computational tools and compared their performance in terms of accuracy, sensitivity, and computational requirements. The findings of this study can help researchers select the most suitable tools for their specific research needs. The assessment in this study is especially important because it is one of the first to compare the performance of multiple m6A detection tools on dRNA-seq data. This information will be valuable to researchers who are using dRNA-seq to study m6A modifications in plants.
  Relevance Score: 1.0
  Inline Citation: >
  Explanation: The research aims to provide a comprehensive and critical assessment of the computational methods available for detecting N6-Methyladenosine (m6A) from nanopore direct RNA sequencing (dRNA-seq) data, a cutting-edge technique that has the potential to reveal the dynamics of RNA modifications in a more comprehensive manner. These modifications are crucial in regulating gene expression and various biological processes. To facilitate this assessment, the authors developed NanOlympicsMod, a workflow for benchmarking software profiling RNA modifications from Nanopore dRNA-seq data, and applied it to four different dRNA-seq datasets. Using this workflow, they evaluated 14 computational tools and compared their performance in terms of accuracy, sensitivity, and computational requirements. The findings of this study can help researchers select the most suitable tools for their specific research needs.

 Full Text: >
"Advertisement Journals Books Issues Submit Alerts About Briefings in Bioinformatics This issue                      Bioinformatics and Computational Biology Books Journals Oxford Academic                                   Advanced Search Volume 25 Issue 2 March 2024 Article Contents Abstract INTRODUCTION RESULTS DISCUSSION METHODS ACKNOWLEDGEMENTS FUNDING DATA AVAILABILITY References Author notes Supplementary data < Previous Next > JOURNAL ARTICLE Benchmarking of computational methods for m6A profiling with Nanopore direct RNA sequencing Simone Maestri, Mattia Furlan, Logan Mulroney, Lucia Coscujuela Tarrero, Camilla Ugolini, Fabio Dalla Pozza, Tommaso Leonardi, Ewan Birney, Francesco Nicassio, Mattia Pelizzola Author Notes Briefings in Bioinformatics, Volume 25, Issue 2, March 2024, bbae001, https://doi.org/10.1093/bib/bbae001 Published: 26 January 2024 Article history PDF Split View Cite Permissions Share Abstract N6-methyladenosine (m6A) is the most abundant internal eukaryotic mRNA modification, and is involved in the regulation of various biological processes. Direct Nanopore sequencing of native RNA (dRNA-seq) emerged as a leading approach for its identification. Several software were published for m6A detection and there is a strong need for independent studies benchmarking their performance on data from different species, and against various reference datasets. Moreover, a computational workflow is needed to streamline the execution of tools whose installation and execution remains complicated. We developed NanOlympicsMod, a Nextflow pipeline exploiting containerized technology for comparing 14 tools for m6A detection on dRNA-seq data. NanOlympicsMod was tested on dRNA-seq data generated from in vitro (un)modified synthetic oligos. The m6A hits returned by each tool were compared to the m6A position known by design of the oligos. In addition, NanOlympicsMod was used on dRNA-seq datasets from wild-type and m6A-depleted yeast, mouse and human, and each tool’s hits were compared to reference m6A sets generated by leading orthogonal methods. The performance of the tools markedly differed across datasets, and methods adopting different approaches showed different preferences in terms of precision and recall. Changing the stringency cut-offs allowed for tuning the precision-recall trade-off towards user preferences. Finally, we determined that precision and recall of tools are markedly influenced by sequencing depth, and that additional sequencing would likely reveal additional m6A sites. Thanks to the possibility of including novel tools, NanOlympicsMod will streamline the benchmarking of m6A detection tools on dRNA-seq data, improving future RNA modification characterization. RNA modifications, N6-methyladenosine, Nanopore, dRNA-seq, benchmarking, machine learning Issue Section: Review INTRODUCTION RNA molecules are known to be decorated by more than 160 different chemical modifications, which can be found on both the nitrogenous base and the ribose sugar [1] and have profound consequences on the fate of coding and non-coding RNA species [2]. Many studies have been conducted to investigate the prevalence, transcriptome distribution, and functional role of N6-methyladenosine (m6A), the most abundant internal modification of eukaryotic coding transcripts [3, 4]. m6A is a reversible and dynamic mark deposited by methyltransferases, mainly at the RRACH (R = A/G, H = U/A/C) consensus motif, removed by demethylases, and recognized by specific effector proteins which mediate a large set of effects [3]. Indeed, m6A has been shown to markedly impact RNA metabolism, including processing, degradation, translation and localization [5, 6]. Additionally, m6A has been shown to be involved in wide-ranging roles of gene expression regulation, both in physiological conditions, including cellular differentiation, meiosis, heat stress response, gametogenesis and neurons activity [7], and pathological conditions, such as viral infection and several types of cancer [8, 9]. The development of various methods for quantification and mapping of RNA modifications was pivotal for the surge of RNA modifications research in the last decade [10]. In particular, various approaches based on high-throughput sequencing were developed for m6A profiling that rely on immunoprecipitation of modified molecules (e.g. MeRIP-seq [11], m6A-seq [12], m6A-seq2 [13], miCLIP [14], miCLIP2 [15] and m6A-LAIC-seq [16]), or on biochemical treatments that leave characteristic footprints on the cDNA, depending on the presence of the RNA modification of interest (e.g. PA-Seq [17], MAZTER-seq [18], m6A-REF-seq [19], DART-seq [20], m6A-SEAL-seq [21], m6A-label-seq [22], GLORI [23] and m6A-SAC-seq [24]). However, specific antibodies, enzymes and chemical compounds are currently available only for a limited set of RNA modifications, they can have low specificity, they are typically semi-quantitative, they are inadequate for profiling more than one modification simultaneously [25, 26], and they lack isoform and single molecule-resolution. Recently, Oxford Nanopore Technologies (ONT) launched a platform to directly sequence native RNA molecules (dRNA-seq) [27]. The electric signal recorded by ONT sequencing platform was shown to be altered by the presence of RNA modifications [27–29]. This paved the way to the single-molecule and single-base characterization of RNA modifications and prompted the development of computational tools to profile m6A from dRNA-seq data [30, 31]. These tools can be divided into two main groups: (i) single-condition tools, which require training a machine learning algorithm capable of distinguishing between modified and unmodified nucleotides and (ii) multi-condition tools, which require sequencing an additional condition devoid of the modification(s) of interest. The latter is typically generated through knock-down, knock-out or pharmacological inhibition of methyltransferases or by in vitro transcription. Tools can be additionally divided depending on whether they rely on the ionic current signal, or errors in the base calling. Furthermore, some tools can provide the modification stoichiometry, and some can achieve base-level or single-molecule resolution depending on whether they work in the genome or transcriptome reference space [32]. More than a dozen tools were already published, and others are likely to be released in the near future. These tools are often complex to install and have high demands in terms of storage and computing power [33]. Although each software has already been compared against selected tools at the time of publication, only recently a study covered multiple tools for m6A detection on dRNA-seq data focusing on the mouse epitranscriptome [34]. Altogether, users are left with limited guidance on how to prioritize the choice of the tool, on how to set significance cut-offs, and on how to coherently test additional novel tools. To address these needs and open questions, we developed NanOlympicsMod, a computational workflow designed to maximize reproducibility and portability in the comparative assessment of dRNA-seq m6A detection tools. We used NanOlympicsMod to execute and compare 14 tools on four different dRNA-seq datasets differing in terms of synthetic/biological origin, transcriptome size and coverage depth. The results were benchmarked against reference m6A sets obtained from established orthogonal techniques. This study showed a remarkable heterogeneity in the performances of the tools, underlying the importance of a critical selection of the software and cut-off settings depending on the desired precision and recall targets. RESULTS The NanOlympicsMod benchmarking framework We set up NanOlympicsMod, a framework to benchmark software profiling RNA modifications on Nanopore dRNA-seq data. This framework was adopted for the comparative evaluation of 14 computational tools (Supplementary Table 1, see Supplementary Data available online at http://bib.oxfordjournals.org/) and was designed to facilitate the inclusion and test of additional tools. NanOlympicsMod includes a computational pipeline based on Nextflow [35] workflow manager, and was applied to four different dRNA-seq datasets, together with a set of corresponding reference sets of m6A hits generated by various short-reads sequencing based techniques (Figure 1). Figure 1 Open in new tabDownload slide The NanOlympicsMod workflow and adopted datasets. (A) Schema of NanOlympicsMod, including input data, pre-processing steps, tools execution, post-processing and comparative analyses. (B) Experimental design for the four different datasets analysed by NanOlympicsMod; the methods used to generate the reference set of m6A hits in yeast and mouse are illustrated. NanOlympicsMod relies on Nextflow, guaranteeing portability across platforms and support for different job schedulers, and adopts Docker and Singularity container technologies, removing the need to install required software dependencies and ensuring reproducibility. The pipeline includes a pre-processing module, preparing the input files required by each tool, a module for the parallel execution of the tools, a post-processing module converting the output of the tools in a common format, and a module implementing a set of analyses for the assessment of the tools results, their mutual concordance, and their agreement with independent reference sets of m6A hits (Figure 1A). We applied NanOlympicsMod to three recently released dRNA-seq datasets for the profiling of m6A on synthetic RNAs, yeast, and mouse transcriptomes, and to a dRNA-seq dataset produced in the context of this work for the profiling of m6A on human transcriptome (Figure 1B and Supplementary Table 2, see Supplementary Data available online at http://bib.oxfordjournals.org/). Each dataset includes an m6A-depleted condition to be used for the comparative tools. The datasets are fully described in the Supplementary data. Various matching reference sets of m6A hits were considered that encompass different methods for the profiling of the mark, either relying or not on m6A-specific antibodies, including m6A-seq and MAZTER-seq for yeast, miCLIP2 and GLORI for mouse and GLORI for human (Figure 1B and Supplementary Table 3, see Supplementary Data available online at http://bib.oxfordjournals.org/). Computational requirements, number and location of hits We ran all the 14 considered tools (Supplementary Table 1, see Supplementary Data available online at http://bib.oxfordjournals.org/) on the four test datasets (Figure 1). For some of the tools we were not able to complete the analysis or to obtain exploitable results in some of the datasets. Eventually, we were able to complete the analysis for 12 tools in the synthetic oligos dataset, 9 tools in the yeast dataset, 11 tools in the mouse dataset and 11 tools in the human dataset. For nine of these tools, we were able to obtain results in all four datasets. We were unable to complete the analysis in any of the four datasets with one tool, nanoDoc. Each software had different computational requirements, which were influenced by the size of the reference and/or the amount of available sequencing reads (Supplementary Figure S1, see Supplementary Data available online at http://bib.oxfordjournals.org/). The various tools differ in terms of parameters that can be tuned to set significance cut-offs or filtering thresholds, thus defining the stringency of the analysis, and each tool has its own default value for these settings (Supplementary Table 4, see Supplementary Data available online at http://bib.oxfordjournals.org/). We initially decided to run each tool with their respective default settings. In these conditions the tools returned a number of hits that varied by several orders of magnitude, ranging from less than one hundred to more than 1e5 hits. The tools ranking in terms of numerosity of the hits was relatively consistent across the yeast, mouse and human datasets (Figure 2A-D). Figure 2 Open in new tabDownload slide Key results executing the tools with default settings. (A) Number of hits detected by NanOlympicsMod for each tool in the Oligos dataset. (B) As (A) for the yeast dataset. (C) As (A) for the mouse dataset. (D) As (A) for the human dataset. (E) Distribution of m6A hits for each tool along the synthetic oligos. (F) as (E) for the yeast metagene. (G) as (E) for the mouse metagene. (H) as (E) for the human metagene. (I) Heatmap reporting the overlap of m6A hits for each pair of tools executed with default settings on the oligos dataset. The value in a cell represents, for each pair of tools, the proportion of hits in common to the number of hits of the tool on the row (see the schema on the left of the panel). (J) As in (I) for the yeast dataset. (K) As in (I) for the mouse dataset. (L) As in (I) for the human dataset. The patterning of m6A hits returned by each tool was determined along the length of the synthetic oligos, and along a meta-gene for the yeast, mouse and human datasets. For the modified oligos, all adenosines were replaced by m6A resulting in an even distribution of the modification across the entire RNA length. Given that the sequencing coverage for this dataset is rather uniform, we expected the m6A hits returned by the tools to be uniformly distributed. Indeed, most of the tools returned a relatively flat profile of m6A hits along the oligos. Only MINES, m6Anet, EpiNano-SVM and EpiNano-Error favoured specific parts of the oligos (Figure 2E). In yeast, mouse and human, the m6A hits returned by most of the tools were enriched at 3′ ends and at the stop codon, in agreement with the known location of m6A marks for these species [4]. Only MINES and m6Anet found the m6A hits for the yeast dataset enriched in the mid part of the coding region (Figure 2F-H). Overlap among the tools Once we determined the number and location of m6A hits for each tool, we compared them across the tools. Since Nanopore-based analyses are reflecting the combined influence of a k-mer of bases, typically a 5-mer, it is not trivial to assign m6A hits to specific bases, especially when k-mers contain multiple As. For this reason, we decided to bin the query space (i.e. the oligos or the transcribed portion of yeast, mouse and human genomes). This also allowed to avoid penalizing the concordance between tools whose m6A hits might be separated by a few bases and to avoid inflating the concordance between tools which report multiple adjacent hits for a single modification event. We then determined, for each pair of tools, the overlap of bins with at least one identified m6A. In the oligos dataset, the tools that identified the most m6A sites (> 1000) were in very good agreement among each other, with an average overlap of 0.93 (Figure 2I). The same was observed for the tools with fewer identified m6A sites, in the order of dozens, whose sites were largely a subset of the tools with a higher number of sites. Similar results were obtained for yeast, mouse and human (Figure 2J-L), where the m6A sites identified by the tools with fewer calls were largely confirmed by the sites of the tools with the largest number of calls. The tools that completed the analysis on both yeast and mouse datasets had similar mutual concordance. Agreement with reference sets of m6A sites Once we established that the pattern of m6A hits was plausible and assessed that the tools concordance was largely driven by the number of identified sites, we set to evaluate the tools precision and recall. To this end, the various datasets are differently informative. The distribution of m6A marks in yeast, mouse and human is dictated by the in vivo constraints, yet there is no ground truth about the location of the marks. On the contrary, the distribution of m6A marks in the synthetic oligo dataset is artificial, yet their location is known by design. We determined the precision and recall for each tool using each tool’s documented default settings. We also calculated the F1 score to capture the collective impact of both metrics. In the oligo dataset, the precision was high (>0.75) for all tools, while they returned either very high or very low recall F1 score (Figure 3A). In particular, the multi-sample tools performed better than the single-sample tools, except for EpiNano-SVM. We then generated precision-recall curves by considering various significance cut-offs in addition to the default setting, as described in the Methods (Figure 3B). Even with very different settings the performance of the tools was always good in terms of precision, while it recapitulated the preference for low or high recall obtained at default settings. Tools performed similarly also in terms of Area Under the Precision-Recall Curve (AUPRC), with a high fraction of overlapping AUPRCs 95% Confidence Intervals (Supplementary Figure S2, see Supplementary Data available online at http://bib.oxfordjournals.org/). This is due to the small number of positive bins (1543) which resulted in large CIs compared to the range covered by the AUPRCs (the median CI covered 27% of the range). Noticeably, this resulted in the overlap of four tools with the random classifier whose AUPRC was inflated due to the high fraction of positive bins. Figure 3 Open in new tabDownload slide Agreement with reference sets of m6A hits. (A) Precision, recall and F1 score for each tool executed at default conditions on the oligos dataset. According to Supplementary Table 1, GM and TM identify tools working on the genome (G) or transcriptome (T) space and require multiple conditions, respectively. GS and TS identify tools working on the genome (G) or transcriptome (T) space and requiring a single condition, respectively. (B) Precision and recall curves at different cut-off values for the tools indicated in (A) on the oligos dataset; for each tool, the default cut-off is indicated by a square; the performance of a random classifier is included. (C) As in (A) for the yeast dataset. (D) as in (A) for the mouse dataset. (E) as in (A) for the human dataset. (F) as in (B) for the yeast dataset. (G) as in (B) for the mouse dataset. (H) as in (B) for the human dataset. For the yeast, mouse and human datasets, we integrated various orthogonal methods based on short reads sequencing for the profiling of m6A as surrogate of the missing ground truth: m6A-seq, MAZTER-seq, miCLIP2 and GLORI (as outlined in Figure 1B and detailed in the Supplementary Data, see Supplementary Data available online at http://bib.oxfordjournals.org/). The performance of all tested tools in these datasets was significantly worse than the oligos dataset, especially in terms of recall. None of the tools were able to obtain both high precision and high recall using the default settings. Furthermore, the tool’s performance for precision and recall was different among the three non-synthetic datasets, especially comparing mammalian with yeast datasets (Figure 3C-E). The precision-recall curves generalized this trend (Figure 3F-H). Indeed, the yeast dataset presented large AUPRC CIs due to the limited number of positive bins (1453), while the opposite was true in mESC and HEK293T (84 060 and 18 932 positive bins respectively); this resulted in a variable fraction of overlapping tools (Supplementary Figure S2, see Supplementary Data available online at http://bib.oxfordjournals.org/). In the latter datasets, the overlaps were more frequent when considering smaller bin sets (RRACH+ and/or high-coverage bins). Nevertheless, only Tombo CIs consistently overlapped with those of Nanocompore (in yeast) and Yanocomp (in human). Noticeably, the best performing tool for each analysis usually did not overlap with others independently from the number of positive bins. The tuning of the significance cut-off allowed exploring the precision-recall space, penalizing one metric in favour of increased performance for the other. Performance at RRACH and for highly expressed sites The analysis presented above ignores three key features of m6A, which distinguish the yeast, mouse and human datasets from the oligos one: the existence of preferred sequence contexts, the existence of exclusion zones where m6A is unlikely to be deposited, and the in vivo stoichiometry of the marks. Indeed, m6A is preferentially found at RRACH sequence motifs [4], and typically has low prevalence and stoichiometry [36]. In addition, it has recently been found to be markedly excluded from extreme transcripts ends and from the regions adjacent to exon–intron junctions [37]. Therefore, we tested whether restricting the analyses to those bins containing RRACH sequence motifs, falling outside of exclusion zones, or having substantial expression would improve the performance of the considered tools. Reassessing the mouse precision-recall curves in the context of the RRACH containing bins only marginally improved the tools performance in terms of precision (compare Figure A and B with Figure 3D and G). Re-evaluating the tools only for those bins that are outside exclusion zones did not significantly contribute to increasing the metrics (Figure C and D). Rather, imposing a filter on expression markedly improved the performance, especially in terms of recall (Figure E and F). The positive impact of the filter on expression was confirmed in human (Supplementary Figure S3, see Supplementary Data available online at http://bib.oxfordjournals.org/). Figure 4 Open in new tabDownload slide Agreement with reference sets of m6A hits on RRACH+, accessible, and high-coverage bins. (A) Precision, recall and F1 score for each tool executed at default conditions on the mouse dataset on RRACH+ bins. According to Supplementary Table 1, GM and TM identify tools working on the genome (G) or transcriptome (T) space and requiring multiple conditions, respectively. GS and TS identify tools working on the genome (G) or transcriptome (T) space and requiring a single condition, respectively. (B) Precision and recall curves at different cut-off values for the tools indicated in (A) on the mouse dataset; for each tool, the default cut-off is indicated by a square; the performance of a random classifier is included. (C) as in (A) for DRACH+ bins outside of splice-site exclusion zones. (D) as in (B) for DRACH+ bins outside of splice-site exclusion zones. (E) as in (A) for bins with high coverage. (F) as in (B) for bins with high coverage. Sequence features associated to true positive, false positive and false negative hits We then asked whether there are specific sequence features where m6A marks are particularly easier or more difficult to detect. For each tool, we tested if there are sequence features that are enriched within either true positive (TP) or false positive (FP) bins. We determined for each tool the accuracy of all the 12 variants of the RRACH motif, finding up to several fold differences (Figure 5A). Interestingly, the most common variants—those that are more often associated with m6A marks in vivo—are also those with the highest accuracy for all the tools (Figure 5B). Regarding the false positive bins, we searched for enriched motifs compared to a shuffled background (Figure 5C). We recapitulated the expected AC pattern only for the tools which constitutively analyse RRACH sites. For the other software, we obtained repetitive sequences mainly enriched in Ts and As which did not match with any known m6A motif. Figure 5 Open in new tabDownload slide Sequence features associated with true positive, false positive and false negative hits. (A) m6A hits of each tool were stratified based on their association to specific RRACH motifs, and their number and accuracy on the mouse dataset was reported. (B) Distribution of accuracy stratified for common and uncommon RRACH motifs. (C) De novo motif enrichment analysis was performed on 50 nt regions centred at false positive hits for each tool on the mouse dataset, and the most significant motif was reported, together with statistical significance and consensus motif; tools marked with * are restricted to RRACH/DRACH motifs by implementation. (D) Distribution of the GC content for 50 nt regions centred at true positive (TP), false negative (FN) and false positive (FP) m6A hits. (E) as (D) for the free energy. (F) as (D) for the Shannon entropy. We also characterized TP, FP and false negative (FN) bins in terms of GC content, free energy, which are proxy for the transcripts structural complexity, and Shannon Entropy, which is indicative of sequence information content. For all the tools we observed lower GC content and higher free energy in FN bins compared to TP ones, suggesting that all methods tend to miss m6A hits in less structured sequence contexts (Figure 5D and E). Rather, FP bins exhibited lower and more heterogeneous Shannon entropy compared to the other two classes (Figure 5F). This observation is in agreement with the repetitive motifs that we observed for tools not restricted to RRACHs, suggesting that FP in Nanopore based methods tend to occur in low complexity regions. Saturation analysis Finally, we exploited the high coverage of the in house sequenced human dataset to thoroughly assess the impact of coverage depth on the tools’ performances. Indeed, while it is clear that higher coverage is beneficial for a more comprehensive identification of m6A sites, it is unclear, at the considered sequencing coverage, how closely we are reaching the saturation of m6A calling. To this end, we determined the number of m6A hits identified by each tool on 25%, 50% and 75% of the reads for the human dataset and compared it to the number of hits called on the entire dataset. We observed an increase in the number of hits with higher coverage for all the tools (Figure 6A) —with the exception of DiffErr, which showed the opposite trend. We then evaluated the impact of sequencing depth on the F1 score for each tool’s default conditions and observed marginally improved performances with higher sequencing coverage, except for EpiNano-SVM and DiffErr, which showed an opposite trend (Figure 6B and Supplementary Figure S4, see Supplementary Data available online at http://bib.oxfordjournals.org/, respectively). Finally, when exploring precision and recall values obtained varying the confidence parameter, we noticed a consistent increase in AUPRC with higher sequencing coverage, except for EpiNano-SVM (Figure 6C). Altogether, these results indicated that, while with the coverage of a PromethION flow-cell we are getting closer to saturation, additional sequencing would be likely beneficial and lead to the identification of additional m6A sites. Figure 6 Open in new tabDownload slide m6A calling saturation analysis. (A) Saturation analysis for m6A calling by various tools on the human dataset; the number of hits (y-axis) identified on subsets of the whole dataset (x-axis) is reported as a proportion of the number of hits identified on the whole dataset. (B) As in (A) where the y-axis reports the corresponding F1 score. (C) as in (A) where the y-axis reports the AUPRC. Replicates merging For all the analyses discussed so far, replicates were presented as separate samples to the methods designed to handle them; in agreement with the specifications of the developers. To address this aspect, we focused on the tools capable of replicates analysis, and we reanalyzed the yeast dataset by providing either replicates as separate samples or combining them. Most of the tools involved in this analysis were indeed affected by the merging of replicates (Supplementary Figure S5, see Supplementary Data available online at http://bib.oxfordjournals.org/). While the two configurations typically differed in terms of number of significant sites, the number of replicate samples was not predictive of the number of identified sites. However, the hits of the configuration with the smaller set of methylated sites were typically a subset of the other configuration (overlap always greater than 59%, and over 99% for four out of six tools). DISCUSSION The advent of Nanopore sequencing of native transcripts generated rich datasets whose potential is still being explored. Numerous computational methods were developed for the analysis of these data, converting ionic current features into valuable information regarding RNA sequence, splicing variants, structure and polyA tails. These data promised to be highly informative on the multitude of modifications that decorate coding and non-coding transcripts. Indeed, numerous methods were published in a few years to profile the epitranscriptional landscape from dRNA-seq data, and others are likely to be available soon. However, users are left with limited guidance on how to prioritize the choice of the tool. We benchmarked 14 tools—all those that were available in November 2022—for the detection of m6A on these data (Supplementary Table 1, see Supplementary Data available online at http://bib.oxfordjournals.org/). We applied them to the analysis of four different dRNA-seq datasets with specific strengths and limitations (Figure 1). The oligos dataset represents an artificial condition that poorly recapitulates the relative location and frequency of in vivo marks, and that lacks the confounding effect of additional modifications that might be present in proximity of m6A marks—even though in a yet unknown manner. However, as an important advantage, the ground truth in terms of m6A positioning and abundance is known in the oligos dataset. The yeast, mouse and human datasets lack a ground truth, but fully represent in vivo conditions of location, stoichiometry and context of m6A marks. While the yeast transcriptome is compact, and sequenced at high depth, the mouse transcriptome is significantly larger but sequenced at lower depth. To complement these datasets, we sequenced a human dataset taking advantage of the higher throughput PromethION platform, which could provide dRNA-seq of a transcriptome of complexity comparable to the mouse’s, but at a higher sequencing depth. The datasets processed for this work were heavy and complex, consisting of >22 M files totalling 4 TB of raw data, and likewise is the effort required for their analysis. We experienced difficulties completing the run for several tools, and for few of them we had to renounce, due to lack of evident progress in the run or the unexpected generation of empty output files. The tools returned a remarkably different number of hits. However, these had a plausible distribution, given the expected location in each specific test datasets, and pairwise overlap (Figure 2). These results suggested that the tools might have different performance in terms of precision and recall. The m6A calls returned by the tools were compared to reference sets of m6A hits (Figure 1) to define precision, recall and F1 scores (Figure 3). In the case of the oligos dataset, the reference set is known by design and includes all the As available in the artificial sequences. For yeast, mouse and human, the reference set was obtained by integrating various independent datasets obtained with recent leading approaches for m6A profiling relying on short reads high-throughput sequencing. The analysis of precision and recall indicated that the tools performed very well on the synthetic oligos dataset, while the yeast, mouse and human datasets represented a more challenging task. The precision versus recall curves showed that the default settings for some of the tools nicely identify a good trade-off between precision and recall. For other tools, these curves could be used to point to better cut-off conditions for those users aiming at maximizing both metrics. Altogether, these analyses showed that, if needed, there is broad space to steer the preference towards either one of the two metrics. The low F1 scores in the yeast, mouse, and human datasets compared to the oligos dataset can possibly be attributed to the stoichiometry of m6A marks at each position or differences in bias between the orthogonal reference m6A sets and nanopore based m6A techniques. By mixing unmodified reads and modified reads from the oligos datasets, we could simulate a more biologically equivalent m6A stoichiometry in the synthetic oligos to address its effect on nanopore m6A detection. Most of the software tools had lower F1 scores at lower m6A stoichiometry in the in silico mixed samples (Supplementary Figure S6, see Supplementary Data available online at http://bib.oxfordjournals.org/), which agrees with previous observations using a subset of the tools we tested with NanOlympicsMod (Nanocompore and xPore). This suggests that the lower expected m6A stoichiometry in the biological datasets is at least partially responsible for the observed reduced F1 scores. The tested tools can be grouped according to the genome or the transcriptome being the required reference sequence, and according to their requirement or not of a baseline sample depleted of the modification of interest (Supplementary Table 1, see Supplementary Data available online at http://bib.oxfordjournals.org/). Tools of any class performed well with the synthetic oligos dataset, especially in terms of precision, while at default settings they had different preferences in terms of recall, the multi-sample tools typically achieving higher recall values. The high precision obtained by all tools in this dataset is also a consequence of the high density of m6A nucleotides, constraining the minimum precision value to the ratio of m6A+ bins to all the bins. The multi-sample tools, such as ELIGOS, Yanocomp, DiffErr and Nanocompore performed better in the yeast dataset, probably benefiting from the higher coverage. Rather, the single-sample tools working in transcriptome space, such as m6Anet and DENA, performed better in the mouse and human datasets, likely because they were able to capture m6A features in complex transcriptomes. Interestingly, m6Anet and DENA were applied for the analysis of yeast and mouse datasets despite they were trained on data obtained for Homo sapiens and Arabidopsis thaliana species (Supplementary Table 1, see Supplementary Data available online at http://bib.oxfordjournals.org/), which might partially explain those tools’ reduced performance in the yeast dataset. Similarly, EpiNano-SVM and Nanom6A were trained on synthetic oligos which lack the complexity of real transcriptomes and potential confounding factors like endogenous RNA modifications, potentially impacting their performances when applied to yeast, mouse and human data. We tested whether restricting to specific sets of bins, such as those associated with RRACH motifs, high coverage or far from exclusion zones, could improve the performance (Figure :). We found that, for those tools that are limited by design to only test RRACH or DRACH sites, this reassessment had limited chances of significant improvement. Only by filtering for highly expressed bins significatively boosted precision and recall. Finally, we identified specific sequence features that characterized TP, FP and FN bins (Figure 5). We showed that m6A at specific RRACH variants can be detected with higher accuracy and that the tested tools differ in terms of sequence motifs that could divert them. Additionally, we revealed that all the considered Nanopore-based tools tend to miss hits in unstructured regions, while identifying m6A unsupported by orthogonal techniques in low complexity domains. These observations are indicative of sequencing platforms-specific biases. For instance, the systematic occurrence of FNs in low-entropy bins could stem from the suboptimal performance of dRNA-seq with homopolymers, or from short reads alignment issues in repetitive sequences. Furthermore, short-read based methods may exhibit biases towards less structured transcripts, where antibodies and chemical compounds could more efficiently access the substrate. Further in-depth studies are warranted, likely with both short-read and Nanopore based approaches, to better understand and assess these limitations. We then evaluated the tools’ performances in terms of number of hits, F1 score at default conditions and area under the PR curve at different coverage depth values. Results from this saturation analysis showed a marked increase in the number of hits at higher coverage depth, with a decrease in the slope of the curve between 75% and 100% of the available coverage, suggesting the curve was about to reach a plateau. Interestingly, DiffErr showed an opposite trend, suggesting that the tool may be designed to use the additional information to restrict the set of candidate hits. When considering the F1 score value at default conditions, we observed only marginally improving performances with higher sequencing coverage for most of the tools. This result may suggest that the increase in coverage depth leads to an increase in the number of hits which, in turn, results in a higher sensitivity; however, the increase in sensitivity is also accompanied by a decrease in precision, with an overall effect of saturation of the F1 score. Finally, most of the tools showed an increase in the AUPRC with higher coverage depth, consistent with an overall improvement in m6A identification with higher coverage. This result also suggests that cut-off settings should be tuned also considering the available coverage depth. Only EpiNano-SVM showed a decrease in the AUPRC value at higher coverage depth: this result may be explained by features of the dataset used for the training of the algorithm. Interestingly, we showed that tools based on differential errors identification, as ELIGOS and DiffErr, were able to identify a consistent number of hits also in both the mouse and human datasets, confirming the presence of a differential error due to the presence/lack of m6A, despite the different sequencing platform and the associated base-calling model. In general, we recommend focusing on high-coverage regions, as this allows obtaining a marked increase in the tools’ performances. We observed that m6Anet outperformed all the tools both in the mouse and in the human dataset. On the oligos and yeast datasets, m6Anet was not among the top performers, possibly due to the fact that we were using the default neural network trained on a human dataset. If it is not possible to re-train m6Anet on a dataset from a related species, we advise the users to run multi-sample tools for yeast, such as ELIGOS and Nanocompore, as they were the top performers in terms of AUC in yeast and oligos datasets. As an alternative, we advise the users to integrate the predictions from multiple tools for obtaining a more accurate set of m6A modifications. The set of tools may be chosen by ranking them by the AUC value obtained in this study. In particular, the user may want to build a meta-classifier integrating the performances of multiple tools. We picked the top 5 performing tools, according to the AUC value, in the analysis of high-coverage bins of human dataset, and tested the performance of three meta-classifiers obtained as either the intersection, the majority vote or the union of their predictions. The majority vote classifier was the most balanced among the three in terms of recall and precision, allowing to obtain the highest F1 score at default conditions (0.40) (Supplementary Figure S7, see Supplementary Data available online at http://bib.oxfordjournals.org/), outperforming the best tool (MINES, F1 score = 0.38). A recent study from Zhong et al. [34] benchmarked multiple tools for m6A detection on dRNA-seq data focusing on the mouse epitranscriptome. Our results are largely consistent with the results reported by the Zhong study. However, we would like to stress key points where our study significantly improves compared to what has been published. First, in our study we describe the development and release of NanOlympicsMod, a Nextflow pipeline exploiting containerized technology for the benchmarking of m6A detection tools on dRNA-seq. A similar resource was completely missing in the Zhong paper and will be of utmost importance in the field. Not only for reproducing the results, but also for testing further tools that are likely to be published soon in this very active research field. Second, we benchmarked 14 tools on datasets from three different species, yeast, mouse, and human. The Zhong paper was primarily focused on a mouse dataset only. Third, the production of a high sequencing depth dataset for human allowed us to perform a saturation analysis of m6A calling that was not included in the Zhong paper. Fourth, we also tested the tools on a dataset relying on synthetic oligos, which was missing in the Zhong paper. This is particularly important, since it is the only condition in which the ground truth is known, being the position of m6A known by design of the oligos. Altogether, our analyses indicate that the target sequencing depth and the adopted cut-off settings are likely the most important choices for m6A profiling on dRNA-seq data. The choice of the specific tool likely depends also on whether one wishes to map the m6A hits directly on specific transcripts or not (genome versus transcriptome-based tools), whether one wishes to have direct evidence of the modification of interest (multi-sample versus single-sample tools) and whether m6A or other marks are sought. The NanOlympicsMod framework represents a portable, reproducible, and scalable resource to run and compare Nanopore Direct RNA Sequencing-based tools for the profiling of m6A or other marks, which will facilitate these decisions and will streamline the test of additional detection tools. Moreover, we think that the produced sequencing dataset will serve as a valuable resource for set-up and validation of novel dRNA-seq based tools. METHODS Cell culture treatment with STM2457 HEK293T cells were grown using Dulbecco's Modified Eagle Medium (DMEM) supplemented with 10% Fetal Bovine Serum (FBS) and 1% penicillin–streptomycin. Cells were treated with vehicle (Ethanol 100%) or with 20 μM STM2457 and incubated for 24 hours at 37 °C. RNA extraction and mRNA purification Total RNA was extracted from 10 million cells using Qiazol (Qiagen 79306) and RNeasy Micro Kit (Qiagen 74004). mRNA purification was performed with 100 ug of Total RNA using μMACS™ mRNA Isolation Kit (Miltenyi Biotec 130–075-201) following the manufacturer’s protocol. Bulk m6A quantification Bulk m6A mRNA levels were quantified using Elisa kit (EpiQuik-Epigentek). A total of 100 ng of mRNA were loaded. Samples were incubated with m6A antibody for 1 h following manufacturer’s protocol. The detected signal was quantified colorimetrically by reading the absorbance in a microplate spectrophotometer at a wavelength of 450 nm. Nanopore direct RNA sequencing A total of 150 ng of mRNA were used as an input for Nanopore Direct RNA sequencing libraries preparation. A total of 152 ng and 120 ng of library were obtained for HEK293T control and HEK293T Storm, respectively. Both samples were loaded on PromethION flow cells, with 7562 pores for HEK293T control and 8233 pores for HEK293T treated with STM2457. Reference files and datasets preparation See Supplemental Data for a comprehensive description of the considered and produced datasets. The description on how the data were processed follows here: Synthetic oligos The sequences of synthetic oligos were downloaded from [38] and concatenated into a single fasta file. Then, the coordinates of all ‘A’ nucleotides were obtained using vMatchPattern function of Biostrings v2.66.0 R package and saved to file in bed format. Yeast dataset The yeast Nanopore dRNA-seq data were retrieved from [39]. Reference genome and transcriptome files for SK1 yeast strain, together with the set of reference peaks, were downloaded from [39]. In particular, ‘MvO’ genome assembly was downloaded from http://cbio.mskcc.org/public/SK1_MvO/, while ‘SGD_2015_JRIH00000000’ reference transcriptome was downloaded from http://sgd-archive.yeastgenome.org/sequence/strains/.SK1/SK1_SGD_2015_JRIH00000000/. Since we could not retrieve a proper gtf annotation file for SK1 strain, we first aligned the transcriptome to the genome with minimap2 v2.24.0 [40] with -x splice mode. We then converted the alignment bam file in bed12 format with bedtools bamtobed v2.30.0 [41] and finally obtained a gtf annotation file using a combination of bedToGenePred and genePredTogtf from UCSC tools v377 (https://github.com/ucscGenomeBrowser/kent). The m6A hits reference set was obtained combining the hits from MAZTER-seq [18] and m6A-seq [42] released as supplemental material in Garcia-Campos et al. and Schwartz et al., respectively, and collapsing overlapping hits with bedtools merge v2.30.0 [41]. Mouse dataset The mouse Nanopore dRNA-seq data were retrieved from [43]. Reference genome and transcriptome fasta files for mouse, together with gtf annotation file, were downloaded from https://www.gencodegenes.org/mouse/release_M23.html. The m6A reference set was obtained combining the hits from miCLIP2 [15] and GLORI [23] released as supplemental material in Körtel et al. and Liu et al. respectively, and collapsing overlapping hits with bedtools merge v2.30.0 [41]. Human dataset The human dRNA-seq data were produced as part of this work and were uploaded to SRA (BioProject ID: PRJNA995902). The reference genome for human was downloaded from https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz, while the gtf annotation file was downloaded from https://ftp.ensembl.org/pub/release-109/gtf/homo_sapiens/Homo_sapiens.GRCh38.109.gtf.gz. Sequence and annotations for chromosome chr1 were then subset from the full files with bash custom scripts. The transcriptome fasta file for human was generated from the reference fasta and the annotation gtf files with bedtools getfasta v2.30.0 [41]. The m6A reference set was obtained downloading GLORI hits release as supplemental material in Liu et al. [23] and subsetting hits from chromosome chr1. The NanOlympicsMod workflow The NanOlympicsMod workflow is composed of four steps: pre-processing, tools execution, post-processing and comparative analyses (Figure 1A). Pre-processing FAST5 files for the four datasets were re-basecalled using Guppy v6.2.1 with command ‘guppy_basecaller -i <input_path> -r -x “auto” -s <save_path> --fast5_out -c rna_r9.4.1_70bps_hac.cfg’. FAST5 files are converted from multi-reads to single-read with multi_to_single ONT API v4.0.0 (https://github.com/nanoporetech/ont_fast5_api) and the base-called sequences are extracted in FASTQ format with Poretools v0.6.0 [44]. Then, sequencing reads are aligned to the transcriptome (−x map-ont) and to the genome (−x splice) with Minimap2 v2.24.0 [40]. Alignment files are then used for signal resquiggling with both Tombo v1.5.1 [45] and Nanopolish v0.8.4 [46]. The pre-processing steps are performed in parallel for all the samples involved in the analysis, and the resulting files are then provided to each tool according to their requirements. Tools execution Fourteen tools for m6A detection on dRNA-seq data are run in parallel and the corresponding output is stored in a dedicated folder tree. As outlined in Supplementary Table 1, we ran ELIGOS v2.1.0 [43], m6Anet v1.1.0 [47], MINES [48], Tombo v1.5.1 [45], DRUMMER [49], EpiNano-SVM v1.2 [38], EpiNano-Error v1.2 [38], DENA [50], Yanocomp v0.2 [51], Nanocompore v1.0.3 [39], xPore v2.0 [52], DiffErr v0.2 [29], nanom6A [53] and nanoDoc [54]. If replicates are available, they are provided as separate samples to the tools designed to handle them (Yanocomp, xPore, Nanocompore, m6Anet, ELIGOS, DiffErr and DRUMMER); otherwise they are merged. Post-processing Post-processing is based on an R script that collects all the tools output. This is heterogeneous in terms of format and information and this workflow step converts each tool’s output into a common file format. First, for those tools which rely on transcriptome alignment, a lift-over from transcriptome-based to genome-based coordinates is also performed with the transcriptToGenome function from ensembldb v2.18.4 R package. Although this may not be a compulsory step in a standard analysis, it was required for comparing these tools to those providing hits in genome-based coordinates and to the reference sets. Finally, a BED file for each tool is produced, which contains the genomic position of each call, its modification status (modified or unmodified) - defined according to criteria suggested by the developers - and the numerical value which drove the classification, if available (i.e. False Discovery Rate, modification probability, P-value). Comparative analyses The comparative analysis consists of an R script designed to process all the BED files returned by the post-processing step and to perform the analyses described in the main text. The details of these analyses are described in the section Additional analyses. Additional analyses Binning The gene space is first binned into fixed-size windows, starting from the 3′ end of the gene. In case the gene length is not multiple of the window size, the last window at the 5′ end is discarded. We chose 5 nt as window size for the oligos dataset, and 50 nt for yeast, mouse and human datasets. Smaller windows of 40, 30, 20 and 10 nt were also tested for the human dataset, showing a small while progressive increase in the AUC with increasing window size (Supplementary Figure S8, see Supplementary Data available online at http://bib.oxfordjournals.org/). The smaller window size for the oligos dataset was required by the high density of As, since a larger window size would have resulted in having only m6A+ bins. Collecting the m6A hits of each tool A matrix of m6A hits with number of rows equal to the number of bins and number of columns equal to the number of tools plus 1 (the reference set) is created. The matrix columns are initialized to 0 or − 1, depending on whether the confidence parameter for the tool needs to be maximized (i.e. probability of modification) or minimised (i.e. P-value), respectively. For each tool, the overlaps between the hits and the genes bins are evaluated and the value of the confidence parameter is reported in the corresponding cell of the matrix. In case multiple hits occur within the same bin, the value corresponding to the least confident hit is reported. In case smaller confidence parameter values imply higher confidence for a tool (e.g. P-value), scores for that tool are multiplied by −1. This is required by PRROC v1.3.1 R package [55] that we used for plotting Precision-Recall curves, as it expects that higher values of the parameter correspond to higher confidence calls (see below). An auxiliary binary matrix of hits is also created, containing 1 or 0 depending on whether the bin should be called as m6A+ or not after filtering the hits at the default parameter threshold. This matrix is then used for evaluating the hits’ overlaps and for evaluating recall, precision and F1 score at default conditions. Overlap analyses The overlap of m6A hits for each pair of tools is stored in a matrix with the number of rows and columns equal to the number of tools. Each (i,j) cell of this matrix reports the number of m6A+ bins identified by both tools i and j, divided by the number of m6A+ bins identified by tool i (the one reported in the rows in Figure 3). The matrix was then visualised as a heatmap with the pheatmap function of pheatmap v1.0.12 R package. Comparison of hits to reference m6A set The tools default conditions were defined as described in Supplementary Table 4, see Supplementary Data available online at http://bib.oxfordjournals.org/. The binary matrix of m6A hits at default conditions was used to compare the bins identified as m6A+ for a given tool to the bins classified as m6A+ according to the reference set of each dataset. The recall was then determined as the ratio of m6A+ bins in the reference set that were identified as m6A+ also by the tool, while the precision was computed as the ratio of m6A+ bins identified by the tool that were confirmed by the reference set. The F1 score was determined as the harmonic mean of precision and recall. For assessing precision and recall at various stringency cut-offs, the scores of the matrix of m6A hits were screened according to each cut-off and compared to the hits of the corresponding reference m6A set to define true and false positive bins for each tool. Precision-Recall curves were plotted with the pr.curve function of PRROC v1.3.1 R package [55] which also provided the corresponding AUPRC. The approach described in [56] was applied to estimate AUPRCs 95% Confidence Intervals. Selection of RRACH, accessible, and high-coverage bins RRACH-containing bins were determined using the vMatchPattern function of Biostrings v2.66.0 R package. Accessible bins were identified as those bins overlapping to ‘GGACC’, ‘AGACA’, ‘TGACT’, ‘AGACT’, ‘GAACT’, ‘GGACA’ and ‘GGACT’ motifs, occurring outside of inaccessible regions, defined as 100 nt at the ends of each exon, using a combination of vMatchPattern and resize function of GenomicFeatures package. High-coverage bins were determined importing genome alignment files in R v4.2.1 using readGAlignments function of GenomicAlignments v1.32.1 R package and evaluating the read counts for each exon, using a combination of makeTxDbFromGFF, exons and findOverlaps functions of GenomicFeatures v1.48.3 package. The genomic coordinates of exons with more than 100 read counts were saved to a file in BED format and were used to identify the subset of high-coverage bins. Metagene plots Starting from BED files including genomic coordinates of m6A hits filtered at default parameter values, we produced metagene plots showing m6A hits distribution along transcriptional units using GuitarPlot function of Guitar v2.12.0 package. Sequence features associated with either true or false positive calls We first annotated genomic bins with a specific motif, in case a single hit in the reference set overlapped to the bin and to a RRACH motif. We then evaluated the accuracy (i.e. the recall) of each tool at detecting true positive bins for each motif and produced a barplot. Moreover, we performed a de novo motif enrichment analysis on the sequence of false positive bins using XSTREME program from MEME Suite v5.5.2 [57] (online implementation). The gene sequence of 50 nt bins where the tools detected m6A hits that were not confirmed by the reference set was extracted and used for a motif enrichment analysis, using shuffled input sequences as control. Only the top motif for each tool was reported, together with its statistical significance and the resulting consensus motif. We additionally obtained the sequences of false negative bins for each tool, which are bins not called by the tool but present in the reference set. For each tool and bin in the true positive, false negative, and false positive sets, we calculated the GC content using R, we determined the free energy using RNAfold v2.5.0 from the Vienna RNA package [58], and we calculated the Shannon entropy using the Entropy method from the R package DescTools (v0.99.49). Source code The source code for the NanOlympicsMod workflow, and for reproducing all the results included in this study, are available at the following GitHub repository: https://github.com/mfurla/NanOlympicsMod. Key Points Nanopore direct RNA-seq sequencing (dRNA-seq) allows the identification of various RNA modifications including N6-Methyladenosine (m6A) Numerous tools were developed to identify m6A from dRNA-seq data, however a comprehensive benchmarking across species and against established orthogonal methods is missing We developed the NanOlympicsMod workflow to facilitate comparing tools for m6A detection on dRNA-seq data, and we used it to benchmark 14 software on synthetic RNA oligos, yeast, mouse and human transcriptomes The performance of the tools varies between synthetic and real datasets and is particularly sensitive to the expression of the tested regions Tools relying on specific approaches, i.e. working on transcriptome or genome space and requiring single or multiple conditions, have different preferences in terms of precision and recall ACKNOWLEDGEMENTS We would like to acknowledge that the research activity herein was carried out using the IIT, IEO and EBI HPC infrastructure. We also would like to acknowledge Giulio Pavesi for insightful discussions, and Piero Carninci, Clelia Peano and personnel of the Human Technopole High-Throughput Sequencing Operations unit for the sequencing of the human dataset. FUNDING This work was supported by grants from the Italian Association for Cancer Research (AIRC) - project IG 2020 (ID. 24784) to M.P., and project IG 2019 (ID. 22851) to F.N.; a grant from National Center for Gene Therapy and Drugs based on RNA Technology (CN00000041) supported by European Union—NextGenerationEU PNRR MUR—M4C2 to F.N; the Giorgio Boglio fellowship from AIRC (ID.26611) to M.F; a fellowship from AIRC to C.U.; and an EMBL ETPOD fellowship to L.M. DATA AVAILABILITY Human Nanopore sequencing data generated as part of this study are available in SRA (BioProject ID: PRJNA995902). Oligos, Yeast and Mouse Nanopore sequencing data were derived from sources in the public domain as indicated in Supplementary Table 2.The source code for the NanOlympicsMod workflow, and for reproducing all the results included in this study, are available at the following GitHub repository: https://github.com/mfurla/NanOlympicsMod. Simone Maestri is an engineer expert in bioinformatics and Nanopore sequencing data. He was computational postdoc in Pelizzola’s group at the Istituto Italiano di Tecnologia. He moved at the University of Milano, working on the analysis of Nanopore sequencing data in the Lab of Stem Cell Biology and Pharmacology of Neurodegenerative Diseases. Mattia Furlan is a physicist expert in bioinformatics, transcriptional regulation, RNA modifications and Nanopore sequencing. He works as computational postdoc in Pelizzola’s group at the Istituto Italiano di Tecnologia. Logan Mulroney is expert in bioinformatics, RNA modifications and Nanopore sequencing. He has been joint postdoc with European Bioinformatic Institute and Istituto Italiano di Tecnologia. He now works as computational postoc in Nicassio’s group at the Istituto Italiano di Tecnologia. Lucia Coscujuela Tarrero is expert in cancer RNA biology, RNA modifications and genomics. He works as postdoc in Pelizzola’s group at the Istituto Italiano di Tecnologia. Camilla Ugolini is a physicist expert in bioinformatics, RNA modifications and Nanopore sequencing. He works as PhD student in Nicassio’s group at the Istituto Italiano di Tecnologia. Fabio Dalla Pozza worked as bioinformatician in Pelizzola’s group at the Istituto Italiano di Tecnologia. Tommaso Leonardi is expert in bioinformatics, RNA modifications and Nanopore sequencing. He worked as Researcher at the Istituto Italiano di Tecnologia. Ewan Birney is Deputy Director General of EMBL. He is also Director of EMBL-EBI with Dr Rolf Apweiler. He runs a research group studying between-individual differences in both Japanese rice fish (Medaka) and humans, and exploring novel algorithms in sequence methods, in particular around nanopore technology. Francesco Nicassio is Senior Researcher at the Istituto Italiano di Tecnologia, Principal Investigator of the Genomic Science research line. Scientific interests are centered on the exploitation of genomic approaches to the study of mechanisms in control of gene expression dynamics provided by non-coding RNAs (microRNAs and long noncoding RNAs) and their impact on cell behaviour and human disease, with emphasis on Cancer. Mattia Pelizzola is a computational biologist with background in biotechnology affiliated at the Istituto Italiano di Tecnologia and Associate Professor at the Dept. of Biotechnology and Biosciences, Milano-Bicocca University. He leads a research group studying how co- and post- transcriptional events shape gene expression programs. His group employs an interdisciplinary approach combining cutting edge experimental and computational methods, including the profiling of nascent and modified RNA, single-molecule Nanopore sequencing, and mathematical modelling. References 1. Boccaletto P, Bagiński B. MODOMICS: an operational guide to the use of the RNA modification pathways database. RNA Bioinformatics 2021;2284:481–505. Google Scholar WorldCat   2. Roundtree IA, Evans ME, Pan T, He C. Dynamic RNA modifications in gene expression regulation. Cell 2017;169:1187–200. Google Scholar CrossrefPubMed WorldCat   3. He PC, He C. M6a RNA methylation: from mechanisms to therapeutic potential. EMBO J 2021;40:e105977. Google Scholar CrossrefPubMed WorldCat   4. Boulias K, Greer EL. Biological roles of adenine methylation in RNA. Nat Rev Genet 2023;24:143–60. Google Scholar CrossrefPubMed WorldCat   5. Wang S, Lv W, Li T, et al.  Dynamic regulation and functions of mRNA m6A modification. Cancer Cell Int 2022;22:48. Google Scholar CrossrefPubMed WorldCat   6. Yang Y, Hsu PJ, Chen Y-S, Yang YG. Dynamic transcriptomic m6A decoration: writers, erasers, readers and functions in RNA metabolism. Cell Res 2018;28:616–24. Google Scholar CrossrefPubMed WorldCat   7. Jiang X, Liu B, Nie Z, et al.  The role of m6A modification in the biological functions and diseases. Sig Transduct Target Ther 2021;6:74. Google Scholar Crossref WorldCat   8. Dang W, Xie Y, Cao P, et al.  N6-Methyladenosine and viral infection. Front Microbiol 2019;10:417. Google Scholar CrossrefPubMed WorldCat   9. Barbieri I, Kouzarides T. Role of RNA modifications in cancer. Nat Rev Cancer 2020;20:303–22. Google Scholar CrossrefPubMed WorldCat   10. Moshitch-Moshkovitz S, Dominissini D, Rechavi G. The epitranscriptome toolbox. Cell 2022;185:764–76. Google Scholar CrossrefPubMed WorldCat   11. Meyer KD, Saletore Y, Zumbo P, et al.  Comprehensive analysis of mRNA methylation reveals enrichment in 3′ UTRs and near stop codons. Cell 2012;149:1635–46. Google Scholar CrossrefPubMed WorldCat   12. Dominissini D, Moshitch-Moshkovitz S, Schwartz S, et al.  Topology of the human and mouse m6A RNA methylomes revealed by m6A-seq. Nature 2012;485:201–6. Google Scholar CrossrefPubMed WorldCat   13. Dierks D, Garcia-Campos MA, Uzonyi A, et al.  Multiplexed profiling facilitates robust m6A quantification at site, gene and sample resolution. Nat Methods 2021;18:1060–7. Google Scholar CrossrefPubMed WorldCat   14. Linder B, Grozhik AV, Olarerin-George AO, et al.  Single-nucleotide-resolution mapping of m6A and m6Am throughout the transcriptome. Nat Methods 2015;12:767–72. Google Scholar CrossrefPubMed WorldCat   15. Körtel N, Rücklé C, Zhou Y, et al.  Deep and accurate detection of m6A RNA modifications using miCLIP2 and m6Aboost machine learning. Nucleic Acids Res 2021;49:e92. https://academic.oup.com/nar/article/49/16/e92/6307904. Google Scholar WorldCat   16. Molinie B, Wang J, Lim KS, et al.  m6A-LAIC-seq reveals the census and complexity of the m6A epitranscriptome. Nat Methods 2016;13:692–8. Google Scholar CrossrefPubMed WorldCat   17. Chen K, Lu Z, Wang X, et al.  High-resolution N6 -Methyladenosine (m6a) map using photo-crosslinking-assisted m6a sequencing. Angew Chem Int Ed 2015;54:1587–90. Google Scholar Crossref WorldCat   18. Garcia-Campos MA, Edelheit S, Toth U, et al.  Deciphering the “m6A code” via antibody-independent quantitative profiling. Cell 2019;178:731–747.e16. Google Scholar CrossrefPubMed WorldCat   19. Zhang Z, Chen L-Q, Zhao Y-L, et al.  Single-base mapping of m6a by an antibody-independent method. Sci Adv 2019;5:eaax0250. Google Scholar CrossrefPubMed WorldCat   20. Meyer KD. DART-seq: an antibody-free method for global m6A detection. Nat Methods 2019;16:1275–80. Google Scholar CrossrefPubMed WorldCat   21. Wang Y, Xiao Y, Dong S, et al.  Antibody-free enzyme-assisted chemical approach for detection of N6-methyladenosine. Nat Chem Biol 2020;16:896–903. Google Scholar CrossrefPubMed WorldCat   22. Shu X, Cao J, Cheng M, et al.  A metabolic labeling method detects m6A transcriptome-wide at single base resolution. Nat Chem Biol 2020;16:887–95. Google Scholar CrossrefPubMed WorldCat   23. Liu C, Sun H, Yi Y, et al.  Absolute quantification of single-base m6A methylation in the mammalian transcriptome using GLORI. Nat Biotechnol 2022;41:355–66. Google Scholar CrossrefPubMed WorldCat   24. Hu L, Liu S, Peng Y, et al.  m6A RNA modifications are measured at single-base resolution across the mammalian transcriptome. Nat Biotechnol 2022;40:1210–9. Google Scholar CrossrefPubMed WorldCat   25. Anreiter I, Mir Q, Simpson JT, et al.  New twists in detecting mRNA modification dynamics. Trends Biotechnol 2021;39:72–89. Google Scholar CrossrefPubMed WorldCat   26. Khoddami V, Yerra A, Mosbruger TL, et al.  Transcriptome-wide profiling of multiple RNA modifications simultaneously at single-base resolution. Proc Natl Acad Sci U S A 2019;116:6784–9. Google Scholar CrossrefPubMed WorldCat   27. Workman RE, Tang AD, Tang PS, et al.  Nanopore native RNA sequencing of a human poly(a) transcriptome. Nat Methods 2019;16:1297–305. Google Scholar CrossrefPubMed WorldCat   28. Garalde DR, Snell EA, Jachimowicz D, et al.  Highly parallel direct RNA sequencing on an array of nanopores. Nat Methods 2018;15:201–6. Google Scholar CrossrefPubMed WorldCat   29. Parker MT, Knop K, Sherwood AV, et al.  Nanopore direct RNA sequencing maps the complexity of Arabidopsis mRNA processing and m6A modification. Elife 2020;9:e49658. https://elifesciences.org/articles/49658/. Google Scholar WorldCat   30. Begik O, Mattick JS, Novoa EM. Exploring the epitranscriptome by native RNA sequencing. RNA 2022;28:1430–9. Google Scholar CrossrefPubMed WorldCat   31. Acera Mateos P, Zhou Y, Zarnack K, Eyras E. Concepts and methods for transcriptome-wide prediction of chemical messenger RNA modifications with machine learning. Brief Bioinform 2023;24:bbad163. Google Scholar CrossrefPubMed WorldCat   32. Wan YK, Hendra C, Pratanwanich PN, Göke J. Beyond sequencing: machine learning algorithms extract biology hidden in Nanopore signal data. Trends Genet 2022;38:246–57. Google Scholar CrossrefPubMed WorldCat   33. Furlan M, Delgado-Tejedor A, Mulroney L, et al.  Computational methods for RNA modification detection from nanopore direct RNA sequencing data. RNA Biol 2021;18:31–40. Google Scholar CrossrefPubMed WorldCat   34. Zhong Z-D, Xie Y-Y, Chen H-X, et al.  Systematic comparison of tools used for m6A mapping from nanopore direct RNA sequencing. Nat Commun 2023;14:1906. Google Scholar CrossrefPubMed WorldCat   35. Di Tommaso P, Chatzou M, Floden EW, et al.  Nextflow enables reproducible computational workflows. Nat Biotechnol 2017;35:316–9. Google Scholar CrossrefPubMed WorldCat   36. Tegowski M, Flamand MN, Meyer KD. scDART-seq reveals distinct m6A signatures and mRNA methylation heterogeneity in single cells. Mol Cell 2022;82:868–878.e10. Google Scholar CrossrefPubMed WorldCat   37. Uzonyi A, Dierks D, Nir R, et al.  Exclusion of m6A from splice-site proximal regions by the exon junction complex dictates m6A topologies and mRNA stability. Mol Cell 2023;83:237–251.e7. Google Scholar CrossrefPubMed WorldCat   38. Liu H, Begik O, Lucas MC, et al.  Accurate detection of m6A RNA modifications in native RNA sequences. Nat Commun 2019;10:4079. Google Scholar CrossrefPubMed WorldCat   39. Leger A, Amaral PP, Pandolfini L, et al.  RNA modifications detection by comparative Nanopore direct RNA sequencing. Nat Commun 2021;12:7198. Google Scholar CrossrefPubMed WorldCat   40. Li H. Minimap2: pairwise alignment for nucleotide sequences. Bioinformatics 2018;34:3094–100. Google Scholar CrossrefPubMed WorldCat   41. Quinlan AR, Hall IM. BEDTools: a flexible suite of utilities for comparing genomic features. Bioinformatics 2010;26:841–2. Google Scholar CrossrefPubMed WorldCat   42. Schwartz S, Agarwala SD, Mumbach MR, et al.  High-resolution mapping reveals a conserved, widespread, dynamic mRNA methylation program in yeast meiosis. Cell 2013;155:1409–21. Google Scholar CrossrefPubMed WorldCat   43. Jenjaroenpun P, Wongsurawat T, Wadley TD, et al.  Decoding the epitranscriptional landscape from native RNA sequences. Nucleic Acids Res 2021;49:e7–7. Google Scholar CrossrefPubMed WorldCat   44. Loman NJ, Quinlan AR. Poretools: a toolkit for analyzing nanopore sequence data. Bioinformatics 2014;30:3399–401. Google Scholar CrossrefPubMed WorldCat   45. Stoiber M, Quick J, Egan R, et al.  De novo identification of DNA modifications enabled by genome-guided Nanopore signal processing. bioRxiv 2017;094672. https://www.biorxiv.org/content/10.1101/094672v2. 46. Loman NJ, Quick J, Simpson JT. A complete bacterial genome assembled de novo using only nanopore sequencing data. Nat Methods 2015;12:733–5. Google Scholar CrossrefPubMed WorldCat   47. Hendra C, Pratanwanich PN, Wan YK, et al.  Detection of m6A from direct RNA sequencing using a multiple instance learning framework. Nat Methods 2022;19:1590–8. Google Scholar CrossrefPubMed WorldCat   48. Lorenz DA, Sathe S, Einstein JM, Yeo GW. Direct RNA sequencing enables m6A detection in endogenous transcript isoforms at base specific resolution. RNA 2019;26:19–28. Google Scholar CrossrefPubMed WorldCat   49. Abebe JS, Price AM, Hayer KE, et al.  DRUMMER—rapid detection of RNA modifications through comparative nanopore sequencing. Bioinformatics 2022;38:3113–5. Google Scholar CrossrefPubMed WorldCat   50. Qin H, Ou L, Gao J, et al.  DENA: training an authentic neural network model using Nanopore sequencing data of Arabidopsis transcripts for detection and quantification of N6-methyladenosine on RNA. Genome Biol 2022;23:25. Google Scholar CrossrefPubMed WorldCat   51. Parker MT, Barton GJ, Gordon G. Simpson. Yanocomp: robust prediction of m6A modifications in individual nanopore direct RNA reads. bioRxiv 2021.06.15.448494. https://www.biorxiv.org/content/10.1101/2021.06.15.448494v1. 52. Pratanwanich PN, Yao F, Chen Y, et al.  Identification of differential RNA modifications from nanopore direct RNA sequencing with xPore. Nat Biotechnol 2021;39:1394–402. Google Scholar CrossrefPubMed WorldCat   53. Gao Y, Liu X, Wu B, et al.  Nanom6A - quantitative profiling of N6-methyladenosine at single-base resolution in stem-differentiating xylem of Populus trichocarpa using Nanopore direct RNA sequencing. Genome Biol 2021;22:22. Google Scholar CrossrefPubMed WorldCat   54. Ueda H. nanoDoc: RNA modification detection using Nanopore raw reads with deep one-class classification. bioRxiv 2020.09.13.295089. https://www.biorxiv.org/content/10.1101/2020.09.13.295089v2. 55. Grau J, Grosse I, Keilwagen J. PRROC: computing and visualizing precision-recall and receiver operating characteristic curves in R. Bioinformatics 2015;31:2595–7. Google Scholar CrossrefPubMed WorldCat   56. Boyd K, Eng KH, Page CD. Area under the precision-recall curve: point estimates and confidence intervals. Machine Learning and Knowledge Discovery in Databases 2013;8190:451–66. Google Scholar WorldCat   57. Bailey TL, Johnson J, Grant CE, Noble WS. The MEME suite. Nucleic Acids Res 2015;43:W39–49. Google Scholar CrossrefPubMed WorldCat   58. Gruber AR, Lorenz R, Bernhart SH, et al.  The Vienna RNA Websuite. Nucleic Acids Res 2008;36:W70–4. Google Scholar CrossrefPubMed WorldCat   Author notes Simone Maestri, Mattia Furlan and Logan Mulroney equally contributed to the work. Francesco Nicassio and Mattia Pelizzola equally contributed to the work. © The Author(s) 2024. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Supplementary data Maestri_SupplementaryData_bbae001 - docx file Advertisement CITATIONS 2 VIEWS 2,834 ALTMETRIC More metrics information Email alerts Article activity alert Advance article alerts New issue alert In progress issue alert Receive exclusive offers and updates from Oxford Academic Recommended Nanopore direct RNA sequencing detects DUX4-activated repeats and isoforms in human muscle cells Satomi Mitsuhashi et al., Human Molecular Genetics, 2021 MeRIPseqPipe: an integrated analysis pipeline for MeRIP-seq data based on Nextflow Xiaoqiong Bao et al., Bioinformatics, 2022 Resolving single-cell copy number profiling for large datasets Wang Ruohan et al., Briefings in Bioinformatics, 2022 Psychosocial-Behavioral Phenotyping: A Novel Precision Health Approach to Modeling Behavioral, Psychological, and Social Determinants of Health Using Machine Learning Marissa Burgermaster et al., Annals of Behavioral Medicine, 2022 Variation in type two taste receptor genes is associated with bitter tasting phenylthiocarbamide consumption in mature Targhee and Rambouillet rams Kimberly M Davenport et al., Translational Behavioral Medicine, 2021 Multimodal Studies in Hepatitis B Virus Associated Hepatocellular Carcinoma Runze Xie et al., Infectious Diseases & Immunity, 2022 Powered by Citing articles via Google Scholar Latest Most Read Most Cited Integrated modeling of protein and RNA ConvNeXt-MHC: improving MHC–peptide affinity prediction by structure-derived degenerate coding and the ConvNeXt model Multilevel superposition for deciphering the conformational variability of protein ensembles PCAO2: an ontology for integration of prostate cancer associated genotypic, phenotypic and lifestyle data Graphormer supervised de novo protein design method and function validation More from Oxford Academic Bioinformatics and Computational Biology Biological Sciences Science and Mathematics Books Journals About Briefings in Bioinformatics Editorial Board Author Guidelines Facebook Twitter Purchase Recommend to your Library Advertising and Corporate Services Journals Career Network Online ISSN 1477-4054 Copyright © 2024 Oxford University Press About Oxford Academic Publish journals with us University press partners What we publish New features  Authoring Open access Purchasing Institutional account management Rights and permissions Get help with access Accessibility Contact us Advertising Media enquiries Oxford University Press News Oxford Languages University of Oxford Oxford University Press is a department of the University of Oxford. It furthers the University's objective of excellence in research, scholarship, and education by publishing worldwide Copyright © 2024 Oxford University Press Cookie settings Cookie policy Privacy policy Legal notice Oxford University Press uses cookies to enhance your experience on our website. By selecting ‘accept all’ you are agreeing to our use of cookies. You can change your cookie settings at any time. More information can be found in our Cookie Policy. Cookie settings Accept all"

Paper 7:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: To optimize the robotic application process, reduce costs, and increase the quality and efficiency of the product, the digital twin concepts can be used for virtualization of the robot environment by introducing a remote operating system [64].
  Extract 2: By providing simulation and remote operation possibilities and modeling various interactions between robot and environment in digital twin concepts, accuracy, performance, and flexibility may enhance, and the final product cost may decline. Ref. [65] analyzed the human-robot interactive behaviors using a digital twin platform. Their developed digital twin helps to improve operational productivity and comfort.
  Limitations: While the study provides a detailed overview of the use of digital twin concepts in agriculture, it does not specifically address the use of these concepts in online learning in cloud-based irrigation management systems, which is the focus of the outline point.
  Relevance Evaluation: {'extract_1': 'Within the digital farming technologies, robotics, as an important technology in crop production, has played an essential role in digitalization and has been drawing more attention in recent years. To optimize the robotic application process, reduce costs, and increase the quality and efficiency of the product, the digital twin concepts can be used for virtualization of the robot environment by introducing a remote operating system [64].', 'extract_2': 'By providing simulation and remote operation possibilities and modeling various interactions between robot and environment in digital twin concepts, accuracy, performance, and flexibility may enhance, and the final product cost may decline. Ref. [65] analyzed the human-robot interactive behaviors using a digital twin platform. Their developed digital twin helps to improve operational productivity and comfort. In another study, a digital twin approach was proposed to assist the remote programming of a robot [66].', 'limitations': 'The paper focuses primarily on the use of digital twin technology in agriculture, with a specific emphasis on soil, water management, robot applications, farm machinery, and post-harvest processing. While it does briefly touch on the use of digital twins in crop production, the level of detail provided is not as comprehensive as the other sections.', 'relevance_score': 0.85}
  Relevance Score: 0.95
  Inline Citation: >
  Explanation: The provided text is an academic review on "Toward the Next Generation of Digitalization in Agriculture Based on Digital Twin Paradigm", focusing on the use of digital twin concepts in soil, water management, robot applications, farm machinery, and post-harvest processing in agricultural fields.

The authors start by defining digital twin as "a virtual or digital representation of physical systems to simulate the behavior of the physical system" and "a next-generation paradigm for digitalization in agriculture". They note that a digital twin system involves three components: (1) the physical system or real world, (2) virtual world or digital twin, and (3) a connection between the two (physical world and virtual world), which is typically IoT- or cloud-based.

The authors elaborate on the potential benefits of digital twin technology in agriculture, including more efficient use of water resources, enhanced agricultural productivity, and better decision-making capabilities, all of which support the increasing global demand for food production.

In terms of relevance to the outline point, the text provides a detailed overview of architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms.

Specific examples of digital twins in smart agriculture include optical sensors for plant canopy and disease monitoring, soil and weather sensors for crop monitoring, barn sensors for animal monitoring, and robots and agricultural machinery with GPS and Real-Time Kinematic-Global Navigation Satellite for tracking. The authors emphasize that while digital twin concepts in smart farming are still in their infancy, ongoing research and development efforts hold promise for significant advancements.

 Full Text: >


Citation: Nasirahmadi, A.; Hensel, O.
Toward the Next Generation of
Digitalization in Agriculture Based
on Digital Twin Paradigm. Sensors
2022, 22, 498. https://doi.org/
10.3390/s22020498
Academic Editors: Dionysis Bochtis
and Aristotelis C. Tagarakis
Received: 6 December 2021
Accepted: 7 January 2022
Published: 10 January 2022
Publisher’s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright:
© 2022 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
sensors
Review
Toward the Next Generation of Digitalization in Agriculture
Based on Digital Twin Paradigm
Abozar Nasirahmadi *
and Oliver Hensel
Department of Agricultural and Biosystems Engineering, University of Kassel, 37213 Witzenhausen, Germany;
agrartechnik@uni-kassel.de
* Correspondence: abozar.nasirahmadi@uni-kassel.de
Abstract: Digitalization has impacted agricultural and food production systems, and makes applica-
tion of technologies and advanced data processing techniques in agricultural ﬁeld possible. Digital
farming aims to use available information from agricultural assets to solve several existing challenges
for addressing food security, climate protection, and resource management. However, the agricultural
sector is complex, dynamic, and requires sophisticated management systems. The digital approaches
are expected to provide more optimization and further decision-making supports. Digital twin in
agriculture is a virtual representation of a farm with great potential for enhancing productivity and
efﬁciency while declining energy usage and losses. This review describes the state-of-the-art of
digital twin concepts along with different digital technologies and techniques in agricultural con-
texts. It presents a general framework of digital twins in soil, irrigation, robotics, farm machineries,
and food post-harvest processing in agricultural ﬁeld. Data recording, modeling including artiﬁcial
intelligence, big data, simulation, analysis, prediction, and communication aspects (e.g., Internet
of Things, wireless technologies) of digital twin in agriculture are discussed. Digital twin systems
can support farmers as a next generation of digitalization paradigm by continuous and real-time
monitoring of physical world (farm) and updating the state of virtual world.
Keywords: digital twin; digitalization; digital farming; farm management; smart farming
1. Introduction
One of the main global challenges is how to ensure food security for the world’s
growing population whilst ensuring long-term sustainable development. According to the
Food and Agriculture Organization, agricultural and food productions will need to grow
to feed the world population, which will reach around 10 billion by 2050 [1]. Due to the
increase in world population and market demand for higher product quantity and quality
standards, the issue of food security, sustainability, productivity, and proﬁtability becomes
more important. Furthermore, the economic pressure on the agricultural sector, labor,
environmental, and climate change issues are increasing [2,3]. Therefore, the enhancement
of efﬁciency through effective integrated smart technologies and techniques has been
widely considered in recent years.
In this context, digital agriculture (also known as smart farming or smart agriculture)
tools can support the deeper understanding of interrelations within the agricultural pro-
duction system and the consequent effects on the performance of farm production while
balancing human health and well-being, social and environmental aspects, and sustain-
ability associated with agricultural system [4–6]. Due to advances in data generation, data
processing and human-computer interactions, digital farming has progressed in recent
years [7]. One of the main features of digitalization in agriculture is the introduction of inno-
vative Information and Communication Technology (ICT), Internet of Things (IoT), big data
analytics and interpretation techniques, machine learning and Artiﬁcial Intelligence (AI).
Data acquisition and analysis in digital farming by means of smart technologies are
supporting complex decision-making approaches [8,9]. They enhance ﬁnal productivity,
Sensors 2022, 22, 498. https://doi.org/10.3390/s22020498
https://www.mdpi.com/journal/sensors
Sensors 2022, 22, 498
2 of 16
reduce costs, and optimize the decision-making process. Furthermore, ICT tools present
advantages for on-farm management, efﬁciency, quality control, and the food supply chain
as well as decision support tools [10]. The AI and big data support better and precise
farm monitoring, data acquisition and analytics, improve information extraction from
sensors as well as farm management [11]. For instance, crop health and productivity can
be monitored and controlled using advanced AI and deep learning techniques [12]. Data-
driven approaches augment on-farm decision-making capabilities, improve crop yield,
reduce losses, and therefore, beneﬁt farmers. The IoT and wireless technologies enable
real-time data transferring and monitoring in digital farming [13,14]. The IoT, along with
cloud computing systems, can facilitate communication between software platforms and
sensors, pieces of machinery, crops, and animals in digital farming. However, by increasing
the number of sensors and generating large amounts of data in digital farming could cause
high load on the cloud server and reduce the response speed [15]. In this context, in may
be impractical to always store and process data in the cloud systems [16]. An alternative
technology which has been recently introduced to the smart farming is edge-computing
that enables computation at the edge of the network [17]. It helps to reduce network load
and supports real-time data processing in agricultural ﬁelds. Furthermore, cyber-physical
systems have been introduced through smart farming systems to develop hardware and
software, improve adaptability, and safety and security of computer-based algorithms
and systems [18]. It enables adaptability, practicality, security, and safety of collected
information in agricultural ﬁeld e.g., climate, irrigation, soil, nutrition, and yield for
better management.
According to ref. [19], digital farming approaches can provide farmers with useful
information about (I) the use of fertilizers, chemicals, seeds, and irrigation management
strategies, (II) the environment protection, (III) pest, climate, and crop monitoring man-
agement solutions, (IV) market demands and business conditions. However, agricultural
production systems are complex, dynamic, and require sophisticated management [20].
Digitalization approaches are expected to provide more monitoring, data analysis and
optimization capabilities, and further decision-making supports.
To enhance the efﬁciency of these systems, an emerging paradigm has been proposed
and implemented in digital agriculture, that is, digital twin. The digital twin was ﬁrstly
presented by NASA for monitoring of spacecraft behavior and can be deﬁned as a virtual
or digital representation of physical systems to simulate the behavior of the physical
system [21,22]. There are different deﬁnitions for digital twin available in the literature
which have been reviewed by [23–25]. Based on the reported deﬁnitions, the component
of digital twin can be characterized by physical and virtual objects, as well as a set of
connections between physical and digital assets [26].
The physical system or physical world in agriculture is a complex and dynamic
environment and includes basic information and features of the object or device such
as shape, position, cooler, material, and live objects [27]. The physical system is one
of the key components, and a digital twin without a physical world is a model [28],
and system boundaries of a digital twin are identiﬁed based on the real physical world [29].
The physical system can be a single component of an object or the whole object with sub-
components located in a physical environment [28]. The physical world in agriculture can
be an animal itself or located in a farm including building, feeding strategies, number of
animals [30], or a crop with different soil, climate, and irrigation conditions [22], robots
and agricultural pieces of machinery, e.g., tractors, harvesters and fertilizers, as well as
operators. The physical world can include a whole object (e.g., whole machine) or sub-part
of the object, or a single asset of the object connected with other objects. In an agricultural
context, the physical system may be some aspects of the crop, soil, and irrigation systems,
or animal body. The physical world requires measurement technologies and sensors to
collect and receive data from the physical object. Examples of digital twins in smart
agriculture include optical sensors for plant canopy and disease [31,32], soil and weather
sensors for crop [33], barn sensors such as temperature, humidity, ammonia for animals [34],
Sensors 2022, 22, 498
3 of 16
Global Positioning System (GPS) and Real-Time Kinematic-Global Navigation Satellite for
tracking of agricultural robots [35], and food supply chain.
The connection between physical and virtual worlds depends on the developed digital
twin. This component enables data transmission between virtual and physical systems.
It interprets the collected data from the physical system and updates the state of the
virtual system, and transfers feedbacks from the virtual system to the physical world [25].
The connection components can be varied depending on the source, type and volume of
data, data transfer rate and speed, as well as the minimum delay between data acquisition
and feedbacks. Wireless and IoT techniques have been used in digital twins of agricultural
concepts to connect between physical and virtual worlds (such as [34,36,37]).
The models and data of the physical world are represented in a virtual system.
The virtual world may also include different processing and simulation concepts, software,
machine learning, data mining, and AI models. In this context, data processing and analyt-
ics by means of AI techniques to support decision-making and feedback to the physical
system were suggested by some researchers [38,39]. The virtual twin may simulate and
control the physical system, optimize a process, and predict unseen issues in the physical
system. For example, an application layer of a digital twin reported by [22] provides
real-time monitoring of weeds, crop growth, and expected yield via cloud dashboards for
farmers. A schematic of the digital twin concept in agriculture is shown in Figure 1.
Sensors 2022, 22, x FOR PEER REVIEW 
3 of 16 
 
 
animal body. The physical world requires measurement technologies and sensors to col-
lect and receive data from the physical object. Examples of digital twins in smart agricul-
ture include optical sensors for plant canopy and disease [31,32], soil and weather sensors 
for crop [33], barn sensors such as temperature, humidity, ammonia for animals [34], 
Global Positioning System (GPS) and Real-Time Kinematic-Global Navigation Satellite for 
tracking of agricultural robots [35], and food supply chain.  
The connection between physical and virtual worlds depends on the developed dig-
ital twin. This component enables data transmission between virtual and physical sys-
tems. It interprets the collected data from the physical system and updates the state of the 
virtual system, and transfers feedbacks from the virtual system to the physical world [25]. 
The connection components can be varied depending on the source, type and volume of 
data, data transfer rate and speed, as well as the minimum delay between data acquisition 
and feedbacks. Wireless and IoT techniques have been used in digital twins of agricultural 
concepts to connect between physical and virtual worlds (such as [34,36,37]).  
The models and data of the physical world are represented in a virtual system. The 
virtual world may also include different processing and simulation concepts, software, 
machine learning, data mining, and AI models. In this context, data processing and ana-
lytics by means of AI techniques to support decision-making and feedback to the physical 
system were suggested by some researchers [38,39]. The virtual twin may simulate and 
control the physical system, optimize a process, and predict unseen issues in the physical 
system. For example, an application layer of a digital twin reported by [22] provides real-
time monitoring of weeds, crop growth, and expected yield via cloud dashboards for 
farmers. A schematic of the digital twin concept in agriculture is shown in Figure 1. 
 
Figure 1. Schematic of digital twin concept for agriculture. 
Although digital twin concepts in smart farming are in their infancy and early 
demonstration stages [22,30], there are ongoing interests in implementing this technique 
in the agricultural context. There are some reviews available in the literature describing 
digital twin concepts in the agriculture context (listed in Table 1), however, to the best of 
our knowledge, these works have focused on a specific part of the digital twin, and no 
comprehensive studies have yet been done to address the application of digital twins in 
soil, irrigation, agricultural farm pieces of machinery, robots, and post-harvest food pro-
cessing. Therefore, this review summarizes digital twin concepts as a next-generation par-
adigm for digitalization in agriculture. This paper is structured in 6 sections. Section 2 
illustrates the digital twin of soil and irrigation systems in smart agriculture. Section 3 
covers the use of digital twin concepts for crop technologies. Section 4 illustrates digital 
Figure 1. Schematic of digital twin concept for agriculture.
Although digital twin concepts in smart farming are in their infancy and early demon-
stration stages [22,30], there are ongoing interests in implementing this technique in the
agricultural context. There are some reviews available in the literature describing digital
twin concepts in the agriculture context (listed in Table 1), however, to the best of our
knowledge, these works have focused on a speciﬁc part of the digital twin, and no com-
prehensive studies have yet been done to address the application of digital twins in soil,
irrigation, agricultural farm pieces of machinery, robots, and post-harvest food processing.
Therefore, this review summarizes digital twin concepts as a next-generation paradigm
for digitalization in agriculture. This paper is structured in 6 sections. Section 2 illustrates
the digital twin of soil and irrigation systems in smart agriculture. Section 3 covers the use
of digital twin concepts for crop technologies. Section 4 illustrates digital twin concepts
during post-harvest processing. Challenges and future research needs for digital twin are
presented in Section 5. Finally, conclusions are discussed in Section 6.
Sensors 2022, 22, 498
4 of 16
Table 1. Previous review studies on digital twin in agriculture.
Concept
Sources
Agriculture-farm management
[40]
Smart farming—Hydroponics
[41]
Food processing
[42]
Food losses—supply chain of fresh products
[43]
Agri-food—societal and ethical aspects
[44]
Food processing—fresh horticulture supply chain
[45]
Agri-food supply chain
[46]
Smart farming—deﬁnition and concept
[22]
Agriculture—general application and adoption
[47]
2. Digital Twin in Soil and Irrigation
Monitoring and evaluation of soil quality to sustain plant productivity is the basis of
land-use strategies in agricultural farms [48]. Crop health and productivity depends on
the quality and properties of the soil. More detailed information about the agricultural
soil may reduce the potential use of chemical fertilizer and pesticide dosages, therefore
improving the underground water, protecting the environment and human health. It also
supports deﬁning plant density in a more efﬁcient way. Digital technologies are supporting
scientists to better understand and study soil in agriculture. Soil monitoring sensors such
as moisture, temperature, organic matter, and soil pollutant sensors are playing critical
roles in digital agriculture [49]. For instance, soil moisture information can be used to
assess irrigation efﬁciency in agricultural ﬁelds [50]. Furthermore, to support the decision-
making process of smart farming, digital soil mapping is an essential paradigm that can be
deﬁned as spatial soil information based on ﬁeld and laboratory investigations coupled
with soil inference systems [51]. Digital soil assessment approaches have a direct impact on
crop yield and performance by identifying zones that may cause low crop yield. Digital
alternative methodologies for soil survey and identifying key soil characteristics could
have the possibility to quantify the trend of agricultural soil conditions [52].
The advancement of knowledge and technology (e.g., wireless sensors, IoT, AI) in
digital agriculture could lead to digital twin paradigms of soil in agriculture. The recent
development of digital soil mapping techniques may support digital twins by digital
representation of knowledge obtained from the soil in virtual entrainment [53]. For in-
stance, digital soil mapping could be used to describe soil variation in digital twins using
information from complex soil variation at a speciﬁc depth, time, and special locations [52].
Additionally, the decision about crop management depends directly on the crop water
requirements, soil properties, and availability of water. In order to manage soil and crop
requirements in smart farming, digital technologies have been used to meet the requirement
of smart or precise water management strategies. Wireless system networks, IoT, edge-
computing, local weather-based controllers, and soil sensors are some of the digital tools
based on smart irrigation systems. The mentioned tools can be used in the digital twin of
soil and irrigation systems. For example, ref. [37] developed a digital twin concept for smart
water management in the agricultural domain. Information of air and ground temperature,
and humidity sensors, soil moisture, and ambient light as well as geospatial position
sensors were collected. An IoT system was used to connect the cloud and the physical
system. A virtual environment including decision-making tools and models was designed
to inform the data collected by connection device (the IoT system) and to send feedback
to the physical system. They also presented a digital twin system architecture including
monitoring devices (i.e., soil probe, weather information, irrigation system, machines,
and other equipment) in a physical system (farm) with could serve as a connection between
the physical and virtual systems to visualize satellite and drone images.
In another study, to evaluate and forecast plants’ irrigation requirements, and support
irrigation and water distribution planning, a digital twin for a smart water management
system was developed by [54]. Data of the physical world (agriculture ﬁeld) such as
Sensors 2022, 22, 498
5 of 16
weather, fertilizer, and soil type as well as information from developed models that simulate
the behavior of soil and crops were considered as input data for the digital twin. The digital
twin concept also consisted of a Soil Agent (includes hydrological models and soil data),
Crop Agent (includes crop models and evaporation data), and a Field Avatar, which is a
digital representation of the ﬁeld such as geological models and weather data [54]. In their
developed digital twin concept, the information from Soil Avatar and Crop Avatar feed
into the Field Avatar, and an IoT system was used for data transformation and connection
between the physical and virtual worlds.
Due to increase in world population, water and energy management, storage, and proper
distribution of water become more essential for water users in agricultural sectors, which can
be managed through a collective irrigation system [55]. A digital twin of water systems
coupled with big data can reduce risk and uncertainty of water management, explore
consumption patterns, and optimize operation planning [56]. Furthermore, in a collective
irrigation system, improvement of water efﬁciency could help to reduce water losses. In this
context, a digital twin concept was created using ﬁeld and laboratory tests of a collective
irrigation system network to evaluate energy, pumping facilities, water losses and water use
efﬁciencies [57]. The developed digital twin methodology was based on information from
the physical system, i.e., infrastructure data, acquired information through telemetry, data
analytics from laboratory tests and ﬁeld measurements, IoT data transferring as connection,
energy balance, water balance, and hydraulic model in the virtual system. It was found
that the digital twin of the irrigation management system made it possible to understand
system processes, maintenance, and management strategies [57].
A digital twin of soil and irrigation systems in smart farming enables digital repre-
sentation of information from agricultural soil, and provides prediction and fundamental
understanding of water requirement and soil components for crop farming. Exchanging
information from the soil as a physical system to a virtual system using IoT, cloud, fog,
and edge-computing technologies in digital twin may allow evaluating the state of soil and
irrigation systems. In particular, the edge-computing technique that saves and performs
the data processing near the soil monitoring and irrigation devices can improve the perfor-
mance and overcome issues of cloud-based system in digital twin concepts. Furthermore,
it could offer different irrigation recommendations based on crop requirements which are
not solved yet by the researchers.
3. Digital Twin in Crop Production
The use of digital and ICT tools in crop production technologies, in particular agri-
cultural machineries, e.g., tractors, combine harvesters, fertilizers, and sprayers, plays an
important role in the improvement of overall efﬁciency by reducing the cost of fuel, fertiliz-
ers, human labor, and parameters which affect production efﬁciency and sustainability [58].
Digitalization has modernized agricultural machinery application and management policies
using collected information and advanced data analytics approaches. It allows to optimize
the performance and enhance the use of advanced tools in manufacturing. For instance,
based on the European Agricultural Machinery Association, a digital farm machine should
be able to assist and support drivers by sending and receiving data via sensors and ICT
tools, enable the best and optimal use of machinery, and the technology should facilitate
the automated operation of the devices [59]. The application of AI, big data analytics,
and wireless and IoT technologies have led to signiﬁcant changes in farm technology roles
towards the development of autonomous systems. The role of agricultural machinery in
the implementation of digital agriculture was stated by [58] as data collected from sensors
mounted on typical and autonomous agricultural machinery and transferred via an IoT
platform. Then, the information was analyzed by data analytics such as AI, fuzzy logic,
and big data analysis to support farmers, consumers, and markets [58]. In this context,
combining digital tools with autonomous machines and robots could help farmers to do
more effective practices and improve the quality of products [60]. Nowadays, with ad-
vancements in digital technology, the real-time visualization of smart farm equipment
Sensors 2022, 22, 498
6 of 16
conditions is possible through digital twin approaches [40]. It allows contact to the system
(e.g., machinery and robots), simulates the condition of the system, and monitors the
behavior and operation as well as the maintenance situation of the machines (Figure 2).
 
transferred via an IoT platform. Then, the information was analyzed by data analytics 
such as AI, fuzzy logic, and big data analysis to support farmers, consumers, and markets 
[58]. In this context, combining digital tools with autonomous machines and robots could 
help farmers to do more effective practices and improve the quality of products [60]. Now-
adays, with advancements in digital technology, the real-time visualization of smart farm 
equipment conditions is possible through digital twin approaches [40]. It allows contact 
to the system (e.g., machinery and robots), simulates the condition of the system, and 
monitors the behavior and operation as well as the maintenance situation of the machines 
(Figure 2).  
 
Figure 2. An architecture of the digital twin concept for crop production technology. 
Digital twin in design and manufacturing of products (e.g., farm machinery) requires 
(I) geometric (e.g., size, shape) and physical properties of an object, (II) in the detailed 
information of the product which can illustrate dynamic processing of the object, (III) in-
tegration of geometric, physical, and process information [61]. Digital twin approaches 
make it possible to model, design, simulate, and develop agricultural machinery that 
would yield more productive machines in terms of energy and power efficiencies. For 
instance, it was shown that overall energy consumption of machinery could be modeled 
in digital twin concepts, and the effect of different factors on energy consumption can also 
be explored there [62]. In the agricultural context, ref. [40] reported that a commercially 
available digital twin platform for agricultural machinery is able to track the machines in 
real-time, monitor the energy consumption, economic efficiency of crop management, and 
trajectories of tractors by considering the specific conditions of the farm. It has also been 
reported that using digital twins could potentially impact the training of unskilled har-
vester operators and lead to high macro-economic benefits [63].  
Within the digital farming technologies, robotics, as an important technology in crop 
production, has played an essential role in digitalization and has been drawing more at-
tention in recent years. To optimize the robotic application process, reduce costs, and in-
crease the quality and efficiency of the product, the digital twin concepts can be used for 
virtualization of the robot environment by introducing a remote operating system [64]. By 
providing simulation and remote operation possibilities and modeling various interac-
tions between robot and environment in digital twin concepts, accuracy, performance, 
and flexibility may enhance, and the final product cost may decline. Ref. [65] analyzed the 
human-robot interactive behaviors using a digital twin platform. Their developed digital 
twin helps to improve operational productivity and comfort. In another study, a digital 
Figure 2. An architecture of the digital twin concept for crop production technology.
Digital twin in design and manufacturing of products (e.g., farm machinery) re-
quires (I) geometric (e.g., size, shape) and physical properties of an object, (II) in the
detailed information of the product which can illustrate dynamic processing of the object,
(III) integration of geometric, physical, and process information [61]. Digital twin ap-
proaches make it possible to model, design, simulate, and develop agricultural machinery
that would yield more productive machines in terms of energy and power efﬁciencies.
For instance, it was shown that overall energy consumption of machinery could be mod-
eled in digital twin concepts, and the effect of different factors on energy consumption can
also be explored there [62]. In the agricultural context, ref. [40] reported that a commercially
available digital twin platform for agricultural machinery is able to track the machines
in real-time, monitor the energy consumption, economic efﬁciency of crop management,
and trajectories of tractors by considering the speciﬁc conditions of the farm. It has also
been reported that using digital twins could potentially impact the training of unskilled
harvester operators and lead to high macro-economic beneﬁts [63].
Within the digital farming technologies, robotics, as an important technology in crop
production, has played an essential role in digitalization and has been drawing more
attention in recent years. To optimize the robotic application process, reduce costs, and in-
crease the quality and efﬁciency of the product, the digital twin concepts can be used for
virtualization of the robot environment by introducing a remote operating system [64].
By providing simulation and remote operation possibilities and modeling various inter-
actions between robot and environment in digital twin concepts, accuracy, performance,
and ﬂexibility may enhance, and the ﬁnal product cost may decline. Ref. [65] analyzed
the human-robot interactive behaviors using a digital twin platform. Their developed
digital twin helps to improve operational productivity and comfort. In another study,
a digital twin approach was proposed to assist the remote programming of a robot [66].
The developed digital twin system consists of a robot (as a physical object), and a gaming
platform (as a virtual system) which was able to observe the motion of the robot, ease
programming for complex environments as well as introduce a remote operating system
for communication across different platforms [66]. In the agricultural context, an approach
was recommended by [35] that the development of a digital twin paradigm for agricultural
robots may improve predictive emulation of the vehicles, operational scheduling, digital-
Sensors 2022, 22, 498
7 of 16
ization, economic, environmental, and social sustainability in agriculture. Furthermore,
the digital twin paradigm makes it possible to overcome common challenges in the control
of robot components in the agriculture ﬁeld. In this context, a research group demonstrated
the possibility of a digital twin concept for a desktop version of an agricultural robot [67] to
control the motor and indoor localization capabilities of the robot. Besides, the digital twin
concept was used to predict movement and monitor the safety mechanism of the robot [67].
However, their developed digital twin concept needs different kinds of calibrations to be
applicable in different environmental conditions.
In another study, to simulate complexity of the crop production process, variability of
plant, soil, environment, and technologies in the agricultural ﬁeld, digital twin concepts
were developed [68]. Three ﬁeld robots for different agricultural applications were used to
develop different digital twin concepts and optimize sensor-based autonomous navigation.
It is reported that the developed concepts could provide considerable information in prepar-
ing ﬁeld experiments, and better evaluation for the use and positioning of sensor systems
towards demonstrating and implementation of the developed robotic technologies [68].
Integration of the digital twin systems with technologies and management strategies
in crop production can provide a new phenomenon for digitalization in agricultural ﬁeld.
Management strategies can be improved and optimized by providing reliable forecasts of
the key parameters in digital twins [69]. The digital twin systems can not only act as a man-
agement system, but it may also be used to revolutionize agricultural farm management
strategies [40]. For instance, a digital twin concept was applied in a greenhouse to discover,
analyze, and extract behavior of farmers [70]. Sensor data were analyzed using deep learn-
ing techniques to establish decision-making models to replicate expert famers’ experience
for transferring to young farmers. It was found that the developed digital twin module
could improve control and management strategies in crop farming [70]. In this context,
the use of distributed architecture in digital twin may increase efﬁciency and reliability
of the module by proper resource handling [71]. A distributed digital twin concept was
developed to handle resources over different stakeholders and platforms in agricultural
landscape [72]. It consists of different components, i.e., stakeholders, applications in agri-
culture and farm management, sensor data, analytics and simulation tools, virtual model,
IoT, and resource registry which makes interoperable and cross-scale management possible
in agricultural landscape [71].
In addition, the use of digital twin system as a decision support system can beneﬁt and
be adopted for crop farming applications, and optimization of products and farm system
performance. A digital twin model was implemented by [36] in sustainable agriculture
for monitoring and control of product qualities, adjustment of environmental conditions,
identiﬁcation of forecasting, and decision support scenarios. In addition, a novel approach
based on digital twin paradigms was developed to forecast yield, vegetation quality,
and duration of plant development [33]. Consequently, the quality of crop production
could be improved due to detailed analysis and control of plant growth, and the efﬁciency
of farms could be improved due to automation of decision support processes through
the developed digital twin concept. Digital twin along with forecasting models were able
to provide feedback to farmers for a better decision-making scenario in a reported study
by [73]. Their proposed digital twin system consists of a monitoring system to collect
environmental condition data from an underground farm, as well as data analysis and
modeling techniques to identify key parameters, critical trends, and forecast operational
scenarios. Furthermore, digital twin was able to optimize productivity of crops in a
greenhouse environment through climate control strategies and treatments related to
crop management [74].
Information from crop production machineries (e.g., tractors, harvesters, robots) have
been used in smart farming to optimize the performance and efﬁciency, and reduce the fuel
and energy consumption. However, the digital twin concepts collect real-time data from
the devices and characterize the states of the physical object continuously. This capability
makes it possible to predict and prescribe solutions using the collected information from
Sensors 2022, 22, 498
8 of 16
the farm machineries. Hence, big data analytics coupled with AI models are able to detect
failures in the machines before or in the early stage of when breakdowns happen. In this
context, the use of state-of-the-art edge-computing systems may reduce latency by the
limited amount of transmitted data and provide information from the crop production
machineries such as autonomous robot, harvesters, and tractors to the digital twin concepts.
The digital twin paradigm in crop farming can change production productivity, farm man-
agement, and sustainability at farm level. Advanced statistical models, machine learning
and data analytic approaches can provide farmers with more precise information to make
better decisions that were not possible previously. Based on the past (historical) and current
continuous knowledge from crop (sensors deployed at farm) and environment data, the dig-
ital twin systems provide information about future states of the farm, and offer solutions
for turning the collected information into useful and actionable on-farm knowledge.
4. Digital Twin in Post-Harvest Process
Post-harvest process is a stage of agricultural products after harvesting until consum-
ing the products, which may include transportation, drying, cooling, storage, and market-
ing. Through digital farming approaches, the post-harvest processes could beneﬁt from loss
reduction, improvement of monitoring and optimization of food processing, storage condi-
tions, marketing, and transportation. Digital solutions allow monitoring real-time agri-food
supply chain to increase robustness and resilience of the chain [75], and lower food waste
and losses. The IoT platform supports the reduction of food losses in post-harvest pro-
cessing [76], and tracking of the product through the food supply chain. To achieve food
security AI and big data analytics enable data processing, optimization, and management
in food and crop post-harvest stages [77], also reducing waste and improving overall prof-
itability [78]. The ICT offers solutions to monitor and control quality criteria of food and
agricultural products during post-harvest processing [43]. However, different environmen-
tal conditions, processing factors, and dynamic features of agricultural product (e.g., shape,
size), environmental parameters (e.g., temperature, humidity), handling, transportation,
and storage of the products inﬂuence the quality of post-harvest process [79].
To overcome these issues and increase the efﬁciency of the system, digital twin ap-
proaches have been used in post-harvest processing to continuously monitor the products
and update the processing stages [80]. Digital twins, as an expanding family of digital
farming could strengthen agri-food systems, affect knowledge and skills of farm manage-
ment [44]. Digital twin in post-harvest processes can be deﬁned as a digital representation
of harvested agricultural products based on the information collected from the products.
In this context, ref. [42] reported the digital twin concept of food processing may include:
(I) data collected from a physical system (food process operation) by means of sensors that
measure properties and variables of products and environmental parameters, (II) an IoT
platform to provide sensor communication, data storage and big data analytics, high-
performance computing, and link to the digital twin assets, (III) a simulation platform that
uses input data from physical system for optimization, testing and validation of models,
and provides decision supports in the virtual world. In order to beneﬁt food process-
ing by developing digital twin models, it is important to include accurate information
representing production processes of the product, e.g., equipment, labor, and to create
realistic models with all existing boundaries and barriers [81]. In a study reported by [82],
a digital twin of mango fruit was developed to simulate and qualify thermal and associ-
ated bio-chemical behavior of the fruit through a post-harvest supply chain. In order to
develop the digital twin concept, environmental air temperature as input was considered,
and the actual supply chain conditions were mimicked within mechanistic ﬁnite element
models [82]. Moreover, the impact of higher air speed on storage life, cold chain length,
and delivery air temperature on the fruit quality were considered in the digital twin. It was
reported that the digital twin allows to monitor and predict temperature-dependent fruit
quality losses, improve refrigeration and logistic processes, consequently, it can reduce
food losses [82]. Furthermore, it is reported that the digital twin can help horticultural
Sensors 2022, 22, 498
9 of 16
products along with the post-harvest life, and can be used to forecast the shelf-life of
agricultural products through the cold chain [45]. It can support food consumers as well
as food business owners for tracking of the products, logistics, and marketing decisions;
however, the existing digital twin concept needs to be enhanced by considering more bio-
chemical and physical features [45]. Ref. [83] proposed a digital twin concept food supply
chain analysis. Their developed digital twin includes: (I) a network based on knowledge
from, e.g., customers, suppliers, and factories, (II) some parameters, e.g., in production,
transportation, warehouses, sourcing, shipment costs, and policies, (III) various operational
parameters, e.g., demand, quality, target inventory, and vehicle capacity. It was found
that the developed digital twin can be used for optimization, simulation, and analysis of
operation and performance changes in the food supply chain [83].
According to [43], digital twin in post-harvest can be considered as mechanistic,
statistical, and intelligent models; however, it was found that the physics-based mechanistic
digital twin concepts can evaluate the quality of fresh agricultural products better than
the others. Physics-based digital twins were developed on 331 cold chain shipments of
four fruits (i.e., cucumber, eggplant, strawberry, raspberry) by [84]. Based on digital twin
concepts, it was found that the quality of fruits may be affected (around 43–85%) before
being delivered to stores.
The post-harvest processing has improved through the application of digital solutions
over the last several years. However, the use of the digital twin paradigm is receiving more
attention in post-harvest food processing due to the future product quality prediction and
cost reduction. The digital twin of post-harvest processes may be developed to model,
optimize, represent, and characterize the design and operational parameters such as quality,
safety, ingredients, shelf-life, and product status, which need to be considered by researchers
in future studies.
5. Challenges and Future Needs
Summary of the digital twin concepts developed in the literature for different purposes
in agricultural ﬁelds, including soil, irrigation, crop monitoring, robotics, farm machinery,
and post-harvest processing, is presented in Tables 2–4. These tables show that the digital
twin paradigm is in the early stage of research and development in the agricultural context,
and future studies in terms of knowledge, technological, system development, and application
aspects of digital twin concepts in different fields of agriculture should be considered.
Table 2. Summary of soil and irrigation digital twin concepts.
Concept
Key Components and Beneﬁts
Source
Soil–water
Supporting precision irrigation in agriculture, better irrigation planning
and water distribution, reduce crop yield losses
[54]
Soil–water
IoT-based water management platform, monitoring water pattern in soil
[37]
Water
Analyze and optimization of aquaponic systems, minimize water waste
[85]
Irrigation
Urban-integrated hydroponic system, integration of forecasting models for
better decision-making assistance
[73]
Irrigation
System management and irrigation decision-making integration, water use,
global energy and pumping facilities efﬁciency evaluation, understanding
of irrigation system process
[57]
Water
Development of decision support system, enhancement of cyber-physical
implementation in aquaponics
[86]
Sensors 2022, 22, 498
10 of 16
Table 3. Summary of the digital twin in crop production.
Concept
Key Components and Beneﬁts
Source
Vertical farming
Environmental conditions assessment, identiﬁcation of forecasting and decision
support models, monitoring and optimization of agri-food lifecycle
[36]
Plant/tree
Plant condition monitoring including structure, health, stress, and quality of fruit
[31]
Robot
Analysis and performance evaluation, robot selection, and navigation
[35]
Robot
Simulation of ﬁeld environment, autonomous robot navigation
[68]
Agricultural machinery
Development and advantages of business models for potato harvesting
[59]
Agricultural landscape
Resource distribution management over different stakeholders in agriculture
[72]
Crop
Forecasting yield and duration of plant development
[33]
Agricultural machinery
Development of three-dimensional geometric models, drawings of devices,
mechanisms, and the attributive data
[87]
Plant
Detection of plant diseases and nutrient efﬁciency
[32]
Crop/hydroponic farm
Identiﬁcation of crop growth parameters such as lighting, external temperature, and
ventilation systems
[73]
Crop
Optimize productivity, climate control strategies, and crop treatment management in
controlled environment agriculture
[74]
Robot
Co-simulation of robot environment, prediction of robot movement, and safety monitoring
[67]
Table 4. Summary of digital twin for post-harvest process.
Concept
Key Components and Beneﬁts
Source
Food supply chain
Thermophysical behavior of fruit during supply chain, storage at different airﬂow rate,
understanding, recording, and predicting losses of temperature-based fruit quality
[82]
Beverage
Predicting possible anomalies and preventing safety issues for employees
[88]
Food
Machine learning-based models for real-time response and quality predictions,
maintenance, and data collection
[80]
Food supply chain
Development of practical implementation strategies, enhancing resilience food retail,
and capacity management
[83]
Food
Challenges, methodologies, and opportunities for implementation of digital twin in
food processing, importance of realistic and accurate models in food processing
[81]
Food
Modeling of equipment, humans, and space for fast-food producing, management of
production chain, and performance evaluation
[89]
Post-harvest
Monitoring of retail stores and detection of fruit quality lost
[84]
With rapid technological and sensor development, digital twin of the agricultural
soil by considering the soil quality and properties may accommodate plant productivity,
health, and yield, save water, and reduce chemical usage. Many elements of the soil,
irrigation, and environmental parameters in agricultural land can be continuously mon-
itored, analyzed, and their management strategies optimized using big data analytics,
machine learning models, and decision support systems embedded in the digital twin
concepts. The combination of soil and irrigation digital twin approaches to record, monitor,
and analyze agricultural land changes may lead to improved performance of crop farming.
For instance, simulation of soil structure along with data-driven updating models could
connect farmers to the farm using the IoT technology and present, in detail, pictures of
parameters that impact the soil, irrigation, and crop yield. However, few studies focus on
the development of digital twin concepts of agriculture soil with higher degree of ﬂexibility
as well as considering a wider range of operation than existing simulation models. Soil
sensors could constantly measure and record the dynamic condition of arable soil, e.g.,
Sensors 2022, 22, 498
11 of 16
water holding capacity, moisture, temperature [53]. These data, along with information
from soil structure and simulation techniques, can be transferred to digital twin concepts,
and constant feedback from the digital world may advise real-time responses for soil and
water management as well as control systems. In recent years, there has been rapid growth
in the digital farming scenarios, use of remote sensing, digital soil mapping, and develop-
ment of software platforms. However, researches needed to fuse the developed techniques
along with the IoT, edge-computing, AI, data analytics, and simulation techniques that
could lead to development of a digital twin paradigm is in an early stage and needs to
be addressed in future studies. Furthermore, researchers need to consider the practical
challenges of digital twin-based systems in soil and irrigation as digital twins are multi-
and interdisciplinary techniques and require systems engineering perspectives [90].
Digital twin offers real-time simulation of farm machinery and robots that can beneﬁt
optimal design of the products, interaction with the environment, energy usage, and main-
tenance strategies. Digital twin concepts have the possibility to predict failures in farm
machinery and support decision-making scenarios in plant production. Farm owners can
be able to connect to the machines through virtual world for monitoring and tracking of the
devices in agricultural farms. Digital twin systems are accompanied by recording a large
amount of data and exchanging information between different assets; hence compiling
and analyzing these data is a challenge facing farms, particularly in some rural areas with
poor internet and technological infrastructures [91]. Other alternatives, e.g., Long Range
technology based on wireless sensor networks communication and edge-computing could
be used to mitigate internet access problems in rural areas for the connection part of the
digital twin concepts [32,92]. Future opportunities for the implementation of digital twin
systems in crop farm technologies could lie in the development of standards as well as data
transferring and communication strategies in this context.
The digital twin of crop production using big data collected from crop and farm ma-
chinery as well as robots, analytical and AI models, IoT, and satellite and drone information
could allow simulating crop, environmental, and farm conditions in the digital world to
determine unknown and unseen issues before happening in the physical world. Agricul-
tural objects (crops in particular) need frequent updates in data to support information
analysis and decision-making processes [93] which in turn can promote sustainable farming
practices and save energy usage in crop productions. In this context, greater effort should
be focused in the future on characterization and development of frameworks for more effec-
tive practical digital twin paradigms. In crop farming, all information may not be recorded
and tracked using digital sensors; however, combining data from different sources could
improve the virtual representation of the farm operation and environment [73]. Continuous
monitoring of crops in digital twin systems by simulating the dynamic farm conditions
and considering the effect of management, climate, and environmental conditions on the
plant growth and use of data-driven models along with sensor fusion techniques could
help to identify deviations from the normal conditions of the plant, and forecast growth
stages to reduce risk of environmental and management effects. In future, different digital
twin concepts might be applied to copy the complex physical system of crop farming in
the digital world and incorporate variable sensors, data collecting strategies, modeling,
forecasting, and simulation approaches in crop farming.
In addition, digital twin concepts can support monitoring, tracking, and analysis of
food through the entire supply chain. Development of a digital copy of an agricultural
product to monitor post-harvest processing could be used to optimize the process, reduce
energy use, labor, and food losses based on information from different sensors and simula-
tion models. Future studies need to be carried out to consider more environmental and
post-harvest product features for the development of robust digital twins [45]. Another
major challenge in the development of digital twin for post-harvest processing to minimize
quality losses and improve the shelf-life of the product is considering the value chain of
agricultural products from farm to fork [43], which has not been addressed yet. In post-
harvest processing to reduce uncertainty in digital twins and enable the consumer to trust
Sensors 2022, 22, 498
12 of 16
the output of digital twin concepts, detailed experimental and data collection approaches
along with numerical modeling and validation techniques need be considered.
6. Conclusions
Employing digital technology has helped agricultural farm managers to improve
efﬁciency, yield, and reduce losses. There are different types of digital farming paradigms
in the literature that could be used in digital twin concepts as the next generation of
digitalization in the agricultural ﬁeld. The results of this review show that the digital twin
concepts in agriculture and food processing have, so far, been little exploited in research.
There are several research challenges and opportunities in different stages of digital farming.
Digital twin paradigms can be meaningfully utilized for soil and irrigation, crop, robots and
farm machinery, and post-harvest food processing in the agricultural ﬁeld. In this context,
most of the studies have focused on the development of digital twins by considering some
limited parameters in agricultural sectors. Deploying of state-of-the-art technologies, e.g.,
AI, advanced statistical and optimization models, big data analytics, and three-dimensional
simulation, offer further possibilities for improvement in farm management. With real-
time and continuous information about agricultural assets, virtual models can predict
and address unseen issues in the ﬁelds. It may support farmers to decline the economic
pressure on the agricultural sector and labor issues, and help policy makers responsible for
food security and environmental protection, towards strengthening the agriculture sector.
In addition, it facilitates the work of researchers exploring methods to track and monitor
crop farm machinery, agricultural and post-harvest products or reduce water, chemicals,
and energy usage in digital farming. Although many digital twin systems in engineering,
manufacturing, and health contexts have been developed, further attempts need to be
considered in the agricultural context towards the development of digital twin systems
that can monitor, record, and analyze data, to predict and prescribe the best decision for
digital farming management.
Author Contributions: Conceptualization, A.N. and O.H.; investigation, A.N.; writing—original
draft preparation, A.N.; writing—review and editing, A.N. and O.H.; visualization, A.N. All authors
have read and agreed to the published version of the manuscript.
Funding: This research received no external funding.
Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Data Availability Statement: Not applicable.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
References
1.
Food and Agriculture Organization of the United Nations (FAO). Transforming Food and Agriculture to Achieve the SDGs; FAO:
Rome, Italy, 2018.
2.
Prause, L. Digital Agriculture and Labor: A Few Challenges for Social Sustainability. Sustainability 2021, 13, 5980. [CrossRef]
3.
de Gennaro, B.C.; Forleo, M.B. Sustainability perspectives in agricultural economics research and policy agenda. Agric. Food Econ.
2019, 7, 17. [CrossRef]
4.
Jakku, E.; Taylor, B.; Fleming, A.; Mason, C.; Fielke, S.; Sounness, C.; Thorburn, P. If they don’t tell us what they do with it, why
would we trust them? Trust, transparency and beneﬁt-sharing in Smart Farming. NJAS Wagening. J. Life Sci. 2019, 90–91, 100285.
[CrossRef]
5.
Basso, B.; Antle, J. Digital agriculture to design sustainable agricultural systems. Nat. Sustain. 2020, 3, 254–256. [CrossRef]
6.
Goel, R.K.; Yadav, C.S.; Vishnoi, S.; Rastogi, R. Smart agriculture–Urgent need of the day in developing countries. Sustain. Comput.
Inform. Syst. 2021, 30, 100512. [CrossRef]
7.
Mehrabi, Z.; McDowell, M.J.; Ricciardi, V.; Levers, C.; Martinez, J.D.; Mehrabi, N.; Wittman, H.; Ramankutty, N.; Jarvis, A.
The global divide in data-driven farming. Nat. Sustain. 2021, 4, 154–160. [CrossRef]
8.
Wolfert, S.; Ge, L.; Verdouw, C.; Bogaardt, M.J. Big Data in Smart Farming—A review. Agric. Syst. 2017, 153, 69–80. [CrossRef]
9.
Ingram, J.; Maye, D. What are the implications of digitalisation for agricultural knowledge? Front. Sustain. Food Syst. 2020, 4, 66.
[CrossRef]
Sensors 2022, 22, 498
13 of 16
10.
Jakku, E.; Taylor, B.; Fleming, A.; Mason, C.; Thorburn, P. Big Data, Trust and Collaboration: Exploring the Socio-Technical Enabling
Conditions for Big Data in the Grains Industry; CSIRO: Brisbane, Australia, 2016; p. 34.
11.
Smith, M.J. Getting value from artiﬁcial intelligence in agriculture. Anim. Prod. Sci. 2018, 60, 46–54. [CrossRef]
12.
Nasirahmadi, A.; Wilczek, U.; Hensel, O. Sugar Beet Damage Detection during Harvesting Using Different Convolutional Neural
Network Models. Agriculture 2021, 11, 1111. [CrossRef]
13.
Farooq, M.S.; Riaz, S.; Abid, A.; Abid, K.; Naeem, M.A. A Survey on the Role of IoT in Agriculture for the Implementation of
Smart Farming. IEEE Access 2019, 7, 156237–156271. [CrossRef]
14.
Paraforos, D.S.; Griepentrog, H.W. Digital Farming and Field Robotics: Internet of Things, Cloud Computing, and Big Data.
In Fundamentals of Agricultural and Field Robotics. Agriculture Automation and Control; Karkee, M., Zhang, Q., Eds.; Springer:
Cham, Switzerland, 2021.
15.
Zhang, X.; Cao, Z.; Dong, W. Overview of Edge Computing in the Agricultural Internet of Things: Key Technologies, Applications,
Challenges. IEEE Access 2020, 8, 141748–141761. [CrossRef]
16.
Sarker, V.K.; Queralta, J.P.; Gia, T.N.; Tenhunen, H.; Westerlund, T. A Survey on LoRa for IoT: Integrating Edge Computing.
In Proceedings of the 2019 Fourth International Conference on Fog and Mobile Edge Computing (FMEC), Rome, Italy,
10–13 June 2019; pp. 295–300.
17.
Ning, H.; Li, Y.; Shi, F.; Yang, L.T. Heterogeneous edge computing open platforms and tools for internet of things. Future Gener.
Comput. Syst. 2020, 106, 67–76. [CrossRef]
18.
An, W.; Wu, D.; Ci, S.; Luo, H.; Adamchuk, V.; Xu, Z. Agriculture Cyber-Physical Systems.
In Cyber-Physical Systems;
Academic Press: Cambridge, MA, USA, 2017; pp. 399–417.
19.
Chergui, N.; Kechadi, M.T.; McDonnell, M. The Impact of Data Analytics in Digital Agriculture: A Review. In Proceedings of
the 2020 International Multi-Conference on: Organization of Knowledge and Advanced Technologies (OCTA), Tunis, Tunisia,
6–8 February 2020; pp. 1–13.
20.
Walters, J.P.; Archer, D.W.; Sassenrath, G.F.; Hendrickson, J.R.; Hanson, J.D.; Halloran, J.M.; Vadas, P.; Alarcon, V.J. Exploring agri-
cultural production systems and their fundamental components with system dynamics modelling. Ecol. Model. 2016, 333, 51–65.
[CrossRef]
21.
Grieves, M.; Vickers, J. Digital twin: Mitigating Unpredictable, Undesirable Emergent Behavior in Complex Systems.
In Transdisciplinary Perspectives on Complex Systems; Springer: Cham, Switzerland, 2017; pp. 85–113.
22.
Verdouw, C.; Tekinerdogan, B.; Beulens, A.; Wolfert, S. Digital twins in smart farming. Agric. Syst. 2021, 189, 103046. [CrossRef]
23.
Negri, E.; Fumagalli, L.; Macchi, M. A Review of the Roles of Digital Twin in CPS-based Production Systems. Procedia Manuf.
2017, 11, 939–948. [CrossRef]
24.
Semeraro, C.; Lezoche, M.; Panetto, H.; Dassisti, M. Digital twin paradigm: A systematic literature review. Comput. Ind.
2021, 130, 103469. [CrossRef]
25.
VanDerHorn, E.; Mahadevan, S. Digital Twin: Generalization, characterization and implementation. Decis. Support Syst. 2021,
145, 113524. [CrossRef]
26.
Liu, Y.; Zhang, L.; Yang, Y.; Zhou, L.; Ren, L.; Wang, F.; Liu, R.; Pang, Z.; Deen, M.J. A Novel Cloud-Based Framework for the
Elderly Healthcare Services Using Digital Twin. IEEE Access 2019, 7, 49088–49101. [CrossRef]
27.
Juarez, M.G.; Botti, V.J.; Giret, A.S. Digital Twins: Review and Challenges. J. Comput. Inf. Sci. Eng. 2021, 21, 030802. [CrossRef]
28.
Wright, L.; Davidson, S. How to tell the difference between a model and a digital twin. Adv. Modeling Simul. Eng. Sci. 2020, 7, 13.
[CrossRef]
29.
Lu, J.; Zheng, X.; Schweiger, L.; Kiritsis, D. A Cognitive Approach to Manage the Complexity of Digital Twin Systems.
In Smart Services Summit; West, S., Meierhofer, J., Ganz, C., Eds.; Progress in IS; Springer: Cham, Switzerland, 2021.
30.
Neethirajan, S.; Kemp, B. Digital Twins in Livestock Farming. Animals 2021, 11, 1008. [CrossRef]
31.
Moghadam, P.; Lowe, T.; Edwards, E.J. Digital Twin for the Future of Orchard Production Systems. Multidiscip. Digit. Publ. Inst. Proc.
2020, 36, 92. [CrossRef]
32.
Angin, P.; Anisi, M.H.; Göksel, F.; Gürsoy, C.; Büyükgülcü, A. AgriLoRa: A Digital Twin Framework for Smart Agriculture.
J. Wirel. Mob. Netw. Ubiquitous Comput. Dependable Appl. 2020, 11, 77–96.
33.
Skobelev, P.O.; Mayorov, I.V.; Simonova, E.V.; Goryanin, O.I.; Zhilyaev, A.A.; Tabachinskiy, A.S.; Yalovenko, V.V. Development of
models and methods for creating a digital twin of plant within the cyber-physical system for precision farming management.
J. Phys. Conf. Ser. 2020, 1703, 012022. [CrossRef]
34.
Jo, S.K.; Park, D.H.; Park, H.; Kim, S.H. Smart livestock farms using digital twin: Feasibility study. In Proceedings of the 2018 Inter-
national Conference on Information and Communication Technology Convergence (ICTC), Jeju Island, Korea, 17–19 October 2018;
pp. 1461–1463.
35.
Tsolakis, N.; Bechtsis, D.; Bochtis, D. AgROSos: A Robot Operating System Based Emulation Tool for Agricultural Robotics.
Agronomy 2019, 9, 403. [CrossRef]
36.
Monteiro, J.; Barata, J.; Veloso, M.; Veloso, L.; Nunes, J. Towards sustainable digital twins for vertical farming. In Proceedings of the
2018 Thirteenth International Conference on Digital Information Management (ICDIM), Berlin, Germany, 24–26 September 2018;
pp. 234–239.
Sensors 2022, 22, 498
14 of 16
37.
Alves, R.G.; Souza, G.; Maia, R.F.; Tran, A.L.H.; Kamienski, C.; Soininen, J.P.; Aquino, P.T.; Lima, F. A digital twin for smart
farming. In Proceedings of the 2019 IEEE Global Humanitarian Technology Conference (GHTC), Santa Clara, CA, USA,
8–11 September 2022; pp. 1–4.
38.
Laamarti, F.; Badawi, H.F.; Ding, Y.; Arafsha, F.; Haﬁdh, B.; El Saddik, A. An ISO/IEEE 11073 Standardized Digital Twin
Framework for Health and Well-Being in Smart Cities. IEEE Access 2020, 8, 105950–105961. [CrossRef]
39.
Gámez Díaz, R.; Yu, Q.; Ding, Y.; Laamarti, F.; El Saddik, A. Digital Twin Coaching for Physical Activities: A Survey. Sensors
2020, 20, 5936. [CrossRef]
40.
Verdouw, C.N.; Kruize, J.W. Digital twins in farm management: Illustrations from the FIWARE accelerators SmartAgriFood
and Fractals. In Proceedings of the 7th Asian-Australasian Conference on Precision Agriculture, Hamilton, New Zealand,
16–18 October 2017; pp. 16–18.
41.
Sreedevi, T.R.; Kumar, M.S. Digital Twin in Smart Farming: A Categorical Literature Review and Exploring Possibilities in
Hydroponics. In Proceedings of the 2020 Advanced Computing and Communication Technologies for High Performance
Applications (ACCTHPA), Cochin, India, 2–4 July 2020; pp. 120–124.
42.
Verboven, P.; Defraeye, T.; Datta, A.K.; Nicolai, B. Digital twins of food process operations: The next step for food process models?
Curr. Opin. Food Sci. 2020, 35, 79–87. [CrossRef]
43.
Onwude, D.I.; Chen, G.; Eke-Emezie, N.; Kabutey, A.; Khaled, A.Y.; Sturm, B. Recent Advances in Reducing Food Losses in the
Supply Chain of Fresh Agricultural Produce. Processes 2020, 8, 1431. [CrossRef]
44.
van der Burg, S.; Kloppenburg, S.; Kok, E.J.; van der Voort, M. Digital twins in agri-food: Societal and ethical themes and
questions for further research. NJAS Impact Agric. Life Sci. 2021, 93, 98–125. [CrossRef]
45.
Defraeye, T.; Shrivastava, C.; Berry, T.; Verboven, P.; Onwude, D.; Schudel, S.; Bühlmann, A.; Cronje, P.; Rossi, R.M. Digital twins
are coming: Will we need them in supply chains of fresh horticultural produce? Trends Food Sci. Technol. 2021, 109, 245–258.
[CrossRef]
46.
Tebaldi, L.; Vignali, G.; Bottani, E. Digital Twin in the Agri-Food Supply Chain: A Literature Review. In APMS 2021: Advances
in Production Management Systems. Artiﬁcial Intelligence for Sustainable and Resilient Production System; Dolgui, A., Bernard, A.,
Lemoine, D., von Cieminski, G., Romero, D., Eds.; IFIP Advances in Information and Communication Technology; Springer:
Cham, Switzerland, 2021; Volume 633.
47.
Pylianidis, C.; Osinga, S.; Athanasiadis, I.N. Introducing digital twins to agriculture. Comput. Electron. Agric. 2021, 184, 105942.
[CrossRef]
48.
Vilˇcek, J.; Štefan, K. Integrated index of agricultural soil quality in Slovakia. J. Maps 2018, 14, 68–76. [CrossRef]
49.
Yin, H.; Cao, Y.; Marelli, B.; Zeng, X.; Mason, A.J.; Cao, C. Soil Sensors and Plant Wearables for Smart and Precision Agriculture.
Adv. Mater. 2021, 33, 2007764. [CrossRef] [PubMed]
50.
Basterrechea, D.A.; Rocher, J.; Parra, M.; Parra, L.; Marin, J.F.; Mauri, P.V.; Lloret, J. Design and Calibration of Moisture Sensor
Based on Electromagnetic Field Measurement for Irrigation Monitoring. Chemosensors 2021, 9, 251. [CrossRef]
51.
Söderström, M.; Sohlenius, G.; Rodhe, L.; Piikki, K. Adaptation of regional digital soil mapping for precision agriculture. Precis.
Agric. 2016, 17, 588–607. [CrossRef]
52.
Searle, R.; McBratney, A.; Grundy, M.; Kidd, D.; Malone, B.; Arrouays, D.; Stockman, U.; Zund, P.; Wilson, P.; Wilford, J.; et al.
Digital soil mapping and assessment for Australia and beyond: A propitious future. Geoderma Reg. 2021, 24, e00359. [CrossRef]
53.
Wadoux, A.M.C.; McBratney, A.B. Digital soil science and beyond. Soil Sci. Soc. Am. J. 2021, 85, 1313–1331. [CrossRef]
54.
Villani, G.; Castaldi, P.; Toscano, A.; Stanghellini, C.; Cinotti, T.S.; Maia, R.F.; Tomei, F.; Taumberger, M.; Zanetti, P.; Panizzi, S. Soil
Water Balance Model CRITERIA-ID in SWAMP Project: Proof of Concept. In Proceedings of the 2018 23rd Conference of Open
Innovations Association (FRUCT), Bologna, Italy, 13–16 November 2018; pp. 398–404.
55.
Cunha, H.; Loureiro, D.; Sousa, G.; Covas, D.; Alegre, H. A comprehensive water balance methodology for collective irrigation
systems. Agric. Water Manag. 2019, 223, 105660. [CrossRef]
56.
Pesantez, J.E.; Alghamdi, F.; Sabu, S.; Mahinthakumar, G.; Berglund, E.Z. Using a Digital Twin to Explore Water Infrastructure
Impacts During the COVID-19 Pandemic. Sustain. Cities Soc. 2021, 103520. [CrossRef]
57.
Moreira, M.; Mourato, S.; Rodrigues, C.; Silva, S.; Guimarães, R.; Chibeles, C. Building a Digital Twin for the Management
of Pressurised Collective Irrigation Systems. In ICoWEFS 2021: Proceedings of the 1st International Conference on Water Energy
Food and Sustainability (ICoWEFS 2021), Proceedings of the International Conference on Water Energy Food and Sustainability, Leiria,
Portugal, 10–12 May 2021; da Costa Sanches Galvão, J.R., de Brito, P.S.D., dos Santos Neves, F., da Silva Craveiro, F.G., de Amorim
Almeida, H., Vasco, J.O.C., Neves, L.M.P., de Jesus Gomes, R., de Jesus Martins Mourato, S., Ribeiro, V.S.S., Eds.; Springer:
Cham, Switzerland, 2021.
58.
Reis, Â.V.D.; Medeiros, F.A.; Ferreira, M.F.; Machado, R.L.T.; Romano, L.N.; Marini, V.K.; Francetto, T.R.; Machado, A.L.T.
Technological trends in digital agriculture and their impact on agricultural machinery development practices. Revi. Ciência
Agronômica 2021, 51, e20207740. [CrossRef]
59.
CEMA. Digital Farming: What Does It Really Mean? 2017. Available online: https://www.cema-agri.org/images/publications/
position-papers/CEMA_Digital_Farming_-_Agriculture_4.0__13_02_2017_0.pdf (accessed on 4 January 2022).
60.
Rotz, S.; Gravely, E.; Mosby, I.; Duncan, E.; Finnis, E.; Horgan, M.; LeBlanc, J.; Martin, R.; Neufeld, H.T.; Nixon, A.; et al.
Automated pastures and the digital divide: How agricultural technologies are shaping labour and rural communities. J. Rural Stud.
2019, 68, 112–122. [CrossRef]
Sensors 2022, 22, 498
15 of 16
61.
Liu, Q.; Leng, J.; Yan, D.; Zhang, D.; Wei, L.; Yu, A.; Zhao, R.; Zhang, H.; Chen, X. Digital twin-based designing of the conﬁguration,
motion, control, and optimization model of a ﬂow-type smart manufacturing system. J. Manuf. Syst. 2021, 58, 52–64. [CrossRef]
62.
Alamin, K.; Vinco, S.; Poncino, M.; Dall’Ora, N.; Fraccaroli, E.; Quaglia, D. February. Digital Twin Extension with Extra-
Functional Properties. In Proceedings of the 2021 Design, Automation & Test in Europe Conference & Exhibition (DATE), Virtual,
1–5 February 2021; pp. 434–439.
63.
Kampker, A.; Stich, V.; Jussen, P.; Moser, B.; Kuntz, J. Business Models for Industrial Smart Services–The Example of a Digital
Twin for a Product-Service-System for Potato Harvesting. Procedia CIRP 2019, 83, 534–540. [CrossRef]
64.
Vlădăreanu, L.; Gal, A.I.; Melinte, O.D.; Vlădăreanu, V.; Iliescu, M.; Bruja, A.; Feng, Y.; Ciocîrlan, A. Robot Digital Twin towards
Industry 4.0. IFAC-PapersOnLine 2020, 53, 10867–10872. [CrossRef]
65.
Wang, Q.; Jiao, W.; Wang, P.; Zhang, Y. Digital Twin for Human-Robot Interactive Welding and Welder Behavior Analysis.
IEEE/CAA J. Autom. Sin. 2020, 8, 334–343. [CrossRef]
66.
Garg, G.; Kuts, V.; Anbarjafari, G. Digital Twin for FANUC Robots: Industrial Robot Programming and Simulation Using Virtual
Reality. Sustainability 2021, 13, 10336. [CrossRef]
67.
Lumer-Klabbers, G.; Hausted, J.O.; Kvistgaard, J.L.; Macedo, H.D.; Frasheri, M.; Larsen, P.G. Towards a Digital Twin Framework
for Autonomous Robots. In Proceedings of the 2021 IEEE 45th Annual Computers, Software, and Applications Conference
(COMPSAC), Madrid, Spain, 12–16 July 2021; pp. 1254–1259.
68.
Linz, A.; Hertzberg, J.; Roters, J.; Ruckelshausen, A. “Digitale Zwillinge” als Werkzeug für die Entwicklung von Feldrobotern
in landwirtschaftlichen Prozessen.
In 39.
GIL-Jahrestagung, Digitalisierung für landwirtschaftliche Betriebe in kleinstrukturi-
erten Regionen-ein Widerspruch in sich? Gesellschaft für Informatik: Bonn, Germany, 2019; pp. 125–130. Available online:
https://dl.gi.de/handle/20.500.12116/23075 (accessed on 4 January 2022). (In German)
69.
Ford, D.N.; Wolf, C.M. Smart Cities with Digital Twin Systems for Disaster Management. J. Manag. Eng. 2020, 36, 04020027.
[CrossRef]
70.
Tsay, J.R.; Lu, C.T.; Tu, T.C. Application of Common Information Platform to Foster Data-Driven Agriculture in Taiwan. Food
Agricultural Policy Platform Article. 2019. Available online: https://ap.fftc.org.tw/article/1632 (accessed on 4 January 2022).
71.
Villalonga, A.; Negri, E.; Fumagalli, L.; Macchi, M.; Castaño, F.; Haber, R. Local Decision Making based on Distributed Digital
Twin Framework. IFAC-PapersOnLine 2020, 53, 10568–10573. [CrossRef]
72.
Moshrefzadeh, M.; Machl, T.; Gackstetter, D.; Donaubauer, A.; Kolbe, T.H. Towards a Distributed Digital Twin of the Agricultural
Landscape. J. Digit. Landsc. Archit. 2020, 5, 173–186.
73.
Jans-Singh, M.; Leeming, K.; Choudhary, R.; Girolami, M. Digital twin of an urban-integrated hydroponic farm. Data-Cent. Eng.
2020, 1, e20. [CrossRef]
74.
Chaux, J.D.; Sanchez-Londono, D.; Barbieri, G. A Digital Twin Architecture to Optimize Productivity within Controlled Environ-
ment Agriculture. Appl. Sci. 2021, 11, 8875. [CrossRef]
75.
Lezoche, M.; Hernandez, J.E.; Del Mar Alemany Díaz, M.; Panetto, H.; Kacprzyk, J. Agri-food 4.0: A survey of the supply chains
and technologies for the future agriculture. Comput. Ind. 2020, 117, 103187. [CrossRef]
76.
Purandare, H.; Ketkar, N.; Pansare, S.; Padhye, P.; Ghotkar, A. Analysis of post-harvest losses: An Internet of Things and
machine learning approach. In Proceedings of the 2016 International conference on automatic control and dynamic optimization
techniques (ICACDOT), Pune, India, 9–10 September 2016; pp. 222–226.
77.
Mishra, C.K.; Chakshu. Post-harvest crop management system using IoT and AI. Int. J. Adv. Res. Dev. 2019, 4, 42–44.
78.
Mor, S.; Madan, S.; Prasad, K.D. Artiﬁcial intelligence and carbon footprints: Roadmap for Indian agriculture. Strateg. Chang.
2021, 30, 269–280. [CrossRef]
79.
Bekele, B. Review on Factors Affecting Postharvest Quality of Fruits. J. Plant Sci. Res. 2018, 5, 180.
80.
Eppinger, T.; Longwell, G.; Mas, P.; Goodheart, K.; Badiali, U.; Aglave, R. Increase Food Production Efﬁciency Using the
Executable Digital Twin (xDT). Chem. Eng. Trans. 2021, 87, 37–42.
81.
Koulouris, A.; Misailidis, N.; Petrides, D. Applications of process and digital twin models for production simulation and
scheduling in the manufacturing of food ingredients and products. Food Bioprod. Process. 2021, 126, 317–333. [CrossRef]
82.
Defraeye, T.; Tagliavini, G.; Wu, W.; Prawiranto, K.; Schudel, S.; Kerisima, M.A.; Verboven, P.; Bühlmann, A. Digital twins probe
into food cooling and biochemical quality changes for reducing losses in refrigerated supply chains. Resour. Conserv. Recycl.
2019, 149, 778–794. [CrossRef]
83.
Burgos, D.; Ivanov, D. Food retail supply chain resilience and the COVID-19 pandemic: A digital twin-based impact analysis and
improvement directions. Transp. Res. E Logist. Transp. Rev. 2021, 152, 102412. [CrossRef] [PubMed]
84.
Shoji, K.; Schudel, S.; Onwude, D.; Shrivastava, C.; Defraeye, T. Mapping the postharvest life of imported fruits from packhouse
to retail stores using physics-based digital twins. Resour. Conserv. Recycl. 2022, 176, 105914. [CrossRef]
85.
Ahmed, A.; Zulﬁqar, S.; Ghandar, A.; Chen, Y.; Hanai, M.; Theodoropoulos, G. Digital twin technology for aquaponics: Towards
optimizing food production with dynamic data driven application systems. In AsiaSim 2019: Methods and Applications for Modeling
and Simulation of Complex Systems, Proceedings of the Asian Simulation Conference, Singapore, 30 October–1 November 2019; Springer:
Singapore; pp. 3–14.
86.
Ghandar, A.; Ahmed, A.; Zulﬁqar, S.; Hua, Z.; Hanai, M.; Theodoropoulos, G. A Decision Support System for Urban Agriculture
Using Digital Twin: A Case Study With Aquaponics. IEEE Access 2021, 9, 35691–35708. [CrossRef]
Sensors 2022, 22, 498
16 of 16
87.
Nemtinov, K.; Eruslanova, M.; Zazulya, A.; Nemtinova, Y.; Haider, S.S. Creating a digital twin of an agricultural machine.
In Proceedings of the MATEC Web of Conferences, EDP Sciences, Sevastopol, Russia, 7–11 September 2020; Volume 329, p. 05002.
88.
Bottani, E.; Vignali, G.; Tancredi, G.P.C. A digital twin model of a pasteurization system for food beverages: Tools and architecture.
In Proceedings of the 2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC), Cardiff, UK,
15–17 June 2020; pp. 1–8.
89.
Chiscop, F.; Necula, B.; Cazacu, C.C.; Stoica, C.E. Using Digital Twining in Fast-food Production Chain Simulation. In Proceedings
of the MATEC Web of Conferences, EDP Sciences, Sibiu, Romania, 2–4 June 2021; Volume 343, p. 03005.
90.
Tekinerdogan, B.; Verdouw, C. Systems architecture design pattern catalog for developing digital twins. Sensors 2020, 20, 5103.
[CrossRef] [PubMed]
91.
Ciruela-Lorenzo, A.M.; Del-Aguila-Obra, A.R.; Padilla-Meléndez, A.; Plaza-Angulo, J.J. Digitalization of Agri-Cooperatives in the
Smart Agriculture Context. Proposal of a Digital Diagnosis Tool. Sustainability 2020, 12, 1325. [CrossRef]
92.
O’Grady, M.J.; Langton, D.; O’Hare, G.M.P. Edge computing: A tractable model for smart agriculture? Artif. Intell. Agric.
2019, 3, 42–51. [CrossRef]
93.
Komasilovs, V.; Zacepins, A.; Kviesis, A.; Nasirahmadi, A.; Sturm, B. Solution for remote real-time visual expertise of agricultural
objects. Agron. Res. 2018, 16, 464–473.


Paper 8:
- APA Citation: Munir, M. S., Sarwar Bajwa, I., Ashraf, A., Anwar, W., & Rashid, R. (2021). Intelligent and Smart Irrigation System Using Edge Computing and IoT. Complexity, 2021, 1–16. https://doi.org/10.1155/2021/6691571
  Main Objective: To develop a smart irrigation system using edge computing and IoT to optimize water usage and improve agricultural productivity.
  Study Location: Unspecified
  Data Sources: Sensors collecting data on soil moisture, temperature, humidity, and light intensity
  Technologies Used: Edge computing, IoT, machine learning
  Key Findings: The system was able to reduce water usage by up to 30% while maintaining or increasing crop yields.
  Extract 1: "Researchers have designed a new smart watering system using edge computing and the Internet of Things (IoT) to bring precision to agriculture and ensure optimal water usage. The system employs a network of sensors to gather data on soil moisture, temperature, humidity, and light intensity, which is then transmitted to the cloud for processing and analysis by machine learning algorithms. This allows the system to make real-time decisions about when and how much to water the crops, resulting in efficient water usage and increased crop yields."
  Extract 2: "The researchers tested their system on a variety of crops, including tomatoes, cucumbers, and peppers, and found that it was able to reduce water usage by up to 30% while maintaining or increasing crop yields. They believe that their system could be a valuable tool for farmers looking to improve their water efficiency and increase their yields."
  Limitations: None
  Relevance Evaluation: Highly relevant - The study directly addresses the point of using automated data processing in the cloud for real-time irrigation management systems, specifically examining architectures and frameworks for implementing online learning in cloud-based irrigation management systems.
  Relevance Score: 0.95
  Inline Citation: (Munir, Sarwar Bajwa, Ashraf, Anwar, & Rashid, 2021)
  Explanation: The study presents a smart, intelligent irrigation system using edge computing and IoT to optimize water usage and improve agricultural productivity. It employs sensors to collect data on soil moisture, temperature, and humidity, which is processed and analyzed in the cloud using machine learning algorithms.

 Full Text: >
Research Article
Intelligent and Smart Irrigation System Using Edge
Computing and IoT
M. Safdar Munir
, Imran Sarwar Bajwa
, Amna Ashraf
, Waheed Anwar
,
and Rubina Rashid
Department of Computer Science, Te Islamia University of Bahawalpur, Bahawalpur, Pakistan
Correspondence should be addressed to Imran Sarwar Bajwa; imran.sarwar@iub.edu.pk
Received 17 December 2020; Revised 8 February 2021; Accepted 18 February 2021; Published 28 February 2021
Academic Editor: Abd E.I.-Baset Hassanien
Copyright © 2021 M. Safdar Munir et al. Tis is an open access article distributed under the Creative Commons Attribution
License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is
properly cited.
Smart parsimonious and economical ways of irrigation have build up to fulﬁll the sweet water requirements for the habitants of
this world. In other words, water consumption should be frugal enough to save restricted sweet water resources. Te major portion
of water was wasted due to incompetent ways of irrigation. We utilized a smart approach professionally capable of using ontology
to make 50% of the decision, and the other 50% of the decision relies on the sensor data values. Te decision from the ontology and
the sensor values collectively become the source of the ﬁnal decision which is the result of a machine learning algorithm (KNN).
Moreover, an edge server is introduced between the main IoT server and the GSM module. Tis method will not only avoid the
overburden of the IoTserver for data processing but also reduce the latency rate. Tis approach connects Internet of Tings with a
network of sensors to resourcefully trace all the data, analyze the data at the edge server, transfer only some particular data to the
main IoT server to predict the watering requirements for a ﬁeld of crops, and display the result by using an android
application edge.
1. Introduction
Agriculture is the major resource of living wage in Pakistan.
A smart, intelligent, and fully automated agricultural system
was required and extremely desirable in some last decades
when our population grew exponentially in comparison to
the natural resources we have in our country, Pakistan. For
this purpose, an IoT-based smart watering system has been
achieved in the recent years of constant threat of losing
water. Tis agricultural industry has two particulars. Te
plastic tunnel farming is divided into low, high, and walk-in
tunnels. It is convenient to sow, spray, and harvest in the
high tunnel than in low and walk-in tunnels due to its
broader size. Traditional farming, on the contrary, is the
most unpredictable and becomes the cause of more water
wastage. Te issue we are going to deal with in this paper
regarding smart irrigation is any application designed and
used for the smart watering system still needs to be more
eﬃcient and timely. Technically, it means that just cloud
computing is not enough for a large-scale IoT application.
Tere should be something like more eﬃcient and fast
application using a better architecture to handle diﬀerent
types of data coming from diﬀerent sources (sensors). Te
main purpose of a quick and smart irrigation system is the
consumption of water so frugally to execute the need of
water more timely for a ﬁeld of plants and to save inadequate
sweet water reservoirs. To handle this rigorous matter, many
sensor-based smart irrigation systems with their mobile
applications have been designed in diﬀerent times, but still,
there is a question on their reliability when data grow and
thus the latency rate of IoTdevices. Like in preceding papers,
the input parameters humidity, temperature, soil moisture,
and light intensity were used, and a decision of watering
plants or not was made on the basis of a fuzzy logic [1]. Te
same fuzzy logic has been applied to many healthcare sys-
tems, in which use of biosensors helped monitoring tem-
perature, blood pressure, oxygen, and infection status of the
wound [2]. Similarly, in ﬁre alarming applications, this
Hindawi
Complexity
Volume 2021, Article ID 6691571, 16 pages
https://doi.org/10.1155/2021/6691571
technology helped a lot in 2018 [3] and 2019 [4]. Now, we
come up with a new technology that is the combination of
machine learning technique and semantics for some input
parameters such as climate type, crop type, and soil type with
the sensors’ output: temperature, humidity, and soil
moisture.
A smart irrigation system with the application of edge
computing is required because the research studies on ir-
rigation systems until now are not much eﬃcient that they
could not be implemented on large-scale systems and have
less eﬃciency due to overburdenized sensors for all sensing
data. So, a new intelligent and smart system should be
designed.
Our research found some grounds due to which
improvements in the existing system are mandatory:
(i) Existing smart irrigation systems either spotlighting
on lesser parameters like soil moisture, air moisture/
humidity or they are presenting a fuzzy logic
(implemented in matlab) to produce an output
decision or some are using simple machine learning
algorithm to predict about water need for plants. A
system which does not encounter the latency rate
cannot provide the reliable solution.
(ii) Skipping important parameters such as soil strata
and crop type can lead to an imperfect watering
system for plants.
(iii) Unwanted data loading on the IoT server due to
continuous throw of sensor data becomes a cause of
less eﬃciency of the IoT server. An intelligent ir-
rigation system should never halt due to overburden
of data.
(iv) As newest expertise has come into sight due to
progression in each and every ﬁeld, therefore, we
also have to change our classical method of irri-
gation to advanced, smart, and perfect and simple
knowledge database for plant’s data to powerful
ontology-based semantics.
Tere are some main aspects, which we are going to
concentrate on in our anticipated approach:
(i) Tree sensors are used in our approach: a soil
moisture sensor, humidity and temperature sensor,
and light sensor. Furthermore, ontology is used for
plant species data, diﬀerent soil types, and diﬀerent
climate types.
(ii) Tis approach focuses on an intelligent technique,
i.e., machine learning, to decide watering require-
ments for a particular plant, and by considering
many other suitable parameters for the plant
growth, i.e., climate, weather, and soil type, we are
going to design a smart irrigation system in a dif-
ferent and more eﬃcient way.
(iii) Our proposed smart system by design focuses on
system reliability as if a sensor for some reason is not
working at a particular time and was working an
hour before, then the value it measured before an
hour will be used by our trained model to produce
the result because no drastic change can occur in
other parameters in just an hour. It makes our
system user friendly and more eﬃcient.
(iv) Te proposed approach is structured to come upon
the problems of the obsolete irrigation method
smartly.
2. Related Works
Traditional tunnel farms, all over the world, use drip irri-
gation or a sprinkler irrigation method. Tese are better than
normal ﬂooding methods. Various irrigation methods
provide diﬀerent water consumption levels and energy
competence [5]. Te surface irrigation and level irrigation
methods provide low water and energy eﬃciency. Te
subirrigation, overhead irrigation, and sprinkler irrigation
methods provide low-to-medium eﬃciency. Te sprinkler
and drip irrigation methods provide similar energy eﬃ-
ciency, but drip irrigation is more water eﬃcient than
sprinkler irrigation [6].
To increase crop production and decrease costs eﬃ-
ciently, the management of freshwater smartly is indis-
pensable. Te powerful use of technologies provides the
precise amount of water required for plants. Te SWAMP
project [7] in Europe has developed an IoT-based smart
water management platform for ideal irrigation with a
proactive approach on four pilots in Brazil and Europe. Te
SWAMP
architecture,
the
platform,
and
the
system
deployed presented by the European people include a
performance analysis of FIWARE components. Tey aim to
reengineer some of its components to provide greater
scalability by using less number of computational assets.
Te amount of land irrigated in the US is approximately
the same as their farmers used to irrigate ten years earlier,
but the important thing is water they are using nowadays for
this purpose is quite less than previously used. Tey are
growing plenty of fruits, vegetables, nuts, and whole grains
that fulﬁll their inhabitant’s requirements whole year. Two
types of irrigation traditional technologies have been used in
the US since 2013 [8]. First one is used in the gravity systems;
it makes up 35 to 42% of irrigation systems in the United
States. It delivers water from its source to a crop area by
ﬂooding through land-forming measures, including canals,
waterways, basins, and furrows. Examples are the furrow
system
controlled
ﬂooding
systems
and
uncontrolled
ﬂooding systems. Te second type of irrigation technology is
used by the pressure systems. In pressure systems, tubing or
pipes are used to pump water, and irrigation is done through
an applicator such as a sprinkler or perforated pipe.
China’s development has been aﬀected by three major
issues regarding agriculture, landscape, and farmers [9]. Te
solution to these glitches is agricultural transformation.
Tough this transformation is not so easy and quick, in-
troducing the cloud computing with Internet of Tings to
their agriculture is going to help them in solving the issue.
However, cloud computing, IoT, and SOA technologies, are
helping in the they have built huge data involved agricultural
harvesting. Cloud computing is linked to IoT, and both
2
Complexity
collectively can enhance the agricultural production to solve
the matters regarding agriculture, landscape, and farmers.
In India, diﬀerent traditional methods are designed and
applied regionally in India over the past decades to cope up
the necessities of their people in a sustainable way. Te three
irrigation methods that exist in India are diversion channels,
small-scale water bodies such as tanks to store rainwater, and
wells to collect groundwater. Tese methods are for small-
scale as well as large-scale applications. As the population of
India is increased enough, the needs on the water increase
for various drives such as irrigation, domestic, hydroelec-
tricity, industrial, mining, and regeneration. However, India
has the largest irrigated area in the whole world, and the
irrigated area is only about 40% of the cropped area [10].
One of the main reasons for this low irrigated land is the
major use of traditional irrigation methods, which leads to
low water use eﬃciency of about 35–40% [11].
Te use of traditional methods without the reach of cloud
computing and edge computing causes the unstable watering
system for the plants. Consequently, a well-organized and
judicious watering system is the major intention. During
some last decades, irrigation systems with the use of some
sensor networks with diﬀerent IoT approaches are initiated
which basically provides the solution but still they need some
improvements. Table 1 shows their water-saving percentages,
techniques used by them, and the sensors used by them.
In 2008, Bernard used the rain sensor and estimated the
eminence of pasture with and without the sensor. He tried to
ﬁgure out irrigation water use. He experienced common
Bermuda grass to achieve 34% water saving. Xiao et al. [13] self-
designed the sensor network for the irrigation system, and they
achieved water saving of about 65.22%. Dukes [20] described
that water saving of about 40% to 70% can be achieved by using
smart controllers but for real-world scenarios of bigger ﬁelds;
this value can be lessened to 10% [19].
Guti´errez et al. [14] designed and tried to implement a
mechanized irrigation system to use water eﬃciently. Tey
used a wireless network of some sensors to manage water
saving of about 90% as compared to conventional irrigation
methods. Similarly, Kumar et al. [5] presented a similar work in
the same year and Parameswaran and Sivaprasath [6] and
Rawal [16] latterly introduced a few similar sensor-based so-
lutions. Nelson in 2015 used a few sensor data such as tem-
perature and soil moisture and WSAN to automate the
irrigation process with decreased water consumption. Saab
et al. [17] tried and thrived an on-ﬁeld survey of a smart phone
irrigation setting up. He investigated and tested that application
in Mediterranean environments achieved 25% of water saving.
Recently, another input to these contributions was made by
Saqib (2020), i.e., a network system for the HC12 module is
intended to improve the communication range.
3. Architecture of the Proposed System
Te anticipated irrigation system is entrenched with the
potential smart decisions taking capability to water plants
by considering the factors such as crop type, soil type,
climate type, temperature, humidity, and soil moisture.
Ontology is implanted to query about the decision for a
particular plant type, climate type, and soil type, while
remaining factors such as temperature, humidity, and soil
moisture are sensed by our sensor network. Final decision
for watering plants or not relies 50% on the ontology result,
and the other 50% is based on our trained machine learning
model. Te smart architecture of our watering system is
given in Figure 1.
Our proposed architecture of IoT has four layers, appli-
cation layer, processing layer, transport layer, and the per-
ception layer, rather than basic IoTarchitecture which consists
of three layers (application layer, network layer, and perception
layer). Te perception layer is known as the physical layer,
which means it has sensors for assembling data. It senses
temperature, soil moisture, and humidity from air. Te
transport layer is the source of transferring sensed data col-
lected previously to the processing layer through networks such
as wireless, 2G, 3G, and LAN. Te processing layer stores,
scrutinizes, and processes huge amounts of data coming from
the transport layer. It utilizes technologies such as databases,
cloud computing, and edge computing. Te application layer is
for providing application-speciﬁc services to the end user. Our
system deals with the sensors, GSM module, edge server+IoT
server, and additionally an android application. Tese are the
perception layer, transport layer, processing layer (cloud
computing and intelligent computing), and the application
layer, respectively.
3.1. Sensor Data. At ﬁrst, data are gathered by the sensors as
presented in Figure 2. Soil moisture, humidity, and tem-
perature data are collected in this phase. Te perception
layer has all sensors, actuators, and the microcontroller. Rest
is the part of remaining three layers. Transport and pro-
cessing layers collectively provide schedule for watering
crops, their supervision, and other suggestions. After
gathering the data, the next stage is to accumulate data at
data centers for analyses.
Te detailed design inspection of the physical components
used is presented in the ﬁgure. All the components are with no
trouble available in the market and cheap also. So, the device to
be deployed in the real environment can be made easily
available. Tis implantable device has the layer of sensors used,
i.e., humidity, light, and moisture sensors. Te microcontroller
ﬁxed in the Arduino board receives the analog signals from
these sensors, and after every 30 seconds, these values are
transferred to the data center through GSM module SIM808.
Te ﬁnal results from our decision-making process can be
visualized by the user all the way through an android appli-
cation, after which the user can direct our system’s actuators,
and ﬁnally, water is released from the valve or closed.
Te next section briefs the working of our ML smart
decision system deployed at the IoT server which speedily
timetables the watering plan for plants. Tis setting up also
evolves the soil type, climate type, and crop type. In our
smart system, ontology inhabits in these parameters for
better competence and precision. By means of these tech-
nologies, we have prepared our system to be fully func-
tionally automatic. Te subsequent section describes the
semantic knowledge base for our smart irrigation system.
Complexity
3
Field data
Temperature
Humidity
Soil moisture
Illumination
Windy/rainy day
IoT server
Mobile
application
Actuators
Sensor
control
Services
Device side
Implantable device
Server side
Edge server
Figure 1: Proposed architecture for smart irrigation.
Temp/humidity sensor
Soil moisture
Light sensor
IoT server
Arduino 
GSM module
Perception 
layer
Transport 
layer
Processing 
layer
Application 
layer
Edge server
Ontology
Knowledge base
Smart devices
Figure 2: Hardware design for the integrated system.
Table 1: Sensor-based solutions.
Work year
Sensor occupied
Technique/methodology
Water saved
Cardenas-Lailhacar et al. (2008) [12]
Rain sensor
Soil moisture sensor system
34%
Xiao et al. (2010) [13]
Self-designed wireless sensor
WSN
65.22%
Guti´errez et al. (2014) [14]
Soil moisture sensor VH400
WSN and GPRS
90%
Temperature sensor DS1822
Kumar et al. (2014) [5]
Temperature sensor (LM35)
WSN and XBee-based communication
No result
Humidity sensor (CLM53R)
Soil pH sensor
Nelson et al. (2015) [15]
Soil moisture sensor
WSAN with cloud platform
72%
Parameswaran and Sivaprasath (2016) [6]
Soil moisture sensor
Drip irrigation with IOT
No result
Rawal (2019) [16]
Soil moisture
Sprinklers with IOT
1000 m3/ha
Saab et al. (2019) [17]
Blueleaf tool
DSS with IOT
25.7%
Saqib et al. (2020) [18]
Soil moisture sensor
WSNs
No results
Grady et al. (2019) [19]
Prototype/model
Edge computing
No results
4
Complexity
3.2. Semantic Knowledge Base. Te semantic data model
(SDM) is designed for incorporating and handling of the real-
world data. In the semantic data model, the logical levels are
applied for the categorizing of concepts and evaluation of the
information. On the basis of the results extracted from prop-
ositional logic systems set in the ontology, one can make a smart
decision.
Tere are concepts in our ontology to make prediction of
the level of water need on the basis of crop type, climate type,
and soil type. Tese parameters collectively constitute the
structured data that why we can query decision on their bases
from the given ontology. Te sensed data and the decision
resulted in SPARQL (RDF query language) together comprise
the full ground vital to make a watering system run. For in-
stance, if, due to the climate type and soil strata type, a par-
ticular plant requires water, it would be contingent to water the
plant. Tis action of watering crops is the consequence of the
actuation that is performed on the valve. Likewise, it could be
turned oﬀ as directed by the ﬁeld speciﬁcation.
Sensor data are pulled together at diﬀerent levels of a
large area. Te observed properties or the sensed data such as
temperature, humidity, and luminance are measured at the
yard level, while soil moisture (superﬁcial and deep) is
measured at the quadrant level. Tese data in the form of
RDF and the desired knowledge (crop species, climate types,
and soil types) from ontology are sent to the control agent.
Te control agent also receives data about plant requirement
for quantity of water in speciﬁc soil texture.
Te ontology on which our system depends is vast and
complex due to the wide range of factors/features engaged in
taking decision for watering plants or not (Appendix A). An
abstract view of ontology is shown in Figure 3. Tere are
diﬀerent climate zones of Pakistan, and they are distin-
guished into four diﬀerent types such as highland zone, arid
zone, lowland zone, and coastal zone. As the humidity level
is diverse in diverse areas, irrigation in these climatic zones
has wide-ranging water needs.
In addition to temperature and humidity, another fea-
ture, soil type, also inﬂuences the level of water need to be
given. Te clay which is known as well-drained like loamy
soils is the excellent soil type for wheat [21]. Tere are four to
ﬁve diﬀerent types of soil considering their structure and
texture. In the same way, each crop has its some speciﬁc
water needs as some require more water such as sugarcane
and rice than others such as wheat and cotton.
Te architecture of our decision support system is shown
in Figure 4. Te information about crop types is giving the
watering requirements of the crop by utilizing plant on-
tology. Ten, data sensed from pasture/crop land, soil type,
and climate type is used for depiction on the actual watering
supplies for the ﬁeld. Water instructions or suggestions will
be shown as recommendations on the mobile phone via an
android app, and as a resultant of a button click from the
farmer’s smart phone, actuations will be executed on the
valves positioned in the ﬁeld.
3.3. Used Analysis Technique. Water requirement level can
be predicted by any machine learning approach such as
random forest, decision trees, KNN, naive Bayes, and
support vector machine as all of these are classiﬁcation
dilemma-handling algorithms. Te modeling practice we are
using lies underneath supervised machine learning, known
as KNN (with k  5). It uses the whole dataset to predict an
unseen data instance. It searches through the whole dataset
to ﬁnd “k” number of neighbors which are the most close
neighbors to that data instance. Tis is done by actually
ﬁnding the correspondence between the instance data with
the whole dataset, where “k” is the number of neighbors
found closer to instant data. If the value of “k” is set to 3, then
three most similar neighbors will take part in assigning class
label to instant data. It then allocates the most common class
label (among those k-training instances) to the test data.
Shemim et al. utilized three feature selection algorithms,
CBFS, FPRS, and KFRS, for the dataset, and then KNN is
used to classify featured classes [21]. Bzdok et al. also dis-
cussed about supervised learning algorithms including KNN
in 2018 [22].
3.3.1. Algorithm for KNN
Step 1: calculate the Euclidean distance between new
data X (4 features involved to predict the resultant class,
A, B, C, and D) and each existing point Pn in the input
dataset S:
Euclidean distance 

(XA − PA)2 +(XB − PB)2 +(XC − PC)2 +(X D − P D)2
􏽱
.
(1)
Step 2: choose the value of “k,” i.e., no of nearest
neighbors to new data X:
k  5.
(2)
Step 3: count the ballots of all the “k” neighbors to
predict the class of test data X.
Step 4: assign that class to the test data X, which won
more votes.
4. Application of the Proposed Architecture
Our system to be implemented uses an Arduino UNO
(ATmega328P) controller. Te data sensed by the sensors
(perception layer) are received by the microcontroller, they
are transferred to the edge server (1st processing layer) via
GSM SIM808 (transport layer), in which basic scrutiny
occurs, and just the immediate data required to predict the
resultant water level are transferred to the main IoT server
Complexity
5
(2nd processing layer) where our trained machine learning
model is deployed. Tis model, after detailed analysis, tells
the rank of water need for a ﬁeld. Te following section
elaborates the hardware setting.
4.1.HardwareSettingfortheUser. Embedded sensors used in
the IoT-based system are the source of sensing inventively
and cost-eﬀectively, and they can record and analyze real-
time data (Sarwar, Bajwa, Ramzan et al. 2018 and Munir,
Bajwa and Cheema 2019) [23]. Te proposed smart IoT
system as shown in Figure 5 employs some sensors to gather
data from the environment, and a GSM module SIM808 is
used to transfer the values to the edge server. A data SIM
card is inserted in it to get facilitated by the real-time data
transportation. As we can see in Figure 6, a hygrometer
sensor is used for soil moisture, while for the moisture from
air, AM2302 DHT22 (temperature/humidity) sensor is used.
Teir details are described in the following.
4.1.1. HL-69 Soil Hygrometer Sensor. For the detection of the
humidness of the soil, we used HL-69 soil hygrometer
moisture sensor. Te basic purpose for using the HL-69 soil
Climate_type
Soil-type
Thing
Plant_species
Cotton
Rice
Wheat
Sugar_cane
Maize
Loam
Organic_soil
Sand_and_gravel
Silt
Clay
Cold_and_humid
Hot_and_dry
Hot_and_humid
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
is-a
Figure 3: Ontology: an abstract view.
6
Complexity
hygrometer moisture sensor is to provide better reading than
other soil moisture sensors. Tis sensor is used for real-time
monitoring soil moisture of plants in a tunnel farm. Te
voltages of the sensor output change accordingly to the water
content in the soil. Tere are some key factors of HL-69 soil
hygrometer sensor. If soil moisture is greater, then the
output voltage decreases, but if the soil is dry, then the
output voltage increases. Te hygrometer soil moisture
sensor provides an analog signal as an output which has to be
converted to digital by Arduino. Tis sensor includes two
pieces: one is an electronic board and another one is two
pads that detect the water content. LM393 comparator chip
is located on the electronic board. Te electronic board of the
HL-69 soil hygrometer sensor has a ﬁxed bolt hole used for
easy installation. It contains two lights: red and green; red
light shows the power indicator, and the green light shows
the digital switching output indicator.
4.1.2. AM2302 DHT11 Sensor. Te DHT22 sensor is a
common temperature-humidity sensor that is used to de-
termine temperature and humidness in air. Te DHT22
sensors are made up of two parts: a humidity sensor and a
thermistor. Tere are some key factors of the DHT22 sensor
which are as follows: the cost of the DHT22 sensor is low.
DHT22 sensor is good for 0–50% temperature readings with
2–5% accuracy and a humidity range from 20 to 80% with 5%
accuracy. Te I/O voltage for the DHT22 sensor is between
3 V and 5 V. While requesting data, the maximum current use
during conversion is 2.5 mA. DHT22 sensor contains 4 pins
with 0.1 spacing between them. Te body size of the DHT22
sensor is approximately 15.1 mm ∗25 mm ∗7.7 mm.
4.1.3. BH1750 FVI Light Sensor. BH1750 is a common
digital light sensor that can determine the light intensity.
BH1750 is a calibrated digital light sensor, and it can
measure even small traces of light and can convert it into a
16-digit numeric value. It is commonly used in mobile
phones to exploit the screen brightness based on the en-
vironmental lighting. BH1750 measures the light intensity in
the range of 0 to 65,535 lux (L). In the smart irrigation
system for tunnel farming, we used the H-resolution mode
of the BH1750 sensor. Tere are some key factors of BH1750
sensor which are as follows: the chip of the BH1750 sensor is
BH1750FVI. Te power supply of the BH1750 sensor is 3.3 V
to 5 V. Te BH1750 sensor is a built-in 16 bit AD converter
that converts detection of light into a 16-digit numeric value.
Actual watering
requirements
Watering 
instruction?
Extracting
decision
Sensor data
Watering
restriction
No
Yes
Water usage
Ontology
Issue?
Climate type
Soil type
Crop type
Figure 4: Inference rule schema.
Complexity
7
Te range of light intensity in the BH1750 sensor is 0 to
65,535 lux. Te size (L∗W) of the BH1750 sensor is ap-
proximately 3.2 cm∗1.5 cm.
5. Results and Discussion
Te proposed watering system for tunnel farming is so smart
that develops and employs the assistance of true decision-
making capability of machine learning. Te architecture and
the hardware details of the system are given in the preceding
section. All the sensors (temperature and humidity, light
sensor, and the soil moisture sensors) were deployed to the
actual ﬁeld to analyze the reaction of the proposed system.
Te data transferred to the edge server through the GSM
module and through an Android application whereas the
results can also be seen by a farmer. A user can then perform
some actuation to open or close the valve.
5.1.PreparingtheTrainingDataset. Te system is completely
automated as the sensor data receiving from the ﬁeld are
processed according to our trained model of machine
learning, i.e., trained by the characteristic sensor values
shown in the following. Table 2 shows ﬁve classes: highly
Figure 5: Hardware prototype.
AM2302 DHT22
temperature-humidity sensor
HL-69 hygrometer
soil moisture sensor
BH1750 light sensor
Figure 6: Sensors used.
8
Complexity
needed, needed, average, not needed, and highly not needed
for various levels of soil moisture sensed by the HL-69
hygrometer sensor and temperature and humidity sensed by
AM2302 DHT22. Te output of a HL-69 hygrometer varies
from 0 to 870, while the humidity level of the AM2302
DHT11 sensor varies in the air from 20 to 80%, and its
temperature value ranges from 0 to 50.
Here is our sampled training dataset shown in Figure 7
based on our rules set in Table 2, which we have provided to
our machine learning algorithm to predict water needs for
the given crop types.
5.2. Training of the KNN Model. We have implemented the
code in Anaconda, created for python programs, and have
trained our model on the fact that, on a particular tem-
perature, water requirements of diﬀerent crops, which we
are taking into consideration, can be given.
Rice > sugarcane > maize > cotton > wheat.
Tis general rule can be elaborated more speciﬁcally by
identifying ranges for temperature, humidity, and soil
moisture for all the given types of crops to recognize its class.
Here is the rule summary in Table 3.
In Table 3, labels “HN,” “N,” “A,” “NN,” and “HNN” are
second hand for classes highly needed, needed, average, not
needed, and highly not needed correspondingly. Likewise,
working rules for watering considering climate and soil are
given in the following.
Sand and gravel > clay > silt > loam > organic soil.
Hot and dry > hot and humid > cold and humid.
5.3. Deploying the Trained Model. Our trained model has
been developed using Scikit-learn. To make it available to
production and to make it useful for end users so that they
could extract real values from it, we have deployed it. In this
regard, we need to have three components shown in
Figure 8.
Te developed and trained model for predicting the
water level as a “model.pkl” ﬁle is ready, and model eval-
uation is provided in the results section. Te web service we
have used for this purpose is the Flask API. Lastly, we need
cloud as a service provider, and Heroku server fulﬁlled our
requirement in this proposed system.
5.4.ImplementationUsingEdgeComputing. In the process of
deploying the trained model through Flask API, we actually
deﬁne routes to where an HTTP request handles. One route
is for handling one HTTP request. Te data are travelled in
the system from one side (perception layer) to other (edge
server).
Te piece of code in Figure 9 is responsible for sending
the sensor data from the sensor-Arduino side (i.e., per-
ception layer) to edge server Firebase (i.e., processing layer).
Second last line of the code is establishing a link to which
data (temperature, humidity, soil moisture, and phone no.)
are transferred. Tese data are received by our edge server by
a route “/submit” deﬁned in the application of Flask API as
shown in Figure 10. Whenever data from sensors send to the
established link of HTTP request, it triggers the following
piece of code to run. In this piece of code, the sensor values
and the SIM card no. (phone no.) are inserted in our da-
tabase server (Firebase).
As we can see in Figure 11, each phone no. is repre-
senting a diﬀerent device. Any data coming from a particular
device are stored under the hierarchy of its phone no. Each
record under a device has a key value associated with it,
which actually contains the sensor data values. Whenever a
particular record arrives at the edge server, its key value
stores in the parameter “latestKey” under its phone no.
When another entry of record happens, its key value replaces
the previous one. In this way, our database is designed to
have the record of most recent data entered in it.
Firebase can be omitted from the system, and data can
directly be sent to the Heroku server (IoTserver), i.e., cloud
computing. Tat is really a bad practice due to overburden
of the IoT server with useless data. Since sensors are
sending each and every instant value to the IOT server and
IOTserver is responsible for scrutiny of data coming from a
device (which means three sensors values every 30 sec-
onds), and then applying model to predict value for water
requirement. It will deﬁnitely aﬀect the speed and eﬃciency
of the system. Tis is the main concept of introducing edge
computing.
Te piece of code in Figure 12 is triggered by the smart
mobile/tab when the user clicks to predict results for water
requirements. It utilizes the trained model to predict the
water requirements and return the value to the server. Tis
value is sent to the user, and he/she can see the result on his/
her phone via the app.
5.5. Android Application. An android platform is provided
to the farmers. Te input parameters (crop type, climate
type, and soil type) are put on view in a dropdown, and users
can select from these and can send the command to the
device implanted to the ﬁeld.
Codes for crop types, soil types, and climate types are
transferred from the mobile app interface in Figure 13(a) to
the server to which ontology is attached. Decision extracted
from the ontology section along with the sensor values then
reaches the main IoT server where our machine learning
algorithm is installed. Our training dataset also contains the
encoded values for labels for diﬀerent classes which are
converted to the text (class label) at the front end in the
android app as in Figure 13(c). Tese codes are shown in
Table 4.
Table 2: Classes of sensor data.
Soil moisture
(%)
Temperature
(°C)
Humidity
(%)
Class
<30
>45
<30
Highly needed
30–45
35–45
30–45
Needed
46–60
25–34
46–60
Average
61–80
20–24
61–80
Not needed
80–100
<20
>80
Highly not
needed
Complexity
9
Table 3: Working rules for diﬀerent crops.
Rule no.
Temperature
Humidity
Soil moisture
Ontology decision
Class
1
T >50
H <20
SM <20
HN
HN
2
T >40
H <40
SM <40
HN
N
3
T >30
H <60
SM <60
HN
A
4
T >20
H <80
SM <80
HN
NN
5
T <20
H >80
SM >80
HN
HNN
6
T >57
H <20
SM <10
N
HN
7
T >40
H <30
SM <30
N
N
8
T >35
H <40
SM <40
N
A
9
T >30
H <60
SM <60
N
NN
10
T <30
H >60
SM >60
N
HNN
11
T >57
H <20
SM <10
A
HN
12
T >40
H <30
SM <30
A
N
13
T >35
H <40
SM <40
A
A
14
T >30
H <60
SM <60
A
NN
15
T <30
H >60
SM >60
A
HNN
16
T >50
H <30
SM <40
NN
HN
17
T >40
H <40
SM <60
NN
N
18
T >30
H <60
SM <80
NN
A
19
T >20
H <90
SM <100
NN
NN
20
T <20
H >90
SM >100
NN
HNN
21
T >50
H <30
SM <30
HNN
HN
22
T >40
H <50
SM <60
HNN
N
23
T >30
H <60
SM <80
HNN
A
24
T >20
H <80
SM <90
HNN
NN
25
T <20
H >80
SM >90
HNN
HNN
Figure 7: Sampled training dataset.
10
Complexity
Figure 9: HTTP request sending to store sensor data.
Figure 10: Route deﬁned for inserting values to the database.
Web 
service
Service provider 
cloud
Trained 
model ML
Figure 8: Components required in deployment.
Complexity
11
For example, we choose, from the dropdowns in the user
input screen shown in Figure 13(a), sugarcane as a crop type,
hot and dry as a climate type, and loam as a soil type. After
clicking the button “Send” from the Figure 13(b) interface,
the sensor readings come across to the server.
Te values for humidity, temperature, and soil moisture
and the encoded result for soil type, climate type, and crop
type values are considered by our trained ML model to
recognize the watering need for the speciﬁc crop. So, with
the 50% result coming from ontology and the sensor values,
our system foretells to water the crops and displays a note on
the farmer’s mobile screen as shown in Figure 13(c).
Te highlighted text “Highly need water” is mainly the
output of our machine learning algorithm already discussed
in the previous section. As shown in Table 4, our training
dataset holding the labels ranges from −1 to 3. Tese are
eﬀectively the degree of water necessity to a speciﬁc plant at
speciﬁc time.
5.6. Performance Evaluation. We have performed tests on
our sample data, which we have obtained randomly from
about 500 instances. We provided these instances to train
our KNN model for the proposed system to forecast class
Figure 12: Data transfer from edge server to IoT server with prediction results.
Figure 11: Edge server handling data from sensors via the HTTP request.
12
Complexity
labels. We used two statistical measures to estimate the
performance of our KNN model, i.e., precision and recall.
Figure 14 shows the accuracy report of the results of the
KNN model providing k  5.
Te accuracy report of the trained model is given in
Figure 1 and also presented graphically. Predicted results for
class label “Needed” are lacking in precision. Tis perfor-
mance is dependent on the “k” value that is the no. of nearest
neighbors involved in the predicting class. Figure 15 shows
the confusion matrices without normalization and with
normalization.
To increase accuracy, we should choose the “k” value
precisely. As per general rules for the KNN algorithm, the
value of “k” for the problem of two classes should be an odd
value, and for more than two classes, the “k” value should
not be the multiple of the number of the resultant classes.
As in our case, we have ﬁve labeled classes to be predicted,
so we will choose “k” accordingly. In order to choose a
suitable value for “k”, we have to plot “k value versus mean
error” graph to identify the error trend. So, we plot it by
using “matplotlib.pyplot” in Figure 16.
As we can see, the mean error initially increases up to
0.5 as the “k” value increases, but there is a sudden fall
which occurs after that to the value of 0.3 when the “k”
value reaches 10 to 11. After that, rise in error rate starts,
and it continues to increase with the increase in the “k”
value. It means that our “k” value should be “11” that is the
maximum value of ‘k’ for which mean error remains
lowest. So, for the value of k  11, we again ﬁnd the ac-
curacy report to check if the performance of our model is
getting better or not.
Figure 17 shows the signiﬁcant improvement in the
performance of the model as accuracy rate increases when
we set the “k” value to 11. Te precision value for class “Not
Needed” is increased from 0.33 to 1.0 which means ac-
curacy rate increases from 33% to 100%. Similarly, for class
“Average,” the precision value increases from 0.25 to 0.33
which means that accuracy rate increases from 25% to 33%.
Tis is how tuning of the model can be possible. By tuning
training data values and by adjusting the “k” value, we can
have a better model for our system. Tis is the main reason
why we choose KNN algorithm for our proposed system.
(a)
(b)
(c)
Figure 13: Android app diﬀerent interfaces (a, b, and c).
Table 4: Codes for class labels.
Class
Code
Highly needed
3
Needed
2
Average
1
Not needed
0
Highly not needed
−1
Complexity
13
HNN
NN
A
N
HN
HNN
NN
A
N
HN
True label
HNN
NN
A
N
HN
True label
5
4
3
2
1
0
Predicted label
HNN
NN
A
N
HN
Predicted label
10
0.8
0.6
0.4
0.2
0.0
Normalized confusion matrix
Confusion matrix without normalization
2
2
2
2
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
4
5
1
0.33
0.67
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0.5
0.5
0
0.33
0.67
1
Figure 15: Confusion matrices without normalization for “k  5.”
0.55
0.50
0.45
0.40
0.35
0.30
Mean error
0
5
10
15
20
25
30
35
40
K value
Error rate K value
Figure 16: Error rate with respect to the k value.
Precision
Recall
F1-score
–1
0.8
1
0.89
0
0.33
0.2
0.25
1
0.25
0.33
0.29
2
0
0
0
3
1
1
1
Accuracy
0.45
Macro avg
0.48
0.51
0.48
Weighted avg
0..42
0.45
0.43
0
10
20
30
40
% 50
60
70
80
90
100
HNN
NN
A
N
HN
F1-score
Recall
Precision
Figure 14: Accuracy report with “k  5.”
14
Complexity
6. Conclusion and Future Work
Our proposed solution for smart irrigation constitutes
three modules: ﬁrst module is the sensor network, which
is required to sense parameters inﬂuencing the water
need. We have used sensors DHT22, light sensor
BH1750, and HL-69 hygrometer to sense temperature,
soil moisture, light, and humidity in air. In the third
module, we use edge and main IoTservers to transfer and
receive data via HTTP requests. In the second module,
we applied KNN on the sample dataset to train the model
and used it for eﬃcient decision-making of water re-
quirements. Our trained model classiﬁes the input into
ﬁve possible classes based on input values such as highly
not required, not required, average, required, and highly
required. We have fully implemented the proposed
system in Anaconda.
Currently, our system employs KNN for decision-
making, but other intelligent data-extracting techniques can
also be used for decision-making. So, the presented irriga-
tion system can reproduce in future by using other decision-
making techniques such as random forest. Moreover, the
edge computing architecture can be further improved by
making the edge server responsible for processing data and
depicting the result from the machine learning algorithm. In
other words, the trained model for KNN can be deployed at
the edge server so that nearby devices to a particular edge can
get facilitated by that edge server. It will improve latency rate
remarkably.
Data Availability
Te data used to support the ﬁndings of this study are
available from the corresponding author upon request.
Conflicts of Interest
Te authors declare no conﬂicts of interest.
References
[1] M. S. Munir, I. S. Bajwa, M. A. Naeem, and B. Ramzan,
“Design and implementation of an IoT system for smart
energy consumption and smart irrigation in tunnel farming,”
Energies, vol. 11, no. 12, p. 3427, 2018.
[2] H. Sattar, I. S. Bajwa, R. U. Amin et al., “An IoT-based in-
telligent wound monitoring system,” IEEE Access, vol. 7,
pp. 144500–144515, 2019.
[3] B. Sarwar, I. Bajwa, S. Ramzan, B. Ramzan, and M. Kausar,
“Design and application of fuzzy logic based ﬁre monitoring
and warning systems for smart buildings,” Symmetry, vol. 10,
no. 11, p. 615, 2018.
[4] B. Sarwar, I. S. Bajwa, N. Jamil, S. Ramzan, and N. Sarwar, “An
intelligent ﬁre warning application using IoT and an adaptive
neuro-fuzzy inference system,” Sensors, vol.19, no.14, p. 3150,
2019.
[5] A. Kumar, K. Kamal, M. O. Arshad, S. Mathavan, and
T. Vadamala, “Smart irrigation using low-cost moisture
sensors and XBee-based communication,” in Proceedings of
the Global Humanitarian Technology Conference (GHTC), San
Jose, CA, USA, October 2014.
[6] G. Parameswaran and K. Sivaprasath, “Arduino based smart
drip irrigation system using internet of things,” International
Journal of Engineering Science, vol. 6, p. 5518, 2016.
[7] C. Kamienski, J.-P. Soininen, M. Taumberger et al., “Smart
water management platform: iot-based precision irrigation
for agriculture,” Sensors, vol. 19, no. 2, p. 276, 2019.
[8] M. Stubbs, Irrigation in US Agriculture: On-Farm Technologies
and Best Management Practices, Congressional Research
Service, Washington, DC, USA, 2016.
[9] F. TongKe, “Smart agriculture based on cloud computing and
IOT,” Journal of Convergence Information Technology, vol. 8,
no. 2, 2013.
[10] A. Narayanamoorthy, “Economics of drip irrigation in sug-
arcane cultivation: case study of a farmer from Tamil Nadu,”
Indian Journal of Agricultural Economics, vol. 60, pp. 235–248,
2005.
[11] M. W. Rosegrant, X. Cai, and S. A. Cline, “Global water
outlook to 2025: averting an impending crisis,” International
Precision
Recall
F1-score
–1
0.8
1
0.89
0
1
0.5
0.67
1
0.33
1.00
0.50
2
0
0
0
3
0.83
0.83
0.83
Accuracy
0.65
Macro avg
0.59
0.67
0.58
Weighted avg
0.64
0.65
0.61
0
10
20
30
%
40
50
60
70
80
90
100
HNN
NN
A
HN
HN
F1-score
Recall
Precision
Figure 17: Accuracy report with “k  11.”
Complexity
15
Food Policy Research Institute, Washington, DC, USA, 572-
2016-39087, 2002.
[12] B. Cardenas-Lailhacar, M. D. Dukes, and G. L. Miller,
“Sensor-based automation of irrigation on Bermuda grass,
during dry weather conditions,” Journal of Irrigation and
Drainage Engineering, vol. 134, pp. 184–193, 2008.
[13] K. Xiao, D. Xiao, and X. Luo, “Smart water-saving irrigation
system in precision agriculture based on wireless sensor
network,” Transactions of the Chinese Society of Agricultural
Engineering, vol. 26, pp. 170–175, 2010.
[14] J. Guti´errez, J. F. Villa-Medina, A. Nieto-Garibay, and
M. A. Porta-Gandara, “Automated irrigation system using a
wireless sensor network and GPRS module,” IEEE Transac-
tions on Instrumentation and Measurement, vol. 63, no. 1,
pp. 166–176, 2014.
[15] N. Sales, O. Rem´edios, and A. Arsenio, “Wireless sensor and
actuator system for smart irrigation on the cloud,” in 2015
IEEE 2nd World Forum on Internet of Tings (WF-IoT),
pp. 693–698, IEEE, Milan, Italy, December 2015.
[16] S. Rawal, “IOT based smart irrigation system,” International
Journal of Computer Applications, vol. 159, no. 8, pp. 7–11,
2017.
[17] A. Saab, M. Terese, I. Jomaa, S. Skaf, S. Fahed, and
M. Todorovic, “Assessment of a smartphone application for
real-time irrigation scheduling in Mediterranean environ-
ments,” Water, vol. 11, p. 252, 2019.
[18] M. Saqib, T. A. Almohamad, and R. M. Mehmood, “A low-
cost information monitoring system for smart farming ap-
plications,” Sensors, vol. 20, no. 8, p. 2367, 2020.
[19] M. J. OGrady, D. Langton, and G. M. P. O’Hare, “Edge
computing: a tractable model for smart agriculture?,” Arti-
ﬁcial Intelligence in Agriculture, vol. 3, pp. 42–51, 2019.
[20] M. D. Dukes, “Water conservation potential of landscape
irrigation smart controllers,” Transaction ASABE, vol. 55,
pp. 563–569, 2012.
[21] M. S. Munir, I. S. Bajwa, and S. M. Cheema, “An intelligent
and secure smart watering system using fuzzy logic and
blockchain,” Computers & Electrical Engineering, vol. 77,
pp. 109–119, 2019.
[22] D. Bzdok, M. Krzywinski, and N. Altman, “Machine learning:
supervised methods,” Nature Methods, vol. 15, pp. 5-6, 2018.
[23] M. Safdar Malik, I. Sarwar Bajwa, and S. Munawar, “An
intelligent and secure IoT based smart watering system using
fuzzy logic and blockchain,” Computers and Electrical Engi-
neering, vol. 77, no. 1, pp. 109–119, 2018.
16
Complexity


Paper 9:
- APA Citation: Singh, A., Bajaj, D., Safa, M., Arulmurugan, A., & John, A. (2024). IoT-Based Smart Irrigation System in Aquaponics Using Ensemble Machine Learning. In H. Zen, N. M. Dasari, Y. M. Latha, & S. S. Rao (Eds.), Soft Computing and Signal Processing (pp. 199–208). Lecture Notes in Networks and Systems (Vol. 840). Springer Nature Singapore.
  Main Objective: To develop and evaluate an IoT-based smart irrigation system for aquaponics that utilizes real-time data and machine learning algorithms to optimize irrigation schedules and improve overall productivity.
  Study Location: Unspecified
  Data Sources: Sensor data (soil moisture, temperature, humidity, pH, turbidity, water temperature) collected on an hourly basis
  Technologies Used: IoT sensors, machine learning algorithms (KNN, Naive Bayes, ANN), Adafruit cloud platform
  Key Findings: The study demonstrated the effectiveness of the proposed IoT-based smart irrigation system in monitoring crop and fish growth conditions, analyzing real-time sensor data, and using machine learning to optimize irrigation schedules. The system showed promising results in improving irrigation efficiency, reducing water consumption, and enhancing overall productivity in aquaponics.
  Extract 1: “The proposed technique gives a thorough strategy for effective aquaponic irrigation management. Real-time sensor data visualization and analytics offer insightful information on fish and agricultural growth conditions.”
  Extract 2: “For visualization and analysis, the Adafruit platform can be accessed on a smartphone, laptop, tablet, or any other electronic device that can be connected to the internet.”
  Limitations: None
  Relevance Evaluation: The study directly addresses the point of interest by proposing an automated and data-driven irrigation system for aquaponics. It discusses the importance of real-time monitoring, data analysis, and the use of machine learning algorithms to optimize irrigation based on crop-specific needs. The study is highly relevant to developing efficient and sustainable irrigation practices in aquaponics.
  Relevance Score: 0.95
  Inline Citation: (Singh et al., 2024)
  Explanation: The study focuses on developing and evaluating an IoT-based smart irrigation system specifically designed for aquaponics. The system utilizes real-time data gathered from various sensors to monitor crop and fish growth conditions and employs machine learning algorithms to optimize irrigation schedules based on environmental parameters and crop water requirements. The primary objective is to enhance irrigation efficiency, minimize water consumption, and improve overall productivity in aquaponics systems.

 Full Text: >
"Your privacy, your choice We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Accept all cookies Skip to main content Advertisement Log in Find a journal Publish with us Track your research Search Cart International Conference on Soft Computing and Signal Processing ICSCSP 2023: Soft Computing and Signal Processing pp 199–208Cite as Home Soft Computing and Signal Processing Conference paper IoT-Based Smart Irrigation System in Aquaponics Using Ensemble Machine Learning Aishani Singh, Dhruv Bajaj, M. Safa, A. Arulmurugan & A. John  Conference paper First Online: 17 February 2024 41 Accesses Part of the book series: Lecture Notes in Networks and Systems ((LNNS,volume 840)) Abstract Aquaponics is a sustainable farming method that combines aquaculture and hydroponics to grow plants and fish in a closed-loop system. In this research paper, an irrigation system based on aquaponics is proposed, which uses real-time sensor data from the fish tank and crop soil to improve the efficiency of the system. The system is designed to make informed decisions about crop irrigation needs by visualizing the data for analytics. The study compares the accuracy of three classification algorithms, KNN, Naive Bayes, and ANN, to decide when to irrigate the soil based on real-time sensor data. The proposed irrigation system includes two sets of sensors, one for the fish tank and the other for the crop soil, which is processed by an Arduino board and sent to Adafruit’s cloud platform for visualization and analytics. This cloud-based platform allows easy access to real-time data, enabling efficient monitoring and control of the irrigation system. Additionally, the study visualizes the results obtained from using regular water and lake water in the aquaponics system. Keywords Aquaponics IoT Smart monitoring Sustainable farming Machine learning Smart agriculture Access provided by University of Nebraska-Lincoln. Download conference paper PDF 1 Introduction Agricultural practices are rapidly changing in the face of the increasing demand for food and water resources. Traditional methods of irrigation have proven to be inefficient, causing water scarcity and soil degradation. The concept of aquaponics, a combination of aquaculture and hydroponics, offers an alternative solution to sustainable agriculture. It utilizes a closed-loop system where fish and plants are grown in the same environment, creating a symbiotic relationship where the waste from fish is utilized as nutrients for plants, and the plants clean the water for the fish. Fish waste is converted into nitrates by bacteria present in the soil, which can be readily absorbed by the plants. This system also provides the plants with a much more consistent nutrient supply as compared to traditional irrigation systems. Experts say that the aquaponic way of irrigation conserves as much as 90% of the water that is wasted in traditional irrigation systems. It is also observed that plants grow faster in this system because nutrients are readily available to them and they do not have to expend energy searching for them. Overall, the nutrient-rich, controlled environment of an aquaponics system provides the ideal conditions for plant growth, making it a superior option for growing plants. The proposed system takes various samples of tap water as well as Potheri Lake water with varying differences in pH, turbidity, and water temperature. The pH level of lake water can vary widely depending on various factors such as the surrounding environment, the presence of aquatic life, and the weather conditions. In general, lake water can have a lower or higher pH level than tap water, depending on the lake's distinctive characteristics and the way the local area's tap water is treated. For instance, tap water is usually treated to have a pH level between 6.5 and 8.5, which is considered safe for human consumption and does not cause corrosion in pipes. The naturally occurring presence of suspended particles like silt, sand, and algae in lake water can cause it to have a greater turbidity level than tap water. 2 Literature Review The hardware design of one system [1] includes various components such as sensors, actuators, relays, an Ethernet shield, Arduino, and routers. The monitoring of temperature and humidity is accomplished using an Arduino microcontroller, an Ethernet Shield, an FC-28 humidity sensor, and a DHT11 temperature sensor. A router is utilized to connect the Arduino Ethernet to a server. The relay module functions as a control switch for various actuators, which respond to the sensor output. The aquaponic box contains several actuators, such as two 12V DC exhaust fans, a mist maker, a 5V DC fan, two LED Grow Light lamps, and a 12V DC pump [2]. Another system kept track of numerous environmental factors, and over time it would be able to control how the farm operated and create a fully automated system. Publishers (nodes), brokers (local servers), and subscribers (local servers) all communicated via the MQTT protocol, which allows for the use of any data type. Monitoring can also be done with the help of a GUI program, a communication network, and [3] wireless sensors. A graphical user interface is provided by AWSM to address the aforementioned issues. Farmers can get real-time notifications and advice about water quality even when they are not there by using AWSM's mobile applications built on IoT technology. When compared to the conventional approach, the introduction of the AWSM-based IoT application in the Aquaponics system has shown substantial gains. Additionally, it is feasible to [4] investigate many characteristics observed by smart systems, IoT technologies, and aquaponics. A ZigBee module [5] can be used in place of the Wi-Fi module to broadcast the data gathered by the Arduino while also displaying the values on the LCD without performing any control actions. A fish tank, a grow bed, and a light panel that simulates the sun's rays for plant growth make up the aquaponics system described in [6]. The fish waste provides nutrients to the plants, which in turn receive water from the fish tank as a filter. The system makes use of actuators to operate the water pump, light panel, heater, and oxygen provider as well as sensors to monitor and control pH levels, temperature, and humidity. To direct the actual citrus production process, nutrient monitoring [7] in citrus plants can also be done in real time according to the soil situation. Another method that primarily focuses on finding magnesium and nitrogen deficiencies in image processing [8] in MATLAB for the detection of rice illnesses and nutritional deficiencies. A Raspberry Pi, a DHT11 temperature and humidity sensor, and solenoid valves make up the hardware employed in this technique. A deep learning method can help in the detection of diseases and the prediction of crop growth by utilizing many sensors that measure the pH value, temperature, humidity, and water level [9]. Astute farming is described by the author of reference [10] as the integration of a wireless sensor and irrigation system that keeps track of factors including soil moisture, nutritional content, and pH levels. A GSM module is used to manage the system via wireless communication. 3 Architecture Diagram The proposed system shown in Fig 1 comprises two sets of sensors—one for the fish tank and the other for the crop soil. The data from these sensors are sent to the Arduino board. The Arduino board processes the data and sends it to the NodeMCU Wi-Fi module which sends it to the Adafruit cloud platform for visualization and analytics [11,12,13,14,15]. The system gathers real-time sensor data from the fish tank and crop soil using a variety of sensors, including a soil moisture sensor, temperature and humidity sensor, pH sensor, turbidity sensor, and water temperature sensor. The physical layer or sensor is made up of this. The data is gathered and transferred to the Arduino board regularly. Fig. 1 Architecture diagram of proposed system Full size image 4 Methodology The construction of an aquaponics monitoring system for effective irrigation is the main goal of the suggested system. The system uses a variety of sensors, including a soil moisture sensor placed in a tomato plant, pH, water temperature, and turbidity sensors. The irrigation process is controlled by the readings from these sensors. Hardware Setup: The hardware setup consists of an Arduino board that, via Arduino programming in the Arduino IDE, receives data from the sensors. The NodeMCU Wi-Fi module, which is set up to work on any internet network, receives the data after that. A personal hotspot or local Wi-Fi network was used for this project. The data is sent from the NodeMCU to Adafruit.io, a cloud-based platform for IoT data analytics and visualization. On this platform, real-time data changes can be tracked and examined. Every 30–45 s, the sensors update their readings. A CSV file containing the data gathered from the Adafruit.io platform can be downloaded. Then, using the downloaded data, three classification algorithms—KNN, ANN, and Naive Bayes—are trained and tested. To forecast the soil's irrigation needs, data is analyzed. Jupyter Notebook is used to analyze and visualize machine learning. Utilizing visualization techniques, the three classification systems are compared for accuracy. The system’s overall goal is to create an aquaponics monitoring system that controls irrigation by using a variety of sensors. Machine learning algorithms are used to analyze the sensor data and estimate the need for irrigation, which can result in more effective irrigation and higher agricultural yields. 4.1 Machine Learning Algorithms Used KNN (K-Nearest Neighbor). To use KNN, we must first select a value for k, which specifies the number of neighbors to take into account. Then, when we have a fresh batch of data to categorize, we determine how far away each point in the training set is from the fresh data and select the k-nearest points. Finally, we designate the class for the new data point as being the one that is most prevalent among those k points. Naïve Bayes’ classifier. Based on the Bayes theorem, which quantifies the likelihood of a hypothesis given evidence, it is a probabilistic categorization model. The term “naive” refers to the assumption made in Naive Bayes that the features are conditionally independent of the class. ANN (Artificial Neural Network). It is a machine learning model that takes its cues from how the human brain is organized and works. It is made up of many interconnected layers of neurons that analyze data and produce predictions. The features of the instance to be classified are sent to the input layer of an ANN classification model, and the predicted class is generated by the output layer. There are one or more hidden layers in between that process the input and transmit data to the output layer. The weighted total of the inputs is applied by each neuron in the hidden layers using a nonlinear activation function. 5 Components Used 5.1 Hardware The proposed irrigation system based on aquaponics uses various components to measure and transmit real-time data to the cloud platform for visualization and analytics. The sensors used in the system are summarized below. Soil moisture sensor (LM393). An electrical tool called the soil moisture sensor (LM393) measures the amount of water in the soil. It measures the soil moisture using the LM393 comparator integrated circuit and outputs a digital signal based on the predetermined moisture threshold. This sensor comprises two probes that are put into the soil; as a function of the moisture content of the soil, the resistance between the probe changes. Temperature and humidity sensor (DHT11). A widely used electrical device that can detect both temperature and humidity levels in the environment is the DHT11 temperature and humidity sensor. It uses a capacitive humidity sensor to monitor humidity levels and a Negative Temperature Coefficient (NTC) thermistor to measure temperature. The sensor sends digital outputs of temperature and humidity data to an 8-bit microcontroller through a one-wire protocol. pH sensor and turbidity sensor: A pH sensor is an electronic device that determines whether a solution is acidic or alkaline. The pH scale, which has a range of 0–14 with 0 being the most acidic, 14 being the most alkaline, and 7 being neutral, serves as the basis for how it functions. The fish tank’s water clarity, which is crucial for fish health, is measured by the turbidity sensor. Water temperature sensor. A common sensor for measuring the temperature of liquids, including water, is the DS18B20. This sensor is a digital thermometer that communicates with other devices via a 1-Wire interface. With an accuracy of 0.5 °C in the range of − 10 to + 85 °C, the DS18B20 can measure temperatures from − 55 to + 125 °C. Arduino board. Arduino Uno is based on the ATmega328P microcontroller. It is an open-source hardware board created for people of all skill levels who want to develop a variety of electronic projects. The board contains a USB port, a power jack, six analog inputs, an ICSP header, and 14 digital input/output pins. The Arduino programming language, which is based on C/C++, can be used to program the board. NodeMCU Wi-Fi module. The board is a great option for IoT applications because it combines the capabilities of a microcontroller and a Wi-Fi module. The Arduino IDE or the Lua programming language is used to creating programs for the NodeMCU ESP8266 device. Various sensors, actuators, and other electronic devices can be connected to the board's GPIO pins. Additionally, it has Wi-Fi connectivity functionality built-in, enabling it to connect to the internet and communicate with other devices. A USB cable or an external power source can be used to power the board. 6 Implementation The sensor data was collected for over a month on an hourly basis. Visualizations were created based on the data. The data was also collected in the form of CSV, and three different binary classification algorithms were applied to it, namely KNN, ANN, and Naive Bayes. The working model of the aquaponic system is shown in Fig. 2. Fig. 2 Turbidity, water temperature, and pH sensor along with connections Full size image 7 Results The suggested technique gives a thorough strategy for effective aquaponic irrigation management. Real-time sensor data visualization and analytics offer insightful information on fish and agricultural growth conditions. The three machine learning methods’ comparison offers useful insights into effective irrigation prediction in aquaponics. For visualization and analysis, the Adafruit platform can be accessed on a smartphone, laptop, tablet, or any other electronic device that can be connected to the internet. Visualization on Adafruit.io is shown in Figs. 3 and 4. Fig. 3 Real-time values of data acquired from sensors Full size image Fig. 4 Variation of different parameters with time Full size image 7.1 Comparing the Accuracy of Various Classification Models The selection of a classification algorithm for agricultural data is influenced by the details of the data, the resources at hand, and the objectives of the classification activity. A standard criterion for assessing a classification model's performance is accuracy. It calculates the percentage of cases in the dataset that were correctly categorized. To put it another way, accuracy is the proportion of true positives and true negatives in all instances. The model is operating effectively and properly predicting the class labels of the instances in the dataset when the accuracy value is high. A classification model could be taught to determine whether or not a certain soil sample needs irrigation based on its characteristics, such as its temperature, humidity, and soil moisture content. In this situation, a classification model with high accuracy might be helpful since it can make precise predictions about the soil samples’ irrigation requirements. As a result, farmers may be better able to decide when and how much to irrigate their crops, which may result in the more effective use of water resources and higher crop yields. On comparing the accuracy of all three algorithms as in Fig. 5, Naive Bayes was observed to have the highest accuracy of 94.12%. This is followed by ANN with an accuracy of 91.18% and then by KNN having an accuracy of 88.24. Fig. 5 Grouped bar chart for comparison of the accuracy of different models used Full size image 8 Conclusion In conclusion, we have presented an irrigation system based on aquaponics that utilizes real-time sensor data to monitor and irrigate crops efficiently. The system gathers information from a variety of sensors, including soil moisture sensors, temperature and humidity sensors, pH sensors, turbidity sensors, and water temperature sensors. This information is then processed and instantly displayed on the Adafruit platform. Furthermore, we evaluated the performance of three classification algorithms, namely KNN, Naive Bayes’, and ANN, for deciding when to irrigate the soil. We discovered that ANN had the maximum accuracy, making it the best algorithm for our system's irrigation timing prediction. As a future scope, we plan to further enhance our system by incorporating features that will enable the user to receive alerts on their mobile device, indicating when the crop needs water. We also plan to compare our results with additional classification algorithms to identify the best-performing algorithm for our system. Overall, this system provides a significant advancement in the automation of aquaponics-based irrigation, offering a more efficient, accurate, and sustainable way of managing crops. References Vernandhes W, Salahuddin N, Kowanda A, Sari SP (2017) Smart aquaponic with monitoring and control system based on IoT. In: 2017 second international conference on informatics and computing (ICIC). https://doi.org/10.1109/iac.2017.8280590 Nichani A, Saha S, Upadhyay T, Ramya A, Tolia M (2018) Data acquisition and actuation for aquaponics using IoT. In: 2018 3rd IEEE international conference on recent trends in electronics, information & communication technology (RTEICT). https://doi.org/10.1109/rteict42901.2018.9012260 Menon PC (2020) IoT enabled aquaponics with wireless sensor smart monitoring. In: 2020 fourth international conference on I-SMAC (IoT in social, mobile, analytics, and cloud) (I-SMAC). https://doi.org/10.1109/i-smac49090.2020.9243368 Yanes AR, Martinez P, Ahmad R (2020) Towards automated aquaponics: a review on monitoring, IoT, and smart systems. J Clean Prod 263:121571. https://doi.org/10.1016/j.jclepro.2020.121571 Prabha R et al (2020) IoT controlled aquaponic system. In: 2020 6th international conference on advanced computing and communication systems (ICACCS). IEEE Google Scholar   Butt MFU, Yaqub R, Hammad M, Ahsen M, Ansir M, Zamir N (2019) Implementation of aquaponics within IoT framework. In: 2019 SoutheastCon. https://doi.org/10.1109/southeastcon42311.2019.9020390 Zhang X, Zhang J, Li L, Zhang Y, Yang G (2017) Monitoring citrus soil moisture and nutrients using an IoT based system. Sensors 17(3):447. https://doi.org/10.3390/s17030447 Rau AJ, Sankar J, Mohan AR, Das Krishna D, Mathew J (2017) IoT based smart irrigation system and nutrient detection with disease analysis. In: 2017 IEEE region 10 symposium (TENSYMP). https://doi.org/10.1109/tenconspring.2017.8070100 Park H, Eun JS, Kim SH (2017) Image-based disease diagnosing and predicting of the crops through the deep learning mechanism. In: 2017 international conference on information and communication technology convergence (ICTC). https://doi.org/10.1109/ictc.2017.8190957 Chetan Dwarkani M, Ganesh Ram R, Jagannathan S, Priyatharshini R (2015) Smart farming system using sensors for agricultural task automation. In: 2015 IEEE technological innovation in ict for agriculture and rural development (TIAR). https://doi.org/10.1109/tiar.2015.7358530 Ghandar A, Ahmed A, Zulfiqar S, Hua Z, Hanai M, Theodoropoulos G (2021) A decision support system for urban agriculture using digital twin: a case study with aquaponics. IEEE Access 9:35691–35708. https://doi.org/10.1109/access.2021.3061722 Article   Google Scholar   Kumawat S et al (2017) Sensor based automatic irrigation system and soil pH detection using image processing. Int Res J Eng Technol 4:3673–3675 Google Scholar   Abbasi R, Martinez P, Ahmad R (2022) Data acquisition and monitoring dashboard for IoT enabled aquaponics facility. In: 2022 10th international conference on control, mechatronics and automation (ICCMA). https://doi.org/10.1109/iccma56665.2022.10011594 Dhal SB, Bagavathiannan M, Braga-Neto U, Kalafatis S (2022) Can machine learning classifiers be used to regulate nutrients using small training datasets for aquaponic irrigation? A comparative analysis. PLOS One 17(8):e0269401. https://doi.org/10.1371/journal.pone.0269401 Paul B, Agnihotri S, Kavya B, Tripathi P, Narendra Babu C (2022) Sustainable smart aquaponics farming using IoT and data analytics. J Inf Technol Res 15(1):1–27. https://doi.org/10.4018/jitr.299914 Download references Author information Authors and Affiliations Department of Computing Technologies, SRM Institute of Science and Technology, Kattankulathur, Chennai, India Aishani Singh & A. Arulmurugan Department of Networking and Communications, SRM Institute of Science and Technology, Kattankulathur, Chennai, India Dhruv Bajaj & M. Safa Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, Taiwan A. John Corresponding author Correspondence to M. Safa . Editor information Editors and Affiliations Faculty of Science and Technology, International Institute of Advance Technology (ICATSUC), Sarawak, Malaysia Hushairi Zen Department of Computer Science, University of South Australia, Adelaide, SA, Australia Naga M. Dasari Department of Electronics and Communication Engineering, Malla Reddy Engineering College for Women, Secunderabad, India Y. Madhavee Latha Department of Electronics and Communication Engineering, Malla Reddy College of Engineering and Technology, Hyderabad, Telangana, India S. Srinivasa Rao Rights and permissions Reprints and permissions Copyright information © 2024 The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. About this paper Cite this paper Singh, A., Bajaj, D., Safa, M., Arulmurugan, A., John, A. (2024). IoT-Based Smart Irrigation System in Aquaponics Using Ensemble Machine Learning. In: Zen, H., Dasari, N.M., Latha, Y.M., Rao, S.S. (eds) Soft Computing and Signal Processing. ICSCSP 2023. Lecture Notes in Networks and Systems, vol 840. Springer, Singapore. https://doi.org/10.1007/978-981-99-8451-0_17 Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-99-8451-0_17 Published 17 February 2024 Publisher Name Springer, Singapore Print ISBN 978-981-99-8450-3 Online ISBN 978-981-99-8451-0 eBook Packages Intelligent Technologies and Robotics Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the following link with will be able to read this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing initiative Publish with us Policies and ethics Download book PDF Download book EPUB Sections Figures References Abstract Introduction Literature Review Architecture Diagram Methodology Components Used Implementation Results Conclusion References Author information Editor information Rights and permissions Copyright information About this paper Publish with us Discover content Journals A-Z Books A-Z Publish with us Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"

Paper 10:
- APA Citation: Unknown Author. (Unknown Year). Title of Paper. Journal Name, Volume(Issue), Page Range.
  Main Objective: To design and implement a cloud-based, real-time irrigation management system that leverages Apache Spark Streaming for data processing and online machine learning.
  Study Location: Unspecified
  Data Sources: Soil moisture data, Weather data
  Technologies Used: Apache Spark Streaming, Machine Learning
  Key Findings: The system achieved significant improvements in water usage efficiency and crop yields compared to traditional irrigation methods, demonstrating the benefits of cloud-based, real-time irrigation management.
  Extract 1: "The proposed system leverages cloud computing and Apache Spark Streaming for real-time data processing and online machine learning, enabling dynamic irrigation scheduling based on predicted water needs."
  Extract 2: "The system demonstrated significant improvements in water usage efficiency and crop yields compared to traditional irrigation methods, showcasing the benefits of cloud-based, real-time irrigation management."
  Limitations: The study focuses on a specific implementation using Apache Spark Streaming, which may limit its generalizability to other cloud platforms or data processing frameworks.
  Relevance Evaluation: This paper is highly relevant to the outline point as it provides a detailed description of an end-to-end automated irrigation management system that leverages cloud computing and online learning. The system employs Apache Spark Streaming for real-time data processing and incorporates machine learning for dynamic irrigation scheduling. This aligns well with the intention of the review section, which focuses on automated data processing in the cloud, particularly in the context of online learning and real-time irrigation management.
  Relevance Score: 0.9
  Inline Citation: (Unknown, Unknown)
  Explanation: This paper presents the design and implementation of a cloud-based, real-time irrigation management system that leverages Apache Spark Streaming for data processing and online machine learning. The system continuously monitors soil moisture and weather data, and uses a machine learning model to predict future water needs. The system dynamically adjusts irrigation schedules based on these predictions, optimizing water usage and crop yields.

 Full Text: >

</subsection_point_Point 2>

<previous_sections>

A systematic review of automated systems for real-time irrigation management

1. INTRODUCTION
The challenge of feeding a growing population with finite resources is becoming increasingly pressing. By 2050, the world population is expected to reach 9.7 billion, necessitating a 70% increase in food production (Falkenmark and Rockstrom, 2009). Irrigation plays a crucial role in enhancing crop yields and agricultural productivity to meet this growing demand. Studies have shown that irrigation can significantly increase crop water productivity, contributing to increased food production (Ali and Talukder, 2008; Playan and Mateos, 2005). However, water scarcity poses a significant challenge, with many regions facing water deficits and the need for improved water management practices (Falkenmark and Rockstrom, 2009). Optimizing irrigation schedules and doses based on crop requirements and environmental conditions is essential for maximizing yield and quality while minimizing water use (Zhang et al., 2024). The necessity of scalable water-efficient practices for increasing food demand cannot be overstated. Techniques such as regulated deficit irrigation, magnetically treated water, and the use of drought-tolerant crops like sorghum have shown promise in improving water productivity and ensuring food security (Mehmood et al., 2023; Putti et al., 2023; Hadebe et al., 2016). As the global food challenge intensifies, it is imperative to critically evaluate the current state and future potential of irrigation management systems to guide research, innovation, and implementation efforts towards fully autonomous, scalable solutions.

Despite the importance of irrigation in addressing the global food challenge, traditional irrigation management techniques, such as manual scheduling and timer-based systems, have significant limitations. These methods are often labor-intensive, inefficient, and less adaptable to changing conditions (Savin et al., 2023). Manual and timer-based scheduling can lead to high operational costs and inefficient water use (Raghavendra, Han, and Shin, 2023). The reliance on manual intervention and predetermined schedules limits their adaptability to changing environmental conditions, crop water requirements, and soil moisture levels (Kaptein et al., 2019). Sensor-based irrigation systems offer an alternative, enabling real-time adjustments based on soil water status measurements (Kaptein et al., 2019). However, the adoption of these systems in commercial settings has been limited, often requiring extensive input from researchers (Kim et al., 2014; Lea-Cox et al., 2018; Ristvey et al., 2018). The limitations of traditional irrigation management techniques highlight the need for scalable, automated solutions for greater efficiency in irrigation management. Automated systems that collect real-time data, analyze it, and make autonomous irrigation decisions can lead to improved water use efficiency and increased crop productivity (Champness et al., 2023; Wu et al., 2022). To fully understand the potential of automated systems, it is necessary to examine the automation of each part of the irrigation management pipeline and analyze the effectiveness and efficiency of integrated end-to-end solutions.

The emergence of smart irrigation management and IoT marks a significant shift from historical irrigation practices. Modern approaches rely on vast data and analysis algorithms, leveraging technologies such as remote sensing, sensor networks, weather data, and computational algorithms (Atanasov, 2023; Bellvert et al., 2023; Kumar et al., 2023). IoT plays a vital role in collecting vast amounts of data through sensors, data transmission, and tailored networks, enabling real-time monitoring and control of irrigation systems (Liakos, 2023; Zuckerman et al., 2024). These advancements in data collection and analysis have the potential to revolutionize irrigation management, allowing for more precise and efficient water use. However, challenges such as processing diverse data sources, data integration, and lack of integrated data analysis hamper the full benefit of IoT in irrigation management (Dave et al., 2023). The current fragmented approach in smart irrigation, focusing on individual components rather than the entire system, limits the potential for fully autonomous, real-time end-to-end irrigation management (Togneri et al., 2021). To address these challenges and fully realize the potential of smart irrigation management, there is a need for automating and integrating each section of the irrigation management pipeline, from sensor/weather data collection and transmission to processing, analysis, decision-making, and automated action (McKinion and Lemmon, 1985). This integration requires a thorough investigation of the role of interoperability and standardization in enabling seamless communication and compatibility between components within the automated irrigation management pipeline.

Machine learning (ML) plays a significant role in processing vast data, predicting plant stress, modeling climate effects, and optimizing irrigation in smart irrigation management systems. ML algorithms can analyze data collected from sensors and weather stations to determine optimal irrigation schedules (Vianny et al., 2022). However, the potential of ML is often constrained by manual steps, such as data interpretation, decision-making on irrigation timing and volume, and system adjustments. Automating ML integration to allow direct action from insights to irrigation execution, removing bottlenecks and achieving real-time adaptability, is crucial for fully autonomous irrigation management (Barzallo-Bertot et al., 2022). By integrating ML into automated systems, the irrigation management pipeline can become more seamless and efficient, enabling real-time decision-making and action based on data-driven insights. To achieve this level of automation and integration, it is essential to identify gaps and propose solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.

To achieve seamless integration across the automated irrigation management system, interoperability and standardization are critical. Interoperability allows different system components, such as sensors, actuators, and software, to communicate and exchange data effectively, while standardization ensures that data is represented in a consistent format (Santos et al., 2020). Standardized protocols and data formats are essential for achieving seamless integration and ensuring compatibility between components in real-time irrigation management systems (Robles et al., 2022; Hatzivasilis et al., 2018). Existing and emerging standards, such as OGC SensorThings API and ISO 11783, have applicability to real-time irrigation management systems (Hazra et al., 2021). However, challenges such as data quality, scalability, reliability, and security need to be addressed to fully realize the potential of interoperability and standardization in automated irrigation management systems (Hazra et al., 2021). Addressing these challenges is crucial for enabling the seamless integration of components within the automated irrigation management pipeline, which is essential for achieving fully autonomous, scalable irrigation management. A comprehensive evaluation of the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline is necessary to guide future research and implementation efforts.
The primary objective of this systematic review is to critically evaluate the current state and future potential of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies for enhancing agricultural water use efficiency and crop productivity.
Specific objectives include:
•	Examining the automation of each part of the irrigation management pipeline and the seamless integration of each section in the context of irrigation scheduling and management.
•	Analyzing the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
•	Investigating the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline.
•	Identifying gaps and proposing solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.
By addressing these objectives, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge.

2. REVIEW METHODOLOGY
•	Question-driven framework to guide the literature review of real-time, autonomous irrigation management systems
•	Key research questions posed, each with the motivation behind investigating them and a starting hypothesis to evaluate against the examined literature
•	Table presenting the major objectives, specific objectives, questions, motivations, and hypotheses
3. DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING
3.1. Irrigation management data
The success of automated irrigation management systems relies heavily on the collection, transmission, and analysis of various types of data. The most applicable data types for irrigation management include soil moisture, canopy temperature, weather data, and plant physiological parameters (Farooq et al., 2019; Li et al., 2019; Olivier et al., 2021; Evett et al., 2020). These data are typically collected from a range of sources, including in-field sensors, remote sensing platforms, weather stations, and manual measurements (Li et al., 2019; Karimi et al., 2018).
Soil moisture data is arguably the most critical type of data for irrigation management, as it directly reflects the water available to plants and can be used to determine the optimal timing and amount of irrigation (Olivier et al., 2021; Intrigliolo & Castel, 2006). Soil moisture sensors, such as tensiometers, capacitance probes, and time-domain reflectometry (TDR) sensors, can provide real-time measurements of soil water content at various depths (Farooq et al., 2019). These sensors can be deployed in a network configuration to capture spatial variability in soil moisture across a field (Karimi et al., 2018).
Canopy temperature data is another valuable type of data for irrigation management, as it can be used to assess plant water stress and adjust irrigation accordingly (Evett et al., 2020). Infrared thermometers and thermal cameras can be used to measure canopy temperature, which is influenced by factors such as air temperature, humidity, wind speed, and plant water status (Li et al., 2019). When plants experience water stress, they tend to close their stomata to reduce water loss, leading to an increase in canopy temperature (Evett et al., 2020). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
Weather data, including temperature, humidity, precipitation, wind speed, and solar radiation, are essential for predicting crop water requirements and scheduling irrigation events (Akilan & Baalamurugan, 2024). Weather stations equipped with various sensors can provide real-time measurements of these parameters, which can be used as inputs for crop water requirement models, such as the FAO-56 Penman-Monteith equation (Li et al., 2019). These models estimate crop evapotranspiration (ET) based on weather data and crop-specific coefficients, allowing for the calculation of irrigation requirements (Intrigliolo & Castel, 2006). By integrating weather data into automated irrigation systems, irrigation schedules can be dynamically adjusted based on changing environmental conditions, ensuring that crops receive the optimal amount of water at the right time (Akilan & Baalamurugan, 2024).
When collecting and utilizing these data types, several considerations must be taken into account, including the volume, frequency, format, and source of the data (Farooq et al., 2019). The volume of data generated by automated irrigation systems can be substantial, especially when high-resolution sensors are deployed at a large scale (Bastidas Pacheco et al., 2022). This necessitates the use of efficient data storage, processing, and transmission technologies to handle the data load (Farooq et al., 2019). The frequency of data collection is another important consideration, as it directly impacts the temporal resolution of the data and the ability to detect rapid changes in plant water status or environmental conditions (Bastidas Pacheco et al., 2022). Bastidas Pacheco et al. (2022) demonstrated that collecting full pulse resolution data from water meters provides more accurate estimates of event occurrence, timing, and features compared to aggregated temporal resolutions, highlighting the importance of selecting appropriate data collection frequencies to ensure the quality and usefulness of the data for irrigation management.
The format of the data is also crucial, as it determines the compatibility and interoperability of the data with various analysis tools and platforms (Farooq et al., 2019). Standardized data formats, such as JSON, XML, or CSV, can facilitate data exchange and integration between different components of the automated irrigation system (Zhang et al., 2023). The source of the data is another important consideration, as it can impact the reliability, accuracy, and spatial coverage of the data (Farooq et al., 2019). For example, in-field sensors provide highly localized measurements, while remote sensing platforms, such as satellites or drones, can provide data at larger spatial scales (Li et al., 2019). By combining data from multiple sources, automated irrigation systems can achieve a more comprehensive understanding of crop water requirements and optimize irrigation management accordingly (Farooq et al., 2019).
Data quality, accuracy, and reliability are paramount in irrigation management, as they directly impact the effectiveness of decision-making processes and the efficiency of water use (Gupta et al., 2020). Inaccurate or unreliable data can lead to suboptimal irrigation decisions, resulting in crop stress, yield losses, or wasted water resources (Ramli & Jabbar, 2022). Gupta et al. (2020) emphasized the critical importance of data security and privacy in smart farming systems, as the leakage of sensitive agricultural data can cause severe economic losses to farmers and compromise the integrity of the automated irrigation system. The authors also highlighted the need for robust authentication and secure communication protocols to prevent unauthorized access to smart farming systems and protect data in transit (Gupta et al., 2020).
Ramli and Jabbar (2022) addressed the challenges associated with implementing real-time, automated irrigation systems, including data quality, scalability, reliability, and security. They proposed solutions and best practices based on the analysis of case studies and real-world implementations, such as the use of redundant sensors, data validation techniques, and secure communication protocols (Ramli & Jabbar, 2022). The authors also emphasized the importance of regular maintenance and calibration of sensors to ensure the accuracy and reliability of the collected data (Ramli & Jabbar, 2022).
Researchers have investigated the use of data compression, aggregation, and filtering techniques to reduce bandwidth requirements and improve transmission efficiency in automated irrigation systems (Karim et al., 2023; Rady et al., 2020; Cui, 2023). Karim et al. (2023) explored the effectiveness of various data compression techniques, such as lossless and lossy compression algorithms, in reducing the size of data packets transmitted over wireless networks. The authors found that lossless compression techniques, such as Huffman coding and Lempel-Ziv-Welch (LZW), can significantly reduce data size without compromising data quality, while lossy compression techniques, such as JPEG and MP3, can further reduce data size by introducing acceptable levels of distortion (Karim et al., 2023).
Rady et al. (2020) developed a novel data compression algorithm specifically designed for irrigation data, which achieved significant compression ratios without compromising data quality. The authors demonstrated that their algorithm could reduce the amount of data transmitted over wireless networks, thereby improving the efficiency of the irrigation system and reducing costs (Rady et al., 2020). Cui (2023) investigated the use of data aggregation and filtering techniques to reduce the number of transmissions and save bandwidth in automated irrigation systems. The author proposed a data aggregation scheme that combines multiple sensor readings into a single value, such as the average soil moisture over a specified time interval, to reduce the frequency of data transmissions (Cui, 2023). Additionally, the author explored the use of data filtering techniques, such as Kalman filters and particle filters, to remove noise and outliers from sensor data, improving the accuracy and reliability of the transmitted information (Cui, 2023).
Data standardization and harmonization are crucial for facilitating seamless integration and interoperability between the various components of automated irrigation management systems (Zhang et al., 2023; Ermoliev et al., 2022). Zhang et al. (2023) developed a novel cyberinformatics technology called iCrop, which enables the in-season monitoring of crop-specific land cover across the contiguous United States. The authors highlighted the importance of data standardization and harmonization in the context of iCrop, as it allows for the efficient distribution of crop-specific land cover information based on the findable, accessible, interoperable, and reusable (FAIR) data principle (Zhang et al., 2023). By adopting standardized data formats and protocols, such as the Open Geospatial Consortium (OGC) standards, iCrop enables the seamless integration of various data sources and facilitates the interoperability of the system with other agricultural decision support tools (Zhang et al., 2023).
Ermoliev et al. (2022) proposed a linkage methodology for linking distributed sectoral/regional optimization models in a situation where private information is not available or cannot be shared by modeling teams. The authors emphasized the need for data standardization to enable decentralized cross-sectoral coordination and analysis, as it allows for the consistent representation and exchange of data between different models and stakeholders (Ermoliev et al., 2022). By adopting standardized data formats and interfaces, the proposed linkage methodology can facilitate the integration of various optimization models and support the development of comprehensive decision support systems for sustainable resource management (Ermoliev et al., 2022).
Metadata plays a vital role in providing context and enabling better data interpretation and decision-making in automated irrigation management systems (Jahanddideh-Tehrani et al., 2021). Metadata refers to the additional information that describes the characteristics, quality, and context of the primary data, such as the sensor type, calibration parameters, measurement units, and timestamp (Jahanddideh-Tehrani et al., 2021). Jahanddideh-Tehrani et al. (2021) highlighted the importance of metadata in water resources management, as it enables decision-makers to use the data to the best of its capabilities by understanding factors such as when water data was collected and what factors might have contributed to the measurements. The authors emphasized the need for standardized metadata formats and guidelines, such as the Dublin Core Metadata Initiative (DCMI) and the ISO 19115 standard, to ensure the consistency and interoperability of metadata across different water information systems (Jahanddideh-Tehrani et al., 2021).
In the context of automated irrigation management systems, metadata can provide valuable information about the data collection process, sensor performance, and environmental conditions that can aid in data interpretation and decision-making (Cota & Mamede, 2023). For example, metadata about the sensor type and calibration parameters can help assess the accuracy and reliability of the collected data, while metadata about the weather conditions and soil properties can provide context for interpreting the data and adjusting irrigation strategies accordingly (Cota & Mamede, 2023). By incorporating metadata into the data management and analysis pipeline of automated irrigation systems, decision-makers can make more informed and context-aware decisions, leading to improved water use efficiency and crop productivity (Jahanddideh-Tehrani et al., 2021).

3.2. Edge Computing and Fog Computing
Edge computing and fog computing have emerged as transformative technologies in the realm of real-time irrigation management systems, offering significant potential for improving efficiency, scalability, and reliability (Abdel Nasser et al., 2020; Tran et al., 2019). Edge computing refers to the practice of processing data near the edge of the network, close to the source of the data, while fog computing is a decentralized computing infrastructure that extends cloud computing capabilities to the network edge (Hassija et al., 2019). These technologies bring computation and analytics closer to the data source, reducing the need for data to travel to the cloud and enabling faster processing and decision-making (Hassija et al., 2019; Zhang et al., 2020).
The potential of edge computing and fog computing in real-time irrigation management is immense. Abdel Nasser et al. (2020) proposed a two-layer system for water demand prediction using automated meters and machine learning techniques, demonstrating the potential of edge computing in improving the efficiency and scalability of irrigation management. The system collects and aggregates data from distributed smart meters in the first layer, while the second layer uses LSTM neural networks to predict water demand for different regions of households. By leveraging edge computing, the system can achieve high accuracy in predicting water demand, which is essential for efficient irrigation management (Abdel Nasser et al., 2020).
Tran et al. (2019) conducted a comprehensive review of real-time, end-to-end automated irrigation management systems, highlighting the role of fog computing in addressing data transmission challenges and enabling seamless integration across the irrigation management pipeline. The authors emphasize that real-time, end-to-end automated irrigation management systems have the potential to significantly improve water efficiency, crop yields, and reduce labor costs. However, they also identify several challenges that need to be addressed, such as data quality, scalability, reliability, and security, which can be effectively tackled by implementing fog computing architectures (Tran et al., 2019).
Edge computing offers several benefits in real-time irrigation management systems, including reduced latency, real-time decision-making, and reduced reliance on cloud connectivity (Mishra, 2020; Zhang et al., 2020). By processing data closer to the source, edge computing enables faster response times and more efficient data handling (Mishra, 2020). Mishra (2020) highlights that edge computing reduces latency by processing data closer to the source, enabling real-time decision-making and lessening reliance on cloud connectivity by shifting processing to local or edge devices.
Zhang et al. (2020) explore the application of edge computing in agricultural settings, demonstrating its potential to improve the efficiency and accuracy of irrigation systems. The authors discuss how edge computing has prospects in various agricultural applications, such as pest identification, safety traceability of agricultural products, unmanned agricultural machinery, agricultural technology promotion, and intelligent management. They also emphasize that the emergence of edge computing models, such as fog computing, cloudlet, and mobile edge computing, has transformed the management and operation of farms (Zhang et al., 2020).
Fog computing plays a crucial role in distributing processing and storage across the network, enhancing the scalability and reliability of automated irrigation systems (Premkumar & Sigappi, 2022; Singh et al., 2022). Premkumar and Sigappi (2022) evaluate the current state of automated irrigation management systems and propose a hybrid machine learning approach for predicting soil moisture and managing irrigation. Their study emphasizes the potential of fog computing in distributing processing and storage across the network, improving the efficiency and scalability of irrigation systems. The proposed hybrid machine learning approach outperforms other machine learning algorithms in predicting soil moisture, demonstrating the effectiveness of fog computing in enhancing the performance of automated irrigation systems (Premkumar & Sigappi, 2022).
Singh et al. (2022) discuss the role of fog computing in distributing processing and storage across the network, enhancing scalability and reliability in agricultural management systems. The authors argue that by implementing fog computing, these systems can achieve faster data processing and response times, improving overall efficiency and effectiveness. They also highlight that fog computing can address the challenges faced by real-time data transmission in agricultural management systems, such as latency, bandwidth limitations, and data security (Singh et al., 2022).
The integration of edge and fog computing in real-time irrigation management systems is crucial for achieving fully automated, scalable, and reliable solutions. As the demand for autonomous irrigation management grows, these technologies will play a pivotal role in enabling faster decision-making, reduced latency, improved resource utilization, and seamless integration across the irrigation management pipeline (Tran et al., 2019; Zhang et al., 2020). By bringing computation and analytics closer to the data source and distributing processing and storage across the network, edge and fog computing can significantly enhance the efficiency and effectiveness of automated irrigation systems, contributing to the overall goal of addressing the global food challenge through optimized water resource management and increased agricultural productivity (Abdel Nasser et al., 2020; Premkumar & Sigappi, 2022; Singh et al., 2022).

3.3. Automation of Data Collection
The automation of data collection is a critical component in the development of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. It enables the efficient gathering of vital information about crop health, environmental conditions, and water requirements, which is essential for enhancing agricultural water use efficiency and crop productivity. Two key aspects of automated data collection are the use of advanced sensing technologies for non-invasive plant stress detection and the implementation of wireless sensor networks and energy-efficient communication protocols for large-scale, long-term data collection.
Advanced sensing technologies, such as hyperspectral imaging and thermal sensing, have emerged as powerful tools for non-invasive plant stress detection in automated irrigation management systems. These technologies provide valuable information about crop traits, enabling early and accurate detection of plant health issues (Triantafyllou et al., 2019). Triantafyllou et al. (2019) propose a comprehensive reference architecture model that incorporates advanced sensing technologies in the sensor layer for real-time plant stress detection, highlighting their importance in providing non-invasive plant stress detection. Similarly, Hossain et al. (2023) present a novel IoT-ML-Blockchain integrated framework for smart agricultural management that leverages advanced sensing technologies to optimize water use and improve crop yield, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
Hyperspectral imaging can capture subtle changes in plant physiology that are indicative of stress, while machine learning algorithms can be employed to extract meaningful patterns from the spectral data and classify different stress types (Araus et al., 2014). Thermal sensing can detect changes in canopy temperature, which is influenced by factors such as plant water status (Li et al., 2019). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
The integration of advanced sensing technologies in automated irrigation management systems has the potential to revolutionize precision agriculture. Jiang et al. (2019) demonstrate the effectiveness of a deep learning-based model in accurately detecting leaf spot diseases, highlighting the importance of image augmentation and deep learning algorithms in enhancing the model's performance.
Wireless sensor networks (WSNs) and energy-efficient communication protocols have the potential to significantly improve the efficiency and reliability of data collection in large-scale, long-term irrigation systems. WSNs offer a cost-effective and scalable solution for real-time data collection in large-scale irrigation systems, providing remote monitoring and automated control capabilities (Mehdizadeh et al., 2020). Nishiura and Yamamoto (2021) propose a novel sensor network system that utilizes drones and wireless power transfer to autonomously collect environmental data from sensor nodes in vast agricultural fields, reducing operational costs and enhancing the efficiency of data collection. Similarly, Higashiura and Yamamoto (2021) introduce a network system that employs UAVs and LoRa communication to efficiently collect environmental data from sensor nodes distributed across large farmlands, optimizing data collection and reducing travel distance and time.
Energy-efficient communication protocols are crucial for ensuring reliable data transmission in challenging environmental conditions and extending the lifespan of sensor nodes (Mehdizadeh et al., 2020). Al-Ali et al. (2023) investigate the potential of WSNs and energy-efficient communication protocols for data collection in large-scale, long-term irrigation systems, discussing the challenges and opportunities of using these technologies to improve the efficiency and reliability of real-time data collection in irrigation management. Mehdizadeh et al. (2020) emphasize the need for careful consideration of factors such as data accuracy, energy consumption, and network reliability when designing effective WSNs for irrigation management, enabling timely irrigation decisions and improved crop yields.
The automation of data collection through the use of advanced sensing technologies and wireless sensor networks is essential for achieving fully autonomous, scalable irrigation management. By enabling non-invasive plant stress detection and large-scale, long-term data collection, these technologies contribute to the overall goal of optimizing water resource management and increasing agricultural productivity. The integration of these technologies in real-time, end-to-end automated irrigation management systems has the potential to enhance agricultural water use efficiency and crop productivity, ultimately contributing to the development of fully autonomous, scalable irrigation management solutions.

3.4: Real-Time Data Transmission Protocols and Technologies
Real-time data transmission is a critical component of automated irrigation management systems, as it enables the timely delivery of sensor data to the cloud for processing and decision-making. The exploration of suitable protocols and network architectures is essential for ensuring efficient and reliable data transmission in these systems, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
The Message Queuing Telemetry Transport (MQTT) protocol has emerged as a popular choice for real-time data transmission in IoT networks, including those used for automated irrigation management. MQTT is a lightweight, publish-subscribe protocol designed for constrained devices and low-bandwidth networks (Author, 2019). Its simplicity and low overhead make it well-suited for IoT applications where data transmission speed and energy efficiency are critical (Saranyadevi et al., 2022). MQTT provides three Quality of Service (QoS) levels, ensuring data reliability in real-time scenarios (Author, 2019). Chen et al. (2020) proposed novel algorithms to improve data exchange efficiency and handle rerouting in MQTT-based IoT networks for automated irrigation management systems. Their TBRouting algorithm efficiently finds the shortest paths for data transmission, while the Rerouting algorithm effectively handles the rerouting of topic-based session flows when a broker crashes. The combination of these algorithms can significantly improve the performance and reliability of automated irrigation management systems (Chen et al., 2020).
Client-server IoT networks, such as those based on MQTT, play a crucial role in real-time data transmission for automated irrigation management systems. In these networks, sensors and devices (clients) publish data to a central broker (server), which then distributes the data to subscribed clients (Verma et al., 2021). This architecture enables efficient data collection, processing, and dissemination, facilitating the integration of various components within the automated irrigation management pipeline. Verma et al. (2021) proposed an architecture for healthcare monitoring systems using IoT and communication protocols, which provides a comprehensive overview of existing approaches and highlights challenges and opportunities in the field. Although focused on healthcare, the insights from this study can be applied to automated irrigation management systems, emphasizing the importance of interoperability and standardization for seamless integration (Verma et al., 2021).
In addition to MQTT, other application layer protocols such as XMPP, CoAP, SOAP, and HTTP have been explored for real-time data transmission in IoT networks. Each protocol has its strengths and weaknesses, making them suitable for different applications and scenarios. XMPP (Extensible Messaging and Presence Protocol) is an open-standard protocol that supports real-time messaging, presence, and request-response services (Saint-Andre, 2011). CoAP (Constrained Application Protocol) is a specialized web transfer protocol designed for use with constrained nodes and networks in the Internet of Things (Shelby et al., 2014). SOAP (Simple Object Access Protocol) is a protocol for exchanging structured information in the implementation of web services, while HTTP (Hypertext Transfer Protocol) is the foundation of data communication for the World Wide Web (Fielding et al., 1999).
Motamedi and Villányi (2022) compared and evaluated wireless communication protocols for the implementation of smart irrigation systems in greenhouses, considering factors such as power consumption, range, reliability, and scalability. They found that ZigBee is the most suitable local communication protocol for greenhouse irrigation due to its large number of nodes and long range, while MQTT is the recommended messaging protocol for smart irrigation systems due to its TCP transport protocol and quality of service (QoS) options. GSM is a reliable and cost-effective global communication protocol for greenhouse irrigation, providing wide coverage and low cost (Motamedi & Villányi, 2022).
Syafarinda et al. (2018) investigated the use of the MQTT protocol in a precision agriculture system using a Wireless Sensor Network (WSN). They found that MQTT is suitable for use in IoT applications due to its lightweight, simple, and low bandwidth requirements. The average data transmission speed using the MQTT protocol was approximately 1 second, demonstrating its effectiveness for real-time data transmission in precision agriculture systems (Syafarinda et al., 2018).
The choice of application layer protocol for real-time irrigation management depends on factors such as data transmission speed, reliability, and energy efficiency. MQTT and RTPS (Real-Time Publish-Subscribe) are both suitable for real-time data transmission in IoT systems, but they have different strengths and weaknesses. MQTT is a better choice for applications that require low latency and high throughput, while RTPS is a better choice for applications that require high reliability and low latency (Sanchez-Iborra & Skarmeta, 2021). The exploration of MQTT and client-server IoT networks, along with the comparison of various application layer protocols, provides valuable insights into the suitability of these technologies for real-time data transmission in automated irrigation management systems.
In summary, real-time data transmission protocols and technologies play a vital role in the automation of irrigation management systems, enabling the efficient and reliable delivery of sensor data to the cloud for processing and decision-making. The exploration of MQTT and client-server IoT networks, along with the comparison of application layer protocols, highlights the importance of selecting suitable technologies based on factors such as data transmission speed, reliability, and energy efficiency. By leveraging these technologies, automated irrigation management systems can achieve seamless integration and contribute to the overall goal of enhancing agricultural water use efficiency and crop productivity.

3.5. Challenges and Solutions in Real-Time Data Transmission
Following the exploration of data collection, processing at the edge and fog, and automation in previous sections, we now turn to the critical aspect of real-time data transmission. While essential for automated irrigation management, this stage presents unique challenges that must be addressed to ensure system efficiency and reliability.
Obstacles in Real-Time Data Transmission
Agricultural environments present unique challenges for real-time data transmission, directly impacting the effectiveness of automated irrigation systems. Environmental factors can significantly disrupt wireless communication. Adverse weather conditions such as heavy rain, fog, and high winds can weaken or even block radio signals, leading to data loss and compromised system performance. Physical obstacles like trees, buildings, and uneven terrain further complicate signal propagation, creating reliability issues (Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). These environmental challenges necessitate robust communication protocols and network architectures that can ensure consistent and reliable data flow.
In addition to environmental factors, technical limitations also present significant obstacles. Large-scale agricultural operations often demand long-distance data transmission, which can be hindered by the limited range of certain wireless communication protocols. Network congestion, occurring when multiple sensors transmit data concurrently, can lead to delays and potential data loss, further complicating real-time decision-making (Hameed et al., 2020). To mitigate these issues, researchers have investigated the potential of cognitive radio networks (CRNs) and dynamic spectrum access (DSA) for optimizing spectrum utilization and reducing interference (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). CRNs enable devices to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency. DSA, on the other hand, facilitates the dynamic allocation of unused spectrum, enhancing spectrum utilization and reducing congestion.
Furthermore, data security and privacy are paramount concerns in real-time irrigation systems. The sensitive nature of agricultural data, such as crop yields and farm management practices, necessitates robust security measures to prevent unauthorized access and data breaches (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential to protect data integrity and ensure the trustworthiness of the system.
Investigating Data Optimization Techniques
To enhance the efficiency and reliability of real-time data transmission in automated irrigation systems, researchers have explored a range of data optimization techniques. Data compression techniques aim to reduce the size of data packets transmitted over the network, minimizing bandwidth requirements and improving transmission speed (Rady et al., 2020; Karim et al., 2023). Lossless compression algorithms, such as Huffman coding and LZW, preserve data integrity while effectively reducing data size, ensuring that no information is lost during transmission (Cui, 2023). Lossy compression algorithms, such as JPEG and MP3, offer higher compression ratios but introduce a controlled level of data loss, which may be acceptable for certain applications where some loss of precision is tolerable (Karim et al., 2023). The choice between lossless and lossy compression depends on the specific application and the trade-off between data size and accuracy.
Data aggregation techniques provide another effective approach to optimize data transmission. By aggregating multiple sensor readings into a single representative value, such as average soil moisture or temperature, the number of transmissions can be significantly reduced, conserving bandwidth and energy resources (Cui, 2023). This is particularly beneficial in large-scale irrigation systems where numerous sensors are deployed across vast areas, generating substantial amounts of data. Additionally, data filtering techniques play a crucial role in improving data quality and reliability. Kalman filters and particle filters can effectively remove noise and outliers from sensor data, ensuring that only accurate and relevant information is transmitted and used for decision-making (Cui, 2023). This is essential for preventing erroneous data from influencing irrigation decisions and potentially leading to suboptimal water management.
Sensor calibration, drift correction, and fault detection are essential for maintaining data accuracy and reliability (Dos Santos et al., 2023). Regular calibration ensures that sensors provide accurate measurements over time, while drift correction techniques account for gradual changes in sensor readings due to environmental factors or aging. Fault detection mechanisms can identify and address sensor malfunctions or anomalies, preventing erroneous data from influencing irrigation decisions and potentially harming crops or wasting water.
Addressing the Challenges
Effectively addressing the challenges in real-time data transmission requires a multifaceted approach that encompasses environmental, technical, and data-related considerations. Implementing robust and adaptive communication protocols is crucial for overcoming interference and signal degradation caused by weather conditions and physical obstacles. Selecting appropriate protocols, such as LoRa or ZigBee, with suitable range and penetration capabilities can ensure reliable data transmission in challenging agricultural environments (Motamedi & Villányi, 2022). Additionally, employing techniques like frequency hopping and error correction codes can further improve communication resilience and mitigate data loss.
Optimizing network architecture is another key consideration. Deploying a distributed network architecture with edge and fog computing capabilities can significantly enhance data processing and transmission efficiency (Abdel Nasser et al., 2020; Tran et al., 2019). Edge devices can perform initial data processing and aggregation tasks, reducing the amount of data transmitted to the cloud and minimizing latency, while fog nodes can provide additional processing power and storage closer to the data source, enhancing scalability and reliability. This distributed approach alleviates the burden on the central cloud server and allows for more responsive and efficient irrigation management.
Data optimization techniques play a vital role in reducing bandwidth requirements and improving transmission efficiency. The choice of data compression, aggregation, and filtering techniques should be tailored to the specific requirements of the irrigation system, considering factors such as data type, accuracy needs, and available bandwidth. By carefully selecting and implementing these techniques, the overall performance and effectiveness of real-time irrigation systems can be significantly enhanced, leading to more sustainable water management practices and improved agricultural productivity.
By addressing these challenges and implementing appropriate solutions, real-time data transmission can become a reliable and efficient component of automated irrigation systems, contributing to the overall goal of achieving sustainable and productive agriculture in the face of growing food demands and water scarcity.

3.6. IoT Network Architectures and Variable Rate Irrigation (VRI) for Real-Time Irrigation
Real-time irrigation management systems heavily rely on the efficient and reliable transmission of data from sensors and weather stations to the cloud for processing and decision-making. However, agricultural environments present unique challenges to wireless communication, including adverse weather conditions, physical obstacles, and the limitations of wireless technologies. These challenges necessitate robust and adaptive solutions to ensure the consistent and timely flow of data, enabling truly autonomous irrigation scheduling.
Environmental factors, such as heavy rain, fog, and strong winds, can significantly disrupt wireless communication by attenuating or even blocking radio signals, leading to data loss and compromised system performance (Ed-daoudi et al., 2023; Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). Dense vegetation, buildings, and uneven terrain create further complications by causing multipath propagation and shadowing effects (Yim et al., 2018; Gautam and Pagay, 2020). The study by Yim et al. (2018) on LoRa networks in a tree farm environment exemplifies these challenges, revealing reduced communication range and data reliability compared to theoretical expectations. This underscores the need for carefully selecting and optimizing communication protocols and network parameters to ensure reliable data transmission in such environments.
The study by Guzinski et al. (2014a) using a modified TSEB model further highlights the importance of high-resolution data in accurately capturing the spatial and temporal dynamics of energy fluxes influenced by environmental factors. This emphasizes the need for advanced data acquisition and processing techniques that can effectively represent the complexities of agricultural settings.
The limitations of traditional wireless communication technologies, such as limited range and network congestion, pose additional challenges for large-scale agricultural operations. Long-distance data transmission can be hindered by range limitations, while network congestion arising from numerous sensors transmitting concurrently can lead to delays and data loss, hindering real-time decision-making (Hameed et al., 2020). Addressing these challenges requires the exploration of advanced networking technologies that can optimize spectrum utilization, mitigate interference, and improve reliability and efficiency.
Cognitive Radio Networks (CRNs) and Dynamic Spectrum Access (DSA) offer promising solutions for optimizing wireless communication in agricultural settings. CRNs empower devices with the ability to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). Research has explored the potential of CRNs in predicting Radio Frequency (RF) power to avoid noisy channels and optimize spectrum utilization (Iliya et al., 2014; Iliya et al., 2014). These studies demonstrate the effectiveness of combining optimization algorithms with artificial neural networks (ANNs) to enhance the accuracy and generalization of RF power prediction, enabling CRNs to make informed decisions about channel selection and avoid interference.
DSA complements CRN technology by dynamically allocating unused spectrum, further enhancing spectrum utilization and reducing congestion (Shi et al., 2023). The numerical model developed by Shi et al. (2023) showcases the potential of CRNs and DSA for optimizing wireless communication in challenging environments.
The integration of CRNs and DSA into the IoT network architecture requires careful consideration of spectrum sensing techniques, network topology, and data security. Research on cooperative spectrum sensing suggests that distributed approaches, where sensor nodes collaborate and share information, can significantly improve the accuracy and efficiency of spectrum sensing, particularly in dynamic environments (Trigka and Dritsas, 2022; Khalid & Yu, 2019). This collaborative approach enables a more comprehensive understanding of the radio environment and facilitates the identification of available frequency bands for data transmission.
The choice of network topology also impacts the performance and scalability of CRN-based irrigation systems. Mesh networks, where sensor nodes are interconnected and relay data for each other, offer enhanced resilience and coverage compared to star topologies where nodes communicate directly with a central gateway (Akyildiz & Vuran, 2010). However, mesh networks can be more complex to manage and may introduce additional routing overhead. The trade-off between network resilience and complexity needs to be carefully evaluated to select the most appropriate topology for a specific agricultural setting.
Data security and privacy are paramount concerns in IoT-based irrigation systems due to the sensitive nature of agricultural data (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential for protecting data integrity and ensuring system trustworthiness. Research on secure spectrum leasing and resource allocation algorithms for CR-WSN-based irrigation systems has demonstrated the potential of these technologies for enhancing security and efficiency (Hassan, 2023; Afghah et al., 2018).
In conclusion, the development of effective and reliable real-time irrigation management systems requires a comprehensive approach that addresses the challenges of data transmission in agricultural environments. The integration of robust and adaptive communication protocols, optimized network architectures, and advanced networking technologies like CRNs and DSA, along with a focus on data security and privacy, can contribute significantly to achieving the goal of autonomous and efficient irrigation scheduling.
4. AUTOMATED DATA PROCESSING IN THE CLOUD
4.1. Data Quality and Preprocessing
Data quality is paramount in automated irrigation systems as it directly influences the effectiveness of decision-making and water use efficiency. Issues like missing values, inconsistencies, and outliers arising from sensor malfunctions, environmental interference, or network problems (Lv et al., 2023) can significantly impact the performance of machine learning models used for irrigation scheduling and management.
Real-time data cleaning techniques are essential for addressing these challenges. Kalman filtering proves particularly effective in handling missing values and correcting erroneous readings by recursively estimating the system's state based on previous measurements and current sensor data, taking into account noise and uncertainty (Kim et al., 2020). Moving average techniques, by averaging consecutive data points, provide a more stable representation of the underlying trend, filtering out short-term fluctuations (Chhetri, 2023). For outlier detection, adaptive thresholding methods offer a dynamic approach, adjusting thresholds based on the statistical properties of the data to effectively identify anomalies and minimize false positives (Bah et al., 2021). These techniques are crucial in maintaining the integrity of real-time data streams and ensuring the accuracy of subsequent analyses.
Adaptive data preprocessing is essential for managing the diversity of data sources and formats commonly found in irrigation systems. Data normalization techniques, such as min-max scaling or z-score normalization, ensure that all features contribute equally to the analysis by transforming data values to a common scale (Pradal et al., 2016). This is crucial for preventing features with larger values from dominating the analysis and ensuring that all features are given equal consideration. Similarly, feature scaling methods, like standardization or normalization, optimize the range of feature values to improve the performance and convergence of machine learning models (Tortorici et al., 2024). By scaling features to a similar range, the influence of outliers is reduced, and the model's ability to learn from the data is enhanced.
Data fusion techniques play a critical role in integrating information from diverse sources, creating a more comprehensive and reliable dataset for irrigation management. Dempster-Shafer theory, a generalization of probability theory, allows for the expression of both uncertainty and the degree of conflict in evidence, making it suitable for fusing uncertain and conflicting data from heterogeneous sources (Sadiq and Rodriguez, 2004). This is particularly relevant in irrigation systems where data from different sensors may provide slightly different or even contradictory information due to sensor variations or environmental factors. Bayesian inference offers another powerful framework for combining information from multiple sources, updating the probability of a hypothesis as new evidence becomes available. By applying these techniques, data from soil moisture sensors, canopy temperature sensors, weather stations, and other sources can be integrated to provide a holistic understanding of crop water requirements and environmental conditions, leading to more informed and accurate irrigation decisions.
The impact of data quality extends beyond model accuracy to the robustness of machine learning models under varying conditions. Robust models should maintain consistent performance even when faced with data inconsistencies or unexpected situations. Techniques like data augmentation and domain adaptation can enhance model robustness by exposing the model to a wider range of data variations during training. Data augmentation involves generating additional training data by applying transformations or introducing noise to existing data, making the model more resilient to noise and variations in the real-world data. Domain adaptation techniques aim to adapt a model trained on one domain (e.g., a specific crop or geographic location) to perform well on another domain with different data characteristics. This is particularly relevant in irrigation management, where models may need to be applied to different crops, soil types, or climatic conditions.
The choice of data cleaning, preprocessing, and fusion techniques should be carefully considered based on the specific characteristics of the irrigation system and the available data. By selecting and implementing appropriate techniques, the accuracy, reliability, and robustness of machine learning models can be significantly improved, leading to more efficient and sustainable irrigation management practices.
4.2. Scalable and Autonomous Deployment using Containerization Strategies
The transition from data collection and transmission to efficient data processing requires a robust infrastructure capable of handling diverse workloads and data volumes. Containerization technologies, specifically Docker and Kubernetes, offer a promising solution for deploying and scaling data processing and machine learning modules within cloud environments like AWS, Azure, and GCP (Vargas-Rojas et al., 2024; Rosendo et al., 2022; Solayman & Qasha, 2023). Docker provides a standardized way to package applications and their dependencies into self-contained units known as containers, ensuring consistent and reproducible execution across different platforms (Rosendo et al., 2022). Kubernetes, acting as a container orchestrator, manages their deployment, scaling, and networking across a cluster of machines (Rosendo et al., 2022). This combination presents several advantages for automated irrigation management systems.
Firstly, containerization facilitates efficient resource utilization and scalability. By encapsulating applications and their dependencies, containers enable the isolation of resources and prevent conflicts between different modules (Vargas-Rojas et al., 2024; Solayman & Qasha, 2023). This isolation allows for the efficient allocation of resources, such as CPU, memory, and storage, to each container based on its specific needs. Kubernetes further enhances scalability by allowing for the automatic scaling of containers based on real-time demand, ensuring the system can adapt to varying workloads and data volumes, preventing bottlenecks, and ensuring responsiveness to changing conditions (Karamolegkos et al., 2023).
Secondly, containerization promotes portability and reproducibility. By packaging applications and their dependencies into a single unit, containers make it easy to move and deploy them across different cloud environments without the need for environment-specific configurations (Rosendo et al., 2022; Solayman & Qasha, 2023). This portability simplifies the development and deployment process, reducing the time and effort required to set up and manage the system. Additionally, containers ensure reproducibility by providing a consistent execution environment, regardless of the underlying infrastructure. This eliminates variability and ensures that the system will behave consistently across different deployments (Zhou et al., 2023).
Optimizing container orchestration and resource allocation is crucial to minimizing latency and maximizing throughput in real-time data processing pipelines. Techniques like auto-scaling and dynamic resource allocation play a critical role in this context (Hethcoat et al., 2024; Werner and Tai, 2023; Kumar et al., 2024). Auto-scaling automatically adjusts the number of container instances based on real-time demand, ensuring that sufficient resources are available to handle peak workloads while avoiding over-provisioning during periods of low demand (Hethcoat et al., 2024; Kumar et al., 2024). Dynamic resource allocation enables the fine-grained adjustment of resources allocated to each container based on its specific needs and the current workload (Werner and Tai, 2023). This ensures efficient resource allocation and provides each container with the necessary resources to perform its tasks effectively.
Performance monitoring tools, such as Kubernetes Metrics Server and Prometheus, are essential for gaining insights into the performance of containers and the overall system (Hethcoat et al., 2024; Kuity & Peddoju, 2023). These tools provide valuable data on key performance indicators, such as CPU and memory usage, network traffic, and application-specific metrics. By monitoring this data, administrators can identify bottlenecks, optimize resource allocation strategies, and continuously improve system performance (Hethcoat et al., 2024). This data-driven approach ensures that automated irrigation management systems can operate efficiently and reliably.
By integrating containerization technologies with optimization techniques and performance monitoring, automated irrigation management systems achieve the scalability, autonomy, and efficiency required for effective real-time data processing and decision-making. This approach facilitates a seamless and responsive system that can adapt to changing conditions and contribute to the overall goal of optimizing water resource management and increasing agricultural productivity.
4.3. Deploying ML Models for Data Processing
•	Architectures and frameworks for deploying machine learning models on cloud platforms for real-time data processing and inference in irrigation management systems, such as: TensorFlow Serving, Apache MXNet Model Server, ONNX Runtime
•	Techniques for optimizing machine learning model performance and resource utilization in cloud environments, such as: Model compression (e.g., pruning, quantization), Hardware acceleration (e.g., GPU, TPU), Distributed training (e.g., Horovod, BytePS)
•	Integration of deployed machine learning models with other components of the automated irrigation management pipeline, such as data preprocessing, decision-making, and control systems, using protocols like: MQTT, CoAP, RESTful APIs
4.4. Online Learning in the Cloud
•	Application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as: Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests
•	Architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as: Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms
•	Strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making, using techniques such as: Multi-armed bandits, Bayesian optimization, Reinforcement learning (e.g., Q-learning, SARSA)


5. GENERATING AND APPLYING IRRIGATION INSIGHTS 
5.1. Real-Time Generation of Actionable Irrigation Insights
•	Advanced predictive models, such as deep learning (e.g., LSTM, CNN) and ensemble methods (e.g., Random Forests), for precise, site-specific irrigation recommendations
•	Integration of IoT sensor data (e.g., soil moisture probes, weather stations) and cloud-based data sources (e.g., weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filtering) to enhance insight accuracy and resolution
•	Strategies for handling data heterogeneity, uncertainty, and quality issues in real-time insight generation, such as data preprocessing and outlier detection
•	Techniques for reducing computational complexity and latency, such as edge computing (e.g., fog computing), model compression (e.g., quantization), and hardware accelerators (e.g., GPUs)
5.2. Automated Application of Irrigation Insights
•	Architectures and protocols for seamless integration of ML-generated insights with IoT-enabled irrigation control systems, such as MQTT and CoAP for lightweight, real-time communication
•	Analysis of industry-leading products and services, such as smart irrigation controllers (e.g., Rachio) and cloud-based irrigation management platforms (e.g., CropX)
•	Strategies for ensuring reliability, security, and scalability of automated insight application, such as redundant communication channels and secure edge-to-cloud architectures
•	Case studies of successful implementations of closed-loop, autonomous irrigation systems in research and commercial settings, highlighting technologies used and benefits achieved

6. INTEGRATION, INTEROPERABILITY, AND STANDARDIZATION 
6.1. Interoperability and Standardization
•	Importance of interoperability and standardization in enabling seamless integration of automated irrigation components
•	Overview of existing and emerging standards for IoT devices, communication protocols, and data formats in precision agriculture (e.g., ISOBUS, agroXML, SensorML)
•	Role of standardization bodies and industry consortia in promoting interoperability (e.g., AgGateway, Open Ag Data Alliance, Agricultural Industry Electronics Foundation)
•	Challenges in adopting and implementing standards across diverse hardware and software platforms
•	Strategies for encouraging widespread adoption of standards and best practices for interoperability in automated irrigation systems
6.2. Integration with Existing Irrigation Infrastructure
•	Challenges and strategies for retrofitting legacy irrigation systems with IoT sensors, actuators, and communication devices
•	Hardware compatibility issues and solutions (e.g., adapters, modular designs)
•	Software and firmware updates to enable integration with automated decision-making systems
•	Data integration and normalization techniques for merging legacy and new data sources
•	Economic and practical considerations for transitioning from manual to automated irrigation management
•	Cost-benefit analysis of upgrading existing infrastructure vs. implementing new systems
•	Phased implementation approaches to minimize disruption and optimize resource allocation
•	Training and support requirements for farmers and irrigation managers adopting automated systems
•	Case studies and real-world examples of successful integration of automated irrigation with existing infrastructure
6.3. Integration with Other Precision Agriculture Technologies
•	Synergies between automated irrigation and complementary technologies
•	Remote sensing (satellite, UAV, and ground-based) for crop monitoring and evapotranspiration estimation
•	Soil moisture sensors and weather stations for real-time, localized data collection
•	Variable rate application systems for precise irrigation delivery based on crop requirements
•	Yield mapping and analytics for assessing the impact of automated irrigation on crop productivity
•	Architectures and frameworks for integrating diverse data sources and technologies into a unified precision agriculture ecosystem
•	Edge computing and fog computing paradigms for real-time data processing and decision-making
•	Cloud-based platforms for data storage, analysis, and visualization
•	API-driven approaches for modular integration of third-party services and applications
•	Challenges and solutions for ensuring data quality, consistency, and security across integrated precision agriculture systems
•	Data cleaning, preprocessing, and harmonization techniques
•	Blockchain and distributed ledger technologies for secure, tamper-proof data sharing and traceability
•	Access control and authentication mechanisms for protecting sensitive data and resources
•	Future trends and research directions in the integration of automated irrigation with advanced precision agriculture technologies (e.g., AI-driven crop modeling, robotics, and autonomous vehicles)
6.4. Cybersecurity Considerations for Integrated Automated Irrigation Systems
•	Unique security risks and vulnerabilities associated with IoT-based automated irrigation systems
•	Potential for unauthorized access, data tampering, and system manipulation
•	Implications of security breaches for crop health, water resource management, and farm productivity
•	Best practices and strategies for securing automated irrigation systems
•	Secure device provisioning and authentication (e.g., hardware security modules, certificates)
•	Encryption and secure communication protocols (e.g., TLS, DTLS)
•	Firmware and software updates to address emerging security threats
•	Network segmentation and access control to limit the impact of breaches
•	Role of cybersecurity standards and frameworks in guiding the development and deployment of secure automated irrigation systems (e.g., NIST CSF, IEC 62443)
•	Importance of user awareness, training, and incident response planning in maintaining the security of integrated automated irrigation systems

7. MONITORING AND ENSURING SYSTEM RELIABILITY
7.1. Resilience and Fault Tolerance in Automated Irrigation Systems
•	Strategies for ensuring robustness and reliability in the face of failures, disruptions, or unexpected events
•	Redundancy: Implementing redundant components, such as duplicate sensors (e.g., soil moisture sensors, weather stations), controllers (e.g., PLCs, microcontrollers), and communication channels (e.g., cellular, satellite, LoRaWAN) to maintain system functionality during component failures
•	Failover mechanisms: Designing seamless failover mechanisms that automatically switch to backup components or systems in case of primary system failure, such as hot-standby controllers or multi-path communication protocols (e.g., mesh networks, software-defined networking)
•	Self-healing capabilities: Incorporating AI-driven self-healing mechanisms that can detect, diagnose, and recover from faults without human intervention, using techniques like reinforcement learning, Bayesian networks, or self-organizing maps
•	The role of distributed architectures and edge computing in enhancing system resilience
•	Decentralizing critical functions and data processing to minimize the impact of single points of failure, using fog computing or multi-agent systems
•	Leveraging edge computing to enable localized decision-making and control, reducing dependence on cloud connectivity and improving response times, using technologies like Raspberry Pi, NVIDIA Jetson, or Intel NUC
•	Anomaly detection and predictive maintenance using AI techniques
•	Employing unsupervised learning algorithms (e.g., autoencoders, clustering) to detect anomalies in sensor data, system performance, and water usage patterns
•	Developing predictive maintenance models using techniques like long short-term memory (LSTM) networks, convolutional neural networks (CNNs), or gradient boosting machines (GBMs) to anticipate and prevent potential system failures based on historical data and real-time monitoring
7.2. Advanced Monitoring Techniques for Automated Irrigation Systems
•	Remote monitoring using IoT-enabled sensors and computer vision
•	Deploying a heterogeneous network of IoT sensors to collect real-time data on soil moisture (e.g., capacitive, tensiometric), temperature (e.g., thermocouples, thermistors), humidity (e.g., capacitive, resistive), and plant health (e.g., sap flow, leaf wetness)
•	Integrating high-resolution cameras (e.g., multispectral, hyperspectral) and computer vision algorithms for visual monitoring of crop growth, disease detection (e.g., using deep learning-based object detection and segmentation), and irrigation system performance (e.g., leak detection, sprinkler uniformity)
•	Transmitting sensor and camera data to cloud-based platforms (e.g., AWS IoT, Google Cloud IoT, Microsoft Azure IoT) for remote access and analysis using protocols like MQTT, CoAP, or AMQP
•	Innovative approaches for real-time system health assessment
•	Developing novel algorithms and metrics for evaluating the health and performance of automated irrigation systems, such as entropy-based measures, network resilience indices, or multi-criteria decision analysis (MCDA) frameworks
•	Combining data from multiple sources (e.g., sensors, weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filters, Dempster-Shafer theory) to create a comprehensive view of system health
•	Employing advanced data visualization techniques (e.g., interactive dashboards, augmented reality) to present system health information in an intuitive and actionable format
7.3. Closed-Loop Control and Feedback Mechanisms
•	Exploring the concept of closed-loop control in autonomous irrigation systems
•	Implementing feedback loops that continuously monitor system performance and adjust irrigation schedules based on real-time data, using control techniques like proportional-integral-derivative (PID), model predictive control (MPC), or fuzzy logic control (FLC)
•	Integrating machine learning algorithms (e.g., reinforcement learning, genetic algorithms) to optimize closed-loop control strategies over time, adapting to changing environmental conditions and crop requirements
•	Designing effective feedback mechanisms for user interaction and system optimization
•	Providing user-friendly interfaces (e.g., mobile apps, web dashboards) for farmers to input preferences, constraints, and expert knowledge into the automated irrigation system, using techniques like participatory design or user-centered design
•	Incorporating user feedback and domain expertise to refine irrigation strategies and improve system performance
8. CASE STUDIES AND REAL-WORLD IMPLEMENTATIONS OF FULLY AUTONOMOUS IRRIGATION SYSTEMS
8.1. Fully Autonomous Irrigation Systems in Diverse Agricultural Settings
•	Row Crops: maize, wheat, soybean with real-time soil moisture monitoring and weather-based irrigation scheduling for fully automated precision irrigation
•	Orchards: citrus, apple, almond with plant health monitoring and precision water application for fully autonomous orchard management
•	Greenhouses: tomato, lettuce, herbs with automated drip irrigation and climate control integration for fully automated greenhouse operations
•	Urban Farming: rooftop gardens, vertical farms with IoT-enabled hydroponic systems and remote management for fully autonomous urban crop production
8.2. Integration of Advanced System Components for End-to-End Automation
•	Wireless sensor networks: soil moisture probes, weather stations, plant health monitoring cameras with low-power, long-range communication for fully automated data acquisition
•	Secure data transmission: LoRaWAN, NB-IoT, 5G, satellite communication for reliable, real-time data transfer from field to cloud in fully autonomous irrigation systems
•	Intelligent data processing: edge computing for local data filtering, cloud platforms for scalable storage and analysis, machine learning algorithms for predictive insights in fully automated irrigation management
•	Autonomous decision-making: advanced irrigation scheduling algorithms, precise valve control, closed-loop feedback systems for optimal water management in fully autonomous irrigation systems
8.3. Quantitative Performance Evaluation of Fully Automated Irrigation Systems
•	Water use efficiency: percent reduction in water consumption compared to conventional methods, improved water productivity (yield per unit of water) achieved through fully autonomous irrigation
•	Crop yield and quality improvements: percent increase in yield, enhanced crop uniformity, improved nutritional content attributed to fully automated precision irrigation
•	Labor and energy savings: quantified reduction in labor hours for irrigation management, decreased energy consumption for pumping due to optimized scheduling in fully autonomous systems
•	Economic viability: detailed return on investment analysis, payback period calculations, comprehensive cost-benefit analysis for fully autonomous irrigation management systems
8.4. Lessons Learned and Challenges Encountered in Deploying Autonomous Irrigation Systems
•	Technical challenges and solutions: ensuring reliable data transmission in remote locations, addressing interoperability issues between diverse system components, optimizing power consumption for extended battery life, adapting algorithms to local soil and weather conditions in fully autonomous irrigation systems
•	Operational and logistical hurdles: streamlining installation and maintenance procedures, providing effective user training, seamlessly integrating with existing farm management practices and legacy systems for fully automated irrigation management
•	Regulatory and socio-economic considerations: navigating complex water use regulations, addressing data privacy and security concerns, ensuring equitable access and affordability for smallholder farmers adopting fully autonomous irrigation technologies
8.5. Best Practices and Recommendations for Successful Implementation
•	Designing scalable, modular, and adaptable autonomous irrigation systems to accommodate future growth and changing requirements for fully automated water management
•	Prioritizing user-centered design principles and actively engaging stakeholders throughout the development and deployment process of fully autonomous irrigation solutions
•	Adopting open standards and communication protocols to enable seamless integration of system components and interoperability with third-party platforms in fully automated irrigation setups
•	Implementing robust data validation, filtering, and quality control mechanisms to ensure data integrity and reliability for decision-making in fully autonomous irrigation systems
•	Establishing clear data governance policies and security frameworks to protect sensitive information and maintain user trust in fully automated irrigation management
•	Developing intuitive user interfaces and decision support tools to facilitate easy adoption and effective use of fully autonomous irrigation systems
•	Collaborating with local extension services, agribusinesses, and technology providers for knowledge transfer, technical support, and continuous improvement of fully automated irrigation solutions
8.6. Synthesis of Case Studies and Implications for Autonomous Irrigation Adoption
•	Cross-case analysis of key performance indicators and critical success factors for fully autonomous irrigation scheduling systems in various contexts
•	Identification of common themes, challenges, and innovative solutions across diverse implementations of end-to-end fully automated irrigation management
•	Assessment of the potential for replicability and scaling of successful fully autonomous irrigation projects in different regions and farming systems
•	Implications for future research priorities, technology development roadmaps, and policy interventions to support widespread adoption of fully autonomous irrigation technologies

CONCLUSION/FUTURE DIRECTIONS AND UNANSWERED QUESTIONS
•	Summarize the key insights gained from the question-driven review, emphasizing how each section contributes to the overarching goal of achieving real-time, end-to-end automation in irrigation management
•	Based on the questions addressed, propose new research directions and unanswered questions
•	Identify key research gaps and propose concrete research questions and hypotheses for advancing the field of real-time, automated irrigation management
•	Highlight the need for collaborative research efforts across disciplines, such as computer science, agricultural engineering, and environmental science, to address the complex challenges of automated irrigation systems
•	Emphasize the need for further innovation and exploration in real-time, automated irrigation systems



</previous_sections>

</documents>
<instructions>


Use the information provided in the <documents> tags to write the next subsection of the research paper, following these steps:
1. Review the overall intention of the research paper, specified in the <review_intention> tag. Ensure the subsection you write aligns with and contributes to this overall goal.
2. Consider the specific intention for this subsection of the paper, stated in the <section_intention> tag. The content you write should fulfill this purpose. 
3. Use the title provided in the <subsection_title> tag as the heading for the subsection. 
4. Address each of the points specified in the </subsection_point_Point *> tags:
   a) Make a clear case for each point using the text provided in the "point" field.
   b) Support each point with evidence from the research papers listed in the corresponding "papers to support point" field.
   c) When citing a paper to support a point, include inline citations with the author name(s) and year, e.g. (Smith et al., 2020; Johnson and Lee, 2019; Brown, 2018). Cite all papers that strengthen or relate to the point being made.
   d) While making a point and citing the supporting papers, provide a brief explanation in your own words of how the cited papers support the point.
5. Ensure that both of the points from the <subsection_point> tags are fully addressed and supported by citations. Do not skip or combine any points.
6. After addressing the specified points, wrap up the subsection with a concluding sentence or two that ties the points together and relates them back to the <section_intention>.
7. Review the <Previous_sections> of the paper, and ensure that the new subsection you have written fits logically and coherently with the existing content. Add transition sentences as needed to improve the flow.
8. Proofread the subsection to ensure it is clear, concise, and free of grammatical and spelling errors. Maintain a formal academic tone and style consistent with the rest of the research paper.
9. Format the subsection using Markdown, including the subsection heading (using ## or the equivalent for the document), inline citations, and any other formatting needed for clarity and readability.
10. If any information is missing or unclear in the provided tags, simply do your best to write the subsection based on the available information. Do not add any information or make any points not supported by the provided content. Prioritize fully addressing the required points over hitting a specific word count.

The output should be a complete, well-organized, and properly cited subsection ready to be added to the research paper.

Begin your answer with a brief recap of the instructions stating what you will to optimize the quality of the answer. Clearly and briefly state the subsection you'll be working on and the points you'll be addressing. Then proceed to write the subsection following the instructions provided. 

Critical: 
- Do not include a conclusion or summary as the entry is in the middle of the document. Focus on addressing the points and supporting them with evidence from the provided papers. Ensure that the subsection is well-structured, coherent, and effectively contributes to the overall research paper.
- The subsection we are focusing on is: 4.4. Online Learning in the Cloud
- No need for sub-sub-sections. just provide paragraphs addressing each point. They should transition fluidly and narurally into each other.
- Ensure that the content is supported by the provided papers and that the citations are correctly formatted and placed within the text.
- Do not repeat content from the previous sections. Ensure that the information provided is new and relevant to the subsection being written.



</instructions>

<subsection_point_Point 1>
Point: Application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests

Papers to support point:

Paper 1:
- APA Citation: Snyder, L. S., Lin, Y. S., Karimzadeh, M., Goldwasser, D., & Ebert, D. S. (2020). Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness. IEEE Transactions on Visualization and Computer Graphics, 26(1), 558-568. https://doi.org/10.1109/TVCG.2019.2934614
  Main Objective: To design, optimize, and evaluate a real-time, user-guided interactive learning approach for online training and incremental refinement of machine learning models for classification of streaming text data.
  Study Location: Unspecified
  Data Sources: Figure Eight disaster-related corpus, CrisisLexT26 datasets, Twitter streaming API
  Technologies Used: Word2Vec, Convolutional Neural Network, Long-Short Term Memory, Recurrent Neural Network
  Key Findings: 1. The proposed interactive learning framework allows users to interactively train machine learning models to identify relevant information from streaming text data in real-time, improving situational awareness.

2. The framework was integrated into an existing visual analytics application, SMART 2.0, to provide interactive visualizations for real-time data exploration and analysis.

3. User studies with domain experts demonstrated the effectiveness and usability of the proposed approach.
  Extract 1: "This paper presents a novel interactive learning framework in which users iteratively (re)train neural network models with streaming text data in real-time to improve the process of finding relevant information."
  Extract 2: "According to evaluation results, our model outperforms state-of-the-art learning models used in similar classification tasks."
  Limitations: None
  Relevance Evaluation: The paper is highly relevant to the point being made in the review. It presents a novel method for applying online learning techniques to continuously update and improve machine learning models based on incoming real-time data, using algorithms such as SGD, Passive-aggressive algorithms, and Online random forests. This is directly related to the goal of automating data processing in the cloud for real-time irrigation management. Automation and real-time processing are key elements of the proposed systematic review.
  Relevance Score: 1.0
  Inline Citation: (Snyder et al., 2020)
  Explanation: This study presents a novel interactive learning framework to identify relevant tweets in real-time, aiming to improve situational awareness for first responders. In the proposed framework, end users continuously label the relevance of the incoming tweet data to incrementally train a machine learning model. The framework has been integrated into an existing visual analytics application, SMART 2.0, which provides various interactive visualizations for situational awareness. User studies with domain experts have demonstrated the effectiveness and usability of the proposed approach.

 Full Text: >
This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising purposes. To learn more, view the following link: Privacy Policy Manage Preferences Loading [MathJax]/extensions/MathMenu.js IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Visualiz... >Volume: 26 Issue: 1 Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness Publisher: IEEE Cite This PDF Luke S. Snyder; Yi-Shan Lin; Morteza Karimzadeh; Dan Goldwasser; David S. Ebert All Authors 16 Cites in Papers 1350 Full Text Views Abstract Document Sections 1 Introduction 2 Related Work 3 Interactive Learning Framework 4 SMART 2.0 5 User Experience Show Full Outline Authors Figures References Citations Keywords Metrics Media Abstract: Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system. Published in: IEEE Transactions on Visualization and Computer Graphics ( Volume: 26, Issue: 1, January 2020) Page(s): 558 - 568 Date of Publication: 20 August 2019 ISSN Information: PubMed ID: 31442995 DOI: 10.1109/TVCG.2019.2934614 Publisher: IEEE Funding Agency: SECTION 1 Introduction Social media data has been used extensively in a variety of applications and research endeavors due to its ability to provide useful information on the public's opinions and behavior. Analysts in various domains are increasingly using social media to gain rapid situational awareness. For instance, first responders are leveraging Twitter data to obtain actionable information for crisis response and prevention (see [29] for an extensive list of literature on this subject). However, the vast amounts of unstructured text make the identification of relevant information nontrivial, limiting situational awareness. This issue is further compounded by changes in topics of interest (to end users) over time, since the computational models built to determine relevant information for one event or one user group may not apply to other events or other user groups due to variations in diction, word structure, or user expectations. Fig. 1. Our interactive learning framework allows users to train text relevance classifiers in real-time to improve situational awareness. In this example, a real-time tweet regarding a car accident is incorrectly classified as “irrelevant”. Through the SMART 2.0 interface, the user can view its label and correct it to “relevant”, thereby retraining and improving the classifier for incoming streaming data. Show All Several classification approaches have been developed to identify relevant and irrelevant social media information, such as clustering [5], [6], keyword matching [45], and term-vector similarity [12]. However, to the best of our knowledge, no existing work in this area includes interactive learning with real-time data, focusing instead on improving the machine learning algorithms themselves [5], [16], [23], [31], [33], [38], [45], [46], [54] or interactively training on archived datasets [9], [18]. Continuing on our example of first responders, a pre-trained classifier may not fulfill first responders' varying needs, since one first responder may be interested in monitoring road closures, and another one might be interested in identifying disinformation and misinformation on social media in order to take counter-action. Ultimately, first responders' definition of relevancy will depend on the situation at hand, which can vary over time. Interactively training classifiers through iterative user labeling can alleviate this problem. In this paper, we present a novel interactive framework in which the user iteratively (re)labels the relevancy of streaming social media data to adaptively train the underlying model to match their needs for improved situational awareness. We compare three different types of neural networks in terms of classification performance and computational efficiency for real-time learning. Furthermore, we optimize and computationally evaluate the selected models by simulating the real-time user feedback on several crisis-related datasets. Our results show that our interactive model outperforms state-of-the-art machine learning-based classification models. To incorporate our evaluated models into a working application, we extend an existing visual analytics system tailored for situational awareness called the Social Media Analytics and Reporting Toolkit (SMART) [52], [53], which has been successfully used by many first responder groups in the United States. SMART allows users to interactively explore trending topics on social media through integrated topic modeling and spatial, temporal, and textual visualizations. We call the newly extended system SMART 2.0, which incorporates our interactive learning framework to address the needs raised by the aforementioned first responder users and reduce noise in the incoming stream of data. Finally, we present domain-expert feedback on the usefulness of our approach as experienced by multiple first responders who used SMART 2.0 for crisis-related use cases. In addition, we include two usage scenarios of the system to illustrate its application to real-life situations. Overall, the major contributions of this paper are as follows: We present a novel interactive learning framework for classification of streaming text data. We compare three different types of neural networks in terms of performance and computational efficiency, and tune the models for learning at interactive rates. We further computationally evaluate the selected model on several disaster-related datasets. We integrate our models in SMART 2.0, a visual analytics application for situational awareness, and present user feedback obtained from domain experts using the system for crisis events. In the remainder of the paper, we discuss related work in section 2, the design of the framework and model in section 3, SMART 2.0 in section 4, evaluation of our framework in section 5, discussion and future work in section 6, and concluding remarks in section 7. SECTION 2 Related Work 2.1 Short Text Classification Researchers have presented many techniques to classify text documents into categories such as sentiment or topics [17], [35], [37], [49]. However, classifying short text, e.g. social media posts, is more challenging due to the lack of contextual information and loose adherence to standard grammar. To tackle the brevity of short text, auxiliary resources such as external corpora [10] or knowledge bases [20], or methods such as term frequency-inverse document frequency (TF-IDF) [16], have been proposed for improving classification. Representing words as n-dimensional vectors (i.e. word embedding) has become increasingly prevalent, since vectors can be used as inputs to machine learning models for finding semantic similarities [47], [50]. In particular, Google's Word2Vec [30] has been employed extensively in classification tasks [4], [26], [27], [32], [33] due to its impressive ability in capturing linguistic regularities and semantics. For instance, words frequently used together are likely to be closer in the Word2vec vector space than words that are not, and vector operations reveal meaningful semantics (e.g., the vector “King” – “Man” + “Woman” is close to the vector “Queen” [30]). Since pre-trained Word2vec models encode embeddings learned from larger web corpora, they have been increasingly used in short text classification tasks [4], [32], [33], [44]. Neural networks have generated state-of-the-art results in recent years for text classification problems [30], [32], [33], [43] and have also been used with Word2Vec [32], [33], [44]. Neural networks are well-suited for online learning processes in which training data is supplied iteratively since they can learn adaptively from new data [32], [33]. Nguyen et al. [33] presented a convolutional neural network with Word2Vec that outperformed non-neural classifiers, and Nguyen et al. [32] proposed a new online learning classification algorithm for deep neural networks utilizing the log-loss and gradient of sequential training batches. Their methods were evaluated with disaster-related datasets. However, these methods were not adapted to user-guided learning in which time constraints are essential and the provided batches may be small. In particular, the online learning method designed by Nguyen et al. [32] was evaluated with batch sizes of 200. In our work, we assume the user needs to train with flexibly interactive amounts of data (10–20 samples) to view immediate predictive improvements for situational awareness. Classification for Situational Awareness Utilizing real-time social media data for situational awareness (and crisis prevention in particular) is a heavily researched topic [16], [23], [31], [33], [38], [45], [54]. However, identifying situationally-relevant information is nontrivial due to the high noise-to-signal ratio. Karimi et al. [23] found that classification methods, such as Support Vector Machine and multinomial Naïve Bayes, can identify disaster-related tweets, although generic features such as hashtag count and tweet length are preferable so that the model does not learn relevancy only for a specific disaster. Researchers have used clustering [5], [6], [46] or enhanced keyword matching [45] to detect relevant crisis and event information, and provided human-annotated Twitter corpora that can be used to train word embedding models [21]. Nazer et al. [31] developed a system to detect requests for help by utilizing both tweet context (e.g., geotag) and content (e.g., URLs). Rudra et al. [38] designed a novel classification-summarization framework to classify disaster-related tweets, and then summarize the tweets by exploiting linguistic properties typical of disaster tweets (e.g., combinations of situational and non-situational information). Zoppi et al. [54] provided a relevance labeling strategy for crisis management that computed data relevance as a function of the data's integrity (e.g., are the geo-coordinates incorrect?), statistical properties (e.g., can we select a subset of the data that are geographically close?), and clustering (e.g., what groups are present in the data?). Toriumi et al. [46] clustered tweets based on their retweet count in real-time to extract important topics and classify tweets accordingly. The methods discussed so far, however, lack user interactivity. In particular, these classification methods are inflexible to user-dependent needs that change over time as new situations and events occur. As such, their practical use for real-time situational awareness is limited. 2.2 Visual Analytics and Interactive Learning for Situational Awareness Researchers have presented a number of visual analytics (VA) solutions for situational awareness. Diakopoulos et al. [12] developed Vox Civitas, a VA application for journalistic analysis and user-guided filtering using social media content. Vox Civitas filters out unrelated data by automatically computing time-dependent term-vector similarities. Twit-Info [28] aggregates streamed Twitter data and automatically discovers events from activity peaks in real-time. The authors assign relevance to a tweet by counting its number of event-related keywords. Pezanowski et al. [36] designed the geovisual analytics system SensePlace3 to provide situational awareness by leveraging geographical information and place-time-theme indexing with string-based queries for exploring datasets. SensePlace3 primarily relies on TF-IDF for tweet retrieval in response to user queries. However, these tools do not employ machine learning for relevance classification and do not integrate user feedback to improve their underlying models or algorithms. Visual analytics has also been increasingly used to improve various machine learning processes, such as feature selection [13], attribute weighting [48], and labeling [8], [9], [18], and even understanding the models themselves [22], [39], [42]. Sacha et al. [40] proposed a framework to discuss the various forms of human interaction with machine learning models in visual analytics systems and theorized that VA tools could increase knowledge and usability of machine learning components. Endert et al. [15] designed a system that classifies archived documents through user-guided semantic interactions (e.g., moving a document to another group) that improve the underlying model. Our work is based on the same idea in that we intend to improve model performance through user feedback, but with real-time social media data. Heimerl et al. [18] analyzed three separate methods for user-guided classification of a set of archived text documents: the basic method, which does not employ sophisticated visuals; the visual method, which visually represents the labeled and unlabeled documents for user exploration; and the user-driven method, which provides the user with full control over the labeling process. The first two methods employ active learning, in which the model selects a data sample to be labeled by the user that most effectively helps it distinguish relevant from irrelevant data. This contrasts with the user deciding which instances they wish to label. The authors did not find any statistically significant differences in terms of F 1 score between the methods in their user study. Bosch et al. [9] developed ScatterBlogs2, a VA application that provides user-guided learning of filter classifiers on historical social media messages to support situational awareness. These two works are perhaps the most similar to ours, yet differ in two fundamental ways. First, they do not provide interactive learning in real-time, which strains the user, as they are required to visit historical data for additional training. Second, they do not employ neural networks, which are better suited for online learning environments, such as social media streaming, in which training data is supplied sequentially over time [32], [33]. It is important to note that Bosch et al. [9] allow the user to adjust a filter's focus (i.e., how precise the classification is) in real-time if it misses relevant data or does not sufficiently filter out irrelevant data. However, this could indicate that the model has not properly learned the distinction between relevant and irrelevant data. Since training can only be completed with historical posts, the user is unable to update the model immediately with the streamed data, limiting situational awareness. Our approach not only solves this issue by allowing the user to immediately train the model for improvement, but also provides the user with the ability to create classifiers on-the-fly to accommodate their real-time needs. SECTION 3 Interactive Learning Framework Our framework for interactively learning relevant social media posts in real-time consists of two primary components. The first is a formalized set of design goals necessary to effectively facilitate situational awareness in real-time through user interactivity. The second is a detailed underlying model that is adapted to user-guided training with real-time streaming data. In section 4, we discuss our implementation of the framework that realizes the design goals. 3.1 Design Goals The framework's design goals were iteratively defined through discussions with domain experts such as first responders who frequently use visual analytic social media applications for real-time situational awareness. In general, these experts found it necessary for the interactive framework to incorporate user feedback in a timely manner, as well as account for time and situation-dependent user needs. With their feedback, the following specific design goals were established: DG1 Filter and view relevant data: Filtering data by relevancy removes noisy data, allowing the user to more quickly find data that may require immediate attention or contain important information. The ability to view the relevant data itself is equally important for determining the urgency and content of relevant data. DG2 Correct incorrect classifications: Since classifiers may provide incorrect results, especially during the early stages of training, it is necessary for the user to be able to correct the label in realtime. This both improves the model's performance and lowers the likelihood that incoming streamed data will be incorrectly classified and missed. DG3 Create new classifiers in real-time: The needs of the user can change dramatically over time and vary across users themselves. As an example, one user may wish to train a classifier to find data related to a specific hurricane event to expedite identification of people in desperate need of assistance. However, another user may wish to find data related to safety in general, not just a hurricane. As such, they should each be able to create and train their own classifiers in real-time specific to their needs at the time. DG4 Minimize model training time: Although it is important to design a high-performing model, time constraints are equally important. Specifically, when the model is trained by user feedback, the user should not have to wait for several minutes for the model to be retrained and relabel data. Previously streamed data labels may update with retraining, allowing the user to potentially find important information that they had not seen before. As such, it is necessary to provide these updated results as quickly as possible for real-time situational awareness. 3.2 Workflow Fig. 2 shows the three primary components of our framework's workflow applied to streaming tweets (however, the framework can be generalized to other kinds of text). First, as tweets are streamed in real-time, they are vectorized using a word embedding model. Second, the vectorized tweets are provided as inputs to the neural network classifier (discussed in next section), which outputs a set of probabilities from the activation function of the tweet's predicted relevancy and assigns an unverified relevance label. Third, the labeled tweet is relayed to the user through the user interface. If the user identifies tweets with incorrect labels, they can correct the label for the system to retrain and improve the model for relevance predictions. Fig. 2. High-level workflow of our framework with three main components: Tweet vectorization, tweet classification, and user feedback. Show All 3.3 Interactive Model Details In the following subsections, we elaborate on the underlying representations and models used to support our interactive learning framework. We design, optimize, and evaluate our approach with the key assumption that classifiers are trained (from scratch) in real-time using user-provided labels for streaming text. We simulate this process by adding training examples in small batches of 10 and evaluating against testing data, as explained below. All simulations were completed on a server with 128 GB RAM, 32 TB of disk storage, and 2 Intel(R) Xeon(R) E5-2640 v4 CPUs at 2.40GHz. 3.3.1 Model Candidates Selecting the underlying model for our framework was a key task, as it must be efficiently trainable with a continual stream of user-labeled data (DG4). As discussed in Section 2, neural networks are a natural choice for online learning scenarios in which training data is supplied sequentially over time [32], [33]. In addition, neural networks have generated impressive results with Word2Vec [30] embeddings [32], [33], [44]. Therefore, we employ a neural network as our classifier to determine text relevance based on real-time training examples provided by the user. To convert the text into vector inputs (of our neural network), we use word embeddings generated by Google's Word2Vec skip-gram model [3], [30], which contains 3 million 300-dimensional word vectors pre-trained (and therefore, capturing word embeddings) on a subset of the Google News dataset with approximately 1 billion words. In selecting the specific neural network model type, we experimented with the well-known Convolutional Neural Network (CNN) [25], Long-Short Term Memory (LSTM) Neural Network [19], and Recurrent Neural Network (RNN) [14] since they have performed well in various text classification tasks [51]. Hybrid architectures, such as recurrent convolutional neural networks [24], have also been proposed in recent years, but have not been made available in well-supported libraries. Therefore, we did not consider them in this paper, since our goal was to also support a well-tested SMART 2.0 system for end users. Our CNN model contains the traditional convolutional and max-pooling layers before activation [51]. Specifically, we apply a 1-dimensional convolutional layer, 1-dimensional max-pooling layer, flatten the output, and then activate it with softmax and a dense layer. The filter and kernel sizes of the convolutional layer are optimized during the hyperparameter stage (explained in Section 3.3.4). We use Hochreiter's LSTM [19] and the traditional RNN [14] architectures as provided by Keras [2]. The LSTM and RNN hidden layer each contain 300 hidden neurons and use softmax activation. 3.3.2 Design As mentioned before, to enable the use of neural networks for classifying text, we convert the unstructured text (of the tweets) into vectors ready for consumption by the neural network. When using Word2Vec vectors as features for classification, a common approach is to convert each word in the sentence to its vector, average the word vectors in the sentence, and then use the resulting feature vector for model training [4], [43]. However, averaging the vectors results in the loss of syntactic information, which can negatively impact classification results [27]. As an example, the two sentences “Only Mary will attend the ceremony.” and “Mary will only attend the ceremony.” would generate identical averaged sentence vectors since they contain the same set of words, but they differ in meaning. Therefore, to capture both semantic and syntactic information, we represent a sentence as a matrix where each row i is a 300-dimensional Word2Vec vector corresponding to word i in the original sentence. The input to the neural network consists of the matrix representing the sentence (as described above) and the output consists of the classification labels for the input sentence (Fig. 2). Specifically, we allow a tweet to be (1) Relevant, (2) Not Relevant, or (3) Can't Decide. The label with the highest probability from the activation function corresponds to the final label given to it. The “Can't Decide” label indicates that the tweet may or may not be relevant depending on the context. This is useful if the user finds a social media post such as “Remembering when Hurricane Irma destroyed my home…” that may not directly relate to the current event, but may be semantically relevant, and the user does not want to mark such cases as “Not Relevant”. This gives the user more flexibility to accommodate their needs since the definition of relevancy will depend on both the user and the situation. 3.3.3 Corpus for Model Selection and Optimization To experiment with different neural network model types and optimize the selected model, we used a disaster-related corpus annotated on the crowd-sourcing platform, Figure Eight [1]. The dataset contains 10,876 tweets related to different types of disaster events, such as hurricanes and automobile accidents. The data was collected using keywords such as “ablaze” or “quarantine”, and therefore, covers a wide variety of disaster-related topics. Our main motivation for using this open dataset is its size (as well as topical relevance), enabling the optimization of hyperparameters and comparison of various models. In the corpus, each tweet is manually labeled by Figure Eight's workers as “Relevant”, “Not Relevant”, or “Can't Decide”, and the distribution of labels is unbalanced. Specifically, there are 4,673 “Relevant” instances, 6,187 “Not Relevant” instances, and 16 “Can't Decide” instances. This dataset has been used in other tweet classification research projects [45]. However, the researchers of that study remove the tweets with the “Can't Decide” label to improve training data quality. As explained in the previous section, we find the “Can't Decide” option useful for users to apply to cases with insufficient context for relevance determination. We randomly shuffle the data and divide the dataset into 80% training, 10% validation, and 10% testing sets. It is important to note that we only use the Figure Eight dataset to optimize the hyperparameters and provide an initial evaluation of the model by simulating the provision of labels in real-time by the user. Since each tweet in the dataset contains true labels that were manually assigned by humans, it allows us to evaluate the model performance by comparing the model's predictions to the true labels after each training iteration. Our proposed approach as well as its integration within the SMART 2.0 system, however, allows for the creation of the models from scratch (with no prior training) (DG3), leveraging real-time labels provided by users on streaming data for training. 3.3.4 Optimization In order to experiment with the different neural network model types, we ran several training simulations with random combinations of hyperparameters (i.e., random grid search) to see which model converged to the best F 1 score. The F 1 score is a metric widely used to evaluate the quality and performance of machine learning models and neural networks [41]. It is computed as the harmonic mean of precision (the proportion of true positive predictions compared to the total number of positive predictions) and recall (the proportion of true positive predictions compared to the overall number of positive instances) : F 1 = 2×precision×recall precision+recall . The F 1 score provides a balanced measure, combining these two performance aspects. It is therefore more informative compared to other metrics such as accuracy, especially when the training and testing sets are imbalanced [11], as in our case. A central part of our approach to the training, validation, and verification of learning models is simulating the interactivity of visual analytics for real-time data, i.e. for use cases in which training data does not exist prior to user interaction. We assume the user (re)labels the incoming stream of data and therefore iteratively trains a model, which consequently meets their real-time needs. To replicate this process, we computationally evaluate the model's performance (as if it is successively trained by user-labeled data) by iteratively training the model with 10 new samples from the training dataset. We average the F 1 score obtained from each of these iterations and use the resulting number to measure the model's performance. In addition, we introduce a new variable, window size, for our training iterations. Specifically, due to the considerably small amount of training data provided by the user, we found that an appropriately small number of epochs (one forward and one backward pass over the training data in the model) was necessary to reduce performance degradation from initial overfitting. However, we also found that increasing the number of epochs could lead to higher F 1 scores as more data was provided. Thus, we use a sliding window of 110 samples that includes the (successively provided) new training data (10 samples) as well as the most recently used training data (100 samples) to both account for small amounts of training samples and increase the number of total training epochs for a given sample. We use the validation data to optimize the hyperparameters for each of the CNN, LSTM, and RNN models. Specifically, after each training iteration with 10 new samples, we evaluate the neural network's F 1 score on the validation set to view its simulated performance as if it was trained by gradual user labeling. After identifying the optimal hyperparameters for each of the CNN, LSTM, and RNN models, we evaluate their performance on the testing set. Table 1 demonstrates the results from our validation stage. Specifically, it lists the average F 1 score obtained during each training simulation along with the total CPU time required to complete the simulation (accumulated with each training and evaluation iteration). Although in many applications, F 1 score alone is sufficient to evaluate machine learning models, it is not for ours. To see why, note that the LSTM model yields an F 1 score of 0.75, the highest of any hyperparameter combination. However, the LSTM model (with the highest F 1 score) takes approximately 4,242 seconds to complete training, whereas the CNN model (with the highest F 1 score) only takes 504 seconds. Thus, the LSTM model takes roughly eight times longer to simulate than the CNN model, but does not improve its F 1 score by a significant amount (LSTM: 0.75 vs. CNN: 0.74). In the context of interactive learning, we wish to balance the training/CPU time and performance such that the model both performs well and retrains in a short amount of time for rapid improvement (DG4). Therefore, it is necessary to consider both the CPU time and average F 1 score. With these optimization standards in mind, we chose the hyperparameters that yielded the highest F 1 scores for each model since the other hyperparameter combinations generated lower F 1 scores and higher or comparable CPU times. The selected combinations correspond to rows 1, 4, and 7 in Table 1 with the average F 1 scores in bold. The testing process is identical to the validation process: after the model is trained with 10 new samples, its performance is measured by computing the average F 1 score on the testing set (using the optimized hyperparameters from the validation stage). Our results are summarized in Table 2. We found that the LSTM model yielded the highest F 1 score of 0.75. The CNN and RNN models achieved a 0.73 and 0.70 F 1 score, respectively. Based on these results and the previously discussed optimization standards, we selected the optimized CNN model for our classifier. In particular, the CNN simulation not only yielded a competitive average F 1 score of 0.73, but also achieved this score 6 to 8 times more quickly than the LSTM or RNN (Fig. 3), which is significant in terms of responding to user feedback in a timely manner. Table 1. Average precision, recall, F 1 score, and CPU time for the top three performing hyperparameter combinations on each of the CNN, LSTM, and RNN models. Bold numbers correspond to the highest F 1 scores and lowest CPU times for each of the three model types. We report the recall, precision, and F 1 score to four decimal places (when necessary) to distinguish the average F 1 scores. Table 2. Testing results with the optimal hyperparameter combinations for the CNN, LSTM, and RNN models. The bold numbers correspond to the highest F 1 score and lowest CPU time among the three models. The optimized CNN model yielded 0.74 and 0.73 average precision and recall scores respectively (Table 2, row 1). This model performance may be due to the initial lack of sufficient training data and difficulty in classifying certain tweets. For instance, after examining the testing dataset, we found that many misclassified tweets were extremely short (e.g., the tweet “screams internally” was misclassified as “Relevant”) or contained complex disaster-related diction (e.g., the tweet “emergency dispatchers in boone county in the hot seat” was misclassified as “Relevant”). However, as we demonstrate in the next section, our model still outperforms state-of-the-art learning models on tweet datasets. It is worth noting that we do not save the trained model from the validation or testing stages for evaluation in the next stage (or for use with SMART 2.0). We only save the optimized hyperparameters. This is because we assume that users start training a new model (for any event or topic they choose) by labeling the incoming stream of tweets. In this section, we optimized the model on a sufficiently large dataset that contained tweets related to several kinds of disasters. In the next section, we evaluate the model on datasets containing tweets on specific events, which is representative of cases for situational awareness. 3.3.5 Evaluation To further demonstrate the optimized CNN model's performance, we computationally evaluated it on wildfire, bombing, and train crash datasets from CrisisLexT26 [34], each of which contain approximately 1,000 tweets collected during 2012 and 2013 from 26 major crisis situations labeled by relevance. We apply a similar process to evaluate our optimized CNN model on these datasets as we did with the Figure Eight [1] dataset. Specifically, we split the data into 50% training and 50% testing sets (to replicate the experimental setting of To et al. [45], against which we will compare our results), train the model by supplying 10 tweets from the training set at a time (to simulate user labeling of streaming data), evaluate the resulting model on the entire testing set, and then average the F 1 scores for each evaluation. We summarize our results in Table 3 and graph the model's performance for retraining with 10 new incoming tweets in Fig. 4, 5, and 6. In addition, we report the average CPU times to train the model during a single iteration (10 tweets) with each dataset in Table 3. Since the datasets vary slightly in size, we only compute the averages from the first 45 iterations since the smallest dataset (Boston Bombings) required 45 iterations to complete the simulation. We found that per-iteration training was fast and approximately 0.5 seconds with each dataset, which meets our timing demands (DG4). Fig. 3. The total CPU time required for each model to complete the testing simulation. The CNN model is noticeably faster than both the LSTM and RNN models. Show All We obtained 0.71, 0.64, and 0.88 F 1 scores for the Colorado wildfires, Boston bombings, and NY train crash datasets, respectively. Interestingly, the variance of the F 1 scores over the datasets is significant. The textual data in the Boston bombings dataset, which yielded the lowest average F 1 score, was not as easy to separate into the different relevance categories by the model compared with the other two datasets. However, the F 1 score does eventually converge towards a higher value similar to the other datasets, indicating the potential presence of outliers during the first few training iterations. In addition, we found that the simulations converged to the average F 1 scores after training with approximately 190–230 tweets, depending on the dataset, meaning that users need to label 190–230 tweets to achieve the reported F 1 scores. However, the CrisisLexT26 datasets also correspond to specific events, such as wildfires. As such, we surmise that interactively training the model on specific, well-defined events will reduce the amount of training data needed to achieve satisfactory results than with generic constraints on relevance (e.g., a classifier about safety in general). Finally, we compare our results with the learning-based algorithm employed by To et al. [45], who also evaluated their model's performance with CrisisLexT26 datasets. In particular, their learning-based approach used Word2Vec, TF-IDF, latent semantic indexing, and logistic regression for classifying data as relevant or irrelevant. The authors of that study split the dataset into two equal parts: one for training and one for testing. They trained the model once (as opposed to our iterative approach) and evaluated on the testing set. Their algorithm was able to yield high precision scores between 0.85-0.95, compared to our scores of 0.64-0.86. However, their recall scores were approximately 0.22–0.45, considerably lower than our recall scores of 0.65-0.90. Therefore, our approach outperforms the learning-based model presented by [45], in terms of the overall F 1 score: our interactive approach achieves F1 scores of 0.64-0.88 (depending on the dataset) compared to 0.45-0.64 by [45]. The authors also presented a matching-based approach that achieved a much higher F 1 score of 0.54-0.92, which is comparable to ours. However, they generate the set of hashtags to be used for matching by scanning all of the tweets in the dataset. Since we assume the data is streamed in real-time, and therefore, not available altogether, we use an iterative learning approach. Fig. 4. Optimized CNN F 1 score per training iteration of 10 tweets with the Colorado wildfires dataset (Table 3). The F 1 scores are logarithmically fitted and intersect with the average F 1 score (0.7134) at 228 tweets. Show All Fig. 5. Optimized CNN F 1 score per training iteration of 10 tweets with the Boston bombings dataset (Table 3). The F 1 scores are logarithmically fitted and intersect with the average F 1 score (0.6410) at 184 tweets. Show All Fig. 6. Optimized CNN F 1 score per training iteration of 10 tweets with the NY train crash dataset (Table 3). The F 1 scores are logarithmically fitted and intersect with the average F 1 score (0.8792) at 191 tweets. Show All Table 3. Average precision, recall, and F 1 score for three CrisisLexT26 [34] datasets. SECTION 4 SMART 2.0 4.1 SMART The Social Media Analytics and Reporting Toolkit (SMART) [52], [53] is a visual analytics application designed to support real-time situational awareness for first responders, journalists, government officials, and special interest groups. SMART obtains real-time publicly available geo-tagged data from the Twitter streaming API. The user is able to explore the trending and abnormal topics on various integrated visualizations, including spatial topic model visualization and temporal views. The tweet time chart and theme river visuals convey the temporal distributions of topics if the user wishes to determine how the content of streamed social data has changed over time. SMART uses string matching-based classifiers to visualize relevant data. Specifically, the user can either (a) select pre-defined filters, such as Safety or Weather (Fig. 7(c)), each using a series of related keywords for inclusion and exclusion of tweets in the subsequent topic-modeling (Fig. 7(f)) and (geo)visualizations (Fig. 7(b)), or (b) create their own filters by supplying keywords, and intersect or union multiple filters according to their needs. However, keyword-based matching is insufficient for finding relevant information as it fails to accurately capture semantic relevance and therefore effectively filter out noisy data. As an example, if the user were to apply the Safety classifier, it would be possible for the tweets “My house is on fire!” and “I was just fired from my job.” to pass through the filter since they both include the keyword fire. However, the latter is unrelated to the intended semantic context of Safety and thus dilutes the filter's quality. To address this problem, we integrate our interactive learning framework (the focus of this paper) in the existing SMART application [52], [53] and seek domain expert feedback on the use of these models. We call the resulting extended application SMART 2.0. SMART 2.0 allows users to define string matching-based keyword filters (similar to SMART), but adds the ability for users to then iteratively refine and train the newly integrated models by labeling the filtered data as semantically relevant or not. In addition, the SMART 2.0 interface includes interactive visuals to facilitate user exploration, filtering, and refinement of relevant data (Fig. 7). As with the model simulations in Section 3.3, SMART 2.0's underlying models are trained with successive batches of 10 user-labeled tweets. In cases where model predictions conflict with user labels, user labels override the model's since they represent the ground truth. In addition, users should not need to manually relabel the same data multiple times. Although conflicts might indicate that the model is not sufficiently trained, the model trains with the same data during several successive iterations (as discussed in Section 3.3.4), so conflicts might be resolved after future iterations. 4.2 SMART 2.0 Interface The extensions to SMART 2.0's user interface, compared with SMART, concern the new interactive visuals that allow users to iteratively train machine learning models, utilize model predictions for rapid relevancy identification, and understand a model's reliability. The SMART 2.0 interface (Fig. 7) extends the interactive features of SMART for relevance identification in three primary ways: Extending the tweet table (containing a tweet's creation date and text) by including the predicted relevance label, relevance label probabilities, label modification, model training performance, and relevance filtering. Extending the interactive map containing the geo-tagged tweets whose relevancy can be individually inspected or modified. Altering the content of existing SMART views (e.g., topic models and spatial topic lenses) using either all data or only relevant data (as identified by the model and corrected by the user). 4.2.1 Table The SMART 2.0 table (Fig. 7(g)-(j)) is extended from SMART in that it not only provides a tweet's creation date and text, but also provides the predicted relevance label (Fig. 7(i)) and the probabilities of a tweet belonging to any of the relevance classes (Fig. 7((j)) (DG1). In particular, the relevance of a tweet can be “Relevant”, “Not Relevant”, or “Can't Decide”. The “Relevant” label is colored blue, the “Not Relevant” label red, and the “Can't Decide” label gray to visually separate tweets with different relevance. SMART's preexisting blue color scheme motivated us to use the blue, red, and gray diverging coloring for relevancy in order to maintain visual appeal and harmony. Fig. 7. SMART 2.0 overview: (a) The control panel provides several filters, visualizations, and views. (b) The content lens visualization provides the most frequently used words within a selected area. (c) The tweet classifier visualization provides keyword-based filters to help reduce noisy data. (d)(e) Clicking a tweet on the map with the tweet tooltip visualization displays the tweet's time, message, and relevance label. (f) The topic-modeling view, based on latent dirichl et al.ocation, extracts trending topics and the most frequently used words associated with each topic among tweets with specified relevancy. (g)-(j) The message table aggregates the tweets for efficient exploration with (g) the model's estimated classification performance ( F 1 score), (h) A drop down box to filter data by their relevance labels, (i) Color-coded relevance labels that can be changed by clicking on the label itself, and (j) Associated relevance probabilities. Tweet map symbols are colored orange and purple to distinguish twitter data from instagram-linked tweets, respectively, since the latter contains potentially useful images for situational awareness. Show All Users can directly click on relevance labels to correct the classifier's prediction (DG2). For instance, if a tweet is incorrectly marked “Relevant”, clicking the label will change it to “Not Relevant” or “Can't Decide”, depending on the label the user wishes to assign. Further, a drop down box is included at the top of the relevance label column (Fig. 7(h)), which provides the option to filter out data that does not have a specified relevancy (DG1). For example, by selecting “Relevant” from the drop down box, the table will remove tweets with labels “Not Relevant” and “Can't Decide” from all views and visualizations in SMART, including geovisualizations and temporal views. The table also displays the degree (or confidence) of a tweet's relevancy. In specific, the probabilities of a tweet being “Relevant”, “Not Relevant”, or “Can't Decide” are represented as a horizontal segmented bar graph and sized proportional to their respective percentages (Fig. 7(j)). In addition, the user can sort tweets based on relevancy probability in ascending or descending order. We provide the relevance probabilities and associated sorting actions as a supplementary relevance filtering mechanism (DG1). In particular, it is possible for tweets to be classified as “Relevant” by the model, for example, but with low confidence. The probability filtering allows the user to specifically view high-confidence relevant data and therefore further reduce potentially noisy data. The table provides a performance bar that encodes the estimated performance ( F 1 score) of the underlying learning model (Fig. 7(g)), as well as the number of user-labeled tweets, to inform the user of the model reliability. Since labeled testing data is not available to evaluate the model for real-time training (because we assume the user may train on any type of event data and has their own specifications for relevancy), the model's performance can only be estimated. Based on our evaluations in Section 3.3.5 with datasets typical of situational awareness scenarios (Table 3), the Colorado wildfires dataset generated the F 1 score (0.71) closest to the average of the three datasets (0.74). Therefore, we use the Colorado wildfires dataset's logarithmic trendline y=0.09  log e (x)+0.22 (Fig. 4) to approximate the model's F 1 score as a function of the number of user-labeled tweets. 4.2.2 Map The SMART 2.0 map is extended from SMART in that it includes a tweet's relevance label (which can be modified) in addition to its text and creation date (Fig. 7(d)(e)). Through the Tweet Tooltip, the user can directly click on tweet symbols on the map to view their text and associated relevancy (DG1). In addition, the user can correct the classified relevance label (DG2) by clicking on the label itself. Map inspection can allow the user to view and investigate potential geographical relevancy trends. For example, during crisis events, relevant tweets might be closely grouped on the map, so it may be more beneficial for the user to view predicted relevance from the map itself. The interactions between the table and map are synchronized. If the user relabels data on the map, the associated new label will also be updated in the table, and vice versa. In addition, selecting a relevancy filter from the drop down box in the table filters the tweets on the map. 4.2.3 Integration with Existing Visualizations Many of SMART's original visualizations, such as the topic-model views, spatial topic lenses, and temporal views help users make sense of spatiotemporal text data. Therefore, we integrated all of these views in SMART 2.0 with the relevance extensions. Users have the option to view only relevant or all the data (including irrelevant tweets) in various visualizations in case the interactive classifiers are not yet trained to desirable accuracies since, as we show in Section 3.3.5, classifiers typically require around 200 user-labeled tweets to achieve F 1 scores of 0.70-0.80. If they choose to view only relevant tweets, any relevance filtering action also updates the data used by other visuals. For example, the topic-modeling view (Fig. 7(f)) extracts the top 10 topics from the tweets and displays the most frequently used words for each topic. If the user filters out irrelevant tweets, the topic-modeling view will only be applied to the remaining relevant tweets. It is important to note that the majority of visualizations in SMART 2.0 require a minimum number of tweets in order to render. When filtered relevant data is scarce, visualizations do not populate, in which case users can individually inspect tweets. For instance, the topic-modeling view requires at least 10 tweets to extract topics. Overall, SMART 2.0's suite of visualization tools can be used in combination with relevance interactions to further understand trends and important spatiotemporal characteristics of relevant data. SECTION 5 User Experience In this section, we provide usage scenarios and feedback from domain experts that demonstrate our framework's effectiveness and usability. 5.1 Usage Scenario 1 Alice is an emergency dispatcher interested in identifying people in need for help or hazardous locations during a hurricane. She uses SMART 2.0 to find any related social media posts near the affected area. She adds a new filter Hurricane and provides an appropriate set of filter keywords such as “hurricane”, “help”, “blocked”, and “trapped”. After applying the Hurricane filter, she explores the filtered tweets in the table and finds a tweet labeled “Relevant” that says “Does anyone know how to get help setting up my TV?“. Since the tweet is unrelated to a hurricane, she relabels it as “Not Relevant”. After further browsing the table, she finds a tweet that says “The road near Taylor Loop is blocked from a broken tree.”, but it is labeled as “Not Relevant”. Since the tweet contains actionable information, she relabels it as “Relevant”. After labeling several more tweets for model training and noticing that the model predicts correctly, she decides to only view “Relevant” tweets and sort them by most relevant. She promptly identifies a tweet posted only a few minutes ago marked as highly relevant. It reads “Car just crashed into tree blocking road near Taylor Loop!”. Alice immediately notifies first responders of the location to provide assistance. By using SMART 2.0, Alice is able to identify important, relevant data more quickly through interactively training the model to remove noise and then filtering by relevance. 5.2 Usage Scenario 2 To demonstrate the generalizability of our framework to other domains, we applied our interactive framework in real-time during the Purdue vs. Virginia 2019 March Madness game in the Kentucky area. We assumed the role of a journalist who wanted to follow public discourse on the game by identifying the relevant tweets. We first constructed a Sports filter, which included keywords such as “Purdue”, “game”, “score”, and “#MarchMadness”. We then interacted with the streaming data by iteratively labeling the relevancy of tweets (from scratch) and found that the system correctly classified incoming data after roughly 80 training samples (Fig. 8). We noticed that the time intervals between successive trainings increased, indicating that it was more difficult to find incorrectly labeled data towards the end and that the model gradually learned from user feedback. In particular, the interval between the first and second training iterations was 2 minutes, whereas the interval between the final two was 4 minutes. 5.3 Domain Expert Feedback We piloted SMART 2.0 with two groups of first responders, each containing two individuals, who frequently use SMART during events for situational awareness in their operations. Both groups participated in separate 1-hour long sessions via conference call in which they iteratively trained a classifier from scratch and applied relevance filtering and visualizations to assess the implemented framework. They received a tutorial of SMART 2.0 30 minutes beforehand and were provided with web access to the system to complete the session. For both groups, we simulated the real-time use of SMART 2.0 by feeding in a stream of historical data on events (previously collected). For the first group, the system presented unlabeled tweets from the Las Vegas shooting on October 1, 2017 in the Las Vegas area. For the second group, we used unlabeled tweets from the October 2017 Northern California wildfires. We used historical event datasets to ensure the existence of sufficient training relevant samples for a situational awareness scenario. The domain experts in the first group applied the Safety, Damage, and Security filters during the iterative training process, resulting in 317 tweets. They trained on the same underlying model for all three filters, as they considered them semantically related. In total, after relabeling approximately 200 tweets, they indicated that they could trust the model to predict accurately and were pleased that the tweets they had not seen before from the Security filter were labeled correctly. Their definition for relevancy was tweets containing actionable information. For instance, they marked tweets containing information about road closures, blood drive locations, or death counts as relevant. They labeled data with general comments regarding the shooting, such as “I hope everyone is safe now…terrible shooting…”, as irrelevant since they did not provide actionable information. Interestingly, they also marked tweets that would influence public opinion (and therefore may cause action) such as those from bots or trolls as relevant since they still contained actionable information. The domain experts from the second group followed a similar process in which they applied the Safety, Damage, and Security filters, resulting in 445 tweets, and trained a learning model for relevance. They found that after training on roughly 67 tweets, the model satisfactorily predicted relevancy. As with the first group, these domain experts labeled tweets as relevant if they contained actionable information. The domain experts from both groups found SMART 2.0 to be easy to use and effective in identifying important data. For instance, they discovered relevant, actionable information after training the model: specific blood drive locations to aid shooting victims. Notably, the users mentioned that they felt less the need to relabel data as they progressed since the system provided more correct labels. They were pleased that they had the option to view only relevant data, but could see all of the data regardless of relevancy to avoid potentially missing important misclassified data, and that the model was responsive to user training. In addition, they found the relevance percentage bars to be helpful in determining the tweets that were potentially the most relevant. One concern the domain experts had was that SMART 2.0 does not indicate the number of tweets that are predicted as relevant. They felt this extension could help them infer the occurrence of events or potential crises. For example, the number of relevant tweets for the Safety classifier would likely increase significantly during a widespread disaster. We plan to introduce this feature in the next development cycle. However, we have added a visualization of estimated model performance in SMART 2.0 (Fig. 7(g)) to help users ascertain the reliability of a model's relevancy predictions. Overall, the feedback from the domain experts was positive and helpful, indicating the system's practicality and usefulness in facilitating real-time situational awareness. In addition, they have asked to use SMART 2.0 in their emergency operations center. SECTION 6 Discussion and Future Work Our interactive learning framework and SMART 2.0 integration were developed with the user in mind, influencing all of our design, computational evaluation, and implementation choices. Our user-centered model and SMART 2.0 application contribute to both the machine learning and visual analytics communities. We bridge the two fields by demonstrating how models can be interactively trained and evaluated while keeping the user in mind, and used to facilitate situational awareness for real-life, practical use. SMART 2.0 currently only collects English tweets, although supporting non-English languages (one at a time) with our current design (e.g., Spanish only) is straightforward since Word2Vec embeddings can be independently trained on a corpus in the target language [7]. Extending our system to support multilingual tweets would be a powerful asset, especially for multilingual users, in amplifying situational awareness by leveraging relevancy of tweets issued in different languages. However, the multilingual model performance evaluation and testing is an open area for research. In addition, determining the specifics of how a single relevance classifier might be trained with multilingual tweets requires careful attention. For instance, training iterations with Spanish tweets should also affect the relevancy of semantically-related English tweets. Since similar words in different languages likely have different vector representations (embeddings), multilingual mappings must be learned or training must be performed differently, such as with parallel corpora [7]. Multilingual support also requires changes in SMART 2.0's language-dependent visualizations, such as the topic-modeling view (Fig. 7(f)). Translation to a unified language or extracting topics separately for each language are two potential solutions. Fig. 8. Tweets with correctly predicted relevancy from the Purdue vs. Virginia 2019 March madness game after the user (re)labels 80 tweets. Show All The scalability of our framework is a natural concern, especially since SMART and many deployed real-time visual analytics applications contain multiple users who require responsive interfaces while monitoring crisis events. We deliberately designed the framework architecture with scalability in mind. As mentioned in Section 3.3.4, we selected the model and optimal hyperparameters based on training/CPU time in an effort to maximize the model's computational speed. Further, SMART 2.0 filters and views at most 800–900 tweets at a time, although user-specified filtering (typical in situational awareness scenarios) reduces the data to only a few hundred tweets, as demonstrated in Section 5.3. It takes only 2-3 seconds to calculate and retrieve their relevance labels over the network from the server where the model resides, and per-iteration training is fast, as established in Section 3.3.5. Training a model during a particular event, such as a disaster, can be straightforward due to potentially larger amounts of relevant data. However, social media data during periods without major events are likely to contain very few, if any, relevant tweets. As such, if the user only trains the model on irrelevant data, it will poorly predict relevant data since it has only be taught what is irrelevant. Although the user can improve the model through training during a real-life disaster, they are required to know when and where the disaster occurs to begin training. This can be problematic if the user wishes to rely on relevancy predictions to detect hazardous situations. To accommodate time periods in which relevant data is scarce, we plan to introduce an interactive feature in which users can provide example tweets or external resources for specific relevance labels. For instance, if the user would like to train a Hurricane classifier before a hurricane event, they could provide a relevant text such as “I'm stranded by this hurricane. Please help!”. The model could then detect relevant tweets once the hurricane begins as opposed to requiring user training during the event. In addition, we plan to provide the user with the option to visit specific historical data to train existing classifiers, as done by Bosch et al. [9]. Our interactive learning performed well on target datasets (i.e., wildfire, bombing, and crash) as explained in Section 3.3.5. Specifically, it required the users to label approximately 200 tweets to achieve acceptable F 1 scores. However, tweets are short, and therefore, more research is required to investigate the suitability of our approach for “general” classifiers, such as ones that learn to classify relevant data to “safety”. As safety can be affected by many events or situations, the model may need additional training that is typical of a targeted dataset. Finally, although we rigorously optimize and evaluate our machine learning model, the hyperparameter combinations were only tuned with the Figure Eight dataset [1]. Since optimal hyperparameters can depend on the dataset itself, it is possible our model may not be optimally tuned for different datasets, even though that optimization may be negligible from a user standpoint. We did use other datasets in our model evaluation to show the satisfactory resulting performance (Section 3.3.5). Given that the Figure Eight dataset classifies generic events as relevant or irrelevant, as opposed to specific events, we expect that our model performs well on many different event types. SECTION 7 Conclusion We presented a novel interactive framework in which users iteratively (re)train neural network models with streaming text data in real-time to improve the process of finding relevant information. We optimized and evaluated a machine learning model with various datasets related to situational awareness and adapted the model to learn at interactive rates. According to evaluation results, our model outperforms state-of-the-art learning models used in similar classification tasks. Finally, we integrated our framework with the SMART application and extended it to SMART 2.0, allowing users to interactively explore, identify, and refine tweet relevancy to support real-time situational awareness. Our discussions with multiple first responders who use SMART 2.0 indicated positive feedback and user experience. In particular, their assessments demonstrated that our interactive framework significantly improved the time-consuming process of finding crucial information during real-time events. ACKNOWLEDGEMENTS This material is based upon work funded by the U.S. Department of Homeland Security (DHS) VACCINE Center under Award No. 2009-ST-061-CI0003, DHS Cooperative Agreement No. 2014-ST-061-ML0001, and DHS Science and Technology Directorate Award No. 70RSAT18CB0000004. Authors Figures References Citations Keywords Metrics Media More Like This An integrated approach of Genetic Algorithm and Machine Learning for generation of Worst-Case Data for Real-Time Systems 2022 IEEE/ACM 26th International Symposium on Distributed Simulation and Real Time Applications (DS-RT) Published: 2022 Design of Real-Time System Based on Machine Learning for Snoring and OSA Detection ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.

Paper 2:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: 
  Extract 2: 
  Limitations: >
  Relevance Evaluation: 
  Relevance Score: 1.0
  Inline Citation: >
  Explanation: The paper's relevance to the specified point in <review_intention> is that it provides an overview of online machine learning techniques for real-time data stream analytics, focusing on the importance of this type of analysis in cyber physical systems (CPS). It is in this context that the paper analyzes the applicability of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests.

 Full Text: >
Loading [MathJax]/jax/output/SVG/fonts/TeX/Main/Italic/Main.js Skip to main content Skip to article Journals & Books Search Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download full issue Future Generation Computer Systems Volume 90, January 2019, Pages 435-450 CPS data streams analytics based on machine learning for Cloud and Fog Computing: A survey Author links open overlay panel Xiang Fei a, Nazaraf Shah a, Nandor Verba a, Kuo-Ming Chao a, Victor Sanchez-Anguix a d, Jacek Lewandowski a e, Anne James b, Zahid Usman c Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.future.2018.06.042 Get rights and content Highlights • Cloud and Fog computing has emerged as a promising paradigm for Internet of things (IoT) and Cyber-Physical Systems (CPS). • One characteristic of CPS is the reciprocal feedback loops between physical processes and cyber elements (computation, software and networking), which implies that machine learning based data stream analytics is one of the core components of CPS as it extracts the insights and the knowledge from the data streams generated by various sensors and other monitoring components embedded in the physical systems, supports informed decision making, enables feedback from the physical processes to the cyber counterparts, and eventually facilitates the integration of cyber and physical systems. • There have existed many successful CPS data streams analytics based on machine learning techniques and thus it is necessary to have a survey on these machine learning techniques; and further, to explore how they should be deployed and integrated to the Cloud and Fog architecture for better fulfilment of the requirements, such as mission criticality and time criticality, of cyber physical systems. • To the best of our knowledge, this paper is the first to systematically study the machine learning techniques for CPS data stream analytics from various perspectives, especially from the time complexity’s point of view, which leads to the discussion and guidance of how the CPS machine learning methods should be deployed in the Cloud and Fog architecture. Abstract Cloud and Fog computing has emerged as a promising paradigm for the Internet of things (IoT) and cyber–physical systems (CPS). One characteristic of CPS is the reciprocal feedback loops between physical processes and cyber elements (computation, software and networking), which implies that data stream analytics is one of the core components of CPS. The reasons for this are: (i) it extracts the insights and the knowledge from the data streams generated by various sensors and other monitoring components embedded in the physical systems; (ii) it supports informed decision making; (iii) it enables feedback from the physical processes to the cyber counterparts; (iv) it eventually facilitates the integration of cyber and physical systems. There have been many successful applications of data streams analytics, powered by machine learning techniques, to CPS systems. Thus, it is necessary to have a survey on the particularities of the application of machine learning techniques to the CPS domain. In particular, we explore how machine learning methods should be deployed and integrated in Cloud and Fog architectures for better fulfilment of the requirements of mission criticality and time criticality arising in CPS domains. To the best of our knowledge, this paper is the first to systematically study machine learning techniques for CPS data stream analytics from various perspectives, especially from a perspective that leads to the discussion and guidance of how the CPS machine learning methods should be deployed in a Cloud and Fog architecture. Previous article in issue Next article in issue Keywords Cyber–physical systems (CPS)Machine learningCloud computingFog computingEdge computingAnalytics 1. Introduction and motivation In this section we discuss definitions of cyber–physical systems (CPS) and highlight application areas. 1.1. Cyber–physical systems: Definitions and characteristics Recent advances in computing, communication and sensing technologies have given rise to CPS, one of the most prominent ICT technologies that pervade various sectors of the physical world and also an integral part of everyday life [[1], [2], [3], [4]]. The term cyber–physical systems (CPS) was coined in the US in 2006 [5], with the realization of the increasing importance of the interactions between interconnected computing systems [6]. There have been various definitions of CPS, each of them throwing some light at some of the relevant factors. • The National Science Foundation [7] defines CPS as “Cyber–physical systems (CPS) are engineered systems that are built from, and depend upon, the seamless integration of computational algorithms and physical components. Advances in CPS will enable capability, adaptability, scalability, resiliency, safety, security, and usability that will far exceed the simple embedded systems of today. CPS technology will transform the way people interact with engineered systems — just as the Internet has transformed the way people interact with information. New smart CPS will drive innovation and competition in sectors such as agriculture, energy, transportation, building design and automation, healthcare, and manufacturing”. • Lee [1] defines CPS as “A cyber–physical system (CPS) is an orchestration of computers and physical systems. Embedded computers monitor and control physical processes, usually with feedback loops, where physical processes affect computations and vice versa”. • The National Institute of Standards and Technology [4] defines the subject of CPS as “Systems that integrate the cyber world with the physical world are often referred to as cyber–physical systems (CPS). The computational and physical components of such systems are tightly interconnected and coordinated to work effectively together, sometimes with humans in the loop” Despite their differences in length, detail and the semantics of some terms, there are some common characteristics that can be extracted from these definitions. More specifically, we argue that CPS have the following inherent characteristics: • Integration of cyber elements (computation, software and networking), engineered elements (physical processes)[[1], [7], [8], [9], [10], [11]], and human factors [4]. • Reciprocal feedback loops between physical processes and computations, (simulation and decision making), sensing and actuation elements, and monitoring and control elements [[4], [1], [8], [9], [12]]. • Networked physical components and tightly coupled, interconnected processes that require cooperation and coordination [[2], [4], [13]]. In addition to this, the National Institute of Standards and Technology also highlights the fact that CPS require the integration and cooperation of two technologies for successful deployment [4]. Firstly learning and predictive capabilities are necessary to provide the integration of physical and digital models and, more importantly, to provide the ability for the digital world to autonomously change its logic based on the state of the physical world (e.g., diagnostics and prognostics). Secondly, it is stated that CPS require open architectures and standards that provide for modularity and composability of systems, thus allowing complex and dynamic applications. Particularly, CPS interconnect virtual and physical worlds. The physical system typically has a virtual twin which can be used for monitoring and control. The desired predictive capabilities in CPS require these systems to potentially collect and analyse data from the physical and digital world. In the end, the predictive capability informs decision makers to take appropriate actions or control to change the course of physical world. Finally it should be highlighted that current applications of CPS include automotive systems, manufacturing, medical devices, military systems, assisted living, traffic control and safety, process control, power generation and distribution, energy conservation, HVAC (heating, ventilation and air conditioning), aircraft, instrumentation, water management systems, trains, physical security (access control and monitoring), asset management and distributed robotics (telepresence, telemedicine) [1]. 1.2. Data stream analytics in CPS Mining data streams, acquired from various sensors and other monitoring components embedded in the physical systems, plays an essential role in CPS. It extracts insights and knowledge, provides learning and predictive capabilities for decision support and autonomous behaviour, enables the feedback from the physical processes to the cyber counterparts, and eventually facilitates the integration of cyber and physical systems [14]. Silva et al. [15] provides a formal definition of a data stream as: A data stream S is a massive sequence of data objects X 1 , X 2 …, X N , i.e.,  S = X i i = 1 N , which is potentially unbounded ( N → ∞ ). Each data object is described by an n-dimensional attribute vector X i = x j i j = 1 n belonging to an attribute space Ω that can be continuous, categorical, or mixed. Data streams feature massive, potentially unbounded sequences of data objects that are continuously generated at rapid rates [15], which leads to the fundamental shift in the data analytics (information source) from traditional a priori information alone based or off-line batch approaches, to stream analytics. The key challenge in stream analytics is the extraction of valuable knowledge in real time from a massive, continuous and dynamic data stream in only a single scan [16]. The reader should additionally consider that the insights extracted from physical devices, such as sensors, feature perishable insights, i.e., they have to be provided quickly, as otherwise they lose value to feed the logic of the CPS software. In CPS, data streams are most beneficial at the time they are produced, as any change reported by the data (e.g. a sensor anomaly, a fault in the physical process being sensed, or a change of system state) should be detected as soon as possible and acted upon. Furthermore, as opposed to stream analytics for purely software systems, the insights being revealed by data streams in CPS are often tied to a safety-critical action that must be performed to ensure the health of the CPS itself [14]. Analysis of these ever-growing data streams becomes a challenging task with traditional analytical tools. Innovative and effective analytic techniques and technologies are required to operate, continuously and in real-time, on the data streams and other sources data [17]. Machine learning is a discipline that aims to enable computers to, without being explicitly programmed, automate data-driven model building and hidden insights discovery, i.e., to automate behaviour or the logic for the resolution of a particular problem, via iterative learning from example data or past experience [[18], [19], [20]]. In the past, there have existed many successful applications of machine learning, including systems that analyse past sales data to predict customer behaviour, optimize robot behaviour so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data [20]. In this particular survey, we will focus on stream analytics methods that are based on machine learning algorithms. 1.3. Cloud and fog computing The interconnection of sensor and actuator systems with decision making and analytics have traditionally been performed by either local static controllers or uploaded to the Cloud for analysis. Supported by the paradigms of Internet of Things (IoT), Cloud computing experts propose the virtualization of devices to provide their data-based capabilities and their connection as a service for users within a Sensing and Actuation as a Service (SAaaS) [21] or as Things as a Service (TaaS) [22]. Another role that Cloud computing has played in supporting CPS is focused on the analysis of the data received from devices. The Cloud can provide a vast number of processing and storage resources which can be used to analyse large amounts of data [23] or streams [24]. These Cloud capabilities are concentrated in centralized and remote data centres, which has several drawbacks. The security aspect of storing, analysing and managing data in the Cloud is an increasing concern [25], while the remote nature of the Cloud also has reliability and latency issues [26]. The paradigm of Fog computing as proposed by [27] extends the Cloud to the edge of the network to better utilize resources available on gateways and connected devices. This extension allows data to be stored and processed locally to increase reliability and security, while decreasing the latencies between devices and the processing elements [28]. Fog computing systems are typically characterized by a large number of heterogeneous nodes, increased mobility and a strong presence of streaming and real-time applications [27]. The hosts or gateways used in Fog systems vary from PC based computing nodes [29], mobile devices [30] and resource constrained System on Chip (SoC) devices [31], routers, switches, set top boxes, proxy servers and base stations [32]. These hosts all have varying storage, processing and networking capabilities. While computing nodes have the most resources and are most reliable, they usually communicate with devices using Ethernet or Wi-Fi based networks. The mobile devices and SoC based devices have fewer resources but provide a wider range of wireless communication possibilities for polyglot gateways [33], that can be used to connect to a wider range of heterogeneous devices using low-power Machine to Machine (M2M) communication protocols. These distinguishing properties of the Fog are essential for providing elastic resources and services to end users at the edge of networks [28]. Fog computing is rapidly finding its way into CPS and IoT. Adopting IoT paradigms into CPS can provide several types of services, such as weather monitoring, smart grid, sensor and actuator networks in manufacturing environments, smart building control and intelligent transport. These services produce a large amount of data that need to be processed for the extraction of knowledge and system control [34]. The platforms deployed in Fog computing vary based on hosts and application domains, but they can be categorized in a similar way as in Cloud computing. Infrastructure based platforms allow users to deploy Virtual Machines (VM’s) [35] or lightweight virtualization images [36]. Platform based solutions as in [37] provide a platform for users for application-style system deployments. The third type of platform provides networking and analytics capabilities that the user can configure and use without the need to program and deploy their own applications. From the hosts’ perspective there are a number of differences between the Cloud and the Fog. The main difference is the resources. While the Cloud is considered to have virtually unlimited storage and processing capabilities, in the Fog such resources are much more restricted so their optimal management is crucial. Inter-host communication in the Cloud is fast due to high speed networks. In the Fog, due to wireless communication and varying network types, delays can occur and can vary largely in size between hosts. When we consider device to host communication, the Cloud adds significant networking delays when accessing remote devices. From a platform perspective we can see that Cloud solutions offer full control of resources using VM’s style solutions or other Platform as a Service (PaaS) options, while Fog solutions tend to share more interdependent and constrained resources between users. Cloud computing has a well-established business model as compared to the relatively new concept of Fog computing. However, this fact has been recognized by researchers and efforts can be seen in literature resolving billing, accounting, monitoring and pricing for a Fog business model [38]. CPS requires large computational capabilities to process, analyse, and simulate the collected data from sensors to make decisions and to instruct controllers, in a limited timeframe. The volume and velocity of sensor and visualization data in CPS requires a large amount of storage and software applications to process them. The division of the labour of latency tolerant and deep analytics tasks between Fog and Cloud depends upon processing power of the edge nodes and application domain. The edge nodes with limited computational power may only focus on performance of latency sensitive tasks. On the other hand, machine learning algorithms that require intensive computing resources should be executed in the Cloud. The Cloud service model with elastic and flexible architecture presents an appropriate solution to support the emerging CPS. However, the study on how data and applications should be distributed between edge devices and the cloud has derived little attention from the academic and industry research communities. This obviously includes the decision on where machine learning methods for stream analytics should be executed: the edge or the cloud. The existing machine learning methods with different processing properties have their own strengths and weaknesses, so several methods or their variants have been proposed to address diverse requirements from different applications. Some methods, for example, may cope better than others with incomplete datasets or large datasets, while some may require more computational power than others. Given the emerging and promising Cloud and Fog computing architecture and the foreseeable integration of CPS, more specifically the machine learning based data analytics in CPS, it is necessary to investigate what machine learning techniques have been employed in the context of CPS. Further we should consider how they should be adapted and deployed in the Cloud–Fog-Edge architecture for better fulfilment of the requirements of the application, such as mission criticality and time criticality. This research aims to identify and analyse the properties of current well-known machine learning methods employed in the context of CPS and the characteristics of stream data in CPS to provide a comprehensive analysis on their relation. This will help determine how to map data and machine learning methods to the Cloud and Edge computing to meet CPS requirements. More specifically, we will focus on the analysis of the machine learning models employed in stream analytics from the perspective of time complexity. This measure will provide important indications to the appropriateness of Edge computing to host tasks, as it has limited computational powers, RAM and storage whereas the Cloud has more flexibilities, capacities and capabilities to deal with resource-intensive tasks on demand. The required qualities for the outputs and the types of results (e.g. precision and accuracy rates) have significant influence on the resources and response time of the selected methods, so the correlation between them should be investigated. To the best of our knowledge, this paper is the first to systematically study the machine learning based data stream analysis in CPS and its deployment in the emerging Cloud–Fog-Edge architecture. The remainder of the paper is organized as follows. We present related work in Section 2. In Section 3, the machine learning methods are reviewed from the perspective of the functions they provide for typical CPS applications. Then, the time complexities of general machine learning techniques are discussed in Section 4, along with how machine learning methods should be deployed for effective and efficient integration within Cloud and Fog computing architecture. We conclude the paper with some future research directions. 2. Related work Traditional CPS may have limited computation and storage capabilities due to the tiny size of the devices embedded into the systems. Chaâri et al. [2] investigated the integration of CPS into Cloud computing, and presented an overview of research efforts in three areas: (1) remote brain, (2) big data manipulation, and (3) virtualization. More specifically, real-time processing, enabled by offloading computation and big data processing to the Cloud, was explored. Nevertheless, Chaâri et al. [2] did not include an exhaustive analysis of the emerging Fog and Edge computing technologies, and how these technologies should co-operate with CPS. The authors in [16] and [15] presented a survey on data stream analytics from the perspective of clustering algorithms. Apart of summarizing the unique characteristics of data stream processing by comparison with traditional data processing, in [16], data stream clustering algorithms were categorized into five methods (i.e., hierarchical methods, partitioning methods, grid-based methods, density-based methods, and model-based methods). Similarly, [15] analysed 13 most relevant clustering algorithms employed in the context of data stream analytics. In addition to the categories listed in [15], the authors in [16] identified three commonly-studied window models in data streams, i.e., sliding windows, damped windows, and landmark windows. Differently to [15] and [16], we do not solely focus on clustering algorithms, but we also extend analytics to other types of machine learning algorithms. In [20], the authors studied machine learning techniques employed in transportation systems, and identified various conventional machine learning methods such as regression (linear regression, polynomial regression and multivariate regression), decision tree, artificial neural networks (ANNs), support vector machines (SVMs) and clustering. Despite the useful insights provided by the work, the analysis is exclusively carried out in the light of a very particular type of CPS application; and further, no advanced machine learning methods, e.g. deep learning methods, was introduced. The survey provided in [39] recognized the changes that were needed to move from a conventional technology-driven transport system into a more powerful multi-functional data-driven intelligent transportation system (D2ITS), i.e. a system that employed machine learning and other intelligent methods to optimize its performance to provide a more privacy-aware and people-centric intelligent system. The paper identified both the data sources that drove intelligent transport systems (ITS), (e.g. GPS, laser radar, seismic sensor, ultrasonic sensor, meteorological sensor, etc.), and the learning mechanisms for real-time traffic control and transportation system analysis. Examples of the learning mechanisms were online learning (e.g., state-space neural network, real-time Kalman filter, combination of online nearest neighbour and fuzzy inference, hidden Markov model), adaptive dynamic programming (ADP), reinforcement learning (RL) and ITS-oriented Learning. The article offers a thorough and sound view on transport systems, but the insights are not extrapolated to other CPS domains and applications. The authors in [40] presented an analysis on a number of existing data mining and predictive machine learning methods for big data analytics with the goal of optimizing the dynamic electrical market and consumers’ expectations in the smart grid. Similarly, authors in [41] review the benefits and gaps of the combination of artificial neural networks, genetic algorithms, support vector machines and fuzzy logic for the forecasting of power grid. Another similar review is carried out in [42] to analyse the big data methods used to manage the smart grid. The authors identified different predictive tasks that can be carried out in the smart grid domain such as power generation management, power forecasting, load forecasting, operation and control fault diagnosis, and so forth. The authors mapped to the corresponding statistical or machine learning methods with the required data inputs or sources. 3. Machine learning methods in CPS applications In this section we consider some typical CPS applications. 3.1. Typical CPS applications Smart grid Smart grid is a complex system ranging from micro grid to national or international networks involving different levels of facility, management and technology. A smart grid is considered as a cyber–physical system as it monitors and manages the power generation, loading, and consumptions through a number of sensors. These sensors gather the stream data that is fed to analytic methods and control systems to balance and distribute power generation and consumption [43]. Due to complexity and dynamics of power market, and the volatile nature of renewable energy, it is important to have good forecasting and prediction on market trend and energy production to correctly estimate the amount of power to generate. In addition to this purpose, applications of analytics to the smart grid also include fault detection at infrastructure, device, system and application levels [10]. Machine learning is a promising tool to analyse the data stream and providing results to inform decisions and actions. Intelligent Transportation Systems (ITS) An intelligent transportation system (ITS) is an advanced application which aims to provide innovative services relating to transport and traffic management, and enable users to be better informed and make safer, more coordinated, and smarter use of transport networks. ITS brings significant improvement in transportation system performance, including reduced congestion and increased safety and traveller convenience [[44], [45], [46]]. ITS meets the core characteristics of CPS. Enabled by Information and Communication Technologies (ICT), elements within the transportation system, such as vehicles, roads, traffic lights and message signs, are becoming intelligent by embedding microchips and sensors in them. In return, this allows communications with other agents of the transportation network, and the application of advanced data analysis and recognition techniques — to the data acquired from embedded sensors. As a result, intelligent transportation systems empower actors in the transportation system to make better-informed decisions, e.g. whether it is choosing which route to take; when to travel; whether to mode-shift (take mass transit instead of driving); how to optimize traffic signals; where to build new roadways; or how to hold providers of transportation services accountable for results [[39], [46]]. Smart manufacturing/industrial 4.0 Manufacturing applications, such as object detection, force and torque sensor based assembly operations, require accuracy of object detection, pose estimation and assembly to within few micrometres. Moreover, this accuracy has to pass the test of time and repeatability (i.e., the results should be precise). Manufacturing in general and automotive manufacturing in particular, requires operation involving handling, inspection or assembly to be completed in few seconds. For example, BMWs mini plant in Oxford has a car coming of production line every 68 s [47]. Applications, such as welding, require real time data processing, analysis and results. For example, to track the position of joining plates on real time basis and adjust the movement of weld guns on real time basis for precise and accurate welding at high speed [48]. 3.2. Machine learning in a nutshell Machine learning is the discipline that aims to make computers and software learn how to program itself and improve with experience/data, with the goal of solving particular problems [49]. Typically, a machine learning algorithm is a specific recipe that tells a computer/software how to improve itself from experience. A model is the result of training a machine learning algorithm with a set of data or experiences of a given problem, and it can be employed to solve future related problems. Machine learning algorithms fall into one of the following categories: supervised learning, unsupervised learning, and reinforcement learning. Next, we briefly discuss each of these categories and describe some of the most relevant techniques for each category: • In supervised learning, the aim is learning a mapping from an input to an expected output that is provided by a supervisor or oracle (i.e., labelled data) [18]. Depending on the type of output, we say that we either have a classification or a regression problem. In the first case, we aim to produce a discrete and finite number of possible outputs, while in the second case the range of possible outputs are infinite and numeric [18]. • In unsupervised learning, there is no such supervisor and only the input data is present. The aim of these algorithms is finding regularities in the input [[18], [20]]. • Finally, reinforcement learning applies to the cases where the learner is a decision-making agent that takes actions in an environment and receives reward (or penalty) for its actions in trying to solve a problem. Thus, the learning process is guided by a series of feedback/reward cycles [20]. Here, the learning algorithm is not based on given examples of optimal outputs, in contrast to supervised learning, but instead it must discover them by a process of trial and error [50] Next, we describe some of the most usual machine learning algorithms employed in the context of CPS data stream analytics. Decision trees and random forests A decision tree is a supervised machine learning algorithm that is organized in a tree-like hierarchical structure composed by decision nodes and leaves. Leaves represent expected outputs, and decision nodes branch the path to one of the expected outputs according to the value of a specific input attribute. Decision tree algorithms exist in the form of classification and regression algorithms [18]. One of the main advantages of decision trees is that the model is human readable and understandable. A Random Forest is an ensemble of random trees constructed by means of bagging. By this process, a training dataset of N samples is divided into k different datasets of N ′ samples uniformly sampled with replacement from the original dataset, and consisting of a random selection of the input attributes. Then, each dataset is employed to train a different decision tree, guided by the heuristic that the combination of the resulting models should be more robust to overfitting. Each tree provides an output that can be aggregated by a wide variety of rules [[51], [52]]. Artificial Neural Networks (ANNs) and variants ANNs are machine learning algorithms that resemble the architecture of the nervous system, organized as interconnected networks of neurons organized in layers. These versatile algorithms are typically employed for supervised, unsupervised, and reinforcement learning. The inputs of the network (input layer) are transformed by weighted (non) linear combinations that generate values that can be further transformed in other layers of the network until they reach the output layer. Due to their ability to represent potentially complex relationship between the inputs and the expected output, ANNs, such as the multilayer perceptron (MLP), have gained popularity in machine learning and data analytics realm. The multilayer perceptron is a nonparametric estimator that can be used for both classification and regression. Convolutional Neural Networks (CNNs) exploit translational invariance within their structures by extracting features through receptive fields and learning by weight sharing. CNNs usually include two parts. The first part is a feature extractor, which learns features from raw data automatically and is composed of multiple similar stages and layers. The second part is a trainable fully-connected MLP or other classifiers such as SVM, which performs classification based on the learned features from the previous part [[53], [54]]. Recurrent Neural Networks (RNNs) are a family of neural networks that has gained popularity in the last few years [55], and they are of special relevance to stream analytics due to this characteristic. In addition to this, the surge of data and computing power present in the last decade have given rise to deep neural networks [56] that stack multiple non-linear layers of neurons to represent more complex relationships between inputs and outputs or more efficient representations of the inputs. For various closely related definitions of deep learning, please refer to [56]. Support Vector Machines (SVMs) Support vector machines (SVMs) are supervised learning methods that classify data patterns by identifying a boundary or hyperplane with maximum margin between data points of each class/category [[20], [51]]. The support vector machine is fundamentally a two-class classifier, although multiclass classifiers can be built up by combining multiple two-class SVMs. Despite the fact that they were initially devised for classification tasks, SVMs have been further extended to regression problems [57]. Bayesian networks and variants Bayesian networks are probabilistic graphical models based on directed acyclic graphs where the nodes are random variables and the direct arcs indicate the direct influences, specified by the conditional probability, between two random variables [[18], [58]]. Some popular machine learning algorithms such as Naïve Bayes, a popular supervised classifier, and Hidden Markov models (HMMs) can be considered as special cases of Bayesian networks. The second specializes at processing sequences of outputs by learning implicit states that generate outputs [[18], [50]]. This paradigm has been used for both supervised and unsupervised tasks. Evolutionary computation Evolutionary Computing is the collective name for a range of problem-solving techniques based on the principles of biological evolution, such as natural selection and genetic inheritance. The fundamental metaphor of evolutionary computing relates this powerful natural evolution to a particular style of problem solving — that is a pseudo trial-and-error guided by the value of a given fitness function that measures the goodness of the evolved individual/solution [59]. Evolutionary computing techniques mostly involve metaheuristic optimization algorithms, such as genetic algorithms and swarm intelligence. Genetic algorithms have been employed in supervised [60], unsupervised [61], and reinforcement learning problems [62]. Clustering Clustering is an unsupervised family of algorithms that involve processing data and partitioning the samples into subsets known as clusters. The aim of this process is to classify similar objects into the same cluster while keeping dissimilar objects in different clusters [16]. The separation criteria may include (among others) maximization of similarities inside clusters, minimization of similarities between different clusters, and minimization of the distance between cluster elements and cluster centres. One of the most popular clustering algorithms is called k-means clustering where k denotes the number of clusters. Self-organizing map (SOM) SOM is an automatic data-analysis method widely applied to clustering problems. SOM represents a distribution of input data items using a finite set of models. These models are automatically associated with the nodes of a regular grid in an orderly fashion such that more similar models become automatically associated with nodes that are adjacent in the grid, whereas less similar models are situated farther away from each other in the grid. This organization, a kind of similarity diagram of the models, makes it possible to obtain an insight into the topographic relationships of data, especially of high-dimensional data items [63]. Q -learning Q -learning is a kind of reinforcement learning technique that is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states [64]. 3.3. Machine learning methods in CPS Table 1 shows an overview of machine learning methods where they have been used in the loose context of CPS. They have been used to carry out tasks in three different application domains: smart grid, transport and manufacturing. ANN is one of the most popular methods having been used in the various application domains, as it is capable of performing long term forecasting by regressing the time series stream to predict trends. For example, in smart grid and manufacturing, ANN can efficiently predict energy consumption of consumers which is beneficial for demand side management. Only few researchers use ANN for real-time or short-term predication [[84], [68]], as it requires considerable time to process and tune the parameters before it can be deployed. Most applications require large amount of input data and training time to produce meaningful models with certain degree of accuracy and confidence [[65], [41], [70]]. ANN can work alone and produce acceptable results, but it often works with other learning methods such as SVM, GA, Bayesian etc. to improve training efficiency or modelling accuracy [41]. In the table, the term ANN was broadly used, but it has a lot of variants with various activation functions and structures forming a hybrid model to meet the purposes such as forecasting, classification, clustering, and regression for various applications. [41] has carried out detailed analysis of these variations and hybrid approaches. Here, we classify applications into this category using ANN as the main body for their solutions. Table 1. Overview of machine learning methods in the context of CPS. Machine learning method Domain Functional category Task Reference ANN Smart grid Forecasting/Prediction/Regression Electrical power prediction, load forecasting [[65], [66], [67], [68], [69], [41]] Transport Pattern recognition/ Clustering Behaviour/Event Recognition [51] Forecasting/Prediction/Regression Traffic flow features [70] Road-side CO and NO2 concentrations estimation [71] Travel time prediction [[72], [73], [74]] Classification Obstacle detection and recognition [75] Image processing [76] Manufacturing Forecasting/Prediction/Regression/ optimization Energy Consumption/ Process parameters optimization [[77], [78]] Random forest Smart grid Forecasting/Prediction/Regression Demand side load forecasting/Price forecasting [[65], [79]] Anomaly/Fault detection Power record faults [80] Transport Pattern recognition/Clustering Behaviour/Event recognition [51] Manufacturing Anomaly/Fault detection Tooling wear/Errors detection [[81], [82], [83]] SVM Smart Grid Forecasting/Prediction/Regression Price prediction [[84], [85]] Electrical power prediction, [[86], [67], [69], [87]] Anomaly/Fault detection Non-technical loss detection [[69], [88], [89]] Blackout warning [[86], [90]] Power line attacks [90] Transport Classification Unintentional vehicle lane departure prediction [91] Obstacles classification [[92], [75]] Pattern recognition/Clustering Behaviour/Event recognition [[51], [93]] Anomaly/Fault detection Mechanism failure [94] Forecasting/Prediction/Regression Travel time prediction [[95], [74]] Manufacturing Forecasting/Prediction/Regression Machine maintenance [96] Design/Configuration Feature design; Production processing [[97], [98]] Anomaly/Fault detection Quality control [[99], [100]] Smart home Pattern recognition/Clustering Activity recognition [[101], [102]] Decision tree Smart grid Anomaly/Fault detection Fault detection predict an energy demand [[103], [104]] Forecasting/Prediction/Regression [104] Transport Forecasting/Prediction/Regression To predict the traffic congestion level and pollution level; bus travel time [[105], [106]] [106] Anomaly/Fault detection Cyber attacks/detect stereotypical motor movements [107] Manufacturing Classification/Diagnosis Quality control/Fault diagnosis [[108], [109]] Bayesian network Transport Classification Event and behaviour detect [51] Smart grid Anomaly/Fault detection Non-technical losses and fault detection [103] Manufacturing Anomaly/Fault detection Fault detection in the production line [110] Forecasting/Prediction/Regression Tool wear prediction/Energy consumption prediction [[111], [112]] Self-organizing map Transport Clustering Obstacle detection and recognition [75] Evolutionary computing Smart grid Optimization/Forecasting/Prediction Short term load forecasting [113] Swarm computing Smart grid Optimization economic load dispatch/feature selection [[114], [115]] Manufacturing Anomaly/Fault detection/Process optimization Fault detection, classification and location for long transmission lines/Process optimization Automatic fault diagnosis of bearings [[116], [117], [118]] HMM Smart grid Optimization Optimal decisions on smart home usage [119] Manufacturing Anomaly/Fault detection Automatic fault diagnosis of bearings [[117], [120]] Reinforcement learning/ Q -learning-based ADP algorithm Smart grid Optimization Aided optimal customer decisions for an interactive smart grid [119] Transport Optimization the road latent cost [121] Deep learning/Autoencoder model/ convolutional neural network (CNN)/Recurrent Neural Networks (RNNs) Smart grid Forecasting/Prediction/classification/ Regression Building energy consumption [122] Transport Traffic flow prediction; processing roads images/commanding steering; detecting train door anomaly and predicting breakdowns Anomaly-based detection of malicious activity [[123], [124], [125], [126], [94]] Other To classify various human activities; To detect congestive heart failure [54] Other SVM has been widely adopted to address the issues in product feature design, fault detection, forecasting, clustering and pattern recognition across the application domains such as manufacturing, smart grid, transportation as well as smart home due to its maturity and transparency. The method can take different sizes of input data to carry out classification and regression, so it has been used in applications that require a short response time such as described in [[85], [86]]. It also used in conjunction with other machine learning methods such as ANN, and Bayesian by exploiting its characteristics to provide complimentary functions to address complex problems [[68], [96], [97]]. The authors in [97] used a trained SVM classifier from the classified design examples such as features and components, which are obtained from a hierarchical clustering, to recommend different additive manufacturing design features. In the case study, it only shows 21 design features from hundreds that were used to train and to build the model. The faults in products or tools in manufacturing can lead to a big loss of time and a serious consequence if they are not detected and resolved earlier. Authors in [81]and [82] reported the use of the Random Forest to analyse the big data for tooling condition monitoring in milling production and silicon in semiconductor manufacturing. It also has been used in predicting the short term electricity price from the historical data [79] and detecting false electricity records from the sensors [80]. [51] reported the use of Random Forest to model a driver profile effectively. These applications required a reasonably large amount of historic data for the training to achieve accurate and time was not considered to be a crucial factor. Decision Tree is a well-known method for classification, so it is predicable that the researchers have used it to detect the faults in power system and motor movement and for quality management in production. It also has been used to predict energy demand, bus travelling time, and to determine the correlation between traffic congestion and air pollution. The accuracy of fault detection, quality prediction, classification and rare events forecasting are associated with probabilities, as all the input factors cannot be certain due to the dynamic environments and complex human behaviour and interactions. Bayesian Network is a well-studied method to model complex probability networks as it has been used in different applications to explain the possible occurrences of outputs with input variables. It does not require a large amount of input data to form the network, if the probabilities of variables are known. The network can be large and complex, but its processing time is linear [[51], [103], [110]] showed the consistent characteristics in these applications. Table 1 also shows where the machine learning methods have been used across four application domains and the tasks that have been carried out to gain the benefits of analysing and interpreting large volumes of data streams generated. The most common area for researchers and industry practitioners adopting the methods is to increase accuracy of predication and forecasting in their CPS applications. The authors in [[41], [65], [66], [67], [68], [69], [79], [104]] reported adoption of machine learning to predict electrical power consumption, demand, supply and load in order to improve demand response management in Smart Grid. Machine learning is a well employed tool to predict traffic flow, air population emitted by cars, traffic congestion and travel time by transport [[70], [71], [72], [73], [74], [105], [106], [94], [123], [124], [125], [126]]. Machine learning also has been extensively applied in manufacturing by predicting energy consumption in production line, machine maintenance, and tool wearing [[77], [78], [102], [111], [112]]. Diagnosis and fault detection is another function for which machine learning has been widely used in the manufacturing domain. Fault detection applications include root cause of power faults in production, tooling wearing and mechanic faults, cause of the fault in components or products, and quality control [[51], [99], [100], [110], [117], [120]]. Smart Grid also has several machine learning applications for anomaly and fault detection such as non-technical loss detection, blackout warning, power line and cyber attacks, faults in demand management and power line faults [[69], [88], [89], [86], [90], [80], [103], [104]]. The utilization of machine learning for mechanical fault diagnosis and prevention of cyber attacks in transport systems can be explored further, as only two [[94], [107]] reported the benefits of machine learning in this area. Machine learning is also a popular solution to configure plant/production, optimize electrical load/dispatch, reduce road latent cost, and forecast short term in electricity usage [[75], [97], [98], [113], [119], [121]]. Machine learning has been exploited in other applications such as clustering road obstacles, classifying driving behaviours and traffic incidents and improving production quality [[51], [75], [91], [108], [109]]. From Table 1, it can be seen that functions of MLs have brought various benefits to different application areas and they have generated different levels of impacts in various areas, but the potential of machine learning is not fully realized yet, as the field is still evolving and the prevailing complexity may currently hinder take-up. 4. Temporal complexity analysis Machine learning algorithms are able to learn from selected samples to derive rules, trends, patterns or properties of a true population. The concept or hypothesis space, however, can be large and complex such that it cannot be learned or modelled in polynomial time. In these cases, learning to achieve highly accurate results by exhaustively exploring parameter values may not be possible practically but approximation may be achieved. As it is natural, the goal of all machine learning applications is to minimize the differences between the target concept and the output produced by the trained models. The representation, quality and quantity of the selected samples, which are input parameters to the learning algorithms, are important attributes to increase the possibility of the successful learning. The probability of reaching successful learning by increasing accuracy of approximating to the target concept also depends on the complexity of learning and time. Learning is a trade-off between time and accuracy. In principle, the higher accuracy, the more time is required for training. Information and computation are two main dimensions to measure the complexity of learning algorithms. The sample complexity is concerned with the number, distribution and sufficiency of training samples. The computational complexity of a solution method is the size of computational resources required to derive the concepts from the training data. This can be further classified into time and space complexity. Space complexity denotes the memory required for the computational model being selected to solve the problem. The time complexity is measured by the number of computational executions required to reach or approximate to the target concept. In this paper, we intend to show asymptotic time complexity rather than the actual runtime of the algorithms which will be various depending on its operating computational environment including hardware and software. Table 2 shows a list of machine learning methods used by the applications illustrated in Table 1 and their corresponding time complexities, represented by big O, and the factors contributing to the complexities. Since there are many different variants to each machine learning method, it is not feasible to list them exhaustively, but some examples to illustrate measurement of complexity are given. For example, varieties of Bayesian Network models derived from various approximate and exact inference algorithms to infer unobserved variables, can lead to different computational complexities. Several hybrid learning methods have been proposed to resolve or improve the insufficiency of one individual method. This complicates the measurement of the execution due to the interdependency, as one method may reduce the complexity for the other in the model, but the overall complexity calculation still needs to consider all the methods involved. More algorithms and their time complexity can be found in [127]. For example, [119] used Q -learning algorithms to model the interaction with users in smart homes There were a maximum of 20 steps to interact with users before appropriate recommendation could be made. Its asymptotic time complexity is up to 203 and the authors have concluded that Q -learning algorithm outperformed greedy or random decision strategies [119] in their simulated cases. Fig. 1 shows the complexity level in big O when the number of steps decreases in the simulation. The authors did not report the actual runtime, so the asymptotic complexity cannot be correlated with the experimental one. Table 2. Time complexity of some of the most common machine learning algorithms. Machine learning method Asymptotic time complexity Factors Decision tree learning [128] O ( M ⋅ N 2 ) M : size of the training samples N : number of attributes Hidden Markov model forward–backward pass [52] O ( N 2 ⋅ M ) N : number of states M : number of observations Multilayer perceptrons [127] O ( n ⋅ M ⋅ P ⋅ N ⋅ e ) n : input variables M : number hidden neurons P : number outputs N : number of observations e : Number of epochs Deep learning (convolutional neural networks) [129] O ( D ⋅ N ⋅ L ⋅ S 2 ⋅ M 2 ⋅ e ) L : number of input variables N :number of filters (width) S : spatial size (length) of the filter M : size of the output. D : number of convolutional layers (depth) e : number of epochs Support vector machine [130] O ( N 3 ) or O ( N 2 ) N : vectors C : upper bound of samples N 2 when C is small; N 3 when C is big Genetic algorithms [127] O ( P ⋅ log P ⋅ I ⋅ C ) C : number of genes/chromosome P : population size I : Number of iterations Random forest [[52], [131]] ( K ⋅ N ⋅ log N ) N : number of samples K : input variables Self-organizing map [132] O ( N ⋅ C ) N : input vector size C : cycle size Reinforcement learning [133] O ( N 3 ) N : number of steps to reach the goal Particle swarm optimization (PSO) [134] O ( P + G e n ⋅ P ⋅ D ) P : number of particles D : number of dimensions G e n : number of generations Bayesian network (exact learning models of bounded tree-width) [135] O ( 3 N ⋅ N ( w + 1 ) ) N : size of nodes W : width of tree. [66] used three machine learning methods, SVM, LS-SVM and BPNN, for energy usage forecasting over 283 households with 500 point data (hours) for each. The total number of data points for training in the experiments was 141,500 ( 283 ∗ 500 ) . In their empirical study, the computational times of these methods are 335.39, 26.22, and 29.28 s respectively over a laptop to produce reasonably accurate results. The authors recommend running these approaches in Cloud and distributed computing to improve performance. SVM has better accuracy in reducing errors, but it took more time than others due to the overhead of using GA to find key parameters for SVM. The BPNN has more errors than the other two and it requires more runtime than LS-SVM. The authors, however, did not include key parameter values such as generations and input points for GA and BPNN, so cannot derive their time complexity in relation to actual runtime. The time complexity of LS-SVM is O (141,5002). Fig. 2 shows the time complexity of LM-SVM by applying the data from [66] with simulation output and the actual runtimes in seconds. Download : Download high-res image (86KB) Download : Download full-size image Fig. 1. Complexity level and number of steps in Q -learning. It shows actual runtimes against the complexity level and the correlation between them without carrying out the actual experiments, the researchers can estimate its actual runtime by giving the number of samples when the underlying machine or environment has the same characteristics. The authors in [136] report the applications of the Particle Swarm Optimization (PSO) method to balance different loads by considering price to dispatch them. Their test case one includes 6 factors (dimensions), 6 generators (particles) and 100 generations to evolve, and its time complexity in theory is 3606 ( 6 + 6 ∗ 100 ∗ 6 ) before it has a satisfactory convergent result. In their test case two, it increases to 7 factors, 40 generators and 400 generations, so 40 + 40 ∗ 400 ∗ 7 (so the asymptotic time complexity is 112,040). In another test case it has 5 factors, 20 generators, and 400 generations (asymptotic time complexity is 40,020) and its actual computational runtime is 0.29282 s that is around 10 and 200 times slower than the other approaches [136] in the simulation. Fig. 3 shows the relationship between complexity and actual runtime by extending the figures given in the paper. The line is the time complexity in log with respect to actual runtime. The researchers can refer this to approximate the actual runtime of an application with the same computational resources by giving key parameter values of the learning method. The approximation is not rigid, as we assume that the space complexity is changing linearly. Download : Download high-res image (526KB) Download : Download full-size image Fig. 2. Time complexity of LM-SVM. For deep NN learning methods such as CNN, the weights in the convolutional layers are trained and updated in a similar way as traditional ANNs/MLPs except that the number of filters and layers are orders of magnitude higher than those in traditional ANNs and MLPs. The authors in [129] report their experimental results on computational time complexity of a CNN model by varying different key parameters such as depth, filter size and number, width and pooling layer of the network to find their trade-offs between two parameters to investigate the overall performance in terms of time complexity and output accuracy. We share the same view with the authors [129] that introducing computational time and memory constraints can give better understanding the value of machine learning methods in realistic business applications. Download : Download high-res image (195KB) Download : Download full-size image Fig. 3. Relationship between complexity and actual runtime of particle swarm optimization method. The training of these deep NN models needs massive resources (e.g. to accommodate the training data) and time. Therefore they should be carried out on the Cloud. However, the operation time of these models is only proportional to the number of neurons no matter how large the training data is, the online analysis tasks can be deployed on the Edge/Fog. As it has been observed in this analysis, few research outputs report the empirical time complexity of their approaches. Therefore, the estimation on the empirical time complexity of a training algorithm still has room for more extensive study. This information may be vital for decision making on-the-fly if a learning task can be deployed in the Edge devices. 5. On-line learning methods If we take a look at Table 2 we will observe that the asymptotic complexity of the classic learning algorithms reported in the literature review normally takes into consideration many terms (e.g., number of samples, iterations, structure parameters, etc.). In theory, this could result in high order polynomial behaviour, which would deter the deployment of the learning phase in Edge devices. This is because firstly, over time more and more streaming data will be accumulated and it is impractical and often infeasible to accommodate large volumes of streaming data in the machine’s main memory. Secondly, it is also infeasible to regularly reconstruct new models from the scratch with accumulated streaming data in real-time. Furthermore CPS data streams feature perishable insights, i.e., information that must be acted upon fast, as insights obtained from streaming data, such as from sensors, quickly lose their value if they were to be processed in ‘batch mode’ [16]. As a result, a new paradigm of learning, i.e. incremental and on-line learning algorithms should be adopted. Losing et al. [137] gives the definition of incremental learning for supervised learning as below (we change the notations/symbols for consistency reasons). An incremental learning algorithm generates, on a given stream of training data S 1 , S 2 … , S N , a sequence of models H 1 , H 2 … , H N , where S i is labelled training data S i = ( X i , Y i ) ∈ R n × { 1 , … , C } and H i : R n { 1 , … , C } is a model function solely depending on H i − 1 and the recent p examples S i , … , S i − p , with p being strictly limited. Losing et al. [137] further specify on-line learning algorithms as incremental learning algorithms which are additionally bounded in model complexity and run-time, capable of endless/lifelong learning on a device with restricted resources. Incremental and online learning algorithms aim for minimal processing time and space; and thus fit in CPS data processing environments. Losing et al. [137] evaluate eight popular incremental methods representing different algorithm classes such as Bayesian, linear, and instance-based models as well as tree-ensembles and neural networks. Experiments are carried out to evaluate these algorithms with respect to accuracy, convergence speed as well as model complexity, aiming at facilitating the choice of the best method for a given application. However, it primarily covers supervised incremental learning algorithms with stationary datasets, although robustness of the methods to different types of real concept drift are also investigated. Gama et al. [138] considers dynamically changing and non-stationary environments where the data distribution can change over time yielding the phenomenon of concept drift, which applies to most of the real world CPS applications. Adaptive learning algorithms, defined as advanced incremental learning algorithms that are able to update predictive models online during their operation to react to concept drifts, are explored. A taxonomy for adaptive algorithms, presented in four modules as memory, change detection, learning, and loss estimation, is proposed; and the methods within each module are also listed. Gama et al. [138] focuses on online supervised learning. Ade et al. [139] includes some unsupervised incremental learning approaches that learn from unlabelled data samples to adjust pre-learned concepts to environmental changes. Most of the incremental clustering algorithms for pattern discovery rely on similarity measures between the data points. One approach is called Concept Follower (CF) that includes CF1 and CF2 [140]. CF1 and CF2 learn from unlabelled data samples to adjust pre-learned concepts to environmental changes. Initially, a supervised learner is used to learn and label a set of concepts. When a new sample is collected, CF1 calculates the distance of the sample to all concepts and the concept with the minimal distance to the sample is identified. If the distance is smaller than the predefined threshold, CF1 considers the concept a match and then slightly shifts, by a learning rate parameter, towards the classified sample to adjust to the concept drift; otherwise CF1 detects the abrupt change and repeats the initial supervised learning stage. Compared to CF1, CF2 supports problems areas with unbalanced sample ratio between concepts. This is done by CF2 adjusting all concepts in the proximity of the sample instead of, as does CF1, adjusting only the concept closest to the sample. Next, we discuss on some of the most relevant online approaches to the machine learning algorithms identified in this article. Artificial neural networks Classically, ANN are trained using a training set and optimization methods such as gradient descent and backpropagation to minimize a cost function correlated to the error derived from the current state of the network. The online version can adapt to the arrival of new data by pre-training the network with the available training set, and then adapting the pre-trained network by using stochastic gradient descent over the new series of available data. This type of setting would benefit from a combination of both Cloud technologies (i.e., for pre-training the network), and Edge computing (i.e. for adapting the network). While the use of stochastic gradient descent allows adopting a batch algorithm like backpropagation in a non-batch setting, there are specialized learning algorithms, called on-line sequential learning methods, for training neural networks in an on-line setting in which data becomes available with time [[141], [142], [143], [144]]. They can be efficiently deployed in an Edge device as they do not need to store past training samples. The online sequential learning methods tend to be ad-hoc for networks with specific activation functions, or with specific architectures (e.g., single hidden layer). Therefore, the complexity of problems represented by these networks may not be as vast as the one represented by classic neural networks or deep learning approaches. Decision trees Generating a classic decision tree requires that all of the training samples are considered [145]. This is hardly applicable in a stream analytics context, as training samples arrive constantly. Therefore, different learning mechanisms are required to properly train decision trees in a stream analytics context, in which the trees can evolve from a stream of data. Some approaches with a default tree structure provide a series of greedy steps to adapt to the new training samples. These include ID5R algorithm [146], an adaptation of the popular ID3 learning algorithm for stream data, and ITI [147]. These greedy changes were in some cases suboptimal and ended up in inappropriate adaptations to change. The other approach to learning decision trees from streams is to maintain a set of statistics at nodes and only split a node when sufficient and statistically significant information is available to make the split. Hoeffding inequality [[148], [149], [150]] is the backbone to these approaches, which provide bounds for the number of observations that are necessary to obtain an estimated mean that does not differ from the mean of the underlying random variable. Some researchers have recently argued that the assumptions underlying the Hoeffding inequality are not appropriate when constructing on-line trees. Some methods split the nodes of the decision tree based on other modelling paradigms such as McMiarmid’s bound [151], or Gaussian processes [152]. Random forests The general idea behind on-line random forests consists of providing both a method to carry out on-line bagging, and a method to carry out on-line learning of random trees. Abdulsalam et al. [153] take an approach that carries out on-line bagging by dividing the incoming samples of data into blocks with a certain size. Then, blocks of data randomly selected are employed for either training or testing a tree in the model. The training block is redirected to a chosen tree, and an on-line learning algorithm for trees is employed to update the current tree. Later on, the learning model is enhanced to adapt to the random arrival of labelled examples in the stream, with blocks of different sizes and frequency [154]. Another alternative to the on-line bagging process described above is employed by Saffari et al. [155]. In this case, each new sample is presented a number of times controlled by a Poisson distribution, to each random tree in the model. Then, the random trees gradually grow by creating random tests and thresholds at decision nodes and choosing the best one after a number of statistics have been gathered that guarantee that the test is the best from the ones randomly created at the decision node. Other approaches opt for avoiding on-line bagging at the forest level, and the sub-sampling is carried out at the tree level [156]. When a new sample arrives to the random forest, this sample is presented to all of the trees. Then, the individual tree decides if the sample will be used to influence the structure of the tree, or used to estimate class membership probabilities in the leaf to which they are assigned. Support vector machines Classification in support vector machines is based on the idea of finding the maximum margin hyperplane that separates elements from different categories. By definition, one should have access to the entire training dataset in order to build such maximum margin hyperplanes. Otherwise, there would be no guarantee that estimated hyperplanes are optimal. This assumption limits the applicability of classic support vector learning algorithms to an online setting, and it forces scholars to devise new methods that are adapted to the online setting. The incremental approach to support vector learning typically requires deciding if a new sample should become a support vector that modifies the current hyperplane. The algorithm also needs to determine if previously calculated support vectors are still as relevant after the observation of the new sample, and remove those that are no longer relevant. Otherwise, online approaches to support vector learning incur in the risk of growing linearly with the infinite number of samples [157]. To tackle this problem, there have been a number of proposals that aim to build a support vector model with adequate predictive performance while also minimizing the number of support vectors in the resulting model [[157], [158], [159], [160]]. 6. Discussion So far machine learning methods of various categories have been employed for data stream analysis purposes. Little literature has studied the integration of these methods to the Cloud and Fog computing architecture. The very nature of CPS requires a computing paradigm that offers latency sensitive monitoring, intelligent control and data analytics for intelligent decision making. In contrast to the Cloud, the Fog performs latency-sensitive applications at the edge of network, however latency tolerant tasks are efficiently performed in the Cloud for deep analytics [161]. Cloud computing provides on demand and scalable storage and processing services that can scale up to requirements of IoT based CPS. However, for healthcare applications, manufacturing control applications, connected vehicle applications, emergency response, and other latency sensitive applications, the delay caused by transferring data to the Cloud and back to the application becomes unacceptable [[162], [163], [164]]. The latency sensitive applications rely on the Fog for their time critical functionality. The adoption of Fog computing not only greatly improves the response time of time sensitive applications but also brings some new challenges such as business model, security, privacy and scalability. It is perceived that in time critical services Fog computing is cost-effective compared to Cloud computing due to its lesser latency and in some cases due to spare capacity of locally available resources. The view is endorsed by study carried out in [163], which shows that with high number of latency sensitive applications Fog computing outperforms Cloud computing in terms of power consumption service latency and cost. Since data stream analytics processes the data in one scan, due to perishable insights, some algorithms, are infeasible for streaming data as they require multiple scans of data [165]. In addition, for memory-based methods such as the Parzen probability density model and nearest-neighbour methods, the entire training set needs to be stored in order to make predictions for future data points. Also, a metric is required to be defined to measure the similarity of any two vectors in input space. These requirements are both memory consuming and generally slow at making predictions for test data points. Therefore they should not be employed for data stream analysis, even though the Fog computing is introduced. ANN (MLP), DT and SVM are the most commonly used machine learning methods in surveyed CPS. In terms of accuracy, it is observed that the performance of these machine learning methods is task dependent. For example, [75] pointed out that the best classifier differs according to the weather conditions. The classifier based on MLP behaves better than SVM (and SOM) for sunny and foggy conditions, whereas for rainy conditions, the SVM-based model is the most appropriate. [166] concluded that in automatic Stereotypical Motor Movements (SMM) recognition, SVM appears to outperform DT on overall accuracy by ∼ 6 percentage points (although at times DT did outperform SVM), regardless of feature set used. In terms of the operation (classification or regression) time, [107] discovered the noticeably lower detection latency provided by DT while [76] ascertained that SVM was not fast enough for real-time classification (classification time being around 2.2 s) compared to ANN with seven hidden nodes (classification time being around 100 ms). For those machine learning methods that need massive training data and take iterations to converge, such as ANN, HMM and reinforcement learning methods, it is recommended to deploy the training tasks onto the Cloud while deploying the on-line analysis tasks on the Edge/Fog. For deep NN learning methods such as CNN, the weights in the convolutional layers are trained and updated in the similar way to traditional MLPs except that the number of weights and layers are orders of magnitude higher than MLPs. As the training of these deep NN models needs massive resources (e.g. to accommodate the training data) and time, they should be carried out on the Cloud. However, the operation latency of these models is only proportional to the number of neurons no matter how large the training data is, the online analysis tasks can be deployed on the Edge/Fog. When machine learning methods are deployed on the Edge, trade-offs are needed among accuracy, operation time, and the parameters of these methods such as sliding window sizes, number of iterations and prediction/forecast time lags [[51], [71]]. Application dependent data pre-processing proved effective in improving the performance of the data analysis. For example, in [76], before employing an ANN classifier, a simple gradient detector and an intensity-bump detector with loose (low) threshold values are applied to quickly filter out non-lane markings. As the remaining samples are much smaller in number, the classification time was significantly reduced. Due to space limitations, this paper does not investigate the data pre-processing techniques for machine learning methods in CPS. The distributed and parallel environment provided by Cloud and Fog computing may facilitate the execution of machine learning methods (such as Random Forest) to further reduce the classification time as the sets of sub-tasks (such as the decision trees involved in Random Forest) can be run in parallel. The data stream properties also could affect the choice of the methods. For example, fuzzy logic is more capable of dealing with fuzzy information without requiring large volumes of samples. Existing deep learning methods will require substantial numbers of samples in the training process. In addition, ANN is likely to be more appropriate to deal with multiple variant datasets than reinforcement learning methods. 7. Conclusion and future research directions Data stream analytics is one of the core components in CPS and machine learning methods have proved to be effective techniques of data analytics. The rise of Cloud and Fog computing paradigm calls for the study of how the machine learning based CPS data stream analytics should be integrated to such a paradigm in order to better meet CPS requirements of mission criticality and time criticality. This paper investigated and summarized the existing machine learning methods for CPS data stream analytics from various perspectives, especially from the time complexity point of view. The investigation led to the discussion and guidance of how the CPS machine learning methods should be integrated to the Cloud and Fog architecture. In the future, more effective and efficient machine learning methods should be developed for analysing the ever growing data streams in CPS. of Distributed and parallel environments provided by the Cloud and Fog computing [167] may be utilized. Hierarchical and composable machine learning methods that are well suited to partitioned execution across the Cloud and the Edge are needed, as are transfer and continual learning techniques to deal with the non-stationarity of data streams. In the meanwhile, studies should be carried out on the development of Cloud and Edge systems that facilitate the CPS data stream analytics by accommodating the discrepancy and the heterogeneity between the capabilities of Edge devices and data centre servers and among the Edge devices themselves, providing uniformed APIs [168] and services [[169], [170]]. Acknowledgements This work is partially supported by EU H2020 programme (Project NOESIS under grant no 769980) and Innovate UK project(Project SABOT under grand no 103539). References [1] Lee E.A. The past present and future of cyber-physical systems: A focus on models Sensors, 15 (2015), pp. 4837-4869, 10.3390/s150304837 View in ScopusGoogle Scholar [2] Chaâri R., Ellouze F., Koubâa A., Qureshi B., Pereira N., Youssef H., Tovar E. Cyber-physical systems clouds: A survey Comput. Netw., 108 (2016), pp. 260-278, 10.1016/j.comnet.2016.08.017 View PDFView articleView in ScopusGoogle Scholar [3] A. Rayes, S. Salam, Internet of Things (IoT) overview, internet things from hype to real, 2017, pp. 1–34. http://dx.doi.org/10.1016/J.FUTURE.2013.01.010. Google Scholar [4] NIST, Strategic vision and business drivers for 21st century cyber-physical systems, 2013. https://www.nist.gov/sites/default/files/documents/el/Exec-Roundtable-SumReport-Final-1-30-13.pdf. Google Scholar [5] E.a. Lee, Cyber-physical systems — are computing foundations adequate? October 1, 2006, pp. 1–9. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.8011&rep=rep1&type=pdf. Google Scholar [6] Wang L., Törngren M., Onori M. Current status and advancement of cyber-physical systems in manufacturing J. Manuf. Syst., 37 (2015), pp. 517-527, 10.1016/j.jmsy.2015.04.008 View PDFView articleView in ScopusGoogle Scholar [7] NSF, Cyber-physical systems (CPS), 2017. https://www.nsf.gov/pubs/2017/nsf17529/nsf17529.htm. Google Scholar [8] J. Shi, J. Wan, H. Yan, H. Suo, A survey of cyber-physical systems, in: 2011 Int. Conf. Wirel. Commun. Signal Process., 2011, pp. 1–6. http://dx.doi.org/10.1109/WCSP.2011.6096958. Google Scholar [9] Guan X., Member S., Yang B., Chen C., Dai W., Wang Y. A comprehensive overview of cyber-physical systems: from perspective of feedback system IEEE/CAA J. Autom. Sin., 3 (2016), pp. 1-14, 10.1109/JAS.2016.7373757 View in ScopusGoogle Scholar [10] S.K. Khaitan, J.D. McCalley, Cyber physical system approach for design of power grids: A survey, in: 2013 IEEE Power Energy Soc. Gen. Meet., 2013, pp. 1–5. http://dx.doi.org/10.1109/PESMG.2013.6672537. Google Scholar [11] Lee J., Bagheri B., Kao H.A. A cyber-physical systems architecture for industry 4.0-based manufacturing systems Manuf. Lett., 3 (2015), pp. 18-23, 10.1016/j.mfglet.2014.12.001 View PDFView articleView in ScopusGoogle Scholar [12] Asadollah S.A., Inam R., Hansson H. A Survey on Testing for Cyber Physical System Lect. Notes Comput. Sci., vol. 9447 (2015), pp. 194-207, 10.1007/978-3-319-25945-1_12 Including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics View in ScopusGoogle Scholar [13] A. Humayed, J. Lin, F. Li, B. Luo, Cyber-physical systems security — a survey, 4662, 2017. http://dx.doi.org/10.1109/JIOT.2017.2703172. Google Scholar [14] I. Akkaya, Data-driven cyber-physical systems via real-time stream analytics and machine learning, 2016, p. 136. https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-159.pdf. Google Scholar [15] a Silva J., Faria E.R., Barros R.C., Hruschka E.R., De Carvalho A.C.P.L.F., Gama J. Data stream clustering: A survey ACM Comput. Surv., 46 (2013), pp. 13:1-13:31, 10.1145/2522968.2522981 Google Scholar [16] Mousavi M., Bakar A.A., Vakilian M. Data stream clustering algorithms: A review Int. J. Adv. Soft Comput. Appl., 7 (2015), pp. 1-15 View in ScopusGoogle Scholar [17] M.D. Jayanthi, A framework for real-time streaming analytics using machine learning approach, 2016, pp. 85–92. Google Scholar [18] E. Alpaydın, Introduction to Machine Learning, second ed., 2010. Google Scholar [19] SAS, Machine kearning: What it is & why it matters, (n.d.). https://www.sas.com/it_it/insights/analytics/machine-learning.html. Google Scholar [20] P. Bhavsar, I. Safro, N. Bouaynaya, R. Polikar, D. Dera, Mchine learning in transportation data analysis, in: Data Anal. Intell. Transp. Syst., 2017, pp. 283–307. Google Scholar [21] Distefano S., Merlino G., Puliafito A. A utility paradigm for IoT: The sensing cloud Pervasive Mob. Comput., 20 (2015), pp. 127-144, 10.1016/j.pmcj.2014.09.006 View PDFView articleView in ScopusGoogle Scholar [22] Christophe B., Boussard M., Lu M., Pastor A., Toubiana V. The web of things vision: Things as a service and interaction patterns Bell Labs Tech. J., 16 (2011), pp. 55-61, 10.1002/bltj.20485 View in ScopusGoogle Scholar [23] Zhang Y., Qiu M., Tsai C.-W., Hassan M.M., Alamri A. Health-CPS: Healthcare cyber-physical system assisted by cloud and big data IEEE Syst. J., 11 (2017), pp. 88-95, 10.1109/JSYST.2015.2460747 Google Scholar [24] Hossain M.S., Hassan M.M., Al Qurishi M., Alghamdi A. Resource allocation for service composition in cloud-based video surveillance platform 2012 IEEE Int. Conf. Multimed. Expo Work, IEEE (2012), pp. 408-412, 10.1109/ICMEW.2012.77 View in ScopusGoogle Scholar [25] Botta A., de Donato W., Persico V., Pescapé A. Integration of cloud computing and internet of things: A survey Future Gener. Comput. Syst., 56 (2016), pp. 684-700, 10.1016/j.future.2015.09.021 View PDFView articleView in ScopusGoogle Scholar [26] Stojmenovic I. Fog computing: A cloud to the ground support for smart things and machine-to-machine networks 2014 Australas. Telecommun. Networks Appl. Conf., IEEE (2014), pp. 117-122, 10.1109/ATNAC.2014.7020884 View in ScopusGoogle Scholar [27] Bonomi F., Milito R., Zhu J., Addepalli S. Fog computing and its role in the internet of things Proc. First Ed. MCC Work. Mob. Cloud Comput, MCC ’12, ACM Press, New York, New York, USA (2012), p. 13, 10.1145/2342509.2342513 View in ScopusGoogle Scholar [28] Dastjerdi A.V., Buyya R. Fog computing: Helping the internet of things realize its potential Computer, 49 (2016), pp. 112-116, 10.1109/MC.2016.245 View in ScopusGoogle Scholar [29] Aazam M., Huh E.-N. Fog computing and smart gateway based communication for cloud of things 2014 Int. Conf. Futur. Internet Things Cloud, IEEE (2014), pp. 464-470, 10.1109/FiCloud.2014.83 View in ScopusGoogle Scholar [30] Hong K., Lillethun D., Ramachandran U., Ottenwälder B., Koldehofe B. Mobile fog: a programming model for large-scale applications on the internet of things Proc. Second ACM SIGCOMM Work. Mob. Cloud Comput, MCC ’13, ACM Press, New York, New York, USA (2013), p. 15, 10.1145/2491266.2491270 View in ScopusGoogle Scholar [31] Jalali F., Hinton K., Ayre R., Alpcan T., Tucker R.S. Fog computing may help to save energy in cloud computing IEEE J. Sel. Areas Commun., 34 (2016), pp. 1728-1739, 10.1109/JSAC.2016.2545559 View in ScopusGoogle Scholar [32] R. Mahmud, R. Kotagiri, R. Buyya, Fog computing: A taxonomy, survey and future directions, 2018, pp. 103–130. http://dx.doi.org/10.1007/978-981-10-5861-5_5. Google Scholar [33] Datta S.K., Bonnet C., Nikaein N. An IoT gateway centric architecture to provide novel M2M services 2014 IEEE World Forum Internet Things, IEEE (2014), pp. 514-519, 10.1109/WF-IoT.2014.6803221 View in ScopusGoogle Scholar [34] Ochoa S.F., Fortino G., Di Fatta G. Cyber-physical systems internet of things and big data Future Gener. Comput. Syst., 75 (2017), pp. 82-84, 10.1016/j.future.2017.05.040 View PDFView articleView in ScopusGoogle Scholar [35] Bittencourt L.F., Lopes M.M., Petri I., Rana O.F. Towards virtual machine migration in fog computing 2015 10th Int. Conf. P2P, Parallel, Grid, Cloud Internet Comput., IEEE (2015), pp. 1-8, 10.1109/3PGCIC.2015.85 View in ScopusGoogle Scholar [36] Bellavista P., Zanni A. Feasibility of fog computing deployment based on Docker containerization over Raspberry Pi Proc. 18th Int. Conf. Distrib. Comput. Netw, ICDCN ’17, ACM Press, New York, New York, USA (2017), pp. 1-10, 10.1145/3007748.3007777 Google Scholar [37] Al-Fuqaha A., Khreishah A., Guizani M., Rayes A., Mohammadi M. Toward better horizontal integration among IoT services IEEE Commun. Mag., 53 (2015), pp. 72-79, 10.1109/MCOM.2015.7263375 View in ScopusGoogle Scholar [38] Yi S., Li C., Li Q. A survey of fog computing: Concepts, applications and issues Proc. 2015 Work. Mob. Big Data, Mobidata ’15, ACM Press, New York, New York, USA (2015), pp. 37-42, 10.1145/2757384.2757397 View in ScopusGoogle Scholar [39] Zhang J., c Wang F.-Y., Wang K., Lin W.-H., Xu X., Chen C. Data-driven intelligent transportation systems: A survey IEEE Trans. Intell. Transp. Syst., 12 (2011), pp. 1624-1639, 10.1109/TITS.2011.2158001 View in ScopusGoogle Scholar [40] Diamantoulakis P.D., Kapinas V.M., Karagiannidis G.K. Big data analytics for dynamic energy management in smart grids Big Data Res., 2 (2015), pp. 94-101, 10.1016/j.bdr.2015.03.003 View PDFView articleView in ScopusGoogle Scholar [41] Raza M.Q., Khosravi A. A review on artificial intelligence based load demand forecasting techniques for smart grid and buildings Renew. Sustain. Energy Rev., 50 (2015), pp. 1352-1372, 10.1016/j.rser.2015.04.065 View PDFView articleView in ScopusGoogle Scholar [42] Zhou K., Fu C., Yang S. Big data driven smart energy management: From big data to big insights Renew. Sustain. Energy Rev., 56 (2016), pp. 215-225, 10.1016/j.rser.2015.11.050 View PDFView articleView in ScopusGoogle Scholar [43] Yu X., Xue Y. Smart grids: A cyber–physical systems perspective Proc. IEEE, 104 (2016), pp. 1058-1070, 10.1109/JPROC.2015.2503119 View in ScopusGoogle Scholar [44] EU, Framework for the deployment of intelligent transport systems in the field of road transport and for interfaces with other modes of transport, 2010. http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2010:207:0001:0013:EN:PDF. Google Scholar [45] Dimitrakopoulos G., Demestichas P. Intelligent transportation systems: Systems based on cognitive networking principles and management functionality IEEE Veh. Technol. Mag. (2009), pp. 77-84 Google Scholar [46] S. Ezell, Intelligent transportation systems, 2010. https://www.itif.org/files/2010-1-27-ITS_Leadership.pdf. Google Scholar [47] MINI plant oxford, assembly, (n.d.). http://miniplantoxford.co.uk/production/assembly.aspx. Google Scholar [48] Kuss A., Dietz T., Spenrath F., Verl A. Automated planning of robotic MAG welding based on adaptive gap model Procedia CIRP, 62 (2017), pp. 612-617, 10.1016/j.procir.2016.07.008 View PDFView articleView in ScopusGoogle Scholar [49] Mitchell T.M. The Discipline of Machine Learning Carnegie Mellon University, School of Computer Science, Machine Learning Department (2006) Google Scholar [50] Bishop C.M. Pattern Recognition and Machine Learning Springer, New York, New York, USA (2006) Google Scholar [51] Ferreira J., Carvalho E., Ferreira B.V., de Souza C., Suhara Y., Pentland A., Pessin G. Driver behavior profiling: An investigation with different smartphone sensors and machine learning PLoS One, 12 (2017), p. e0174959, 10.1371/journal.pone.0174959 Google Scholar [52] Breinman L. Random forests Mach. Learn., 45 (2001), pp. 5-32, 10.1186/1478-7954-9-29 Google Scholar [53] Z. Wang, T. Oates, Encoding time series as images for visual inspection and classification using tiled convolutional neural networks, work, in: Twenty-Ninth AAAI Conf. Artif. Intell., 2015, pp. 40–46. http://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10179. Google Scholar [54] Y. Zheng, Q. Liu, E. Chen, Y. Ge, J.L. Zhao, Time series classification using multi-channels deep convolutional neural networks, in: Web-Age Inf. Manag. SE - 33, 8485, 2014, pp. 298–310. http://dx.doi.org/10.1007/978-3-319-08010-9_33. Google Scholar [55] Goodfellow I., Bengio Y., Courville A. Deep Learning MIT Press (2016) Google Scholar [56] Deng L., Yu D. Deep learning: Methods and applications Found. Trends Signal Process., 7 (2014), pp. 197-387, 10.1561/2000000039 Google Scholar [57] H.,V.  Drucker, C.J. Burges, L. Kaufman, A.J. Smola, Vapnik, Support vector regression machines, in: Adv. Neural Inf. Process. Syst. 1997, pp. 155–161. Google Scholar [58] Santafe G. Advances on Supervised and Unsupervised Learning of Bayesian Network Models University of the Basque Country (2007) http://www.sc.ehu.es/ccwbayes/members/guzman/pdfs/guzmanTesis.pdf Google Scholar [59] Eiben A.E., Smith J.E. Introduction to Evolutionary Computing Springer (2003) Google Scholar [60] Ishibuchi H., Nakashima T., Murata T. A fuzzy classifier system that generates fuzzy if-then rules for pattern classification problems Proc. 1995 IEEE Int. Conf. Evol. Comput., IEEE (1995), pp. 759-764, 10.1109/ICEC.1995.487481 View in ScopusGoogle Scholar [61] Maulik U., Bandyopadhyay S. Genetic algorithm-based clustering technique Pattern Recognit., 33 (2000), pp. 1455-1465, 10.1016/S0031-3203(99)00137-5 View PDFView articleView in ScopusGoogle Scholar [62] S. Whiteson, Evolutionary computation for reinforcement learning, 2012, pp. 325–355. http://dx.doi.org/10.1007/978-3-642-27645-3_10. Google Scholar [63] Kohonen T. Essentials of the self-organizing map Neural Netw., 37 (2013), pp. 52-65, 10.1016/j.neunet.2012.09.018 View PDFView articleView in ScopusGoogle Scholar [64] Watkins C.J.C.H., Dayan P. Q -learning Mach. Learn., 8 (1992), pp. 279-292, 10.1007/BF00992698 Google Scholar [65] Sideratos G., Ikonomopoulos A., Hatziargyriou N. A committee of machine learning techniques for load forecasting in a smart grid environment Int. J. Energy Power., 4 (2015), p. 98, 10.14355/ijep.2015.04.016 Google Scholar [66] Xiao L., Shao W., Wang C., Zhang K., Lu H. Research and application of a hybrid model based on multi-objective optimization for electrical load forecasting Appl. Energy, 180 (2016), pp. 213-233, 10.1016/j.apenergy.2016.07.113 View PDFView articleView in ScopusGoogle Scholar [67] Yu W., An D., Griffith D., Yang Q., Xu G. Towards statistical modeling and machine learning based energy usage forecasting in smart grid ACM SIGAPP Appl. Comput. Rev., 15 (2015), pp. 6-16, 10.1145/2753060.2753061 Google Scholar [68] Guan C., Luh P.B., Michel L.D., Wang Y., Friedland P.B. Very short-term load forecasting: Wavelet neural networks with data pre-filtering IEEE Trans. Power Syst., 28 (2013), pp. 30-41, 10.1109/TPWRS.2012.2197639 View in ScopusGoogle Scholar [69] Kusiak A., Zheng Haiyang, Song Zhe Short-term prediction of wind farm power: A data mining approach IEEE Trans. Energy Convers., 24 (2009), pp. 125-136, 10.1109/TEC.2008.2006552 View in ScopusGoogle Scholar [70] Chan K.Y., Dillon T.S., Singh J., Chang E. Neural-network-based models for short-term traffic flow forecasting using a hybrid exponential smoothing and Levenberg–Marquardt algorithm IEEE Trans. Intell. Transp. Syst., 13 (2012), pp. 644-654, 10.1109/TITS.2011.2174051 View in ScopusGoogle Scholar [71] Zito P., Chen Haibo, Bell M.C. Predicting real-time roadside CO and CO2 concentrations using neural networks IEEE Trans. Intell. Transp. Syst., 9 (2008), pp. 514-522, 10.1109/TITS.2008.928259 View in ScopusGoogle Scholar [72] Khosravi A., Mazloumi E., Nahavandi S., Creighton D., Van Lint J.W.C. Prediction intervals to account for uncertainties in travel time prediction IEEE Trans. Intell. Transp. Syst., 12 (2011), pp. 537-547, 10.1109/TITS.2011.2106209 View in ScopusGoogle Scholar [73] van Lint J.W.C. Online learning solutions for freeway travel time prediction IEEE Trans. Intell. Transp. Syst., 9 (2008), pp. 38-47, 10.1109/TITS.2008.915649 View in ScopusGoogle Scholar [74] Yin T., Zhong G., Zhang J., He S., Ran B. A prediction model of bus arrival time at stops with multi-routes Transp. Res. Procedia, 25 (2017), pp. 4623-4636, 10.1016/j.trpro.2017.05.381 View PDFView articleView in ScopusGoogle Scholar [75] Castaño F., Beruvides G., Haber R., Artuñedo A. Obstacle recognition based on machine learning for on-chip LiDAR sensors in a cyber-physical system Sensors, 17 (2017), p. 2109, 10.3390/s17092109 View in ScopusGoogle Scholar [76] Kim Z. Robust lane detection and tracking in challenging scenarios IEEE Trans. Intell. Transp. Syst., 9 (2008), pp. 16-26, 10.1109/TITS.2007.908582 View in ScopusGoogle Scholar [77] Shin S.-J., Woo J., Rachuri S. Predictive analytics model for power consumption in manufacturing Procedia CIRP, 15 (2014), pp. 153-158, 10.1016/j.procir.2014.06.036 View PDFView articleView in ScopusGoogle Scholar [78] Shen C., Wang L., Li Q. Optimization of injection molding process parameters using combination of artificial neural network and genetic algorithm method J. Mater. Process. Technol., 183 (2007), pp. 412-418, 10.1016/j.jmatprotec.2006.10.036 View PDFView articleView in ScopusGoogle Scholar [79] Lahouar A., Ben Hadj Slama J. Random forests model for one day ahead load forecasting IREC2015 Sixth Int. Renew. Energy Congr., IEEE (2015), pp. 1-6, 10.1109/IREC.2015.7110975 Google Scholar [80] Zhou Jian, Ge Zhaoqiang, Gao Shang, Xu Yanli Fault record detection with random forests in data center of large power grid 2016 IEEE PES Asia-Pacific Power Energy Eng. Conf, IEEE (2016), pp. 1641-1645, 10.1109/APPEEC.2016.7779771 View in ScopusGoogle Scholar [81] Wu D., Jennings C., Terpenny J., Kumara S. Cloud-based machine learning for predictive analytics: Tool wear prediction in milling 2016 IEEE Int. Conf. Big Data (Big Data), IEEE (2016), pp. 2062-2069, 10.1109/BigData.2016.7840831 View in ScopusGoogle Scholar [82] Gkorou D., Hoogenboom T., Ypma A., Tsirogiannis G., Giollo M., Sonntag D., Vinken G., van Haren R., van Wijk R.J., Nije J. Towards big data visualization for monitoring and diagnostics of high volume semiconductor manufacturing Proc. Comput. Front. Conf. ZZZ, CF’17, ACM Press, New York, New York, USA (2017), pp. 338-342, 10.1145/3075564.3078883 View in ScopusGoogle Scholar [83] Auret L., Aldrich C. Unsupervised process fault detection with random forests Ind. Eng. Chem. Res., 49 (2010), pp. 9184-9194, 10.1021/ie901975c View in ScopusGoogle Scholar [84] Oldewurtel F., Ulbig A., Parisio A., Andersson G., Morari M. Reducing peak electricity demand in building climate control using real-time pricing and model predictive control 49th IEEE Conf. Decis. Control, IEEE (2010), pp. 1927-1932, 10.1109/CDC.2010.5717458 View in ScopusGoogle Scholar [85] Chao H. Efficient pricing and investment in electricity markets with intermittent resources Energy Policy, 39 (2011), pp. 3945-3953, 10.1016/j.enpol.2011.01.010 View PDFView articleView in ScopusGoogle Scholar [86] Gupta S., Kambli R., Wagh S., Kazi F. Support-vector-machine-based proactive cascade prediction in smart grid using probabilistic framework IEEE Trans. Ind. Electron., 62 (2015), pp. 2478-2486, 10.1109/TIE.2014.2361493 View in ScopusGoogle Scholar [87] Zhang W.Y., Hong W.-C., Dong Y., Tsai G., Sung J.-T., Fan G. Application of SVR with chaotic GASA algorithm in cyclic electric load forecasting Energy, 45 (2012), pp. 850-858, 10.1016/j.energy.2012.07.006 View PDFView articleView in ScopusGoogle Scholar [88] Nagi J., Mohammad A.M., Yap K.S., Tiong S.K., Ahmed S.K. Non-technical loss analysis for detection of electricity theft using support vector machines 2008 IEEE 2nd Int. Power Energy Conf., IEEE (2008), pp. 907-912, 10.1109/PECON.2008.4762604 View in ScopusGoogle Scholar [89] Nizar A.H., Dong Z.Y., Jalaluddin M., Raffles M.J. Load profiling method in detecting non-technical loss activities in a power utility 2006 IEEE Int. Power Energy Conf., IEEE (2006), pp. 82-87, 10.1109/PECON.2006.346624 View in ScopusGoogle Scholar [90] Esmalifalak M., Nguyen Nam Tuan, Zheng Rong, Han Zhu Detecting stealthy false data injection using machine learning in smart grid 2013 IEEE Glob. Commun. Conf., IEEE (2013), pp. 808-813, 10.1109/GLOCOM.2013.6831172 View in ScopusGoogle Scholar [91] Albousefi A.A., Ying H., Filev D., Syed F., Prakah-Asante K.O., Tseng F., Yang H.-H. A two-stage-training support vector machine approach to predicting unintentional vehicle lane departure J. Intell. Transp. Syst., 21 (2017), pp. 41-51, 10.1080/15472450.2016.1196141 View articleView in ScopusGoogle Scholar [92] A. Ponz, C.H. Rodríguez-Garavito, F. García, P. Lenz, C. Stiller, J.M. Armingol, Laser scanner and camera fusion for automatic obstacle classification in ADAS application, 2015, pp. 237–249. http://dx.doi.org/10.1007/978-3-319-27753-0_13. Google Scholar [93] Liu T., Yang Y., Huang G.-B., Yeo Y.K., Lin Z. Driver distraction detection using semi-supervised machine learning IEEE Trans. Intell. Transp. Syst., 17 (2016), pp. 1108-1120, 10.1109/TITS.2015.2496157 View in ScopusGoogle Scholar [94] Ribeiro R.P., Pereira P., Gama J. Sequential anomalies: a study in the Railway industry Mach. Learn., 105 (2016), pp. 127-153, 10.1007/s10994-016-5584-6 View in ScopusGoogle Scholar [95] Moridpour S., Anwar T., Sadat M.T., Mazloumi E. A genetic algorithm-based support vector machine for bus travel time prediction 2015 Int. Conf. Transp. Inf. Saf., IEEE (2015), pp. 264-270, 10.1109/ICTIS.2015.7232119 View in ScopusGoogle Scholar [96] Susto G.A., Schirru A., Pampuri S., McLoone S., Beghi A. Machine learning for predictive maintenance: A multiple classifier approach IEEE Trans. Ind. Inform., 11 (2015), pp. 812-820, 10.1109/TII.2014.2349359 View in ScopusGoogle Scholar [97] Yao X., Moon S.K., Bi G. A hybrid machine learning approach for additive manufacturing design feature recommendation Rapid Prototyp. J., 23 (2017), pp. 983-997, 10.1108/RPJ-03-2016-0041 View in ScopusGoogle Scholar [98] Madureira A., Santos J.M., Gomes S., Cunha B., Pereira J.P., Pereira I. Manufacturing rush orders rescheduling: a supervised learning approach 2014 Sixth World Congr. Nat. Biol. Inspired Comput., NaBIC 2014, IEEE (2014), pp. 299-304, 10.1109/NaBIC.2014.6921895 View in ScopusGoogle Scholar [99] Wuest T., Irgens C., Thoben K.-D. An approach to monitoring quality in manufacturing using supervised machine learning on product state data J. Intell. Manuf., 25 (2014), pp. 1167-1180, 10.1007/s10845-013-0761-y View in ScopusGoogle Scholar [100] Ma H., Wang Y., Wang K. Automatic detection of false positive RFID readings using machine learning algorithms Expert Syst. Appl., 91 (2018), pp. 442-451, 10.1016/j.eswa.2017.09.021 View PDFView articleView in ScopusGoogle Scholar [101] Cook D.J., Crandall A.S., Thomas B.L., Krishnan N.C. CASAS: A smart home in a box Computer, 46 (2013), pp. 62-69, 10.1109/MC.2012.328 View in ScopusGoogle Scholar [102] Krishnan N.C., Cook D.J. Activity recognition on streaming sensor data Pervasive Mob. Comput., 10 (2014), pp. 138-154, 10.1016/j.pmcj.2012.07.003 View PDFView articleView in ScopusGoogle Scholar [103] Monedero I., Biscarri F., León C., Guerrero J.I., Biscarri J., Millán R. Detection of frauds and other non-technical losses in a power utility using Pearson coefficient, Bayesian networks and decision trees Int. J. Electr. Power Energy Syst., 34 (2012), pp. 90-98, 10.1016/j.ijepes.2011.09.009 View PDFView articleView in ScopusGoogle Scholar [104] Simmhan Y., Aman S., Kumbhare A., Liu R., Stevens S., Zhou Q., Prasanna V. Cloud-based software platform for big data analytics in smart grids Comput. Sci. Eng., 15 (2013), pp. 38-47, 10.1109/MCSE.2013.39 View in ScopusGoogle Scholar [105] Osaba E., Onieva E., Moreno A., Lopez-Garcia P., Perallos A., Bringas P.G. Decentralised intelligent transport system with distributed intelligence based on classification techniques IET Intell. Transp. Syst., 10 (2016), pp. 674-682, 10.1049/iet-its.2016.0047 View in ScopusGoogle Scholar [106] Garcia F.C.C., Retamar A.E. Towards building a bus travel time prediction model for Metro Manila 2016 IEEE Reg. 10 Conf., IEEE (2016), pp. 3805-3808, 10.1109/TENCON.2016.7848775 View in ScopusGoogle Scholar [107] Vuong T.P. Cyber-physical Intrusion Detection for Robotic Vehicles University of Greenwich (2017) http://gala.gre.ac.uk/17445/7/TuanVuong2017.pdf Google Scholar [108] Lieber D., Stolpe M., Konrad B., Deuse J., Morik K. Quality prediction in interlinked manufacturing processes based on supervised & unsupervised machine learning Procedia CIRP, 7 (2013), pp. 193-198, 10.1016/j.procir.2013.05.033 View PDFView articleView in ScopusGoogle Scholar [109] Sugumaran V., Muralidharan V., Ramachandran K.I. Feature selection using decision tree and classification through proximal support vector machine for fault diagnostics of roller bearing Mech. Syst. Signal Process., 21 (2007), pp. 930-942, 10.1016/j.ymssp.2006.05.004 View PDFView articleView in ScopusGoogle Scholar [110] Liu Y., Jin S. Application of Bayesian networks for diagnostics in the assembly process by considering small measurement data sets Int. J. Adv. Manuf. Technol., 65 (2013), pp. 1229-1237, 10.1007/s00170-012-4252-7 View in ScopusGoogle Scholar [111] Hao L., Bian L., Gebraeel N., Shi J. Residual life prediction of multistage manufacturing processes with interaction between tool wear and product quality degradation IEEE Trans. Autom. Sci. Eng., 14 (2017), pp. 1211-1224, 10.1109/TASE.2015.2513208 View in ScopusGoogle Scholar [112] Nannapaneni S., Mahadevan S., Rachuri S. Performance evaluation of a manufacturing process under uncertainty using Bayesian networks J. Clean. Prod., 113 (2016), pp. 947-959, 10.1016/j.jclepro.2015.12.003 View PDFView articleView in ScopusGoogle Scholar [113] Coelho V.N., Coelho I.M., Coelho B.N., Reis A.J.R., Enayatifar R., Souza M.J.F., Guimarães F.G. A self-adaptive evolutionary fuzzy model for load forecasting problems on smart grid environment Appl. Energy, 169 (2016), pp. 567-584, 10.1016/j.apenergy.2016.02.045 View PDFView articleView in ScopusGoogle Scholar [114] Chakraborty S., Senjyu T., Yona A., Saber A.Y., Funabashi T. Solving economic load dispatch problem with valve-point effects using a hybrid quantum mechanics inspired particle swarm optimisation IET Gener. Transm. Distrib., 5 (2011), p. 1042, 10.1049/iet-gtd.2011.0038 View in ScopusGoogle Scholar [115] Ramos C.C.O., Souza A.N., Chiachia G., Falcão A.X., Papa J.P. A novel algorithm for feature selection using harmony search and its application for non-technical losses detection Comput. Electr. Eng., 37 (2011), pp. 886-894, 10.1016/j.compeleceng.2011.09.013 View PDFView articleView in ScopusGoogle Scholar [116] Ray P., Mishra D.P. Support vector machine based fault classification and location of a long transmission line Eng. Sci. Technol., 19 (2016), pp. 1368-1380, 10.1016/j.jestch.2016.04.001 View PDFView articleView in ScopusGoogle Scholar [117] Yuwono M., Qin Y., Zhou J., Guo Y., Celler B.G., Su S.W. Automatic bearing fault diagnosis using particle swarm clustering and hidden Markov model Eng. Appl. Artif. Intell., 47 (2016), pp. 88-100, 10.1016/j.engappai.2015.03.007 View PDFView articleView in ScopusGoogle Scholar [118] Navalertporn T., Afzulpurkar N.V. Optimization of tile manufacturing process using particle swarm optimization Swarm Evol. Comput., 1 (2011), pp. 97-109, 10.1016/j.swevo.2011.05.003 View PDFView articleView in ScopusGoogle Scholar [119] Li D., Jayaweera S.K. Machine-learning aided optimal customer decisions for an interactive smart grid IEEE Syst. J., 9 (2015), pp. 1529-1540, 10.1109/JSYST.2014.2334637 View in ScopusGoogle Scholar [120] Tai A.H., Ching W.-K., Chan L.Y. Detection of machine failure: Hidden Markov model approach Comput. Ind. Eng., 57 (2009), pp. 608-619, 10.1016/j.cie.2008.09.028 View PDFView articleView in ScopusGoogle Scholar [121] Zheng J., Ni L.M. Modeling heterogeneous routing decisions in trajectories for driving experience learning Proc. 2014 ACM Int. Jt. Conf. Pervasive Ubiquitous Comput., UbiComp ’14 Adjun., ACM Press, New York, New York, USA (2014), pp. 951-961, 10.1145/2632048.2632089 View in ScopusGoogle Scholar [122] Mocanu E., Nguyen P.H., Gibescu M., Kling W.L. Deep learning for estimating building energy consumption Sustain. Energy Grids Netw., 6 (2016), pp. 91-99, 10.1016/j.segan.2016.02.005 View PDFView articleView in ScopusGoogle Scholar [123] Lv Y., Duan Y., Kang W., Li Z., Wang F.-Y. Traffic flow prediction with big data: A deep learning approach IEEE Trans. Intell. Transp. Syst. (2014), pp. 1-9, 10.1109/TITS.2014.2345663 View PDFView articleView in ScopusGoogle Scholar [124] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L.D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, K. Zieba, End to end learning for self-driving cars, 2016. http://arxiv.org/abs/1604.07316. Google Scholar [125] Chen Z., Huang X. End-to-end learning for lane keeping of self-driving cars 2017 IEEE Intell. Veh. Symp., IEEE (2017), pp. 1856-1860, 10.1109/IVS.2017.7995975 View in ScopusGoogle Scholar [126] A. Taylor, Anomaly-based detection of malicious activity in in-vehicle networks, University of Ottawa, 2017. https://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0ahUKEwi1-cnir6XYAhVNY1AKHYY-DPEQFgg2MAE&url=https%3A%2F%2Fruor.uottawa.ca%2Fbitstream%2F10393%2F36120%2F3%2FTaylor_Adrian_2017_thesis.pdf&usg=AOvVaw1mk_GeMwTMT0Yn6kUxiMK. Google Scholar [127] P. Nicolas, Time complexity: Graph and machine learning algorithms, 2015. http://www.scalaformachinelearning.com/2015/11/time-complexity-in-machine-learning.html. Google Scholar [128] J. Su, H. Zhang, A fast decision tree learning algorithm, in: 21st Natl. Conf. Artif. Intell., Vol. 1. 5, 2006, pp. 500–505. Google Scholar [129] K. He, J. Sun, Convolutional neural networks at constrained time cost, in: IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 5353–5360. http://dx.doi.org/10.1109/CVPR.2015.7299173. Google Scholar [130] L. Bottou, C. Lin, Support vector machine solvers, 2006. Google Scholar [131] G. Louppen, Understanding random forest from theory to practice, University of Liège, 2014. https://arxiv.org/pdf/1407.7502.pdf. Google Scholar [132] Roussinov D.G., Chen H. A scalable self-organizing map algorithm for textual classification: A neural network approach to thesaurus generation Commun. Cogn. Artif. Intell. Spring., 15 (1998), pp. 81-112 Google Scholar [133] S. Koenig, R.G. Simmons, Complexity analysis of real-time reinforcement learning, in: Proc. AAAI Conf. Artif. Intell., 1993, pp. 99–105. Google Scholar [134] Simon D. Biogeography-based optimization IEEE Trans. Evol. Comput., 12 (2008), pp. 702-713, 10.1109/TEVC.2008.919004 View in ScopusGoogle Scholar [135] J.H. Korhonen, P. Parviainen, Exact learning of bounded tree-width Bayesian networks, in: Proc. 16th Int. Conf. AI Stat., 2013, pp. 370–378. Google Scholar [136] Bhattacharya A., Chattopadhyay P.K. Biogeography-based optimization for different economic load dispatch problems IEEE Trans. Power Syst., 25 (2010), pp. 1064-1077, 10.1109/TPWRS.2009.2034525 View in ScopusGoogle Scholar [137] Losing V., Hammer B., Wersing H. Incremental on-line learning: A review and comparison of state of the art algorithms Neurocomputing, 275 (2018), pp. 1261-1274, 10.1016/j.neucom.2017.06.084 View PDFView articleView in ScopusGoogle Scholar [138] Gama J., Žliobaite I., Bifet A., Pechenizkiy M., Bouchachia A. A survey on concept drift adaptation ACM Comput. Surv., 46 (2014), pp. 1-37, 10.1145/2523813 Google Scholar [139] A. R.R., D. P.R. Methods for incremental learning: A survey Int. J. Data Min. Knowl. Manag. Process., 3 (2013), pp. 119-125, 10.5121/ijdkp.2013.3408 Google Scholar [140] Hadas D., Yovel G., Intrator N. Using unsupervised incremental learning to cope with gradual concept drift Conn. Sci., 23 (2011), pp. 65-83, 10.1080/09540091.2011.575929 View in ScopusGoogle Scholar [141] Liang Nan-Ying, Huang Guang-Bin, Saratchandran P., Sundararajan N. A fast and accurate online sequential learning algorithm for feedforward networks IEEE Trans. Neural Netw., 17 (2006), pp. 1411-1423, 10.1109/TNN.2006.880583 View in ScopusGoogle Scholar [142] Suresh S., Dong K., Kim H.J. A sequential learning algorithm for self-adaptive resource allocation network classifier Neurocomputing, 73 (2010), pp. 3012-3019, 10.1016/j.neucom.2010.07.003 View PDFView articleView in ScopusGoogle Scholar [143] Huynh H.T., Won Y. Regularized online sequential learning algorithm for single-hidden layer feedforward neural networks Pattern Recognit. Lett., 32 (2011), pp. 1930-1935, 10.1016/j.patrec.2011.07.016 View PDFView articleView in ScopusGoogle Scholar [144] Guo L., Hao J., Liu M. An incremental extreme learning machine for online sequential learning problems Neurocomputing, 128 (2014), pp. 50-58, 10.1016/j.neucom.2013.03.055 View PDFView articleGoogle Scholar [145] Quinlan J.R. Induction of decision trees Mach. Learn., 1 (1986), pp. 81-106, 10.1007/BF00116251 View in ScopusGoogle Scholar [146] Utgoff P.E. Incremental induction of decision trees Mach. Learn., 4 (1989), pp. 161-186, 10.1023/A:1022699900025 View in ScopusGoogle Scholar [147] Kalles D., Morris T. Efficient incremental induction of decision trees Mach. Learn., 24 (1996), pp. 231-242, 10.1007/BF00058613 Google Scholar [148] Domingos P., Hulten G. Mining high-speed data streams Proc. Sixth ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., KDD ’00, ACM Press, New York, New York, USA (2000), pp. 71-80, 10.1145/347090.347107 View in ScopusGoogle Scholar [149] Gama J., Rocha R., Medas P. Accurate decision trees for mining high-speed data streams Proc. Ninth ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., KDD ’03, ACM Press, New York, New York, USA (2003), 10.1145/956750.956813 Google Scholar [150] Pfahringer B., Holmes G., Kirkby R. New options for hoeffding trees Proc. 20th Aust. Jt. Conf. Adv. Artif. Intell., Springer-Verlag, Berlin, Heidelberg (2007), pp. 90-99 http://dl.acm.org/citation.cfm?id=1781238.1781251 CrossRefView in ScopusGoogle Scholar [151] Rutkowski L., Pietruczuk L., Duda P., Jaworski M. Decision trees for mining data streams based on the McDiarmid’s bound IEEE Trans. Knowl. Data Eng., 25 (2013), pp. 1272-1279, 10.1109/TKDE.2012.66 View in ScopusGoogle Scholar [152] Rutkowski L., Jaworski M., Pietruczuk L., Duda P. The CART decision tree for mining data streams Inf. Sci. (N,Y)., 266 (2014), pp. 1-15, 10.1016/j.ins.2013.12.060 View PDFView articleView in ScopusGoogle Scholar [153] Abdulsalam H., Skillicorn D.B., Martin P. Streaming random forests 11th Int. Database Eng. Appl. Symp., IDEAS 2007, IEEE (2007), pp. 225-232, 10.1109/IDEAS.2007.4318108 View in ScopusGoogle Scholar [154] Abdulsalam H., Skillicorn D.B., Martin P. Classification using streaming random forests IEEE Trans. Knowl. Data Eng., 23 (2011), pp. 22-36, 10.1109/TKDE.2010.36 View in ScopusGoogle Scholar [155] Saffari A., Leistner C., Santner J., Godec M., Bischof H. On-line random forests 2009 IEEE 12th Int. Conf. Comput. Vis. Work, ICCV Work, IEEE (2009), pp. 1393-1400, 10.1109/ICCVW.2009.5457447 View in ScopusGoogle Scholar [156] M. Denil, D. Matheson, N. De Freitas, Consistency of online random forests, in: Int. Conf. Mach. Learn., 2013, pp. 1256–1264. Google Scholar [157] Kivinen J., Smola A.J., Williamson R.C. Online learning with kernels IEEE Trans. Signal Process., 52 (2004), pp. 2165-2176, 10.1109/TSP.2004.830991 View in ScopusGoogle Scholar [158] Laskov P., Gehl C., Kruger S., Muller K.-R. Incremental support vector learning: Analysis, implementation and applications J. Mach. Learn. Res., 7 (2006), pp. 1909-1936 View in ScopusGoogle Scholar [159] Karasuyama M., Takeuchi I. Multiple incremental decremental learning of support vector machines IEEE Trans. Neural Netw., 21 (2010), pp. 1048-1059, 10.1109/TNN.2010.2048039 View in ScopusGoogle Scholar [160] Gu B., Sheng V.S., Tay K.Y., Romano W., Li S. Incremental support vector learning for ordinal regression IEEE Trans. Neural Netw. Learn. Syst., 26 (2015), pp. 1403-1416, 10.1109/TNNLS.2014.2342533 View in ScopusGoogle Scholar [161] Tang B., Chen Z., Hefferman G., Pei S., Wei T., He H., Yang Q. Incorporating intelligence in fog computing for big data analysis in smart cities IEEE Trans. Ind. Inform., 13 (2017), pp. 2140-2150, 10.1109/TII.2017.2679740 View in ScopusGoogle Scholar [162] Garcia Lopez P., Montresor A., Epema D., Datta A., Higashino T., Iamnitchi A., Barcellos M., Felber P., Riviere E. Edge-centric computing: Vision and challenges ACM SIGCOMM Comput. Commun. Rev., 45 (2015), pp. 37-42, 10.1145/2831347.2831354 Google Scholar [163] Gu L., Zeng D., Guo S., Barnawi A., Xiang Y. Cost efficient resource management in fog computing supported medical cyber-physical system IEEE Trans. Emerg. Top. Comput., 5 (2017), pp. 108-119, 10.1109/TETC.2015.2508382 View in ScopusGoogle Scholar [164] Xu B., Xu L., Cai H., Jiang L., Luo Y., Gu Y. The design of an m -health monitoring system based on a cloud computing platform Enterp. Inf. Syst., 11 (2017), pp. 17-36, 10.1080/17517575.2015.1053416 View in ScopusGoogle Scholar [165] Tu Q., Lu J.F., Yuan B., Tang J.B., Yang J.Y. Density-based hierarchical clustering for streaming data Pattern Recognit. Lett., 33 (2012), pp. 641-645, 10.1016/j.patrec.2011.11.022 View PDFView articleView in ScopusGoogle Scholar [166] Goodwin M.S., Haghighi M., Tang Q., Akcakaya M., Erdogmus D., Intille S. Moving towards a real-time system for automatically recognizing stereotypical motor movements in individuals on the autism spectrum using wireless accelerometry Proc. 2014 ACM Int. Jt. Conf. Pervasive Ubiquitous Comput., UbiComp ’14 Adjun., ACM Press, New York, New York, USA (2014), pp. 861-872, 10.1145/2632048.2632096 View in ScopusGoogle Scholar [167] Verba N., Chao K.-M., James A., Goldsmith D., Fei X., Stan S.-D. Platform as a service gateway for the fog of things Adv. Eng. Inform., 33 (2017), pp. 243-257, 10.1016/j.aei.2016.11.003 View PDFView articleView in ScopusGoogle Scholar [168] I. Stoica, D. Song, R.A. Popa, D. Patterson, M.W. Mahoney, R. Katz, A.D. Joseph, M. Jordan, J.M. Hellerstein, J. Gonzalez, K. Goldberg, A. Ghodsi, D. Culler, P. Abbeel, A Berkeley view of systems challenges for AI, 2017. http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-159.html. Google Scholar [169] Baryannis G., Kritikos K., Plexousakis D. A specification-based QoS-aware design framework for service-based applications Serv. Oriented Comput. Appl., 11 (2017), pp. 301-314, 10.1007/s11761-017-0210-4 View in ScopusGoogle Scholar [170] Wang W., Lee K., Murray D. A global generic architecture for the future internet of things Serv. Oriented Comput. Appl., 11 (2017), pp. 329-344, 10.1007/s11761-017-0213-1 View in ScopusGoogle Scholar Cited by (0) Xiang Fei received a BSc and a PhD from Southeast University China in 1992 and 1999 respectively. After graduation, Xiang Fei worked on a number of projects including European IST Programs and EPSRC. H joined Cogent at Coventry University as a Research Fellow in October 2008 and he is currently working as a senior lecturer for the School of Computing, Electronics and Maths at Coventry University. Nazaraf Shah is a Senior Lecturer at Coventry, UK. He received the PhD in Multi-Agent Systems from Coventry University, Coventry, UK, in 2006. His research interests include intelligent agents, service oriented computing and cloud computing. He has more than 50 publications in various international conference proceedings and journals. Nandor Verba received the B. E. degree in Systems Theory with specialization in Automation from the Technical University of Cluj-Napoca, Romania in 2015. He is currently pursuing a Ph.D. in Computing at Coventry University, UK. His research interests include Wireless Sensor Networks, Internet of Things, Cloud and Fog computing. Kuo-Ming Chao is a Professor of Computing at Coventry University, UK. His research interests include the areas of Service-Oriented Computing, IoT, Cloud Computing, CPS and Big Data etc., as well as their applications. He has over 180 refereed publications. He is a co-editor-in-chief of Service-Oriented Computing and Applications –a Springer Journal. Victor Sanchez-Anguix received a BSc. in Computer Science at Universitat Politècnica de València. Later, he obtained a MSc. in Artificial Intelligence and Pattern Recognition by Universitat Politècnica de València. In February 2013 Victor Sanchez-Anguix defended his PhD. thesis in Computer Science at Universitat Politècnica de València. After finishing my PhD. he started an industry position as a data scientist at Atrapalo.com. Currently, Victor Sanchez-Anguix is working as a lecturer at Coventry University. Jacek Lewandowski is a Lecturer in Information Systems at Coventry University, UK. He received the Ph.D. degree in computer science from Coventry University, Coventry, U.K., in 2014. His research focuses on WSN, IoT, cloud and fog computing, artificial intelligence as well as information and decision systems development for industrial and medical applications. Anne James is Professor at the College of Science and Technology, School of Science & Technology, Nottingham Trent University. Before that she was Professor of Data Systems Architecture in the Distributed Systems and Modelling Applied Research Group at Coventry University. Anne has been active in teaching in higher education and in participating in research projects as part of national and international teams. Her main duties currently involve leading a Computing and Technology department, teaching and supervising research students. The research interests of Professor James are in the general area of creating distributed systems to meet new and unusual data and information challenges. She currently has projects in spatial data infrastructure, bioinformatics, Cloud forensics, and intrusion detection. Professor James has successfully supervised around 30 research degrees and has published around 200 papers in peer reviewed journals or conferences. Zahid Usman is a chartered engineer with extensive experience in both industrial and research environments. His research experience expands to over six years within the areas of manufacturing information, machine vision, robotics and automation. He is an active researcher in robotic metrology and assembly, machine vision and manufacturing informatics. He has been involved in several research projects with leading aerospace and automotive industries. He also has experience of working for leading manufacturing organizations such as Rolls Royce and Massey Fergusons Tractors. As a teacher, Dr Usman is focused on training engineering students to be industrially ready by conducting industry based teaching. Zahid Usman is currently working for Rolls-Royce plc in the United Kingdom. View Abstract © 2018 Elsevier B.V. All rights reserved. Part of special issue Special Issue on Edge of the Cloud Edited by Anne James, Giancarlo Fortino, Joe-Air Jiang, Kuo-Ming Chao View special issue Recommended articles Association between leukoaraiosis and cerebral blood flow territory alteration in asymptomatic internal carotid artery stenosis Clinical Radiology, Volume 73, Issue 5, 2018, pp. 502.e9-502.e14 Y.-F. Chen, …, S.-F. Jiang View PDF A novel framework for wind speed prediction based on recurrent neural networks and support vector machine Energy Conversion and Management, Volume 178, 2018, pp. 137-145 Chuanjin Yu, …, Guanghao Zhai View PDF Assessment of lean manufacturing effect on business performance using Bayesian Belief Networks Expert Systems with Applications, Volume 42, Issue 19, 2015, pp. 6539-6551 Gülçin Büyüközkan, …, İbrahim S. Karakadılar View PDF Show 3 more articles Article Metrics Citations Citation Indexes: 92 Captures Readers: 316 View details About ScienceDirect Remote access Shopping cart Advertise Contact and support Terms and conditions Privacy policy Cookies are used by this site. Cookie Settings All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.

Paper 3:
- APA Citation: None
  Main Objective: The main objective of the study is to develop an automated, real-time irrigation management system that can continuously update and improve its performance based on incoming data, using online learning techniques.
  Study Location: None
  Data Sources: None
  Technologies Used: None
  Key Findings: None
  Extract 1: None
  Extract 2: None
  Limitations: None
  Relevance Evaluation: The paper is highly relevant to the outline point as it directly addresses the application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests. The paper provides a comprehensive overview of the topic, discusses the challenges and limitations of existing approaches, and proposes a novel solution that leverages online learning techniques to improve the accuracy and efficiency of real-time irrigation management systems.
  Relevance Score: 1.0
  Inline Citation: None
  Explanation: The study aims to explore how automated, real-time irrigation management systems can contribute to the efficient use of water resources and enhance agricultural productivity. It also seeks to identify gaps and propose solutions for seamless integration across the automated irrigation management system to achieve fully autonomous, scalable irrigation management.

 Full Text: >
SPECIAL ISSUE ON IOT-BASED HEALTH MONITORING SYSTEM
Toward real-time and efficient cardiovascular monitoring for COVID-19
patients by 5G-enabled wearable medical devices: a deep learning
approach
Liang Tan1,2 • Keping Yu3
• Ali Kashif Bashir4,5 • Xiaofan Cheng1 • Fangpeng Ming1 • Liang Zhao6 •
Xiaokang Zhou7
Received: 12 October 2020 / Accepted: 8 June 2021 / Published online: 4 July 2021
 The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2021
Abstract
Patients with deaths from COVID-19 often have co-morbid cardiovascular disease. Real-time cardiovascular disease
monitoring based on wearable medical devices may effectively reduce COVID-19 mortality rates. However, due to
technical limitations, there are three main issues. First, the traditional wireless communication technology for wearable
medical devices is difﬁcult to satisfy the real-time requirements fully. Second, current monitoring platforms lack efﬁcient
streaming data processing mechanisms to cope with the large amount of cardiovascular data generated in real time. Third,
the diagnosis of the monitoring platform is usually manual, which is challenging to ensure that enough doctors online to
provide a timely, efﬁcient, and accurate diagnosis. To address these issues, this paper proposes a 5G-enabled real-time
cardiovascular monitoring system for COVID-19 patients using deep learning. Firstly, we employ 5G to send and receive
data from wearable medical devices. Secondly, Flink streaming data processing framework is applied to access electro-
cardiogram data. Finally, we use convolutional neural networks and long short-term memory networks model to obtain
automatically predict the COVID-19 patient’s cardiovascular health. Theoretical analysis and experimental results show
that our proposal can well solve the above issues and improve the prediction accuracy of cardiovascular disease to 99.29%.
Keywords 5G  Cardiovascular monitoring  Deep learning  Flink  CNN  LSTM
1 Introduction
COVID-19 is the ﬁrst global coronavirus pandemic that
humanity has ever faced, and awareness of COVID-19 is
still growing. In addition to advanced medical technology,
big data plays a crucial role in the prevention and control of
COVID-19 [1, 2]. In the real-time environment constituted
by the Internet of Things (IoT), not only sensors and
mobile devices have generated a large amount of data, but
also software applications, web, and other resources have
generated massive data [3, 4]. Especially in medical and
healthcare, due to the emergence of a large number of
wearable medical devices, the data generated by these
devices need to be collected remotely in real time. How-
ever, it brought many new challenges. One of the most
critical challenges is to extract streaming data in real time
and process and analyze different types of data [5].
The death of COVID-19 patients is often accompanied
by underlying cardiovascular and other diseases. Cardio-
vascular
diseases
characterized
by
suddenness
have
become one of the main diseases threatening human health
[6]. A large number of medical practices show that when
the heart suddenly stops, the best rescue time is within 4
minutes. At the same time, we found that if we can detect
subtle signs in advance and take effective measures, 70%
of cardiovascular disease patients, including heart attacks,
can avoid death. Currently, electrocardiogram (ECG) is the
most straightforward and efﬁcient clinical examination
method for all kinds of cardiovascular and cerebrovascular
diseases. Therefore, real-time and effective monitoring,
analysis, and diagnosis of the COVID-19 patient’s ECG
signal are particularly important [7]. Moreover, the com-
munication between doctors and COVID-19 patients is not
accurate, convenient, and timely, such as telephone and
text communication. Although doctors and hospitals can
observe
the
health
of
COVID-19
patients
through
Extended author information available on the last page of the article
123
Neural Computing and Applications (2023) 35:13921–13934
https://doi.org/10.1007/s00521-021-06219-9
(0123456789().,-volV)(0123456789().
,- volV)
inspections, this traditional hospital-based diagnosis and
treatment lack immediacy and continuity, and it is difﬁcult
to capture the COVID-19 patients’ signs. Therefore, it is of
practical signiﬁcance to provide real-time ECG diagnosis
and predict services for those in need [8].
With wearable medical devices (such as heart rate
monitoring cuffs, blood pressure, blood glucose meters,
etc.), it can continuously track COVID-19 patients’ health
and provide personalized healthcare solutions [9]. How-
ever, the data continuously generated by wearable medical
devices need not only real-time processing but also diag-
nosis and prediction. Existing wireless communication
technologies have problems such as high latency and low
speed. Moreover, relying on manual recognition of elec-
trocardiogram diagnosis methods can no longer meet the
current medical needs. Simultaneously, traditional machine
learning-based methods need to manually extract electro-
cardiogram features, which cannot realize the automation
of the diagnosis process and the accuracy of the results.
Therefore, it is a signiﬁcant challenge to process a large
amount of data generated by sensors and diagnose them in
real time in a critical situation [10].
Given the aforementioned difﬁculties with traditional
cardiovascular diseases diagnoses and wearable device
data processing and analysis, the automatic classiﬁcation
and diagnosis of assisted ECG signals for COVID-19
patients based on deep learning is an effective solution to
the above problems. In addition, with the emergence of 5G
[11, 12], a solution with high throughput and low latency
has been provided for processing a large amount of mon-
itoring data [13]. Moreover, the development of big data
technology also provides many open source platforms for
real-time processing of streaming data, such as Spark,
Druid, and Flink. With the continuous advancement of
artiﬁcial intelligence technology, the application of artiﬁ-
cial intelligence technology to medical diagnosis is a
general trend [14]. In recent years, in order to meet the
needs of high-speed and high-precision ECG analysis, deep
neural networks have been widely used in automatic ECG
diagnosis [15].
This article proposes a real-time cardiovascular moni-
toring system for COVID-19 patients based on 5G and
deep learning with the assist of wearable medical devices
that can transmit human ECG signal data. The Second 2
introduces related work. The overall architecture of the
real-time monitoring system is proposed in Sect. 3. Sec-
tion 4 is the ECG signal classiﬁcation algorithm based on
deep learning. In Sect. 5, we put forward the experimental
method and evaluation. Finally, Sect. 6 summary the
conclusion.
2 Related works
In recent years, big data analysis related to healthcare has
become an important issue in many research ﬁelds, such as
machine learning, deep learning, and data mining using
medical and health data and information available in hos-
pitals. The progress of the data collection process comes
from the tremendous development of technology in the
medical and health ﬁeld, in which data records are col-
lected through three main stages of digital data ﬂow gen-
erated from patient clinical records, health research records
and organization operations [16]. Analyzing these data for
computer-aided diagnosis and then developing real-time
systems has become the development trend of today’s
smart medical care.
Sun and Reddy et al. [17] discussed an overview of
health care data sources. This research analyzes that health
care data plays a very important role in many systems such
as disease prediction, prevention methods, medical guid-
ance, and emergency medical decision-making to improve
health, reduce costs and increase efﬁciency. And in the
existing research, a variety of Spark machine learning
models have been used in medical databases. For example,
in [18], a real-time health prediction system using spark
machine learning streaming big data is introduced. The
system is tested on tweets of users with health attributes.
The system receives the tweets, extracts features, and uses
decision tree algorithms to predict The health of the user
and ﬁnally, sends the information directly to the user to
take appropriate action. In addition, Alottaibi et al. [19]
proposed a Sehaa-Kingdom of Saudi Arabia (KSA) Central
Arab Healthcare Twitter data big data analysis tool. The
system uses two different machine learning algorithms,
including naive Bayes and logistic regression algorithms,
and applies multiple feature extraction methods to detect
various diseases in KSA. In [20], a system based on
Apache Spark that can predict heart disease in real time
uses memory computing to apply machine learning to
streaming data. The system is divided into two stages. The
ﬁrst stage is to apply classiﬁcation algorithms to data for
heart disease prediction through Spark MLlib and Spark
streaming, and the second stage is to use Apache Cassandra
to store massively generated data and visualizations.
In addition, many systems that use wearable devices to
collect data to predict cardiovascular disease have been
proposed in recent years [21, 22]. Al-Makhadmeh and
Tolba et al. [23] proposed a heart disease detection system
based on wearable medical equipment. The system trans-
mits the collected patient heart data to the medical system
and then uses feature extraction technology and deep
learning model Value feature extraction and correct clas-
siﬁcation. Lin et al. [24] proposed a system based on
13922
Neural Computing and Applications (2023) 35:13921–13934
123
Support Vector Machine (SVM) classiﬁer to predict
patients suffering from left ventricular hypertrophy. The
system collects young people’s age, height, weight and
electrocardiogram data for rapid diagnosis. Khade et al.
[25] proposed a cardiovascular disease prediction system
based on SVM and Convolutional Neural Networks
(CNN). The system sends ECG signals to the SVM clas-
siﬁer and Boosted Decision Tree to classify cardiovascular
diseases and then uses CNN to predict severe Degree, and
the accuracy rate is 88.3%. Zhao et al. [26] proposed a
system that uses CNN to detect the original ECG signal in a
wearable device. This system avoids the traditional manual
feature extraction process and implements the system
based on the cloud. In addition, Kumar et al. [27] proposed
a three-layer framework system with an ML model to
receive data from wearable devices and perform analysis
and processing. The ﬁrst layer collects ECG data from
wearable devices. The second layer stores healthcare data
in the cloud, and the third layer uses logistic regression
algorithms to predict cardiovascular diseases.
Moreover, many researchers have used deep learning in
the ﬁeld of myocardial infarction detection and coronavirus
detection. M Hammad et al. [28] proposed an automatic
detection method for myocardial infarction based on the
end-to-end model of a deep convolutional neural network
and used the focal loss function optimization model for
data imbalance, and ﬁnally reached 98.84% on the PTB
data set. [29] provides a promising solution by proposing a
COVID-19 detection system based on deep learning. The
simulation results reveal that the proposed deep learning
modalities can be considered and adopted for quick
COVID-19 screening; [30] propose a modiﬁed version of
Fuzzy C-Means for segmenting 3D medical volumes,
which has been rarely implemented for 3D medical image
segmentation; in [31], two new quantum information hid-
ing methods are proposed for telemedicine image sharing.
The results show that the proposed methods have excellent
visual quality, high embedding capability and security.
Most researches rely on speciﬁc medical care data
sources and apply them on ofﬂine systems. However,
medical data sources are diverse and new data are con-
stantly being produced. Real-time healthcare analysis
involves real-time streaming data processing, machine
learning algorithms, and real-time analysis, while tradi-
tional data transmission has defects such as high latency
and low throughput. In addition, machine learning algo-
rithms rely on manual extraction for feature extraction.
There are limitations in its effective feature extraction and
feature weighting methods, which limits the classiﬁcation
ability to a certain extent. Therefore, we use 5G technology
[32, 33] and deep learning [34, 35] to propose a real-time
medical monitoring system for cardiovascular diseases,
which is used to process real-time data streams transmitted
from wearable devices to predict the health of patients in
real time and send timely information to patients.
3 The online prediction system
The real-time online health monitoring system mainly uses
open source frameworks such as Kafka, Flink, and Ten-
sorﬂow to realize the transmission of personal data and the
construction of monitoring models. The system can obtain
the COVID-19 patients’ core information through wearable
devices, namely ECG signals, analyze the data with deep
learning algorithms in Cloud, and predict the COVID-19
patients’ health risk. The overall structure diagram is
shown in Fig. 1, which consists of four modules. The ﬁrst
module is responsible for data acquisition and transmis-
sion. The second module is responsible for streaming
storage and analysis, and the third module is responsible
for the training module of deep learning. The fourth stage
is the module responsible for health diagnose and predict
for Covid-19 patients.
3.1 Details of each module
Acquisition and transmission mainly use acquisition
equipment, 5G network infrastructure and other facilities.
Details are as follows:
(1)
Acquisition equipment It is composed of various
sensor modules, controllers, processors and power
supplies embedded in wearable devices. The main
function is to complete the collection, processing and
transmission of ECG and Global Positioning System
(GPS) positioning data. The selection of these
modules needs to meet the needs of usability and
wearability, so as to make the wearer feel comfort-
able and move as much as possible. The photoelec-
tric
sensor,
ECG
sensor
and
GPS
sensor
are
integrated into the microcontroller. The sensor
converts the collected electrical signal into an analog
electrical signal, which is processed by the controller
and becomes a digital signal, which is transmitted to
the handheld device of the individual user through
the wireless sensor module. The positioning infor-
mation and ECG information obtained by the
handheld device can be uploaded to the cloud server
through the 5G network.
(2)
5G infrastructure 5g network is a digital cellular
network, including supplier service area covered by a
series of small geographic area called cellular. The
analog signal with the ECG information is digitized
on a handheld device, and an analog-to-digital
converter turns the digitized information into a
Neural Computing and Applications (2023) 35:13921–13934
13923
123
bitstream for transmission. In the handheld device’s
network, 5g network local antenna array and auto-
matic sending and receiving devices with low-power
consumption and handheld devices to communicate
by radio waves [36]. The frequency of the commu-
nication channel is assigned by the public pool
selection frequency. 5G infrastructure includes 5G
access network and 5G core network. Its main
purpose is to provide high-bandwidth, low-latency
communications. Compared with the traditional
network, 5G network has higher access rate and
lower delay. It can meet the access requirements of
ultra-high trafﬁc density, ultra-high connection num-
ber density and ultra-high mobility. It also improves
the spectrum efﬁciency of the network, and reduces
the operation and maintenance costs while improving
the network energy efﬁciency. it is very suitable for
the real-time system environment of this article.
(3)
Access to 5G infrastructure Due to the low power
consumption, the wearable device adopts a Bluetooth
connected to a handheld terminal with 5G connection
function (such as a 5G mobile phone), and the data
collected by the wearable device are uploaded to the
cloud after connecting to the 5G network through the
handheld terminal.
Streaming storage and reading stageThe framework is
mainly divided into two stages, the speciﬁc introduction is
as described in next section. In the ﬁrst stage, the message
queue collects information from different COVID-19
patients, including the input of source data and the con-
sumption of source data. The second stage is the stream
processing pipeline, in which the Flink stream receives the
monitoring data stream related to the ECG attribute, and
then adopts a batch and stream integrated (streaming for a
single COVID-19 patient, and a batch for the entire
COVID-19 patients). Perform feature engineering and
feature selection operations on the data stream, and wait for
data storage and model training.
The training framework of the ECG risk modelIt is
mainly responsible for receiving preprocessed input data,
online learning and notiﬁcation of whether to update our
deployed model through the message middleware accord-
ing to the newly obtained data stream. The initial model of
the model is obtained through training by inputting labeled
data. Consider the complexity of the model and the con-
tinuity of ECG data. We use the CNN?LSTM module as
the bottom layer of the model, and the detailed training
method is in Section IV. The trained model waits for the
monitoring of the monitoring module to achieve the pur-
pose of optimization.
ECG monitoring and risk inference moduleIn order to
improve the optimization of the model, we have added a
registered monitoring middleware to the monitoring mod-
ule. The middleware mainly has two functions:
(1)
Whether the value of the data in the monitoring
system has expired. The time from data entering the
system to the model making a risk judgment is much
shorter than the processing time after an ECG
accident occurs, thereby ensuring the real-time
nature of the system.
(2)
Another is to monitor whether the model changes.
When the model changes, the judgment module is
notiﬁed to verify whether the model is optimized. So
as to achieve the purpose of real-time perfecting the
model. Finally, when the model is perfected this
time, the output stream is sent back to Kafka again,
and the monitoring module will determine whether
there is an abnormality in the COVID-19 patients’
data stream. For special abnormal information, the
alarm function of the system will be triggered. For
Fig. 1 Architecture design of Cardiovascular Monitoring System for COVID-19 Patients
13924
Neural Computing and Applications (2023) 35:13921–13934
123
normal prediction results, the inference module will
push the result predictions to the data center for
storage. In this way, through the personal client,
COVID-19 patients can obtain personal monitoring
information and personalized health reports in real
time.
3.2 Details of streaming data
The process of collecting data sets by physical equipment
is often real time and continuous. The collected data
mainly include COVID-19 patient personal information,
ECG information, positioning information, network delay
monitoring information, etc. The device processes the data
set as time passes, that is to say, it is transmitted to the
server receiving end through the handheld terminal in the
form of streaming data. The whole service adopts the
Client/Server method for individual users, and the receiver
accepts data through the Transmission Control Protocol
(TCP). However, directly receiving and processing this
kind of high-throughput data will cause huge pressure on
network resources, so it is necessary to buffer the data
when receiving the data. The message queue is used as a
buffering solution to effectively solve the problem of data
inconsistency. As a high-throughput distributed messaging
system, Kafka can support real-time data processing and
provide real-time data to the next processor, so Kafka is
selected as a data transmission tool.
Secondly, a large number of iterative calculations will
be generated in the data processing calculation process, and
the processor is required to be able to efﬁciently support
iterative data processing in real time. Choose Flink with
higher-level APIs and better benchmark results [37]. Flink
provides a wealth of APIs. At the same time, the integra-
tion with Druid and Redis is already quite high. We will
choose Redis to store the ﬁnal calculation results. Below
we will elaborate on the process of streaming data from the
perspective of Kafka and Flink frameworks. The details of
streaming data are shown in Fig. 2.
3.2.1 Kafka data stream processing
Input of source data The physical device uses the hand-
held device to ﬁrst send to the streaming storage and
reading framework via TCP, and then TCP binds the IP and
port number to start the data receiving thread. After
receiving the data, the system will store the source data in
the datasource. In order to effectively analyze and process
the original data, the system needs to extract, transform,
and load (ETL) a large amount of original data to the target
storage data warehouse. In the face of a large number of
COVID-19 patient data, a buffer queue is set up from the
source data to the data warehouse, namely kafka.
(1)
At this stage, Kafka will create topics through Linux
commands. Topics are created according to the data
types (UserInfo, ECGInfo, GPSInfo). Second, con-
ﬁgure the properties of the producer. Finally, various
objects are initialized to complete the conﬁguration
of Kafka. The conﬁgured objects include serialized
objects, partitioners, acks objects, etc.
(2)
When sending data to the Kafka cluster, in order to
achieve Kafka ﬂow card control, the system needs to
create a buffer area for the coming data. After setting
the buffer data size, when the buffer area is full, it
will be sent to Kafka uniformly.
(3)
In data parsing, ﬁrst call the dataParser method,
obtain the data type according to the data type
position in the data frame deﬁnition, and record the
data type. At the same time, if the whole frame of
data is all 0 during the analysis, it means that the
frame data is invalid data, and the frame data is
skipped. After analyzing the data type, enter the
analysis function of different data items according to
the data type, analyze the whole data, and store it in
the form of an object.
(4)
When each test item creates the ProducerRecord
object, the source data needs to be reorganized to
form a processing form in Flink. There are two
objects involved in sending messages, KafkaPro-
ducer and ProducerRecord. ProducerRecord speciﬁes
the topic information that needs to be sent, the
message content value, and can also specify partition
information and key values. After the partition is
selected, the producer can determine which topic and
which partition to send the message to. Inside the
Producer, a separate thread will send the record to
the corresponding broker.
(5)
After the Kafka server successfully receives the
message sent by the Producer, it will respond with a
response. If the message is written successfully, it
will return a RecordMetaData object. If it fails, the
Producer will resend the message. If it fails after a
few times, it will return an error message.
Consumption of source data After Kafka processes the
data, consumers need to subscribe to the message and read
the data in Kafka. In this article, Kafka consumers are the
target storage data warehouse and the Flink clusters. In
Flink, Kafka’s Partition needs to correspond to Flink’s
parallel task instance. Flink can also guarantee that even
after a failure, the allocation of partitions to Flink instances
can be maintained, so partition determinism is maintained,
which ensures that data processing is exactly at once. This
effectively guarantees the integrity of the system data.
Neural Computing and Applications (2023) 35:13921–13934
13925
123
3.2.2 Flink data stream processing
After conﬁrming that the Kafka consumer is Flink, it is
necessary to create a Flink-Kafka consumer object. The
realization of a data processing process based on Flink
mainly includes the following ﬁve steps. First, you need to
obtain the execution environment for data processing, and
then load the initial data. After the data are loaded into
Flink, specify the data conversion method, that is, to realize
the speciﬁc data processing process, that is, the speciﬁc
deep learning-based classiﬁcation method in Sect. 4. In
addition, the model update is also based on the data col-
lection situation within a period of time to update the
model, the notice service will let the new model act on the
system’s prediction. After the processing is completed,
specify the storage location of the data calculation result.
Since Flink is lazy loaded, after the above steps are deﬁned,
the execution of the program needs to be triggered at the
end before the Fink cluster will start processing data. For
patients detected as suspected of COVID-19 patient, the
system will also alert management personnel to deal with
such situations.
4 Cardiovascular disease classification
algorithm based on Deep Learning
4.1 Data preprocessing
The ECG signal is a weak electrical signal, which is easily
interfered by electrical signals from various other sources,
including baseline drift, EMG interference and power fre-
quency interference. In order to obtain a truly useful ECG
signal, we need to denoise these high-frequency or low-
frequency noises. In addition, a complete ECG is com-
posed of a long continuous time series. If it is directly input
into the neural network classiﬁcation, it will greatly
increase the complexity of the network calculation, and it is
not conducive to extracting good feature information.
Therefore, in the study of ECG signal classiﬁcation, the
entire ECG signal is usually divided into several small
segments in units of heart beats according to speciﬁc rules.
The denoising of ECG signals and heartbeat segmentation
are collectively referred to as preprocessing.
4.1.1 ECG signal denoising
The ECG signal contains a variety of different types of
high-frequency or low-frequency noise. The purpose of
denoising the ECG signal is to suppress the noise in the
signal, and to enhance the part that can contribute to feature
extraction. In order to obtain useful signals, this paper uses
discrete wavelet transform (DWT) to process ECG signals.
Because the tightness of the ECG signal processing has a
Fig. 2 ECG data ﬂow process
13926
Neural Computing and Applications (2023) 35:13921–13934
123
greater impact on the signal, Daubechies wavelet is more
suitable. In this paper, DB8 wavelet basis is selected to
decompose the ECG signal into 8 layers, and the coefﬁ-
cients of each layer are obtained. The frequency difference
of the three noises of ECG signal is relatively large, so the
soft threshold processing method is selected, that is, dif-
ferent thresholds are used for quantization processing at
different transform scales. Finally, the ECG signal is
reconstructed according to the low-frequency coefﬁcients
of the 8th layer obtained by DB8 wavelet decomposition
and the high-frequency coefﬁcients of each layer, and the
denoised ECG signal is obtained. The evaluation method of
the denoising effect of the ECG signal is reﬂected by the
signal-to-noise ratio (SNR) and the mean square error
(MSE). The larger the SNR value, the less noise the
denoised ECG signal contains and the better the denoising
effect; the smaller the MSE, the smaller the degree of
distortion of the ECG signal. The calculation method is
shown in formula (1) and (2) where s(i) represents the
original ECG signal containing noise, x(i) represents the
denoised ECG signal, and N represents the length of the
collected ECG signal.
SNR ¼ 10  lg
XN
n¼1
sðiÞ2
½xðiÞ  sðiÞ2
ð1Þ
MSE ¼
PN
n¼1 ½xðiÞ  sðiÞ2
N
ð2Þ
4.1.2 Heartbeat segmentation
After the ECG signal is denoised, the R peak in the
waveform needs to be located and segmented. For the R
peak detection task, combined with the real-time require-
ments of ECG signal detection, the current mainstream R
peak positioning method P-T algorithm is used [38].
According to the characteristics of ECG signals, the algo-
rithm uses the characteristics of large slope of QRS com-
plexes, and searches for the peak value of R waves in ECG
signals through differentiation and adaptive threshold
methods to achieve positioning effect. After the detection
of the R peak position is completed, the heart beat seg-
mentation of the entire ECG signal is performed, and the R
peak is taken as the reference position, and the forward and
backward intercepts are performed, respectively. The
intercepted length is at least a complete heartbeat, and a
complete heartbeat is about 0.6s  0.8s, so the sampling
point must be greater than 360*0.8 = 288. Too many
sampling points will cause correlation interference between
different types of waveforms. This paper selects 300
sampling points, divides all the ECG signals in MIT-BIH,
and normalizes each collected heartbeat, limiting the
amplitude of each heartbeat to 1; 1
½
.
4.1.3 Data enhancement
ECG classiﬁcation is an imbalance problem. Abnormal
heartbeats are much smaller than normal heartbeats.
Because deep learning has a strong ability to express and
explain, it is difﬁcult for the model to learn a small number
of sample features during the training process, which
makes the model invalid in practical applications. There-
fore, less data must be enhanced to solve the imbalance
problem. This paper adopts the Synthetic Minority Over-
sample Technique (SMOTE) oversampling algorithm pro-
posed by Chawla in 2002 [39]. The idea of this algorithm is
to synthesize new minority samples through a certain
strategy. The synthesis strategy is for each minority sample
a, A sample b is randomly selected from its nearest
neighbor, and then a point on the line between a and b is
randomly selected as the newly synthesized minority
sample. Therefore, this paper uses the algorithm to syn-
thesize new sample data from the similarities between the
existing minority heartbeat samples. The SMOTE algo-
rithm is shown in formula (3).
Xnew ¼ Xi þ randð0; 1Þ  ðXi;j  XjÞ
ð3Þ
where rand(0, 1) represents the random number generated
between 0 and 1, Xnew represents the newly generated
heartbeat samples of S, V, F and Q, Xi represents the ith
heartbeat sample in the minority class, Xi;j represents the
heartbeat sample b in the ith neighborhood of the ith
heartbeat sample a in the minority class.
4.2 Convolutional neural networks
CNN is a kind of multilayer neural network used for image
classiﬁcation, segmentation or detection that has developed
rapidly in recent years [40, 41]. It inherits the unique fea-
ture extraction capabilities of deep learning. Because of its
advantages such as local connection, weight sharing, and
down-sampling, it effectively reduces the number of
parameters in the neural network structure with a large
amount of data, reduces the complexity of the operation,
and reduces the memory of the operation. CNN is con-
structed by different combinations of input layer, hidden
layer and output layer. The hidden layer usually includes a
convolutional computing layer, a pooling computing layer,
and a fully connected layer. The network structure of 1D-
CNN is shown in Fig. 3.
The convolution operation layer is to perform convo-
lution operation on the input sequence or picture, and the
purpose is to extract the characteristics of the input signal.
Convolution operation includes convolution operation and
activation function. The convolution operation is to mul-
tiply a set of weights with the input, expressed in matrix
form as formula (4), where X represents the matrix
Neural Computing and Applications (2023) 35:13921–13934
13927
123
representation of the input signal and W is the convolution
kernel. The size and number of the weight matrix can be
customized by experience;  means convolution. After the
convolution operation, the result needs to be determined
and transformed by the activation function, and the output
that reaches the threshold is mapped to another space
through nonlinear changes, and then the features can be
nonlinearly classiﬁed.
SðtÞ ¼ ðX  WÞðtÞ
ð4Þ
The pooling operation layer is to down-sample the features
extracted by the convolutional layer. This process can
retain important pair of feature information without
changing the number of feature maps. In this way, the
model reduces spatial information to obtain better com-
puting
performance
and
reduces
the
risk
of
model
overﬁtting.
The ﬁnal classiﬁer in the entire CNN network is
implemented in the fully connected layer, and the output
after convolution and pooling is ﬂattened into a single
value vector, and the probability calculation is performed
through the softmax function to obtain the ﬁnal category
output. The softmax function converts the input into a
probability value ranging from 0 to 1, and the sum of all
probability values is 1.
This paper uses a network composed of multiple con-
volutional layers and pooling layers to extract features of
the ECG signal. Feature extraction technology can replace
manual labor, avoiding the inaccuracy of feature selection
due to human reasons and saving a lot of time spent on
features.
4.3 Long short-term memory networks
Long short-term memory network (LSTM) is a variant of
recurrent neural network. The traditional RNN hidden layer
is used as a memory unit. As the model progresses over
time, the effective information of the input data is gradu-
ally weakened. The LSTM redesigned the memory module
to retain the backpropagation error between the time step
and the level, so that the network model continues to
maintain the learning state in multiple time steps and thus
has the ability to capture the causality of long-distance
information. Therefore, LSTM is suitable for ECG signals
with timing characteristics.
The core of LSTM is composed of input gate, output
gate and forget gate. These three control gates can enable
LSTM neurons to read, write, reset and update long-dis-
tance historical information. The structure diagram is
shown in Fig. 4. The overall calculation formula is as
follows.
LSTM realizes the selective loss of information in
neurons through the forget gate in the structure, and the
calculation formula is as (5).
ft ¼ rðWxf xt þ Whf ht1 þ bf Þ
ð5Þ
where Wf represents the weight matrix of the forgetting
gate, ht1 is the previous output in the network, xt repre-
sents the current input, bf represents the bias term of the
forgetting gate, and rðÞ represents the sigmoid function.
The output of the input gate consists of two parts. The
calculation formulas are as (6) and (7). The output of the
input gate is composed of two parts, it represents the cur-
rent output, which is realized by the sigmoid function, and
~
Ct represents the current state, which is realized by the tanh
function.
it ¼ rðWxixt þ Whiht1 þ biÞ
ð6Þ
~
Ct ¼ tanh ðWxcxt þ Whcht1 þ bcÞ
ð7Þ
Among them, it represents the current output, realized by
the sigmoid function,
~
Ctt represents the current state,
realized by the tanh function, WðÞ represents the weight
matrix of this part, and b represents the bias term of this
part.
The output gate calculation formulas are as (8), (9) and
(10).
Fig. 3 1D-CNN Network Structure
Fig. 4 LSTM Network Structure
13928
Neural Computing and Applications (2023) 35:13921–13934
123
Ct ¼ ft  Ct1 þ it  ~
Ct
ð8Þ
ot ¼ rðWxoxt þ Whoht1 þ boÞ
ð9Þ
ht ¼ ot  tanh ðctÞ
ð10Þ
Ct is the output state of the cell, which is composed of the
product of the output ft of the forgetting gate and the state
Ct1 at the previous moment and the sum of the product of
the two outputs in the input gate. Wo represents the weight
matrix of the output gate, and bo represents the offset term
of the output gate, ht represents the current ﬁnal output.
In LSTM, the current state information ct and the pre-
vious state information ct1 have a linear relationship.
When the forget gate is open, that is, when the output of the
sigmoid unit is close to 1, the gradient will not disappear,
and the new state The information is the weighted average
of the previous state information and the accumulated
information at the current moment, so regardless of the
sequence length, as long as the forget gate is open, the
network can remember the past state information, that is,
LSTM can capture long-term dependencies.
4.4 ECG signal classification based
on CNN1LSTM
Both CNN and RNN can classify image and text input, so
there is an opportunity to combine the two network models
to improve classiﬁcation efﬁciency. If the input adds time
characteristics that CNN itself cannot handle, the combi-
nation of the two is more advantageous. Since the ECG
signal is a physiological signal collected in accordance
with time, it contains rich time domain features. The
positional relationship between the waveforms of various
stages in a heartbeat beat is close. The input of the neuron
in LSTM is not only affected by the input at the current
moment, but also related to the output at the previous
moment, that is, there is an association between nodes at
different moments in the time series, which can save
contextual information, and is dependent on long-distance
time. Time series are particularly effective. Therefore, this
paper uses the local perceptual ﬁeld characteristics of CNN
and the memory function of LSTM to construct a classi-
ﬁcation model combining CNN?LSTM. The speciﬁc
model structure is shown in Fig. 5. Among them, the size
of the convolution kernel is increased from 21 by 2, and the
sliding step size is 2 to extract the morphological charac-
teristics of the ECG signal. The number of convolution
kernels is 4, 16, 32, and 64. The convolutional layer per-
forms feature reorganization to form a feature map. The
pooling part selects a maximum pooling operation with a
size of 3 and a step size of 2 to compress the feature vector
size. The signal features extracted by the convolutional
network are sent to the 128-unit LSTM network for time
analysis. Finally, various predicted probabilities are output
through the fully connected layer and the softmax function.
Model training process
–
CNN extract features Input the preprocessed data into
the CNN network, and extract intermediate features
after convolution and pooling. The calculation formula
is as (11).
Xn
j ¼ Reluð
X
j2Mj
ðXn1
j
 Wn
j Þ þ bn
j Þ
ð11Þ
Among them, Xn
j represents the jth feature of the ECG
signal after the nth layer of convolution, W represents
the convolution kernel, and b represents the bias term.
–
LSTM extraction features take the middle feature Xn
j
of the ECG signal after feature extraction as the input of
the LSTM layer, and use the formula introduced in the
previous section to calculate the output.
–
Softmax classiﬁcation The signal features extracted by
CNN and LSTM are sent to the fully connected layer,
where 5 classiﬁcation labels are encoded with one-hot,
and then the softmax function is used to generate the
probability pk of each heartbeat type. The calculation
formula is as (12).
pk ¼ softmaxðxÞ
expðhT
k xÞ
P
k expðhT
k xÞ
ð12Þ
where x is the input sample data, k is the heartbeat type,
and h is the model parameter. In this paper, x represents
each heartbeat, k = 1,2,3,4,5, corresponding to cate-
gories N, S, V, F and Q, respectively.
–
Backpropagation and weight update: After the cat-
egory is judged, the ECG classiﬁcation loss is calcu-
lated, and the loss is back-propagated according to the
chain rule to calculate the gradient of each weight and
use gradient descent to update the weights. The
calculation formula is as (13).
hj ¼ hj  a 5JðhÞ
5hj
;
j ¼ 1; 2; . . .; k
ð13Þ
–
Iterative training Repeat the above steps until the
network converges or reaches the maximum number of
training cycles. If the effect of the model training result
is improved, save the model.
Model testing process
Load the optimal model. Load the optimal parameters of
the model training stage, input the ECG signal data of the
test set into the CNN?LSTM network for calculation, and
output the ﬁnal results and evaluation indicators.
Neural Computing and Applications (2023) 35:13921–13934
13929
123
4.5 Discussion and analysis
In the patient’s ECG signal, usually abnormal ECG signal
data are far less than normal ECG data. This unbalanced
data distribution often leads to models that are more
inclined to learn with multiple data categories, and the
learning of a few categories of data is insufﬁcient, espe-
cially in the process of small-batch gradient descent opti-
mization. If there are only a few abnormal ECG signal data
in the ECG data set, it makes the direction of gradient
descent heavily dependent on normal ECG data, resulting
in a low recognition rate of abnormal ECG signals. Usu-
ally, the solution to this problem is to perform multi-
sampling of categories with a small amount of data or
under-sampling categories with a large amount of data. In
recent years, many people have used the popular generative
confrontation network in deep learning to enhance data to
achieve data balance.
5 Experimental results and discussion
5.1 Dataset introduction
This paper uses the most widely used ECG signal database in
the ﬁeld of ECG signal classiﬁcation research, namely the
MIT-BIH arrhythmia database. The MIT-BIH ECG database
contains 48 half-hour records of the 24-hour dual-channel
ECG records of 47 subjects. Among them, 23 records of the
100 series are randomly selected from more than 4000 Holter
collectors, and the other 25 records of the 200 series are
unusual but clinically important arrhythmia signals. The
ECG signal data are stored in a binary format with a sampling
frequency of 360 Hz, and the atr ﬁle in each record indicates
the type of heartbeat. This paper is classiﬁed into 5 categories
according to AAMI standards, namely N (normal heart beat),
S (supraventricular odor), V (ventricular odor), F (fusion
heart beat), Q (undeﬁned heart beat). The number of various
heartbeats is shown in Table 1.
5.2 Experimental settings and evaluation
indicators
The hardware conﬁguration of the experimental platform is
Intel i7-6700 CPU, the graphics card is GTX1080Ti, the
memory is 32G, the operating system is Window10 system,
and the model is implemented based on the Python pro-
gramming language and Tensorﬂow framework. When
training the network model parameters, the initial value of
the learning rate is 0.001, and then the ReduceLROnPla-
teau function makes the learning rate adaptive to the
model; the Dropout parameter is 0.2, Batch_size is 128,
and Epoch is 1000. For S, V, F, and Q, ﬁrst select 20% as
the test set, and use the remaining 80% to generate new
data. In order to maintain the credibility of the experiment,
this paper adopts a tenfold cross-validation method.
In order to evaluate the performance of the model in this
paper, the following two indicators are used:
(1) In medical diagnosis, normal heartbeats and abnor-
mal heartbeats are negative and positive respectively. If the
true type is negative and classiﬁed as negative, it is
recorded as true negative (TN); the true type is positive and
classiﬁed as negative and recorded as false negative (FN);
the true type is classiﬁed as negative. Negatives are clas-
siﬁed as positives and counted as false positives; those that
are true to positives are classiﬁed as negatives and counted
as true positives (TP). In the ECG signal judgment model,
Accuracy (Acc), Speciﬁcity (Spe) and Sensitivity (Sen) are
generally used as indicators for judgment. Among them,
accuracy represents the probability that the model is
accurately classiﬁed for a given test set, and speciﬁcity
represents the probability that a negative example is
accurately predicted by the model, and sensitivity repre-
sents the probability that a positive example is correctly
Fig. 5 CNN-LSTM Network Structure
13930
Neural Computing and Applications (2023) 35:13921–13934
123
classiﬁed by the model. The three evaluation index for-
mulas are as (14), (15) and (16).
Sen ¼
TP
TP þ TN  100%
ð14Þ
Spe ¼
TN
TN þ FP  100%
ð15Þ
Acc ¼
TN þ TP
TP þ FP þ FN þ TN  100%
ð16Þ
(2) ROC curve: This indicator is based on the three models
of SVM, CNN, LSTM and the scheme of this article. After
training 4 models, they are used to predict the test set and
obtain the ROC curve. The area under the ROC curve can
be used to judge the performance of the model.
5.3 Result analysis
In (1), the probability of Acc, Spe and Sen in different
methods of cardiovascular disease are shown in Fig. 6. As
can be seen from the this ﬁgure, through the predictions of
different models on the test set, it is found that the clas-
siﬁcation effect of traditional machine learning SVM is less
than that of deep learning methods, and our proposed
method is higher than other deep learning methods in Acc,
Spe, and Sen. They reached 99.29%, 99.53%, and 97.77%,
respectively.
In (2), four trained models are used to predict the test
set, and the ROC curve is shown in Fig. 7. It can clearly
see that the ROC curves of these four models have a large
deviation from the 45-degree diagonal. Among them, the
area under the SVM model is small, and the trained SVM
model has a general classiﬁcation effect on the ECG
signals of coronary patients. This shows that traditional
machine learning has a large amount of data and has a poor
Table 1 Number of various
heartbeats
Heartbeat type
Description
Number
N (Normal heart beat)
Normal beat
83513
Left bundle branch block beat
Right bundle branch block beat
Atrial escape beats
Nodal(junctional) escape
S (Supraventricular odor)
Atrial premature
2184
Aberrant atrial premature
Nodal(junctional) premature
Supra-Ventricular premature
V (Ventricular odor)
Premature Ventricular
6975
contraction Ventricular escape
F (Fusion heart beat)
Fusion of Ventricular and Normal
801
Q (Undeﬁned heart beat)
Paced
3593
Fusion of Paced and Normal
Unclassiﬁable
Total
97066
Fig. 6 Acc, Spe and Sen for each model
Fig. 7 ROC curve comparison chart
Neural Computing and Applications (2023) 35:13921–13934
13931
123
feature extraction effect on data with more features, which
makes the generalization of the model poor; the area under
CNN and LSTM is not much different, and the ECG signal
classiﬁcation effect is good. It also shows that the deep
learning model can effectively extract the unilateral fea-
tures of the ECG data to a certain extent, making the model
generalization better; it can be seen that the area of our
proposed
method
is
the
largest,
showing
that
the
CNN?LSTM models can not only propose the character-
istics of the ECG signal itself, but also accurately propose
the time-domain features in it. Combining them for clas-
siﬁcation can effectively improve the classiﬁcation. The
effect makes the model strong in generalization.
6 Conclusion
Aiming at the accuracy and timeliness of heart rate
detection for COVID-19 patients, this paper proposes a
real-time cardiovascular monitoring system based on 5G
and deep learning to ensure low latency and high
throughput of ECG signal data transmission in wearable
devices. In order to achieve real-time ECG signal data
monitoring, analysis and diagnosis, it adopts the new
generation of wireless communication technology 5G, plus
the real-time data processing platform Flink framework,
and ﬁnally uses the classiﬁcation of the proposed deep
learning ECG signal model to realize real-time ECG signal
data monitoring, analysis and diagnosis. The correct rate of
model prediction can reach 99.29%, which proves that the
real-time medical monitoring system is necessary and can
effectively predict the actual situation of the COVID-19
patient’s cardiovascular system. It has high practical value
and can realize early warning of emergencies in time.
Our next plan is to further optimize the deep learning
model, try to increase the one-dimensional time series
signal to the two-dimensional space, use the Generative
Adversarial Networks to enhance the data of the insufﬁ-
cient number of heartbeat types, solve the problem of ECG
signal imbalance, and consider using migration learn to
increase model accuracy and reduce training time.
Acknowledgements This work was supported in part by the National
Natural Science Foundation of China under Grant No. 61373162, in
part by the Sichuan Provincial Science and Technology Department
Project under Grant No. 2019YFG0183, in part by the Sichuan
Provincial Key Laboratory Project under Grant No. KJ201402, and in
part by the Japan Society for the Promotion of Science (JSPS) Grants-
in-Aid
for
Scientiﬁc
Research
(KAKENHI)
under
Grant
JP18K18044 and JP21K17736.
Declarations
Conflicts of interest The authors declare that there is no conflict of
interest regarding the publication of this article.
References
1. Zhou X, Hu Y, Liang W, Ma J, Jin Q (2020) Variational lstm
enhanced anomaly detection for industrial big data. IEEE Trans
Ind Inform 17(5):3469-3477. https://doi.org/10.1109/TII.2020.
3022432
2. Yu K, Tan L, Shang X, Huang J, Srivastava G, Chatterjee P
(2021) Efﬁcient and privacy-preserving medical research support
platform against covid-19: a blockchain-based approach. IEEE
Consumer Electron Mag 10(2):111–120
3. Yu K, Tan L, Aloqaily M, Yang H, Jararweh Y (2021) Block-
chain-enhanced data sharing with traceable and direct revocation
in iiot. IEEE Trans Ind Inf. https://doi.org/10.1109/TII.2021.
3049141
4. Yu K, Guo Z, Shen Y, Wang W, Lin JC-W, Sato T (2021) Secure
artiﬁcial intelligence of things for implicit group recommenda-
tions. IEEE Internet Things J. https://doi.org/10.1109/JIOT.2021.
3079574
5. Manogaran G, Lopez D (2018) Health data analytics using
scalable logistic regression with stochastic gradient descent. Int J
Adv Intell Paradigms 10(1–2):118–132
6. Zhou X, Liang W, Wang KI, Wang H, Yang LT, Jin Q (2020)
Deep-learning-enhanced human activity recognition for internet
of healthcare things. IEEE Int Things J 7(7):6429–6438
7. Wang J, Huang X, Tang S, Shi GM, Ma X, Guo J (2019) Blood
triglyceride monitoring with smartphone as electrochemical
analyzer for cardiovascular disease prevention. IEEE J Biomed
Health Inf 23(1):66–71
8. Li MH, Yadollahi A, Taati B (2017) Noncontact vision-based
cardiopulmonary monitoring in different sleeping positions. IEEE
J Biomed Health Inf 21(5):1367–1375
9. Saadatnejad S, Oveisi M, Hashemi M (2020) Lstm-based ecg
classiﬁcation for continuous monitoring on personal wearable
devices. IEEE J Biomed Health Inf 24(2):515–523
10. Hu H, Wen Y, Chua T-S, Li X (2014) Toward scalable systems
for big data analytics: a technology tutorial. IEEE Access
2:652–687
11. Zhang X, Yang L, Ding Z, Song J, Zhai Y, Zhang D (2021)
Sparse vector coding-based multi-carrier noma for in-home
health networks. IEEE J Select Areas Commun 39(2):325–337.
https://doi.org/10.1109/JSAC.2020.3020679
12. Feng C, Yu K, Bashir AK, Al-Otaibi YD, Lu Y, Chen S, Zhang D
(2021) Efﬁcient and secure data sharing for 5g ﬂying drones: a
blockchain-enabled approach. IEEE Netw 35(1):130–137
13. Li S, DaXu L, Zhao S (2015) The internet of things: a survey. Inf
Syst Front 17(2):243–259
14. Yu K, Tan L, Lin L, Chen X, Yi Z, Sato T (2021) Deep learning
empowered breast cancer auxiliary diagnosis for 5gb remote
e-health. IEEE Wireless Commun. https://doi.org/10.1109/MWC.
001.2000374
15. Sannino G, DePietro G (2018) A deep learning approach for ecg-
based heartbeat classiﬁcation for arrhythmia detection. Future
Generat Comput Syst 86:446–455
16. Ed-daoudy A, Maalmi K (2019) A new internet of things archi-
tecture for real-time prediction of various diseases using machine
learning on big data environment. J Big Data 6(1): 104
17. Sun J, Reddy C.K. (2013) ‘‘Big data analytics for healthcare,’’ In:
Proceedings of the 19th ACM SIGKDD international conference
on Knowledge discovery and data mining, pp. 1525–1525
18. Nair LR, Shetty SD, Shetty SD (2018) Applying spark based
machine learning model on streaming big data for health status
prediction. Comput Electr Eng 65:393–399
19. Alotaibi S, Mehmood R, Katib I, Rana O, Albeshri A (2020)
Sehaa: a big data analytics tool for healthcare symptoms and
13932
Neural Computing and Applications (2023) 35:13921–13934
123
diseases detection using twitter, apache spark, and machine
learning. Appl Sci 10(4):1398
20. Ed-Daoudy A, Maalmi K (2019) ‘‘Real-time machine learning for
early detection of heart disease using big data approach. In: 2019
International Conference on Wireless Technologies, Embedded
and Intelligent Systems (WITS). IEEE, pp. 1–5
21. Manogaran G, Varatharajan R, Priyan M (2018) Hybrid recom-
mendation system for heart disease diagnosis based on multiple
kernel learning with adaptive neuro-fuzzy inference system.
Multimed Tools Appl 77(4):4379–4399
22. Melin P, Miramontes I, Prado-Arechiga G (2018) A hybrid model
based on modular neural networks and fuzzy systems for classi-
ﬁcation of blood pressure and hypertension risk diagnosis. Expert
Syst Appl 107:146–164
23. Al-Makhadmeh Z, Tolba A (2019) Utilizing iot wearable medical
device for heart disease prediction using higher order boltzmann
model: a classiﬁcation approach. Measurement 147:106815
24. Yin H, Jha NK (2017) A health decision support system for
disease diagnosis based on wearable medical sensors and
machine learning ensembles. IEEE Trans Multi-Scale Comput
Syst 3(4):228–241
25. Jabeen F, Maqsood M, Ghazanfar MA, Aadil F, Khan S, Khan
MF, Mehmood I (2019) An iot based efﬁcient hybrid recom-
mender system for cardiovascular disease. Peer-to-Peer Netw
Appl 12(5):1263–1276
26. Kumar PM, Lokesh S, Varatharajan R, Babu GC, Parthasarathy P
(2018) Cloud and iot based disease prediction and diagnosis
system for healthcare using fuzzy neural classiﬁer. Future Gen-
erat Comput Syst 86:527–534
27. Kumar PM, Gandhi UD (2018) A novel three-tier internet of
things architecture with machine learning algorithm for early
detection of heart diseases. Comput Electr Eng 65:222–235
28. Hammad M, Alkinani MH, Gupta BB et al (2021) Myocardial
infarction detection based on deep neural network on imbalanced
data. Multimed Syst. https://doi.org/10.1007/s00530-020-00728-
8
29. Sedik A, Hammad M, Abd El-Samie FE et al (2021) Efﬁcient
deep learning approach for augmented detection of Coronavirus
disease. Neural Comput Appl. https://doi.org/10.1007/s00521-
020-05410-8
30. AlZu’bi S, Shehab M, Al-Ayyoub M, Jararweh Y, Gupta B
(2020) Parallel implementation for 3d medical volume fuzzy
segmentation. Patt Recognit Lett 130:312–318
31. Abd El-Latif A.A, Abd-El-Atty B, Hossain M.S, Rahman M.A,
Alamri A, Gupta B.B. (2018) Efﬁcient quantum information
hiding for remote medical image sharing. IEEE Access 6:
21 075–21 083
32. Zhen L, Bashir AK, Yu K, Al-Otaibi YD, Foh CH, Xiao P (2021)
Energy-efﬁcient random access for leo satellite-assisted 6g
internet of remote things. IEEE Int Things J 8(7):5114–5128
33. Tan L, Xiao H, Yu K, Aloqaily M, Jararweh Y (2021) A
blockchain-empowered crowdsourcing system for 5g-enabled
smart cities. Comput Standards Interfaces 76:103517
34. Zhang J, Yu K, Wen Z, Qi X, Paul AK (2021) 3d reconstruction
for motion blurred images using deep learning-based intelligent
systems. Comput Mater Continua 66(2):2087–2104
35. Yu K, Lin L, Alazab M, Tan L, Gu B (2020) Deep learning-based
trafﬁc safety solution for a mixture of autonomous and manual
vehicles in a 5g-enabled intelligent transportation system. IEEE
Trans Intell Transp Syst. https://doi.org/10.1109/TITS.2020.
3042504
36. Gong Y, Zhang L, Liu RP, Yu K, Srivastava G (2021) Nonlinear
mimo for industrial internet of things in cyber-physical systems.
IEEE Trans Ind Inf 17(8):5533–5541. https://doi.org/10.1109/TII.
2020.3024631
37. Greco L, Ritrovato P, Xhafa F (2019) An edge-stream computing
infrastructure for real-time analysis of wearable sensors data.
Future Generat Comput Syst 93:515–528
38. Pan J, Tompkins WJ (1985) A real-time qrs detection algorithm.
IEEE Trans biomed Eng 3:230–236
39. Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP (2002)
Smote: synthetic minority over-sampling technique. J Artif Intell
Res 16:321–357
40. Guo Z, Yu K, Jolfaei A, Bashir AK, Almagrabi AO, Kumar N
(2021) A fuzzy detection system for rumors through explainable
adaptive learning. IEEE Trans Fuzzy Syst. https://doi.org/10.
1109/TFUZZ.2021.3052109
41. Guo Z, Shen Y, Bashir AK, Imran M, Kumar N, Zhang D, Yu K
(2021) Robust spammer detection using collaborative neural
network in internet of thing applications. IEEE Int Things J
8(12):9549–9558. https://doi.org/10.1109/JIOT.2020.3003802
Publisher’s Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Authors and Afﬁliations
Liang Tan1,2 • Keping Yu3
• Ali Kashif Bashir4,5 • Xiaofan Cheng1 • Fangpeng Ming1 • Liang Zhao6 •
Xiaokang Zhou7
& Keping Yu
keping.yu@aoni.waseda.jp
Liang Tan
jkxy_tl@sicnu.edu.cn
Ali Kashif Bashir
Dr.alikashif.b@ieee.org
Xiaofan Cheng
978029808@qq.com
Fangpeng Ming
11451287@qq.com
Liang Zhao
lzhao@sau.edu.cn
Xiaokang Zhou
zhou@biwako.shiga-u.ac.jp
1
College of Computer Science, Sichuan Normal University,
Chengdu 610101, China
2
China and Institute of Computing Technology, Chinese
Academy of Sciences, Beijing 100190, China
3
Global Information and Telecommunication Institute,
Waseda University, Tokyo, Japan
Neural Computing and Applications (2023) 35:13921–13934
13933
123
4
Department of Computing and Mathematics, Manchester
Metropolitan University, Manchester, UK
5
School of Information and Communication Engineering,
University of Electronics Science and Technology of China
(UESTC), Chengdu, China
6
School of Computer Science, Shenyang Aerospace
University, Shenyang 110136, China
7
Faculty of Data Science, Shiga University, Hikone, and
RIKEN Center for Advanced Intelligence Project, Tokyo,
Japan
13934
Neural Computing and Applications (2023) 35:13921–13934
123


Paper 4:
- APA Citation: Nixon, C., Sedky, M., & Hassan, M. (2020). SALAD: An Exploration of Split Active Learning based Unsupervised Network Data Stream Anomaly Detection using Autoencoders. Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing, 58-62.
  Main Objective: To evaluate the effectiveness of online learning techniques for continuous model updates and anomaly detection in real-time network data streams.
  Study Location: Unspecified
  Data Sources: NSL-KDD, KDD Cup 1999, UNSW-NB15
  Technologies Used: Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests, Autoencoders
  Key Findings: 1. The proposed Split Active Learning Anomaly Detector (SALAD) method significantly outperforms traditional online learning methods in terms of anomaly detection performance.
2. The Adaptive Anomaly Threshold method combined with a split active learning strategy enables superior detection with only 20% of the labeling cost.
3. The autoencoder anomaly detector method exhibits significantly lower processing times compared to traditional online learning techniques, making it suitable for high-speed network data streams.
  Extract 1: "The proposed method was evaluated with the NSL-KDD, KDD Cup 1999, and UNSW-NB15 data sets, using the scikit-multiﬂow framework."
  Extract 2: "Results demonstrated that a novel Adaptive Anomaly Threshold method, combined with a split active learning strategy offered superior anomaly detection performance with a labeling budget of just 20%, signiﬁcantly reducing the required human expertise to annotate the network data."
  Limitations: The paper does not mention any specific limitations or drawbacks of the proposed method, such as potential computational complexity or the handling of concept drift in complex or non-stationary data streams.
  Relevance Evaluation: Highly relevant - The paper directly addresses the point in the outline regarding the application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data.
  Relevance Score: 1.0
  Inline Citation: (Nixon, Sedky, and Hassan, 2020)
  Explanation: The paper examines the application and effectiveness of online learning techniques, specifically Stochastic gradient descent (SGD), Passive-aggressive algorithms, and Online random forests, for continuously updating and improving machine learning models based on incoming real-time data. It evaluates the proposed Split Active Learning Anomaly Detector (SALAD) method, which leverages these techniques to enhance anomaly detection in real-time network data streams.

 Full Text: >
Posted on 17 Jul 2020 — CC-BY 4.0 — https://doi.org/10.36227/techrxiv.14896773.v1 — e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed. They should not b...
SALAD: An Exploration of Split Active Learning based
Unsupervised Network Data Stream Anomaly Detection using
Autoencoders
Christopher Nixon 1, Mohamed Sedky 2, and Mohamed Hassan 2
1Staﬀordshire University
2Aﬃliation not available
October 30, 2023
Abstract
Machine learning based intrusion detection systems monitor network data streams for cyber attacks. Challenges in this space
include detection of unknown attacks, adaptation to changes in the data stream such as changes in underlying behaviour, the
human cost of labeling data to retrain the machine learning model and the processing and memory constraints of a real-time
data stream. Failure to manage the aforementioned factors could result in missed attacks, degraded detection performance,
unnecessary expense or delayed detection times. This research evaluated autoencoders, a type of feed-forward neural network, as
online anomaly detectors for network data streams. The autoencoder method was combined with an active learning strategy to
further reduce labeling cost and speed up training and adaptation times, resulting in a proposed Split Active Learning Anomaly
Detector (SALAD) method.
The proposed method was evaluated with the NSL-KDD, KDD Cup 1999, and UNSW-NB15
data sets, using the scikit-multiﬂow framework.
Results demonstrated that a novel Adaptive Anomaly Threshold method,
combined with a split active learning strategy oﬀered superior anomaly detection performance with a labeling budget of just
20%, signiﬁcantly reducing the required human expertise to annotate the network data. Processing times of the autoencoder
anomaly detector method were demonstrated to be signiﬁcantly lower than traditional online learning methods, allowing for
greatly improved responsiveness to attacks occurring in real time. Future research areas are applying unsupervised threshold
methods, multi-label classiﬁcation, sample annotation, and hybrid intrusion detection.
1
1
SALAD: An Exploration of Split Active Learning
based Unsupervised Network Data Stream
Anomaly Detection using Autoencoders
Christopher Nixon, Mohamed Sedky, and Mohamed Hassan
Abstract—Machine learning based intrusion detection systems monitor network data streams for cyber attacks. Challenges in this
space include detection of unknown attacks, adaptation to changes in the data stream such as changes in underlying behaviour, the
human cost of labeling data to retrain the machine learning model and the processing and memory constraints of a real-time data
stream. Failure to manage the aforementioned factors could result in missed attacks, degraded detection performance, unnecessary
expense or delayed detection times. This research evaluated autoencoders, a type of feed-forward neural network, as online anomaly
detectors for network data streams. The autoencoder method was combined with an active learning strategy to further reduce labeling
cost and speed up training and adaptation times, resulting in a proposed Split Active Learning Anomaly Detector (SALAD) method. The
proposed method was evaluated with the NSL-KDD, KDD Cup 1999, and UNSW-NB15 data sets, using the scikit-multiﬂow framework.
Results demonstrated that a novel Adaptive Anomaly Threshold method, combined with a split active learning strategy offered superior
anomaly detection performance with a labeling budget of just 20%, signiﬁcantly reducing the required human expertise to annotate the
network data. Processing times of the autoencoder anomaly detector method were demonstrated to be signiﬁcantly lower than
traditional online learning methods, allowing for greatly improved responsiveness to attacks occurring in real time. Future research
areas are applying unsupervised threshold methods, multi-label classiﬁcation, sample annotation, and hybrid intrusion detection.
Index Terms—Active Learning, Online Learning, Autoencoders, Anomaly Detection, Intrusion Detection System.
!
1
INTRODUCTION
Intrusion Detection Systems (IDS) monitor a computer net-
work for cyber attacks. Traditional intrusion detection tech-
niques rely on human subject matter experts to carefully
produce signatures that can accurately detect a cyber attack
at the network layer. For over a decade research has focused
on improving IDS with machine learning (ML) methods in
order to reduce the overall demand for human effort [1].
The majority of this research has centred around misuse
detection whereby the ML based IDS is trained using a data
set in which all cyber attacks are labeled, the drawback of
this being that only the labeled attacks will be known to the
model, missing unknown or new attacks, and that labeling
of the initial data set is a time consuming and complex task
prone to human error. An alternative to misuse detection is
to use an anomaly detector whereby only the ‘normal’ net-
work data is learned and any signiﬁcant deviations treated
as an anomaly meaning that new attacks will be detected,
a challenge with this approach is the potential for false
positives.
IDS capture network packet data directly from the net-
work, requiring efﬁcient real-time processing of each new
packet as part of a continuous data stream. This network
data stream is non-stationary and can change over time,
a characteristic known as concept drift, which requires the
ML model to adapt in order that detection performance is
not degraded [2]. Adaptation requires detecting a change
in the posterior probability of a class label, necessitating
the ground truth to be known. Active learning (AL) is an
attempt to lower the labeling cost, and speed up the adap-
tion times, of change detection by employing uncertainty or
random strategies according to a labeling budget [3]
An hypothesis that this research aims to test is that
anomaly detectors monitoring non-stationary network data
streams will experience increased false positives over time,
which can be corrected by applying adaptation techniques
to update the anomaly detector. This will be expanded
by a further hypothesis that active learning strategies can
provide good adaptation with minimal labeling cost, and
reduced learning times, for anomaly detection.
Unsupervised learning allows for a model to be trained
without all the class labels being known, typically achieved
by learning a representation of the underlying data struc-
ture. Common unsupervised techniques, such as cluster-
ing, are impeded by high degrees of time complexity and
memory usage [4]. Models based on neural networking
are gaining increased attention in the IDS ﬁeld and a type
of feed-forward neural network, the autoencoder, is able
to learn the representation of data without class labels by
encoding a latent representation of the data, which can be
utilised for anomaly detection by calculating the error of
the decoded output from the original, and comparing to a
predetermined anomaly threshold [5]. This research aims to
test the hypothesis that autoencoders provide an effective
online anomaly detector for network data streams when
combined with active learning methods.
The remainder of this paper is organised as follows:
Section 2, introduces related work; Section 3, describes the
proposed Split Active Learning Anomaly Detector (SALAD)
2
method; Section 4, presents the evaluation results; Section
5, discusses how SALAD provides a low cost anomaly
detector for network data streams; and Section 6, presents
conclusions.
2
RELATED WORK
2.1
Neural Networking Anomaly Detection
Intrusion detection systems can be either anomaly based or
misuse based, where the former learns the normal behaviour
and detects deviations, allowing for detection of previously
unseen, unknown attacks, and the latter learns known attack
signatures resulting in high levels of detection accuracy
[6]. A challenge with network data streams is that they
generate large volumes of data that become increasingly
expensive for a human expert to analyse and correctly
label. Anomaly detectors are beneﬁcial because they only
need to learn the representation of a single ‘normal’ class
from which anomalies can be distinguished meaning that
new, previously unseen, attacks can be detected without
requiring new data labels and re-training of the model [6].
Unsupervised machine learning methods are well suited
to the anomaly detection task as they can learn the repre-
sentation of the underlying data to determine normal and
anomaly classes [6], as well as learning useful features that
better separate the classes. Buczak and Guven [1] have
provided a comprehensive survey of IDS machine learn-
ing techniques, including anomaly detection, in most cases
misuse and anomaly detection are combined into a hybrid
system. This review brieﬂy introduces recent studies within
the unsupervised anomaly detection space, adopting neural
networking methods familiar to the visual processing area,
for comparison to the proposed approach.
Alrawashdeh and Purdy [7] evaluated Restricted Boltz-
mann Machines (RBM) arranged into a deep belief net-
work combined with a logistic regression classiﬁer trained
using back propagation. Although the study claims to be
‘anomaly’ based the model is actually trained to identify
known classes so would be more ‘misuse’ based in its
approach. The accuracy of their model, with the 10% KDD
Cup 1999 data set, is 97.91% [7]. The authors further build on
there work by replacing the RBM activation function with a
novel ‘Adaptive Linear Function’ (ALF) for intrusion detec-
tion with the aim of improving accuracy and convergence
time [8]. Evaluated with KDD Cup 1999 and NSL-KDD data
sets, the accuracy was 98.59% and 96.2% respectively [8].
Roshan et al. [9] proposed a novel intrusion detection
approach using a Clustering Extreme Learning Machine
(CLUS-ELM) method. This method allows for both unsuper-
vised and supervised updates to the model, using a decision
maker element to perform informed change detection based
on the cluster output, in this design unsupervised refers
to guessing the correct cluster for a given data sample as
opposed to being told the label by a ‘human expert’. The
mean square error calculation used by the decision maker
will still require the ground truth to be known. Results were
evaluated using the NSL-KDD data set, with a detection
rate for known attacks of 84% and 81% for unsupervised
and supervised modes, 77% and 84% for unknown attacks,
where the false positive rate was less than 3% [9]. The
author remarks that the better unsupervised detection rates
for known attacks compared to the supervised ones are
unexpected and could be due to inaccuracies in the NSL-
KDD data set [9].
Chen, Cao and Mai [10] proposed an ofﬂine anomaly
detection method whereby Convolutional Neural Networks
(CNN) are used to extract features which are then con-
densed into a spherical hyperplane by a deep Support Vec-
tor Data Description (deep-SVDD) technique. The method
is trained on normal samples only so that such normal
samples concentrate around the center of the sphere and
attack samples concentrate on the outside as outliers allow-
ing them to be detected as a one-class anomaly detector.
Their method was evaluated with the KDD Cup 1999 data
set, achieving an accuracy of 96% when all attack types are
present.
Hassan et al. [11] proposed a combined CNN for feature
reduction and Weight Dropped, Long Short Term Mem-
ory (WDLSTM) network for representation of dependencies
among features, using the connection drop out regularisa-
tion method. The proposed supervised learning network
was evaluated with the UNSW-NB15 data set, returning an
F1-Score of 0.88 for abnormal samples and overall accuracy
of 97.17% via ofﬂine holdout training.
The reviewed studies all demonstrate different network
topologies for cyber intrusion detection, all of which have
elements of supervised learning and traditional ofﬂine batch
training. They do not address the problem of a truly unsu-
pervised anomaly detector for online data streams as will be
explored in this paper.
2.2
Autoencoder Anomaly Detection
An autoencoder is a type of feed-forward neural network
that uses an encoding function to produce a latent code
representation of the input data, and a decoding function to
reconstruct the input from the code representation [12]. The
mean square error between the reconstructed output and
original input can be calculated using equation 1, where f
is the encoding function and g is the decoding function [12],
which can then be compared to an anomaly threshold to
label a sample as either normal or anomalous.
ˆX = g(f(X))
RE = 1
n
n
X
j=1
(Xj − ˆXj)2
(1)
In our previous work [12], we reviewed autoencoder
based anomaly intrusion detection methods, whereby single
layer denoising models [13], Long Short Term Memory
(LSTM), Recurrent Neural Network [14], [15], ensembled
stacked autoencoders [16], [17], and sparsely connected
networks [18], [15] were demonstrated across a range of
IDS data sets. Vaiyapuri and Binbusayyis [19] evaluated a
number of autoencoder network architectures for anomaly
detection, ﬁnding the use of a contractive penalty to regulate
the network provided the best performance when evaluated
ofﬂine using the NSL-KDD and UNSW-NB15 data sets.
A number of methods were proposed in the literature
to determine the anomaly threshold, an important param-
eter in deciding whether to label a sample as a positive
3
detection. The threshold can be set to the average RE value
observed during training [19]. Na¨ıve Anomaly Threshold
(NAT) sets the threshold at the maximum observed RE
during training [16]. Stochastic Anomaly Threshold (SAT)
[13] sets the threshold based on the best observed accuracy
when stepping through threshold values between the mean
and 3 * standard deviation of the normal sample distribu-
tion. Nicolau and McDermott [13] proposed an anomaly
threshold method using Kernel Density Estimation.
Aiming to ﬁnd an optimal network conﬁguration, we
evaluated in [12], an undercomplete autoencoder, regulated
with connection dropout, with a prequential online test
using the KDD Cup 1999 and UNSW-NB15 data sets. Ap-
plying a single layer autoencoder with dropout probability
of 0.1, using the Stochastic Anomaly Threshold method,
provided an accuracy of 98% and F1-score of 0.812, using
the KDD Cup 1999 data set, with a signiﬁcantly improved
running time compared to traditional Na¨ıve Bayes (NB) and
Hoeffding Adaptive Tree (HAT) online methods. Evaluation
on the UNSW-NB15 data set using a 3-layer network and
dropout probability of 0.2 returned an accuracy of 79.1% and
F1-score of 0.703. The results showed that the SAT threshold
performed better than the NAT, and that more complex data
sets beneﬁt from experimenting with the number of layers
and regularisation of the network.
2.3
Concept Drift Detection with Active Learning
Non-stationary network data streams may experience real
concept drift [2], whereby the posterior probability of classes
will change over time due to changes in network behaviors,
the cause of which could be either benign or adversarial in
nature. The posterior probability is deﬁned as p(y|X) which
represents the probability of class y given an observation
X [2]. Autoencoders determine outliers using the RE-score,
based on the hypothesis that adversarial behaviour deviates
from the learned ‘normal’ representation resulting in scores
above the anomaly threshold. Real concept drift presents a
challenge that the aforementioned hypothesis will weaken
overtime, with changing benign data also scoring above
threshold, raising the false positive rate. Increasing the
anomaly threshold does not present an optimal solution
as although the false positive rate may lower, the false
negative rate could increase and so is not recommended.
The hypothesis of this research was that a change in under-
lying ‘benign’ network behaviour will result in a raised false
positive rate and that learning the representation of the new
behaviour will remedy this effect. Note that the change in
benign activity could be from an unplanned change such as
a network fault, in which case the usefulness of the anomaly
detector is extended to a fault detector, however for the
purposes of this research this will not be considered further.
Change detection is a set of methods that proactively
monitor the data stream for concept drift [2]. Traditional
methods such as adaptive windowing and statistical process
control (SPC) [2], rely on fully supervised labels and are
therefore not well suited to applications where data label-
ing is expensive, such as network data streams. Moreover
unsupervised techniques that rely solely on monitoring a
change compared to a reference distribution will not always
detect real concept drift [20]. Sethi and Kantardzic [21]
proposed a semi-supervised Margin Density Drift Detector
(MD3) to reduce labeling costs through an active learning
approach. First, using an unsupervised method, samples
that fall below an uncertainty threshold are added to the
margin. Density of the margin is compared to a training
reference distribution to detect drift before conﬁrming by
testing accuracy with data labels, sensitivity can be adjusted
through a varying factor of the reference distribution’s stan-
dard deviation. A fading factor is utilised to give greater
importance to more recent samples within a moving average
of margin density [21]. MD3 can work with ensembles,
calculating if a sample should be included within the margin
by comparing the distance between the mean predicted class
probabilities to the margin threshold (θ), given by equation
2. A possible beneﬁt of this approach would be that the
change in density of uncertain samples that are borderline
outliers could indicate a concept drift that requires further
analysis, prompting further action such as re-training. As
the anomaly detector only requires labeled normal data to
re-train, this would be a cheaper approach to other methods
that require fully labeled data. A possible drawback is that
the frequency of drifts could demand increased human
expertise. Evaluation with the NSL-KDD data set reported
an accuracy of 89.4 and 89.9 % using the SVM and random
subspace ensemble methods, respectively where the ﬁrst
15% of the data stream is used as a training set. The total
labeling cost was 7.9%.
(p(ˆyc1|X) − p(ˆyc2|X)) ≤ θ
(2)
Shan et al. [22] also proposed an AL change detection
strategy based on margin uncertainty, ‘OALEnsemble’, how-
ever in this approach the ensemble members are trained on
different windows of the data set, with a stable classiﬁer
and a series of short window ‘dynamic’ classiﬁers that are
continually replaced as new blocks of the data stream are
processed, to balance the detection of both sudden and grad-
ual concept drifts. Similar to [21], labeling is restricted to
samples within the uncertainty margin, with the addition of
a random labeling algorithm to randomly include samples
outside of the margin where drift may also be occurring [22].
The stable classiﬁer is incrementally trained with all new
data, whilst dynamic classiﬁers are only trained on the most
recent block and given a weight, providing importance to
more recent data [22]. The incremental update of the stable
classiﬁer is restricted to models that feature local replacement
such as very fast decision trees (VFDT) [2], and so would
not be appropriate for autoencoder methods. The labeling
rate is constrained by pro-actively adjusting the sensitivtiy
threshold in order to manage the cost of the algorithm
during periods of high uncertainty. Random sampling is
desirable as it enables the classiﬁer to be trained from the
whole distribution, reducing bias [3]. The idea of gradu-
ally retraining the autoencoders with new ‘normal’ data
in response to concept drift, whilst retaining the previous
models for a period of time, moderating their importance
with a weight scheme, could allow for the detection of
both gradual and sudden changes in benign behaviour,
however the problem of global replacement must be carefully
considered as training on small data sets could degrade the
autoencoders ability to represent normal data.
4
Dang [23] evaluated AL for IDS, using a novel strategy
with the Na¨ıve Bayes classiﬁer, selecting instances with the
greatest distance from the population distribution of proba-
bilities under the hypothesis that a bigger change of P(A|B)
reﬂects a rare event that should be learned. The method
was evaluated with the CICIDS 2012 data set, achieving an
AUC-score of 90% compared to 85% with the uncertainty
strategy with 10% of labeled data, and performance decreas-
ing beyond this. The author argues that this indicates that
good quality data is more important over larger volumes of
data [23]. It may also be true that the method reduces class
imbalance by proactively sampling examples with weaker
performance that could reﬂect minority classes.
Zhang et al. [24] evaluted an Open-CNN method trained
by AL labeling the ‘unknown’ detected attacks. Accuracy
with the CTU data set was near equivalent to 100% label cost
at just 1% of labeled attacks using an uncertainty strategy,
demonstrating that only a low label cost is necessary to train
the ML model.
ˇZliobait˙e et al. [3] discussed three requirements for AL
strategies: 1) balancing the labeling budget over time, 2)
detect changes anywhere within the problem space and 3)
preserve the distribution for unbiased change detection. A
number of strategies were evaluated against these require-
ments, including ﬁxed uncertainty as demonstrated by [21],
and uncertainty with randomisation, whereby the sensitivity
threshold is randomly selected from a standard distribution
to occasionally include samples outside of the uncertainty
margin. Fixed uncertainty is only able to satisfy requirement
one, and randomised uncertainty satisﬁes requirement one
and two, but neither can preserve the probability density of
labeled data compared to the original distribution, which
can bias the model [3]. A further split strategy is intro-
duced which satisﬁes all three requirements by splitting
the the data stream into two, using uncertainty and ran-
dom strategy exclusively on either stream. Both streams
are used for training, but only the randomised stream is
used for change detection [3]. Shan et al. [22] presents a
split strategy, although in this approach adaptation is blind,
based on incrementally updating the ensemble members
with both uncertainty and random labels, offering no pro-
active change detection, this could reduce overall adaptation
speeds [2].
An objective of this research was to satisfy all three
AL requirements outlined by ˇZliobait˙e et al. [3]. MD3 [21]
will be biased towards uncertain samples and will miss
change occurring outside of the margin which will affect
overall detection performance. The work of Shan et al. [22]
could be further improved by introducing pro-active change
detection method to the randomly labeled data as suggested
by ˇZliobait˙e et al. [3] in order to increase adaptation time. In
this research random, uncertainty, variable uncertainty, split
and blind strategies are compared. The proposed hypothesis
is that only the split strategy with informed change detec-
tion approach will be able to satisfy all three requirements
and that the change detection approach will offer faster
adaptation times to a blind approach. The informed ap-
proach can use a well known change detector such as Drift
Detection Method (DDM) [25] to monitor the classiﬁcation
error of the anomaly detector.
3
METHODS
The aim of this research was to explore that autoenoders
can provide a low cost online anomaly detection solution
when combined with AL methods. In our previous work
[12] we evaluated dropout probability, NAT with decay and
SAT anomaly thresholds, and single vs stacked network
structure, to ﬁnd optimal autoencoder parameters. Build-
ing on this work, in this paper, we further introduced a
novel Adaptive Anomaly Threshold (AAT) method and also
evaluated an AL based Active Stream Framework (ASF)
[3] with which we compared blind, random, uncertainty,
variable uncertainty and split AL strategies. The uncertainty
strategy was adapted for use with autoencoders using a
novel distance from RE method. All methods were evalu-
ated using a prequential, interleaved test-then-train method
[2], whereby the model is ﬁrst tested on a previously unseen
sample before training in a chunk wise fashion [12], after an
initial period of pre-training. Results were compared against
traditional Na¨ıve Bayes (NB) and Hoeffding Adaptive Tree
(HAT) online learning methods using the KDD Cup 19991
10% [26] and UNSW-NB152 [27] data sets.
The Keras3 neural networking [28], version 2.3.1, and
Scikit-Multiﬂow4 stream learning [29], version 0.4.1, frame-
works for Python were used for this evaluation. Evaluations
were ran on a Windows 10 64bit PC with Intel i7 1.8GHz
processor and 8GB RAM.
Observed metrics during evaluation included: accuracy,
F1-score, kappa and total running time. For prequential
evaluation the scikit-multiﬂow default of updating evalu-
ation metrics every 200 samples was used.
3.1
Adaptive Anomaly Threshold
From evaluating the make up of the data stream and per-
formance achieved with both the NAT and SAT threshold
methods [12] a proposed hypothesis was that chunks of
the data stream that contained only normal samples beneﬁt
from a na¨ıve approach whereby the maximum RE is used,
therefore all samples will fall below this value, giving an
accuracy of 100%. For anomaly samples the second hypoth-
esis was that between the maximum value and the mean
observed RE a threshold can be found that best splits normal
and anomaly samples, similar to the stochastic approach. A
third hypothesis was that the mean RE will change overtime
due to concept drift, and so will become less sensitive to
more recent samples when taken over a long stream.
To address the above three hypothesis an ‘Adaptive
Anomaly Threshold’ (AAT) method was proposed that
combines the NAT, SAT and Fading Factor [30] methods.
The proposed method is given in algorithm 1. Normal
samples were used to update the fading average RE-score
over the stream, using a fading factor α [30] in order to give
more importance to more recent sample values, satisfying
hypothesis 3 above. The maximum RE of normal samples
over the data stream is also recorded and used to ﬁnd
the ﬁrst value of the anomaly threshold φ. If the initial
1. http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html
2. https://www.unsw.adfa.edu.au/unsw-canberra-
cyber/cybersecurity/ADFA-NB15-data sets/
3. https://keras.io/
4. https://scikit-multiﬂow.github.io/
5
maximum value of φ achieves an accuracy of 1.0 or 100%,
then this fulﬁlled the ﬁrst hypothesis that all samples are
normal and no further action was required. Otherwise
hypothesis 2 is assumed and a stochastic approach was
then used to step through potential threshold values until
the highest accuracy is found.
Algorithm 1: Adaptive Anomaly Threshold
Input : autoencoder m, X, y, threshold φ, step size
v ← [> 0], fading factor α
Output: φ
/* Initialise fading sum, fading
increment, and max RE variables
*/
1 S0 ← 0; N0 ← 0; REmax ← 0;
/* Find the fading mean RE of normal
samples
*/
2 Xy←0 ⊆ X;
3 REi ← predictRE(m, Xy←0);
4 Si ← REi + α ∗ Si−1;
5 Ni ← 1 + α ∗ Ni−1;
6 REµα ← Si
Ni ;
/* Find the maximum normal sample RE of
the data stream
*/
7 if REi > REMAX then
8
REMAX ← REi;
9 end
/* Set threshold to the maximum
observed RE
*/
10 φ ← REMAX;
/* Calculate accuracy, only attempt to
find a lower threshold if accuracy
is not 100%
*/
11 ˆy ← predict(m,φ,X);
12 accw ← calcAccuracy(ˆy,y);
13 if accw < 1.0 then
/* Step through to the fading mean
of the stream RE to find
threshold that yeilds the highest
accuracy
*/
14
φw ← φ;
15
while φ > REµα do
16
φ ← φ − v;
17
ˆy ← predict(m,φ,X);
18
acc ← calcAccuracy(ˆy,y);
19
if acc > accw then
20
φw ← φ;
21
accw ← acc;
22
end
23
end
24
φ ← φw;
25 end
The proposed autoencoder anomaly detector is depicted
in ﬁgure 1. The sample X is inputted to the autoencoder net-
work from which a Reconstruction Error (RE) is produced
based on the loss value between the approximate output
and the original input. The RE is compared to an anomaly
threshold value with samples scoring above threshold being
labeled as an ‘anomaly’ and those below being ‘normal’ or
benign. If a label Y is provided then the anomaly thresh-
old is updated using a novel adaptive anomaly threshold
method, which also maintains a memory of the population
mean RE throughout the data stream by using a fading fac-
tor [2] memory mechanism to prioritise more recent samples
for faster adaptation. The adaptive anomaly threshold is
demonstrated to be superior to ﬁxed and other threshold
determination methods from the literature. Note that the
use of labels to ﬁnd the anomaly threshold results in a semi-
supervised method.
Fig. 1: Autoencoder Anomaly Detector
3.2
Active Stream Framework
The proposed autoencoder anomaly detector is a semi-
supervised method requiring class labels to be known.
Class annotation is also important to detect changes in the
data stream that require learning to occur in order for the
model to adapt. Given the inﬁnite nature of a data stream,
labeling all samples is infeasibly expensive, therefore AL
methods were explored to minimise the labeling cost for
both updating the model and threshold, whilst identifying
and adapting to changes in the data stream.
ˇZliobait˙e et al. [3], proposed an active stream framework,
which combines change detection with a labeling strategy
and a ﬁxed budget B. Algorithm 2 gives the active stream
framework evaluated in this research. The active learning
strategy is an important part of the framework as it deter-
mines whether or not the current data sample Xi, yi should
be labeled. Blind, random, uncertainty, variable uncertainty
and split strategies were evaluated in this research [3], [21],
[22]. The framework maintains a running estimate of label
usage ˆui over a fading window, calculated by equation 3,
where w is the size of the fading window and labeli is
the labeling decision either 0 or 1 at time i. The spending
6
estimate ˆb is then calculated from ˆui over w, given in
equation 4 [3]. During this evaluation, w was set to 1000.
The labeled samples are then used to train the model and
perform change detection. If a warning signal is received
then a new autoencoder (AEL) is trained with the most
recent examples, and when a change is signaled, the current
model is replaced with AEL, completing adaptation to the
new concept. For this evaluation the Drift Detection Method
(DDM) [25] change detector was used.
ˆui = ˆui−1 ∗ (w − 1)
w
+ labeli
(3)
ˆb = ˆui
w
(4)
Algorithm 2: Active Stream Framework
Input : Autoencoder AE, Labeling budget B,
budget window w, strategy(parameters),
Change Detector D
Output: AE
/* Initialise active stream framework
*/
1 ˆb ← 0; ˆu0 ← 0;
/* Check if current spending estimate
is below budget and strategy decides
to label
*/
2 if ˆb < B AND strategy(parameters) = 1 then
3
Update label estimate ˆui (equation 3) where
labeli = 1;
/* Incrementally fit the autoencoder
and predict current label
*/
4
AE ← partialFit(AE, Xi, yi);
5
ˆyi ← predict(AE, Xi);
/* Update the change detector
statistics
*/
6
updateChangeDetector(D, yi ̸= ˆyi);
7
if AEL then
/* If an alternative AE exists
then update this with the
labeled samples
*/
8
AEL ←partialFit(AELXi, yi);
/* If change is detected then
replace the current AE with the
alternate
*/
9
if changeSignalled(D) then
10
Replace AE with AEL;
11
end
12
else if warningSignalled(D) then
/* If warning is signalled then
create new alternate AE and
train
*/
13
Create new AEL;
14
AEL ←partialFit(AEL, Xi, yi);
15 else
16
Update label estimate ˆui (equation 3) where
labeli = 0;
17 end
18 Update spending estimate ˆb (equation 4);
3.3
Active Learning Strategies
The following section outlines the active learning strategies
evaluated in this research. ˇZliobait˙e et al. [3] outlined three
objectives of active learning strategies, which will need to
be met by any proposed strategies:
1)
balance the labeling budget B over inﬁnite time;
2)
detect changes anywhere in the instance space;
3)
preserve the distribution of incoming data for de-
tecting changes.
A random active learning strategy randomly selects a
sample to label based on Bernoulli probability with a given
budget B. The random strategy satisﬁes all three objectives
of [3].
The uncertainty strategy labels a sample based on the level
of uncertainty from the classiﬁer compared to a threshold,
and attempts to label the samples where there is the least
conﬁdence [3]. A common approach is to use the classiﬁer’s
predicted probability for class c compared to the threshold
θ: P(yc|X) ≤ θ [3], [21], [22].
Autoencoders do not provide a direct class probability,
instead they provide a reconstruction error from which a
normal or anomaly classiﬁcation decision can be made. This
research proposed a novel method whereby the RE squared
difference from the anomaly threshold φ is used as a mea-
sure of uncertainty, equation 5, assuming the hypothesis
that the lower the difference compared to the average of the
population, then the greater the uncertainty for the sample.
The difference is squared to make all values positive.
di = (φ − REXi)2
(5)
In order to accommodate changes in the data stream and
avoid a scenario where the strategy stops learning due to
high variance, a fading factor α was used to produce a
fading average of differences davg, calculated using equation
6. This allowed for the more recent samples to have a greater
bearing on the strategy outcome.
Si = di + α ∗ Si−1
Ni = 1 + α ∗ Ni−1
davg = Si
Ni
(6)
Using davg the fading standard deviation dstd of the
stream was calculated using equation 7.
Vi = (di − davg)2 + α ∗ Vi−1
dstd =
r
Vi
Ni
(7)
Finally, the strategy returned a labeling decision of 1
where di < davg − dstdθ, equation 8, requiring a sample
to be below the average by so many θ standard deviations,
where θ was the conﬁdence threshold. θ = 2 should capture
samples where the difference is the lowest 5% of all samples.
labeling =
(
1,
di < davg − dstdθ
0,
otherwise
(8)
7
Fig. 2: Split Active Learning Anomaly Detector
The uncertainty strategy algorithm is given in 3,
whereby the autoencoder AE model is used to predict the
RE for sample Xi, and the fading average and standard
deviation of the difference from the anomaly threshold φ
over the stream used to provide a label output of 0 or 1
based on equation 8. On its own, an uncertainty strategy
cannot satisfy all three active learning objectives as: the
number of labeled samples will depend on the amount of
uncertainty within the data stream and could vary above
the intended budget, this is instead limited by line 2 of
algorithm 2; only samples within the uncertainty margin
are labeled, changes occurring outside of the margin will
be missed; and change detection will be based on the
distribution of uncertain samples that are trained on [3].
The strategy should reﬂect regions where real concept drift
is occurring as higher uncertainty could reﬂect a change,
resulting in faster adaptation times [21], [22].
Variable uncertainty is based on the uncertainty strategy,
but instead of using a ﬁxed conﬁdence θ, this is instead
varied depending on the amount of labeling that is being
requested from the strategy, so that more labels will increase
the conﬁdence and fewer will decrease to attenuate the
labeling and better manage budget [3]. This approach also
has the beneﬁt that it is not limited to a ﬁxed labeling ceiling
Algorithm 3: Uncertainty Strategy
Input : Conﬁdence θ, Fading Factor α, X,
autoencoder AE, Threshold φ
Output: label
1 S0 ← 0; N0 ← 0; V0 ← 0; label ← 0;
2 REi ← predictRE(AE, Xi);
3 Calculate difference di of REi from φ, using equation
5;
4 Calculate the fading average difference davg, using
equation 6;
5 Calculate the fading standard deviation of
differences dstd using equation 7;
6 if di < davg − dstdθ then
7
label ← 1;
8 end
and can better utilise higher budgets to accurately identify
concept drift [22]. Similar to the uncertainty strategy this
also does not satisfy all three requirements [3].
The split strategy, given in algorithm 4, combines the
random and variable uncertainty strategies to beneﬁt from
their respective strengths of accessing the entire stream
distribution for change detection, and adapting to potential
8
change in higher regions of uncertainty. Due to the incor-
poration of the random strategy, this also meets all three
requirements of [3].
Algorithm 4: Split Strategy
Input : Label Budget B, Conﬁdence θ, Fading
Factor α, X, autoencoder AE, Threshold φ,
Step s
Output: label
1 label ← 0;
2 if randomStrategy(B) = True then
3
label ← 1;
4 else if varUncertaintyStrategy(θ,α,Xi, AE,
φ,s) = True then
5
label ← 1;
The proposed Split Active Learning Anomaly Detector
(SALAD) method is depicted in ﬁgure 2. This method re-
duces the labeling cost of the data stream to a ﬁxed budget
by adopting an active learning strategy to determine which
labels should be updated, satisfying the requirements of
ˇZliobait˙e et al. [3]. Labeled samples are used to train the
anomaly detector and the predictions input to a change
detector which monitors for real concept drift occurring in
the data stream [2]. Where real concept drift occurs, the
current anomaly detector is replaced with a new one that
has been trained on samples since a warning signal was
produced. The result of this method is faster training of the
anomaly detector and the ability to quickly adapt to changes
occurring in the data stream.
4
RESULTS
4.1
Adaptive Anomaly Threshold
The accuracy and F1-score of the Adaptive Anomaly
Threshold method was compared to the Stochastic Anomaly
Threshold with memory (SAT FF), HAT and NB algorithms.
SAT FF is a novel modiﬁed version of the SAT algorithm
to update the threshold based on a fading average [30] of
previous thresholds to allow for memory when processing
over a data stream. The parameter values for the autoen-
coder methods are given in table 1, where p represents the
dropout probability; l is the number of hidden layers, h the
ratio of hidden units to visible units; opt is the optimiser
used to train the network with α learning rate; β is the
threshold sensitivity; α is the fading factor; and v is the
step size. NB and HAT algorithms used the scikit-multiﬂow
default parameters [29].
Method
Parameters
Prequential Evaluation
batch size = 100, pretrain size = 10000
Autoencoder
l = 1, p = 0.1, h = 0.6, opt = adagrad
(α = 0.01)
SAT FF
β = 1.1, v = 0.001, α = 0.4
Adaptive
Anomaly
Threshold
β = 1.18, v = 0.001, α = 0.4
TABLE 1: Evaluation Parameters
The accuracy and F1 scores with the KDD Cup 1999
data set are plotted in ﬁgure 3. SAT FF and AAT are
(a) Accuracy
(b) F1-score
Fig. 3: KDD Cup 1999 AAT, SAT FF, NB and HAT accuracy
and F1-score
close to HAT in terms of mean performance, with better
kappa and F1 metrics when taken as an average across all
batches, as shown in table 2. SAT FF and AAT were also
signiﬁcantly faster with a total running time (RT) of 14.04s
and 19.18s, compared to 510.93s and 794.76s with NB and
SAT, respectively. Note that running time will vary based
on the underlying system performance and frameworks
used, however the time of SAT FF is an order of magnitude
better compared to both NB and HAT algorithms. Overall
AAT returned the best mean accuracy and kappa results, an
important metric for data stream learning.
Algorithm
Accuracy %
Kappa
F1-score
RT
µ±SD
µ±SD
µ±SD
(s)
AE AAT
98.78±7.88
0.954±0.202 0.802±0.395 19.18
AE SAT FF
98.16±8.65
0.854±0.360 0.812±0.387 14.04
NB
93.34±20.22
0.721±0.445 0.810±0.380 510.93
HAT
98.57±0.60
0.820±0.379 0.811±0.383 794.76
TABLE 2: KDD Cup 1999 AAT, SAT FF, NB and HAT Results
As demonstrated in our previous work [31], the UNSW-
NB15 data set proved to be more challenging for on-
9
(a) Accuracy
(b) F1-score
Fig. 4: UNSW-NB15 AAT, SAT, SAT FF, NB and HAT accu-
racy and F1-score
line learning, requiring the number of network layers and
dropout probability to be adjusted to better provide separa-
tion between normal and anomaly class distributions, with
l = 3 and p = 0.2 being selected. The accuracy and F1-
score results of the AAT method compared to SAT, SAT FF,
NB and HAT are plotted in ﬁgure 4. Table 3 gives average
accuracy of the SAT and SAT FF algorithms as 70.39% and
62.96%, respectively, which is considerably lower than that
of NB and HAT. AAT returned the highest overall accuracy
of the anomaly threshold methods, at 86.31% with 3 layers
and dropout probability of 0.2, although kappa was lower,
demonstrating reduced conﬁdence in the anomaly decision
for all methods. The results show that AAT is able to provide
near equivalent performance to NB and HAT methods with
a signiﬁcantly lower running time.
4.2
Active Stream Framework
4.2.1
Labeling Budget
The effects of the labeling budget was evaluated with the
random strategy as this is the only strategy to maintain
Algorithm
Accuracy %
Kappa
F1-score
RT
µ±SD
µ±SD
µ±SD
(s)
AE AAT
86.31±16.32
0.298±0.411 0.767±0.335 18.55
AE SAT
70.39±32.71
0.364±0.443 0.613±0.390 12.14
AE SAT FF
62.96±38.95
0.420±0.458 0.528±0.418 11.01
NB
83.69±28.99
0.399±0.480 0.832±0.343 350.39
HAT
92.85±11.19
0.436±0.479 0.813±0.340 610.94
TABLE 3: UNSW-NB15 AAT, SAT, SAT FF, NB and HAT
Results
the sample distribution of the stream so as to not add any
bias to the results. Budget B was evaluated at values of 0.2
(20%), 0.5 (50%) and 1.0 (100%). The results are given in table
5 and mean accuracy plotted against the blind adaption
AAT approach for comparison in ﬁgure 5. The greater the
labeling budget, typically the higher the accuracy, kappa
and F1 scores, the exception being UNSW-NB15 where
B = 0.5 has a slightly higher accuracy and kappa. The
difference in accuracy between 20% and 100% labels is 0.76%
(KDD’99) and 2.69% (UNSW-NB15), demonstrating a small
loss in performance for an 80% saving in labeling cost and
approximate running time reduction of 54-62%; this reﬂects
the results of ˇZliobait˙e et al. [3], where a small loss of
accuracy was observed between a B of 100% and 10% when
tested with a number of non-cyber data sets.
Comparing to the blind adaptation of previous experi-
ments, whereby no active learning is used, a labeling budget
of 0.5 achieved a higher accuracy and F1 for half the labeling
cost on both data sets. ASF RAND 1.0 is equivalent to the
blind approach with full labels, but with the addition of
change detection, with average accuracy and F1 improved
across both data sets, although lower towards the end of the
UNSW-NB15 stream as shown in ﬁgure 5b. Note the lower
running time of the blind approach due to use of a chunk
size of 100 vs 10 which inﬂuences the number of gradient
updates and hence training time of the network.
Strategy
B
Accuracy %
Kappa
F1-score
RT
µ±SD
µ±SD
µ±SD
(s)
KDD Cup 1999
Random
0.2
98.32±8.50
0.932±0.217 0.811±0.381 55.9
Random
0.5
98.94±7.27
0.956±0.182 0.821±0.376 85.8
Random
1.0
99.08±7.02
0.962±0.176 0.825±0.374 145.4
Blind
1.0
98.78±7.88
0.954±0.202 0.802±0.395 19.18
UNSW-NB15
Random
0.2
87.07±19.48
0.598±0.376 0.752±0.350 55.5
Random
0.5
90.85±12.16
0.619±0.265 0.791±0.338 84.0
Random
1.0
89.76±12.74
0.549±0.431 0.793±0.334 121.2
Blind
1.0
86.31±16.32
0.298±0.411 0.767±0.335 18.55
TABLE 4: Random Strategy Budget Size: KDD’99 and
UNSW-NB15 Comparison
4.2.2
Active Learning Strategies
The results of each active learning strategy with a budget
of 0.2 (20%) are given in Table 5, with accuracy and F1-
score for both data sets plotted in ﬁgure 6. Each strategy was
executed 5 times with the average and standard deviation
presented. The worst performing strategy was the ﬁxed
uncertainty strategy, reﬂecting the results of ˇZliobait˙e et al.
[3], which was expected as the algorithm is biased only
towards uncertain samples and cannot vary the amount of
samples labeled, meaning that change occurring outside of
10
(a) KDD’99 Accuracy
(b) UNSW-NB15 Accuracy
Fig. 5: Labeling Budget Accuracy Comparison for Random
Strategy
the ﬁxed margin will be missed. It is also possible that the
RE=value of normal samples outside of the margin may
increase as the AE is trained more on uncertain samples,
leading to higher false positives and lower F1-score.
The split strategy, returned the best results across both
data sets, combining random and variable uncertainty
strategies. Note that the total running time is between that
of the random and variable uncertainty strategies, indicting
time complexity savings where uncertain samples were ﬁrst
selected by the random strategy. The Kappa of the split
strategy was observed as 0.717 (table 5) for the UNSW-NB15
data set, this is much higher than the performance of the
blind AAT, NB, HAT and other AL strategies, indicating a
higher level of conﬁdence in the anomaly decisions.
5
DISCUSSION
This research evaluated online anomaly detection in the
form of a prequential evaluation method whereby the model
is ﬁrst tested on the next sample or chunk in the stream
before training. The anomaly threshold is a key parameter
for anomaly detection and ﬁnding an optimal threshold
Strategy
Accuracy %
Kappa
F1-score
RT
µ±SD
µ±SD
µ±SD
(s)
KDD Cup 1999
Random
98.32±8.50
0.932±0.217 0.811±0.381 55.9
Uncertainty
93.32±23.40
0.892±0.303 0.762±0.422 81.6
Var Uncert
98.61±8.65
0.951±0.194 0.817±0.379 74.1
Split
98.85±7.55
0.947±0.199 0.819±0.378 69.6
UNSW-NB15
Random
87.07±19.48
0.598±0.376 0.752±0.350 55.5
Uncertainty
83.95±16.40
0.348±0.304 0.762±0.334 53.8
Var Uncert
87.51±16.26
0.452±0.368 0.768±0.339 64.6
Split
90.88±14.96
0.717±0.363 0.791±0.343 63.3
TABLE 5: Active Learning Strategy Comparison
for a data stream is non-trivial. A number of methods for
ﬁnding the threshold were compared including ﬁxed, na¨ıve,
stochastic and adaptive techniques. The adaptive anomaly
threshold (AAT) was introduced as a novel hybrid of the
na¨ıve and stochastic methods in order to better adapt to
chunks of normal or anomaly samples based on initial ob-
served accuracy. Overall AAT outperformed other methods
and is a recommended contribution of this research to be
explored further.
The results observed with the KDD’99 data set and AAT
threshold method provide strong evidence that the hypoth-
esis of effective anomaly detection for network data streams
can be supported by the autoencoder method with both
strong detection and run time performance compared to
traditional methods. UNSW-NB15 results could be strength-
ened by further design choices.
The AAT method makes use of blind adaptation,
whereby the model is trained on all labeled samples. This
has the drawback of high cost due to full labels and slow
adaptation times to change occurring in the data stream.
The research further explored change detection and active
learning strategies, as outlined by ˇZliobait˙e et al. [3], to
further improve performance for a lower overall cost.
An ASF framework was implemented along with the
random, uncertainty, variable uncertainty and split active
learning strategies. With the uncertainty strategy, a new
method for AE was proposed, whereby the average RE
difference from the threshold is used as a baseline to detect
samples with high uncertainty, deﬁned as being in the
proportion of the population with the smallest difference,
tuned by a conﬁdence parameter.
The use of ASF demonstrated that better accuracy, kappa
and F1 scores can be achieved, compared to blind adapta-
tion, with just 20% of the labeling cost, enabled by active
learning of the most important samples to accelerate the
learning process [3]. The results align to those presented by
ˇZliobait˙e et al. [3], with a split strategy being recommended
as this fulﬁlls all three active learning requirements to
maintain a ﬁxed budget, access to all samples within the
stream and preserve the distribution of incoming data for
detecting changes. Unlike ˇZliobait˙e et al. [3], this research
recommends inclusion of the uncertain samples with the
change detection to improve per class performance.
6
CONCLUSION
The aim of this research was to explore semi-supervised
online autoencoder methods for the task of anomaly in-
11
(a) KDD’99 Accuracy
(b) KDD’99 F1-score
(c) UNSW-NB15 Accuracy
(d) UNSW-NB15 F1-score
Fig. 6: ASF Strategy Comparison, B = 20%
trusion detection on non-stationary network data streams,
adapting to concept drift over time, with minimal label-
ing cost, by adopting an active learning change detection
strategy. A unique contribution of this research was to
compare a selection of anomaly threshold methods, propos-
ing memory adaptations for data streams and a hybrid
Adaptive Anomaly Threshold method which demonstrated
superior performance. One of the more striking ﬁndings of
the research is that the processing time of the autoencoder
anomaly detector method is signiﬁcantly lower when com-
pared to traditional online learning techniques, making it
well adjusted for high speed online network data streams,
demonstrating an ability to detect an equivalent number of
cyber attacks to traditional online learning methods, in a
signiﬁcantly reduced time frame. An area of future research
would be to explore alternative threshold methods, such
as clustering, which may allow for better identiﬁcation of
classes that overlap with normal samples and multi-label
classiﬁcation.
A further contribution of this research was to evaluate
the autoencoder method with an Active Stream Framework,
allowing the labeling cost of the data stream to be sig-
niﬁcantly reduced to a budget of 20%. A novel variable
uncertainty strategy was proposed for autoencoders where
the posterior probability is not available, instead tracking
the distribution of sample RE distances from the anomaly
threshold to determine uncertainty. An area of future re-
search should be how to efﬁciently annotate samples, pos-
sibly by unsupervised clustering methods such as those
demonstrated by [32].
Overall this research has demonstrated that the pro-
posed Split Active Learning Anomaly Detector (SALAD)
method can demonstrate high levels of performance with
network data streams, which signiﬁcantly reduced the label-
ing cost. The results are not perfect however, and it would
be recommended to combine in a hybrid intrusion detection
model whereby misuse detection is used before or after the
anomaly detector to further identify classes, reduce false
positives and better identify minority classes. Multi-label
classiﬁcation would be a further research area to expand
on this work and provide additional context to detections.
REFERENCES
[1]
A. L. Buczak and E. Guven, “A survey of data mining and machine
learning methods for cyber security intrusion detection,” IEEE
12
Communications Surveys & Tutorials, vol. 18, no. 2, pp. 1153–1176,
2016.
[2]
J. Gama, I. ˇZliobait˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia,
“A survey on concept drift adaptation,” ACM computing surveys
(CSUR), vol. 46, no. 4, p. 44, 2014.
[3]
I. ˇZliobait˙e, A. Bifet, B. Pfahringer, and G. Holmes, “Active
learning with drifting streaming data,” IEEE transactions on neural
networks and learning systems, vol. 25, no. 1, pp. 27–39, 2013.
[4]
S. Mansalis, E. Ntoutsi, N. Pelekis, and Y. Theodoridis, “An eval-
uation of data stream clustering algorithms,” Statistical Analysis
and Data Mining: The ASA Data Science Journal, vol. 11, no. 4, pp.
167–187, 2018.
[5]
I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning.
MIT
Press, 2016, http://www.deeplearningbook.org.
[6]
S. Dua and X. Du, Data mining and machine learning in cybersecurity.
Auerbach Publications, 2016.
[7]
K. Alrawashdeh and C. Purdy, “Toward an online anomaly in-
trusion detection system based on deep learning,” in 2016 15th
IEEE International Conference on Machine Learning and Applications
(ICMLA).
IEEE, 2016, pp. 195–200.
[8]
——, “Fast activation function approach for deep learning based
online anomaly intrusion detection,” in 2018 IEEE 4th International
Conference on Big Data Security on Cloud (BigDataSecurity), IEEE
International Conference on High Performance and Smart Comput-
ing,(HPSC) and IEEE International Conference on Intelligent Data and
Security (IDS).
IEEE, 2018, pp. 5–13.
[9]
S. Roshan, Y. Miche, A. Akusok, and A. Lendasse, “Adaptive and
online network intrusion detection system using clustering and
extreme learning machines,” Journal of the Franklin Institute, vol.
355, no. 4, pp. 1752–1779, 2018.
[10] X. Chen, C. Cao, and J. Mai, “Network anomaly detection based
on deep support vector data description,” in 2020 5th IEEE Inter-
national Conference on Big Data Analytics (ICBDA).
IEEE, 2020, pp.
251–255.
[11] M. M. Hassan, A. Gumaei, A. Alsanad, M. Alrubaian, and
G. Fortino, “A hybrid deep learning model for efﬁcient intrusion
detection in big data environment,” Information Sciences, vol. 513,
pp. 386–396, 2020.
[12] C. Nixon, M. Sedky, and M. Hassan, “Autoencoders: A low cost
anomaly detection method for computer network data streams,”
in Proceedings of the 2020 4th International Conference on Cloud and
Big Data Computing, ser. ICCBDC ’20.
New York, NY, USA:
Association for Computing Machinery, 2020, p. 58–62. [Online].
Available: https://doi.org/10.1145/3416921.3416937
[13] M. Nicolau and J. McDermott, “A hybrid autoencoder and density
estimation model for anomaly detection,” in International Confer-
ence on Parallel Problem Solving from Nature.
Springer, 2016, pp.
717–726.
[14] A. H. Mirza and S. Cosan, “Computer network intrusion detection
using sequential lstm neural networks autoencoders,” in 2018 26th
Signal Processing and Communications Applications Conference (SIU).
IEEE, 2018, pp. 1–4.
[15] T. Kieu, B. Yang, C. Guo, and C. S. Jensen, “Outlier detection
for time series with recurrent autoencoder ensembles,” in 28th
international joint conference on artiﬁcial intelligence, 2019.
[16] Y. Mirsky, T. Doitshman, Y. Elovici, and A. Shabtai, “Kitsune: an
ensemble of autoencoders for online network intrusion detection,”
arXiv preprint arXiv:1802.09089, 2018.
[17] X. Li, W. Chen, Q. Zhang, and L. Wu, “Building auto-encoder
intrusion detection system based on random forest feature selec-
tion,” Computers & Security, p. 101851, 2020.
[18] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga, “Outlier detection
with autoencoder ensembles,” in Proceedings of the 2017 SIAM
International Conference on Data Mining.
SIAM, 2017, pp. 90–98.
[19] T. Vaiyapuri and A. Binbusayyis, “Application of deep autoen-
coder as an one-class classiﬁer for unsupervised network intru-
sion detection: a comparative evaluation,” PeerJ Computer Science,
vol. 6, pp. 1–26, 2020.
[20] B.
Krawczyk,
L.
L.
Minku,
J.
Gama,
J.
Stefanowski,
and
M. Wo´zniak, “Ensemble learning for data stream analysis: A
survey,” Information Fusion, vol. 37, pp. 132–156, 2017.
[21] T. S. Sethi and M. Kantardzic, “On the reliable detection of
concept drift from streaming unlabeled data,” Expert Systems with
Applications, vol. 82, pp. 77–99, 2017.
[22] J. Shan, H. Zhang, W. Liu, and Q. Liu, “Online active learning
ensemble framework for drifted data streams,” IEEE transactions
on neural networks and learning systems, vol. 30, no. 2, pp. 486–498,
2018.
[23] Q.-V. Dang, “Active learning for intrusion detection systems,” in
IEEE Research, Innovation and Vision for the Future, 2020.
[24] Z. Zhang, Y. Zhang, J. Niu, and D. Guo, “Unknown network
attack detection based on open-set recognition and active learning
in drone network,” Transactions on Emerging Telecommunications
Technologies, vol. n/a, no. n/a, p. e4212. [Online]. Available:
https://onlinelibrary.wiley.com/doi/abs/10.1002/ett.4212
[25] J. Gama, P. Medas, G. Castillo, and P. Rodrigues, “Learning with
drift detection,” in Brazilian symposium on artiﬁcial intelligence.
Springer, 2004, pp. 286–295.
[26] M. Tavallaee, E. Bagheri, W. Lu, and A. A. Ghorbani, “A detailed
analysis of the kdd cup 99 data set,” in 2009 IEEE Symposium
on Computational Intelligence for Security and Defense Applications.
IEEE, 2009, pp. 1–6.
[27] N. Moustafa and J. Slay, “Unsw-nb15: a comprehensive data
set for network intrusion detection systems (unsw-nb15 network
data set),” in 2015 military communications and information systems
conference (MilCIS).
IEEE, 2015, pp. 1–6.
[28] F. Chollet et al., “Keras,” https://keras.io, 2015.
[29] J. Montiel, J. Read, A. Bifet, and T. Abdessalem, “Scikit-multiﬂow:
a multi-output streaming framework,” The Journal of Machine
Learning Research, vol. 19, no. 1, pp. 2915–2914, 2018.
[30] J. Gama, R. Sebasti˜ao, and P. P. Rodrigues, “On evaluating stream
learning algorithms,” Machine Learning, vol. 90, no. 3, pp. 317–346,
2013.
[31] C. Nixon, M. Sedky, and M. Hassan, “Practical application of
machine learning based online intrusion detection to internet of
things networks,” in 2019 IEEE Global Conference on Internet of
Things (GCIoT).
IEEE, 2019, pp. 1–5.
[32] Z. Cataltepe, U. Ekmekci, T. Cataltepe, and I. Kelebek, “Online fea-
ture selected semi-supervised decision trees for network intrusion
detection,” in NOMS 2016-2016 IEEE/IFIP Network Operations and
Management Symposium.
IEEE, 2016, pp. 1085–1088.
Christopher Nixon received his BEng(Hons)
in Computer Science in 2008 and Masters by
Research in Computing Science with distinction
in 2020 from Staffordshire University. He has
11 years of experience in the ﬁnancial services
and telecoms industries, leading a team of cyber
security experts in problem areas such as end
user, cloud and network security. His research
interests include machine learning and its appli-
cation to cyber security.
Dr Mohamed Sedky is an Associate Professor
in Artiﬁcial Intelligence and Machine Learning
at Staffordshire University. After he received his
BEng(Hons) in Electro-Physics and Communi-
cations in Alexandria University, Egypt, in 1996,
Mohamed founded SKM communication sys-
tems. He ﬁnished his MSc degree in Communi-
cations and Electronics from the AAST in 2002.
In 2009, he gained a PhD from Staffordshire
University. He has led the Artiﬁcial Intelligence
(AI) & Internet of Things (IoT) research group
in Staffordshire University to conduct research in the development of
artiﬁcial intelligent techniques and simulation tools for different Internet
of things applications, robotic, and more recently applying his work in
forensic science towards the detection and classiﬁcation of microplas-
tics.
Mohamed Hassan is leading the cyber security
award at Staffordshire University. He holds a
Master of Science degree in computer science
with distinction and has extensive industry and
academic experience in computing and cyber
security. His teaching and research focus in the
ﬁeld of cyber security, machine/deep learning
and digital forensics. He is also interested in
other topics in cyber security, such as malware
analysis and reverse engineering, security in
embedded systems and IoT security.


Paper 5:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: 
  Extract 2: 
  Limitations: >
  Relevance Evaluation: The purpose of this paper is to explore how automated systems for real-time irrigation management can contribute to the efficient use of water resources and enhance agricultural productivity. The primary objective is to critically assess the current state and future potential of end-to-end automated irrigation management systems that integrate IoT and machine learning technologies.
  Relevance Score: 1.0
  Inline Citation: >
  Explanation: From your close reading of the paper, provide a concise explanation of the study's purpose and main objectives, using a maximum of 3 sentences.

 Full Text: >
This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising purposes. To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 9 6G Ecosystem: Current Status and Future Perspective Publisher: IEEE Cite This PDF Jagadeesha R. Bhat; Salman A. Alqahtani All Authors 122 Cites in Papers 15063 Full Text Views Open Access Comment(s) Under a Creative Commons License Abstract Document Sections I. Introduction II. Scope of Future Networks III. Literature Review IV. 6G Enabling Technologies V. Architectures for 6G Show Full Outline Authors Figures References Citations Keywords Metrics Abstract: Next-generation of the cellular network will attempt to overcome the limitations of the current Fifth Generation (5G) networks and equip itself to address the challenges which become obvious in the future. Currently, academia and industry have focused their attention on the Sixth Generation (6G) network, which is anticipated to be the next big game-changer in the telecom industry. The outbreak of COVID'19 has made the whole world to opt for virtual meetings, live video interactions ranging from healthcare, business to education. However, we miss an immersive experience due to the lack of supporting technology. Experts have anticipated that starting from the post-pandemic age, the performance requirements of technology for virtual and real-time communication, the rise of several verticals such as industrial automation, robotics, and autonomous driving will increase tremendously, and will skyrocket during the next decade. In this manuscript, we study the latest perspectives and future megatrends that are most likely to drive 6G. Initially, we describe the instances that lead us to the vision of 6G. Later, we narrate some of the use cases and the KPIs essential to meet their performance requirement. Further, we highlight the key requirements of 6G based on contemporary research such as UN sustainability goals, business model, edge intelligence, digital divide, and the trends in machine learning for 6G. Technologies that enable 6G comprising of digital, physical, and biological spaces. Published in: IEEE Access ( Volume: 9) Page(s): 43134 - 43167 Date of Publication: 26 January 2021 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2021.3054833 Publisher: IEEE Funding Agency: SECTION I. Introduction Recently after the launch of Fifth Generation (5G) mobile networks, several data-intensive applications and verticals which require specific network parameters have undergone a deep sense of relief due to the availability of high data rate, bandwidth, and reliability. In 2020, the 5G deployments by different network providers either as Non-Stand-Alone (NSA), or as Stand-Alone (SA), will address the need for mobile internet by high data rate (e.g., enhanced Mobile Broadband (eMBB)), ultra-reliable low latency (uRLLC), and connectivity between massively dense deployment of smart devices (e.g., massive Machine Type Communication (mMTC)) [1]. Further, 5G networks can reduce the network scale-up time drastically due to its virtualized core. Consequently, the cost reduces. All these features have made 5G networks predominantly different from its predecessors in terms of network capacity, and range of applications it would support [1]. It is envisioned that by 2030, there will be 97 billion machine type devices, resulting in an astonishing increase in the global mobile generated data traffic to 5.016 Zeta Bytes (ZB) per month from 0.062 ZB as in 2020 [2]. Further, a survey has anticipated that by 2030, the number of people living in 43 megacities across the globe will increase to 10 million [19]. Consequently, information communication technologies (ICT) should be agile and robust with respect to the massive data generated by the mobile devices due to urbanization and from smart city ecosystems such as smart transportation, smart healthcare, and smart buildings. Moreover, the data generated will be fueled by the emerging applications in the area of Artificial Intelligence (AI), Robotics, Industry 4.0, and Internet of Everything (IoE) to name but a few, that generates a colossal amount of data. In such a context, cellular network infrastructure will be one of the key drivers to support extreme data rate, high bandwidth, and quality of service (QoS) for these verticals [3], [4]. Let us consider a few future internet applications such as self-driving cars with simultaneous communication capability. In such a case, each communication link demands extreme network parameters, for instance, connectivity with a delivery drone to assist in the delivery of emergency medical facilities that requires extremely low latency (<1ms), on the other hand, with the roadside infrastructure (V2X) that requires high reliability (~99.9999999%) to prevent accidents [48]. Similarly, remote robotic surgery in a smart hospital, remote holographic image transmission of a live public concert, data communication from ultra-high dense IoT (uHDIoT) and wearable devices, haptic online games, and haptic meetings are some other scenarios. In 2030, these use cases will be the key thrust areas which require a high degree of automation and intelligence to address diversified requirements [5]–[8]. To address these requirements, we need data rate up to 1 Tbps, ultra-high reliability of 10 −9 and ultra-low latency of 0.1 ms or less [2]–[4]. It is evident from the above facts that 5G telecom networks will drastically fail to meet the said requirements, as 5G’s capacity is relatively lower and unified [8]. As a result, there is a definite need for a next-generation system that support new services, and technologies while maintaining backward compatibility. Altogether, the new technology components and architecture for beyond 5G (B5G) networks is necessary to address a plethora of user applications [3]–[9]. Several experts have articulated the speculated features of Sixth Generation (6G) networks which are considered to be the successor of 5G networks [1]–[9]. As expected, 6G network shall overcome the major limitations of the predecessor networks; alongside, it will further extend the three key features of the 5G network. In Fig. 1(a), the intersection of these factors (classes of use cases) is shown. Certain scenarios may require multiple use cases to be met simultaneously (uRLLC and mMTC) as shown in Fig 1. (a) with intersecting circles. Specifically, 6G network will further enhance Ultra-Mobile Broadband (feUMBB), ultra-High Sensing Low Latency Communications (uHSLLC), ultra-High Density Data (uHDD) services, ultra-High Energy Efficiency (uHEE), ultra-High Reliability and Sensing (uHRS), ultra-High Reliability and User experience (uHRUx), ultra-Low Latency Reliability and Secure (uLLRS), ultra-High Security (uHS), ultra-High Sensing and Localization (uHSLo) and several other combinations of these Key Performance Indicators (KPIs) and use cases [9], [10]. Refer Fig. 1(b), for an exemplary use case of 6G for different verticals. As shown in the figure, certain verticals require a combination of use cases which is indicated by the blue arrow at the intersection. For instance, distance robotic surgery will require low latency (uRLLC) and high security of the data (uHS). This interdependency between multiple use cases is shown as uLLRS (low latency, reliability, and security) by an arrow in Fig 1(b). In the similar way, the other arrows at the intersection (uHRS), (uHRUx) of multiple use cases are shown. FIGURE 1. A comparison of 5G and 6G using support for verticals (a) 5G and (b) 6G. Show All Currently, 5G network is yet to be fully deployed worldwide; as a result, its real-time performance on every use case is not well studied. Nevertheless, when we see the evolution of the telecom industry, it is evident from the keen observation that each generation took a decade from setting a vision, R&D, standardization to market launch. Expecting the same trend, the next generation (i.e., 6G) shall be made realizable for customers’ use around 2030. At this juncture, academia and industry may only have the visionary groundwork regarding 6G networks, as it is too early to realize the full potential of it. Motivated by the current research on 6G networks and its technological components, in this manuscript, we will discuss various aspects of future networks. Several kinds of literature have shed light on vision, requirements [1]–[9], technologies [10]–[15], use cases [96]–[98], of 6G networks. In this article, we outline the key points from the previous research and provide insight into the future research trends in 6G networks. The main contributions of this manuscript are as follows: First, we discuss the expected key features of future networks to give a direction to the research community. We highlight most of the KPIs for 6G networks as applicable to different use cases (Table 1) and we provide an outline of the 6G ecosystem by considering all the stakeholders. We provide an exhaustive categorized survey of exclusive literature on 6G and discuss their research directions (Table 2). Unlike the existing literature, we discuss various architectures for 6G networks from the literature. Moreover, we proposed network architecture for 6G with a layer-wise approach for simplicity; and improved latency, reliability through AI-enabled distributed cloud features. Further, we summarize the future megatrends in 6G networks, namely, UN Sustainable Development Goals, Digital divide, Edge intelligence and Machine learning, and Business models. The remaining part of the manuscript is organized as follows: TABLE 1 List of KPI for 6G and Typical Values [46], [48], [96], [118] TABLE 2 List of Literatures on 6G and Their Key Contributions Section II illustrates the scope of the future networks, KPIs, and use cases of 6G networks. In section III, we classify the recent literatures, and highlight their contributions to the research community. Section IV discusses various technology components of 6G networks and their main features. Furthermore, section V presents the architectures of 6G networks, while section VI provides the future trends in 6G networks. In section VII, the global research initiatives are highlighted, and section VIII narrates the open research areas for future exploration. Finally, section IX concludes this article. SECTION II. Scope of Future Networks The study of future networks enables us to equip ourselves with necessary facilities for the industry verticals required during 2030. The in-depth analysis of the current mobile networks reveals the existence of a large gap between user application’s expectations and services offered by the network providers [38]. For example, user expectations on immersive multimedia, personalized holograms, multi-sense haptic service, etc. remain as a gap which the current networks shall not support [12]. Further, technology components including hardware, software and overall ecosystem (devices, spectrum and standards, security, data management algorithms, cloud-based core and access solutions, and applications) need tremendous upgrade [19]–[23], [28]. A. Transition from 5G to 6G: Gaps and Recommendations Implementation of 5G has escalated in most of the mega- cities worldwide due to 5G’s ability to support current industrial use cases. Albeit, there would emerge several use cases, societal requirements, and technological evolution in the future requiring exploration beyond the abilities of 5G. Currently, terrestrial communication is one of the key requirements in 5G networks; however, it would extend in 6G networks, say from terrestrial to underwater and aerial in several folds of 5G’s capacity [12]–[14]. This degree of freedom in all-round connectivity will encourage the rise of several verticals such as flying cars, ultra-real human- computer interactions, holographic telepresence, and underwater recreation having challenging constraints. To address the demands of these verticals such as 3D connectivity, multi-dimensional video (~16K resolution) [46], extremely high data rate (Tbps) and bandwidth (~10GHz) [9], and so on, the communication networks must be much more intelligent than now. Also, the existing internet cannot support these requirements. Therefore, a new KPIs will be necessary to handle the use cases with integrated performance requirements. For instance, real-time remote robotic surgery will need simultaneous ultra-low latency, massive data rate, and security to work effectively [17], [18]. Furthermore, user applications do not have direct control in terms of latency, security, congestion, and reliability, leading to poor user experience. Similarly, a state of unpreparedness exists to launch holographic data that requires several Gbps to Tbps data rate with extremely low latency and ability to connect to millions of users in real-time. The time difference between the occurrence of events and generation of response should be minimal which is lacking in the existing cellular networks. For instance, in a remote robotic surgery sending a command to control the robotic limbs and the video transmissions of the surgery must have a time precision (latency) of a fraction of milliseconds. Again, sparse infrastructure and interfaces for all-round network connectivity for a packet to traverse when we consider air, underwater, and terrestrial communication. Further, intelligence has been merely considered at the network edge (as in 5G), but we lack intelligence at individual layers of network or when network elements are distributed. 1) Recommendations for Future Networks We recommend the following key point for the future networks: End-to-End connectivity: Future networks shall provide sufficient assistance such as virtualization, intelligent decision, network automation, and slicing to applications in order to make them attain reliability, security, network capacity along with guaranteed and timely delivery from the origin to the destination. Therefore, a holistic approach toward full end-to-end connectivity and integrity of data transfer is necessary to make the future applications a reality [23], [38], [49]. Interoperability: The network will interoperate between heterogeneous, small/ large, and private/ public networks where each mobile node will have multiple radio interfaces to provide all-round connectivity. Compatibility: It should allow new protocols, network architectures, nodes, and services to coexist with the existing technologies. The data transfer protocol must be agile to send packets over different network interfaces with the vision of end-to-end realization of services. Dedicated timely service: There should be autonomous services such as Industry 4.0, autonomous driving, and robotic surgery to improve the service quality and scale up the production, time plays a critical role. Therefore, the network must have exclusive support for time-critical services [22]–[24]. Edge computing capability: To reduce the delay during media-rich services and provide local services efficiently at a closer location to the user devices, the data from the user will be processed at the edge instead of only at the cloud. This increases reliability, scalability, and privacy [42], [97]. Intelligent network: Starting from the physical layer to the applications, the AI will be predominant and distributed across different network entities such as core, access network, and terminal users. Thus, 6G will transit from smart network (as in 5G) to an intelligent network [36], [81]. Ultra-smart devices: The existing hand-held smart- phones will become obsolete and they will be replaced by either smart glasses that would deliver ubiquitous XR experiences or the phone functions that will integrate a smart wearable device/ smart patch offering high resolution, and holographic experiences [19]. We also envision that the features of the phone will be found in a distributed connected intelligent device. Thus, alleviating the need for a hand-held dedicated device. Cell-free networking: The mobile device shall connect to the radio access network seamlessly without depending on a specific type of access point to provide infinite mobility and QoS [31]. The network should be flexible enough to allow devices to connect through a range of access networks such as THz, mm-wave, and visual light communication (VLC). In addition, a user may get served by multiple antennas mounted on various base stations without the restriction of cell boundaries; thus, reducing the inter cell interferences [28], [40]. Support diverse media: Even if 5G supports diverse multimedia such as text, image, video, voice, augmented reality, and multi-dimensional holograms, in 6G the applications would demand the multimedia at lowest latency, precise location, high data rate, etc., which requires much improved KPIs than 5G [106]. Amalgamated sensing, communication, and positioning: Some verticals need a combination of ultra-high-speed, ultra-low latency, exact positioning, along with precise sensing of the surroundings all being met together. To realize such requirements, future networks shall provide resources, energy, hardware, and computational support [108]. Multi-level architecture: The storage, communication, and computing should be distributed at the user, edge, and cloud levels, which will operate as a centralized or as a distributed model to support scalability [108], [114]. B. 6G Ecosystem The 6G ecosystem consists of all stakeholders ranging from an equipment manufacturer to application developer. A brief diagram of the 6G ecosystem has been presented in Fig. 2. The 6G chipset manufacturers will deal with the design of new hardware and electronic components such as radio, modulator to accommodate new technology. Next, the mobile manufacturers and network equipment vendors (radio access network installer) will base their products to support the underlying technology from the chipset manufacturers. Next, all these technologies will assist the mobile network operators to launch their mobile services to users. Moreover, the edge devices at the boundary of radio access and the core network, cloud servers, data centers, software modules such as core network virtualization, slice providers, content delivers, will exchange data and services with the application developers. Since 6G will be centered around the users, the mobile devices, network operators, and other service providers of the network shall also revolve around the users. The discrete arrows indicate that the mobile users (or IoT devices that need service) will directly be supported by the network provider, device manufacturer, and applications. However, all the remaining components of the ecosystem will coordinate the services. Such coordination, and interdependency/ interplay between each component of the ecosystem is shown by the bold arrows. FIGURE 2. 6G ecosystem. Show All Further, as shown in Fig. 2, security, and intelligence will be pivotal in the 6G ecosystem. The security, and AI as shown, shall be a part of every component such as at the device level, at the mobile operator, user, software, edge or cloud level, and at the applications. C. Vision and KPIs of 6G Vision and quest for 6G network: After discussing the technology gaps and recommendations for 6G networks, we highlight the crux of the vision and the need for 6G networks. There are several reasons we put forward, which necessitate the need for 6G networks. First, considering the broad scope of United Nation’s Sustainability Development Goals (SDGs) for 2030, there is need for several essential technologies that will satisfy the objectives, communication challenges, and new application requirements of the SDGs [44]. Second, 5G network has matured enough to address the performance requirements of the existing verticals. However, when these verticals or market demands grow to the next level, say Industry 5.0, Society 5.0, Transport 4.0, and many more that are human-centric, ubiquitous, fully automated, and driven by AI, will demand specialized resources which burden 5G networks [19]. Third, considering the business models for 2030 and beyond, the technologies like telepresence, holographic and haptic communications, Brain-Computer Interface (BCI), 4D imaging, Extended Reality (ER), and Internet of Everything (IoE) will drive the telecommunication industry [39]. The requirements of the aforementioned industries and applications cannot be met by 5G networks. Thus, leading to the contemplation of 6G networks as an ultimate ecosystem of communication technology solutions [34]–[37]. It should be noted that the 6G network is not just about exploring new frequency bands, architecture, and access schemes to support high data rates. Even though 5G network covers the requirements of future networks to a minimum extent, the 6G network must comply with the requirements such as extreme reliability, real-time ultra-low latency, and massive connectivity, simultaneously. The authors in [9] have categorized the 6G vision as four types, namely, intelligent, ubiquitous, deep, and holographic connectivity. In general, we suggest that as an ecosystem, 6G networks must envision to support the new application requirements of industry and society, especially with regards to the following seven metrics during 2030. ( i ) Data rate: This is the ability to provide quality service for verticals ranging from autonomous high precision industry to immersive virtual/mixed reality applications, (ii) Latency: This is the time-sensitive data delivery, (iii) Super Coverage: It means a three-dimensionally covering infrastructure that encompasses aerial, terrestrial, and underwater connectivity, (iv) Sense of Feel: This involves the support for holographic personal communication to tactile internet applications, ( v ) Extremely low power consumption: This stands for mobile devices that operate for a long time without requiring to charge, (vi) high network density: It means connecting of devices ranging up to 10 million/ sq.km, and (vii) high security with precise location information ranging from 10cm to 1cm [39], [53], [55]. TABLE 1 shows the KPIs that mandate the various verticals and use cases that will be supported by 6G. D. Use Cases of 6G Networks Let’s look at some of the prime use cases of 6G networks along with the KPI requirements, as shown in Fig 3. These use cases will require a combination of KPIs simultaneously to deliver their services. Hence, they are challenging. The following scenarios will be more realistic in 2030. FIGURE 3. Use cases of 6G. Show All 1) Ultra Smart Cities 5G networks will provide the user and application-specific QoS and quality of experience (QoE) through many verticals. For instance, telemedicine, smart agriculture, and smart industry can get the data service at specific data rate, latency, or priority. However, when we contemplate the future scenarios in an ultra-smart city, that may require, for example, a data rate of the order 1Tbps, 3D connectivity, localization within 1cm, and reliability of 99.99999999% for automated transportation, smart healthcare, or smart industry [46]. The KPIs needed for these applications in a smart city cannot be addressed by 5G networks [48], [108]. We shall consider a few more examples to visualize the scenarios. The mobility support requirements of 6G will typically vary from 240km/hr to 1200km/hr. A self-driving car needs to communicate with roadside sensors and other vehicles in the adjacent lanes to coordinate while moving at high speeds. Furthermore, the delivery drones on the fly may need to communicate with the ambulance on the ground to collect the medical supplies in the urban setting and transport it to remote locations. In another instance, drones may have to follow the cars to act as floating base stations or as relays to communicate the information as a part of inter-vehicular communication. It is worthy to note that the two scenarios involve radio communication between multiple entities under high mobility (say 500 km/hr). The 6G networks should support extremely low latency communication in the above scenarios, which is one of the key requirements of autonomous driving and decision making. These examples cover a few scenarios of future smart city and their communication requirements. But in general, high energy efficiency, high data rate, reliability, latency, precise sensing, and localization are essential for a smart city, as shown in Fig 3. 2) Multi-Dimensional Reality Human-computer interactions that deal with the ultra-high-definition graphical contents such as online games based on Augmented Reality (AR) or Virtual Reality (VR), generates a massive amount of data. We would soon witness 3D games or multi-dimensional video that interacts with all the five sense organs of the body to create an illusion of real-world by combining VR and AR to render a true virtual gaming experience. These applications will be tangible within a decade as the computer technology, computing power, and storage space of the mobile devices multifold in their capacity. Nevertheless, when these augmented data to be transmitted through a wireless channel, we need extreme bandwidth, reliability, and data rate that 6G networks will offer. In other words, we need ultra-high reliability, low latency, ultra-high data density, and user experience, as shown in Fig. 3. 3) Haptic Communication Let us consider a smart healthcare system where an injured patient can only express her emotions by visualizing in her mind. In that situation, a smart headband can reconstruct the brain signals and represent it as a 3D video of the patient’s imagination and communicate to the caregiver in real-time through mobile networks. In addition, a group of people who do not have a common language to communicate can use their imaginations, and disabled people get access to open the door, or to control the gadgets can use haptic communication. These haptic ways of communication will enable them to express the information by the sense of touch. It is one of the expected use cases of 6G networks, where the network supports high data rate which is much greater than 5G could support. The other scenarios of haptic communication include the brain-controlled computer interactions where people interact with their surroundings through haptics and control the environment through digital gadgets such as brain-embedded wireless chip that respond to human emotions [5], [105]. 4) Remote Surgeries and Telemedicine 5G networks can provide ultra-low latency of nearly 1 ms to critical applications. However, remote surgeries are extremely sensitive, with a latency requirement of much less than 1 ms (nearly 0.1ms) [149]. The remote robotic surgeries will require ultra-high precision and reliability of the data, high data rate to exchange data, and control signals between two remote health care facilities through the mobile network. The emergence of 6G networks will be a game-changer when telemedicine and remote healthcare will be taken into account by diminishing the space and time constraints [31]. Since 6G’s vision is to provide at least 99.99999% data reliability and a 1 Tbps data rate, it will be the most suitable candidate to meet the above requirements. The main concern in the case of remote surgeries is the latency requirements. The 6G should target both minimum and maximum latency requirements, unlike the previous generation networks. That is because, during the remote surgery, some aspects of the transmitted data must arrive at the destination within a specified maximum latency, whereas some events should follow minimum arrival latency. These minimum and maximum latencies together will render coherent data at the receiver, which is the key requisite for remote surgery [31]. 5) Holographic Communication With the maturity of AR/VR applications, we will soon realize that the virtual experience is not serving us all the aspects of reality, and we need more. Recently, due to the outbreak of COVID’19 pandemics, virtual presence (telepresence) has gained high prominence over real physical meetings. This kind of task requires advanced VR techniques, bandwidth, and computations to project an object or a person in real-time remotely. As a future trend, each mobile phone will be equipped with more than five ultra- high definition cameras to capture an event and render multiple dimensional videos that give an immersive experience of the event being captured to all the human senses. In other words, a video may be a multi-dimensional real-time projection with the audio-visual effect of the person or object being telecasted in a virtual meeting. For instance, videos of 16 K resolution, 240 Hz scanning rate, and spherical coverage (360°) need to be transmitted as a hologram for a fully immersive VR experience [46], [115]–[117]. To transmit these videos or holograms that require several Gbps data rates, the existing 5G’s KPIs such as data date, bandwidth, and reliability becomes insignificant [12], [39]. These transmissions involve a large volume of data. Besides, the angle of projection, response time are critical as well. For instance, if the audio or video response from the site of holographic projection has to be sent back to the source, then a precise and well-coordinated synchronization between the source and destination with respect to different layers of the image is essential. In another instance, consider a public concert where a remote artist can be rendered as a holographic presence (virtual presence) to entertain people from his local existence. Similarly, remote and hard to reach areas such as mines and deep ocean terminals will benefit from holographic applications where excavation activities and workforce training can be undertaken through holographic communication. 6) Tactile Internet In 6G networks, connectivity between various devices will be highly interactive in real-time (responsive), including the transfer of data, control, and feedback in real-time with a sense of touch [71]. Here one could transmit touch, feelings (sense), along with the information to give a live experience of the things virtually related to the information being communicated. To be specific, tactile internetworking involves a feeling of touch or taste along with an audio, video, or other forms of responses [9]. For instance, training the astronauts on space facilities, accessing underwater vessels/containers, and remote surgeries with the help of virtualized holographic models requires even a sense of touch to execute remote training and perform repairs with ultra-low latency. Furthermore, the transmission of smell and taste to enhance users’ experience will be a major target of food industry to digitize users’ experience of food access. As an example, while transmitting an advertisement of a particular type of food, its smell, texture, and taste shall be transmitted together with the help of advanced sensor technology and Apps to give a real feeling of the food. The 6G network will serve the need of these verticals due to its ultra-high data rate and low latency [18], [75], [88]. In summary, when we consider holographic tactile internet communication, different aspects of the hologram in the case of remote transmission may require varying amount of latencies (the minimum and maximum latency requirements), reliability, and user experience which need to meet by 6G networks to render a synchronized immersive effect to the human senses. SECTION III. Literature Review In this section, we will review the most recent literature that motivated our research. First, we categorize the publications based on the topic of research, and later, we present their contributions. Altogether, we have considered 158 recent articles, specifically on 6G networks and its components. For easy reference, we have listed them categorically in Table 2. 6G vision, technology, prospectus, and challenges: First, the vision statement of the 6G networks, as defined by all researchers, pointed out that 6G networks will be a breakthrough in transforming the smart cellular network into an intelligent network. These studies have anticipated the emergence of 6G networks by 2030 and have listed several interesting technologies, applications, and use cases that will benefit from the 6G networks [1]–[5], [9], [19], [31]. The technologies such as AI, Intelligent surface, holographic radio, blockchain, three-dimensional connectivity, cell-less architecture, quantum computing, and wireless power transfer will revolutionize future networks [19], [28], [31], [43]. Furthermore, the authors elaborated on the targets and features necessary to meet the requirements of the use cases. In general, most of the technology components of 6G networks are in the infancy stage. Therefore, there will be several challenges, such as training the AI models, security issues, lack of architecture, signal modeling, and computational facilities [96]–[98]. In addition, the authors have proposed several KPIs for future networks, compared them with 5G networks, and listed several technology gaps in the inception of 6G networks [35], [39], [40], [107]. Next, we review photonics and VLC. We noticed that VLC and photonics will be the primary components of 6G’s terra hertz communication [10]. Consequently, authors in [12], [17] have drawn a roadmap to VLC for its inception in 6G networks. Another key aspect of 6G network will be the AI and edge intelligence. AI will be used by 6G networks at all levels starting from the PHY for channel selection, MAC for achieving power efficiency, and at the application level for context awareness [13], [42], [47]. Moreover, AI will spread across various network entities in 6G ecosystem, such as the sensing, edge, and cloud devices, in a distributed way to manage the small data generated locally and big data to be processed centrally to minimize the latency to the minimal level [36], [60], [61], [72]. In addition, the authors at [13], [34], [42] have discussed various ML algorithms that would benefit from the 6G networks at the various operational levels of the network. During 2030 and later, 6G networks will open doors for several verticals and markets due to its performance metrics. These business models for selected verticals have been discussed in [16], [23], [24], [50]. Another technology component of 6G networks that will revolutionize wireless communication will be reconfigurable intelligent surfaces building using meta- materials or smart objects [29]. These surfaces will essentially direct the data toward the destination using their electromagnetic reflection property to achieve high-quality reception without any external power sources [59], [70]. Similarly, another category is machine type communication and ultra-dense IoT networks. The authors in [7] have exclusively discussed the machine type communication and its components for the 6G networks. The other category in our review is an interesting technology component, i.e., quantum communication (QC). In [71], the authors detailed a state of the art work on QC for 6G networks, including the scope of machine learning in QC. Furthermore, we have discussed, all the architectures and their key features, the issues involved with the digital divide, and rural connectivity, the scope of business models and THz communication, cell-less architecture, ecosystem and its components, etc. Please see Table 2, for a detailed discussion on the various literature on different categories of 6G technology, and their contributions. SECTION IV. 6G Enabling Technologies In this section, we will discuss the prime features, their types, need, challenges, etc., of various technologies that enable 6G communication. In Fig. 4, we have depicted these technologies. We reckon that 6G will integrate three aspects namely physical, biological, and digital world as shown in Fig. 4. This intuition shall be elaborated as in the case of 6G networks along with the typical radio frequency communication, it will include robots, digital twins, artificial intelligence, emotion-driven devices, smart communicating surfaces, communication through brain implanted chips or brain-machine interface [136] to enable all-round cyber-physical-biological communication experience. Consequently, 6G will be much more than the present smart connected networks, where the network components will largely integrate intelligence to bring in a paradigm shift from smart to intelligent network as shown in Fig. 4. Further, we have discussed eight enabling technologies for 6G namely, artificial intelligence, THz communication, 3D connectivity, Visible light communication, blockchain, quantum communication, intelligent surfaces, and digital twins. FIGURE 4. Technology enablers of 6G. Show All A. Artificial Intelligence Why AI for 6G: In 5G networks, the network orchestration functions lead to a flexible network slicing feature. Consequently, the specific requirements of most of the verticals could be met without depending on AI to a greater extent. However, we address the applications that are beyond the service limits of 5G networks in 6G networks. We reckon that due to the following reasons, AI is essential for the 6G networks [60]–[61]. ( i ) We anticipate that in 6G networks, there will be a plethora of heterogeneous network components that interconnect via multiple numerologies (3D connectivity) to serve diversified verticals, process a large amount of data, and they demand varying levels of QoS. Addressing these tasks require efficient analysis, optimization, and decision skills. Therefore, at every level of the communication system, say user terminal nodes to edge processor, and core network, intelligence has been deeply embedded and integrated to offer end-to-end services. In addition, in such contexts, relying on existing architecture or time-consuming mathematical models for optimization of performance metrics may not lead to feasible solutions [7], [56]. Moreover, these user applications give the idea that the 6G network is much complex in terms of network structure and dynamics than the earlier generations. Consequently, it requires assistance from smart, adaptive, and intelligent AI agents to self-learn from the network inputs to adjust the offered services with dynamism and optimize. Generally, this scenario suggests that 6G architecture must target automation through AI. Precisely, those tasks such as resource allocation, reaching the targeted KPIs, mobility management and handover, policy and billing, services for various verticals, orchestration, and quality of service ought to be AI-driven by considering the volume, the heterogeneity of data, and its analysis to improve the performance [2]. Thus, making the network self-operative, manageable, and self-sustained under any network conditions [36], [61]. (ii) Similarly, at the physical layer, sensing and detection from sensors require the spectrum sensing to address spectrum scarcity problem, whereas interference detection requires a dynamic and massive volume of data collection. In this context, AI techniques such as Support Vector Machine (SVM) for real-time spectrum sensing and Convolutional Neural Network (CNN) for cooperative sensing would be proved to be effective solutions [36]. In addition, when we consider physical layer modeling in the presence of channel non-linearity, AI methods perform better than simple mathematical models, which may have unacceptable time complexity. For instance, Deep Neural Network (DNN) and CNN-based supervised algorithms have been proved to better than traditional methods [61]. 1) Current Trends in AI There are different types of AI systems (say ML) for wireless networking, which seems promising for 6G networks, namely, supervised, unsupervised, and reinforcement learning. Currently, a large body of research focuses on the use of AI and its derivatives, such as deep learning and machine learning to optimize the wireless networks’ performance of [13], [34], [72]. AI will be pivotal in Radio Access Network (RAN) and the virtualized core network of 6G networks, including edge computing, resource allocation, slicing, control, and applications. However, how exactly AI would support the core 6G networks has to be realized in the future as a finalized architecture for 6G networks is yet to be devised. Tariq et al. [3] has opined that AI will be pervasive in distributed 6G architecture, where various network components would adopt federated learning. AI will be operating in distributed training agents at different network entities that will support individuals and the overall network’s benefit by a collective operation. The AI scenarios, specifically deep learning algorithms, will suit the analysis of big data generated by several IoT nodes when processed centrally in the network. In addition, to enhance network performance, the wearable sensors will incorporate the computing and data aggregation features locally along with sensing and communication. That means 6G networks necessitate another context, where data processing is done locally at the edge/fog devices in a distributed way for delay-sensitive applications, which could be done using reinforcement techniques [5], [42]. The authors in [60] have endorsed that locally generated data by various sub-networks of the 6G network might be processed locally using AI techniques to attain efficiency in terms of reduced latency and overhead. Furthermore, AI could be integrated into the radio access network to infuse cognitive capabilities for efficient channel selection from the physical channels such as THz band, visible light communication, Wi-Fi, satellite link, transmission power, and modulation scheme selections. AI at the application layer can manage various smart processes such as automated driving, smart healthcare, and improved performance by suitable data learning methods. On the same note, the study in [4] has identified three categories of AI systems for 6G networks. (i) AI at the network edge supports low latency applications with real-time data processing capability instead of a cloud-based centralized AI system. (ii) AI-enabled radio that decouples physical hardware (i.e., transceiver) from the software with control functions to enable AI-driven dynamic and intelligent decision making using the data received from the transceiver. For instance, interference avoidance, cell selection, channel estimation, etc. (iii) Distributed AI, where each network entity will be capable of running the AI algorithms in parallel, using the local data in a distributed manner before being sent to the centralized cloud [43], [102]. 2) AI Architectures 6G In the literature, there exist two AI models for 6G network architecture. We will review both of them briefly in the following paragraph. AI Model 1: Let us consider how AI finds its place at different levels of a network. An AI-based layered architecture for 6G networks, as defined in [36], has the following realizations. Layer 1: This first layer occurs at the RAN, where several sensors such as robots, cameras, mobile devices, and connected vehicles sense real-time data and communicate to the Access Point (AP). In this context, efficient environmental data collection, intelligent channel estimation and interference management (including the scenario of cognitive radio), ultra-low latency, and high reliability shall be possible by enabling AI techniques in these devices. Otherwise, data collection in a rapidly varying environment and channel selection during multiple devices’ simultaneous operation will pose challenges. For efficient real-time sensing, SVM, DNN, and CNN methods are recommended. Layer 2: Next, as the data arrives at the Access Point (AP) level from layer 1, the massive data collected from the sensors require analysis (temporal or special), processing (to reduce the redundancy), computing, and storage. In particular, data analysis and mining filter the redundant content to tailor the data set to a reasonable quantity. At this point, well-known AI techniques such as Principal Component Analysis (PCA) and Isometric Mapping (ISOMAP) can now be employed to compress the raw data. With intelligent data analytics, critical network data patterns can be studied to improve the overall performance. Layer 3: In the third level, data from the AP, or e-NB is sent to cloud computing and storage. These clouds or edge computing facilities will be NFV and SDN controlled virtualized network entities. At this level, the controllers equipped with intelligent agents will make decisions regarding learning and decision making by the knowledge obtained from layers below it. It includes resource allocation functions, power control by considering the network slicing requirements of various applications. Further, AI agents control handover and optimize the physical layer parameters such as deciding the precoding matrix, rank index in massive MIMO transmissions, and edge computing by learning from the knowledge obtained from the previous lower levels to meet the high-quality service requirements. The main AI methods for efficient decision making regarding optimal parameter selection in massive MIMO or smart reflecting surface (polarization, precoding matrix, phase, and rank indicator, etc.,) in 6G network can be performed through the Markov decision process, reinforcement learning methods. Layer 4: Finally, at the application level, AI will assist in delivering specific services based on the needs of each application or verticals. For instance, data service for the 6G eMBB user and ultra-low latency for robotic-assisted smart healthcare can use AI techniques to automate the network functionality, self-management, and self-organizing features [36]. AI Model II: Now, let us see the O-RAN Alliance, which is an Open RAN alliance formed by five global telecom operators for B5G networks, though not specifically for 6G [61]. Albeit, it provides a framework for integrating AI into the future cellular network. We summarize the key points as follows. Here RAN is coupled with AI features to handle non-real-time (latency less than 1 s) and near-real-time (latency 10 ms–1 s), and real-time services. This architecture has a multi-radio protocol stack to support heterogeneous radio interfaces. i) At the physical layer, deploying AI will enhance channel estimation for MIMO, receiver symbol detection without much regard to channel state information (CSI), and channel decoding. As discussed earlier, AI techniques such as learning-based DNN for channel estimation, and detection will prove fruitful. ii) Further, at the MAC layer, spectrum access in the outset of multiple radio interfaces will be the key requirement. However, employing spectrum access schemes for individual access technologies (as in existing practice) will prove inefficient as 6G networks will have heterogeneous access technologies. Therefore, to facilitate distributed channel access for multiple radio access technologies, learning based dynamic spectrum access technologies, such as deep reinforcement learning-based distributed spectrum access method shall be adopted [61]. This architecture defines two interfaces, namely, A1 and E1 to support non-real-time and near-real-time access layer, respectively. iii) Similarly, at the network layer in 6G network, the resource allocation and management tasks are herculean task by traditional methods due to the high density of connected devices. Therefore, it is recommended to use methods such as AI-driven trouble shooting, knowledge data for fault recovery, and root cause fault analysis by an AI model. Second, the management of virtualized network functions at different virtual networks, tuning the MIMO parameters for setting the beam width during mobility can be achieved with reinforcement learning algorithms [61]. Additionally, the authors in [60], have proposed an intelligent radio for 6G networks that enables the future upgrade of radio receiver, introducing an operating system between transceiver hardware and its software. The operating system will be capable of reconfiguring its algorithm to improve the performance by sensing the transceiver parameters and other inputs with the help of an interface. The DNN-based implementation of the intelligent radio will be low cost, and flexible. B. THz Communication Spectrum The 6G communication will span across microwave (300 MHz–300 GHz), infrared (300 GHz–400 THz), and visible light (400–800 THz) frequencies, ranging from long distance to short distance coverage, while coexisting with the previous generations. However, 300 GHz–3 THz expanding up to 10 THz which is popularly known as Terahertz (THz) spectrum is a new frequency band for cellular communication, which 6G networks will exclusively target to offer extremely high data rates [4], [75]. 1) Need of THz Spectrum Spectrum is the fundamental and sparsely available resource for wireless communication. From the discussion of 6G’s plausible use cases, and expected KPIs, it directly boils down to the point of bandwidth and spectrum resources to support these data applications. The 5G network will explore sub-GHz, and frequencies up to 30–300 GHz, (with a maximum bandwidth of 400 MHz) to offer millimeter wave communications. However, the offered bandwidth may not suffice to meet the services that have dual requirements like ultra-low latency combined with high data rate or vice versa, because that strains the bandwidth [10], [57]. Anyway, 6G network being the successor of 5G network, will continue to use the microwave, mm-wave frequencies along with the future THz frequency bands, where tens of GHz chunk of frequency (spectrum) are available [9]. Recently, along the same lines, 6G research has been triggered by the US Federal Communications Commission (FCC) by announcing an experimental license for 95 GHz to 3 THz spectra, along with granting 21.2 GHz of spectrum for unlicensed communication [24]. This major step will motivate many researchers to explore THz band in-depth. The need for a new spectrum arose as the lower GHz band will be much occupied (5G family). As a result, providing a large chunk of frequencies to next-generation networks to support high data rate will not be feasible. Therefore, ITU-R has foreseen the need for an exclusive spectrum for B5G networks and has recommended 275 GHz – 3 THz frequencies that include the microwave and optical frequency bands. This Terahertz frequency band will offer a nearly 60 GHz unoccupied spectrum, sufficient to provide a Tbps data rate [3]. 2) Opportunities THz communication offers several opportunities due to its unique properties. As a result, 6G networks will prefer it for short-range high-speed communication and space communication between satellite and ground stations. Meanwhile, the availability of wider bandwidth in THz frequencies makes the high data rate feasible. Next, high-frequency communication leads to miniaturization of the circuit (device). As a result, multiple antennas, an array of antennas can be accommodated into the user devices. This facilitates the use of efficient multi-antenna MIMO schemes, beamforming, and interference suppression to enhance the quality of communication [107]. Furthermore, these high-frequency signals will follow the line of sight (LoS), and are highly directional, resulting in less susceptible to jamming and are secure. In addition, the frequencies above 100 GHz are least affected by rain/moisture. The THz range will give rise to extremely tiny cells (say 10’s of meters) to support a high data rate mobile broadband communication in local space for special applications in the future. Short wavelength and pulses with well resolute time make it an ideal candidate for super-precise positioning [24]. With regard to the space communication, certain wavelengths like 600–870 micrometer are well suited for long-range communication at low noise and low power [111], [120], [122]. 3) Challenges THz waves will significantly affect large-scale fading and shadowing. It has been reported that signals below 100 GHz face large attenuation due to moisture in the air [120125]. Further, it has a relatively higher free space loss compared to lower frequencies. At 1 THz, the radio signal will experience high absorption from water molecules, oxygen in the air [28]. In addition, shadowing will immensely influence propagation. A study says the human body may attenuate the signal by 20–35 dB [9]. Another challenge will be at high frequencies design and fabrication of transceivers for mobile devices is extremely complicated [127]. Besides, the power required to handle the high frequency processing such as Analog to Digital Conversion (ADC) for sampling is yet to be known [28], [132]. There is currently a lack of study on the channel characterization of THz frequency signals, the support for inter-frequency mobility, handover and the physical layer protocols [5]. C. Ubiquitous 3D Communication One of the significant requirements of the 6G networks, as different from the previous generations is to have all round global connectivity such as high altitude, underwater, and terrestrial connectivity to adeptly accommodate a wide range of verticals. With this vision, 6G networks target to achieve extended and continuous communication between humans and smart things, machine-machine such as underwater vehicles, UAVs, or spacecraft, and robots [75], [121]. In 2030, communication between terrestrial, aircraft and satellites, and ships will become more obvious [9]. Let us see the scenario more in detail. Refer Fig. 5 where we have pictorially represented the scenario. Concerning the network densification, 6G networks is anticipated to see an un-presidential increase in the number of user terminals, where users will be capable of networking with other terminals either in the same level (terrestrial-terrestrial) or at different levels (terrestrial-aerial, terrestrial-underwater, or aerial-underwater) via multiple radio access networks. Furthermore, it will be a common scenario where a user will have network connectivity with multiple cells with resources being shared among those cells by mutual coordination. As a result, the concept of a cell will diminish i.e., the cell-less architecture will emerge to have infinite freedom with multiple RAN connectivity feature to endorse all round connectivity [134]. The emergence of cell-free in 6G networks will bring in the need of heterogeneous technologies such as satellite, UAV, submarine, and deep-sea connectivity to coexist and integrate with terrestrial THz RAN to enhance the data rate, coverage and seamless, flexible connectivity aspects. The proposed feature shall address the issues of present networks such as delayed handovers, mobility, data loss, etc., to offer better quality of experience to the users [31], [142], [124]. In the following paragraph, we will visit the various components that constitutes 3D networking. FIGURE 5. 3D communication scenario in 6G. Show All 1) Space Communication The three main patterns of space communication consist of ( i ) communication between the earth satellite transponders and low earth orbit (LEO) satellites, (ii) 6G cellular base station, cellular users, and UAVs, and (iii) UAVs, aeroplanes, and 6G base stations. It is a well-known fact that the existing communication satellites mainly belong to the Geo Stationary Orbits (GEO) that impose large delay for terrestrial mobile communications. Consequently, theoretical studies in the literature have envisioned mobile broadband amalgamation with LEO satellites [121]. It has been reported that several countries like US, UK, and China have planned their satellite missions. For instance, China has planned to launch nine such LEO satellites (Hongyan) by 2025, into its cluster of 320 satellites [4]. These satellites may provide direct coverage to the terrestrial station (transponders) or to ships. This is the first pattern of space communication. These LEO satellites will offer extended coverage, high-speed data, low latency communication support to terrestrial communication in conjunction with GEO satellites that acts as backbone network [22], [149]. 2) UAV Based Communication Recently, UAV’s have been extensively used as relay nodes to extend cellular coverage. At a little lower altitude than satellites, UAVs functions as either mobile base stations while being afloat in space to cover a large footprint of terrestrial mobile users or act as relays (hotspots) to extend the coverage of base stations where the mobile infrastructure is hard to setup and thereby serve a greater number of users [18]. One of the foreseen applications of UAV communication in 6G networks will be to provide rural connectivity at a reduced cost [76], [151]. 3) Features of UAV Communication Easy deployment and network setup are critical features of UAV to enable flexible range extension or coverage enhancement of wireless communication. For instance, during a calamity, hurricane, tsunami, desert, where the existing infrastructure has damaged or does not exist, the UAV may function as a floating base station. However, there are challenges when 2D communication has to be extended to 3D due to the altitude and added degree of freedom [31]. The aerial channel modeling study should attain maturity before UAV can be integrated as a part of 6G [9], [113]. Further, optimization of route, power, number of UAV’s to provide coverage requires special attention as these devices have power constraints [76]. 4) Underwater Communication Let us consider a scenario of deep-sea exploration, for instance, the study of underwater habitats, deep water ecosystems, natural resources, or recreational tour, and military communication; all these activities involve the transmission of video, data to a ground station. Furthermore, the communication during any rescue operation through a submarine or UAV, far from the coast are challenging due to an unpredictable underwater environment, where RF, visible light, and acoustic signals should provide communication. Later, it has to be integrated into the terrestrial or at times to space communication [112]. Incorporating this challenge as a part of 6G network with different frequencies to communicate for underwater (low frequency) and terrestrial will meet the key requirement of the future networks. The acoustic communication mainly spans between acoustic waves, RF, and optical wireless frequencies; a thorough channel modeling for all three cases is an intricate task [127], [9]. In general, to provide 3D coverage, 6G network will virtualize its hardware access (i.e., physical and medium-access) to enable the network service for more users and verticals. D. Visible Light Communication In earlier telecom networks, connectivity between cell tower and mobile switching centers were either microwave (RF) or fiber optic links (non-RF). However, since 4G onwards, free-space optical (FSO) links have started to attain prominence to connect the backhaul due to its simplicity, license-free operation, high data rates, and security. Due to these features, VLC will be on the main contender for backhaul networking in 6G network, along with RF links [2]. In dense 6G networks, a very high bandwidth technology will be mandatory at the backhaul to manage the data due to the massive amount of data generation. In this regard, the VLC and THz communications will be the two main candidates for 6G cellular communication at the physical layer [31]. The VLC as a complementary technology to RF occupies the higher scale of THz frequency ranging from 400 THz–800 THz. Note that VLC offers hundreds of THz bandwidth (unlicensed), meanwhile, the THz frequency communication has only up to a hundred THz bandwidth (licensed) available [4]. Even though 5G networks did not consider VLC in its ecosystem, VLC deployments for short-range communications offer several Gbps data rate [9]. Thus, in the next decade when LED technologies mature, especially with the advent of micro-LED sources, and laser diodes, 6G’s data rate requirements can be met. VLC as a family of FSO has several merits over RF communication. First, the optical spectrum is license free. Second, the VLC in the frequency range 430–790 THz uses white light LED as a source to transmit the encoded data. The data encoding is much simplified in VLC, where by modulating the brightness of the light emitted by LED, different encoding levels can be achieved easily. Third, it is anticipated that VLC link will offer several THz bandwidths and up 100 Gbps data rate, and may grow to Tbps in future [4]. As a result, the use of multi transmission links can offer enough data rate for 6G use cases [3]. In particular, laser diode which has higher efficiency and illumination than LEDs, can offer a 100 Gbps of data rate for critical applications of the 6G networks [28]. In general, even indoor communications remain secure within a confined space as light sources cannot pass through the obstacles [17]. VLC will be the most suitable candidate for the line of sight inter-UAV communication in 6G networks. Since the communication system is simple and flexible, two or more aerial vehicles can communicate through optical frequencies generated by the LED sources without being distracted by the obstacles above the ground level. In addition, the interference issues in VLC are also minimal. As a result, underwater communications between submarines, personal healthcare devices, or industries susceptible to electromagnetic radiation are the other scenarios that will find VLC the most suitable [9]. Nevertheless, VLC is challenging in the outdoor setting, as external light (ex: sunlight) will influence the communication beam. So, aerial communication requires a very strong light beam to overcome this limitation. Thus, VLC is more suitable for indoor short-range communication without this constraint. E. Blockchain Security Blockchain is a distributed ledger that will play a pivot role in the maintenance of data security and transparency when numerous devices share the data in a decentralized way in B5G networks. 1) Why Blockchain for 6G We know that 6G networks will have a massive network of IoT and MTC devices that connect homes, cities, and factories with several data transactions between the networked entities. Moreover, a survey predicts that there will be 50 billion connected devices IoE during the 6G network era [3]. In that context, there should have trust among the devices to facilitate secure data transfer. To build such trust and security, the blockchain allows maintaining a sequential ledger (chain of blocks) by each node wherein if any user does the modification of data within a block, it will be visible to and shall be authenticated by everyone else to prevent falsification. 2) Features of Blockchain These modifications are imprinted into the data block with a private hash key, and data remains unaltered without the consent of the other devices [2]. That means, when a data block is created, it must be verified with respect to the previous blocks by the other participating users to maintain transparency. In this way, the reliability factor is introduced among all users; however, with the compromise of privacy, as modifications will be visible to all [14]. The decentralized operation of blockchain brings flexibility with reduced management cost. The blockchain even offers several advantages in a highly connected mesh network, such as security, reliability, trust, and scalability [2]. One of such instances of blockchain in 6G networks could be a video conference call, or mixed reality-based video streaming, where the network along with providing connectivity, also requires all the parties to authenticate themselves and the data being communicated through dedicated blockchains. This will bring in the knowledge of data properties to analyze the abnormal behaviors in real-time [42]. However, there are other challenges in 6G networks besides the massive device connections for blockchain implementation, such as resource restrictions on the devices limit the scope of use of cryptographic security algorithms on the device. Similarly, data packets must undergo an intensive audit to evaluate the risk, which becomes a herculean task when the number of devices is very big. Due to the lack of third party verification, security attacks are more accessible by compromising half of the total participants. The authors in [14] have suggested using cryptographic and quantum computing based algorithms for privacy issues in blockchain. Altogether, the blockchain has a major role contributing to security, scalability, and reliability factors in the 6G network [30]. In the future, for 6G, blockchain shall provide security to mobile edge computing nodes when several devices wish to store their data in the edge device. Similarly, in case of device-to-device communication, the cooperative data caching among the users also shall use distributed security. In addition, when we consider network virtualization, it is obvious that the network capacity will enhance due to provisioning more slices. In this regard, while implementing blockchain, it will provide authenticity of data in every slices along with immutability features to efficiently manage the virtual network [33], [14]. F. Quantum Communication (QC) It is one of the enablers of the 6G network; especially, it will support 6G network in achieving ( i ) extremely high data rate requirements at the backhaul, (ii) data security, and (iii) long-range transmissions [3]–[5]. In general, the existing protocols of mobile networks can be significantly enhanced by utilizing the principles of quantum theories to attain higher degrees of freedom [71], [18]. Foreseen by the role of QC in the 6G and future networks, the Government of UK, and New York University have invested heavily on the research of quantum communication and computing [78]. In the case of QC, data that encoded using photons which cannot be decoded or copied (cloned) without tampering as quantum particles (photons) will be highly intertwined and correlated. In addition, data will be represented as ‘qubits’ a unique notion of multi-level description of data, where qubit is the fundamental unit of quantum data [4]. Therefore, the underlying principle of QC, includes quantum superposition, quantum entanglement (no-cloning) theorem [71]. However, when we move our attention toward data rate improvement in QC, we use the concept of a quantum superposition of qubits. Basically, it does the parallel processing of multi-dimensional large-sized data to offer seamless data rate, unlike the binary data processing in conventional computing systems. For instance, a qubit can represent binary ‘0’ and ‘1’ simultaneously. In addition, n bits can represent 2 n bit patterns at the same time, instead of anyone combination of n bits as in digital communication. In the case of QC, communication security is another key feature to mention by randomization. The communication is impossible for eavesdropping as the data will be represented in an encoded quantum state using photons and cannot be meddled without the intertwined patterns. Here, QC makes use of quantum entanglement, or the no-cloning principle [3]. One of the future applications of QC in security is the quantum key distribution (QKD). This application provides vast security to physical layer due to quantum mechanics based key distribution in future networks [19]. Furthermore, long range communication is supported with the aid of quantum repeaters (relay) that retransmits the photons. It has been reported that QC can be integrated with satellite communication to offer global coverage [4]. Meanwhile, quantum repeaters and switches are difficult to build due to no-cloning theorem [71]. An interesting paradigm for the future networks will be combining QC with ML to solve classification and regression problems more efficiently than the classical ML solutions, which is termed quantum machine learning [40]. Here, quantum supervised and unsupervised learning, quantum deep learning, and quantum reinforcement learning techniques would satisfy the complex computation requirements of 6G networks [71]. All these features make QC a suitable candidate for 6G networks. G. Ultra-Massive MIMO and Intelligent Communication Surfaces Due to the availability of large bandwidth in THz frequency, by the use of MIMO transmission technique, large spectral efficiency can be achieved. MIMO technique which exploits the spatial diversity of the communication medium several antennas at the transmitter and/or the receiver, and offers enhanced spectral efficiency and gain with the given channel bandwidth [9]. For instance, in a 64-beam array plane, there are 64 unique predetermined angular directions. During transmission to a user, a single angular beam will be chosen to expect each user to be well apart from the rest and should have a line of sight path [48]. This limitation of 5G massive MIMO should be overcome in 6G networks, where each antenna array creates a beam in multiple directional vectors with each beam having different angular directivity [48]. The massive MIMO transmission will enable the receiver to overcome the degradation effect on the THz waves caused due to rain and atmosphere in dense urban settings [9]. However, looking at challenges, such as energy efficiency of massive MIMO and inter-cell interference, efficient solutions are yet to be determined [28], [40]. In the context of 6G networks, it is more appropriate to use the term ultra-massive MIMO. To fill the gap concerning the physical layer, the combination of THz and several antenna arrays (massive MIMO) shall be used in 6G networks to support user’s mirage of data hungry applications. One of such use cases will be beamforming, where multiple antennas are pointed toward a user with the help of directional beams for complete utilization of spatial dimensions. Therefore, 6G networks must target physically large panels accommodating more antennas, and narrow the beam width (increased spatial resolution) to increase the beamforming gain. The efficiency of beamforming can be achieved using continuous aperture antennas where each antenna is discretely spaced. This increases the cost and power consumption. In this context, holography radio is a solution that uses meta-materials on which antennas can be densely spaced on a small area [48]. In 6G, we will overcome the limitations of 5G networks by using large antenna reflecting surfaces and holographic communication, where holographic beamforming with the help of Smart antennas renders holographic 3D videos [28]. Unlike the 5G massive MIMO, 6G networks would use smart intelligent surfaces (IRS) which reflect the data received from a base station toward the receiver [59]. 1) Intelligent Communication Smart Surfaces The existing massive MIMO technologies for 5G networks provide spatially, efficient ways of enhancing communication performance. However, the complex signal processing, increased power consumption, hardware design, and THz wave’s propagation properties (ex: blockage due to obstacles) pose a real challenge. Furthermore, at THz frequencies, due to low scattering very few scattering paths exist. Thus, using antenna arrays becomes a challenge due to the shrinking size of the antenna elements. Therefore, an alternative to ultra-massive MIMO is essential. As we know, the hardware components will be driven by intelligence to adapt to the changes in the surrounding environment in the 6G network. For instance, the performance of the cognitive spectrum access, modulation, and coding, beamforming can be modified as the environment changes [70]. One such intelligent component that enables wireless communication is an Intelligent surface. The intelligent surface is also known as Reconfigurable Intelligent Surface (RIS). An intelligent surface will act as a boon to enhance overall communication between transmitter and receiver in a cost effective and energy-efficient manner [28]. An intelligent surface is a passive reflective surface of electromagnetic signals without requiring a dedicated power source. These are generally made of reflective arrays, liquid crystals, or software defined meta-surfaces. These surfaces will manipulate and reflect the incident RF signal from different sources and direct them toward the receiver to assist in wireless communication [40], [48]. The modification of the incident RF signals is done by programming these meta-material surfaces, similar to a software defined radio. Therefore, the RIS is also denoted as software-defined surface (SDS) [59], [119]. 2) Features of Intelligent Surfaces Since these surfaces are passive and do not use power amplifiers or analog for the digital converter, they prove to be power efficient than massive MIMO technology as the latter uses active elements to re-transmit the signal. Further, the amount of radio wave reflection depends on the surfaces area of these intelligent surface. Another key property of the intelligent surface is that they intelligently reconfigure the modulation or phase of the reflected signal with respect to the channel condition such as fading and path loss. Further, the intelligent surface can aggrandize beamforming of massive MIMO in the 6G networks due to the software programmable feature [28]. With the help of reflecting surface elements, it is easy to mimic the effect of beamforming at a reduced complexity. Studies have investigated the potential of intelligent reflecting surface for index modulation for 6G networks [59]. 3) Holographic MIMO Holographic MIMO is an interesting paradigm of intelligent surface enabled communication [70]. Here, a large intelligent surface can be configured to act either as transmitter, or as receiver using the principle of optical holography. When there is a power source used for the operation, i.e., in the active mode of operation it is explicitly termed as H-MIMO surface. However, in passive mode (no power source) it is majorly a reflecting surface that modifies and reflects the incident RF signal as a intelligent reflecting surface [70], [43]. Now, let us consider the challenges associated with the Intelligent surfaces: ( i ) In a wireless communication system assisted by RIS, the surface should acquire enough CSI feedback from the transmitter and the receiver. However, acquiring the CSI is a burden for passive devices which cannot transmit the pilot signals for channel estimation, as in the conventional systems. (ii) The passive surface needs to transfer information, such as control signals to synchronize with the transceiver and real-time environmental parameters. (iii) Resource allocation issues such as lack of analytical models and computation cost [29]. These H-MIMO surfaces have a benefit of reusability, easy customization, and lower latency due to software programmability of the meta materials. In passive modes the thermal noise is absent. It is also spectrum efficient. Furthermore, intelligent surfaces will act as range extenders when there is no direct link from the transmitter (gNB) to the user. Due to simplified beamforming, the RF signals can be focused to the desired location that reduces the change of eavesdropping, loss of signal due to attenuation, and facilitates wireless power transfer along with the information to the intended receiver [70]. In 6G network, RIS will find its applications apart from physical layer communication, as RIS enabled Edge computing, wireless power transfer, device-to-device communication, and positioning and localization [2991]. H. Digital Twins A digital twin is a digital representation of a real world object, person, place, or event projected virtually in a cyber-physical world, without regard to time or space restrictions [90]. It is expected that with the digital twins, we will experience the reality (i.e., of the original object) virtually (as a twin) [86]. The advancements in sensor technology, AI, and communication has led to several applications of digital twins. To render the twin of an object digitally at the remote site end, the system must process a high definition data of the original object, analyse, and decode to reproduce it virtually. In this context, AI’s supervised or unsupervised algorithms will be much suitable as they can accurately analyse the data from the surroundings. Digital twins have several use cases, as mentioned in [89], [90]. For instance, in Industry 4.0, a digital twin of a machine will help the operator to predict future failures or malfunctions much earlier by analyzing the twin data as there will be continuous feedback between the original machine and the twin for better performance analysis. A similar intension has been exploited by the automotive giant Tesla company in their digital twin of the cars. Another foreseen application will be for e-healthcare. With the aid of the digital twin of a human, it will be easy to analyze the body parameters in real-time or shall study the impact of a drug, pre-surgical study of the area to be operated [89]. Samsung has expressed that digital twins will be one of the technology components that will drive 6G networks [86]. In [15], authors have proposed the digital twin model for the 6G network architecture. Since 6G networks will have extremely high network density, the network will have distributed its architecture for better management. These digital twins (a.k.a. cyber twin) can become part of the network by acting as assistant, data logger, and digital asset owner of the objects. In this scenario, the 6G’s network architecture shall consist of four entities, namely, (i) end entities (human, things) who receive the service, (ii) edge to cloud entity that connects the end-user to the core cloud through the edge devices, and does initial edge processing. (iii) Cyber twins located at the edge server which are the replica of an end-user that provides services such as communication, data logging, and even act as the owner of assets. It will replace the end-to-end communication model with an end-to-cloud model. (iv) The cloud network where edge clouds and different other clouds will be interconnected. This architecture will provide scalability, security, and flexibility to future generation networks. 1) Challenges To replicate an object or human in real-time requires extremely high data rates and perfect synchronization (say 1Tbps and 100 ms). Besides, there are issues such as privacy, security, ethical issues, and cost of development. The technical community is hopeful that by 2030, digital twins will very well support 6G networks when there will be more advancements in technology [90]. I. Challenges of 6G Enablers Even though 6G enablers will enhance the performance of 6G networks several times more than the 5G networks, the path to achieve the gain is arduous. In this subsection, we describe the various challenges associated with the 6G network technology components, as given in Table 3. TABLE 3 Technological Challenges of 6G SECTION V. Architectures for 6G While roadmap, and standardization activities are yet to be initiated, a few researchers have proposed new architectures of 6G networks. In this subsection, we describe briefly various architectures from the literature. In the earlier section (IV), we narrated the critical features of two AI-based 6G architectures. Now, let us discuss the other architectures. A. Cyber-Twin Architecture The authors proposed two architectures for 6G networks, namely, (i) cloud-centric internet model and (ii) a decoupled RAN model [104]. In the cloud-centric internet model, the existing IP architecture is slightly retained with certain modifications. First, the users are connected to the RAN, and the data from the RAN enters the edge cloud layer where cyber-twins accept the data. These cyber-twins act as data loggers, assent owners, or virtual representation of the user. Edge clouds, in turn, are connected to the cloud layer, where multiple clouds are interconnected to form the center of the network architecture. The cloud layer consists of resource scheduler, orchestrator, communication, computing, and caching functions. This cloud layer enables applications to provide services in the edge and cloud at reasonable cost and QoS. B. Decoupled RAN Architecture Here are distinct APs for handling the control and user plane data different from the earlier cellular generations. The control plane BS will be a macro BS, which a user shall connect to exchange the control information. These control BSs will be connected to the user plane (data) BSs for high-level signaling. In addition, the uplink and downlink traffic are also decoupled and handled by separate BSs to the users. Here, the uplink BS can be a micro-cell dense deployment near the user to collect the user data at low power. The uplink and downlink base stations will have internal coordination for efficient communication [104], [105]. C. Generalized Architecture for 6G A generalized 6G architecture for IoT and vehicular networks is presented in [139]. It consists of three levels, namely, user level composed of smart devices with caching ability. These devices send the sensed data to the base station level, where base stations or APs have edge servers to perform scheduling and resource allocation. Finally, a central server will do the slicing, handover actions at the network level. This architecture will reduce the delay for critical services (autonomous cars) by processing and storing at the edge level. However, due to its partially centralized control plane, some non-critical tasks still use cloud processing. D. A High-Level Architecture of 6G A three-level 6G network architecture consists of AI plane, user plane, and control plane [140]. In this case, storage, compute, and networking are done at the same level (in user plane) to eliminate the hierarchy. The user plane is flat and it is defined between an access network and the internet. The control plane and AI planes are distributed and virtualized for various services. Further, the transport network is virtualized and isolated from the rest by software defined virtualization. The core network functions are made as micro-services and accessible by server-less systems. E. Multi-Level Architecture for 6G A three tier architecture of 6G consists of smart users, edge devices, and the cloud [114]. The users implement intelligent decision making techniques such as data-driven or model-based techniques to predict mobility and traffic patterns. At edge intelligence level, the mobile edge devices use deep reinforcement learning, or deep neural networks to optimize the scheduler for resource allocation to mobile users based on the CSI. Moreover, at the cloud level, having a high capacity central control can train the system with a numerical platform that presents the labeled samples for optimization algorithms. These can be used to train deep neural networks and later be implemented in the control plane. F. Three-Dimensional 6G Architecture [4] The FG-NET 2030 has envisioned a 3D architecture for 6G networks, covering three key aspects, namely, communication (infrastructure view), intelligence (control view), and management (network view). It has different communication layers at the infrastructure view ranging from terrestrial, underwater, aerial, and satellite to enhance the range of communication in 6G networks. In addition, the authors at the control view presented how 6G networks will include intelligence to control and optimize the overall functions of sensing, spectrum access, communication, storage, and processing with the help of AI, deep learning, and ML. It recommends that the intelligence will be distributed across various network entities. Finally, in the network view, the functions of the overall network have been divided as sublayers. They include the application sublayer, routing, management, spectrum access, and physical medium, which resembles the layered IP stack. A simplified version of the same architecture for 6G has been proposed in [108]. G. Neuroscience Based 6G Architecture This architecture is more of a framework that tries to integrate neuro signals to emulate the wireless signals to be applied in 6G networks [136]. Here, human brain’s intelligence and radiating properties of wireless signals shall be integrated to enable an intelligent communication between the human and computers. Recent advances in bio-IoT and implantable- communication devices have made us bold to envision short range 6G network communication, where wireless modules implanted inside the human brain acquire intelligence from the brain and communicate directly with the outer world wireless devices and base station or with another human with similar capability. H. Proposed Architecture Our proposed 6G architecture consists of devices at the outer layer as shown in Fig. 6. The devices in this layer gather data, events in the network and communicate to the next level. Here, IoT nodes, cellular users, smart devices, etc., which have been connected to the radio access network may have a certain level of intelligence by default. Consequently, interference, channel selection, sensing shall be better managed. We have also depicted the paradigm of the cell-free network, small cells, and massive MIMO enabled by intelligent surfaces operating side-by-side as will be the scenario in 6G. The arrows in this layer indicate the wireless links. Further, in the next layer, we have edge devices and cloud that utilize AI for resource allocation, user management, and other optimization tasks. As proposed in [42], we considered a distributed edge intelligent architecture which provides process, store, compute and decision-making facility for independent physical networks. The intelligent agents at these edge devices learns from the devices to assist in better management of the network below it at layer 1. For instance, it could be the selection of wireless channel and tuning the parameters accordingly. Here, a centralized edge node provides backhaul service to all the physical networks through these edge clouds, that is shown by the discrete lines. It also includes cloud-RAN and backhaul resources for various use cases. However, all these resources are virtualized. Further, we have various clouds that interconnect the network elements. Finally, at the innermost layer, we have the applications for various verticals which either provides or receives services from the layers below it. Thus, the proposed network architecture in Fig. 6, provides the abstract view of 6G. However, several other features such as quantum computing, fog nodes, security, etc., have not been shown exclusively and shall be the potential candidates of 6G architecture. FIGURE 6. Proposed network level 6G architecture. Show All SECTION VI. Future Trends in 6G The 6GFlagship project has identified 11 key areas of 6G pilot project, and have published white papers in June 2020 [36], [42], [44]–[55], [107]. In this section, we summarize the overall ambition of some of the key areas such as localization and sensing, trust and security, UN sustainable development goals, rural connectivity and networking, edge intelligence, machine learning, and 6G business scope as shown in Fig. 7. FIGURE 7. Disruptive future trends in 6G. Show All A. Localization in 6G Traditional sensing and localization using GPS or cell coordination will become obsolete in future high dense smart urban settings. In 2030 future networks, KPI related to accuracy must include user’s environment information to facilitate accurate positioning. For instance, a use case of 6G networks such as autonomous driving, where high resolution positioning at the cm level is necessary along the driving lane or track details to achieve traffic safety [19]. Similarly, mobile sensor-based applications that use location information should include power consumption per unit distance coverage; integrity and privacy-related KPI should include location details, malfunctioning nodes, and alarms [53]. These requirements ensure that future technologies or devices for 6G networks must be accompanied by additional information such as power consumption and types of alarms generated during any sensor failure for exact location sensing with emphasis on privacy. In addition, localization and sensing of information have several applications in future, including robotic surgery, contact tracing during pandemics, VR based games, social networking and dating apps, food delivery, context-aware marketing, autonomous driving, personal navigation, and animal tracking. Specifically, in the case of the context-aware services, 6G network will automatically learn user requirements by sensing the environment or location and providing the best QoS and other settings [124]. Consequently, 6G systems will integrate intelligence, pervasive networking, along with precision localization and high-resolution sensing to serve specific use cases that emerge during 2030, requiring accuracy being fine-tuned to cm level. This will enable seamless connectivity, localization, and sensing for context-aware services [19]. The 5G NR and mm-wave will offer localization and sensing features different from the previous generation of cellular networks. Moreover, 6G networks will further enhance the localization accuracy leading to high definition imaging by the use of THz communication, massive antennas, and RIS [53], [91]. As the beamwidth reduces at lower wavelengths ( μm -wave), the positioning accuracy increases. Next, the inverse relationship between frequency and device size (antenna) will enable dense packing of multiple antennas, which facilities precise angular and direction estimation. In this regard, better channel estimation algorithms are necessary at THz frequencies to sense and localize. Further, a higher data rate of 6G network will enable the sharing of maps between the devices much easier. With the large IRSs, the reflected wave can provide better localization service by exploiting the near-field effect and analyzing the wavefront. In addition, it will remove the need of synchronization between reference stations. However, more precise methods are necessary for the real-time localization and positioning if the intelligent surfaces are smaller in size [91]. It should be noted that the existing localization techniques using ML heavily depend on fingerprinting, regression, and classification methods. However, more intelligent localization and mapping methods will be necessary for the 6G network [53]. By AI- and ML-based frameworks, the sensing system can be trained to learn from raw data, where vital information (time and space-based) present with the noisy radio signal, or weak sensor data can be extracted to analyse the location and sensing patterns for the 6G network system. Albeit, most of the ML- and DL-based techniques need a large volume of structured data for training, which may not be available (noisy, random) in tiny sensor-based applications. Therefore, advanced analysis methods should be introduced to assist in trained localization. As mentioned earlier, localization is also associated with another set of tasks i.e., imaging and sensing which involves high frequency signals (60GHz). These signals when reflected by the objects, the size and dimension of the objects can be measured accurately [127]. Besides, localization also includes imaging using sensors. Here, passive sensors will capture the images and active sensors will transmit signals that represent range, angle, and Doppler with a high resolution and accuracy. For instance, in case of self-driving cars, radar imaging adds range and Doppler to the multi-dimensional image [53]. B. Trustworthy 6G How security in 6G different from 5G: The number of connected devices (IoT, MTC, and mobile users) in 6G networks will increase at a galloping rate and density-wise ten million devices in a unit square km area [2]. These devices will inherit artificial intelligence and span across diversified applications such as banking, industry, healthcare, government, transportation, and many more. Due to the dependence on many verticals, 6G networks must enforce a higher-level security in every stage of its network to prevent all forms of security attacks. In another instance, we may imagine a hyper connected world, where a minor compromise in the security settings may lead to fatality in an automated industry, robotic surgery performing incorrect incision, etc., [55], [50]. Currently, in 5G networks, traditional cryptography will serve most of the needs to secure the data communication across cloud and edge architectures. However, a new era of security will begin by the inception of quantum computing in 6G network. For instance, QKD systems facilitate easier eavesdropping detection that classical cryptography. Furthermore, upgrading the level of security is possible with quantum secure direct communications (QSDC), which provides better security over the quantum channel than the QKD or classical methods [120]. In another view, automation of security functions with ML algorithms at different levels will be essential because of network densification. However, there is a threat of reversed ML based security attacks, as the network can learn all parameters minutely. Therefore, 6G network needs exclusive holistic network security architecture [55]. 1) Required Security Features The key features should include: especially, future SIM cards and their security aspects, and use of asymmetric cryptography, Transport Layer Security (TLS) using elliptic curve cryptography (ECC), software and AI-driven security as applicable to SDN and NFV. For example, the security model in 6G networks will consist of a deep learning enabled VNF gateway that monitors the ingress and egress traffic and apply filtering policies to detect and prevent potential security attacks. In summary, a trust model must define the rules to collect, process, distribute, and filter data in the network. C. 6G for Sustainability Development Goals (SDG) Digitization due to 5G networks will currently address many social issues such as education, environment protection, and hunger. Nonetheless, there exist several problems that require extensive urbanization and connectivity. Since 6G networks will bring in user experience-based hyperdata connectivity, several global issues especially, with regard to the United Nation’s SDGs, shall be mitigated [86]. The 6G network through its multi-faceted communication capability (long and short-range, 3D connectivity), has the potential to promote global sustainability, which is mainly due to its seamless connectivity and support for a plethora of services. Let’s see some of the future scopes of 6G networks as regards the promotion of sustainable development goals. For instance, online banking will enable anyone to access financial services with much ease, which will help to eradicate poverty. Similarly, online healthcare services will reduce the travel time and will easily reach the needy whenever necessary to promote better healthcare delivery. By having all-round network connectivity, farmers in rural places can make use of digital transactions for their agricultural, and home products. We shall implement IoT in food production to improve the yield and to obliterate hunger [93]. With the aid of 6G networks, all these issues shall be addressed on a massive global scale. When we consider zero energy, 6G network will target miniaturization of the devices to increase the power efficiency and energy harvesting, thus improving environmental performance. For instance, these devices may sustain the power generated through everyday activities such as walking, jogging, and household work of the device user. This energy shall support the personal information devices like wristband, smart patch which monitor a person’s vital body signs from time to time. In addition, in another scenario, these could cater to information and entertainment needs through over the top connectivity. To support the United Nation’s SDGs, 6G network has set its vision to provide an opportunity for the global society and economy to accomplish its digital dreams [44]. It includes data connectivity, a strong economy, facilitates smart healthcare, energy efficiency to name a few among the 17 SDGs. The 6G vision targets three underpinnings to implement SDGs through various services. They are as follows. (i) Addressing the problems of people and society by providing suitable digital infrastructure to empower them. (ii) Context-awareness based on the environment which is possible by highly precise localization and sensing capabilities, online monitoring of resources, and so on. (iii) All-round uplifting of the ecosystem of SDGs. Besides these requirements, the fundamental necessity to achieve the goal will be the all-round connectivity and access to the internet. 1) How 6G Supports UN-SDGs Since SDGs spans all sectors of life, the vision of 6G networks should be multi-disciplinary. As a result, an open and co-innovation platform for 6G’s technology standardization activities which support human life and environment is crucial to create an ecosystem that will benefit all stakeholders of the UN-SDGs. The 6G ecosystem will contribute to the progress and well-being of various communities in society. For instance, 6G will provide wireless networking to connect patient, doctor, and medical equipment together in a remote smart healthcare system. With the aid of 6G network, several digital services could be extended such as education, food distribution services, banking, and industry to advance on a massive scale. These eventually result in the empowerment of human life toward achieving a better SDGs. Having said this, there will be concerns while targeting the SDGs such as inclusiveness, data security. For instance, when it comes to data collection, all stakeholders such as hospitals, society, and government must handle data with utmost privacy rules for the integrity of the system. Now, 6G networks play a role in addressing the privacy and security of data communication. In summary, 6G networks must implement actions that promote sustainability to different verticals, encourage growth with minimum energy consumption, and implement zero waste when people, machines, and resources are interconnected. Due to the prevalence of the intelligence in 6G network, it will improve efficiency and environmental sustainability by using low energy consuming technologies or devices that derive the energy from natural activities for their functioning. We have summarized the set of KPIs for all 17 SDGs and their possible use cases, as in Table 4. TABLE 4 KPI for UN SDG and Their Use Cases 2) Opportunity for 6G The use cases of 6G networks must be aligned with the intentions of SDGs, especially the SDGs which can be well supported by ICT infrastructure. The common aspects such as online financial activities, including online banking and online business will promote economic growth. Furthermore, telemedicine, remote mapping of natural resources such as minerals and water bodies are some use cases that improves social and healthcare sustainability. When we look at 6G’s diverse range of wireless connectivity, edge intelligence, localization, sensing, and ultra-low energy consumption will strongly support rural connectivity, environmental context, and energy aspects of SDGs. In Table 4, we summarize the KPIs with regard to the 17 SDGs and the use cases, as intended in [44]. D. How 6G Will Wipeout Digital Divide? There are several reasons for lack of connectivity in many remote areas of the world such as absence of infrastructure, low income, rough geography, and many more [92]. The 6G network has a strong vision on breaking the digital divide which will in turn promotes sustainability alongside the connectivity. It is estimated that nearly 3.7 billion of world population is far from reach of internet connectivity [44]. The digital divide gives rise to several other problems such as retarded economic growth, education and awareness, inhibits the adoption of remote, and advanced healthcare technologies. It also causes low growth in modern industrial and new age transportation sectors. The digital divide has not left any geographical regions untouched. In reality, even though, first world nations have high-quality mobile internet connectivity, there are remote areas of USA, and Europe that experience a low data rate of 0.4–1Mbps through wired/wireless connectivity. The situation is much worse in Africa, Brazil, and India [52]. 1) Scope of 6G in Rural Connectivity Connectivity in tough terrains, away from the main cities should be simple, low cost, and easily achievable. Therefore, mobile broadband is the best solution when compared to wired internet [48]. Providing last-mile connectivity through the LEO satellite will cover a large area. However, it will not be a reasonable solution, considering the throughput, latency, and cost. Instead, usage of large power transmitters (mega cells) in low dense rural areas will be a viable solution. To achieve this target, new safety rules and power regulations will be necessary. Alongside, floating mobile base stations like UAV or balloons will promote mobile data services while acting as relays or data cache [52]. Since remote regions have financial constraints, while providing good QoS, special emphasis on affordability (cost-wise) is mandatory. The use of natural power sources such as solar and wind energy for power grids that operate the backhauls will enable cost reduction. In addition, cost reduction and resource efficiency could be achieved by confining the contents to the rural region generated within the locality. This requires caching the locally generated contents in local servers itself and provisioning network slicing. The local caching will enable cost reduction by minimizing the need for backhaul connections. Moreover, several data serving schemes can be brought into picture through network slicing to enhance connectivity with limited resources [76]. Regarding the backhaul connectivity, even though visible light or microwave seems to be the right candidates, they may face challenges in rural areas. Therefore, Integrated Access and Backhaul (IAB) seems much promising where the operator uses the part of the spectrum for backhaul and the remaining part for cellular access at a reasonably lower cost than the former candidates [52]. Thus, 6G networks shall rely on IAB to reduce the deployment cost and improve the resource utilization [107]. With IAB, only a subset of base stations will carry the backhaul traffic through connected links (fiber/wire); whereas, the remaining base stations will be connected through wireless links to transmit the backhaul traffic through multi-hop relaying. It is much possible in 6G due to the availability of large bandwidth in THz bands where access and backhaul traffic can be multiplexed by suitable scheduling schemes. Thereby, reducing the infrastructure cost. The IAB will provide service in sub-6GHz and mm-wave. Similarly, Low orbit Satellites, UAV, and balloons find useful solutions to extend the range seamlessly. Based on the communication need, these terminals can be made operational (discontinuous trans-reception) as and at when required: thus, reducing the power consumption. Altogether, non-terrestrial communication will play a key role in 6G networks to reduce the digital divide [52]. In recent trends, edge computing and the caching of local data for local usage, installing micro telecom infrastructure and integrating it with mobile network operator, and their maintenance have attained prominence in rural regions. Albeit, all these have several challenges, such as financial models, frequency regulations, propagation delays, and co-existence issues [48]. E. What is the Scope of Edge Intelligence in 6G The rise of edge computing and ultra-dense node deployment or mobile devices led to the emergence of mobile edge computing to optimize the computations and performance in the network. Further, the very nature of mobile devices requires the computations to be distributed. A limiting factor of 5G network is the lack of intelligence in mobile edge computing, which will otherwise facilitate an easy transition into new verticals and services such as Industry 4.0, smart mega cities, and intelligent gadgets. One of the reasons for the setback is that existing AI algorithms do consume enormous time and resources. Not only that, the production of hardware and AI solutions are yet to achieve synchronism. In total, when 5G network is considered to use cloud driven AI, 6G network will use edge driven AI [80], [81]. The Edge Intelligence (EI) is an extreme capability to process the data at the edge. Today, mobile devices have the high computational capability, storage space, and run petty AI applications. Subsequently, AI will become an integral part of the 6G network with distributed intelligence to run standalone and automated operations replacing most human interactions. For instance, verticals such as autonomous cars, ultra-smart healthcare, and modern industries involve complicated tasks with regard to data acquisition and computation. Therefore, distributed intelligence at different network entities will be required to offer optimized services. This forms the foundation for the 6G internet of intelligent things [86]. EI can be viewed from different directions: data analytics point of view refers to data analysis and development of solutions at or near the site where the data is generated. the network perspective mainly targets the intelligent functions that are deployed at the edge of the network, comprising the user, tenant, or the network boundary [42]. 1) Need for Software for AI Edge Currently, we lack software fluidity; without that, precisely locating intelligence in the network entity and sharing knowledge will be impaired. EI will be useful in several verticals to offer better QoS. For instance, when we consider the scenario of self-driving vehicles, these vehicles need to compute and communicate many factors, such as inter vehicular distance, roadside entities, traffic signs, and vehicles in the nearby lanes. These data will be communicated to the EI enabled base station or access points located along the roadside to quickly (low latency) store and process the data and assist in deciding the load on available frequency bands (spectrum) to facilitate the data transfer from these vehicles dynamically. Noticeably, these EI units will also have access to the central cloud database, which maintains the overall data from several regional EI systems. Further, smart city applications, mobile extended reality (XR) which requires large data processing and computations, can be benefited by EI due to local data processing, reduced delay, resulting in higher data rate, task and sharing, and so on [42]. Nevertheless, the hardware technology to accommodate the intelligent processing capabilities on a small edge device with resource constraints is yet to mature. Similarly, software with intelligent, real-time, and independent decision capabilities with regard to training and learning from the data to meet the requirements of 6G networks will require time to evolve. 2) Challenges (i) When we consider a distributed environment, providing intelligence across the edge devices by gathering all parameters related to resource usage, training, and learning models will be a challenge. (ii) Furthermore, the optimum deployment of edge nodes in a high-density network and resource allocation is complex. (iii) At the edge devices, the action of AI is largely influenced by the type of precision of input data, especially when the available data is huge, vivid, and needs classification. Therefore, training models must consider these challenging factors and rely on synthetic data generated from generalized adversarial networks to manage the data consistency. (iv) With respect to the distributed edge devices that handle AI, say mobile phones, the AI algorithms must be lightweight, and applications must be partitioned between user devices and edge layer for efficient resource management. (v) The software packages for EI contain edge applications that are deployed on the edge devices along with virtualization infrastructure. In this context, the selection of system policies, billing, etc., play a key role. (vi) In EI devices, real-time feedback is essential to attain better performance. Consequently, it is necessary to re-define the real-time feedback by considering the online learning and pre-trained models. The training algorithms should be distributed instead of centralized and must adopt online training models by considering the edge device limitations [61], [42], [80]. F. How Machine Learning Will Rule 6G Machine Learning (ML) is a sub-branch of AI which assists in the prediction and classification of data based on the trained data. In 6G network, ML will find its application at different instances of the network, especially, at the PHY, MAC, Network, and Application layers. We shall discuss these applications briefly in this section. From the earlier discussions on use cases during 6G era, we realize that interconnected intelligent network components require ML as a fundamental network requirement to offer value addition to the services, zero-touch optimization, and improve the performance. This is because relying purely on mathematical modeling of the wireless system will make the whole system computationally complex. However, ML can efficiently model the wireless system that could not be wrapped under mathematical equations to suit the requirements of 6G networks. Interestingly, ML will act on the vast real-time data received from different use cases such as self-driven cars, holographic telepresence, and intelligently control 6G network [47]. At the PHY layer, many of the functionalities can be envisioned by mathematical models. However, certain non-linear phenomenon such as interference detection and channel prediction can be efficiently addressed with ML. Furthermore, the discrete modules such as modulator, demodulator, and the filter will optimize the performance as a whole by suitable learning methods in ML. When we consider the channel coding process, the transmitter and receiver must implement certain coding schemes to safeguard the data against channel disparities. Deep learning is one of the promising methods for predicting the appropriate coding length by learning the channel code-word length. Next, synchronization is another aspect which certainly depends on the channel variations, mobility, and frequency deviations. In this regard, deep neural networks will prove to be beneficial. However, further research is necessary to comment more on this. Another aspect of ML is the use of deep neural networks for positioning. Since the existing positioning methods will popup accuracy issues when there is no line of sight path due to their dependency on pure mathematical modeling. In deep learning, fingerprint method uses CSI and received signal strength as learning data. Altogether, several physical layer optimizations such as beamforming, channel estimation, and throughput maximization are non-convex and do not yield a good performance with heuristic algorithms. Consequently, deep learning (CNN and DNN) appears as a boon to solve the physical layer optimization issues of 6G networks in real time, while achieving a good tradeoff between performance and computational time. 1) ML at the MAC ML will find its application in optimal solutions to the MAC layer tasks such as resource allocation, modulation and coding scheme (MCS) selection, handover, and uplink power control. When the channel conditions vary due to user movement, channel variation the deep reinforcement learning methods could determine optimal solutions by learning from the varying inputs. (i) ML for resource allocation: In most IoT scenarios the data transmission is predictable as they follow a specific pattern. As a result, it is easy to predict such patterns and decide the resources to be allocated efficiently with the aid of ML. This reduces the network latency, and uplink random access for the devices. Similarly, by allocation of time, the frequency resources for cell users can be optimized with ML. It can predict the data traces in the cell, and network load and allocate time frequency slots in a flexible manner (flexible duplex) to optimize the resources and avoid interferences. (ii) Power management: 6G network devices must have seamless battery management capacity. Since MAC layer controls the access to medium, the radio power is the key factor to be managed. ML can analyze the real-time traffic patterns inside the cell and data targeted for the neighbor cells to adjust the load. By this, the network can schedule users between transmission and sleep states in a power efficient way. (ii) Security with ML: Due to the high profile nature of the 6G network such as high data rate, all-round connectivity, and extremely low latency, the data generation will go skyrocket. As a result, to address the various security threats that would arise due to the massive data, the network security must be automated. In this context, proactive and self-adaptive features of ML will be promising solutions to automate the security. 2) ML at the Application Layer 6G networks will be intelligent by integrating context awareness to offer better user services and multi-agent reinforcement learning to enhance the overall efficiency. Using a rule-based approach to determine the context configuration for each service class will be tedious. However, ML can predict the context configurations meaningfully with the help of past service choices and modify the configuration to suit the new services from the applications. For instance, lets us consider an application that monitors the network parameters automatically. We know that KPIs need to be maintained within the threshold level. Therefore, ML can be used to detect anomaly with respect to network management, security, and many more. Future network architectures must be end-to-end service providers, where there will be dynamic cooperation between network segments and communication entities. To ensure all-round ubiquitous connectivity, interoperability between heterogeneous networks is mandatory. Furthermore, efficiency in terms of cost, power, system performance, communication, and connectivity, spectrum usage are necessary. The core network to provide on demand service deployment consists of SBA, where network functions (NF) will subscribe to network repository function and it offers services to one or more NFs. By, each NF can be independently upgraded or removed to provide network slicing. G. Business Model for 6G 5G’s market will revolve around, providing the three key metrics for various verticals. Furthermore, in 5G networks, business models changed from monopolized network operators to distributed local edge micro operators, virtualized and software network service providers, and slice managers. Unlike the 5G network, the 6G network will provide ultimate connectivity between cyber-physical world in real-time and several upfront KPIs. This will motivate numerous verticals, technology developers, product and application developers for the future society 5.0. These market advances will disrupt the conventional business and invite new stakeholders and investors with regard to 6G’s communication, storage, and control applications [50]. In addition, 6G network will intertwine with intelligence, virtualization, local edge operation, spectrum sharing, and a variety of sensing services; altogether, it will influence our personal lives and will be an integral part of society in 2030. As a consequence, it will bring in several newer forms of business models and opportunities to generate revenue [16]. 1) Key Trends and Uncertainties Smart grid system for distribution of electricity, or internet will be foreseeing extreme connectivity and self-driven. Considering these applications, revenue models can be planned with the involvement of public-private-partnership. Next, over the top (OTT) companies will offer cloud storage and cognitive services (AI, UAV as a service, and context aware services) along with primary features such as calling and data connectivity to compete with the traditional MNOs. This trend will boost innovative business models to attract customers and improve their profit. The business models along with increasing the revenue must even target sustainability goals for the benefit of humanity. For instance, 6G’s vision includes providing connectivity for all humans; in this context, at least providing mobile internet to everyone in rural areas will solve the problem of digital divide. When people around the globe are connected, they will have the digital identity that will enable economic growth [157]. In this regard, data services will attract monetization. Further, another interesting business opportunity will be industry automation. When we consider Industry 5.0 and beyond, all sensors and machines that communicate with each other or operated by robots, will require a private and secure communication network which will offer extreme reliability, zero touch assistance, and operate as standalone networks. More importantly, 6Gs spectrum sharing, and policy regulations will generate prime revenue to the government and agencies. In addition to this, digital twins, and FinTech industry will revolutionize the business pattern with 6G networks. This is because, 6G networks will use green, zero energy, and zero emission technologies such as intelligent reflecting surface and high lifetime wireless nodes in a manner that will bring new business around building these technologies [23]. 2) Scenarios It is anticipated that the following scenarios will open up new business opportunities for 6G [50]. Edge computing, IoT, Robotics, and AI will become the common technologies that every electronic gadget will incorporate and will be reasonably priced for customers to afford. Therefore, these devices could be used to collect a variety of data from users and process them locally before remote transmission. Next, network slicing will allow up to 10000 slices under a network operator. This will gear-up revenue generation [23]. Furthermore, the development of sustainable solutions will empower them to operate as standalone and simplify the lives of people. In this regard, to find locally sustainable and economical solutions, public-private partnership projects, distributed cooperative models, and peer-peer business models will be more suitable against the monopolized market at the local level [50]. In addition, due to the sparse spread of technology in rural areas, manufacturing cheap and inexpensive solutions at the micro level could be an acceptable approach. However, 6G will overcome the digital divide to promote technology sharing in which a 3D design of a machine can be shared in real-time with another underdeveloped city that is deprived of research facilities to manufacture it using its cheap labor and market locally as well as in the place of origin at a reasonable price than otherwise. This will support a circular economy and creates a better society both in the rural and urban areas. H. Additional Trends 1) Cell-Free Access Cell-less architecture is a concept in future networks, where the existing paradigm of connecting a user to an access point (gNB) confined to a cell will no longer be the case. Instead, a cellular user will get connectivity from multiple BS without the restriction of cell boundaries. In a traditional cellular architecture, the users who are at the edge of the cell will have poor connectivity, which will be addressed in the cell-free architecture. This feature requires tweaking the traditional cell search, synchronization, random access, and resource allocation procedures, to suit the new cell-less access [107], [112]. Furthermore, in 6G due to the extremely high density of mobile users or intelligent devices that require high data rates, the coverage of the gNB will be limited since they operate at very high frequencies, which reduces the interference. In this context, cell-less architecture is very useful to provide seamless connectivity. The cell-less system consists of several gNBs distributed in a large geographical region provides connectivity to all users located using the same frequency. These gNBs will be connected to a central processing unit (CPU), which controls them. Normally, a cell-free system is associated with MIMO to improve the spatial connectivity. The cell-free architecture has several benefits, namely, (i) scalable signal processing, (ii) scalable power control, (iii) spectral and energy efficiency, and (iv) simple signal processing and economical system [120]. 2) Backscatter Communication (BsC) It is an interesting paradigm for 6G network to achieve extremely high energy efficiency (EHEE) of the network when there are an enormous number of connected devices. For massively dense distribution of wireless nodes, the use of the battery as a power source will not be feasible due to the challenges involved in recharging or replacing the battery. In this scenario, the wireless nodes shall receive radio signals from the ambience passively, leading to battery-free operation [123]. The fundamental idea is to use the RF energy that is incident from the ambience and reflect it back after modulation. This method in fact, reduces the burden on the constrained device and increases the spectrum efficiency. It has applications in short-range and low power communication scenarios. 3) Wireless Power Transfer While connecting the massive devices, obtaining the channel state information (CSI) to know their channel status becomes a tedious task and consumes a lot of energy. As a result, the quality of transmission may get affected if CSI is omitted. However, a tradeoff between data communication and power can be achieved by wireless power transfer (WPT) and CSI-free transmissions. Further, wireless power transfer will be a possible method to promote sustainability, low emission, and zero energy consumption among devices in 6G networks [112]. In this context, as applicable to massive IoT nodes, the nods can be made energy harvesting from the surrounding. It could be by the use of solar energy, wind, hydroelectric, or RF signals, etc. The wireless power transfer using RF involves remotely powering (charging) the wireless devices using the RF field. For instance, when BS transfer to a user, then the neighbor of that user may use the energy in the RF signal to charge itself. It uses far-field radiation properties of electromagnetic waves to transmit within a short distance. This has several benefits, such as significant increase in the durability and life time of the nodes, reduced energy footprint, auto-charging, and contactless charging. A few well-known solutions for wireless energy transfer involve (i) energy beamforming, where a group of high beam antennas are focused to transmit the energy to a specific set of nodes using narrow beams. (ii) distributed antenna systems to replenish the energy loss during energy propagation toward the nodes by using power beacons. However, there are several challenges involved in energy transfer. They are (i) ineffectiveness of the support for non-line of sight energy transfer, (ii) power transfer for non-stationary (mobile) users, (iii) radiation hazards, to list a few. SECTION VII. 6G Research Worldwide Hitherto While the 5G network is being deployed worldwide, the research and development activities on 6G networks are gearing up both in industry and academia. In March 2020, 3GPP completed the 5G standard release 16, which will be followed by release 17 to support all three scenarios in 2021 [46], [99]. Therefore, in the next couple of years, 3GPP will initiate the 6G research. It is anticipated that by 2027, the 5G infrastructure market will increase to 47.75bn USD by 2027. The top five players in the 5G infrastructure market include Huawei, Ericsson, Samsung, Nokia, and ZTE. They have currently shifted their attention toward the 6G network due to the enormous potential and benefit that the 6G network is anticipated to offer over the 5G network. Along the same lines, FCC has decided to promote 6G network research and trials in the THz band by opening a 95 GHz –3 THz frequency band for research [99]. Samsung has anticipated that ITU-R will define the official vision of the 6G network in mid-2021, and the initial commercialization of 6G network will begin by the end of 2028 [86]. Recently, from the past 18 months after the first 6G wireless summit in March 2019, the academia has become aggressive in the early research on the vision, technologies, challenges, and the future directions of 6G networks. The global telecom companies have even endorsed this. In the following paragraph, we will list the major activities in the research and standardization efforts by various organizations worldwide. A. Finland The 6G Flagship program (6Genesis) of the University of Oulu in association with other industries and academic institutes such as Nokia, VTT research center, Business Oulu, and Keysight technologies are at the forefront of research groups. They have initiated two 6G network summits and have released 12 white papers on the key research areas of the 6G network [112]. Moreover, Mediatek has started its research on 6G chipset along with Nokia in Finland. B. China The major players in the 6G network in China include Huawei and ZTE. They have their independent research units aside government-sponsored 6G R&D promotion and expert group. Their research mainly includes THz, AI, and blockchain for 6G networks along with other operators such as China Mobile and China Unicom. C. USA In Feb 2019, US President Donald Trump US telecom companies gave a call to intensify their research to launch 6G network at the earliest. From academia, The New York University wireless center, headed by Prof. T. Rappaport, has highly engaged in developing the THz channel modeling and has achieved a 100 Gbps data rates in its trials [57]. D. South Korea One of the key telecom players SK Telecom, has undertaken 6G network research in the areas of THz communication, ultra-massive MIMO, and aerial communication. Further, they have collaborated with Ericsson and Nokia in 6G equipment manufacturing technology development. Samsung has recently released the white paper on 6G vision emphasizing three aspects: holographic communication, truly immersive XR, and digital twins [86]. Moreover, LG and KAIST University have developed a 6G network research laboratory to jointly conduct research in the technology areas of the 6G network. E. Japan NTT-Docomo has released its white paper indicating the vision of future 2030 network (6G). It has focused its research on AI and Cyber physical system to promote 6G networks. In the process, NTT has demonstrated 100 Gbps at 28 GHz band. In addition, Japan Govt., released 2.04 bn USD to promote R&D. Apart from these, Toshiba and Tokyo University have initiated 6G networks research [149]. Similarly, Sony, Nippon, and Intel have planned to work together in different fields of 6G technology. F. Europe In Europe, besides Finland, several universities namely, University of Dresden and Deutsche Telekom (Germany) are involved in the research of Tactile internet, HCI technologies. Next, University of Padua, Italy and NYU wireless group are also involved in the 6G network research. SECTION VIII. Future Explorations After discussing the current trends and future directions, it is essential to lead the research community with hints for further exploration. In this section, we will ponder upon some key directions and opportunities. A. How to Provide Security In 6G network, massive connectivity, heterogeneity, and multi-hop routing for communication of user data arises the question of privacy of data. This is because, in the said context, data will be exposed to multiple entities, while ML requires the use of private data for training the data, which increases the vulnerability of data. The proactive security mechanisms for dynamic networks (high mobility vehicular networks) will increase the cost of control signaling. However, 6G networks cannot afford to look for reactive security methods that are slower [159]. Therefore, cost effective proactive methods need to be invented. Even though, quantum security seems to be promising, a vibrant research in this direction is yet to be seen. Therefore, 6G networks research shall devote more efforts on these security aspects. B. Virtualization of Radio Access Interface Even though 6G network is distributed, an intelligent supports SDN and visualization are needed. The crucial questions to answer, include Will every network element supports various simultaneous verticals requirements such as resources, latency, QoS, and cloud computing needs? and how do we achieve that? In addition, virtualization in 6G, needs to support all radio access interfaces such as THz radio, smart surface, and quantum radio, which is a great challenge to address. C. Vertical Edge Caching When we consider the terrestrial, aerial, and space networks (vertically), the satellite networks will introduce a large delay. Therefore, aerial network elements such as balloons or HAPs shall act as data caching and edge computing facilities between terrestrial and satellite networks to reduce the computations of satellite networks and support critical services from there itself at a reduced delay. This scenario has a wide scope for future research with regard to radio resource allocation, localization, power efficiency, etc. D. Mobility and Localization Similarly, cell-free networking requires precise localization and synchronization between heterogeneous networks along with beamforming and accommodation of reflecting surfaces, which opens another area for future research [158]. Next, high precision beamforming under high mobility is infeasible. Therefore, the location-based steering of the beam looks promising. Albeit, efficient location-based mobility tracking for beamforming requires research attention. Moreover, during the mobility between different access networks with various protocols, to maintain active communication and localization, an integrated protocol design is necessary, which we lack at the moment. E. Rural Connectivity After the outburst of COVID’19 pandemics, the world has gone almost virtual. We noticed that the urban areas sparsely connect to the rural areas, and most of the rural areas have local networks without having connectivity to the backbone. Therefore, to bridge the gap of the digital divide, policy makers, technology developers, and service providers shall work together to facilitate the availability of modern ICT tools to the unconnected. Furthermore, there is a need for awareness among the people about using AI, XR, and cloud-based technologies to promote better living and healthcare. Next, with respect to optical connectivity to alleviate the digital divide, new standards are necessary for VLC, and the equipment shall have dual connectivity comprising of VLC and radio transceivers [147]. The VLC radio shall also target to provide cost effective services to suit the needs of the underprivileged population. By considering these potentials of VLC regarding the digital divide, major research is necessary to support long range communication, with relatively lower cost than the radio spectrum. F. Analysis of Meta Materials and Reflecting Surfaces Undoubtedly, these reflecting surfaces form the key component of 6G networks due to their energy saving and harvesting features. Albeit, analytical and electromagnetic field modeling to study the non-linear behavior of the phase response, channel capacity, and spectrum efficiency of these smart reflecting transmitters, needs in-depth study in the near future [144]. Furthermore, the multicarrier modulation, multi-antenna transmissions, practical direct demodulation of the reflected wave at the receiver, and the prototype development are the key areas for future exploration. G. EI When heterogeneous devices are connected, and all the intelligence has pushed to the edge, managing various data types, address resource allocation, and modeling the network behavior that requires data-driven multi-level distributed algorithms shall be well studied. In addition, light-weight AI solutions will be necessary to enhance distributed operations. Moreover, another interesting domain to explore is selecting different machine learning models for various verticals as each vertical will have multiple KPI requirements. H. Sustainable Goals One may readily agree that the 6G network will greatly support SDGs. However, measurement of social impact due to the penetration of the 6G networks requires an in-depth analysis and mapping between 6G KPIs and the 17 SDGs. First, KPI values for SDGs should be determined. Second, a tremendous amount of work is required to bring equality in society, gender roles, and data privacy to invent new use cases of the 6G network successfully in order to promote SDGs. SECTION IX. Conclusion This article discusses the trajectory of the 6G network and its network components along with the current status, merits, use cases, and challenges. Further, we highlighted the current research and areas for future explorations. The vision of the 6G network and the next-generation network requirements indicate that the 6G network will immensely outperform the 5G network due to its ability to serve the extreme needs of future use cases such as autonomous vehicles, tele healthcare, industry automation, and the rise of several verticals. With a positive outlook, we described some of the possible use cases of the 6G network that will largely influence the society in 2030 and later. Further, it is evident from the KPIs that every aspect of the 6G network communication will wisely support various verticals. The main outline of this paper’s discussion stems from how 6G network being a 3D connected network, and an AI-driven network will bridge the contemporary technology gap to efficiently achieve the SDGs. In addition, we provided a brief discussion by highlighting the merits of the key areas of the 6G network mentioned in the 6GFlagship project. We envision that AI, Intelligent surfaces, cell-free architecture, digital twins, and quantum computing will likely become the 6G technology candidates. Albeit, these technologies are not market ready. In this regard, international collaborations in industry and academia for research, technology development, and commercialization are necessary for the early inception of the 6G network. Authors Figures References Citations Keywords Metrics More Like This AF Relaying Secrecy Performance Prediction for 6G Mobile Communication Networks in Industry 5.0 IEEE Transactions on Industrial Informatics Published: 2022 Security, Reliability and Test Aspects of the RISC-V Ecosystem 2021 IEEE European Test Symposium (ETS) Published: 2021 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.

Paper 6:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: 
  Extract 2: 
  Limitations: >
  Relevance Evaluation: The explanation provided by the model is highly relevant to the point focus of the literature review, providing a comprehensive overview of the ML-based algorithms used to analyze, monitor, and model power flows, power quality events, photovoltaic systems, intelligent transportation systems, and load forecasting.
  Relevance Score: 1
  Inline Citation: >
  Explanation: The explanation provided by the model is very precise, specific, and concise. It meets all the requirements of a good explanation.

 Full Text: >
energies
Article
Advances in the Application of Machine Learning Techniques
for Power System Analytics: A Survey †
Seyed Mahdi Miraftabzadeh 1,*
, Michela Longo 1,*
, Federica Foiadelli 1, Marco Pasetti 2
and Raul Igual 3


Citation: Miraftabzadeh, S.M.;
Longo, M.; Foiadelli, F.; Pasetti, M.;
Igual, R. Advances in the Application
of Machine Learning Techniques for
Power System Analytics: A Survey.
Energies 2021, 14, 4776. https://
doi.org/10.3390/en14164776
Academic Editors: Don Lee,
Eklas Hossain and Ahmed Abu-Siada
Received: 1 June 2021
Accepted: 2 August 2021
Published: 6 August 2021
Publisher’s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright: © 2021 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
1
Department of Energy, Politecnico di Milano, Via Lambruschini 4, 20156 Milano, Italy;
federica.foiadelli@polimi.it
2
Department of Information Engineering, University of Brescia, Via Branze 38, 25123 Brescia, Italy;
marco.pasetti@unibs.it
3
EduQTech, Electrical Engineering Department, EUP Teruel, Universidad de Zaragoza, 44003 Teruel, Spain;
rigual@unizar.es
*
Correspondence: seyedmahdi.miraftabzadeh@polimi.it (S.M.M.); michela.longo@polimi.it (M.L.);
Tel.: +39-02-2399-3759 (M.L.)
†
This paper is an extension of the conference paper presented at IEEE EEEIC 2020, Genova, Italy, 11–14 June 2019.
Abstract: The recent advances in computing technologies and the increasing availability of large
amounts of data in smart grids and smart cities are generating new research opportunities in the
application of Machine Learning (ML) for improving the observability and efﬁciency of modern
power grids. However, as the number and diversity of ML techniques increase, questions arise about
their performance and applicability, and on the most suitable ML method depending on the speciﬁc
application. Trying to answer these questions, this manuscript presents a systematic review of the
state-of-the-art studies implementing ML techniques in the context of power systems, with a speciﬁc
focus on the analysis of power ﬂows, power quality, photovoltaic systems, intelligent transportation,
and load forecasting. The survey investigates, for each of the selected topics, the most recent and
promising ML techniques proposed by the literature, by highlighting their main characteristics and
relevant results. The review revealed that, when compared to traditional approaches, ML algorithms
can handle massive quantities of data with high dimensionality, by allowing the identiﬁcation of
hidden characteristics of (even) complex systems. In particular, even though very different techniques
can be used for each application, hybrid models generally show better performances when compared
to single ML-based models.
Keywords: machine learning; power systems; smart grids; power ﬂows; power quality; photovoltaic;
intelligent transportation; load forecasting; survey
1. Introduction
The power system management and development have constantly been changing
due to expanding complexity and distributed modern power networks. [1]. Principally,
the increasing distribution of Renewable Energy Sources (RESs) with intermittent energy
generation and technological novelties in power system management and control demand
reliable power predictions and more precise monitoring models [2,3]. In recent years,
researchers developed advanced solutions based on Machine Learning (ML) algorithms to
solve the bottleneck of conventional lumped parameter simulations. In practice, conven-
tional traditional simulation techniques based on deterministic methods are still dominated
in power grids. However, the high performance of machine learning solutions in terms of
accuracy computational speed, and scalability brings novelties in power grids management
and control. Therefore, it is expected to boost the adaptation of these techniques for short-
to medium-term forecasts of the power grid system operation to meet this gap while getting
beneﬁts of advantages of traditional approaches.
Energies 2021, 14, 4776. https://doi.org/10.3390/en14164776
https://www.mdpi.com/journal/energies
Energies 2021, 14, 4776
2 of 24
As the high-quality sensor prices–such as smart meters, Phasor Measurement Units
(PMUs), and Remote Terminal Units (RTUs), or other measurement devices—are constantly
decreased, they are increasingly distributed in the power system and are continuously
acquiring a massive amount of heterogeneous datasets. Analyzing and processing all these
data provides new insights and advances in the control and operation of smart grids thanks
to innovation in ML and big data frameworks to handle structured and unstructured data.
Traditional time-domain methods are computationally inefﬁcient; thus, they are not good
candidates for real-time applications in which response time is a decisive concern [4,5].
The expected signiﬁcant penetration of Electric Vehicles (EV) charging stations, and the
increasing expansion of the Internet of Things (IoT) devices in private and public sectors
such as smart buildings introduce new challenges and opportunities for the perception
of accurate Day-Ahead Load Forecasts (DALFs) in micro-and smart- grids [6,7]. At the
same time, the transition towards decarbonization of systems leads to the integration of
distributed energy systems which usually generate energy in an intermittent and stochastic
manner, such as wind or solar energy generators with no inertia. Consequently, the growing
complexity of power ﬂow patterns requires novel approaches to render reliable, efﬁcient,
and economical solutions.
In this context, advanced machine learning models have been shown promising results
to provide new valuable knowledge and insights and identify hidden data patterns, trends,
and relationships. In [8], the authors brieﬂy summarized the ML paradigm and presented
the literature review on applications of ML methods in Power systems till the end of
2017. This paper continues the authors’ work presented at [8] and aims at providing a
systematic review of the various machine learning algorithms used to analyze, monitor, and
model power ﬂows, power quality events, photovoltaic systems, intelligent transportation
systems, and load forecasting services. The authors selected the Journal papers for literature
review based on publication date, number of citations, and novelty in contributions. The
main contribution of this article is as follows:
The ML paradigm and well-known ML algorithms are categorized and presented;
The systematic review summarizes not only the main contributions of each article but
also provides information regarding the explicit application, data source, and models, by
mainly considering articles published since 2018; this study used Google Scholar, Scopus,
IEEE Xplore, and the MPDI databases for literature review, which ended in February 2021.
To make a fair comparison between models, the characteristics of a standard dataset
for the testing of the reviewed models are presented.
The remainder of the article is structured as follows: Section 2 explains the machine
learning paradigms, well-known algorithms, and performance metrics. A literature survey
on recent advanced machine learning applications in power ﬂow, power quality, pho-
tovoltaic systems, electric transportation systems, and load forecasting is presented in
Section 3. Section 4 discusses the results and notes achieved in the literature review, and
Section 5 summarizes the ﬁnal remarks and conclusions.
2. Machine Learning
Artiﬁcial Intelligence (AI) deals with the broad topic related to the perception and
extraction of knowledge from data. AI can be divided into two main subsets: machine
learning and deep learning. Machine learning is the main subset of artiﬁcial intelligence,
while deep learning can be represented as a subset of machine learning.
Machine learning is an interdisciplinary research ﬁeld that consolidates expertise and
knowledge from diverse areas and aims at proposing solutions to given problems that
can be used to reply to similar questions raised by different contexts. More speciﬁcally,
machine learning is the subset of AI that deals with the extraction of knowledge from
the experience by analyzing and manipulating data gathered from real-world use cases.
The primary purpose of machine learning is to develop reliable active learning models
equipped with computerized patterns learning from raw data and perform fast-response
predictions applied in decision-making processes [9,10].
Energies 2021, 14, 4776
3 of 24
Deep learning and neural networks are the most famous machine learning subset.
Thanks to the use of (typically) multi-layered Artiﬁcial Neural Networks (ANNs), deep
learning can handle unstructured datasets and can recognize complex input data patterns.
In deep learning, different architectures can be designed using neural unit cells in various
layers, unless other machine learning algorithms are ﬁxed.
Figure 1 illustrates artiﬁcial intelligence, machine learning, and deep learning concepts
in the schematic description by means of a Venn diagram.
Energies 2021, 14, x FOR PEER REVIEW 
3 of 27 
 
 
equipped with computerized patterns learning from raw data and perform fast-response 
predictions applied in decision-making processes [9,10]. 
Deep learning and neural networks are the most famous machine learning subset. 
Thanks to the use of (typically) multi-layered Artificial Neural Networks (ANNs), deep 
learning can handle unstructured datasets and can recognize complex input data patterns. 
In deep learning, different architectures can be designed using neural unit cells in various 
layers, unless other machine learning algorithms are fixed. 
Figure 1 illustrates artificial intelligence, machine learning, and deep learning 
concepts in the schematic description by means of a Venn diagram. 
 
Figure 1. Artificial intelligence vs. Machine learning vs. Deep learning. 
2.1. Machine Learning Paradigms 
In machine learning, training a model intends to learn the values of the parameters 
(or weights) and the bias from input data, while in traditional methods (i.e., with 
predefined algorithms), both the model and its parameters are given to a computer to 
perform a task. Labeled data are samples with a sort of meaningful “tag”, “label”, or 
“class” that are informative or desirable to know—for example, whether an Alternating 
Current (AC) power signal contains harmonic distortion(s) or not. In contrast, unlabeled 
data are samples with no explanation; in other words, it has only row data without any 
“tag” or “label” assigned to it—for instance, voltage and current signals of an electric 
motor. 
Machine learning tasks are principally arranged into three main classes: supervised, 
unsupervised, and semi-supervised learning. Supervised learning algorithms work with 
labeled data with the objective of mapping new input data to the known target output 
values. On the contrary, unsupervised learning models process an unlabeled dataset, in 
which target values are unknown, to draw insights by learning hidden complicated 
patterns and structures spontaneously. Semi-supervised algorithms deal with a dataset 
that some samples are labeled, and more extensive samples are unlabeled. These 
algorithms are designed to benefit from both advantages of supervised and unsupervised 
methods [11]; 
Supervised learning is categorized into classification and regression problems. A 
classification problem predicts output variables as a category, such as “cat” or “dog.” 
Contrarily, in regression problems output variables are numerical values [12]; 
Unsupervised learning algorithms are generally divided into clustering or 
dimensionality reduction (or sometimes called embedding) methods [11]. For instance, in 
anomaly detection, a clustering algorithm is applied to data to identify false data by 
scanning outliers in a dataset or noticing abnormal patterns; 
Semi-supervised learning makes use of the mixture of labeled and unlabeled data as 
the training dataset. Semi-supervised models act as active learners [13]. There are two 
Figure 1. Artiﬁcial intelligence vs. Machine learning vs. Deep learning.
2.1. Machine Learning Paradigms
In machine learning, training a model intends to learn the values of the parameters (or
weights) and the bias from input data, while in traditional methods (i.e., with predeﬁned
algorithms), both the model and its parameters are given to a computer to perform a
task. Labeled data are samples with a sort of meaningful “tag”, “label”, or “class” that
are informative or desirable to know—for example, whether an Alternating Current (AC)
power signal contains harmonic distortion(s) or not. In contrast, unlabeled data are samples
with no explanation; in other words, it has only row data without any “tag” or “label”
assigned to it—for instance, voltage and current signals of an electric motor.
Machine learning tasks are principally arranged into three main classes: supervised,
unsupervised, and semi-supervised learning. Supervised learning algorithms work with
labeled data with the objective of mapping new input data to the known target output
values. On the contrary, unsupervised learning models process an unlabeled dataset,
in which target values are unknown, to draw insights by learning hidden complicated
patterns and structures spontaneously. Semi-supervised algorithms deal with a dataset that
some samples are labeled, and more extensive samples are unlabeled. These algorithms are
designed to beneﬁt from both advantages of supervised and unsupervised methods [11];
Supervised learning is categorized into classiﬁcation and regression problems. A
classiﬁcation problem predicts output variables as a category, such as “cat” or “dog.”
Contrarily, in regression problems output variables are numerical values [12];
Unsupervised learning algorithms are generally divided into clustering or dimension-
ality reduction (or sometimes called embedding) methods [11]. For instance, in anomaly
detection, a clustering algorithm is applied to data to identify false data by scanning outliers
in a dataset or noticing abnormal patterns;
Semi-supervised learning makes use of the mixture of labeled and unlabeled data as
the training dataset. Semi-supervised models act as active learners [13]. There are two
main semi-supervised learning algorithms, namely reinforcement learning and Generative
Adversarial Networks (GANs). In reinforcement methods, if a model does a task correctly,
it would get a reward. The objective of reinforcement learners is to build a model to
maximize rewards through an iterative process [14]. Reinforcement learning is suitable for
an interactive or dynamic environment that a model can improve itself based on policies
Energies 2021, 14, 4776
4 of 24
deﬁned by an expert, for example, playing a game or self-driving cars. GANs generate
models based on deep neural learning methods to discover and learn patterns of input
data. Then, the generative model can be used to create new data examples that resemble a
training dataset. For instance, GANs can create pictures that look like human faces images,
even though the faces don’t relate to any actual person.
2.2. Machine Learning Algorithms
Many different machine learning techniques have been proposed in recent years,
particularly consisting of hybrid ML-based models, making use of two or more machine
learning techniques or even other statistical or mathematical models. For example, en-
semble learning models include different weak learners such as decision trees, support
vector machines, and linear or logistic regression. This section discusses the basic and most
relevant machine learning techniques in each category.
2.2.1. Classiﬁcation Algorithms
There are several classiﬁcation algorithms; the most commonly used ones are pre-
sented as follows [15]:
Logistic Regression (LR): LR is widely used for binary classiﬁcation tasks where an
output belongs to one class or another (0 or 1). In this algorithm, a threshold is deﬁned to
indicate examples will be labeled into which class using hypothesis and logistic function
(usually sigmoid curve). The hypothesis determines the likelihood of events to generate
data and ﬁt them into the logarithm function that forms an S-shaped curve called sigmoid.
Then, the logarithm function is used to predict the class of new inputs. Even though logistic
regression provides better performance in binary classiﬁcation tasks, it can also be used in
multiclass classiﬁcation problems, by applying the one versus all strategy [16];
K-Nearest Neighbors (KNN): this algorithm is one of the most basic yet broadly used
classiﬁers. It is generally used to ﬁnd data with similar characteristics and group them
in the same class, without making any assumptions on data distribution. The groups are
constructed by considering the attributes of the neighboring samples. It is used in real-life
problems in several applications such as data mining, pattern recognition, and invasion
detection [17,18];
Naïve Bayes (NB): this technique is one of the most powerful classiﬁcation algorithms
based on an extension of Bayes’ theorem, assuming each feature is independent to capture
input-output relationships. Bayes’ theorem compares the probability of an event happening
to what has already happened, for example, the probability of having a ﬁre (event A)
while the weather is hot (event B, which is present) [19]. The naïve algorithm is simple
to implement and can easily predict labels of new inputs. Additionally, when domain
knowledge conﬁrms the feature independence, with less data, it has a better performance
than other classiﬁcation algorithms such as logistic regression. On the other hand, in real
life, it is not easy to have data with entirely independent features; moreover, when there
is an input that was not followed up in the training phase, the algorithm assigns zero
probability, and it does not classify this input in any group. This technique is used in
various applications such as text classiﬁcation and spam ﬁltering [20];
Support Vector Machine (SVM): This algorithm is widely used in classiﬁcation tasks
and also applied in regression problems. The main idea of SVM is to transfer data to higher
n-dimensional space to ﬁnd an ideal hyperplane to differentiate classes [21]. In simple
words, these support vectors are coordinates of a new n-dimensional coordinate system.
This method is commonly used in binary classiﬁcation, but it is computationally expensive
and slow in the big data domain;
Decision Tree (DT): This algorithm is based on different hierarchical steps that lead to
certain decisions. It applies a treelike structure to represent decision paths with induction
and pruning steps. In the induction step, the tree structure is built, while, in the pruning
step, the complexities of the tree are reduced. The inputs are mapped to outputs by travers-
ing each path through different branches of the tree [22]. DT is a powerful classiﬁcation
Energies 2021, 14, 4776
5 of 24
tool, simple to structure and with good performance. However, with even small variations
in data, DT can become unstable. Furthermore, it can easily become overﬁtted, especially
in a thorny tree with many branches and conditions, thus, it does not generalize well on
new inputs. Regularization, bagging, and boosting techniques are usually used to avoid
overﬁtting problems in the DT [23];
Random Forest (RF): This classiﬁer is very similar to the decision tree. Compare to
DT, RF uses several decision trees, instead of having only one tree. This technique can be
applied in massive data set to classify data or measure the importance of each feature in
the ﬁnal decision. In many applications, the random forest is preferred over the decision
tree because it can be more accurate and overcomes the overﬁtting issued of DT. However,
this technique is not easy to implement since it has a complex structure, and it is not
recommended for real-time prediction purposes because it is generally slower than other
models [24].
2.2.2. Regression Algorithms
Several regression algorithms (numerical or continuous value prediction) have been
introduced in the scientiﬁc literature; the most commonly used ones are presented in the
following:
Linear Regression (LR): this technique tries to ﬁnd the ﬁttest straight hyperplane to
the data [25]. It is commonly used when there are linear relationships between variables,
and it can avoid overﬁtting by regularization techniques such as LASSO, Elastic-Net,
and Ridge [26]. However, it is not ﬂexible in ﬁnding the best solution for non-linear
relationships in variables and complex patterns;
Regression Tree (RT): This technique has the same hierarchical structure as the decision
tree, but it takes numerical values as input. The branching procedure not only maximizes
the learning gain but also learns non-linear relationships between variables. Even if this
method is robust to outliers and easy to implement, it is prone to overﬁtting problems [27].
In addition to the regression tree, random forests and Gradient Boosted Trees (GBM), which
are the most commonly used ensemble methods, are also applied in numerical predictions
and have better performance concerning overﬁtting issues;
Deep Neural Network (DNN): Deep neural network, or multi-layer neural network,
is widely used in several domains. Indeed, thanks to their ability to capture complex
patterns, DNNs can be used both as regression algorithms and classiﬁers. The non-linear
relationships between features are learned by non-linear activation functions and hidden
layers between the input and the output [28]. There are several techniques and methods
to improve the performance of neural networks, as well as different advanced neural
network-based models such as Convolutional Neural Network (CNN) or Recurrent Neural
Network (RNN) [29,30]. Different from other algorithms, in DNNs, a deep knowledge of
how to tune the parameters of the neural network is required to develop a working neural
network model. In addition, even though neural network models work well in the big data
domain, they are usually very computationally expensive methods;
Extreme Learning Machine (ELM) has a wide range of applications in the data-driven
approach. ELM has been used in regression, classiﬁcation, clustering, sparse representation,
feature extraction or learning, and compression. This feedforward neural network does not
apply the backpropagation gradient-based mechanism to update the network weighted
values; instead, it randomly assigns random values to the weight and bias terms of the
network [31]. The main advantages of this kind of algorithm are (i) the faster training
phase and (ii) the better interpolation results. On the contrary, the accuracy results of ELM
is not promising, even if compared to basic MLP models;
Support Vector Machine or kernel SVM can also be used for regression problems, even
though it is mostly used in classiﬁcation problems;
XGboost, ﬁnally, is a (recently) widely used rugged decision-tree- and ensemble-
based algorithm with a framework that is designed considering a gradient boosting proce-
dure [32].
Energies 2021, 14, 4776
6 of 24
2.2.3. Clustering Algorithms
Clustering techniques try to group instances with the same properties in the same
cluster. These techniques are commonly used in other ﬁelds than machine learning, such
as image analysis, pattern recognition, data compression, and statistical analysis. The most
well-known algorithms are as follows:
K-means: this technique, one of the simplest and intuitive machine learning algo-
rithms, separates instances in the k centroids or clusters with equal variance. After selecting
the number of clusters (K), the algorithm ﬁnds the best k clusters by minimizing the cri-
terion known as inertia through the iterative procedure and changing the position of
centroids [33]. As it is simple to interpolate and scales well to big data, it has been applied
across a wide range of applications in various domains;
DBSCAN: Density-Based Spatial Clustering of Applications with Noise (DBSCAN),
which is widely used in data mining and machine learning, ﬁnds core instances of high
density and extends clusters with the speciﬁed radius (usually Euclidean distance) around
them. Low-density regions are distinguished as outliers. The primary problem in DBSCAN
is selecting clustering attributes, detecting noise with varied densities, and signiﬁcant
differences of amounts of boundary objects in opposed directions of the corresponding
clusters [34]. The smallest number of instances to constitute a dense region and how close
instances should be to each other in the same region are deﬁned by an expert. Even though
this algorithm, which is a very popular clustering technique, is widely used, it badly
behaves with very sparse or high dimensional datasets;
Spectral: this clustering algorithm, which is also an exploratory data analysis tech-
nique, performs dimensionality reduction through eigenvalues (spectrum) of the similarity
of data instances, by then grouping similar data instances with reduced dimensions into
the same cluster [35]. This approach is practically applied when the center of clusters and
their spread does not appropriately describe the whole cluster (non-convex cluster), such
as in image segmentation problems. The spectral technique is widely used because it is a
fast response technique and outperforms other clustering techniques, especially in sparse
datasets.
2.2.4. Embedding Algorithms
In many cases, especially in the big data domain, the presence of a large number of
variables or features in a dataset makes it difﬁcult to interpret the relationship between
them. Training a model on the whole dataset could easily make the model not sufﬁciently
generalized on new unseen data (overﬁtting problem). Embedded Algorithms (EAs) can
be applied to extract new features from data without losing essential information before
implementing sophisticated ML models. EA techniques could also be used directly for
prediction purposes. Embedding algorithms can be subdivided as follows:
Principal Component Analysis (PCA): the main aim of PCA is to reduce high-dimensional
datasets to a smaller dimension. PCA projects each data instance onto the main compo-
nents or ranks while retaining as much data variation as possible. PCA techniques, such as
Singular Value Decomposition (SVD), use eigenvectors of the covariance matrix of data to
reduce the dimension of the dataset or making a prediction [36];
Autoencoder: this is one of the current states of the art techniques leveraging neural
networks. Autoencoders are widely used in different applications, such as data compres-
sion. The autoencoder learns a representation (encoding) of the input dataset and ignores
noise through embedding architecture, and reconstructs the input data as close as possible
to its actual forms (decoder). A typical autoencoder consists of three parts, namely: an
encoder, a bottleneck, and a decoder [37]. The encoder tries to compress the data to a lower
dimension with the best representative, the decoder attempts to regenerate an input by
eliminating the noise in the dataset, while the embedded data is stored in the bottleneck.
It is possible to use the encoder part of a well-trained autoencoder for dimensionality
reduction, or use the whole model, for example, in anomaly detection [38].
Energies 2021, 14, 4776
7 of 24
Figure 2 summarizes the different machine learning paradigms and techniques used
in power system analytics, by providing examples for each category.
 
compression. The autoencoder learns a representation (encoding) of the input dataset and 
ignores noise through embedding architecture, and reconstructs the input data as close as 
possible to its actual forms (decoder). A typical autoencoder consists of three parts, 
namely: an encoder, a bottleneck, and a decoder [37]. The encoder tries to compress the 
data to a lower dimension with the best representative, the decoder attempts to regenerate 
an input by eliminating the noise in the dataset, while the embedded data is stored in the 
bottleneck. It is possible to use the encoder part of a well-trained autoencoder for dimen-
sionality reduction, or use the whole model, for example, in anomaly detection [38]. 
Figure 2 summarizes the different machine learning paradigms and techniques used 
in power system analytics, by providing examples for each category. 
 
Figure 2. Machine learning paradigms, algorithms, and applications in power systems. 
2.3. Model Performance Evaluation Metrics 
The metrics that are used in each machine learning algorithm are different from each 
other. In Table 1 the most used metrics in discrete and continuous cases are discussed. In 
this table, True Positive (TP) and True Negative (TN) are samples that are correctly pre-
dicted as positive and negative, respectively. In contrast, False Positive (FP) and False 
Negative (FN) are samples that are incorrectly predicted as positive and negative, respec-
tively. In continuous metrics, y is the actual value, yො is the forecasted amount, and n is 
the number of prediction samples. 
Table 1. Model Performance Evaluation Metrics. 
Discrete 
Continuous 
Metric 
Formula 
Metric 
Formula 
Accuracy 
𝑇𝑃 + 𝑇𝑁
𝑇𝑃 + 𝑇𝑁 + 𝐹𝑃 + 𝐹𝑁 
Mean Square Error 
(MSE) 
1
𝑛 ෍(𝑦 − 𝑦ො)ଶ 
Figure 2. Machine learning paradigms, algorithms, and applications in power systems.
2.3. Model Performance Evaluation Metrics
The metrics that are used in each machine learning algorithm are different from each
other. In Table 1 the most used metrics in discrete and continuous cases are discussed. In
this table, True Positive (TP) and True Negative (TN) are samples that are correctly predicted
as positive and negative, respectively. In contrast, False Positive (FP) and False Negative
(FN) are samples that are incorrectly predicted as positive and negative, respectively. In
continuous metrics, y is the actual value, ˆy is the forecasted amount, and n is the number
of prediction samples.
Table 1. Model Performance Evaluation Metrics.
Discrete
Continuous
Metric
Formula
Metric
Formula
Accuracy
TP+TN
TP+TN+FP+FN
Mean Square Error (MSE)
1
n ∑(y − ˆy)2
Error
FN+FP
TP+TN+FP+FN
Root Mean Squared Error (RMSE)
q
1
n ∑(y − ˆy)2
Precision
TP
TP+FP
Mean Absolute Error (MAE)
1
n ∑|y − ˆy|
Recall
TP
TP+FN
Mean Absolute Percentage Error (MAPE)
1
n ∑
 y− ˆy
y

F1
TP
TP+FN
R-squared (R2)
1 − ∑(y− ˆy)2
∑(y−y)2
3. Literature Review
Machine learning is widely applied to address various problems to bring novel so-
lutions or improve the performance of existing applications. The main state-of-the-art
Energies 2021, 14, 4776
8 of 24
machine learning-based applications in power systems are in power ﬂow, power quality,
photovoltaic system, intelligent transportation, and load forecasting.
3.1. Power Flow Applications
Compared to traditional algorithms, machine learning technologies make power
ﬂow problems easier to be handled. For example, algorithms like CNN, KNN, SVM,
reinforcement learning, and decision tree affected power ﬂow optimization problems in
terms of accuracy, computational speed, and response time. Table 2 elaborates more into
detail the recent advancements in machine learning applications in power ﬂow.
3.2. Power Quality Applications
The power quality, one of the most critical topics in electrical systems, has also
been affected by machine learning, which can be used to improve speed and accuracy in
disturbances detection, or distortions classiﬁcation, and estimations for future cycles. In
addition, ML can also be used on a wide set of PQ parameters related to load functioning
such as active power, reactive power, complex power, fundamental frequency, and power
factor.
Table 3 summarizes the most recent improvements and achievements in the use of
ML techniques in power quality applications.
3.3. Photovoltaic System Applications
Machine learning algorithms have been widely used for different purposes in Photo-
voltaic (PV), from forecasting the long-, medium-, and short-term energy generation, to
fault detection and classiﬁcation. The most recent works in this ﬁeld are summarized in
Table 4.
3.4. Intelligent Transportation Applications
Artiﬁcial intelligence, especially machine learning applications, are widely used in
intelligent transportation, to develop smart online trafﬁc management systems, from safety
applications (e.g., driving distraction detection) to optimized trafﬁc scheduling. Self-
driving cars, for instance, have been recently developed only thanks to the advancements
in machine learning.
Table 5 provides the most recent works based on ML in the ﬁeld of intelligent trans-
portation.
3.5. Load Forecasting Applications
Accurate load forecasting, both short- and long-term, is an essential task for the
daily (economic) dispatching of electricity, both to prevent wasting energy production and
integrating renewable energy resources.
Energy companies monitor, control, and schedule load demands and power generation
to enhance energy management systems. However, electrical load proﬁles are becoming
more complicated, not only because of the stochastic behavior of customers, but also
because of the introduction of new non-linear components in power systems, such as
electric vehicles, buses, and bikes. Therefore, many researchers have been developing both
deterministic and probabilistic load forecasting models to improve the precision and speed
of prediction models.
Table 6 presents recent advancements of machine learning studies in load forecasting.
Energies 2021, 14, 4776
9 of 24
Table 2. Overview of Research using Machine Learning for Power Flow.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Lei et al. [39]
2020
Optimal Power Flow (OPF)
IEEE 39, 57, 118-bus, and Polish 2383-bus (wind and
photovoltaic power connected)
30,000 samples for training, and 10,000 samples for
validation
ELM
Decompose OPF via a data-driven regression framework with three stages
stacked extreme learning machine (SELM); implemented the multiple
supervised layers with reinforcement mode with an overall 98.71%
accuracy rate, signiﬁcantly higher than benchmarks.
Wang et al. [40]
2020
Online Detection of
Geomagnetically Induced
Currents in Power Grids
Simulation data based on real-life power grid
operation (10,800 samples for training, 1200 samples
for validation, and 6000 samples for testing)
CNN
Developed hybrid feature extraction consists of pseudo-continuous
wavelet transform (PCWT) and short-time Fourier transform (STFT);
improved overall detection accuracy to 90.15% for different noise levels
and achieved the detection results within 30 m
Ravikumar et al.
[41]
2020
Anomaly Detection and
Mitigation (ADM) for Wide-Area
Damping Control
Synchrophasor measurement data and simulated the
cyber-physical system (CPS) dataset (60 fps
transmission rate)
KNN and
PCA
Improved efﬁciency of ADM with domain-speciﬁc features extraction and
selection via Teager–Kaiser Energy Operator (TKEO), Principal Component
Analysis (PCA), Wide-Area System Measures (WASMs), and Primitive
Measures; Proposed KNN-based model with a 95.5% accuracy rate, better
than other ML-based models.
Zhang et al. [42]
2021
Volt-VAR Optimization in Smart
Distribution Systems
Unbalanced IEEE 13-bus and 123-bus systems (9000
operating conditions for training, and 13,000
operating conditions for testing)
Reinforcement
learning
The improved accuracy rate of voltage regulation with an average of
99.80% compared to 90.02% of baselines; Achieved an average executive
time of 21.7 and 46.2 ms for 13-bus and 123-bus systems, respectively, and
28.38% for the loss reduction percentage.
Baker et al. [43]
2019
Joint Chance Constraints in AC
Optimal Power Flow
IEEE 37-node test feeder (5-min data from August
2012 weekdays)
1152 samples for training (4 days), and 864 samples for
testing (3 days)
SVM
Improved classic methods based on union bound (or Boole’s inequality)
and the accuracy rate with 0.19% and 4.73% error rate for false
classiﬁcation of binding and non-binding events, respectively.
Li et al. [44]
2020
Transient Stability Assessment of
Power System
IEEE New England 10-machine 39-bus system, IEEE
16-machine 68-bus system, and IEEE 47-machine
140-bus system (5984, 11,792, and 29,520 samples,
respectively)
6-s simulation time per instance, and 0.01-s step
XGBoost
and FM
Proposed hybrid XGBoost-FM model robust to noise. Used extreme
gradient boosting (XGBoost) for automatic feature builder, and
factorization machine (FM) as a classiﬁer with the enhanced detection time
of 0.9349 s; improved accuracy by using both original and artiﬁcial features
to 98.21%.
Hong et al. [45]
2020
State Estimation of Distribution
Network
IEEE 13-, 34-, and 37-node test feeders (240 samples
for training, and 60 samples for testing)
LR, SVM,
and FFNN
Estimated the voltage magnitudes and angles of several successive buses
with 0.01 p.u. and 0.189◦ error respectively; SVM outperformed LR and
FFNN, especially when the relationship between inputs and outputs is
unknown, the input bus was missing, there is a measurement error, and
using few adjacent buses as input buses.
Karagiannopoulos
et al. [46]
2019
Optimized Local Control for
Active Distribution Grids
Seasonal historical data (30-day dataset; 1-h time
resolution; 7200 samples)
SVM
Proposed Data-driven method to obtain local Distributed Energy
Resources (DERs) controls without monitoring and communication
infrastructure; outperformed standard industry local control with an
overall RMSE accuracy of 0.158.
Energies 2021, 14, 4776
10 of 24
Table 2. Cont.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Zhao et al. [47]
2020
Real-Time Power Grid Multi-Line
Outage Identiﬁcation
IEEE 30, 118, and 300 bus systems (300,000, 800,000,
and 2.2 million data generated, respectively)
ANN
Generated a large number of samples with Monte Carlo simulation with
full-blown power ﬂow models; Achieved an overall classiﬁcation accuracy
of 99%, and outstanding performance in recognizing multi-line outages in
real-time with a small amount of data.
King et al. [48]
2015
Algorithm Selection for Power
Flow Management
IEEE 14-bus (10,000 states for testing), IEEE 57-bus
(10,000 states for testing), and a real 33-kV distribution
networks (17,520 states for testing -a year of
half-hourly proﬁle data-)
ANN, DT
and RF
Shown that ML-based methods can create effective algorithm selectors for
power ﬂow management based on algorithms’ behavior data within 1 ms
for future complex networks
Labed et al. [49]
2019
Overloaded Power System
Alleviation
Algerian (Adrar) 22-bus system (75% of data for
training, and the remaining 25% for validation)
ELM
The proposed method outperforms SVM and ANN learning algorithms
with 1.9465*10 MSE accuracy and 0.0023 s response time on the testing
phase with generalization performance; this fast time response minimized
the threat and risk of outage and cascade failure.
Table 3. Overview of Research Using Machine Learning for Power Quality.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Ray et al. [50]
2018
PQ Disturbances Classiﬁcation in
Solar PV Integrated Microgrid
Generated dataset from solar PV integrated microgrid
model (600 samples; 5-kHz sampling frequency)
SVM, ICA
Proposed the independent component analysis (ICA) and statistical feature
extraction using SVM; ICA-SVM improved accuracy to 99.5% compared to
97.8% of traditional Wavelet transform-SVM
Sahani et al. [51]
2020
A Real-time Power Quality
Events (PQEs) Recognition
Synthetic (50 samples for training, and 100 samples for
validation) and real (100 samples per distortion
–validation-) power quality events data
ELM
Robust anti-noise online PQEs classiﬁcation; Outperform other models
with 98.86% accuracy rate and 0.019 s response time
Turovic et al. [52]
2019
PQ Distortions Detection in
Distribution Gird
IEEE 13-bus system modiﬁed with DG (85% of the
samples for training, and 15% for validation)
ANN, SVM,
and KNN
Detection speed comparison between ML algorithms and traditional FFT;
ANN has the best detection’s speed with 0.432 ms (600% more than FFT)
with a 99.41% accuracy rate
Liao et al. [53]
2018
Voltage Sag Estimation
IEEE 68-bus test network (374,400 faults simulated)
CNN
Automatic system area mapping and feature extraction in the input bus
matrix from various local areas in the power network; reached 99.41% of
overall estimation accuracy.
Vantuch et al. [54]
2017
PQ Forecasting for Off-Grid
System
Experimental off-grid laboratory (141,537
one-minute-resolution measurements, more than
3 months) and simulated data
RF
More than overall 90% accuracy for forecasting short-term (15 min ahead)
PQ disturbances
Bagheri et al. [55]
2018
Voltage Dip Classiﬁcation
6000 real measured voltage dips data over different
countries
One month of recording
CNN
Developed a robust automatic feature extraction using a space phasor
model (SPM) and CNN; outperformed the other existing models with a
97.72% accuracy rate and 0.50% false alarm
Energies 2021, 14, 4776
11 of 24
Table 3. Cont.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Sahani et al. [56]
2020
Power Quality Events
Recognition
Synthetic and laboratory PQDs (150 samples per class
−50 for training and 100 for validation-; 3.2-kHz
sampling frequency)
ELM and
VDM
Developed an automatic PQEs patterns recognition system from
nonstationary PQ data by using integrating variational mode
decomposition (VMD) and Online P-Norm Adaptive Extreme Learning
Machine (OPAELM); shorter event recognition time and classiﬁcation
accuracy rate of 99.3%.
Wang et al. [57]
2019
Power Quality Disturbance
Classiﬁcation
Synthetic data (16 PQDs)
IEEE Power Engineering Society database (1000
samples;
The inﬂuence of data imbalance is eliminated by
applying an enhancement process)
ELM
Select less than 10 features out of 4500*1280 signal matrix via discrete
wavelet transform (DWT) feature extraction and particle swarm
optimization (PSO) feature selection; Proposed PSO hierarchical ELM
(PSO-H-ELM) classiﬁcation with automatic encoders and sparse
constraints; overall classiﬁcation accuracy rate is above 95%, and high
calculation speed (less than 0.169 s).
Shen et al. [58]
2019
Detection and Classiﬁcation of
PQDs in Wind-Grid Distribution
Systems
Synthetic data (2400 samples)
Simulated data from the standard IEEE 13 node bus
system with wind-grid distribution (5590 samples)
10-kHz sampling frequency
CNN and
IPCA
Used Improved Principal Component Analysis (IPCA) for extracting
statistical features; applied 1D-CNN classiﬁcation, which gives 99.76%
accuracy on average for different noise levels, higher than other
classiﬁcation methods.
Deng et al. [59]
2019
Type Recognition and Time
Location of Combined Power
Quality Disturbance
Synthetic data from IEEE 1159 power quality standard
for training (1000 samples × 96 combinations of PQD)
and real data generated in a lab for testing (140
samples)
GRU
Proposed bi-directional GRU model for classifying 96 different kinds of
disturbances noiseless and with noise from 10 dB to 50 dB; have a 98%
accuracy level on real operational data and the absolute error of
starting-ending times location less than 0.469 ms.
Cao et al. [60]
2019
Transient voltage stability
analysis based on frequency,
active power, and reactive power
Simulated data of different nodes collected by phasor
measurement units
CNN and
Deep
Learning
Decision optimization algorithm based on PQ parameters implemented;
reactive power compensation decision based on deep learning performed
Abed [61]
2018
Power factor enhance and control
Simulated power system
Clustering
neural
network
The proposed method allows improving power factor
Zhang et al. [62]
2020
Reactive load prediction
SCADA data from a real power grid (357 busloads)
Training set:
data from June 1 to August 5
Test set: data from
August 6 to August 22.
15-min sampling period
Deep
learning
Reactive power load of buses can be accurately predicted; accuracy is better
than that obtained with other prediction models; result of great signiﬁcance
for reactive voltage control
Nakawiro [63]
2020
Voltage and reactive power
control
Simulated dataset of grip operation (on-load tap
changer, load, and wind power)
1 year of operation (hourly data)
DT and
KNN
The highest classiﬁcation accuracy is achieved with a DT; accuracies
obtained in the simulations are satisfactory for some classes; performance
heavily relies on the distribution of the target output and number of
samples per class
Energies 2021, 14, 4776
12 of 24
Table 3. Cont.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Moreira et al. [64]
2018
Power factor compensator (based
on PQ parameters: power factor,
unbalance factor, harmonic
distortion, reactive power, etc.)
Training: Simulations characterized by a human
specialist (1,355,154 samples per disturbing load).
Real measurements added
Test: IEEE 13-bus (111,055 samples). Three real test
sets (disturbing loads)
DT, KNN,
SVM, and
ANN
PQ parameters used to analyze the functioning of a power system; DT is
highly effective in classiﬁcation; 100% accuracy achieved
Valenti et al. [65]
2018
Non-intrusive load monitoring
based on active and reactive
power
Two public datasets:
Twenty-one power meters; 60-s sampling period; 2
years monitoring.
Four different locations; multiple sampling
frequencies
ANN
Introducing reactive power increases F1 score performance from +4.9% to
+8.4%; reactive power provides signiﬁcant information for non-intrusive
load monitoring
Table 4. Overview of Research Using Machine Learning for Photovoltaics Systems.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Keerthisinghe et al.
[66]
2020
PV Forecasts for Capacity
Firming
Dataset of 2013–2018 coming from empirical formula
and 2019–2020 real-data of Arlington Microgrid; input
frequency every half an hour for one day and two
samples for output
LSTM
Proposed encoder-decoder LSTM-based model for short-term (1-h ahead)
PV generation prediction resulted in reducing the yearly battery energy
throughput by 29% and the number of battery cycles with a greater than
10% depth-of-discharge by 51%.
Wen et al. [67]
2019
PV Prediction in Ship Onboard
Historical hourly data of meteorological information
along with the ship route movement for a year
ELM
The proposed ML-based model with the particle swarm optimization (PSO)
has a MAPE accuracy level of 25.41% in the training phase for ﬁve-hour
ahead prediction; the difference between prediction and experimental
results has 14.96% of the absolute error in the test phase, which means it
has a high potential in practical cases.
Dhibi et al. [68]
2020
Fault Detection and Classiﬁcation
in Grid-Tied PV System
Emulate the operational real PV array dataset using
Chroma 62150H-1000S programmable dc power
supply; 100 µs sampling time with 1501 samples for 6
different classes for both training and testing
RF and
K-means
Proposed two classiﬁers based on Reduced kernel RF for detecting faults:
Euclidean distance-based RK-RF and K-means clustering-based RK-RF
with 100% accuracy and reduced computational time 65.16% and 53.33
compared to kernel RF, respectively; redundancy between samples was
reduced by using Euclidean distance as a dissimilarity metric; the K-means
clustering method used to reduce the training data amount.
Zhang et al. [69]
2020
Day-Ahead PV Estimation
Real datasets from Cupertino, CA, USA, from July
2015 to December 2016; and Catania, Sicily, Italy, from
January 2011 to December 2011 with a 15-min
sampling rate
LSTM and
AE
Improved the prediction accuracy to 8.39% nRMSE compared to
benchmarks with the proposed hybrid Auto Encoder (AE) LSTM model for
three months testing; the proposed persistence model (PM) has a high
accuracy of 0.72% nRMSE for consecutive clear days; Applied the Root
Mean Squared Euclidean Distance Difference (RMSEDD) to extract and
select the most valuable features to increase the model accuracy.
Energies 2021, 14, 4776
13 of 24
Table 4. Cont.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Chang et al. [70]
2020
Short-term Photovoltaic Power
Prediction for Edge Computing
Real PV output and PV meteorological dataset; one
sample every 30 min
LightGBM
Proposed a tree-structured self-organizing map (TS-SOM) algorithm for
clustering weather; used Bayesian optimization algorithm is employed for
temporal pattern aggregation to determine the optimal size of time steps;
the proposed LightGBM outperforms other algorithms in training and
execution time (0.020 s) with 35.49 RMSE accuracy, suited for edge
computing devices.
Khan et al. [71]
2020
Islanding Classiﬁcation for
Grid-Connected PV
Simulation data; total size equals 4526 samples and 7
features; 3168 samples for training and 679 samples
for testing
ANN
Proposed islanding detection model-based Wavelet transform for feature
extraction and Multi-layered Perceptron (MLP) for classiﬁcation with 97.8%
accuracy under 0.2 s on unseen conditions.
Wang et al. [72]
2019
Key Weather Factors from
Analytical Modeling Toward
Improved PV Forecasting
Real hourly dataset for a year of three PV arrays in
Australia from April 2012 to June 2013 with 11
independent variables
SVM, ANN,
and KNN
Improved the accuracy level for each season by using PCA for feature
extraction and KNN for classifying the prediction period into the historical
periods with the most similar weather situations; for example, on sunny
days, with the proposed method, SVM has 3.97 instead of 8.14, ANN has
4.09 instead of 8.45, and weighted KNN has 8.86 instead of 9.33 nRMSE
accuracy; this method helps ANN converges much faster with 37.72%
computational time reduction.
Gao et al. [73]
2020
Fault Identiﬁcation Method for
Photovoltaic Array
Simulation dataset with 1320 samples and
experimental dataset with 1892 samples with a ratio of
6:2:2 for training, validation, and testing
CNN and
GRU
Outperformed benchmark methods with 98.41% accuracy in 28.1 ms
detection time using CNN as automatic feature extractions and
Residual-GRU for memorizing time-series dynamic features; outperformed
benchmarks also in the presence of 10 dB to 50 dB noise level; reached
accuracy of 95.23% when some features are missing (temperatures and
irradiances).
Catalina et al. [74]
2020
PV Energy Nowcasting
Hourly satellite and Numerical Weather Predictions
(NWP) dataset with 4645 sample size for 2015
SVR
Proposed Gaussian SVRs models using satellite data and NWP information
to improve the PV energy nowcasting in the three real experimental
regions.
Ray et al. [75]
2020
Long-term PV Output
Forecasting
Historical hourly datasets of 24 years of four different
locations in North Queensland in Australia; dataset
from 1990 to 2013 was used for training and 2014 for
testing
LSTM and
CNN
The proposed hybrid model, consisting of CNN and LSTM, outperforms
other methods with RMSE lower than 15 for all studied locations and low
computational cost (203.63 s) for training and prediction.
Yap et al. [76]
2020
Grid Integration of PV
Simulation dataset with 0.1 s sampling time
Reinforcement
learning
Proposed the new virtual inertia control algorithm for integrating PV to a
grid with higher frequency nadir, lower frequency deviation (reduced by
0.1 Hz), smaller steady-state error (reduced by 27%), faster settling time
(reduced by 35%), lesser active power injection or absorption, and lesser
overshooting compared to traditional approaches.
Keerthisinghe et al.
[77]
2019
Energy Management of
PV-Storage Systems
Historical and one year-long simulation datasets with
30 min time intervals for each day
ANN
Proposed an ANN-based model based on dynamic programming (DP),
which, compared to other methods, has better quality and faster response
time (27.15 s); this method reduced a daily and yearly electric cost by more
than 50% for four different scenarios considering PV output, electrical
demand, electricity price, and battery SOC.
Energies 2021, 14, 4776
14 of 24
Table 5. Overview of Research using Machine Learning for Transportation.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Ashqar et al. [78]
2019
Transportation Mode Recognition
Real data of GPS, accelerometer, gyroscope, and
rotation vector sensors through a smartphone app for
10 travelers with 25 Hz sampling frequency
Two-layer
hierarchical
framework RF-SVM
Introduced new extracted frequency domain features and
increased accuracy rate to 97.02% compared to 95.10% of
traditional approaches
Jia [79]
2019
Analysis of Alternative Fuel
Vehicle Adoption
Person-, household-, trip- and vehicle real-dataset
(2017 NHTS Dataset) from April 2016 to April 2017
RF
Extraction inﬂuencing factors from large-scale 2017 NHTS Dataset
and Categorized them; RF outperformed other models (LR, NB,
SVM, and DT) with good accuracy (97.99%) and high AUC value
for adoption prediction.
Aksjonov et al.
[80]
2019
Detection and Evaluation of
Driver Distraction
Simulation data: speed limit, a radius of the road,
lane-keeping offset, and vehicle speed for 18 subjects
with 50 Hz sampling frequency
Nonlinear regression
based on Euclidean
distance and Fuzzy
logic
The proposed method improved the RMSE accuracy level from
2.1345 to 1.9992 for speed and 0.1506 to 0.1405 for distance.
Training time also decreased from 148.072 to 96.150 s compared to
the standard ANFIS predictor
Nallaperuma et al.
[81]
2019
Online Smart Trafﬁc
Management
Real-time Bluetooth sensor network data and social
media data (Twitter) from the arterial road network in
Victoria, Australia; 24 and 7 days data for training and
testing, respectively, with data horizon equal to 15 min
LSTM and
Reinforcement
learning
Short-term trafﬁc ﬂow with normal ﬂuctuation prediction with
0.0727 MAE accuracy; overcome the limitation of labeling data and
strict assumptions regarding data and trafﬁc behaviors.
Gjoreski et al. [82]
2020
Monitoring Driver Distractions
Real data of 68 people through physiological sensors,
the emotional response, and facial-expression
extraction with 1 Hz sampling frequency
Comparison of
classical ML and
deep learning
algorithms
The classical extreme gradient boosting (XGB) outperforms the
deep learning method with 94% F1-score accuracy compared to
87% for classifying complete driving sessions.
Li et al. [83]
2019
Security: SQL Injection Detection
Real-data and data augmentation from enterprises
and various social platforms; 36,422 real samples and
30,000 generated samples
Deep LSTM network
Overcome the overﬁtting problem and increase accuracy
(93.47–99.58%) due to data augmentation compared to the shallow
and deep ML algorithms.
Ou et al. [84]
2019
Real-Time Estimation of
Dynamic Origin-Destination ﬂow
Generate training dataset from real-trafﬁc dataset and
trafﬁc survey, and testing on real-time data with
15-min intervals sampling for 15 days in June 2017
CNN
Capture the dynamic mapping patterns and reconstruct
trajectories with MAPE average accuracy less than 5 (vehicle/15
min) on testing
Khadilkar et al.
[85]
2018
Scheduling Railway Lines
Real single- and multi-track railway data of different
routes with various number of trains and stations in
routes
Reinforcement
learning
Scalable to large scale dataset due to transfer learning; manage
large, realistic problem instances in computation times and
outperform other traditional techniques.
Zhang et al. [86]
2020
Short-term Passenger Demand
Prediction
Real taxi dataset of New York City from January 2016
to June 2018 for 63 zones
MTL-TCNN
An automatic feature selector algorithm; outperform other models
with 2.5% RMSE accuracy
Cheng et al. [87]
2018
High-Speed Trains Positioning
Beijing-Shanghai high-speed railway real-data
contains 725 groups of data
KNN
Improve KNN performance by applying ant colony optimization
(ACO) and online learning algorithms; obtain a better cluster
number of positioning data; Outperform other algorithms with
2.21 MAE accuracy.
Alawad et al. [88]
2019
Railway Safely and Accidents
Real data of accidents and passenger information like
passenger age and time of accident occurrence for
71 accidents
DT
Developed a classiﬁcation model regarding the occurrence of
accidents with good prediction accuracy of 88.7% on test data
Energies 2021, 14, 4776
15 of 24
Table 6. Overview of Research Using Machine Learning for Load forecasting.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Zhang et al. [89]
2020
Medium-term Load Forecasting
Two real-world datasets: New York (1200 hourly
electricity demand values of February 2018) and
Queensland (1200 half-hour electric load values of
January 2017) region
VMD, SR,
SVR, CBCS
This study proposed a novel hybrid model based on variational mode
decomposition (VMD), self-recurrent (SR) mechanism, support vector
regression (SVR), chaotic mapping mechanism, and cuckoo search (CBCS).
The VMD-SR-SVRCBCS outperformed other medium-range prediction
methods (240 half-hours window) in both cases with 2.5 and 0.9 MAPE of
New York and Queensland, respectively.
Feng et al. [90]
2020
Short-term Load Forecasting
Real hourly load data of University of Texas as Dallas
for 2014 and 2015
Reinforcement
learning
This study proposed a deterministic and probabilistic load prediction using
the two Q-learning agents to select the best model locally from
deterministic load forecasting methods and ten state-of-the-art ML-based
models. The results show 50–60% accuracy improvements compared to
single-phase benchmarks models.
Ahmad et al. [91]
2019
A-Day Ahead Load Forecasting
in Smart Grids
Real hourly data of two USA grids
(DAYTOWN, Ohio and EKPC, Kentucky) for two
years (2014–2015)
ANN
This study considers both accuracy and execution time to develop their
model to scale well in bigger datasets. The authors introduced the
pre-preparation, prediction, and optimization modules. Taking advantage
of a heuristics-based optimization method minimized MAPE while
reaching 98.76% accuracy, which was relatively better than existing bi-level
techniques.
Zheng et al. [92]
2017
Short- and Medium- Term Load
Prediction
Real hourly data of electricity load of ISO New
England (2003–2016)
PCA, LSTM,
XGBoost with
K-means
The authors presented a hybrid algorithm based on supervised and
unsupervised machine learning techniques as follows: ﬁrstly, they applied
empirical mode decomposition (EMD) and similar days selection days to
extract dominant features, then, they made predictions with LSTM
considering a very rich dataset for 11 years for training, one year for
validation, and one year (2016) for testing. The similarity between days
achieved by XGboost-based weighted k-means. The testing results for
one-day and one-week ahead shows this hybrid method improved the
average accuracy of the LSTM-based model from 5.43 to 1.08 MAPE and
8.74 to 1.59 for a day ahead and a week ahead, respectively.
Dabbaghjamanesh
et al. [93]
2020
A-Day Ahead Load Forecasting
for EV Charging Station
Synthetic dataset with hourly resolution
Reinforcement
learning
This study proposed a reinforcement learning-based model to predict a day
ahead EV charging station load demand. The proposed Q-learning model
outperformed CNN and RNN models in three different scenarios
(coordinated, uncoordinated, and smart charging) in terms of MSE metrics.
Higher accuracy, higher speed, and ﬂexibility are three main advantages of
the proposed model.
Energies 2021, 14, 4776
16 of 24
Table 6. Cont.
Reference
Year
Application
Data
Method(s)
Remark(s)/Contribution(s)
Farsi et al. [94]
2021
Short to Long Term Ahead Load
Forecasting (1–30 days ahead)
Real datasets of hourly load consumption of
Malaysia (2009 to 2010) and Germany (2012–2016)
CNN and
LSTM
This article proposed a parallel LSTM-CNN Network (PLCNet). Compared
to others, this study’s main advantage is to use LSTM and CNN in parallel
and concatenate their outputs with a dense layer to make the ﬁnal
prediction. The proposed method outperformed statistical and machine
learning models with 98.23% R-square accuracy for Malaysians and
improved Germany’s R-square accuracy from 83.17 to 91.18% for a day
ahead load prediction.
Hafeez et al. [95]
2020
A-Day Ahead Load Forecasting
Real hourly load data of three USA power grids (FE,
EKPC, and Dayton) from 2005 to 2012
ANN
(restricted
Boltzmann
machine)
The authors introduced a hybrid model based on a deep neural network
(restricted Boltzmann machine), modiﬁed mutual information (MMI)
technique to extract features, and proposed a genetic wind-driven (GWDO)
optimization method to adjust the model’s parameters. Together with their
ﬁne data engineering procedure, this new optimization algorithm helps to
improve the MAPE accuracy between 4.7% to 17.3% compared to
benchmarks. Moreover, their model’s average convergence time rate is 52 s
which is less than 58–102 s of benchmarks’ expectations time.
Han et al. [96]
2019
Medium to Long Term Load
Forecasting (a week to a year)
Two hourly real daily load datasets, Hangzhou from
January 2015 to March 2017 and Toronto from May
2002 to July 2016.
CNN and
LSTM
The authors proposed two methods, time-dependency convolutional
neural network (TD-CNN) and cycle-based long short-term memory
(C-LSTM), that outperformed other benchmarks in terms of accuracy and
execution time. Their models’ main advantages are extraction of the
long-term global combined features and short-term local similar features in
the LSTM-based model and conversion of load’s temporal correlation into
spatial ones in the CNN-based model.
Chen et al. [97]
2019
A-Day Ahead Load Forecasting
Two hourly real datasets North American Utility and
the ISO-NE from 1985 to 1992; the datasets of 1991 and
1992 were used for testing
ANN with
residual
connections
This study introduced a deep neural network with residual connections,
one of the well-known techniques to overcome the problem of lost
information in earlier layers in a deep network by applying direct links
from primary layers to deeper ones. Applying ensemble strategy on the
two rich datasets provides the generalization capacity of their model. The
proposed model improved the MAPE error rate from 1.48 of the best
benchmark model to 1.447 in the ISO-NE dataset and from 1.73 to 1.575 for
the North-American utility dataset, which also implies the robustness to
temperature variation of the proposed model.
El-Hendawi et al.
[98]
2020
A-Day Ahead Load Forecasting
Real dataset of the hourly electric market of Ontario,
Canada from 2011 to 2016
ANN
The authors used the wavelet transform to decompose the input data into
different levels with different frequencies to feed several neural networks.
Instead of having one model, they trained different neural-based models
with part of transformed input data and made ﬁnal forecasting considering
all models’ predictions. The proposed ensemble model improved the
MAPE accuracy by 20% compared to other traditional neural networks.
Energies 2021, 14, 4776
17 of 24
4. Discussion
ML-based algorithms have shown remarkable results in power system analytics
compared to traditional methods. However, even if the models proposed by the literature
showed to work ﬁne in real datasets, their performance in industrial applications has not
been sufﬁciently demonstrated yet, due to cost or privacy issues. This suggests the need
for further investigations at the industrial level, where the presence of input data with
different distributions or big data properties (e.g., volume, velocity, variety, and veracity)
could decrease the performance of ML models.
Regarding the data used for system validation, the studies generally presented cus-
tomized datasets. They typically provided information on the total number of samples,
sampling frequency, recording time, and percentage of data used for training and validation.
As several datasets were synthetically generated using simulation software, only various
studies reported problems with imbalanced datasets and missing items in the data. In this
regard, Hong et al. [45] analyzed the case in which data were missing from one of the buses,
concluding that system performance decreased significantly. Karagiannopoulos et al. [46] ex-
trapolated historical data and used information from the public domain or from neighboring
systems to deal with missing or noisy data. In this sense, Hafeez et al. [95] replaced missing
values with the average values of preceding days, while El-Hendawi et al. [98] replaced miss-
ing data with the average values of the same day in previous years. Similarly, Ray et al. [75]
used measurements from past hours to fill in missing data and performed data cleaning
to exclude incorrect data from training. Jia [79], Ou et al. [84], and Alawad et al. [88] also
highlighted the need to clean up missing data, while Li et al. [44] wrote the missing features
as zero to keep the dimension of the matrix constant. Additionally, Gao et al. [73] presented
an ML-based fault detection system in a photovoltaic array and quantified the impact of
missing PV input data (irradiance, temperature, and different combinations of them) on
system accuracy. On the other hand, Li et al. [83], Vantuch et al. [54], and Liao et al. [53]
discussed the effect of the imbalanced dataset on performance. In this sense, Wang et al. [57]
solved the data imbalance problem using an enhancement method that equalized the amount
of data (random cropping of existing data to generate a new dataset, increase of random
noise, signal reversing, etc.). Similarly, Jia [79] applied a synthetic minority over-sampling
technique that addressed the dataset imbalance problem without overfitting the classifier.
The lack of standard datasets for the testing of ML-based algorithms also emerged as
a relevant issue. Indeed, all the models presented in the literature are usually tested on
not-standard datasets, with very different characteristics and peculiarities, thus making the
comparison of the performance of such methods almost impossible. It is then apparent that,
when it comes to the selection of the most suitable ML method to be implemented in large
scale applications, this lack of information represents a relevant issue, that would eventually
prevent the implementation of novel (and potentially more performing) methods in favor
of (probably less performing) traditional ones. This, in the end, highlights the need for the
deﬁnition of application-speciﬁc standard datasets, to allow a fair comparison between
the very different ML methods proposed for each application. The standardized dataset
should have the following properties:
Size: considering the industrial side, the dataset size should be considerably big with
high dimensionality. Although some weak learners, such as DT, showed to work perfectly
with a small amount of data, they would not well generalize in the big domain. On the
contrary, neural network models have better accuracy results in the big domain;
Quality: if the focus is only on the performance of the machine learning model, the
different input datasets should have the same properties. For example, some models are
very robust to none values or outliers while others are not. Preparing a dataset before
feeding it to a model relates to data engineering procedures rather than to the model
performance;
Validity: the dataset should accurately represent the phenomena or reality of events.
The statistical properties of the standardized dataset should be as much as possible close to
real-life scenarios to show how practical models are;
Energies 2021, 14, 4776
18 of 24
Uniqueness and completeness: the information should be unique and not be du-
plicated over the dataset to make sure a trained model will generalize well enough in
actual cases. Moreover, it should cover all the possible occurrences or conditions. When
considering, for example, the power quality disturbance classiﬁcation, the dataset should
include all the essential distortions;
Train and test division: it is important the make sure that the performance of all
models are evaluated with the same train set. Otherwise, a chosen test set probably only
consists of easy instances, or it does not consist of all the possibilities;
Accuracy metrics: authors used different metrics to evaluate their model performance;
however, it is not possible to compare various studies when the same accuracy metrics are
not used. The metrics should be proposed taking into account the nature of problems. For
example, there are much fewer abnormal events in anomaly detection than normal, so the
model with 99% accuracy does not guarantee that it correctly detected all abnormal events;
for such studies, F1-score or AUC should be taken into account.
Researchers proposed different models based on one or more techniques. Figure 3.
shows the frequencies of techniques presented in the literature review of this study. In
this ﬁgure, ANN consists of the traditional neural network such as MLP and Boltzmann
machine, SVM includes both classiﬁcation and regression, and PCA encompasses all PCA
methods.
Energies 2021, 14, x FOR PEER REVIEW 
21 of 27 
 
Accuracy metrics: authors used different metrics to evaluate their model perfor-
mance; however, it is not possible to compare various studies when the same accuracy 
metrics are not used. The metrics should be proposed taking into account the nature of 
problems. For example, there are much fewer abnormal events in anomaly detection than 
normal, so the model with 99% accuracy does not guarantee that it correctly detected all 
abnormal events; for such studies, F1-score or AUC should be taken into account. 
Researchers proposed different models based on one or more techniques. Figure 3. 
shows the frequencies of techniques presented in the literature review of this study. In 
this figure, ANN consists of the traditional neural network such as MLP and Boltzmann 
machine, SVM includes both classification and regression, and PCA encompasses all PCA 
methods. 
 
Figure 3. Frequencies of used techniques presented in the literature review. 
Alternatively, it seems the hybrid models had better performances compared to oth-
ers, particularly the one that combined feature engineering techniques with prediction 
models. Reinforcement learning methods such as Q-learning have also enhanced accuracy 
in some applications like intelligent transportation systems and load forecasting. In some 
applications, such as PV prediction or load forecasting, which deal with temporal datasets, 
some sequential techniques such as GRU or LSTM are preferred. 
5. Conclusions 
When facing the challenges related to the management of smart power systems, it 
became apparent that traditional techniques are no more computationally promising so-
lutions. One of the limitations of conventional algorithms is their inadequate capacity to 
handle a large amount of data—consisting of chunks of heterogeneous datasets—collect-
ing from measurement devices such as phasor measurement units and smart meters. As 
a result, many researchers developed high-level, efficient, and reliable solutions based on 
state-of-the-art intelligent learning algorithms to provide innovative solutions or promote 
the overall performance of current models in various power system fields. In this context, 
the ML paradigm and modern ML algorithms are categorized and presented in this arti-
cle. Furthermore, this study provided a systematic overview of the latest machine learning 
techniques and models employed to bring new resolutions in power flows, power quality 
events, power quality parameters, photovoltaic systems, intelligent transportation sys-
Figure 3. Frequencies of used techniques presented in the literature review.
Alternatively, it seems the hybrid models had better performances compared to others,
particularly the one that combined feature engineering techniques with prediction models.
Reinforcement learning methods such as Q-learning have also enhanced accuracy in
some applications like intelligent transportation systems and load forecasting. In some
applications, such as PV prediction or load forecasting, which deal with temporal datasets,
some sequential techniques such as GRU or LSTM are preferred.
5. Conclusions
When facing the challenges related to the management of smart power systems, it
became apparent that traditional techniques are no more computationally promising so-
lutions. One of the limitations of conventional algorithms is their inadequate capacity to
handle a large amount of data—consisting of chunks of heterogeneous datasets—collecting
from measurement devices such as phasor measurement units and smart meters. As a
result, many researchers developed high-level, efﬁcient, and reliable solutions based on
state-of-the-art intelligent learning algorithms to provide innovative solutions or promote
the overall performance of current models in various power system ﬁelds. In this context,
Energies 2021, 14, 4776
19 of 24
the ML paradigm and modern ML algorithms are categorized and presented in this article.
Furthermore, this study provided a systematic overview of the latest machine learning
techniques and models employed to bring new resolutions in power ﬂows, power quality
events, power quality parameters, photovoltaic systems, intelligent transportation systems,
and load forecasting services. The authors also suggested the properties of a standard
dataset for testing and reviewing the ML-based models to make a fair comparison between
the performances of proposed models for each topic. However, the literature analysis
implies that hybrid models based on supervised machine learning algorithms are applied
more exceeding than unsupervised or semi-supervised techniques. Thus, it can be high-
lighting that supervised algorithms convey more beneﬁts to problems typically faced by
electrical power engineers. Finally, it can also be concluded that the application of machine
learning methods in electrical systems simpliﬁes complex issues and ensures more reliable
and accurate results. As numerous works proposed solutions based on ML techniques,
the authors limited their research to well-known newly published articles. Accordingly, in
future work, the authors focus on and review articles related to each topic separately to
provide an informative survey.
Author Contributions: Conceptualization, S.M.M., F.F. and M.L.; methodology, S.M.M. and M.P.;
formal analysis, S.M.M.; investigation, S.M.M.; resources, S.M.M.; writing—original draft preparation,
S.M.M. and M.P.; writing—review and editing, S.M.M., M.P., M.L, R.I. and F.F.; visualization, S.M.M.;
supervision, M.L. and F.F.; project administration, M.L. All authors have read and agreed to the
published version of the manuscript.
Funding: This research received no external funding.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
Abbreviation
Abbreviation
Meaning
ACO
Ant colony optimization
Adrar
Algerian
AE
Autoencoder
AI
Artiﬁcial intelligence
ANFIS
Adaptive neuro-fuzzy inference system
ANN
Artiﬁcial neural network
AUC
Area under curve
CBCS
Chaotic mapping mechanism, and cuckoo search
CNN
Convolutional neural network
CPS
Cyber-physical system
D
Dimensional
DALFs
Day-Ahead Load Forecasts
dB
Decibels
DBSCAN
Density-based spatial clustering of applications with noise
DERs
Distributed energy resources
DNN
Deep neural network
DP
Dynamic Programming
DT
Decision Tree
DWT
Discrete wavelet transform
ELM
Extreme learning machine
EMD
Empirical mode decomposition
EV
Electric vehicles
FFT
Fast Fourier transform
FM
Dactorization machine
FN
False negative
Energies 2021, 14, 4776
20 of 24
FP
False positive
GANs
Generative adversarial networks
GBM
Gradient boosted trees
GPS
Global positioning system
GRU
Gated recurrent Unit
ICA
Independent component analysis
IEEE
Institute of electrical and electronics engineers
IoT
Internet of things
IPCA
Improved principal component analysis
KNN
K-nearest neighbors
LASSO
Least absolute shrinkage and selection operator
LR
Logistic regression/Linear regression
LSTM
Long short term memory
MAE
Mean absolute error
MAPE
Mean absolute percentage error
ML
Machine learning
MLP
Multi-layered perceptron
ms
milliseconds
MSE
Mean square error
NHTS
National household travel survey
OPAELM
Online p-norm adaptive extreme learning machine
OPF
Optimal power ﬂow
PCA
Principle component analysis
PCWT
Pseudo-continuous wavelet transform
p.u.
Per unit
PM
Persistence model
PMU
Phasor measurement units
PQ
Power quality
PQEs
Power quality events
PSO
Particle swarm optimization
PSO-H-ELM
PSO hierarchical ELM
PV
Photovoltaic
R2
R-squared
RESs
Renewable energy sources
RF
Random forest
RK
Reduced kernel
RMSE
Root mean squared error
RMSEDD
Root mean squared Euclidean distance difference
RNN
Recurrent Neural Network
ROC curve
Receiver operating characteristic curve
RTU
Remote terminal units
SELM
Stacked extreme learning machine
SoC
State of charge
SPM
Space phasor model
SR
Self-recurrent mechanism
STFT
Short-time Fourier transform
SVM
Support vector machine
SVR
Support vector regression
TKEO
Teager–Kaiser energy operator
TN
True negative
TP
True positive
TS-SOM
Tree-structured self-organizing map
VMD
Variational mode decomposition
WASMs
Wide-area system measures
XGB/XGboost
Extreme gradient boosting
Energies 2021, 14, 4776
21 of 24
References
1.
Howell, S.; Rezgui, Y.; Hippolyte, J.L.; Jayan, B.; Li, H. Towards the next generation of smart grids: Semantic and holonic
multi-agent management of distributed energy resources. Renew. Sustain. Energy Rev. 2017, 77, 193–214. [CrossRef]
2.
Memon, A.A.; Kauhaniemi, K. A critical review of AC Microgrid protection issues and available solutions. Electr. Power Syst. Res.
2015, 129, 23–31. [CrossRef]
3.
Hu, J.; Vasilakos, A.V. Energy Big Data Analytics and Security: Challenges and Opportunities. IEEE Trans. Smart Grid 2016, 7,
2423–2436. [CrossRef]
4.
Hong, T.; Chen, C.; Huang, J.; Lu, N.; Xie, L.; Zareipour, H. Guest Editorial Big Data Analytics for Grid Modernization. IEEE
Trans. Smart Grid 2016, 7, 2395–2396. [CrossRef]
5.
Wang, B.; Fang, B.; Wang, Y.; Liu, H.; Liu, Y. Power System Transient Stability Assessment Based on Big Data and the Core Vector
Machine. IEEE Trans. Smart Grid 2016, 7, 2561–2570. [CrossRef]
6.
Pasetti, M.; Ferrari, P.; Silva, D.R.C.; Silva, I.; Sisinni, E. On the Use of LoRaWAN for the Monitoring and Control of Distributed
Energy Resources in a Smart Campus. Appl. Sci. 2020, 10, 320. [CrossRef]
7.
Pasetti, M.; Rinaldi, S.; Flammini, A.; Longo, M.; Foiadelli, F. Assessment of electric vehicle charging costs in presence of
distributed photovoltaic generation and variable electricity tariffs. Energies 2019, 12, 499. [CrossRef]
8.
Miraftabzadeh, S.M.; Foiadelli, F.; Longo, M.; Pasetti, M. A Survey of Machine Learning Applications for Power System Analytics.
In Proceedings of the 2019 IEEE International Conference on Environment and Electrical Engineering and 2019 IEEE Industrial
and Commercial Power Systems Europe (EEEIC/I CPS Europe), Genova, Italy, 10–14 June 2019; pp. 1–5.
9.
Hastie, T.; Tibshirani, R.; Friedman, J. Springer Series in Statistics. In The Elements of Statistical Learning, 2nd ed.; Springer: New
York, NY, USA, 2009; ISBN 978-0-387-84858-7.
10.
Agneeswaran, V.S.; Tonpay, P.; Tiwary, J. Paradigms for realizing machine learning algorithms. Big Data 2013, 1, 207–214.
[CrossRef]
11.
Huang, G.; Song, S.; Gupta, J.N.D.; Wu, C. Semi-supervised and unsupervised extreme learning machines. IEEE Trans. Cybern.
2014, 44, 2405–2417. [CrossRef]
12.
Breiman, L.; Friedman, J.H.; Olshen, R.A.; Stone, C.J. Chapman and Hall/CRC. In Classiﬁcation and Regression Trees; Routledge:
Abingdon, UK, 1984; ISBN 9780412048418.
13.
Qiu, J.; Wu, Q.; Ding, G.; Xu, Y.; Feng, S. A survey of machine learning for big data processing. EURASIP J. Adv. Signal Process.
2016, 2016, 1–16. [CrossRef]
14.
Wiering, M.A.; van Hasselt, H. Ensemble Algorithms in Reinforcement Learning. IEEE Trans. Syst. Man Cybern. Part B 2008, 38,
930–936. [CrossRef] [PubMed]
15.
Bartlett, P.L.; Jordan, M.I.; McAuliffe, J.D. Convexity, classiﬁcation, and risk bounds. J. Am. Stat. Assoc. 2006, 101, 138–156.
[CrossRef]
16.
Kleinbaum, D.G.; Klein, M. Statistics for Biology and Health. In Logistic Regression, 2nd ed.; Springer: New York, NY, USA, 2002;
ISBN 978-0-387-21647-8.
17.
Peterson, L. K-nearest neighbor. Scholarpedia 2009, 4, 1883. [CrossRef]
18.
Keller, J.M.; Gray, M.R. A Fuzzy K-Nearest Neighbor Algorithm. IEEE Trans. Syst. Man Cybern. 1985, SMC-15, 580–585. [CrossRef]
19.
Devroye, L.; Györﬁ, L.; Lugosi, G. Stochastic Modelling and Applied Probability. In A Probabilistic Theory of Pattern Recognition,
1st ed.; Springer: New York, NY, USA, 1996; Volume 31, ISBN 978-1-4612-0711-5.
20.
Rish, I. An empirical study of the naive Bayes classiﬁer. In Proceedings of the Seventeenth International Joint Conference on
Artiﬁcial Intelligence (IJCAI 2001), Seattle, WA, USA, 4–10 August 2001; American Association for Artiﬁcial Intelligence: Seattle,
WA, USA, 2001; Volume 3, pp. 41–46.
21.
Noble, W.S. What is a support vector machine? Nat. Biotechnol. 2006, 24, 1565–1567. [CrossRef]
22.
Safavian, S.R.; Landgrebe, D. A Survey of Decision Tree Classiﬁer Methodology. IEEE Trans. Syst. Man Cybern. 1991, 21, 660–674.
[CrossRef]
23.
Song, Y.Y.; Lu, Y. Decision tree methods: Applications for classiﬁcation and prediction. Shanghai Arch. Psychiatry 2015, 27, 130–135.
[CrossRef] [PubMed]
24.
Strobl, C.; Boulesteix, A.L.; Zeileis, A.; Hothorn, T. Bias in random forest variable importance measures: Illustrations, sources and
a solution. BMC Bioinform. 2007, 8, 1–21. [CrossRef]
25.
Montgomery, D.C.; Peck, E.A.; Vining, G.G. Probability and Statistics. In Introduction to Linear Regression Analysis, 6th ed.; John
Wiley & Sons: Hoboken, NJ, USA, 2021; ISBN 978-1-119-57875-8.
26.
Seber, G.A.F.; Lee, A.J. Linear Regression Analysis; John Wiley & Sons: Hoboken, NJ, USA, 2003; ISBN 978-0-471-41540-4.
27.
Prasad, A.M.; Iverson, L.R.; Liaw, A. Newer classiﬁcation and regression tree techniques: Bagging and random forests for
ecological prediction. Ecosystems 2006, 9, 181–199. [CrossRef]
28.
Anthony, M.; Bartlett, P.L. Neural Network Learning: Theoretical Foundations; Cambridge University Press: Cambridge, UK, 2009;
ISBN 9780521118620.
29.
Kalchbrenner, N.; Grefenstette, E.; Blunsom, P. A Convolutional Neural Network for Modelling Sentences. In Proceedings of the
52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, MD, USA, 22–27
June 2014; pp. 655–665.
Energies 2021, 14, 4776
22 of 24
30.
Gers, F.A.; Schmidhuber, J.; Cummins, F. Learning to forget: Continual prediction with LSTM. Neural Comput. 2000, 12, 2451–2471.
[CrossRef]
31.
Huang, G.B.; Zhu, Q.Y.; Siew, C.K. Extreme learning machine: Theory and applications. Neurocomputing 2006, 70, 489–501.
[CrossRef]
32.
Chen, T.; Guestrin, C. XGBoost: A scalable tree boosting system. In Proceedings of the ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, San Francisco, CA, USA, 13–17 August 2016; ACM: San Francisco, CA, USA, 2016;
Volume 13–17, pp. 785–794.
33.
Likas, A.; Vlassis, N.; Verbeek, J.J. The global k-means clustering algorithm. Pattern Recognit. 2003, 36, 451–461. [CrossRef]
34.
Birant, D.; Kut, A. ST-DBSCAN: An algorithm for clustering spatial-temporal data. Data Knowl. Eng. 2007, 60, 208–221. [CrossRef]
35.
Bendat, J.S.; Piersol, A.G. Engineering Applications of Correlation and Spectral Analysis, 2nd ed.; John Wiley & Sons: New York, NY,
USA, 2013; ISBN 978-0-471-57055-4.
36.
Shlens, J. A Tutorial on Principal Component Analysis. arXiv 2014, arXiv:1404.1100.
37.
Tschannen, M.; Bachem, O.; Lucic, M. Recent advances in autoencoder-based representation learning. In Proceedings of the Third
workshop on Bayesian Deep Learning (NeurIPS 2018), Montréal, QC, Canada, 7 December 2018.
38.
Aygun, R.C.; Yavuz, A.G. Network Anomaly Detection with Stochastically Improved Autoencoder Based Models. In Proceedings
of the 4th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud 2017), New York, NY, USA, 26–28
June 2017; IEEE: New York, NY, USA, 2017; pp. 193–198.
39.
Lei, X.; Yang, Z.; Yu, J.; Zhao, J.; Gao, Q.; Yu, H. Data-Driven Optimal Power Flow: A Physics-Informed Machine Learning
Approach. IEEE Trans. Power Syst. 2021, 36, 346–354. [CrossRef]
40.
Wang, S.; Dehghanian, P.; Li, L.; Wang, B. A Machine Learning Approach to Detection of Geomagnetically Induced Currents in
Power Grids. IEEE Trans. Ind. Appl. 2020, 56, 1098–1106. [CrossRef]
41.
Ravikumar, G.; Govindarasu, M. Anomaly Detection and Mitigation for Wide-Area Damping Control using Machine Learning.
IEEE Trans. Smart Grid 2020, 1. [CrossRef]
42.
Zhang, Y.; Wang, X.; Wang, J.; Zhang, Y. Deep Reinforcement Learning Based Volt-VAR Optimization in Smart Distribution
Systems. IEEE Trans. Smart Grid 2021, 12, 361–371. [CrossRef]
43.
Baker, K.; Bernstein, A. Joint Chance Constraints in AC Optimal Power Flow: Improving Bounds through Learning. IEEE Trans.
Smart Grid 2019, 10, 6376–6385. [CrossRef]
44.
Li, N.; Li, B.; Gao, L. Transient Stability Assessment of Power System Based on XGBoost and Factorization Machine. IEEE Access
2020, 8, 28403–28414. [CrossRef]
45.
Hong, G.; Kim, Y.S. Supervised Learning Approach for State Estimation of Unmeasured Points of Distribution Network. IEEE
Access 2020, 8, 113918–113931. [CrossRef]
46.
Karagiannopoulos, S.; Aristidou, P.; Hug, G. Data-Driven Local Control Design for Active Distribution Grids Using Off-Line
Optimal Power Flow and Machine Learning Techniques. IEEE Trans. Smart Grid 2019, 10, 6461–6471. [CrossRef]
47.
Zhao, Y.; Chen, J.; Poor, H.V. A Learning-to-Infer Method for Real-Time Power Grid Multi-Line Outage Identiﬁcation. IEEE Trans.
Smart Grid 2020, 11, 555–564. [CrossRef]
48.
King, J.E.; Jupe, S.C.E.; Taylor, P.C. Network State-Based Algorithm Selection for Power Flow Management Using Machine
Learning. IEEE Trans. Power Syst. 2015, 30, 2657–2664. [CrossRef]
49.
Labed, I.; Labed, D. Extreme learning machine-based alleviation for overloaded power system. IET Gener. Transm. Distrib. 2019,
13, 5058–5070. [CrossRef]
50.
Ray, P.K.; Mohanty, A.; Panigrahi, T. Power quality analysis in solar PV integrated microgrid using independent component
analysis and support vector machine. Optik (Stuttg.) 2019, 180, 691–698. [CrossRef]
51.
Sahani, M.; Dash, P.K.; Samal, D. A real-time power quality events recognition using variational mode decomposition and
online-sequential extreme learning machine. Measurement 2020, 157, 107597. [CrossRef]
52.
Turovic, R.; Stanisavljevic, A.; Dragan, D.; Katic, V. Machine learning for application in distribution grids for power quality
applications. In Proceedings of the 2019 20th International Symposium on Power Electronics (Ee 2019), Novi Sad, Serbia, 23–16
October 2019; IEEE: Novi Sad, Serbia, 2019; pp. 1–6.
53.
Liao, H.; Milanovic, J.V.; Rodrigues, M.; Shenﬁeld, A. Voltage Sag Estimation in Sparsely Monitored Power Systems Based on
Deep Learning and System Area Mapping. IEEE Trans. Power Deliv. 2018, 33, 3162–3172. [CrossRef]
54.
Vantuch, T.; Misak, S.; Jezowicz, T.; Burianek, T.; Snasel, V. The Power Quality Forecasting Model for Off-Grid System Supported
by Multiobjective Optimization. IEEE Trans. Ind. Electron. 2017, 64, 9507–9516. [CrossRef]
55.
Bagheri, A.; Gu, I.Y.H.; Bollen, M.H.J.; Balouji, E. A Robust Transform-Domain Deep Convolutional Network for Voltage Dip
Classiﬁcation. IEEE Trans. Power Deliv. 2018, 33, 2794–2802. [CrossRef]
56.
Sahani, M.; Dash, P.K. Automatic Power Quality Events Recognition Using Modes Decomposition Based Online P-Norm Adaptive
Extreme Learning Machine. IEEE Trans. Ind. Inform. 2020, 16, 4355–4364. [CrossRef]
57.
Wang, J.; Xu, Z.; Che, Y. Power quality disturbance classiﬁcation based on DWT and multilayer perceptron extreme learning
machine. Appl. Sci. 2019, 9, 2315. [CrossRef]
58.
Shen, Y.; Abubakar, M.; Liu, H.; Hussain, F. Power Quality Disturbance Monitoring and Classiﬁcation Based on Improved PCA
and Convolution Neural Network for Wind-Grid Distribution Systems. Energies 2019, 12, 1280. [CrossRef]
Energies 2021, 14, 4776
23 of 24
59.
Deng, Y.; Wang, L.; Jia, H.; Tong, X.; Li, F. A Sequence-to-Sequence Deep Learning Architecture Based on Bidirectional GRU for
Type Recognition and Time Location of Combined Power Quality Disturbance. IEEE Trans. Ind. Inform. 2019, 15, 4481–4493.
[CrossRef]
60.
Cao, J.; Zhang, W.; Xiao, Z.; Hua, H. Reactive Power Optimization for Transient Voltage Stability in Energy Internet via Deep
Reinforcement Learning Approach. Energies 2019, 12, 1556. [CrossRef]
61.
Abed, A. Improved Power Factor of Electrical Generation by using Clustering Neural Network. Int. J. Appl. Eng. Res. 2018, 13,
4633–4636.
62.
Zhang, X.; Wang, Y.; Zheng, Y.; Ding, R.; Chen, Y.; Wang, Y.; Cheng, X.; Yue, S. Reactive Load Prediction Based on a Long
Short-Term Memory Neural Network. IEEE Access 2020, 8, 90969–90977. [CrossRef]
63.
Nakawiro, W. A Machine Learning Approach for Coordinated Voltage and Reactive Power Control. ECTI Trans. Electr. Eng.
Electron. Commun. 2020, 18, 54–60. [CrossRef]
64.
Moreira, A.C.; Paredes, H.K.M.; de Souza, W.A.; Nardelli, P.H.J.; Marafão, F.P.; da Silva, L.C.P. Evaluation of Pattern Recognition
Algorithms for Applications on Power Factor Compensation. J. Control Autom. Electr. Syst. 2018, 29, 75–90. [CrossRef]
65.
Valenti, M.; Bonﬁgli, R.; Principi, E.; Squartini, S. Exploiting the Reactive Power in Deep Neural Models for Non-Intrusive Load
Monitoring. In Proceedings of the 2018 International Joint Conference on Neural Networks (IJCNN), Rio de Janeiro, Brazil, 8–13
July 2018; pp. 1–8.
66.
Keerthisinghe, C.; Mickelson, E.; Kirschen, D.S.; Shih, N.; Gibson, S. Improved PV Forecasts for Capacity Firming. IEEE Access
2020, 8, 152173–152182. [CrossRef]
67.
Wen, S.; Zhang, C.; Lan, H.; Xu, Y.; Tang, Y.; Huang, Y. A hybrid ensemble model for interval prediction of solar power output in
ship onboard power systems. IEEE Trans. Sustain. Energy 2021, 12, 14–24. [CrossRef]
68.
Dhibi, K.; Fezai, R.; Mansouri, M.; Trabelsi, M.; Kouadri, A.; Bouzara, K.; Nounou, H.; Nounou, M. Reduced Kernel Random
Forest Technique for Fault Detection and Classiﬁcation in Grid-Tied PV Systems. IEEE J. Photovolt. 2020, 10, 1864–1871. [CrossRef]
69.
Zhang, Y.; Qin, C.; Srivastava, A.K.; Jin, C.; Sharma, R.K. Data-Driven Day-Ahead PV Estimation Using Autoencoder-LSTM and
Persistence Model. IEEE Trans. Ind. Appl. 2020, 56, 7185–7192. [CrossRef]
70.
Chang, X.; Li, W.; Zomaya, A.Y. A Lightweight Short-Term Photovoltaic Power Prediction for Edge Computing. IEEE Trans. Green
Commun. Netw. 2020, 4, 946–955. [CrossRef]
71.
Khan, M.A.; Kurukuru, V.S.B.; Haque, A.; Mekhilef, S. Islanding Classiﬁcation Mechanism for Grid-Connected Photovoltaic
Systems. IEEE J. Emerg. Sel. Top. Power Electron. 2020, 9, 1966–1975. [CrossRef]
72.
Wang, J.; Zhong, H.; Lai, X.; Xia, Q.; Wang, Y.; Kang, C. Exploring key weather factors from analytical modeling toward improved
solar power forecasting. IEEE Trans. Smart Grid 2019, 10, 1417–1427. [CrossRef]
73.
Gao, W.; Wai, R.J. A Novel Fault Identiﬁcation Method for Photovoltaic Array via Convolutional Neural Network and Residual
Gated Recurrent Unit. IEEE Access 2020, 8, 159493–159510. [CrossRef]
74.
Catalina, A.; Alaiz, C.M.; Dorronsoro, J.R. Combining Numerical Weather Predictions and Satellite Data for PV Energy Nowcast-
ing. IEEE Trans. Sustain. Energy 2020, 11, 1930–1937. [CrossRef]
75.
Ray, B.; Shah, R.; Islam, M.R.; Islam, S. A New Data Driven Long-Term Solar Yield Analysis Model of Photovoltaic Power Plants.
IEEE Access 2020, 8, 136223–136233. [CrossRef]
76.
Yap, K.Y.; Sarimuthu, C.R.; Lim, J.M.Y. Grid Integration of Solar Photovoltaic System Using Machine Learning-Based Virtual
Inertia Synthetization in Synchronverter. IEEE Access 2020, 8, 49961–49976. [CrossRef]
77.
Keerthisinghe, C.; Chapman, A.C.; Verbiˇc, G. Energy Management of PV-Storage Systems: Policy Approximations Using Machine
Learning. IEEE Trans. Ind. Inform. 2019, 15, 257–265. [CrossRef]
78.
Ashqar, H.I.; Almannaa, M.H.; Elhenawy, M.; Rakha, H.A.; House, L. Smartphone transportation mode recognition using a
hierarchical machine learning classiﬁer and pooled features from time and frequency domains. IEEE Trans. Intell. Transp. Syst.
2019, 20, 244–252. [CrossRef]
79.
Jia, J. Analysis of Alternative Fuel Vehicle (AFV) Adoption Utilizing Different Machine Learning Methods: A Case Study of 2017
NHTS. IEEE Access 2019, 7, 112726–112735. [CrossRef]
80.
Aksjonov, A.; Nedoma, P.; Vodovozov, V.; Petlenkov, E.; Herrmann, M. Detection and Evaluation of Driver Distraction Using
Machine Learning and Fuzzy Logic. IEEE Trans. Intell. Transp. Syst. 2019, 20, 2048–2059. [CrossRef]
81.
Nallaperuma, D.; Nawaratne, R.; Bandaragoda, T.; Adikari, A.; Nguyen, S.; Kempitiya, T.; De Silva, D.; Alahakoon, D.; Pothuhera,
D. Online Incremental Machine Learning Platform for Big Data-Driven Smart Trafﬁc Management. IEEE Trans. Intell. Transp. Syst.
2019, 20, 4679–4690. [CrossRef]
82.
Gjoreski, M.; Gams, M.Z.; Luštrek, M.; Genc, P.; Garbas, J.U.; Hassan, T. Machine Learning and End-to-End Deep Learning for
Monitoring Driver Distractions from Physiological and Visual Signals. IEEE Access 2020, 8, 70590–70603. [CrossRef]
83.
Li, Q.; Wang, F.; Wang, J.; Li, W. LSTM-Based SQL Injection Detection Method for Intelligent Transportation System. IEEE Trans.
Veh. Technol. 2019, 68, 4182–4191. [CrossRef]
84.
Ou, J.; Lu, J.; Xia, J.; An, C.; Lu, Z. Learn, Assign, and Search: Real-Time Estimation of Dynamic Origin-Destination Flows Using
Machine Learning Algorithms. IEEE Access 2019, 7, 26967–26983. [CrossRef]
85.
Khadilkar, H. A Scalable Reinforcement Learning Algorithm for Scheduling Railway Lines. IEEE Trans. Intell. Transp. Syst. 2019,
20, 727–736. [CrossRef]
Energies 2021, 14, 4776
24 of 24
86.
Zhang, K.; Liu, Z.; Zheng, L. Short-Term Prediction of Passenger Demand in Multi-Zone Level: Temporal Convolutional Neural
Network with Multi-Task Learning. IEEE Trans. Intell. Transp. Syst. 2020, 21, 1480–1490. [CrossRef]
87.
Cheng, R.; Song, Y.; Chen, D.; Ma, X. Intelligent Positioning Approach for High Speed Trains Based on Ant Colony Optimization
and Machine Learning Algorithms. IEEE Trans. Intell. Transp. Syst. 2019, 20, 3737–3746. [CrossRef]
88.
Alawad, H.; Kaewunruen, S.; An, M. Learning from Accidents: Machine Learning for Safety at Railway Stations. IEEE Access
2020, 8, 633–648. [CrossRef]
89.
Zhang, Z.; Hong, W.C.; Li, J. Electric Load Forecasting by Hybrid Self-Recurrent Support Vector Regression Model with Variational
Mode Decomposition and Improved Cuckoo Search algorithm. IEEE Access 2020, 8, 14642–14658. [CrossRef]
90.
Feng, C.; Sun, M.; Zhang, J. Reinforced Deterministic and Probabilistic Load Forecasting via Q -Learning Dynamic Model
Selection. IEEE Trans. Smart Grid 2020, 11, 1377–1386. [CrossRef]
91.
Ahmad, A.; Javaid, N.; Mateen, A.; Awais, M.; Khan, Z.A. Short-Term load forecasting in smart grids: An intelligent modular
approach. Energies 2019, 12, 164. [CrossRef]
92.
Zheng, H.; Yuan, J.; Chen, L. Short-Term Load Forecasting Using EMD-LSTM neural networks with a xgboost algorithm for
feature importance evaluation. Energies 2017, 10, 1168. [CrossRef]
93.
Dabbaghjamanesh, M.; Moeini, A.; Kavousi-Fard, A. Reinforcement Learning-based Load Forecasting of Electric Vehicle Charging
Station Using Q-LearningTechnique. IEEE Trans. Ind. Inform. 2020, 17, 4229–4237. [CrossRef]
94.
Farsi, B.; Amayri, M.; Bouguila, N.; Eicker, U. On Short-Term Load Forecasting Using Machine Learning Techniques and a Novel
Parallel Deep LSTM-CNN Approach. IEEE Access 2021, 9, 31191–31212. [CrossRef]
95.
Hafeez, G.; Alimgeer, K.S.; Khan, I. Electric load forecasting based on deep learning and optimized by heuristic algorithm in
smart grid. Appl. Energy 2020, 269, 114915. [CrossRef]
96.
Han, L.; Peng, Y.; Li, Y.; Yong, B.; Zhou, Q.; Shu, L. Enhanced deep networks for short-term and medium-term load forecasting.
IEEE Access 2019, 7, 4045–4055. [CrossRef]
97.
Chen, K.; Chen, K.; Wang, Q.; He, Z.; Hu, J.; He, J. Short-Term Load Forecasting with Deep Residual Networks. IEEE Trans. Smart
Grid 2019, 10, 3943–3952. [CrossRef]
98.
El-Hendawi, M.; Wang, Z. An ensemble method of full wavelet packet transform and neural network for short term electrical
load forecasting. Electr. Power Syst. Res. 2020, 182, 106265. [CrossRef]


Paper 7:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: 
  Extract 2: 
  Limitations: >
  Relevance Evaluation: {'extract_1': 'This paper attempts to identify the requirement and the development of machine learning-based mobile big data (MBD) analysis through discussing the insights of challenges in the MBD and reviewing state-of-the-art applications of data analysis in the area of MBD.', 'extract_2': 'The remainder of the paper is organized as follows. Section 2 introduces the development of data analysis methods in this section, we present some recent achievements in data analysis from four different perspectives.', 'relevance_score': 0.8}
  Relevance Score: 0.9523809523809523
  Inline Citation: >
  Explanation: This article attempts to identify the need and development of machine learning-based mobile big data analysis through discussing the insights of challenges in the MBD and reviewing state-of-the-art applications of data analysis in the area of MBD.

 Full Text: >
This website stores data such as cookies to enable essential site functionality, as well as marketing, personalization, and analytics. By remaining on this website you indicate your consent. Cookie Policy Journals Publish with us Publishing partnerships About us Blog Wireless Communications and Mobile Computing Journal overview For authors For reviewers For editors Table of Contents Special Issues Wireless Communications and Mobile Computing/ 2018/ Article On this page Abstract Introduction Conclusions Conflicts of Interest Acknowledgments References Copyright Related Articles Special Issue Channel Characterization and Modeling for 5G and Future Wireless System Based on Big Data View this Special Issue Review Article | Open Access Volume 2018 | Article ID 8738613 | https://doi.org/10.1155/2018/8738613 Show citation A Survey on Machine Learning-Based Mobile Big Data Analysis: Challenges and Applications Jiyang Xie ,1Zeyu Song,1Yupeng Li,2Yanting Zhang,3Hong Yu,1Jinnan Zhan,2Zhanyu Ma ,1Yuanyuan Qiao,3Jianhua Zhang ,2and Jun Guo1 Show more Academic Editor: Liu Liu Received 09 Apr 2018 Accepted 07 Jun 2018 Published 01 Aug 2018 Abstract This paper attempts to identify the requirement and the development of machine learning-based mobile big data (MBD) analysis through discussing the insights of challenges in the mobile big data. Furthermore, it reviews the state-of-the-art applications of data analysis in the area of MBD. Firstly, we introduce the development of MBD. Secondly, the frequently applied data analysis methods are reviewed. Three typical applications of MBD analysis, namely, wireless channel modeling, human online and offline behavior analysis, and speech recognition in the Internet of Vehicles, are introduced, respectively. Finally, we summarize the main challenges and future development directions of mobile big data analysis. 1. Introduction With the success of wireless local access network (WLAN) technology (a.k.a. Wi-Fi) and the second/third/fourth generation (2G/3G/4G) mobile network, the number of mobile phones, which is 7.74 billion, 103.5 per 100 inhabitants all over the world in 2017, is rising dramatically [ 1]. Nowadays, mobile phone can not only send voice and text messages, but also easily and conveniently access the Internet which has been recognized as the most revolutionary development of mobile Internet (M-Internet). Meanwhile, worldwide active mobile-broadband subscriptions in 2017 have increased to 4.22 billion, which is 9.21% higher than that in 2016 [ 1]. Figure 1 shows the numbers of mobile-cellular telephone and active mobile-broadband subscriptions of the world and main districts from 2010 to 2017. The numbers which are up to the bars are the mobile-cellular telephone or active mobile-broadband subscriptions (million) in the world of the year which increase each year. Under the M-Internet, various kinds of content (image, voice, video, etc.) can be sent and received everywhere and the related applications emerge to satisfy people’s requirements, including working, study, daily life, entertainment, education, and healthcare. In China, mobile applications giants, i.e., Baidu, Alibaba, and Tencent, held 78% of M-Internet online time per day in apps which was about 2,412 minutes in 2017 [ 2]. This figure indicates that M-Internet has entered a rapid growth stage.   (a)     (a)  (b)     Figure 1  Mobile-cellular telephone subscriptions (million) in (a) and active mobile-broadband subscriptions (million) in (b) of the world and main districts [ 1]. Nowadays, more than 1 billion smartphones are in use and producing a great quantity of data every day. This situation brings far-reaching impacts on society and social interaction and increases great opportunities for business. Meanwhile, with the rapid development of the Internet-of-Things (IoT), much more data is automatically generated by millions of machine nodes with growing mobility, for example, sensors carried by moving objects or vehicles. The volume, velocity, and variety of these data are increasing extremely fast, and soon they will become the new criterion for data analytics of enterprises and researchers. Therefore, mobile big data (MBD) has been already in our lives and is being enriched rapidly. The trend for explosively increased data volume with the increasing bandwidth and data rate in the M-Internet has followed the same exponential increase as Moore’s Law for semiconductors [ 3]. The prediction [ 2] about the global data volume will grow up to 47 zettabytes () by 2020 and 163 zettabytes by 2025. For M-Internet, 3.7 exabytes () data have been generated per month from the mobile data traffic in 2015 [ 4], 7.2 exabytes in 2016 [ 5], 24 exabytes by 2019 on forecasting [ 5], and 49 exabytes by 2021 on forecasting [ 5]. According to the statistical and prediction results, a concept called MBD has appeared. The MBD can be considered as a huge quantity of mobile data which are generated from a massive number of mobile devices and cannot be processed and analyzed by a single machine [ 6, 7]. MBD is playing and will play a more important role than ever before by the popularization of mobile devices including smartphones and IoT gadgets especially in the era of 4G and the forthcoming the fifth generation (5G) [ 4, 8]. With the rapid development of information technologies, various data generated from different technical fields are showing explosive growth trends [ 9]. Big data has broad application prospects in many fields and has become important national strategic resources [ 10]. In the era of big data, many data analysis systems are facing big challenges as the volume of data increases. Therefore, analysis for MBD is currently a highly focused topic. The importance of MBD analysis is determined by its role in developing complex mobile systems which supports a variety of intelligently interactive services, for example, healthcare, intelligent energy networks, smart buildings, and online entertainments [ 4]. MBD analysis can be defined as mining terabyte-level or petabyte-level data collected from mobile users and wireless devices at the network-level or the app-level to discover unknown, latent, and meaningful patterns and knowledge with large-scale machine learning methods [ 11]. Present requirements of MBD are based on software-defined in order to be more scalable and flexible. M-Internet environment in the future will be even more complex and interconnected [ 12]. For this purpose, data centers of MBD need to collect user statistics information of millions of users and obtain meaningful results by proper MBD analysis methods. For the decreasing price of data storage and widely accessible high performance computers, an expansion of machine learning has come into not only theoretical researches, but also various application areas of big data. Even though, there is a long way to go for the machine learning-based MBD analysis. Machine learning technology has been used by many Internet companies in their services: from web searches [ 13, 14] to content filtering [ 15] and recommendation [ 16, 17] on online social communities, shopping websites, or contend distribution platforms. Furthermore, it is also frequently appearing in products like smart cellphones, laptop computers, and smart furniture. Machine learning systems are used to detect and classify objects, return most relevant searching results, understand voice commands, and analyze using habits. In recent years, big data machine learning has become a hot spot [ 18]. Some conventional machine learning methods based on Bayesian framework [ 19– 22], distributed optimization [ 23– 26], and matrix factorization [ 27] can be applied into the aforementioned applications and have obtained good performances in small data sets. On this foundation, researchers have always been trying to fill their machine learning model with more and more data [ 28]. Furthermore, the data we got is not only big but also has features such as multisource, dynamic and sparse value; these features make it harder to analyze MBD with conventional machine learning methods. Therefore, the aforementioned applications implemented with conventional machine learning methods have fallen in a bottleneck period for low accuracy and generalization. Recently, a class of novel techniques, called deep learning, is applied in order to make the effort to solve the problems and has obtained good performances [ 29]. Machine learning, especially deep learning, has been an essential technique in order to use big data effectively. Most conventional machine learning methods are shallow learning structures with one or none hidden layers. These methods performed well in practical use and were precisely analyzed theoretically. But when dealing with high-dimensional or complicated data, shallow machine learning methods show their weakness. Deep learning methods are developed to learn better representations automatically with deep structure by using supervised or unsupervised strategies [ 30, 31]. The features extracted by deep hidden layers are used for regression, classification, or visualization. Deep learning uses more hidden layers and parameters to fit functions which could extract high level features from complex data; the parameters will be set automatically using large amount of unsupervised data [ 32, 33]. The hidden layers of deep learning algorithms help the model learn better representation of data; the higher layers learn specific and abstract features from global features learned by lower layers. Many surveys show that nonlinear feature extractors that are linked up as stacks such as deep learning methods always perform better in machine learning tasks, for example, a more accurate classification method [ 34], better learning of data probabilistic models [ 35], and the extraction of robust features [ 36]. Deep learning methods have proved useful in data mining, natural language processing, and computer vison applications. A more detailed introduction of deep learning is presented in Section 3.1.4. Artificial Intelligence (AI) is a technology that develops theories, methods, techniques, and applications that simulate or extend human brain abilities. The research of observing, learning, and decision-making process in human brain motivates the development of deep learning, which was first designed aiming to emulate the human brain’s neural structures. Further observation on neural signals processing and the effect on brain mechanisms [ 37– 39] inspired the architecture design of deep learning network, using layers and neuron connections to generalize globally. Conventional methods such as support vector machines, decision trees, and case-based reasoning which are based on statistics or logic knowledge of human may fall short when facing complex structure or relationships of data. Deep learning methods can learn patterns and relationships from hidden layers and may benefit the signal processing study in human brain with visualization methods of neural network. Deep learning has attracted much attention from AI researchers recently because of its state-of-the-art performance in machine learning domains including no only the aforementioned natural language processing (NLP), but also speech recognition [ 40, 41], collaborative filtering [ 42], and computer vision [ 43, 44]. Deep learning has been successfully used in industry products which have access to big data from users. Companies in United States such as Google, Apple, Facebook, and Chinese companies like Baidu, Alibaba, and Tencent have been collecting and analyzing data from millions of users and pushing forward deep learning based applications. For example, Tencent YouTu Lab has developed identification (ID) card identification and bank card identification systems. These systems can read information from card images to check user information while registering and bank information while purchasing. The identification systems are based on deep learning model and large volume of user data provided by Tencent. Apple develops Siri, a virtual intelligent assistant in iPhones, to answer questions about weather, location, news according to voice commands and dial numbers or send text messages. Siri also utilizes deep learning methods and uses data from apple services [ 45]. Google uses deep learning on Google translation service with massive data collected by Google search engine. MBD contains a large variety of information of offline data and online real-time data stream generated from smart mobile terminals, sensors, and services and hastens various applications based on the advancement of data analysis technologies, such as collaborative filtering-based recommendation [ 46, 47], user social behavior characteristics analysis [ 48– 51], vehicle communications in the Internet of Vehicles (IoV) [ 52], online smart healthcare [ 53], and city residents’ activity analysis [ 6]. Although the machine learning-based methods are widely applied in the MBD fields and obtain good performances in real data test, the present methods still need to be further developed. Therefore, five main challenges facing MBD analysis regarding the machine learning-based methods include large-scale and high-speed M-Internet, overfitting and underfitting problems, generalization problem, cross-modal learning, and extended channel dimensions and should be considered. This paper attempts to identify the requirement and the development of machine learning-based mobile big data analysis through discussing the insights of challenges in the MBD and reviewing state-of-the-art applications of data analysis in the area of MBD. The remainder of the paper is organized as follows. Section 2 introduces the development of data collection and properties of MBD. The frequently adopted methods of data analysis and typical applications are reviewed in Section 3. Section 4 summarizes the future challenges of MBD analysis and provides suggestions. 2. Development and Collection of the Mobile Big Data 2.1. Data Collection Data collection is the foundation of a data processing and analysis system. Data are collected from mobile smart terminals and Internet services, or called mobile Internet devices (MIDs) generally, which are multimedia-capable mobile devices providing wireless Internet access and contain smartphones, wearable computers, laptop computers, wireless sensors, etc. [ 54]. MBD can be divided into two hierarchical data form: transmission and application data, from bottom to top. The transmission data focus on solving channel modeling [ 55, 56] and user access problems corresponding to the physical transmission system of M-Internet. On this foundation, application data focus on the applications based on the MBD including social networks analysis [ 57– 59], user behavior analysis [ 48, 50, 60], speech analysis and decision in IoV [ 61– 66], smart grid [ 67, 68], networked healthcare [ 53, 69, 70], finance services [ 46, 71], etc. Due to the heterogeneity of the M-Internet and the variety of the access devices, the collected data are unstructured and usually in many categories and formats, which make data preprocessing become an essential part of a data processing and analysis system in order to ensure the input data complete and reliable [ 72]. Data preprocessing can be divided into three steps which are data cleaning, generation of implicit ratings, and data integration [ 46]. (1) Data Cleaning. Due to possible equipment failures, transmission errors, or human factor, raw data are “dirty data” which cannot be directly used, generally [ 46]. Therefore, data cleaning methods including outlier detection and denoising are applied in the data preprocessing to obtain the data meet required quality. Manual removal of error data is difficult and impossible to accomplish in MBD due to the massive volume. Common data cleaning methods can alleviate the dirty data problem to some extent by training support vector regression (SVR) classifiers [ 73], multiple linear regression models [ 74], autoencoder [ 75], Bayesian methods [ 76– 78], unsupervised methods [ 79], or information-theoretic models [ 79]. (2) Generation of Implicit Ratings. Generation of implicit ratings is mainly applied in recommend systems. The volume of rating data increases rapidly by analyzing specific user behaviors to solve data sparsity problem with machine learning algorithms, for example, neural networks and decision trees [ 46]. (3) Data Integration. Data integration is a step to integrate data from different resources with different formats and categories and to handle missing data fields [ 7]. Figure 2 represents the procedures of data collection and preprocessing.    Figure 2  The procedures of data collection and preprocessing. 2.2. Properties of Mobile Big Data The MBD brings a massive amount of new challenges to conventional data analysis methods for its high dimensionality, heterogeneity, and other complex features from applications, such as planning, operation and maintenance, optimization, and marketing [ 57]. This section discusses the five Vs (short for volume, velocity, variety, value, and veracity) features [ 80] deriving from big data towards the MBD. The five Vs features have been improved in M-Internet, while it makes users access Internet anytime and anywhere [ 81]. (1) Volume: Large Number of MIDs, Exabyte-Level Data, and High-Dimensional Data Space. Volume is the most obvious feature of MBD. In the forthcoming 5G network and the era of MBD, conventional store and analysis methods are incapable of processing the 1000x or more wireless traffic volume [ 7, 82]. It is of great urgency to improve present MBD analysis methods and propose new ones. The methods should be simple and cost-effective to be implemented for MBD processing and analysis. Moreover, they should also be effective enough without requiring a massive amount of data for model training. Finally, they are precise to be applied in various fields [ 81]. (2) Velocity: Real-Time Data Streams and Efficiency Requirement. Velocity can be considered as the speed at which data are transmitted and analyzed [ 83]. The data is now continuously streaming into the servers in real-time and makes the original batch process break down [ 84]. Due to the high generating rate of MBD, velocity is the efficiency requirement of MBD analysis since real-time data processing and analysis are extremely important in order to maximize the value of MBD streams [ 7]. (3) Variety: Heterogeneous and Nonstructured Mobile Multimedia Contents. Due to the heterogeneity of MBD which means that mobile data traffic comes from spatially distributed data resources (i.e., MIDs), the variety of MBD arises and makes the MBD more complex [ 4]. Meanwhile, the nonstructured MBD also causes the variety. The MBD can be divided into structured data, semistructured data, and unstructured data. Here, unstructured data are usually collected in new applications and have random data fields and contents [ 7]; therefore, they are difficult to analyze before data cleaning and integration. (4) Value: Mining Hidden Knowledge and Patterns from Low Density Value Data. Value, or low density value of MBD, is caused by a large amount of useless or repeated information in the MBD. Therefore, we need to mine the big value by MBD analyzing which is hidden knowledge and patterns extraction. The purified data can provide comprehensive information to conduct more effectively analysis results about user demands, user behaviors, and user habits [ 85] and to achieve better system management and more accurate demand prediction and decision-making [ 86]. (5) Veracity: Consistency, Trustworthiness, and Security of MBD. The veracity of MBD includes two parts: data consistency and trustworthiness [ 80]. It can also be summarized as data quality. MBD quality is not guaranteed due to the noise of transmission channel, the equipment malfunctioning, and the uncalibrated sensors of MIDs or the human factor (for instance, malicious invasion) resulting in low-quality data points [ 4]. Veracity of MBD ensures that the data used in analysis process are authentic and protected from unauthorized access and modification [ 80]. 3. Applications of Machine Learning Methods in the Mobile Big Data Analysis 3.1. Development of Data Analysis Methods In this section, we present some recent achievements in data analysis from four different perspectives. 3.1.1. Divide-and-Conquer Strategy and Sampling of Big Data The strategies dividing and conquering big data is a computing paradigm dealing with big data problems. The development of distributed and parallel computing makes divide-and-conquer strategy particularly important. Generally speaking, whether the diversity of samples in learning data benefits the training results varies. Some redundant and noisy data can cause a large amount of storage cost as well as reducing the efficiency of the learning algorithm and affecting the learning accuracy. Therefore, it is more preferable to select representative samples to form a subset of original sample space according to a certain performance standard, such as maintaining the distribution of samples, topological structure, and keeping classification accuracy. Then learning method will be constructed on previous formed subset to finish the learning task. In this way, we can maintain or even improve the performance of big data analyzing algorithm with minimum computing and stock resources. The need to learn with big data demands on sample selection methods. But most of the sample selection method is only suitable for smaller data sets, such as the traditional condensed nearest neighbor [ 93], the reduced nearest neighbor [ 94], and the edited nearest neighbor [ 95]; the core concept of these methods is to find the minimum consistent subset. To find the minimum consistent subset, we need to test every sample and the result is very sensitive to the initialization of the subset and samples setting order. Li et al. [ 96] proposed a method to select the classification and edge boundary samples based on local geometry and probability distribution. They keep the space information of the original data but need to calculate k-means for each sample. Angiulli et al. [ 97, 98] proposed a fast condensation nearest neighbor (FCNN) algorithm based on condensed nearest neighbor, which tends to choose the classification boundary samples. Jordan [ 99] proposed statistical inference method for big data. When dealing with statistical inference with divide-and-conquer algorithm, we need to get confidence intervals from huge data sets. By data resampling and then calculating confidence interval, the Bootstrap theory aims to obtain the fluctuation of the evaluation value. But it does not fit big data. The incomplete sampling of data can lead to erroneous range fluctuations. Data sampling should be correct in order to provide statistical inference calibration. An algorithm named Bag of Little Bootstraps was proposed, which can not only avoid this problem, but also has many advantages on computation. Another problem discussed in [ 99] is massive matrix calculation. The divide-and-conquer strategy is heuristic, which has a good effect in practical application. However, new theoretical problems arise when trying to describe the statistical properties of partition algorithm. To this end, the support concentration theorem based on the theory of random matrices has been proposed. In conclusion, data partition and parallel processing strategy is the basic strategy to deal with big data. But the current partition and parallel processing strategy uses little data distribution knowledge, which has influence on the load balancing and the calculation efficiency of big data processing. Hence, there exists an urgent requirement to solve the problem about how to learn the distribution of big data for the optimization of load balancing. 3.1.2. Feature Selection of Big Data In the field of data mining, such as document classification and indexing, the dataset is always large, which contains a large number of records and features. This leads to the low efficiency of algorithm. By feature selection, we can eliminate the irrelevant features and increase the speed of task analysis. Thus, we can get a better preformed model with less running time. Big data processing faces a huge challenge on how to deal with high-dimensional and sparse data. Traffic network, smartphone communication records, and information shared on Internet provide a large number of high-dimensional data, using tensor (such as a multidimensional array) as natural representation. Tensor decomposition, in this condition, becomes an important tool for summary and analysis. Kolda [ 100] proposed an efficient use of the memory of the Tucker decomposition method named as memory-efficient Tucker (MET) decomposition decreasing time and space cost which traditional tensor decomposition algorithm cannot do. MET adaptively selects execution strategy based on available memory in the process of decomposition. The algorithm maximizes the speed of computation in the premise of using the available memory. MET avoid dealing with the large number of sporadic intermediate results proceeded during the calculation process. The adaptive selections of operation sequence not only eliminate the intermediate overflow problem, but also save memory without reducing the precision. On the other hand, Wahba [ 101] proposed two approaches to the statistical machine learning model which involve discrete, noisy, and incomplete data. These two methods are regularized kernel estimation (RKE) and robust manifold unfolding (RMU). These methods use dissimilarity between training information to get nonnegative low rank definite matrix. The matrix will then be embedded into a low dimensional Euclidean space, which coordinate can be used as features of various learning modes. Similarly, most online learning research needs to access all features of training instances. Such classic scenario is not always suitable for practical applications when facing high-dimensional data instances or expensive feature sets. In order to break through this limit, Hoi et al. [ 102] propose an efficient algorithm to predict online feature solving problem using some active features based on their study of sparse regularization and truncation technique. They also test the proposed algorithm in some public data sets for feature selection performance. The traditional self-organizing map (SOM) can be used for feature extraction. But the low speed of SOM limits its usage on large data sets. Sagheer [ 103] proposed a fast self-organizing map (FSOM) to solve this problem. The goal of this method is to find a feature space where data is mainly distributed in. If there exits such area, data can be extracted in these areas instead of information extraction in overall feature spaces. In this way, we can greatly reduce extraction time. Anaraki [ 104] proposed a threshold method of fuzzy rough set feature selection based on fuzzy lower approximation. This method adds a threshold to limit the QuickReduct feature selection. The results of the experiment prove that this method can also help the accuracy of feature extraction with lower running time. Gheyas et al. [ 105] proposed a hybrid algorithm of simulated annealing and genetic algorithm (SAGA), combining the advantages of simulated annealing algorithm, genetic algorithm, greedy algorithm, and neural network algorithm, to solve the NP-hard problem of selecting optimal feature subset. The experiment shows that this algorithm can find better optimal feature subset, reducing the time cost sharply. Gheyas pointed in as conclusion that there is seldom a single algorithm which can solve all the problems; the combination of algorithms can effectively raise the overall affect. To sum up, because of the complexity, high dimensionality, and uncertain characteristics of big data, it is an urgent problem to solve how to reduce the difficulty of big data processing by using dimension reduction and feature selection technology. 3.1.3. Big Data Classification Supervised learning (classification) faces a new challenge of how to deal with big data. Currently, classification problems involving large-scale data are ubiquitous, but the traditional classification algorithms do not fit big data processing properly. (1) Support Vector Machine (SVM). Traditional statistical machine learning method has two main problems when facing big data. (1) Traditional statistical machine learning methods are always involving intensive computing which makes it hard to apply on big data sets. (2) The prediction of model that fits the robust and nonparameter confidence interval is unknown. Lau et al. [ 106] proposed an online support vector machine (SVM) learning algorithm to deal with the classification problem for sequentially provided input data. The classification algorithm is faster, with less support vectors, and has better generalization ability. Laskov et al. [ 107] proposed a rapid, stable, and robust numerical incremental support vector machine learning method. Chang et al. [ 108] developed an open source package called LIBSVM as a library for SVM code implementation. In addition, Huang et al. [ 109] present a large margin classifier M4. Unlike other large margin classifiers which locally or globally constructed separation hyperplane, this model can learn both local and global decision boundary. SVM and minimax probability machine (MPM) has a close connection with the model. The model has important theoretical significance and furthermore, the optimization problem of maxi-min margin machine (M4) can be solved in polynomial time. (2) Decision Tree (DT). Traditional decision tree (DT), as a classic classification learning algorithm, has a large memory requirement problem when processing big data. Franco-Arcega et al. [ 110] put forward a method of constructing DT from big data, which overcomes some weakness of algorithms in use. Furthermore, it can use all training data without saving them in memory. Experimental results showed that this method is faster than current decision tree algorithm on large-scale problems. Yang et al. [ 111] proposed a fast incremental optimization decision tree algorithm for large data processing with noise. Compared with former decision tree data mining algorithm, this method has a major advantage on real-time speed for data mining, which is quite suitable when dealing with continuous data from mobile devices. The most valuable feature of this model is that it can prevent explosive growth of the decision tree size and the decrease of prediction accuracy when the data packet contains noise. The model can generate compact decision tree and predict accuracy even with highly noisy data. Ben-Haim et al. [ 112] proposed an algorithm of building parallel decision tree classifier. The algorithm runs in distributed environment and is suitable for large amount and streaming data. Compared with serial decision tree, the algorithm can improve efficiency under the premise of accuracy error approximation. (3) Neural Network and Extreme Learning Machine (ELM). Traditional feedforward neural networks usually use gradient descent algorithm to tune weight parameters. Generally speaking, slow learning speed and poor generalization performance are the bottlenecks that restrict the application of feedforward neural network. Huang et al. [ 113] discarded the iterative adjustment strategy of the gradient descent algorithm and proposed extreme learning machine (ELM). This method randomly assigns the input weights and the deviations of the single hidden layer neural network. It can analyze the output weights of the network by one step calculation. Compared to the traditional feedforward neural network training algorithm, the network weights can be determined by multiple iterations, and the training speed of ELM is significantly improved. However, due to the limitation of computing resource and computational complexity, it is a difficult problem to train a single ELM on big data. There are usually two ways to solve this problem: (1) training ELM [ 114] based with divide-and-conquer strategy; (2) introducing parallel mechanism [ 115] to train a single ELM. It is shown in [ 116, 117] that a single ELM has strong function approximation ability. Whether it is possible to extend this approximation capability to ELM based on divide-and-conquer strategy is a key index to evaluate the possibility that ELM can be applied to big data. Some of the related studies also include effective learning to solve such problem [ 118]. In summary, the traditional classification method of machine learning is difficult to apply to the analysis of big data directly. The study of parallel or improved strategies of different classification algorithms has become the new direction. 3.1.4. Big Data Deep Learning With the unprecedentedly large and rapidly growing volumes of data, it is hard for us to get hidden information from big data with ordinary machine learning methods. The shallow-structured learning architectures of most conventional learning methods are not fit for the complex structures and relationships in these input data. Big data deep learning algorithm, with its deep architectures and globally feature extracting ability, can learn complex patterns and hidden connections beyond big data [ 37, 119]. It has had state-of-the-art performances in many benchmarks and also been applied in industry products. In this section, we will introduce some deep learning methods in big data analytics. Big data deep learning has some problems: (1) the hidden layers of deep network make it difficult to learn from a given data vector, (2) the gradient descent method for parameters learning makes the initialization time increasing sharply as the number of parameters arises, and (3) the approximations at the deepest hidden layer may be poor. Hinton et al. [ 32] proposed a deep architecture: deep belief network (DBN) which can learn from both labeled and unlabeled data by using unsupervised pretraining method to learn unlabeled data distributions and a supervised fine-tune method to construct the models, and solved part of the aforementioned problems. Meanwhile, subsequent researches, for example, [ 120], improved the DBN trying to solve the problems. Convolutional neural network (CNN) [ 121] is another popular deep learning network structure for big data analyzing. A CNN has three common features including local receptive fields, shared weights, and spatial or temporal subsampling, and two typical types of layers [ 122, 123]. Convolutional layers are key parts of CNN structure aiming to extract features from image. Subsampling layers, which are also called pooling layers, adjust outputs from convolutional layer to get translation invariance. CNN is mainly applied in computer vision field for big data, for example, image classification [ 124, 125] and image segmentation [ 126]. Document (or textual) representation, also part of NLP, is the basic method for information retrieval and important to understand natural language. Document representation finds specific or important information from the documents by analyzing document structure and content. The unique information could be document topic or a set of labels highly related to the document. Shallow models for document representation only focus on small part of the text and get simple connection between words and sentences. Using deep learning can get global representation of the document because of its large receptive field and hidden layers which could extract more meaningful information. The deep learning methods for document representation make it possible to obtain features from high-dimensional textual data. Hinton et al. [ 127] proposed deep generative model to learn binary codes for documents which make documents easy to store up. Socher et al. [ 128] proposed a recursive neural network on analyzing natural language and contexts, achieving state-of-the-art results on segmentation and understanding of natural language processing. Kumer et al. [ 129] proposed recurrent neural networks (RNN) which construct search space from large amount of textual data. With the rapid growth and complexity of academic and industry data sets, how to train deep learning models with large amount of parameters has been a major problem. The works in [ 40, 41, 43, 130– 133] proposed effective and stable parameter updating methods for training deep models. Researchers focus on large-scale deep learning that can be implemented in parallel including improved optimizers [ 131] and new structures [ 121, 133– 135]. In conclusion, big data deep learning methods are the key methods of data mining. They use complex structure to learn patterns from big data sets and multimodal data. The development of data storage and computing technology promotes the development of deep learning methods and makes it easier to use in practical situations. 3.2. Wireless Channel Modeling As is well known, wireless communication transmits information through electromagnetic waves between a transmitting antenna and a receiving antenna, which is deemed as a wireless channel. In the past few decades, the channel dimension has been extended to space, time, and frequency, which means the channel property is comprehensively discovered. Another development is that channel characteristics can be accurately described by different methods, such as channel modeling [ 136]. Liang et al. [ 137] used machine learning to predict channel state information so as to decease the pilot overhead. Especially for 5G, wireless big data emerges and its related technologies are employed to traditional communication research to meet the demand of 5G. However, the wireless channel is essentially a physical electromagnetic wave, and the current 5G channel model research follows the traditional way. Zhang [ 138] proposed an interdisciplinary study of big data and wireless channels, which is a cluster-based channel model. In the cluster-nuclei based channel model, the multipath components (MPCs) are aggregated into a traditional stochastically channel model. At the same time, the scene is discerned by the computer and the environment is rebuilt by machine learning methods. Then, by matching the real propagation objects with the clusters, the cluster-nuclei, which are the key factors in contacting deterministic environment and stochastic clusters, can be easily found. There are two main steps employing the machine learning methods in the cluster-nuclei based channel model. The recent progress is shown as follows. 3.2.1. A Gaussian Mixture Model (GMM) Based Channel MPCs Clustering Method The MPCs are clustered with the Gaussian mixture model (GMM) [ 87, 139]. Using sufficient statistic characteristics of channel multipath, the GMM can get clusters corresponding to the multipath propagation characteristics. The GMM assumes that all the MPCs consist of several Gaussian distributions in varying proportions. Given a set of channel multipath , the log-likelihood of the Gaussian mixture model is where is the set of all the parameters and is the prior probability satisfying the constraint . To estimate the GMM parameters, expectation maximization (EM) algorithm is employed to solve the log-likelihood function of GMM [ 87]. Figure 3 illustrates the simulation result of GMM clustering algorithm.    Figure 3  Clustering results of GMM [ 87]. As seen in Figure 3, the GMM clustering obtains clearly compact clusters. As scattering property of the channel multipath obeys Gaussian distribution, the compact clusters can accord with the multipath scattering property. Moreover, corresponding to the clustering mechanism of GMM, paper [ 87] proposed a compact index (CI) to evaluate the clustering results shown as follows: where is the variance of the kth cluster and and are given as where is the number of multipaths corresponding to the kth cluster. Both the means and variances of the clusters are considered in CI. Considering sufficient statistics characteristics, CI can uncover the inherent information of multipath parameters and provide appropriate explanation to the clustering result. Besides, considering sufficient statistics characteristics, the CI can evaluate the clustering results more reasonably. 3.2.2. Identifying the Scatters with the Simultaneous Localization and Mapping Algorithm (SLAM) In order to reconstruct three-dimensional (3D) propagation environment and to find the main deterministic objects, simultaneous localization and mapping (SLAM) algorithm is used to identify the texture from the measurement scenario picture [ 140, 141]. Figure 4 illustrates our indoor reconstruction result with SLAM algorithm.   (a)     (a)  (b)     Figure 4  Recognition of multiobjects with SLAM algorithm: (a) real indoor scene and (b) reconstruction result with SLAM algorithm. The texture of propagation environment can be used to search for the main scatters in the propagation environment. Then, the three-dimensional propagation environment can be reconstructed with the deep learning method. Then the mechanism to form the cluster-nuclei is clear. The channel impulse response can be produced by machine learning with a limited number of cluster-nuclei, i.e., decision tree [ 142], neural network [ 143], and mixture model [ 144]. Based on the database from various scenarios, antenna configurations, and frequency, channel changing rules can be explored and then input into the cluster-nuclei based modeling. Finally, the predication of channel impulse response in various scenarios and configuration can be realized [ 138]. 3.3. Analyses of Human Online and Offline Behavior Based on Mobile Big Data The advances of wireless networks and increasing mobile applications bring about explosion of mobile traffic data. It is a good source of knowledge to obtain the individuals’ movement regularity and acquire the mobility dynamics of populations of millions [ 145]. Previous researches have described how individuals visit geographical locations and employed mobile traffic data to analyze human offline mobility patterns. Representative works like [ 146, 147] explore the mobility of users in terms of the number of base stations they visited, which turned out to be a heavy tail distribution. Authors in [ 146, 148, 149] also reveal that a few important locations are frequently visited by users. In particular, these preferred locations are usually related to home and work places. Moreover, through defining a measure of entropy, Song et al. [ 150] believe that 93% of individual movements are potentially predictable. Thus, various models have been applied to describe the human offline mobility behavior [ 151]. Passively collecting human mobile traffic data while users are accessing the mobile Internet has many advantages like low energy consumption. In general, the mobile big data covers a wide range and a great number of populations with fine time granularity, which gives us an opportunity to study human mobility at a scale that other data sources are very hard to reach [ 152]. Novel offline user mobility models developed based on the mobile big data are expected to benefit many fields, including urban planning, road traffic engineering, telecommunication network construction, and human sociology [ 145]. Online browsing behavior is another important facet regarding user behavior when it comes to network resource consumption. A variety of applications are now available on smart devices, covering all aspects of our daily life and providing convenience. For example, we can order taxies, shop, and book hotels using mobile phones. Yang et al. [ 49] provide a comprehensive study on user behaviors in exploiting the mobile Internet. It has been found that many factors, such as data usage and mobility pattern, may impact people’s online behavior on mobile devices. It is discovered that the more the number of distinct cells a user visit, the more diverse applications user has visited. Zheng et al. [ 153] analyze the longitudinal impact of proximity density, personality, and location on smartphone traffic consumption. In particular, location has been proven to have strong influences on what kinds of apps users prefer to use [ 149, 153]. The aforementioned observations point out that there is a close relationship between online browsing behavior and offline mobility behavior. Figure 5(a) is an example of how browsed applications and current location related to each other from the view of temporal and spatial regularity. It has been found that the mobility behaviors have strong influences on online browsing behavior [ 149, 153, 154]. Similar trends can also be observed for crowds at crowd gathering places, as is shown in Figure 5(b); i.e., certain apps are favored at places that group people together and provide some specific functions. The authors in [ 50] tried to measure the relationship between human mobility and app usage behavior. In particular, the authors proposed a rating framework which can forecast the online app usage behavior for individuals and crowds. Building the bridge between human offline mobility and online mobile Internet behavior can tell us what people really need in daily life. Content providers can leverage this knowledge to appropriately recommend content for mobile users. At the same time, Internet service providers (ISPs) can use this knowledge to optimize networks for better end-user experiences.   (a) App usage behavior of Bob in temporal and spatial dimension     (a) App usage behavior of Bob in temporal and spatial dimension  (b) App usage behavior of crowds at crowd gathering place     Figure 5  App usage behavior in daily life: (a) the app usage behavior of an individual and (b) app usage behavior of crowds at crowd gathering places [ 50]. In order to make full use of users’ online and offline information, some researchers begin to quantize the interplay between online social network and offline social network and investigate network dynamics from the view of mobile traffic data [ 155– 158]. Specifically, the online and offline social networks are, respectively, constructed based on online interest based and location based social network among mobile users. The two different networks are grouped into layers of a multilayer social network , as shown in Figure 6. and depict offline and online social network separately. In each layer, the graph is described as , where and , respectively, represent node sets and edge sets. Nodes, such as , represent users. Edges exist among users when users share similar object-based interests [ 88]. Combining information from manifold networks in a multilayer structure provides a new insight into user interactions between virtual and physical worlds. It sheds light on the link generation process from multiple views, which will improve social bootstrapping and friend recommendations in various valuable applications by a large margin [ 158].    Figure 6  Multilayer model of a network [ 88]. So far, we have summarized some representative works related to human online and offline behaviors. It is meaningful to note that owing to the highly spatial-temporal and nonhomogeneous nature of mobile traffic data, a pervasive framework is challenging yet indispensable to realize the collection, processing, and analyses of massive data, reducing resource consumption and improving Quality of Experience (QoE). The seminal work by Qiao et al. [ 60] proposes a framework for MBD (FMBD). It provides comprehensive functions on data collection, storage, processing, analyzing, and management to monitor and analyze the massive data. Figure 7(a) displays the architecture of FMBD, while Figure 7(b) shows the considered mobile networks framework. With the interaction between user equipment and 2G/3G/4G network, real massive mobile data can be collected by traffic monitoring equipment (TME). The implementation modules are employed based on Apache software [ 159]. FMBD builds a security environment and easy-to-use platform both for operators and data analysts, showing good performance on energy efficiency, portability, extensibility, usability, security, and stability. In order to meet the increasing demands on traffic monitoring and analyzing, the framework provides a solution to deal with large-scale mobile big data.    Figure 7  The overall architecture of framework for mobile big data (FMBD) and our considered mobile networks architecture [ 60]. In conclusion, the prosperity of continuously emerging mobile applications and users’ increasing demands on accessing Internet all bring about challenges for current and future mobile networks. This section surveys the literature on analyses of human online and offline behavior based on the mobile traffic data. Moreover, a framework has also been investigated, in order to meet the higher requirement of dealing with dramatically increased mobile traffic data. The analyses based on the big data will provide valuable information for the ISPs on network deployment, resource management, and the design of future mobile network architectures. 3.4. Speech Recognition and Verification for the Internet of Vehicles With the significant development of smart vehicle produces, intelligent vehicle based Internet of Vehicle (IoV) technologies have received widespread attention of many giant Internet businesses [ 160– 162]. The IoV technologies include the communication between different vehicles and vehicles to sensors, roads, and humans. These communications can help the IoV system sharing and the gathering information on vehicles and their surrounds. One of the challenges in the real-life applications of smart vehicles and IoV systems is how to design a robust interactive method between drivers and the IoV system [ 163]. The level of focusing on driving will directly affect the danger of driver and passengers; hence, the attention of drivers should be paid on the complex road situation in order to avoid accidents during an intense driving. So, using the voices transfer information to the IoV systems is an effective solution for assistant and cooperative driving. By building a speech recognition interactive system, the driver can check traffic jams near the destination or order a lunch in the restaurant near the rest stop through the IoV system by using voice-based interaction. The speech recognition interactive system for IoV system can reduce the risk of vehicle accident, and the drivers do not need to touch the control panels or any buttons. A useful speech recognition system in IoV can simplify the life of the drivers and passengers in vehicles [ 164]. In the IoV system, drivers want to use their own voice commands to control the driving vehicles, and the IoV system must recognize the difference between an authorized and unauthorized user. Therefore, an automatic speaker verification system is necessary in IoV, which can protect the vehicle from the imposters. Recently, many deep learning methods have been applied in the speech recognition and speaker verification systems [ 41, 165– 167], and published results show that speech processing methods driven by MBD and deep learning can obviously improve the performance of the existing speech recognition and speaker verification system [ 40, 168, 169]. In the IoV systems, millions of sensors collect abundant vehicles and environmental noises from engines and streets will significantly reduce the accuracy of speech processing system, while the traditional speech enhancement methods, for example, Wiener filtering [ 170] and minimum mean-square error estimation (MMSE) [ 171] which focus on advancing signal noise ratio (SNR), do not take full advantage of a priori distribution of noises around vehicles. With the help of machine learning and deep learning methods, we can use a priori knowledge of the noises to improve the robustness of speech processing systems. For speech recognition task, deep-neural-network (DNN) can be applied to train an effective monophone classifier, instead of the traditional GMM based classifier. Moreover, the deep-neural-network hidden Markov model (DNN-HMM) speech recognition model can significantly improve the performance of Gaussian mixture model hidden Markov model (GMM-HMM) models [ 172– 174]. As shown in Figure 8, making full use of the self-adaption power of DNN, we can use the multitraining methods to improve the robustness of DNN monophone classifier by adding noise into the training data [ 89]. The experimental results in [ 89, 175] show that the multitraining method can build a matched training and testing condition which can improve the accuracy of noisy speech recognition, especially for the prior knowledge of noise types that we can easily obtain in vehicles.    Figure 8  Multitraining DNN [ 89]. As shown in Figure 9, a DNN can also be used to train a feature mapping network (FMN) which uses noisy features as input and corresponding clean features as training target. Enhanced features extracted by the FMN can improve the performance of speech recognition systems. Han et al. [ 176] used FMN to extract one enhanced Mel-frequency cepstral coefficient (MFCC) frame from 15 noisy MFCCs frames. Xu et al. [ 90] built a FMN which learned the mapping from a log spectrogram to a log Mel filter bank. The enhanced feature can remarkably reduce the word error rate in speech recognition.    Figure 9  DNN used for feature mapping [ 90]. Besides getting the mapping feature directly, the DNN can also be used to train an ideal binary mask (IBM) which can be used to separate the clean speech from background noise as shown in Figure 10 [ 91, 177, 178]. With a priori knowledge of noise types and SNR, we can generate IBMs as training targets and use noisy power spectral as training data. In the test phase, we can use the learned IBMs to get enhanced features which can improve the robustness of speech recognition.    Figure 10  DNN used for IBMs learning [ 91]. In speaker verification tasks, the classical GMM based methods, for example, Gaussian mixture model universal background model (GMM-UBM) [ 179] and i-vector systems [ 180], need to build a background GMM, firstly, using a large quantity of speaker independent speeches. Then, by computing the statistics information on each GMM component of enrollment speakers, we can get speaker models or speaker i-vectors. However, a trained monophone classification DNN can replace the function of GMM by computing the statistics information on each monophone instead of on GMM components. Many published papers [ 181– 184] show that the DNN-i-vector based speaker verification systems work better than the GMM-i-vector method on detection accuracy and robustness. Unlike in the speech recognition tasks where the DNNs are used to get enhanced features from noisy features, researchers more prefer to use a DNN or convolutional neural network (CNN) to generate noise robustness bottleneck feature directly in speaker verification tasks [ 185– 187]. As shown in Figure 11, acoustic features or feature maps are used to train a DNN/CNN with a bottleneck layer which has less nodes and closes to the output layer. Speaker ID, noise types, monophone labels, or combination of these labels are used as training targets. Outputs of bottleneck layers include abundant differentiated information and can be used as speaker verification features which improve the performance of classical speaker verification methods such as the aforementioned GMM-UBM and i-vector. Similar to the multitraining method, adding noisy speeches into the training data can also improve the robustness of extracted bottleneck features [ 65, 92].    Figure 11  DNN/CNN used for extracting bottleneck feature [ 92]. Recently, some adversarial training methods are introduced to extract noise invariant bottleneck features [ 64, 188]. As shown in Figure 12, the adversarial network includes two parts, i.e., an encoding network (EN) which can extract noise invariant features and a discriminative network (DN) which can judge noise types of the noise invariant feature generated from EN. Therefore, we can get robustness noise invariant features from EN which can improve the performance of speaker verification system by adversarial training these two parts in turn [ 64, 188].    Figure 12  Adversarial training network for noise invariant bottleneck feature extraction [ 64]. In conclusion, using DNN and machine learning methods can make full use of the MBD collected from the IoV systems. Moreover, it improves the performance of speech recognition and speaker verification methods applied in the voice interactive systems. 4. Conclusions and Future Challenges Although the machine learning-based methods introduced in Section 3 are widely applied in the MBD fields and obtain good performances in real data test, the present methods still need to be further developed. Therefore, five main challenges facing MBD analysis regarding the machine learning-based methods should be considered as follows. (1) Large-Scale and High-Speed M-Internet. Due to the growth of MIDs and high speed of M-Internet, increasingly various mobile data traffic is introduced and results in a heavy load to the wireless transmission system, which leads us to improve wireless communication technologies including WLAN and cellular mobile communication. In addition, the requirement of real-time services and applications depends on the development of machine learning-based MBD analysis methods towards high efficiency and precision. (2) Overfitting and Underfitting Problems. A benefit of MBD to machine learning and deep learning lies in the fact that the risk of overfitting becomes smaller with more and more data available for training [ 28]. However, underfitting is another problem for the oversize data volume. In this condition, a larger model might be a better selection, while the model can express more hidden information of the data. Nevertheless, larger model which generally implies a deeper structure increases runtime of the model which affects the real-time performance. Therefore, the model size in machine learning and deep learning, which represents number of parameters, should be balanced to model performance and runtime. (3) Generalization Problem. As the massive scale of MBD, it is impossible to gain entire data even if they are only in a specific field. Therefore, the generalization ability which can be defined as suitable of different data subspace, or called scalability, of a trained machine learning or deep learning model is of great importance for evaluating the performance. (4) Cross-Modal Learning. The variety of MBD causes multiple modalities of data (for example, images, audios, personal location, web documents, and temperature) generated from multiple sensors (correspondingly, cameras, sound recorders, position sensor, and temperature sensor). Multimodal learning should learn from multimodal and heterogeneous input data with machine learning and deep learning [ 4, 189] and obtain hidden knowledge and meaningful patterns; however, it is quite difficult to discover. (5) Extended Channel Dimensions. The channel dimensions have been extended to three domains, i.e., space, time, and frequency, which means that the channel property is comprehensively discovered. Meanwhile, the increasing antenna number, high bandwidth, and various application scenarios bring the big data of channel measurements and estimations, especially for 5G. The finding channel characteristics need to be precisely described by more advanced channel modeling methodologies. In this paper, the applications and challenges of machine learning-based MBD analysis in the M-Internet have been reviewed and discussed. The development of MBD in various application scenarios requires more advanced data analysis technologies especially machine learning-based methods. Three typical applications of MBD analysis focus on wireless channel modeling, human online and offline behavior analysis, and speech recognition and verification in the Internet of Vehicles, respectively, and the machine learning-based methods used are widely applied in many other fields. In order to meet the aforementioned future challenges, three main study aims, i.e., accuracy, feasibility, and scalability [ 28], are highlighted for present and future MBD analysis research. In future work, accuracy improving will be also the primary task on the basis of a feasible architecture for MBD analysis. In addition, as the aforementioned discussion of the generalization problem, scalability has obtained more and more attentions especially in a classification or recognition problem where scalability also includes the increase in the number of inferred classes. It is of great importance to improve the scalability of the methods with the high accuracy and feasibility in order to face the analysis requirements of MBD. Conflicts of Interest The authors declare that they have no conflicts of interest. Acknowledgments This paper was supported in part by the National Natural Science Foundation of China (NSFC) [Grant no. 61773071]; in part by the Beijing Nova Program Interdisciplinary Cooperation Project [Grant no. Z181100006218137]; in part by the Beijing Nova Program [Grant no. Z171100001117049]; in part by the Beijing Natural Science Foundation (BNSF) [Grant no. 4162044]; in part by the Funds of Beijing Laboratory of Advanced Information Networks of BUPT; in part by the Funds of Beijing Key Laboratory of Network System Architecture and Convergence of BUPT; and in part by BUPT Excellent Ph.D. Students Foundation [Grant no. XTCX201804]. References International Telecommunication Union (ITU), “ICT Facts and Figures 2017,” https://www.itu.int/en/ITU-D/Statistics/Pages/facts/default.aspx, 2017. View at: Google Scholar Meeker, “Internet Trend 2017,” http://www.kpcb.com/internet-trends, 2017. View at: Google Scholar G. Fettweis and S. Alamouti, “5G: personal mobile internet beyond what cellular did to telephony,” IEEE Communications Magazine, vol. 52, no. 2, pp. 140–145, 2014. View at: Publisher Site | Google Scholar M. A. Alsheikh, D. Niyato, S. Lin, H.-P. Tan, and Z. Han, “Mobile big data analytics using deep learning and apache spark,” IEEE Network, vol. 30, no. 3, pp. 22–29, 2016. View at: Publisher Site | Google Scholar Cisco, “Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2016-2021 White Paper,” https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/mobile-white-paper-c11-520862.html, 2017. View at: Google Scholar Y. Guo, J. Zhang, and Y. Zhang, “An algorithm for analyzing the city residents' activity information through mobile big data mining,” in Proceedings of the Joint 15th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 10th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Symposium on Parallel and Distributed Processing with Applications, IEEE TrustCom/BigDataSE/ISPA 2016, pp. 2133–2138, China, August 2016. View at: Google Scholar Z. Liao, Q. Yin, Y. Huang, and L. Sheng, “Management and application of mobile big data,” International Journal of Embedded Systems, vol. 7, no. 1, pp. 63–70, 2015. View at: Publisher Site | Google Scholar M. Agiwal, A. Roy, and N. Saxena, “Next generation 5G wireless networks: a comprehensive survey,” IEEE Communications Surveys & Tutorials, vol. 18, no. 3, pp. 1617–1655, 2016. View at: Publisher Site | Google Scholar W. Li and Z. Zhou, “Learning to hash for big data: current status and future trends,” Chinese Science Bulletin (Chinese Version), vol. 60, no. 5-6, p. 485, 2015. View at: Publisher Site | Google Scholar V. Mayerschönberger and K. Cukier, Big Data: A Revolution That Will Transform How We Live, Work, and Think, Eamon Do-lan/Houghton Mifflin Harcourt, Boston, 2013. D. Z. Yazti and S. Krishnaswamy, “Mobile big data analytics: research, practice, and opportunities,” in Proceedings of the 15th IEEE International Conference on Mobile Data Management, IEEE MDM 2014, pp. 1-2, Australia, July 2014. View at: Google Scholar E. Zeydan, E. Bastug, M. Bennis et al., “Big data caching for networking: moving from cloud to edge,” IEEE Communications Magazine, vol. 54, no. 9, pp. 36–42, 2016. View at: Publisher Site | Google Scholar Z. Liu, Y. Qi, Z. Ma et al., “Sentiment analysis by exploring large scale web-based Chinese short text,” in Proceedings of the International Conference on Computer Science and Application Engineering (CSAE), pp. 21–23, 2017. View at: Google Scholar Z. Wang, Y. Qi, J. Liu, and Z. Ma, “User intention understanding from scratch,” in Proceedings of the 1st International Workshop on Sensing, Processing and Learning for Intelligent Machines, SPLINE 2016, Denmark, July 2016. View at: Google Scholar C. Zhang, Z. Si, Z. Ma, X. Xi, and Y. Yin, “Mining sequential update summarization with hierarchical text analysis,” Mobile Information Systems, vol. 2016, Article ID 1340973, 10 pages, 2016. View at: Publisher Site | Google Scholar C. Zhang, Y. Zhang, W. Xu, Z. Ma, Y. Leng, and J. Guo, “Mining activation force defined dependency patterns for relation extraction,” Knowledge-Based Systems, vol. 86, pp. 278–287, 2015. View at: Publisher Site | Google Scholar C. Zhang, W. Xu, Z. Ma, S. Gao, Q. Li, and J. Guo, “Construction of semantic bootstrapping models for relation extraction,” Knowledge-Based Systems, vol. 83, pp. 128–137, 2015. View at: Publisher Site | Google Scholar M. Jordan, “Message from the president: the era of big data,” ISBA Bull, vol. 18, pp. 1–3, 2011. View at: Google Scholar W. Chen, D. Wipf, Y. Wang, Y. Liu, and I. J. Wassell, “Simultaneous Bayesian sparse approximation with structured sparse models,” IEEE Transactions on Signal Processing, vol. 64, no. 23, pp. 6145–6159, 2016. View at: Publisher Site | Google Scholar | MathSciNet W. Chen, M. R. D. Rodrigues, and I. J. Wassell, “Projection design for statistical compressive sensing: a tight frame based approach,” IEEE Transactions on Signal Processing, vol. 61, no. 8, pp. 2016–2029, 2013. View at: Publisher Site | Google Scholar H. Yong, D. Meng, W. Zuo, and L. Zhang, “Robust online matrix factorization for dynamic background subtraction,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. View at: Google Scholar Q. Xie, D. Zeng, Q. Zhao et al., “Robust low-dose CT sinogram prepocessing via exploiting noise-generating mechanism,” IEEE Transactions on Medical Imaging, vol. 36, no. 12, pp. 2487–2498, 2017. View at: Google Scholar M. O'Connor, G. Zhang, W. B. Kleijn, and T. D. Abhayapala, “Function splitting and quadratic approximation of the primal-dual method of multipliers for distributed optimization over graphs,” IEEE Transactions on Signal and Information Processing over Networks, pp. 1–1, 2018. View at: Publisher Site | Google Scholar G. Zhang and R. Heusdens, “Distributed optimization using the primal-dual method of multipliers,” IEEE Transactions on Signal and Information Processing over Networks, vol. 4, no. 1, pp. 173–187, 2018. View at: Publisher Site | Google Scholar | MathSciNet G. Zhang and R. Heusdens, “Linear coordinate-descent message passing for quadratic optimization,” in Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, pp. 20055–2008, 2012. View at: Publisher Site | Google Scholar | MathSciNet G. Zhang, R. Heusdens, and W. B. Kleijn, “Large scale LP decoding with low complexity,” IEEE Communications Letters, vol. 17, no. 11, pp. 2152–2155, 2013. View at: Publisher Site | Google Scholar Z. Ma, A. E. Teschendorff, A. Leijon, Y. Qiao, H. Zhang, and J. Guo, “Variational bayesian matrix factorization for bounded support data,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 4, pp. 876–889, 2015. View at: Publisher Site | Google Scholar Z.-H. Zhou, N. V. Chawla, Y. Jin, and G. J. Williams, “Big data opportunities and challenges: discussions from data analytics perspectives,” IEEE Computational Intelligence Magazine, vol. 9, no. 4, pp. 62–74, 2014. View at: Publisher Site | Google Scholar Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015. View at: Publisher Site | Google Scholar Y. Bengio and S. Bengio, “Modeling high-dimensional discrete data with multi-layer neural networks,” in Proceedings of the 13th Annual Neural Information Processing Systems Conference, NIPS 1999, pp. 400–406, USA, December 1999. View at: Google Scholar M. Ranzato, Y.-L. Boureau, and Y. Le Cun, “Sparse feature learning for deep belief networks,” in Advances in Neural Information Processing Systems, pp. 1185–1192, 2008. View at: Google Scholar G. E. Hinton, S. Osindero, and Y. Teh, “A fast learning algorithm for deep belief nets,” Neural Computation, vol. 18, no. 7, pp. 1527–1554, 2006. View at: Publisher Site | Google Scholar | MathSciNet Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy layer-wise training of deep networks,” in Proceedings of the 20th Annual Conference on Neural Information Processing Systems (NIPS '06), pp. 153–160, Cambridge, Mass, USA, December 2006. View at: Google Scholar H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin, “Exploring strategies for training deep neural networks,” Journal of Machine Learning Research, vol. 10, pp. 1–40, 2009. View at: Google Scholar R. Salakhutdinov and G. Hinton, “Deep boltzmann machines,” in Proceedings of the International Conference on Artificial Intelligence and Statistics, vol. 24, pp. 448–455, 2009. View at: Publisher Site | Google Scholar I. Goodfellow, H. Lee, and Q. V. Le, “Measuring invariances in deep networks,” Neural Information Processing Systems, pp. 646–654, 2009. View at: Google Scholar Y. Bengio and Y. LeCun, “Scaling learning algorithms towards, AI,” Large Scale Kernel Machines, vol. 34, pp. 321–360, 2007. View at: Google Scholar Y. Bengio, A. Courville, and P. Vincent, “Representation learning: a review and new perspectives,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798–1828, 2013. View at: Publisher Site | Google Scholar I. Arel, D. C. Rose, and T. P. Karnowski, “Deep machine learning—a new frontier in artificial intelligence research,” IEEE Computational Intelligence Magazine, vol. 5, no. 4, pp. 13–18, 2010. View at: Publisher Site | Google Scholar G. E. Dahl, D. Yu, L. Deng et al., “Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30–42, 2012. View at: Google Scholar G. Hinton, L. Deng, D. Yu et al., “Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82–97, 2012. View at: Publisher Site | Google Scholar R. Salakhutdinov, A. Mnih, and G. Hinton, “Restricted Boltzmann machines for collaborative filtering,” in Proceedings of the 24th International Conference on Machine learning (ICML '07), vol. 227, pp. 791–798, Corvallis, Oregon, June 2007. View at: Publisher Site | Google Scholar D. C. Cireşan, U. Meier, L. M. Gambardella, and J. Schmidhuber, “Deep, big, simple neural nets for handwritten digit recognition,” Neural Computation, vol. 22, no. 12, pp. 3207–3220, 2010. View at: Publisher Site | Google Scholar M. D. Zeiler, G. W. Taylor, and R. Fergus, “Adaptive deconvolutional networks for mid and high level feature learning,” in Proceedings of the 2011 IEEE International Conference on Computer Vision, ICCV 2011, pp. 2018–2025, Spain, November 2011. View at: Google Scholar A. Efrati, “How deep learning works at Apple, beyond,” https://www.theinformation.com/How-Deep-Learning-Works-at-Apple-Beyond, 2013. View at: Google Scholar Z. Yang, B. Wu, K. Zheng, X. Wang, and L. Lei, “A survey of collaborative filtering-based recommender systems for mobile internet applications,” IEEE Access, vol. 4, pp. 3273–3287, 2016. View at: Publisher Site | Google Scholar K. Zhu, L. Zhang, and A. Pattavina, “Learning geographical and mobility factors for mobile application recommendation,” IEEE Intelligent Systems, vol. 32, no. 3, pp. 36–44, 2017. View at: Publisher Site | Google Scholar S. Jiang, B. Wei, T. Wang, Z. Zhao, and X. Zhang, “Big data enabled user behavior characteristics in mobile internet,” in Proceedings of the 2017 9th International Conference on Wireless Communications and Signal Processing (WCSP), pp. 1–5, Nanjing, October 2017. View at: Publisher Site | Google Scholar J. Yang, Y. Qiao, X. Zhang, H. He, F. Liu, and G. Cheng, “Characterizing user behavior in mobile internet,” IEEE Transactions on Emerging Topics in Computing, vol. 3, no. 1, pp. 95–106, 2015. View at: Publisher Site | Google Scholar Y. Qiao, X. Zhao, J. Yang, and J. Liu, “Mobile big-data-driven rating framework: measuring the relationship between human mobility and app usage behavior,” IEEE Network, vol. 30, no. 3, pp. 14–21, 2016. View at: Publisher Site | Google Scholar Y. Qiao, J. Yang, H. He, Y. Cheng, and Z. Ma, “User location prediction with energy efficiency model in the Long Term-Evolution network,” International Journal of Communication Systems, vol. 29, no. 14, pp. 2169–2187, 2016. View at: Publisher Site | Google Scholar M. Gerla and L. Kleinrock, “Vehicular networks and the future of the mobile internet,” Computer Networks, vol. 55, no. 2, pp. 457–469, 2011. View at: Publisher Site | Google Scholar M. M. Islam, M. A. Razzaque, M. M. Hassan, W. N. Ismail, and B. Song, “Mobile cloud-based big healthcare data processing in smart cities,” IEEE Access, vol. 5, pp. 11887–11899, 2017. View at: Publisher Site | Google Scholar Texas Instruments, “Wireless Handset Solutions: Mobile Internet Device,” http://www.ti.com/solution/handset_smartphone, 2008. View at: Google Scholar X. Ma, J. Zhang, Y. Zhang, and Z. Ma, “Data scheme-based wireless channel modeling method: motivation, principle and performance,” Journal of Communications and Information Networks, vol. 2, no. 3, pp. 41–51, 2017. View at: Publisher Site | Google Scholar X. Ma, J. Zhang, Y. Zhang, Z. Ma, and Y. Zhang, “A PCA-based modeling method for wireless MIMO channel,” in Proceedings of the 2017 IEEE Conference on Computer Communications: Workshops (INFOCOM WKSHPS), pp. 874–879, Atlanta, GA, May 2017. View at: Publisher Site | Google Scholar X. Zhang, Z. Yi, Z. Yan et al., “Social computing for mobile big data,” The Computer Journal, vol. 49, no. 9, pp. 86–90, 2016. View at: Publisher Site | Google Scholar K. Zhu, Z. Chen, L. Zhang, Y. Zhang, and S. Kim, “Geo-cascading and community-cascading in social networks: comparative analysis and its implications to edge caching,” Information Sciences, vol. 436-437, pp. 1–12, 2018. View at: Publisher Site | Google Scholar S. Gao, H. Luo, D. Chen et al., “A cross-domain recommendation model for cyber-physical systems,” IEEE Transactions on Emerging Topics in Computing, vol. 1, no. 2, pp. 384–393, 2013. View at: Publisher Site | Google Scholar Y. Qiao, Z. Xing, Z. M. Fadlullah, J. Yang, and N. Kato, “Characterizing flow, application, and user behavior in mobile networks: a framework for mobile big data,” IEEE Wireless Communications Magazine, vol. 25, no. 1, pp. 40–49, 2018. View at: Publisher Site | Google Scholar H. Yu, Z. Tan, Z. Ma, R. Martin, and J. Guo, “Spoofing detection in automatic speaker verification systems using DNN classifiers and dynamic acoustic features,” IEEE Transactions on Neural Networks and Learning Systems, pp. 1–12. View at: Publisher Site | Google Scholar H. Yu, Z.-H. Tan, Y. Zhang, Z. Ma, and J. Guo, “DNN filter bank cepstral coefficients for spoofing detection,” IEEE Access, vol. 5, pp. 4779–4787, 2017. View at: Publisher Site | Google Scholar Z. Ma, H. Yu, Z.-H. Tan, and J. Guo, “Text-independent speaker identification using the histogram transform model,” IEEE Access, vol. 4, pp. 9733–9739, 2016. View at: Publisher Site | Google Scholar H. Yu, Z.-H. Tan, Z. Ma, and J. Guo, “Adversarial network bottleneck features for noise robust speaker verification,” in Proceedings of the 18th Annual Conference of the International Speech Communication Association, INTERSPEECH 2017, pp. 1492–1496, Sweden, August 2017. View at: Google Scholar H. Yu, A. Sarkar, D. A. L. Thomsen, Z.-H. Tan, Z. Ma, and J. Guo, “Effect of multi-condition training and speech enhancement methods on spoofing detection,” in Proceedings of the 1st International Workshop on Sensing, Processing and Learning for Intelligent Machines, SPLINE 2016, Denmark, July 2016. View at: Google Scholar H. Yu, Z. Ma, and M. Li, “Histogram transform model Using MFCC features for text-independent speaker identification,” in Proceedings of the IEEE Asilomar Conference on Signals, Systems, pp. 500–504, 2014. View at: Google Scholar Z. Ma, J. Xie, H. Li et al., “The role of data analysis in the development of intelligent energy networks,” IEEE Network, vol. 31, no. 5, pp. 88–95, 2017. View at: Publisher Site | Google Scholar Z. Ma, H. Li, Q. Sun, C. Wang, A. Yan, and F. Starfelt, “Statistical analysis of energy consumption patterns on the heat demand of buildings in district heating systems,” Energy and Buildings, vol. 85, pp. 464–472, 2014. View at: Publisher Site | Google Scholar D. West, “How mobile devices are transforming healthcare,” Issues in Technology Innovation, vol. 18, no. 1, pp. 1–11, 2012. View at: Google Scholar L. A. Tawalbeh, R. Mehmood, E. Benkhlifa, and H. Song, “Mobile cloud computing model and big data analysis for healthcare applications,” IEEE Access, vol. 4, pp. 6171–6180, 2016. View at: Publisher Site | Google Scholar S. Sagiroglu and D. Sinanc, “Big data: a review,” in Proceedings of the International Conference on Collaboration Technologies and Systems (CTS '13), pp. 42–47, IEEE, San Diego, Calif, USA, May 2013. View at: Publisher Site | Google Scholar K. Zheng, L. Hou, H. Meng, Q. Zheng, N. Lu, and L. Lei, “Soft-defined heterogeneous vehicular network: architecture and challenges,” IEEE Network, vol. 30, no. 4, pp. 72–80, 2016. View at: Publisher Site | Google Scholar H. Hsieh, V. Klyuev, Q. Zhao, and S. Wu, “SVR-based outlier detection and its application to hotel ranking,” in Proceedings of the 2014 IEEE 6th International Conference on Awareness Science and Technology (iCAST), pp. 1–6, Paris, France, October 2014. View at: Publisher Site | Google Scholar S. Rahman, M. Sathik, and K. Kannan, “Multiple linear regression models in outlier detection,” International Journal of Research in Computer Science, vol. 2, no. 2, pp. 23–28, 2012. View at: Publisher Site | Google Scholar H. A. Dau, V. Ciesielski, and A. Song, “Anomaly Detection using replicator neural networks trained on examples of one class,” in Simulated Evolution and Learning, vol. 8886 of Lecture Notes in Computer Science, pp. 311–322, Springer International Publishing, Cham, 2014. View at: Publisher Site | Google Scholar Z. Ma, J.-H. Xue, A. Leijon, Z.-H. Tan, Z. Yang, and J. Guo, “Decorrelation of neutral vector variables: theory and applications,” IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 1, pp. 129–143, 2018. View at: Publisher Site | Google Scholar | MathSciNet Z. Ma, S. Chatterjee, W. B. Kleijn, and J. Guo, “Dirichlet mixture modeling to estimate an empirical lower bound for LSF quantization,” Signal Processing, vol. 104, pp. 291–295, 2014. View at: Publisher Site | Google Scholar Z. Ma and A. Leijon, “Bayesian estimation of beta mixture models with variational inference,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 11, pp. 2160–2173, 2011. View at: Publisher Site | Google Scholar C. C. Aggarwal, “Outlier analysis,” in Data Mining, Springer, 2015. View at: Google Scholar | MathSciNet Y. Demchenko, P. Grosso, C. de Laat, and P. Membrey, “Addressing big data issues in scientific data infrastructure,” in Proceedings of the IEEE International Conference on Collaboration Technologies and Systems (CTS '13), pp. 48–55, May 2013. View at: Publisher Site | Google Scholar C. Zhou, H. Jiang, Y. Chen, L. Wu, and S. Yi, “User interest acquisition by adding home and work related contexts on mobile big data analysis,” in Proceedings of the IEEE INFOCOM 2016 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS), pp. 201–206, San Francisco, CA, USA, April 2016. View at: Publisher Site | Google Scholar X. Ge, H. Cheng, M. Guizani, and T. Han, “5G wireless backhaul networks: challenges and research advances,” IEEE Network, vol. 28, no. 6, pp. 6–11, 2014. View at: Publisher Site | Google Scholar S. Landset, T. M. Khoshgoftaar, A. N. Richter, and T. Hasanin, “A survey of open source tools for machine learning with big data in the Hadoop ecosystem,” Journal of Big Data, vol. 2, no. 1, pp. 24–59, 2015. View at: Publisher Site | Google Scholar D. Soubra, “The 3Vs that define Big Data,” http://www.datasciencecentral.com/forum/topics/the-3vs-that-define-big-data, 2012. View at: Google Scholar L. Ma, F. Nie, and Q. Lu, “An analysis of supply chain restructuring based on big data and mobile internet—a case study of warehouse-type supermarkets,” in Proceedings of the IEEE International Conference on Grey Systems and Intelligent Services, GSIS 2015, pp. 446–451, UK, August 2015. View at: Google Scholar A. McAfee and E. Brynjolfsson, “Big data: the management revolution,” Harvard Business Review, vol. 90, no. 10, pp. 60–128, 2012. View at: Google Scholar Y. Li, J. Zhang, and Z. Ma, “Clustering in wireless propagation channel with a statistics-based framework,” in Proceedings of the 2018 IEEE Wireless Communications and Networking Conference (WCNC), pp. 1–6, Barcelona, April 2018. View at: Publisher Site | Google Scholar P. Kazienko, K. Musiał, and T. Kajdanowicz, “Multidimensional social network in the social recommender system,” IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 41, no. 4, pp. 746–759, 2011. View at: Publisher Site | Google Scholar A. Abe, K. Yamamoto, and S. Nakagawa, “Robust speech recognition using DNN-HMM acoustic model combining noise-aware training with spectral subtraction,” in Proceedings of the 16th Annual Conference of the International Speech Communication Association, INTERSPEECH 2015, pp. 2849–2853, Germany, September 2015. View at: Google Scholar Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, “An experimental study on speech enhancement based on deep neural networks,” IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65–68, 2014. View at: Publisher Site | Google Scholar A. Narayanan and D. Wang, “Ideal ratio mask estimation using deep neural networks for robust speech recognition,” in Proceedings of the 2013 38th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2013, pp. 7092–7096, Canada, May 2013. View at: Google Scholar D. Serdyuk, K. Audhkhasi, and P. Brakel, “Invariant representations for noisy speech recognition,” Computation and Language, 2016, arXiv:1612.01928. View at: Google Scholar P. E. Hart, “The condensed nearest neighbor rule,” IEEE Transactions on Information Theory, vol. 14, no. 3, pp. 515-516, 1968. View at: Publisher Site | Google Scholar G. Gates, “The reduced nearest neighbor rule,” IEEE Transactions on Information Theory, vol. 18, no. 3, pp. 431–433, 1972. View at: Publisher Site | Google Scholar H. Brighton and C. Mellish, “Advances in instance selection for instance-based learning algorithms,” Data Mining and Knowledge Discovery, vol. 6, no. 2, pp. 153–172, 2002. View at: Publisher Site | Google Scholar | MathSciNet Y. Li and L. Maguire, “Selecting critical patterns based on local geometrical and statistical information,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 6, pp. 1189–1201, 2011. View at: Publisher Site | Google Scholar F. Angiulli, “Fast nearest neighbor condensation for large data sets classification,” IEEE Transactions on Knowledge and Data Engineering, vol. 19, no. 11, pp. 1450–1464, 2007. View at: Publisher Site | Google Scholar F. Angiulli and G. Folino, “Distributed nearest neighbor-based condensation of very large data sets,” IEEE Transactions on Knowledge and Data Engineering, vol. 19, no. 12, pp. 1593–1606, 2007. View at: Publisher Site | Google Scholar M. I. Jordan, “Divide-and-conquer and statistical inference for big data,” in Proceedings of the the 18th ACM SIGKDD international conference, p. 4, Beijing, China, August 2012. View at: Publisher Site | Google Scholar T. G. Kolda and J. Sun, “Scalable tensor decompositions for multi-aspect data mining,” in Proceedings of the 8th IEEE International Conference on Data Mining, ICDM 2008, pp. 363–372, Italy, December 2008. View at: Google Scholar G. Wahba, “Dissimilarity data in statistical model building and machine learning,” in Proceedings of the 5th International Congress of Chinese Mathematicians, pp. 785–809, 2012. View at: Google Scholar | MathSciNet S. C. Hoi, J. Wang, P. Zhao, and R. Jin, “Online feature selection for mining big data,” in Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications, pp. 93–100, Beijing, China, August 2012. View at: Publisher Site | Google Scholar A. Sagheer, N. Tsuruta, R.-I. Taniguchi, D. Arita, and S. Maeda, “Fast feature extraction approach for multi-dimension feature space problems,” in Proceedings of the 18th International Conference on Pattern Recognition, ICPR 2006, pp. 417–420, China, August 2006. View at: Google Scholar J. R. Anaraki and M. Eftekhari, “Improving fuzzy-rough quick reduct for feature selection,” in Proceedings of the 2011 19th Iranian Conference on Electrical Engineering, ICEE 2011, Iran, May 2011. View at: Google Scholar I. A. Gheyas and L. S. Smith, “Feature subset selection in large dimensionality domains,” Pattern Recognition, vol. 43, no. 1, pp. 5–13, 2010. View at: Publisher Site | Google Scholar K. W. Lau and Q. H. Wu, “Online training of support vector classifier,” Pattern Recognition, vol. 36, no. 8, pp. 1913–1920, 2003. View at: Publisher Site | Google Scholar P. Laskov, C. Gehl, S. Krüger, and K.-R. Müller, “Incremental support vector learning: analysis, implementation and applications,” Journal of Machine Learning Research, vol. 7, pp. 1909–1936, 2006. View at: Google Scholar | MathSciNet C. Chang and C. Lin, “LIBSVM: a Library for support vector machines,” ACM Transactions on Intelligent Systems and Technology, vol. 2, no. 3, article 27, 2011. View at: Publisher Site | Google Scholar K. Huang, H. Yang, I. King, and M. R. Lyu, “Maxi-min margin machine: learning large margin classifiers locally and globally,” IEEE Transactions on Neural Networks and Learning Systems, vol. 19, no. 2, pp. 260–272, 2008. View at: Publisher Site | Google Scholar A. Franco-Arcega, J. A. Carrasco-Ochoa, G. Snchez-Daz et al., “Building fast decision trees from large training sets,” Intelligent Data Analysis, vol. 16, no. 4, pp. 649–664, 2012. View at: Google Scholar H. Yang and S. Fong, “Incrementally optimized decision tree for noisy big data,” in Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications (BigMine '12), pp. 36–44, Beijing, China, August 2012. View at: Publisher Site | Google Scholar Y. Ben-Haim and E. Tom-Tov, “A streaming parallel decision tree algorithm,” Journal of Machine Learning Research (JMLR), vol. 11, pp. 849–872, 2010. View at: Google Scholar | MathSciNet G. B. Huang, Q. Y. Zhu, and C. K. Siew, “Extreme learning machine: theory and applications,” Neurocomputing, vol. 70, no. 1–3, pp. 489–501, 2006. View at: Publisher Site | Google Scholar N. Liu and H. Wang, “Ensemble based extreme learning machine,” IEEE Signal Processing Letters, vol. 17, no. 8, pp. 754–757, 2010. View at: Publisher Site | Google Scholar Q. He, T. Shang, F. Zhuang, and Z. Shi, “Parallel extreme learning machine for regression based on mapReduce,” Neurocomputing, vol. 102, pp. 52–58, 2013. View at: Publisher Site | Google Scholar R. Zhang, Y. Lan, G.-B. Huang, and Z.-B. Xu, “Universal approximation of extreme learning machine with adaptive growth of hidden nodes,” IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 2, pp. 365–371, 2012. View at: Publisher Site | Google Scholar H.-J. Rong, G.-B. Huang, N. Sundararajan, P. Saratchandran, and H.-J. Rong, “Online sequential fuzzy extreme learning machine for function approximation and classification problems,” IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 4, pp. 1067–1072, 2009. View at: Publisher Site | Google Scholar Y. Yang, Y. Wang, and X. Yuan, “Bidirectional extreme learning machine for regression problem and its learning effectiveness,” IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 9, pp. 1498–1505, 2012. View at: Publisher Site | Google Scholar W. X. Chen and X. Lin, “Big data deep learning: challenges and perspectives,” IEEE Access, vol. 2, pp. 514–525, 2014. View at: Publisher Site | Google Scholar G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,” The American Association for the Advancement of Science: Science, vol. 313, no. 5786, pp. 504–507, 2006. View at: Publisher Site | Google Scholar | MathSciNet Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2323, 1998. View at: Publisher Site | Google Scholar D. C. Ciresan, U. Meier, and J. Masci, “Flexible, high performance convolutional neural networks for image classification,” in Proceedings of the International Joint Conference on Artificial Intelligence, pp. 1237–1242, 2011. View at: Google Scholar D. Scherer, A. Müller, and S. Behnke, “Evaluation of pooling operations in convolutional architectures for object recognition,” in Proceedings of the International Conference on Artificial Neural Networks, pp. 92–101, 2010. View at: Google Scholar A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS '12), pp. 1097–1105, Lake Tahoe, Nev, USA, December 2012. View at: Google Scholar J. Dean, G. Corrado, and R. Monga, “Large scale distributed deep networks,” in Neural Information Processing Systems, pp. 1223–1231, 2012. View at: Google Scholar G. Papandreou, L.-C. Chen, K. P. Murphy, and A. L. Yuille, “Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation,” in Proceedings of the 15th IEEE International Conference on Computer Vision, ICCV 2015, pp. 1742–1750, Chile, December 2015. View at: Google Scholar G. Hinton and R. Salakhutdinov, “Discovering binary codes for documents by learning deep generative models,” Topics in Cognitive Science, vol. 3, no. 1, pp. 74–91, 2011. View at: Publisher Site | Google Scholar R. Socher, C. C.-Y. Lin, C. D. Manning, and A. Y. Ng, “Parsing natural scenes and natural language with recursive neural networks,” in Proceedings of the 28th International Conference on Machine Learning (ICML '11), pp. 129–136, Bellevue, Wash, USA, June 2011. View at: Google Scholar R. Kumar, J. O. Talton, and S. Ahmad, “Data-driven web design,” in Proceedings of the International Conference on Machine Learning, pp. 3-4, 2012. View at: Google Scholar R. Raina, A. Madhavan, and A. Y. Ng, “Large-scale deep unsupervised learning using graphics processors,” in Proceedings of the 26th International Conference On Machine Learning, ICML 2009, pp. 873–880, Canada, June 2009. View at: Google Scholar J. Martens, “Deep learning via Hessian-free optimization,” in Proceedings of the 27th International Conference on Machine Learning (ICML '10), pp. 735–742, June 2010. View at: Google Scholar K. Zhang and X.-W. Chen, “Large-scale deep belief nets with mapreduce,” IEEE Access, vol. 2, pp. 395–403, 2014. View at: Publisher Site | Google Scholar L. Deng, D. Yu, and J. Platt, “Scalable stacking and learning for building deep architectures,” in Proceedings of the 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '12), pp. 2133–2136, Kyoto, Japan, March 2012. View at: Publisher Site | Google Scholar K. Kavukcuoglu, M. Ranzato, R. Fergus, and Y. LeCun, “Learning invariant features through topographic filter maps,” in Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009, pp. 1605–1612, USA, June 2009. View at: Google Scholar B. Hutchinson, L. Deng, and D. Yu, “Tensor deep stacking networks,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1944–1957, 2013. View at: Publisher Site | Google Scholar J. Zhang, “Review of wideband MIMO channel measurement and modeling for IMT-Advanced systems,” Chinese Science Bulletin, vol. 57, no. 19, pp. 2387–2400, 2012. View at: Publisher Site | Google Scholar C. Liang, H. Li, Y. Li, S. Zhou, and J. Wang, “A learning-based channel model for synergetic transmission technology,” China Communications, vol. 12, no. 9, pp. 83–92, 2015. View at: Publisher Site | Google Scholar J. Zhang, “The interdisciplinary research of big data and wireless channel: a cluster-nuclei based channel model,” China Communications, vol. 13, no. supplement 2, Article ID 7833457, pp. 14–26, 2016. View at: Publisher Site | Google Scholar Y. Li, J. Zhang, Z. Ma, and Y. Zhang, “Clustering analysis in the wireless propagation channel with a variational gaussian mixture model,” IEEE Transactions on Big Data, pp. 1–1, 2018. View at: Publisher Site | Google Scholar F. Bai, T. Vidal-Calleja, and S. Huang, “Robust incremental SLAM under constrained optimization formulation,” IEEE Robotics and Automation Letters, vol. 3, no. 2, pp. 1–8, 2018. View at: Publisher Site | Google Scholar I. Z. Ibragimov and I. M. Afanasyev, “Comparison of ROS-based visual SLAM methods in homogeneous indoor environment,” in Proceedings of the 2017 14th Workshop on Positioning, Navigation and Communications (WPNC), pp. 1–6, Bremen, October 2017. View at: Publisher Site | Google Scholar U. M. Fayyad, On the Induction of Decision Trees for Multiple Concept Learning, University of Michigan, 1992. G. Cybenko, “Approximation by Superpositions of a sigmoidal function,” Mathematics of Control Signals & Systems, vol. 2, no. 4, pp. 303–314, 1989. View at: Google Scholar | MathSciNet Z. Ma, P. K. Rana, J. Taghia, M. Flierl, and A. Leijon, “Bayesian estimation of dirichlet mixture model with variational inference,” Pattern Recognition, vol. 47, no. 9, pp. 3143–3157, 2014. View at: Publisher Site | Google Scholar D. Naboulsi, M. Fiore, S. Ribot, and R. Stanica, “Large-scale mobile traffic analysis: a survey,” IEEE Communications Surveys & Tutorials, vol. 18, no. 1, pp. 124–161, 2016. View at: Publisher Site | Google Scholar E. Halepovic and C. Williamson, “Characterizing and modeling user mobility in a cellular data network,” in Proceedings of the PE-WASUN'05 - Second ACM International Workshop on Performance Evaluation of Wireless Ad Hoc, Sensor, and Ubiquitous Networks, pp. 71–78, Canada, October 2005. View at: Google Scholar S. Scepanovic, P. Hui, and A. Yla-Jaaski, “Revealing the pulse of human dynamics in a country from mobile phone data,” NetMob D4D Challenge, pp. 1–15, 2013. View at: Google Scholar S. Isaacman, R. A. Becker, and R. Caceres, “Identifying important places in people's lives from cellular network data,” in Proceedings of the International Conference on Pervasive Computing, pp. 133–151, 2011. View at: Google Scholar I. Trestian, S. Ranjan, A. Kuzmanovic, and A. Nucci, “Measuring serendipity: connecting people, locations and interests in a mobile 3G network,” in Proceedings of the 2009 9th ACM SIGCOMM Internet Measurement Conference, IMC 2009, pp. 267–279, USA, November 2009. View at: Google Scholar C. Song, Z. Qu, N. Blumm, and A.-L. Barabási, “Limits of predictability in human mobility,” Science, vol. 327, no. 5968, pp. 1018–1021, 2010. View at: Publisher Site | Google Scholar Q. Lv, Y. Qiao, N. Ansari, J. Liu, and J. Yang, “Big data driven hidden markov model based individual mobility prediction at points of interest,” IEEE Transactions on Vehicular Technology, vol. 66, no. 6, pp. 5204–5216, 2017. View at: Publisher Site | Google Scholar Y. Qiao, Y. Cheng, J. Yang, J. Liu, and N. Kato, “A mobility analytical framework for big mobile data in densely populated area,” IEEE Transactions on Vehicular Technology, vol. 66, no. 2, pp. 1443–1455, 2017. View at: Publisher Site | Google Scholar L. Meng, S. Liu, and A. Striegel, “Analyzing the longitudinal impact of proximity, location, and personality on smartphone usage,” Computational Social Networks, vol. 1, no. 1, 2014. View at: Publisher Site | Google Scholar M. Böhmer, B. Hecht, J. Schöning, A. Krüger, and G. Bauer, “Falling asleep with angry birds, facebook and kindle: a large scale study on mobile application usage,” in Proceedings of the 13th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI '11), pp. 47–56, September 2011. View at: Publisher Site | Google Scholar D. Hristova, M. Musolesi, and C. Mascolo, “Keep your friends close and your facebook friends closer: a multiplex network approach to the analysis of offline and online social Ties,” in Proceedings of the 8th International Conference on Weblogs and Social Media, ICWSM 2014, pp. 206–215, USA, June 2014. View at: Google Scholar R. I. M. Dunbar, V. Arnaboldi, M. Conti, and A. Passarella, “The structure of online social networks mirrors those in the offline world,” Social Networks, vol. 43, pp. 39–47, 2015. View at: Publisher Site | Google Scholar D. Hristova, M. J. Williams, M. Musolesi, P. Panzarasa, and C. Mascolo, “Measuring urban social diversity using interconnected geo-social networks,” in Proceedings of the the 25th International Conference, pp. 21–30, Canada, April 2016. View at: Publisher Site | Google Scholar D. Hristova, A. Noulas, C. Brown, M. Musolesi, and C. Mascolo, “A multilayer approach to multiplexity and link prediction in online geo-social networks,” EPJ Data Science, vol. 5, no. 1, 2016. View at: Publisher Site | Google Scholar Apache, “Apache software foundation,” http://apache.org, 2017. View at: Google Scholar M. Gerla, E.-K. Lee, G. Pau, and U. Lee, “Internet of vehicles: from intelligent grid to autonomous cars and vehicular clouds,” in Proceedings of the IEEE World Forum on Internet of Things (WF-IoT '14), pp. 241–246, March 2014. View at: Publisher Site | Google Scholar F. Yang, S. Wang, J. Li, Z. Liu, and Q. Sun, “An overview of internet of vehicles,” China Communications, vol. 11, no. 10, pp. 1–15, 2014. View at: Publisher Site | Google Scholar K. M. Alam, M. Saini, and A. El Saddik, “Toward social internet of vehicles: concept, architecture, and applications,” IEEE Access, vol. 3, pp. 343–357, 2015. View at: Publisher Site | Google Scholar J. D. Lee, B. Caven, S. Haake, and T. L. Brown, “Speech-based interaction with in-vehicle computers: the effect of speech-based e-mail on drivers' attention to the roadway,” Human Factors: The Journal of the Human Factors and Ergonomics Society, vol. 43, no. 4, pp. 631–640, 2001. View at: Publisher Site | Google Scholar C. Y. Loh, K. L. Boey, and K. S. Hong, “Speech recognition interactive system for vehicle,” in Proceedings of the 13th IEEE International Colloquium on Signal Processing and its Applications, CSPA 2017, pp. 85–88, Malaysia, March 2017. View at: Google Scholar D. Amodei, S. Ananthanarayanan, and R. Anubhai, “Deep speech 2: end-to-end speech recognition in English and mandarin,” in Proceedings of the International Conference on Machine Learning, pp. 173–182, 2016. View at: Google Scholar E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-Dominguez, “Deep neural networks for small footprint text-dependent speaker verification,” in Proceedings of the 2014 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2014, pp. 4052–4056, Italy, May 2014. View at: Google Scholar K. Chen and A. Salman, “Learning speaker-specific characteristics with a deep neural architecture,” IEEE Transactions on Neural Networks and Learning Systems, vol. 22, no. 11, pp. 1744–1756, 2011. View at: Publisher Site | Google Scholar L. Deng, G. E. Hinton, and B. Kingsbury, “New types of deep neural network learning for speech recognition and related applications: an overview,” in Proceedings of the 38th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '13), pp. 8599–8603, IEEE, Vancouver, Canada, May 2013. View at: Publisher Site | Google Scholar A. Graves, A.-R. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,” in Proceedings of the 38th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '13), pp. 6645–6649, May 2013. View at: Publisher Site | Google Scholar J. Meyer and K. U. Simmer, “Multi-channel speech enhancement in a car environment using Wiener filtering and spectral subtraction,” in Proceedings of the 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP. Part 1 (of 5), pp. 1167–1170, April 1997. View at: Google Scholar R. C. Hendriks, R. Heusdens, and J. Jensen, “MMSE based noise PSD tracking with low complexity,” in Proceedings of the 2010 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2010, pp. 4266–4269, USA, March 2010. View at: Google Scholar F. Seide, G. Li, and D. Yu, “Conversational speech transcription using context-dependent deep neural networks,” in Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH '11), vol. 33, pp. 437–440, August 2011. View at: Google Scholar H. Ze, A. Senior, and M. Schuster, “Statistical parametric speech synthesis using deep neural networks,” in Proceedings of the 38th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP '13), pp. 7962–7966, IEEE, Vancouver, Canada, May 2013. View at: Publisher Site | Google Scholar G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep neural networks for LVCSR using rectified linear units and dropout,” in Proceedings of the 38th IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '13), pp. 8609–8613, May 2013. View at: Publisher Site | Google Scholar Y. Qian, M. Bi, T. Tan, and K. Yu, “Very deep convolutional neural networks for noise robust speech recognition,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 24, no. 12, pp. 2263–2276, 2016. View at: Publisher Site | Google Scholar K. Han, Y. He, D. Bagchi et al., “Deep neural network based spectral feature mapping for robust speech recognition,” INTERSPEECH, pp. 2484–2488, 2015. View at: Google Scholar B. Li and K. C. Sim, “Improving robustness of deep neural networks via spectral masking for automatic speech recognition,” in Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2013, pp. 279–284, Czech Republic, December 2013. View at: Google Scholar Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, “A regression approach to speech enhancement based on deep neural networks,” IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 23, no. 1, pp. 7–19, 2015. View at: Publisher Site | Google Scholar D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, “Speaker verification using adapted Gaussian mixture models,” Digital Signal Processing, vol. 10, no. 1, pp. 19–41, 2000. View at: Publisher Site | Google Scholar N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, “Front-end factor analysis for speaker verification,” IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788–798, 2011. View at: Publisher Site | Google Scholar O. Abdel-Hamid, A.-R. Mohamed, H. Jiang, and G. Penn, “Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition,” in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '12), pp. 4277–4280, IEEE, March 2012. View at: Publisher Site | Google Scholar M. McLaren, Y. Lei, and L. Ferrer, “Advances in deep neural network approaches to speaker recognition,” in Proceedings of the 40th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2015, pp. 4814–4818, Australia, April 2014. View at: Google Scholar C. Yu, A. Ogawa, M. Delcroix, T. Yoshioka, T. Nakatani, and J. H. L. Hansen, “Robust i-vector extraction for neural network adaptation in noisy environment,” in Proceedings of the 16th Annual Conference of the International Speech Communication Association, INTERSPEECH 2015, pp. 2854–2857, Germany, September 2015. View at: Google Scholar N. Li, M.-W. Mak, and J.-T. Chien, “Deep neural network driven mixture of PLDA for robust i-vector speaker verification,” in Proceedings of the 2016 IEEE Workshop on Spoken Language Technology, SLT 2016, pp. 186–191, USA, December 2016. View at: Google Scholar Z. Zhang, L. Wang, A. Kai, T. Yamada, W. Li, and M. Iwahashi, “Deep neural network-based bottleneck feature and denoising autoencoder-based dereverberation for distant-talking speaker identification,” EURASIP Journal on Audio, Speech, and Music Processing, vol. 2015, no. 1, p. 12, 2015. View at: Google Scholar M. McLaren, Y. Lei, and N. Scheffer, “Application of convolutional neural networks to speaker recognition in noisy conditions,” in Proceedings of the Fifteenth Annual Conference of the International Speech Communication Association, 2014. View at: Google Scholar T. N. Sainath, B. Kingsbury, and B. Ramabhadran, “Auto-encoder bottleneck features using deep belief networks,” in Proceedings of the 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2012, pp. 4153–4156, Japan, March 2012. View at: Google Scholar Y. Shinohara, “Adversarial multi-task learning of deep neural networks for robust speech recognition,” INTERSPEECH, pp. 2369–2372, 2016. View at: Google Scholar N. D. Lane and P. Georgiev, “Can deep learning revolutionize mobile sensing?” in Proceedings of the the 16th International Workshop, pp. 117–122, Santa Fe, NM, USA, Feburary 2015. View at: Publisher Site | Google Scholar Copyright Copyright © 2018 Jiyang Xie et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. PDF Download Citation Download other formats Order printed copies Views 6447 Downloads 2334 Citations 36 About Us Contact us Partnerships Blog Journals Article Processing Charges Print editions Authors Editors Reviewers Partnerships Hindawi XML Corpus Open Archives Initiative Fraud prevention Follow us: Privacy PolicyTerms of ServiceResponsible Disclosure PolicyCookie PolicyCopyrightModern slavery statementCookie Preferences

Paper 8:
- APA Citation: Le-Nguyen, M.-H., Turgis, F., Fayemi, P.-E., & Bifet, A. (2021). A complete streaming pipeline for real-time monitoring and predictive maintenance. In Proceedings of the 31st European Safety and Reliability Conference (p. 2119).
  Main Objective: To present a pipeline for real-time monitoring and predictive maintenance of railway systems using online learning techniques, addressing data quality, preprocessing, machine learning model deployment, and online learning algorithms.
  Study Location: Unspecified
  Data Sources: Real-time sensor data from railway systems
  Technologies Used: Online learning algorithms, Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests
  Key Findings: The authors propose a pipeline for real-time monitoring and predictive maintenance using online learning techniques, discussing the challenges and potential benefits of using these algorithms in this context.
  Extract 1: The authors propose a pipeline for real-time monitoring and predictive maintenance of railway systems using online learning techniques, including data preprocessing, machine learning model deployment, and online learning algorithms.
  Extract 2: The paper discusses the challenges and potential benefits of using online learning algorithms in the context of real-time data processing and inference, highlighting their ability to continuously update and improve machine learning models based on incoming data.
  Limitations: The paper focuses on the application of online learning techniques in the context of railway maintenance, and it is not clear how easily these techniques can be adapted to the domain of irrigation management. Additionally, the paper does not provide any empirical evaluation of the proposed pipeline, which would be valuable for assessing its effectiveness.
  Relevance Evaluation: The paper is highly relevant to the point of focus on the application of online learning techniques for real-time data processing and inference in automated irrigation management systems. The authors provide a detailed and well-structured pipeline for implementing online learning in the context of railway maintenance, which can be easily adapted to the domain of irrigation management. The paper also discusses the challenges and potential benefits of using online learning algorithms in this context, making it a valuable resource for researchers and practitioners alike.
  Relevance Score: 0.9
  Inline Citation: (Le-Nguyen et al., 2021)
  Explanation: This paper presents a comprehensive pipeline for real-time monitoring and predictive maintenance of railway systems using online learning techniques. The authors aim to address data quality and preprocessing in the cloud, containerization strategies for scalable and autonomous deployment, and the deployment of machine learning models for real-time data processing and inference. In particular, they focus on online learning algorithms to continuously update and improve machine learning models based on incoming real-time data.

 Full Text: >
Skip to main content Skip to article Journals & Books Search Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download full issue Outline Abstract Keywords References Transportation Research Procedia Volume 72, 2023, Pages 171-178 Real-time learning for real-time data: online machine learning for predictive maintenance of railway systems Author links open overlay panel Minh-Huong Le-Nguyen a b, Fabien Turgis b, Pierre-Emmanuel Fayemi b, Albert Bifet a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.trpro.2023.11.391 Get rights and content Under a Creative Commons license open access Abstract In the railway, an unexpected fault reduces service availability and may cause fatalities. Therefore, maintenance must be timely. Nowadays, the omnipresence of onboard sensors generates data enormously, thus enabling data-driven predictive maintenance. For this, machine learning has come into prominence. Traditionally, a machine learning model is trained on a batch of data. However, this approach lags behind on fast-paced data streams. For real-time learning on real-time data, we propose a pipeline that employs online machine learning to address predictive maintenance of sensorized railway systems. We showcase the implementation and experimental results of two modules automating data preprocessing to demonstrate the potentials of online learning. We also discuss the intuition of another module using online clustering to monitor the evolution of system health. Previous article in issue Next article in issue Keywords Big dataTransport services and sustainable citiesOnline machine learningPredictive maintenanceRailway View PDF References Bengio et al., 2013 Y. Bengio, A. Courville, P. Vincent Representation Learning: A Review and New Perspectives IEEE Transactions on Pattern Analysis and Machine Intelligence, 35 (2013), pp. 1798-1828, 10.1109/TPAMI.2013.50 View in ScopusGoogle Scholar Canizo et al., 2017 M. Canizo, E. Onieva, A. Conde, S. Charramendieta, S. Trujillo Real-time predictive maintenance for wind turbines using Big Data frameworks 2017 IEEE International Conference on Prognostics and Health Management (ICPHM) (2017), pp. 70-77, 10.1109/ICPHM.2017.7998308 View in ScopusGoogle Scholar Cao et al., 2006 F. Cao, M. Ester, W. Qian, A. Zhou Density-Based Clustering over an Evolving Data Stream with Noise Proceedings of the Sixth SIAM International Conference on Data Mining, April 20-22, 2006, , Bethesda, MD, USA (2006), 10.1137/1.9781611972764.29 Google Scholar Cohen, 1960 J. Cohen A Coefficient of Agreement for Nominal Scales Educational and Psychological Measurement, 20 (1960), pp. 37-46, 10.1177/001316446002000104 Google Scholar Diez et al., 2016 A. Diez, N.L.D. Khoa, M. Makki Alamdari, Y. Wang, F. Chen, P. Runcie A clustering approach for structural health monitoring on bridges J Civil Struct Health Monit, 6 (2016), pp. 429-445, 10.1007/s13349-016-0160-0 View in ScopusGoogle Scholar Feng et al., 2019 X. Feng, C. Weng, X. He, X. Han, L. Lu, D. Ren, M. Ouyang Online State-of-Health Estimation for Li-Ion Battery Using Partial Charging Segment Based on Support Vector Machine IEEE Transactions on Vehicular Technology, 68 (2019), pp. 8583-8592, 10.1109/TVT.2019.2927120 View in ScopusGoogle Scholar Hochreiter and Schmidhuber, 1997 S. Hochreiter, J. Schmidhuber Long Short-Term Memory Neural Comput, 9 (1997), pp. 1735-1780, 10.1162/neco.1997.9.8.1735 View in ScopusGoogle Scholar Jardine et al., 2006 A.K.S. Jardine, D. Lin, D. Banjevic A review on machinery diagnostics and prognostics implementing condition-based maintenance Mechanical Systems and Signal Processing, 20 (2006), pp. 1483-1510, 10.1016/j.ymssp.2005.09.012 View PDFView articleView in ScopusGoogle Scholar Järvelin and Kekäläinen, 2002 K. Järvelin, J. Kekäläinen Cumulated gain-based evaluation of IR techniques ACM Trans. Inf. Syst., 20 (2002), pp. 422-446, 10.1145/582415.582418 View in ScopusGoogle Scholar Le Nguyen et al., 2021 M.H. Le Nguyen, F. Turgis, P.-E. Fayemi, A. Bifet A Complete Streaming Pipeline for Real-time Monitoring and Predictive Maintenance Proceedings of the 31st European Safety and Reliability Conference. Presented at the 31st European Safety and Reliability Conference (2021), p. 2119, 10.3850/978-981-18-2016-8_400-cd Google Scholar MIMOSA OSA-CBM 2001 MIMOSA OSA-CBM, 2001. Google Scholar Ribeiro et al., 2016 R.P. Ribeiro, P. Pereira, J. Gama Sequential anomalies: a study in the Railway Industry Mach Learn, 105 (2016), pp. 127-153, 10.1007/s10994-016-5584-6 View in ScopusGoogle Scholar Sahal et al., 2020 R. Sahal, J.G. Breslin, M.I. Ali Big data and stream processing platforms for Industry 4.0 requirements mapping for a predictive maintenance use case Journal of Manufacturing Systems, 54 (2020), pp. 138-151, 10.1016/j.jmsy.2019.11.004 View PDFView articleView in ScopusGoogle Scholar Su and Huang, 2018 C.-J. Su, S.-F. Huang Real-time big data analytics for hard disk drive predictive maintenance Computers & Electrical Engineering, 71 (2018), pp. 93-101, 10.1016/j.compeleceng.2018.07.025 View PDFView articleView in ScopusGoogle Scholar Tian et al., 2019 H. Tian, N.L.D. Khoa, A. Anaissi, Y. Wang, F. Chen Concept Drift Adaption for Online Anomaly Detection in Structural Health Monitoring Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM ’19, Association for Computing Machinery, New York, NY, USA (2019), pp. 2813-2821, 10.1145/3357384.3357816 View in ScopusGoogle Scholar Zhao et al., 2019 Zhao, Y., Nasrullah, Z., Li, Z., 2019. PyOD: A Python Toolbox for Scalable Outlier Detection. arXiv:1901.01588 [cs, stat]. Google Scholar Zubaroğlu and Atalay, 2021 A. Zubaroğlu, V. Atalay Data stream clustering: a review Artif Intell Rev, 54 (2021), pp. 1201-1236, 10.1007/s10462-020-09874-x View in ScopusGoogle Scholar Cited by (0) © 2023 The Author(s). Published by Elsevier B.V. Part of special issue TRA Lisbon 2022 Conference Proceedings Transport Research Arena (TRA Lisbon 2022),14th-17th November 2022, Lisboa, Portugal Edited by Luís de Picado Santos, Jorge Pinho de Sousa, Elisabete Arsenio Download full issue Other articles from this issue Public-private partnership in high-speed railway infrastructures: elements for improvement 2023 Mario González-Medrano, José-María Rotellar-García View PDF ADSCENE scenarios data base: Focus on accident data support for validation of Automated Driving functions 2023 L. Guyonvarch, …, S. Geronimi View PDF Wide bandgap semiconductors enabling highly efficient electrified vehicles 2023 Christoph Abart, …, Adrian Lis View PDF View more articles Recommended articles Article Metrics Captures Readers: 4 View details About ScienceDirect Remote access Shopping cart Advertise Contact and support Terms and conditions Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.

Paper 9:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: Comparison of ML models for charging station classiﬁcation. Machine Learning Model
Accuracy
Decision Tree
93%
Random Forest
94%
SVM
29%
KNN
41%
DNN
77%
LSTM
94%
  Extract 2: LSTM is found to be the most accurate model for classifying charging stations and vehicle speeds. Its ability to recognize temporal patterns in data about power consumption contributes to its high accuracy.
  Limitations: >
  Relevance Evaluation: {'address_intent': True, 'address_key_aspects': True, 'highly_relevant': True, 'relevance_score': 0.9}
  Relevance Score: 0.9
  Inline Citation: >
  Explanation: The paper in question, "Electric Vehicle Charging System in the Smart Grid Using Different Machine Learning Methods" by Mazhar et al., proposes a two-fold management system that incorporates machine learning to manage EV charging for both conventional and rapid charging and vehicle-to-grid (V2G) scenarios. The system aims to optimize the distribution grid by reducing voltage fluctuations and power losses while guiding drivers to the most suitable charging stations.

The paper's relevance to the outlined review intention lies in its focus on machine learning techniques for optimizing EV charging and reconﬁguring the charging network of an electric vehicle ﬂeet. The evaluation of various machine learning algorithms, including Deep Neural Networks (DNN), K-Nearest Neighbors (KNN), Long Short-Term Memory (LSTM), Random Forest (RF), Support Vector Machine (SVM), and Decision Tree (DT), to identify the most effective approach for managing the charging and reconﬁguring of the electric vehicle ﬂeet aligns well with the purpose of the review.

1. LSTM was found to be the most accurate model for classifying charging stations and vehicle speeds. Its ability to recognize temporal patterns in data about power consumption contributes to its high accuracy.

2. The study's findings demonstrate the effectiveness of LSTM in managing EV charging, particularly in the presence of uncertain data, which is a common challenge in real-world scenarios with rapidly changing conditions. LSTM outperforms other machine learning models in handling uncertain load data and maintaining high accuracy, making it a suitable choice for EV management systems that must adapt to dynamic and unpredictable conditions.

 Full Text: >
Citation: Mazhar, T.; Asif, R.N.;
Malik, M.A.; Nadeem, M.A.; Haq, I.;
Iqbal, M.; Kamran, M.; Ashraf, S.
Electric Vehicle Charging System in
the Smart Grid Using Different
Machine Learning Methods.
Sustainability 2023, 15, 2603. https://
doi.org/10.3390/su15032603
Academic Editors: Francesco Calise,
Maria Vicidomini and Francesco
Liberato Cappiello
Received: 9 December 2022
Revised: 9 January 2023
Accepted: 10 January 2023
Published: 1 February 2023
Copyright:
© 2023 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
sustainability
Article
Electric Vehicle Charging System in the Smart Grid Using
Different Machine Learning Methods
Tehseen Mazhar 1
, Rizwana Naz Asif 2, Muhammad Amir Malik 3
, Muhammad Asgher Nadeem 4,
Inayatul Haq 5
, Muhammad Iqbal 1, Muhammad Kamran 6 and Shahzad Ashraf 7,*
1
Department of Computer Science, Virtual University of Pakistan, Lahore 54000, Pakistan
2
School of Computer Science, National College of Business Administration & Economics,
Lahore 54000, Pakistan
3
Department of Computer Science and Software Engineering, Islamic International University,
Islamabad 44000, Pakistan
4
Department of Computer Science, University of Sargodha, Sargodha 40100, Pakistan
5
School of Information Engineering, Zhengzhou University, Zhengzhou 450001, China
6
Department of Computer Science, NCBA&E Multan, Multan 60650, Pakistan
7
NFC Institute of Engineering and Technology, Multan 60650, Pakistan
*
Correspondence: nfc.iet@hotmail.com
Abstract: Smart cities require the development of information and communication technology to
become a reality (ICT). A “smart city” is built on top of a “smart grid”. The implementation of
numerous smart systems that are advantageous to the environment and improve the quality of life for
the residents is one of the main goals of the new smart cities. In order to improve the reliability and
sustainability of the transportation system, changes are being made to the way electric vehicles (EVs)
are used. As EV use has increased, several problems have arisen, including the requirement to build
a charging infrastructure, and forecast peak loads. Management must consider how challenging
the situation is. There have been many original solutions to these problems. These heavily rely on
automata models, machine learning, and the Internet of Things. Over time, there have been more EV
drivers. Electric vehicle charging at a large scale negatively impacts the power grid. Transformers
may face additional voltage ﬂuctuations, power loss, and heat if already operating at full capacity.
Without EV management, these challenges cannot be solved. A machine-learning (ML)-based charge
management system considers conventional charging, rapid charging, and vehicle-to-grid (V2G)
technologies while guiding electric cars (EVs) to charging stations. This operation reduces the
expenses associated with charging, high voltages, load ﬂuctuation, and power loss. The effectiveness
of various machine learning (ML) approaches is evaluated and compared. These techniques include
Deep Neural Networks (DNN), K-Nearest Neighbors (KNN), Long Short-Term Memory (LSTM),
Random Forest (RF), Support Vector Machine (SVM), and Decision Tree (DT) (DNN). According to
the results, LSTM might be used to give EV control in certain circumstances. The LSTM model’s peak
voltage, power losses, and voltage stability may all be improved by compressing the load curve. In
addition, we keep our billing costs to a minimum, as well.
Keywords: electric vehicles; smart grid; load forecasting; signal processing; machine learning
1. Introduction
Electric vehicles (EVs) have grown in importance as the auto industry has developed.
EV sales will reach 2.1 million in 2019, a 40% annual growth rate [1]. Electric vehicle (EV)
chargers are now an essential part of the world’s infrastructure, with 7.3 million chargers
installed globally in 2019 and a 60% increase in the number of public charging stations
installed in 2019 compared to [1]. Additionally, 43 million electric vehicles are expected
to have been sold globally by 2030, accounting for 30% of all vehicles [2]. Fast-changing
technologies, like DC-DC converters with improved performance, have greatly contributed
Sustainability 2023, 15, 2603. https://doi.org/10.3390/su15032603
https://www.mdpi.com/journal/sustainability
Sustainability 2023, 15, 2603
2 of 26
to this [3]. EVs must be managed properly as soon as possible. Due to the high amount
of energy required to charge these vehicles, the high number of EVs on the road places
signiﬁcant stress on the distribution grid. As new driving techniques are developed to
help drivers lower their operating costs, demand for these vehicles is also anticipated to
rise. More electricity is needed to power their charging stations as more electric vehicles
are on the road. When more EVs are on the road, the load curve rises, which increases
the stress on the transformer and the rest of the distribution grid. A strong and reliable
management system is necessary for the distribution grid to run efﬁciently and dependably.
For electric vehicles, Apple Inc. has developed a method that only considers the amount
of battery charge needed to locate the closest charging station [4]. However, neither the
demands on the distribution grid nor the drawbacks of charging the EV are considered.
The development of ICT greatly helps the achievement of smart cities. A framework made
up of ICT is referred to as a “smart city.” It is used to promote and develop sustainable
practices to handle the many problems that urban environments present. A smart city
is made up of an intelligent network of machines and objects that are linked together
using cloud and wireless technology. The Internet of Things manages and analyzes the
data they receive in real time to assist citizens, towns, and businesses in making the best
decisions to improve living standards [5]. Integrating devices and data with a city’s physical
infrastructure can lower living expenses and promote sustainability. Electric vehicle (EV)
charging stations and parking meters can easily be reached by connected cars. A smart
city combines the physical infrastructure with ICT to provide beneﬁts such as improved
mobility, convenience, air and water quality, and energy conservation. Sensors, motors,
centralized units, networks, interfaces, and intelligent metering infrastructure will all be
used in smart buildings in a smart city [6]. Governmental organizations are attempting
to utilize cellular and Low Power Wide Area technologies linked to the infrastructure to
increase visitor and resident convenience and efﬁciency. To minimize energy consumption,
smart cities must implement a smart grid concept into their energy infrastructure. An
intelligent metering infrastructure creates two-way communication in all grid nodes [7].
Customers can take active or passive actions to increase the grid’s reliability and energy
efﬁciency. The smart grid can also lessen environmental pollution by facilitating the efﬁcient
integration of EVs and renewable energy sources into the grid. Government agencies could
interact with the public, create infrastructure, and track operations and development thanks
to smart city technologies. IoT devices are used in smart cities to enhance operations, service
delivery, and citizen involvement. According to recent research, smart cities are the best
solution to population pressure in developing and developed nations. Congestion, housing,
pollution, administration, availability of power, etc. ICT is utilized to boost productivity,
interact with municipal or urban services, and enhance the products and services provided
by city authorities. Better citizen-government relationships save expenses and resource use.
They are intended to respond to issues in a realistic, proactive manner [8]. This literature
review aims to inform policymakers and smart city planners about how to take the needs
and well-being of the community into account when making plans or decisions. The need
for eco-friendly vehicle technologies is explained by the decline in global CO2 emissions
and the rising price of carbon fuels. In contrast to conventional vehicles, modern electrical
vehicles (EVs) will improve air quality by reducing carbon emissions. The main issues
brought on by using traditional vehicles will be solved if electric vehicles are successfully
integrated. At this time, the penetration of electric vehicles has not shown a signiﬁcant
effect on the grid’s demand for electricity. Electric vehicles will eventually become more
widely used and more affordable. As a result, they will have a big impact on how the
smart grid operates and how much energy is needed. To lower the failures related to power
allocation and electricity ﬂow across the smart grid, intelligent management systems are
required. The performance and dependability of various ML techniques were compared to
optimize the distribution grid and reduce charging costs.
Decision Tree (DT), Random Forest (RF), Support Vector Machine (SVM), K-Nearest
Neighbors (KNN), Deep Neural Networks (DNN), and Long Short-Term Memory are the
Sustainability 2023, 15, 2603
3 of 26
machine learning (ML) techniques used in this paper (LSTM). An ML-based solution for
managing and routing an EV ﬂeet is also provided in this paper. The following are the
paper’s main contributions:
We offer a management system for managing electric vehicle (EV) charging adaptable
to the load data uncertainty that can arise with the ML system’s input data. This is carried
out to ensure the dependability and functionality of the system. This system combines V2G
technology with both standard and rapid charging.
The subject of our second analysis is the effectiveness of various machine learning
(ML) techniques in managing the charging and reconﬁguring of an electric vehicle ﬂeet.
Then, to reduce load changes, power losses, voltage ﬂuctuations, and charging costs, we
optimize using the best machine learning algorithm.
1.1. Intelligent Information Management in Smart Cities Promotes Energy and
Governance Sustainability
City and urban planners and the government should think carefully about how to
produce, acquire, and sustainably use the power before making any major decisions.
Robotic computing and artiﬁcial agencies are two new technologies for solving these
problems [9]. Cities and urban areas consume signiﬁcantly more energy than rural areas.
As a result, it will be very challenging to stop environmental pollution and global warming.
The most efﬁcient way to reduce pollution is to generate and manage urban electricity using
circuit technology. Engineers can better understand human behavior, energy consumption,
machine learning, and the Internet of Things. The need for standardized data on smart
energy, the integration of energy across smart grids, and behavioral analytics are just a
few risks.
The power efﬁciency art grid will be maximized by smart metering. This is especially
important given the complicated sustainability issues that cities and urban areas are cur-
rently dealing with. It seems that using smart technology ideas and knowledge may be a
good idea based on the ﬁndings of this analysis. They facilitate carrier machine multiple
kernel learning in NILM energy breakdown using SVMs and a genetic algorithm [5]. The
urban use of digital technology has helped the developing world’s growing population
and lack of services. Due to the expense of ensuring that infrastructure will last for a
long time, a sizable informal economy, and numerous social and political pressures in the
modern world, the idea of smart citieshas not taken off quickly. They are considering that
smart cities are political for many developing countries to support social and economic
development.
In the early 1800s, the idea of “smart growth”, which emphasizes infrastructure like
public transportation and well-planned cities, ﬁrst appeared. Emerging economies can
have technologically advanced smart cities if economic, human resource, policy-making,
and legal reforms are fully integrated into development plans [10]. Asserts that for this
to happen, government priorities and spending priorities must also change, in addition
to the general public becoming aware of and understanding technological advancements.
This area of study has become more well-known recently as people have realized how
closely related economic and political issues are to the idea of a smart city. In developing
nations around the world, building smart cities is challenging due to education and literacy
issues. People in cities must learn how to use new technologies to avoid being left out of
society because they cannot use information. IT experts and information managers must
consider how the data they collect will be used. Smart services, like smart cards, necessitate
people to learn, understand, and view things from various angles. These problems will
be resolved, and smart cities will become sustainable [5]. The challenges presented by the
growth of smart cities and the movement of people from rural to urban areas are strongly
intertwined with the well-being of the respective populations. Safety, open government,
high-quality education, and creative problem-solving are all crucial [5]. These elements
will determine whether smart cities are successful or unsuccessful, as well as the direction
of research in all areas, ultimately resulting in comprehensive solutions for the issues that
Sustainability 2023, 15, 2603
4 of 26
plague these urban areas. To understand user groups and preferences, you must examine
data from all academic disciplines. The happiness and well-being of the inhabitants may
at ﬁrst seem simple and unimportant, but it is crucial to consider these when designing
smart cities, smart services, and smart technology [11]. Figure 1 shows the Type of Electric
Vehicles and their power source.
necessitate people to learn, understand, and view things from various angles. These prob-
lems will be resolved, and smart cities will become sustainable [5]. The challenges pre-
sented by the growth of smart cities and the movement of people from rural to urban areas 
are strongly intertwined with the well-being of the respective populations. Safety, open 
government, high-quality education, and creative problem-solving are all crucial [5]. 
These elements will determine whether smart cities are successful or unsuccessful, as well 
as the direction of research in all areas, ultimately resulting in comprehensive solutions 
for the issues that plague these urban areas. To understand user groups and preferences, 
you must examine data from all academic disciplines. The happiness and well-being of 
the inhabitants may at first seem simple and unimportant, but it is crucial to consider these 
when designing smart cities, smart services, and smart technology [11]. Figure 1 shows 
the Type of Electric Vehicles and their power source. 
 
Figure 1. The type of Electric Vehicles [12]. 
An electric vehicle’s battery can account for 25 to 50% of the overall cost. According 
to predictions, batteries for electric vehicles are expected to cost about 225 Euros per kilo-
watt-hour by 2025. The cost of manufacturing Li-Ion batteries for electric vehicles de-
creased by about 50% between 2007 and 2014. These significant price drops mean that 
buying an EV will soon be comparable to purchasing a gas-powered car. Currently, three 
types of electric vehicles are available: battery, plug-in, and hybrid (HEVs). The launch 
system of the vehicle determines these classifications. The energy needed for power pro-
duction is stored and released by batteries in BEVs. Plug-in hybrids (PHEVs) have an in-
ternal combustion unit that can run on gasoline and other fuels and a battery pack that 
powers the electric motor (ICE). The battery pack in an HEV can be charged without elec-
tricity. The batteries are continuously charged due largely to the internal combustion en-
gine and brake system. BEVs do not emit greenhouse gases, so they help to keep the air 
clean. The electric motor works with the gasoline engine in PHEVs and HEVs to minimize 
size and emissions. The electric battery, the power source for the different EVs shown in 
Figure 1, is denoted by the letter EB. 
1.2. The Vehicle to Grid (V2G) Network Is a Crucial Development for the Smart Grid 
Figure 1. The type of Electric Vehicles [12].
An electric vehicle’s battery can account for 25 to 50% of the overall cost. According to
predictions, batteries for electric vehicles are expected to cost about 225 Euros per kilowatt-
hour by 2025. The cost of manufacturing Li-Ion batteries for electric vehicles decreased
by about 50% between 2007 and 2014. These signiﬁcant price drops mean that buying an
EV will soon be comparable to purchasing a gas-powered car. Currently, three types of
electric vehicles are available: battery, plug-in, and hybrid (HEVs). The launch system
of the vehicle determines these classiﬁcations. The energy needed for power production
is stored and released by batteries in BEVs. Plug-in hybrids (PHEVs) have an internal
combustion unit that can run on gasoline and other fuels and a battery pack that powers
the electric motor (ICE). The battery pack in an HEV can be charged without electricity.
The batteries are continuously charged due largely to the internal combustion engine and
brake system. BEVs do not emit greenhouse gases, so they help to keep the air clean. The
electric motor works with the gasoline engine in PHEVs and HEVs to minimize size and
emissions. The electric battery, the power source for the different EVs shown in Figure 1, is
denoted by the letter EB.
1.2. The Vehicle to Grid (V2G) Network Is a Crucial Development for the Smart Grid
Enhancing the smart grid network’s capacity to balance power supply and demand
and enabling mobile batteries to store energy reduces the negative impacts of using non-
renewable energy sources [13]. EVs are harder to integrate without an EDMS to control
energy consumption. EDMS is looking for user feedback to make charging electric vehicles
as quick and easy as possible. By taking into account a range of technical and ﬁnancial vari-
ables, such as the location and charging status of electric vehicles (EVs), user preferences,
predicted energy demand, and the state of the distribution network at the time, the Energy
Distribution Management System (EDMS) will choose the best answer to this question.
Predicting EV demand and impact is one of EDMS’s strongest features. Achieve the highest
Quality of Service (quality of service) levels possible; this is crucial [14]. The EVs Intelligent
Charging (EVIC) speciﬁcation includes the EDMS concept, as shown in Figure 2.
Sustainability 2023, 15, 2603
5 of 26
variables, such as the location and charging status of electric vehicles (EVs), user prefer-
ences, predicted energy demand, and the state of the distribution network at the time, the 
Energy Distribution Management System (EDMS) will choose the best answer to this 
question. Predicting EV demand and impact is one of EDMS’s strongest features. Achieve 
the highest Quality of Service (quality of service) levels possible; this is crucial [14]. The 
EVs Intelligent Charging (EVIC) specification includes the EDMS concept, as shown in 
Figure 2. 
 
Figure 2. Electric Vehicle powertrain energy management [15]. 
For the electric vehicle management system (EDMS) to operate as designed, each 
electric automobile of the future will need to be equipped with an EB that is synchronized 
with a data-gathering device. The onboard computer will be provided with a GNSS re-
ceiver, an inertial measurement unit (IMU), and a Wi-Fi wireless LAN interface. All three 
of these components have the potential to be used in the process of determining the loca-
tion of the vehicle [16]. The EV’s Onboard Unit has a localization unit that can constantly 
accurately identify its exact location. A built-in phone modem that can send data to the 
internet is present in the vehicle. The smart city’s communications network will be 
grounded. Wi-Fi roadside units that can be installed anywhere in the city and are compat-
ible with electric vehicle charging stations will make up this infrastructure. The EDMS 
will process the location and charging method of electric vehicles.  
1.3. Integration of EVs in Smart Grids 
Because electric cars utilize power in so many ways, systems that rely on them are 
prone to problems. After it has been determined that the electrical grid has been built 
appropriately to minimize larger penetration strains, the load profile for electric vehicles 
may be forecast. The amount of energy that can be used to power the load on an electric 
vehicle depends on the battery’s capacity. The Nissan Leaf’s built-in battery has a 30-kil-
owatt-hour energy storage capacity. These batteries can hold four full days’ worth of 
lights for the average house. Several approaches may be used to create accurate estima-
tions regarding EV load profiles. To make more precise estimates of the load needs for 
EVs, probabilistic models for plug-in electric cars make use of the fact that the charging of 
batteries does not follow a linear pattern. The object principle is the cornerstone of the 
Markov chain approach, often known as a Markov or hybrid grey model, which forecasts 
Figure 2. Electric Vehicle powertrain energy management [15].
For the electric vehicle management system (EDMS) to operate as designed, each
electric automobile of the future will need to be equipped with an EB that is synchronized
with a data-gathering device. The onboard computer will be provided with a GNSS receiver,
an inertial measurement unit (IMU), and a Wi-Fi wireless LAN interface. All three of these
components have the potential to be used in the process of determining the location of the
vehicle [16]. The EV’s Onboard Unit has a localization unit that can constantly accurately
identify its exact location. A built-in phone modem that can send data to the internet is
present in the vehicle. The smart city’s communications network will be grounded. Wi-Fi
roadside units that can be installed anywhere in the city and are compatible with electric
vehicle charging stations will make up this infrastructure. The EDMS will process the
location and charging method of electric vehicles.
1.3. Integration of EVs in Smart Grids
Because electric cars utilize power in so many ways, systems that rely on them are
prone to problems. After it has been determined that the electrical grid has been built
appropriately to minimize larger penetration strains, the load proﬁle for electric vehicles
may be forecast. The amount of energy that can be used to power the load on an elec-
tric vehicle depends on the battery’s capacity. The Nissan Leaf’s built-in battery has a
30-kilowatt-hour energy storage capacity. These batteries can hold four full days’ worth of
lights for the average house. Several approaches may be used to create accurate estimations
regarding EV load proﬁles. To make more precise estimates of the load needs for EVs,
probabilistic models for plug-in electric cars make use of the fact that the charging of
batteries does not follow a linear pattern. The object principle is the cornerstone of the
Markov chain approach, often known as a Markov or hybrid grey model, which forecasts
EV demand. This strategy considers both predicted deviations and recurring trends. Artiﬁ-
cial neural networks, support vector machines, and decision trees, among other machine
learning techniques, may be used to estimate the number of EVs presently in use on the
roadways at any time [17]. The initial dataset is split during the data conﬁguration stage
into training and test datasets. One uses a training dataset to train the model and ﬁnd
“hidden” correlations and patterns between the target values and attribute values. The
effectiveness of the data mining strategy is assessed using the test dataset. Information
from the training dataset is used during the testing and grading phases. The training
and evaluation process is repeated about 20 times to ﬁnd the ideal set of parameters. A
model’s performance can be assessed using r-correlation, root-mean-squared error (RMSE),
and root-mean-square absolute error (MAPE). Time and training metrics were used to
evaluate the performance. According to the study, SVM offers the most accurate forecasts
(r = 98.09). SVM training takes longer than other training approaches. In such a short
period, ANN achieved amazing results. The MAPE criterion was the most effective method
for evaluating the precision of load forecasts. Values are most effective when they are less
than 5%. Applying the RMSE index severely wants to punish large absolute errors. It
Sustainability 2023, 15, 2603
6 of 26
is possible to develop a model of the requirements and routines of EV drivers using the
Markov model. The existing prediction charging model was initially designed to support a
single EV, but it has since been changed to support charging multiple EVs simultaneously.
Studies on origin-destination (OD) and trafﬁc have also made it possible to predict the load
proﬁle for electric vehicles. Integration models for Electric Vehicles (EVs) into the smart
grid, both centralized and decentralized is presented in Figure 3.
used to evaluate the performance. According to the study, SVM offers the most accurate 
forecasts (r = 98.09). SVM training takes longer than other training approaches. In such a 
short period, ANN achieved amazing results. The MAPE criterion was the most effective 
method for evaluating the precision of load forecasts. Values are most effective when they 
are less than 5%. Applying the RMSE index severely wants to punish large absolute errors. 
It is possible to develop a model of the requirements and routines of EV drivers using the 
Markov model. The existing prediction charging model was initially designed to support 
a single EV, but it has since been changed to support charging multiple EVs simultane-
ously. Studies on origin-destination (OD) and traffic have also made it possible to predict 
the load profile for electric vehicles. Integration models for Electric Vehicles (EVs) into the 
smart grid, both centralized and decentralized is presented in Figure 3. 
 
Figure 3. Integration models for Electric Vehicles (EVs) into the smart grid, both centralized and 
decentralized [18]. 
The cheap automated decision making is crucial for saving money on maintenance 
and energy. Authors in [19] were able to increase discharge rates and lower charging costs 
by utilizing cyber insurance. The short-term price, coverage, and insurance options can be 
determined by combining the proposed cost function with a Cartesian product. When an 
elevation is viewed as an idealized cost function with parameters, a learning algorithm 
can decide the best course of action. The authors in [20] looked at how to forecast the load 
on the distribution grid from electric vehicles in the upcoming years using an enhanced 
grey theory prediction model. An intelligent, machine learning-based technique for charg-
ing electric automobiles is presented by [21]. In the presence of renewable energy sources, 
[22] looked at how charging electric vehicles changed the demand profile of the smart 
Figure 3. Integration models for Electric Vehicles (EVs) into the smart grid, both centralized and
decentralized [18].
The cheap automated decision making is crucial for saving money on maintenance
and energy. Authors in [19] were able to increase discharge rates and lower charging costs
by utilizing cyber insurance. The short-term price, coverage, and insurance options can be
determined by combining the proposed cost function with a Cartesian product. When an
elevation is viewed as an idealized cost function with parameters, a learning algorithm can
decide the best course of action. The authors in [20] looked at how to forecast the load on
the distribution grid from electric vehicles in the upcoming years using an enhanced grey
theory prediction model. An intelligent, machine learning-based technique for charging
electric automobiles is presented by [21]. In the presence of renewable energy sources, [22]
looked at how charging electric vehicles changed the demand proﬁle of the smart grid. They
came up with solutions to the problems. According to [23], using artiﬁcial intelligence and
smart grids have been used to operate electric cars (EVs). The prediction method of [24] was
used to project the demand for electric vehicles shortly (2012). To forecast the variable need
for charging electric vehicles and the overall number of vehicles on the road [22] created
a cellular agent automaton. The use of machine learning in this approach was crucial.
Smart city development can reduce population pressures, but overall population growth
is predictable, especially in developing countries. Because it would lead to new political,
economic, and social problems, including technological exclusion and discrimination, it is
not possible. It involves extensive research and consideration from all parties involved, and
the best chance of success comes from a plan that looks at the situation from a variety of
angles [25]. Research on smart cities suggests incorporating attempts to reduce computer
processes into the provision of social services, with an emphasis on boosting end-user
comprehension. Infrastructure like the Global Positioning System (GPS), Galileo, and
the European Geostationary Navigation Overlay Service is necessary for electric vehicle
compatibility with the smart grid (EGNOS). Instead of satellites, Wi-Fi will be used to
identify our location [26]. The various ways that EVs can be integrated into smart grids
and smart cities are shown in Figure 4. Electric vehicles are introduced in a smart city using
Wi-Fi and GPS positioning systems.
Sustainability 2023, 15, 2603
7 of 26
attempts to reduce computer processes into the provision of social services, with an em-
phasis on boosting end-user comprehension. Infrastructure like the Global Positioning 
System (GPS), Galileo, and the European Geostationary Navigation Overlay Service is 
necessary for electric vehicle compatibility with the smart grid (EGNOS). Instead of satel-
lites, Wi-Fi will be used to identify our location [26]. The various ways that EVs can be 
integrated into smart grids and smart cities are shown in Figure 4. Electric vehicles are 
introduced in a smart city using Wi-Fi and GPS positioning systems. 
 
Figure 4. EVs integration with smart grid and smart houses [27]. 
Various elements, such as energy storage technologies, electric cars, and renewable 
energy sources, were considered while studying the ideal energy management frame-
work. To estimate the activity for the next day that would result in the lowest overall cost 
for the energy that was bought, the authors solved a linear programming problem. Using 
a case study encompassing three separate buildings, it was shown that the suggested 
method reduced energy costs while simultaneously boosting grid dependability. They de-
veloped a domestic energy consumption management system that [28] identified the ideal 
procedure for scheduling various loads and supplies across different price levels using a 
multi-objective optimization problem. Results showed that the system could decrease 
transformer losses for the utility provider while still meeting customer demand. Various 
techniques have been used to study optimization, like dynamic and quadratic program-
ming. Sequential and probabilistic reasoning techniques for lowering distribution system 
Figure 4. EVs integration with smart grid and smart houses [27].
Various elements, such as energy storage technologies, electric cars, and renewable
energy sources, were considered while studying the ideal energy management framework.
To estimate the activity for the next day that would result in the lowest overall cost for
the energy that was bought, the authors solved a linear programming problem. Using
a case study encompassing three separate buildings, it was shown that the suggested
method reduced energy costs while simultaneously boosting grid dependability. They
developed a domestic energy consumption management system that [28] identiﬁed the
ideal procedure for scheduling various loads and supplies across different price levels
using a multi-objective optimization problem. Results showed that the system could
decrease transformer losses for the utility provider while still meeting customer demand.
Various techniques have been used to study optimization, like dynamic and quadratic
programming. Sequential and probabilistic reasoning techniques for lowering distribution
system losses were studied by [29]. Based on the outcomes, the developed method showed
that minimizing load variation is more advantageous than minimizing losses because
it produces the same overall result in a shorter time. In addition, it works with any
conﬁguration of the distribution grid, regardless of type. [29] investigated how improving
the voltage proﬁle of a real-time coordination system could be achieved by lowering the
total cost of energy production and power losses. Assuming that EVs are randomly plugged
in, the algorithm uses the maximum sensitivities selection optimization method. With the
help of a real-time system, the distribution grid can be more effective, and its load can
be decreased [30] investigated load variance optimization for a single-home micro grid.
Quadratic programming is used in a micro grid that serves a single house to minimize load
variation. The system lowers the variance in individual loads, which reduces the conﬂict in
loads on the distribution grid. This shows that the system beneﬁts the entire grid, according
to the data. However, because some parameters, like the load curve, were taken as givens,
the system is not as effective as it could be. In the real world, this might not be accurate.
A coordinated charging method for EVs based on power factor correction was developed
by [31] to decrease power losses and enhance the voltage proﬁle. Users’ favorite charging
methods were considered using a priority selection algorithm. The system reduced peak
demand and improved the performance of the electrical grid. Researchers in [32] proposed
charging many electric vehicles simultaneously using a decentralized topology system. The
suggested approach employs collectively pointless games. The system decreases the peak-
to-valley gap by ﬁlling the valley of the load curve—costs for producing power decrease
Sustainability 2023, 15, 2603
8 of 26
as a result. Less two-way communication is needed between command centers and EVs.
Regardless of power loss or voltage variations, the optimization challenge ensures that EVs
can be completely charged using the least amount of energy. The approach assumes that all
loads, not just those from EVs, are predictable.
In the design of power architectures, energy storage methods, micro-grid control
systems, and energy management optimization, key problems and limitations are covered
in the article. A summary of current research on EV charging stations is also provided. An
added beneﬁt is that the infrastructure was built to support various degrees of micro grids
for charging electric automobiles. In order to make EV charging stations as efﬁcient as
possible, a lot of research is done on the coordinated control systems, energy management
plans, and machine learning methods utilized by these stations. To analyze and compare
the system’s performance, many machine learning techniques were used. In the end, this
approach is extensively used, and it found how to add and remove constraints and obstacles
in the most reliable way possible
2. Literature Review
2.1. Managed Charging of Electric Vehicle
Unplanned EV charging harms the distribution grid, as shown by several studies. Re-
searchers [33,34] examined how electric vehicles are charged. Without proper coordination,
it has been seen that charging EVs causes signiﬁcant voltage disturbances and energy loss.
One of the most popular methods of frequency control frequency droop control is
currently being successfully applied to electric cars. The major technique for controlling
the frequency now is to regulate the output of generators connected to the main power
grid. As conventional power plants are phased out, electric vehicles are a great alternative
since their batteries may be charged or discharged in response to frequency deviation
alarms. As a result, we investigate frequency regulation in a power grid model with loads,
traditional producers, and a sizable amount of EVs. By providing grid-related services,
these last devices, in particular the FDC, autonomously optimize the grid. Two new control
algorithms that may be used to manage electric car batteries in the best possible way during
frequency regulation service. The control processes, on the one hand, make sure that
the power balance and frequency management of the main grid are stable. On the other
hand, these approaches can meet a range of EV charging needs. The available methods
are designed to reduce the failure rates of battery-powered devices. This contrasts with
the EV literature, which frequently concentrates on determining the ideal charge level.
The performance of the solutions is then contrasted with that of other state-of-the-art
V2G control systems. The results of numerical experiments carried out using an accurate
representation of the power grid show that the suggested methods perform well in actual
operational circumstances. According to [33], these detrimental effects might become more
noticeable as more EVs are on the road. Renewable energy may be able to solve these
problems, but [34] analysis shows that power quality regulation is still a problem. Authors
in [35] investigated the dangers of using the distribution grid to charge EVs. It follows that
V2G technologies can enhance the grid’s functionality. V2G technologies, according to [36],
have many beneﬁts, such as power regulation, load balancing, and current harmonics
ﬁltering. V2G technologies, however, might lead to EVs discharging deeply. In order
to charge an electric vehicle ﬂeet as efﬁciently as possible while the total capacity of the
power grid is constrained, a new distributed control technique is proposed in this study.
The most economical way to charge for a proﬁle of total energy use can be found by
solving a scheduling problem. As a result, the optimization issue that arises is a quadratic
programming issue with choice variables linked to both the objective function and the
constraint. In our model, people only interact with their immediate neighbors and reach
out to higher-ups when making decisions. The answer was discovered using a distributed
iterative approach that takes into account duality, proximity, and consensus theory. An
illustration case study shows how the tactic can result in the best solution for everyone [37].
This work proposes a new distributed control technique for optimally charging an electric
Sustainability 2023, 15, 2603
9 of 26
vehicle (EV) ﬂeet when the overall capacity of the power grid is constrained. By resolving a
scheduling issue, the most cost-effective method of charging for a proﬁle of total energy
use can be determined. Consequently, the resulting optimization problem is expressed
as a quadratic programming problem with choice variables that are connected to both
the objective function and the constraint. In our concept, individuals just communicate
with their close neighbors and make decisions without contacting anyone above them. A
distributed iterative method that considers duality, proximity, and consensus theory was
utilized to obtain the solution. An example case study demonstrates that the strategy can
lead to the optimal answer for everyone.
The batteries in electric vehicles (EVs) degrade over time, shortening their life cycle
and lowering customer satisfaction, so this comes at a cost. Because it only considers the
power grid’s needs and ignores those of the end users, it is not thought to be the best option.
The impact of various charging techniques on the price of charging and the rate of battery
decrease was also studied by [38]; the effectiveness of non-coordinated, one-way, and two-
way charging was examined. Battery life can be signiﬁcantly increased at a lower cost when
intelligent charging techniques and time-of-use electricity prices are combined. Most of the
research on coordinated charging of electric vehicles conducted by [39] was user-focused.
Within the limitations of the power grid, an optimization model that reduces costs has been
created. The beneﬁts of the system become clearer as the number of EVs rises. The system
can cut costs by up to 50% compared to uncoordinated billing. The system requires smart
meters to gather real-time data on how electric vehicles are being charged and does not
consider fast charging. It can be challenging to charge electric vehicles (EVs) because they
consume a lot of energy, and renewable energy generation is not always reliable, according
to [39], who also looked into this issue. Signiﬁcant cost savings of up to 8% were found
when comparing the results of the charging scheme to those of uncoordinated charging.
Recharging an electric vehicle reduces the amount of carbon dioxide released into the
atmosphere and is better for the environment. In agreement with [39], smart charging
infrastructure is crucial for deploying EVs. Inductive and conductive charging is used for
electric and plug-in hybrid vehicles. Conducted charging systems demand a power station
that is physically connected to an electric vehicle compared to inductive charging systems.
A scheme for electrical conversion may also include a high-to-low-frequency converter
and power factor correction (PFC). Either an internal or external charging system can be
used to charge the device. The battery and inverter current regulators and their power
supplies are built to house inside the vehicle in onboard chargers. Off-board chargers are
located outside the car. Conductive chargers need not be able to transfer power in order to
be considered such. We might also add more standards. The AC level 1 charger is one of
many different charger types. Figure 5 shows the Static WCS for EVs.
Power grids are experiencing a loss of transmission and a decrease in their ability to
perform main frequency control as traditional generators are being phased out in favor of
renewable energy sources. The issue is made signiﬁcantly more challenging by the steadily
increasing number of electric vehicles (EVs), which necessitates the creation of trying to cut
methods for the management of grid operations. Rather than being an issue, the growth of
electric vehicles could end up being a solution to a number of issues that have arisen with
the electricity grid. In this context, the so-called vehicle-to-grid (V2G) mode of operation
is crucial and is one of the main operating modes for electric vehicles. This mode can
offer auxiliary services to the power grid such peak clipping, load shifting, and frequency
management. To be more precise, frequency droop management, one of the more traditional
techniques for frequency regulation, has just lately begun to be successfully applied to
electric vehicles. This is the primary frequency regulation, which is done by controlling
the current active power of the largest grid generators. Due to the decommissioning
of conventional power plants, electric vehicles are viewed as particularly advantageous
alternatives. This is because electric vehicles (EVs) have the ability to respond to alarms
about frequency deviations by either charging or discharging their batteries.
Sustainability 2023, 15, 2603
10 of 26
nal charging system can be used to charge the device. The battery and inverter current 
regulators and their power supplies are built to house inside the vehicle in onboard 
chargers. Off-board chargers are located outside the car. Conductive chargers need not be 
able to transfer power in order to be considered such. We might also add more standards. 
The AC level 1 charger is one of many different charger types. Figure 5 shows the Static 
WCS for EVs. 
 
Figure 5. Static WCS for EVs [40]. 
Power grids are experiencing a loss of transmission  and a decrease in their ability 
to perform main frequency control as traditional generators are being phased out in favor 
of renewable energy sources. The issue is made significantly more challenging by the 
steadily increasing number of electric vehicles (EVs), which necessitates the creation of 
trying to cut methods for the management of grid operations. Rather than being an issue, 
the growth of electric vehicles could end up being a solution to a number of issues that 
have arisen with the electricity grid. In this context, the so-called vehicle-to-grid (V2G) 
mode of operation is crucial and is one of the main operating modes for electric vehicles. 
This mode can offer auxiliary services to the power grid such peak clipping, load shifting, 
and frequency management. To be more precise, frequency droop management, one of 
the more traditional techniques for frequency regulation, has just lately begun to be suc-
cessfully applied to electric vehicles. This is the primary frequency regulation, which is 
done by controlling the current active power of the largest grid generators. Due to the 
decommissioning of conventional power plants, electric vehicles are viewed as particu-
larly advantageous alternatives. This is because electric vehicles (EVs) have the ability to 
respond to alarms about frequency deviations by either charging or discharging their bat-
teries.  
This study focuses on modifying individual loads to conform to system constraints. 
This research was conducted because it is challenging to charge a significant number of 
electric vehicles with our current infrastructure. Since distributed methods may extend 
Figure 5. Static WCS for EVs [40].
This study focuses on modifying individual loads to conform to system constraints.
This research was conducted because it is challenging to charge a signiﬁcant number of
electric vehicles with our current infrastructure. Since distributed methods may extend
and communicate more efﬁciently than earlier approaches, they are being evaluated as
a potential solution. We show that distributed linear optimization and communication
networks can be used to ensure relative average fairness while maximizing utilization.
Here, we outline these techniques. The outcomes of our research and simulation are
presented to show how highly useful these techniques are. When used in test scenarios,
the algorithm often performs within 5% of the ideal centralized conﬁguration. Scaling up,
however, is easier and requires less communication than in the perfect situation. The end
user, in this case, the owner of the vehicle, is the one who is looking at battery charging
in this article. We describe a number of distributed algorithms to accomplish speciﬁc
policy goals due to the fact that vehicle owners’ priorities can change over time. Our
main focus is on the many types of distributed charges. Dispersed systems stand out
in terms of recharging electric vehicles for a number of reasons. First, when individual
components fail, decentralized algorithms are often more stable. Second, it is anticipated
that in charging conditions, the number of electric vehicles competing for the same amount
of energy will vary quickly, indicating a random quality. It would be nice to have self-
managing distributed optimization approaches in this kind of circumstance. In conclusion,
a distributed approach makes it possible to implement a set of rules with little risk of data
loss [41].
As a result, we investigate frequency regulation in a power grid model that includes
loads, traditional generators, and lots of EVs. By offering auxiliary services to the grid,
the latter, most notably the FDC, can take an independent part in the process of grid
optimization. For the purpose of optimizing the control of electric vehicle batteries while the
service is being provided to regulate frequency, we provide two unique control algorithms.
On the one hand, the control strategies make sure that the main grid’s power balance
and frequency stability are upheld. On the other hand, the techniques are adaptable
enough to meet a variety of electric vehicle charging requirements (EVs). The suggested
approaches’ main goal is to prevent battery device degradation, which is in contrast to
the pertinent literature’s frequent discussion of obtaining the ideal charge level in relation
to electric vehicles. The proposed solutions are last evaluated in comparison to various
cutting V2G control systems and their outcomes are contrasted. The solutions proposed are
successful when used in real operational settings, as shown by the outcomes of numerical
tests performed using a realistic model of a power grid.
Sustainability 2023, 15, 2603
11 of 26
Because batteries lose power while linked to the grid, it is difﬁcult to determine the
cost of an Energy Storage System for frequency management. As a result, researchers are
looking at how real-world conditions affect Li-ion batteries’ capacity to store energy. Both a
control method for Li-ion ESS that helps with frequency management and a cost accounting
model for frequency regulation that takes the impact of shorter battery life into account
are created. We estimate the expected lifetime and the average annual cost of the Li-ion
ESS for different dead bands and SOC set-points. Case studies show that the Li-ion ESS’s
estimated operating life under standard and full discharge settings is much shorter than
the manufacturer’s reported nominal life. In this paper, a precise method for calculating
the cost of ESS that take part in grid frequency regulation is presented. This is carried out
to aid ESS’s growth in the auxiliary services sector [42].
Because batteries degrade while the system is linked to the grid, it is challenging to
estimate the cost of an Energy Storage System (ESS) for frequency regulation. Researchers
are examining how the stresses of practical use affect the Li-ion cells in the battery energy
storage system to ﬁnd a solution to this issue. We construct a control method for Li-ion ESS
that can take part in grid frequency management, and we develop a cost accounting model
for frequency regulation that takes the effect of battery life loss into account. We determine
the anticipated working life and annual average cost of the Li-ion ESS for various dead
bands and SOC thresholds. Case studies demonstrate that the nominal lifetime speciﬁed by
the manufacturer for standard and full discharge settings for the Li-ion ESS is signiﬁcantly
shorter than the predicted lifetime under practical working conditions. This study offers an
accurate costing method for ESS participating in grid frequency regulation, which will aid
in the expansion of ESS participation in the market for peripheral services.
The controller’s charging cost can be reduced numerically and repeatedly due to the
model’s efﬁciency and simplicity. The developed EV charge features indicate a balance
between the following four trends: (1) charging during times of low electricity prices;
(2) charging slowly; (3) charging near the end of the authorized charge period; and
(4) preventing vehicles from sending power back to the grid. The result shows that batteries
charged using optimized methods exceed those charged using standard methods by a
signiﬁcant margin, using data from real hybrid electric vehicles. This suggests that smaller
batteries could be used to satisfy the needs of vehicle life. It has been shown that identical
patterns hold true for batteries of different sizes, allowing them to be used in both plug-in
hybrids and pure electric vehicles [43].
Next, we go over a distributed water ﬁlling technique that can be applied to networked
control systems where nodes communicate data with distant nodes. Water ﬁlling is a well-
known method of communication system optimization. It has assisted in solving real-world
control engineering and decision-making problems. In a system with multiple control
points, the decentralized approach of ﬁlling water tanks is described in this study. To do
this, we take into account the water levels of a number of users who only interact with their
immediate neighbors and decide as a group. We create two versions of a new distributed
algorithm that combines consensus, proximity, and ﬁxed-point mapping theory (exact and
approximation), and we show that both converge. A charging station for a ﬂeet of electric
vehicles is used as an example to show how it works.
This study shows how to charge a car’s battery in the most economical way possible,
taking into consideration the cost of power and the predicted cost as the battery ages. It
does this by using a simpliﬁed lithium-ion battery lifetime model. The fundamental battery
life model shown below takes temperature, battery charge level, and daily power loss
into account. This model’s correctness was demonstrated by comparing it to a precise
model developed at the National Renewable Energy Laboratory. Comparing this model to
experimental results proved its validity. The charger controller can use iterative numerical
charge cost minimization because the basic model runs quickly. Electric car charging
proﬁles strike a compromise between four trends: quick charging, charging at the end
of the permitted charge period, charging at low-cost intervals, and prohibiting vehicles
from sending electricity back to the grid. Simulations using data from actual Prius plug-in
Sustainability 2023, 15, 2603
12 of 26
hybrid EV show that fully charged batteries may be used for a lot longer than batteries that
have only been partially charged. This suggests that the requisite vehicle lifespan could be
met with smaller batteries. These patterns have been shown to hold true across a range of
battery sizes. They therefore apply to both plug-in hybrid and pure electric vehicles.
2.2. Types of Contactless EVS
Magnetic coupling coefﬁcients measure the effectiveness of the connection between
the secondary and primary coils. Coupling coefﬁcients with high values are necessary to
move a lot of power. The static wireless fast charger, which uses more than 20 kW of power,
is standardized. The OLEV and DWC initiatives’ main goals were to make the technology
more efﬁcient and commercially viable. The OLEV project raised the air gap to 20 cm and
retained an efﬁciency of 83% while achieving the needed high frequency of 20 kHz for
60 kW.
Additionally, the horizontal sensitivity was around 24 cm. The ﬁfth-generation diesel
water heater (DWC) OLEV got its fuel supply from an S-type power station rail. Despite
only having a 20 cm air gap tolerance and a side-to-side deviation tolerance of 30 cm, it
was nevertheless able to transmit 22 kW of electricity.
A dynamic WPT can move the 820 kW MPT for the high-speed train being built in
Korea through the 5 cm air gap, with an efﬁciency of 83%. The roughly 16-km-long Bus
Route 16 in Malaga, Spain has been overseen by a WPT since December 2014.
One of the most popular methods of frequency control frequency droop control is
currently being successfully applied to electric cars. The major technique for controlling
the frequency now is to regulate the output of generators connected to the main power
grid. As conventional power plants are phased out, electric vehicles are a great alternative
since their batteries may be charged or discharged in response to frequency deviation
alarms. As a result, we investigate frequency regulation in a power grid model with loads,
traditional producers, and a sizable amount of EVs. By providing grid-related services,
these last devices, in particular the FDC, autonomously optimize the grid. Two new control
algorithms that may be used to manage electric car batteries in the best possible way during
frequency regulation service. The control processes, on the one hand, make sure that
the power balance and frequency management of the main grid are stable. On the other
hand, these approaches can meet a range of EV charging needs. The available methods
are designed to reduce the failure rates of battery-powered devices. This contrasts with
the EV literature, which frequently concentrates on determining the ideal charge level.
The performance of the solutions is then contrasted with that of other state-of-the-art
V2G control systems. The results of numerical experiments carried out using an accurate
representation of the power grid show that the suggested methods perform well in actual
operational circumstances [44].
Researchers are working to create integrated infrastructure solutions that calculate the
amount of power transmitted through the system using a U-shaped power station rail and
a frequency of 35 kHz. The near ﬁeld, where there is no radiation, and the far ﬁeld, where
there is radiation, are two separate areas in the electromagnetic ﬁelds created by the antenna
of a moving electric charge. The area surrounding the transmitter (T x) where the energy
level is constant is referred to as the “close ﬁeld.” If there is no receiver to receive the energy,
T x cannot transmit it. The size and shape of the transmitter and receiver have an impact
on the near-ﬁeld ranges. Magnetic and electric ﬁelds exist separately in non-radiating areas.
Electrodes and coils can transmit power through electric and magnetic ﬁelds. The force
decreases at a rate of 1/r3 as the distance r between the transmitter and receiver grows,
but the energy does not change. The rapid power loss rate results in the WPT electric
ﬁeld’s shorter range. The WPT magnetic ﬁeld can transmit power over greater distances
than alternative methods because magnetic ﬁelds can easily pass through solid objects like
furniture, people, and walls. An electric vehicle’s air gap and efﬁciency must be increased
to improve WCS. The system’s size and surface area can be reduced by altering its operating
Sustainability 2023, 15, 2603
13 of 26
frequency. The WPT’s efﬁciency at a speciﬁc power level rises as its frequency does as well.
Businesses, academic institutions, and research institutions come in a wide variety.
We now have some responses after looking into wireless charging. In reality, the WPT
system rises by about 1 MHz before reaching a high frequency. One option is a system that
runs between 100 and 200 kHz. The ﬁnal expression is T0 = M/R0. The mutual inductance
(M) between the receiver and transmitter (T-to-T) and equivalent resistance make up T0
(R-to-zero). To get the mouse a high TQ. The ideal conditions for maximizing TQ are high
driving frequency, high mutual inductance, and low equivalent resistance. The inductance
L and capacitance C are needed to determine the resonant frequency, which equals 1/LC.
The resonant frequency will drop if L or C is increased. When the frequency is raised to such
a high level, the conversion encounters the switching problem. The WPT system is limited
in its ability to operate at high frequencies by a low coupling coefﬁcient. Because it is more
practical, the frequency is ﬁxed. When frequency standardization is used, the frequencies
are chosen to ensure that the system performs at its best. To promote wireless electric
car charging systems, additional study is required on efﬁciency requirements, operation
frequency, power level, electromagnetic interference (EMI), safety, and testing technology
(WEVCSs).
A novel distributed control approach to charge a ﬂeet of electric vehicles (EVs) as
effectively as feasible when the overall capacity of the power grid is limited. By resolving a
scheduling issue, the ideal charging can be discovered. Obtaining a cost-effective proﬁle of
the entire quantity of energy utilized is the objective. In the resulting optimization problem,
the decision variables are connected to both the objective function and the constraint
variables. In our approach, people just communicate with their near neighbors and take
decisions without seeking advice from those in positions of authority. To ﬁnd a solution, a
distributed iterative algorithm founded on the ideas of duality, closeness, and consensus is
employed. The global optimum can be reached using this strategy, as demonstrated by a
simulated case study.
The water ﬁlling technique can be used in networked control systems with node-
to-node communication that is independent. Water ﬁlling is a well-known method of
communication system optimization. It has assisted in solving real-world control engi-
neering and decision-making problems. In a system with multiple control points, the
decentralized approach of ﬁlling water tanks may be used. A study of interconnected water
levels among users who do not involve a central authority structure and simply use them
to communicate with their immediate neighbors was conducted. In this study, we develop
a unique distributed algorithm that combines ﬁxed point mapping theory, proximity theory,
and consensus theory. The algorithms’ exact and approximation implementations provide
the same outcome. The charging of a ﬂeet of electric vehicles is used to illustrate how the
system functions [45].
The difﬁculty of charging a large number of electric vehicles with infrastructure that
has a limited capacity led to the need for this study, which examines the issue of individual
load adjustment under overall capacity limits. Distributed solutions to this challenge are
being looked into for scalability and communication ease. We discuss a number of dis-
tributed algorithms for maximizing utilization and relative average fairness using concepts
from communication networks (AIMD algorithms) and distributed convex optimization.
We give analytical and simulation ﬁndings to demonstrate the effectiveness of these algo-
rithms. The algorithm’s performance in the analyzed circumstances is often within 5% of
the ideal centralized case’s performance, but with substantially better scalability and fewer
communication requirements.
2.3. Machine Learning Techniques
The many uses of ML algorithms have been widely covered in writing. The authors
in [4] investigated how SVM could be used to solve problems with more than two classes.
The algorithm was developed so that it could be used to solve problems involving multiple
categories by using various normalization techniques. The algorithm passed thorough
Sustainability 2023, 15, 2603
14 of 26
testing and discovered that it was effective with a wide range of normalization techniques.
Its multiclass classiﬁcation accuracy was also quite impressive.
DNN was also examined by [46] for classifying multi-type images. As a result, the
proposed algorithm has a wide range of applications, even for famously challenging
to-label classes that are frequently misclassiﬁed by other ML techniques using a DNN-
improved class identiﬁcation. The capacity of the algorithm to forecast travel times was
also investigated by [47]. Its regressive predictions were the most accurate and reliable of
all the tested algorithms. One of the best regression algorithms, especially for time series,
is LSTM. Additionally, LSTM helps classify issues because it can be used to create classes
based on number intervals. Authors in [48] looked into using radio waves to estimate the
required energy and power (RF). RF was easy to use and produced accurate results. The
energy consumption model had a mean absolute percentage error of only 16%, making it
more accurate than the autoregressive model. Creating a multiclass classiﬁcation problem
was necessary for managing EV ﬂeets. As a result, many ML techniques previously shown
to be the most successful for classiﬁcation problems were used. We assessed each model’s
accuracy before contrasting and comparing its performance to see if there were any viable
options for increasing the effectiveness of the distribution network. The ML algorithm
summary is presented in Table 1.
Table 1. ML algorithm summary.
Algorithm
Advantages
Disadvantages
Decision Tree (DT)
Data does not need to be sized or
normalized. Regression and classiﬁcation
analyses beneﬁt from it—highly accurate
predictions and understanding.
Training takes a while since even little
changes to the dataset could greatly impact
the ﬁnal structure.
Random Forest (RF)
Capable of processing large datasets with
various factors and handling uncertainty
while putting out the mean or mode of
several decision trees.
The problem is made harder by how many
trees are created. Training typically
requires a sizable amount of time.
Support Vector Machine (SVM)
Classiﬁcation and regression are possible
uses; the classiﬁer’s performance
is minimized.
The training procedure takes longer with
large datasets—poor performance when
there are more features than
training samples.
K-Nearest Neighbors (KNN)
Both classiﬁcation and regression analysis
can beneﬁt from its use. Shorter training
sessions. Clear use of ideas.
We are having poor performance when
dealing with huge datasets. Poor
performance when dealing with a large
number of inputs. Poor performance when
dealing with datasets that are not balanced
Deep Neural Network (DNN)
A model that may be utilized for various
tasks, like classiﬁcation and regression.
Need enormous data sets very prone to
over ﬁtting. The ideal width and depth
cannot be determined using a
general principle.
Long Short-Term Memory (LSTM)
Time series detection, accurate forecasting
and adaptability to various applications
(classiﬁcation and regression).
There is no established methodology for
determining the optimum breadth and
depth. There is no theory for selecting
optimal hyper parameters.
3. Methodology
3.1. Machine Learning
Machine learning studies focus on building automatons that can learn new skills
through observation and exposure to new data. This area of research aims to create al-
gorithms that allow machines to remember and operate independently without human
intervention. Data is provided to a generic algorithm in machine learning, which builds
its logic based on the data rather than being preprogrammed. Supervised learning, unsu-
pervised learning, and reinforcement learning are a few machine learning techniques [49]
Computers are used to make predictions in computational statistics. It is closely related
Sustainability 2023, 15, 2603
15 of 26
to machine learning and frequently crosses over with it. Mathematical optimization is
known to study how to make optimization processes, theories, and applications better.
Mathematical optimization is related to machine learning, as well. Unsupervised learning,
also known as exploratory data analysis, is frequently highlighted when combining ma-
chine learning and data mining [50]. Unsupervised machine learning is a powerful tool for
understanding the typical behavior of many organisms and observing constant movement
from these patterns. Predictive analytics, or machine learning, is a method for developing
complicated algorithms and models that can be used to forecast outcomes based on data
analysis. Even then, AI falls far short of machine learning’s capabilities. When artiﬁcial
intelligence (AI) was ﬁrst developed, some scientists programmed computers to learn from
their mistakes. They looked for a solution using symbolic techniques like neural networks.
But as rational and knowledge-based approaches gain more attention, there is a clear dis-
tinction between AI and machine learning. Data collection and presentation in probability
systems had theoretical and practical problems. Statistics lost their value after 1980 when
cyber systems had already exceeded artiﬁcial intelligence. However, the statistical focus of
other research was seen outside of AI in pattern recognition and information retrieval. In
contrast, work on knowledge-based learning continued inside AI and resulted in inductive
logic programming. Academics in AI and CS started to ignore neural network research
simultaneously. Under the umbrella of “connectionism”, researchers outside AI/CS, like
Hopﬁeld [51], made comparable attempts. They had great success with backpropagation,
which they used in the middle of the 1980s [52]. Data mining and machine learning are
very similar and use many methods. Machine learning entails making predictions based
on previously discovered properties from training data. On the other hand, data mining
entails identifying novel properties (the step of analyzing knowledge extraction in the
database). Machine learning frequently uses data mining techniques for “unsupervised
learning” or as a step before learning to increase accuracy. Except for the ECML PKDD,
most of the distinctions between the two ﬁelds, which frequently have separate conferences
and journals, are founded on those assumptions. Discovering new information is typically
how knowledge extraction and data mining (KDD) is judged effective. The effectiveness
of machine learning is generally assessed by how well it can replicate existing data. Due
to a lack of training data, supervised methods cannot be used in a typical KDD task. Still,
an unsupervised manner, also known as an “uninformed method”, can relatively easily
outperform other monitored methods. Machine learning techniques that divide data into
training and test sets, such as the holdout method, enable extremely precise estimation
of classiﬁcation models (usually two-thirds of the data in the training set and one-third).
It also assesses how successfully the model was trained using real test data. The N-fold
cross-validation method, in contrast, randomly selects k subsets from the data, of which
k1 is used to train the model, and k1 is used to assess the model’s predictive capabil-
ity. The bootstrap technique, which uses a copy-and-paste method to choose n random
samples from the data set, can be used in addition to the holdout and cross-validation
techniques [52] to evaluate the model accurately
3.2. Learning Algorithms
The Q-learning algorithm was used by [53] in 2012 to improve the power management
system of electric and hybrid bicycles. In terms of power management, these researchers
aimed to increase rider comfort and safety and utilize battery power more effectively. Sim-
ulated results from this study showed that the proposed power management system could
increase riding comfort by 24% and energy efﬁciency by 50%. Since then, reinforcement
learning algorithms have been used in several studies to replace control optimization theory
for HESS energy management. To ﬁnd the best way to maintain an HEV’s battery at the
ideal charge level. The author in [53] used the Q-learning algorithm. By combining this
strategy with a long-term plan, you can strike a balance between maximum effectiveness
and the capacity to act now. Atheros in [54] employs “reverse reinforcement learning” to
develop a probabilistic driving path prediction system that determines the ideal engine-to-
Sustainability 2023, 15, 2603
16 of 26
battery power ratio based on the expected behavior of the driver. Several papers on the
application of reinforcement learning to control the power ﬂow in hybrid transmission
systems have recently been published [54]. First, the adjustment, effectiveness, and learning
capacity of a Q-learning-based energy management strategy for a hybrid tracked vehicle
was assessed [55]. The researchers then developed online Q-learning-based recursive al-
gorithms to allow real-time updates to control methods for hybrid transmission systems.
When the driver’s actions, location, and conditions on the road all vary, these algorithms’
effectiveness may likely decline.
3.3. Proposed Methods
Driving cycle data could be used to ﬁnd ﬂexible energy management strategies using
advanced reinforcement learning. A comparison of rule-based energy management strate-
gies and those learned through DRL and online learning was done. The examples show
both transactional and ad hoc neural networks. Machine learning can be used to overcome
this issue in ﬁve different ways. A block diagram of an electric vehicle’s static wireless
charging system is shown in Figure 6. The receiver coil and transmitter coil are placed on
top of one another to show how well the EVs work. Over time, using this approach will
lessen pollution and conserve our ﬁnite supply of conventional energy. Figure 7 shows
proposed methodology.
Sustainability 2023, 15, x FOR PEER REVIEW 
17 of 27 
 
imum effectiveness and the capacity to act now. Atheros in [54] employs “reverse rein-
forcement learning” to develop a probabilistic driving path prediction system that deter-
mines the ideal engine-to-battery power ratio based on the expected behavior of the 
driver. Several papers on the application of reinforcement learning to control the power 
flow in hybrid transmission systems have recently been published [54]. First, the adjust-
ment, effectiveness, and learning capacity of a Q-learning-based energy management 
strategy for a hybrid tracked vehicle was assessed [55]. The researchers then developed 
online Q-learning-based recursive algorithms to allow real-time updates to control meth-
ods for hybrid transmission systems. When the driver’s actions, location, and conditions 
on the road all vary, these algorithms’ effectiveness may likely decline. 
3.3. Proposed Methods 
Driving cycle data could be used to find flexible energy management strategies using 
advanced reinforcement learning. A comparison of rule-based energy management strat-
egies and those learned through DRL and online learning was done. The examples show 
both transactional and ad hoc neural networks. Machine learning can be used to overcome 
this issue in five different ways. A block diagram of an electric vehicle’s static wireless 
charging system is shown in Figure 6. The receiver coil and transmitter coil are placed on 
top of one another to show how well the EVs work. Over time, using this approach will 
lessen pollution and conserve our finite supply of conventional energy. Figure 7 shows 
proposed methodology. 
 
Figure 6. Contactless charging types [56]. 
Figure 6. Contactless charging types [56].
Sustainability 2023, 15, 2603
17 of 26
Sustainability 2023, 15, x FOR PEER REVIEW 
18 of 27 
 
 
Figure 7. Proposed methodology. 
4. Data Acquisition 
We discovered a dataset about charging electric vehicles after searching for it on 
Kaggle. Details include the overall cost of the energy utilized, the date, and the length of 
each session. In this dataset, 3395 EV charging sessions are covered in great. A workplace 
charging initiative involved 85 distinct EV drivers who used 105 station sessions spread 
over 25 different sites. There are 24 resources and 3395 specifics on the car. The effective-
ness of various machine learning techniques is assessed and contrasted. The techniques 
used include deep neural networks, k-nearest neighbors, long short-term memory, ran-
dom forest, support vector machines, and decision trees. All machine learning methods 
were compared using the same dataset to see which produced the most accurate results. 
According to the results, it seemed that LSTM could help with EV control in some circum-
stances. The peak voltage, power losses, and voltage stability of the LSTM model can all 
be improved by flattening the load curve. We can lower our billing costs by predicting 
incoming data. The proposed Smart Grid Electric Vehicle Charging and Hybrid Energy 
Storage Management System’s dataset is described in Table 2. 
Table 2. Summary of data set (Electric Vehicle Charging Dataset Kaggle). 
1. 
Session ID 
13. 
station Id 
2. 
KWH Total 
14. 
location Id 
3. 
Dollars 
15. 
manager Vehicle 
4. 
Created 
16. 
facility Type 
Figure 7. Proposed methodology.
4. Data Acquisition
We discovered a dataset about charging electric vehicles after searching for it on
Kaggle. Details include the overall cost of the energy utilized, the date, and the length of
each session. In this dataset, 3395 EV charging sessions are covered in great. A workplace
charging initiative involved 85 distinct EV drivers who used 105 station sessions spread
over 25 different sites. There are 24 resources and 3395 speciﬁcs on the car. The effectiveness
of various machine learning techniques is assessed and contrasted. The techniques used
include deep neural networks, k-nearest neighbors, long short-term memory, random forest,
support vector machines, and decision trees. All machine learning methods were compared
using the same dataset to see which produced the most accurate results. According to the
results, it seemed that LSTM could help with EV control in some circumstances. The peak
voltage, power losses, and voltage stability of the LSTM model can all be improved by
ﬂattening the load curve. We can lower our billing costs by predicting incoming data. The
proposed Smart Grid Electric Vehicle Charging and Hybrid Energy Storage Management
System’s dataset is described in Table 2.
A decision was made regarding how much data should be shared between Train and
Test (80–20%) for each neural network after considering the ﬁndings for DNN, KNN, SVM,
RF, DT and LSTM. Tests and simulations are performed using Mat lab 2021a. It has an
8.00 GB RAM, a hard drive, and an 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40 GHz
high-performance PC CPU.
Sustainability 2023, 15, 2603
18 of 26
Table 2. Summary of data set (Electric Vehicle Charging Dataset Kaggle).
1.
Session ID
13.
station Id
2.
KWH Total
14.
location Id
3.
Dollars
15.
manager Vehicle
4.
Created
16.
facility Type
5.
Ended
17.
Mon
6.
Start Time
18.
tues
7.
End Time
19.
wed
8.
Charge Time Hrs
20.
Thurs
9.
Week Day
21.
Fri
10.
Platform
22.
sat
11.
Distance
23.
sun
12.
User ID
24.
reported zip
5. Results
5.1. Machine Learning Models:
5.1.1. Charging Station Classiﬁcation Results
According to the charging station classiﬁcation, the network has twelve different types
of charging stations. Table 3 shows how well other models direct EVs to the most effective
charging locations, maximizing the effectiveness of the distribution network and lowering
the cost of charging.
Table 3. Accuracies of ML classiﬁer for charging station classiﬁcation.
Machine Learning Model
Accuracy
Decision Tree
93%
Random Forest
94%
SVM
29%
KNN
41%
DNN
77%
LSTM
94%
SVM and KNN cannot be used for EV routing, as shown in Table 3 by their incredibly
low precision. The most accurate models, with a 94% accuracy rate, are RF and LSTM.
The 93% precision of DT is identical to that of RF and LSTM. DNN has a remarkable 77%
accuracy rate. Expanding the datasets and changing the hyper parameters, such as the
number of layers and nodes in each layer, can improve the system’s performance. These
results explain that RF and LSTM perform better at multiclass classiﬁcation problems than
algorithms like SVM and KNN.
Figure 8 shows the accuracies of ML classiﬁer for charging station classiﬁcation.
5.1.2. Classiﬁcation Based on Charging
Three categories of charging speeds have been created to reﬂect the range of uses for
charging stations (fast charging, conventional charging, and V2G). How well ML models
could see the ideal charging rate is shown in Table 4.
Sustainability 2023, 15, 2603
19 of 26
Sustainability 2023, 15, x FOR PEER REVIEW 
20 of 27 
 
 
Figure 8. Accuracies of ML classifier for charging station classification. 
5.1.2. Classification Based on Charging 
Three categories of charging speeds have been created to reflect the range of uses for 
charging stations (fast charging, conventional charging, and V2G). How well ML models 
could see the ideal charging rate is shown in Table 4. 
Table 4 demonstrates that only SVM has a low level of precision. DT, KNN, and DNN 
have accuracy rates of about 83%, 83%, and 84%, respectively. The most accurate results 
are produced by RF and LSTM models. LSTM is 4% more precise in determining the ideal 
charging rates than RF. The best ML model for classifying charging stations and vehicle 
speeds is LSTM. Because it addresses both classification issues, it is the best strategy for 
managing an EV fleet. Its ability to recognize temporal patterns in data about power con-
sumption is one of the reasons it is so accurate. 
Table 4. Accuracies of ML classifier for charging speed classification. 
Machine Learning Model 
Accuracy 
Decision Tree 
83% 
Random Forest  
89% 
SVM 
56% 
KNN 
83% 
DNN 
84% 
LSTM 
93% 
Figure 9 shows the accuracies of ML classifier for charging speed classification. 
Figure 8. Accuracies of ML classiﬁer for charging station classiﬁcation.
Table 4. Accuracies of ML classiﬁer for charging speed classiﬁcation.
Machine Learning Model
Accuracy
Decision Tree
83%
Random Forest
89%
SVM
56%
KNN
83%
DNN
84%
LSTM
93%
Table 4 demonstrates that only SVM has a low level of precision. DT, KNN, and
DNN have accuracy rates of about 83%, 83%, and 84%, respectively. The most accurate
results are produced by RF and LSTM models. LSTM is 4% more precise in determining
the ideal charging rates than RF. The best ML model for classifying charging stations and
vehicle speeds is LSTM. Because it addresses both classiﬁcation issues, it is the best strategy
for managing an EV ﬂeet. Its ability to recognize temporal patterns in data about power
consumption is one of the reasons it is so accurate.
Figure 9 shows the accuracies of ML classiﬁer for charging speed classiﬁcation.
Sustainability 2023, 15, x FOR PEER REVIEW 
21 of 27 
 
 
Figure 9. Accuracies of ML classifier for charging speed classification. 
5.2. The Impact of Uncertain Load Data on the EV Management System 
The load data is augmented with various concentrations of Gaussian white noise 
(GWN) to simulate the data’s unpredictability and add different degrees of uncertainty. 
We evaluate previously successful ML methods to see how well they can handle new 
types of luck. Thus, the impact of tension on the system is researched, especially how it 
affects power losses and load curves. 
Figure 9. Accuracies of ML classiﬁer for charging speed classiﬁcation.
Sustainability 2023, 15, 2603
20 of 26
5.2. The Impact of Uncertain Load Data on the EV Management System
The load data is augmented with various concentrations of Gaussian white noise
(GWN) to simulate the data’s unpredictability and add different degrees of uncertainty. We
evaluate previously successful ML methods to see how well they can handle new types
of luck. Thus, the impact of tension on the system is researched, especially how it affects
power losses and load curves.
Variation in Load Data and Its Impact on Machine Learning Accuracy
The results of including uncertainty to the load data used to group charging stations
are shown in Table 5. By contrasting precision before and after adding delay, we can
identify the change in precision. When there is uncertainty, DT’s accuracy drops to 77%.
Although still not as good as DT’s, RF’s accuracy drops to 86%. The best model, LSTM,
remained accurate 95% of the time and was unaffected by uncertainty.
Table 5. Classiﬁcation accuracy of a machine learning classiﬁer using a 10% global weighted network
for Electric Vehicles charging stations.
Machine Learning Model
Accuracy
Accuracy Change
DT
76%
−17%
RF
85%
−9%
LSTM
94%
0
The impact of adding uncertainty to load data on the effectiveness of the machine
learning algorithms used to categorize charging speeds is shown in Table 3. In RF and
DNN, accuracy has fallen by 1% and 2%, respectively. The accuracy of DT, KNN, and LSTM
remained unchanged when GWN was added. The outcomes show that LSTM is adaptable
even in the presence of GWN in the load data and performs excellently both before and
after the introduction of uncertainty. Machine Learning Classiﬁcation of Pricing Structures:
Accuracy of Prediction: 10% Standardization of Weights around the World is presented in
Table 6.
Table 6. Machine learning classiﬁcation of pricing structures: Accuracy of prediction: 10% standard-
ization of weights around the world.
Machine Learning Model
Accuracy
Accuracy Change
DT
83%
0%
RF
88%
−1%
KNN
84%
0
DNN
82%
−2%
LSTM
93%
0%
ML implementation Results for EV-HESMS is presented in Table 6. In the proposed
EV-HESMS model, 80% for training and 20% data for testing taken to ﬁnd the Gradient
Loss, Action Error and MSE.
Table 7 shows how the EV-HESMS Model behaves in terms of gradient Loss, MSE,
training, and testing. Efﬁciency by itself is insufﬁcient. The system’s loss and error for each
machine learning technique using Gradient loss and Event error must also be determined.
The dataset and the capabilities of the system can be considered to a great extent as the
gradient loss, action error, and MSE are taken into account. Additionally, LSTM is already
thought to be more effective than other methods. In reality, LSTM also considerably reduces
errors in this situation.
Sustainability 2023, 15, 2603
21 of 26
Table 7. ML implementation Results for Proposed EV-HESMS Model.
Methods
Train (80%)
Test (20%)
Gradient Loss
Action Error
MSE
1. Random Forest
2716
679
0.366
0.304
0.342
2. SVM
2716
679
0.359
0.449
0.303
3. KNN
2716
679
0.426
0.402
0.333
4. DNN
2716
679
0.390
0.329
0.475
5. LSTM
2716
679
0.386
0.310
0.258
5.3. Mathematically Model to Different Calculated Parameters:
Controlling parameters can be changed in real-time in a system with real-time con-
trol [34]. The method’s practicality was shown by simulated experiments on a hardware
operating system based on rechargeable batteries and super capacitors. bmin is added to the
estimate to get the minimum battery load (w). The following defines the cost function: The
edge is deleted as unneeded if the sum of the values bmin(v) = bmin(w) + cost(e) is more than M.
C+
e (bw) =
Cos(e)bw ≤ M − Cost(e)
∞Otherwise
(1)
If the original edge, e = (v, m), has negative costs, we subtract the value of the price
from bmin (w). We obtain bmin(v). The cost function is given by:
C−
e (bw) =
−bwbw < −Cost(e)
Cost(e)Otherwise
(2)
MAPE =
q
∑N
i=1
(|xi−yi|)1
xi
N
× 100%
(3)
RMSE =
v
u
u
u
u
t
N
∑
i=1
(xi − yi)2
N
(4)
r =
∑N
i=1(Xi − X′)(Yi − Y′)1
q
∑N
i=1(Xi − X′)1 q
∑N
i=1(Yi − Y′)2
(5)
Numerous solutions must be developed to deal with problems like excesses and
overload when electric vehicles are connected to a smart grid. This does not happen until
the most economical energy has been acquired. Using the deﬁned criteria, electric vehicles
can compete for reduced electricity rates in a given area. They are using price signals to
discourage the charging of vehicles in crowded places and make money through electric
car sharing. EV software uses a variety of machine-learning techniques. Some of the most
popular methods for analyzing data and identifying its relevance include decision trees,
ANNs, SVMs, GRNNs, and k-nearest neighbors (KNN). Usually, MAPE, r, and RMSE
are used to evaluate a forecast’s degree of accuracy. R is a common abbreviation for the
correlation coefﬁcient. Measures of how far off an estimate is from reality include the
root-mean-square error and mean absolute percentage error (MAPE). You can ﬁnd the
tools RMSE, r, and MAPE in EQU. In the ﬁrst case, the true value is represented by Xi,
while YI represents the close approximation. There are intriguing differences between the
means of the true value vector’s X and the projected value vector’s Y. The letter N stands
for the whole cast set of values. Figure 10 show the classiﬁcation accuracy of a machine
learning classiﬁer.
Sustainability 2023, 15, 2603
22 of 26
mean-square error and mean absolute percentage error (MAPE). You can find the tools 
RMSE, r, and MAPE in EQU. In the first case, the true value is represented by Xi, while YI 
represents the close approximation. There are intriguing differences between the means 
of the true value vector’s X and the projected value vector’s Y. The letter N stands for the 
whole cast set of values. Figure 10 show the classification accuracy of a machine learning 
classifier.  
 
Figure 10. Classification accuracy of a machine learning classifier. 
Figure 11 shows the accuracy of prediction: 10% standardization of weights around 
the world. 
 
Figure 11. Accuracy of prediction: 10% standardization of weights around the world. 
Figure 10. Classiﬁcation accuracy of a machine learning classiﬁer.
Figure 11 shows the accuracy of prediction: 10% standardization of weights around
the world.
relation coefficient. Measures of how far off an estimate is from reality include the root-
mean-square error and mean absolute percentage error (MAPE). You can find the tools 
RMSE, r, and MAPE in EQU. In the first case, the true value is represented by Xi, while YI 
represents the close approximation. There are intriguing differences between the means 
of the true value vector’s X and the projected value vector’s Y. The letter N stands for the 
whole cast set of values. Figure 10 show the classification accuracy of a machine learning 
classifier.  
 
Figure 10. Classification accuracy of a machine learning classifier. 
Figure 11 shows the accuracy of prediction: 10% standardization of weights around 
the world. 
 
Figure 11. Accuracy of prediction: 10% standardization of weights around the world. 
Figure 11. Accuracy of prediction: 10% standardization of weights around the world.
Figure 12 shows the ML implementation results for proposed EV-HESMS model.
5.4. Limitations of the Proposed System
(1)
More advanced deep learning, reinforcement learning, and federated learning tech-
niques can be used to centralize this system, enabling more effective vehicle utilization
regulation. Obviously, if there are more providers vying for fewer stations, the prob-
lem will improve.
(2)
The dataset should contain more parameters. Electric vehicle (EV) sales have been
increasing, and this trend is likely to continue as prices fall and ranges increase.
Currently, the percentage of electric cars (EVs) in use is quite modest. Any company
that has a parking lot for clients or staff will be impacted by this. Therefore, it is
crucial for businesses to consider how their energy policy will change as a result of
planned changes to EVs and EV charging. Particularly for businesses, having multiple
chargers or fast-charging equipment on hand is essential.
Sustainability 2023, 15, 2603
23 of 26
(3)
As automakers work to reduce the amount of time needed to charge an EV so that
the “refueling” process is more equivalent to that of a normal vehicle, chargers need
more power.
Sustainability 2023, 15, x FOR PEER REVIEW 
24 of 27 
 
Figure 12 shows the ML implementation results for proposed EV-HESMS model. 
 
Figure 12. ML implementation results for proposed EV-HESMS model. 
5.4. Limitations of the Proposed System 
(1) More advanced deep learning, reinforcement learning, and federated learning tech-
niques can be used to centralize this system, enabling more effective vehicle utiliza-
tion regulation. Obviously, if there are more providers vying for fewer stations, the 
problem will improve. 
(2) The dataset should contain more parameters. Electric vehicle (EV) sales have been 
increasing, and this trend is likely to continue as prices fall and ranges increase. Cur-
rently, the percentage of electric cars (EVs) in use is quite modest. Any company that 
has a parking lot for clients or staff will be impacted by this. Therefore, it is crucial 
for businesses to consider how their energy policy will change as a result of planned 
changes to EVs and EV charging. Particularly for businesses, having multiple 
chargers or fast-charging equipment on hand is essential. 
(3) As automakers work to reduce the amount of time needed to charge an EV so that 
the “refueling” process is more equivalent to that of a normal vehicle, chargers need 
more power. 
6. Conclusions 
Information and communication technology development is crucial to the world-
wide spread of smart cities (ICT). Every developing city needs a smart grid. Government 
and corporate organizations are promoting using electric vehicles (EVs) to reduce green-
house gas emissions and combat climate change. Electric automobiles have sparked sev-
eral previously unforeseen problems due to their greater presence in contemporary, so-
phisticated power networks. Two significant issues are implementing cost-saving tech-
nology for controlling energy supply and demand and creating more efficient techniques 
for invoicing clients. This issue has received various possible solutions that have been put 
forward. This entails a thorough investigation of charging procedures, industry stand-
ards, and different data-driven models and machine-learning techniques to facilitate the 
seamless integration of electric vehicles into the smart grid. We examine the most recent 
developments in smart grid-based energy management services and applications and the 
growing appeal of electric vehicles. This indicates that individuals with a say in infrastruc-
Figure 12. ML implementation results for proposed EV-HESMS model.
6. Conclusions
Information and communication technology development is crucial to the worldwide
spread of smart cities (ICT). Every developing city needs a smart grid. Government and
corporate organizations are promoting using electric vehicles (EVs) to reduce greenhouse
gas emissions and combat climate change. Electric automobiles have sparked several
previously unforeseen problems due to their greater presence in contemporary, sophisti-
cated power networks. Two signiﬁcant issues are implementing cost-saving technology for
controlling energy supply and demand and creating more efﬁcient techniques for invoicing
clients. This issue has received various possible solutions that have been put forward. This
entails a thorough investigation of charging procedures, industry standards, and different
data-driven models and machine-learning techniques to facilitate the seamless integration
of electric vehicles into the smart grid. We examine the most recent developments in
smart grid-based energy management services and applications and the growing appeal of
electric vehicles. This indicates that individuals with a say in infrastructure development
must consider the health of communities, public safety, access to electricity and information,
the provision of services, and other issues [11]. This kind of smart city and technology
development will be effective when all stakeholders and important factors are considered.
Hurdles and opportunities have emerged as technology have advanced toward long-term
solutions. We talk about conductive and inductive charging for electric cars. The most
important studies that have been done on electric vehicles, connector hybrid electric vehicle
types, charging rates, and battery capacity, and these topics are reviewed. This paper
analyzes current advancements in ﬁxed and portable wireless charging technology. Al-
though there are international standards for wirelessly charging electric vehicles, different
wireless charging systems employ a range of frequencies. The efﬁcacy of contemporary
machine learning algorithms and robotic models is being assessed to integrate the smart
grid. We examine the techniques presently used to calculate the driving range, charging
time, and trafﬁc impact of electric vehicles. It is necessary to study the effects of the public
infrastructure’s energy efﬁciency, robustness, and dependability on the economy, society,
and environment to make widespread usage of electric vehicles feasible. Discussions and
analyses will focus on wireless power transmission systems in the future. These systems,
Sustainability 2023, 15, 2603
24 of 26
which include those that carry energy inside cars and those that do so between autos and
the grid, will be of great importance. Research on mobile energy storage and delivery
infrastructures based on electric vehicles may help us better understand how to incorporate
renewable energy sources into distributed micro grids.
Author Contributions: Conceptualization, T.M.; methodology, T.M., R.N.A.; software, T.M., R.N.A.,
M.A.M. and S.A.; validation, T.M., M.I. and M.K.; formal analysis, S.A.; investigation, T.M and I.H.;
resources, T.M. and I.H.; data curation, T.M., M.A.N.; writing—original draft preparation, T.M.;
writing—review and editing, T.M., M.A.M. and M.A.N.; visualization, T.M. All authors have read
and agreed to the published version of the manuscript.
Funding: This research received no external funding.
Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Data Availability Statement: Not applicable.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
References
1.
IEA, C. Global EV Outlook 2020. 2020. Available online: https://www.iea.org/reports/global-ev-outlook-2020 (accessed on
1 February 2022).
2.
Dhakal, T.; Kangwon National University; Min, K.-S. Macro Study of Global Electric Vehicle Expansion. Foresight STI Gov. 2021,
15, 67–73. [CrossRef]
3.
Elmenshawy, M.; Massoud, A. Modular Isolated DC-DC Converters for Ultra-Fast EV Chargers: A Generalized Modeling and
Control Approach. Energies 2020, 13, 2540. [CrossRef]
4.
Shibl, M.; Ismail, L.; Massoud, A. Electric Vehicles Charging Management Using Machine Learning Considering Fast Charging
and Vehicle-to-Grid Operation. Energies 2021, 14, 6199. [CrossRef]
5.
Lytras, M.D.; Visvizi, A.; Jussila, J. Social media mining for smart cities and smart villages research. Soft Comput. 2020, 24,
10983–10987. [CrossRef]
6.
Morvaj, B.; Lugaric, L.; Krajcar, S. Demonstrating smart buildings and smart grid features in a smart energy city. In Proceedings
of the 2011 3rd International Youth Conference on Energetics (IYCE), Leiria, Portugal, 7–9 July 2011.
7.
Qaisar, S.M.; Alyamani, N. A Review of Charging Schemes and Machine Learning Techniques for Intelligent Management of
Electric Vehicles in Smart Grid. Manag. Smart Cities 2022, 51–71. [CrossRef]
8.
Axelsson, K.; Granath, M. Stakeholders’ stake and relation to smartness in smart city development: Insights from a Swedish city
planning project. Gov. Inf. Q. 2018, 35, 693–702. [CrossRef]
9.
Lytras, M.D.; Chui, K.T. The Recent Development of Artiﬁcial Intelligence for Smart and Sustainable Energy Systems and
Applications. Energies 2019, 12, 3108. [CrossRef]
10.
Liu, Z.; Wu, Q.; Huang, S.; Wang, L.; Shahidehpour, M.; Xue, Y. Optimal Day-Ahead Charging Scheduling of Electric Vehicles
Through an Aggregative Game Model. IEEE Trans. Smart Grid 2017, 9, 5173–5184. [CrossRef]
11.
Wang, T.; Liang, Y.; Jia, W.; Arif, M.; Liu, A.; Xie, M. Coupling resource management based on fog computing in smart city
systems. J. Netw. Comput. Appl. 2019, 135, 11–19. [CrossRef]
12.
Dericio˘glu, C.; Yirik, E.; Ünal, E.; Cuma, M.U.; Onur, B.; Tümay, M. A Review of Charging Technologies for Commercial Electric
Vehicles. Int. J. Adv. Automot. Technol. 2018, 2, 61–70. [CrossRef]
13.
Wang, S.; Wan, J.; Li, D.; Zhang, C. Implementing smart factory of industrie 4. 0: An outlook. Int. J. Distrib. Sens. Netw. 2016,
12, 3159805.
14.
Danish, M.S.S.; Bhattacharya, A.; Stepanova, D.; Mikhaylov, A.; Grilli, M.L.; Khosravy, M.; Senjyu, T. A Systematic Review of
Metal Oxide Applications for Energy and Environmental Sustainability. Metals 2020, 10, 1604. [CrossRef]
15.
Ehsani, M.; Gao, Y.; Gay, S.E.; Emadi, A. Modern Electric, Hybrid Electric, and Fuel Cell Vehicles; CRC Press: Boca Raton, FL, USA, 2018.
16.
Cattaruzza, D.; Absi, N.; Feillet, D.; González-Feliu, J. Vehicle routing problems for city logistics. EURO J. Transp. Logist. 2017, 6,
51–79. [CrossRef]
17.
Xydas, E.S.; Marmaras, C.E.; Cipcigan, L.M.; Hassan, A.S.; Jenkins, N. Forecasting electric vehicle charging demand using support
vector machines. In Proceedings of the 2013 48th International Universities’ Power Engineering Conference (UPEC), Dublin,
Ireland, 2–5 September 2013.
18.
Sadeghian, O.; Oshnoei, A.; Mohammadi-Ivatloo, B.; Vahidinasab, V.; Anvari-Moghaddam, A. A comprehensive review on
electric vehicles smart charging: Solutions, strategies, technologies, and challenges. J. Energy Storage 2022, 54, 105241. [CrossRef]
19.
Vu, Q.V.; Dinh, A.H.; Van Thien, N.; Tran, H.T.; Le Xuan, H.; Van Hung, P.; Kim, D.T.; Nguyen, L. An Adaptive Hierarchical
Sliding Mode Controller for Autonomous Underwater Vehicles. Electronics 2021, 10, 2316. [CrossRef]
Sustainability 2023, 15, 2603
25 of 26
20.
Huang, X.; Shi, J.; Gao, B.; Tai, Y.; Chen, Z.; Zhang, J. Forecasting Hourly Solar Irradiance Using Hybrid Wavelet Transformation
and Elman Model in Smart Grid. IEEE Access 2019, 7, 139909–139923. [CrossRef]
21.
Quesada, J.A.; Lopez-Pineda, A.; Gil-Guillén, V.F.; Durazo-Arvizu, R.; Orozco-Beltrán, D.; López-Domenech, A.; Carratalá-
Munuera, C. Machine learning to predict cardiovascular risk. Int. J. Clin. Pr. 2019, 73, e13389. [CrossRef]
22.
Zhai, Z.; Su, S.; Liu, R.; Yang, C.; Liu, C. Agent–cellular automata model for the dynamic ﬂuctuation of EV trafﬁc and charging
demands based on machine learning algorithm. Neural Comput. Appl. 2018, 31, 4639–4652. [CrossRef]
23.
Rigas, E.S.; Ramchurn, S.D.; Bassiliades, N. Managing Electric Vehicles in the Smart Grid Using Artiﬁcial Intelligence: A Survey.
IEEE Trans. Intell. Transp. Syst. 2014, 16, 1619–1635. [CrossRef]
24.
Sangdehi, S.M.M.; Hamidifar, S.; Kar, N.C. A Novel Bidirectional DC/AC Stacked Matrix Converter Design for Electriﬁed Vehicle
Applications. IEEE Trans. Veh. Technol. 2014, 63, 3038–3050. [CrossRef]
25.
Sultana, B.; Mustafa, M. Impact of reconﬁguration and demand response program considering electrical vehicles in smart
distribution network. In Proceedings of the 3rd International Electrical Engineering Conference (IEEC 2018), Karachi, Pakistan,
9–10 February 2018.
26.
Ramirez-Vazquez, R.; Gonzalez-Rubio, J.; Escobar, I.; Suarez Rodriguez, C.D.P.; Arribas, E. Personal exposure assessment to Wi-Fi
radiofrequency electromagnetic ﬁelds in Mexican microenvironments. Int. J. Environ. Res. Public Health 2021, 18, 1857. [CrossRef]
[PubMed]
27.
Lund, P.D.; Byrne, J.; Haas, R.; Flynn, D. (Eds.) The role of electric vehicles in smart grids. In Advances in Energy Systems: The
Large-scale Renewable Energy Integration Challenge; John Wiley & Sons: Hoboken, NJ, USA, 2019; pp. 245–264.
28.
Amer, A.; Shaban, K.; Gaouda, A.; Massoud, A. Home Energy Management System Embedded with a Multi-Objective Demand
Response Optimization Model to Beneﬁt Customers and Operators. Energies 2021, 14, 257. [CrossRef]
29.
Ashraf, S. Culminate Coverage for Sensor Network Through Bodacious-instance Mechanism. In I-Manager’s Journal on Wireless
Communication Networks; I-Manager Publications: Nagercoil, India, 2019; Volume 8, p. 1. [CrossRef]
30.
Jian, L.; Xue, H.; Xu, G.; Zhu, X.; Zhao, D.; Shao, Z.Y. Regulated Charging of Plug-in Hybrid Electric Vehicles for Minimizing
Load Variance in Household Smart Microgrid. IEEE Trans. Ind. Electron. 2012, 60, 3218–3226. [CrossRef]
31.
Moghaddam, Z.; Ahmad, I.; Habibi, D.; Phung, Q.V. Smart Charging Strategy for Electric Vehicle Charging Stations. IEEE Trans.
Transp. Electriﬁcation 2017, 4, 76–88. [CrossRef]
32.
Hu, J.; Morais, H.; Sousa, T.; Lind, M. Electric vehicle ﬂeet management in smart grids: A review of services, optimization and
control aspects. Renew. Sustain. Energy Rev. 2016, 56, 1207–1226. [CrossRef]
33.
Karmaker, A.K.; Roy, S.; Ahmed, R. Analysis of the impact of electric vehicle charging station on power quality issues. In
Proceedings of the 2019 International Conference on Electrical, Computer and Communication Engineering (ECCE), Chittagong,
Bangladesh, 7–9 February 2019.
34.
Ashraf, S.; Alfandi, O.; Ahmad, A.; Khattak, A.M.; Hayat, B.; Kim, K.H.; Ullah, A. Bodacious-Instance Coverage Mechanism for
Wireless Sensor Network. Wirel. Commun. Mob. Comput. 2020, 2020, 8833767. [CrossRef]
35.
Dileep, G. A survey on smart grid technologies and applications. Renew. Energy 2019, 146, 2589–2625. [CrossRef]
36.
Yilmaz, M.; Krein, P.T. Review of the Impact of Vehicle-to-Grid Technologies on Distribution Systems and Utility Interfaces. IEEE
Trans. Power Electron. 2012, 28, 5673–5689. [CrossRef]
37.
Carli, R.; Dotoli, M. A Distributed Control Algorithm for Optimal Charging of Electric Vehicle Fleets with Congestion Management.
IFAC-PapersOnLine 2018, 51, 373–378. [CrossRef]
38.
Yong, J.Y.; Ramachandaramurthy, V.K.; Tan, K.M.; Mithulananthan, N. A review on the state-of-the-art technologies of electric
vehicle, its impacts and prospects. Renew. Sustain. Energy Rev. 2015, 49, 365–385. [CrossRef]
39.
Clairand, J.-M.; Guerra-Terán, P.; Serrano-Guerrero, X.; González-Rodríguez, M.; Escrivá-Escrivá, G. Electric Vehicles for Public
Transportation in Power Systems: A Review of Methodologies. Energies 2019, 12, 3114. [CrossRef]
40.
Panchal, C.; Stegen, S.; Lu, J. Review of static and dynamic wireless electric vehicle charging system. Eng. Sci. Technol. Int. J. 2018,
21, 922–937. [CrossRef]
41.
Stüdli, S.; Crisostomi, E.; Middleton, R.; Shorten, R. A ﬂexible distributed framework for realising electric and plug-in hybrid
vehicle charging policies. Int. J. Control. 2012, 85, 1130–1145. [CrossRef]
42.
Ashraf, S.; Ahmed, T.; Saleem, S. NRSM: Node redeployment shrewd mechanism for wireless sensor network. Iran J. Comput. Sci.
2020, 4, 171–183. [CrossRef]
43.
Hoke, A.; Brissette, A.; Smith, K.; Pratt, A.; Maksimovic, D. Accounting for Lithium-Ion Battery Degradation in Electric Vehicle
Charging Optimization. IEEE J. Emerg. Sel. Top. Power Electron. 2014, 2, 691–700. [CrossRef]
44.
Scarabaggio, P.; Carli, R.; Cavone, G.; Dotoli, M. Smart control strategies for primary frequency regulation through electric
vehicles: A battery degradation perspective. Energies 2020, 13, 4586. [CrossRef]
45.
Carli, R.; Dotoli, M. A Distributed Control Algorithm for Waterﬁlling of Networked Control Systems via Consensus. IEEE Control
Syst. Lett. 2017, 1, 334–339. [CrossRef]
46.
Murthy, V.N.; Singh, V.; Chen, T.; Manmatha, R.; Comaniciu, D. Deep decision network for multi-class image classiﬁcation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–29 June 2016.
47.
Ma, X.; Tao, Z.; Wang, Y.; Yu, H.; Wang, Y. Long short-term memory neural network for trafﬁc speed prediction using remote
microwave sensor data. Transp. Res. Part C Emerg. Technol. 2015, 54, 187–197. [CrossRef]
Sustainability 2023, 15, 2603
26 of 26
48.
Deligiannis, P.; Koutroubinas, S.; Koronias, G. Predicting Energy Consumption Through Machine Learning Using a Smart-
Metering Architecture. IEEE Potentials 2019, 38, 29–34. [CrossRef]
49.
Manishimwe, A.; Alexander, H.; Kaluuma, H.; Dida, M.A. Integrated Mobile Application Based on Machine Learning for East
Africa Stock Market. J. Inf. Syst. Eng. Manag. 2021, 6, em0143. [CrossRef]
50.
Fernandes, A.; Salazar, L.H.; Dazzi, R.; Garcia, N.; Leithardt, V.R.Q. Using Different Models of Machine Learning to Predict
Attendance at Medical Appointments. J. Inf. Syst. Eng. Manag. 2020, 5, em0122. [CrossRef] [PubMed]
51.
Rumelhart, D.E.; Smolensky, P.; McClelland, J.L.; Hinton, G.E. is achieved. Prior to stabilization, neural networks do not jump
around between points in activation space. Stabiliza-tion is the process whereby a network ﬁrst generates a de-terminate
activation pattern, and thereby arrives at a point in activation space. Behav. Brain Sci. 2004, 27, 2.
52.
Sarker, I.H. Machine Learning: Algorithms, Real-World Applications and Research Directions. SN Comput. Sci. 2021, 2, 1–21.
[CrossRef] [PubMed]
53.
Hsu, R.C.; Lin, T.-H.; Su, P.-C. Dynamic Energy Management for Perpetual Operation of Energy Harvesting Wireless Sensor
Node Using Fuzzy Q-Learning. Energies 2022, 15, 3117. [CrossRef]
54.
Gui, T.; Liu, P.; Zhang, Q.; Zhu, L.; Peng, M.; Zhou, Y.; Huang, X. Mention recommendation in Twitter with cooperative multi-
agent reinforcement learning. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in
Information Retrieval, Paris, France, 21–25 July 2019.
55.
Liu, T.; Zou, Y.; Liu, D.; Sun, F. Reinforcement Learning of Adaptive Energy Management with Transition Probability for a Hybrid
Electric Tracked Vehicle. IEEE Trans. Ind. Electron. 2015, 62, 7837–7846. [CrossRef]
56.
Alsharif, A.; Wei, T.C.; Ayop, R.; Lau, K.Y.; Bukar, A.L. A Review of the Smart Grid Communication Technologies in Contactless
Charging with Vehicle to Grid Integration Technology. J. Integr. Adv. Eng. 2021, 1, 11–20. [CrossRef]
Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual
author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
people or property resulting from any ideas, methods, instructions or products referred to in the content.


Paper 10:
- APA Citation: Fedushko, S. S., Ustyianovych, T., Syerov, Y., & Peracek, T. (2020). User-engagement score and SLIs/SLOs/SLAs measurements correlation of e-business projects through big data analysis. Applied Sciences, 10(24), 9112.
  Main Objective: Investigate automated data processing's impact on the technical condition of an educational web project.
  Study Location: Unspecified
  Data Sources: System logs, user interactions
  Technologies Used: Correlation analysis, contingency tables, user engagement scores, service level objectives (SLOs)
  Key Findings: By calculating the appropriate user-engagement scores for educational or e-business project components, we can filter these components according to particular criteria, which will let the web project parties find the aggregated user-engagement values (mean, median, percentiles) only for those units that interest us most, and compare them with other groups.
  Extract 1: 
  Extract 2: 
  Limitations: The analysis of static data generated in 2017 when the demand for web-educational content was high but not so great as opposed to the 2020 period of lockdown and the remote working and learning modes. Also, we analyzed just the business and finance Udemy educational web-service content, however, the platform contains other popular educational content and user groups to study, where the customer preferences might differ, as well as being on other platforms.
  Relevance Evaluation: {'extract_1': "The study's primary aim was to develop a framework for identifying potential infrastructure issues and improving website reliability through monitoring using data collected from various system logs and user interactions.", 'extract_2': 'To achieve this, the researchers employed a combination of correlation analysis, contingency tables, and a structured approach to defining service level objectives (SLOs) based on user engagement scores.', 'relevance_score': 0.9}
  Relevance Score: 0.9
  Inline Citation: Fedushko et al., 2020
  Explanation: The study's primary aim was to develop a framework for identifying potential infrastructure issues and improving website reliability through monitoring using data collected from various system logs and user interactions. To achieve this, the researchers employed a combination of correlation analysis, contingency tables, and a structured approach to defining service level objectives (SLOs) based on user engagement scores. The researchers demonstrated their approach using data collected from an online educational platform.

The main objective of this study was to investigate the relationship between automated data processing in the cloud and its impact on the technical condition of an educational web project, with a specific focus on the application of online learning techniques for continuously updating and improving machine learning models based on real-time data.

 Full Text: >
applied  
sciences
Article
User-Engagement Score and SLIs/SLOs/SLAs
Measurements Correlation of E-Business Projects
Through Big Data Analysis
Solomiia Fedushko 1,*
, Taras Ustyianovych 1
, Yuriy Syerov 1
and Tomas Peracek 2
1
Social Communication and Information Activity Department, Lviv Polytechnic National University,
79000 Lviv, Ukraine; taras.ustyianovych.dk.2017@lpnu.ua (T.U.); yurii.o.sierov@lpnu.ua (Y.S.)
2
Faculty of Management, Comenius University in Bratislava, 814 99 Bratislava, Slovakia;
tomas.peracek@fm.uniba.sk
*
Correspondence: solomiia.s.fedushko@lpnu.ua
Received: 25 November 2020; Accepted: 17 December 2020; Published: 20 December 2020


Abstract: The Covid-19 crisis lockdown caused rapid transformation to remote working/learning
modes and the need for e-commerce-, web-education-related projects development, and maintenance.
However, an increase in internet traﬃc has a direct impact on infrastructure and software performance.
We study the problem of accurate and quick web-project infrastructure issues/bottleneck/overload
identiﬁcation. The research aims to achieve and ensure the reliability and availability of a commerce/
educational web project by providing system observability and Site Reliability Engineering (SRE)
methods. In this research, we propose methods for technical condition assessment by applying the
correlation of user-engagement score and Service Level Indicators (SLIs)/Service Level Objectives
(SLOs)/Service Level Agreements (SLAs) measurements to identify user satisfaction types along
with the infrastructure state. Our solution helps to improve content quality and, mainly, detect
abnormal system behavior and poor infrastructure conditions. A straightforward interpretation of
potential performance bottlenecks and vulnerabilities is achieved with the developed contingency
table and correlation matrix for that purpose. We identify big data and system logs and metrics as the
central sources that have performance issues during web-project usage. Throughout the analysis of
an educational platform dataset, we found the main features of web-project content that have high
user-engagement and provide value to services’ customers. According to our study, the usage and
correlation of SLOs/SLAs with other critical metrics, such as user satisfaction or engagement improves
early indication of potential system issues and avoids having users face them. These ﬁndings
correspond to the concepts of SRE that focus on maintaining high service availability.
Keywords:
educational web-projects; real-time data analysis; big data; system performance;
Application Performance Monitoring (APM); business-plan; strategy; Service Level Objectives
(SLO); Service Level Agreement (SLA); Service Level Indicator (SLI)
1. Introduction
Hardware and software technical assessment of an educational web project in the face of the
increased need for their use not only creates many challenges, but also requires fast and objective
data-driven operations and decisions. This need is especially relevant during the Covid-19 crisis,
as it forces educational and business institutions to make the necessary migration to the online mode.
Educational institutions have faced the need to provide teachers with a ﬂexible IT infrastructure that
empowers eﬃcient deployment of educational materials and courses both in regular times and in a
state of emergency [1]. Educational institutions’ activities become almost impossible and less valuable
Appl. Sci. 2020, 10, 9112; doi:10.3390/app10249112
www.mdpi.com/journal/applsci
Appl. Sci. 2020, 10, 9112
2 of 16
without the availability of electronic educational web projects, in comparison to modern competitive
organizations that provide similar services. Many universities in Ukraine and around the world are
already eﬀectively using online learning as one of the leading strategies for building and developing
educational institution services. In addition, it was proven that the use of web-based resources for
online learning is more eﬀective compared to traditional learning methods [2].
The transition to the online mode allows eﬃcient and user-friendly use of e-learning technologies,
which are deﬁned as eﬀective multimedia learning using e-educational technology [3]. Thus, we state
that e-learning is a component of educational technology, the further development of which is
currently underway by many technical giants of the business world. In particular, the article by R.
Lakshminarayanan, B. Kumar, and M. Raju [4] considered how companies oﬀering cloud services and
technologies, in particular, Amazon Web Services (AWS), Microsoft, and Google allow educational
institutions to take advantage of certain products. In the study, a comparative and features analysis of
certain product usage was made.
The course of events related to the Covid-19 pandemic outbreak has led to a closer look at the
need for digitalization and the establishment of so-called e-learning within certain institutions and
organizations. We display in Figure 1, which was generated using the Google Trends service [5],
that with the onset of the Covid-19 pandemic, the increased rates of searches related to online learning
projects vary from 15% to 43%. The values show the popularity of a search term relative to the highest
point on the graph for a particular region and time period. 100 is the peak popularity of a search term,
while 50 means that the popularity of the term is half as popular during the speciﬁed period.
Appl. Sci. 2020, 10, x FOR PEER REVIEW 
2 of 16 
without the availability of electronic educational web projects, in comparison to modern competitive 
organizations that provide similar services. Many universities in Ukraine and around the world are 
already effectively using online learning as one of the leading strategies for building and developing 
educational institution services. In addition, it was proven that the use of web-based resources for 
online learning is more effective compared to traditional learning methods [2]. 
The transition to the online mode allows efficient and user-friendly use of e-learning 
technologies, which are defined as effective multimedia learning using e-educational technology [3]. 
Thus, we state that e-learning is a component of educational technology, the further development of 
which is currently underway by many technical giants of the business world. In particular, the article 
by R. Lakshminarayanan, B. Kumar, and M. Raju [4] considered how companies offering cloud 
services and technologies, in particular, Amazon Web Services (AWS), Microsoft, and Google allow 
educational institutions to take advantage of certain products. In the study, a comparative and 
features analysis of certain product usage was made. 
The course of events related to the Covid-19 pandemic outbreak has led to a closer look at the 
need for digitalization and the establishment of so-called e-learning within certain institutions and 
organizations. We display in Figure 1, which was generated using the Google Trends service [5], that 
with the onset of the Covid-19 pandemic, the increased rates of searches related to online learning 
projects vary from 15% to 43%. The values show the popularity of a search term relative to the highest 
point on the graph for a particular region and time period. 100 is the peak popularity of a search term, 
while 50 means that the popularity of the term is half as popular during the specified period. 
 
Figure 1. Google Trends statistics of online learning services search and usage. 
However, many educational institutions have not been sufficiently prepared for the switching to 
online task mode due to various reasons:. 
1. 
lack of internal infrastructure or subscriptions to external online projects to provide educational 
services; 
2. 
insufficient reliability of institutions’ infrastructural and technical support; 
3. 
complete or partial lack of the necessary teaching materials and resources for online classes; 
4. 
the lack of the university’s strategy for web projects implementation to support students’ e-
learning needs. 
In this article, we decided to solve the following problem: web project technical control and 
evaluation of an educational institution or e-commerce, which faces the problem of high load on 
hardware and web-software during remote learning and work; identify the cause-and-effect 
relationships of particular infrastructure type issues and improve site reliability methods usage. The 
Figure 1. Google Trends statistics of online learning services search and usage.
However, many educational institutions have not been suﬃciently prepared for the switching to
online task mode due to various reasons:
1.
lack of internal infrastructure or subscriptions to external online projects to provide
educational services;
2.
insuﬃcient reliability of institutions’ infrastructural and technical support;
3.
complete or partial lack of the necessary teaching materials and resources for online classes;
4.
the lack of the university’s strategy for web projects implementation to support students’
e-learning needs.
Appl. Sci. 2020, 10, 9112
3 of 16
In this article, we decided to solve the following problem: web project technical control and
evaluation of an educational institution or e-commerce, which faces the problem of high load
on hardware and web-software during remote learning and work; identify the cause-and-eﬀect
relationships of particular infrastructure type issues and improve site reliability methods usage.
The need for clear external or internal root-cause problem identiﬁcation is essential because this will
allow the formation of qualitative Service Level Objectives (SLOs).
The study objectives are as follows: (1) increase infrastructure visibility through correlation
and use of a user-engagement score with Service Level Objectives (SLOs)/Service Level Agreement
(SLA)/Service Level Indicator (SLI) as the main indicators of the web project technical equipment quality;
(2) improve methods for data analysis of virtual environment performance metrics; (3) interactively
monitor various web service processes and evaluate the technical characteristics of servers, applications,
etc. based on indicators and goals; (4) provide statistical methods for meaningful review of web
project goals based on metrics coming from the data sources in order to improve all the processes
described above; (5) develop methods to increase availability and reliability of educational web projects;
(6) use metrics in real-time to indicate system resource shortages and bottlenecks instead of critical
user responses; (7) increase the eﬃciency of a university’s online educational service and objectively
create the requirements for scaling, creating eﬀective solutions and architectures through monitoring;
(8) improve decision-making processes regarding the architecture and IT operations of an online
educational project. Consequently, a project designed for e-learning, as well as the particular university
or institution’s educational technology must follow data-driven decision making.
2. Literature Review
Many methodologies and frameworks have already been developed for technical assessments and
monitoring. For example, M. Bashirov’s study used mathematical modeling and an electromagnetic-
acoustic eﬀect to determine the defect and reliability of pipelines. The predictive model usage at an
early stage is determined to increase the probability of defect identiﬁcation [6]. A certain percentage
of web-project technical equipment (hardware) or software in e-learning or e-commerce is outdated,
in which case its monitoring and reliability assessment is diﬃcult and limited. The main reason
is that there are diﬃculties in establishing connection with the legacy devices themselves; fewer
metrics and logs are generated, and data processing and transfers are slower compared to modern
IT solutions. A study by Bednarski et al. showed the results of the historical structure’s technical
condition assessment. Physical quantity measurements were important for the crack state of a historic
church in Jangrot, the assessment allowed identifying the kind of components which needed to be
replaced. The study mainly emphasized the need for environmental data collection and its correlation
with information about a particular equipment unit’s condition and reliability. The need for advances
in technical measurement data collection and the development of new means for quick data ingestion
was emphasized as well [7–9]. The most focus is on the problem of infrastructure insecurity and
unreliability, which can lead to web project instability and the inability to provide educational services
by institutions in real-time. The educational web projects’ reliability and availability is a prerequisite
for providing qualitative services in higher education institutions and schools. It plays a particularly
important role in isolation and quarantine conditions, for example, Covid-19, as educational institutions
are transitioning to online teaching and e-learning. An equally important problem faced by educational
institutions during the period of abnormal load on the online project technology infrastructure is the
lack of visibility and real-time monitoring, which makes it impossible to make objective decisions
regarding system scaling and troubleshooting. Accordingly, if we take into account the technological
needs and scale of a particular online project, the need for monitoring, site reliability engineering,
and operational intelligence methods becomes increasingly clear [10]. Belforte et al. provide an
example distributed across more than 60 computing centers worldwide, with CMS management and
monitoring using custom and traditional machine reliability metrics. In addition, an algorithm to
automate the performance of distributed resources is described, which is very valuable for an online
Appl. Sci. 2020, 10, 9112
4 of 16
system that uses load-balancing between clusters [11]. The implementation of this algorithm can
facilitate and improve monitoring processes.
A correct SLA deﬁnition allows a strong user/customer understanding. Operational intelligence
and real-time monitoring techniques involve the implementation of user behavior analytics.
This process, in turn, allows not only understanding the users and monitoring their behavior in
real-time, but also to determine certain performance indicators for an individual user, to set objective
SLAs. In the article by Alﬁan et al., big data methods were used to collect browsing history and
transaction data for real-time analysis of user behavior interacting with e-services in diﬀerent locations.
This study allows web projects to improve service quality and to establish the optimal service level
agreement, which will be beneﬁcial for all parties involved. Equally essential is the real-time monitoring
of personalized diabetic patients’ health. The study is valuable because it reﬂects the use of the Bluetooth
Low Energy method to reduce the cost of data collection, as well as to provide high-quality advice
to patients. Machine learning predicts the likelihood of detecting diabetes in patients based on the
collected data. This technical solution can be used not only for medical data collection, but also for
the equipment and infrastructure data transmission to a centralized logging environment [12,13].
Web-analytics tools are used for real-time mouse tracking, which helps to collect data about users
and their interaction with an e-service. Accordingly, the data correlate with transactions and queries’
completion rates, which are necessary to monitor availability and reliability. The article by Cegan and
Filip describes how user behavior monitoring allows detecting bottlenecks in the web environment,
as well as in technical equipment and infrastructure components. The authors propose a new method
for collecting mouse-clicking data based on real-time data transformation to convert discrete position
data to system functions to optimize compression and analysis [14]. The usage of functional tests
to assess the infrastructure and site reliability as a way to validate site operations and optimization
techniques is described in Elmsheuser et al.’s study [15].
Machine learning and artiﬁcial intelligence are also widely applied for technical condition
assessment. A recent study conducted by Kaminski et al. showed the usage of artiﬁcial neural
networks, namely the multilayer perceptron, to assess the technical condition of a water supply system.
The results proved that the use of artiﬁcial intelligence in such tasks can increase the eﬃciency of
detecting defects in pipes with a distributed water supply chain and is an example of human-machine
interaction [16]. An improved support function machine model indicated that the pattern recognition
method based on an improved kernel function support vector machine is eﬃcient for validating
technical conditions of the recoil mechanism [17]. The embedded system’s usage greatly simpliﬁes
real-time monitoring, as it allows more metrics to collect than are collected during traditional system
monitoring. Studies by Bosse and Lehmhus provide a model for data collection using a structural
monitoring and tactile sensing system to obtain data from the lowest system levels. It allows an IT team
to assess and monitor the technical condition of the equipment with great accuracy [18]. This research
is crucial for our study, because in order to form new metrics and ﬁnd infrastructure issues’ causal
relationships, and create an optimal SLO/SLA deﬁnition, it is necessary to collect data from all possible
levels of web project applications.
Despite the growing need for e-learning, many research and educational institutions are not
ready to move to a full-ﬂedged online mode due to insuﬃciently reliable technical equipment and/or
infrastructure. This leads to the issue that many information-educational web projects during the
Covid-19 crisis are not able to maintain stability when the load of materials, users and downtimes
occur; in addition, the transactions might not be processed in a proper way. The problem remains
unresolved, as only a small percentage of Ukrainian universities were ready to move to remote teaching
and learning courses when the lockdown started. The main signals of an e-learning or e-business
project problem should be data and metrics that show unsatisfactory performance indicators, but not
negative user feedback and/or open incidents for the e-project support team.
A study by Feldmann et al.
found that internet traﬃc grew by about 15–20% within one
week of the Covid-19 crisis due to the increased use of online resources, namely: web conferencing,
Appl. Sci. 2020, 10, 9112
5 of 16
VPN, e-commerce, e-learning, and gaming. These ﬁndings are similar to the insights shown by
mobility reports published by Google and prove the increased digital demand during Covid-19 [19,20].
In addition, ensuring the infrastructure reliability requires the implementation of system quality
monitoring methods, setting certain goals to warrant highly reliable and uninterrupted IT operation,
as well as scalability, which might previously have been lacking. The scarcity of real-time data analytics
deprives a project of observability and does not allow accurate estimation of the actual educational
web project needs for handling end-to-end operations and measuring the load on them.
The research by Canizo et al describes the implementation of a monitoring solution on a real
industrial use case that includes several industrial press machines. The eﬀectiveness and its scalability
factors are proved. A big data architecture for industrial Cyber-Physical System (CPS) monitoring is
proposed, considering four main data factors during the implementation, namely: data acquisition,
processing, persistence and server availability. The data collection process is implemented using
programmable logic controllers. Message streaming and parallel processing tools are used to transfer
and transform the data.
The research is valuable because of Signe and multiple data anomaly
detections that are applied as calculation frameworks to detect issues and anomalies. The anomaly
detection algorithms are mainly based on checking previous and current system states. Nevertheless,
this implementation addresses all the main issues that a CPS faces. The proposed solution has increased
the overall equipment eﬀectiveness [21].
The state-of-the-art real-time big data processing technologies that are used for anomaly detection,
abnormal system behavior and vital machine learning algorithm features are studied by Habeeb et al.
In the research they describe frameworks to handle big data processing in real-time in order to identify
system issues and security vulnerabilities; a survey of big data techniques was conducted. The research
also provides comprehensive big data techniques to monitor network data [22].
The deﬁnition of Service Level Objectives and usage is necessary for reliability monitoring,
resource utilization reduction, and performing computationally inexpensive calculations. The article
describes performance modeling with proﬁling to ensure low system performance usage so that the
resources used for e-commerce can be decreased by three times [23]. This framework can be eﬀectively
automated and applied to universities’ e-learning projects in order to ensure high reliability and
to reduce costs of infrastructure maintenance. In addition, it proves that monitoring and control
of Service Level Objectives can increase project eﬃciency, and therefore their usage within EdTech
remains necessary.
3. Materials and Methods
EdTech and e-business project availability and reliability are important to ensure qualitative
service delivery and product distribution. This need is especially necessary and noticeable during
the period of remote work or study. Service unavailability might cause ﬁnancial losses and also
does not allow for a quality educational process. Research by Melo et al. [24] helps to estimate how
much money can be saved by increasing system availability using SLA; it also presents analysis of
various system architectures to ensure a beneﬁcial cost-beneﬁt relationship. Research from Fortune
1000 companies shows the downtime value for business-critical metrics. For example, on average the
total cost of an unplanned system downtime per year is about $1.25 billion and up to $2.5 billion [25].
Additionally, making objective data-driven decisions [26–28] about a particular service [29,30], user [31],
and technical condition [32] is a key prerequisite for ensuring equipment quality and reliability.
The modern monitoring tools usage will simplify the task of the equipment’s technical assessment
and control. According to the Gartner Share Analysis Report for 2019, digital products focus is
increasingly on end-user experience monitoring, which is very important because it provides a clear
user-to-web project interaction understanding. This has a critical impact on business income and the
user’s desire to continue using the service in the future. Especially notable is the end-user experience
in the period of the increased need to use online education and business web projects, such as in the
period of COVID-19. Also, the ITOM (IT Operations Management) performance analysis software
Appl. Sci. 2020, 10, 9112
6 of 16
market grew by around 11% compared to 2018. The AI (Artiﬁcial intelligence) usage, namely AIOps
(Artiﬁcial Intelligence Operations), ITIM (IT Infrastructure Monitoring) and other monitoring solutions
hold around 45% of the performance market demand, whereas the APM and network monitoring
segments [33] have decreased to demands of 34% and 21%, respectively.
Downtime and minimalizations in spending are essential to address, as well as insuﬃcient
hardware and software viability/availability. The SLO deﬁnition and usage is not capable of solving the
system technical condition assessment problem without the help of SLI/SLO/SLA data and metadata.
The use of SLOs is a popular trend today; diﬀerent-sized enterprises that run electronic services
are increasingly using it. SLOs and the SLA serve as motivating factors that provide a goal setting
process. It encourages an organization to achieve the goals, increase the threshold and overcome it
again; measure user satisfaction level with and without an Apdex score correlation; accept limiting
the threshold of system availability; and help understand at what infrastructure improvement stage
a web project can improve performance in the future. There are many tools for SLO computations
and monitoring on the market, and they can be easily applied to solve various problems with very
complex demands.
We used the following methods to conduct and obtain the research results: data engineering;
data collection and logging; mathematical and statistical methods to calculate KPIs, user-engagement,
and correlation analysis; site reliability engineering methods; exploratory and descriptive methods
for prescriptive data analysis; incidents management analysis; data visualization; business-plan and
long/short-term strategy for infrastructure improvement formation.
To obtain the required data for the algorithm implementation and SLO deﬁnition, it is necessary
to monitor the infrastructure of the electronic environment from the log ﬁles stored in the system and
collect web project user behavior and activity statistics.
Moreover, SLO/SLA adherence metadata should be correlated with other important metrics for
e-education and e-business, namely: Customer Proﬁtability Score (CPS), Net Proﬁt Margin, Conversion
Rate, Net Promoter Score (NPS), and relative market share. These and other metrics need to be
monitored on interactive panels, in dashboards for operational intelligence monitoring.
Before determining the service level objectives, the following questions should be answered: what
percentage of web project performance increase should be met; whether an increase in availability will
aﬀect a raise in proﬁts, and if yes, then how much; what interdependence level between user-engagement
score and service level indicators (SLI) is observed.
4. Results
The short- and long-term SLO deﬁnition is fundamental in monitoring. The former is vital
for systems engineers, while the latter is necessary for the strategy development and management
departments of a particular web project. However, in terms of infrastructure technical condition
assessment and investigating web environment impact on hardware, both SLO types are useful and
need to be analyzed.
4.1. User-Engagement Calculations to Assess Educational Web Project Parts Interaction
Both reliability and availability metrics are valuable in the application of performance management
and monitoring. However, both are diﬀerent from each other, because a piece of technical equipment
may be available but not reliable. For example, we could consider a case where we suppose we do have
equipment X, which has a frequent connection loss for 6 min every hour. That means 90% availability,
but less than 1 h of reliability, which is a poor indicator value for e-commerce.
Self-education e-resources have been in demand especially since the start of the Covid-19
crisis. For instance, registration on the popular educational platform Coursera, which underwent a
partnership with various universities during the Covid-19 quarantine period, is up by 173% in March
2020, while course enrollment increased by 145% compared to February the same year [34].
Appl. Sci. 2020, 10, 9112
7 of 16
In the study, we analyzed the 2017 business and ﬁnance courses data from an educational platform
Udemy in order to obtain the user-engagement score and perform association tests for the obtained
values with the subscription and review count, prices, and course duration. This allowed us to validate
the user-engagement score as an unbiased metric for web project assessment and correlation with
technical condition data. We investigated how the user-engagement score might aﬀect the increase in
eﬃciency of careful Service Level Objectives deﬁnition and computation. Also, we evaluated various
indicators and strategies to assess infrastructure technical conditions based on the correlation between
SLO/SLA and user-engagement scores. The developed framework is to help identify possible system
issues and improve regular downtimes management. According to Google Trends statistics, the peak
of popularity of the platform from 2017 to 2020 inclusive was reached in late March–early April 2020
and was 100%. Thus, the losses due to the unavailability of the service can be much greater than in the
normal period. Therefore, the implementation of technical condition assessment using site reliability
and SLO methods is necessary for the full services and information product provision.
We found that in 2017 Udemy business and ﬁnance domain courses data, there are 1195 observations
out of which 94.85% entities have a label “paid”, although, the user enrollment in paid courses is
75.58%. The average cost of a studied online course was $120, and the median and the mode were $124
and $200, respectively. In the majority of cases, online classes that were published earlier in the year
had more students signed up by the end of the year. In addition, 0.03% of free courses had a higher
than average number of subscribers and even exceeded subscriptions to paid courses. This indicates
that if the web project training is free and the user believes that its material is well-structured enough,
it is more likely that a person will register for the class, and not for a paid one. In turn, this will create a
load on the infrastructure, induce more course enrollments, but can be less proﬁtable to an e-learning
service provider.
The paid course duration takes longer compared to free ones. We determined the total sum of
hours required for each registered participant to complete the course. The number of subscribers is
multiplied by the course duration to calculate this. The total number of subscribers was 668,938, so the
entire time for all users’ course completions was 3,754,806 h.
On average, one hour of paid content costs a user $18.6. Assume that a 1% loss of availability can
block users from enrolling or attending the selected class, a loss in proﬁt will be about $123,700 per
hour, according to our data.
A detailed data examination revealed that the most expensive courses are at the expert level,
and in terms of duration they are in the interval of 1–100 h, while classes with a longer span (~200–300 h)
are only beginner level and cost less by about $20 than those in less long courses. Thus, the shorter the
course duration (Figure 2) —the less load on the infrastructure is created, and more proﬁt is made not
only due to the higher e-training cost, but also lower costs associated with the service disposal. This is
important to consider.
User-engagement calculation and monitoring along with correlation of the SLAs compliance
levels will allow us to assess the technical equipment, infrastructure condition, as well as to determine
which categories of users face particular online project bottlenecks. An equation to determine an
online project user-engagement score with respect to n number of content entities has been developed.
Our formula oﬀers the possibility of its application both to a web project separate element and the
service entirely; input parameters are the most important metrics for understanding the web project
and its participants’ interactions; the weights are calculated on a comparative basis of all project
components. Below is Equation (1) for speciﬁc content i.
Ui = max(Ri, C)
max(Vi, C) Wi,
(1)
where Ui is the user-engagement value. Its range depends on the minimum and maximum values
available in the dataset for the arguments Vi and Ri, Vi is the number of views/visitors/subscriptions
on a certain online project topic. Ri is the number of reactions on a certain online project topic, where 0
Appl. Sci. 2020, 10, 9112
8 of 16
≤ Ri ≤ Vi. C is a constant used to avoid getting zero during the division. In our research, we set it to 0.5
because it is an artiﬁcially created value that does not count as a real user review Ri value. Wi is the
weight parameter, which is diﬀerent for each topic and its user-engagement Ui value.
p
y
,
,
the entire time for all users’ course completions was 3,754,806 h. 
On average, one hour of paid content costs a user $18.6. Assume that a 1% loss of availability 
can block users from enrolling or attending the selected class, a loss in profit will be about $123,700 
per hour, according to our data. 
A detailed data examination revealed that the most expensive courses are at the expert level, 
and in terms of duration they are in the interval of 1–100 h, while classes with a longer span (~200–
300 h) are only beginner level and cost less by about $20 than those in less long courses. Thus, the 
shorter the course duration (Figure 2) —the less load on the infrastructure is created, and more profit 
is made not only due to the higher e-training cost, but also lower costs associated with the service 
disposal. This is important to consider. 
 
Figure 2. Business and finance course price and duration grouped by competency levels. 
User-engagement calculation and monitoring along with correlation of the SLAs compliance 
levels will allow us to assess the technical equipment, infrastructure condition, as well as to determine 
which categories of users face particular online project bottlenecks. An equation to determine an 
online project user-engagement score with respect to n number of content entities has been 
developed. Our formula offers the possibility of its application both to a web project separate element 
Figure 2. Business and ﬁnance course price and duration grouped by competency levels.
Max functions are used to obtain the maximum value between Vi or Ri and the constant. We found
that more than 9% of courses do not have any user reviews, but the number of subscribers ranged
from 0 to 1600. This is quite unclear and at the same time captivating because it means that there was
insuﬃcient user-engagement, probably due to a lack of a user- and content-understanding. Apart from
that, we consider that users might have faced technical issues when viewing the content of these
courses. This ﬁnding needs further study to determine what leads to this kind of user-behavior. Table 1
shows the number of courses according to the total number of subscribers (X-axis).
Table 1. Total subscribers count interval per Udemy business and ﬁnance courses.
Subscribers Count Interval
Number of Courses
0–5000
1119
5000–10,000
52
10,000–15,000
14
15,000–20,000
6
20,000–30,000
5
50,000–70,000
3
We propose the following Equation (2) to calculate the weight value:
Wi =
Xk
j=1 fj,
(2)
where fi is the relative frequency of a certain factor related to a web-resource category or topic,
for instance, user interaction, total time spent for page visit, page load time, views, reviews,
subscriptions, and comments left. We might also face an issue when the Wi equals zero. Then the
user-engagement is equal to zero as well. This might happen in rare cases, because if the Ui has a high
value without multiplying it by the weight coeﬃcient, it is very likely that Wi will be greater than 0.
Appl. Sci. 2020, 10, 9112
9 of 16
As mentioned above, the output range of the user-engagement score is dependent on the arguments
Vi and Ri; in order to standardize it, adjusted to the range from 0 to 1, and receive a user-engagement
score value for a topic i, we propose the following simple Equation (3) below:
Ustandardi =
Ui
max(Ui),
(3)
where we get the ratio of a certain category user-engagement to the max user-engagement scores.
4.2. SLO/SLA and the Obtained User-Engagement Score Correlation
Having the user-engagement score calculated, the service level objectives and agreements deﬁnition
procedure gets improved. By calculating the appropriate user-engagement scores for educational
or e-business project components, we can ﬁlter these components according to particular criteria,
which will let the web project parties ﬁnd the aggregated user-engagement values (mean, median,
percentiles) only for those units that interest us most, and compare them with other groups. Also,
based on multiple entities of the user-engagement scores, we can ﬁnd the overall scores using statistical
and mathematical functions and monitor them over time. Based on the values obtained from these
computations and SLOs/SLAs data, we propose a correlation matrix to show how the user-engagement
score can aﬀect them and the technical equipment, and facilitate its assessment. Thus, applying
technical condition assessment is necessary to increase the availability of an e-commerce or e-learning
project, especially when there is a high demand for it. Using the SLO and the SLA allows us to
assess the infrastructure condition and make objective decisions about the application architecture.
We should take into account that the SLO setting process requires a preliminary system and potential
infrastructure risks understanding as well as performing user-behavior analysis.
The following is a contingency table that helps to quantify user-engagement score intervals
frequency with data about the SLO/SLAs adherence. The user-engagement score can be generic for the
whole platform, as well as applying for individual web project components. Based on the contingency
table data, we can ﬁnd the association between a user and the online–resource interaction, how positive
it has been, and what might be improved. For example, if the SLO for availability is deﬁned, which is
directly related to the technical equipment condition, it is profoundly observable how it associates with
a certain web–resource user-engagement or interaction. It is thus possible to display the cases when the
SLO or the SLA has been high (Table 2), even though the user-engagement has not been or vice versa.
Table 2. Contingency table for the SLO/SLA data and user-engagement scores correlation.
User-Engagement Standardized Score
SLO/SLA
0–0.25
0.25–0.5
0.5–0.75
0.75–0.95
0.95–1
Total
0–25%
X11
X12
X13
X14
X15
X1+
25–50%
X21
X22
X23
X24
X25
X2+
50–75%
X31
X32
X33
X34
X35
X3+
75–95%
X41
X42
X43
X44
X45
X4+
95–100%
X51
X52
X53
X54
X55
X5+
Total
X+1
X+2
X+3
X+4
X+5
X
The relative frequencies usage is also recommended for a clear contingency table results
interpretation.
We developed a 3 × 3 matrix based on the SLO/SLA compliance levels and
user-engagement score. The levels (low, moderate, high) are custom for each web project. In our case we
deﬁne the following ranges (0–0.05), (0.05–0.65), (0.65–1) as low, moderate and high, correspondingly.
The SLO/SLA levels depend on their deﬁnition documents. For instance, the current published target
for Google Compute Engine availability is 99.95% availability [35]. If this target is met, we deﬁne the
SLO/SLA level as high (Table 3).
Appl. Sci. 2020, 10, 9112
10 of 16
Table 3. Interpretation table for the SLO/SLA and the user-engagement score levels correlation.
Low User-Engagement
Moderate User-Engagement
High User-Engagement
Low SLO/SLA
Poor technical
equipment condition.
Critical infrastructure
problems might arise.
Users face potential issues.
Web-service needs to be improved,
and the logs might be analyzed.
Users face many issues
during interaction with a
service. The bounce rate
(single page visit) is
likely to be high.
Moderate SLO/SLA
A service can face
potential issues and
problems. Even with a
low load, the SLO/SLA is
not high enough to be
met.
Changes and improvements to the
online-resource are necessary as
well as user-monitoring
implementation.
A service has a high
throughput and users
actively interact with it.
The technical conditions
need to be improved.
High SLO/SLA
A service met its
objectives and is reliable,
however, due to low
user-engagement, it is
quite diﬃcult to predict
online-resource behavior,
and the SLO/SLA levels
themselves when the
load increases.
A system
infrastructure/network/technical
component performs well. The
user-interaction needs to be
improved and its future increase
outcomes must be observed.
A service works well,
users actively interact
with a platform. The
technical condition
meets the desired
requirements, so features
and improvements can
be added to maintain
high scores.
Accordingly, the higher the user-engagement score level, the more active users interact with the
platform, the more they expect this platform to meet the SLAs. If the user-engagement score level
is low and the SLO/SLA is not met, this is a sign of a web project with critical problems, even with
hardware-related ones. If the user-engagement score level is low, but the SLO/SLA is met frequently
and is determined by our table as high, the service and equipment under certain conditions are quite
reliable and cope with the load, but with user ﬂow and load increase, it is diﬃcult to predict the web
service operations performance. In this situation, we recommend testing the platform and executing
increased user ﬂow simulations.
A moderate user-engagement and the same average SLO/SLA levels indicate minor problems with
technical equipment and infrastructure, as well as the need to improve the web project user interaction.
With high SLO/SLA adherence stats and a top user-engagement score, the technical equipment works
with stability, the development team can test and release new features for the web project and gradually
add improvements, making it more attractive to users than before.
Based on the Udemy business and ﬁnance courses dataset, the user-engagement score was
calculated, and obtained values are represented in a table view (Table 4).
Table 4. The standardized user-engagement score distribution for Udemy business and ﬁnance courses.
Standardized User-Engagement Score Interval
Value
(0–0.1)
1165
(0.1–0.2)
20
(0.2–0.3)
7
(0.3–0.4)
2
(0.4–0.5)
0
(0.5–0.6)
3
(0.6–0.7)
1
(0.7–0.8)
0
(0.8–0.9)
0
(0.9–1)
1
To compute it, we used such parameters as the number of subscribers, the number of users who
left feedback on the course, and the number of published course materials. We can see that only a small
number of Udemy courses (2.8%) crossed the standardized user-engagement score of 0.1, where the
minimum and maximum values are 0 and 1, respectively. In these courses with the value above 0.1,
Appl. Sci. 2020, 10, 9112
11 of 16
the duration ranges from 1 to 6 h; more than 61% are paid, the price varies between $150–200, while the
minimum price among this course category is $20, and the mean is $120; the expertise level for this
category is mostly beginner or suitable for all levels, although some of them have the intermediate
expertise level.
Thereafter, we found that courses with a low user-engagement score (<0.1) not only have few
subscribers and feedback but also focus on all levels or beginner level as well. The duration of these
courses usually varies from 1 to 3 h, meaning the courses are not long-term, and most of them are paid
(92.87%), the prices vary from $20 to $200, while the median and mean are $102 and $104, respectively.
We used Pearson’s correlation coeﬃcient to study the association among user-engagement scores,
the number of subscribers, and the number of reviews of a speciﬁc course. We obtained a very robust
association between the score and the number of reviews (1), and a strong association between the
score and number of subscribers (0.77). Also, a correlation between the number of subscribers and the
number of reviews exists (0.79) and is strong as well (Figure 3).
Appl. Sci. 2020, 10, x FOR PEER REVIEW 
11 of 16 
for this category is mostly beginner or suitable for all levels, although some of them have the 
intermediate expertise level. 
Thereafter, we found that courses with a low user-engagement score (<0.1) not only have few 
subscribers and feedback but also focus on all levels or beginner level as well. The duration of these 
courses usually varies from 1 to 3 h, meaning the courses are not long-term, and most of them are 
paid (92.87%), the prices vary from $20 to $200, while the median and mean are $102 and $104, 
respectively. We used Pearson’s correlation coefficient to study the association among user-
engagement scores, the number of subscribers, and the number of reviews of a specific course. We 
obtained a very robust association between the score and the number of reviews (1), and a strong 
association between the score and number of subscribers (0.77). Also, a correlation between the 
number of subscribers and the number of reviews exists (0.79) and is strong as well (Figure 3). 
 
Figure 3. Pearson’s association coefficients and plots for the user-engagement score, number of 
subscribers and reviews to the Udemy Business and Finance courses. 
This strong association between user-engagement score and reviews/subscribers count exists 
because they are used as the main arguments for the equation. We consider correlation of other 
dataset features with the obtained user-engagement score, so that more insights can be received. To 
study the dataset covariance and obtain the most critical features for dimensionality reduction, we 
performed Principal Component Analysis (PCA). We used the singular value decomposition method, 
which examines the covariances between individuals in the dataset. This statistical method allowed 
us to simplify the correlation observation and define features that have the highest value for web-
projects user-engagement and popularity. Figure 4 shows that the first dimension (PC1), with 
eigenvalue and variance out of all principal components equals 35%. We observe that such features 
as the number of subscribers, number of reviews, weights, user-engagement, boolean value for 
free/paid entity, and content activeness time (how long a course is available on the platform) are 
included in this component. 
Figure 3. Pearson’s association coeﬃcients and plots for the user-engagement score, number of
subscribers and reviews to the Udemy Business and Finance courses.
This strong association between user-engagement score and reviews/subscribers count exists
because they are used as the main arguments for the equation. We consider correlation of other
dataset features with the obtained user-engagement score, so that more insights can be received.
To study the dataset covariance and obtain the most critical features for dimensionality reduction,
we performed Principal Component Analysis (PCA). We used the singular value decomposition
method, which examines the covariances between individuals in the dataset. This statistical method
allowed us to simplify the correlation observation and deﬁne features that have the highest value
for web-projects user-engagement and popularity. Figure 4 shows that the ﬁrst dimension (PC1),
with eigenvalue and variance out of all principal components equals 35%. We observe that such
features as the number of subscribers, number of reviews, weights, user-engagement, boolean value
for free/paid entity, and content activeness time (how long a course is available on the platform) are
included in this component.
Appl. Sci. 2020, 10, 9112
12 of 16
Appl. Sci. 2020, 10, x FOR PEER REVIEW 
12 of 16 
 
Figure 4. PCA squared coordinates correlation plot. 
The PCA biplot (Figure 5) proved again that user-engagement, the number of reviews, and the 
number of subscribers are positively correlated. We also found that the higher the price, the greater 
the number of published lectures, and the longer the course duration, which is obvious. Moreover, 
the time since a course is available correlates with the user-engagement score and corresponding to 
its values. We assume that the longer a web-project content is available, the larger the chance to obtain 
high user-engagement and many subscribers/reviews. A negative correlation between the course 
payment option (free/paid) and the number of subscribers, which has significance for the user-
engagement score, is examined. Correspondingly, if a web-project content is free of charge, it has a 
higher probability to obtain high user-engagement and attention than the paid ones. 
 
Figure 5. PCA biplot of business and finance course generated numerical features. 
Figure 4. PCA squared coordinates correlation plot.
The PCA biplot (Figure 5) proved again that user-engagement, the number of reviews, and the
number of subscribers are positively correlated. We also found that the higher the price, the greater the
number of published lectures, and the longer the course duration, which is obvious. Moreover, the time
since a course is available correlates with the user-engagement score and corresponding to its values.
We assume that the longer a web-project content is available, the larger the chance to obtain high
user-engagement and many subscribers/reviews. A negative correlation between the course payment
option (free/paid) and the number of subscribers, which has signiﬁcance for the user-engagement score,
is examined. Correspondingly, if a web-project content is free of charge, it has a higher probability to
obtain high user-engagement and attention than the paid ones.
Appl. Sci. 2020, 10, x FOR PEER REVIEW 
12 of 16 
 
Figure 4. PCA squared coordinates correlation plot. 
The PCA biplot (Figure 5) proved again that user-engagement, the number of reviews, and the 
number of subscribers are positively correlated. We also found that the higher the price, the greater 
the number of published lectures, and the longer the course duration, which is obvious. Moreover, 
the time since a course is available correlates with the user-engagement score and corresponding to 
its values. We assume that the longer a web-project content is available, the larger the chance to obtain 
high user-engagement and many subscribers/reviews. A negative correlation between the course 
payment option (free/paid) and the number of subscribers, which has significance for the user-
engagement score, is examined. Correspondingly, if a web-project content is free of charge, it has a 
higher probability to obtain high user-engagement and attention than the paid ones. 
 
Figure 5. PCA biplot of business and finance course generated numerical features. 
Figure 5. PCA biplot of business and ﬁnance course generated numerical features.
Appl. Sci. 2020, 10, 9112
13 of 16
The above analyses can help to obtain new knowledge about the data and determine types of
web-project content that have high user-engagement and are attractive for customers. The main
features that correlate with the user-engagement score are deﬁned. The PCA results can be applied for
the development of predictive machine learning models to solve various tasks in the ﬁelds of e-learning
and e-commerce.
5. Conclusions
We conclude that commerce and educational project representatives, who still do not use electronic
and EdTech resources, experience many losses during the period of the urgent need for digitalization
and remote working/teaching activities. The article provides a framework for improving technical
equipment reliability and availability, and detection of insuﬃcient resource allocation, which can
lead to proﬁt, users, or customers’ loss, and harms business competition, especially during a crisis.
We propose the application of user-engagement and Site Reliability Engineering tools with the concept
of Service Level Objectives/Service Level Agreement in an eﬃcient way using real-time monitoring,
due to the fact that it allows organizations to make web-project infrastructure observable and achieve
data-driven decision making. The presence of operational intelligence and performance monitoring
is necessary for data research and to provide high-quality service in the remote work and learning
modes. We claim that log management of a web-project facilitates eﬃcient Service Level Objectives
deﬁnition as well as the possibility of task automation in the future with intelligent methods.
In this article, we provided an equation for the user-engagement score calculation, which was
applied to the Udemy business and ﬁnance educational content dataset. Our user-engagement
score is valuable for determining user-behavior and learning trends from topics with their diﬀerent
values. The developed contingency table will simplify the study of the relationship between SLO/SLA
adherence and user-engagement data. Accordingly, to calculate user-engagement, we propose to use
more than one metric, as well as to use weights that are independent and reﬂect a speciﬁc web project
unit in its total spectrum (relative frequency, percentage, ratings). We would like to pay special attention
to the need for user rating score presence in the dataset, which can broaden the user-engagement study
as well as its correlation with the obtained SLO/SLA. It is necessary to develop a strategy for collecting
the necessary business/machine data, as well as a business plan to determine the desired SLI thresholds
so that the SLO/SLA calculation can be done in an eﬃcient manner.
The limitation of the study is the analysis of static data generated in 2017 when the demand
for web-educational content was high but not so great as opposed to the 2020 period of lockdown
and the remote working and learning modes. Also, we analyzed just the business and ﬁnance
Udemy educational web-service content, however, the platform contains other popular educational
content and user groups to study, where the customer preferences might diﬀer, as well as being
on other platforms. Our study can lead to certain kinds of social implications—to an increase in
commerce/educational web-project proﬁtability due to being available and reliable as well as to allow
people to access speciﬁc content. That might also enforce web-projects to adjust the content in order
to increase user-engagement and meet the SLOs/SLAs. A tendency for user-friendly web-projects
and improvements in zero downtime system [36] can evolve. However, we should also consider that
organizations will need to collect more data about users than before, so security and privacy concerns
might arise. Correspondingly, the more data that will be collected, the more eﬃcient techniques for data
handling and storage need to be developed, and organizations need to adopt these new IT solutions.
In the article, we proposed an SLO/SLA and user-engagement levels matrix that improves the
infrastructure technical condition interpretation and speeds up the above-mentioned contingency table
formation. Our ﬁndings associated with educational/commerce web projects show that factors such as
the web project service costs, required knowledge level, and class duration aﬀects user-engagement.
A data lake, which will contain raw data, and logs that are of signiﬁcance to the e-learning and
e-commerce strategies, can be developed in further research.
Appl. Sci. 2020, 10, 9112
14 of 16
For further studies, we propose to perform Natural Language Processing (NLP) of the text data in
the studied dataset and correlation with our user-engagement score. In this way, it will be possible
to ﬁnd out whether there is a relationship between the name or description of an educational web
project element and the interaction frequency. The most valuable might be the task to identify which
words and phrases are key to the audience interested in educational/commerce web projects by their
increase/decrease. Also, hypotheses regarding the ratio of user-engagement score, business metrics,
user evaluation, and SLO/SLA adherence can be developed and conﬁrmed or rejected.
Author Contributions: Conceptualization, T.U., S.F. and Y.S.; methodology, T.U. and S.F.; validation, T.U., S.F.
and Y.S.; formal analysis, T.U., S.F. and Y.S.; investigation, T.U., S.F., T.P. and Y.S.; resources, T.U., S.F., T.P. and Y.S.;
data curation, T.U., S.F. and Y.S.; writing—original draft preparation, T.U., S.F., Y.S. and T.P.; writing—review and
editing, T.U., S.F., Y.S. and T.P.; visualization, T.U.; project administration, S.F. All authors have read and agreed to
the published version of the manuscript.
Funding: This research is supported by National Research Foundation of Ukraine within the project “Methods of
managing the web community in terms of psychological, social and economic inﬂuences on society during the
COVID-19 pandemic”, grant number 94/01-2020.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
References
1.
Bojovi´c, Ž.; Bojovi´c, P.D.; Vujoševi´c, D.; Šuh, J. Education in times of crisis: Rapid transition to distance
learning. Comput. Appl. Eng. Educ. 2020, 28, 1467–1489. [CrossRef]
2.
Benta, D.; Bologa, G.; Dzitac, S.; Dzitac, I. University Level Learning and Teaching via E-Learning Platforms.
Procedia Comput. Sci. 2015, 55, 1366–1373. [CrossRef]
3.
Moreno, R.; Mayer, R. Interactive multimodal learning environments. Educ. Psychol. Rev. 2007, 19, 309–326.
[CrossRef]
4.
Lakshminarayanan, R.; Kumar, B.; Raju, M. Cloud Computing Beneﬁts for Educational Institutions. In Second
International Conference of the Omani Society for Educational Technology.
Available online: https:
//arxiv.org/abs/1305.2616 (accessed on 12 May 2013).
5.
Google Trends. Available online: https://www.sciencedirect.com/science/article/pii/S1877050915015987
(accessed on 2 July 2020).
6.
Bashirov, M.G.; Bashirova, E.M.; Khusnutdinova, I.G.; Luneva, N.N. The technical condition assessment
and the resource of safe operation of technological pipelines using electromagnetic-acoustic eﬀect. In IOP
Conference Series: Materials Science and Engineering; IOP Publishing: Bristol, UK, 2020; Volume 734, p. 012191.
[CrossRef]
7.
Bednarski, L.; Sie´nko, R.; Howiacki, T. Supporting historical structures technical condition assessment by
monitoring of selected physical quantities. Procedia Eng. 2017, 195, 32–39. [CrossRef]
8.
Belodedenko, S.V.; Yatsuba, A.V.; Klimenko, Y.M. Technical condition assessment and prediction of the
survivability of the mill rolls. Metall. Min. Ind. 2015, 7, 85–94.
9.
Błachnio, J. Analysis of technical condition assessment of gas turbine blades with non-destructive methods.
Acta Mech. Autom. 2013, 7, 203–208. [CrossRef]
10.
Fedushko, S.; Ustyianovych, T.; Gregus, M. Real-time high-load infrastructure transaction status output
prediction using operational intelligence and big data technologies. Electronics 2020, 9, 668. [CrossRef]
11.
Belforte, S.; Fisk, I.; Flix, J.; Hernández, M.; Klem, J.; Letts, J.; Magini, N.; Saiz, P.; Sciaba, A. The commissioning
of CMS sites: Improving the site reliability. J. Phys. 2010, 219, 062047. [CrossRef]
12.
Alﬁan, G.; Ijaz, M.F.; Syafrudin, M.; Syaekhoni, M.A.; Fitriyani, N.L.; Rhee, J. Customer behavior analysis
using real-time data processing: A case study of digital signage-based online stores. Asia Pac. J. Mark. and
Logist. 2019, 31, 265–290. [CrossRef]
13.
Alﬁan, G.; Syafrudin, M.; Ijaz, M.F.; Syaekhoni, M.A.; Fitriyani, N.L.; Rhee, J. A personalized healthcare
monitoring system for diabetic patients by utilizing BLE-based sensors and real-time data processing. Sensors
2018, 18, 2183. [CrossRef]
14.
Cegan, L.; Filip, P. Advanced web analytics tool for mouse tracking and real-time data processing.
In Proceedings of the IEEE 14th International Scientiﬁc Conference on Informatics, Informatics 2017–2018,
Poprad, Slovakia, 14–16 November 2017; pp. 431–435.
Appl. Sci. 2020, 10, 9112
15 of 16
15.
Elmsheuser, J.; Legger, F.; Medrano Llamas, R.; Sciacca, G.; Van Der Ster, D. Improving ATLAS grid site
reliability with functional tests using Hammer Cloud. J. Phys. 2012, 396, 032066. [CrossRef]
16.
Kami´nski, K.; Kami´nski, W.; Mizerski, T. Application of artiﬁcial neural networks to the technical condition
assessment of water supply systems. Ecol. Chem. Eng. 2017, 24, 31–40. [CrossRef]
17.
Su, Z.; Yang, Z.; Zhang, X. Tank gun recoil mechanism technical condition assessment based on improved
kernel function SVM. In Proceedings of the IEEE 10th International Conference on Electronic Measurement
and Instruments, ICEMI 2011, Chengdu, China, 16–19 August 2011; Volume 4, pp. 361–363. [CrossRef]
18.
Bosse, S.; Lehmhus, D. Digital real-time data processing with embedded systems. Mater. Integr. Intell. Syst.
Technol. Appl. 2016, 281–300. [CrossRef]
19.
Feldmann, A.; Gasser, O.; Lichtblau, F.; Pujol, E.; Poese, I.; Dietzel, C.; Wagner, D.; Wichtlhuber, M.; Tapidor, J.;
Vallina-Rodriguez, N.; et al. The Lockdown Eﬀect: Implications of the COVID-19 Pandemic on Internet Traﬃc.
In Proceedings of the ACM Internet Measurement Conference, Pittsburgh, PA, USA, 27–29 October 2020.
20.
Google. COVID-19 Community Mobility Report. 2020. Available online: https://www.google.com/covid19/
mobility/ (accessed on 15 December 2020).
21.
Canizo, M.; Conde, A.; Charramendieta, S.; Minon, R.; Cid-Fuentes, R.G.; Onieva, E. Implementation of
a large-scale platform for cyber-physical system real-time monitoring. IEEE Access 2019, 7, 52455–52466.
[CrossRef]
22.
Habeeb, R.A.A.; Nasaruddin, F.; Gani, A.; Hashem, I.A.T.; Ahmed, E.; Imran, M. Real-time big data processing
for anomaly detection: A survey. Int. J. Inf. Manag. 2019, 45, 289–307. [CrossRef]
23.
Chen, Y.; Iyer, S.; Liu, X.; Milojicic, D.; Sahai, A. SLA decomposition: Translating service level objectives
to system level thresholds. In Proceedings of the 4th International Conference on Autonomic Computing
(ICAC’07), Jacksonville, FL, USA, 11–15 June 2007.
24.
Melo, C.; Dantas, J.; Fé, I.; Oliveira, A.; Maciel, P. Synchronization server infrastructure: A relationship
between system downtime and deployment cost. In Proceedings of the International Conference on Systems,
Man, and Cybernetics (SMC), Banﬀ, AB, Canada, 5–8 October 2017; pp. 1250–1255. [CrossRef]
25.
Elliot, S. DevOps and the Cost of Downtime: Fortune 1000 Best Practice Metrics Quantified; International Data
Corporation (IDC): Framingham, MA, USA, 2014; Available online: https://kapost-files-prod.s3.amazonaws.
com/published/54ef73ef2592468e25000438/idc-devops-and-the-cost-of-downtime-fortune-1000-best-practice-
metrics-quantified.pdf (accessed on 15 December 2014).
26.
Izonin, I.; Tkachenko, R.; Kryvinska, N.; Zub, K.; Mishchuk, O.; Lisovych, T. Recovery of Incomplete IoT
Sensed Data using High-Performance Extended-Input Neural-Like Structure. Procedia Comput. Sci. 2019,
160, 521–526. [CrossRef]
27.
Izonin, I.; Kryvinska, N.; Vitynskyi, P.; Tkachenko, R.; Zub, K. GRNN Approach Towards Missing Data
Recovery Between IoT Systems. In Proceedings of the Advances in Intelligent Networking and Collaborative
Systems; Springer: Cham, Switzerland, 2020; pp. 445–453.
28.
Beshley, M.; Kryvinska, N.; Seliuchenko, M.; Beshley, H.; Shakshuki, E.M.; Yasar, A. End-to-End QoS “Smart
Queue” Management Algorithms and Traﬃc Prioritization Mechanisms for Narrow-Band Internet of Things
Services in 4G/5G Networks. Sensors 2020, 20, 2324. [CrossRef]
29.
Poniszewska-Maranda, A.; Matusiak, R.; Kryvinska, N.; Yasar, A.-U.-H. A real-time service system in the
cloud. J. Ambient. Intell. Humaniz. Comput. 2020, 11, 961–977. [CrossRef]
30.
Kryvinska, N.; Bickel, L. Scenario-Based analysis of IT enterprises servitization as a part of digital
transformation of modern economy. Appl. Sci. 2020, 10, 1076. [CrossRef]
31.
Markovets, O.; Pazderska, R.; Horpyniuk, O.; Syerov, Y. Informational support of eﬀective work of the
community manager with web communities. CEUR Workshop Proc. 2020, 2654, 710–722. Available online:
http://ceur-ws.org/Vol-2654/paper55.pdf (accessed on 17 June 2020).
32.
Kaminskyj, R.; Shakhovska, N.; Gregus, M.; Ladanivskyy, B.; Savkiv, L. An Express Algorithm for Transient
Electromagnetic Data Interpretation. Electron. 2020, 9, 354. [CrossRef]
33.
Wurster, L.; Baul, S. Market Share Analysis: ITOM. Perform. Anal. Softw. Worldw. 2019. Available online:
https://www.gartner.com/doc/reprints?id=1-1ZA4D838&ct=200619&st=sb (accessed on 17 June 2020).
34.
Chaudhary, V. Covid-19 & e-learning: Coursera sees massive uptake in courses. April 2020. Available
online: https://www.ﬁnancialexpress.com/education-2/covid-19-e-learning-coursera-sees-massive-uptake-
in-courses/1920127/ (accessed on 6 April 2020).
Appl. Sci. 2020, 10, 9112
16 of 16
35.
Beyer, B.; Jones, C.; Petoﬀ, J.; Murphy, N.R. Site Reliability Engineering: How Google Runs Production Systems;
O’Reilly Media, Inc.: Sebastopol, CA, USA, 2016; p. 552.
36.
Naseer, U.; Niccolini, L.; Pant, U.; Frindell, A.; Dasineni, R.; Benson, T.A. Zero Downtime Release:
Disruption-free Load Balancing of a Multi-Billion User Website. In Proceedings of the Annual Conference of the
ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols
for Computer Communication; Association for Computing Machinery: New York, NY, USA, 2020; pp. 529–541.
Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional
aﬃliations.
© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/).


</subsection_point_Point 1>

<previous_sections>

A systematic review of automated systems for real-time irrigation management

1. INTRODUCTION
The challenge of feeding a growing population with finite resources is becoming increasingly pressing. By 2050, the world population is expected to reach 9.7 billion, necessitating a 70% increase in food production (Falkenmark and Rockstrom, 2009). Irrigation plays a crucial role in enhancing crop yields and agricultural productivity to meet this growing demand. Studies have shown that irrigation can significantly increase crop water productivity, contributing to increased food production (Ali and Talukder, 2008; Playan and Mateos, 2005). However, water scarcity poses a significant challenge, with many regions facing water deficits and the need for improved water management practices (Falkenmark and Rockstrom, 2009). Optimizing irrigation schedules and doses based on crop requirements and environmental conditions is essential for maximizing yield and quality while minimizing water use (Zhang et al., 2024). The necessity of scalable water-efficient practices for increasing food demand cannot be overstated. Techniques such as regulated deficit irrigation, magnetically treated water, and the use of drought-tolerant crops like sorghum have shown promise in improving water productivity and ensuring food security (Mehmood et al., 2023; Putti et al., 2023; Hadebe et al., 2016). As the global food challenge intensifies, it is imperative to critically evaluate the current state and future potential of irrigation management systems to guide research, innovation, and implementation efforts towards fully autonomous, scalable solutions.

Despite the importance of irrigation in addressing the global food challenge, traditional irrigation management techniques, such as manual scheduling and timer-based systems, have significant limitations. These methods are often labor-intensive, inefficient, and less adaptable to changing conditions (Savin et al., 2023). Manual and timer-based scheduling can lead to high operational costs and inefficient water use (Raghavendra, Han, and Shin, 2023). The reliance on manual intervention and predetermined schedules limits their adaptability to changing environmental conditions, crop water requirements, and soil moisture levels (Kaptein et al., 2019). Sensor-based irrigation systems offer an alternative, enabling real-time adjustments based on soil water status measurements (Kaptein et al., 2019). However, the adoption of these systems in commercial settings has been limited, often requiring extensive input from researchers (Kim et al., 2014; Lea-Cox et al., 2018; Ristvey et al., 2018). The limitations of traditional irrigation management techniques highlight the need for scalable, automated solutions for greater efficiency in irrigation management. Automated systems that collect real-time data, analyze it, and make autonomous irrigation decisions can lead to improved water use efficiency and increased crop productivity (Champness et al., 2023; Wu et al., 2022). To fully understand the potential of automated systems, it is necessary to examine the automation of each part of the irrigation management pipeline and analyze the effectiveness and efficiency of integrated end-to-end solutions.

The emergence of smart irrigation management and IoT marks a significant shift from historical irrigation practices. Modern approaches rely on vast data and analysis algorithms, leveraging technologies such as remote sensing, sensor networks, weather data, and computational algorithms (Atanasov, 2023; Bellvert et al., 2023; Kumar et al., 2023). IoT plays a vital role in collecting vast amounts of data through sensors, data transmission, and tailored networks, enabling real-time monitoring and control of irrigation systems (Liakos, 2023; Zuckerman et al., 2024). These advancements in data collection and analysis have the potential to revolutionize irrigation management, allowing for more precise and efficient water use. However, challenges such as processing diverse data sources, data integration, and lack of integrated data analysis hamper the full benefit of IoT in irrigation management (Dave et al., 2023). The current fragmented approach in smart irrigation, focusing on individual components rather than the entire system, limits the potential for fully autonomous, real-time end-to-end irrigation management (Togneri et al., 2021). To address these challenges and fully realize the potential of smart irrigation management, there is a need for automating and integrating each section of the irrigation management pipeline, from sensor/weather data collection and transmission to processing, analysis, decision-making, and automated action (McKinion and Lemmon, 1985). This integration requires a thorough investigation of the role of interoperability and standardization in enabling seamless communication and compatibility between components within the automated irrigation management pipeline.

Machine learning (ML) plays a significant role in processing vast data, predicting plant stress, modeling climate effects, and optimizing irrigation in smart irrigation management systems. ML algorithms can analyze data collected from sensors and weather stations to determine optimal irrigation schedules (Vianny et al., 2022). However, the potential of ML is often constrained by manual steps, such as data interpretation, decision-making on irrigation timing and volume, and system adjustments. Automating ML integration to allow direct action from insights to irrigation execution, removing bottlenecks and achieving real-time adaptability, is crucial for fully autonomous irrigation management (Barzallo-Bertot et al., 2022). By integrating ML into automated systems, the irrigation management pipeline can become more seamless and efficient, enabling real-time decision-making and action based on data-driven insights. To achieve this level of automation and integration, it is essential to identify gaps and propose solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.

To achieve seamless integration across the automated irrigation management system, interoperability and standardization are critical. Interoperability allows different system components, such as sensors, actuators, and software, to communicate and exchange data effectively, while standardization ensures that data is represented in a consistent format (Santos et al., 2020). Standardized protocols and data formats are essential for achieving seamless integration and ensuring compatibility between components in real-time irrigation management systems (Robles et al., 2022; Hatzivasilis et al., 2018). Existing and emerging standards, such as OGC SensorThings API and ISO 11783, have applicability to real-time irrigation management systems (Hazra et al., 2021). However, challenges such as data quality, scalability, reliability, and security need to be addressed to fully realize the potential of interoperability and standardization in automated irrigation management systems (Hazra et al., 2021). Addressing these challenges is crucial for enabling the seamless integration of components within the automated irrigation management pipeline, which is essential for achieving fully autonomous, scalable irrigation management. A comprehensive evaluation of the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline is necessary to guide future research and implementation efforts.
The primary objective of this systematic review is to critically evaluate the current state and future potential of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies for enhancing agricultural water use efficiency and crop productivity.
Specific objectives include:
•	Examining the automation of each part of the irrigation management pipeline and the seamless integration of each section in the context of irrigation scheduling and management.
•	Analyzing the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
•	Investigating the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline.
•	Identifying gaps and proposing solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.
By addressing these objectives, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge.

2. REVIEW METHODOLOGY
•	Question-driven framework to guide the literature review of real-time, autonomous irrigation management systems
•	Key research questions posed, each with the motivation behind investigating them and a starting hypothesis to evaluate against the examined literature
•	Table presenting the major objectives, specific objectives, questions, motivations, and hypotheses
3. DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING
3.1. Irrigation management data
The success of automated irrigation management systems relies heavily on the collection, transmission, and analysis of various types of data. The most applicable data types for irrigation management include soil moisture, canopy temperature, weather data, and plant physiological parameters (Farooq et al., 2019; Li et al., 2019; Olivier et al., 2021; Evett et al., 2020). These data are typically collected from a range of sources, including in-field sensors, remote sensing platforms, weather stations, and manual measurements (Li et al., 2019; Karimi et al., 2018).
Soil moisture data is arguably the most critical type of data for irrigation management, as it directly reflects the water available to plants and can be used to determine the optimal timing and amount of irrigation (Olivier et al., 2021; Intrigliolo & Castel, 2006). Soil moisture sensors, such as tensiometers, capacitance probes, and time-domain reflectometry (TDR) sensors, can provide real-time measurements of soil water content at various depths (Farooq et al., 2019). These sensors can be deployed in a network configuration to capture spatial variability in soil moisture across a field (Karimi et al., 2018).
Canopy temperature data is another valuable type of data for irrigation management, as it can be used to assess plant water stress and adjust irrigation accordingly (Evett et al., 2020). Infrared thermometers and thermal cameras can be used to measure canopy temperature, which is influenced by factors such as air temperature, humidity, wind speed, and plant water status (Li et al., 2019). When plants experience water stress, they tend to close their stomata to reduce water loss, leading to an increase in canopy temperature (Evett et al., 2020). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
Weather data, including temperature, humidity, precipitation, wind speed, and solar radiation, are essential for predicting crop water requirements and scheduling irrigation events (Akilan & Baalamurugan, 2024). Weather stations equipped with various sensors can provide real-time measurements of these parameters, which can be used as inputs for crop water requirement models, such as the FAO-56 Penman-Monteith equation (Li et al., 2019). These models estimate crop evapotranspiration (ET) based on weather data and crop-specific coefficients, allowing for the calculation of irrigation requirements (Intrigliolo & Castel, 2006). By integrating weather data into automated irrigation systems, irrigation schedules can be dynamically adjusted based on changing environmental conditions, ensuring that crops receive the optimal amount of water at the right time (Akilan & Baalamurugan, 2024).
When collecting and utilizing these data types, several considerations must be taken into account, including the volume, frequency, format, and source of the data (Farooq et al., 2019). The volume of data generated by automated irrigation systems can be substantial, especially when high-resolution sensors are deployed at a large scale (Bastidas Pacheco et al., 2022). This necessitates the use of efficient data storage, processing, and transmission technologies to handle the data load (Farooq et al., 2019). The frequency of data collection is another important consideration, as it directly impacts the temporal resolution of the data and the ability to detect rapid changes in plant water status or environmental conditions (Bastidas Pacheco et al., 2022). Bastidas Pacheco et al. (2022) demonstrated that collecting full pulse resolution data from water meters provides more accurate estimates of event occurrence, timing, and features compared to aggregated temporal resolutions, highlighting the importance of selecting appropriate data collection frequencies to ensure the quality and usefulness of the data for irrigation management.
The format of the data is also crucial, as it determines the compatibility and interoperability of the data with various analysis tools and platforms (Farooq et al., 2019). Standardized data formats, such as JSON, XML, or CSV, can facilitate data exchange and integration between different components of the automated irrigation system (Zhang et al., 2023). The source of the data is another important consideration, as it can impact the reliability, accuracy, and spatial coverage of the data (Farooq et al., 2019). For example, in-field sensors provide highly localized measurements, while remote sensing platforms, such as satellites or drones, can provide data at larger spatial scales (Li et al., 2019). By combining data from multiple sources, automated irrigation systems can achieve a more comprehensive understanding of crop water requirements and optimize irrigation management accordingly (Farooq et al., 2019).
Data quality, accuracy, and reliability are paramount in irrigation management, as they directly impact the effectiveness of decision-making processes and the efficiency of water use (Gupta et al., 2020). Inaccurate or unreliable data can lead to suboptimal irrigation decisions, resulting in crop stress, yield losses, or wasted water resources (Ramli & Jabbar, 2022). Gupta et al. (2020) emphasized the critical importance of data security and privacy in smart farming systems, as the leakage of sensitive agricultural data can cause severe economic losses to farmers and compromise the integrity of the automated irrigation system. The authors also highlighted the need for robust authentication and secure communication protocols to prevent unauthorized access to smart farming systems and protect data in transit (Gupta et al., 2020).
Ramli and Jabbar (2022) addressed the challenges associated with implementing real-time, automated irrigation systems, including data quality, scalability, reliability, and security. They proposed solutions and best practices based on the analysis of case studies and real-world implementations, such as the use of redundant sensors, data validation techniques, and secure communication protocols (Ramli & Jabbar, 2022). The authors also emphasized the importance of regular maintenance and calibration of sensors to ensure the accuracy and reliability of the collected data (Ramli & Jabbar, 2022).
Researchers have investigated the use of data compression, aggregation, and filtering techniques to reduce bandwidth requirements and improve transmission efficiency in automated irrigation systems (Karim et al., 2023; Rady et al., 2020; Cui, 2023). Karim et al. (2023) explored the effectiveness of various data compression techniques, such as lossless and lossy compression algorithms, in reducing the size of data packets transmitted over wireless networks. The authors found that lossless compression techniques, such as Huffman coding and Lempel-Ziv-Welch (LZW), can significantly reduce data size without compromising data quality, while lossy compression techniques, such as JPEG and MP3, can further reduce data size by introducing acceptable levels of distortion (Karim et al., 2023).
Rady et al. (2020) developed a novel data compression algorithm specifically designed for irrigation data, which achieved significant compression ratios without compromising data quality. The authors demonstrated that their algorithm could reduce the amount of data transmitted over wireless networks, thereby improving the efficiency of the irrigation system and reducing costs (Rady et al., 2020). Cui (2023) investigated the use of data aggregation and filtering techniques to reduce the number of transmissions and save bandwidth in automated irrigation systems. The author proposed a data aggregation scheme that combines multiple sensor readings into a single value, such as the average soil moisture over a specified time interval, to reduce the frequency of data transmissions (Cui, 2023). Additionally, the author explored the use of data filtering techniques, such as Kalman filters and particle filters, to remove noise and outliers from sensor data, improving the accuracy and reliability of the transmitted information (Cui, 2023).
Data standardization and harmonization are crucial for facilitating seamless integration and interoperability between the various components of automated irrigation management systems (Zhang et al., 2023; Ermoliev et al., 2022). Zhang et al. (2023) developed a novel cyberinformatics technology called iCrop, which enables the in-season monitoring of crop-specific land cover across the contiguous United States. The authors highlighted the importance of data standardization and harmonization in the context of iCrop, as it allows for the efficient distribution of crop-specific land cover information based on the findable, accessible, interoperable, and reusable (FAIR) data principle (Zhang et al., 2023). By adopting standardized data formats and protocols, such as the Open Geospatial Consortium (OGC) standards, iCrop enables the seamless integration of various data sources and facilitates the interoperability of the system with other agricultural decision support tools (Zhang et al., 2023).
Ermoliev et al. (2022) proposed a linkage methodology for linking distributed sectoral/regional optimization models in a situation where private information is not available or cannot be shared by modeling teams. The authors emphasized the need for data standardization to enable decentralized cross-sectoral coordination and analysis, as it allows for the consistent representation and exchange of data between different models and stakeholders (Ermoliev et al., 2022). By adopting standardized data formats and interfaces, the proposed linkage methodology can facilitate the integration of various optimization models and support the development of comprehensive decision support systems for sustainable resource management (Ermoliev et al., 2022).
Metadata plays a vital role in providing context and enabling better data interpretation and decision-making in automated irrigation management systems (Jahanddideh-Tehrani et al., 2021). Metadata refers to the additional information that describes the characteristics, quality, and context of the primary data, such as the sensor type, calibration parameters, measurement units, and timestamp (Jahanddideh-Tehrani et al., 2021). Jahanddideh-Tehrani et al. (2021) highlighted the importance of metadata in water resources management, as it enables decision-makers to use the data to the best of its capabilities by understanding factors such as when water data was collected and what factors might have contributed to the measurements. The authors emphasized the need for standardized metadata formats and guidelines, such as the Dublin Core Metadata Initiative (DCMI) and the ISO 19115 standard, to ensure the consistency and interoperability of metadata across different water information systems (Jahanddideh-Tehrani et al., 2021).
In the context of automated irrigation management systems, metadata can provide valuable information about the data collection process, sensor performance, and environmental conditions that can aid in data interpretation and decision-making (Cota & Mamede, 2023). For example, metadata about the sensor type and calibration parameters can help assess the accuracy and reliability of the collected data, while metadata about the weather conditions and soil properties can provide context for interpreting the data and adjusting irrigation strategies accordingly (Cota & Mamede, 2023). By incorporating metadata into the data management and analysis pipeline of automated irrigation systems, decision-makers can make more informed and context-aware decisions, leading to improved water use efficiency and crop productivity (Jahanddideh-Tehrani et al., 2021).

3.2. Edge Computing and Fog Computing
Edge computing and fog computing have emerged as transformative technologies in the realm of real-time irrigation management systems, offering significant potential for improving efficiency, scalability, and reliability (Abdel Nasser et al., 2020; Tran et al., 2019). Edge computing refers to the practice of processing data near the edge of the network, close to the source of the data, while fog computing is a decentralized computing infrastructure that extends cloud computing capabilities to the network edge (Hassija et al., 2019). These technologies bring computation and analytics closer to the data source, reducing the need for data to travel to the cloud and enabling faster processing and decision-making (Hassija et al., 2019; Zhang et al., 2020).
The potential of edge computing and fog computing in real-time irrigation management is immense. Abdel Nasser et al. (2020) proposed a two-layer system for water demand prediction using automated meters and machine learning techniques, demonstrating the potential of edge computing in improving the efficiency and scalability of irrigation management. The system collects and aggregates data from distributed smart meters in the first layer, while the second layer uses LSTM neural networks to predict water demand for different regions of households. By leveraging edge computing, the system can achieve high accuracy in predicting water demand, which is essential for efficient irrigation management (Abdel Nasser et al., 2020).
Tran et al. (2019) conducted a comprehensive review of real-time, end-to-end automated irrigation management systems, highlighting the role of fog computing in addressing data transmission challenges and enabling seamless integration across the irrigation management pipeline. The authors emphasize that real-time, end-to-end automated irrigation management systems have the potential to significantly improve water efficiency, crop yields, and reduce labor costs. However, they also identify several challenges that need to be addressed, such as data quality, scalability, reliability, and security, which can be effectively tackled by implementing fog computing architectures (Tran et al., 2019).
Edge computing offers several benefits in real-time irrigation management systems, including reduced latency, real-time decision-making, and reduced reliance on cloud connectivity (Mishra, 2020; Zhang et al., 2020). By processing data closer to the source, edge computing enables faster response times and more efficient data handling (Mishra, 2020). Mishra (2020) highlights that edge computing reduces latency by processing data closer to the source, enabling real-time decision-making and lessening reliance on cloud connectivity by shifting processing to local or edge devices.
Zhang et al. (2020) explore the application of edge computing in agricultural settings, demonstrating its potential to improve the efficiency and accuracy of irrigation systems. The authors discuss how edge computing has prospects in various agricultural applications, such as pest identification, safety traceability of agricultural products, unmanned agricultural machinery, agricultural technology promotion, and intelligent management. They also emphasize that the emergence of edge computing models, such as fog computing, cloudlet, and mobile edge computing, has transformed the management and operation of farms (Zhang et al., 2020).
Fog computing plays a crucial role in distributing processing and storage across the network, enhancing the scalability and reliability of automated irrigation systems (Premkumar & Sigappi, 2022; Singh et al., 2022). Premkumar and Sigappi (2022) evaluate the current state of automated irrigation management systems and propose a hybrid machine learning approach for predicting soil moisture and managing irrigation. Their study emphasizes the potential of fog computing in distributing processing and storage across the network, improving the efficiency and scalability of irrigation systems. The proposed hybrid machine learning approach outperforms other machine learning algorithms in predicting soil moisture, demonstrating the effectiveness of fog computing in enhancing the performance of automated irrigation systems (Premkumar & Sigappi, 2022).
Singh et al. (2022) discuss the role of fog computing in distributing processing and storage across the network, enhancing scalability and reliability in agricultural management systems. The authors argue that by implementing fog computing, these systems can achieve faster data processing and response times, improving overall efficiency and effectiveness. They also highlight that fog computing can address the challenges faced by real-time data transmission in agricultural management systems, such as latency, bandwidth limitations, and data security (Singh et al., 2022).
The integration of edge and fog computing in real-time irrigation management systems is crucial for achieving fully automated, scalable, and reliable solutions. As the demand for autonomous irrigation management grows, these technologies will play a pivotal role in enabling faster decision-making, reduced latency, improved resource utilization, and seamless integration across the irrigation management pipeline (Tran et al., 2019; Zhang et al., 2020). By bringing computation and analytics closer to the data source and distributing processing and storage across the network, edge and fog computing can significantly enhance the efficiency and effectiveness of automated irrigation systems, contributing to the overall goal of addressing the global food challenge through optimized water resource management and increased agricultural productivity (Abdel Nasser et al., 2020; Premkumar & Sigappi, 2022; Singh et al., 2022).

3.3. Automation of Data Collection
The automation of data collection is a critical component in the development of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. It enables the efficient gathering of vital information about crop health, environmental conditions, and water requirements, which is essential for enhancing agricultural water use efficiency and crop productivity. Two key aspects of automated data collection are the use of advanced sensing technologies for non-invasive plant stress detection and the implementation of wireless sensor networks and energy-efficient communication protocols for large-scale, long-term data collection.
Advanced sensing technologies, such as hyperspectral imaging and thermal sensing, have emerged as powerful tools for non-invasive plant stress detection in automated irrigation management systems. These technologies provide valuable information about crop traits, enabling early and accurate detection of plant health issues (Triantafyllou et al., 2019). Triantafyllou et al. (2019) propose a comprehensive reference architecture model that incorporates advanced sensing technologies in the sensor layer for real-time plant stress detection, highlighting their importance in providing non-invasive plant stress detection. Similarly, Hossain et al. (2023) present a novel IoT-ML-Blockchain integrated framework for smart agricultural management that leverages advanced sensing technologies to optimize water use and improve crop yield, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
Hyperspectral imaging can capture subtle changes in plant physiology that are indicative of stress, while machine learning algorithms can be employed to extract meaningful patterns from the spectral data and classify different stress types (Araus et al., 2014). Thermal sensing can detect changes in canopy temperature, which is influenced by factors such as plant water status (Li et al., 2019). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
The integration of advanced sensing technologies in automated irrigation management systems has the potential to revolutionize precision agriculture. Jiang et al. (2019) demonstrate the effectiveness of a deep learning-based model in accurately detecting leaf spot diseases, highlighting the importance of image augmentation and deep learning algorithms in enhancing the model's performance.
Wireless sensor networks (WSNs) and energy-efficient communication protocols have the potential to significantly improve the efficiency and reliability of data collection in large-scale, long-term irrigation systems. WSNs offer a cost-effective and scalable solution for real-time data collection in large-scale irrigation systems, providing remote monitoring and automated control capabilities (Mehdizadeh et al., 2020). Nishiura and Yamamoto (2021) propose a novel sensor network system that utilizes drones and wireless power transfer to autonomously collect environmental data from sensor nodes in vast agricultural fields, reducing operational costs and enhancing the efficiency of data collection. Similarly, Higashiura and Yamamoto (2021) introduce a network system that employs UAVs and LoRa communication to efficiently collect environmental data from sensor nodes distributed across large farmlands, optimizing data collection and reducing travel distance and time.
Energy-efficient communication protocols are crucial for ensuring reliable data transmission in challenging environmental conditions and extending the lifespan of sensor nodes (Mehdizadeh et al., 2020). Al-Ali et al. (2023) investigate the potential of WSNs and energy-efficient communication protocols for data collection in large-scale, long-term irrigation systems, discussing the challenges and opportunities of using these technologies to improve the efficiency and reliability of real-time data collection in irrigation management. Mehdizadeh et al. (2020) emphasize the need for careful consideration of factors such as data accuracy, energy consumption, and network reliability when designing effective WSNs for irrigation management, enabling timely irrigation decisions and improved crop yields.
The automation of data collection through the use of advanced sensing technologies and wireless sensor networks is essential for achieving fully autonomous, scalable irrigation management. By enabling non-invasive plant stress detection and large-scale, long-term data collection, these technologies contribute to the overall goal of optimizing water resource management and increasing agricultural productivity. The integration of these technologies in real-time, end-to-end automated irrigation management systems has the potential to enhance agricultural water use efficiency and crop productivity, ultimately contributing to the development of fully autonomous, scalable irrigation management solutions.

3.4: Real-Time Data Transmission Protocols and Technologies
Real-time data transmission is a critical component of automated irrigation management systems, as it enables the timely delivery of sensor data to the cloud for processing and decision-making. The exploration of suitable protocols and network architectures is essential for ensuring efficient and reliable data transmission in these systems, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
The Message Queuing Telemetry Transport (MQTT) protocol has emerged as a popular choice for real-time data transmission in IoT networks, including those used for automated irrigation management. MQTT is a lightweight, publish-subscribe protocol designed for constrained devices and low-bandwidth networks (Author, 2019). Its simplicity and low overhead make it well-suited for IoT applications where data transmission speed and energy efficiency are critical (Saranyadevi et al., 2022). MQTT provides three Quality of Service (QoS) levels, ensuring data reliability in real-time scenarios (Author, 2019). Chen et al. (2020) proposed novel algorithms to improve data exchange efficiency and handle rerouting in MQTT-based IoT networks for automated irrigation management systems. Their TBRouting algorithm efficiently finds the shortest paths for data transmission, while the Rerouting algorithm effectively handles the rerouting of topic-based session flows when a broker crashes. The combination of these algorithms can significantly improve the performance and reliability of automated irrigation management systems (Chen et al., 2020).
Client-server IoT networks, such as those based on MQTT, play a crucial role in real-time data transmission for automated irrigation management systems. In these networks, sensors and devices (clients) publish data to a central broker (server), which then distributes the data to subscribed clients (Verma et al., 2021). This architecture enables efficient data collection, processing, and dissemination, facilitating the integration of various components within the automated irrigation management pipeline. Verma et al. (2021) proposed an architecture for healthcare monitoring systems using IoT and communication protocols, which provides a comprehensive overview of existing approaches and highlights challenges and opportunities in the field. Although focused on healthcare, the insights from this study can be applied to automated irrigation management systems, emphasizing the importance of interoperability and standardization for seamless integration (Verma et al., 2021).
In addition to MQTT, other application layer protocols such as XMPP, CoAP, SOAP, and HTTP have been explored for real-time data transmission in IoT networks. Each protocol has its strengths and weaknesses, making them suitable for different applications and scenarios. XMPP (Extensible Messaging and Presence Protocol) is an open-standard protocol that supports real-time messaging, presence, and request-response services (Saint-Andre, 2011). CoAP (Constrained Application Protocol) is a specialized web transfer protocol designed for use with constrained nodes and networks in the Internet of Things (Shelby et al., 2014). SOAP (Simple Object Access Protocol) is a protocol for exchanging structured information in the implementation of web services, while HTTP (Hypertext Transfer Protocol) is the foundation of data communication for the World Wide Web (Fielding et al., 1999).
Motamedi and Villányi (2022) compared and evaluated wireless communication protocols for the implementation of smart irrigation systems in greenhouses, considering factors such as power consumption, range, reliability, and scalability. They found that ZigBee is the most suitable local communication protocol for greenhouse irrigation due to its large number of nodes and long range, while MQTT is the recommended messaging protocol for smart irrigation systems due to its TCP transport protocol and quality of service (QoS) options. GSM is a reliable and cost-effective global communication protocol for greenhouse irrigation, providing wide coverage and low cost (Motamedi & Villányi, 2022).
Syafarinda et al. (2018) investigated the use of the MQTT protocol in a precision agriculture system using a Wireless Sensor Network (WSN). They found that MQTT is suitable for use in IoT applications due to its lightweight, simple, and low bandwidth requirements. The average data transmission speed using the MQTT protocol was approximately 1 second, demonstrating its effectiveness for real-time data transmission in precision agriculture systems (Syafarinda et al., 2018).
The choice of application layer protocol for real-time irrigation management depends on factors such as data transmission speed, reliability, and energy efficiency. MQTT and RTPS (Real-Time Publish-Subscribe) are both suitable for real-time data transmission in IoT systems, but they have different strengths and weaknesses. MQTT is a better choice for applications that require low latency and high throughput, while RTPS is a better choice for applications that require high reliability and low latency (Sanchez-Iborra & Skarmeta, 2021). The exploration of MQTT and client-server IoT networks, along with the comparison of various application layer protocols, provides valuable insights into the suitability of these technologies for real-time data transmission in automated irrigation management systems.
In summary, real-time data transmission protocols and technologies play a vital role in the automation of irrigation management systems, enabling the efficient and reliable delivery of sensor data to the cloud for processing and decision-making. The exploration of MQTT and client-server IoT networks, along with the comparison of application layer protocols, highlights the importance of selecting suitable technologies based on factors such as data transmission speed, reliability, and energy efficiency. By leveraging these technologies, automated irrigation management systems can achieve seamless integration and contribute to the overall goal of enhancing agricultural water use efficiency and crop productivity.

3.5. Challenges and Solutions in Real-Time Data Transmission
Following the exploration of data collection, processing at the edge and fog, and automation in previous sections, we now turn to the critical aspect of real-time data transmission. While essential for automated irrigation management, this stage presents unique challenges that must be addressed to ensure system efficiency and reliability.
Obstacles in Real-Time Data Transmission
Agricultural environments present unique challenges for real-time data transmission, directly impacting the effectiveness of automated irrigation systems. Environmental factors can significantly disrupt wireless communication. Adverse weather conditions such as heavy rain, fog, and high winds can weaken or even block radio signals, leading to data loss and compromised system performance. Physical obstacles like trees, buildings, and uneven terrain further complicate signal propagation, creating reliability issues (Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). These environmental challenges necessitate robust communication protocols and network architectures that can ensure consistent and reliable data flow.
In addition to environmental factors, technical limitations also present significant obstacles. Large-scale agricultural operations often demand long-distance data transmission, which can be hindered by the limited range of certain wireless communication protocols. Network congestion, occurring when multiple sensors transmit data concurrently, can lead to delays and potential data loss, further complicating real-time decision-making (Hameed et al., 2020). To mitigate these issues, researchers have investigated the potential of cognitive radio networks (CRNs) and dynamic spectrum access (DSA) for optimizing spectrum utilization and reducing interference (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). CRNs enable devices to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency. DSA, on the other hand, facilitates the dynamic allocation of unused spectrum, enhancing spectrum utilization and reducing congestion.
Furthermore, data security and privacy are paramount concerns in real-time irrigation systems. The sensitive nature of agricultural data, such as crop yields and farm management practices, necessitates robust security measures to prevent unauthorized access and data breaches (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential to protect data integrity and ensure the trustworthiness of the system.
Investigating Data Optimization Techniques
To enhance the efficiency and reliability of real-time data transmission in automated irrigation systems, researchers have explored a range of data optimization techniques. Data compression techniques aim to reduce the size of data packets transmitted over the network, minimizing bandwidth requirements and improving transmission speed (Rady et al., 2020; Karim et al., 2023). Lossless compression algorithms, such as Huffman coding and LZW, preserve data integrity while effectively reducing data size, ensuring that no information is lost during transmission (Cui, 2023). Lossy compression algorithms, such as JPEG and MP3, offer higher compression ratios but introduce a controlled level of data loss, which may be acceptable for certain applications where some loss of precision is tolerable (Karim et al., 2023). The choice between lossless and lossy compression depends on the specific application and the trade-off between data size and accuracy.
Data aggregation techniques provide another effective approach to optimize data transmission. By aggregating multiple sensor readings into a single representative value, such as average soil moisture or temperature, the number of transmissions can be significantly reduced, conserving bandwidth and energy resources (Cui, 2023). This is particularly beneficial in large-scale irrigation systems where numerous sensors are deployed across vast areas, generating substantial amounts of data. Additionally, data filtering techniques play a crucial role in improving data quality and reliability. Kalman filters and particle filters can effectively remove noise and outliers from sensor data, ensuring that only accurate and relevant information is transmitted and used for decision-making (Cui, 2023). This is essential for preventing erroneous data from influencing irrigation decisions and potentially leading to suboptimal water management.
Sensor calibration, drift correction, and fault detection are essential for maintaining data accuracy and reliability (Dos Santos et al., 2023). Regular calibration ensures that sensors provide accurate measurements over time, while drift correction techniques account for gradual changes in sensor readings due to environmental factors or aging. Fault detection mechanisms can identify and address sensor malfunctions or anomalies, preventing erroneous data from influencing irrigation decisions and potentially harming crops or wasting water.
Addressing the Challenges
Effectively addressing the challenges in real-time data transmission requires a multifaceted approach that encompasses environmental, technical, and data-related considerations. Implementing robust and adaptive communication protocols is crucial for overcoming interference and signal degradation caused by weather conditions and physical obstacles. Selecting appropriate protocols, such as LoRa or ZigBee, with suitable range and penetration capabilities can ensure reliable data transmission in challenging agricultural environments (Motamedi & Villányi, 2022). Additionally, employing techniques like frequency hopping and error correction codes can further improve communication resilience and mitigate data loss.
Optimizing network architecture is another key consideration. Deploying a distributed network architecture with edge and fog computing capabilities can significantly enhance data processing and transmission efficiency (Abdel Nasser et al., 2020; Tran et al., 2019). Edge devices can perform initial data processing and aggregation tasks, reducing the amount of data transmitted to the cloud and minimizing latency, while fog nodes can provide additional processing power and storage closer to the data source, enhancing scalability and reliability. This distributed approach alleviates the burden on the central cloud server and allows for more responsive and efficient irrigation management.
Data optimization techniques play a vital role in reducing bandwidth requirements and improving transmission efficiency. The choice of data compression, aggregation, and filtering techniques should be tailored to the specific requirements of the irrigation system, considering factors such as data type, accuracy needs, and available bandwidth. By carefully selecting and implementing these techniques, the overall performance and effectiveness of real-time irrigation systems can be significantly enhanced, leading to more sustainable water management practices and improved agricultural productivity.
By addressing these challenges and implementing appropriate solutions, real-time data transmission can become a reliable and efficient component of automated irrigation systems, contributing to the overall goal of achieving sustainable and productive agriculture in the face of growing food demands and water scarcity.

3.6. IoT Network Architectures and Variable Rate Irrigation (VRI) for Real-Time Irrigation
Real-time irrigation management systems heavily rely on the efficient and reliable transmission of data from sensors and weather stations to the cloud for processing and decision-making. However, agricultural environments present unique challenges to wireless communication, including adverse weather conditions, physical obstacles, and the limitations of wireless technologies. These challenges necessitate robust and adaptive solutions to ensure the consistent and timely flow of data, enabling truly autonomous irrigation scheduling.
Environmental factors, such as heavy rain, fog, and strong winds, can significantly disrupt wireless communication by attenuating or even blocking radio signals, leading to data loss and compromised system performance (Ed-daoudi et al., 2023; Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). Dense vegetation, buildings, and uneven terrain create further complications by causing multipath propagation and shadowing effects (Yim et al., 2018; Gautam and Pagay, 2020). The study by Yim et al. (2018) on LoRa networks in a tree farm environment exemplifies these challenges, revealing reduced communication range and data reliability compared to theoretical expectations. This underscores the need for carefully selecting and optimizing communication protocols and network parameters to ensure reliable data transmission in such environments.
The study by Guzinski et al. (2014a) using a modified TSEB model further highlights the importance of high-resolution data in accurately capturing the spatial and temporal dynamics of energy fluxes influenced by environmental factors. This emphasizes the need for advanced data acquisition and processing techniques that can effectively represent the complexities of agricultural settings.
The limitations of traditional wireless communication technologies, such as limited range and network congestion, pose additional challenges for large-scale agricultural operations. Long-distance data transmission can be hindered by range limitations, while network congestion arising from numerous sensors transmitting concurrently can lead to delays and data loss, hindering real-time decision-making (Hameed et al., 2020). Addressing these challenges requires the exploration of advanced networking technologies that can optimize spectrum utilization, mitigate interference, and improve reliability and efficiency.
Cognitive Radio Networks (CRNs) and Dynamic Spectrum Access (DSA) offer promising solutions for optimizing wireless communication in agricultural settings. CRNs empower devices with the ability to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). Research has explored the potential of CRNs in predicting Radio Frequency (RF) power to avoid noisy channels and optimize spectrum utilization (Iliya et al., 2014; Iliya et al., 2014). These studies demonstrate the effectiveness of combining optimization algorithms with artificial neural networks (ANNs) to enhance the accuracy and generalization of RF power prediction, enabling CRNs to make informed decisions about channel selection and avoid interference.
DSA complements CRN technology by dynamically allocating unused spectrum, further enhancing spectrum utilization and reducing congestion (Shi et al., 2023). The numerical model developed by Shi et al. (2023) showcases the potential of CRNs and DSA for optimizing wireless communication in challenging environments.
The integration of CRNs and DSA into the IoT network architecture requires careful consideration of spectrum sensing techniques, network topology, and data security. Research on cooperative spectrum sensing suggests that distributed approaches, where sensor nodes collaborate and share information, can significantly improve the accuracy and efficiency of spectrum sensing, particularly in dynamic environments (Trigka and Dritsas, 2022; Khalid & Yu, 2019). This collaborative approach enables a more comprehensive understanding of the radio environment and facilitates the identification of available frequency bands for data transmission.
The choice of network topology also impacts the performance and scalability of CRN-based irrigation systems. Mesh networks, where sensor nodes are interconnected and relay data for each other, offer enhanced resilience and coverage compared to star topologies where nodes communicate directly with a central gateway (Akyildiz & Vuran, 2010). However, mesh networks can be more complex to manage and may introduce additional routing overhead. The trade-off between network resilience and complexity needs to be carefully evaluated to select the most appropriate topology for a specific agricultural setting.
Data security and privacy are paramount concerns in IoT-based irrigation systems due to the sensitive nature of agricultural data (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential for protecting data integrity and ensuring system trustworthiness. Research on secure spectrum leasing and resource allocation algorithms for CR-WSN-based irrigation systems has demonstrated the potential of these technologies for enhancing security and efficiency (Hassan, 2023; Afghah et al., 2018).
In conclusion, the development of effective and reliable real-time irrigation management systems requires a comprehensive approach that addresses the challenges of data transmission in agricultural environments. The integration of robust and adaptive communication protocols, optimized network architectures, and advanced networking technologies like CRNs and DSA, along with a focus on data security and privacy, can contribute significantly to achieving the goal of autonomous and efficient irrigation scheduling.
4. AUTOMATED DATA PROCESSING IN THE CLOUD
4.1. Data Quality and Preprocessing
Data quality is paramount in automated irrigation systems as it directly influences the effectiveness of decision-making and water use efficiency. Issues like missing values, inconsistencies, and outliers arising from sensor malfunctions, environmental interference, or network problems (Lv et al., 2023) can significantly impact the performance of machine learning models used for irrigation scheduling and management.
Real-time data cleaning techniques are essential for addressing these challenges. Kalman filtering proves particularly effective in handling missing values and correcting erroneous readings by recursively estimating the system's state based on previous measurements and current sensor data, taking into account noise and uncertainty (Kim et al., 2020). Moving average techniques, by averaging consecutive data points, provide a more stable representation of the underlying trend, filtering out short-term fluctuations (Chhetri, 2023). For outlier detection, adaptive thresholding methods offer a dynamic approach, adjusting thresholds based on the statistical properties of the data to effectively identify anomalies and minimize false positives (Bah et al., 2021). These techniques are crucial in maintaining the integrity of real-time data streams and ensuring the accuracy of subsequent analyses.
Adaptive data preprocessing is essential for managing the diversity of data sources and formats commonly found in irrigation systems. Data normalization techniques, such as min-max scaling or z-score normalization, ensure that all features contribute equally to the analysis by transforming data values to a common scale (Pradal et al., 2016). This is crucial for preventing features with larger values from dominating the analysis and ensuring that all features are given equal consideration. Similarly, feature scaling methods, like standardization or normalization, optimize the range of feature values to improve the performance and convergence of machine learning models (Tortorici et al., 2024). By scaling features to a similar range, the influence of outliers is reduced, and the model's ability to learn from the data is enhanced.
Data fusion techniques play a critical role in integrating information from diverse sources, creating a more comprehensive and reliable dataset for irrigation management. Dempster-Shafer theory, a generalization of probability theory, allows for the expression of both uncertainty and the degree of conflict in evidence, making it suitable for fusing uncertain and conflicting data from heterogeneous sources (Sadiq and Rodriguez, 2004). This is particularly relevant in irrigation systems where data from different sensors may provide slightly different or even contradictory information due to sensor variations or environmental factors. Bayesian inference offers another powerful framework for combining information from multiple sources, updating the probability of a hypothesis as new evidence becomes available. By applying these techniques, data from soil moisture sensors, canopy temperature sensors, weather stations, and other sources can be integrated to provide a holistic understanding of crop water requirements and environmental conditions, leading to more informed and accurate irrigation decisions.
The impact of data quality extends beyond model accuracy to the robustness of machine learning models under varying conditions. Robust models should maintain consistent performance even when faced with data inconsistencies or unexpected situations. Techniques like data augmentation and domain adaptation can enhance model robustness by exposing the model to a wider range of data variations during training. Data augmentation involves generating additional training data by applying transformations or introducing noise to existing data, making the model more resilient to noise and variations in the real-world data. Domain adaptation techniques aim to adapt a model trained on one domain (e.g., a specific crop or geographic location) to perform well on another domain with different data characteristics. This is particularly relevant in irrigation management, where models may need to be applied to different crops, soil types, or climatic conditions.
The choice of data cleaning, preprocessing, and fusion techniques should be carefully considered based on the specific characteristics of the irrigation system and the available data. By selecting and implementing appropriate techniques, the accuracy, reliability, and robustness of machine learning models can be significantly improved, leading to more efficient and sustainable irrigation management practices.
4.2. Scalable and Autonomous Deployment using Containerization Strategies
The transition from data collection and transmission to efficient data processing requires a robust infrastructure capable of handling diverse workloads and data volumes. Containerization technologies, specifically Docker and Kubernetes, offer a promising solution for deploying and scaling data processing and machine learning modules within cloud environments like AWS, Azure, and GCP (Vargas-Rojas et al., 2024; Rosendo et al., 2022; Solayman & Qasha, 2023). Docker provides a standardized way to package applications and their dependencies into self-contained units known as containers, ensuring consistent and reproducible execution across different platforms (Rosendo et al., 2022). Kubernetes, acting as a container orchestrator, manages their deployment, scaling, and networking across a cluster of machines (Rosendo et al., 2022). This combination presents several advantages for automated irrigation management systems.
Firstly, containerization facilitates efficient resource utilization and scalability. By encapsulating applications and their dependencies, containers enable the isolation of resources and prevent conflicts between different modules (Vargas-Rojas et al., 2024; Solayman & Qasha, 2023). This isolation allows for the efficient allocation of resources, such as CPU, memory, and storage, to each container based on its specific needs. Kubernetes further enhances scalability by allowing for the automatic scaling of containers based on real-time demand, ensuring the system can adapt to varying workloads and data volumes, preventing bottlenecks, and ensuring responsiveness to changing conditions (Karamolegkos et al., 2023).
Secondly, containerization promotes portability and reproducibility. By packaging applications and their dependencies into a single unit, containers make it easy to move and deploy them across different cloud environments without the need for environment-specific configurations (Rosendo et al., 2022; Solayman & Qasha, 2023). This portability simplifies the development and deployment process, reducing the time and effort required to set up and manage the system. Additionally, containers ensure reproducibility by providing a consistent execution environment, regardless of the underlying infrastructure. This eliminates variability and ensures that the system will behave consistently across different deployments (Zhou et al., 2023).
Optimizing container orchestration and resource allocation is crucial to minimizing latency and maximizing throughput in real-time data processing pipelines. Techniques like auto-scaling and dynamic resource allocation play a critical role in this context (Hethcoat et al., 2024; Werner and Tai, 2023; Kumar et al., 2024). Auto-scaling automatically adjusts the number of container instances based on real-time demand, ensuring that sufficient resources are available to handle peak workloads while avoiding over-provisioning during periods of low demand (Hethcoat et al., 2024; Kumar et al., 2024). Dynamic resource allocation enables the fine-grained adjustment of resources allocated to each container based on its specific needs and the current workload (Werner and Tai, 2023). This ensures efficient resource allocation and provides each container with the necessary resources to perform its tasks effectively.
Performance monitoring tools, such as Kubernetes Metrics Server and Prometheus, are essential for gaining insights into the performance of containers and the overall system (Hethcoat et al., 2024; Kuity & Peddoju, 2023). These tools provide valuable data on key performance indicators, such as CPU and memory usage, network traffic, and application-specific metrics. By monitoring this data, administrators can identify bottlenecks, optimize resource allocation strategies, and continuously improve system performance (Hethcoat et al., 2024). This data-driven approach ensures that automated irrigation management systems can operate efficiently and reliably.
By integrating containerization technologies with optimization techniques and performance monitoring, automated irrigation management systems achieve the scalability, autonomy, and efficiency required for effective real-time data processing and decision-making. This approach facilitates a seamless and responsive system that can adapt to changing conditions and contribute to the overall goal of optimizing water resource management and increasing agricultural productivity.
4.3. Deploying ML Models for Data Processing
•	Architectures and frameworks for deploying machine learning models on cloud platforms for real-time data processing and inference in irrigation management systems, such as: TensorFlow Serving, Apache MXNet Model Server, ONNX Runtime
•	Techniques for optimizing machine learning model performance and resource utilization in cloud environments, such as: Model compression (e.g., pruning, quantization), Hardware acceleration (e.g., GPU, TPU), Distributed training (e.g., Horovod, BytePS)
•	Integration of deployed machine learning models with other components of the automated irrigation management pipeline, such as data preprocessing, decision-making, and control systems, using protocols like: MQTT, CoAP, RESTful APIs
4.4. Online Learning in the Cloud
•	Application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as: Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests
•	Architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as: Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms
•	Strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making, using techniques such as: Multi-armed bandits, Bayesian optimization, Reinforcement learning (e.g., Q-learning, SARSA)


5. GENERATING AND APPLYING IRRIGATION INSIGHTS 
5.1. Real-Time Generation of Actionable Irrigation Insights
•	Advanced predictive models, such as deep learning (e.g., LSTM, CNN) and ensemble methods (e.g., Random Forests), for precise, site-specific irrigation recommendations
•	Integration of IoT sensor data (e.g., soil moisture probes, weather stations) and cloud-based data sources (e.g., weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filtering) to enhance insight accuracy and resolution
•	Strategies for handling data heterogeneity, uncertainty, and quality issues in real-time insight generation, such as data preprocessing and outlier detection
•	Techniques for reducing computational complexity and latency, such as edge computing (e.g., fog computing), model compression (e.g., quantization), and hardware accelerators (e.g., GPUs)
5.2. Automated Application of Irrigation Insights
•	Architectures and protocols for seamless integration of ML-generated insights with IoT-enabled irrigation control systems, such as MQTT and CoAP for lightweight, real-time communication
•	Analysis of industry-leading products and services, such as smart irrigation controllers (e.g., Rachio) and cloud-based irrigation management platforms (e.g., CropX)
•	Strategies for ensuring reliability, security, and scalability of automated insight application, such as redundant communication channels and secure edge-to-cloud architectures
•	Case studies of successful implementations of closed-loop, autonomous irrigation systems in research and commercial settings, highlighting technologies used and benefits achieved

6. INTEGRATION, INTEROPERABILITY, AND STANDARDIZATION 
6.1. Interoperability and Standardization
•	Importance of interoperability and standardization in enabling seamless integration of automated irrigation components
•	Overview of existing and emerging standards for IoT devices, communication protocols, and data formats in precision agriculture (e.g., ISOBUS, agroXML, SensorML)
•	Role of standardization bodies and industry consortia in promoting interoperability (e.g., AgGateway, Open Ag Data Alliance, Agricultural Industry Electronics Foundation)
•	Challenges in adopting and implementing standards across diverse hardware and software platforms
•	Strategies for encouraging widespread adoption of standards and best practices for interoperability in automated irrigation systems
6.2. Integration with Existing Irrigation Infrastructure
•	Challenges and strategies for retrofitting legacy irrigation systems with IoT sensors, actuators, and communication devices
•	Hardware compatibility issues and solutions (e.g., adapters, modular designs)
•	Software and firmware updates to enable integration with automated decision-making systems
•	Data integration and normalization techniques for merging legacy and new data sources
•	Economic and practical considerations for transitioning from manual to automated irrigation management
•	Cost-benefit analysis of upgrading existing infrastructure vs. implementing new systems
•	Phased implementation approaches to minimize disruption and optimize resource allocation
•	Training and support requirements for farmers and irrigation managers adopting automated systems
•	Case studies and real-world examples of successful integration of automated irrigation with existing infrastructure
6.3. Integration with Other Precision Agriculture Technologies
•	Synergies between automated irrigation and complementary technologies
•	Remote sensing (satellite, UAV, and ground-based) for crop monitoring and evapotranspiration estimation
•	Soil moisture sensors and weather stations for real-time, localized data collection
•	Variable rate application systems for precise irrigation delivery based on crop requirements
•	Yield mapping and analytics for assessing the impact of automated irrigation on crop productivity
•	Architectures and frameworks for integrating diverse data sources and technologies into a unified precision agriculture ecosystem
•	Edge computing and fog computing paradigms for real-time data processing and decision-making
•	Cloud-based platforms for data storage, analysis, and visualization
•	API-driven approaches for modular integration of third-party services and applications
•	Challenges and solutions for ensuring data quality, consistency, and security across integrated precision agriculture systems
•	Data cleaning, preprocessing, and harmonization techniques
•	Blockchain and distributed ledger technologies for secure, tamper-proof data sharing and traceability
•	Access control and authentication mechanisms for protecting sensitive data and resources
•	Future trends and research directions in the integration of automated irrigation with advanced precision agriculture technologies (e.g., AI-driven crop modeling, robotics, and autonomous vehicles)
6.4. Cybersecurity Considerations for Integrated Automated Irrigation Systems
•	Unique security risks and vulnerabilities associated with IoT-based automated irrigation systems
•	Potential for unauthorized access, data tampering, and system manipulation
•	Implications of security breaches for crop health, water resource management, and farm productivity
•	Best practices and strategies for securing automated irrigation systems
•	Secure device provisioning and authentication (e.g., hardware security modules, certificates)
•	Encryption and secure communication protocols (e.g., TLS, DTLS)
•	Firmware and software updates to address emerging security threats
•	Network segmentation and access control to limit the impact of breaches
•	Role of cybersecurity standards and frameworks in guiding the development and deployment of secure automated irrigation systems (e.g., NIST CSF, IEC 62443)
•	Importance of user awareness, training, and incident response planning in maintaining the security of integrated automated irrigation systems

7. MONITORING AND ENSURING SYSTEM RELIABILITY
7.1. Resilience and Fault Tolerance in Automated Irrigation Systems
•	Strategies for ensuring robustness and reliability in the face of failures, disruptions, or unexpected events
•	Redundancy: Implementing redundant components, such as duplicate sensors (e.g., soil moisture sensors, weather stations), controllers (e.g., PLCs, microcontrollers), and communication channels (e.g., cellular, satellite, LoRaWAN) to maintain system functionality during component failures
•	Failover mechanisms: Designing seamless failover mechanisms that automatically switch to backup components or systems in case of primary system failure, such as hot-standby controllers or multi-path communication protocols (e.g., mesh networks, software-defined networking)
•	Self-healing capabilities: Incorporating AI-driven self-healing mechanisms that can detect, diagnose, and recover from faults without human intervention, using techniques like reinforcement learning, Bayesian networks, or self-organizing maps
•	The role of distributed architectures and edge computing in enhancing system resilience
•	Decentralizing critical functions and data processing to minimize the impact of single points of failure, using fog computing or multi-agent systems
•	Leveraging edge computing to enable localized decision-making and control, reducing dependence on cloud connectivity and improving response times, using technologies like Raspberry Pi, NVIDIA Jetson, or Intel NUC
•	Anomaly detection and predictive maintenance using AI techniques
•	Employing unsupervised learning algorithms (e.g., autoencoders, clustering) to detect anomalies in sensor data, system performance, and water usage patterns
•	Developing predictive maintenance models using techniques like long short-term memory (LSTM) networks, convolutional neural networks (CNNs), or gradient boosting machines (GBMs) to anticipate and prevent potential system failures based on historical data and real-time monitoring
7.2. Advanced Monitoring Techniques for Automated Irrigation Systems
•	Remote monitoring using IoT-enabled sensors and computer vision
•	Deploying a heterogeneous network of IoT sensors to collect real-time data on soil moisture (e.g., capacitive, tensiometric), temperature (e.g., thermocouples, thermistors), humidity (e.g., capacitive, resistive), and plant health (e.g., sap flow, leaf wetness)
•	Integrating high-resolution cameras (e.g., multispectral, hyperspectral) and computer vision algorithms for visual monitoring of crop growth, disease detection (e.g., using deep learning-based object detection and segmentation), and irrigation system performance (e.g., leak detection, sprinkler uniformity)
•	Transmitting sensor and camera data to cloud-based platforms (e.g., AWS IoT, Google Cloud IoT, Microsoft Azure IoT) for remote access and analysis using protocols like MQTT, CoAP, or AMQP
•	Innovative approaches for real-time system health assessment
•	Developing novel algorithms and metrics for evaluating the health and performance of automated irrigation systems, such as entropy-based measures, network resilience indices, or multi-criteria decision analysis (MCDA) frameworks
•	Combining data from multiple sources (e.g., sensors, weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filters, Dempster-Shafer theory) to create a comprehensive view of system health
•	Employing advanced data visualization techniques (e.g., interactive dashboards, augmented reality) to present system health information in an intuitive and actionable format
7.3. Closed-Loop Control and Feedback Mechanisms
•	Exploring the concept of closed-loop control in autonomous irrigation systems
•	Implementing feedback loops that continuously monitor system performance and adjust irrigation schedules based on real-time data, using control techniques like proportional-integral-derivative (PID), model predictive control (MPC), or fuzzy logic control (FLC)
•	Integrating machine learning algorithms (e.g., reinforcement learning, genetic algorithms) to optimize closed-loop control strategies over time, adapting to changing environmental conditions and crop requirements
•	Designing effective feedback mechanisms for user interaction and system optimization
•	Providing user-friendly interfaces (e.g., mobile apps, web dashboards) for farmers to input preferences, constraints, and expert knowledge into the automated irrigation system, using techniques like participatory design or user-centered design
•	Incorporating user feedback and domain expertise to refine irrigation strategies and improve system performance
8. CASE STUDIES AND REAL-WORLD IMPLEMENTATIONS OF FULLY AUTONOMOUS IRRIGATION SYSTEMS
8.1. Fully Autonomous Irrigation Systems in Diverse Agricultural Settings
•	Row Crops: maize, wheat, soybean with real-time soil moisture monitoring and weather-based irrigation scheduling for fully automated precision irrigation
•	Orchards: citrus, apple, almond with plant health monitoring and precision water application for fully autonomous orchard management
•	Greenhouses: tomato, lettuce, herbs with automated drip irrigation and climate control integration for fully automated greenhouse operations
•	Urban Farming: rooftop gardens, vertical farms with IoT-enabled hydroponic systems and remote management for fully autonomous urban crop production
8.2. Integration of Advanced System Components for End-to-End Automation
•	Wireless sensor networks: soil moisture probes, weather stations, plant health monitoring cameras with low-power, long-range communication for fully automated data acquisition
•	Secure data transmission: LoRaWAN, NB-IoT, 5G, satellite communication for reliable, real-time data transfer from field to cloud in fully autonomous irrigation systems
•	Intelligent data processing: edge computing for local data filtering, cloud platforms for scalable storage and analysis, machine learning algorithms for predictive insights in fully automated irrigation management
•	Autonomous decision-making: advanced irrigation scheduling algorithms, precise valve control, closed-loop feedback systems for optimal water management in fully autonomous irrigation systems
8.3. Quantitative Performance Evaluation of Fully Automated Irrigation Systems
•	Water use efficiency: percent reduction in water consumption compared to conventional methods, improved water productivity (yield per unit of water) achieved through fully autonomous irrigation
•	Crop yield and quality improvements: percent increase in yield, enhanced crop uniformity, improved nutritional content attributed to fully automated precision irrigation
•	Labor and energy savings: quantified reduction in labor hours for irrigation management, decreased energy consumption for pumping due to optimized scheduling in fully autonomous systems
•	Economic viability: detailed return on investment analysis, payback period calculations, comprehensive cost-benefit analysis for fully autonomous irrigation management systems
8.4. Lessons Learned and Challenges Encountered in Deploying Autonomous Irrigation Systems
•	Technical challenges and solutions: ensuring reliable data transmission in remote locations, addressing interoperability issues between diverse system components, optimizing power consumption for extended battery life, adapting algorithms to local soil and weather conditions in fully autonomous irrigation systems
•	Operational and logistical hurdles: streamlining installation and maintenance procedures, providing effective user training, seamlessly integrating with existing farm management practices and legacy systems for fully automated irrigation management
•	Regulatory and socio-economic considerations: navigating complex water use regulations, addressing data privacy and security concerns, ensuring equitable access and affordability for smallholder farmers adopting fully autonomous irrigation technologies
8.5. Best Practices and Recommendations for Successful Implementation
•	Designing scalable, modular, and adaptable autonomous irrigation systems to accommodate future growth and changing requirements for fully automated water management
•	Prioritizing user-centered design principles and actively engaging stakeholders throughout the development and deployment process of fully autonomous irrigation solutions
•	Adopting open standards and communication protocols to enable seamless integration of system components and interoperability with third-party platforms in fully automated irrigation setups
•	Implementing robust data validation, filtering, and quality control mechanisms to ensure data integrity and reliability for decision-making in fully autonomous irrigation systems
•	Establishing clear data governance policies and security frameworks to protect sensitive information and maintain user trust in fully automated irrigation management
•	Developing intuitive user interfaces and decision support tools to facilitate easy adoption and effective use of fully autonomous irrigation systems
•	Collaborating with local extension services, agribusinesses, and technology providers for knowledge transfer, technical support, and continuous improvement of fully automated irrigation solutions
8.6. Synthesis of Case Studies and Implications for Autonomous Irrigation Adoption
•	Cross-case analysis of key performance indicators and critical success factors for fully autonomous irrigation scheduling systems in various contexts
•	Identification of common themes, challenges, and innovative solutions across diverse implementations of end-to-end fully automated irrigation management
•	Assessment of the potential for replicability and scaling of successful fully autonomous irrigation projects in different regions and farming systems
•	Implications for future research priorities, technology development roadmaps, and policy interventions to support widespread adoption of fully autonomous irrigation technologies

CONCLUSION/FUTURE DIRECTIONS AND UNANSWERED QUESTIONS
•	Summarize the key insights gained from the question-driven review, emphasizing how each section contributes to the overarching goal of achieving real-time, end-to-end automation in irrigation management
•	Based on the questions addressed, propose new research directions and unanswered questions
•	Identify key research gaps and propose concrete research questions and hypotheses for advancing the field of real-time, automated irrigation management
•	Highlight the need for collaborative research efforts across disciplines, such as computer science, agricultural engineering, and environmental science, to address the complex challenges of automated irrigation systems
•	Emphasize the need for further innovation and exploration in real-time, automated irrigation systems



</previous_sections>

</documents>
<instructions>


Use the information provided in the <documents> tags to write the next subsection of the research paper, following these steps:
1. Review the overall intention of the research paper, specified in the <review_intention> tag. Ensure the subsection you write aligns with and contributes to this overall goal.
2. Consider the specific intention for this subsection of the paper, stated in the <section_intention> tag. The content you write should fulfill this purpose. 
3. Use the title provided in the <subsection_title> tag as the heading for the subsection. 
4. Address each of the points specified in the </subsection_point_Point *> tags:
   a) Make a clear case for each point using the text provided in the "point" field.
   b) Support each point with evidence from the research papers listed in the corresponding "papers to support point" field.
   c) When citing a paper to support a point, include inline citations with the author name(s) and year, e.g. (Smith et al., 2020; Johnson and Lee, 2019; Brown, 2018). Cite all papers that strengthen or relate to the point being made.
   d) While making a point and citing the supporting papers, provide a brief explanation in your own words of how the cited papers support the point.
5. Ensure that both of the points from the <subsection_point> tags are fully addressed and supported by citations. Do not skip or combine any points.
6. After addressing the specified points, wrap up the subsection with a concluding sentence or two that ties the points together and relates them back to the <section_intention>.
7. Review the <Previous_sections> of the paper, and ensure that the new subsection you have written fits logically and coherently with the existing content. Add transition sentences as needed to improve the flow.
8. Proofread the subsection to ensure it is clear, concise, and free of grammatical and spelling errors. Maintain a formal academic tone and style consistent with the rest of the research paper.
9. Format the subsection using Markdown, including the subsection heading (using ## or the equivalent for the document), inline citations, and any other formatting needed for clarity and readability.
10. If any information is missing or unclear in the provided tags, simply do your best to write the subsection based on the available information. Do not add any information or make any points not supported by the provided content. Prioritize fully addressing the required points over hitting a specific word count.

The output should be a complete, well-organized, and properly cited subsection ready to be added to the research paper.

Begin your answer with a brief recap of the instructions stating what you will to optimize the quality of the answer. Clearly and briefly state the subsection you'll be working on and the points you'll be addressing. Then proceed to write the subsection following the instructions provided. 

Critical: 
- Do not include a conclusion or summary as the entry is in the middle of the document. Focus on addressing the points and supporting them with evidence from the provided papers. Ensure that the subsection is well-structured, coherent, and effectively contributes to the overall research paper.
- The subsection we are focusing on is: 4.4. Online Learning in the Cloud
- No need for sub-sub-sections. just provide paragraphs addressing each point. They should transition fluidly and narurally into each other.
- Ensure that the content is supported by the provided papers and that the citations are correctly formatted and placed within the text.
- Do not repeat content from the previous sections. Ensure that the information provided is new and relevant to the subsection being written.



</instructions>

