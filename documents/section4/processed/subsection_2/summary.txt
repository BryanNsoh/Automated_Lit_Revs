<instructions>


Use the information provided in the <documents> tags to write the next subsection of the research paper, following these steps:
1. Review the overall intention of the research paper, specified in the <review_intention> tag. Ensure the subsection you write aligns with and contributes to this overall goal.
2. Consider the specific intention for this subsection of the paper, stated in the <section_intention> tag. The content you write should fulfill this purpose. 
3. Use the title provided in the <subsection_title> tag as the heading for the subsection. 
4. Address each of the points specified in the </subsection_point_Point *> tags:
   a) Make a clear case for each point using the text provided in the "point" field.
   b) Support each point with evidence from the research papers listed in the corresponding "papers to support point" field.
   c) When citing a paper to support a point, include inline citations with the author name(s) and year, e.g. (Smith et al., 2020; Johnson and Lee, 2019; Brown, 2018). Cite all papers that strengthen or relate to the point being made.
   d) While making a point and citing the supporting papers, provide a brief explanation in your own words of how the cited papers support the point.
5. Ensure that both of the points from the <subsection_point> tags are fully addressed and supported by citations. Do not skip or combine any points.
6. After addressing the specified points, wrap up the subsection with a concluding sentence or two that ties the points together and relates them back to the <section_intention>.
7. Review the <Previous_sections> of the paper, and ensure that the new subsection you have written fits logically and coherently with the existing content. Add transition sentences as needed to improve the flow.
8. Proofread the subsection to ensure it is clear, concise, and free of grammatical and spelling errors. Maintain a formal academic tone and style consistent with the rest of the research paper.
9. Format the subsection using Markdown, including the subsection heading (using ## or the equivalent for the document), inline citations, and any other formatting needed for clarity and readability.
10. If any information is missing or unclear in the provided tags, simply do your best to write the subsection based on the available information. Do not add any information or make any points not supported by the provided content. Prioritize fully addressing the required points over hitting a specific word count.

The output should be a complete, well-organized, and properly cited subsection ready to be added to the research paper.

Begin your answer with a brief recap of the instructions stating what you will to optimize the quality of the answer. Clearly and briefly state the subsection you'll be working on and the points you'll be addressing. Then proceed to write the subsection following the instructions provided. 

Critical: 
- Do not include a conclusion or summary as the entry is in the middle of the document. Focus on addressing the points and supporting them with evidence from the provided papers. Ensure that the subsection is well-structured, coherent, and effectively contributes to the overall research paper.
- The subsection we are focusing on is: 4.4. Online Learning in the Cloud
- No need for sub-sub-sections. just provide paragraphs addressing each point. They should transition fluidly and narurally into each other.
- Ensure that the content is supported by the provided papers and that the citations are correctly formatted and placed within the text.
- Do not repeat content from the previous sections. Ensure that the information provided is new and relevant to the subsection being written.



</instructions>

<documents>
<review_intention>
  
the purpose and intention of this systematic review on automated systems for real-time irrigation management can be interpreted as follows:
Addressing the global food challenge: The review aims to explore how automated, real-time irrigation management systems can contribute to the efficient use of water resources and enhance agricultural productivity to meet the growing demand for food.
Evaluating the current state and future potential: The primary objective is to critically assess the current state of end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. The review also seeks to identify gaps and propose solutions for seamless integration across the automated irrigation management system to achieve fully autonomous, scalable irrigation management.
Examining automation across the entire pipeline: The review intends to systematically analyze the automation of each component of the irrigation management pipeline, from data collection and transmission to processing, analysis, decision-making, and automated action. It aims to investigate the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
Highlighting the role of interoperability and standardization: The review seeks to emphasize the importance of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline. It aims to identify existing and emerging standards and their applicability to real-time irrigation management systems.
Identifying challenges and proposing solutions: The review intends to uncover the challenges associated with implementing real-time, automated irrigation systems, such as data quality, scalability, reliability, and security. It aims to propose solutions and best practices based on the analysis of case studies and real-world implementations.
Guiding future research and innovation: By identifying research gaps and proposing new research questions and hypotheses, the review aims to provide a roadmap for advancing the field of real-time, automated irrigation management. It seeks to encourage collaborative research efforts across disciplines to address the complex challenges of automated irrigation systems.
In summary, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge.
</review_intention>

<section_intention>
AUTOMATED DATA PROCESSING IN THE CLOUD: Examines the importance of data quality and preprocessing in the cloud, containerization strategies for scalable and autonomous deployment, and the deployment of machine learning (ML) models for real-time data processing and inference.
</section_intention>

<subsection_title>
4.2. Scalable and Autonomous Deployment using Containerization Strategies
</subsection_title>

<subsection_point_Point 2>
Point: Optimization techniques for container orchestration and resource allocation to minimize latency and maximize throughput in real-time data processing pipelines, such as Auto-scaling, Dynamic resource allocation, Performance monitoring using tools like Kubernetes Metrics Server and Prometheus

Papers to support point:

Paper 1:
- APA Citation: Author, A. A. (2023). Title of paper. Journal Name, xx(x), xxx-xxx. https://doi.org/xx.xxxx/xxxxx
  Main Objective: To develop a system for automating the processes of data collection, processing, and provisioning for real-time irrigation management.
  Study Location: Unspecified
  Data Sources: Not specified
  Technologies Used: Not specified
  Key Findings: The system is based on a proactive approach to irrigation, predicting future irrigation needs and automatically allocating resources based on these predictions.
  Extract 1: The paper presents a system for automating the processes of data collection, processing, and provisioning for real-time irrigation management. The system is designed to be scalable to large-scale irrigation systems, and it is expected to improve water use efficiency and reduce water waste.
  Extract 2: The system is based on a proactive approach to irrigation, predicting future irrigation needs and automatically allocating resources based on these predictions.
  Limitations: The paper does not provide any specific data on the performance of the system in terms of water use efficiency or water waste reduction. Additionally, the system has not been tested in a real-world setting, so its scalability and effectiveness in practice are unknown.
  Relevance Evaluation: The paper is relevant to the point being made in the literature review because it presents a new system for automating the processes of data collection, processing, and provisioning for real-time irrigation management. This system has the potential to improve water use efficiency and reduce water waste, which are important goals for sustainable agriculture.
  Relevance Score: 0.9
  Inline Citation: (Author, 2023)
  Explanation: The purpose of this study is to develop a system for automating the processes of data collection, processing, and provisioning for real-time irrigation management. This system takes a proactive approach to irrigation, predicting future irrigation needs and automatically allocating resources based on these predictions. The system is designed to be scalable to large-scale irrigation systems, and it is expected to improve water use efficiency and reduce water waste.

 Full Text: >
Proactive resource management for cloud of
services environments
Gonçalo Marques 
Instituto de Telecomunicações
Carlos Senna  (  cr.senna@av.it.pt )
Instituto de Telecomunicações
Susana Sargento 
Instituto de Telecomunicações
Luís Carvalho 
Veniam, Portugal
Luís Pereira 
Veniam, Portugal
Ricardo Matos 
Veniam, Portugal
Research Article
Keywords: Cloud service management, cloud monitoring, proactive resource management, vehicular
cloud resources
Posted Date: October 18th, 2022
DOI: https://doi.org/10.21203/rs.3.rs-2165603/v1
License:   This work is licensed under a Creative Commons Attribution 4.0 International License.  
Read Full License
Journal of Cloud Computing
Proactive resource management for cloud of
services environments
Gon¸calo Marques1, Carlos Senna1*, Susana Sargento1,2, Lu´ıs
Carvalho3, Lu´ıs Pereira3 and Ricardo Matos3
1*Instituto de Telecomunica¸c˜oes, Campus Universit´ario de
Santiago, Aveiro, 3810-193, Portugal.
2DETI, University of Aveiro, Campus Universit´ario de Santiago,
Aveiro, 3810-193, Portugal.
3Veniam, Porto, 4000-098, Portugal.
*Corresponding author(s). E-mail(s): cr.senna@av.it.pt;
Contributing authors: gjmarques@ua.pt; susana@ua.pt;
lcarvalho@veniam.com; lpereira@veniam.com;
rmatos@veniam.com;
Abstract
Microservices oﬀer advantages such as better fault isolation, smaller and
faster deployments, scalability in addition to streamlining the devel-
opment of new applications through composition of services. However,
their large-scale usage, speciﬁc purposes and requirements increase the
challenges of management systems that require a monitoring capa-
ble of accurately detecting the speciﬁc resources that are lacking
for a service, and providing them according to the requirements.
To meet these new management challenges, we propose a monitoring
and management system for microservices, containers and container clus-
ters that autonomously predicts load variations and resource scarcity,
which is capable of making new resources available in order to ensure the
continuity of the process without interruptions. Our solution’s architec-
ture allows for customizable service-speciﬁc metrics that are used by the
load predictor to anticipate resource consumption peaks and proactively
allocate them. In addition, our management system, by identifying/pre-
dicting low demand, frees up resources making the system more eﬃcient.
We evaluated our solution in the Amazon Web Services environment,
where diﬀerent services were installed for users of a vehicular network,
1
Journal of Cloud Computing
2
Marques et al.
an environment that is characterized by high mobility, dynamic topolo-
gies caused by disconnection, dropped packets, and delay problems. Our
results show that our solution improves the eﬃciency of escalation poli-
cies, and reduces response time by improving the QoS/QoE of the system.
Keywords: Cloud service management, cloud monitoring, proactive resource
management, vehicular cloud resources
1 Introduction
Microservices are the latest trend in software service design, development, and
delivery [1], that include the emphasis on dividing the system into small and
lightweight services that are purposely built to perform a very cohesive business
function. It is an evolution of the traditional service oriented architecture style,
which is currently drawing a lot of attention from application developers and
service providers, such as Amazon, Netﬂix or eBay between others [2].
The growing adoption of microservices in the cloud is motivated by the eas-
iness of deploying and updating the software, as well as the provisioned loose
coupling provided by dynamic service discovery and binding. Structuring the
software to be deployed in the cloud as a collection of microservices allows cloud
service providers to oﬀer higher scalability guarantees through more eﬃcient
consumption of cloud resources, and to dynamically and quickly restructure
software to accommodate growing consumer demand [3]. Moreover, microser-
vice applications increase the number and diversity of infrastructure resources
that must be continuously monitored and managed at runtime. Finally, ser-
vices can be deployed across multiple regions and availability zones, which
adds to the challenge of gathering up-to-date information [4].
Within the scope of smart cities, the advantages of adopting services and
their compositions are notorious, as they allow agile development to meet
the enormous demand. In this context, Vehicular ad hoc Networks (VANETs)
play an important role as a communication infrastructure, since people spend
much of their time inside vehicles, especially in big cities. For these reasons,
we chose a VANET and its applications to evaluate the performance of our
service monitoring and management solution. VANETs are based on a set of
communication systems that provide vehicles with access to many diﬀerent
applications. The pervasive roadside infrastructure oﬀers high-speed internet
access by advanced wireless communication technologies (3/4/5 G, ITS-5G
or C-V2X), and brings innovative and divergent beneﬁts, but also some chal-
lenges [5], such as instability of resources which make the management of
vehicular networks complicated.
Due to the high mobility and dynamic topologies of VANETs, monitor-
ing and managing microservices becomes an extremely complex issue that can
incur high costs unless the services are managed properly. This requires a
monitoring system capable of eﬃciently managing services to optimize overall
Journal of Cloud Computing
Marques et al.
3
data, ensure delivery of resources to customers, and detect resource shortages
or overspends to keep costs to a minimum. This is usually achieved with mon-
itoring solutions that collect metrics from each observed resource, which are
later analyzed by applications to detect problems in the system and ensure
that there is no over/underprovisioning of resources [6].
To address these challenges, we propose an architecture for accurate mon-
itoring of custom metrics for service-based cloud applications, which will be
used for automatic provisioning and scaling of compute resources in cloud
environments, while addressing services in very challenging scenarios such as
VANETs. Our predictive approach, designed for the AWS/EKS environment,
is capable of eﬃciently provisioning disparate services to users in VANETs.
We implement several services with diﬀerent requirements to evaluate the
performance and versatility of our monitoring system. In our evaluation, we
analyze data collected on a real vehicular network that was used to train
our machine learning algorithm and produce predictions that are sent to the
resource provisioning system.
The proposed architecture led to the following contributions:
• design and implementation of a generic monitoring architecture able to
collect metric data from various services with an open source framework,
enabling horizontal scaling;
• analysis and implementation of a network prediction model using statistical
and machine learning techniques; the analysis includes exploration of the
datasets with time-series techniques and tests;
• analysis of the most common services provided and their diﬀerent require-
ments;
• design and implementation of a cloud management system that proac-
tively predicts the required resources for the multitude of services in a very
dynamic environment.
The remainder of this article is organized as follows. Section 2 presents
the related work about monitoring systems and service provisioning. Section 3
describes our service monitoring system with proactive resource management.
Section 4 discusses the tests made to validate the proposed monitoring system.
Finally, Section 5 presents the conclusion and directions for future work.
2 Basic Concepts and Related Work
Regarding the proposed architecture, there are two relevant aspects: monitor-
ing tools & frameworks for clouds and proactive cloud management.
Big cloud providers, such as Google, Microsoft, Amazon between others,
have monitoring systems, but can only be used in their own cloud environ-
ments. For this reason we have not included details about their monitoring
systems, and concentrate on scientiﬁc publications about cloud monitoring and
management.
Journal of Cloud Computing
4
Marques et al.
Al-Shammari et al [7] presented MonSLAR, a User-centric middleware for
Monitoring Service Level Agreements (SLA) for Restful services in Software as
a Service (SaaS) cloud computing environments. MonSLAR uses a distributed
architecture that allows SLA parameters and the monitored data to be embed-
ded in the requests and responses of the REST protocol. This solution aims
to monitor the customer’s QoS and does not deal with the management of
services or any optimization of resources in the cloud. Lee et al. [8] proposed
a cloud-native workload proﬁling system with Kubernetes-orchestrated multi-
cluster conﬁguration. The solution monitors the resource usage when deploying
service cases in multiple clouds and identiﬁes the correlation between services
and resource usage. However, it does not provide a predictive and proactive
resource management.
There are also speciﬁc tools for monitoring resources, applications and ser-
vices for the cloud. Prometheus1 is an open-source system monitoring and
alerting toolkit that collects and stores its metrics as time series data along-
side optional key-value pairs called labels. Prometheus extracts metrics, either
directly or through a gateway, storing them locally. It also oﬀers the option
to run rules for aggregation and generate alerts. Stored metrics can be viewed
through dashboards like Graphana or consumed through Application Program-
ming Interfaces (APIs). Jaeger2 is a open source distributed tracing system
used for monitoring and troubleshooting microservices-based distributed sys-
tems. The Jaeger project is primarly a tracing backend that receives tracing
telemetry data and provides processing, aggregation, data mining, and visu-
alizations of that data. Open Telemetry3 resulted from a merger between the
OpenTracing and OpenCensus projects; it provides a uniﬁed set of instrumen-
tation libraries and speciﬁcations for observability telemetry [9]. The project
is a set of APIs, a software development kit, tooling and integrations designed
for the creation and management of telemetry data such as traces, metrics, and
logs. The project provides a vendor-agnostic implementation that can be con-
ﬁgured to send telemetry data to any backend. It supports a variety of popular
open-source projects including Jaeger and Prometheus. In the testbed that we
set up to evaluate our solution, we chose to use the Prometheus monitoring
system, as it is the most complete solution, with ease of customization and the
ability to produce metrics without aﬀecting the application’s performance.
Next-generation scaling approaches attempt to move beyond the limita-
tions of such reactive systems, by instead attempting to predict the near-future
workloads [10]. In such systems, the forecasting component can be considered
the most important element, and the diﬀerentiating factor when comparing
elastic systems [11]. In service oriented clouds, the forecasting problem is more
complex since it involves the collection, processing and analysis of diﬀerent
data sets, which can be traces of resource usage, such as CPU, memory, net-
work bandwidth or metrics related to provided applications/services, such as
1https://prometheus.io/
2https://www.jaegertracing.io/
3https://opentelemetry.io/
Journal of Cloud Computing
Marques et al.
5
the number of requests that are being served, the application architecture,
etc [12].
Wang et al. [13] proposed a self-adapting resource management framework
for cloud software services. For a given service, an iterative QoS model is
trained based on historical data, which is capable of predicting a QoS value
of a management operation using the information about the current running
workload, allocated resources, actual value of QoS in a resource allocation
operation. A runtime decision algorithm based on Particle Swarm Optimiza-
tion (PSO) together with the predicted QoS value are employed to determine
future resource allocation operations.
Chen et al. [14] proposed a microservice-based deployment problem based
on the heterogeneous and dynamic characteristics in the edge-cloud hybrid
environment, including heterogeneity of edge server capacities, dynamic geo-
graphical information of IoT devices, and changing device preference for
applications and complex application structures. Their algorithm leverages
reinforcement learning and neural networks to learn a deployment strategy
without any human instruction. The main objective of this model is to min-
imize the waiting time of the Internet of Things (IoT) devices, while our
proposed model focuses on minimizing the delay of scaling operations by
predictively allocating resources to cloud services based on the incoming
workload.
Zhao et al. [15] introduced a predictive auto-scaler for Kubernetes clusters
to improve the eﬃciency of autoscaling containers. They proposed a monitor-
ing module that obtains the indicator data of the pod which is used by the
prediction module to estimate the number of pods to the auto-scaler module for
scaling services. While the authors choose to use the original responsive strat-
egy of Horizontal Pod Autoscaler (HPA), we perform scale down operations
based on the values calculated by the predictor, reducing the over provisioning
of resources.
Zhou et al. [16] proposed an Ensemble Forecasting Approach for highly-
dynamic cloud workload based on Variational Mode Decomposition (VMD)
and R-Transformer. To decrease the non-stationarity and high randomness
of highly-dynamic cloud workload sequences, workloads are decomposed into
multiple Intrinsic Mode Functionss (IMFs) by VMD. The IMFs are then
imported into the ensemble forecasting module, based on R-Transformer and
Autoregressive models, in order to capture long-term dependencies and local
non-linear relationship of workload sequences. However, the authors do not
present any solutions on how to use a non-decomposition and eﬃcient method
to reduce the instability and randomness of highly-dynamic workload data,
which would be advantageous for reducing the training cost of deep neural
networks.
Liu et al. [17] presented a workload migration model to minimize migra-
tion times and improve the node processing performance. To forecast the
workload more accurately, a model that combines Autoregressive Moving Aver-
age (ARMA) with the Elman Neural Network (ENN) are proposed. In order
Journal of Cloud Computing
6
Marques et al.
to meet cost constraints and deadline constraints, an elastic resource manage-
ment model based on cost and deadline constraints is proposed. Both the cost
and the deadline of tasks are considered. Workload migration can be used to
balance the workload of each resource node, making the cluster’s response time
faster. However, combining on-demand resource provision method with mobile
edge computing is worth studying, since in a smart city environment, diﬀerent
areas require diﬀerent resources depending on the amount of data process-
ing, so the on-demand resource provision can reduce service expenditure while
meeting user needs.
Zhu et al. [18] proposed a model using the Long Short-term Memory
(LSTM) encoder-decoder network with attention mechanism to improve the
workload prediction accuracy. The model uses an encoder to map the histori-
cal workload sequence to a ﬁxed-length vector according to the weight of each
time step supported by the attention module, and uses a decoder to map the
context vectors back to a sequence. Finally, the output layer transforms the
sequence into the ﬁnal output. Moreover, the authors propose a scroll predic-
tion method to reduce the error occurring during long-term prediction, which
splits a long-term prediction task into several small tasks.
The presented works mostly focus on developing a predictor which is opti-
mized for their speciﬁc use case. However, in environments with multiple
distinct service applications, a single prediction algorithm might not present
the best results for all applications. In our solution, we enable the integration
and usage of diﬀerent predictors for each service. This is a very useful feature,
because it allows the usage of the optimal predictors for each case instead of
a generic predictor, which may not have the best performance for some appli-
cations. Moreover, with the rapid advances in this ﬁeld, the ability to easily
integrate new predictors as they appear is another important characteristic of
the system.
3 Proactive Cloud Resource Management
It is essential that a monitoring system is able to accurately monitor cloud
resources (hardware and software) and make automated decisions. There are
several options that monitor hardware resources, others that monitor the
software involved, whether monolithic applications, services or service compo-
sitions. However, most of them react linearly to the increase in demand, which
can cause the loss of QoS, mainly due to the time required for new resources
to be ready for use.
To ensure that the monitoring system is able to allocate resources while
maintaining QoS, we propose an architecture that allows managers to easily
deﬁne which custom metrics they want to monitor for each service or ser-
vice composition using custom metrics exporters. In addition, our solution
has a forecasting service that identiﬁes consumption peaks (up and down)
and proactively makes scale adjustments, in order to guarantee QoS (up), and
Journal of Cloud Computing
Marques et al.
7
at the same time optimize resource consumption of the cloud, lowering the
operational cost (down).
As shown in Figure 1, our conceptual architecture exposes the desired met-
rics to the Metric Collector that can be integrated with the Autoscaler, to make
scaling decisions based on metrics other than Central Process Unit (CPU),
memory and communication.
Fig. 1 Main components of the proposed conceptual monitoring architecture.
The Metric Exporters (ME) collect the metrics from the services and
expose them to the Metrics Collector (MC) which will aggregate the data from
all services. The Prediction API (PAPI) queries the MC to obtain histori-
cal data from the services to update the prediction models, and then exposes
its updated predictions as metrics in an HTTP endpoint that is periodically
scraped by the MC. The MC gathers data from the services themselves and
the PAPI, and exposes the metrics in two ways. First, MC sends metrics to the
System Manager (dashboard), allowing the manager to observe and analyse the
behaviour of the services. Second, MC sends metrics to the Kubernetes Con-
troller which will use the metrics to detect service failures and automatically
adjust the resources.
Using our conceptual architecture as a basis, we designed our cloud services
monitoring solution, the details of which are presented below. First, we describe
the main components related to the monitoring itself, and then we present the
proactive version where we detail our predictor service.
Journal of Cloud Computing
8
Marques et al.
3.1 Full metrics monitoring pipeline
In Kubernetes, application monitoring does not depend on a single monitoring
solution. The resource metrics pipeline provides a limited set of metrics related
to cluster components such as the HPA controller. The full metrics pipeline
gives access to richer metrics. Figure 2 shows a high level view of the updated
monitoring system architecture with support for full metrics. Its main com-
ponents are: Service Apps, Metrics Server, Prometheus Monitor, Prometheus
Kubernetes Adapter, the Kubernetes Metrics API and the HPA.
Fig. 2 High-level view of the custom/external metrics monitoring architecture.
To monitor additional metrics, a custom exporter is integrated in each
service to publish metrics such as throughput and number of client sessions,
streams or ﬁle transfers, depending on the service. Moreover, in any monitoring
system, it is extremely important to observe the collected data, in order to
understand the applications resource requirements. However, the metrics API
does not allow the visualization of the historical data. To do that, we chose
Prometheus, an open source monitoring and alerting platform. Prometheus
collects data from the sources in a time-series format at a set interval, and
stores it locally on a disk identiﬁed by the metric name and key/value pairs.
Details about the main components of our architecture are presented below.
3.1.1 Service Apps
The Service Apps represent the generic services or compositions of services,
that will be monitored and scaled according to the memory and CPU usage
values gathered by the Metrics Server. The applications are exposed internally
through a Kubernetes service, an abstraction which deﬁnes a logical set of Pods
and a policy by which to access them; they are accessible to clients through a
Network Load Balancer (NLB). The NLB selects a target (targets may be EC2
instances, containers or IP addresses) using a ﬂow hash algorithm based on the
protocol, source IP address, source port, destination IP address, destination
port, and Transmission Control Protocol (TCP) sequence number. The TCP
connections from a client have diﬀerent source ports and sequence numbers,
and can be routed to diﬀerent targets. Each individual TCP connection is
Journal of Cloud Computing
Marques et al.
9
routed to a single target for the life of the connection. The Service Apps have
been integrated with a custom exporter that enables the collection of additional
metrics, the most relevant ones being downstream throughput and number
of active user sessions. Although some applications may support Prometheus
metrics out of the box, for the most part of them we need to develop a custom
adapter (CA). The CA helps the application to monitor additional metrics
other then memory and CPU usage, collected by the Kubernetes metrics-server
by default. Moreover, we need an additional Exporter for the Prometheus
server, which collects the values from the Prometheus monitoring server and
publishes them in the Kubernetes metrics API where they become available
to Autoscaler. Our custom exporters are GO servers that collect information
from the applications at run time, and exposes them in an Hypertext Transfer
Protocol (HTTP) endpoint according to the Prometheus format, so they can
be periodically scraped by the Prometheus Monitor.
3.1.2 Metrics Server
The Metrics Server is a cluster-wide aggregator of resource usage data. Figure
3 shows in detail the simultaneous implementation of the custom/external
metrics pipeline and the resource metrics pipeline.
Fig. 3 Detailed view of the custom/external metrics monitoring architecture.
The Kubernetes Metrics Server collects and exposes metrics from appli-
cations. It discovers all nodes on the cluster, and queries each node’s kubelet
for CPU and memory usage. CPU is reported as the average usage, in CPU
cores, over a period of time derived by taking a rate over a cumulative CPU
counter provided by the kernel. Memory usage is reported as the working set,
in bytes, at the instant the metric was collected. In an ideal world, the “work-
ing set” is the amount of memory in-use that cannot be freed under memory
Journal of Cloud Computing
10
Marques et al.
pressure. However, the calculation of the working set varies by host Operating
System (OS), and generally makes heavy use of heuristics to produce an esti-
mate. It includes all anonymous (non-ﬁle-backed) memory, since Kubernetes do
not support the swap process. The metric typically includes also some cached
(ﬁle-backed) memory, because the host OS cannot always reclaim such pages.
The monitoring pipeline fetches metrics from the Kubelet which is the pri-
mary agent running in every node that works in terms of PodSpecs, which
are YAML or JSON objects that describe the containers running in that node.
The Kubelet takes the PodSpecs provided by the API Server, and ensures
that the containers described in them are running and healthy. The Kubelet
acts as a bridge between the Kubernetes master and the nodes, managing
the pods and containers running on a machine. The Kubelet translates each
pod into its constituent containers, and fetches individual container usage
statistics from the container runtime through the container runtime interface.
The Kubelet fetches this information from the integrated cAdvisor, a running
daemon that collects real-time monitoring data from the containers for the
legacy Docker integration. It then exposes the aggregated pod resource usage
statistics through the Metrics Server API.
To collect custom metrics from deployed apps, we need to integrate a Met-
rics Exporter built speciﬁcally for each app. This exporter collects the desired
metrics and exposes them to an HTTP endpoint that is periodically scraped by
Prometheus. The Prometheus monitor collects metrics directly from the appli-
cation exporters, as well as the Metrics Server. The Prometheus Kubernetes
Exporter queries the Prometheus monitor to collect speciﬁc metrics and pub-
lish them in the Custom/External Metrics API (CEMAPI). Finally, metrics
can be used by the HPA to make scaling decisions.
3.1.3 Prometheus Monitor
The Prometheus Monitor queries the data sources (Metrics Exporters) at a
speciﬁc polling frequency, at which time the exporters present the values of
the metrics at the endpoint queried by Prometheus. The exporters are auto-
matically discovered by creating a Kubernetes entity called Service Monitor, a
custom Kubernetes resource that declaratively speciﬁes how groups of services
should be monitored and which endpoints contain the metrics. The Prometheus
aggregates the data from the exporters into a local on-disk time series database
that stores the data in a highly eﬃcient format. Data is stored with its spe-
ciﬁc name and an arbitrary number of key=value pairs (Labels). Labels can
include information on the data source and other application-speciﬁc break-
down information, such as the HTTP status code (for metrics related to HTTP
responses), query method (GET versus POST), endpoint, etc.
Supported by basic and custom metrics implemented, the HPA auto-
matically scales the number of Pods in a replication controller, deployment,
replica set or stateful set based on application-provided metrics instead of
only CPU and memory usage metrics. Moreover, with custom metrics, we
can detect malfunctions and resource shortages that would go unnoticed if
Journal of Cloud Computing
Marques et al.
11
we were just monitoring the CPU application and memory usage, making
the metrics pipeline complete, a valuable addition and a great improvement
to the tracking system. From the most basic perspective, the HPA controller
operates on the ratio between desired metric value and current metric value,
as formalized in the Equation 1 below:
desiredReplicas = ceil[currentReplicas∗
(currentMetricV alue/desiredMetricV alue)]
(1)
For example, if the current metric value is 200m, and the desired value
is 100m, the number of replicas will be doubled, since 200.0/100.0 = 2.0. If
the current value is instead 50m, we will halve the number of replicas, since
50.0/100.0 = 0.5.
This monitoring system is, however, still purely reactive and susceptible
to the unpredictability of the workload characteristics of service cloud envi-
ronments. To minimize the impact of these ﬂuctuations, we have integrated
a forecasting module capable of predicting resource usage and allowing the
monitoring system to allocate resources in a preventive way, reducing system
response time and wasted resources. Below, we present our complete solution
that includes the Predictor Service.
3.2 Proactive monitoring for service provisioning
Our prediction module forecasts the network workload spikes based on machine
learning algorithms, and sends the expected values to the monitoring system
so it can proactively allocate resources. Figure 4 contains a high-level depiction
of our Proactive Monitoring Framework and its six main component: Service
Apps, Prometheus Monitor, Prometheus Kubernetes Adapter, Kubernetes Met-
rics API, HPA, Prediction Exporter and Predictor Service, whose adaptations
and diﬀerences from the previous version (Section 3.1) are below.
The Service Apps are now scaled according to the Key Performance Indi-
cator (KPI) values calculated by the Predictor Service, which allows the
applications to begin the scaling process before the workload spikes occur.
The Prometheus Monitor must be conﬁgured to add the Prediction
Exporter to the scrape targets, so that it collects the predicted values from the
exporter, and these metrics need to be added to the Prometheus Kubernetes
Adapter in order for them to be available to the HPA through the Kubernetes
Metrics API.
The Prediction Exporter is a standalone deployment based on a container
running a GO server that acts as the liaison between the monitoring framework
in the cloud and the Predictor Service. It queries the Prometheus Monitor to
gather data from the metrics used for scaling the applications in a ﬁle, which
is periodically sent to the Predictor Service as historical data to continuously
Journal of Cloud Computing
12
Marques et al.
Fig. 4 High-level view of the Proactive Monitoring Framework.
train and consequently improve the accuracy of the predictor. The exporter is
also a data source responsible for receiving the predicted KPI values from the
Predictor Service, and exposing them in an HTTP endpoint which is scraped
by the Prometheus Monitor to make the predicted values available to the HPA.
The Predictor Service uses a framework for distributed real-time time series
forecasting [19] that makes predictions for various dynamic systems simulta-
neously, and provides straightforward horizontal scaling, increased modularity,
high robustness and a simple interface for users.
The high-level view of the prediction framework, depicted in Figure 5, is
composed of ﬁve main components: the Prediction API, the Predictor Message
Broker (PMB), the Metric Predictor Component (MPC), the Metric Message
Transformation Component (MMTC) and the Metric Prediction Orchestration
Component (MPO).
The Prediction API is the interface that allows a client to predict a value
for a time series system/metric, train the predictor for a given metric, save
new time series data for later training and change the training parameters.
The Prediction API is a Representational State Transfer (REST) microservice
that encapsulates the functionalities of the predictor.
To train the predictor for a certain metric we need to ﬁrst provide the
dataset and send the start and end date of the data to consider for the training
phase to the Prediction API which receives the requests from external clients
Journal of Cloud Computing
Marques et al.
13
Fig. 5 High-level view of the prediction architecture [19].
and sends a message to the PMB that will forward the message to the cor-
responding metric predictor or message transformation component, according
to the internal message protocol. The core component of the framework is
the MPC, which is responsible for predicting the time series data for a given
dynamic system/metric and is composed by four sub-components: the Metric
Predictor (MPC-MP), the Metric Predictor Message Broker (MPMB), the
Predictor Training (MPC-PT) and the Time-series Persistence (MPC-TsP).
The MPMB is the internal message broker of the predictor component
and it routes messages between the three sub-components. The MPC-PT does
the training of the model and updates it with the newly received data. The
new model will be uploaded to the MPC via the internal message broker.
The MPC-TsPstores the data to optimize the read and write operations for
time-series data, to be used when training the model. To enable an ensemble
prediction using multiple predictors, it is also needed to perform a reduction
step by joining the predictor results and performing adequate transformations,
which are done using the MMTC by receiving the messages in the prediction
message broker and applying custom functions for each predictor component
messages, before inserting the transformed messages into the broker.
The instances of the metric predictor and the transformation components
are orchestrated by the MPO, which has two internal components: the mon-
itoring component, which accesses the logs of the predictor and transformer
as well as the exchanged messages, to better understand the current state of
operation of the various components; and the conﬁguration and management
component, which instantiates and manages the life cycle of the predictor and
the transformation components.
Journal of Cloud Computing
14
Marques et al.
Once the predictor is trained, the Prediction API can start to predict the
values for the metric. The Prediction Exporter queries the Prometheus Monitor
to collect the current value of the time series, and send it to the Prediction
API along with their timestamp. The Prediction API will obtain the result
of the predictor and return it to the client, along with the timestamp of the
predicted point.
3.3 Main characteristics
Our Proactive Monitoring Framework is scalable, since a Prometheus instance
can monitor several thousands of services. Moreover, it is versatile and can
use other monitor engines, such as Thanos Prometheus4, to enable the aggre-
gation of queries from several Prometheus instances and save the collected
metrics in an external storage device, overcoming the scraping and storage
limitations of a single replica. It is also ﬂexible when it comes to add metrics,
services or replace components, since it facilitates the addition of services and
the collection of custom metrics speciﬁc to each service’s requirements with
the custom metrics exporters. Moreover, it can be used in any public, private
or hybrid Kubernetes based cloud regardless of the provider. The Prediction
Service modularity also makes it easy to replace with any other forecasting
algorithm. The Prometheus monitor can be replaced by any other metric col-
lector, although the alternatives such as Dynatrace5 are often paid and harder
to implement. The monitoring system components have the capability to mon-
itor themselves and adjust and allocate additional resources as required, so as
long as there are resources available, the system will scale and adjust to the
number of active services.
4 Evaluation
The assessment of the proposed approach is performed in three steps. First,
we set up a testbed with the currently most used monitoring tools in the
cloud. We use this initial testbed to evaluate solutions and identify unmet
challenges. Based on this assessment, we reﬁne our testbed and perform a
new assessment. The results show the eﬃciency of this reﬁnement. Finally,
we include a prediction service in our solution. Again, the results show the
eﬃciency of this prediction approach.
Since this approach is tested in a cloud running services in a VANET envi-
ronment, the tests use a real dataset provided by Veniam6 from the network
of vehicles in Oporto, Portugal, which contains the list of Internet sessions
from the users in the vehicles, login and logout timestamps, session duration
and number of bytes transferred from 2020-09-07 at 00:00:00 to 2021-04-27
at 23:00:00. Veniams’ vehicular network infrastructure is a large-scale deploy-
ment of On Board Units (OBUs) in vehicles and infrastructural Road Side
4https://thanos.io/
5https://www.dynatrace.com/
6https://veniam.com/
Journal of Cloud Computing
Marques et al.
15
Units (RSUs) that enables Vehicle to Vehicle (V2V) and Vehicle to Infrastruc-
ture (V2I) communication, in the city of Porto, Portugal [20]. Currently, 600+
ﬂeet vehicles are equipped with OBUs, pertaining to the public transportation
authority (400+ buses) and waste disposal department (garbage-collection and
road-cleaning trucks). Additionally, more than 50 RSUs have been deployed.
The GPS traces and metadata of passenger Wi-Fi connections are collected
by each OBU and stored in the backend infrastructure. As an example, the
number of sessions per hour (Wi-Fi connections) is shown in Figure 6.
Fig. 6 Number of sessions per hour in the timeframe used for forecasting
Two separate sets of tests are conducted to validate the monitoring and
prediction model. The ﬁrst set is run on a local small scale Kubernetes cluster
to facilitate testing, and the second set is run on a larger private Kubernetes
cluster owned by Veniam. The local testbed infrastructure runs on a desktop
(Intel® Core™ i7-8750H CPU @ 2.20GHz × 12, 8GB RAM) using Ubuntu
Linux, running a local Kubernetes cluster in a Minikube Virtual Machine
(VM), which was used to simulate an environment where diﬀerent services were
being provided to users through a URL and monitored using Prometheus. The
large scale tests run in an infrastructure based on the private cluster provided
by Veniam running on Elastic Kubernetes Service (EKS), a managed service
used to run Kubernetes on Amazon Web Services (AWS) without needing to
install, operate, and maintain the Kubernetes control plane or nodes.
Amazon EKS runs and scales the Kubernetes control plane across multiple
AWS Availability Zones to ensure high availability, automatically scales con-
trol plane instances based on load, detects and replaces unhealthy control plane
instances, and provides automated version updates and patching for them. It
Journal of Cloud Computing
16
Marques et al.
is also integrated with many AWS services to provide scalability and secu-
rity for your applications, including the following capabilities: Amazon Elastic
Container Registry (ECR) for container images, Elastic Load Balancing for
load distribution, Identity and Access Management (IAM) for authentication,
and Amazon Virtual Private Cloud (VPC) for isolation. The Kubernetes clus-
ter consists of 13 nodes, each one with a 110 pod capacity and using Amazon
Linux 2.
The monitoring of CPU and memory usage, the two metrics supported by
default by Kubernetes, may be inadequate for some services. Therefore, we
will be running stress tests on three diﬀerent services deployed in a Kubernetes
environment, a Video Stream Service, a File Transfer Service and a Custom
Iperf Service. Figure 7 shows a high level architecture of the monitoring system
and the deployed services.
Fig. 7 High-level view of the tested services in the monitoring architecture.
The Video Stream Service is based on NGINX7, an open source software
that provides a Real Time Media Protocol (RTMP) HTTP Live Streaming
(HLS) module to be used in the streaming server. We use Fast Forward Motion
Picture Experts Group (FFmpeg) to encode and publish in real-time a looped
video ﬁle in H.264 format with a 1280x720 pixel resolution at a frame rate of
24 frames per second. The NGINX server allows clients to stream the video
through an HTTP endpoint.
7https://www.nginx.com/resources/glossary/nginx/
Journal of Cloud Computing
Marques et al.
17
The File Transfer Service is an HTTP server written in GO language and
uses the net/http package. The server receives HTTP requests from clients
and responds with a randomly generated ﬁle. The clients can send requests to
diﬀerent endpoints in order to obtain ﬁles of diﬀerent sizes.
The Custom Iperf Service8 is a standard Iperf3 server, a tool for active
measurements of the maximum achievable bandwidth on IP networks that
supports tuning of various parameters related to timing, buﬀers and protocols
(TCP, User Datagram Protocol (UDP), Stream Control Transmission Protocol
(SCTP) with IPv4 and IPv6) written in C, and a mixer module, developed
by Veniam, that creates multiple instances of the Iperf3 server, up to a preset
limit, allowing a single instance of the service to receive multiple client sessions
simultaneously (this feature is not supported in the default implementation).
Since it is impossible to diﬀerentiate the type of sessions recorded in the
Veniam dataset, we will be making an estimate based on the percentage values
from Sandine’s report [21]. According to the study presented in this report,
64% of the total sessions could be associated with video streams, 8% with ﬁle
transfers and Iperf will be given 1% of the total sessions. Stress tests based
on these values were created to establish a baseline for each service’s memory,
CPU and throughput requirements, allowing the discovery of bottlenecks for
each speciﬁc service.
4.1 Basic monitoring system evaluation
To determine whether the metrics server and resource metrics pipeline were
suﬃcient to accurately monitor the service applications, we developed a simple
load test. First, there is a steadily increasing number of sessions, and afterwards
the number of sessions decreases again on steady one minute intervals, as
shown in Figure 8. The test consisted of simulating the number of sessions in
each service and then analysing the memory and CPU usage metrics to see if
all services can be accurately proﬁled using these metrics.
Figure 9 shows the memory usage, and Figure 10 shows the CPU usage for
the video stream service.
Neither of the metrics is able to accurately proﬁle the status of the service
and the incoming connections, since both metrics remain constant during the
full test, with minimal variations that are not related to the incoming streams.
From the above tests, we can conclude that, exclusively monitoring memory
and CPU usage is insuﬃcient to have an accurate monitoring system. Even
though there may exist services for which these two metrics are suﬃcient, such
as the ﬁle transfer service, for video stream service and the custom Iperf service,
these metrics fail to give an accurate representation of the service status and
are unsuitable to be used as the basis for scaling decisions. This means that, in
order to accurately monitor the services, additional metrics must be gathered
from the services, which will be presented in the next section.
8https://iperf.fr/
Journal of Cloud Computing
18
Marques et al.
Fig. 8 Number of sessions in the load test.
Fig. 9 Video Stream memory usage with resource metrics.
Fig. 10 Video Stream CPU usage with resource metrics.
4.2 Custom metrics monitoring system evaluation
To determine whether additional metrics could be used to monitor the ser-
vice applications, we repeated the load test shown in Figure 8 to analyze
the throughput and active users graphs to determine if they can be used to
accurately proﬁle the service load.
Journal of Cloud Computing
Marques et al.
19
Fig. 11 Video Stream active streams with custom metrics.
Figure 11 shows the number of active video streams in the server at a given
time. This metric is a strong candidate to be used in the autoscaler, since it
shows the load of the service without too many ﬂuctuations, making it easier
to establish thresholds for the autoscaler. Figure 12 shows the throughput out
of the video stream service during the test. This metric is able to represent
the status of the service, since throughput is directly related to the number
of active streams. However, the rapid ﬂuctuations in the values due to the
accumulated changes in the throughput rate of each individual stream make
it diﬃcult to establish thresholds for it to be used in the autoscaler.
Fig. 12 Video Stream throughput with custom metrics.
From these tests, we conclude that the custom metrics enable a more com-
plete and versatile monitoring system. The custom exporters are a signiﬁcant
improvement, since they allow to conﬁgure any additional metric which may
seem necessary for a speciﬁc service instead of relying solely on the metrics
available from the metric server.
4.3 Service Stress Tests
Before implementing the HPA, we need to analyse each services’ behavior and
identify which resources are critical to its execution, and the amount that
should be used to scale the number of pods so that the service runs smoothly.
The load tests will be based on the values of the second to last day, since the
last day corresponds to a holiday with anomalous sessions numbers (Figure
13).
Journal of Cloud Computing
20
Marques et al.
Fig. 13 Number of sessions per hour in a single day
To evaluate the beneﬁts of automatically scaling applications based on the
collected metrics, we must ﬁrst determine which metrics are most critical to
the correct operation of the service. Figure 14 shows that the memory usage
reaches the maximum possible value regardless of the number of active streams.
Fig. 14 Video Stream memory usage without HPA.
This behaviour makes this metric unsuitable to be used in the Autoscaler,
and shows that the parameters monitored by default in Kubernetes are, in
some cases, not capable of giving an accurate depiction of the status of the
service. This indicates the need for additional speciﬁc metrics. In this way, the
values of active streams shown in Figure 15 are much more representative of
the status of the service. This test was done without any automatic scaling
(without HPA), therefore, no additional replicas were created to respond to the
increase of users which resulted in the service completely stopping to respond,
as we can see in both Figures 14 and 15.
This situation shows the importance of allocating resources dynamically
to prevent both scenarios where the services are unable to respond to the
Journal of Cloud Computing
Marques et al.
21
Fig. 15 Video Stream active streams without HPA.
incoming workload and where resources are being wasted during periods of
low workloads.
4.4 Stress Tests with optimized HPA
Figures 16 and 17 show the active sessions and the number of instantiated
pods during the test when using the HPA to automatically adjust resources.
It is notorious that the HPA is able to keep the metrics that are being tracked
for each service within acceptable values, and is able to prevent the service
from stopping to respond to higher workloads. However, in order to improve
the quality of service and take full advantage of the Autoscaler, we must make
changes to its conﬁguration and reduce its reactiveness by adding a stabiliza-
tion period to both scaling up and down operations, thus reducing the workload
placed on the orchestrator by rapidly creating and deleting pods.
Fig. 16 Video Stream active streams per pod with HPA.
In the following tests, a stabilization period is added. This requires the
value of a metric to be above or below the threshold for a conﬁgurable period
of time before signaling a scale up or down. Additionally, it requires a policy
to limit the rate at which pods are added or removed to 1 every 15 seconds.
These conditions are added to the HPA conﬁguration and are used to assess
whether keeping the pods longer, even though the metric value might be below
the threshold, can reduce the load on the Orchestrator without leading to
situations where we overrun the provision of resources for long periods of time.
Journal of Cloud Computing
22
Marques et al.
Fig. 17 Video Stream number of pods with HPA.
Figures 18 and 19 show the active sessions and the number of instantiated
pods during the test when using the optimized HPA with a stabilization period.
Similarly to the tests with the default conﬁguration of the Autoscaler, the
streams are split between the pods, preventing a single pod from being satu-
rated and unable to respond to the clients. However, there are far less instances
of pods being created or deleted during this test, which can be conﬁrmed in
Figure 19.
Fig. 18 Video Stream active streams per pod with optimized HPA.
Fig. 19 Video Stream number of pods with optimized HPA.
With default conﬁgurations, a total of 10 pods are used during the test.
However, the addition of the stabilization period reduced the reactivity of the
Autoscaler which made pod deletion less frequent, and so the total number of
Journal of Cloud Computing
Marques et al.
23
pods used for this test was only 4, resulting in a lesser load for the Orchestrator
and a smaller number of dropped streams on pod deletions.
4.5 Stress Tests with Machine Learning Predictor
By itself, the Autoscaler with stabilization period is an eﬀective way of
responding to large spikes in client requests. However, it is a reactive system,
and there is a delay in the response to these spikes. By adding a predictive mod-
ule, we can minimize this delay using machine learning algorithms to forecast
the number of incoming sessions and thus ensuring the desired QoS.
First, we need to create a model to forecast the number of sessions in the
vehicular network. The forecasting task consists of, given the past values of
the number of sessions per hour in the vehicular network y(0), y(1), ..., y(t)
and other possible additional features, forecast the value y(t + 1) , where y(t)
represents the number of sessions in the network at hour t.
Fig. 20 Vehicular network dataset split in training set, cross-validation set and test set.
With the current dataset, two weeks will be used as cross-validation data
(from the 31 of October to the 14 of November) to optimize the forecasting
models, and the last two weeks (from the 14 of November to the 28 of Novem-
ber) will be used as test data, to perform the ﬁnal tests and calculate the
performance metrics. The previous weeks (from the 9 of September to the 31
of October) will be used as training data (Figure 20).
In this work, we chose to use the predictor module developed by Ferreira et
al [19]. The performance metric used to evaluate the accuracy of the forecasts
will be the Root Mean Square Error (RMSE), which is the square root of the
average of squared diﬀerences between the forecasts and the actual observa-
tions. Predictors with lower RMSE values are more accurate than those with
higher RMSE values.
Next, we discuss the performance of the monitoring and management sys-
tem with the prediction service. Figure 21 shows the distribution of sessions
Journal of Cloud Computing
24
Marques et al.
in pods using the optimized Autoscaler, without the prediction module. Since
the Autoscaler needs to wait for session spikes to occur before it commands the
Orchestrator to create an additional pod, the original pod will have to endure
the user spike by itself, which can cause the degradation of service perfor-
mance. The annotation (i) shows a period in which a spike of users occurred.
During these periods, the average number of sessions in a single pod may far
exceed its threshold and cause the pod to stop responding.
Fig. 21 Active streams per pod without the prediciton module.
The addition of a forecasting module enables the Autoscaler to anticipate
the client spikes and react preemptively, minimizing the time a pod exceeds the
threshold of active sessions. Figure 22 depicts the distribution of sessions per
pod using the prediction module. In this case, the Autoscaler creates additional
pods preemptively, allowing the pods to have time to start up before the spike
of clients actually occurs in the period indicated by annotation (i).
Fig. 22 Average active streams with prediction.
Figure 23 shows the distribution of streams per pod, and how the predictor
enables the autoscaler to anticipate client spikes and create additional pods
preemptively (i). Figure 24 shows the number of sessions forecasted by the
prediction module. The predicted values are very similar to the real ones shown
in Figure 25, demonstrating the accuracy of the predictor.
The initial tests were performed in a local Kubernetes cluster with limited
resources with the objective of determining if the tools provided by Kubernetes
Journal of Cloud Computing
Marques et al.
25
Fig. 23 Active streams per pod using the prediction module.
Fig. 24 Forecasted number of sessions.
Fig. 25 Average active streams per pod.
were suﬃcient to accurately monitor any generic service that may be deployed.
Following, we tested the custom metrics pipeline using the Prometheus frame-
work to complement the metrics server and enable the collection of custom
service-speciﬁc metrics. This model proved to be much more eﬀective as it
allows the operator to add new metrics for each service as needed, and was
used as a basis for the tests below.
The next objective was to take advantage of the collected metrics to make
automated scaling decisions based on them. The tests were done in the Veniam
Kubernetes cluster, since the local deployment lacked the resources to simulate
the required numbers of users. Initially, the tests were done without any scaling
algorithms, which led to situations where the services crashed and/or stopped
responding because they lacked the resources to respond to the amount of
Journal of Cloud Computing
26
Marques et al.
users being simulated, underlining the need for an automated scaling system
capable of responding to workload spikes in real time. A scaling system was
proposed using the Kubernetes HPA, and the custom metrics were collected
from the services to trigger scaling operations whenever the selected metric
surpassed certain thresholds. This addition was not enough to provide major
beneﬁts, since the Autoscaler was far too reactive to changes in the workloads
which were themselves highly volatile. In fact, the Autoscaler was permanently
scaling up and down, disconnecting users and placing a big workload in the
Orchestrator, so in order to make the system viable, the algorithm had to be
reviewed.
The most eﬀective way to reduce Autoscaler reactivity was to introduce a
stabilization period on the Autoscaler, either by increasing or decreasing the
scale. This modiﬁcation greatly improved the system performance, services
still reacted quickly to changes in workloads, preventing services from running
out or wasting resources. While eﬃcient, the monitoring system is still not fast
enough to prevent any variation triggering a scaling operation, reducing the
load placed on the orchestrator and user disconnection rates.
Finally, we evaluated the integration of a prediction service that allows the
system to anticipate workloads variations and adjust proactively instead of
reactively. We began by analysing the research done to select the most accurate
predictor for our scenario, and used the vehicular dataset previously referred
for training the predictor. The results of the load tests with the predictor
module show reduced periods of threshold violations, since the orchestrator
is able to proactively allocate resources, improving the response time of the
system and preventing service degradation caused by insuﬃcient resources.
5 Conclusion and Future work
This article proposed a framework for a Kubernetes based cloud, focusing on
collecting comprehensive metrics about agnostic services with diﬀerent require-
ments, on automating scaling operations and predicting load surges using a
forecasting module. The proposed monitoring and management system was
extended to provide autoscaling and to include a load prediction service that
allows the system to act preventively in order to maintain the desired QoS
while dealing with the eﬃcient use of resources in peaks of low consumption.
The Predictor Service publishes predicted user session values for the next time
period as service metrics, which are collected by the monitoring system and
used by the orchestrator to make predictive scheduling decisions. By using
these service metrics, the monitoring system improved the response time on
escalation responses and reduced service degradation. These improvements
were made possible by variable workloads compared to reactive monitoring
systems.
The proposed system is well suited for micro services environments, due to
its ability to collect customized real time performance metrics from services.
This allows system managers to overcome the limitations of the standalone
Journal of Cloud Computing
Marques et al.
27
Kubernetes monitoring tools, and have a complete view of status of the applica-
tions deployed, detect malfunctions and conﬁgure alerts. The system can easily
integrate workload predictors that enable it to predict variations in service
workloads, and ensures the provisioning of resources to the deployed services
by automatically scaling applications according to workloads, minimizing the
waste of resources.
As future work, there are some elements that can be enhanced: monitor
and analyse additional pods, such as load balancers for an even more complete
view of the system; automate the behavioral analysis of services to discover
which of the collected metrics is critical for each service; and create service
speciﬁc predictors instead of a central prediction module to reduce the load
placed on the prediction module, and increase the modularity of the system.
Acknowledgements This work is supported by the European Regional
Development Fund (FEDER), through the Regional Operational Programme
of Lisbon (POR LISBOA 2020) and the Competitiveness and Internation-
alization Operational Programme (COMPETE 2020) of the Portugal 2020
framework Projects City Catalyst POCI-01-0247-FEDER-046119 and IMMI-
NENCE POCI-01-0247-FEDER-112314.
Declarations
• Ethical Approval: not applicable.
• Competing interests: not applicable.
• Authors’ contributions:
G. Marques: designed the monitoring system and analyzed the results, pre-
pared the evaluation scenarios, implemented the monitoring system and
contributed to the writing of the paper.
C. Senna: designed the monitoring system and analyzed the results, pre-
pared the evaluation scenarios, supervised the entire research process and
contributed to the writing of the paper.
S. Sargento: designed the monitoring system and analyzed the results, pre-
pared the evaluation scenarios, supervised the entire research process and
contributed to the writing of the paper.
L. Carvalho: supervised the entire research process and contributed to the
writing of the paper.
L. Pereira: supervised the entire research process and contributed to the
writing of the paper.
R. Matos: supervised the entire research process and contributed to the
writing of the paper.
• Funding:
This work is supported by the European Regional Development Fund
(FEDER), through the Regional Operational Programme of Lisbon (POR
Journal of Cloud Computing
28
Marques et al.
LISBOA 2020) and the Competitiveness and Internationalization Oper-
ational Programme (COMPETE 2020) of the Portugal 2020 framework
Projects City Catalyst POCI-01-0247-FEDER-046119 and IMMINENCE
POCI-01-0247-FEDER-112314.
• Availability of data and materials: not applicable.
References
[1] O. Zimmermann, Microservices tenets. Computer Science-Research and
Development 32(3), 301–310 (2017)
[2] W.H.C. Almeida, L. de Aguiar Monteiro, R.R. Hazin, A.C. de Lima,
F.S. Ferraz, Survey on microservice architecture-security, privacy and
standardization on cloud computing environment. ICSEA 2017 p. 210
(2017)
[3] C. Esposito, A. Castiglione, K.K.R. Choo, Challenges in delivering soft-
ware in the cloud as microservices. IEEE Cloud Computing 3(5), 10–14
(2016). https://doi.org/10.1109/MCC.2016.105
[4] P. Jamshidi, C. Pahl, N.C. Mendon¸ca, J. Lewis, S. Tilkov, Microservices:
The journey so far and challenges ahead. IEEE Software 35(3), 24–35
(2018). https://doi.org/10.1109/MS.2018.2141039
[5] K. Naseer Qureshi, F. Bashir, S. Iqbal, Cloud computing model for vehic-
ular ad hoc networks, in 2018 IEEE 7th International Conference on
Cloud Networking (CloudNet) (2018), pp. 1–3. https://doi.org/10.1109/
CloudNet.2018.8549536
[6] C.B. Hauser, S. Wesner, Reviewing cloud monitoring: Towards cloud
resource proﬁling, in 2018 IEEE 11th International Conference on Cloud
Computing (CLOUD) (2018), pp. 678–685.
https://doi.org/10.1109/
CLOUD.2018.00093
[7] S. Al-Shammari, A. Al-Yasiri, Monslar: a middleware for monitoring sla
for restful services in cloud computing, in 2015 IEEE 9th International
Symposium on the Maintenance and Evolution of Service-Oriented and
Cloud-Based Environments (MESOCA) (2015), pp. 46–50. https://doi.
org/10.1109/MESOCA.2015.7328126
[8] S. Lee, S. Son, J. Han, J. Kim, Reﬁning micro services placement over
multiple kubernetes-orchestrated clusters employing resource monitoring,
in 2020 IEEE 40th International Conference on Distributed Comput-
ing Systems (ICDCS) (2020), pp. 1328–1332. https://doi.org/10.1109/
ICDCS47774.2020.00173
Journal of Cloud Computing
Marques et al.
29
[9] A. Koschel, A. Hausotter, R. Buchta, A. Grunewald, M. Lange, P. Nie-
mann, Towards a microservice reference architecture for insurance compa-
nies, in SERVICE COMPUTATION 2021 : The Thirteenth International
Conference on Advanced Service Computing (Porto, Portugal, 2021)
[10] I.K. Kim, W. Wang, Y. Qi, M. Humphrey, Empirical evaluation of
workload forecasting techniques for predictive cloud resource scaling, in
2016 IEEE 9th International Conference on Cloud Computing (CLOUD)
(2016), pp. 1–10. https://doi.org/10.1109/CLOUD.2016.0011
[11] F.J. Baldan, S. Ramirez-Gallego, C. Bergmeir, F. Herrera, J.M. Benitez, A
forecasting methodology for workload forecasting in cloud systems. IEEE
Transactions on Cloud Computing 6(4), 929–941 (2018). https://doi.org/
10.1109/TCC.2016.2586064
[12] R. Weing¨artner, G.B. Br¨ascher, C.B. Westphall, Cloud resource manage-
ment: A survey on forecasting and proﬁling models. Journal of Network
and Computer Applications 47, 99–106 (2015).
https://doi.org/https:
//doi.org/10.1016/j.jnca.2014.09.018.
URL https://www.sciencedirect.
com/science/article/pii/S1084804514002252
[13] H.
Wang,
Y.
Ma,
X.
Zheng,
X.
Chen,
L.
Guo,
Self-adaptive
resource management framework for software services in cloud, in
2019 IEEE Intl Conf on Parallel Distributed Processing with Appli-
cations, Big Data Cloud Computing, Sustainable Computing Com-
munications,
Social
Computing
Networking
(ISPA/BDCloud/Social-
Com/SustainCom) (2019), pp. 1528–1529.
https://doi.org/10.1109/
ISPA-BDCloud-SustainCom-SocialCom48970.2019.00223
[14] L. Chen, Y. Xu, Z. Lu, J. Wu, K. Gai, P.C.K. Hung, M. Qiu, Iot microser-
vice deployment in edge-cloud hybrid environment using reinforcement
learning. IEEE Internet of Things Journal 8(16), 12,610–12,622 (2021).
https://doi.org/10.1109/JIOT.2020.3014970
[15] H. Zhao, H. Lim, M. Hanif, C. Lee, Predictive container auto-scaling
for cloud-native applications, in 2019 International Conference on Infor-
mation and Communication Technology Convergence (ICTC) (2019), pp.
1280–1282. https://doi.org/10.1109/ICTC46691.2019.8939932
[16] S. Zhou, J. Li, K. Zhang, M. Wen, Q. Guan, An accurate ensemble
forecasting approach for highly dynamic cloud workload with vmd and
r-transformer. IEEE Access 8, 115,992–116,003 (2020). https://doi.org/
10.1109/ACCESS.2020.3004370
[17] B. Liu, J. Guo, C. Li, Y. Luo, Workload forecasting based elastic
resource management in edge cloud. Computers & Industrial Engineer-
ing 139, 106,136 (2020).
https://doi.org/https://doi.org/10.1016/j.cie.
Journal of Cloud Computing
30
Marques et al.
2019.106136.
URL https://www.sciencedirect.com/science/article/pii/
S0360835219306059
[18] Y. Zhu, W. Zhang, Y. Chen, H. Gao, A novel approach to work-
load prediction using attention-based lstm encoder-decoder network in
cloud environment. EURASIP Journal on Wireless Communications and
Networking 2019(1), 1–18 (2019)
[19] D. Ferreira, C. Senna, S. Sargento, Distributed real-time forecasting
framework for iot network and service management, in NOMS 2020 - 2020
IEEE/IFIP Network Operations and Management Symposium (IEEE
Press, 2020), p. 1–4. https://doi.org/10.1109/NOMS47738.2020.9110456.
URL https://doi.org/10.1109/NOMS47738.2020.9110456
[20] P.M. Santos, J.G.P. Rodrigues, S.B. Cruz, T. Louren¸co, P.M. d’Orey,
Y. Luis, C. Rocha, S. Sousa, S. Cris´ostomo, C. Queir´os, S. Sargento,
A. Aguiar, J. Barros, Portolivinglab: An iot-based sensing platform for
smart cities. IEEE Internet of Things Journal 5(2), 523–532 (2018). https:
//doi.org/10.1109/JIOT.2018.2791522
[21] Sandvine,
Global
internet
phenomena
report.
Technical
report,
Sandvine
(2012).
URL
https://www.sandvine.com/hubfs/
Sandvine Redesign 2019/Downloads/Internet%20Phenomena/
2012-2h-global-internet-phenomena-report.pdf


</subsection_point_Point 2>

<previous_sections>

A systematic review of automated systems for real-time irrigation management

1. INTRODUCTION
The challenge of feeding a growing population with finite resources is becoming increasingly pressing. By 2050, the world population is expected to reach 9.7 billion, necessitating a 70% increase in food production (Falkenmark and Rockstrom, 2009). Irrigation plays a crucial role in enhancing crop yields and agricultural productivity to meet this growing demand. Studies have shown that irrigation can significantly increase crop water productivity, contributing to increased food production (Ali and Talukder, 2008; Playan and Mateos, 2005). However, water scarcity poses a significant challenge, with many regions facing water deficits and the need for improved water management practices (Falkenmark and Rockstrom, 2009). Optimizing irrigation schedules and doses based on crop requirements and environmental conditions is essential for maximizing yield and quality while minimizing water use (Zhang et al., 2024). The necessity of scalable water-efficient practices for increasing food demand cannot be overstated. Techniques such as regulated deficit irrigation, magnetically treated water, and the use of drought-tolerant crops like sorghum have shown promise in improving water productivity and ensuring food security (Mehmood et al., 2023; Putti et al., 2023; Hadebe et al., 2016). As the global food challenge intensifies, it is imperative to critically evaluate the current state and future potential of irrigation management systems to guide research, innovation, and implementation efforts towards fully autonomous, scalable solutions.

Despite the importance of irrigation in addressing the global food challenge, traditional irrigation management techniques, such as manual scheduling and timer-based systems, have significant limitations. These methods are often labor-intensive, inefficient, and less adaptable to changing conditions (Savin et al., 2023). Manual and timer-based scheduling can lead to high operational costs and inefficient water use (Raghavendra, Han, and Shin, 2023). The reliance on manual intervention and predetermined schedules limits their adaptability to changing environmental conditions, crop water requirements, and soil moisture levels (Kaptein et al., 2019). Sensor-based irrigation systems offer an alternative, enabling real-time adjustments based on soil water status measurements (Kaptein et al., 2019). However, the adoption of these systems in commercial settings has been limited, often requiring extensive input from researchers (Kim et al., 2014; Lea-Cox et al., 2018; Ristvey et al., 2018). The limitations of traditional irrigation management techniques highlight the need for scalable, automated solutions for greater efficiency in irrigation management. Automated systems that collect real-time data, analyze it, and make autonomous irrigation decisions can lead to improved water use efficiency and increased crop productivity (Champness et al., 2023; Wu et al., 2022). To fully understand the potential of automated systems, it is necessary to examine the automation of each part of the irrigation management pipeline and analyze the effectiveness and efficiency of integrated end-to-end solutions.

The emergence of smart irrigation management and IoT marks a significant shift from historical irrigation practices. Modern approaches rely on vast data and analysis algorithms, leveraging technologies such as remote sensing, sensor networks, weather data, and computational algorithms (Atanasov, 2023; Bellvert et al., 2023; Kumar et al., 2023). IoT plays a vital role in collecting vast amounts of data through sensors, data transmission, and tailored networks, enabling real-time monitoring and control of irrigation systems (Liakos, 2023; Zuckerman et al., 2024). These advancements in data collection and analysis have the potential to revolutionize irrigation management, allowing for more precise and efficient water use. However, challenges such as processing diverse data sources, data integration, and lack of integrated data analysis hamper the full benefit of IoT in irrigation management (Dave et al., 2023). The current fragmented approach in smart irrigation, focusing on individual components rather than the entire system, limits the potential for fully autonomous, real-time end-to-end irrigation management (Togneri et al., 2021). To address these challenges and fully realize the potential of smart irrigation management, there is a need for automating and integrating each section of the irrigation management pipeline, from sensor/weather data collection and transmission to processing, analysis, decision-making, and automated action (McKinion and Lemmon, 1985). This integration requires a thorough investigation of the role of interoperability and standardization in enabling seamless communication and compatibility between components within the automated irrigation management pipeline.

Machine learning (ML) plays a significant role in processing vast data, predicting plant stress, modeling climate effects, and optimizing irrigation in smart irrigation management systems. ML algorithms can analyze data collected from sensors and weather stations to determine optimal irrigation schedules (Vianny et al., 2022). However, the potential of ML is often constrained by manual steps, such as data interpretation, decision-making on irrigation timing and volume, and system adjustments. Automating ML integration to allow direct action from insights to irrigation execution, removing bottlenecks and achieving real-time adaptability, is crucial for fully autonomous irrigation management (Barzallo-Bertot et al., 2022). By integrating ML into automated systems, the irrigation management pipeline can become more seamless and efficient, enabling real-time decision-making and action based on data-driven insights. To achieve this level of automation and integration, it is essential to identify gaps and propose solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.

To achieve seamless integration across the automated irrigation management system, interoperability and standardization are critical. Interoperability allows different system components, such as sensors, actuators, and software, to communicate and exchange data effectively, while standardization ensures that data is represented in a consistent format (Santos et al., 2020). Standardized protocols and data formats are essential for achieving seamless integration and ensuring compatibility between components in real-time irrigation management systems (Robles et al., 2022; Hatzivasilis et al., 2018). Existing and emerging standards, such as OGC SensorThings API and ISO 11783, have applicability to real-time irrigation management systems (Hazra et al., 2021). However, challenges such as data quality, scalability, reliability, and security need to be addressed to fully realize the potential of interoperability and standardization in automated irrigation management systems (Hazra et al., 2021). Addressing these challenges is crucial for enabling the seamless integration of components within the automated irrigation management pipeline, which is essential for achieving fully autonomous, scalable irrigation management. A comprehensive evaluation of the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline is necessary to guide future research and implementation efforts.
The primary objective of this systematic review is to critically evaluate the current state and future potential of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies for enhancing agricultural water use efficiency and crop productivity.
Specific objectives include:
•	Examining the automation of each part of the irrigation management pipeline and the seamless integration of each section in the context of irrigation scheduling and management.
•	Analyzing the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
•	Investigating the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline.
•	Identifying gaps and proposing solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.
By addressing these objectives, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge.

2. REVIEW METHODOLOGY
•	Question-driven framework to guide the literature review of real-time, autonomous irrigation management systems
•	Key research questions posed, each with the motivation behind investigating them and a starting hypothesis to evaluate against the examined literature
•	Table presenting the major objectives, specific objectives, questions, motivations, and hypotheses
3. DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING
3.1. Irrigation management data
The success of automated irrigation management systems relies heavily on the collection, transmission, and analysis of various types of data. The most applicable data types for irrigation management include soil moisture, canopy temperature, weather data, and plant physiological parameters (Farooq et al., 2019; Li et al., 2019; Olivier et al., 2021; Evett et al., 2020). These data are typically collected from a range of sources, including in-field sensors, remote sensing platforms, weather stations, and manual measurements (Li et al., 2019; Karimi et al., 2018).
Soil moisture data is arguably the most critical type of data for irrigation management, as it directly reflects the water available to plants and can be used to determine the optimal timing and amount of irrigation (Olivier et al., 2021; Intrigliolo & Castel, 2006). Soil moisture sensors, such as tensiometers, capacitance probes, and time-domain reflectometry (TDR) sensors, can provide real-time measurements of soil water content at various depths (Farooq et al., 2019). These sensors can be deployed in a network configuration to capture spatial variability in soil moisture across a field (Karimi et al., 2018).
Canopy temperature data is another valuable type of data for irrigation management, as it can be used to assess plant water stress and adjust irrigation accordingly (Evett et al., 2020). Infrared thermometers and thermal cameras can be used to measure canopy temperature, which is influenced by factors such as air temperature, humidity, wind speed, and plant water status (Li et al., 2019). When plants experience water stress, they tend to close their stomata to reduce water loss, leading to an increase in canopy temperature (Evett et al., 2020). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
Weather data, including temperature, humidity, precipitation, wind speed, and solar radiation, are essential for predicting crop water requirements and scheduling irrigation events (Akilan & Baalamurugan, 2024). Weather stations equipped with various sensors can provide real-time measurements of these parameters, which can be used as inputs for crop water requirement models, such as the FAO-56 Penman-Monteith equation (Li et al., 2019). These models estimate crop evapotranspiration (ET) based on weather data and crop-specific coefficients, allowing for the calculation of irrigation requirements (Intrigliolo & Castel, 2006). By integrating weather data into automated irrigation systems, irrigation schedules can be dynamically adjusted based on changing environmental conditions, ensuring that crops receive the optimal amount of water at the right time (Akilan & Baalamurugan, 2024).
When collecting and utilizing these data types, several considerations must be taken into account, including the volume, frequency, format, and source of the data (Farooq et al., 2019). The volume of data generated by automated irrigation systems can be substantial, especially when high-resolution sensors are deployed at a large scale (Bastidas Pacheco et al., 2022). This necessitates the use of efficient data storage, processing, and transmission technologies to handle the data load (Farooq et al., 2019). The frequency of data collection is another important consideration, as it directly impacts the temporal resolution of the data and the ability to detect rapid changes in plant water status or environmental conditions (Bastidas Pacheco et al., 2022). Bastidas Pacheco et al. (2022) demonstrated that collecting full pulse resolution data from water meters provides more accurate estimates of event occurrence, timing, and features compared to aggregated temporal resolutions, highlighting the importance of selecting appropriate data collection frequencies to ensure the quality and usefulness of the data for irrigation management.
The format of the data is also crucial, as it determines the compatibility and interoperability of the data with various analysis tools and platforms (Farooq et al., 2019). Standardized data formats, such as JSON, XML, or CSV, can facilitate data exchange and integration between different components of the automated irrigation system (Zhang et al., 2023). The source of the data is another important consideration, as it can impact the reliability, accuracy, and spatial coverage of the data (Farooq et al., 2019). For example, in-field sensors provide highly localized measurements, while remote sensing platforms, such as satellites or drones, can provide data at larger spatial scales (Li et al., 2019). By combining data from multiple sources, automated irrigation systems can achieve a more comprehensive understanding of crop water requirements and optimize irrigation management accordingly (Farooq et al., 2019).
Data quality, accuracy, and reliability are paramount in irrigation management, as they directly impact the effectiveness of decision-making processes and the efficiency of water use (Gupta et al., 2020). Inaccurate or unreliable data can lead to suboptimal irrigation decisions, resulting in crop stress, yield losses, or wasted water resources (Ramli & Jabbar, 2022). Gupta et al. (2020) emphasized the critical importance of data security and privacy in smart farming systems, as the leakage of sensitive agricultural data can cause severe economic losses to farmers and compromise the integrity of the automated irrigation system. The authors also highlighted the need for robust authentication and secure communication protocols to prevent unauthorized access to smart farming systems and protect data in transit (Gupta et al., 2020).
Ramli and Jabbar (2022) addressed the challenges associated with implementing real-time, automated irrigation systems, including data quality, scalability, reliability, and security. They proposed solutions and best practices based on the analysis of case studies and real-world implementations, such as the use of redundant sensors, data validation techniques, and secure communication protocols (Ramli & Jabbar, 2022). The authors also emphasized the importance of regular maintenance and calibration of sensors to ensure the accuracy and reliability of the collected data (Ramli & Jabbar, 2022).
Researchers have investigated the use of data compression, aggregation, and filtering techniques to reduce bandwidth requirements and improve transmission efficiency in automated irrigation systems (Karim et al., 2023; Rady et al., 2020; Cui, 2023). Karim et al. (2023) explored the effectiveness of various data compression techniques, such as lossless and lossy compression algorithms, in reducing the size of data packets transmitted over wireless networks. The authors found that lossless compression techniques, such as Huffman coding and Lempel-Ziv-Welch (LZW), can significantly reduce data size without compromising data quality, while lossy compression techniques, such as JPEG and MP3, can further reduce data size by introducing acceptable levels of distortion (Karim et al., 2023).
Rady et al. (2020) developed a novel data compression algorithm specifically designed for irrigation data, which achieved significant compression ratios without compromising data quality. The authors demonstrated that their algorithm could reduce the amount of data transmitted over wireless networks, thereby improving the efficiency of the irrigation system and reducing costs (Rady et al., 2020). Cui (2023) investigated the use of data aggregation and filtering techniques to reduce the number of transmissions and save bandwidth in automated irrigation systems. The author proposed a data aggregation scheme that combines multiple sensor readings into a single value, such as the average soil moisture over a specified time interval, to reduce the frequency of data transmissions (Cui, 2023). Additionally, the author explored the use of data filtering techniques, such as Kalman filters and particle filters, to remove noise and outliers from sensor data, improving the accuracy and reliability of the transmitted information (Cui, 2023).
Data standardization and harmonization are crucial for facilitating seamless integration and interoperability between the various components of automated irrigation management systems (Zhang et al., 2023; Ermoliev et al., 2022). Zhang et al. (2023) developed a novel cyberinformatics technology called iCrop, which enables the in-season monitoring of crop-specific land cover across the contiguous United States. The authors highlighted the importance of data standardization and harmonization in the context of iCrop, as it allows for the efficient distribution of crop-specific land cover information based on the findable, accessible, interoperable, and reusable (FAIR) data principle (Zhang et al., 2023). By adopting standardized data formats and protocols, such as the Open Geospatial Consortium (OGC) standards, iCrop enables the seamless integration of various data sources and facilitates the interoperability of the system with other agricultural decision support tools (Zhang et al., 2023).
Ermoliev et al. (2022) proposed a linkage methodology for linking distributed sectoral/regional optimization models in a situation where private information is not available or cannot be shared by modeling teams. The authors emphasized the need for data standardization to enable decentralized cross-sectoral coordination and analysis, as it allows for the consistent representation and exchange of data between different models and stakeholders (Ermoliev et al., 2022). By adopting standardized data formats and interfaces, the proposed linkage methodology can facilitate the integration of various optimization models and support the development of comprehensive decision support systems for sustainable resource management (Ermoliev et al., 2022).
Metadata plays a vital role in providing context and enabling better data interpretation and decision-making in automated irrigation management systems (Jahanddideh-Tehrani et al., 2021). Metadata refers to the additional information that describes the characteristics, quality, and context of the primary data, such as the sensor type, calibration parameters, measurement units, and timestamp (Jahanddideh-Tehrani et al., 2021). Jahanddideh-Tehrani et al. (2021) highlighted the importance of metadata in water resources management, as it enables decision-makers to use the data to the best of its capabilities by understanding factors such as when water data was collected and what factors might have contributed to the measurements. The authors emphasized the need for standardized metadata formats and guidelines, such as the Dublin Core Metadata Initiative (DCMI) and the ISO 19115 standard, to ensure the consistency and interoperability of metadata across different water information systems (Jahanddideh-Tehrani et al., 2021).
In the context of automated irrigation management systems, metadata can provide valuable information about the data collection process, sensor performance, and environmental conditions that can aid in data interpretation and decision-making (Cota & Mamede, 2023). For example, metadata about the sensor type and calibration parameters can help assess the accuracy and reliability of the collected data, while metadata about the weather conditions and soil properties can provide context for interpreting the data and adjusting irrigation strategies accordingly (Cota & Mamede, 2023). By incorporating metadata into the data management and analysis pipeline of automated irrigation systems, decision-makers can make more informed and context-aware decisions, leading to improved water use efficiency and crop productivity (Jahanddideh-Tehrani et al., 2021).

3.2. Edge Computing and Fog Computing
Edge computing and fog computing have emerged as transformative technologies in the realm of real-time irrigation management systems, offering significant potential for improving efficiency, scalability, and reliability (Abdel Nasser et al., 2020; Tran et al., 2019). Edge computing refers to the practice of processing data near the edge of the network, close to the source of the data, while fog computing is a decentralized computing infrastructure that extends cloud computing capabilities to the network edge (Hassija et al., 2019). These technologies bring computation and analytics closer to the data source, reducing the need for data to travel to the cloud and enabling faster processing and decision-making (Hassija et al., 2019; Zhang et al., 2020).
The potential of edge computing and fog computing in real-time irrigation management is immense. Abdel Nasser et al. (2020) proposed a two-layer system for water demand prediction using automated meters and machine learning techniques, demonstrating the potential of edge computing in improving the efficiency and scalability of irrigation management. The system collects and aggregates data from distributed smart meters in the first layer, while the second layer uses LSTM neural networks to predict water demand for different regions of households. By leveraging edge computing, the system can achieve high accuracy in predicting water demand, which is essential for efficient irrigation management (Abdel Nasser et al., 2020).
Tran et al. (2019) conducted a comprehensive review of real-time, end-to-end automated irrigation management systems, highlighting the role of fog computing in addressing data transmission challenges and enabling seamless integration across the irrigation management pipeline. The authors emphasize that real-time, end-to-end automated irrigation management systems have the potential to significantly improve water efficiency, crop yields, and reduce labor costs. However, they also identify several challenges that need to be addressed, such as data quality, scalability, reliability, and security, which can be effectively tackled by implementing fog computing architectures (Tran et al., 2019).
Edge computing offers several benefits in real-time irrigation management systems, including reduced latency, real-time decision-making, and reduced reliance on cloud connectivity (Mishra, 2020; Zhang et al., 2020). By processing data closer to the source, edge computing enables faster response times and more efficient data handling (Mishra, 2020). Mishra (2020) highlights that edge computing reduces latency by processing data closer to the source, enabling real-time decision-making and lessening reliance on cloud connectivity by shifting processing to local or edge devices.
Zhang et al. (2020) explore the application of edge computing in agricultural settings, demonstrating its potential to improve the efficiency and accuracy of irrigation systems. The authors discuss how edge computing has prospects in various agricultural applications, such as pest identification, safety traceability of agricultural products, unmanned agricultural machinery, agricultural technology promotion, and intelligent management. They also emphasize that the emergence of edge computing models, such as fog computing, cloudlet, and mobile edge computing, has transformed the management and operation of farms (Zhang et al., 2020).
Fog computing plays a crucial role in distributing processing and storage across the network, enhancing the scalability and reliability of automated irrigation systems (Premkumar & Sigappi, 2022; Singh et al., 2022). Premkumar and Sigappi (2022) evaluate the current state of automated irrigation management systems and propose a hybrid machine learning approach for predicting soil moisture and managing irrigation. Their study emphasizes the potential of fog computing in distributing processing and storage across the network, improving the efficiency and scalability of irrigation systems. The proposed hybrid machine learning approach outperforms other machine learning algorithms in predicting soil moisture, demonstrating the effectiveness of fog computing in enhancing the performance of automated irrigation systems (Premkumar & Sigappi, 2022).
Singh et al. (2022) discuss the role of fog computing in distributing processing and storage across the network, enhancing scalability and reliability in agricultural management systems. The authors argue that by implementing fog computing, these systems can achieve faster data processing and response times, improving overall efficiency and effectiveness. They also highlight that fog computing can address the challenges faced by real-time data transmission in agricultural management systems, such as latency, bandwidth limitations, and data security (Singh et al., 2022).
The integration of edge and fog computing in real-time irrigation management systems is crucial for achieving fully automated, scalable, and reliable solutions. As the demand for autonomous irrigation management grows, these technologies will play a pivotal role in enabling faster decision-making, reduced latency, improved resource utilization, and seamless integration across the irrigation management pipeline (Tran et al., 2019; Zhang et al., 2020). By bringing computation and analytics closer to the data source and distributing processing and storage across the network, edge and fog computing can significantly enhance the efficiency and effectiveness of automated irrigation systems, contributing to the overall goal of addressing the global food challenge through optimized water resource management and increased agricultural productivity (Abdel Nasser et al., 2020; Premkumar & Sigappi, 2022; Singh et al., 2022).

3.3. Automation of Data Collection
The automation of data collection is a critical component in the development of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. It enables the efficient gathering of vital information about crop health, environmental conditions, and water requirements, which is essential for enhancing agricultural water use efficiency and crop productivity. Two key aspects of automated data collection are the use of advanced sensing technologies for non-invasive plant stress detection and the implementation of wireless sensor networks and energy-efficient communication protocols for large-scale, long-term data collection.
Advanced sensing technologies, such as hyperspectral imaging and thermal sensing, have emerged as powerful tools for non-invasive plant stress detection in automated irrigation management systems. These technologies provide valuable information about crop traits, enabling early and accurate detection of plant health issues (Triantafyllou et al., 2019). Triantafyllou et al. (2019) propose a comprehensive reference architecture model that incorporates advanced sensing technologies in the sensor layer for real-time plant stress detection, highlighting their importance in providing non-invasive plant stress detection. Similarly, Hossain et al. (2023) present a novel IoT-ML-Blockchain integrated framework for smart agricultural management that leverages advanced sensing technologies to optimize water use and improve crop yield, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
Hyperspectral imaging can capture subtle changes in plant physiology that are indicative of stress, while machine learning algorithms can be employed to extract meaningful patterns from the spectral data and classify different stress types (Araus et al., 2014). Thermal sensing can detect changes in canopy temperature, which is influenced by factors such as plant water status (Li et al., 2019). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
The integration of advanced sensing technologies in automated irrigation management systems has the potential to revolutionize precision agriculture. Jiang et al. (2019) demonstrate the effectiveness of a deep learning-based model in accurately detecting leaf spot diseases, highlighting the importance of image augmentation and deep learning algorithms in enhancing the model's performance.
Wireless sensor networks (WSNs) and energy-efficient communication protocols have the potential to significantly improve the efficiency and reliability of data collection in large-scale, long-term irrigation systems. WSNs offer a cost-effective and scalable solution for real-time data collection in large-scale irrigation systems, providing remote monitoring and automated control capabilities (Mehdizadeh et al., 2020). Nishiura and Yamamoto (2021) propose a novel sensor network system that utilizes drones and wireless power transfer to autonomously collect environmental data from sensor nodes in vast agricultural fields, reducing operational costs and enhancing the efficiency of data collection. Similarly, Higashiura and Yamamoto (2021) introduce a network system that employs UAVs and LoRa communication to efficiently collect environmental data from sensor nodes distributed across large farmlands, optimizing data collection and reducing travel distance and time.
Energy-efficient communication protocols are crucial for ensuring reliable data transmission in challenging environmental conditions and extending the lifespan of sensor nodes (Mehdizadeh et al., 2020). Al-Ali et al. (2023) investigate the potential of WSNs and energy-efficient communication protocols for data collection in large-scale, long-term irrigation systems, discussing the challenges and opportunities of using these technologies to improve the efficiency and reliability of real-time data collection in irrigation management. Mehdizadeh et al. (2020) emphasize the need for careful consideration of factors such as data accuracy, energy consumption, and network reliability when designing effective WSNs for irrigation management, enabling timely irrigation decisions and improved crop yields.
The automation of data collection through the use of advanced sensing technologies and wireless sensor networks is essential for achieving fully autonomous, scalable irrigation management. By enabling non-invasive plant stress detection and large-scale, long-term data collection, these technologies contribute to the overall goal of optimizing water resource management and increasing agricultural productivity. The integration of these technologies in real-time, end-to-end automated irrigation management systems has the potential to enhance agricultural water use efficiency and crop productivity, ultimately contributing to the development of fully autonomous, scalable irrigation management solutions.

3.4: Real-Time Data Transmission Protocols and Technologies
Real-time data transmission is a critical component of automated irrigation management systems, as it enables the timely delivery of sensor data to the cloud for processing and decision-making. The exploration of suitable protocols and network architectures is essential for ensuring efficient and reliable data transmission in these systems, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
The Message Queuing Telemetry Transport (MQTT) protocol has emerged as a popular choice for real-time data transmission in IoT networks, including those used for automated irrigation management. MQTT is a lightweight, publish-subscribe protocol designed for constrained devices and low-bandwidth networks (Author, 2019). Its simplicity and low overhead make it well-suited for IoT applications where data transmission speed and energy efficiency are critical (Saranyadevi et al., 2022). MQTT provides three Quality of Service (QoS) levels, ensuring data reliability in real-time scenarios (Author, 2019). Chen et al. (2020) proposed novel algorithms to improve data exchange efficiency and handle rerouting in MQTT-based IoT networks for automated irrigation management systems. Their TBRouting algorithm efficiently finds the shortest paths for data transmission, while the Rerouting algorithm effectively handles the rerouting of topic-based session flows when a broker crashes. The combination of these algorithms can significantly improve the performance and reliability of automated irrigation management systems (Chen et al., 2020).
Client-server IoT networks, such as those based on MQTT, play a crucial role in real-time data transmission for automated irrigation management systems. In these networks, sensors and devices (clients) publish data to a central broker (server), which then distributes the data to subscribed clients (Verma et al., 2021). This architecture enables efficient data collection, processing, and dissemination, facilitating the integration of various components within the automated irrigation management pipeline. Verma et al. (2021) proposed an architecture for healthcare monitoring systems using IoT and communication protocols, which provides a comprehensive overview of existing approaches and highlights challenges and opportunities in the field. Although focused on healthcare, the insights from this study can be applied to automated irrigation management systems, emphasizing the importance of interoperability and standardization for seamless integration (Verma et al., 2021).
In addition to MQTT, other application layer protocols such as XMPP, CoAP, SOAP, and HTTP have been explored for real-time data transmission in IoT networks. Each protocol has its strengths and weaknesses, making them suitable for different applications and scenarios. XMPP (Extensible Messaging and Presence Protocol) is an open-standard protocol that supports real-time messaging, presence, and request-response services (Saint-Andre, 2011). CoAP (Constrained Application Protocol) is a specialized web transfer protocol designed for use with constrained nodes and networks in the Internet of Things (Shelby et al., 2014). SOAP (Simple Object Access Protocol) is a protocol for exchanging structured information in the implementation of web services, while HTTP (Hypertext Transfer Protocol) is the foundation of data communication for the World Wide Web (Fielding et al., 1999).
Motamedi and Villányi (2022) compared and evaluated wireless communication protocols for the implementation of smart irrigation systems in greenhouses, considering factors such as power consumption, range, reliability, and scalability. They found that ZigBee is the most suitable local communication protocol for greenhouse irrigation due to its large number of nodes and long range, while MQTT is the recommended messaging protocol for smart irrigation systems due to its TCP transport protocol and quality of service (QoS) options. GSM is a reliable and cost-effective global communication protocol for greenhouse irrigation, providing wide coverage and low cost (Motamedi & Villányi, 2022).
Syafarinda et al. (2018) investigated the use of the MQTT protocol in a precision agriculture system using a Wireless Sensor Network (WSN). They found that MQTT is suitable for use in IoT applications due to its lightweight, simple, and low bandwidth requirements. The average data transmission speed using the MQTT protocol was approximately 1 second, demonstrating its effectiveness for real-time data transmission in precision agriculture systems (Syafarinda et al., 2018).
The choice of application layer protocol for real-time irrigation management depends on factors such as data transmission speed, reliability, and energy efficiency. MQTT and RTPS (Real-Time Publish-Subscribe) are both suitable for real-time data transmission in IoT systems, but they have different strengths and weaknesses. MQTT is a better choice for applications that require low latency and high throughput, while RTPS is a better choice for applications that require high reliability and low latency (Sanchez-Iborra & Skarmeta, 2021). The exploration of MQTT and client-server IoT networks, along with the comparison of various application layer protocols, provides valuable insights into the suitability of these technologies for real-time data transmission in automated irrigation management systems.
In summary, real-time data transmission protocols and technologies play a vital role in the automation of irrigation management systems, enabling the efficient and reliable delivery of sensor data to the cloud for processing and decision-making. The exploration of MQTT and client-server IoT networks, along with the comparison of application layer protocols, highlights the importance of selecting suitable technologies based on factors such as data transmission speed, reliability, and energy efficiency. By leveraging these technologies, automated irrigation management systems can achieve seamless integration and contribute to the overall goal of enhancing agricultural water use efficiency and crop productivity.

3.5. Challenges and Solutions in Real-Time Data Transmission
Following the exploration of data collection, processing at the edge and fog, and automation in previous sections, we now turn to the critical aspect of real-time data transmission. While essential for automated irrigation management, this stage presents unique challenges that must be addressed to ensure system efficiency and reliability.
Obstacles in Real-Time Data Transmission
Agricultural environments present unique challenges for real-time data transmission, directly impacting the effectiveness of automated irrigation systems. Environmental factors can significantly disrupt wireless communication. Adverse weather conditions such as heavy rain, fog, and high winds can weaken or even block radio signals, leading to data loss and compromised system performance. Physical obstacles like trees, buildings, and uneven terrain further complicate signal propagation, creating reliability issues (Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). These environmental challenges necessitate robust communication protocols and network architectures that can ensure consistent and reliable data flow.
In addition to environmental factors, technical limitations also present significant obstacles. Large-scale agricultural operations often demand long-distance data transmission, which can be hindered by the limited range of certain wireless communication protocols. Network congestion, occurring when multiple sensors transmit data concurrently, can lead to delays and potential data loss, further complicating real-time decision-making (Hameed et al., 2020). To mitigate these issues, researchers have investigated the potential of cognitive radio networks (CRNs) and dynamic spectrum access (DSA) for optimizing spectrum utilization and reducing interference (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). CRNs enable devices to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency. DSA, on the other hand, facilitates the dynamic allocation of unused spectrum, enhancing spectrum utilization and reducing congestion.
Furthermore, data security and privacy are paramount concerns in real-time irrigation systems. The sensitive nature of agricultural data, such as crop yields and farm management practices, necessitates robust security measures to prevent unauthorized access and data breaches (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential to protect data integrity and ensure the trustworthiness of the system.
Investigating Data Optimization Techniques
To enhance the efficiency and reliability of real-time data transmission in automated irrigation systems, researchers have explored a range of data optimization techniques. Data compression techniques aim to reduce the size of data packets transmitted over the network, minimizing bandwidth requirements and improving transmission speed (Rady et al., 2020; Karim et al., 2023). Lossless compression algorithms, such as Huffman coding and LZW, preserve data integrity while effectively reducing data size, ensuring that no information is lost during transmission (Cui, 2023). Lossy compression algorithms, such as JPEG and MP3, offer higher compression ratios but introduce a controlled level of data loss, which may be acceptable for certain applications where some loss of precision is tolerable (Karim et al., 2023). The choice between lossless and lossy compression depends on the specific application and the trade-off between data size and accuracy.
Data aggregation techniques provide another effective approach to optimize data transmission. By aggregating multiple sensor readings into a single representative value, such as average soil moisture or temperature, the number of transmissions can be significantly reduced, conserving bandwidth and energy resources (Cui, 2023). This is particularly beneficial in large-scale irrigation systems where numerous sensors are deployed across vast areas, generating substantial amounts of data. Additionally, data filtering techniques play a crucial role in improving data quality and reliability. Kalman filters and particle filters can effectively remove noise and outliers from sensor data, ensuring that only accurate and relevant information is transmitted and used for decision-making (Cui, 2023). This is essential for preventing erroneous data from influencing irrigation decisions and potentially leading to suboptimal water management.
Sensor calibration, drift correction, and fault detection are essential for maintaining data accuracy and reliability (Dos Santos et al., 2023). Regular calibration ensures that sensors provide accurate measurements over time, while drift correction techniques account for gradual changes in sensor readings due to environmental factors or aging. Fault detection mechanisms can identify and address sensor malfunctions or anomalies, preventing erroneous data from influencing irrigation decisions and potentially harming crops or wasting water.
Addressing the Challenges
Effectively addressing the challenges in real-time data transmission requires a multifaceted approach that encompasses environmental, technical, and data-related considerations. Implementing robust and adaptive communication protocols is crucial for overcoming interference and signal degradation caused by weather conditions and physical obstacles. Selecting appropriate protocols, such as LoRa or ZigBee, with suitable range and penetration capabilities can ensure reliable data transmission in challenging agricultural environments (Motamedi & Villányi, 2022). Additionally, employing techniques like frequency hopping and error correction codes can further improve communication resilience and mitigate data loss.
Optimizing network architecture is another key consideration. Deploying a distributed network architecture with edge and fog computing capabilities can significantly enhance data processing and transmission efficiency (Abdel Nasser et al., 2020; Tran et al., 2019). Edge devices can perform initial data processing and aggregation tasks, reducing the amount of data transmitted to the cloud and minimizing latency, while fog nodes can provide additional processing power and storage closer to the data source, enhancing scalability and reliability. This distributed approach alleviates the burden on the central cloud server and allows for more responsive and efficient irrigation management.
Data optimization techniques play a vital role in reducing bandwidth requirements and improving transmission efficiency. The choice of data compression, aggregation, and filtering techniques should be tailored to the specific requirements of the irrigation system, considering factors such as data type, accuracy needs, and available bandwidth. By carefully selecting and implementing these techniques, the overall performance and effectiveness of real-time irrigation systems can be significantly enhanced, leading to more sustainable water management practices and improved agricultural productivity.
By addressing these challenges and implementing appropriate solutions, real-time data transmission can become a reliable and efficient component of automated irrigation systems, contributing to the overall goal of achieving sustainable and productive agriculture in the face of growing food demands and water scarcity.

3.6. IoT Network Architectures and Variable Rate Irrigation (VRI) for Real-Time Irrigation
Real-time irrigation management systems heavily rely on the efficient and reliable transmission of data from sensors and weather stations to the cloud for processing and decision-making. However, agricultural environments present unique challenges to wireless communication, including adverse weather conditions, physical obstacles, and the limitations of wireless technologies. These challenges necessitate robust and adaptive solutions to ensure the consistent and timely flow of data, enabling truly autonomous irrigation scheduling.
Environmental factors, such as heavy rain, fog, and strong winds, can significantly disrupt wireless communication by attenuating or even blocking radio signals, leading to data loss and compromised system performance (Ed-daoudi et al., 2023; Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). Dense vegetation, buildings, and uneven terrain create further complications by causing multipath propagation and shadowing effects (Yim et al., 2018; Gautam and Pagay, 2020). The study by Yim et al. (2018) on LoRa networks in a tree farm environment exemplifies these challenges, revealing reduced communication range and data reliability compared to theoretical expectations. This underscores the need for carefully selecting and optimizing communication protocols and network parameters to ensure reliable data transmission in such environments.
The study by Guzinski et al. (2014a) using a modified TSEB model further highlights the importance of high-resolution data in accurately capturing the spatial and temporal dynamics of energy fluxes influenced by environmental factors. This emphasizes the need for advanced data acquisition and processing techniques that can effectively represent the complexities of agricultural settings.
The limitations of traditional wireless communication technologies, such as limited range and network congestion, pose additional challenges for large-scale agricultural operations. Long-distance data transmission can be hindered by range limitations, while network congestion arising from numerous sensors transmitting concurrently can lead to delays and data loss, hindering real-time decision-making (Hameed et al., 2020). Addressing these challenges requires the exploration of advanced networking technologies that can optimize spectrum utilization, mitigate interference, and improve reliability and efficiency.
Cognitive Radio Networks (CRNs) and Dynamic Spectrum Access (DSA) offer promising solutions for optimizing wireless communication in agricultural settings. CRNs empower devices with the ability to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). Research has explored the potential of CRNs in predicting Radio Frequency (RF) power to avoid noisy channels and optimize spectrum utilization (Iliya et al., 2014; Iliya et al., 2014). These studies demonstrate the effectiveness of combining optimization algorithms with artificial neural networks (ANNs) to enhance the accuracy and generalization of RF power prediction, enabling CRNs to make informed decisions about channel selection and avoid interference.
DSA complements CRN technology by dynamically allocating unused spectrum, further enhancing spectrum utilization and reducing congestion (Shi et al., 2023). The numerical model developed by Shi et al. (2023) showcases the potential of CRNs and DSA for optimizing wireless communication in challenging environments.
The integration of CRNs and DSA into the IoT network architecture requires careful consideration of spectrum sensing techniques, network topology, and data security. Research on cooperative spectrum sensing suggests that distributed approaches, where sensor nodes collaborate and share information, can significantly improve the accuracy and efficiency of spectrum sensing, particularly in dynamic environments (Trigka and Dritsas, 2022; Khalid & Yu, 2019). This collaborative approach enables a more comprehensive understanding of the radio environment and facilitates the identification of available frequency bands for data transmission.
The choice of network topology also impacts the performance and scalability of CRN-based irrigation systems. Mesh networks, where sensor nodes are interconnected and relay data for each other, offer enhanced resilience and coverage compared to star topologies where nodes communicate directly with a central gateway (Akyildiz & Vuran, 2010). However, mesh networks can be more complex to manage and may introduce additional routing overhead. The trade-off between network resilience and complexity needs to be carefully evaluated to select the most appropriate topology for a specific agricultural setting.
Data security and privacy are paramount concerns in IoT-based irrigation systems due to the sensitive nature of agricultural data (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential for protecting data integrity and ensuring system trustworthiness. Research on secure spectrum leasing and resource allocation algorithms for CR-WSN-based irrigation systems has demonstrated the potential of these technologies for enhancing security and efficiency (Hassan, 2023; Afghah et al., 2018).
In conclusion, the development of effective and reliable real-time irrigation management systems requires a comprehensive approach that addresses the challenges of data transmission in agricultural environments. The integration of robust and adaptive communication protocols, optimized network architectures, and advanced networking technologies like CRNs and DSA, along with a focus on data security and privacy, can contribute significantly to achieving the goal of autonomous and efficient irrigation scheduling.
4. AUTOMATED DATA PROCESSING IN THE CLOUD
4.1. Data Quality and Preprocessing
Data quality is paramount in automated irrigation systems as it directly influences the effectiveness of decision-making and water use efficiency. Issues like missing values, inconsistencies, and outliers arising from sensor malfunctions, environmental interference, or network problems (Lv et al., 2023) can significantly impact the performance of machine learning models used for irrigation scheduling and management.
Real-time data cleaning techniques are essential for addressing these challenges. Kalman filtering proves particularly effective in handling missing values and correcting erroneous readings by recursively estimating the system's state based on previous measurements and current sensor data, taking into account noise and uncertainty (Kim et al., 2020). Moving average techniques, by averaging consecutive data points, provide a more stable representation of the underlying trend, filtering out short-term fluctuations (Chhetri, 2023). For outlier detection, adaptive thresholding methods offer a dynamic approach, adjusting thresholds based on the statistical properties of the data to effectively identify anomalies and minimize false positives (Bah et al., 2021). These techniques are crucial in maintaining the integrity of real-time data streams and ensuring the accuracy of subsequent analyses.
Adaptive data preprocessing is essential for managing the diversity of data sources and formats commonly found in irrigation systems. Data normalization techniques, such as min-max scaling or z-score normalization, ensure that all features contribute equally to the analysis by transforming data values to a common scale (Pradal et al., 2016). This is crucial for preventing features with larger values from dominating the analysis and ensuring that all features are given equal consideration. Similarly, feature scaling methods, like standardization or normalization, optimize the range of feature values to improve the performance and convergence of machine learning models (Tortorici et al., 2024). By scaling features to a similar range, the influence of outliers is reduced, and the model's ability to learn from the data is enhanced.
Data fusion techniques play a critical role in integrating information from diverse sources, creating a more comprehensive and reliable dataset for irrigation management. Dempster-Shafer theory, a generalization of probability theory, allows for the expression of both uncertainty and the degree of conflict in evidence, making it suitable for fusing uncertain and conflicting data from heterogeneous sources (Sadiq and Rodriguez, 2004). This is particularly relevant in irrigation systems where data from different sensors may provide slightly different or even contradictory information due to sensor variations or environmental factors. Bayesian inference offers another powerful framework for combining information from multiple sources, updating the probability of a hypothesis as new evidence becomes available. By applying these techniques, data from soil moisture sensors, canopy temperature sensors, weather stations, and other sources can be integrated to provide a holistic understanding of crop water requirements and environmental conditions, leading to more informed and accurate irrigation decisions.
The impact of data quality extends beyond model accuracy to the robustness of machine learning models under varying conditions. Robust models should maintain consistent performance even when faced with data inconsistencies or unexpected situations. Techniques like data augmentation and domain adaptation can enhance model robustness by exposing the model to a wider range of data variations during training. Data augmentation involves generating additional training data by applying transformations or introducing noise to existing data, making the model more resilient to noise and variations in the real-world data. Domain adaptation techniques aim to adapt a model trained on one domain (e.g., a specific crop or geographic location) to perform well on another domain with different data characteristics. This is particularly relevant in irrigation management, where models may need to be applied to different crops, soil types, or climatic conditions.
The choice of data cleaning, preprocessing, and fusion techniques should be carefully considered based on the specific characteristics of the irrigation system and the available data. By selecting and implementing appropriate techniques, the accuracy, reliability, and robustness of machine learning models can be significantly improved, leading to more efficient and sustainable irrigation management practices.
4.2. Scalable and Autonomous Deployment using Containerization Strategies
The transition from data collection and transmission to efficient data processing requires a robust infrastructure capable of handling diverse workloads and data volumes. Containerization technologies, specifically Docker and Kubernetes, offer a promising solution for deploying and scaling data processing and machine learning modules within cloud environments like AWS, Azure, and GCP (Vargas-Rojas et al., 2024; Rosendo et al., 2022; Solayman & Qasha, 2023). Docker provides a standardized way to package applications and their dependencies into self-contained units known as containers, ensuring consistent and reproducible execution across different platforms (Rosendo et al., 2022). Kubernetes, acting as a container orchestrator, manages their deployment, scaling, and networking across a cluster of machines (Rosendo et al., 2022). This combination presents several advantages for automated irrigation management systems.
Firstly, containerization facilitates efficient resource utilization and scalability. By encapsulating applications and their dependencies, containers enable the isolation of resources and prevent conflicts between different modules (Vargas-Rojas et al., 2024; Solayman & Qasha, 2023). This isolation allows for the efficient allocation of resources, such as CPU, memory, and storage, to each container based on its specific needs. Kubernetes further enhances scalability by allowing for the automatic scaling of containers based on real-time demand, ensuring the system can adapt to varying workloads and data volumes, preventing bottlenecks, and ensuring responsiveness to changing conditions (Karamolegkos et al., 2023).
Secondly, containerization promotes portability and reproducibility. By packaging applications and their dependencies into a single unit, containers make it easy to move and deploy them across different cloud environments without the need for environment-specific configurations (Rosendo et al., 2022; Solayman & Qasha, 2023). This portability simplifies the development and deployment process, reducing the time and effort required to set up and manage the system. Additionally, containers ensure reproducibility by providing a consistent execution environment, regardless of the underlying infrastructure. This eliminates variability and ensures that the system will behave consistently across different deployments (Zhou et al., 2023).
Optimizing container orchestration and resource allocation is crucial to minimizing latency and maximizing throughput in real-time data processing pipelines. Techniques like auto-scaling and dynamic resource allocation play a critical role in this context (Hethcoat et al., 2024; Werner and Tai, 2023; Kumar et al., 2024). Auto-scaling automatically adjusts the number of container instances based on real-time demand, ensuring that sufficient resources are available to handle peak workloads while avoiding over-provisioning during periods of low demand (Hethcoat et al., 2024; Kumar et al., 2024). Dynamic resource allocation enables the fine-grained adjustment of resources allocated to each container based on its specific needs and the current workload (Werner and Tai, 2023). This ensures efficient resource allocation and provides each container with the necessary resources to perform its tasks effectively.
Performance monitoring tools, such as Kubernetes Metrics Server and Prometheus, are essential for gaining insights into the performance of containers and the overall system (Hethcoat et al., 2024; Kuity & Peddoju, 2023). These tools provide valuable data on key performance indicators, such as CPU and memory usage, network traffic, and application-specific metrics. By monitoring this data, administrators can identify bottlenecks, optimize resource allocation strategies, and continuously improve system performance (Hethcoat et al., 2024). This data-driven approach ensures that automated irrigation management systems can operate efficiently and reliably.
By integrating containerization technologies with optimization techniques and performance monitoring, automated irrigation management systems achieve the scalability, autonomy, and efficiency required for effective real-time data processing and decision-making. This approach facilitates a seamless and responsive system that can adapt to changing conditions and contribute to the overall goal of optimizing water resource management and increasing agricultural productivity.
4.3. Deploying ML Models for Data Processing
•	Architectures and frameworks for deploying machine learning models on cloud platforms for real-time data processing and inference in irrigation management systems, such as: TensorFlow Serving, Apache MXNet Model Server, ONNX Runtime
•	Techniques for optimizing machine learning model performance and resource utilization in cloud environments, such as: Model compression (e.g., pruning, quantization), Hardware acceleration (e.g., GPU, TPU), Distributed training (e.g., Horovod, BytePS)
•	Integration of deployed machine learning models with other components of the automated irrigation management pipeline, such as data preprocessing, decision-making, and control systems, using protocols like: MQTT, CoAP, RESTful APIs
4.4. Online Learning in the Cloud
•	Application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as: Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests
•	Architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as: Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms
•	Strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making, using techniques such as: Multi-armed bandits, Bayesian optimization, Reinforcement learning (e.g., Q-learning, SARSA)


5. GENERATING AND APPLYING IRRIGATION INSIGHTS 
5.1. Real-Time Generation of Actionable Irrigation Insights
•	Advanced predictive models, such as deep learning (e.g., LSTM, CNN) and ensemble methods (e.g., Random Forests), for precise, site-specific irrigation recommendations
•	Integration of IoT sensor data (e.g., soil moisture probes, weather stations) and cloud-based data sources (e.g., weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filtering) to enhance insight accuracy and resolution
•	Strategies for handling data heterogeneity, uncertainty, and quality issues in real-time insight generation, such as data preprocessing and outlier detection
•	Techniques for reducing computational complexity and latency, such as edge computing (e.g., fog computing), model compression (e.g., quantization), and hardware accelerators (e.g., GPUs)
5.2. Automated Application of Irrigation Insights
•	Architectures and protocols for seamless integration of ML-generated insights with IoT-enabled irrigation control systems, such as MQTT and CoAP for lightweight, real-time communication
•	Analysis of industry-leading products and services, such as smart irrigation controllers (e.g., Rachio) and cloud-based irrigation management platforms (e.g., CropX)
•	Strategies for ensuring reliability, security, and scalability of automated insight application, such as redundant communication channels and secure edge-to-cloud architectures
•	Case studies of successful implementations of closed-loop, autonomous irrigation systems in research and commercial settings, highlighting technologies used and benefits achieved

6. INTEGRATION, INTEROPERABILITY, AND STANDARDIZATION 
6.1. Interoperability and Standardization
•	Importance of interoperability and standardization in enabling seamless integration of automated irrigation components
•	Overview of existing and emerging standards for IoT devices, communication protocols, and data formats in precision agriculture (e.g., ISOBUS, agroXML, SensorML)
•	Role of standardization bodies and industry consortia in promoting interoperability (e.g., AgGateway, Open Ag Data Alliance, Agricultural Industry Electronics Foundation)
•	Challenges in adopting and implementing standards across diverse hardware and software platforms
•	Strategies for encouraging widespread adoption of standards and best practices for interoperability in automated irrigation systems
6.2. Integration with Existing Irrigation Infrastructure
•	Challenges and strategies for retrofitting legacy irrigation systems with IoT sensors, actuators, and communication devices
•	Hardware compatibility issues and solutions (e.g., adapters, modular designs)
•	Software and firmware updates to enable integration with automated decision-making systems
•	Data integration and normalization techniques for merging legacy and new data sources
•	Economic and practical considerations for transitioning from manual to automated irrigation management
•	Cost-benefit analysis of upgrading existing infrastructure vs. implementing new systems
•	Phased implementation approaches to minimize disruption and optimize resource allocation
•	Training and support requirements for farmers and irrigation managers adopting automated systems
•	Case studies and real-world examples of successful integration of automated irrigation with existing infrastructure
6.3. Integration with Other Precision Agriculture Technologies
•	Synergies between automated irrigation and complementary technologies
•	Remote sensing (satellite, UAV, and ground-based) for crop monitoring and evapotranspiration estimation
•	Soil moisture sensors and weather stations for real-time, localized data collection
•	Variable rate application systems for precise irrigation delivery based on crop requirements
•	Yield mapping and analytics for assessing the impact of automated irrigation on crop productivity
•	Architectures and frameworks for integrating diverse data sources and technologies into a unified precision agriculture ecosystem
•	Edge computing and fog computing paradigms for real-time data processing and decision-making
•	Cloud-based platforms for data storage, analysis, and visualization
•	API-driven approaches for modular integration of third-party services and applications
•	Challenges and solutions for ensuring data quality, consistency, and security across integrated precision agriculture systems
•	Data cleaning, preprocessing, and harmonization techniques
•	Blockchain and distributed ledger technologies for secure, tamper-proof data sharing and traceability
•	Access control and authentication mechanisms for protecting sensitive data and resources
•	Future trends and research directions in the integration of automated irrigation with advanced precision agriculture technologies (e.g., AI-driven crop modeling, robotics, and autonomous vehicles)
6.4. Cybersecurity Considerations for Integrated Automated Irrigation Systems
•	Unique security risks and vulnerabilities associated with IoT-based automated irrigation systems
•	Potential for unauthorized access, data tampering, and system manipulation
•	Implications of security breaches for crop health, water resource management, and farm productivity
•	Best practices and strategies for securing automated irrigation systems
•	Secure device provisioning and authentication (e.g., hardware security modules, certificates)
•	Encryption and secure communication protocols (e.g., TLS, DTLS)
•	Firmware and software updates to address emerging security threats
•	Network segmentation and access control to limit the impact of breaches
•	Role of cybersecurity standards and frameworks in guiding the development and deployment of secure automated irrigation systems (e.g., NIST CSF, IEC 62443)
•	Importance of user awareness, training, and incident response planning in maintaining the security of integrated automated irrigation systems

7. MONITORING AND ENSURING SYSTEM RELIABILITY
7.1. Resilience and Fault Tolerance in Automated Irrigation Systems
•	Strategies for ensuring robustness and reliability in the face of failures, disruptions, or unexpected events
•	Redundancy: Implementing redundant components, such as duplicate sensors (e.g., soil moisture sensors, weather stations), controllers (e.g., PLCs, microcontrollers), and communication channels (e.g., cellular, satellite, LoRaWAN) to maintain system functionality during component failures
•	Failover mechanisms: Designing seamless failover mechanisms that automatically switch to backup components or systems in case of primary system failure, such as hot-standby controllers or multi-path communication protocols (e.g., mesh networks, software-defined networking)
•	Self-healing capabilities: Incorporating AI-driven self-healing mechanisms that can detect, diagnose, and recover from faults without human intervention, using techniques like reinforcement learning, Bayesian networks, or self-organizing maps
•	The role of distributed architectures and edge computing in enhancing system resilience
•	Decentralizing critical functions and data processing to minimize the impact of single points of failure, using fog computing or multi-agent systems
•	Leveraging edge computing to enable localized decision-making and control, reducing dependence on cloud connectivity and improving response times, using technologies like Raspberry Pi, NVIDIA Jetson, or Intel NUC
•	Anomaly detection and predictive maintenance using AI techniques
•	Employing unsupervised learning algorithms (e.g., autoencoders, clustering) to detect anomalies in sensor data, system performance, and water usage patterns
•	Developing predictive maintenance models using techniques like long short-term memory (LSTM) networks, convolutional neural networks (CNNs), or gradient boosting machines (GBMs) to anticipate and prevent potential system failures based on historical data and real-time monitoring
7.2. Advanced Monitoring Techniques for Automated Irrigation Systems
•	Remote monitoring using IoT-enabled sensors and computer vision
•	Deploying a heterogeneous network of IoT sensors to collect real-time data on soil moisture (e.g., capacitive, tensiometric), temperature (e.g., thermocouples, thermistors), humidity (e.g., capacitive, resistive), and plant health (e.g., sap flow, leaf wetness)
•	Integrating high-resolution cameras (e.g., multispectral, hyperspectral) and computer vision algorithms for visual monitoring of crop growth, disease detection (e.g., using deep learning-based object detection and segmentation), and irrigation system performance (e.g., leak detection, sprinkler uniformity)
•	Transmitting sensor and camera data to cloud-based platforms (e.g., AWS IoT, Google Cloud IoT, Microsoft Azure IoT) for remote access and analysis using protocols like MQTT, CoAP, or AMQP
•	Innovative approaches for real-time system health assessment
•	Developing novel algorithms and metrics for evaluating the health and performance of automated irrigation systems, such as entropy-based measures, network resilience indices, or multi-criteria decision analysis (MCDA) frameworks
•	Combining data from multiple sources (e.g., sensors, weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filters, Dempster-Shafer theory) to create a comprehensive view of system health
•	Employing advanced data visualization techniques (e.g., interactive dashboards, augmented reality) to present system health information in an intuitive and actionable format
7.3. Closed-Loop Control and Feedback Mechanisms
•	Exploring the concept of closed-loop control in autonomous irrigation systems
•	Implementing feedback loops that continuously monitor system performance and adjust irrigation schedules based on real-time data, using control techniques like proportional-integral-derivative (PID), model predictive control (MPC), or fuzzy logic control (FLC)
•	Integrating machine learning algorithms (e.g., reinforcement learning, genetic algorithms) to optimize closed-loop control strategies over time, adapting to changing environmental conditions and crop requirements
•	Designing effective feedback mechanisms for user interaction and system optimization
•	Providing user-friendly interfaces (e.g., mobile apps, web dashboards) for farmers to input preferences, constraints, and expert knowledge into the automated irrigation system, using techniques like participatory design or user-centered design
•	Incorporating user feedback and domain expertise to refine irrigation strategies and improve system performance
8. CASE STUDIES AND REAL-WORLD IMPLEMENTATIONS OF FULLY AUTONOMOUS IRRIGATION SYSTEMS
8.1. Fully Autonomous Irrigation Systems in Diverse Agricultural Settings
•	Row Crops: maize, wheat, soybean with real-time soil moisture monitoring and weather-based irrigation scheduling for fully automated precision irrigation
•	Orchards: citrus, apple, almond with plant health monitoring and precision water application for fully autonomous orchard management
•	Greenhouses: tomato, lettuce, herbs with automated drip irrigation and climate control integration for fully automated greenhouse operations
•	Urban Farming: rooftop gardens, vertical farms with IoT-enabled hydroponic systems and remote management for fully autonomous urban crop production
8.2. Integration of Advanced System Components for End-to-End Automation
•	Wireless sensor networks: soil moisture probes, weather stations, plant health monitoring cameras with low-power, long-range communication for fully automated data acquisition
•	Secure data transmission: LoRaWAN, NB-IoT, 5G, satellite communication for reliable, real-time data transfer from field to cloud in fully autonomous irrigation systems
•	Intelligent data processing: edge computing for local data filtering, cloud platforms for scalable storage and analysis, machine learning algorithms for predictive insights in fully automated irrigation management
•	Autonomous decision-making: advanced irrigation scheduling algorithms, precise valve control, closed-loop feedback systems for optimal water management in fully autonomous irrigation systems
8.3. Quantitative Performance Evaluation of Fully Automated Irrigation Systems
•	Water use efficiency: percent reduction in water consumption compared to conventional methods, improved water productivity (yield per unit of water) achieved through fully autonomous irrigation
•	Crop yield and quality improvements: percent increase in yield, enhanced crop uniformity, improved nutritional content attributed to fully automated precision irrigation
•	Labor and energy savings: quantified reduction in labor hours for irrigation management, decreased energy consumption for pumping due to optimized scheduling in fully autonomous systems
•	Economic viability: detailed return on investment analysis, payback period calculations, comprehensive cost-benefit analysis for fully autonomous irrigation management systems
8.4. Lessons Learned and Challenges Encountered in Deploying Autonomous Irrigation Systems
•	Technical challenges and solutions: ensuring reliable data transmission in remote locations, addressing interoperability issues between diverse system components, optimizing power consumption for extended battery life, adapting algorithms to local soil and weather conditions in fully autonomous irrigation systems
•	Operational and logistical hurdles: streamlining installation and maintenance procedures, providing effective user training, seamlessly integrating with existing farm management practices and legacy systems for fully automated irrigation management
•	Regulatory and socio-economic considerations: navigating complex water use regulations, addressing data privacy and security concerns, ensuring equitable access and affordability for smallholder farmers adopting fully autonomous irrigation technologies
8.5. Best Practices and Recommendations for Successful Implementation
•	Designing scalable, modular, and adaptable autonomous irrigation systems to accommodate future growth and changing requirements for fully automated water management
•	Prioritizing user-centered design principles and actively engaging stakeholders throughout the development and deployment process of fully autonomous irrigation solutions
•	Adopting open standards and communication protocols to enable seamless integration of system components and interoperability with third-party platforms in fully automated irrigation setups
•	Implementing robust data validation, filtering, and quality control mechanisms to ensure data integrity and reliability for decision-making in fully autonomous irrigation systems
•	Establishing clear data governance policies and security frameworks to protect sensitive information and maintain user trust in fully automated irrigation management
•	Developing intuitive user interfaces and decision support tools to facilitate easy adoption and effective use of fully autonomous irrigation systems
•	Collaborating with local extension services, agribusinesses, and technology providers for knowledge transfer, technical support, and continuous improvement of fully automated irrigation solutions
8.6. Synthesis of Case Studies and Implications for Autonomous Irrigation Adoption
•	Cross-case analysis of key performance indicators and critical success factors for fully autonomous irrigation scheduling systems in various contexts
•	Identification of common themes, challenges, and innovative solutions across diverse implementations of end-to-end fully automated irrigation management
•	Assessment of the potential for replicability and scaling of successful fully autonomous irrigation projects in different regions and farming systems
•	Implications for future research priorities, technology development roadmaps, and policy interventions to support widespread adoption of fully autonomous irrigation technologies

CONCLUSION/FUTURE DIRECTIONS AND UNANSWERED QUESTIONS
•	Summarize the key insights gained from the question-driven review, emphasizing how each section contributes to the overarching goal of achieving real-time, end-to-end automation in irrigation management
•	Based on the questions addressed, propose new research directions and unanswered questions
•	Identify key research gaps and propose concrete research questions and hypotheses for advancing the field of real-time, automated irrigation management
•	Highlight the need for collaborative research efforts across disciplines, such as computer science, agricultural engineering, and environmental science, to address the complex challenges of automated irrigation systems
•	Emphasize the need for further innovation and exploration in real-time, automated irrigation systems



</previous_sections>

</documents>
<instructions>


Use the information provided in the <documents> tags to write the next subsection of the research paper, following these steps:
1. Review the overall intention of the research paper, specified in the <review_intention> tag. Ensure the subsection you write aligns with and contributes to this overall goal.
2. Consider the specific intention for this subsection of the paper, stated in the <section_intention> tag. The content you write should fulfill this purpose. 
3. Use the title provided in the <subsection_title> tag as the heading for the subsection. 
4. Address each of the points specified in the </subsection_point_Point *> tags:
   a) Make a clear case for each point using the text provided in the "point" field.
   b) Support each point with evidence from the research papers listed in the corresponding "papers to support point" field.
   c) When citing a paper to support a point, include inline citations with the author name(s) and year, e.g. (Smith et al., 2020; Johnson and Lee, 2019; Brown, 2018). Cite all papers that strengthen or relate to the point being made.
   d) While making a point and citing the supporting papers, provide a brief explanation in your own words of how the cited papers support the point.
5. Ensure that both of the points from the <subsection_point> tags are fully addressed and supported by citations. Do not skip or combine any points.
6. After addressing the specified points, wrap up the subsection with a concluding sentence or two that ties the points together and relates them back to the <section_intention>.
7. Review the <Previous_sections> of the paper, and ensure that the new subsection you have written fits logically and coherently with the existing content. Add transition sentences as needed to improve the flow.
8. Proofread the subsection to ensure it is clear, concise, and free of grammatical and spelling errors. Maintain a formal academic tone and style consistent with the rest of the research paper.
9. Format the subsection using Markdown, including the subsection heading (using ## or the equivalent for the document), inline citations, and any other formatting needed for clarity and readability.
10. If any information is missing or unclear in the provided tags, simply do your best to write the subsection based on the available information. Do not add any information or make any points not supported by the provided content. Prioritize fully addressing the required points over hitting a specific word count.

The output should be a complete, well-organized, and properly cited subsection ready to be added to the research paper.

Begin your answer with a brief recap of the instructions stating what you will to optimize the quality of the answer. Clearly and briefly state the subsection you'll be working on and the points you'll be addressing. Then proceed to write the subsection following the instructions provided. 

Critical: 
- Do not include a conclusion or summary as the entry is in the middle of the document. Focus on addressing the points and supporting them with evidence from the provided papers. Ensure that the subsection is well-structured, coherent, and effectively contributes to the overall research paper.
- The subsection we are focusing on is: 4.4. Online Learning in the Cloud
- No need for sub-sub-sections. just provide paragraphs addressing each point. They should transition fluidly and narurally into each other.
- Ensure that the content is supported by the provided papers and that the citations are correctly formatted and placed within the text.
- Do not repeat content from the previous sections. Ensure that the information provided is new and relevant to the subsection being written.



</instructions>

<subsection_point_Point 1>
Point: Leveraging containerization technologies (e.g., Docker, Kubernetes) for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP)

Papers to support point:

Paper 1:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: Leveraging containerization technologies (e.g., Docker, Kubernetes) for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP)
  Extract 2: Importance of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline.
  Limitations: The main limitations of the paper are as follows:

- Limited scope: The paper primarily focuses on the technical aspects of automated data processing, containerization strategies, and the significance of interoperability and standardization. It does not delve into other crucial aspects of real-time, automated irrigation management systems, such as water quality monitoring, soil moisture sensing, or crop health assessment.

- Lack of empirical evaluation: The paper lacks a thorough empirical evaluation of the proposed solutions. While it discusses the theoretical benefits and potential advantages of containerization strategies and interoperability, it does not provide concrete evidence or case studies demonstrating their effectiveness in real-world irrigation management scenarios.

- Tangential focus on edge computing: Although the paper briefly mentions edge computing as a potential deployment environment for data processing and machine learning modules, it does not delve deeply into the specific challenges and opportunities associated with edge computing in the context of automated irrigation management.
  Relevance Evaluation: The paper is exceptionally relevant to the outline point and review intention because it directly addresses the need for automated data processing in the cloud, containerization strategies for scalable and autonomous deployment, and the significance of interoperability and standardization in the context of automated irrigation management systems. It provides valuable insights and concrete solutions for addressing these aspects and aligns well with the overall goal of the literature review to provide a comprehensive evaluation of the current state and future potential of real-time, automated irrigation management systems.
  Relevance Score: 1.0
  Inline Citation: Rosendo, D., Costan, A., Valduriez, P., & Antoniu, G. (2022). Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review. Journal of Parallel and Distributed Computing, 166, 71-94.
  Explanation: Sure, here is a concise summary of the key points of the paper as they relate to the outline point and review intention, along with a succinct yet detailed explanation of how the specifics of the paper contribute to addressing the point within the larger context and intent of the literature review.:

**Key Point 1:** Utilizing containerization technologies, such as Docker, Kubernetes, for efficient deployment and scaling of data processing and machine learning modules in cloud environments. 

**Contribution to review intention:** This addresses the review intention to examine automation across the entire pipeline by systematically analyzing the automation of each component of the irrigation management pipeline, from data collection and transmission to processing, analysis, decision-making, and automated action.

**Key Point 2:** The use of scalable and autonomous deployment using containerization strategies for data processing and machine learning (ML) modules in both cloud and edge environments.

**Contribution to review intention:** This contributes to the review's purpose to identify challenges and propose solutions for seamless integration across the automated irrigation management system to achieve fully autonomous, scalable irrigation management

**Key Point 3:** The importance of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline.

**Contribution to review intention:** Interoperability and standardization are critical factors for the review's objective to highlight the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline. 

**Relevance Score: 0.9-1.0: Exceptionally Relevant**

The paper is exceptionally relevant to the outline point and review intention because it directly addresses the need for automated data processing in the cloud, containerization strategies for scalable and autonomous deployment, and the significance of interoperability and standardization in the context of automated irrigation management systems. It provides valuable insights and concrete solutions for addressing these aspects and aligns well with the overall goal of the literature review to provide a comprehensive evaluation of the current state and future potential of real-time, automated irrigation management systems.

 Full Text: >
"Skip to main content Skip to article Journals & Books Search Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download full issue Outline Abstract Keywords 1. Introduction 2. Comparison with other surveys 3. Edge AI for IoMT 4. Case study 5. Final considerations CRediT authorship contribution statement Declaration of competing interest Acknowledgments Data availability References Show full outline Figures (5) Tables (2) Table 1 Table 2 Computers and Electrical Engineering Volume 116, May 2024, 109202 Edge AI for Internet of Medical Things: A literature review☆ Author links open overlay panel Atslands Rocha a 1, Matheus Monteiro a 2, César Mattos b 1, Madson Dias b 3, Jorge Soares c 1, Regis Magalhães d 1, José Macedo b 1 Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.compeleceng.2024.109202 Get rights and content Abstract The Internet of Things (IoT) consists of heterogeneous devices such as wearables and monitoring devices that collect data to provide autonomous decision-making and smart applications. IoT technologies, such as the Internet of Medical Things (IoMT), have become gradually popular for medical purposes, combining IoT and medical devices to achieve good health and well-being. However, IoMT devices are often tight and have resource constraints, which leads to limited local data processing in the device. Edge computing provides access to additional computation and storage resources for IoMT devices, bringing intelligent processing closer to the data sources. This technology opens up great possibilities for IoMT applications, especially when combined with Artificial Intelligence (AI). Edge AI runs AI computations close to the IoT devices and users instead of centralized services such as cloud servers. This paper investigates the potential of Edge AI and IoMT. In this sense, this survey is the first work to further detail Edge AI and Machine Learning Operations in IoMT domains and wearable technology, thus contributing to the literature by comprehensively exploring the potential of ML strategies and operations at the network’s edge and intelligence distribution. This study also presents a case study on heart anomaly detection. Previous article in issue Next article in issue Keywords Edge intelligenceEdge computingArtificial intelligenceMachine learning operationInternet of medical thingsSmart health 1. Introduction The Internet of Things (IoT) enables real-world devices to be connected to the Internet to monitor and control their status and the surrounding environment for automatized tasks and remote data access. By using embedded sensors, the smart devices monitor variables such as images, chemicals, and biosignals. Based on events and decisions, actuators such as electric motors and hydraulic cylinders can perform actions. IoT is a technology widely applied in several domains, such as agriculture and industry, in order to facilitate new services and improve results and production. IoT has been a disruptive technology providing valuable practices in the health domain resulting in innovative medical devices and new applications such as user attendance, emergency home care, fall detection, enhanced medical diagnosis, and activity recognition. Thus, IoT has extended to the Internet of Medical Things (IoMT). IoMT combines IoT technology and medical devices to achieve good and personalized health and well-being for society. Health is essential for human beings to lead a productive lifestyle. Medical devices of IoMT include embedded sensors such as electrocardiogram (ECG), body temperature, blood pressure (BP), glucose receptors, and mobile devices and wearables. Different types of wearables can be accessories, clothing, implanted devices, and even skin tattoos. According to Statista (The Statistics Portal for Market Data), the amount of data generated by all connected smart devices worldwide is forecasted to reach 79.4 zettabytes by 2025. There is a requirement for efficient data analysis methods due to the increasing volume and velocity of data generated by IoT devices. Currently, artificial intelligence (AI) techniques process data collected from IoMT devices, aiding in decision-making. This use of AI empowers devices to address problems and make choices in ways that resemble human thought processes. However, IoMT devices often lead to limited local data processing because they have resource constraints (CPU, memory). At the same time, IoMT applications often require a low-time response and handle sensitive data related to the individual’s health. Thus, they usually rely on different nodes to process their data using AI models. However, despite promising results, several approaches are reliant on cloud-based architectures. Cloud computing, a paradigm defined in 1997 [1], provides high-resource and on-demand power capacity and storage availability. However, this paradigm faces latency, energy, bandwidth, and privacy issues, particularly in health applications dealing with sensitive data. Since its emergence in 2009 [1], Edge Computing has addressed the previous challenges by providing computational resources in edge nodes, strategically located between IoMT devices and cloud data centers. This arrangement allows IoMT devices to access enhanced computation and storage for more sophisticated data processing, bridging the gap between the data source and the cloud. Examples of edge nodes are smartphones, gateways, single-board computers, drones, and servers. Edge computing promises low latency and bandwidth reduction since the edge nodes are near the data source devices. This aspect also facilitates acquiring context and location awareness. Other benefits are security and privacy because data is processed in edge nodes. Unlike cloud data centers, edge environments are distributed and have no single point of failure. Edge computing differs in terms of paradigms in its diverse distributed computing environment, such as Micro Data Centers (Microsoft), Cloudlets, Fog (Cisco), and Multi-access Edge Computing (European Telecommunications Standards Institute, ETSI). It is important to note these differences and understand how they can benefit various computing needs besides the characteristics, applications, and technologies associated with each paradigm. For instance, micro data centers refer to distributed clouds for mobile users that need a global hardware infrastructure with high-memory servers. Cloudlet is a small data center with enhanced mobility for casual and transient mobile device users. Fog Computing is a regular solution for IoT applications, which can use a gateway or powerful computing devices (servers, desktops) near the IoT devices to distribute computing and connect to the cloud. Mobile Edge Computing (MEC) was initially projected as remotely distributed data centers to meet the massive and increasing demand for mobile device processing. ETSI renamed MEC to Multi-access Edge Computing (MEC) to include non-mobile devices in wireless networks (Wi-Fi, 3G-6G) as well. Edge computing is a complementary paradigm of cloud computing. Their corroborative relationship originated the term Edge-to-Cloud Continuum [2]. In general, the Edge-to-cloud Continuum is organized in a three-level hierarchy: (i) source data nodes (IoT devices) responsible for collecting data; (ii) edge nodes responsible for processing data and making decisions; and (iii) cloud nodes responsible for processing data in a holistic view or when it is sophisticated and unfeasible to perform in edge nodes. This technology opens up possibilities for IoMT applications, especially when combined with AI. Artificial Intelligence at the Edge (Edge AI) [1], also known as Edge Intelligence (EI), emerged in 2021. It combines edge computing and artificial intelligence to distribute intelligence among devices. Edge AI is a promising solution since IoMT data can be analyzed in devices at the network’s edge. Instead of a centralized cloud service, Edge AI enables IoT devices, edge, and even cloud nodes to process their data with lightweight AI algorithms, thereby distributing intelligence. There have been several discussions about cloud independence in the literature, but the most common architecture for Edge AI systems includes the edge and cloud layers. This is discussed in detail in [3]. AI computations can be distributed across various levels, allowing for a combination of cloud and edge processing, where AI model training and inference collaboratively occur across these diverse levels. Besides a low real-time response to process data and make decisions, the Edge AI concept improves data security and the quality of user experience since it reduces problems involving network connection failures (intermittent in several cases) and delays. Evolving from DevOps and DataOps practices matured respectively by the software and data engineering communities, the so-called Machine Learning Operations (MLOps). MLOps are essential in Edge AI for IoMT applications, focusing on rapid development, deployment, and management of machine learning systems. MLOps architecture enables effective model management directly at data generation sources, reducing latency and ensuring real-time patient data analysis. It also supports collaborative model training on edge devices, preserving data confidentiality and adapting models to evolving patient conditions. Furthermore, MLOps address challenges in resource-limited edge environments, ensuring privacy, security, and adaptability in AI-based health solutions, crucial for healthcare applications. Briefly, the Internet of Medical Things benefits from several technologies, including the evolution of computers, the popularization of sensors and IoT, artificial intelligence, and edge computing. These technologies are often combined into the paradigm of Edge AI to create new applications in the IoMT domain. For example, some IoMT applications are remote diagnosis and monitoring health parameters such as blood oxygen, blood pressure, and blood sugar. These applications can be built into smart devices such as wristbands and smartwatches. Many recent IoMT applications, such as orthopedic screening, use image recognition and machine learning to achieve low response times and efficiency. Fig. 1 depicts the timeline of the relationship between the combined technologies to benefit the IoMT applications. Vast and diverse research has been conducted in the Edge AI domain. In contrast to prior surveys on Edge AI and related research areas (distributed intelligence or distributed machine learning), this study applies a detailed view of Edge AI for the IoMT domain. Download : Download high-res image (324KB) Download : Download full-size image Fig. 1. Timeline of the relationship between the technologies. To accomplish this objective, three formulated Research Questions (RQs) aim to answer in detail: 1. RQ1: Which intelligence distribution techniques in Edge AI are often used for the IoMT domain? 2. RQ2: How is ML training in most studies in the IoMT domain? 3. RQ3: What MLOps architectures and techniques are applied in the IoMT domain? In summary, the contributions of this study are in three aspects: 1. A review and theoretical and practical discussion of aspects of the state-of-the-art on Edge AI and IoMT, including distributed intelligence architectures, techniques, and ML training strategies. 2. A review and discussion of MLOps for edge computing that provides fast development and deployment of ML systems in edge environments. To the best of our knowledge, this survey is the first work that details further Edge AI and MLOps in e-health, IoMT, and wearable technology. 3. An introduction and proposal of an architecture of a case study for a case study on detecting heart anomalies, exploring the potential of Edge AI for IoMT applications. The paper is organized as follows. Section 2 presents a comparison with related surveys. This section is divided into two research areas: Edge AI and IoMT. The methodology adopted to select the papers is presented in Section 3. Section 3 also answers the formulated research questions and presents a comprehensive literature survey. This section is divided into two subsections: Distributed Intelligence Techniques and MLOps on Edge. Section 4 presents an IoMT case study. Finally, Section 5 concludes the paper, pointing to challenges and future research directions in this field. 2. Comparison with other surveys Several current works discuss the technologies and concepts involved or compile new Edge-AI-based solutions. However, a minority of those surveys compile solutions towards the IoMT applications domain. At the same time, the concepts of IoMT domain (sensors, applications, architecture, and platforms) have been compiled in recent surveys but do not usually involve approaches based on Artificial Intelligence on Edge (Edge AI). The following subsections review Edge AI and IoMT surveys and highlight their contribution, besides comparing them to this survey. The main objective is to explore a literature gap and investigate the potential of Edge AI approaches in IoMT applications. 2.1. Artificial Intelligence on Edge (Edge AI) Several researchers have conducted surveys on Edge AI as a potential alternative to IoT data processing for autonomous decision-making in recent years. Edge computing is essential to understanding architectures and the levels where edge intelligence is distributed. As discussed in [3], several works define Edge AI in two groups: (i) evolutionary, which defines Edge AI as the next stage of current edge computing where edge nodes process their data using AI algorithms, and (ii) revolutionary, where Edge AI is a new paradigm that combines novel and existing approaches, techniques, and tools such as edge computing, AI, approximate computing and cognitive science, to perform a fully distributed intelligence among the nodes. In both definition groups, edge computing is involved. Architecture. An important aspect is the architecture, where the devices process the data originated by IoT devices. For example, the studies [2], [3], [4] examine the relationship between the complementary paradigms of edge computing and cloud computing, emphasizing the synergy between edge and cloud levels (even a third level, called fog level) in the Edge-cloud Continuum to distribute lightweight ML algorithms and the evolution to Edge AI. These studies have also discussed the Edge AI approaches to deploying AI algorithms and models on edge nodes, typically resource-constrained devices. Work [5] presents an architecture that classifies edge levels based on their latency and resource constraints of the nodes. Edge AI and MLOps. The surveys [1], [4], [6], [7], [8], [9], [10] review important concepts for the optimization of ML models and deployment for resource-constrained edge nodes. For example, one key concept is federated learning training, a contemporary learning method that allows devices to learn ML models collaboratively. The work [6] focuses on mobile edge nodes in MEC architectures, while the studies [1], [8] expand the revised methods to devices in other variants of edge computing architectures (cloudlets, fog). The work [11] presents ML-based computation offloading mechanisms in the MEC paradigm considering other training perspectives: reinforcement learning and supervised and unsupervised learning. Besides, it also examines offloading metrics, evaluation tools, applications, and drawbacks. Studies on Edge AI have surveyed various aspects of AI computation. The study [12] discusses techniques for distributing ML across multiple nodes, such as hyperparameter optimization, ensemble methods (combination of multiple models), and topologies of data aggregation. Work [13] includes model partitioning, compression, scheduling, and other techniques. The survey [14] compiles the capacity of edge intelligence, including multisource, real-time, and context-aware fusion. The authors assume an effective edge intelligence system consists of primary functions: collection, communication, computing, caching, control, and collaboration. On the other hand, the study [15] summarizes the primary functions of Edge AI as caching, computing (training and inference), and offloading. The works [5], [16] investigated training activities performed in different locations (cloud, cloud-assisted edge, edge, IoT device) and recommended best practices. The work [16] focuses on techniques for deep learning (DL) at the edge, including model splitting, data parallelism, federated learning, and model adaptation. It also covers metrics for evaluating DL algorithms. The study [17] presents fundamental concepts of federated learning for data privacy preservation, including in healthcare, which is discussed briefly. Regarding resource management, it is important to explore the suitability of this kind of mechanism for Edge AI. The survey [18] presents training methods (federated, decentralized, offline, and online) and discusses which resource management categories (discovery, estimation, placement, orchestration, scheduling and allocation, provisioning, offloading, and load balancing) are more proper to use. Platforms, tools, virtualization. Regarding practical and operational aspects, the surveys [2], [3], [7], [8], [9], [12] compile tools (simulation, emulation, and deployment), techniques, algorithms, frameworks, hardware, and metrics for ML application in the Edge-to-Cloud Continuum. The authors [7] discuss several technologies to adapt edge to AI, such as hardware adaptation and containerization for hosting AI models. For the success of Edge AI, well-known challenges must be addressed, such as resource management, energy efficiency, communication, device failure, device heterogeneity, and data privacy [4], [13]. Table 1 summarizes related surveys about AI on Edge. This survey primarily focuses on the following subjects when compiling related surveys: (i) Edge AI architecture, which refers to the infrastructure or architecture of the nodes that support the data processing; (ii) Edge AI, which involves AI computation tasks; (iii) Machine Learning Operations, if the work covers this research area ; (iv) Wearables or IoMT, if the work tackles the target application domain of this study; and (v) Platforms, Frameworks, Tools, or Virtualization, if the survey discusses practical aspects of Edge AI or not. Table 1 contains comprehensive information on each subject, including the column Wearables or IoMT. Some works investigate this topic extensively, while others do not (indicated by Yes or Not). Occasionally, some works only briefly mention or introduce this subject in a short section, referred to as Mention in Table 1. The contribution of this study is summarized as follows. The related surveys mentioned above discuss Edge AI from the perspective of AI strategies, key concepts, and practical aspects. A minority of related work [7], [8], [13], [17] mention briefly the health application domain as a use case for Edge AI. On the other hand, this study further reviews edge intelligence applications and features related to the IoMT and health domain. Moreover, while a limited number of surveys [9], [16] have covered only the deployment activity of ML models methodology at the edge, this survey covers all the methodology steps needed to develop, evaluate, deploy, monitor, and maintain ML models at the network edge. Additionally, it tackles practices and workflows of MLOps, with a particular emphasis on IoMT solutions. Table 1. Characterization of the collected surveys on Artificial Intelligence on Edge. Work Architecture Edge AI MLOps Wearables or IoMT Platforms, tools or virtualization [1] Variants of edge computing Yes No No Yes [2] Edge-to-Cloud Continuum Inference, Training distribution No No Yes [3] Edge-to-Cloud Continuum ML modeling inference and training No No Yes [4] Cloud versus Edge Yes No No Yes [5] Hierarchical edge Distributed ML No No No [6] MEC Federated learning No No No [7] Centralized versus Distributed Yes No Mention Yes [8] Edge-to-Cloud Continuum Edge Intelligence, Federated learning No Mention Yes [9] Edge-to-Cloud Continuum Distributed ML Deployment No Yes [10] Variants of edge computing Edge analytic No No Yes [11] MEC Training No No No [12] Centralized and Decentralized ML Distribution No No Yes [13] No Distributed ML No Mention Yes [14] No Information fusion at edge No No No [15] Centralized versus Edge Inference, Decentralized training No No Yes [16] Centralized and Distributed Training and Inference Deployment No No [17] Edge-to-Cloud Continuum Federated learning No Mention No [18] No Decentralized ML No No Yes This study Edge-to-Cloud Continuum Training and Inference Yes Yes Yes 2.2. IoMT and smart health IoMT is a powerful technology for improving traditional healthcare systems. Some key topics are considered to compare related work as follows. Architecture. Recent studies [19], [20], [21], [22] have conducted a comprehensive survey on IoT convergence for smart health, mainly concentrating on edge architectures, including wearables as a data source. The study [23] focuses on the multi-access edge computing (MEC) paradigm detailing diverse aspects of wearable technology on the data source level addressing, for example, security issues. Edge AI and MLOps. Edge computing is a widely accepted approach to analyzing real-time data near patients. However, using AI techniques in edge computing (Edge AI) is a new development. The study [21] provides a brief overview of the recent trend of deploying AI on the edge for processing health data. The survey [19] delves into edge optimization, medical signals fusion, and data processing lifecycle. The survey [24] investigates TinyML technology, including compression of neural networks based on architectures, design constraints, and performance metrics. The work [25] focuses on Edge AI for classification and prediction of health data, compiling proposals grouped on scenarios such as physiological, rehabilitation, skin disease and diet, epidemic prevention, and diabetes treatment. Combined other technologies. The integration of Edge AI with various technologies has shown great promise in the field of Internet of Medical Things (IoMT) solutions. One such technology is blockchain, which can be used to manage sensitive health data. By recording data in a sequential and immutable manner, blockchain provides enhanced data security, access management, and patient privacy in IoMT systems. The works [22], [25] discuss blockchain as a combined technology of IoMT system to safeguard patient’s data. Platforms, tools and virtualization. Concerning practical aspects (platforms, tools, visualizations, frameworks, hardware, and software aspects), The survey [24] investigates embedded ML, detailing hardware, software, and frameworks for wearable systems incorporating microcontroller units (MCUs) as edge inference devices. Besides, the work also presents a design flow of wearable devices as a TinyML technology. The study [25] presents platforms (Raspberry Pi, GPU, Arduino, smartphone, Intel Edison, and PC) as devices used in several health applications. Table 2 summarizes the comparison of related work about IoMT and smart health. This survey primarily focuses on the following subjects when compiling related surveys: (i) edge computing architecture, which refers to the infrastructure or architecture of the nodes that support the data processing; (ii) Platforms, Frameworks, Tools, or Virtualization, whether the survey discusses practical aspects of Edge AI or not; (iii) MLOps, if the work covers this research area; (iv) Edge AI, which involves AI computation tasks; and (iv) combined other technologies applied in the IoMT domain (e.g., blockchain), if the survey includes this topic. Table 2 contains comprehensive information on each subject, including the column Edge AI. Some works investigate this topic extensively, while others do not (indicated by Yes or Not). Occasionally, some works focus only on techniques related to AI inference (do not investigate AI training techniques), referred to as Inference in the table. Notably, a concise set of related works [19], [24], [25] concentrates on Edge AI for IoMT applications. Besides, any mentioned related work does not include MLOps in their discussion. The main contribution of this study concerns discussing Edge AI, besides MLOps visions, for IoMT domains. Edge MLOps is important because it aims to develop and deploy high-quality ML systems in edge environments rapidly. To the best of our knowledge, this survey is the first work that details further Edge AI and MLOps in smart health, IoMT, or wearable technology. Considering the advantages and challenges of this research field, this survey will fully explore the potential of ML strategies, algorithms, and operations at the network’s edge. Table 2. Characterization of the collected surveys on IoMT and Smart health. Work Architecture Platforms or Virtualization MLOps Edge AI Combined other technologies [19] Yes No No Yes No [20] Yes No No No No [21] Yes Yes No No No [22] MEC No No No Blockchain [23] MEC, wearables No No No No [24] Microcontrollers as edge MCU No Yes No [25] Yes Yes No Yes Blockchain This study Yes Yes Yes Yes Yes 3. Edge AI for IoMT This section focuses on presenting the methodology and results compiled from this research. 3.1. Research goals and methodology This paper aims to answer 3 research questions to review Edge AI and MLOPs approaches that have been applied to the IoMT and health domain: 1. RQ1: Which intelligence distribution techniques in Edge AI are often used for the IoMT domain? 2. RQ2: How is ML training in most studies in the IoMT domain? 3. RQ3: What MLOps architectures and techniques are applied in the IoMT domain? To achieve this objective, the first step was to construct a search query for study selection based on three formulated research questions in this study. Then, the second step was to conduct keyword searches on Scopus and ACM databases because they are the primary and most well-known libraries in the academic domain. The following search terms were: • (“artificial intelligence” OR “machine learning”) AND “edge computing” AND (IoMT OR health OR IoHT OR wearable) • (“edge intelligence” OR “edge AI”) AND (IoMT OR health OR IoHT OR wearable) The inclusion of a primary study in this survey depends on the assessment of 3 research questions. However, there are specific criteria for excluding papers, such as studies that are not written in English, non-peer-reviewed papers, short papers (extended abstracts and posters), out-of-scope papers, theoretical works (without implementation or experiments), and surveys or review studies. After carefully analyzing the three research questions, a total of 10 papers out of 56 were selected for this survey. Any duplicate or unavailable papers were not considered. The main findings are discussed as follows. 3.2. Distributed intelligence techniques To answer Research Question 1 (RQ1): “Which intelligence distribution techniques in Edge AI are often used for the IoMT domain?”, this study reviews edge inference and training strategies. The inference step refers to the process of predicting or classifying new real-time data in the proximity of data source devices. Designing models with low computational requirements for deployment on resource-constrained edge nodes through Model Compression and Inference Acceleration can help reduce the high resource consumption of AI models. To reduce the workload on the node and improve the inference performance, a portion or portions of the designed model can be partitioned and offloaded to another edge node or multiple nodes. This process is known as Model Partitioning. It is necessary to optimize the model while maintaining the baseline accuracy. The training step can be conducted using a single device or distributed multiple devices working collaboratively to train a shared model or algorithm. Cloud computing architectures have been the traditional infrastructure for AI computations related to IoT applications (including the health domain), such as ML training and inference tasks. Nevertheless, cloud architectures cannot properly perform these activities for IoMT applications, which require low latency or handling sensitive data. Recent works distribute intelligence (AI computations) at the different levels of the Edge-Cloud Continuum. The inference and training of AI models can be performed on different levels and combinations: (i) cloud only, (ii) cloud, edge, end device co-training, (iii) edge and end device, and (iv) end device only. These approaches with different levels and combinations are discussed as follows. Cloud only. The more common approach is to train ML models on the cloud level before running the data inference using these pre-trained models on the edge level (Fig. 2(i)). This approach is often because of the resource constraints of the edge nodes. This approach is presented in the studies [26], [27], [28]. Other studies [29], [30], [31], [32] combine pre-trained ML models on the cloud level with some pre-processing tasks performed on the edge level. This approach reduces communication costs at the cloud level. For example, the study [29] runs pre-processing on the edge level to detect anomalies in medical vitals signals (ECG, heart rate, and blood pressure) related to cardiac health. Edge nodes also run data inference. Download : Download high-res image (626KB) Download : Download full-size image Fig. 2. AI model training and inference on different levels and combinations. Cloud, edge, end devices. The second approach is training a machine learning model distributed on multiple devices at the edge and cloud in collaboration. In the studies [33], [34], edge and cloud devices collaboratively learn shared ML models without sharing data at the cloud level but just sharing crucial parameters (Fig. 2(ii)). The studies present federated learning involving user devices in training. Therefore, in this case, the end devices level (wearables, IoT devices) are included in this discussion. Edge only. The third approach involves training and inference tasks entirely in the edge nodes since these devices are improved regarding computational resources (Fig. 2(iii)). The study [35] explores this technique for human-centric IoT applications using different edge nodes with computational power. The study [33] evaluate three distributed models of AI computations performed in terms of execution time and accuracy: (i) training and inference edge only, (ii) pre-trained on cloud and inference on edge, (iii) distributed training (federated learning) and inference at the edge. The models have been tested using SVM and health databases. Their simulated experiments show the edge inference time is always faster than cloud time for all tested datasets for each proposed model. This is because of cloud communication costs. At the same time, pre-trained on the cloud is faster than training on the edge and distributed training. End device only. A new direction in the Edge AI research area is to run AI computation (training and inference) even in IoT devices (end devices) requiring advanced computing platforms (Fig. 2(iv)). Recent IoT platforms are leveraging artificial intelligence accelerators and processors. To apply in IoMT applications, accelerators and processors should be used, and optimization and Tiny ML methods must be applied carefully to enable this approach, mainly to supply training activities. Deployment platforms. Since AI training and inference are moving to the edge, including IoT devices, it is interesting to consider the adopted platforms as edge devices because of their resource constraints. Historically, servers and smartphones have been considered edge-level technologies, as presented in the study [26]. On the other hand, due to their energy efficiency and low cost, a trend is the presence of resource-constrained nodes at this level, such as system-on-chip clusters and single-board computers. For example, the studies [27], [28], [29], [31] are edge architectures tailored (or tested) for single-board computers (Raspberry Pi and Odroid M1). Raspberry Pi is a popular platform for edge devices due to its low cost, versatility, and lightweight design. Choosing the most acceptable type of edge node requires knowledge about the data and ML models to be applied. The performance analysis helps to decide the suitable type of edge node. However, many proposals show that resource-constrained nodes have a good performance. For IoMT applications, the study [34] makes a performance comparison of edge nodes with low and high computational power, analyzing several ML strategies, such as federated, semi-supervised, transfer, and multi-task learning. In that context, Raspberry Pi outperforms a server for lung segmentation detection, but the server achieved slightly better results for COVID-19 detection. The study [35] emphasizes the importance of comprehending the initial data and signal characteristics to meet the cost–accuracy requirements. They conducted a performance evaluation to analyze the trade-off between the computational cost and the different classification models’ accuracy (KNN, RF, SVM, among others) for activity recognition applications. Raspberry Pi and Raspberry Zero devices were evaluated and achieved good results in terms of performance during the training and inference phases regarding time processing and accuracy. Heterogeneous technologies and resource consumption. The variety of deployment platforms is closely related to the expected heterogeneity of the devices across a distributed Edge AI solution. Beyond the aforementioned trade-off between computational cost and accuracy, the actual applicability of different technologies, especially communication technologies, plays an important role in this scenario. For example, the data collection in short distances may use Wi-Fi, Bluetooth, or ZigBee, while faster, wider and more flexible connections would require 5G or newer mobile technologies [6], [20]. Software and hardware heterogeneity is especially relevant in the context of computational offloading, where metrics such as energy consumed, computational latency, response time, total execution cost, quality of service (QoS), and quality of experience (QoE) are essential to monitor the performance of deployed applications [11]. Once again, there are several trade-offs to consider. For instance, higher QoS and QoE requirements could lead to higher power consumption. Importantly, while distributed intelligence approaches provide scalability and flexibility, they also may bring communication overhead and consequent cost increases. Strategies such as edge caching can alleviate the overall resource consumption [20]. IoMT sensors. Healthcare technology is making significant progress. IoMT applications depend on specific sensors to gather the required data. Heterogeneous sensors are embedded within various medical and IoT devices and wearables, offering a paradigm transformation in patient monitoring and data collecting. The most contemporary solutions even use data fusion, combining signals acquired from IoT sensors that get data directly from the patient and the environment to which the patient is exposed. For instance, the study [26] explores the integration of heart rate and accelerometer sensors for comprehensive activity tracking and stress monitoring. Study [27], a multifaceted approach, incorporates sensors for blood pressure, respiratory rate, consciousness state, temperature, and oxygen saturation to infer the clinical risk level of the patients. The study [28] deals with elderly patients, using ECG signals, temperature, relative humidity, heart rate, and oxygen saturation. The study [29] combines many input data and uses temperature, heart rate, blood pressure, respiratory rate, and oxygen saturation information to keep track of the patient’s cardiovascular health and to remotely monitor this data. The study [30] uses GPS, heart rate, insulin level, blood pressure, temperature, sweating, relative humidity, and accelerometer to analyze diabetes risk levels. The studies [31], [35] use a dataset of accelerometer information to classify user activities. The study [32] works with speech recognition and proposes context-aware task offloading based on wearable sensors, such as accelerometers, magnetometers, gyroscopes, and pedometers. The study [33] works with many different datasets and contexts, but regarding medical data, it uses blood pressure and blood glucose datasets to analyze problems like body fat, breast cancer, and diabetes. Finally, the study [34] uses radiography images to analyze COVID-19 and to do lung segmentation. Summary of IoMT applications. A Sankey Diagram (Fig. 3) compiles the research on Distributed Intelligence for IoMT Applications. A Sankey Diagram is a visualization technique that displays the flows of resources and represents their quantity. Fig. 3 depicts the compiled research on Distributed Intelligence for IoMT Applications (RQ1 answer) and represents the IoT sensors, cloud and edge platforms (discussed previously), AI models and intelligence distribution techniques found in the collected primary studies; discussed. Sankey Diagram displays comprehensive information on each subject. For example, it displays the recent IoMT applications domain whose distributed intelligence techniques are used to resolve issues related to human health. The works aim to use Edge AI to monitor stress levels [26], clinical risk [27], elderly patients [28], cardiovascular health [29], body fat, cancer, and diabetes [30], [33] activity recognition [31], [35], speech recognition [32], covid-19 and lung segmentation detection [34]. Additionally, classification [26], [27], [28] and prediction [29] are the data analysis methods most required for IoMT applications that use Edge AI. Download : Download high-res image (979KB) Download : Download full-size image Fig. 3. Characterization of the collected works on distributed intelligence for IoMT applications. Note that the same work may follow multiple paths. 3.3. Model training in the IoMT domain To answer RQ2: “How is ML training in most studies in the IoMT domain?” The main findings compile training methods in the primary studies: cloud-pre-trained [26], [27], [28], [29], [30], [31], [32], [33], [35], embedded edge [33], [35], and federated learning [33], [34]. This study discusses the key ML training methods based on whether they are pre-trained (cloud), online training (edge), or federated training (edge and cloud) as follows. Centralized. Cloud pre-training is the most famous method in prior works about edge AI. Fig. 3 illustrates the most common cloud services used in the primary studies (for example, Amazon Service). In this approach, machine learning models are trained in cloud data centers and then deployed into edge devices to perform ML inference based on actual data coming from IoT devices. The benefit of this approach is the saving of limited edge resources, which may not be suitable for machine learning training tasks or in situations where training is performed in centralized mode due to the large amount of decentralized data arriving from geographically distant data. The shortcoming of this approach is keeping the performance of AI models, which were deployed on edge devices, updated or performing satisfactorily compared to the models in the cloud. Thus, monitoring and evaluating the performance of ML models is crucial to making decisions regarding retraining or updating the deployed ML model. Preserving data privacy is another concern since user data must be uploaded (and stored) to the cloud data center to be used for ML pre-training tasks. This aspect is very important since health applications deal with sensitive data from users: medical history, diseases, and clinical risks. Although cloud data centers leverage security mechanisms (authentication and confidentiality), transmitting data, unauthorized access, or changes to this kind of data would be problematic. Performing ML training activities on edge devices can be more efficient than transmitting data to a cloud data center for training, as it reduces communication overhead. However, to ensure the success of training on embedded edge devices, minimizing the number of trainable parameters is crucial to reduce the overall model size. Online learning. Despite the benefits of centralized training on the cloud, data frequency in edge environments is continuous. The pre-trained models are based on static datasets and cannot learn new knowledge when the training is over [8]. Online Learning is a machine learning method where a learner attempts to tackle some decision-making task by learning from a sequence of data instances one by one at each time and updates its parameters continuously as new data arrives [36]. In online learning, the method of updating ML parameters is continuous and incremental. It uses new data as it comes in, rather than processing data in batches. This means that models can adapt to changing and evolving data in real time, making them useful for scenarios where data is streaming continuously. Additionally, online learning algorithms require less memory than batch learning algorithms because they do not need to store the entire dataset in memory. They also handle concept drift, which occurs when there is a change in the underlying data distribution over time. The model can adapt to these changes and update its parameters accordingly. Therefore, online learning fits edge environments well because the user data flow is intense, constantly changing, and requires fewer computational resources than in batch or offline learning. Federated learning. FL [6] is a distributed and collaborative learning approach that aims to train sets and models located in decentralized and local positions and learn a shared model. This technique is based on the Distributed Selective Stochastic Gradient Descent (DSSGD). In federated learning, each edge device downloads from the cloud a generic global model for local training. After, the global model is improved with local edge data. Then, edge devices upload an encrypted gradient to the cloud. Finally, the average update of local models occurs in the cloud, and a renewed global model is transmitted to edge devices. The strong point of Federated Learning is the privacy-preserving technology of user data and saving bandwidth since it is a decentralized and local training method. Thus, data is not uploaded to cloud center data. Federated Learning can protect user privacy and provide personalized recommendations [8]. FL promises reduced energy consumption on devices since it is a collaborative method. The key shortcoming of federated learning is resource constraints since typical recent edge devices are smartphones, single-board computers, embedded sensors, and IoT devices. Thus, more than their resources are needed to train ML models. Another shortcoming is the high frequency of communication between edge devices operated in shared ML training because of the intermittent network connection, low bandwidth, and regular high costs. Besides, the success of the federated learning approach also depends on the number of available clients to train ML models, and mobile devices can be intermittently offline. 3.4. Machine learning operations on edge Including machine learning models in production environments is a challenge that has remained since the first studies in the field. Several difficulties in this task act as barriers for deploying models from a large portion of machine learning projects [37]. However, following the success of methodologies for continuous software development, a set of methods and techniques has been recently proposed with the objective of minimizing most of the existing problems. Such strategies also aim to implement improvements to production at the moment they are developed. The practice of adopting automation, monitoring tools, and methodologies at all stages of the development of machine learning solutions, including integration, testing, delivery, deployment, and infrastructure management, has been called Machine Learning Operations (MLOps) [38]. The main objective of MLOps is to achieve fast development and deployment of machine learning systems with high quality, reproducibility requirements, and tracking of all the parts of the end-to-end solution [39]. It is worth emphasizing that machine learning systems are built through a series of components that include not only the model and learning algorithm code. In [40], the authors emphasize that only a small portion of a real-world machine learning system comprises strict machine learning code. There are several other diverse components that turn such systems into complex structures to analyze and maintain. Section 3.4.1 reviews some of those components. As the demand for sophisticated ML capabilities increases, deploying ML systems at the network’s edge, closer to the data sources and end-users, has gained prominence. Edge computing offers advantages like reduced latency, improved privacy, and enhanced device autonomy in resource-constrained environments. This convergence of ML and edge computing introduces new challenges and opportunities in designing and managing ML systems. Despite MLOps use being more frequent in edge environments nowadays, developers need to handle additional issues. For instance, edge environments often present resource constraints, varying network connection quality, and node heterogeneity [41]. Thus, standard operational approaches initially developed for centralized systems must be adapted or extended. Section 3.4.2 reviews some MLOps workflows available in the literature, focusing on edge solutions and the IoMT domain. As previously mentioned, cloud and edge are part of a continuum of steps for data collecting, storing and processing, as well as making decisions. Thus, cloud and edge computing are complementary in IoT and IoMT solutions. Nevertheless, their intrinsic differences result in specificities in developing and deploying ML systems. In that sense, a comparative review and discussion between MLOps strategies over cloud and edge ML components is presented in Section 3.4.3. The following presentation focuses on general definitions, architectures, methodologies, and pipelines. For a comprehensive review of MLOps tools, the reader is referred to the recent work by [37]. 3.4.1. Machine learning system components Understanding the crucial components of an ML system and their implications when deployed at the edge is fundamental. As follows, the main components of an ML solution are described and their interaction are explored in the context of edge computing. Importantly, the intricacies of edge-based ML operations are emphasized in the face of the specificities of a decentralized architecture, the typical scenario where IoMT is deployed. Configuration. The set of all configuration parameters necessary for the system to function is not concentrated in a single specific step but permeates the entire process [42]. In the context of edge computing, the configuration of ML systems takes on added significance, as it needs to account for the unique constraints and resource limitations inherent to edge devices. Fine-tuning configuration settings becomes essential to ensure optimal performance and resource utilization, enabling efficient and accurate ML model deployment at the edge [43]. For instance, in the presence of distinct edge devices, e.g., camera sensors and wearables, that interact with a given model, it is necessary to guarantee suitable configurations for every device, e.g., proper image resolution and transmission rate [44]. Data collection. The data gathering steps constitute a critical bottleneck of machine learning systems [45]. This component concerns the process of collecting and measuring information about variables in a given system, allowing others to answer relevant questions and evaluate results [42]. In line with edge computing, data collection becomes even more challenging as edge devices, especially IoMT sensors, often operate in resource-constrained environments with limited network connectivity. Efficiently gathering data at the edge while ensuring data quality and privacy becomes a significant consideration in the overall ML system design [46]. It is worth noting the importance of active learning [47] when dealing with distributed guided data collection locally in edge devices which enables faster data labeling and cleaning processes [48]. Feature engineering. The process of using domain knowledge to create features from raw data to feed learning models is called feature engineering. Data collection and feature engineering are fundamental components of the overall ML solution. In fact, most of the time spent running projects that involve end-to-end learning is used for preparing the data, which includes collecting, cleaning, analyzing, visualizing, and performing feature engineering [45]. Conventionally, every feature engineering task has been centralized in the cloud to leverage abundant computational resources [43]. Nevertheless, certain scenarios offer the possibility of executing this phase directly on the edge devices. In such instances, the feature engineering operations, including feature selection steps, require customization to align with the inherent constraints and capabilities of edge computing. This is necessary to ensure that relevant features are extracted efficiently while minimizing computational overhead and power consumption [49], usually a relevant bottleneck in the context of IoMT. Data verification. Data migrated from one source to another must be checked for correctness, ensuring no inconsistencies were introduced [50]. In edge computing, where intermittent connectivity and unreliable network conditions can be common, data verification becomes particularly important to ensure the integrity and accuracy of the data being transmitted between edge devices and centralized systems. Caching edge data on multiple edge servers is critical to provide reliable services, which raises the importance of having a continuous inspection for data integrity and localization of corrupted edge data [51]. Resource management. A deployed ML system should present a component responsible for identifying and accurately providing all the necessary infrastructure for the seamless operation of the ML models, as they require a wide availability of resources [50]. In edge computing, resource management becomes even more critical due to the limited computational power and storage capacity of edge devices. Efficiently allocating and managing resources on edge devices becomes essential to ensure that ML models can be effectively deployed and run at the edge. Due to rapidly changing environments and the distributed nature of edge computing, there have been several approaches to manage edge resources automatically [52]. Model analysis. It must be ensured that the available trained models follow what has been planned before going into production. Generally, this step is performed by applying extensive cross-validation experiments [53]. In the context of edge computing, the model analysis should also consider the model’s performance under varying edge conditions, such as low network bandwidth or intermittent connectivity, to assess the robustness and reliability of the models in real-world edge deployments. An important aspect of model analysis in edge devices is comparing the tradeoffs between accuracy and speed, or accuracy and latency [54]. This critical balance has brought the proposal of several model architectures [55] tailored towards resource-limited environments, such as IoMT [56], [57], [58]. Process management. In a production environment, process management plays a crucial role, especially as the ML system size and the number of components increase. It orchestrates individual processes and ensures seamless integration into customized solutions for each component. Thus, a dedicated process control and management component becomes essential to meet these demands effectively. In edge computing, process management, which can also be treated as AI life-cycle management, becomes more complex due to the distributed nature of edge systems, requiring effective coordination and synchronization between edge devices and centralized components [59]. The operations in the edge also involve network management, fault detection and handling, service management, and overall performance management [52]. Metadata management. Each and every execution of an ML system must be recorded in order to help users capture artifacts and possible comparisons and guarantee reproducibility. This component is responsible for capturing any information that can somehow be used later. Metadata management in edge computing becomes crucial to track and trace model versions deployed across multiple edge devices, enabling efficient model updates and version control in edge environments [59]. Additionally, the meta-information from local data at the edge must also be carefully managed to enable consistency across the system [60]. Serving infrastructure. When interacting with the end user, the ML system must properly handle the available infrastructure to efficiently answer incoming requests. This can include the usage of CPUs, GPUs, storage locations, and even network optimization for each solution [50]. In edge computing, serving infrastructure must be optimized to cater to the diverse hardware and network configurations of edge devices, ensuring low latency and responsive model inference at the edge [43]. Approaches commonly pursued at the edge to limit inference delay include model compression, parallelism, and service scheduling [61]. Monitoring. In ML systems, it is crucial to know that a trained model continues to work in production long after its release. This component is responsible for monitoring the model serving services, as well as the training and input data pipelines [40]. In edge computing, real-time monitoring becomes essential to detect and address any performance degradation or anomalies in the model behavior, ensuring continuous and reliable operation [59]. The monitoring component also aids in determining when a new model is required by analyzing key performance indicators [41]. A model update is then performed by feeding recent data collected from the model in production [62]. IoMT applications, which are often personalized, require device-specific monitoring since fine-tuning results in slightly different models per user [63]. 3.4.2. MLOps workflows The complex interconnection between edge computing and IoMT is significantly influenced by the architectures and methodologies of MLOps. This convergence brings about a fundamental transformation in the utilization of medical data to improve healthcare and wellness outcomes. Central to this combination is a comprehensive MLOps workflow, which manages the complete lifecycle of ML models [64]. In the specific context of edge computing and IoMT, the MLOps architecture facilitates the seamless deployment, continuous monitoring, and effective management of ML models directly at the data generation source, i.e., close to the edge [65]. This approach mitigates data transmission latency and ensures real-time analysis of patient data, thus enabling rapid responses and well-informed medical judgments [66]. Advanced techniques, such as federated learning, play a key role in enabling collaborative model training across dispersed edge devices while preserving the confidentiality of sensitive data without central aggregation. This effectively protects patient privacy [67]. Furthermore, the integration of continuous model refinement methodologies, such as online learning, guarantees the adaptability of these models to the evolution of patient conditions over time [68]. In essence, the combined influence of MLOps architectures and techniques increases the potentials of edge computing and IoMT [8]. In order to try to answer the RQ3: What MLOps architectures and techniques are applied in the IoMT domain?, it is of significant importance the discussion of methodologies aimed at providing guidance for data science projects. The CRISP-ML(Q) framework. The CRoss-Industry Standard Process model for the development of machine learning applications with Quality assurance methodology (CRISP-ML(Q), [69]) is a methodology that aims to address issues related to technical debt when deploying and maintaining ML systems as part of a product or service [69]. This framework is based on the well-known CRISP-DM, which defines a reference methodology for data mining projects [70]. In the CRISP-ML(Q), the project activities are organized in six phases: business understanding, data understanding, data preparation, modeling, evaluation, and deployment, as can be seen in Fig. 4. These phases compose a cycle and include iterations to review previous steps if the success or completion criteria are not met. The result is a waterfall cycle with backtracking [71]. The main differences between CRISP-ML(Q) and CRISP-DM are: (i) CRISP-ML(Q) merges the first two phases (business understanding and data understanding) and creates the last one (monitoring and maintenance), and (ii) the quality assurance step is inserted at each phase and task of the process [69]. The initial phase, business and data understanding, aims to define project objectives, create measurable success criteria via Key Performance Indicators (KPIs), and collect and analyze necessary data. Data preparation, also known as data engineering, involves readying data sets for modeling, addressing missing data, encoding variables, normalization, and other essential tasks. Download : Download high-res image (130KB) Download : Download full-size image Fig. 4. Illustration of the main CRISP-ML(Q) steps. Next, in the modeling phase, various ML models are developed and assessed based on the specific business problem, encompassing model selection, experimental design, and construction. These models are packaged into a repeatable pipeline for training. Evaluation, or offline testing, assesses trained models using unseen test data and appropriate metrics. It is worth noting the growing interest in explainable models to enhance confidence in decision-making [72]. The deployment phase integrates the model into existing software, provides predictive tools, and involves tasks like hardware definition and production environment evaluation. Once the model is in production, ongoing monitoring and maintenance are essential. Real data evaluation identifies retraining needs when performance falls below predefined thresholds set during the business and data understanding phase, potentially using incremental learning techniques [69], [70]. Automated pipelines. Each of the outlined procedural stages is amenable to integration within pipelines, thereby enabling partial or complete automation facilitated by a suite of specialized tools. Numerous studies have proffered innovative pipeline frameworks tailored for distinct problem domains, while some even serve as versatile templates. Illustratively, [39] exhibits an example of a pipeline architecture employed in forecasting electricity supply market trends, demonstrating practical implementation. Likewise, [73] advances a template founded upon canonical components of a conventional ML system. Hybrid workflows. Apart from fully cloud-based solutions, several hybrid workflows have been proposed by some authors [43], [48], [61], [65], [74]. In general, they advocate the distribution of tasks between the cloud and edge environments with the aim to address the limitations associated with cloud-exclusive approaches. The pipeline introduced by [61] presents an integrated MLOps framework that enables developers to schedule individual components of an ML workflow efficiently. The presented MLOps framework is an integrated system merging Kubeflow’s4 detailed ML workflow management with Kubernetes’5 automated deployment, administration, and scaling capabilities. The framework takes into account the requirements of each component in terms of computing and memory capacity, as well as network latency, and allows for an optimized and efficient scheduling of the components across the continuum while also preserving high utilization and energy efficiency with a minimal manual effort from the developers. In a similar approach, [48] introduces Edge Impulse, a comprehensive online framework that simplifies data aggregation, streamlines deep learning model training and supports deployment on embedded and edge computing devices. The authors in [48] highlight the challenges associated with the ML workflow and features of Edge Impulse that tackle those challenges. These instances typify ML pipeline proposals, wherein cloud and edge technologies are applied to remedy the aforementioned technical liabilities. The panorama of analogous solutions further extends through contributions such as [43], [65], [74] The previously outlined stages form a cohesive ML framework that systematically addresses complex business challenges through data-driven models. Recent studies, such as [39], [73], showcase the adaptability of this approach and the emergence of hybrid solutions. This is the path that has been pursued by some other authors, including [48], [61], [74], which indicates a movement towards optimizing resource use, combining both cloud and edge technologies. These innovations highlight the dynamism of the field and the potential for transformative impact through the integration of edge computing in practical ML scenarios. 3.4.3. Comparing cloud and edge MLOps in IoMT environments The previous sections highlighted some requirements and challenges when considering MLOps at the edge, for instance, in IoMT applications. Although still a recent research subject, the main principles, roles, components, workflows, and best practices of centralized MLOps at the cloud are growing mature [75]. However, analyzing and adapting this body of knowledge from the angle of edge computing is an ongoing effort. Resources and infrastructure. Cloud deployment presents the advantage of presenting enough resources to handle complex ML pipelines, which are usually more straightforward to maintain, given the more homogeneous infrastructure. On the other hand, a cloud-based solution requires a large bandwidth capacity, may induce delayed responses, and hinders data privacy [9]. Edge computing limits communication demands and enables better handling of data privacy. Yet, due to the diversity of resources, features, and configurations in edge devices, customized frameworks are often required [41]. In the above context, when lightweight distributed modules are considered, such as IoT networks, microservices applications constitute a viable alternative [76]. This approach opposes a monolithic cloud-based service and aims for scalability and reduced latency. Containerization tools, such as Docker, are important ingredients of such a solution since they enable deploying and running applications in diverse environments [77]. Model training and inference. While cloud solutions can afford to train large models from centralized databases, techniques such as pre-training, federated learning, and online learning constitute important strategies to train and maintain models in the edge, especially in the context of personalized IoMT applications [78], [79]. Performing inference in resource-rich cloud systems is usually a simpler task than receiving requests and sending model predictions to the edge. To reduce computation and storage requirements, model pruning, low-rank approximations, model quantization, knowledge distillation, model partitioning, and edge caching are examples of techniques frequently pursued [8]. It is worth emphasizing that for both training and inference steps, the issues of energy efficiency and power consumption remain among the central challenges of IoMT applications [80]. Privacy and security. The concentration of large knowledge bases collected from multiple sources rises privacy and legal concerns in cloud-based AI services [81]. Distributed solutions at the edge are able to avoid sending sensitive raw data to untrusted data curators while still being able to extract patterns and provide useful information [82]. AI-based health solutions must also deal with security requirements in the cloud or at the edge. IoMT applications are exposed to several threats and attacks that require specific intrusion detection systems (IDSs), which must stay reliable even in the face of limited resources [83], [84], [85], [86]. The continuum. Since modern workflows have pursued hybrid solutions with cloud and edge components, especially in the IoMT domain, recognizing the differences between MLOps practices in both environments is important towards a more efficient overall solution. This is mostly relevant when distinct products and services require implementing components of the ML system in distinct positions due to particular constraints or requirements. Moreover, the same system component may present steps distributed across the edge-cloud continuum, which enables harvesting advantages and avoiding drawbacks of the target production environments. 4. Case study Considering the tools, operations, and architectures presented in this document, this section shows a case study and proposes an architecture to solve a well-known task in the IoMT context: detecting heart anomalies. The case study consists of collecting data for the personalized training of a model capable of receiving PPG data and generating an ECG signal to detect heart anomalies like atrial fibrillation and, in real-time, inform the patient or medical entities. Thus, the patient has the benefit of receiving a medical diagnosis in real time. ECG (Electrocardiogram) data is valuable information for detecting arrhythmia and atrial fibrillation. Despite this, collecting ECG data continuously can become a problem since the collection of this data is active, requiring the user to remain in specific conditions and sometimes to press or hold a button. Nowadays, smartwatches can collect ECG signals and PPG (photoplethysmogram) data. Collecting the PPG is easier and more convenient than the electrocardiogram, given the fact that it does not need user actuation. It is a passive data acquisition that uses a light sensor. Given the PPG measurement, the PP2ECG model reconstructs an ECG signal with PPG signals as input and ECG signals as target labels. Using a subject-based deep learning model [87], it is possible to create an ECG signal that represents the user’s condition and shows possible heart diseases inferred from the PPG data. Then, it is possible to use the built signal as the input for an ECG classification model and detect those anomalies. This approach requires a model specific to each patient, given that the correlation between ECG and PPG works differently for each user. Fig. 5 presents the Edge AI and MLOps steps performed on different levels of the Edge-to-Cloud Continuum regarding this case study that requires two AI models: a PPG2ECG model and an ECG classification model. The PPG2ECG model converts PPG signals to ECG data, while the ECG classification model is used to infer heart anomalies from the converted ECG data. As follows, it is given a description of the ML operations performed in each layer. End devices. The smartwatch collects data from a specific user’s ECG and PPG sensors and sends them to the Fog/Edge Nodes, which use the data to train a PPG2ECG model. The smartwatch can send data collected to Fog/Edge Nodes using Wi-Fi or Bluetooth networks. Next, the end device deploys the model and uses it to infer ECG data from new PPG data collected in real time. The ECG data itself is not enough to detect a heart anomaly. Therefore, an ECG classification model is required to complement the PPG2ECG model. Such a model is trained in the cloud and sent to the end device, which deploys and uses it to infer heart conditions from the ECG data reconstructed from the real-time PPG user data. Continuous monitoring and maintenance are required to ensure the availability and performance of the deployed models. Periodically, the edge device requests new collections of ECG and PPG data from the user to improve the PPG2ECG model. Download : Download high-res image (302KB) Download : Download full-size image Fig. 5. Architecture proposal for the case study of detecting heart anomalies. Fog/Edge nodes. The Fog/Edge nodes receive PPG and ECG data collected from a specific user and prepare such data to train a PPG2ECG model for reconstructing an ECG signal from the PPG data. The model is evaluated, and if a given threshold metric is satisfied, it is sent to the end device. Otherwise, the end device is notified to send more ECG and PPG user data for training a new model with a suitable metric value. Monitoring and maintenance are required to continuously get new data from users to improve their PPG2ECG models. The devices can communicate with each other using Wi-Fi, Bluetooth, or Ultra Wideband (UWB) networks. Cloud. This case study proposes preparing and using publicly available ECG datasets to train and evaluate an ECG classification model for detecting heart anomalies. The cloud layer is responsible for maintaining the business rules and, using public datasets, training, generating indicators, and offloading ECG classification models that will be taken to the final layer. 5. Final considerations The Internet of Medical Things (IoMT) actively transforms traditional healthcare systems, and Edge AI plays a critical role in this evolution by allowing the protection of sensitive patient data. As Edge AI operates on devices at the network’s edge, where resource availability is more constrained than in cloud environments, it necessitates a scholarly exploration. Despite the well-established applications and case studies available in the literature, this survey tackled some important gaps related to the required methodology to develop, evaluate, deploy, monitor, and maintain ML models at the edge. In that direction, diverse practices and workflows of the so-called MLOps field were covered, with focus on IoMT solutions and edge interactions. Besides, the potential of Edge AI for IoMT applications was approached by examining the methods of intelligence distribution, model training and inference, and the comparison between cloud and edge MLOps in IoMT environments. It emphasizes the importance of customized MLOps frameworks for edge devices due to the diversity of resources and configurations. The challenges and advantages of training and inference at the edge, including the need for energy efficiency and privacy, are thoroughly discussed. Furthermore, the document reviews the CRISP-ML(Q) framework as a methodology for developing machine learning applications with quality assurance. It also explores the comparison between centralized cloud deployment and distributed edge solutions, highlighting the considerations for privacy, security, and the continuum of edge-cloud workflows. In conclusion, the document provides a comprehensive analysis of the potential, challenges, and best practices of Edge AI for IoMT applications, shedding light on the complexities and opportunities of implementing MLOps in distributed edge environments. CRediT authorship contribution statement Atslands Rocha: Conceptualization, Methodology, Formal analysis, Writing – original draft, Visualization, Read and agreed to the published version of the manuscript. Matheus Monteiro: Writing—original draft, Visualization, Read and agreed to the published version of the manuscript. César Mattos: Conceptualization, Methodology, Formal analysis, Writing – original draft, Read and agreed to the published version of the manuscript. Madson Dias: Formal analysis, Writing – original draft, Visualization, Read and agreed to the published version of the manuscript. Jorge Soares: Writing – review & editing, Supervision, Read and agreed to the published version of the manuscript. Regis Magalhães: Writing – original draft, Writing – review & editing, Visualization, Supervision, Read and agreed to the published version of the manuscript. José Macedo: Resources, Writing – review & editing, Supervision, Read and agreed to the published version of the manuscript. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgments Part of the results presented in this work were obtained through the project “CENTER OF EXCELLENCE IN ARTIFICIAL INTELLIGENCE – AI4WELLNESS”, funded by Samsung Eletrônica da Amazônia Ltda., at Federal University of Ceará, Brazil , under the Information Technology Law No. 8.248/91. Data availability No data was used for the research described in the article. References [1] Singh R., Gill S.S. Edge ai: A survey Internet Things Cyber-Phys Syst, 3 (2023), pp. 71-92, 10.1016/j.iotcps.2023.02.004 View PDFView articleView in ScopusGoogle Scholar [2] Rosendo D., Costan A., Valduriez P., Antoniu G. Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review J Parallel Distrib Comput, 166 (2022), pp. 71-94, 10.1016/j.jpdc.2022.04.004 URL https://www.sciencedirect.com/science/article/pii/S0743731522000843 View PDFView articleView in ScopusGoogle Scholar [3] Barbuto V., Savaglio C., Chen M., Fortino G. Disclosing edge intelligence: A systematic meta-survey Big Data Cogn Comput, 7 (2023), p. 44, 10.3390/bdcc7010044 View in ScopusGoogle Scholar [4] Hoffpauir K, Simmons J, Schmidt N, Pittala R, Briggs I, Makani S, Jararweh Y. A survey on edge intelligence and lightweight machine learning support for future applications and services. J Data Inf Qual http://dx.doi.org/10.1145/3581759. Google Scholar [5] Mwase C., Jin Y., Westerlund T., Tenhunen H., Zou Z. Communication-efficient distributed AI strategies for the IoT edge Future Gener Comput Syst, 131 (2022), pp. 292-308, 10.1016/j.future.2022.01.013 View PDFView articleView in ScopusGoogle Scholar [6] Singh A., Satapathy S.C., Roy A., Gutub A. AI-based mobile edge computing for IoT: Applications, challenges, and future scope Arab J Sci Eng, 47 (8) (2022), pp. 9801-9831, 10.1007/s13369-021-06348-2 View in ScopusGoogle Scholar [7] Douch S., Abid M.R., Zine-Dine K., Bouzidi D., Benhaddou D. Edge computing technology enablers: A systematic lecture study IEEE Access, 10 (2022), pp. 69264-69302, 10.1109/ACCESS.2022.3183634 URL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132750947&doi=10.11092FACCESS.2022.3183634&partnerID=40&md5=88667ef450dd686759983878427de051 View in ScopusGoogle Scholar [8] Su W., Li L., Liu F., He M., Liang X. AI on the edge: a comprehensive review, Springer Netherlands (2022), 10.1007/s10462-022-10141-4 Google Scholar [9] Sarwar Murshed M.G., Murphy C., Hou D., Khan N., Ananthanarayanan G., Hussain F. Machine learning at the network edge: A survey ACM Comput Surv, 54 (8) (2022), pp. 1-35, 10.1145/3469029 arXiv:1908.00080 Google Scholar [10] Nayak S, Patgiri R, Waikhom L, Ahmed A. A review on edge analytics: Issues, challenges, opportunities, promises, future directions, and applications. In: Digital communications and networks. http://dx.doi.org/10.1016/j.dcan.2022.10.016,. Google Scholar [11] Shakarami A, Ghobaei-Arani M, Shahidinejad A. A survey on the computation offloading approaches in mobile edge computing: A machine learning-based perspective. Comput Netw 182(August). http://dx.doi.org/10.1016/j.comnet.2020.107496. Google Scholar [12] Verbraeken J, Wolting M, Katzy J, Kloppenburg J, Verbelen T, Rellermeyer JS. A Survey on Distributed Machine Learning. ACM Comput Surv 53(2). http://dx.doi.org/10.1145/3377454,. Google Scholar [13] Filho C.P., Marques E., Chang V., Dos Santos L., Bernardini F., Pires P.F., Ochi L., Delicato F.C. A systematic literature review on distributed machine learning in edge computing Sensors, 22 (7) (2022), pp. 1-36, 10.3390/s22072665 Google Scholar [14] Zhang Y., Jiang C., Yue B., Wan J., Guizani M. Information fusion for edge intelligence: A survey Inf Fusion, 81 (2021) (2022), pp. 171-186, 10.1016/j.inffus.2021.11.018 View PDFView articleGoogle Scholar [15] Xu D, Li T, Li Y, Su X, Tarkoma S, Hui P. A survey on edge intelligence, CoRR abs/2003.12172 arXiv:2003.12172 URL https://arxiv.org/abs/2003.12172. Google Scholar [16] Joshi P., Hasanuzzaman M., Thapa C., Afli H., Scully T. Enabling all in-edge deep learning: A literature review IEEE Access, 11 (2023), pp. 3431-3460, 10.1109/ACCESS.2023.3234761 URL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147210326&doi=10.11092FACCESS.2023.3234761&partnerID=40&md5=ecbad1122c4ce3f54b1bb922c71e5fac View in ScopusGoogle Scholar [17] Boobalan P., Ramu S.P., Pham Q.V., Dev K., Pandya S., Maddikunta P.K.R., Gadekallu T.R., Huynh-The T. Fusion of federated learning and industrial Internet of Things: A survey Comput Netw, 212 (May) (2022), Article 109048, 10.1016/j.comnet.2022.109048 arXiv:2101.00798 View PDFView articleView in ScopusGoogle Scholar [18] Iftikhar S., Gill S.S., Song C., Xu M., Aslanpour M.S., Toosi A.N., Du J., Wu H., Ghosh S., Chowdhury D., Golec M., Kumar M., Abdelmoniem A.M., Cuadrado F., Varghese B., Rana O., Dustdar S., Uhlig S. AI-based fog and edge computing: A systematic review, taxonomy and future directions Internet Things, 21 (2023), Article 100674, 10.1016/j.iot.2022.100674 URL https://www.sciencedirect.com/science/article/pii/S254266052200155X View PDFView articleView in ScopusGoogle Scholar [19] Alshehri F., Muhammad G. A comprehensive survey of the Internet of Things (IoT) and AI-based smart healthcare IEEE Access, 9 (2021), pp. 3660-3678, 10.1109/ACCESS.2020.3047960 View in ScopusGoogle Scholar [20] Sun L., Sun L., Jiang X., Ren H., Ren H., Guo Y. Edge-cloud computing and artificial intelligence in internet of medical things: Architecture, technology and application IEEE Access, 8 (2020), pp. 101079-101092, 10.1109/ACCESS.2020.2997831 View in ScopusGoogle Scholar [21] Greco L., Percannella G., Ritrovato P., Tortorella F., Vento M. Trends in IoT based solutions for health care: Moving AI to the edge Pattern Recognit Lett, 135 (2020), pp. 346-353, 10.1016/j.patrec.2020.05.016 View PDFView articleView in ScopusGoogle Scholar [22] Awad A.I., Fouda M.M., Khashaba M.M., Mohamed E.R., Hosny K.M. Utilization of mobile edge computing on the internet of medical things: A survey ICT Express, 9 (3) (2023), pp. 473-485, 10.1016/J.ICTE.2022.05.006 View PDFView articleView in ScopusGoogle Scholar [23] Jin X., Li L., Dang F., Chen X., Liu Y. A survey on edge computing for wearable technology Digit Signal Process: Rev J, 125 (2022), Article 103146, 10.1016/j.dsp.2021.103146 View PDFView articleView in ScopusGoogle Scholar [24] Diab M.S., Rodriguez-Villegas E. Embedded machine learning using microcontrollers in wearable and ambulatory systems for health and care applications: A review IEEE Access, 10 (June) (2022), pp. 98450-98474, 10.1109/ACCESS.2022.3206782 View in ScopusGoogle Scholar [25] Amin S.U., Hossain M.S. Edge intelligence and Internet of Things in healthcare: A survey IEEE Access, 9 (2021), pp. 45-59, 10.1109/ACCESS.2020.3045115 URL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098753480&doi=10.11092FACCESS.2020.3045115&partnerID=40&md5=f9e13c14e5ad00459c922f1656a0f543 View in ScopusGoogle Scholar [26] Sim S.-H., Paranjpe T., Roberts N., Zhao M. Exploring edge machine learning-based stress prediction using wearable devices 2022 21st IEEE international conference on machine learning and applications, ICMLA (2022), pp. 1266-1273, 10.1109/ICMLA55696.2022.00203 View in ScopusGoogle Scholar [27] Pazienza A., Anglani R., Fasciano C., Tatulli C., Vitulano F. Evolving and explainable clinical risk assessment at the edge Evol Syst, 13 (3) (2022), pp. 403-422, 10.1007/s12530-021-09403-3 View in ScopusGoogle Scholar [28] Debauche O, Nkamla Penka JB, Mahmoudi S, Lessage X, Hani M, Manneback P, Lufuluabu UK, Bert N, Messaoudi D, Guttadauria A. RAMi: A New Real-Time Internet of Medical Things Architecture for Elderly Patient Monitoring. Information 13(9). http://dx.doi.org/10.3390/info13090423, URL. Google Scholar [29] Talha M., Mumtaz R., Rafay A. Paving the way to cardiovascular health monitoring using Internet of Medical Things and Edge-AI 2022 2nd international conference on digital futures and transformative technologies, iCoDT2 (2022), 10.1109/ICoDT255437.2022.9787432 Google Scholar [30] Devarajan M., Subramaniyaswamy V., Vijayakumar V., Ravi L. Fog-assisted personalized healthcare-support system for remote patients with diabetes J Ambient Intell Humaniz Comput, 10 (10) (2019), pp. 3747-3760, 10.1007/s12652-019-01291-5 View in ScopusGoogle Scholar [31] Al-Rakhami M., Gumaei A., Alsahli M., Hassan M.M., Alamri A., Guerrieri A., Fortino G. A lightweight and cost effective edge intelligence architecture based on containerization technology World Wide Web, 23 (2) (2020), pp. 1341-1360, 10.1007/s11280-019-00692-y View in ScopusGoogle Scholar [32] Yang Y., Geng Y., Qiu L., Hu W., Cao G. Context-aware task offloading for wearable devices 2017 26th international conference on computer communication and networks, ICCCN (2017), pp. 1-9, 10.1109/ICCCN.2017.8038470 View PDFView articleGoogle Scholar [33] Pattnaik B.S., Pattanayak A.S., Udgata S.K., Panda A.K. Advanced centralized and distributed SVM models over different IoT levels for edge layer intelligence and control Evol Intell, 15 (1) (2022), pp. 481-495, 10.1007/s12065-020-00524-3 View in ScopusGoogle Scholar [34] Ul Alam M, Rahmani R. Federated semi-supervised multi-task learning to detect covid-19 and lungs segmentation marking using chest radiography images and raspberry pi devices: An internet of medical things application. Sensors 21(15). http://dx.doi.org/10.3390/s21155025. Google Scholar [35] Gómez-Carmona O., Casado-Mansilla D., Kraemer F.A., López-de Ipiña D., García-Zubia J. Exploring the computational cost of machine learning at the edge for human-centric Internet of Things Future Gener Comput Syst, 112 (2020), pp. 670-683, 10.1016/j.future.2020.06.013 URL https://www.sciencedirect.com/science/article/pii/S0167739X20304106 View PDFView articleView in ScopusGoogle Scholar [36] Hoi S.C.H., Sahoo D., Lu J., Zhao P. Online learning: A comprehensive survey Neurocomputing, 459 (2021), pp. 249-289, 10.1016/j.neucom.2021.04.112 URL https://www.sciencedirect.com/science/article/pii/S0925231221006706 View PDFView articleView in ScopusGoogle Scholar [37] Symeonidis G., Nerantzis E., Kazakis A., Papakostas G.A. Mlops - definitions, tools and challenges 12th IEEE annual computing and communication workshop and conference, CCWC 2022, las vegas, NV, USA, January 26-29, 2022, IEEE (2022), pp. 453-460, 10.1109/CCWC54503.2022.9720902 View in ScopusGoogle Scholar [38] Mäkinen S., Skogström H., Laaksonen E., Mikkonen T. Who needs mlops: What data scientists seek to accomplish and how can mlops help? 1st IEEE/ACM workshop on AI engineering - software engineering for AI, wAIN@iCSE 2021, madrid, Spain, May 30-31, 2021, IEEE (2021), pp. 109-112, 10.1109/WAIN52551.2021.00024 View in ScopusGoogle Scholar [39] Subramanya R., Sierla S., Vyatkin V. From devops to mlops: Overview and application to electricity market forecasting Appl Sci, 12 (19) (2022), p. 9851, 10.3390/app12199851 View in ScopusGoogle Scholar [40] Breck E., Cai S., Nielsen E., Salib M., Sculley D. The ML test score: A rubric for ML production readiness and technical debt reduction 2017 IEEE international conference on big data (big data), IEEE (2017), pp. 1123-1132 CrossRefView in ScopusGoogle Scholar [41] Kolltveit A.B., Li J. Operationalizing machine learning models - A systematic literature review Proceedings - workshop on software engineering for responsible AI, SE4RAI, Vol. 2022 (2022), pp. 1-8, 10.1145/3526073.3527584 View in ScopusGoogle Scholar [42] Sculley D., Holt G., Golovin D., Davydov E., Phillips T., Ebner D., Chaudhary V., Young M., Crespo J., Dennison D. Hidden technical debt in machine learning systems Advances in neural information processing systems, Vol. 28 (2015), pp. 2503-2511 URL https://proceedings.neurips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html Google Scholar [43] Wu C., Brooks D., Chen K., Chen D., Choudhury S., Dukhan M., Hazelwood K.M., Isaac E., Jia Y., Jia B., Leyvand T., Lu H., Lu Y., Qiao L., Reagen B., Spisak J., Sun F., Tulloch A., Vajda P., Wang X., Wang Y., Wasti B., Wu Y., Xian R., Yoo S., Zhang P. Machine learning at facebook: Understanding inference at the edge 25th IEEE international symposium on high performance computer architecture, HPCA 2019, washington, DC, USA, February 16-20, 2019, IEEE (2019), 10.1109/HPCA.2019.00048 Google Scholar [44] Min C, Mathur A, Acer UG, Montanari A, Kawsar F. Sensix++: Bringing mlops and multi-tenant model serving to sensory edge devices, arXiv preprint arXiv:2109.03947. Google Scholar [45] Roh Y., Heo G., Whang S.E. A survey on data collection for machine learning: A big data - AI integration perspective IEEE Trans Knowl Data Eng, 33 (4) (2021), pp. 1328-1347, 10.1109/TKDE.2019.2946162 View in ScopusGoogle Scholar [46] Lv Z., Chen D., Lou R., Wang Q. Intelligent edge computing based on machine learning for smart city Future Gener Comput Syst, 115 (2021), pp. 90-99, 10.1016/j.future.2020.08.037 View PDFView articleView in ScopusGoogle Scholar [47] Budd S., Robinson E.C., Kainz B. A survey on active learning and human-in-the-loop deep learning for medical image analysis Med Image Anal, 71 (2021), Article 102062 View PDFView articleView in ScopusGoogle Scholar [48] Hymel S, Banbury CR, Situnayake D, Elium A, Ward C, Kelcey M, Baaijens M, Majchrzycki M, Plunkett J, Tischler D, Grande A, Moreau L, Maslov D, Beavis A, Jongboom J, Reddi VJ. Edge impulse: An mlops platform for tiny machine learning, http://dx.doi.org/10.48550/arXiv.2212.03332 CoRR abs/2212.03332 arXiv:2212.03332. Google Scholar [49] Ding C., Zhou A., Liu X., Ma X., Wang S. Resource-aware feature extraction in mobile edge computing IEEE Trans Mob Comput, 21 (1) (2022), pp. 321-331, 10.1109/TMC.2020.3007456 View in ScopusGoogle Scholar [50] Hazelwood K.M., Bird S., Brooks D.M., Chintala S., Diril U., Dzhulgakov D., Fawzy M., Jia B., Jia Y., Kalro A., Law J., Lee K., Lu J., Noordhuis P., Smelyanskiy M., Xiong L., Wang X. Applied machine learning at facebook: A datacenter infrastructure perspective IEEE international symposium on high performance computer architecture, HPCA, IEEE Computer Society (2018), pp. 620-629, 10.1109/HPCA.2018.00059 View in ScopusGoogle Scholar [51] Cui G., He Q., Li B., Xia X., Chen F., Jin H., Xiang Y., Yang Y. Efficient verification of edge data integrity in edge computing environment IEEE Trans Serv Comput, 15 (6) (2021), pp. 3233-3244 Google Scholar [52] Shahraki A., Ohlenforst T., Kreyß F. When machine learning meets network management and orchestration in edge-based networking paradigms J Netw Comput Appl, 212 (2023), Article 103558 View PDFView articleView in ScopusGoogle Scholar [53] Zaharia M., Chen A., Davidson A., Ghodsi A., Hong S.A., Konwinski A., Murching S., Nykodym T., Ogilvie P., Parkhe M., Xie F., Zumar C. Accelerating the machine learning lifecycle with mlflow IEEE Data Eng Bull, 41 (4) (2018), pp. 39-45 URL http://sites.computer.org/debull/A18dec/p39.pdf Google Scholar [54] Chen J., Ran X. Deep learning with edge computing: A review Proc IEEE, 107 (8) (2019), pp. 1655-1674 CrossRefView in ScopusGoogle Scholar [55] Li Y., Yuan G., Wen Y., Hu J., Evangelidis G., Tulyakov S., Wang Y., Ren J. Efficientformer: Vision transformers at mobilenet speed Adv Neural Inf Process Syst, 35 (2022), pp. 12934-12949 Google Scholar [56] Dwivedy V., Shukla H.D., Roy P.K. Lmnet: Lightweight multi-scale convolutional neural network architecture for covid-19 detection in iomt environment Comput Electr Eng, 103 (2022), Article 108325 View PDFView articleView in ScopusGoogle Scholar [57] Datta Gupta K., Sharma D.K., Ahmed S., Gupta H., Gupta D., Hsu C.-H. A novel lightweight deep learning-based histopathological image classification model for iomt Neural Process Lett, 55 (1) (2023), pp. 205-228 CrossRefView in ScopusGoogle Scholar [58] Ogundokun R.O., Misra S., Akinrotimi A.O., Ogul H. Mobilenet-svm: A lightweight deep transfer learning model to diagnose bch scans for iomt-based imaging sensors Sensors, 23 (2) (2023), p. 656 CrossRefView in ScopusGoogle Scholar [59] Rausch T., Hummer W., Muthusamy V., Rashed A., Dustdar S. Towards a serverless platform for edge AI Ahmad I., Sundararaman S. (Eds.), 2nd USeNIX workshop on hot topics in edge computing, hotEdge 2019, renton, WA, USA, July 9, 2019, USENIX Association (2019), pp. 1-7 URL https://www.usenix.org/conference/hotedge19/presentation/rausch CrossRefGoogle Scholar [60] Navaz A.N., Serhani M.A., El Kassabi H.T. Federated quality profiling: A quality evaluation of patient monitoring at the edge 2022 international wireless communications and mobile computing, IWCMC, IEEE (2022), pp. 1015-1021 CrossRefView in ScopusGoogle Scholar [61] Syrigos I., Angelopoulos N., Korakis T. Optimization of execution for machine learning applications in the computing continuum 2022 IEEE conference on standards for communications and networking, CSCN, IEEE (2022), pp. 118-123 CrossRefView in ScopusGoogle Scholar [62] Bhattacharjee A., Chhokra A.D., Sun H., Shekhar S., Gokhale A., Karsai G., Dubey A. Deep-edge: An efficient framework for deep learning model update on heterogeneous edge 2020 IEEE 4th international conference on fog and edge computing, ICFEC, IEEE (2020), pp. 75-84 CrossRefView in ScopusGoogle Scholar [63] Zhu T., Kuang L., Daniels J., Herrero P., Li K., Georgiou P. Iomt-enabled real-time blood glucose prediction with deep learning and edge computing IEEE Internet Things J, 10 (5) (2022), pp. 3706-3719 Google Scholar [64] Tamburri D.A. Sustainable mlops: Trends and challenges 22nd international symposium on symbolic and numeric algorithms for scientific computing, SYNASC 2020, timisoara, Romania, September 1-4, 2020, IEEE (2020), pp. 17-23, 10.1109/SYNASC51798.2020.00015 View in ScopusGoogle Scholar [65] Chen R., Pu Y., Shi B., Wu W. An automatic model management system and its implementation for AIOps on microservice platforms J Supercomput, 79 (10) (2023), pp. 11410-11426, 10.1007/s11227-023-05123-4 View in ScopusGoogle Scholar [66] Yumo L. An open-source and portable mLOps pipeline for continuous training and continuous deployment (Ph.D. thesis) Faculty of Science - University of Helsinki (2023) Google Scholar [67] Kairouz P., McMahan H.B., Avent B., Bellet A., Bennis M., Bhagoji A.N., Bonawitz K.A., Charles Z., Cormode G., Cummings R., D’Oliveira R.G.L., Eichner H., Rouayheb S.E., Evans D., Gardner J., Garrett Z., Gascón A., Ghazi B., Gibbons P.B., Gruteser M., Harchaoui Z., He C., He L., Huo Z., Hutchinson B., Hsu J., Jaggi M., Javidi T., Joshi G., Khodak M., Konečný J., Korolova A., Koushanfar F., Koyejo S., Lepoint T., Liu Y., Mittal P., Mohri M., Nock R., Özgür A., Pagh R., Qi H., Ramage D., Raskar R., Raykova M., Song D., Song W., Stich S.U., Sun Z., Suresh A.T., Tramèr F., Vepakomma P., Wang J., Xiong L., Xu Z., Yang Q., Yu F.X., Yu H., Zhao S. Advances and open problems in federated learning Found Trends Mach Learn, 14 (1–2) (2021), pp. 1-210, 10.1561/2200000083 View in ScopusGoogle Scholar [68] Xu J., Chen L., Ren S. Online learning for offloading and autoscaling in energy harvesting mobile edge computing IEEE Trans Cogn Commun Netw, 3 (3) (2017), pp. 361-373, 10.1109/TCCN.2017.2725277 View in ScopusGoogle Scholar [69] Studer S., Bui T.B., Drescher C., Hanuschkin A., Winkler L., Peters S., Müller K. Towards CRISP-ML(Q): A machine learning process model with quality assurance methodology Mach Learn Knowl Extr, 3 (2) (2021), pp. 392-413, 10.3390/make3020020 View in ScopusGoogle Scholar [70] Wirth R, Hipp J. Crisp-dm: Towards a standard process model for data mining. In: Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining. Vol. 1, Manchester; 2000, p. 29–39. Google Scholar [71] Marbán O., Segovia J., Ruiz E.M., Fernández-Baizán C. Toward data mining engineering: A software engineering approach Inf Syst, 34 (1) (2009), pp. 87-107, 10.1016/j.is.2008.04.003 View PDFView articleView in ScopusGoogle Scholar [72] Reddy S., Allan S., Coghlan S., Cooper P. A governance model for the application of AI in health care J Am Med Inform Assoc, 27 (3) (2020), pp. 491-497, 10.1093/jamia/ocz192 View in ScopusGoogle Scholar [73] Skogström H. The MLOps stack (2020) https://valohai.com/blog/the-mlops-stack/ Google Scholar [74] Antonini M., Pincheira M., Vecchio M., Antonelli F. An adaptable and unsupervised tinyml anomaly detection system for extreme industrial environments Sensors, 23 (4) (2023), p. 2344, 10.3390/s23042344 View in ScopusGoogle Scholar [75] Kreuzberger D, Kühl N, Hirschl S. Machine learning operations (mlops): Overview, definition, and architecture. IEEE Access. Google Scholar [76] Hossain MD, Sultana T, Akhter S, Hossain MI, Thu NT, Huynh LN, Lee G-W, Huh E-N. The role of microservice approach in edge computing: Opportunities, challenges, and research directions. ICT Express. Google Scholar [77] Zhang H., Xiao J., Wang J., Yang H. Resource virtualization in edge computing: A review 2022 global conference on robotics, artificial intelligence and information technology, GCRAIT, IEEE (2022), pp. 123-127 CrossRefView in ScopusGoogle Scholar [78] Nguyen D.C., Ding M., Pathirana P.N., Seneviratne A., Li J., Poor H.V. Federated learning for internet of things: A comprehensive survey IEEE Commun Surv Tutor, 23 (3) (2021), pp. 1622-1658 CrossRefView in ScopusGoogle Scholar [79] Oyebode O., Fowles J., Steeves D., Orji R. Machine learning techniques in adaptive and personalized systems for health and wellness Int J Hum–Comput Interact, 39 (9) (2023), pp. 1938-1962 CrossRefView in ScopusGoogle Scholar [80] Kakhi K., Alizadehsani R., Kabir H.D., Khosravi A., Nahavandi S., Acharya U.R. The internet of medical things and artificial intelligence: trends, challenges, and opportunities Biocybern Biomed Eng, 42 (3) (2022), pp. 749-771 View PDFView articleView in ScopusGoogle Scholar [81] Thapa C., Camtepe S. Precision health data: Requirements, challenges and existing techniques for data security and privacy Comput Biol Med, 129 (2021), Article 104130 View PDFView articleView in ScopusGoogle Scholar [82] Vajar P., Emmanuel A.L., Ghasemieh A., Bahrami P., Kashef R. The internet of medical things (iomt): a vision on learning, privacy, and computing 2021 international conference on electrical, computer, communications and mechatronics engineering, ICECCME, IEEE (2021), pp. 1-7 CrossRefGoogle Scholar [83] Koutras D., Stergiopoulos G., Dasaklis T., Kotzanikolaou P., Glynos D., Douligeris C. Security in iomt communications: A survey Sensors, 20 (17) (2020), p. 4828 CrossRefGoogle Scholar [84] Hameed S.S., Hassan W.H., Latiff L.A., Ghabban F. A systematic review of security and privacy issues in the internet of medical things; the role of machine learning approaches PeerJ Comput Sci, 7 (2021), Article e414 CrossRefGoogle Scholar [85] Rbah Y., Mahfoudi M., Balboul Y., Fattah M., Mazer S., Elbekkali M., Bernoussi B. Machine learning and deep learning methods for intrusion detection systems in iomt: A survey 2022 2nd international conference on innovative research in applied science, engineering and technology, IRASET, IEEE (2022), pp. 1-9 CrossRefGoogle Scholar [86] Papaioannou M., Karageorgou M., Mantas G., Sucasas V., Essop I., Rodriguez J., Lymberopoulos D. A survey on security threats and countermeasures in internet of medical things (iomt) Trans Emerg Telecommun Technol, 33 (6) (2022), Article e4049 View in ScopusGoogle Scholar [87] Tang Q, Chen Z, Guo Y, Liang Y, Ward R, Menon C, Elgendi M. Robust reconstruction of electrocardiogram using photoplethysmography: A subject-based model. Front Physiol 13. http://dx.doi.org/10.3389/fphys.2022.859763. Google Scholar Cited by (0) ☆ Part of the results presented in this work were obtained through the project “CENTER OF EXCELLENCE IN ARTIFICIAL INTELLIGENCE – AI4WELLNESS”, funded by Samsung Eletrônica da Amazônia Ltda., at Federal University of Ceará, Brazil, under the Information Technology Law No. 8.248/91. 1 Ph.D. 2 High School, Undergraduate Student. 3 M.Sc. 4 https://www.kubeflow.org/. 5 https://kubernetes.io/. View Abstract © 2024 Elsevier Ltd. All rights reserved. Recommended articles OCReP: An Optimally Conditioned Regularization for pseudoinversion based neural training Neural Networks, Volume 71, 2015, pp. 76-87 Rossella Cancelliere, …, Luca Rubini View PDF Enhancing urban economic efficiency through smart city development: A focus on sustainable transportation Computers and Electrical Engineering, Volume 116, 2024, Article 109058 Haojie Jiang, Manjiang Xing View PDF Intensions and extensions of granules: A two-component treatment International Journal of Approximate Reasoning, Volume 169, 2024, Article 109182 Tamás Mihálydeák, …, Mihir K. Chakraborty View PDF Show 3 more articles About ScienceDirect Remote access Shopping cart Advertise Contact and support Terms and conditions Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply."

Paper 2:
- APA Citation: Hethcoat, M. G., Jain, P., Parisien, M. A., Skakun, R., Rogic, L., & Whitman, E. (2024). Unrecorded tundra fires in Canada, 1986–2022. Remote Sensing, 16(2), 230. https://doi.org/10.3390/rs16020230
  Main Objective: To detect and map unrecorded tundra fires in Canada using Landsat data.
  Study Location: Tundra region of Canada
  Data Sources: Landsat imagery
National Burned Area Composite (NBAC) fire perimeters
  Technologies Used: Landsat imagery
Google Earth Engine
Random Forest algorithm
  Key Findings: Detected 209 new fires in the Canadian tundra region, increasing the known burned area by 30%.
The median fire size was 22.6 hectares, and most fires were less than 100 hectares in size.
69.5% of the newly detected fires did not have any satellite-derived hotspots associated with them.
  Extract 1: We used existing NBAC fire perimeters from within the study area to settle on the thresholds outlined below.
  Extract 2: Candidate fires that met all four criteria were exported as vectors from GEE at 90 m spatial resolution (Figure 3).
  Limitations: Limited to the temporal range of available Landsat imagery (1986-2022).
Did not quantify uncertainties or confidence intervals in fire perimeter mapping.
The study did not include an assessment of fire severity or post-fire ecosystem recovery.
  Relevance Evaluation: The paper is highly relevant to my evaluation because it provides a comprehensive and accurate dataset of wildfires in the Canadian tundra region, a vital resource that was previously unavailable. The methodology developed by the researchers addresses the specific point of optimizing the detection of tundra fires in remote and sparsely vegetated areas. The study's findings contribute to the limited body of research on fire dynamics in tundra ecosystems, enhancing our understanding of their frequency, size, and behavior.

The accuracy and reliability of the dataset make it a valuable resource for further research on fire ecology, climate change impacts, and land management practices in the tundra biome. It can serve as a baseline for future studies, enabling researchers to track trends and patterns in fire occurrence and behavior over time.
  Relevance Score: 1.0
  Inline Citation: (Hethcoat et al., 2024)
  Explanation: The study used Landsat imagery to generate a dataset of wildfires in the tundra region of Canada spanning more than three decades. It increased the known previously burned area by 30%. The researchers employed a three-step workflow, starting with the identification of candidate fire detections based on four criteria derived from existing fire perimeters within the area. These candidates were inspected visually and re-mapped at a finer resolution if confirmed. 

Several spectral indices and Random Forest methods were employed to identify changes in fire indices between pre- and post-fire imagery. This approach allowed them to reduce false positives and produce a commission error-free dataset.

 Full Text: >
"This website uses cookies We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Consent Selection Necessary Preferences Statistics Marketing Show details                 Deny Allow selection Allow all   Journals Topics Information Author Services Initiatives About Sign In / Sign Up Submit   Search for Articles: Remote Sensing All Article Types Advanced   Journals Remote Sensing Volume 16 Issue 2 10.3390/rs16020230 Submit to this Journal Review for this Journal Propose a Special Issue Article Menu Academic Editor Jungho Im Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links Article Views 812 Table of Contents Abstract Introduction Materials and Methods Results Discussion Limitations Conclusions Author Contributions Funding Data Availability Statement Acknowledgments Conflicts of Interest Appendix A Appendix B References share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open AccessArticle Unrecorded Tundra Fires in Canada, 1986–2022 by Matthew G. Hethcoat 1,*, Piyush Jain 1, Marc-André Parisien 1, Rob Skakun 1, Luka Rogic 2 and Ellen Whitman 1 1 Northern Forestry Centre, Canadian Forest Service, Natural Resources Canada, 5320 122 Street NW, Edmonton, AB T6H 3S5, Canada 2 Department of Electrical and Computer Engineering, University of British Columbia, 5500-2332 Main Mall, Vancouver, BC V6T 1Z4, Canada * Author to whom correspondence should be addressed. Remote Sens. 2024, 16(2), 230; https://doi.org/10.3390/rs16020230 Submission received: 28 November 2023 / Revised: 20 December 2023 / Accepted: 4 January 2024 / Published: 6 January 2024 (This article belongs to the Section Environmental Remote Sensing) Download keyboard_arrow_down     Browse Figures Versions Notes Abstract Climate-driven changes in fire regimes are expected across the pan-Arctic region. Trends in arctic fires are thought to be generally increasing; however, fire mapping across the region is far from comprehensive or systematic. We developed a new detection workflow and built a dataset of unrecorded tundra fires in Canada using Landsat data. We built a reference dataset of spectral indices from previously mapped fires in northern Canada to train a Random Forest model for detecting new fires between 1986 and 2022. In addition, we used time series information for each pixel to reduce false positives and narrow the large search space down to a finite set of regions that had experienced changes. We found 209 previously undetected fires in the Arctic and sub-Arctic regions, increasing the mapped burned area by approximately 30%. The median fire size was small, with roughly 3/4 of the fires being <100 ha in size. The majority of newly detected fires (69%) did not have satellite-derived hotspots associated with them. The dataset presented here is commission error-free and can be viewed as a reference dataset for future analyses. Moreover, future improvements and updates will leverage these data to improve the detection workflow outlined here, particularly for small and low-severity fires. These data can facilitate broader analyses that examine trends and environmental drivers of fire across the Arctic region. Such analyses could begin to untangle the mechanisms driving heterogeneous fire responses to climate observed across regions of the Circumpolar North. Keywords: Arctic; Google Earth Engine; hotspots; lightning; MODIS; NBAC; Normalized Burn Ratio; Random Forest; Tasseled Cap; VIIRS 1. Introduction Empirical data and modeling indicate a growing likelihood of fire in the Arctic [1,2]. There are increasing trends in the length of the fire season [1,3], air temperature, fuel load and availability, and a host of fire-related abiotic factors in the Arctic and sub-Arctic zones [4,5,6]. While many northern regions are changing faster than those at lower- and mid-latitudes [2], global patterns of Arctic and tundra fire activity indicate substantial spatio-temporal variation in burning. Fires have increased in Alaska and Siberia [1,7], but a slight decreasing trend has been observed in the Northwest Territories of Canada [6,8]. The accurate spatio-temporal mapping of fires is fundamental to understanding changes in the burned area, fire severity, post-fire vegetation recovery, and for determining any climate- and human-driven changes in fire regimes globally. Furthermore, such maps offer an essential baseline from which to measure changes, particularly over periods in which rapid changes are forecasted. In recent decades, a suite of space-borne satellite sensors have facilitated large-scale fire monitoring and mapping, enabling a global-scale analyses of the patterns, trends, and drivers of wildfires [9,10]. Indeed, since approximately 1979, beginning with the Advanced Very High Resolution Radiometer, satellites have acquired sub-daily observations of fire activity [11]. However, many of the purpose-built fire products derived from thermal anomalies (referred to hereafter as hotspots) are spatially coarse and thus cannot detect small fires nor delineate perimeters effectively to produce detailed burned area estimates. The Landsat missions, with their increased spatial resolution (over Moderate Resolution Imaging Spectroradiometer (MODIS) or Visible Infrared Imaging Radiometer Suite (VIIRS) sensors) and global coverage, have enabled a fine-tuned understanding of burn severity and area burned [12]. Canada has two federally maintained national fire databases—the Canadian National Fire Database (CNFDB) and the National Burned Area Composite (NBAC)—that collect fire data from a variety of sources (e.g., provincial and territorial agencies, federal departments and agencies, satellite mapping techniques, etc.). The CNFDB is composed of fire maps of varying quality, produced by jurisdictional fire management agencies and compiled into a single national database [13]. The NBAC uses a consistent methodology to detect and map new fires, as well as refine the perimeters of historic fires that were not accurately delineated within the CNFDB [13,14]. Critically, data from the NBAC are used by the government of Canada to report on progress toward international commitments to monitor anthropogenic emissions and are regarded as the authoritative source on more recent fires in Canada [15,16]. Historically, fire suppression activity has been absent or minimal in the remote northern areas of Canada and, consequently, fires that have occurred outside of forested regions (i.e., north of the treeline) or far from inhabited areas have not been monitored or mapped [17]. Indeed, the territory of Nunavut does not have a fire management agency. Moreover, even newer methodological techniques to improve fire mapping in Canada have limited their study domain to exclude regions north of the treeline (e.g., [13,18]). The omission is likely driven by assumptions about a lack of fire in the region and the fact that disturbance mapping in tundra environments is known to be extremely challenging with remotely sensed data [19]. Consequently, the NBAC and CNFDB do not represent a complete picture of fires in Canada and there is a clear gap in our understanding of fires in remote northern regions, particularly above the treeline. The primary objective of this study was to detect and map unrecorded tundra fires within Canada using Landsat data. There is a growing need to assess and understand potential changes in fire regimes within Canada, particularly in light of the 2023 fire season, where a record-breaking area of >15 M ha burned [20]. Given the lack of systematic fire monitoring outside of forested regions, we sought to advance the build-out and maintenance of a more comprehensive accounting of fires in northern Canada since 1986. 2. Materials and Methods 2.1. Study Area We focused on three ecozones in northern Canada [21] covering areas roughly north of the treeline—the Arctic Cordillera, Northern Arctic, and Southern Arctic ecozones (Figure 1). The study area stopped at approximately 74.5 degrees north (i.e., in the Arctic Ocean north of Banks, Victoria, Prince of Wales, Somerset, and the Baffin Islands). The vegetation present across the region is diverse and is generally north of the treeline, consisting of a combination of herbaceous plants, shrubs, mosses, and lichens (see [22] for a more complete breakdown of vegetation characteristic and communities present across the various subzones within the region). Across the study area, systematic fire monitoring has historically been of low priority. Although there are many small communities throughout, it is very sparsely populated. Land use mainly consists of low-impact traditional and cultural activities, with few permanent roads or energy corridors; however, various mines operate across the region. Figure 1. Canadian ecozones and broad land cover types (from North American Land Change Monitoring System [23]) over the study area searched for tundra fires between 1986 and 2022. The Southern Arctic is outlined in dark blue, the Northern Arctic is outlined in pink, and the Arctic Cordillera is outlined in yellow. Needleleaf forests are dark green, shrublands are brown, grassland-lichen-moss are light green, barren-lichen-moss are medium green, water and wetlands are blue, barren ground is light grey, and snow is white. Provincial and territorial borders are in white, major lakes are light blue, and ecozones outside the study area are outlined in black. 2.2. Detecting Fires 2.2.1. Datasets, Satellite Imagery, and Initial Modeling We first sought to build a reference dataset of spectral fire indices (Table A1 and Table A2) for northern Canada to assist with fire detection. We sampled burned and unburned point locations from all NBAC fires that occurred within the study area and the next adjacent ecozones south (i.e., the northernmost fires available) between 2014 and 2020 (n = 253). Most NBAC fires had 25 burned pixels and 30 unburned pixels randomly sampled, with unburned locations coming from within a 1 km buffer surrounding fire perimeters. More unburned pixels were sampled in an attempt to balance out sampling, because burned areas were usually larger and therefore had a smaller chance of containing masked water bodies, which resulted in failed sampling. We sampled differenced fire indices, subtracting imagery 1-year post-fire from 1-year pre-fire. Some smaller fires could not fit 25 burned and 30 unburned sample pixels (e.g., a fire less than two hectares has fewer than 25 pixels in the entire burn scar), and the final dataset contained 11,056 points, of which 4989 (45%) were unburned and 6067 (55%) were burned. These data were then used to train and validate a Random Forest (RF) model for detecting fires in our study area (details below). We used Landsat (5, 7, 8, and 9) Collection 2 surface reflectance data in Google Earth Engine (GEE) to calculate spectral indices associated with fire mapping and vegetation change [24]. Pixels covered by clouds, shadows, snow, and water were removed using the quality flags provided within the QA_PIXEL band in Landsat Collection 2. A summer-season mosaic was generated for each year using the median of all available scenes between 15 June and 15 September. We removed permanent water bodies using the global water mask developed by the European Commission’s Joint Research Centre [25]. Preliminary model training and testing was conducted in a Google Colaboratory notebook [26] to optimize RF tuning parameters and select fire indices that accurately classified burned areas (Table A3 and Table A4). We used a suite of Scikit-learn [27] functions for model selection, feature reduction, and k-fold cross-validation to select a final RF model for efficient computation in GEE (Appendix B). The top RF model showed strong predictive performance during model validation (Table A4); however, during model deployment to the study area—a region not well represented in the training data—this model regularly predicted sparsely vegetated regions and atmospheric artifacts as having a high probability of being burned, despite having not experienced a fire (Figure 2). Through subsequent testing of candidate variables, we identified three metrics that maintained strong detection while greatly reducing spurious detections in our northern study area: Normalized Burn Ratio 2 (NBR2), Tasseled Cap Brightness (TCB), and Tasseled Cap Greenness (TCG, Figure 2, Table A4). This RF model was then used as part of the fire detection methodology detailed below. Figure 2. Comparison of Random Forest (RF) model predictions using the top-performing model during validation (a,c) versus the chosen model for the study area after further regional testing (b,d). Probability of being burned is displayed in shades of red, with darker red being higher probability of fire. The top images show higher incidences of false positives from insufficient cloud removal near the fires (circled in (a,c)). Noticeable banding present in imagery was a common artifact from the missing data in Landsat 7 scan-line corrector error images. 2.2.2. Fire Detection We used a three-step workflow to map fires (Figure 3). First, candidate fire detections—pixels that met a four-rule criteria (details below)—were exported from GEE. Next, candidate fires were visually inspected to confirm a change event was fire-related, as changes in hydrology, thaw slumps, mining activities, road expansions, phenological differences, etc. were frequently detected. Finally, we used R Statistical Computing Software, version 4.2.2 [28], to revisit confirmed fires and delineate a refined fire perimeter at 30-m resolution, utilizing the ‘rgee’ package [29]. This multi-stage approach was used to reduce the large search space down to a finite set of areas that had experienced change events and stay within the computation limits of GEE. Figure 3. Fire detection workflow used to locate and confirm Canadian tundra fires between 1986 and 2022. The first step in the multi-stage fire detection methodology was to identify candidate fires in GEE using a four-rule criteria. We used existing NBAC fire perimeters from within the study area to settle on the thresholds outlined below. The first criterion retained only pixels with ≥90% of the RF votes identified as being burned. A high RF threshold maintained detection and reduced false positives in sparsely vegetated areas and pixels with insufficient cloud removal. Despite a high RF threshold, many false positives remained and we incorporated two additional rules based on time series information. Criteria two and three compared post-fire imagery to a pre-fire median of up to three years. For criterion two, pixels were retained if there was a >50% decrease in post-fire NBR2, compared to a three-year pre-fire median. Inter-annual differences in phenology and apparent drought stress resulted in NBR2 drops > 30% in some years; we sought to account for these variations. The third criterion required the difference between the post-fire NBR2 and the three-year pre-fire median to be ≤−0.1. Again, inter-annual differences in seasonal mosaics resulted in NBR2 differences of ≅0 and we wanted to exclude those occurrences as well. Finally, the post-fire NBR values needed to be <0. Candidate fires that met all four criteria were exported as vectors from GEE at 90 m spatial resolution (Figure 3). This spatial resolution (i.e., 90 m rather than 30 m) was required in order to stay within the computation limits of GEE, given the size of the study area. The second step in the methodology required a visual inspection of each candidate fire ≥ four pixels in size (encompassing 50–100 candidate areas per year). Four pixels was chosen for a minimum size (approximately four hectares) because some atmospheric effects still satisfied the four criteria and resulted in isolated one- to two-pixel noise, particularly over snowy and alpine regions in the northeast of the study area. A single reviewer used a suite of custom GEE scripts to visualize a time series of imagery over each candidate fire to confirm a change event was associated with a fire. Examples of events that were not fire-related are widespread regional hydrological changes visible throughout the landscape, landslides or thaw slumps, and human footprint expansion through mining and road building. This manual validation step was preferred, rather than a fully automated workflow, because of the dynamic nature of the landscape—changes in hydrology, thaw slumps, mining activities, road expansion, etc., frequently satisfied the four-rule criteria. We finalized perimeters for confirmed fire events using the RF model from step one; however, this time retaining pixels with ≥50% of the votes as burned for perimeter mapping. The 50% threshold was chosen by comparing a range of thresholds against existing NBAC fire perimeters from within the study area. In general, fire perimeters were generated via RF prediction on differenced fire imagery, subtracting 1-year pre-fire imagery from 1-year post-fire imagery. However, some small or low-severity (n ≅ 20) fire perimeters were output with null geometries (i.e., no pixels had ≥ 50% votes as burned because of rapid vegetation recovery), and 1-year pre-fire imagery was subtracted from year-of-fire in those cases. 2.3. Validation We cross-referenced our dataset against two data sources: (1) existing fires within the NBAC dataset and (2) satellite-derived hotspots from MODIS Terra (MOD14A1), MODIS Aqua (MYD14A1), and VIIRS (VNP141A). Sixty-six fires within the study area were already in the NBAC database and we calculated the omission rate of known fires against our new dataset. When using hotspots, we searched for additional fires potentially missed by our methodology. We buffered each hotspot by 3 km and output any potential fire perimeters using the RF prediction, keeping pixels ≥ 50% class votes as burned. As Terra began acquiring data in 2000, Aqua in 2002, and VIIRS in 2012, hotspots were searched over this subset of the study period (i.e., 2000–2022). 3. Results We detected 206 new fires in our study area between 1986 and 2022 using our workflow, with an additional 3 fires found during hotspot validation, for a total of 209 newly mapped fires. Thus, including the existing NBAC fires, the region had 275 documented fires over the study period (Figure 4). In the combined dataset, the median number of fires in a given year was five and the median annual burned area was 219 hectares. The median fire size was 22.6 hectares (bootstrapped 95% CI = 16.2–28.3 ha) and 210 of the 275 total fires were <100 ha in size. No fires were detected from any source in 2004 or 2005. In total, the new fires added approximately 12,622 ha (a 32.8% increase) to the previously known total for the region (38,470 ha). Figure 4. Distribution of (a) fire size in hectares, (b) number of fires per year, and (c) burned area per year within the study area from 1986 to 2022. The National Burned Area Composite (NBAC) data are in grey and the new fires detected in this study are in black, with the median fire size denoted by the vertical dashed lines (in (a)). Nearly half of the new fires were in the Arctic (n = 96; 45.9%), defined as latitudes above 66° N by the Arctic Monitoring and Assessment Programme (Figure 5). No new fires were found within the Arctic Cordillera ecozone, though a single fire from 2022 was already mapped in the NBAC (>3500 ha), inside Kuururjuaq National Park, QC, representing the lone fire in the region over the study period. The Northern Arctic ecozone had 41 fires, most generally southwest of Wager Bay, NU, except for 1 fire on Baffin Island in 2009 (25 ha) and another in the Qikiqtaaluk Region, NU in 2020 (2 ha). The majority of newly detected fires occurred in the Southern Arctic ecozone, with 168, spread across three general regions: (1) the western edge of the ecoregion—forming a strip between Great Bear Lake and Tuktoyaktuk, NT; (2) northwest of Hudson Bay; and (3) northern Quebec (Figure 5). Figure 5. Spatial distribution of newly detected tundra fires that occurred between 1986 and 2022 (orange circles in (a), with individual fires north of 66°N shaded lighter orange. Fires previously mapped in the NBAC are black stars. Panels (b–d) are centered over each sub-region of fire clusters, showing the fire area as graduated point sizes and the year of burning as colors. Of the 66 fires already in the NBAC database, our methodology failed to detect 5 (i.e., an 7.6% omission rate), with areas of approximately 9.4, 4.5, 3.4, 1.9, and 0.9 hectares. In terms of omission of the burned area, those five fires accounted for 0.05% of the total burned area previously mapped (approximately 20 ha out of 38,470). Using satellite-derived hotspots between 2002 and 2022, we found three additional fires missed by our methodology, with areas of approximately 12, 8, and 8 hectares. The majority of the fires already in the NBAC database had hotspots associated with them (60.1%), reflecting one of the ways these fires have historically been detected and mapped; however, an additional 15 fires in the NBAC database did not have any hotspots and were likely opportunistically found due to their proximity to other fires targeted for mapping. In contrast, 69.5% of the newly detected fires with our method did not have any hotspots associated with them (Table 1). The median fire size was larger for fires that had hotspots (64.3 ha) compared to those without (13.4 ha) and all fires >~150 ha had hotspots. Table 1. Presence of thermal anomalies (i.e., hotspots) for fires within the study region, 2000–2022. 4. Discussion Satellite data are critical for monitoring fire dynamics globally, particularly in remote regions faced with rapid climate- and human-driven changes. We used Landsat imagery to generate a dataset of wildfires for the tundra region of Canada since 1986, increasing the known burned area by about 30%. In general, we did not find widespread, unaccounted-for burning across the study area. In an average year (i.e., median), the region—an area only slightly smaller than Argentina—had five fires and burned roughly 200 ha. The approach outlined here combines elements, conceptually, from existing methods used to map fires in Canada [13,14,30]; however, some adjustments were made to tailor the detection approach to the study region. Specifically, our RF model (i.e., criterion 1) used bi-temporal Landsat imagery to detect changes in fire indices, akin to [13] and [14], but our approach used multiple indices (dNBR2, dTCB, and dTCG) and RF modeling as opposed to thresholding dNBR values. Similarly, changes in pixel value time series (i.e., criteria 2 and 3) are analogous to the breakpoint methods used by Hermosilla et al. [30] to detect fires, albeit much simplified to efficiently run in GEE. Moreover, we chose to use a multi-stage approach, whereby candidate fires were examined individually and re-mapped at a finer scale if confirmed, because attempts to produce a fully automated workflow (like the approaches referenced above) persistently included land-use and land-cover changes that were not fire-related (e.g., thaw slumps, road development, mine expansion, hydrologic changes). Thus, our methodological adjustments were focused on reducing false positives and producing a commission error-free dataset. Our RF model used three spectral indices less commonly used (dNBR2, dTCB, and dTCG) for fire mapping (but see [31,32,33,34]). During model development and training we assessed many of the spectral indices commonly used for mapping fire perimeters and characterizing burn severity. However, during model deployment across our study area, we found that two of the tasseled cap bands (brightness and greenness) and NBR2 performed well at reducing false positives (Figure 2). NBR2 is known to penetrate smoke and clouds effectively, because of the incorporation of longer wavelength bands in the calculation [34], and is likely one of the major contributions to the reduction in false positives observed (Figure 2). In addition, NBR2 has been shown to perform better at detecting surface fires, relative to NBR [35], and might explain why NBR performed well during model training but NBR2 performed better over the study area. That is, the majority of the training data came from northern boreal fires and were thus trained and validated against spectral characteristics of forest fires, whereas surface fires were more common across the study area. Tasseled cap indices (TCs), particularly brightness and greenness, have a long history in change detection applications across a variety of ecosystems [31,32], but are more often used to model post-fire recovery trajectories rather than map fire perimeters [36]. However, a number of studies from high latitudes have highlighted the usefulness of TCs for mapping fires [32,33,37]. TCs calculations decompose the visible, near-infrared, and short-wave infrared Landsat bands (n = 6) into three orthogonal bands and thus provide information about spectral changes across the full range of reflectance values (i.e., dimensionality reduction). Consequently, because TCs were not derived for a particular vegetation type, biome, or range of the electromagnetic spectrum measured using Landsat, they are likely sensitive to changes across diverse vegetation types present in tundra environments [32,37]. For example, the spectral signatures of nonvascular plants (e.g., Bryophytes) and lichens can be significantly influenced by moisture content [19], with both photosynthetic activity and respiration tied to water availability. Given that this region lacks a strong reflectance signal from chlorophyll in tall vegetation, as relied on for fire mapping in more southern ecosystems, these alternate indices may be better suited to capture fire-induced change in low-vegetation and low-fuel environments. Our finding that the majority of fires in the study area did not have hotspots associated with them aligns with other recent studies that have highlighted the lack of a thermal signal for smaller, high-latitude fires [38,39]. Moreover, it suggests that prior studies that have used hotspots to constrain their search for Arctic and/or tundra fires likely omitted many smaller fires [12,40,41]. The lack of reliable hotspots is precisely why we tailored our methodology to not rely on them as a data source. In mapping exercises, the omission of small fires (<100 ha) tends to be more problematic in regions where they account for a significant proportion of the total area burned [42]. Indeed, the 200+ additional fires we found amounted to an increased burned area of about 30%, whereas recent work in sub-Saharan Africa found small fires (<100 ha) added >80% to previously mapped totals [42]. The annual burned areas in the boreal and taiga regions of Canada tend to be dominated by a small number of large fires, with 90–95% of the burned area coming from 5–10% of fires annually [8,43,44]. Our dataset does not reflect this pattern as strongly, with >90% of the burned area coming from approximately 23% of the fires (62/275) and >75% of fires being <100 ha in size (210/275). Moreover, these proportions are probably conservative, because we surely omitted some small fires in this initial study. Between 2012 and 2022—the study period during which both the VIIRS and MODIS sensors were operating in the region—we found an unexpected pattern in hotspot detections for the fires in our dataset (Table 2). In particular, the MODIS sensors were more likely to detect a fire, despite having a coarser spatial resolution than the VIIRS sensors (1 km versus 375 m). This was a somewhat surprising finding, as others have shown that the VIIRS I-Band offers an improvement over the MODIS sensors in detection efficiency for small fires in other high-latitude and low-biomass regions [45,46]. We initially thought this difference might be related to overpass time and, possibly, early detection of short, wind-driven burn events (as the Terra platform passes over the equator at approximately 10:30 a.m. local time, followed by one of the VIIRS platforms at 12:40 p.m., then both the Aqua platform and the second VIIRS platform at approximately 1:30 p.m.). However, of the seven fires only seen by the MODIS sensors, the two smallest fires were detected only by the Aqua platform—the platform more temporally matched with the VIIRS sensors. Thus, it seems unlikely that overpass timing was the cause. Canada’s Arctic region is known to be extremely cloudy [47,48], and it is also possible that differences in cloud masking procedures between the two sensors resulted in altered detection efficiencies. Table 2. Summary of thermal anomalies detected by the MODIS and VIIRS satellite sensors between 2012 and 2022 (i.e., the study period during which both sensors were operational). All fire sizes are listed for MODIS and VIIRS, but only the smallest five detected by both sensors and the largest five neither sensor detected are listed. Superscripts a and t (for MODIS) indicate if the fire was exclusively detected by the Aqua or Terra platform, respectively (with no subscript representing detection by both platforms). An alternative explanation may be due to the higher fire radiative power (FRP) detected by the MODIS sensors at northern latitudes [49]. While FRP is not part of the detection algorithm on its own, it is calculated directly from brightness values (in Kelvin) detected by each sensor [49]. Thus, the relatively lower FRP (which is derived from brightness) observed by the VIIRS sensor, suggests it is possible that the VIIRS fire detection algorithm may be unintentionally calibrated in a manner that misses fires in this region. Specifically, the VIIRS and MODIS fire-detection algorithms are similar at their core—both relying on the MODIS C6 algorithm—but the VIIRS algorithm separates fire pixels from background pixels using only the fixed thresholds test (see [50]), while the MODIS algorithm uses the same fixed thresholds test but also incorporates a newer dynamic threshold test [49,50,51]. Indeed, a post-hoc examination of the archived near-real-time Fire Information for Resource Management System (FIRMS) Global VIIRS and MODIS data [52] over fires observed by both sensors found generally higher brightness values for the MODIS sensors and a consistent saturation from the VIIRS sensors (Figure 6a). Moreover, given brightness is constrained onboard VIIRS, there is a resulting disconnect in the relationship between brightness and FRP (Figure 6b). Figure 6. Brightness values recorded for VIIRS (black) and MODIS (grey) over tundra fires both sensors detected between 2012 and 2020 (a). Relationship between detected brightness and fire radiative power (b). We were unable to distinguish the cause of the fire ignitions (human or lightning caused) with the data available. Future analyses using data from the Canadian Lightning Detection Network [53] may enable some discrimination between human- and lightning-caused fires for the region. Fire occurrence in Arctic Canada is determined using a mix of top-down (weather) and bottom-up (fuel) controls, with weather being particularly important [54]. As anthropogenic climate change has created severe warming in the Arctic [55], there have been observations of increasing fuel availability from taller and more productive vegetation [56] and an increase in fire-conducive weather conditions [57]. With increasing fuel and worsening fire weather acting as dual positive feedbacks to fire in this region, our findings further highlight the importance of monitoring and continued research to better understand arctic fire regimes, where a few small fires could shift to become a much more substantial area burned, if thresholds are surpassed [58]. Although none have been reported in recent decades, the potential for large (e.g., >1000 ha) wildfires likely exists across large parts of the study area. Neighboring Alaska has experienced a number of significant tundra wildfires in recent decades, including one > 100,000 ha [59]. While the conditions conducive to tundra wildfires of this extent are unlikely in much of the Canadian tundra, a formal examination of wildfire potential in this biome represents an important future area of research. 5. Limitations While our results represent a step toward providing a more complete picture of burning over the preceding decades, we almost certainly missed some small fires, particularly in light of the frequency of smaller fires in the data (e.g., nearly 1/3 of fires were ≤10 ha; Figure 1a). Our method only examined candidate fires that were exported at 90 m pixel resolution and were at least 4 connected pixels in size. This resolution was initially determined with GEE computation limitations, but we continue to work toward the goal of using the native Landsat pixel size (i.e., 30 m) in future versions to more consistently identify smaller fires. The use of smaller pixels may help maintain the connectedness of burned patches, ensuring they are of sufficient size to be verified by the methodology. For example, a long, narrow fire might not have had four connected pixels, whereas a small circular one would be more likely to be connected. In addition, the patchiness of fire intensity would also influence whether four contiguous pixels met the criteria—even if more than four pixels fell within what would ultimately have formed the fire perimeter. Consequently, future versions will use native Landsat resolution and refine the criteria around the minimum connected pixels for examining candidate fires. We used a somewhat conservative study area and could have extended the study domain farther south and undoubtedly map more fires missing from the NBAC database. We opted to initially focus on the Arctic and sub-Arctic regions because these have been left out of recent advances in automated burn mapping for Canada [13,18,30]. The region’s exclusion has been, in part, the result of prior assumptions about a lack of fire in the region. In addition, disturbance mapping in the tundra is notoriously challenging with remotely sensed data [19] and is likely another reason the area has been omitted from so many mapping exercises over the years. The spectral signature of tundra environments is highly sensitive to species and community composition, inter- and intra-seasonal phenological events, and a host of abiotic factors [19,60]. Collectively, this has made automated disturbance mapping for the region fraught with commission and omission errors and is precisely why we included a manual validation step in our initial workflow. There are certainly more fires that still need to be folded into the NBAC database; however, the dataset here can be used to build an improved workflow with training data from a broader area, now that a more robust dataset of fires exists for the region. 6. Conclusions The global Arctic and sub-Arctic are undergoing unprecedented changes. Continued warming has the potential to increase fire risk in the Arctic via increased lightning, increased human activity, drying of peatlands, migration of fuels (e.g., northern advance of the treeline), altered species composition, and more [1,3,4,61]. Yet, the region will likely see the continued thawing of permafrost, increased variability in rainfall, and shifting hydrology [4,62], and so the extent to which these myriad changes will interact and how burning will be impacted remains unknown. Moreover, given recent findings that tundra fire locations are more likely to become major methane sources [63], as a consequence of permafrost thaw, the climate implications of increased fire activity are vital to understand. Regardless of the pattern or trend in future burning, accurate historical mapping of Arctic and sub-Arctic fires is a critical first step to understand potential changes in fire regimes. The dataset presented here can be viewed as a reference or benchmark dataset for future analyses to characterize trends and drivers of fire across the region or to assess impacts to ecosystem functions and recovery. Such analyses could begin to untangle the mechanisms driving heterogeneous fire responses observed between Alaska, Canada, and Russia [3]. Author Contributions Conceptualization, P.J., M.-A.P. and E.W.; methodology, M.G.H., P.J., M.-A.P. and E.W.; validation, M.G.H., R.S., L.R. and E.W.; formal analysis, M.G.H., L.R. and E.W.; writing—original draft preparation, M.G.H.; writing—review and editing, M.G.H., P.J., M.-A.P., R.S., L.R. and E.W.; visualization, M.G.H.; supervision, P.J., M.-A.P. and E.W.; project administration, E.W.; funding acquisition, P.J., M.-A.P. and E.W. All authors have read and agreed to the published version of the manuscript. Funding LR was supported by funding through Natural Resources Canada and the BC Student Co-op Program. Data Availability Statement The new fire perimeters will be incorporated into the NBAC dataset, which are openly available at https://cwfis.cfs.nrcan.gc.ca/datamart/metadata/nbac (accessed on 20 December 2023). Relevant code will be available at https://github.com/HethcoatMG/CADtundraFires (accessed on 20 December 2023). In addition, a Google Earth Engine App to produce vectors of candidate fires is available at https://mghethcoat.users.earthengine.app/view/tundrafirerf (accessed on 20 December 2023). Acknowledgments We would like to thank Jurjen van der Sluijs from the Government of Northwest Territories for early discussions around tundra fires and the Landsat Long-Term Change Detection dataset. We thank Matthew Coyle from the Government of Northwest Territories for helpful discussions around MODIS and VIIRS hotspot detections at high latitudes. In addition, we thank four anonymous reviewers for their comments and suggestions that helped improve the manuscript. Conflicts of Interest The authors declare no conflicts of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results. Appendix A Table A1. Landsat band abbreviations used in equations within Table A2.      Table A2. Equations for fire indices used in Random Forest models built for fire detection (double asterisk represents exponentiation). Tasseled Cap coefficients are from [64]. Appendix B Preliminary model training and testing was conducted in a Google Colaboratory notebook [26] to optimize RF tuning parameters and select fire indices that accurately classified burned areas. We explored different combinations of hyperparameters using a suite of tools from scikit-learn [27]. Initially, we ran 100 different permutations with RandomizedSearchCV to find hyperparameters. Next we used GridSearchCV to exhaustively search over a narrower range of hyperparameters identified by RandomizedSearchCV. We then used RFECV to find the optimal set of features via Recursive feature elimination with cross-validation. We performed a 5-fold cross-validation, using RepeatedStratifiedKFold, to test optimal hyperparameter and feature selection vs. the base RandomForestClassifier model with all variables. At each point, we compared model accuracy against the same model using only 100 trees to assess the accuracy of a model with shorter computation time (Table A3 and Table A4). Similarly, we compared the optimal model against a model that used the three best predictors, calculated using the gini criterion, to assess the accuracy of a model with an even shorter computation time (Table A4). Finally, after initial model deployment, persistent false positives were further reduced using a subset of indices (i.e., tundra variables in Table A4). Table A3. Model sets and hyperparameters used to detect fires in Random Forest models. Tuning parameters from only the highest performing model are listed and were used across all final model sets. Table A4. Average of 5-fold cross-validation from Random Forest models built in Google Colaboratory using randomly sampled burned and unburned points from 253 fires in northern Canada 2014–2020. References Descals, A.; Gaveau, D.L.A.; Verger, A.; Sheil, D.; Naito, D.; Peñuelas, J. Unprecedented Fire Activity above the Arctic Circle Linked to Rising Temperatures. Science 2022, 378, 532–537. [Google Scholar] [CrossRef] [PubMed] Walsh, J.E.; Ballinger, T.J.; Euskirchen, E.S.; Hanna, E.; Mård, J.; Overland, J.E.; Tangen, H.; Vihma, T. Extreme Weather and Climate Events in Northern Areas: A Review. Earth-Sci. Rev. 2020, 209, 103324. [Google Scholar] [CrossRef] McCarty, J.L.; Aalto, J.; Paunu, V.-V.; Arnold, S.R.; Eckhardt, S.; Klimont, Z.; Fain, J.J.; Evangeliou, N.; Venäläinen, A.; Tchebakova, N.M.; et al. Reviews and Syntheses: Arctic Fire Regimes and Emissions in the 21st Century. Biogeosciences 2021, 18, 5053–5083. [Google Scholar] [CrossRef] Berner, L.T.; Massey, R.; Jantz, P.; Forbes, B.C.; Macias-Fauria, M.; Myers-Smith, I.; Kumpula, T.; Gauthier, G.; Andreu-Hayles, L.; Gaglioti, B.V.; et al. Summer Warming Explains Widespread but Not Uniform Greening in the Arctic Tundra Biome. Nat. Commun. 2020, 11, 4621. [Google Scholar] [CrossRef] [PubMed] Leipe, S.C.; Carey, S.K. Rapid Shrub Expansion in a Subarctic Mountain Basin Revealed by Repeat Airborne LiDAR. Environ. Res. Commun. 2021, 3, 071001. [Google Scholar] [CrossRef] York, A.; Bhatt, U.S.; Gargulinski, E.; Grabinski, Z.; Jain, P.; Soja, A. Wildland Fire in High Northern Latitudes; Arctic Report Card, 2020, Thoman, R.L., Richter-Menge, J., Druckenmiller, M.L., Eds.; NOAA: Silver Spring, MD, USA, 2020. [Google Scholar] Kasischke, E.S.; Verbyla, D.L.; Rupp, T.S.; McGuire, A.D.; Murphy, K.A.; Jandt, R.; Barnes, J.L.; Hoy, E.E.; Duffy, P.A.; Calef, M.; et al. Alaska’s Changing Fire Regime—Implications for the Vulnerability of Its Boreal forests. Can. J. For. Res. 2010, 40, 1313–1324. [Google Scholar] [CrossRef] Hanes, C.C.; Wang, X.; Jain, P.; Parisien, M.-A.; Little, J.M.; Flannigan, M.D. Fire-Regime Changes in Canada over the Last Half Century. Can. J. For. Res. 2019, 49, 256–269. [Google Scholar] [CrossRef] Jones, M.W.; Abatzoglou, J.T.; Veraverbeke, S.; Andela, N.; Lasslop, G.; Forkel, M.; Smith, A.J.P.; Burton, C.; Betts, R.A.; van der Werf, G.R.; et al. Global and Regional Trends and Drivers of Fire Under Climate Change. Rev. Geophys. 2022, 60, e2020RG000726. [Google Scholar] [CrossRef] Tyukavina, A.; Potapov, P.; Hansen, M.C.; Pickens, A.H.; Stehman, S.V.; Turubanova, S.; Parker, D.; Zalles, V.; Lima, A.; Kommareddy, I.; et al. Global Trends of Forest Loss Due to Fire From 2001 to 2019. Front. Remote Sens. 2022, 3, 825190. [Google Scholar] [CrossRef] Wooster, M.J.; Roberts, G.J.; Giglio, L.; Roy, D.P.; Freeborn, P.H.; Boschetti, L.; Justice, C.; Ichoku, C.; Schroeder, W.; Davies, D.; et al. Satellite Remote Sensing of Active Fires: History and Current Status, Applications and Future Requirements. Remote Sens. Environ. 2021, 267, 112694. [Google Scholar] [CrossRef] Talucci, A.C.; Loranty, M.M.; Alexander, H.D. Siberian Taiga and Tundra Fire Regimes from 2001–2020. Environ. Res. Lett. 2022, 17, 025001. [Google Scholar] [CrossRef] Skakun, R.; Castilla, G.; Metsaranta, J.; Whitman, E.; Rodrigue, S.; Little, J.; Groenewegen, K.; Coyle, M. Extending the National Burned Area Composite Time Series of Wildfires in Canada. Remote Sens. 2022, 14, 3050. [Google Scholar] [CrossRef] Hall, R.J.; Skakun, R.S.; Metsaranta, J.M.; Landry, R.; Fraser, R.H.; Raymond, D.; Gartrell, M.; Decker, V.; Little, J.; Hall, R.J.; et al. Generating Annual Estimates of Forest Fire Disturbance in Canada: The National Burned Area Composite. Int. J. Wildland Fire 2020, 29, 878–891. [Google Scholar] [CrossRef] Kurz, W.A.; Apps, M.J. Developing Canada’s National Forest Carbon Monitoring, Accounting and Reporting System to Meet the Reporting Requirements of the Kyoto Protocol. Mitig. Adapt. Strateg. Glob. Chang. 2006, 11, 33–43. [Google Scholar] [CrossRef] Metsaranta, J.M.; Shaw, C.H.; Kurz, W.A.; Boisvenue, C.; Morken, S. Uncertainty of Inventory-Based Estimates of the Carbon Dynamics of Canada’s Managed Forest (1990–2014). Can. J. For. Res. 2017, 47, 1082–1094. [Google Scholar] [CrossRef] Tymstra, C.; Stocks, B.J.; Cai, X.; Flannigan, M.D. Wildfire Management in Canada: Review, Challenges and Opportunities. Prog. Disaster Sci. 2020, 5, 100045. [Google Scholar] [CrossRef] Hermosilla, T.; Wulder, M.A.; White, J.C.; Coops, N.C.; Hobart, G.W.; Campbell, L.B. Mass Data Processing of Time Series Landsat Imagery: Pixels to Data Products for Forest Monitoring. Int. J. Digit. Earth 2016, 9, 1035–1054. [Google Scholar] [CrossRef] Nelson, P.R.; Maguire, A.J.; Pierrat, Z.; Orcutt, E.L.; Yang, D.; Serbin, S.; Frost, G.V.; Macander, M.J.; Magney, T.S.; Thompson, D.R.; et al. Remote Sensing of Tundra Ecosystems Using High Spectral Resolution Reflectance: Opportunities and Challenges. J. Geophys. Res. Biogeosci. 2022, 127, e2021JG006697. [Google Scholar] [CrossRef] Canadian Interagency Forest Fire Centre (CIFFC). Available online: https://ciffc.ca/ (accessed on 20 October 2023). Ecological Stratification Working Group. A National Ecological Framework for Canada; Centre for Land and Biological Resources Research: Hull, QC, Canada, 1996. [Google Scholar] Walker, D.A.; Raynolds, M.K.; Daniëls, F.J.A.; Einarsson, E.; Elvebakk, A.; Gould, W.A.; Katenin, A.E.; Kholod, S.S.; Markon, C.J.; Melnikov, E.S.; et al. The Circumpolar Arctic Vegetation Map. J. Veg. Sci. 2005, 16, 267–282. [Google Scholar] [CrossRef] Natural Resources Canada 2020 Land Cover of Canada. Available online: https://open.canada.ca/data/en/dataset/ee1580ab-a23d-4f86-a09b-79763677eb47 (accessed on 16 December 2023). Gorelick, N.; Hancher, M.; Dixon, M.; Ilyushchenko, S.; Thau, D.; Moore, R. Google Earth Engine: Planetary-Scale Geospatial Analysis for Everyone. Remote Sens. Environ. 2017, 202, 18–27. [Google Scholar] [CrossRef] Pekel, J.-F.; Cottam, A.; Gorelick, N.; Belward, A.S. High-Resolution Mapping of Global Surface Water and Its Long-Term Changes. Nature 2016, 540, 418–422. [Google Scholar] [CrossRef] [PubMed] Bisong, E. Google Colaboratory. In Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners; Bisong, E., Ed.; Apress: Berkeley, CA, USA, 2019; pp. 59–64. ISBN 978-1-4842-4470-8. [Google Scholar] Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss, R.; Dubourg, V. Scikit-Learn: Machine Learning in Python. J. Mach. Learn. Res. 2011, 12, 2825–2830. [Google Scholar] R Core Team. R: A Language and Environment for Statistical Computing; R Foundation for Statistical Computing: Vienna, Austria, 2022. [Google Scholar] Aybar, C.; Qiusheng, W.; Bautista, L.; Yali, R.; Barja, A. rgee: An R package for interacting with Google Earth Engine. J. Open Source Softw. 2020, 5, 2272. [Google Scholar] [CrossRef] Hermosilla, T.; Wulder, M.A.; White, J.C.; Coops, N.C.; Hobart, G.W. An Integrated Landsat Time Series Protocol for Change Detection and Generation of Annual Gap-Free Surface Reflectance Composites. Remote Sens. Environ. 2015, 158, 220–234. [Google Scholar] [CrossRef] Healey, S.; Cohen, W.; Zhiqiang, Y.; Krankina, O. Comparison of Tasseled Cap-Based Landsat Data Structures for Use in Forest Disturbance Detection. Remote Sens. Environ. 2005, 97, 301–310. [Google Scholar] [CrossRef] Fraser, R.H.; Olthof, I.; Kokelj, S.V.; Lantz, T.C.; Lacelle, D.; Brooker, A.; Wolfe, S.; Schwarz, S. Detecting Landscape Changes in High Latitude Environments Using Landsat Trend Analysis: 1. Visualization. Remote Sens. 2014, 6, 11533–11557. [Google Scholar] [CrossRef] Loboda, T.V.; French, N.H.F.; Hight-Harf, C.; Jenkins, L.; Miller, M.E. Mapping Fire Extent and Burn Severity in Alaskan Tussock Tundra: An Analysis of the Spectral Response of Tundra Vegetation to Wildland Fire. Remote Sens. Environ. 2013, 134, 194–209. [Google Scholar] [CrossRef] Trigg, S.; Flasse, S. An Evaluation of Different Bi-Spectral Spaces for Discriminating Burned Shrub-Savannah. Int. J. Remote Sens. 2001, 22, 2641–2647. [Google Scholar] [CrossRef] French, N.H.F.; Graham, J.; Whitman, E.; Bourgeau-Chavez, L.L. Quantifying Surface Severity of the 2014 and 2015 Fires in the Great Slave Lake Area of Canada. Int. J. Wildland Fire 2020, 29, 892–906. [Google Scholar] [CrossRef] Marcos, B.; Gonçalves, J.; Alcaraz-Segura, D.; Cunha, M.; Honrado, J.P. Assessing the Resilience of Ecosystem Functioning to Wildfires Using Satellite-Derived Metrics of Post-Fire Trajectories. Remote Sens. Environ. 2023, 286, 113441. [Google Scholar] [CrossRef] Olthof, I.; Fraser, R.H. Detecting Landscape Changes in High Latitude Environments Using Landsat Trend Analysis: 2. Classification. Remote Sens. 2014, 6, 11558–11578. [Google Scholar] [CrossRef] Miller, E.A.; Jones, B.M.; Baughman, C.A.; Jandt, R.R.; Jenkins, J.L.; Yokel, D.A. Unrecorded Tundra Fires of the Arctic Slope, Alaska USA. Fire 2023, 6, 101. [Google Scholar] [CrossRef] Roteta, E.; Bastarrika, A.; Padilla, M.; Storm, T.; Chuvieco, E. Development of a Sentinel-2 Burned Area Algorithm: Generation of a Small Fire Database for Sub-Saharan Africa. Remote Sens. Environ. 2019, 222, 1–17. [Google Scholar] [CrossRef] Zhang, Z.; Wang, L.; Xue, N.; Du, Z. Spatiotemporal Analysis of Active Fires in the Arctic Region during 2001–2019 and a Fire Risk Assessment Model. Fire 2021, 4, 57. [Google Scholar] [CrossRef] Lizundia-Loiola, J.; Franquesa, M.; Khairoun, A.; Chuvieco, E. Global Burned Area Mapping from Sentinel-3 Synergy and VIIRS Active Fires. Remote Sens. Environ. 2022, 282, 113298. [Google Scholar] [CrossRef] Ramo, R.; Roteta, E.; Bistinas, I.; van Wees, D.; Bastarrika, A.; Chuvieco, E.; van der Werf, G.R. African Burned Area and Fire Carbon Emissions Are Strongly Impacted by Small Fires Undetected by Coarse Resolution Satellite Data. Proc. Natl. Acad. Sci. USA 2021, 118, e2011160118. [Google Scholar] [CrossRef] Coogan, S.C.P.; Daniels, L.D.; Boychuk, D.; Burton, P.J.; Flannigan, M.D.; Gauthier, S.; Kafka, V.; Park, J.S.; Wotton, B.M. Fifty Years of Wildland Fire Science in Canada. Can. J. For. Res. 2021, 51, 283–302. [Google Scholar] [CrossRef] Stocks, B.J.; Mason, J.A.; Todd, J.B.; Bosch, E.M.; Wotton, B.M.; Amiro, B.D.; Flannigan, M.D.; Hirsch, K.G.; Logan, K.A.; Martell, D.L.; et al. Large Forest Fires in Canada, 1959–1997. J. Geophys. Res. Atmos. 2002, 107, FFR 5-1–FFR 5-12. [Google Scholar] [CrossRef] Waigl, C.F.; Stuefer, M.; Prakash, A.; Ichoku, C. Detecting High and Low-Intensity Fires in Alaska Using VIIRS I-Band Data: An Improved Operational Approach for High Latitudes. Remote Sens. Environ. 2017, 199, 389–400. [Google Scholar] [CrossRef] Fu, Y.; Li, R.; Wang, X.; Bergeron, Y.; Valeria, O.; Chavardès, R.D.; Wang, Y.; Hu, J. Fire Detection and Fire Radiative Power in Forests and Low-Biomass Lands in Northeast Asia: MODIS versus VIIRS Fire Products. Remote Sens. 2020, 12, 2870. [Google Scholar] [CrossRef] Ju, J.; Roy, D.P. The Availability of Cloud-Free Landsat ETM+ Data over the Conterminous United States and Globally. Remote Sens. Environ. 2008, 112, 1196–1211. [Google Scholar] [CrossRef] Comiso, J.C.; Hall, D.K. Climate Trends in the Arctic as Observed from Space. WIREs Clim. Chang. 2014, 5, 389–409. [Google Scholar] [CrossRef] [PubMed] Li, F.; Zhang, X.; Kondragunta, S.; Csiszar, I. Comparison of Fire Radiative Power Estimates From VIIRS and MODIS Observations. J. Geophys. Res. Atmos. 2018, 123, 4545–4563. [Google Scholar] [CrossRef] Giglio, L.; Descloitres, J.; Justice, C.O.; Kaufman, Y.J. An Enhanced Contextual Fire Detection Algorithm for MODIS. Remote Sens. Environ. 2003, 87, 273–282. [Google Scholar] [CrossRef] Giglio, L.; Boschetti, L.; Roy, D.P.; Humber, M.L.; Justice, C.O. The Collection 6 MODIS Burned Area Mapping Algorithm and Product. Remote Sens. Environ. 2018, 217, 72–85. [Google Scholar] [CrossRef] [PubMed] NASA-FIRMS. Available online: https://firms.modaps.eosdis.nasa.gov/map/ (accessed on 23 November 2023). Environment and Climate Change Canada Lightning Density Data. Available online: https://open.canada.ca/data/en/dataset/75dfb8cb-9efc-4c15-bcb5-7562f89517ce (accessed on 18 December 2023). Qu, Y.; Miralles, D.G.; Veraverbeke, S.; Vereecken, H.; Montzka, C. Wildfire Precursors Show Complementary Predictability in Different Timescales. Nat. Commun. 2023, 14, 6829. [Google Scholar] [CrossRef] [PubMed] Rantanen, M.; Karpechko, A.Y.; Lipponen, A.; Nordling, K.; Hyvärinen, O.; Ruosteenoja, K.; Vihma, T.; Laaksonen, A. The Arctic Has Warmed Nearly Four Times Faster than the Globe since 1979. Commun. Earth Environ. 2022, 3, 168. [Google Scholar] [CrossRef] Myers-Smith, I.H.; Kerby, J.T.; Phoenix, G.K.; Bjerke, J.W.; Epstein, H.E.; Assmann, J.J.; John, C.; Andreu-Hayles, L.; Angers-Blondin, S.; Beck, P.S.A.; et al. Complexity Revealed in the Greening of the Arctic. Nat. Clim. Chang. 2020, 10, 106–117. [Google Scholar] [CrossRef] Jain, P.; Castellanos-Acuna, D.; Coogan, S.C.P.; Abatzoglou, J.T.; Flannigan, M.D. Observed Increases in Extreme Fire Weather Driven by Atmospheric Humidity and Temperature. Nat. Clim. Chang. 2022, 12, 63–70. [Google Scholar] [CrossRef] Young, A.M.; Higuera, P.E.; Abatzoglou, J.T.; Duffy, P.A.; Hu, F.S. Consequences of Climatic Thresholds for Projecting Fire Activity and Ecological Change. Glob. Ecol. Biogeogr. 2019, 28, 521–532. [Google Scholar] [CrossRef] Mack, M.C.; Bret-Harte, M.S.; Hollingsworth, T.N.; Jandt, R.R.; Schuur, E.A.G.; Shaver, G.R.; Verbyla, D.L. Carbon Loss from an Unprecedented Arctic Tundra Wildfire. Nature 2011, 475, 489–492. [Google Scholar] [CrossRef] [PubMed] Stow, D.A.; Hope, A.; McGuire, D.; Verbyla, D.; Gamon, J.; Huemmrich, F.; Houston, S.; Racine, C.; Sturm, M.; Tape, K.; et al. Remote Sensing of Vegetation and Land-Cover Change in Arctic Tundra Ecosystems. Remote Sens. Environ. 2004, 89, 281–308. [Google Scholar] [CrossRef] Chen, Y.; Romps, D.M.; Seeley, J.T.; Veraverbeke, S.; Riley, W.J.; Mekonnen, Z.A.; Randerson, J.T. Future Increases in Arctic Lightning and Fire Risk for Permafrost Carbon. Nat. Clim. Chang. 2021, 11, 404–410. [Google Scholar] [CrossRef] Bintanja, R.; van der Wiel, K.; van der Linden, E.C.; Reusen, J.; Bogerd, L.; Krikken, F.; Selten, F.M. Strong Future Increases in Arctic Precipitation Variability Linked to Poleward Moisture Transport. Sci. Adv. 2020, 6, eaax6869. [Google Scholar] [CrossRef] Yoseph, E.; Hoy, E.; Elder, C.D.; Ludwig, S.M.; Thompson, D.R.; Miller, C.E. Tundra Fire Increases the Likelihood of Methane Hotspot Formation in the Yukon–Kuskokwim Delta, Alaska, USA. Environ. Res. Lett. 2023, 18, 104042. [Google Scholar] [CrossRef] DeVries, B.; Pratihast, A.K.; Verbesselt, J.; Kooistra, L.; Herold, M. Characterizing Forest Change Using Community-Based Monitoring Data and Landsat Time Series. PLoS ONE 2016, 11, e0147121. [Google Scholar] [CrossRef] Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.  © 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite MDPI and ACS Style Hethcoat, M.G.; Jain, P.; Parisien, M.-A.; Skakun, R.; Rogic, L.; Whitman, E. Unrecorded Tundra Fires in Canada, 1986–2022. Remote Sens. 2024, 16, 230. https://doi.org/10.3390/rs16020230 AMA Style Hethcoat MG, Jain P, Parisien M-A, Skakun R, Rogic L, Whitman E. Unrecorded Tundra Fires in Canada, 1986–2022. Remote Sensing. 2024; 16(2):230. https://doi.org/10.3390/rs16020230 Chicago/Turabian Style Hethcoat, Matthew G., Piyush Jain, Marc-André Parisien, Rob Skakun, Luka Rogic, and Ellen Whitman. 2024. \"Unrecorded Tundra Fires in Canada, 1986–2022\" Remote Sensing 16, no. 2: 230. https://doi.org/10.3390/rs16020230 Note that from the first issue of 2016, this journal uses article numbers instead of page numbers. See further details here. Article Metrics Citations No citations were found for this article, but you may check on Google Scholar Article Access Statistics Article access statistics Article Views 7. Jan 17. Jan 27. Jan 6. Feb 16. Feb 26. Feb 7. Mar 17. Mar 27. Mar 0 1000 250 500 750 For more information on the journal statistics, click here. Multiple requests from the same IP address are counted as one view.   Remote Sens., EISSN 2072-4292, Published by MDPI RSS Content Alert Further Information Article Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications and newsletters from MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions Privacy Policy"

Paper 3:
- APA Citation: Werner, S., & Tai, S. (2023). A reference architecture for serverless big data processing. Future Generation Computer Systems, 155, 179-192.
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: Crew improves significantly over the baseline and, in most runs, even outperforms the Spark Operator. For all but the heavy broadcast queries, CREW and Spark performed similarly to each other, indicating that the inter-worker communication in Spark might be a bottleneck in the Experimental Environment5 deployment. We examined these observations in more detail in Fig. 5.
  Extract 2: Overall, serverless data processing proved to be highly performant for ad-hoc data processing, especially in on-prem deployments. With the right storage backends, scale and performance can be increased to fit the needs of each use-case, especially in cloud settings. The design of CREW enables highly elastic scaling, fitting the needs of each processing step to the data while allowing data analysts to control cost by setting fixed budgets.
  Limitations: The evaluation was conducted using a relatively modest experimental environment, which may limit the generalizability of the results to larger-scale deployments.

The evaluation focused on the performance characteristics and resource utilization of Crew and Spark but did not delve deeply into other aspects of the systems, such as reliability, fault tolerance, and security.
  Relevance Evaluation: Exceptionally relevant - The paper is directly focused on the use of Crew, a serverless data processing framework, and provides a thorough evaluation of its performance and resource consumption compared to Spark in the context of TPC-H benchmark queries.
  Relevance Score: 1.0
  Inline Citation: Werner, S., & Tai, S. (2023). A reference architecture for serverless big data processing. Future Generation Computer Systems, 155, 179-192.
  Explanation: The provided paper offers a detailed exploration and evaluation of Crew, a framework for serverless data processing that leverages co-design practices to address the inherent tensions between data processing and serverless platform constraints. The evaluation compares Crew against Spark, analyzing their performance characteristics and resource utilization in the context of TPC-H benchmark queries. The findings highlight Crew's competitive performance, resource efficiency, and ability to adapt to the specific needs of data processing tasks.

 Full Text: >
"Skip to main content Skip to article Journals & Books Search Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download full issue Outline Highlights Abstract Keywords 1. Introduction 2. Serverless big data processing 3. CREW – a next-generation serverless data processing system 4. Experiment-driven evaluation 5. Related work 6. Conclusion CRediT authorship contribution statement Declaration of competing interest Acknowledgments Appendix A. Systematic literature review Appendix B. Experiment setup Data availability References Vitae Show full outline Figures (7) Show 1 more figure Tables (5) Table 1 Table 2 Table A.2 Table B.3 Table B.4 Future Generation Computer Systems Volume 155, June 2024, Pages 179-192 A reference architecture for serverless big data processing☆ Author links open overlay panel Sebastian Werner, Stefan Tai Show more Share Cite https://doi.org/10.1016/j.future.2024.01.029 Get rights and content Under a Creative Commons license open access Highlights • A Reference Architecture for Serverless Big Data Processing. • Introduction and Definition of Tensions in Current Serverless Data Processing Architectures. • Implementation and Evaluation of a next-generation Serverless Data Processing System. Abstract Despite significant advances in data management systems in recent decades, the processing of big data at scale remains very challenging. While cloud computing has been well-accepted as a solution to address scalability needs, cloud configuration and operation complexity persist and often present themselves as entry barriers, especially for novice data analysts. Serverless computing and Function-as-a-Service (FaaS) platforms have been suggested to reduce such entry barriers by shifting configuration and operational responsibilities from the application developer to the FaaS platform provider. Naturally, “serverless data processing (SDP)”, that is, using FaaS for (big) data processing, has received increasing interest in recent years. However, FaaS platforms were never intended to support large data processing tasks primarily. SDP, therefore, manifests itself through workarounds and adaptations on the application level, addressing various quirks and limitations of the FaaS platforms in use for data processing needs. This, in turn, creates tensions between the platforms and the applications using them, again encouraging the constant (re-)design of both. Consequently, we present lessons learned from a series of application and platform re-designs that address these tensions, leading to the development of an SDP reference architecture and a platform instantiation and implementation thereof called CREW. Mitigating the tensions through the process of application platform co-design proves to reduce both entry barriers and costs significantly. In some experiments, CREW outperforms traditional, non-SDP big data processing frameworks by factors. Previous article in issue Next article in issue Keywords Serverless data processingApplication platform co-designServerless reference architectureFunction as a ServiceSoftware engineeringCloud computing 1. Introduction Continuously and with ever-increasing speed, applications are being built on top of software platforms. The software platforms correspondingly must support many use cases, which creates a wide range of both features but also quirks and platform-specific constraints driving application architecture and changes. Application demands, in turn, force developers to seek platforms with minimal constraints and the features best suited to address their individual needs, thus incentivizing platforms to adapt to changing demands. Alternatively, changes in application demands may suggest the development of a new (variant of a) software platform better suited to address application needs with minimal adaptation costs for applications. Managing the duality of addressing these demands is the goal of application platform co-design [1] – an approach to address the design tensions in applications driving new platform designs and platforms driving new application designs. One area of particular interest are applications that must process large quantities of data. Over the last decades, it has become increasingly simple to generate and collect data [2], be it from extensive physics experiments, from IoT, or from logging and monitoring large distributed systems. While business and research collect data at an extraordinary pace, systems to analyze this data, however, must scale to handle the increasing volume, variety, and velocity of the data [3], [4], [5]. Consequently, industry actors and researchers must continually (re-) design and expand systems to store, evaluate, and process big data at an increasing scale. Notably, developers can choose between a magnitude of different environments and components to build their data processing systems for each application domain. However, these choices usually also come with varied configuration, management and usage complexity, creating increasingly high entry barriers for the average user. Contemporary approaches to bridge this complexity gap use cloud platforms, reducing the choice to either fully managed or partially self-managed data analytics platforms. For example, a user can elect to run Apache Spark [6], [7] in a fully managed manner, e.g., using Amazon EMR [8], and thereby relies on cloud providers for creating and owning large-scale data processing clusters. However, “distributed computing remains inaccessible to a large number of users” [9], as many tuning, sizing, programming, and operating challenges remain. One promising trend in this direction is the use of serverless platforms, e.g., Function-as-a-Service (FaaS) platforms to run large-scale data processing [10], reducing barriers to entry by sharply separating operational concerns between development matters and those related to usage [9], [11], [12], [13]. Here, all scaling and infrastructure decisions are made by the cloud provider. Applications built on such a platform can quickly respond to workload changes and scale even down to zero if no work arrives. Thus, industry actors and researchers have started to utilize FaaS to process data, as “ultimate elasticity and operational simplicity [...] in the context of data analytics sounds appealing” [14]. Data processing approaches that utilize FaaS obviate management and tuning of clusters and reduce the need for resource-based sizing and scaling decisions [10], [15], [16]. We argue that existing entry barriers, e.g., high configuration complexity and cost of ownership, are addressable FaaS approaches [17], [18]. At the same time, serverless solutions can outperform traditional big data processing systems in terms of cost and performance for exploratory data analysis [19]. Still, while generic FaaS platforms offer a simplified computing model, they do not specifically address data processing needs [12], [13], [14], [20], thereby creating design tensions between data processing applications and serverless platforms to address these needs. Consequently, in this paper, we ask the question: How can we co-design serverless data processing applications and platforms to reduce entry barriers and ease existing design tensions? Toward that end, we present the following contributions: • A reference architecture for implementing serverless data processing systems • CREW – a new serverless data processing framework and serverless platform that addresses tensions using application platform co-design • A demonstration and evaluation of CREW Note, while we present CREW as a new serverless data processing framework and platform, it is not intended as a production-ready system. Instead, we use CREW to demonstrate the feasibility of the application platform co-design approach to address design tensions we observed through our reference architecture. The remainder of this paper is structured as follows: First, in Section 2, we present the serverless data processing reference architecture and currently manifested tensions. Moreover, in Section 3, we introduce CREW, a new serverless data processing framework and platform that addresses these tensions. In Section 4, we evaluate CREW compared to Apache Spark. Lastly, we discuss related work in Section 5 and conclude this work. 2. Serverless big data processing In the following, we aim to identify “a family of similar systems and to standardize nomenclature”[21] for serverless data processing (SDP) using a literature study (see Appendix A). Moreover, we present common tensions found in existing serverless data processing architectures, propose a reference architecture for SDP and pinpoint the interactions within the reference architecture that are common sources of these tensions, providing an outlook on how to address these tensions in the next generation of serverless data processing systems. 2.1. Common serverless data processing systems Serverless data processing typically involves multiple, often interchangeable, platforms and services to address different functional and non-functional goals. Besides Function-as-a-Service (FaaS) offerings, fully managed storage services and fully managed orchestrators such as AWS StepFunctions are common. The main benefit of using serverless platforms instead of analytics engines with a traditional deployment model such as Apache Flink lies in the highly elastic, event-driven resource provisioning with minimal operational overhead offered through the serverless model. Consequently, any data processing task that benefits from ad-hoc computation, as well as, distributed batch data processing [9], [12], [13], [14], [22], [23], [24], [25] shows to benefit significantly. However, this creates a large number of “moving parts” that each require attentive configuration and management, which limits the ease of use and may create different entry barriers [9]. Hence, programming frameworks and tools have emerged to automate the generation, deployment and orchestration of serverless data processing applications. In Table 1, we present an overview of the most predominant frameworks discovered in a systematic literature review conducted from 2018–2022, along with the advancements they introduced. These advances all represent workarounds or approaches to deal with the tensions between applications and platforms discussed in this section. Note that some of the frameworks are not publicly available or no longer maintained, while a small number are still in development. 2.2. A reference architecture Based on a careful review of the architectures of the discovered frameworks (Table 1), we compiled a reference architecture for serverless data processing. Fig. 1 gives an initial high-level overview of the components, the relationships and the domains of responsibility of this reference architecture. It further highlights the different platforms (blue) that must interact with application components (gray), including interactions typically handled by the supporting processing framework (green). The components within this architecture are the most commonly modified parts of the original serverless model and thus represent the defining features of serverless data processing applications. We separate serverless data processing applications into four logical planes. Firstly, on the User Plane, data analysts define a processing job using, for example, query languages [26], map-reduce APIs [35], [36], or full-fledged Spark APIs [22]. Secondly, the Control Plane is responsible for translating, packaging, and deploying the serverless execution engine and managing the execution at runtime. Thirdly, the Execution Plane is responsible for realizing the analytics logic, usually packaged as multiple small functions or containers. Lastly, the Data Plane is responsible for storing and retrieving intermediate data and raw data for the processing job,1 as well as for enabling the observability for the control plane. Table 1. Overview of serverless data processing frameworks extracted through an SLR conducted between 2018–2022 (see Appendix A). Note that some of the frameworks are not publicly available or are no longer maintained. Name Release Year Last Activity Driver Computing Storage PyWren [9] 2016 2018 ✓ ✓ gg [12] 2017 2022 ✓ ✓ ✓ Flint [22] N.A. N.A. ✓ ✓ Lambada [14] N.A. N.A. ✓ ✓ ✓ Starling [26] N.A. N.A. ✓ ✓ ✓ CRUCIAL [20] 2019 2021 ✓ ✓ ✓ Locus [23] 2016 N.A. ✓ Qubole [27] 2018 2019 ✓ ✓ ✓ Opvis [28] N.A. N.A. ✓ ✓ MARLA [29] 2017 2021 ✓ ✓ ✓ Ooso [30] 2017 2017 ✓ ✓ ✓ Wukong [24] 2019 2023 ✓ ✓ ✓ Berkley ML [31] N.A. N.A. ✓ ✓ SCAR [32] 2017 N.A. ✓ ✓ ✓ CCoDaMiC [33] N.A. N.A. ✓ ✓ Corral [34] 2018 2021 ✓ ✓ ✓ Lithops [35] 2018 2023 ✓ ✓ ✓ CREW 2020 2023 ✓ ✓ ✓ Empty Cell Download : Download high-res image (649KB) Download : Download full-size image Fig. 1. Generalized serverless data processing application reference architecture, showing how the different platforms (blue) interact with the application (gray) and the supporting processing framework (green). Including both platform- and application-side components that can influence overall processing qualities. In the following, we will briefly discuss these planes and how cross-cutting concerns within reveal tensions in the design of serverless data processing applications that framework developers must address. User plane. Within the User Plane, the data analyst is defining all the functionality, e.g., the tasks and configuration that the serverless data processing framework will use to perform the analytics job. Naturally, the way in which these functions and configurations are created is limited by the framework’s API and depends on the provided programming abstractions. Although platform details are often hidden from the data analyst, the underlying platform can still influence the means offered to define processing jobs. For example, writing code to run on AWS might need different considerations in terms of data sizes per function than writing code for OpenWhisk, as the limits of OpenWhisk can be tuned more than those for AWS. Moreover, it is up to the SDP framework to optimize and plan how the provided configurations and user defined functions (UDFs) are turned into a series of processing steps executed by serverless function runtimes. Control plane. The control plane typically contains a job driver, equivalent to a driver in traditional processing systems, responsible for translating a user-defined job description into something that can be executed. Thus, these drivers embody the core functionality of an SDP framework, translating UDFs into concrete deployment packages (functions) and providing execution plan (events) to run the job. Thus, the driver is strongly coupled to the Worker-Function, which receives the event and executes the corresponding task in accordance with the UDFs. This strong coupling also means that the driver has to observe event completion, error recovery, straggler mitigation and configuration optimizations. Consequently, the driver is a central component responsible for managing the interaction between applications and platforms and, thus, a central point of tension: T1 Tension from the mapping of application tasks (e.g., processing operations) to concrete deployment and configuration, as this is needed in a far more granular way than in other data processing systems. For example, platform-driven limitations on package sizes, runtimes, and memory can limit the possible functionality of provided code and, thus, the capability of processing tasks, which can lead to increased cold-starts [37] or increased overhead due as functions need to be split up. T2 Tension from the coarse configuring. For example, a function that is configured with too little memory can lead to faults, execution issues, or increases in management overhead, while a function with too much memory can lead to unnecessary costs. At the same time, the needed memory is often not known in advance, as it depends on the input data generated by the previous stage [38], [39]. T3 Tension from the scheduling and observation between the driver and execution platform. As, in many cases, the driver and the platform share responsibilities for scheduling the execution of the processing job, it is crucial to align both. Towards that end, the platform must provide consistent and timely feedback to the driver, and the driver should not overload the platform with status requests. Here, many workarounds are used such as the Job-Server and Workpulling-Functions [24], [28] which bypass the platforms resource optimizations, the Function-Chaining [14], [29] which add more complex fault mitigation needs [40] or approaches such as bootstraping [13] which introduced additional cost and overhead. Execution plane. The execution plane provides the processing power to perform each task that a driver generates. The goal of each serverless data processing application (SDPA) is to scale in response to the workload without the interaction of the data analyst. Naturally, the selected platform strongly influences the performance and cost [41] of an SDPA. Each platform presents unique limitations that a driver must work around [1], e.g., configuration-sensitivities [39], event generation support [1], observability [40] and acceleration [42]. Moreover, how worker functions are built also strongly influences system quality. For example, the use of pre-compiled binaries to execute specific operations leads to smaller deployment packages, which can reduce cold-starts [37], [43]. The capabilities and limitations of the serverless execution platform are thus a central point of tension for SDPA. Among them, the following tensions are of particular importance: T4 Tension from the atypical use of serverless systems for data processing, e.g., briefly running fleets of functions from a singular client.2 Which can significantly increase cold-start panelties [37], [44]. T5 Tensions from the stateless programming model, which leads to the need to store intermediate results in a separate storage system. Thus, data must always move between the storage system and the serverless platform, even for small results [45] Data plane. The data plane mainly consists of means to handle intermediate data (T5) and the bulk data that needs to be processed. For bulk data, object stores are most common [19]. Some approaches reuse the bulk data object store for intermediate results, while other approaches utilize fully managed message queues, in-memory databases, or purpose-built middleware for intermediate storage. However, all these approaches result in a large amount of data movement between the storage system and the serverless platform, leading to significant tensions: T6 Tension from the quickly shifting data access patterns (e.g., dynamic partitioning, shuffling and merging of data, and bulk access, as well as, line-wise or byte-wise access). Limiting how specialized a storage system can be for serverless data processing [46]. T7 Tension from the highly parallel nature of serverless processing. An SDPA might spawn thousands of functions that access the same object, each reading only very small parts. Thus, the storage system must be able to distribute the load adequately and in a more fine-grained way than for other use cases such as Spark [19]. T8 Tension from heavy usage of metadata queries. Differently from other applications, SDPA heavily misuses metadata queries to orchestrate the processing, e.g., to see if a series of functions is finished. Thus, the storage system must provide performant and consistent metadata to ensure smooth operation. At the same time, SDPA should limit the number of needed queries. For each of these tensions, we found manifold workarounds in the literature that aim to address platform limitations. However, we note that most of these workarounds solely address the symptoms of the tensions and not the underlying causes, namely missing serverless data processing features or storage systems features on the platform side. Features hopefully addressed in the next generation of serverless data processing. 2.3. Next-generation serverless data processing The model of event-driven scalability and other desirable properties of serverless platforms have fueled the development of many serverless data processing frameworks (see Table 1). However, a careful review of these frameworks also reveals a fundamental issue with current serverless platforms, such as AWS Lambda or Google Cloud Functions, which is that these platforms were never designed with data processing in mind. Thus, some features and behaviors of these platforms create challenges in using their full potential. Using the reference architecture (Fig. 1), we more closely identify these tensions within the boundaries between the application/platform and platform/platform. From our review of existing literature and source code, we see that so far, tensions have only been addressed through workarounds on the application level or by finding novel service compositions. However, we argue that these tensions must be addressed by both platforms and applications to realize this untapped potential. For example, bypassing the event system to invoke functions [24] to improve function utilization impacts the platform’s ability to predict workloads. Hence, it may improve performance but may impact stability. However, if the platform would offer a feature such as used in [24], these stability issues could be addressed. This dichotomy of where and how tensions are addressed also explains why some tensions, i.e., T5/T7/T8, are not discussed at all by current applications, limiting the potential and current qualities of SDP. Consequently, we argue that the next generation of serverless data processing systems must change both application architectures and serverless platforms (serverless compute platforms, storage systems and other composable services) to address these tensions. Showcasing the benefits of this application platform co-design [47] approach, we present CREW in Section 3. These re-designs also reveal potential future serverless platform features that could, with little impact on the current business model, be adapted by current serverless platforms. Moreover, we can demonstrate that these changes can lead to significant improvements in performance, cost, and reliability of serverless data processing applications and start to become competitive with traditional data processing systems like Apache Spark. However, we must point out that the concrete approach on how to best select appropriate re-designs is a complex engineering task [1], [47]. 3. CREW – a next-generation serverless data processing system In this section, we introduce and showcase CREW – Corral+Whisk: Rapid Elastic data processing with corral and OpenWhisk – A new serverless data processing system, combining both platform and application re-designs to resolve the most predominant tensions of data processing performance for ad-hoc tasks, following closely the reference architecture defined. For that, we first briefly introduce CREW, highlighting different re-design options we utilized to address the tensions. Then, we present the execution and processing steps that enable it to improve on state-of-the-art systems. We conclude this section by reviewing current shortcomings and open issues of CREW before presenting a comparative evaluation in the next section. However, we want to highlight that CREW is not aiming to replace existing, well-adopted solutions such as Lithops or Wukong but to demonstrate the potential of applying application platform co-design to serverless data processing. 3.1. Addressing tensions through re-designs Based on the reference architecture (see Fig. 1) and existing tensions in serverless data processing (see Section 2.3), we present a series of re-design options that address them through the augmentation of serverless data processing applications and serverless platforms. The presented re-designs are an excerpt of re-designs performed in the last few years by the authors and several students working on serverless data processing [47]. All re-designs were performed on the open-source serverless platform OpenWhisk [48] and the serverless data processing framework Corral [34]. For each of the following experimental re-designs, we studied several implementation options and evaluated the impact of using the re-designed systems for typical serverless workloads and data processing workloads, carefully selecting the most promising designs to present. We note that the selection of Corral as a starting point was mainly motivated by the excellent performance we observed in prior experiments [19]. Moreover, the existing architecture of Corral was elegant and simple, allowing us to quickly prototype augmentations. While evaluating all solutions presented in Table 1, we found that many were not actively maintained or had complex and historically grown architectures that would have made fast prototyping more difficult.3 Similarly, we selected OpenWhisk, as it was one of the few available and, at the time, actively maintained open-source serverless platforms that exhibited most properties of a serverless system. While other options exist, many use Kubernetes as their sole orchestrator, while OpenWhisk also supports direct runc environments and custom orchestration, image loading, and caching. Thus, it functions as a good proxy compared to AWS Lambdas backend architecture [49], which gives us the confidence that the proposed platform re-designs could also be used in commercial settings. Batching. Addressing T4 through event batching yielded one of the most significant improvements by offering serverless data processing as a better means to send events. While serverless computation systems are often designed for web-facing tasks, they rarely offer the means to push correlated events at once. However, serverless data processing must simultaneously push hundreds or thousands of events to start massively parallel processing steps. Thus causing network overhead for spawning and observing executions. A common solution for such overheads is the batching of events into fewer messages to reduce traffic. However, this can hide the complexity of workloads from the platform in case these batches are only processed on the function level and also may require additional workarounds like bootstrapping [13]. After evaluating several options, we selected a platform-re-design, where we modified the OpenWhisk Invocation-API. Here, we introduced the option to send multiple invocation events in a new batch-event message. It is still up to the developer to choose the size of each batch and the time interval between sending batches, and the platform can still impose rate-limiting and throttling logic as before. The complexity of adding this feature in OpenWhisk is relatively low, as we only need to modify a single component in the controller. Lifecyclehooks. The nature of serverless systems implies that resources are often only provisioned in response to events. Thus, the cold-start of resources has been identified as one of the critical issues for serverless applications [37]. Furthermore, resources only execute code during the processing of events, thus requiring that all computations such as code management, monitoring, and logging can only happen during the response to events [50]. Due to this design, most serverless applications perform computations that do not directly contribute to a response, impacting cost and execution times (T7, T5). Based on the observation that some of these tasks, e.g., connecting to a database or logging, do not need to happen for every event or only need to happen for very special life cycle events, e.g., starting a runtime, we implemented an extension to the serverless execution model that allows developers to react to events outside the normal event-response cycle, expanding on the ideas of Hunhoff et al. [51]. Specifically, we define additional, developer-defined code execution points that run when a runtime is starting, paused, stopped, or responding to events (see Fig. 2). Download : Download high-res image (156KB) Download : Download full-size image Fig. 2. Overview of FaaS platform life-cycle (gray) and added life-cycle-hooks (orange) and typical function execution (blue). While generally beneficial, this change enables data processing applications to preload data on runtime creation or write data after job completion instead of doing so during every event. Moreover, we could also allow developers to react to execution failures, e.g., offering the developer strategies to influence the re-execution of the task. This simple yet versatile modification of the serverless programming model offers many opportunities to the developer to move code away from the critical execution path of a serverless data processing application. However, overly long-running life-cycle hooks can increase cold starts and also reserve resources of runtime hosts longer. Function hinting. Serverless data processing also creates unique demands on computing platforms (T4, T6), specifically, when it comes to the volatile number of invocations that are needed to perform processing jobs. Here, cold starts due to an unexpected increase in invocations, which can quickly lead to long waiting times. However, the number of invocations and computing demand can be predicted relatively accurately during the execution of each processing step. Thus, we implemented hinting, which allows the driver to communicate associations between running functions and future invocations, which can be used to speed up sequences of functions. Knowing which runtimes are needed next and in what relative volume, a scheduler can extend the pre-warming concept that many platforms use without user intervention. Here, we combined part of the life cycle hook implementation and allowed drivers to send upcoming execution hints to the platform, including anticipated duration per event. The platform would then trigger the freshen life-cycle-hook to perform a function warm-up or startup, depending on the state of the requested runtimes. For this, we expanded the OpenWhisk controller and the OpenWhisk controller to be able to prepare runtimes depending on received hints. Naturally, a production system should select careful limits on the type and size of hints to balance the ability to remain highly elastic and utilize all resources most effectively. Automatic re-configuration. We discussed the uniform, one-size-fits-all model of current serverless (FaaS) deployments (T2, T3). An application/framework developer has only a limited set of options to address the sizing problem [39]. However, prior work in using samples of runtime execution for different configuration variations [39] showed promising results in quickly finding suitable runtime configurations even for large function compositions, such as they are typical in serverless data processing. Here, the sizing problem can be even more drastic for very skewed data, where, depending on the data, some functions may be lacking resources while others are done far too quickly and cause higher costs than would be needed. Thus, we expanded the approach presented in CAT [39] by allowing the sizing of not only compositions but also the sizing of the same functions based on input mapping by implementing automatic re-configuration. This strategy utilizes two modifications of typical serverless data processing systems: (1) we deploy each function for the same step in a processing job multiple times to be able to manage skewed data by only sizing functions receiving more demanding inputs, and (2) we automate the observation of sizing data to quickly select appropriate sizes of all functions in a processing composition. The first part of this strategy is implemented on the driver side by dividing and deploying jobs across multiple deployments for each run. The second part of the strategy is implemented inside OpenWhisk. Here, we make use of the fact that a serverless platform, especially for data processing workloads, deploys many runtimes for the same function. Thus, we can size a portion of deployed runtimes differently to obtain the necessary samples for the CAT-Sizer [39]. This way, the serverless processing job quickly (within a few event executions) converges to a better-fitting configuration without developer interventions. In a production setting, a cloud provider may want to carefully select the number of sampling runtimes and also offer means to define the lower and upper bounds for runtimes to ensure that the sizeres’ dynamic optimization causes no degradation in performance. Feedbackapi. One platform-driven limitation of current serverless computing platforms is the way in which drivers can observe the execution functions. Essentially, a driver can either (1) perform synchronous invocations, which is infeasible at the typical scale of serverless data processing, (2) use asynchronous invocations and observe the completion by polling the serverless platform until the completion is observed, (3) remove the need for observation by letting functions directly call other functions or lastly use platform features such as orchestrates, e.g., AWS StepFunctions. Letting functions call others directly can quickly lead to run-away computations or make debugging significantly more difficult [40]. Using the platform options may work in some cases, but it often is limited in the possible parallelism and can add significant costs. Thus, most existing frameworks use the polling method as it is the quickest and most effective option. However, polling also creates a lot of network overhead and stress on the serverless platform. To address this issue, we implemented a Feedback API that changes how the completion of invocation is communicated to applications (T1, T3). Specifically, the feedback API sends a message from the serverless system for each completed invocation (or group of invocations) using a callback address. This way, a driver can listen to the callbacks instead of implementing any polling. Moreover, allowing to group completions based on event type enables the significant reduction of network traffic between driver and platform. In a production system, we may need to enable means to abuse this system. For the tested serverless data processing applications, we could observe significant improvements, especially in combination with event batching and Hinting. Storageapi. Serverless data processing generates intermediate data that typically requires temporary storage and can be easily recreated. Since data objects are typically accessed by only a subset of functions, availability only needs to meet job requirements. The scalability of the storage system is crucial for effective data processing with parallel tasks. To balance the need for intermediate data and ingestion of raw data, many serverless data processing approaches utilize different data storage systems for raw and intermediate data [19]. However, finding a perfect match is challenging due to issues with data size, storage duration, and supported parallel reads (T6-T8). To address this challenge, we implemented a transparent StorageAPI, allowing the automatic selection of storage services based on runtime needs. The system supports Redis, NTFS, S3, and DynamoDB, enabling the driver to switch between them based on observations, such as the number and size of intermediate results generated. Future improvements could involve alternative algorithms for more backends and fine-grained decisions. Even with the simple rules implemented, significant improvements in execution speed and throughput were observed. Application aware function scheduling. In this design, we use the knowledge about the structure of a data processing application to aid task scheduling and pre-warming decisions made by a platform (T3, T4). Similar to the Hinting-Design, this strategy enables developers to communicate relationships between deployments and access patterns that can be used by a scheduler beforehand. We build this design based on a preliminary design by A. Fuerst [52], but instead of online predictions, we use two types of application information to aid the scheduling and especially the removal of runtimes in favor of others. In the first approach, we use information about the needed libraries within a deployment package to determine cold-start times. The assumption is that some libraries take significantly longer to load due to their size or functionality than others. Further, the presence of some libraries strongly correlates with initialization needs, such as connecting to databases before a runtime becomes truly operational [50]. We can also further include the use of starting and pausing life-cycle hooks in this equation, as these also indicate longer cold-start times. For the second approach, we focus on knowledge about the composition of a serverless application. By knowing how functions interact with each other, we can determine how likely a function will be called in the near future based on the last observed invocations. With this, long-running complex compositions can ensure that runtimes that are needed later are not removed prematurely. Here, we combine the hinting approach discussed before to ensure that we always prioritize runtimes that will be needed in the near future while removing runtimes that are quick to restart and also not needed by any currently running application. 3.2. Implementation Fig. 3 shows a high-level overview of CREW components, following the same reference architecture design presented in Fig. 1. CREW is divided into four distinct parts, see Fig. 3. Firstly, the core driver component, which is an extension of the Corral-Project [34]. While the basis of this code already existed, the CREW version contains about four times as many lines of code and extends the original functionality significantly, making it a fully unique software artifact. It was used as a convenient starting point but mainly served to inform early prototyping efforts. Secondly, the modified version of OpenWhisk [48] also includes significant code changes to the original4 in addition to significant changes to the runtime environments. Further, the Storage-API, a set of libraries and interfaces to interact with multiple cloud storage backends in the context of CREW without the need to know how to deploy and configure them, and the storage sidecar, a set of plugins that can dynamically be loaded and used by CREW to configure and deploy storage backends. In the following, we describe how each component is implemented, how the re-designs from the previous section are integrated into the respective components, and how the overall design enables ad-hoc serverless data processing. Download : Download high-res image (815KB) Download : Download full-size image Fig. 3. High-level component overview of CREW, highlighting all integrated augmentations in comparison to the baseline (green, bold). As a first step, a data analyst needs to implement a workflow using the map-reduce pattern, typically implemented as a single Golang file containing both the map and reduce function and the main function that initializes CREW to run the job. The job itself is driven by two things: a config file, which contains all the information to deploy all the functions and access the storage backend, and tunable values to drive the job. In the following, we describe the steps CREW performs to execute such a job on the example of TPC-H Query 6, a simple query to count items in a relation in the TPC-H benchmark. Once the main function starts, the following six steps are performed to perform the job: Validation First, CREW checks the configuration to validate that all the necessary access rights are present. In the case of our exemplary query, this includes validating the Minio credentials, the OpenWhisk credentials, and the Kubernetes credentials. Preparation Next, CREW queries the storage backend, evaluating based on the job definition which files are needed to start the job. This step informs the number of invocations needed to perform the job. Further, in this step, the StorageAPI feature is used to select an appropriate secondary storage backend. Using the meta-data information, we might also select parts of the requested data for preloading using the OnStart-LifeCycleHook feature. In the running example, that entails the discovery of the roughly 120 files needed to read the relation based on the total size. Here, a Redis database is selected with no preloading. Deployment Once the appropriate strategy for the job is selected, CREW deploys all needed components for the job. If we use a secondary storage backend, we instruct the storage sidecar to deploy that backend. In parallel, CREW compiles an executable version for the targeted serverless platform. In either case, the user-provided go file source code is compiled into a binary that can run on the target platform, including preloading operations. During compilation, we also inject all needed configurations to access the storage backends and other resources of the serverless composition. Lastly, after the successful compilation, we might also use the Hint feature to prewarm the first set of runtimes before the job is started. Furthermore, CREW might deploy more than one compiled binary to enable Automatic Re-configuration experiments during execution. In the example, this includes the deployment of Redis in the Kubernetes cluster using a Helm client. Further, based on the average file size and capability reported by the OpenWhisk platform, we anticipate 128 parallel invocations. Accordingly, we use 128 as a hint. Execution Finally, the job is executed in a stage-by-stage manner by subdividing all input files across several invocation events. The number of invocation events is determined by the used storage backed, the size of the input data, the maximum concurrency of the target platform, and the goal definition in the job config, e.g., if either speed or cost should be maximized. Once the number of invocations is determined, the invocations are sent to the target platform either using the Batching feature or using the Invocation-API of the platform. After performing all invocations in a stage, CREW observes the storage backend and serverless platform to wait for the completion of a stage. Once the stage is completed, the next stage is started. Before starting the next stage, CREW might trigger some Re-configuration, send Hints to the platform, or switch the storage backend to a different one based on the input data for the next stage. In the case of TPC-H Query 6, we performed over 300 map invocations and a single reduce invocation. During the execution, CREW reconfigured the functions to use 512 MB of memory and reduced the number of parallel invocations to 64, as the throughput of the Minio backend was limited. Moreover, it utilized a single-server Redis instance to store the intermediate data. Completion Once all stages are completed, CREW can download the final results from the storage backend or provide the analyst with a link if the results are too large to download. CREW can also be tasked to trigger a post-process step, e.g., invocating a different serverless application to generate a set of plots from the resulting data. In the running example, the final result is a single value immediately downloaded and displayed. Termination Once the job is completed and all the results are collected, CREW terminates all deployed components if not otherwise specified in the config. Thus, the deployed database, deployed functions, and intermediate data are removed. In the case of the TPC-H Query 6 example, we remove the Redis deployment and remove the deployed functions. Each step might change based on the targeted platforms, supported platform features, and observations during the job execution. However, note that CREW is built to be self-contained. Thus, all resources needed to perform a processing job are created and removed for each run, embracing the ad-hoc nature of serverless data processing. 3.3. Limitations CREW already offers performance, cost and scalability advantages for ad-hoc data processing. However, CREW uses a rather rudimentary map/reduce interface. While this is a very powerful primitive, novice developers will need ample time to understand it. A more accessible interface, e.g., a query language or a higher-level abstraction such as a SPARK API, could significantly improve the ease of use. However, using the existing primitives, a query language could easily be implemented. Moreover, error recovery could be improved, in general, and is still an open issue in CREW; on the one hand, re-trying a single failed invocation will not significantly increase the cost or processing time of a job, but recovery of multiple errors will. In its current form, about 30% of CREW’s codebase is dedicated to error recovery. With the increasing complexity of the serverless platform, this percentage will increase. Here, a more transparent way to deal with errors on the driver and platform side could significantly improve code quality and overall performance. From the platform perspective, the current implementation still uses very little information on the global use of all functions. Consequently, both scheduling and load-balancing strategies could be improved to exploit locality and reduce throughput bottlenecks. Lastly, regarding error recovery, the current platform still offers little observability and troubleshooting capabilities. We investigate the integration of tracing into OpenWhisk [40], which reveals that such a feature is easily integrated and will not significantly impact the system. Overall, most of the platform modifications presented are not specific to OpenWhisk and could be applied to other serverless platforms, such as AWS Lambda. We can only assume, based on our observations on OpenWhisk, that both the platform impacts and application benefits would be similar; however, specific cost and resource models for other platforms might hinder adoption. 4. Experiment-driven evaluation In the following, we show an evaluation of CREW using the TPC-H benchmark on a comparison of the original Corral+Openwhisk, herein referred to as Baseline, and a Spark-Operator [53] all using the same four node Kubernetes cluster and a connected high-performance minio cluster. CREW will behave as described in the previous sections and select these designs based on the job requirements. This evaluation aims to demonstrate the benefits of the application platform co-design approach in addressing performance and cost issues in serverless data processing. Hence, we compare the performance of CREW to the baseline to showcase the improvements that are possible and we compare CREW to Spark to show that serverless data processing can be competitive with more established tools on the same resources, thus, closing the gap between serverless and established tools while gaining the benefits of the serverless operation model. Here, we must point out that Spark is a far more mature tool than CREW and has been optimized for many years, but Spark-Operators is a fairly recent development and is far closer to the serverless data processing model than the original Spark implementation. In this evaluation, we focus on the execution time, throughput and cost of the queries. When an organization performs queries such as in this evaluation on a regular and predictable basis, a dedicated cluster will be more cost-effective. However, for ad-hoc queries, where resources are only needed for a short time, and for less technical data analysis, serverless data processing is far more attractive [18], [19]. Thus, for all following experiments, we always tested the performance from a fully cold cluster to target ad-hoc use. 4.1. Workload & protocol For the evaluation, we use the TPC-H benchmark [54] as a workload. TPC-H is a decision support benchmark that measures the performance of ad-hoc queries on large data sets, originally designed for relational databases. However, the benchmark is also used to evaluate other data processing systems, such as Apache Spark [53] and is also used in the serverless domain to evaluate serverless data processing systems [19], [55]. For the Baseline and CREW, we implemented the TPC-H queries in the map-reduce-paradigm, and for the Spark-Operator, we utilized the Spark-SQL library. For the evaluation, we utilize a subset of TPC-H queries (see Table B.3) with scaling factors 1 and 10. We limit the scaling factor to 10 due to the bandwidth within the experiment environment; otherwise, it can become a limiting factor, especially for the baseline. We repeat each query five times without reusing any deployed resources other than the Minio storage in between runs. The order of query execution is randomized to remove any bias. For each query, the tools had to deploy all workers in an empty Kubernetes cluster as we were interested in the ad-hoc performance. For the Spark-Operator, we must manually size, scale, and configure the operator to fit the workload. However, the Spark-Operator does not allow using fractional vCPU in Kubernetes. Thus, the available CPU resources to the Spark Operator baseline were often higher than both the Baseline and CREW experiments. Towards that end, we sized the operator to match as close as possible to the maximum resource used by CREW for the same treatment. We also calculated the cost for executing each query by assuming the AWS Lambda pricing model for each runtime instance and fixed hourly cost for the Redis/Minio instances. 4.2. Results In the flowing, we first present the results of the TPC-H queries for the selected scaling factors in terms of execution latency and throughput in Fig. 4 and Fig. 6 respectively. We then present selected results in detail, showcasing some of the differences introduced by using the different re-designs in CREW. Lastly, we present a summary table typical for the TPC-H benchmark as an extension to work presented in Werner et al. [19] in Table 2. In all results, we compare the baseline, an unmodified version of OpenWhisk and Corral, CREW and Spark, all using the same workload and hardware. Firstly, in Fig. 4, we observe that the baseline is well suited for simple queries and able to outperform the Spark-Operator in some runs, thus confirming the results of Werner et al. [19]. However, for larger data sets, the baseline performance significantly decreases, indicating that the baseline implementation suffers from a set of inefficiencies. Here, CREW improves the performance over the baseline significantly and, in most runs, even outperforms the Spark Operator. For all but the heavy broadcast queries, CREW and Spark performed similarly to each other, indicating that the inter-worker communication in Spark might be a bottleneck in the Experimental Environment5 deployment. We examined these observations in more detail in Fig. 5. In the zoomed latency figure, we can see that CREW can start executions faster than both the baseline and Spark and distribute more work across available workers. For the baseline, we can see clear steps in the execution, a sign that the worker idles either due to IO operations or slow orchestration. The spark operator uses roughly the same amount of worker6 through the execution. Although in Q1, we can see a delay between the first stage in the job and the execution across all worker nodes, this delay only happens in some runs and is unrelated to cold-start effects, as we ensured a warm execution environment for each experiment. We saw such delays of 1–20 s in about 10% of the runs, significantly impacting the overall performance of the Spark approach. For CREW, the executions started fast and remained high, and gaps appeared for write-heavy operations, where the storage backend could not cope with incoming requests. However, these gaps are overall relatively small (between 0.5–2 s). We assume that deployments on a hyperscaler, e.g., AWS, would not suffer similar issues, other than that CREW improves significantly over the baseline. Download : Download high-res image (97KB) Download : Download full-size image Fig. 4. Aggregated mean job execution time for the TPC-H application workload using Spark-Operator, CREW and the baseline for scale factors 1 and 10. Looking into the contributing factors for this improvement, we firstly see a significantly higher throughput for CREW over the baseline, see Fig. 6. Note here that the maximum throughput for the used environment is about 10 Gbps. Since we calculated the throughput based on total data read/written during the execution time, we can see that CREW is able to operate at about half of this maximum capacity and manages to max out this capacity for simple queries with little random read/write access. For longer queries, the communication traffic within Kubernetes and OpenWhisk reduces the available bandwidth. For broadcast, heavy queries with lots of read and write operations (e.g., TPC-H Query 21), the Spark-Operator is able to operate at a higher throughput than the other two approaches, as Spark can also exploit node locality to share data, skipping the network, thus, having overall improved read/write throughput. Download : Download high-res image (263KB) Download : Download full-size image Fig. 5. In-depth execution distribution view of TPC-H Query 1 and TPC-H Query 18 for the TPC-H application workload for scale factor 10 for a single run, showing initialization gaps and orchestration/scheduling delays. Note how the baseline performs most work at the end due to stragglers in prior stages and how Spark needed time to initialize the workers. However, due to the fixed worker size and available resources, Spark is not necessarily able to outperform CREW even with the higher throughput, as some tasks benefit from higher parallelism, contributing to the overall lower execution time of CREW. This higher parallelism can also be observed in Fig. 7. Here, we can see that CREW used almost the same number of tasks as Spark, but while in Spark, tasks shared a fixed number of workers, in CREW, each task runs in a different function, allowing for higher total parallelism. This performance difference is also due to the more flexible worker (function) allocation in CREW that can spawn a large number of small works at the beginning of queries and then uses fewer high-memory functions in later stages. The higher memory consumption of CREW in Table 2 for the smaller scaling factor also indicates this. However, we point out that the analyst or operator of both Spark and CREW can always configure the maximum memory that should be allocated by setting the maximum number of workers/functions and the memory per worker. Download : Download high-res image (102KB) Download : Download full-size image Fig. 6. Throughput overview for the TPC-H application workload using Spark-Operator, CREW and the serverless OpenWhisk baseline for scale factors 1 and 10, aggregated for Low, Medium, and High IO-intensive queries. We summarize the performance of Spark, CREW, and the baseline in Table 2. Here, we can first see that all three platforms are sensitive to the data volume (SF). This, however, is more an artifact of the used Exp-Env., as we could not scale the network to the storage backend. Download : Download high-res image (92KB) Download : Download full-size image Fig. 7. Runtime usage for the TPC-H application workload using Spark-Operator, CREW and the serverless OpenWhisk baseline for scale factors 1 and 10, aggregated for Low, Medium, and High IO-intensive queries, see Table B.3. Nevertheless, CREW can almost perform 1000 queries per hour for SF = 1 and 130 for SF = 10, indicating that a better-scaled storage environment could archive similar performance for SF = 10 as for SF = 1. We also see that CREW is the most expensive,7 primarily due to the extra costs of running a Redis instance. Note that at the same time, CREW never used more than two full vCPUs as each function never uses more than 40mili vCPUs for most operations, while Spark needed one vCPU per worker at a minimum + 1 vCPU for the Spark master. Thus, for On-Prem deployments, where vCPUs might be limited, using an approach such as CREW might be more beneficial as fewer CPU resources are committed. On the other hand, CREW will consume available memory more aggressively, as evident by the high memory usage for SF = 1. At the same time, Spark is very predictable as the works need to be preconfigured. However, we argue that the configuration limits still allow control of maximum memory consumption while utilizing the available resources more effectively. Table 2. Main TPC-H application workload results for Spark-Operator, CREW and OpenWhisk (Baseline) for scale factors 1 and 10, expressing theoretical queries per hour (QphH), cost per 1000 queries per Hour (cost/kQphH) and used resources for each scale factor. Platform Scale-factor QphH cost/kQphH [$]* Peak memory allocation[GB] vCPU IO [TB/h] Baseline 1 159 0.08 3 1 1.51 Crew 1 992 2.50 15 1 5.56 Spark 1 133 0.68 6 2 3.07 Baseline 10 26 3.70 6 3 4.61 Crew 10 130 6.51 32 2 16.30 Spark 10 32 5.51 32 15 14.33 4.3. Analysis The results of the evaluation show that serverless systems can be similarly performant as far more advanced platforms such as Apache Spark. The high scalability and flexible worker deployment of serverless data processing approaches, in combination with high-performance storage options available in cloud environments, makes platforms like CREW competitive. Some of the optimizations that Spark developed over time, e.g., sorting and partitioning strategies, could also be applied to CREW, likely improving the performance further. Concerning development, Spark is still far simpler, offering developers many abstraction libraries that let them use SQL or simple domain-specific languages (DSL) to define their data processing pipelines. However, deploying and operating Spark clusters, even using the simplification of Kubernetes, can quickly remove these advantages. A developer must pick the precise libraries, operator, and worker images to get a Spark operator to work, leading to much complexity. Using any additional libraries, such as the S3 connector, forced us to build a custom version of the spark operator and worker images. Further, we observed that the deployment, while eased, still required a lot of sizing-related tasks removed in the serverless data processing. Some of these issues are due to the high complexity of Sparks architecture, thus likely needing to be solved in the future. While approaches such as CREW can, over time, develop similar technical debt, the benefits of fully automated operational tasks will remain. With regards to the available programming abstractions, serverless data processing frameworks likely can, over time, adopt similar approaches as established tools such as Spark. Based on a detailed analysis of individual queries, we can see that CREW now is constrained mainly by the available bandwidth and application behavior, e.g., how the queries are written and how data is shuffled and accessed. Thus, the serverless platform itself is not the limiting factor in the execution and performance of ad-hoc data processing. Instead, application logic and design now need improvements. Overall, serverless data processing proved to be highly performant for ad-hoc data processing, especially in on-prem deployments. With the right storage backends, scale and performance can be increased to fit the needs of each use-case, especially in cloud settings. The design of CREW enables highly elastic scaling, fitting the needs of each processing step to the data while allowing data analysts to control cost by setting fixed budgets. All modifications in the serverless platform (OpenWhisk) did not affect the stability or performance of other applications (tenants). Some modifications, such as batching or feedback, introduce increased resource demand on the platform side. However, the reduced time applications run due to these changes and the reduction in polling requests compensate for these resource demands. Still, further improvements both to the application- and platform- side can further improve the performance. 5. Related work Serverless computing is still an emerging field, with many opportunities and challenges [15]. Within the context of serverless, works by Jonas et al. [10] as well as Hellerstein et al. [56] and Castro et al. [57] present a foundational understanding of these opportunities and challenges. Indeed serverless data processing is especially affected by these challenges such as the need for “Serverless Ephemeral Storage and Serverless Durable Storage” [10], as well as the challenge that serverless functions “can only communicate through an autoscaling intermediary service” [56] but also provide specific application-platform re-designs that address these challenges in CREW through the FeedbackAPI and StorageAPI. Serverless data processing is one of these opportunities, which find wide interest in research and industry [9], [13], [16], [24], [25], [27], [36], [58], [59], [60], [61]. Consequently, several excellent frameworks and tools to perform serverless data processing (See Table 1) have been proposed to enable serverless data processing. Many of them found very novel and needed workarounds to address existing challenges not only in storage or communication but also for the unique tensions that serverless platforms present. Here, novel solutions to tackle increased effects of cold-starts [13] or novel solutions of task distribution [24] already present very helpful solutions. With CREW, we benefited from these many findings and created yet another serverless compute framework; however, rather uniquely, we asked the question if some of the platform-driven limitations are necessary and should remain. Thus, resolving these workarounds by proving platform-level features to reduce cold-starts (Hinting, LifeCycleHooks) and task distribution (Batching, FeedbackAPI). Additionally, many practitioners are looking into the limitations of existing serverless platforms [37], [62], [63], [64] with the aim to improve and adapt applications to fit the serverless model. Hence, these limitations all reveal current tensions in serverless platforms that can similarly be addressed at the platform level. While we specifically focused on the task of serverless data processing and therefore addressed challenges for improved scheduling and deployment sizing in CREW for this use-case, we argue that the practice of application platform co-design can be used to address other existing limitations. Moreover, several excellent approaches exist to deal with limitations in serverless data processing applications [65], [66], [67]. With CREW we strongly benefited from these approaches to further reduce the operational tasks a data analyst is faced when using cloud-based models. Indeed, we foresee that these approaches can and should also be integrated through application platform co-design [1] into both serverless platforms not only for the benefit of serverless data processing workloads but for serverless applications in general. 6. Conclusion Platform-driven limitations and application workloads in serverless data processing create significant tensions, resulting in performance and cost impacts. This paper presents an initial review of these tensions and proposes a set of re-design options to mitigate the cost and performance issues following the application platform co-design approach [47]. By expanding engineering efforts to the serverless platform, we effectively address these tensions and introduce CREW, an innovative serverless data processing framework and computing platform tailored for ad-hoc processing [1]. Our evaluation using the TPC-H benchmark demonstrates that CREW outperforms conventional serverless data processing systems and performs comparably to Spark Operators on the same resources, validating the approach to tackle arising cloud computing complexity by also looking at the platform side. Serverless data processing offers comparable performance to established methods while significantly reducing operational tasks for developers and data analysts. Therefore, it should be considered a viable alternative for big data processing. To meet the emerging application demand, platforms should adapt and re-design their limitations accordingly. Although our focus in this paper was on ad-hoc query processing, other trends in serverless computation, such as hardware resource abstraction, may enable serverless systems to tackle different processing tasks like machine learning model training and hyperparameter optimization [42]. Similar to the evolution of database management systems (DBMS) decades ago [68], we anticipate the divergence of commercial serverless platforms from the one-size-fits-all model. This will give rise to new platform variants that cater to specific application requirements. Moreover, as more fields require distributed, scalable, cost-effective, and on-demand solutions, the trend of delegating operational, management, and execution tasks to platforms will continue to grow. Consequently, there will be a need for diversified serverless platforms to accommodate these diverse application needs. Therefore, the development of new engineering methods to create such diversified application platforms becomes a crucial next step. CRediT authorship contribution statement Sebastian Werner: Writing – review & editing, Writing – original draft, Methodology, Formal analysis, Data curation, Conceptualization. Stefan Tai: Writing – review & editing, Supervision, Resources, Project administration, Funding acquisition. Declaration of competing interest The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: Sebastian Werner reports financial support, article publishing charges, and travel were provided by TU Berlin University. Acknowledgments Funded by the European Union (TEADAL, 101070186). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. Appendix A. Systematic literature review With the aim of extracting a reference architecture for serverless data processing, we performed a systematic literature review (SLR) of serverless data processing and serverless platforms. While many systematic literature reviews for serverless computing already exists [16], [58], [69], [70], [71], [72] none focuses on serverless data processing. For the review, we excluded publications published before 2014, as serverless computing was first publicized by the release of AWS Lambda in 2014 [10], only consider English peer-reviewed publications that were at least two pages in length, exclude all works published outside of scientific journals or conferences, and we exclude all secondary studies. However, since serverless computing research remains an emerging field, we do make an exception for pre-prints if these were already accepted at a conference or journal or by authors that published other peer-reviewed work before. In total, the searches yielded 484 candidates. After de-duplication and filtering, we had a total of 323 candidates. From these, we identified a total of 92 as relevant for extraction. Within this search, we found 22 general-purpose serverless data processing frameworks and a total of 61 data processing applications. While some of these applications represent either one-time implementations or implementations of sub-problems, others are built to solve generalized data processing queries, map-reduce jobs, or generalized encoding/transcoding jobs. Further, we identified four main use cases, ranging from typical data analytics tasks such as batch and stream processing, machine learning tasks, data encoding/transcoding tasks, and operations automation, mostly built on AWS [10]. Table A.2 shows a list of highly relevant publications identified by the search. These publications can be separated into four groups of contributions in the field of serverless data processing: (1) frameworks that enable developers to utilize serverless systems, (2) use-case implementations, (3) assessment methods, and (4) contributions proposing new features to address platform problems. In terms of use cases for serverless data processing, we identified a total of 121, that can be broadly categorized into three groups, with some works presenting more than one. The majority of use-cases target: (1) batch and stream data processing tasks, consisting of scientific analysis or exploratory data analysis using map-reduce jobs, followed by (2) data encoding/transcoding tasks, such as video compression, and recently (3) machine learning tasks, such as inference workloads and even some model training. A more detailed executed literature study can be found in [47] and by reviewing the raw data in the repository.8 Table A.2. Excerpt of the search results depicting the most relevant findings. Mete-data Year Target platform Approach References Empty Cell AWS GCF MAF ICF/Whisk Other Empty Cell Jonas et al. [9] 2017 ✓ Framework Kim et al. [22] 2018 ✓ Framework Sampe et al. [13] 2018 ✓ Framework Klimovic et al. [73] 2018 ✓ Feature Lopex et al. [74] 2019 ✓ ✓ Framework Fouladi et al. [12] 2019 ✓ Framework Pons et al. [20] 2019 ✓ ✓ ✓ Feature Perez et al. [75] 2019 ✓ Feature Carver et al. [24] 2019 ✓ Framework Perez et al. [76] 2019 ✓ Framework Gimenez-Alventos et al. [29] 2019 ✓ Assessment Müller et al. [14] 2020 ✓ Framework Perron et al. [26] 2020 ✓ Framework Goli et al. [77] 2020 ✓ Assessment Sanchez et al. [78] 2020 ✓ Feature Daw et al. [79] 2020 ✓ Feature Jain et al. [80] 2020 ✓ Framework Werner et al. [19] 2020 ✓ Assessment Jarachanthan et al. [81] 2021 ✓ Framework Sampe et al. [35] 2021 ✓ ✓ ✓ ✓ Framework Appendix B. Experiment setup For the evaluation of CREW, we use the following experiment environment and adhere to the recommendations of cloud service benchmarking [82] for measurement and evaluation practices. For evaluating OpenWhisk-based deployments, we use a Kubernetes-Cluster consisting of four worker nodes, a single master, and additional VMs to deploy auxiliary services in the same 10 Gbit network. We use Kubernetes in version 1.21.0 and base all changes to OpenWhisk on the commit d741c87. For the Spark workload, we used Spark-Operator version v1beta2-1.3.7-3.1.1. Specifically, we used an on-premises experiment environment (Exp-Env.) consisting of 6 machines; a detailed overview of the setup is depicted in Table B.4. For reproduction, any similarity-sized Kubernetes cluster should be sufficient. However, it is essential to provide at least a capacity for 128 parallel function runtimes9 in OpenWhisk and at least 32 GB memory for the Minio deployment co-located in the same 10 Gbit network. Note that we needed to tune Minio to handle 10Gbit/s traffic for a sustained request traffic.10 We select a modified version of the decision support benchmark from the Transaction Processing Council for Ad-hoc/decision support (TPC-H) [83]. The ad-hoc nature of the queries fits well with the identified workloads, including mostly semi-structured data format seen as workloads during the literature review. The benchmark is well established with public comparable performance data on many different processing systems. Hence, this allows us to compare the performance of different serverless implementations with comparable processing systems such as Apache Spark. While the benchmark is targeted for Database systems, it has been used throughout research and industry to evaluate other data processing systems [19], [84], [85], [85], [86]. However, as the benchmark is originally designed to evaluate database systems, it also includes some database-specific workload elements, such as concurrent updates and modifications that are supposed to run in parallel to the processing workload to test the ability of a database to handle updates to the raw data as well as transaction management. However, these operations would mostly test the storage systems we use (Minio) instead of the serverless compute platforms and processing frameworks we are evaluating. Thus, we omit these concurrent updates from the benchmark, which is common in evaluating the performance of processing systems [19], [84], [85], [85], [86]. We implement all queries listed in Table B.3 using CREW in a program called corral-tpch.11 The implementation is largely compatible with the original version of Corral and the modified version presented, thus allowing for a comparison between an unmodified Corral version and CREW. We can trigger different design features and other CREW configurations through a config file. Table B.3. Used TPC-H queries, grouped by data volume and complexity. Empty Cell Tables Joins Groups Unions In-volume IO Out-volume Label Q01 1 1 Mid Low constant D1 Q06 1 Mid Low Q14 2 1 Mid Low Q15 1 1 Mid Low small, sparse D2 Q18 3 3 1 Mid High Q02 5 4 1 Low Mid scaling D3 Q11 3 2 1 Low Mid Q17 2 1 1 High Mid Q21 4 5 1 High High Table B.4. Cluster overview of Exp-Env. connected by a 10 Gb/s network. Name CPU Model Cores Mem [GB] Client Xeon E3-1270 v6 4 64 KMaster Xeon E5–2690 8 32 KNode1 Xeon E5–2690 8 32 Knode2 2x Xeon E5–2430 12 32 Knode4 Xeon Silver 4114 10 16 Knode5 Xeon Silver 4114 10 16 Minio Xeon Silver 4114 10 48 Data availability Links to the code and data are part of the manuscript. References [1] Werner S., Tai S. Application-Platform Co-design for Serverless Data Processing Hacid H., Kao O., Mecella M., Moha N., Paik H.y. (Eds.), Service-Oriented Computing, Springer International Publishing, Cham (2021), pp. 627-640 CrossRefView in ScopusGoogle Scholar [2] Fragkoulis M., Carbone P., Kalavri V., Katsifodimos A. A survey on the evolution of stream processing systems (2020) arXiv:2008.00842 Google Scholar [3] Berghel H.L. Simplified integration of prolog with rdbms SIGMIS Database, 16 (1985), pp. 3-12, 10.1145/2147769.2147770 View in ScopusGoogle Scholar [4] Abouzeid A., Bajda-Pawlikowski K., Abadi D., Silberschatz A., Rasin A. Hadoopdb: An architectural hybrid of mapreduce and dbms technologies for analytical workloads Proc. VLDB Endow, 2 (2009), pp. 922-933, 10.14778/1687627.1687731 View in ScopusGoogle Scholar [5] Hahmann M., Hartmann C., Kegel L., Habich D., Lehner W. Big by blocks: Modular analytics it Inf. Technol., 58 (2016), pp. 176-185, 10.1515/itit-2016-0003 View in ScopusGoogle Scholar [6] Apache Software Foundation Apache spark (2014) http://spark.apache.org/. (Online; Accessed 21 December 2022) Google Scholar [7] M. Zaharia, M. Chowdhury, M.J. Franklin, S. Shenker, I. Stoica, Spark: Cluster computing with working sets, in: 2nd USENIX Workshop on Hot Topics in Cloud Computing, HotCloud 10, 2010. Google Scholar [8] Amazon Web Services, Inc. AWS emr (2009) https://aws.amazon.com/emr/. (Online; Accessed 21 December 2022) Google Scholar [9] Jonas E., Pu Q., Venkataraman S., Stoica I., Recht B. Occupy the cloud: Distributed computing for the 99 percent Proceedings of the 2017 Symposium on Cloud Computing, ACM, New York, NY, USA (2017), pp. 445-451, 10.1145/3127479.3128601 View in ScopusGoogle Scholar [10] Jonas E., Schleier-Smith J., Sreekanti V., Tsai C.C., Khandelwal A., Pu Q., Shankar V., Carreira J., Krauth K., Yadwadkar N., Gonzales J.E., Popa R.A., Stoica I., Patterson D.A. Cloud programming simplified: A berkeley view on serverless computing (2019) URL: http://arxiv.org/abs/1812.03651 Google Scholar [11] Fox G.C., Ishakian V., Muthusamy V., Slominski A. Status of serverless computing and function-as-a-service(faas) in industry and research (2017) CoRR abs/1708.08028. URL: http://arxiv.org/abs/1708.08028 Google Scholar [12] Fouladi S., Romero F., Iter D., Li Q., Chatterjee S., Kozyrakis C., Zaharia M., Winstein K. From laptop to lambda: Outsourcing everyday jobs to thousands of transient functional containers 2019 USENIX Annual Technical Conference, USENIX, Renton, WA, USA (2019), pp. 475-488 URL: https://www.usenix.org/conference/atc19/presentation/fouladi View in ScopusGoogle Scholar [13] Sampé G., Sánchez-Artigas P. Serverless data analytics in the IBM cloud Proceedings of the 19th International Middleware Conference Industry, ACM, New York, NY, USA (2018), pp. 1-8, 10.1145/3284028.3284029 View in ScopusGoogle Scholar [14] Müller I., Marroquın R., Alonso G. Lambada: Interactive data analytics on cold data using serverless cloud infrastructure Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, ACM, New York, NY, USA (2020), pp. 115-130, 10.1145/3318464.3389758 View in ScopusGoogle Scholar [15] Kuhlenkamp J., Werner S., Tai S. The IFS and buts of less is more: A serverless computing reality check 2020 IEEE International Conference on Cloud Engineering, IC2E, IEEE (2020), pp. 154-161 CrossRefView in ScopusGoogle Scholar [16] Leitner P., Wittern E., Spillner J., Hummer W. A mixed-method empirical study of function-as-a-service software development in industrial practice J. Syst. Softw., 149 (2018), pp. 340-359, 10.1016/j.jss.2018.12.013 URL: http://www.sciencedirect.com/science/article/pii/S0164121218302735 Google Scholar [17] Markl V. Mosaics in big data: Stratosphere, apache flink, and beyond Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems, Association for Computing Machinery, New York, NY, USA (2018), pp. 7-13, 10.1145/3210284.3214344 Google Scholar [18] Werner S., Kuhlenkamp J., Klems M., Müller S. Serverless big data processing using matrix multiplication as example Proceedings of the IEEE International Conference on Big Data, IEEE, Seattle, WA, USA (2018), pp. 358-365, 10.1109/BigData.2018.8622362 View in ScopusGoogle Scholar [19] Werner S., Girke R., Kuhlenkamp J. An evaluation of serverless data processing frameworks Proceedings of the 2020 Sixth International Workshop on Serverless Computing, ACM, New York, NY, USA (2020), pp. 19-24, 10.1145/3429880.3430095 View in ScopusGoogle Scholar [20] Barcelona-Pons D., Sánchez-Artigas M., París G., Sutra P., García-López P. On the faas track: Building stateful distributed applications with serverless architectures Proceedings of the 20th International Middleware Conference, ACM, New York, NY, USA (2019), pp. 41-54, 10.1145/3361525.3361535 View in ScopusGoogle Scholar [21] Klein J. Reference Architectures for Big Data Systems Carnegie Mellon University, Software Engineering Institute’s Insights (blog) (2017) URL: https://insights.sei.cmu.edu/blog/reference-architectures-for-big-data-systems/. (Online; Accessed 23 June 2023) Google Scholar [22] Kim Y., Lin J. Serverless data analytics with flint 11th International Conference on Cloud Computing, IEEE, New York, NY, USA (2018), pp. 451-455, 10.1109/CLOUD.2018.00063 View in ScopusGoogle Scholar [23] Pu Q., Venkataraman S., Stoica I. Shuffling, fast and slow: scalable analytics on serverless infrastructure 16th USENIX Symposium on Networked Systems Design and Implementation, USENIX Association, Boston, MA, USA (2019), pp. 193-206 URL: https://www.usenix.org/conference/nsdi19/presentation/pu View in ScopusGoogle Scholar [24] Carver B., Zhang J., Wang A., Cheng Y. In search of a fast and efficient serverless dag engine 2019 IEEE/ACM Fourth International Parallel Data Systems Workshop, PDSW (2019), pp. 1-10, 10.1109/PDSW49588.2019.00005 View in ScopusGoogle Scholar [25] Sampe J., Garcia-Lopez P., Sanchez-Artigas M., Vernik G., Roca-Llaberia P., Arjona A. Toward multicloud access transparency in serverless computing IEEE Softw., 38 (2021), pp. 68-74, 10.1109/MS.2020.3029994 View in ScopusGoogle Scholar [26] Perron M., Castro Fernandez R., DeWitt D., Madden S. Starling: A scalable query engine on cloud functions Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC (2020), pp. 131-141, 10.1145/3318464.3380609 View in ScopusGoogle Scholar [27] Qubole Apache spark on AWS lambda (2017) https://www.qubole.com/blog/spark-on-aws-lambda/. (Online; Accessed 21 December 2022) Google Scholar [28] Oliveira F., Suneja S., Nadgowda S., Nagpurkar P., Isci C. Opvis: Extensible, cross-platform operational visibility and analytics for cloud Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference: Industrial Track, Association for Computing Machinery, New York, NY, USA (2017), pp. 43-49, 10.1145/3154448.3154455 View in ScopusGoogle Scholar [29] Giménez-Alventosa V., Moltó G., Caballer M. A framework and a performance assessment for serverless mapreduce on aws lambda Future Gener. Comput. Syst., 97 (2019), pp. 259-274, 10.1016/j.future.2019.02.057 URL: https://www.sciencedirect.com/science/article/pii/S0167739X18325172 View PDFView articleView in ScopusGoogle Scholar [30] Nahyl O. GitHub repository: Ooso (2017) https://github.com/d2si-oss/ooso. (Online; Accessed 21 December 2022) Google Scholar [31] J. Carreira, P. Fonseca, A. Tumanov, A. Zhang, R. Katz, A case for serverless machine learning, in: Workshop on Systems for ML and Open Source Software at NeurIPS, 2018. Google Scholar [32] Pérez A., Moltó G., Caballer M., Calatrava A. Serverless computing for container-based architectures Future Gener. Comput. Syst., 83 (2018), pp. 50-59 View PDFView articleView in ScopusGoogle Scholar [33] Dehury C.K., Srirama S.N., Chhetri T.R. Ccodamic: A framework for coherent coordination of data migration and computation platforms Future Gener. Comput. Syst., 109 (2020), pp. 1-16, 10.1016/j.future.2020.03.029 URL: https://www.sciencedirect.com/science/article/pii/S0167739X19330924 View PDFView articleView in ScopusGoogle Scholar [34] Congdon B. GitHub repository: corral (2018) http://github.com/bcongdon/corral. (Online; Accessed 26 February 2019) Google Scholar [35] Sampe J., Sanchez-Artigas M., Vernik G., Yehekzel I., Garcia-Lopez P. Outsourcing data processing jobs with lithops IEEE Trans. Cloud Comput. (2021), p. 1, 10.1109/TCC.2021.3129000 View in ScopusGoogle Scholar [36] Congdon B. Introducing Corral: A serverless MapReduce framework (2018) http://benjamincongdon.me/blog/2018/05/02/Introducing-Corral-A-Serverless-MapReduce-Framework/. (Online; Accessed 26 February 2019) Google Scholar [37] Manner J., Endreß M., Heckel T., Wirtz G. Cold start influencing factors in function as a service Proceedings of the 3rd International Workshop on Serverless Computing, IEEE, New York, NY, USA (2018), pp. 181-188, 10.1109/UCC-Companion.2018.00054 View in ScopusGoogle Scholar [38] Eismann S., Grohmann J., van Eyk E, Herbst N., Kounev S. Predicting the costs of serverless workflows ACM/SPEC International Conference on Performance Engineering, ACM, New York, NY, USA (2020), 10.1145/3358960.3379133 Google Scholar [39] Kuhlenkamp J., Werner S., Tran C.H., Tai S. Synthesizing configuration tactics for exercising hidden options in serverless systems International Conference on Advanced Information Systems Engineering, Springer (2022), pp. 36-44, 10.1007/978-3-031-07481-3_5 URL: https://arxiv.org/abs/2205.15904 View in ScopusGoogle Scholar [40] Borges M.C., Werner S., Kilic A. FaaSter Troubleshooting - Evaluating Distributed Tracing Approaches for Serverless Applications 2021 IEEE International Conference on Cloud Engineering, IC2E, IEEE (2021) Google Scholar [41] Kuhlenkamp J., Werner S., Borges M.C., E. Tal K., Tai S. An evaluation of FAAS platforms as a foundation for serverless big data processing Conference on Utility and Cloud Computing, ACM, New York, NY, USA (2019), pp. 1-9, 10.1145/3344341.3368796 View in ScopusGoogle Scholar [42] Werner S., Schirmer T. Hardless: A Generalized Serverless Compute Architecture for Hardware Processing Accelerators 2022 IEEE International Conference on Cloud Engineering, IC2E (2022), pp. 79-84, 10.1109/IC2E55432.2022.00016 View in ScopusGoogle Scholar [43] Bermbach D., Karakaya A.S., Buchholz S. Using application knowledge to reduce cold starts in faas services 35th ACM/SIGAPP Symposium on Applied Computing, ACM, New York, NY, USA (2020), 10.1145/3341105.3373909 Google Scholar [44] Kuhlenkamp J., Werner S., Borges M.C., Ernst D. All but One: Faas Platform Elasticity Revisited SIGAPP Appl. Comput. Rev, 20 (2020), pp. 5-19, 10.1145/3429204.3429205 Google Scholar [45] Schirmer T., Scheuner J., Pfandzelter T., Bermbach D. Fusionize: Improving serverless application performance through feedback-driven function fusion (2022) arXiv:2204.11533 Google Scholar [46] Sampé J., Sánchez-Artigas M., García-López P., París G. Data-driven serverless functions for object storage Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference, ACM, New York, NY, USA (2017), pp. 121-133, 10.1145/3135974.3135980 View in ScopusGoogle Scholar [47] Werner S. Serverless Data Processing: Application Platfrom Co-Design (Ph.D. thesis) TU Berlin (2023) Google Scholar [48] Apache Software Foundation Apache openwhisk (2018) http://openwhisk.incubator.apache.org. (Online; Accessed 15 October 2017) Google Scholar [49] Agache A., Brooker M., Iordache A., Liguori A., Neugebauer R., Piwonka P., Popa D.M. Firecracker: Lightweight virtualization for serverless applications 17th USENIX Symposium on Networked Systems Design and Implementation, USENIX Association, Santa Clara, CA (2020), pp. 419-434 URL: https://www.usenix.org/conference/nsdi20/presentation/agache View in ScopusGoogle Scholar [50] Werner S., Kuhlenkamp J., Pallas F., Anders N., Mucaj N., Tsaplina O., Schmidt C., Yildirim K. Diminuendo! Tactics in support of faas migrations Paasivaara M., Kruchten P. (Eds.), Agile Processes in Software Engineering and Extreme Programming – Workshops, Springer International Publishing (2020), pp. 125-132, 10.1007/978-3-030-58858-8_13 View in ScopusGoogle Scholar [51] Hunhoff E., Irshad S., Thurimella V., Tariq A., Rozner E. Proactive serverless function resource management Proceedings of the 2020 Sixth International Workshop on Serverless Computing, Association for Computing Machinery, New York, NY, USA (2020), pp. 61-66, 10.1145/3429880.3430102 View in ScopusGoogle Scholar [52] Fuerst A., Sharma P. Faascache: Keeping serverless computing alive with greedy-dual caching Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Association for Computing Machinery, New York, NY, USA (2021), pp. 386-400, 10.1145/3445814.3446757 View in ScopusGoogle Scholar [53] Foundation A.S. Apache spark on kubernetes (2021) https://spark.apache.org/docs/latest/running-on-kubernetes.html. (Online; Accessed 21 December 2022) Google Scholar [54] Counci T.P.P. TPC benchmark H standard specification revision 3.0.1 (2022) https://www.tpc.org/tpc_documents_current_versions/pdf/tpc-h_v3.0.1.pdf. (Online; Accessed 21 December 2022) Google Scholar [55] Spillner J. Transformation of python applications into function-as-a-service deployments (2017) URL: http://arxiv.org/abs/1705.08169. unpublished Google Scholar [56] Hellerstein J.M., Faleiro J.M., Gonzalez J.E., Schleier-Smith V., Tumanov A., Wu C. Serverless computing: One step forward, two steps back CIDR 2019, 9th Biennial Conference on Innovative Data Systems Research, CIDR (2019) URL: http://cidrdb.org/cidr2019/papers/p119-hellerstein-cidr19.pdf Google Scholar [57] Castro P., Ishakian V., Muthusamy V., Slominski A. The rise of serverless computing Commun. ACM, 62 (2019), pp. 44-54, 10.1145/3368454 View in ScopusGoogle Scholar [58] J. Kuhlenkamp, Werner S. Benchmarking FaaS Platforms: Call for Community Participation Proceedings of the 3rd International Workshop on Serverless Computing, IEEE, Z̃rich, Switzerland (2018), pp. 189-194, 10.1109/UCC-Companion.2018.00055 Google Scholar [59] Yussupov V., Breitenbücher U., Leymann F., Wurster M. A systematic mapping study on engineering function-as-a-service platforms and tools Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing, ACM, New York, NY, USA (2019), pp. 229-240, 10.1145/3344341.3368803 View in ScopusGoogle Scholar [60] D. Taibi, N. E. Ioini, C. Pahl, J.R.S. Niederkofler, Patterns for serverless functions (function-as-a-service): A multivocal literature review, in: Proceedings of the 10th International Conference on Cloud Computing and Services Science, CLOSER, Research Gate. Preprint, 2020. Google Scholar [61] Scheuner J., Leitner P. Function-as-a-service performance evaluation: A multivocal literature review J. Syst. Softw. (2020), Article 110708, 10.1016/j.jss.2020.110708 URL: http://www.sciencedirect.com/science/article/pii/S0164121220301527 View PDFView articleView in ScopusGoogle Scholar [62] Grambow M., Pfandzelter T., Burchard L., Schubert C., Zhao M., Bermbach D. Befaas: An application-centric benchmarking framework for faas platforms 2021 IEEE International Conference on Cloud Engineering, IC2E, IEEE (2021), pp. 1-8 CrossRefGoogle Scholar [63] Jackson D., Clynch G. An investigation of the impact of language runtime on the performance and cost of serverless functions Proceedings of the 3rd International Workshop on Serverless Computing, IEEE, New York, NY, USA (2018), pp. 154-160, 10.1109/UCC-Companion.2018.00050 URL: http://ieeexplore.ieee.org/abstract/document/8605773 View in ScopusGoogle Scholar [64] van Eyk E., Scheuner J., Eismann S., Abad C.L., Iosup A. Beyond microbenchmarks: The spec-rg vision for a comprehensive serverless benchmark 2020 ACM/SPEC International Conference on Performance Engineering, ACM, New York, NY, USA (2020), pp. 197-208, 10.1145/3375555.3384381 Google Scholar [65] P.G. López, Sánchez-Artigas M., Parıs G., Pons D.B., Ollobarren A.R., Pinto D.A. Comparison of faas orchestration systems Proceedings of the 3rd International Workshop on Serverless Computing, IEEE, New York, NY, USA (2018), pp. 148-153, 10.1109/UCC-Companion.2018.00049 Google Scholar [66] Gupta V., Carrano D., Yang Y., Shankar V., Courtade T., Ramchandran K. Serverless straggler mitigation using local error-correcting codes (2020) arXiv:2001.07490 Google Scholar [67] Eismann S., Bui L., Grohmann J., Abad C.L., Herbst N., Kounev S. Sizeless: Predicting the optimal size of serverless functions (2021) Preprint Google Scholar [68] Stonebraker M., Cetintemel U. “One size fits all”: An idea whose time has come and gone 21st International Conference on Data Engineering, ICDE’05 (2005), pp. 2-11, 10.1109/ICDE.2005.1 View in ScopusGoogle Scholar [69] Eismann S., Scheuner J., Van Eyk E., Schwinger M., Grohmann J., Herbst N., Abad C., Iosup A. The state of serverless applications: Collection, characterization, and community consensus IEEE Trans. Softw. Eng. (2021), p. 1, 10.1109/TSE.2021.3113940 Google Scholar [70] Hassan H.B., Barakat S.A., Sarhan Q.I. Survey on serverless computing J. Cloud Comput., 10 (2021), pp. 1-29 CrossRefGoogle Scholar [71] Yussupov V., Breitenbücher U., Leymann F., Wurster M. A systematic mapping study on engineering function-as-a-service platforms and tools 12th IEEE/ACM International Conference on Utility and Cloud Computing, ACM, New York, NY, USA (2019), pp. 229-240, 10.1145/3344341.3368803 View in ScopusGoogle Scholar [72] Shafiei H., Khonsari A., Mousavi P. Serverless computing: A survey of opportunities, challenges and applications (2019) arXiv preprint arXiv:1911.01296 Google Scholar [73] Klimovic A., Wang Y., Stuedi P., Trivedi A., Pfefferle J., Kozyrakis C. Pocket: Elastic ephemeral storage for serverless analytics 12th USENIX Conference on Operating Systems Design and Implementation, USENIX Association, Berkeley, CA, USA (2018), pp. 427-444 URL: http://dl.acm.org/citation.cfm?id=3291168.3291200 Google Scholar [74] Garćıa-López P., Sańchez-Artigas M., Shillaker S., Pietzuch P., Breitgand D., Vernik G., Sutra P., Tarrant T., Ferrer A.J. Servermix: Tradeoffs and challenges of serverless data analytics (2019) arXiv:1907.11465 Google Scholar [75] Pérez A., Moltó G., Caballer M., Calatrava A. A programming model and middleware for high throughput serverless computing applications Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, Association for Computing Machinery, New York, NY, USA (2019), pp. 106-113, 10.1145/3297280.3297292 View in ScopusGoogle Scholar [76] Pérez A., Risco S., Naranjo D.M., Caballer M., Moltó G. On-premises serverless computing for event-driven data processing applications 2019 IEEE 12th International Conference on Cloud Computing, CLOUD (2019), pp. 414-421, 10.1109/CLOUD.2019.00073 View in ScopusGoogle Scholar [77] Goli A., Hajihassani O., Khazaei H., Ardakanian O., Rashidi M., Dauphinee T. Migrating from monolithic to serverless: A fintech case study Companion of the ACM/SPEC International Conference on Performance Engineering, ACM, New York, NY, USA (2020), p. 2025, 10.1145/3375555.3384380 Google Scholar [78] Sánchez-Artigas M. Eizaguirre G.T., Vernik G., Stuart L., García-López P. Primula: A practical shuffle/sort operator for serverless computing Proceedings of the 21st International Middleware Conference Industrial Track, Association for Computing Machinery, New York, NY, USA (2020), pp. 31-37, 10.1145/3429357.3430522 Google Scholar [79] Daw N., Bellur U., Kulkarni P. Xanadu: Mitigating cascading cold starts in serverless function chain deployments Proceedings of the 21st International Middleware Conference, Association for Computing Machinery, New York, NY, USA (2020), pp. 356-370, 10.1145/3423211.3425690 View in ScopusGoogle Scholar [80] Jain A., Baarzi A.F., Kesidis G., Urgaonkar B., Alfares N., Kandemir M. Splitserve: Efficiently splitting apache spark jobs across FAAS and IAAS Proceedings of the 21st International Middleware Conference, Association for Computing Machinery, New York, NY, USA (2020), pp. 236-250, 10.1145/3423211.3425695 View in ScopusGoogle Scholar [81] Jarachanthan J., Chen L., Xu F., Li B. Astra: Autonomous serverless analytics with cost-efficiency and QOS-awareness 2021 IEEE International Parallel and Distributed Processing Symposium, IPDPS (2021), pp. 756-765, 10.1109/IPDPS49936.2021.00085 View in ScopusGoogle Scholar [82] Bermbach D., Wittern E., Tai S. Cloud Service Benchmarking: Measuring Quality of Cloud Services from a Client Perspective Springer International Publishing, Cham (2017) Google Scholar [83] Poess M., Floyd C. New tpc benchmarks for decision support and web commerce SIGMOD Rec, 29 (2000), pp. 64-71, 10.1145/369275.369291 View in ScopusGoogle Scholar [84] M. Wawrzoniak, R. Müller, G. Alonso, Boxer: Data analytics on network-enabled serverless platforms, in: 11th Annual Conference on Innovative Data Systems Research, CIDR 2021, 2021. Google Scholar [85] D. Justen, Cost-efficiency and performance robustness in serverless data exchange, in: Proceedings of the 2022 International Conference on Management of Data, 2022, pp. 2506–2508. Google Scholar [86] T. Bodner, T. Pietz, L.J. Bollmeier, D. Ritter, Doppler: Understanding serverless query execution, in: Proceedings of the International Workshop on Big Data in Emergent Distributed Environments, 2022, pp. 1–4. Google Scholar Cited by (0) Sebastian Werner is a senior researcher at the Information Systems Engineering chair at TU Berlin, Germany. He completed his Ph.D. in Computer Science in 2023 in the area of serverless technology and the design of distributed cloud-based applications. Sebastian has long experience working on European and national projects and is currently a work package leader in Horizon Europe Project TEADAL, focusing on accountability and trustworthiness in federated data lakes. His current research focuses on platform-based applications’ sustainability, maintainability, and trustworthiness. Sebastian is passionate about addressing the challenges of the next generation of cloud-computing platforms in his research and teaching. Stefan Tai is Full Professor and Head of Chair Information Systems Engineering at TU Berlin, Germany (2014-present). Prior to that, he was a Full Professor at the Karlsruhe Institute of Technology (2007–2014) and a Research Staff Member at the IBM Thomas J.Watson Research Center in New York, USA (1999–2007). He also held concurrent posts as Director of research labs and is a member of corporate supervisory and advisory boards. He earned his Ph.D. in Computer Science from TU Berlin in 1999. Stefan’s research focuses on next-generation, platform-based distributed software systems that meet complex system qualities, providing for technology, social, and business innovation. Platforms of interest include cloud platforms and blockchain networks, and their meaningful combination and interplay. ☆ This document is in part published as Werner (2023). 1 Usually in the form of managed storage service, such as S3. 2 A different usage as than serving web requests, which may lead to increased and even unforeseen overhead on the platform side. 3 Admittedly, CREW now also is one of those complex systems that would be difficult to augment. 4 additions in the codebase (excluding configurations and build files) 5 See Appendix B. 6 For Spark, we considered each thread as a separate worker, even if multiple workers run in parallel on the same deployed worker node. 7 We calculated the prices using the AWS Lambda for all function executions and prices for m6a.4xlarge instances for the Minio/Redis instances as well as for the spark worker instances. Based on prices from April 2022.. 8 https://docs.google.com/spreadsheets/d/1ZIP712dKKmDOnd4oZ77pMg2YiVfn7-P2Q9ESEddTt0U. 9 Each runtime needs, on average, 512 MB of memory. Thus, around 64 GB of free runtime memory after deploying OpenWhisk. 10 We observed that setting Minio to allow up to 100000 requests instead of the memory-based configuration works well in our experiments. 11 https://github.com/ISE-SMILE/corral-tpc-h . © 2024 The Author(s). Published by Elsevier B.V. Part of special issue Serverless computing for next-generation application development Edited by Adel N. Toosi, Alexandru Iosup, Bahman Javadi, Evgenia Smirni, Schahram Dustdar View special issue Recommended articles HyperFlow: A model of computation, programming approach and enactment engine for complex distributed workflows Future Generation Computer Systems, Volume 55, 2016, pp. 147-162 Bartosz Balis View PDF TLR: Traffic-aware load-balanced routing for industrial IoT Internet of Things, Volume 25, 2024, Article 101093 Abdeldjalil Tabouche, …, Abdelmalek Ghefrane Elaziz View PDF A fine-grained robust performance diagnosis framework for run-time cloud applications Future Generation Computer Systems, Volume 155, 2024, pp. 300-311 Ruyue Xin, …, Zhiming Zhao View PDF Show 3 more articles Article Metrics Captures Readers: 1 View details About ScienceDirect Remote access Shopping cart Advertise Contact and support Terms and conditions Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply."

Paper 4:
- APA Citation: Vargas-Rojas, L., Ting, T.-C., Rainey, K. M., Reynolds, M., & Wang, D. R. (2024). AgTC and AgETL: open-source tools to enhance data collection and management for plant science research. Frontiers in Plant Science, 15, 1265073. Retrieved from https://www.frontiersin.org/articles/10.3389/fpls.2024.1265073
  Main Objective: To investigate the use of containerization technologies for the efficient deployment and scaling of data processing and machine learning modules in cloud environments for real-time irrigation management.
  Study Location: Unspecified
  Data Sources: Large-scale real-world dataset
  Technologies Used: Docker, Kubernetes
  Key Findings: Containerization technologies (Docker, Kubernetes) can be effectively leveraged to facilitate the efficient deployment and scaling of data processing and machine learning modules in cloud environments for real-time irrigation management.
  Extract 1: "Containerization has emerged as a viable solution for deploying and scaling distributed systems in cloud environments. Docker and Kubernetes are two widely adopted containerization technologies that provide features such as isolation, resource management, and scalability. In this paper, we present an approach to leverage these technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments."
  Extract 2: "We also evaluate the scalability of our approach using a real-world dataset and demonstrate that it can effectively handle large-scale data processing and machine learning tasks in a scalable and efficient manner."
  Limitations: The study does not provide an in-depth analysis of the potential challenges and limitations associated with implementing containerization technologies for real-time irrigation management systems, and it does not consider the specific requirements and constraints of agricultural scenarios.
  Relevance Evaluation: The paper discusses the importance of leveraging containerization technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments, which is highly relevant to the outline point regarding scalable and autonomous deployment using containerization strategies.
  Relevance Score: 1.0
  Inline Citation: (Vargas-Rojas et al., 2024)
  Explanation: The study explores the use of containerization technologies (Docker, Kubernetes) to facilitate the efficient deployment and scaling of data processing and machine learning modules within cloud environments for real-time irrigation management.

 Full Text: >
"Top bar navigation About us All journals All articles Submit your research Search Login Frontiers in Plant Science Sections Articles Research Topics Editorial Board About journal Download article 545 Total views 135 Downloads View article impact View altmetric score Share on Edited by Jennifer Clarke University of Nebraska-Lincoln, United States Reviewed by Geng(Frank) Bai Department of Biological Systems Engineering, University of Nebraska - Lincoln, United States Alan Cleary National Center for Genome Resources, United States Table of contents Abstract 1 Introduction 2 Methods 3 Results and discussion 4 Conclusion Data availability statement Author contributions Funding Acknowledgments Conflict of interest Publisher’s note Supplementary material References Supplemental data Export citation Check for updates People also looked at Physiological responses and transcriptomic analysis of StCPD gene overexpression in potato under salt stresses Xiangyan Zhou, Yanming Ma, Rong Miao, Caijuan Li, Ziliang Liu, Dan Zhang, Sijin Chen, Jiaqi Luo and Wenhui Tang Advances in Quercus ilex L. breeding: the CRISPR/Cas9 technology via ribonucleoproteins Vera Pavese, Andrea Moglia, Anna Maria Milani, Lorenzo Antonio Marino, Maria Teresa Martinez, Daniela Torello Marinoni, Roberto Botta and Elena Corredoira Quantifying physiological trait variation with automated hyperspectral imaging in rice To-Chia Ting, Augusto C. M. Souza, Rachel K. Imel, Carmela R. Guadagno, Chris Hoagland, Yang Yang and Diane R. Wang SpaTemHTP: A Data Analysis Pipeline for Efficient Processing and Utilization of Temporal High-Throughput Phenotyping Data Soumyashree Kar, Vincent Garin, Jana Kholová, Vincent Vadez, Surya S. Durbha, Ryokei Tanaka, Hiroyoshi Iwata, Milan O. Urban and J. Adinarayana Ontologies for increasing the FAIRness of plant research data Kathryn Dumschott, Hannah Dörpholz, Marie-Angélique Laporte, Dominik Brilhaus, Andrea Schrader, Björn Usadel, Steffen Neumann, Elizabeth Arnaud and Angela Kranz TECHNOLOGY AND CODE article Front. Plant Sci., 20 February 2024 Sec. Technical Advances in Plant Science Volume 15 - 2024 | https://doi.org/10.3389/fpls.2024.1265073 This article is part of the Research Topic Women in Plant Science - Linking Genome to Phenome View all 5 articles AgTC and AgETL: open-source tools to enhance data collection and management for plant science research Luis Vargas-Rojas1To-Chia Ting1Katherine M. Rainey1Matthew Reynolds2Diane R. Wang1* 1Department of Agronomy, Purdue University, West Lafayette, IN, United States 2Wheat Physiology Group, International Maize and Wheat Improvement Center (CIMMYT), Texcoco, Mexico Advancements in phenotyping technology have enabled plant science researchers to gather large volumes of information from their experiments, especially those that evaluate multiple genotypes. To fully leverage these complex and often heterogeneous data sets (i.e. those that differ in format and structure), scientists must invest considerable time in data processing, and data management has emerged as a considerable barrier for downstream application. Here, we propose a pipeline to enhance data collection, processing, and management from plant science studies comprising of two newly developed open-source programs. The first, called AgTC, is a series of programming functions that generates comma-separated values file templates to collect data in a standard format using either a lab-based computer or a mobile device. The second series of functions, AgETL, executes steps for an Extract-Transform-Load (ETL) data integration process where data are extracted from heterogeneously formatted files, transformed to meet standard criteria, and loaded into a database. There, data are stored and can be accessed for data analysis-related processes, including dynamic data visualization through web-based tools. Both AgTC and AgETL are flexible for application across plant science experiments without programming knowledge on the part of the domain scientist, and their functions are executed on Jupyter Notebook, a browser-based interactive development environment. Additionally, all parameters are easily customized from central configuration files written in the human-readable YAML format. Using three experiments from research laboratories in university and non-government organization (NGO) settings as test cases, we demonstrate the utility of AgTC and AgETL to streamline critical steps from data collection to analysis in the plant sciences. 1 Introduction As the cost of genotyping continues to decrease, acquiring and managing data associated with plant phenotypes and environmental conditions have emerged as considerable limiting factors in plant science research. In response, technological advancements in data acquisition have been able to greatly increase the volume of data that researchers are able to collect from experiments (Eitzinger, 2021; Machwitz et al., 2021). Despite improvement in increasing the throughput of measurements, new instrumentation has not entirely replaced traditional methods; rather, they are often used to complement the repertoire of conventional methodologies employed by research groups, especially for experiments carried out under field conditions (Coppens et al., 2017; Crain et al., 2022). For instance, standard methods used at the International Maize and Wheat Improvement Center (CIMMYT) for plant phenotyping in their applied crop research programs include all of the following: traditional observation-based methods, high-throughput and low-cost phenotyping tools, and highly specialized equipment (Reynolds et al., 2020). The situation is similar for university-based research labs, where new instruments and techniques are continuously being tested and adopted, but complementary ground-reference measurements are still retained (e.g., Ting et al., 2023). Given the diversity of measurements made by plant science research groups, labs currently experience several challenges related to the collection, processing, and management of data (e.g., protocols presented in Pask et al., 2012). First, many kinds of measurements are still recorded on paper. This is true not only for those collected by hand but also for measurements collected using electronic instruments that have limited memory, i.e. only storing a small number of observations. For example, the chlorophyll meter SPAD-502plus (Konica Minolta; Osaka, Japan) can save just 30 measurements in memory; for this reason, researchers still commonly record these data on paper (Mullan and Mullan, 2012). Newer versions of devices can sometimes enable greater data storage (e.g., the Chlorophyll Meter SPAD 502DL Plus with Data Logger can store up to 4,096 measurements). However, researchers often only have access to the older versions due to budget constraints that limit the upgrading of still-functional equipment. The second challenge concerns the heterogeneous nature of data files (i.e. those differing in format and structure), as measurements commonly collected in plant science research can originate from different instruments or methods. This creates issues in efficient data integration and management (Neveu et al., 2019). The final challenge lies in the storage and management of research data after they are integrated, which commonly rely on spreadsheet files on personal computers (Elsayed and Saleh, 2018) or with file storage cloud services using non-standard naming conventions and nested directories. This creates potential issues for sharing data in standardized ways with version control. Overall, these observations likely indicate that the data landscape for experiments carried out in plant science research domains has become increasingly complex. Improving the data pipeline from collection and processing to storage and management would help enhance data interpretation to ultimately enable new discoveries. A data pipeline is a sequence of processes that begins with collection and includes extraction, transformation, aggregation, and validation, and is complete when data are loaded into a database for eventual analysis (Munappy et al., 2020). Even though data pipelines are designed to enhance research productivity, their successful implementation is often hindered by infrastructural and organizational challenges. Munappy et al. (2020) speculated that several human aspects underlie impediments to the adoption of these pipelines, including resistance to change, and development complexity. In plant science research, numerous commercial and open-source tools for improving various steps in the data pipeline have been developed. For example, software applications have been made available for phenotyping in breeding programs to improve data collection in the field. These include Field Book, an open-source Android application that enables direct data entry with a user-friendly interface using experimental information loaded by users via files known as field files (Rife and Poland, 2014); Phenobook, an open-source web application for collaborative research (Crescente et al., 2017); and AgroFIMS, an open-source web tool that was initially developed as an analytics platform for breeding, whose current version has been expanded for data collection (Devare et al., 2021). The Integrated Breeding Platform, another example, is a commercial data management service for plant breeding programs that provides software, support, and services for breeding data pipelines (Malosetti et al., 2016). Breedbase is a web-based application that allows management of phenotyping data, stores genotypic information, and can perform analyses related to genomic prediction (Morales et al., 2022). Enterprise Breeding System is an open-source software for breeding programs that enables management of germplasm trials and nurseries as well as data management and analysis (CGIAR Excellence in Breeding Platform, 2022). More recently, PhytoOracle was released to provide a suite of tools that integrates open-source distributed computing frameworks for processing lettuce and sorghum phenotypic traits from RGB, thermal, PSII chlorophyll fluorescence, and 3D laser scanner datasets (Gonzalez et al., 2023). For a comprehensive recent review of digital tools developed for field-based plant data collection and management, we refer the reader to Dipta et al. (2023). Despite the repertoire of software described, current tools have several potential barriers to adoption in the broader plant science research community: they may be (1) commercial platforms; (2) open source or freely available but specialized for breeding application; (3) freely available but indicate that specialized IT knowledge is required; or (4) advertised as freely available but not actually available upon investigation. To address these gaps, we describe the development of two generic tools, called AgTC and AgETL, to enhance data collection and management in plant science research (Figure 1). These are alternatives for research groups that may not have the budget for a licensed plant-research database software platform or may require additional specialized IT knowledge to implement available free options. AgTC and AgETL also address the need for data collection and management tools to enhance data pipelines for plant science experiments that have objectives different from those of plant breeding programs (for which many tools are already available). For instance, they can be used in experiments where physiological traits or environmental factors are sampled at different time points with either traditional or modern techniques. The new AgETL tool is also amenable to help standardize data tables resulting from phenomics pipelines for final storage. Importantly, both AgTC and AgETL were designed based on extensive first-hand experiences of the primary tool developer as a field-based researcher who led data collection campaigns. Together, the two tools aim to reduce the time consumed on data processing and improve data storage to make downstream data analysis more efficient and accessible. While both tools work independently, they have a similar structure consisting of (1) an ipynb file executed on Jupyter Notebook, (2) function files containing Python functions that execute the steps for each tool, and (3) configuration files that contain user-specified parameters to run the functions; these are written in YAML, a data serialization language whose human-readable format functions correctly with Python (Ben-Kiki et al., 2021). Here, we demonstrate the utility of AgTC and AgETL in ongoing experiments on soybean, rice, and wheat carried out at Purdue University and at CIMMYT. figure 1 Figure 1 AgTC and AgETL can support plant science research from data collection to analysis. (A) Elements in the white background represent a typical series of steps taken in field- and controlled environment-based plant science research and (B) elements with grey background show how the processes of AgTC and AgETL can fit into within this overall framework. The proposed steps are numbered as follows: (1) Comma-separated value (CSV) template files for collecting data are created and uploaded to a computer or mobile device, where (2a) data are entered under experimental settings. (2b) AgTC-derived templates are not needed when data are collected using instruments that contain their data storage systems. Data that have been collected using the template from AgTC (3a) or downloaded from scientific instruments (3b) are next extracted using AgETL Extract functions and transformed using AgETL’s Transform process into a standard format in a single CSV file (4). (5a) Data quality assurance/quality control (QA/QC) processes are carried out outside the AgETL pipeline. (5b) When QA/QC is complete, data are ready to be loaded into a PostgreSQL database using the Load process of AgETL. CSV files containing clean data can be used directly for analysis (6a) or analyzed after it is downloaded from the database (6b). 2 Methods In this section, we describe the objectives and structure of the AgTC and AgETL Python functions along with implementation details and options for deployment. Experimental details from three test cases where data were collected using AgTC-generated templates are provided. Definitions for key terms mentioned throughout are first outlined below. Database: A structured data collection stored and accessed electronically by a database management system (DBMS) such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server, or Oracle Database. One of the characteristics of a DBMS is that it maintains data integrity; for instance, it does not permit storing different types of data in the same field or column or storing duplicated records based on the primary key, and data will persist as complete, accurate, and reliable. Dataframe: A Python object where data are organized in tabular format (rows and columns). In contrast to a database that is stored on disk, a dataframe is not persistent because it is stored in memory (RAM). Database table: A database object where data are stored in tabular format as records (rows) and fields (columns). Primary key: Column of a database table that contains unique fields to enable the identification of single rows. 2.1 Agricultural data template creator AgTC aims to standardize data collection for experiments conducted in field or controlled environment conditions. Its output is a CSV template file containing tabular meta-data related to the target observation in separate columns, such as crop species, experiment name, treatment, measurement name, unit of measurement, season, or other temporal designation, and one column that contains a unique identifier for each observation. These columns in the template are generated via two procedures. The first group of columns describes the experiment and is generated using basic metadata that are contained in an input CSV file. This may include a list of the experimental units (i.e., plots or pots), replications, genotype names, and any other information related to the experimental design. This set of information should be unique for each experimental unit. The second group of columns is generated using the parameters specified in the user-modified configuration YAML file, which also serves as an input file to AgTC. In contrast with the first group of columns, these columns, which are created using the YAML configuration file parameters, are repeated in all rows on a sequence basis. For clarification, Figure 2 shows an example of an output template CSV created by AgTC and maps how information from input files is used to complete rows and columns in the output file. figure 2 Figure 2 Input and output files of AgTC. Information from the input.csv file (A) and parameters from the configuration.yml file (B) are used to generate the columns found in the output template file (C). All arguments (i.e., information passed into functions) for AgTC Python functions are taken directly from the user-specified configuration file; in this way, the user is able to add, delete, or modify parameters without the need to code in Python. The YAML configuration file is divided into six chunks of parameters known as block collections, where each block is identified with uppercase letters. A block collection may have keys, values, or sequences (Table 1). Considering that the block collection content can be modified to write variable names and content of rows, this enables the user to use controlled vocabularies and ontologies (Arnaud et al., 2020) or namings of crops, traits, and variables as recommended by the FAIR (Findable, Accessible, Interoperable, and Reusable) data principles (Devare et al., 2021). Once all the parameters are specified, the main.ipynb file can be executed on Jupyter Notebook without requiring the user to modify any line code. Since the template output is a CSV file, it can be opened by any spreadsheet software independent of operating system on a computer or mobile device to enter the observation values. Alternatively, the CSV file can be directly uploaded as a field file in the Field Book Android application (Rife and Poland, 2014) to facilitate data collection on the experiment in situ. The template created by AgTC fulfills Field Book's field file requirement of having a column containing unique observation identifiers and other columns that can be used to navigate within the application, such as plot number and treatment. table 1 Table 1 The block collections of the AgTC configuration file. 2.2 Agricultural data extract, transform, and load framework The objectives of the Extract, Transform, and Load (ETL) tool are to process CSV data files from different plant science experiments and aggregate them into a standard database table in a central repository. There, data are available to use for a variety of downstream analyses. The execution of functions in AgETL is divided into two Jupyter Notebook and configuration files. The first set of functions runs the Extract and Transform processes. This outputs a CSV file where the data from different source files have been aggregated and standardized into a single format. The second group of functions is used to load data into a single table in the database. The ETL functions are divided into these two separate groups (i.e., the Extract and Transform functions in one group and the Load functions in another) to enable users to carry out data quality control in between (Figure 1). Extract and Transform processes: The Extract and Transform steps aim to merge different data files into a single and standard dataframe containing all necessary information to run subsequent analyses without needing extra information. These individual data files may be ones that were generated using template files from AgTC or those that were downloaded directly from scientific instruments. Files require a CSV extension and a grid-like format of rows and columns, with the first row containing column header names. Parameters are specified in a series of collection blocks found in the configuration file (Table 2), however, only the parameters within the FILES_TO_PROCESS collection block are required. The other parameters depend on the specific transformations that the data need for formatting into the final standardized dataframe. Therefore, the user can leave them empty if the data do not require transformation. All measurement values from different files are moved to the same column in the standardized dataframe, and they are differentiated from each other, adding the name and units of the variable in two different columns (see Supplementary Figure 1). The rest of the transformations are used to standardize row and column values, which are helpful for data aggregation. In this step, any unnecessary columns are also dropped. A CSV file containing all processed data in a tabular structure is exported at the end of the process. table 2 Table 2 Block collections that are part of the two AgETL configuration files. Load processes: The objective of the Load processes is to upload the data into a database, with which users can interact to perform queries or carry out data analysis. Even if data reflect different species, experiments, seasons, variables, and sources, the Load functions are flexible to upload them in the same database table. This facilitates queries across experiments, researchers, species, etc., later. Those functions can also run structured query language (SQL) statements and open the database connection using six chunks of parameters in the configuration file. Out of the block collections, only four are required to load a new dataframe into the database table (Table 2). The other block collections are only required the first time a new table is created in the database or when a new column is appended to the table. The database is created in PostgreSQL, a DBMS based on a client-server architecture with a variant of the standard SQL as the query language (Herzog, 1998). Three functions create SQL statements to interact with the database. One of them allows creating a table (sql_statement_create_table_if_not_exist), another one inserts a new column in a preexisting table (sql_statement_add_column_if_table_exists), and another enables loading of the data into the table (insert_dataframe_to_database). Finally, execute_sql_statement is the function that executes the SQL statements. Additional SQL statements and commands found in the Jupyter Notebook allow the user to make changes and queries in the database. Since the Load operations are independent of the Extract and Transform steps, the process is amenable to uploading additional data files (e.g., metadata) and enables users to perform any SQL actions. For instance, the user can create tables and load data following an object-relational database model, which facilitates using Minimum Information about Plant Phenotyping Experiments (MIAPPE) standards (Papoutsoglou et al., 2020). 2.3 Implementation and deployment options AgTC (source code available at https://github.com/DS4Ag/AgTC) and AgETL (source code available at https://github.com/DS4Ag/AgETL) are open-source and are available on GitHub. They can be executed under various versions of Jupyter. For example, they can run on a local server using a simple installation of JupyterLab or Jupyter Notebook, or they can run under environment management such as Conda, Mamba, or Pipenv. Another option to run these tools is on the cloud using a Jupyter Hub environment. The configuration file for the AgETL Load steps can establish a PostgreSQL database connection in a local host, such as installing it on a local development computer or using any of the standard service models of the database cloud service offered as Infrastructure as a Service, Platform as a Service, or Software as a Service (Jain and Mahajan, 2017). These options enable research labs to work with their institutional IT infrastructure to set up individual workflows or create their database using commercially available cloud services, including Amazon Web Services, Google Cloud Platform, or Microsoft Azure. 2.4 Test case 1: soybean evaluation under field conditions (USA) As a first test case, we collected data on a soybean (Glycine max) experiment. This experiment evaluated 25 soybean genotypes with four repetitions and two treatments: early planting (planted on May 30, 2022) and late planting (planted on June 9, 2022). The trial was conducted at the Purdue University Agronomy Center for Research and Education (ACRE; 40° 28′ 20.5″ N 86° 59′ 32.3″ W) in West Lafayette, Indiana, USA. Various plant traits, such as height, width, growth stages, and photographs of a fully expanded trifoliate leave for each plot, were collected in the field using the Field Book application after creating templates using AgTC. These measurement campaigns occurred five times throughout the crop cycle, starting at late vegetative stages (V6) and finishing at approximately the R6 stage (full seed). In addition to data collected directly in the field, AgTC-generated CSV template files were used to record trifoliate dry weights on a computer in a lab setting. During the same measurement campaigns, volumetric soil water content (using HydroSense II; Campbell Scientific; UT, USA) and Leaf Area Index (LAI) (using the LAI-2200C Plant Canopy Analyzer; LI-COR Inc., NE, USA) were collected. In both these cases, data were initially stored in each of the devices’ internal memory. Thus, for soil moisture and LAI, there was no need to use the AgTC template. Finally, after the R8 growth stage (full maturity), plants were sampled and processed in the lab to obtain yield components (plants per meter, pods per plant, and seeds per pod). Data were entered into template files created by AgTC using a computer for these measurements. 2.5 Test case 2: wheat evaluation under field conditions (Mexico) For our second test case, we collected data on wheat (Triticum aestivum) in an experiment established at CIMMYT’s research station, Campo Experimental Norman E. Borlaug (CENEB), located near Ciudad Obregon, Sonora, Mexico (27° 23′ 46″ N, 109° 55′ 42″ W). The trial evaluated a panel of 14 wheat genotypes with three repetitions under three environments: well-watered (WW), drought (DR), and high temperature (HT). Data were collected throughout the 2022 and 2023 crop growth seasons. Seedling emergence occurred in early December for the WW and DR treatments in both seasons. However, the HT trial was planted only for the 2022 season, with an emergence date in early March (Supplementary Table 1). Direct and proximal sensing measurements were made during the two crop seasons. There were four direct measurement sampling campaigns throughout each cycle. The first occurred before sowing, and the second was 40 days after seedling emergence. The third and fourth sampling campaigns were scheduled based on each genotype’s specific growth stages (GS) (Zadoks et al., 1974), with the third carried out 12 days after heading (GS54) and the fourth at physiological maturity (GS87). Proximal sensing measurements were made every week from canopy closure to GS87. Template files created by AgTC were used to enter data for gravimetric soil water content, above-ground biomass, and yield components. Samples were first collected from the field and then processed in the laboratory. Growth stages, including seedling emergence (GS10), flag leaf sheath extending (GS41), GS54, and GS87, and one plant height measurement after GS87, were collected directly in the field using the AgTC template and the Field Book application. The proximal sensing data collected include chlorophyll content (using the SPAD-502 chlorophyll meter; Konica Minolta; Osaka, Japan), canopy temperature (using the Sixth Sense LT300 Infrared Thermometer; TTI Instruments; VT, USA), normalized difference vegetation index (using the GreenSeeker hand-held optical sensor; N-Tech Industries; CA, USA) and hyperspectral reflectance (using the ASD Field Spec 3; ASDInc., CO, USA). 2.6 Test case 3: rice evaluation in a controlled-environment facility (USA) For our final test case, we collected data on cultivated Asian rice (Oryza sativa) in a growth chamber environment at Purdue University’s Ag Alumni Seed Phenotyping Facility (AAPF) (West Lafayette, Indiana, USA). Six genotypes were chosen based on their documented genetic information, constrained flowering dates, diverse geographical backgrounds, or potential genetic value (Rice Diversity Panels 1 and 2 from the USDA-ARS Dale Bumper National Rice Research Center, Stuttgart, Arkansas, Genetic Stocks Oryza Collection (www.ars.usda.gov/GSOR)) and raised for 82 days during Summer and Fall of 2022. The facility has two large growth chambers (Conviron®, Winnipeg, Canada) with a weight-based automated irrigation system (Bosman Van Zaal, Aalsmeer, The Netherlands), and both chambers were leveraged in this experiment; one had CO2 concentration at 700 ppm (high CO2 chamber) and the other at 415 ppm (ambient CO2 chamber). The rice plants were grown in pots under two levels of CO2 and two levels of drought. Each treatment had two replications with a total of 48 plants. According to the timing of drought, the experimental period could be divided into three timepoints: before drought (42-47 DAS, timepoint 1, TP1), during the mid of drought treatment (59-61 DAS, timepoint2 A, TP2-A), at the very end of drought (66-67 DAS, timepoint2 B, TP2-B) and upon recovering (77-82 DAS, timepoint3, TP3). In this test case, AgTC was utilized to create Field Book field files to (1) collect photographs for later calculation of Specific Leaf Area (SLA), similar to the soybean test case (during TP2-B and TP3) and (2) record leaf water potential (LWP, MPa) measurements at 0800, 1400, and 1800 hr during TP2-B. For both types of measurements, the youngest fully expanded leaf from one plant was selected for each observation. For LWP, we used the Model 1000 pressure bomb (PMS Instrument Company, OR, USA). 3 Results and discussion This section proposes a workflow using AgTC and AgETL to support plant science experiments from data collection to analysis. We describe the results of implementing the entire data pipeline in the two field experiments and utilizing AgTC in all three experiments. 3.1 Proposed data pipeline We propose a pipeline for data collection and management using AgTC and AgETL. The first step is to create templates using AgTC. Users can then open the CSV output file to enter data in a standardized fashion (Figure 1). If data are acquired in the lab, as would be the case for any sample destructively collected in the field or greenhouse, this can be accomplished using spreadsheet software on a computer (Figure 3). Alternatively, the template can be used directly as a field file for the Field Book application to facilitate data collection on-site, such as for data collection in the field, greenhouse, or growth chamber. When data collection is carried out using instruments that come with their own internal data storage system (e.g., the LI-6800 Portable Photosynthesis System), users download resultant files directly. After data acquisition, AgETL functions Extract and Transform data files from different sources to standardize their formats. From there, researchers can run exploratory data analysis, such as data aggregation, visualization, outlier detection to perform data QC. After data are QC-ed, they are ready for downstream analyses, such as modeling. At this point, it is also advisable to load the QC-ed data into a database using AgETL’s Load functions. In addition to enhancing data analysis capabilities via interaction with data analysis dashboards, data are securely stored and easily accessible. figure 3 Figure 3 Application of AgTC in wheat experiments. Left: An example of using an AgTC-generated template in laboratory conditions. Here, wet and dry soil weights are measured (A) and entered into an AgTC template directly on the computer (B). Right: An example of using an AgTC-generated template in field conditions. In this example, wheat plant height is measured manually (C), and entered digitally in the field via the Field Book application that utilizes an AgTC-generated template (D). Below, we describe the application of AgTC and AgETL in several test cases. AgTC was utilized in experiments on soybean, rice, and wheat under field and controlled environmental conditions, while AgETL functions were utilized for generating soybean and wheat datasets. In addition to the specific test cases presented here, AgTC has been used on another soybean experiment carried out in Wanatah, Indiana, USA, in 2022 and two more wheat experiments in the same research center at CIMMYT in Mexico during the 2023 field season. 3.2 Application of AgTC Templates created using AgTC were used for data collection for 12 measurements across three species in three experimental settings (Table 3). We observed that the new tool enhanced data collection in at least two ways: (1) utilization reduced the total steps required for data collection, and (2) application of AgTC helped improve data file organization. table 3 Table 3 Overview of measurements collected using AgTC and/or processed by AgETL from three experimental test cases. Reduction in the number of steps for data collection. In the soybean and rice experimental test cases, SLA was measured at multiple timepoints (five for soybean and two for rice). SLA is computed as the ratio of leaf area and leaf dry weight. Conventionally, leaf area estimates are made using leaf scanners or image-based software, whereby image files are manually reamed utilizing labels found within each image itself prior to image processing. This renaming step is tedious and may be affected by human error. To improve on the conventional method for estimating SLA, AgTC was used to create one template per sampling campaign, which was uploaded as a field file in the Field Book application. Then, using the picture function of Field Book, a photo of the target leaf was taken using tablets in the field. The advantage of this system is that Field Book directly uses the values of the observation identifier column generated by AgTC as the names of the image files. Compared with our traditional method, this saves time by precluding the need to manually rename image files before extracting leaf area values using downstream software such as Easy Leaf Area (Easlon and Bloom, 2014). Another example where the use of a template generated from AgTC as a field file in the Field Book application helped decrease the number of steps in data collection was for wheat canopy temperature measurements in the field. The conventional method employed by the CIMMYT Wheat Physiology group requires that one researcher takes temperature readings from the crop canopy using a sensor device while another records these values on a paper field form. Then, values are transferred manually to a digital spreadsheet. When using an AgTC-generated template opened either in a mobile device or uploaded as a field file in Field Book, measurements are digitized directly in the field, enhancing the efficiency of data collection and potentially reducing human error involved in converting paper records into digital formats. Improvement of data file organization. For some measurements, AgTC did not minimize the overall number of steps involved in data collection but can improve data management. As an illustration, we describe the process for recording wheat biomass measurements. Conventionally, the data are obtained in a laboratory setting, where the samples are processed, and data are entered directly into electronic files. Using AgTC, the overall process does not change. However, AgTC automatically generates metadata columns and unique identifiers for each observation in the electronic template files without any need to manually copy and paste information across spreadsheets. Additionally, the standardized format for naming template files by AgTC facilitates file organization in storage directories. 3.3 Application of AgETL AgETL was tested successfully for processing data collected from soybean experiments of the 2022 summer season and for data collected from the wheat experiments of winter seasons 2021-2022 and 2022-2023. The Extract and Transform functions were executed separately from the Load functions for these data. Resultant standardized dataframes were loaded into a PostgreSQL database using the Load process after the Extract and Transform processes. The main objective of the Extract and Transform processes of AgETL is to generate dataframes with a standard format and structure using heterogeneously formatted lab- or field-generated data. Extract and Transform functions that perform the extraction process first take in CSV files from the directory indicated on the configuration file. The next step compares each file’s column names to identify unique and duplicated names. The output of this execution, a list of similar and different column names, helps the user decide which transformations are needed to alter dataframe columns. In the following step, dataframes obtained from the extracted files are concatenated, resulting in a single dataframe where original columns with shared names are combined, and original columns with different names are retained separately. The resulting dataframe is then combined with other files containing additional relevant information (e.g., genotype names) if the user indicated these parameters in the configuration file. Subsequent transformation steps depend on the kinds of alterations the data needs to undergo quality assurance (QA) or enter the Load process. At this stage, trait column names can be unified. For example, ‘LAI’, ‘Leaf Area Index’, ‘lai’, and ‘leaf area index’ refer to the same trait yet would be treated differently by database management systems; a transformation to update column names is needed. This and other options for transformations are illustrated in Supplementary Figure 1 and include dropping undesired columns, creating new columns, updating row values, and updating primary key values. From our testing of Extract and Transform processes on the soybean and wheat field experiments, we found AgETL useful for several scenarios. Scenario 1: AgETL was applied on data files derived from templates generated using AgTC. For example, we simultaneously processed 42 different canopy temperature files from three treatments and two crop growth seasons in the wheat experiment. In this case, only transformations for updating column names were needed. Scenario 2: AgETL was used to process data files derived from lab-collected measurements. From the same wheat experiments, we implemented AgETL on ten biomass files that were collected from samples processed in the lab. After files were joined, multiple column names were updated to unify columns with the same meaning, one column was added to indicate the unit, and several extraneous columns used in intermediate stages of biomass estimation were dropped (e.g., bag dry weight). Scenario 3: AgETL was used to process data collected via sensors that had their own unique storage systems. For instance, in the soybean experiment, data files originated from hand-held instruments such as the Campbell HydroSense II and LAI2200C (Table 3); in this case, the Extract and Transform functions of AgETL were used to merge the files into a single dataframe and join genotype names from another data file. Standardized, uniformly formatted data files resulting from the execution of Extract and Transform functions were next uploaded into a PostgreSQL database server to test the utility of AgETL’s Load process. The first step is connecting with a PostgreSQL database (either a cloud-hosted instance or a local installation). The configuration file is flexible, allowing the user to write parameters for either of the two options. We successfully tested the database connection in three database scenarios: one was a local instance (Figure 4A), and the other two were commercial cloud service providers, i.e., Database as a Service (DBaaS). The two cloud services were Cloud SQL (https://cloud.google.com), offered by Google Cloud Platform (Bisong, 2019), and the Railway PostgreSQL database service (https://railway.app), shown in Figure 4B. Instructions for establishing the database connection have been made available in the AgETL GitHub repository. One significant advantage of processing and managing data using AgETL was observed for canopy temperature and SPAD measurements collected weekly during wheat experiments. These data were processed and loaded immediately after the data were gathered. The dashboard automatically reflected updated information in its data visualization features because the database was connected with a real-time data visualization online interface. figure 4 Figure 4 The AgETL Load process facilitates the loading of clean data into databases. Shown here are data loaded (A) in a PostgreSQL localhost server and (B) in Railway, a commercial database cloud service provider. After establishing the database connection, users can create the first table by writing the names of the columns and the data types using the PostgreSQL names for native data types on the configuration file. Each table only needs to be created once, and it is possible to create many tables if needed. The recommendation is to create tables based on specific research goals. For instance, data from different experiments that will be used to calibrate crop models should be uploaded to the same table in the database. Once the table exists, the data from the files specified for the user are uploaded. Moreover, AgETL can add more columns when the table already exists. Additionally, the records in the database can be upgraded by loading the data file with the updated row and having the same primary key. Furthermore, AgETL gives the user access to the four standard SQL actions of database systems called CRUD, which comes from create, read, update, and delete (van den Brink et al., 2007; Krogh, 2018). For that reason, the Jupyter Notebook file has a section with SQL statements, called useful SQL statements, that allows the deletion of a column and a table. Finally, AgETL permits users to write and execute their own SQL statements, like SQL queries, directly from Jupiter Notebook without another extra configuration. We additionally tested the connection to the DBaaS using R (R Core Team, 2021) and Python, two widely used programming languages in data analysis for research (Fahad and Yahya, 2018; Rahmany et al., 2020). We successfully connected R with the two DBaaS providers using the R packages, DBI (R Special Interest Group on Databases (R-SIG-DB) et al., 2022), and odbc (Hester and Wickham, 2023). Finally, the RPostgreSQL R package was used to access the PostgreSQL database. For testing with Python, the PostgreSQL connection was successfully created using the Psycopg2 library (https://www.psycopg.org). From our experience developing and testing AgTC and AgETL in the soybean, wheat, and field experiments, we have five suggestions to best leverage their use in small to mid-sized research groups: (1) Users who collect data should be the ones that create their own template files with AgTC, and (2) these templates should be saved on a centralized repository. The creation of multiple nested folders should be avoided since the template files can be sorted by their names, which are automatically generated in a structured, consistent, and meaningful way. For the Extract and Transform process using AgETL, it may be best to (3) appoint a single responsible person to handle these steps, while (4) individual researchers execute the QA/QC themselves on the files generated from the Extract and Transform steps, following and documenting steps appropriate for their own research project. Finally, (5) an appointed individual in the research group (or IT specialist collaborating with the research group) handles uploading clean, QA/QC-ed files to the database using AgETL’s Load process. This should occur on a regular basis that is sensible for the research group, e.g., after each field season, to keep the database up-to-date. 3.4 Areas for improvement We have identified several areas for future improvements to AgTC and AgETL. Even though AgTC templates allow the collection of multiple variables at the same time (e.g., as would occur if plant height and canopy width are collected with the same AgTC-generated template), multivariable files used as input in AgETL need to be processed as separate steps during the Extract and Transform stages mainly because the observation ID need to be updated. This was to make the configuration file simpler for the user, such that they do not need to specify many parameters. Streamlining these processing steps into a single step for multivariable files could be an improvement in an updated version of the tool. Additionally, documentation to implement ETL steps using functions on workflow management platforms such as Apache Airflow would help to automate AgETL. Finally, documentation to use both AgTC and AgETL in command-line mode could enable the implementation of these tools as part of a larger workflow, which may be particularly useful for moderate to large-sized laboratories. 4 Conclusion We have developed two tools to address observed challenges in data collection, processing, and management in plant science research called AgTC and AgETL. These tools are simple to use and do not require experience in programming. Additionally, they are adaptable for data collection and data processing in the field or the lab and are agnostic to crop species, experimental design, and scale. The templates generated by AgTC can be used for data collection in the field or the lab and can reduce the number of steps required for data collection and improve data file organization. AgETL enables the extraction of information from multiple files from diverse sources and measurements and merges them into the same data table. This tool facilitates loading and wrangling data on a local host or using a database cloud service, which can be readily managed by appointed individuals within research labs or by collaborating with institutional IT support. Finally, we have developed user documentation, with English and Spanish versions available in the README section of the AgTC and AgETL GitHub repositories. Data availability statement The source code for AgTC and AgETL can be found here: https://github.com/DS4Ag/AgETL, https://github.com/DS4Ag/AgTC. Author contributions LV-R: Conceptualization, Methodology, Software, Writing – original draft, Writing – review & editing. T-CT: Methodology, Writing – review & editing. KR: Resources, Writing – review & editing. MR: Resources, Funding acquisition, Project administration, Writing – review & editing. DW: Funding acquisition, Resources, Writing – review & editing, Conceptualization, Supervision, Writing – original draft. Funding The author(s) declare financial support was received for the research, authorship, and/or publication of this article. LV-R was supported by a CONACYT graduate fellowship from the Mexican government. Funding for the experimental test cases was provided by HedWIC #DFs-19-0000000013 to MR and USDA NIFA #2022-67013-36205 to DW. Acknowledgments The authors thank Abby Seybert and Ava Antic for help with soybean data collection; Makala Hammons and Sam Schafer for help with rice data collection; Lucia Nevescanin, and Pablo Rivera for help with wheat data collection. Eric Vince Seal planted the soybean trial. Conflict of interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Publisher’s note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. Supplementary material The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fpls.2024.1265073/full#supplementary-material References Arnaud, E., Laporte, M.-A., Kim, S., Aubert, C., Leonelli, S., Miro, B., et al. (2020). The ontologies community of practice: A CGIAR initiative for big data in agrifood systems. Patterns 1, 100105. doi: 10.1016/j.patter.2020.100105 PubMed Abstract | CrossRef Full Text | Google Scholar Ben-Kiki, O., Evans, C., döt Net, I. (2021) YAML Ain’t Markup Language (YAML™) revision 1.2.2. Available at: https://yaml.org/spec/1.2.2/. Google Scholar Bisong, E. (2019). An Overview of Google Cloud Platform Services. In Building Machine Learning and Deep Learning Models on Google Cloud Platform. Ed. Bisong, E. (Berkeley, CA: Apress), 7–10. doi: 10.1007/978-1-4842-4470-8_2 CrossRef Full Text | Google Scholar CGIAR Excellence in Breeding Platform (2022) The Enterprise Breeding System (EBS). Available at: https://excellenceinbreeding.org/toolbox/tools/enterprise-breeding-system-ebs. Google Scholar Coppens, F., Wuyts, N., Inzé, D., Dhondt, S. (2017). Unlocking the potential of plant phenotyping data through integration and data-driven approaches. Curr. Opin. Syst. Biol. 4, 58–63. doi: 10.1016/j.coisb.2017.07.002 PubMed Abstract | CrossRef Full Text | Google Scholar Crain, J., Wang, X., Evers, B., Poland, J. (2022). Evaluation of field-based single plant phenotyping for wheat breeding. Plant Phenome J. 5, e20045. doi: 10.1002/ppj2.20045 CrossRef Full Text | Google Scholar Crescente, J. M., Guidobaldi, F., Demichelis, M., Formica, M. B., Helguera, M., Vanzetti, L. S. (2017). Phenobook: an open source software for phenotypic data collection. GigaScience 6, giw019. doi: 10.1093/gigascience/giw019 CrossRef Full Text | Google Scholar Devare, M., Aubert, C., Benites Alfaro, O. E., Perez Masias, I. O., Laporte, M.-A. (2021). AgroFIMS: A tool to enable digital collection of standards-compliant FAIR data. Front. Sustain. Food Syst. 5. doi: 10.3389/fsufs.2021.726646 CrossRef Full Text | Google Scholar Dipta, B., Sood, S., Devi, R., Bhardwaj, V., Mangal, V., Thakur, A. K., et al. (2023). Digitalization of potato breeding program: Improving data collection and management. Heliyon 9, e12974. doi: 10.1016/j.heliyon.2023.e12974 PubMed Abstract | CrossRef Full Text | Google Scholar Easlon, H. M., Bloom, A. J. (2014). Easy Leaf Area: Automated digital image analysis for rapid and accurate measurement of leaf area. Appl. Plant Sci. 2, 1400033. doi: 10.3732/APPS.1400033 CrossRef Full Text | Google Scholar Eitzinger, A. (2021). Data collection smart and simple: evaluation and metanalysis of call data from studies applying the 5Q approach. Front. Sustain. Food Syst. 5. doi: 10.3389/fsufs.2021.727058 CrossRef Full Text | Google Scholar Elsayed, A. M., Saleh, E. I. (2018). Research data management and sharing among researchers in Arab universities: An exploratory study. IFLA J. 44, 281–299. doi: 10.1177/0340035218785196 CrossRef Full Text | Google Scholar Fahad, S. K. A., Yahya, A. E. (2018). “Big data visualization: allotting by R and python with GUI tools,” in 2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE). (Shah Alam, Malaysia: IEEE) 2018, 1–8. doi: 10.1109/ICSCEE.2018.8538413 CrossRef Full Text | Google Scholar Gonzalez, E. M., Zarei, A., Hendler, N., Simmons, T., Zarei, A., Demieville, J., et al. (2023). PhytoOracle: Scalable, modular phenomics data processing pipelines. Front. Plant Sci. 14. doi: 10.3389/fpls.2023.1112973 CrossRef Full Text | Google Scholar Herzog, R. (1998). PostgreSQL–the linux of databases. Linux J. 46, 1–es. doi: 10.5555/327239.327240 CrossRef Full Text | Google Scholar Hester, J., Wickham, H. (2023) odbc: Connect to ODBC Compatible Databases (using the DBI Interface) (R package version 1.3.4.). Available at: https://CRAN.R-project.org/package=odbc. Google Scholar Jain, A., Mahajan, N. (2017). “Introduction to Database as a Service,” in The Cloud DBA-Oracle: Managing Oracle Database in the Cloud. Eds. Jain, A., Mahajan, N. (Berkeley, CA: Apress), 11–22. Available at: doi: 10.1007/978-1-4842-2635-3_2 CrossRef Full Text | Google Scholar Krogh, J. W. (2018). “SQL Tables,” in MySQL Connector/Python Revealed: SQL and NoSQL Data Storage Using MySQL for Python Programmers. Ed. Krogh, J. W. (Berkeley, CA: Apress), 371–401. Available at: doi: 10.1007/978-1-4842-3694-9_8 CrossRef Full Text | Google Scholar Machwitz, M., Pieruschka, R., Berger, K., Schlerf, M., Aasen, H., Fahrner, S., et al. (2021). Bridging the gap between remote sensing and plant phenotyping—Challenges and opportunities for the next generation of sustainable agriculture. Front. Plant Sci. 12. doi: 10.3389/fpls.2021.749374 PubMed Abstract | CrossRef Full Text | Google Scholar Malosetti, M., Bustos-Korts, D., Boer, M. P., van Eeuwijk, F. A. (2016). Predicting responses in multiple environments: issues in relation to genotype × Environment interactions. Crop Sci. 56, 2210–2222. doi: 10.2135/cropsci2015.05.0311 CrossRef Full Text | Google Scholar Morales, N., Ogbonna, A. C., Ellerbrock, B. J., Bauchet, G. J., Tantikanjana, T., Tecle, I. Y., et al. (2022). Breedbase: a digital ecosystem for modern plant breeding. G3 Genes|Genomes|Genetics 12, jkac078. doi: 10.1093/g3journal/jkac078 CrossRef Full Text | Google Scholar Mullan, D., Mullan, D. (2012). “Chlorophyll content,” in Physiological Breeding II: A Field Guide to Wheat Phenotyping. Eds. Pask, A., Pietragalla, J., Mullan, D., Reynolds, M. (Mexico: CIMMYT), 41–43. Available at: https://repository.cimmyt.org/bitstream/handle/10883/1288/96144.pdf Google Scholar Munappy, A. R., Bosch, J., Olsson, H. H. (2020). “Data Pipeline Management in Practice: Challenges and Opportunities,” in Product-Focused Software Process Improvement. Eds. Morisio, M., Torchiano, M., Jedlitschka, A. (Cham, Switzerland: Springer International Publishing), 168–184. Google Scholar Neveu, P., Tireau, A., Hilgert, N., Nègre, V., Mineau-Cesari, J., Brichet, N., et al. (2019). Dealing with multi-source and multi-scale information in plant phenomics: the ontology-driven Phenotyping Hybrid Information System. New Phytol. 221, 588–601. doi: 10.1111/nph.15385 PubMed Abstract | CrossRef Full Text | Google Scholar Papoutsoglou, E. A., Faria, D., Arend, D., Arnaud, E., Athanasiadis, I. N., Chaves, I., et al. (2020). Enabling reusability of plant phenomic datasets with MIAPPE 1.1. New Phytol. 227, 260–273. doi: 10.1111/nph.16544 PubMed Abstract | CrossRef Full Text | Google Scholar Pask, A., Pietragalla, J., Mullan, D., Reynolds, M. P. (Eds.) (2012). Physiological breeding II: a field guide to wheat phenotyping (Mexico, D.F.: CIMMYT). Available at: https://repository.cimmyt.org/bitstream/handle/10883/1288/96144.pdf Google Scholar Rahmany, M., Mohd Zin, A., Sundararajan, E. A. (2020). Comparing tools provided by python and r for exploratory data analysis. Int. J. Inf. System Comput. Science(IJISCS) 4, 131–142. doi: 10.56327/ijiscs.v4i3.933 CrossRef Full Text | Google Scholar R Core Team (2021). R: A language and environment for statistical computing (Vienna, Austria: R Foundation for Statistical Computing). Available at: https://www.R-project.org/. Google Scholar Reynolds, M., Chapman, S., Crespo-Herrera, L., Molero, G., Mondal, S., Pequeno, D. N. L., et al. (2020). Breeder friendly phenotyping. Plant Sci. 295, 110396. doi: 10.1016/j.plantsci.2019.110396 PubMed Abstract | CrossRef Full Text | Google Scholar Rife, T. W., Poland, J. A. (2014). Field book: an open‐Source application for field data collection on android. Crop Sci. 54, 1624–1627. doi: 10.2135/cropsci2013.08.0579 CrossRef Full Text | Google Scholar R Special Interest Group on Databases (R-SIG-DB), Wickham, H., Müller, K. (2022) DBI: R Database Interface (R package version 1.1.3). Available at: https://CRAN.R-project.org/package=DBI. Google Scholar Ting, T.-C., Souza, A., Imel, R. K., Guadagno, C. R., Hoagland, C., Yang, Y., et al. (2023). Quantifying physiological trait variation with automated hyperspectral imaging in rice. Front. Plant Sci. 14. doi: 10.3389/fpls.2023.1229161 CrossRef Full Text | Google Scholar van den Brink, H., van der Leek, R., Visser, J. (2007). “Quality assessment for embedded SQL,” in Seventh IEEE International Working Conference on Source Code Analysis and Manipulation (SCAM 2007). (Paris, France: IEEE), 163–170. doi: 10.1109/SCAM.2007.23 CrossRef Full Text | Google Scholar Zadoks, J. C., Chang, T. T., Konnzak, C. F. (1974). A decimal code for growth stages in cereals. Weed Res. 14, 415–42. doi: 10.1111/j.1365-3180.1974.tb01084.x CrossRef Full Text | Google Scholar Keywords: data pipeline, extract-transform-load, database, data aggregation, data processing, plant phenotyping Citation: Vargas-Rojas L, Ting T-C, Rainey KM, Reynolds M and Wang DR (2024) AgTC and AgETL: open-source tools to enhance data collection and management for plant science research. Front. Plant Sci. 15:1265073. doi: 10.3389/fpls.2024.1265073 Received: 21 July 2023; Accepted: 30 January 2024; Published: 21 February 2024. Edited by: Jennifer Clarke, University of Nebraska-Lincoln, United States Reviewed by: Alan Cleary, National Center for Genome Resources, United States Geng (Frank) Bai, University of Nebraska - Lincoln, United States Copyright © 2024 Vargas-Rojas, Ting, Rainey, Reynolds and Wang. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Diane R. Wang, drwang@purdue.edu Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Footer Guidelines Author guidelines Editor guidelines Policies and publication ethics Fee policy Explore Articles Research Topics Journals Outreach Frontiers Forum Frontiers Policy Labs Frontiers for Young Minds Connect Help center Emails and alerts Contact us Submit Career opportunities Follow us © 2024 Frontiers Media S.A. All rights reserved Privacy policy | Terms and conditions We use cookies Our website uses cookies that are necessary for its operation and other cookies to track its performance or to improve and personalize our services. To manage or reject non-essential cookies, please click \"Cookies Settings\". For more information on how we use cookies, please see ourCookie Policy Cookies Settings Accept Cookies"

Paper 5:
- APA Citation: 
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: Leveraging containerization technologies (e.g., Docker, Kubernetes) for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP)
  Extract 2: Our experimental results show that the power-performance prediction model can achieve 91.39% accuracy on average in real-time with a negligible overhead of 1.6% of the total computing power per node due to power and performance daemons.
  Limitations: The paper does not discuss the potential limitations of their approach, such as its scalability to larger HPC systems or its impact on application performance in the presence of resource contention.
  Relevance Evaluation: {'extract_1': 'Leveraging containerization technologies (e.g., Docker, Kubernetes) for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP)', 'extract_2': 'Our experimental results show that the power-performance prediction model can achieve 91.39% accuracy on average in real-time with a negligible overhead of 1.6% of the total computing power per node due to power and performance daemons.', 'limitations': 'The paper does not discuss the potential limitations of their approach, such as its scalability to larger HPC systems or its impact on application performance in the presence of resource contention.', 'relevance_score': 1.0}
  Relevance Score: 1.0
  Inline Citation: >
  Explanation: The paper proposes a hybrid power conservation approach for containerized High Performance Computing (HPC) environments, called pHPCe. It utilizes a machine learning (ML) methodology for real-time hybrid power-performance estimation and a novel power-cap determination framework with resource contention awareness.

The specific point of focus is the leveraging of containerization technologies (e.g., Docker, Kubernetes) for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). The authors evaluate the performance of their proposed approach using the NAS Parallel Benchmark (NPB) and HPC Challenge benchmark (HPCC) applications, demonstrating significant power savings up to 13.1%.

 Full Text: >
"Your privacy, your choice We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Accept all cookies Skip to main content Log in Find a journal Publish with us Track your research Search Cart Home Cluster Computing Article pHPCe: a hybrid power conservation approach for containerized HPC environment Published: 22 July 2023 (2023) Cite this article Download PDF Access provided by University of Nebraska-Lincoln Cluster Computing Aims and scope Submit manuscript Animesh Kuity & Sateesh K. Peddoju  109 Accesses Explore all metrics Abstract Reducing power consumption with tolerable performance degradation is a fundamental challenge in today’s containerized High Performance Computing (HPC). Most Dynamic Power Management (DPM) approaches proposed for the HPC environment are based on profile-guided power-performance prediction techniques. However, the complexity of DPM approaches in a multi-tenant containerized HPC environment (cHPCe) increases significantly due to the varying demands of users and the contention of shared resources. Moreover, there is limited research into software-level monitoring of power consumption in the popular Docker container environment since it is not designed keeping HPC in mind. The proposed research in this paper aims to present a real-time hybrid power-performance prediction approach using Long Short-Term Memory (LSTM) machine learning model. Unlike state-of-the-art techniques, LSTM with the rolling update mechanism accurately predicts the relationships between the tail latencies in time-series power performance data. It updates the training sample sequences according to the currently predicted power consumption and duration to predict sporadic power surges. It also proposes a power-cap determination framework with resource contention awareness to fine-tune power consumption in real time at the thread level. The proposed containerized environment is designed from scratch keeping HPC requirements in mind. Hence, we name the proposed approach as power-aware cHPCe (pcHPCe) and is evaluated and compared with native BareMetal execution using the NAS Parallel Benchmark (NPB) and HPC Challenge benchmark (HPCC) applications. Our experimental results show that the power-performance prediction model achieves an accuracy of 91.39% on average in real-time with an overhead of 1.6% of the total computing power per node. Our resource contention-aware power-cap selection framework attains significant power saving up to 13.1%. Similar content being viewed by others Docker platform aging: a systematic performance evaluation and prediction of resource consumption Article 13 March 2022 Power Consumption Modeling and Prediction in a Hybrid CPU-GPU-MIC Supercomputer Chapter © 2016 Operation-Aware Power Capping Chapter © 2020 1 Introduction Often enormous demands flood the data centres with HPC facilities for flexible computing environments. To meet the user’s requirements, the HPC system administrators adopt a lightweight process virtualization technique called container to deploy customizable computing environments while maximizing throughput. Widely available container-based HPC solutions, like, Shifter, UberCloud, Singularity, and cHPCe, fulfill the demands for cloud-like flexible HPC without sacrificing much on performance. On the other hand, the ever-increasing demand for HPC infrastructure results in massive power consumption. Environmental Protection Agency (EPA) [1] reported that the worldwide data centers consume 220-320 Tera Watt hours (TWh) of electricity in 2021, amounting to 0.9\\(-\\)1.3% of the world electricity demand. High power demand in data centers increases the overall operational costs and impacts the system’s performance in terms of availability and reliability. Therefore, the HPC community started paying attention to reducing power consumption in every aspect of the environment [2]. The entire HPC stack starting from hardware to operating system to middleware to application needs to be explored again from scratch, keeping power in mind, apart from other aspects of the environment. Moreover, optimizing application performance in a power-constrained environment is gaining popularity increasingly. In this respect, a fixed power-cap may not work well due to job-specific varying requirements. To alleviate this problem, research [3,4,5,6,7,8] has been carried out using either static code analysis or profile-guided power performance prediction approaches. The studied prediction models primarily use an additional system or application-level monitoring agent, which introduces significant overhead in a multi-tenant containerized environment. Performance predictability is an essential requirement for most applications in a power-constrained HPC environment. Most HPC applications are not designed and implemented, keeping power in mind. For this reason, the system architects must evaluate the power and performance in the early realization process based on the application requirements to ensure the efficient use of both resources and power. With the wide variety of possible system configurations and multi-dimensional resources, it is not easy to rapidly analyze application performance and power. The challenges increase significantly due to the unavailability of hardware-assisted methods to monitor container power consumption directly in the HPC environment. Substantial research [9,10,11,12] has been conducted to envisage the relationship between power using the performance monitoring counter and the regressive model to manage power consumption. However, more research is yet to be undertaken in containerized HPC space, especially for hardware-assisted real-time fine-grained software-based power monitoring and control methods without performance degradation. Previous research [13, 14] in this direction addressed the problem using either static workload characteristics or raw power/performance data from the hypervisor level to userspace, which incurs extra overhead to schedule the workload. They solved the problem either in standalone applications or Virtual Machine (VMs). To simplify power consumption measurement at the application thread level in a containerized environment, the DeepMon [15], proposed a power monitoring tool using unhalted_core_cycles performance counter correlation with power. It utilizes the ability to perform Just In Time (JIT) compilation in the recent advancement of the Berkeley Packet Filter (BPF) feature in Linux kernel 4.13 to develop a monitoring tool, which may not work in the legacy kernel used in the HPC environment. Similarly, WattsApp [16] also presented a software-based power prediction approach using WhatsUp.Net hardware to measure system-level power consumption and use resource usage statistics. However, using resource utilization statistics, the strategy provides limited insight into achieving fine-grain power estimation at the container level. 1.1 Motivation Most previous works used Docker as a container manager, which is not designed keeping HPC in mind and has several flaws to address [17]. Also, none of the previous works addressed the contention of shared resources like L3 cache, a memory bus, and memory controller for co-running threads present inherently in a multi-tenant containerized power-constrained HPC environment in real-time due to unpredicted resource usage pattern [3]. Although the previous contention aware studies [18, 19] for shared resources used the learning phase for co-located applications to conclude the interference factor, which is not feasible in the real-time job submission perspective. Herefore, it needs an accurate estimation of the power-performance behavior of an application container in real-time. Hence, our research motivation is two-fold. Firstly, to introduce cloud-like flexibility in a power-constrained environment. Secondly, to add a rapid hybrid power performance estimation approach across various hardware resources in a containerized power-constrained environment. Job scheduling middleware must also incorporate a power optimization strategy to utilize power resources optimally, along with resource contention awareness, without affecting the overall performance to bridge this gap. The proposed work in this paper addresses all these critical concerns. 1.2 Contribution The significant contributions of this work are as follows: A real-time hybrid power-performance estimation technique of an application executing inside a multi-tenant containerized environment is presented using a Machine Learning (ML) methodology. A novel power-cap determination framework based on the application’s computation phase and system resources’ usage is proposed to diminish resource contention by co-scheduling threads without degrading much on performance in terms of overall execution time. The power-performance tradeoff using the NPB and HPCC benchmarks is evaluated in our proposed pcHPCe framework with BareMetal environment and compared with the state-of-the-art solution. In this paper, we construct a weighted performance variation table to overcome manufacturing variability. After that, our power-performance estimation approach evaluates the performance of an application executing inside a container environment using an LSTM [20] model under the power budget constraints. The LSTM is a Recurrent Neural Network (RNN) and is proven helpful in predicting power-performance relationships at the software level based on a long history of multivariate feature sequences and accurately capturing future trends. Previous work [21] reported a model called cHPCe for analytical-based data locality and memory bandwidth contention-aware container placement and its performance analysis. This paper proposed a unique pcHPCe by extending cHPCe on power monitoring and fine-grained power management technique at the application container level. In summary, the proposed work in this paper extends the previous contribution [21] to conserve power efficiently in a multi-tenant containerized HPC environment and provides an opportunity to enhance the job scheduler to achieve enhanced power savings. 1.3 Organization The rest of this paper is organized as follows: Sect. 2 presents a detailed background of existing power-aware techniques in the field of HPC. Section 3 discusses the state-of-the-works in HPC. Section 4 describes the framework of the proposed pcHPCe model. Section 5 presents the preliminary results of memory-intensive benchmark applications. We present the details of the experimental setup in Sect. 5.1. Section 6 reports a summary and comparison of work. Section 7 concludes the work and outlines the future work. 2 Background Over the past few years, most researchers have focused on efficiently managing power without sacrificing much on performance. This section presents power conservation requirements in a cHPCe. It also covers the ML models studied so far for power-performance prediction. Our background study categorizes the discussion into two parts: Power-Constrained Container-based HPC and ML Models for Power-Performance Prediction. 2.1 Power-constrained container-based HPC Nowadays, HPC facilities are getting overwhelmed due to the dynamic requirements of users. The inherent overhead of hypervisor-based solutions to provide a flexible user environment puts a question mark on its use. The Linux container, a lightweight Operating System-level virtualization technology, provides an isolated userspace to run an application and embarks its presence to fulfill all the users’ needs giving comparable performance similar to the BareMetal system. In a virtualized/containerized environment, researchers track power and performance counter metrics by incorporating monitoring agents at the system level or application level. It may introduce significant overhead in a multi-tenant containerized environment. The multi-tenant environment has additional challenges due to its dynamic power consumption, contributing to resource sharing and workload heterogeneity. The challenges increase significantly due to the unavailability of hardware-assisted methods to monitor container power consumption directly in the HPC environment. In such a situation, contention for shared resources is also a limiting factor for overall performance due to the unpredictable use of resources among applications. A key concern in current-generation containerized HPC is the inherent performance degradation due to its resource interference and co-hosted applications [21]. 2.2 ML model for power-performance prediction Substantial research has been conducted to manage power consumption at a different level of the modern computing environment. In this regard, the performance monitoring counter and the regression models prove to be effective in envisaging the relationship between power performance. Earlier studies [9,10,11,12] on the ML algorithm already showed their effectiveness in predicting the trends in power and performance during application execution based on its characteristics. We envisage LSTM [20]—an alternative prediction model to the AutoRegressive Moving Average (ARMA) model—to present as a hybrid power consumption prediction model at the software level. Such RNNs can predict output based on a long multivariate feature sequence history and accurately capture future trends. The ML model is proven helpful in predicting power-performance relationships in our experiment. LSTM, with the rolling update mechanism, accurately predicts the relationships between the tail latencies in time-series power performance data. It updates the training sample sequences according to the currently predicted power consumption and duration. It considers job characteristics and memory usage to predict sporadic power surges due to a sudden large number of scientific computations across modules simultaneously. Section 4 provides the details of its working principle. Hardware performance counters monitor the occurrences of the hardware events without any performance penalty in modern systems. These counters can effectively estimate the power consumption of the different subsystems. Our experiment executes several micro-benchmarks to find the correlation between various performance metrics and power consumption. We examine the correlation of various performance metrics such as Instructions Per Cycle (IPC) excluding mis-speculated instructions, micro-ops retired per cycle, L1 cache misses, L2 hit rate, L3 total cache misses, loads and stores retired, branch mispredictions, average memory wait cycle to the measured power across few micro benchmarks. The correlation coefficient of IPC, uPC, L1 cache misses, L2 hit rate, L3 cache misses, loads and stores retired, and memory wait cycle is 0.6982, 0.5546, 0.5936, 0.5428, 0.6157, 0.1964, and 0.6256, respectively. We also find that mispredicted branch is strongly related to the derived parameter memory wait cycle. We select input features for the LSTM model based on the indicative correlation factor. We see a strong correlation between the performance counter consisting of IPC, L1 and L3 cache misses, memory wait cycle, and power consumption. These parameters are selected for the LSTM model. 3 Related work Initially, researchers in this field stepped into the power conservation of the HPC environment either by shutting down idle resources or finding idle time to put resources in a conservative power state. However, the significant contributions [2, 12] in this field are based on application power-performance profile characterization, which mainly falls into these categories: static code analysis-based solution, a prior knowledge-based solution, profile-guided variation lookup table-based solution, an analytical model and empirical analysis based solution, and workload dynamics using a derived benchmark methodology-based solution. Static code analysis-based solutions focus on power-performance tradeoffs using static code analysis, which fails to provide actual insight into power consumption during application execution in real time. Based on preset voltage-frequency knowledge settings, the prior knowledge-based solutions put a power-cap for a specific time interval during program execution, which may not fit well for a new upcoming application. The approaches in profile-guided variation and lookup table-based solutions address power consumption using variations in lookup tables using static profiles without considering feedback. It might happen that the new application in execution does not resemble the feature of the applications used for generating the power-performance profile. Our proposed approach reassembles with workload dynamics using a derived benchmark methodology-based solution. 3.1 Workload dynamics using derived benchmark methodology-based solution Karpowicz et al. [22] proposed a customized energy-aware controller to accurately estimate CPU power consumption from the model-specific register. It built CPU workload dynamics using a derived benchmark methodology to adjust CPU frequency for the application-specific workload patterns dynamically. HaPPy [13] and XeMPower [14] models addressed the problem using either by knowing static workload characteristics or raw power/performance data from hypervisor level to userspace, which incurred extra overhead to schedule the workload. They solved the problem in standalone applications or VMs. Whereas Brondolin et al. [15] also proposed a monitoring tool to measure power consumption at the application and thread-level considering power-performance tradeoff. The model evaluated the power-performance tradeoff of a container-based distributed environment managed by Kubernetes [23]. It measured power consumption at the application thread level using weighted unhalted clock cycles correlated with power during application execution. They leveraged JIT compilation in the recent advancement of the BPF feature in Linux kernel 4.13 to develop a monitoring tool, which may not work in the legacy kernel used in the HPC environment. WattsApp [16] also presented a software-based power prediction approach using WhatsUp.Net hardware to measure system-level power consumption and resource usage statistics. It maintained power-cap using container migration or deallocating resources to the container. Nevertheless, using resource utilization statistics, the strategy provides limited insight into achieving fine-grained power estimation at the container level. Holding the same standpoint, Fieni et al. [24] proposed a software-defined power meter, SmartWatts, to estimate power consumption at the granularity of the software container. They used unhalted clock cycle and last-level cache misses for package and Dynamic random-access memory (DRAM) power consumption estimation, respectively, along with hardware power monitoring sensors and Intel’s Running Average Power Limit (RAPL). Enes et al. [25] also proposed a similar power consumption prediction technique for cluster environment using Docker container runtime and cgroup power-capping technique to throttle CPU share, thereby limiting energy consumption. Table 1 depicts the summary of the state-of-the-art contributions in the power-aware containerization for the HPC environment. Table 1 Summary of Related Work on power-aware containerized HPC Full size table However, most previous works used Docker as a container manager, which needs to be designed keeping HPC in mind. In the broader perspective, the Docker containerized environment has several flaws that must be addressed before being used in HPC [17]. Very few works have been reported for power conservation in a containerized multi-tenant environment designed from the ground up for HPC purposes. Also, like most previous work on the BareMetal, we find a strong correlation between performance counter and power, consisting of IPC, L3 data cache misses, and the number of references to the last level cache. Most of the above approaches rely on a few system parameters and lack an accurate real-time estimation of an application’s power-performance behavior. Moreover, none of the previous works addressed the contention of shared resources like an L3 cache, a memory bus, and a controller for co-running threads present inherently in a multi-tenant containerized power-constraint HPC environment. Previous literature [21] presented a resource contention-aware container placement strategy for containerized multi-tenant HPC environment. However, it lacks an energy awareness perspective. Our work addresses all such concerns with a novel power-cap determination framework with resource contention awareness based on the application’s computation phase and system resources’ usage at the thread level without sacrificing much on performance. 4 Containerized power-aware HPC The dynamic sharing of hardware resources in a multi-tenant HPC environment leads to complex per-task power-performance relations due to concurrent task execution with varying power consumption among resources. Users demand a flexible HPC environment like a Cloud to fulfill their execution environment needs similar to the home/native environment. The cHPCe: a container-based HPC environment using x86 and OpenPOWER systems [17] meets the users’ requirements without sacrificing much on performance compared to the BareMetal environment. In this environment, a group of processes is isolated from another group of processes that constitute an individual application that executes on the available hardware thread densely. Hence, predicting the fine-grained power consumption in such an environment introduces an extra layer of complexity. 4.1 Calibration to mitigate manufacturing variability Earlier, researchers in the HPC field felt that processors on a single die from a particular manufacturer behave similarly to other processors. However, recent research [27] shows that some processors are less power efficient due to semiconductor process variations, even with the exact architectural specification. This power variation among processors leads to performance variation even in perfectly balanced HPC applications. In a power-constraint environment, this manufacturing uncertainty will incur performance invariability where we enforce the hardware to power-cap to conserve the power. To alleviate this uncertainty, we calibrate the power-cap value for each CPU like the previous study [7, 27]. However, we use a few widely accepted representative benchmarks that stress different subsystems, such as CPU, DRAM, and IO of an environment than the earlier study with a single micro-benchmark. This hybrid benchmark strategy describes the power-performance behavior of the system more accurately. The proposed pseudo-code to overcome manufacturing variability is given in Algorithm 1. The calibration for manufacturing variability model selects each representative micro benchmark to execute on different sockets individually with different power-cap settings. It collects the power and performance of each execution against each power-cap and store it in the database. The average performance of all benchmarks is calculated using 20 times repeated execution samples. In our experiment, we use a collection of representative micro benchmarks that exhibit compute, memory, and IO boundness to capture the power characteristics of CPU and DRAM with minimal prediction error. We maintain the power-performance variation table against the CPU socket and power-cap for each benchmark. The job scheduling module utilizes the suitable power-performance variation table entry while putting power-cap constraints. The list of notations used throughout the paper is given in Table  2. Table 2 List of notations Full size table The power-performance variability impacts CPU power consumption significantly, making it extremely difficult to predict application-level power performance across different computing elements. We also spot that performance variation worsens with a strictly enforced power-cap. Our proposed algorithm helps to mitigate performance inhomogeneity in a power constraint containerized HPC environment. Our solution uses representative benchmarks to estimate performance variability, which may not have online application characteristics in a remote scenario. An approach to deal with such a scenario is to refine the Performance Variation Table (PVT) as and when a new application enters dynamically. 4.2 Predicting power-performance in a cHPCe Very few research has been reported to date for a power-aware containerized HPC environment. Considering the imperfection and shortcomings of the existing implementations, we propose a prototype of a dynamic pcHPCe in Fig. 1. We inherit the cHPCe model [17] with power-aware capability. Several modules of the proposed model are discussed here. 4.2.1 User environment In the proposed pcHPCe model, users prepare the runtime image for container instantiation describing applications’ requirements. Users push the image(s) to the private/public repository to be used later for creating a containerized environment for their application execution. 4.2.2 User interface Users log into the job submission site using their credentials and submit the job describing all the QoS requirements, mainly the deadlines to complete the job and runtime images to be used for container instantiation. 4.2.3 Orchestration The job scheduler maintains a queue of jobs and the total allotted power budget for the environment. We modify the widely accepted Simple Linux Utility for Resource Management (SLURM) [28] job scheduler to determine the CPU topology accurately. It pins a particular migrated application thread to the hardware thread in case of colocated applications’ threads interference beyond an acceptable threshold by considering predicted performance interference using the average memory wait cycle. The job scheduler invokes the container manager to create a containerized environment for an application and maintains the currently available power budget for the environment. It also gives the predicted power-cap to the container manager to throttle a particular package and DRAM power corresponding to the specific job. 4.2.4 Decision In the proposed pcHPCe model, the decision module mainly comprises two components: the container manager and the power-performance prediction model. It accurately predicts the power-performance relationship of an application container running on a system and makes decisions for the imposed power-cap on the CPU and DRAM. 4.2.5 Container manager The primary function of the container manager is to facilitate the container runtime environment by invoking the cHPCe daemon. It receives a list of selected compute nodes and a power-cap at runtime from the job scheduler. It calls the performance and power daemon in the compute node to capture instantaneous performance and power consumption in real-time during application execution. Fig. 1 High-level prototype for pcHPCe Full size image 4.3 Power-performance prediction agent A power-performance prediction model envisages the power-performance relationship using a multi-step, multi-lag, multi-variate LSTM with fine-tuning in real-time. It receives the accumulated performance metrics and instantaneous consumed power from a shared location to be input to the model. We train the LSTM model with performance metrics and the consumed power of several representative benchmarks to stress different subcomponents of the computing system. Our model uses hardware events and consumed power to predict power-performance relation at thread level mapped to a container to provide a fine-grain software-level solution. Figure  2 presents the detailed working principle of the real-time power-performance prediction model backed by LSTM of the proposed pcHPCe, as shown in Figure  1. The traditional LSTM model is unable to predict large power swings. Therefore, a standard decomposition-based prediction approach can not apply. The Rolling update optimization mechanism is utilized with LSTM to update the training sample sequences according to the current predicted power consumption and duration. It considers job characteristics and memory usage patterns to predict sporadic power surges due to a sudden large number of scientific computations across modules simultaneously. The pseudo-code to predict the real-time power performance of an application is given in Algorithm 2. The real-time power-performance prediction algorithm consists of a hybrid offline and online approach to improve the convergence speed of the learning phase with optimal prediction accuracy. It first evaluates the execution time of all the representative micro-benchmarks stressing different system sub-components with the optimal number of CPUs using no power cap and stored in the database. We calculate the average performance of all benchmarks using 20 times repeated execution samples. Various power-cap starting with minimum power-cap value or the training samples are created using different applied power-caps, corresponding consumed power, and Performance Monitoring Counters (PMCs) applied power-caps, along with corresponding consumed power and PMCs, aid in creating the training samples. The algorithm converts time series profile data into supervised learning model data and partitions the data into train and test sets. The model further takes feedback measurements when a new job comes to improve long-term contextual time series information accuracy. The training process stops when Root Mean Square Error (RMSE) is below a certain threshold. This trained model predicts performance for a given power-cap value in real-time during application execution. Fig. 2 Working principle of LSTM Full size image Using a stochastic approach, our proposed multi-variate LSTM-based real-time power-performance prediction model accurately estimates input and output relationships. It utilizes monitored present and past hardware event matrices and consumed power at the application container thread level. The proposed method is application agnostic and estimates the power consumption much faster than the measurement sample without tapping the internal system power line. The model’s convergence and prediction accuracy can be further improved by investigating different model topologies, dropout layers, and optimizers associated with varying hidden and unit layers. 4.4 Compute node The compute node mainly covers performance and power daemons, and cHPCe, where the job executes inside a containerized environment. A bridge network connects all the compute nodes using a dedicated network channel. 4.4.1 Performance daemon It is a very lightweight daemon to capture performance-related metrics in each compute node to obtain the runtime performance metrics related to all jobs executing on the node. We use the Performance Application Programming Interface (PAPI) [29], a cross-platform interface due to the support of Graphics Processing Unit (GPU) and other component plug-ins for future use, to get available PAPI preset and user-defined events of interest. To monitor the hardware performance metrics of a CPU Socket housing multiple threads, we maintain a thread for accumulating performance counter values of all threads using a shared memory concept for performance reasons. These real-time consolidated performance metrics with instantaneous power consumption corresponding to each package are given to the power-performance prediction agent. 4.4.2 Power Daemon Power daemon resides in each compute node to capture the instantaneous power consumption at the package and DRAM level in millisecond intervals. We utilize RAPL [30] interface by reading values from the sysfs power-cap interface. It also puts the power-cap value received from the container manager by writing power-limiting interface files of the Kernel at package and DRAM level for the predicted time interval. The dedicated threads driven by signal carry out all such activities for non-interrupted service by pinning to a particular hardware thread. 4.4.3 cHPCe The cHPCe daemon activates whenever a new job request comes from the container manager. It creates a container environment by using the image from the shared storage and forms a bridge-based overlay network among the containers for a specific application. 4.5 Power-cap selection in job execution To conserve power in a power-constraint multi-tenant cHPCe, we need an efficient fine-tune power-cap selection algorithm during application execution. The cHPCe needs to deal with various user applications with unique characteristics. A universal power-cap selection strategy may not work well for all applications in such a situation. To date, most researchers emphasize the power conservation of a single node executing one similar job at a given time interval. However, we propose a power-cap determination methodology based on the applications’ computation phase and system resource usage without degrading much performance in a multi-tenant containerized HPC environment. The pseudo-code of selecting the power-cap of an application in real-time is given in Algorithm 2. The power-cap selection module calculates the colocated thread interference factor using Eq. (4) and (5) supported by the analytical interference model [21]. In our proposed pcHPCe framework, the job scheduler migrates a thread when the colocated threads’ interference exceeds the empirically determined threshold. It employs the Checkpoint/Restore In Userspace (CRIU) thread migration strategy, an approach to save thread context, transfer, and restore thread context in the migrated host without violating power budget constraints. Otherwise, the predicted socket power is adjusted to accommodate the impact on energy consumption due to the change in memory usage pattern, determined from the regression model. The job scheduler calculates the final predicted power taking manufacturing variability into account. It also determines the power-cap duration from applications’ phase behavior and time interval. The power-cap daemon receives the final predicted power-cap and duration for the application container from the job scheduler maintaining power budget constraint. Our model presents the impact on energy consumption due to colocated application container threads. In this context, the job scheduler tries to assign all the threads belonging to a particular application container to a single socket/die as densely as possible to achieve acceptable application performance by minimizing contention of the shared resource and suitable power-cap enforcement. The individual benchmark application runs on demanded resources to collect individual resource pressure in a contention-free environment. Then it is executed with different co-hosted application benchmarks to observe performance degradation. Our empirical analysis shows a strong correlation between the memory wait cycles and energy consumption of the processor socket due to the wait state causing a slowdown of the clock. The penalty of the last level cache misses, which causes the memory wait cycle to introduce in the execution path, can reach up to the typical 100 cycles in the modern processor. More memory access delays indicate less processor power consumption. Idling the processor during memory access time results in lower dynamic processor power use, thus leading to a lower average system power usage. The memory wait cycle depends on phase behavior and the instruction executed for all the co-hosted application containers. We utilize an analytical model [21] backed by the empirical study to determine the resource interference characteristics from the average memory wait cycle by co-hosted applications. However, the contributing factor to the average memory wait cycle due to different memory types is left unexplored. The future work also targets to address the contention due to communication. Using a regression model, the mentioned numbers in Algorithm 3 are obtained from an empirical study like based on the change in memory usage impact on energy consumption. We perceive that creating the checkpoint for thread migration, saving it into faster storage, and restoring it to the destination node/socket takes an order of tens of milliseconds, as studied in previous literature [16, 26]. Our empirical study using a regression model for colocated threads interference follows a similar trend as derived below: The total power consumption of the processing core of a socket, \\(P_{s}\\) is calculated using $$\\begin{aligned} \\begin{aligned} P_{s} = P_{c} + P_{m} \\end{aligned} \\end{aligned}$$ (1) where \\(P_{c}\\) is the computational power consumption with data residing in register/cache, \\(P_{m}\\) represents the computation power requirement for retrieving/storing data from/to memory. Each socket has a private L1 and L2 cache and access to a shared L3 cache and memory. The computational power consumption due to memory access can be calculated by adopting the equation $$\\begin{aligned} \\begin{aligned} P_{m} = \\sum _{i=1}^{T} \\sum _{j=1}^{d} \\sum _{k=1}^{m} S^{i}_{j} * u^{j}_{ik} * p_{ik} \\end{aligned} \\end{aligned}$$ (2) where T, d, and m are threads, data access from memory, and memory partition. I, j, and k are used for the corresponding indices. \\(p_{ik}\\) denotes the computational power consumed when a unit of data is accessed from \\(m_{ik}\\), \\(S^{i}_{j}\\) represents jth data size retrieved/ stored by an ith thread, \\(u^{j}_{ik}\\) maintains unity when data is available in the memory, \\(m_{k}\\) otherwise 0. The \\(p_{ik}\\) can be formulated by applying the equation $$\\begin{aligned} \\begin{aligned} p_{ik} = intrfnc^{sckt}_{CoLocThr_{i}} * PowToAcessMem * T_{i} \\end{aligned} \\end{aligned}$$ (3) where \\(intrfnc^{sckt}_{CoLocThr_{i}}\\) indicates the colocated thread interference induced by an \\(i_{th}\\) thread in the socket. PowToAcessMem denotes power consumption to access memory. \\(T_{i}\\) is the total execution time of the ith thread. Consider a set of n threads in a particular socket. \\(\\Psi (TchngMemWaitCycl)\\) calculates the total change in memory wait cycles for n threads in a specified time frame, t due to the change in memory wait cycles of ith thread. $$\\begin{aligned} \\begin{aligned}&\\left. \\Psi (TchngMemWaitCycl) = \\right. \\\\&\\quad \\sum _{i=1}^{n} \\frac{\\delta (ChngMemWaitCycl_{i}) (LLCMissRate^{*}_{i})}{\\delta (t)} \\end{aligned} \\end{aligned}$$ (4) Where \\(LLCMissRate^{*}_{i}\\) represents the last level cache miss rate of ith thread over t time interval of n threads. Finally, \\(intrfnc^{sckt}_{CoLocThr_{i}}\\) can be calculated in a specified time interval, t using equation $$\\begin{aligned} \\begin{aligned} intrfnc^{sckt}_{CoLocThr_{i}} = \\frac{(\\Psi (TchngMemWaitCycl))}{(\\delta (t))} \\end{aligned} \\end{aligned}$$ (5) In the final power-cap selection given n modules and their associated module power, the job scheduler maintains the following invariability (ignoring the static part of power consumption, which remains constant throughout application execution) $$\\begin{aligned} \\begin{aligned}&Pow_{cap, module} = Pow_{cap, CPU} + Pow_{cap, DRAM}\\\\&Pow_{budget} \\le \\sum _{j=1}^{n} Pow_{cap, module}^{j}, and PowCap_{module}^{min} \\end{aligned} \\end{aligned}$$ (6) Algorithm 4 describes the detailed procedure of selecting power-cap by the job scheduler. The selected power-cap scheduling module first calculates available power for the ith module by subtracting the total consumed power of all modules from the power budget at the time of the application container scheduling instance. If the available power for the ith module is greater than the final predicted CPU and DRAM power-cap for the application container, then allocate the selected power-cap for CPU and DRAM for the predicted duration. Otherwise, calculate the available power-cap for the CPU by subtracting the final predicted DRAM power from the available power for the ith module. If the performance variation due to the selected power-cap for CPU and DRAM is within the constraint of QoS requirement, then allocate the selected power-cap for CPU and DRAM for the predicted duration. The database also stores information regarding the module running with a throttle power-cap, which shall be appropriately adjusted in the next scheduling interval. If the performance variation cannot fulfill the required QoS requirement at that time instance, allocate the application container with minimum CPU and DRAM, and wait for the next power-cap determination slot. Our proposed power-cap selection scheme enforces the desired power-cap based on the applications’ computation phase and system resources usage while maintaining power budget constraints. This process utilizes colocated thread interference with applications’ computation phase to enforce a power-capping limit, unlike other methods in literature mainly based on power budget constraint violation. Our proposed pcHPCe solution applies a reduced power-cap to maintain the total power consumption by all running containers within the power budget constraint with tolerable performance degradation. It is a lightweight process compared to the presently available methods, such as deallocating resources from containers or the restore-checkpoint technique used in practice. In the power-cap selection scheme, contention due to communication interference is left unexplored. 5 Performance evaluations This section presents the results obtained from experiments conducted on the proposed pcHPCe. We evaluate the performance of our proposed environment on a cluster of x86 nodes allotted for our analysis in the HPC Lab. Since power usage by system components other than the CPU package and DRAM, such as Disk, motherboard, and NIC, is almost static, we calculate the total available power budget for distribution by subtracting this offset power from absolute power. Our investigation does not consider power consumption by the cooling system and power source because they are machine-dependent, typically constant, and outside of the scope of this paper. 5.1 Experimental testbed In this section, we summarize the experimental environment, as shown in Table 3. The smart manage 10-Gigabit network switch interconnects all the nodes in the environment. We deploy a cHPCe comprising x86 systems. The cHPCe uses the Linux kernel feature, namely namespace, as in previous work [17]. In this work, we enhance the widely popular workload manager, SLURM, to detect the used x86 processor architecture and pin a particular application thread to a hardware thread. SLURM is an open-source, highly configurable workload manager and allows modifying the scheduler to introduce the power awareness concept. The power-performance monitoring ability of the environment is also included using a plug-in to it. The programming interface PAPI helps us track our interest’s application performance metrics using preset and user-defined events. We implement a performance daemon, which actively monitors the performance of each thread on a socket specific to a job. We also build a power daemon that monitors and controls the power-cap for a compute node. The RAPL assists us in monitoring instantaneously consumed package and DRAM power in real-time. Table 3 System Characteristics Full size table 5.2 Overhead analysis Our pcHPCe introduces some overhead in compute nodes due to monitoring and controlling power using a dedicated daemon process. The power monitoring daemon thread executes on a dedicated hardware thread to capture instantaneous power consumption at package and DRAM levels in a one-millisecond interval. The captured power consumption information is written to fast shared storage, which the power-performance prediction agent utilizes. A pre-assigned thread is executed after a predicted time interval by a signaling mechanism to control the power-cap by writing the power interface file of the Kernel. The communication latency of reading and controlling power by the daemon process is nearly negligible due to fast shared storage among nodes and time-driven signaling mechanisms. To assess overhead due to the dedicated daemon process, we collected the execution time of several benchmarks with and without the daemon process. Furthermore, the experiments present how much additional load the proposed framework introduces to the BareMetal environment. The performance of all the benchmarks executing 20 times repeatedly with a \\(95\\%\\) confidence level is reported. The computing overhead of power monitoring and controlling daemon is about \\(1.6\\%\\) of the total computing power per node, as shown in Fig. 3, which is insignificant, considering power-saving and overall negligible performance degradation, as discussed in Section  5.5. For the FT benchmark, due to the identical iteration of spikes and valleys with regular patterns, which are interleaved with different computation phases, monitoring and controlling daemon takes \\(0.6\\%\\) more computing power than other benchmarks. It consumes \\(0.5\\%\\) less computing power due to the compute-intensive nature of LU with uniform patterns. Fig. 3 Execution time overhead Full size image 5.3 Power-performance variation and calibration In this section, we present CPU power and performance measurement across all the nodes at the thread to socket level in detail. We measure the module-level power variation and its impact on the performance of different benchmark applications such as MT-DGEMM, STREAM, GRAPH500, b_eff, and NPB3.3-MPI in different power-cap scenarios. Each socket of computes node executes every benchmark application separately with varying power-cap conditions, from 40 watts up to 130 watts. The y-axis represents the normalized execution time by considering no cap power condition, whereas the x-axis reports the module-level power consumption in watts. The power-performance (P-P) variation result has been reported for individual benchmark applications with performance normalized to no power-cap condition. In our experiments, median values of the probability density distribution of the obtained results over 20 repetitions are considered to plot the graphs. To reduce space usage, we only show the representative benchmark result plot (the detailed results are not demonstrated in this paper). 5.3.1 Stream benchmark A simple synthetic benchmark, Stream measures the sustainable memory bandwidth of the computing system. It mainly comprises four computation kernels, such as ‘COPY’ to assess transfer rate without arithmetic operation, ‘SCALE’ to perform a simple arithmetic operation, ‘SUM’ to check multiple load and store operations, and ‘TRIAD’ to allow overlapped/ chained/fused add/ multiply operations. It calculates the effective memory bandwidth using Equation (7). $$\\begin{aligned} \\begin{aligned}&\\left. COPY: c = a, \\right. \\\\&SCALE: b = \\alpha c, \\\\&ADD: c = a + b, \\\\&TRIAD: a = b + \\alpha c.\\\\&where \\ a, \\ b, \\ c \\ are \\ vectors \\ and \\ \\alpha \\ is \\ a \\ scalar. \\end{aligned} \\end{aligned}$$ (7) Fig. 4 P-P variation in stream benchmark Full size image We see from the Stream benchmark experiment in Fig. 4 that the benchmark performance degrades if we apply a power-cap less than 70 watts to any package. We do not observe any additional benefits by putting a power-cap of more than 80 watts. Therefore, power-cap variations indicate performance invariability for a value less than 60 watts with a worst-case performance variation of 0.05 to 0.21 relative to the no power-cap scenario. We also witness that in socket0, all computing systems give the same power-performance invariability, whereas in socket1, all systems show similar behavior. 5.3.2 Graph500 This benchmark stresses the communication subsystem. It mainly consists of three computation kernels. The first kernel forms an undirected graph, the second performs a Breadth-First Search (BFS), and the third kernel constructs all single-source shortest path computations on the chart. Figure 5a shows that reducing the CPU power-cap value from 70 watts to 50 watts increases the worst-case performance variation of 0.14 to 0.16 relative to the no power-cap scenario, indicating a 16% difference in CPU frequencies across packages at 50 watts. We also notice almost no performance variation across the same CPU packages with higher power-cap values. A worst-case performance variation of 0.03 relative to the no power-cap scenario is seen. Fig. 5 P-P variation in a Graph500. b mt-DGEMM Full size image 5.3.3 mt-DGEMM A simple multithreaded dense matrix multiplication benchmark, mt-DGEMM, captures the sustainable floating-point computational rate of double precision matrix-matrix multiplication of a single node. It maintains the following computational kernel $$\\begin{aligned} \\begin{aligned} C = \\alpha AB + \\beta C; \\end{aligned} \\end{aligned}$$ (8) where A, B, and C are matrices of the dimensions \\(M \\times K, K \\times N\\), and \\(M \\times N\\), respectively. Figure 5b shows that reducing the CPU power-cap value from 70 watts to 50 watts increases the worst-case performance variation of 0.08 to 0.11 relative to the no power-cap scenario, indicating an 11% difference in CPU frequencies across packages at 50 watts. We notice almost no performance variation across the same CPU packages. 5.3.4 NAS parallel benchmark We evaluate the performance of a highly parallel supercomputer using the NAS parallel benchmark called NPB3.3-MPI. It comprises five kernels and three pseudo applications to assess the computation and data movement characteristics of the computational fluid dynamics (CFD) applications. The computation kernel, Embarrassingly Parallel (EP), generates independent Gaussian Random variates using Marsalia polar methods. Conjugate Gradient (CG) focuses on random memory access and communication. It also contains a 3D discrete fast Furious Transform (FT) for all to all communications. Block Tri-diagonal solver (BT), Scalar Penta-diagonal solver (SP), and Lower-Upper Gauss-Seidel solver (LU) are three pseudo applications in the benchmark. We note from the result that reducing the CPU power-cap value from 70 watts to 50 watts increases the worst-case performance variation of 0.047 to 0.048 for the SP benchmark, relative to the no power-cap scenario, indicating a maximum of 6.4% difference in CPU frequencies across packages at 50 watts. We also perceive almost no performance variation across the same CPU packages with higher power-cap values, whereas the worst-case performance variation of 0.028 when compared to the no power-cap scenario. We notice the power variation across and within the package in the experiments, leading to performance variation. We also notice that sometimes these power variations do not necessarily correlate with performance variation. This performance variation becomes more prominent when we put the power-cap. Hence, the proposed pcHPCe framework considers performance variation while putting power-cap to conserve power. Our experimental results conclude that a single power-performance variation relation may not fit all the application characteristics equally. Hence to deal with different features of applications, we generate different PVT, as shown in Table 4, by considering different categories of benchmarks and using corresponding PVT. Table 4 Performance variation table (PVT) Full size table 5.4 Power-performance prediction model evaluation In this sub-section, we evaluate the performance of the implemented power-performance prediction model. We utilize a multi-step, multi-lag, multi-variate LSTM model with fine-tuning to predict instantaneous power-performance relations in real time. We run Stream, mt-DGEMM, b_eff, Graph500, and HPL benchmarks inside a cHPCe twenty times each to collect repeat power-performance profiles. The power- and performance-daemons capture all the performance-related metrics such as TOT_CYC, L1_DCM, L3_TCM, MEM_WCY, instantaneous package, and DRAM power consumption during execution. The sampling interval of all the selected parameters is 1 ms, captured by power and performance daemon. These daemons consolidate the power-performance metrics and write them into a shared location for input to the power-performance prediction model. Our LSTM model converts time series profile data into supervised learning model data and partitions the data into train and test sets. In this step, we evaluate the model accuracy starting from one-to-twenty millisecond past profile data to predict one-to-two millisecond future data. The LSTM network is built after transforming the data into the LSTM format. In multiple and dropout core layers, we use neurons of sizes 75, 50, and 30 in the model building. The proposed model utilizes the Adam optimizer and Mean Absolute Loss Function to measure the magnitude of errors. First, the repeated profile data of each benchmark train the model, and further training applies to the saved model with another set of benchmark profile data. The model’s accuracy is evaluated with 10-fold cross-validation. Our final power-performance model achieves a validation accuracy of approximately 91.39%. When using the LSTM with a window size of 10 and offset size of 2, the Mean Absolute Percentage Error (MAPE) and the RMSE are approximately 2%\\(-\\)3.29% and 8.06%, respectively. Figure 6 shows the training data and the corresponding validation accuracy. Our model takes past performance and power data for twenty milliseconds and predicts two-milliseconds power performance with an approximate accuracy of 91.39%. Fig. 6 Typical LSTM a input runtime performance metrics. b Power-performance prediction Full size image 5.5 Power-cap selection algorithm evaluation This subsection presents the power-cap evaluation results for the NAS parallel and HPCC benchmarks. The CPU’s Thermal Design Power (TDP) is 95 watts and 20 watts for the DRAM in each system. We find these from the PACKAGE_ENERGY_PACKAGE and DRAM_ENERGY_PACKAGE model-specific Registers (MSR). Each node with two CPU sockets consumes 230 watts. The experimental environment is allotted a power budget of 1.65kW, including all system subcomponents. Assume that other components of the system draw constant power for our experiment. Initially, the job scheduler allocates a uniform power budget across all the sockets. The job scheduler dynamically divides the total power budget to all the containers following the proposed strategy according to their aggregated energy requirements. We calculate the total power budget allocated to each job by multiplying the number of CPUs with the power-cap assigned to each CPU which can satisfy the performance requirement of the job. 5.5.1 NAS parallel benchmark We select BT and LU of the NAS parallel benchmark to evaluate the performance of the proposed Power-cap selection algorithm. Due to space constraints, we show the representative result plot only. We also analyze the performance of these two pseudo applications relative to BareMetal performance. The implemented power-performance prediction model receives the performance metrics and power consumption from the performance and power daemons, respectively. It can accurately predict power performance with high accuracy. However, we train the prediction model in a stacked manner with all the representative benchmarks to stress different system subcomponents. The expected power does not reflect the interference caused by the colocated applications. Hence, we utilize colocated thread interference using the average memory wait cycle with a threshold from empirical study, phase behavior of the application, predicted power to determine the power-cap, and duration for the next time interval of applications execution. Figure 7a presents the behavior of Memory Wait Cycles (MWC) to socket power consumption in a typical LU application benchmark execution using a different package power-cap. Figure 7b represents typical colocated threads interference in clusterhost1. Figures  8,  9, and  10 demonstrate the evaluated power-cap for the next time interval of applications; LU and BT using Power-cap selection algorithm. The execution duration of all the benchmarks is in the order of minutes. However, we zoom all the result plots to better visualize the effectiveness of our proposed pcHPCe framework due to the millisecond scale power-cap imposed. Fig. 7 a MCW vs. socket power consumption. b Colocated threads interference in clusterhost1 Full size image Fig. 8 Power-cap selection a Host1 Socket1. b Host2 Socket0 for BT Full size image Fig. 9 Phase behavior of LU benchmark Full size image Fig. 10 Power-cap selection a Host1 Socket0. b Host2 Socket1 for LU Full size image In the case of the bt application benchmark, for example, \\(k_{th}\\) execution time instance, the power-performance prediction model predicts the package power to be 50.537 watts. We study empirically that colocated application interference during execution can be estimated by average global memory wait cycles. We also observe that an increase of 1500 average global memory wait cycles decreases package power consumption by approximately 1.528 watts at the beginning of the subsequent application phase. Hence, the total power-cap decided by the container manager is (50.537—4.584)/1.065 = 43.15 watts after incorporating manufacturing invariability. The job scheduler evaluates the power-cap for the socket according to the current power budget availability and QoS requirements of maintaining applications. The application phase behavior decides the power-cap duration based on IPC and L3 average Total Cache Miss (TCM) because giant cache misses usually indicate CPU is in a non-compute intensive phase. We retain the DRAM power-cap in a coarse-grained manner due to its frequent change that may introduce unpredictable performance behavior. To maintain power consumption constraints, the container manager finally sends these power-cap values and time duration to the power daemon in the compute node. Figure  8 presents the power-cap selection for BT benchmark, and Fig. 10 for LU benchmark using proposed power-cap determination framework based on the application’s computation phase and system’s resources usage to diminish resource contention by co-scheduling container threads. Before running the benchmark, the power-cap selection agent enforces the minimum possible power-cap in an idle state due to the running power and performance daemons. Once the application is submitted to execute, the orchestrator imposes a strict power-cap with target performance and power budget constraints. Figures  8 and 10 depict the achieved significant power-cap selection accuracy. We analyze behavior of colocated threads and power consumption on clusterhost1, socket0 and socket1, respectively, for LU and BT benchmarks. We notice that CPU and DRAM utilization increases despite the decrease in power consumption. It is due to the strict enforcement of power-cap with performance degradation limit using the MSR register. Of course, the strict enforcement of the power-cap does not change the application’s overall performance with a slight increase in execution time. Figures  8 and  10 show that the orchestrator starts with the minimum possible power-cap since the system is in idle state. Then, it captures the application’s behavior and allocates the power budget to run its maximum. We note that BT consumes more DRAM power than others, which leaves the orchestrator less opportunity to save power since strict power-cap in DRAM may have an adverse effect. Moreover, the power-cap selection agent exploits core power for LU applications opportunistically, resulting in power saving with minimal performance loss. However, there are instances where our model shows apparent deviation from real power consumption-for example, 2 s and 17 s of BT benchmark and 7 s in LU benchmark in clusterhost2. Our model takes average power during this time range while actual power consumption oscillates in nature. By analyzing the IPC pattern, we conclude that relative IPC was high around this time frame, contributing to this deviation. 5.5.2 High performance computing challenge benchmark HPC Challenge benchmark [31] examines different subsystems of the participating nodes, such as processor, memory, and interconnect, to evaluate the performance of the environments made of BareMetals and containers. It consists of seven benchmarks: HPL, DGEMM, FFT, PTRANS, STREAM, RandomAccess, and b_eff Latency/Bandwidth. To fulfill the benchmark evaluation criteria, we use the largest problem size that occupies 70%, which is around 90GB, of the total memory for all the experiments. The block size and problem size are of the order of 100 and 100000, respectively. The process grid ratios of 1: 4, 1: 8, 1: 16, 1: 24, 1: 32, 1: 48, 1: 56, and 1: 64 are input to assign to the hardware thread. We use the HPCC benchmark to provide performance bound for real applications with its unique power consumption profile and power-cap selection exercise by our proposed scheme. All the plots present the different parts of the benchmark execution on nodes. 5.5.3 HPL and StarDGEMM Figures  11 and  12 show the performance, power consumption, and power-cap selection by the proposed scheme for HPL and Star DGEMM benchmarks, respectively. HPL and Star DGEMM show a high spatial and temporal locality. They spend most of the time exercising the processor’s floating-point unit and little time on memory. The processor consumes more power compared to other tests. HPL solves a dense linear system of equations using LU factorization with a partial row pivoting method interleaved with short communication. We discover very high variation in processors’ power consumption, reaching up to 150 watts, and it can drop as low as 70 watts during short communication. We do not notice much fluctuation in memory power consumption due to high spatial and temporal memory locality, and it does not incur cache misses frequently. Hence, our proposed scheme correctly identifies the application computation phase and optimized power consumption by selecting the power-cap accordingly. Fig. 11 a Performance. b Power-cap selection for HPL Full size image Fig. 12 a Performance. b Power-cap selection for StarDGEMM Full size image 5.5.4 PTRANS PTRANS investigate the system interconnects total communication capacity by exchanging messages simultaneously between each pair of processors. Figure  13 presents the performance, power profile, and power-cap selection by the proposed scheme. A matrix of size 50000 × 50000 processes is taken as an input. PTRANS power profile consists of spikes in the computation phase and valleys in the systemwide significant data movement phase. Power consumption varies within the iteration also. We observe stable memory power consumption due to high spatial memory locality and relatively low cache misses. Our scheme correctly identifies (i) sufficiently long communication phases that reduce power consumption and (ii) data movement-dependent computation phases that perform matrix transposition resulting in a lowering of power consumption. We conduct experiments to estimate the performance based on injection rate in the number of messages/second. The injection bandwidth capability into the network fabric is 3.2GBs for our experiments. We conduct the experiments by varying levels of injection bandwidth degradation like half, quarter, and eighth in terms of the complete compute node’s injection bandwidth when the message size is 16 MB. The effect of injection bandwidth degradation, while applications send many messages simultaneously, is noticeable. Figure  14 plots the achieved network injection bandwidth to compute the balance ratio v/s injected bandwidth degradation. The performance penalties do not grow with scale. Fig. 13 a Performance. b Power-cap selection for PTRANS Full size image Fig. 14 PTRANS: Injection bandwidth to compute balance ratio against injected bandwidth degradation Full size image 5.5.5 FFT This benchmark measures the floating-point rate of execution of double-precision complex one-dimensional Discrete Fourier Transform (DFT) of size m. It captures inter-process communication using large messages. Figure  15 shows the performance, power consumption, and power-cap selection by the proposed scheme. MPIFFT power profile shows the trend to decrease power consumption at function boundaries. The spikes in power consumption represent computation-intensive phases, and during this time, memory power consumption goes up due to low spatial locality leading to a higher miss rate. This benchmark spends most of its time on global transposition for complex vectors using asynchronous inter-process communications. Valleys represent the power consumption drop during these communication phases. Our proposed scheme accurately captures the entire power profile behavior and selects the power-cap. Fig. 15 a Performance. b Power-cap selection for MPIFFT Full size image 5.5.6 RandomAccess This benchmark assesses the memory subsystem’s peak capacity by updating the system memory’s random remote location. Figure  16 shows the performance, power consumption, and power-cap selection by the proposed scheme. The benchmark shows a steady step function of relatively low processor power during random index generation and decreases during communication time with other processes. The benchmark works on a large distributed table of size \\(2^{p}\\), occupying approximately half of the system memory and profiling the system’s memory architecture. It possesses a low spatial and temporal locality memory access profile. It has relatively higher memory power consumption than other benchmarks. The memory power consumption increases on the local node once all the indices are received. Based on different phases, the proposed scheme accurately selects the power-cap. Fig. 16 a Performance. b power-cap selection for MPIRandomAccess Full size image In our proposed pcHPCe, to maintain the total power consumption by all running containers within the power budget constraint, reduced power-cap applies with tolerable performance degradation on the container that causes the power-cap violation. Figure  17 presents the peak power consumption against the power-cap imposed by the proposed model and RAPL power-cap (Default). Fig. 17 Peak power consumption of jobs Full size image 6 Summary and comparison Here, we summarize and compare the proposed work with the BareMetal solution. Figure  18a presents the relative execution time of the different benchmark applications in the pcHPCe against BareMetal environment. We calculate the relative execution time of the benchmarks to the full-scale execution time. Our pcHPCe offers an almost near-native execution time against BareMetal environment. It introduces a maximum execution time overhead of 1.35% against BareMetal. Figure 18b depicts the power consumption distribution in pcHPCe against no power-cap BareMetal scenario for LU and BT benchmark. Our resource contention-aware novel power-cap selection framework attains considerable power savings of 10.25% and 13.1% compared to the BareMetal environment for BT and LU benchmark applications, respectively. Following the proposed scheme, the job scheduler always sets the optimal power-cap for the executing jobs while saving much available power budget for the waiting jobs, thereby achieving higher throughput. The proposed scheme does not allow power budget violations. The job scheduler always tries to maintain the total available power budget constraint. However, due to strict power budget constraints and performance requirements, the turnaround time of a few jobs extends, as shown in Fig. 18. Table 5 compares the obtained results using the proposed scheme with the state-of-the-art works to the maximal overhead and power saving. The comparison result between state-of-the-art works is representative only and might depend on the kind of workload in the experiments. We determine the required performance empirically based on the job profile. Figure 19 shows the performance degradation of all jobs against the performance of the job applying average power-cap to maintain the required performance (Nominal). Fig. 18 a Relative execution time. b Power saving in pcHPCe Full size image Table 5 Comparison of the experimental results with the state-of-the-art work Full size table Fig. 19 Performance degradation of jobs Full size image 7 Conclusion In the proposed pcHPCe, we implement the multi-tenant containerized power-aware HPC environment using a real-time power-performance prediction approach and a power-cap determination framework with resource contention awareness. In detail, our experiments present the performance of a pcHPCe to the BareMetal environment using NAS parallel and HPCC benchmarks. The experiment shows the need for an application’s real-time power-performance estimation approach using a state-of-the-art ML approach, LSTM, and a weighted power-performance variation table. We also present the necessity of the application’s computation phase behavior and resource contention awareness by co-scheduling threads to determine power-cap for Package and DRAM wisely. Our experimentation results show that the power-performance prediction model can achieve 91.39% accuracy on average in real-time with a negligible overhead of 1.6% of the total computing power per node due to power and performance daemons. Moreover, our resource contention-aware novel power-cap selection framework attains considerable power savings of up to 13.1% compared to the BareMetal environment. Our evaluation results demonstrate that the overall performance of the proposed method is better than the state-of-the-art methods, even for colocated applications with contention. 7.1 Future work In the future, our focus would be to explore the scalability test with communication interference awareness among colocated application threads. We intend to further investigate P-states and Turbo Boost in the modern processor in the context of the proposed methodology to boost the performance in the future. Data availability The data used to support the finding of this study are available from the corresponding author upon request. References Masanet, E., Shehabi, A., Lei, N., et al.: Recalibrating global data center energy-use estimates. Science 367(6481), 984–986 (2020) Article   Google Scholar   Murana, J., Nesmachnow, S., Armenta, F., et al.: Characterization, modeling and scheduling of power consumption of scientific computing applications in multicores. Clust. Comput. 22, 839–859 (2019) Article   Google Scholar   Soltesz, S., Potzl, ea: Container-based operating system virtualization: a scalable, high-performance alternative to hypervisors. In: Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems, pp 275–287 (2007) Young, B.D., Pasricha, S., et al.: Heterogeneous makespan and energy-constrained dag scheduling. In: Proceedings of the Workshop on Energy Efficient High Performance Parallel and Distributed Computing, pp 3–12 (2013) Wang, R., Tiwari, eaDevesh: Low power job scheduler for supercomputers: a rule-based power-aware scheduler. In: IEEE International Conference on Data Science and Data Intensive Systems, pp 732–733 (2015) Lai, Z., Lam, K.T., et al.: Powerock: power modeling and flexible dynamic power management for many-core architectures. IEEE Syst. J. 11(2), 600–612 (2016) Article   Google Scholar   Cao, T., He, eaYuan: Demand-aware power management for power-constrained hpc systems. In: 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, pp 21–31 (2016) Leon, E.A., Karlin ea: Program optimizations: the interplay between power, performance, and energy. Parallel Comput. 58, 56–75 (2016) Article   MathSciNet   Google Scholar   Kumar, A.S., Mazumdar, S.: Forecasting hpc workload using arma models and ssa. In: International Conference on Information Technology, IEEE, pp 294–297 (2016) Otoom, M., Trancoso, P., et al.: Machine learning-based energy optimization for parallel program execution on multicore chips. Arab. J. Sci. Eng. 43(12), 7343–7358 (2018) Article   Google Scholar   Rovnyagin, M.M., Hrapov, A.S., et al.: Ml-based heterogeneous container orchestration architecture. In: IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, pp 477–481 (2020) Xiao, P., Ni, Z., Liu, D., et al.: A power and thermal-aware virtual machine management framework based on machine learning. Clust. Comput. 24, 2231–2248 (2021) Article   Google Scholar   Arnaboldi, M., Brondolin, eaRolando: Hyppo: Hybrid performance-aware power-capping orchestrator. In: IEEE International Conference on Autonomic Computing, pp 71–80 (2018) Ferroni, M., Colmenares, J.A., et al.: Enabling power-awareness for the xen hypervisor. ACM SIGBED Rev. 15(1), 36–42 (2018) Article   Google Scholar   Brondolin, R., Sardelli, eaTommaso: Deep-mon: Dynamic and energy efficient power monitoring for container-based infrastructures. In: IEEE International Parallel and Distributed Processing Symposium Workshops, pp 676–684 (2018) Mehta, H.K., Harvey, P., et al.: Wattsapp: Power-aware container scheduling. In: IEEE/ACM 13th International Conference on Utility and Cloud Computing, pp 79–90 (2020) Kuity, A., Peddoju, S.K.: Performance evaluation of container-based high performance computing ecosystem using openpower. In: High performance computing ISC high performance, pp. 290–308. Springer, Berlin (2017) Chapter   Google Scholar   Chen, W., Ye, K., Xu, C.Z.: Co-locating online workload and offline workload in the cloud: An interference analysis. In: IEEE 21st International Conference on High Performance Computing and Communications, pp 2278–2283 (2019) Chen, W.Y., Ye, eaKe-Jiang.: Interference analysis of co-located container workloads: a perspective from hardware performance counters. J. Comput. Sci. Technol. 35, 412–417 (2020) Article   Google Scholar   Sak, H., Senior, A.W., et al.: Long short-term memory recurrent neural network architectures for large scale acoustic modeling (2014) Kuity, A., Peddoju, S.K.: chpce: Data locality and memory bandwidth contention-aware containerized hpc. In: 24th International Conference on Distributed Computing and Networking, pp 160–166 (2023) Karpowicz, M.P., Arabas ea: Design and implementation of energy-aware application-specific cpu frequency governors for the heterogeneous distributed computing systems. Future Gener. Comput. Syst. 78, 302–315 (2018) Article   Google Scholar   Beltre, A.M., Saha, P., et al.: Enabling hpc workloads on cloud infrastructure using kubernetes container orchestration mechanisms. In: IEEE/ACM International Workshop on Containers and New Orchestration Paradigms for Isolated Environments in HPC, pp 11–20 (2019) Fieni, G., Rouvoy, eaRomain: Smartwatts: Self-calibrating software-defined power meter for containers. In: 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, pp 479–488 (2020) Enes, J., Fieni, G., et al.: Power budgeting of big data applications in container-based clusters. In: IEEE International Conference on Cluster Computing, pp 281–287 (2020) Rocha, I., Christian, Göttel, ea: Heats: Heterogeneity-and energy-aware task-based scheduling. In: IEEE 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP), pp 400–405 (2019) Inadomi, Y., Patki, T., et al.: Analyzing and mitigating the impact of manufacturing variability in power-constrained supercomputing. In: SC’15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, pp 1–12 (2015) Ellsworth, D., Patki, T., et al.: A unified platform for exploring power management strategies. In: 4th International Workshop on Energy Efficient Supercomputing, IEEE, pp 24–30 (2016) McCraw, H., Ralph, J., Danalis, A., et al.: Power monitoring with papi for extreme scale architectures and dataflow-based programming models. In: IEEE International Conference on Cluster Computing, pp 385–391 (2014) David, H., Gorbatov, E., et al.: Rapl: Memory power estimation and capping. In: ACM/IEEE International Symposium on Low-Power Electronics and Design, pp 189–194 (2010) Dongarra, J., Luszczek, P.: Overview of the hpc challenge benchmark suite. In: Proceeding of SPEC Benchmark Workshop (2006) Download references Funding No funds have been received from any agency for this research. Author information Authors and Affiliations Department of Computer Science and Engineering, Indian Institute of Technology Roorkee, Haridwar, Uttarakhand, 247667, India Animesh Kuity & Sateesh K. Peddoju Contributions AK and SKP designed the study. AK performed the simulations and wrote the paper. All authors reviewed and edited the manuscript. All authors read and approved the final manuscript. Corresponding authors Correspondence to Animesh Kuity or Sateesh K. Peddoju. Ethics declarations Conflict of interest The authors declare that they have no competing interests. Additional information Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. Reprints and permissions About this article Cite this article Kuity, A., Peddoju, S.K. pHPCe: a hybrid power conservation approach for containerized HPC environment. Cluster Comput (2023). https://doi.org/10.1007/s10586-023-04105-8 Download citation Received 27 April 2023 Revised 30 June 2023 Accepted 05 July 2023 Published 22 July 2023 DOI https://doi.org/10.1007/s10586-023-04105-8 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing initiative Keywords High performance computing (HPC) Cloud computing Container technology Power-aware HPC Containerized HPC Use our pre-submission checklist Avoid common mistakes on your manuscript. Sections Figures References Abstract Introduction Background Related work Containerized power-aware HPC Performance evaluations Summary and comparison Conclusion Data availability References Funding Author information Ethics declarations Additional information Rights and permissions About this article Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"

Paper 6:
- APA Citation: Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J. W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., & Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), Article 1. https://doi.org/10.1038/sdata.2016.18
  Main Objective: 
  Study Location: 
  Data Sources: 
  Technologies Used: 
  Key Findings: 
  Extract 1: " FAIR data are Digital objects with FAIR (Findable, Accessible, Interoperable and Reusable) metadata - metadata that is rich enough that the object can be found by a machine or a human being, that is accessible over a network with minimal authentication, that uses open standards and that can be re-used for different purposes by different applications or workflows. FAIR metadata will allow us to integrate our data (and make them available for re-use) with other data that is already out there across the globe. \n\nFAIR data stewardship therefore involves all activities required to make data FAIR, including data collection, quality control, data description, documentation including metadata and data dictionaries, storage, preservation, and re-use."
  Extract 2: Defining a FAIR data management plan can enhance both efficiency and transparency in the execution and dissemination of research findings. Research funders and publishers are increasingly requiring a FAIR data management plan as part of the proposal and publication process. The FAIR principles address the fundamental need for data integration and re-use in many research disciplines, including the environmental sciences, which are increasingly data-intensive and interdisciplinary in nature. The FAIR data principles provide a common framework to describe, document, discover, access and integrate data, such that these data can be re-used, interpreted, and combined in a variety of ways. This can enhance insight and innovation in environmental research and help to address complex environmental challenges.
  Limitations: This paper focuses on the principles of FAIR data management and does not provide specific guidance on how to implement these principles in practice.
  Relevance Evaluation: 1.0
  Relevance Score: 1.0
  Inline Citation: (Wilkinson et al., 2016)
  Explanation: Sure. Here is the requested in-line citation for the JSON you provided:

<results_evaluation>
{ 
 "explanation": "From your close reading of the paper, provide a concise summary (3-5 sentences) of the key points of the paper as they relate to the outline point. Include this in the "explanation" field of the JSON.", 
 "relevance_evaluation": "On a scale from 0.0 to 1.0, parsimoniously rate the relevance of the paper to the point you are making in your review, with 1.0 being the most relevant.", 
 "relevance_score": "0.9-1.0", 
 "extract_1": "Select the most relevant verbatim quote from the paper that supports your point, using a maximum of 3 sentences.", 
 "extract_2": "Select the second most relevant verbatim quote from the paper that supports your point, using a maximum of 3 sentences.", 
 "limitations": "List the main limitations of the study, separated by commas using a maximum of 2 sentences.", 
 "inline_citation": "Provide the inline citation for the paper using the format: (Author Surname, Publication Year), and a full APA style reference under "apa_citation"." 
}

 Full Text: >
"Your privacy, your choice We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Accept all cookies Skip to main content Log in Find a journal Publish with us Track your research Search Cart Home Metabolomics Article Evaluating LC-HRMS metabolomics data processing software using FAIR principles for research software Review Article Published: 06 February 2023 Volume 19, article number 11, (2023) Cite this article Download PDF Access provided by University of Nebraska-Lincoln Metabolomics Aims and scope Submit manuscript Xinsong Du, Farhad Dastmalchi, Hao Ye, Timothy J. Garrett, Matthew A. Diller, Mei Liu, William R. Hogan, Mathias Brochhausen & Dominick J. Lemas  4239 Accesses 3 Citations 3 Altmetric Explore all metrics Abstract Background Liquid chromatography-high resolution mass spectrometry (LC-HRMS) is a popular approach for metabolomics data acquisition and requires many data processing software tools. The FAIR Principles – Findability, Accessibility, Interoperability, and Reusability – were proposed to promote open science and reusable data management, and to maximize the benefit obtained from contemporary and formal scholarly digital publishing. More recently, the FAIR principles were extended to include Research Software (FAIR4RS). Aim of review This study facilitates open science in metabolomics by providing an implementation solution for adopting FAIR4RS in the LC-HRMS metabolomics data processing software. We believe our evaluation guidelines and results can help improve the FAIRness of research software. Key scientific concepts of review We evaluated 124 LC-HRMS metabolomics data processing software obtained from a systematic review and selected 61 software for detailed evaluation using FAIR4RS-related criteria, which were extracted from the literature along with internal discussions. We assigned each criterion one or more FAIR4RS categories through discussion. The minimum, median, and maximum percentages of criteria fulfillment of software were 21.6%, 47.7%, and 71.8%. Statistical analysis revealed no significant improvement in FAIRness over time. We identified four criteria covering multiple FAIR4RS categories but had a low %fulfillment: (1) No software had semantic annotation of key information; (2) only 6.3% of evaluated software were registered to Zenodo and received DOIs; (3) only 14.5% of selected software had official software containerization or virtual machine; (4) only 16.7% of evaluated software had a fully documented functions in code. According to the results, we discussed improvement strategies and future directions. Similar content being viewed by others Dissemination and analysis of the quality assurance (QA) and quality control (QC) practices of LC–MS based untargeted metabolomics practitioners Article 12 October 2020 Data standards can boost metabolomics research, and if there is a will, there is a way Article Open access 17 November 2015 Reference materials for MS-based untargeted metabolomics and lipidomics: a review by the metabolomics quality assurance and quality control consortium (mQACC) Article Open access 09 April 2022 1 Introduction Metabolomics is the systematic study of small molecules (metabolites) within cells, biofluids, tissues, or organisms, and it has been widely used in clinical studies (Zhang et al., 2020). Liquid Chromatography – High Resolution Mass Spectrometry (LC-HRMS) is a popular data acquisition technique due to its high sensitivity and specificity (Zhang et al., 2020). Data processing is the first computational step of a metabolomics study and is very important for downstream analysis, such as statistical analysis and data interpretation (Du et al., 2022). Therefore, our study focused on LC-HRMS metabolomics data processing steps. The FAIR Principles – Findability, Accessibility, Interoperability, and Reusability – were proposed to promote open science and reusable data management, and to maximize the benefit obtained from contemporary and formal scholarly digital publishing. To date, numerous software has been developed for LC-HRMS metabolomics data processing (Spicer et al., 2017) such as XCMS (Smith et al., 2006; Tautenhahn et al., 2012), MZmine (Pluskal et al., 2010), and MS-DIAL (Tsugawa et al., 2015); however, there is limited information on the FAIRness of LC-HRMS metabolomics data processing software. To enhance the propensity of data and other digital objects for sharing and reuse by humans and at scale by machines, the FAIR Principles (Findable, Accessible, Interoperable, and Reusable) originated in the Netherlands during the 2014 Lorentz Workshop “Jointly Designing a Data FAIRport”. Following consultation via the Future of Research Communications and e-Scholarship (FORCE11), the FAIR Principles with 15 detailed guiding principles were published by Wilkinson et al., in 2016 (Wilkinson et al., 2016). With such an arresting and rhetorically useful acronym, the FAIR Principles have gained greater uptake than earlier encapsulation of these ideas (Directorate-General for Research and Innovation (European Commission), 2018). To date, FAIR Principles have been implemented in several areas such as precision oncology data sharing and bioinformatics data management (Mayer et al., 2021; Vesteghem et al., 2020). The adoption of FAIR Principles does not only require FAIR data, but also other FAIR digital objects such as software (Chue Hong et al., 2022). FAIR for Research Software (FAIR4RS) Working Group (Katz, Barker, et al., 2021) was established in 2020 to develop community-endorsed FAIR Principles for Research Software (Chue Hong et al., 2022). In the released FAIR4RS Principles, “Findable” (F) means the software and its metadata can be easily found by humans and machines; “Accessible” (A) means the software and its metadata can be retrieved via standardized protocols; “Interoperable” (I) means software can interoperates with other software; and “Reusable” (R) means software can be understood, modified, built upon, or incorporated into other software (Chue Hong et al., 2022). Clinical studies conducted with FAIR research software are associated with better transparency, reproducibility, and reusability (Barker et al., 2022), leading to more useful research and better translational potential (Goodman et al., 2016). However, the FAIR4RS Principles is a general description that only depicts a continuum of features that moves a research software closer to that goal (Wilkinson et al., 2019). Since FAIR speaks to machine-actionable operations, FAIR digital objects should be amenable to unambiguous forms of validation and evaluation, and a practical interpretation of the FAIR4RS Principles and the development of detailed evaluation guidelines are needed for the implementation (Wilkinson et al., 2019). In the field of LC-HRMS metabolomics data processing, a detailed implementation solution for FAIR4RS Principles has not been investigated yet. The purpose of this study is to implement FAIR4RS Principles in the evaluation of LC-HRMS metabolomics data processing software. Through a systematic review, we identified relevant LC-HRMS metabolomics data processing software. Next, we evaluated them using criteria related to FAIR4RS Principles, which were based on published papers regarding best practices of research software and internal discussions (XD, MAD, HY, DJL). Each of the criteria was also mapped to the corresponding FAIR4RS Principles. To remove potential ambiguity, detailed evaluation guidelines were also developed and refined through discussion. The goal of our analysis is focused on identifying strategies to improve the FAIRness of software for LC-HRMS metabolomics data processing. 2 Methods 2.1 Study selection process As illustrated by Fig. 1, we followed PRISMA guidelines for literature review (Liberati et al., 2009). Steps include keyword search, duplicate removal, title-abstract scanning, full-text review, and tool extraction. Search databases include Web of Science (WOS), PubMed, and Embase. The literature search strategy was established by consulting a librarian. As shown by Table S1, search terms have words related to LC-HRMS metabolomics, software, reproducibility, and LC-HRMS data processing. During the Boolean search, we restricted search fields to title and abstract, as well as subject headings for some keywords, and only studies published during the past 5 years as of 2021 August were included. During the paper screening, Covidence software was employed to help with the process (Covidence - Better Systematic Review Management, n.d.). We extracted potentially related software from eligible papers based on sentences around their names in the paper or descriptions in the tool’s paper if the tool has a publication. Then, the software was selected and evaluated based on eligibility and evaluation criteria via reading all their available documentation, publications, and code repositories. Notably, each paper was screened by two reviewers and each software was evaluated by two reviewers. All criteria could be considered as a question like “does the paper/software has a specific feature”. When there is a disagreement between the two reviewers, the reviewer said the paper/software had the feature that would need to show evidence and persuade the other reviewer, then the final label of a specific criterion was decided based on the result of the persuasion. Detailed guidelines for resolving discrepancies were illustrated in Table S3. Fig. 1 Consort diagram for the literature review and the computational tool extraction. The screening has two phases: literature screening and tool screening. During literature screening, 1396 papers published in the past 5 years were obtained through keyword search, 70 papers were finally included as relevant ones. In tool screening phase, 122 potentially relevant software were extracted from the 70 papers. We added two software that were recommended by an expert but were not mentioned in the 70 papers. All 124 software were reviewed in more detailed by reading other available resources online such as documentations and code repositories. 61 software were finally considered eligible for final FAIR4RS review. Full size image 2.2 Inclusion and exclusion criteria 2.2.1 Related steps We refer LC-HRMS metabolomics data processing to all steps after data acquisition and before statistical analysis. We documented 13 relevant steps for each software tool. A detailed list and descriptions of the steps are included in the supplementary material Table S2 along with literature references (Clasquin et al., 2012; DeFelice et al., 2017; FillPeaks-Methods, n.d.; Liu et al., 2020; Mayer et al., 2013; Smith et al., 2006; Zhou et al., 2012). We created a controlled vocabulary for these steps based on ontologies and literature. The steps were categorized into four main categories: 1. Data preparation: Steps included in this category happen immediately after data acquisition. These steps make the data usable with computational software in the lab. File format conversion and parameter optimization are the only two steps involved. 2. Feature generation: This category represents processes of generating features and their intensity values in the peak table. Nine steps are included in this category: mass detection, chromatogram building, deconvolution, peak grouping, retention time alignment, peak filling, ion annotation, and batch effect correction. 3. Quality control: Evaluating the analytical variability of the data to make sure the acquired data have good quality for downstream analysis. 4. Metabolite annotation: The process of determining the identity or chemical structure of a metabolite underlying a group of peaks, methods that are used to enhance the proximation result are also included in this category. 2.2.2 Paper screening The goal of paper screening was to select papers that mentioned or used software related to LC-HRMS metabolomics data processing. Therefore, during title-abstract scanning, we excluded studies based on the following criteria: 1. We excluded papers that were not written in English. 2. We excluded papers whose instrumental analysis did not include LC-HRMS. For example, studies used gas chromatography-mass spectrometry (GC-MS) or nuclear magnetic resonance (NMR). 3. We excluded papers that were not about metabolomics, such as papers focusing on proteomics. 4. We excluded papers that did not evaluate computational steps. For instance, some studies only focused on sample processing steps. 5. We excluded reviews that did not describe specific LC-HRMS metabolomics software. During the full-text reading stage, we used information provided within the paper and used the following exclusion criteria for further filtering: 1. The paper did not include a computational tool. 2. No tool in the paper was about metabolomics. 3. Some software in the paper was about metabolomics but not related to LC-HRMS data processing steps. 2.2.3 Tool screening For initial screening, the relevance of a certain tool was judged according to its context in the paper. Software for which at least one reviewer thought it could perform related steps was included. During tool screening, we further investigated whether a tool had a certain function based on its documentation. For example, if a tool did not contain a tutorial regarding how to do quality control, we would think it did not have the function; this applied even if a researcher might figure out a way to do quality control with the tool. Then, the software was further filtered by reading publicly available documentation, publications, and code repositories. The exclusion criteria were: 1. The tool was no longer available. 2. The tool was produced by a commercial company. 3. Functions included in the tool were not related to the steps in Table S2. 4. The tool was an extension of an existing tool and did not have its webpage or documentation. 2.3 Information gathering process To extract information manually, the full texts of selected articles were read by two reviewers. Relevant software included in these articles was deduplicated and evaluated by two reviewers for general information - name, first release date, supported operating systems, major programming languages, literature citation, supported mode (i.e., web-based/standalone/plug-in), and open-source or not - about the tool and specific FAIR4RS related criteria. Data processing steps a software tool can perform were extracted by going through the documentation as well as literature containing relevant information (Li et al., 2018; Misra, 2018, 2021; Misra et al., 2017; Misra & Mohapatra, 2019; O’Shea & Misra, 2020; Stanstrup et al., 2019). To ensure the correctness of the annotation of steps that a software tool can perform, we first did the annotation by ourselves, then contacted all senior authors of included software via emails for confirmation, correction, and comments. We also sent a follow-up email to software authors that did not respond within five days. The software authors we contacted were recorded in Table S4. FAIR4RS-related evaluation criteria were extracted by going through publications related to research software best practices, and additional criteria were added based on internal discussions (XD, MAD, HY, DJL). Each selected evaluation criterion was then assigned to one or more FAIR4RS categories through discussions between two bioinformaticians (XD and MAD) and one reproducibility expert (HY). The detailed evaluation guideline for each criterion was created and refined through discussion between the two reviewers (XD and FD) to make sure the description was not ambiguous. We screened the latest version of each tool at the time of evaluation, the time spanned from October 2021 to August 2022. During the evaluation process, each criterion was assigned a value of “TRUE”, “FALSE” or “NA” based on publicly available documentation and code repositories. “TRUE” represents that the tool met the criterion; “FALSE” meant the tool did not meet the criterion; “NA” meant the criterion does not apply to the tool. For example, if the tool did not have the command-line option, then the criterion of “include help command” would be not applicable (NA). 2.4 Synthesis of results The percentage of criteria fulfillment of each tool was calculated by dividing the number of “TRUE”s by the number of applicable criteria of the tool (i.e. ignoring any NA values). We also calculated the percentage of software that met each criterion, which was calculated by dividing the number of “TRUE”s by the number of software that could apply the criterion. For example, a tool may have 45 applicable criteria out of the total 47 criteria, and 35 criteria were labeled “TRUE”, then the percentage of criteria fulfillment would be 35/45*100%=77.8%. This can identify FAIR4RS-related criteria that merit further attention from the authors of each tool as well as future developers. 3 Results 3.1 Paper screening As illustrated in Fig. 1, the initial search generated 1396 records and 954 articles after removing duplicates. The title-abstract screening reduced the number to 87 articles. During the full-text screening, 17 articles were excluded: 4 did not include a computational tool; 8 mentioned computational software but they were not about metabolomics; and 5 included metabolomics software but the software was not for LC-HRMS data processing. Paper screening record exported from Covidence is included in the supplementary material Table S4. 3.2 Tool screening We extracted 122 potentially related software from the 70 eligible papers, then added two relevant software (EI-MAVEN, MS-FLO) we knew but were not mentioned in the 70 papers. That said, the two software were not mentioned in the notes produced by reviewers (Table S5 -> sheet “full_text_included” -> column “Notes”) during the full-text review. The number of software was reduced to 61 after reviewing their documentation in detail. In terms of function annotation, senior authors of 49 out of the 61 included software responded to our inquiry emails. General information and detailed evaluation results of the 61 selected software (Adusumilli & Mallick, 2017; Agrawal et al., 2019; Alonso et al., 2011; Broeckling et al., 2014; Brunius et al., 2016; Bueschl et al., 2017; Cai et al., 2015; Capellades et al., 2016; Chokkathukalam et al., 2013; Chong & Xia, 2018; Clasquin et al., 2012; Creek et al., 2012; Davidson et al., 2016; De Livera et al., 2018; DeFelice et al., 2017; Del Carratore et al., 2019; Dührkop et al., 2019; Fischer et al., 2022; Franceschi et al., 2014; Gatto et al., 2021; Giacomoni et al., 2015; Guo et al., 2021; Helmus et al., 2021; Huan & Li, 2015a, b; Huang et al., 2014; F. Huber, Ridder, et al., 2021; Hughes et al., 2014; Jaitly et al., 2009; Ji et al., 2017, 2019; Kantz et al., 2019; Kuhl et al., 2012; Kutuzova et al., 2020; Li et al., 2017; Libiseller et al., 2015; Liggi et al., 2018; Lommen, 2009; Loos, 2016; Mahieu et al., 2016; Müller et al., 2020; Olivon et al., 2018; Palarea-Albaladejo et al., 2018; Pang et al., 2021; Pluskal et al., 2010; Protsyuk et al., 2018; Ridder et al., 2012; Ross et al., 2020; Röst et al., 2016; Ruttkies et al., 2016; Shen et al., 2019; Smith et al., 2006; Tautenhahn et al., 2012; Teo et al., 2020; Tsugawa et al., 2015, 2016; Uppal et al., 2013, 2017; Weber & Viant, 2010; Yu et al., 2009; Zhou et al., 2014) was included in the supplementary material Table S5. Venn diagrams regarding technical properties were illustrated in the supplementary material Figure S1. In the 63 excluded software, 8 were excluded because they were not available, 24 were excluded due to their association with commercial companies, 28 were excluded because functions were not relevant, and 3 were excluded because they were not independent (i.e., an extension of another selected tool). Here “independent” means the tool has its documentation or webpage. For example, we consider MetaboAnalystR a different tool from MetaboAnalyst since MetaboAnalystR has its own GitHub page and webpage. However, we considered CSI:FinderID part of SIRIUS since we did not find CSI:FinderID to have its documentation at the time we did our evaluation. Similarly, MSnBase and CAMERA are included in the xcms software tool, but they were considered “independent” in our study since they had their webpage and documentation. All excluded software and detailed reasons for exclusion were shown in the supplementary material Table S6. 3.3 FAIR4RS evaluation FAIR4RS evaluation results were illustrated in Fig. 2 and Fig. 3. As illustrated in Table 1, we summarized 47 FAIR4RS-related criteria in total, among which 41 were from the literature regarding research software best practices, and 6 were based on internal discussions (XD, MAD, HY, DJL). At the time of our evaluation, the minimum, first quartile, median, third quartile, and maximum percentages of the fulfillment of criteria were about 21.6%, 39.5%, 47.7%, 57.9%, and 71.8%. Two software (OpenMS and patRoon) fulfilled no less than 70% applicable criteria. Ten software met 60-70% applicable criteria, they were MSnbase, EI-MAVEN, smartPeak, Spec2Vec, SIRIUS, ProteoWizard-msConvert, W4M, xcms, MetaboAnalystR, and MAGMa. Additionally, we found seven software (OpenMS, patron, EI-MAVEN, W4M, MetaboAnalystR, MS-DIAL, MetaboAnalyst) could perform the greatest number of steps (9 out of 13) involved in LC-HRMS metabolomics data processing. Three software were powered by artificial intelligence (AI) techniques (Spec2Vec, DeepMASS, and No_NAME), and they fulfilled 62.2%, 47.4%, and 26.3% of our evaluation criteria. We also found most of the included software was open source except MetaboAnalyst, MetAlign, and XCMS-Online, and none of the closed-source software was ranked within the first quartile of all included software. Figure 4 represented the percentage of criteria fulfilled in each category. Notably, some criteria were related to multiple FAIR4RS categories. An overview of the evaluation results in terms of each category is illustrated below: 1. Findability: We identified eight criteria that were related to findability. Among the eight criteria, “Have web-based documentation” had the best fulfillment of 100%. “Register to Zenodo and get DOI” was associated with the least fulfillment of ~ 6% and “Include any semantic annotation with controlled vocabulary underlying ontologies in the documentation” had the lowest fulfillment of 0%. 2. Accessibility: We identified four criteria that were corresponding to accessibility. Among the four criteria, “Code has version control” had the greatest fulfillment of over 80%. However, “Provide official software containerization or virtual machine” had ~ 14% fulfillment, and “Register to Zenodo and get DOI” was associated with the least fulfillment of ~ 6%. 3. Interoperability: We identified 12 criteria that were about interoperability. Among the 12 criteria, “Use conventional input and output” and “Explain file format in documentation if a new format is created” had the best fulfillment of 100%. Nevertheless, “Have fully documented functions in code” had ~ 17% fulfillment, “Provide official software containerization or virtual machine” had ~ 14% fulfillment, “Document exit status” had ~ 2% fulfillment, and “Include any semantic annotation with controlled vocabulary underlying ontologies in the documentation” had 0% fulfillment. 4. Reusability: We identified 41 criteria that were related to reusability. Among the 41 criteria, three criteria (“Use conventional input and output”, “Have user documentation”, and “Explain file format in documentation if a new format is created”) had 100% fulfillment. However, we found 13 criteria with a fulfillment of < 20%: “Have FAQ in documentation”, “Have developer documentation”, “Have information on potential errors and warnings as well as ways to resolve them in documentation”, “Have fully documented functions in code”, “Have an output log file containing loaded modules/dependencies during the execution”, “Have code coverage assessment”, “Provide official software containerization or virtual machine”, “Have a searchable forum”, “Have comment of creator’s email in code”, “Have historical contribution record in documentation”, “Have comment of creation date in code”, “Have automated code quality checks”, “Document exit status”, and “Include any semantic annotation with controlled vocabulary underlying ontologies in the documentation”. 3.4 FAIRness over time Table S5 had information of the release time of every included tool. We observed that there was only one included tool was released in 2006 (xcms), 2008 (MZmine), and 2021 (DaDIA), and no tool was released in 2007. Multiple included software was associated with a release time within each year between 2009 and 2020. Figure 5 is a scatter plot representing the change of FAIRness through time. Pearson’s correlation (Schober et al., 2018) indicated: (1) there was no significant (p < 0.05) FAIRness improvement across time; (2) findability has a trend of decrease; (3) all four categories were positively correlated with a statistical significance, and interoperability and reusability had the highest coefficient. To investigate what factors might contribute to the decrease of findability, we did a multiple linear regression using criteria related to findability and found four factors were significantly and positively correlated to findability: (1) have a name; (2) have a citation guide in documentation; (3) register to Zenodo and get DOI; (4) have dependencies and version numbers in documentation. Table 1 Information about evaluation criteria Full size table Fig. 2 Summary of basic results. Functions associated with each tool were displayed on the left side. Percentages of evaluating criteria that a software tool fulfilled are shown in the middle of the figure. The right panel indicates the percentage of criteria fulfillment of each tool. Additionally, names of software powered by AI techniques are highlighted in red, and software that are closed source are highlighted in black. Full size image Fig. 3 Line chart reflecting the criteria fulfillment of each category. X-axis represents tool names, ranked by overall percentage of fulfillment from left to right. Y-axis on the left side stands for percentage of fulfilled criteria. Overall fulfillment is represented by the black solid line. Fulfillments of findability, accessibility, interoperability, and reusability categories are represented by blue, yellow, green, and red dash line separately. Software are ranked by their release time, which is represented by the grey line along with the secondary y-axis. Full size image Fig. 4 Percentage of software that fulfilled each evaluation criterion. X-axes are values for percentage of fulfillment, and y-axes list all evaluation criteria. There are 4 categories and 47 criteria in total. (A) Blue bars represent findability related criteria; (B) yellow bars represent criteria related to accessibility; (C) green bars represent criteria related to interoperability; and (D) red bars stand for criteria regarding reusability. Full size image Fig. 5 FAIRness trend across time. X-axis are years representing selected software? first release times, y-axis stands for the averaged %fulfillment of software released in each year. (A) represents the relationship between findability and tool release times; (B) represents the relationship between accessibility and tool release times; (C) represents the relationship between interoperability and tool release times; (D) represents the relationship between reusability and tool release times; (E) represents the overall FAIR4RS criteria fulfillment and tool release times. Results of Pearson?s correlation are also included in each sub-figure; (F) includes information regarding correlations among categories. Full size image Table 2 Multiple linear regression for criteria related to findability Full size table 4 Discussion The application of FAIR Principles in the metabolomics field has largely focused on data sharing and management (Mendez et al., 2019; Rocca-Serra & Sansone, 2019; Savoi et al., 2021). More recently, the FAIR Data Principles have been translated into research software (Hasselbring et al., 2020; Katz, Gruenpeter, et al., 2021; Lamprecht et al., 2020), and the first version of FAIR4RS Principles was released in May 2022 (Chue Hong et al., 2022). In this study, we used FAIR4RS-related criteria to evaluate 61 selected data processing software, including 58 open-source software and 3 closed-source software. Most (41 out of 61) included criteria were related to reusability, which is consistent with previous findings (Wolf et al., 2021), and we also found the four categories were positively related to each other. Notably, open-source code is not required by FAIR Principles (Katz, Gruenpeter, et al., 2021), thus, it was not one of our evaluation criteria. To date, no study has been conducted to investigate how to implement FAIR4RS Principles in the metabolomics field. Therefore, our study extends previous work by implementing FAIR4RS Principles to assess LC-HRMS metabolomics data processing software. 4.1 FAIRness improvement strategies The primary findings from our evaluation revealed that semantic annotation of key information was associated with 0% fulfillment among all evaluated software, notably, the criterion is related to findability, interoperability, and reusability. Semantic annotation of key information includes functions, input and output data type, and format. Describing key information using controlled vocabulary underlying an ontology makes it readable and discoverable by both humans and machines (Lamprecht et al., 2020). Although we noticed that many software such as XCMS has been registered to bio.tools (Ison et al., 2019) that include semantic annotation of software, none of the official documentation mentioned this feature at the time of our evaluation. Notably, semantic annotation does not only improve the findability of the tool but also enables machines to find other similar software based on the annotation (Lamprecht et al., 2021). Semantic annotation of data processing software can be used to find relevant software to develop workflows automatically (Lamprecht et al., 2021), such systems have been used for analyzing proteomics data (Kasalica et al., 2021; Palmblad et al., 2019) and DNA sequence data (Zheng et al., 2015). Therefore, adding semantic annotation information to the official documentation may improve the FAIRness of LC-HRMS metabolomics data processing software dramatically. We also found three other criteria that were related to multiple FAIR categories for which only a small percentage of software was compliant. Firstly, only about 6% of software was registered to Zenodo and received a digital object identifier (DOI). As explained in Table 1, unlike other identifiers such as GitHub page URL or the DOI assigned by Bioconductor, Zenodo assigns each version of the software a persistent and distinct DOI (Berrios et al., n.d.; van de Sandt et al., 2019), facilitating findability and accessibility. Notably, we also found the criterion of registering software to Zenodo contributes to the decrease in findability over time. The FAIR4RS Working Group created a community on Zenodo to ensure the FAIRness of research outputs (Chue Hong et al., 2022). Notably, GitHub now has an integration of Zenodo as a third-party tool to help users with referencing and citing content (Referencing and Citing Content, n.d.). The low percentage of fulfillment (6%) is also expected since we observed most of the included software are R packages (39 out of 61 as illustrated in Figure S1). Unlike other popular repositories for R packages such as GitHub, R-Force, CRAN, and Bioconductor (Decan et al., 2015); users cannot install an R package deposited to Zenodo in the console using “install.packages”. Therefore, to improve software FAIRness, we recommend future developers deposit packages to both Zenodo and an R repository. We also recommend Zenodo developers provide a wrapper or guide regarding how to install R packages using a Zenodo URL. Additionally, we found only about 15% of software provided an official containerized version, which is relevant to accessibility, interoperability, and reusability. Providing a containerized version enables users to execute the software on different machines smoothly and without worrying about the installation of dependencies (Georgeson et al., 2019; Senington et al., 2018). In terms of R-based software, containerization including only a single package may not be that useful since users may want to use multiple packages for the analysis. The RofMassSpectrometry Initiative (Rainer et al., 2022; RforMassSpectrometry, n.d.) and metaRbolomics Toolbox in Bioconductor (Stanstrup et al., 2019) are currently two popular ecosystems for R-based metabolomics software. Therefore, a community-wide containerized ecosystem including all commonly-used and mature R-based software for the metabolomics community would be very helpful. Another poorly fulfilled criterion is “fully documented functions in code”, which is related to interoperability and reusability. Documenting input, output, and errors that a function may raise makes it easier for users to inspect the software and learn how the software can interact with other software (Lee, 2018). Therefore, in addition to semantic annotation, our results also demonstrate the FAIRness of LC-HRMS metabolomics data processing software also needs: (1) registering to Zenodo and getting version controlled DOIs, (2) providing official software containerization or virtual machine, and (3) providing fully documented functions in code. Advances in academic publishing of software have required checklists to ensure the quality of submissions, such as the Journal of Open Source Software (Review Checklist — JOSS Documentation, n.d.). Our results revealed that 59 out of 61 software were associated with a peer-reviewed publication. Checklists have been used to promote transparent reporting of metabolomics studies (Considine & Salek, 2019; Fiehn et al., 2007; Goodacre et al., 2007; Snyder et al., 2014; Sumner et al., 2007). Our recent work proposed a checklist to promote reproducible computational analysis of clinical metabolomics research (Du et al., 2022). The evaluation guideline in this study can also be used as a checklist by journals for promoting FAIR4RS Principles. A recent study shows that a direct request from the editor for research resource identifiers (RRID) is a more effective way for users to provide RRID than merely writing it in the journal’s author instructions document (Menke et al., 2020). Similarly, we expect a direct request from the journal editor regarding some criteria in our evaluation guideline will be a more effective way for FAIRness improvement than just providing a checklist in the author’s guideline. Additionally, some journals such as Analytical Chimica Acta now require an assessment of the tool by an independent advanced user, who is not a software developer or bioinformatician (Analytica Chimica Acta | Journal | ScienceDirect.Com by Elsevier, n.d.). In summary, our evaluation guideline can be used during peer-review as a checklist to improve the FAIRness of software associated with a scientific publication. 4.2 Strengths, limitations, and future works Our study has several strengths. We followed PRISMA guidelines to extract literature in the past 5 years (from 2016 to 2021) that might include a metabolomics data processing tool. Our evaluation checklist synthesized FAIR4RS-related criteria from existing literature regarding best software practices, including reproducible software development practices (Heil et al., 2021; Jiménez et al., 2017; Ram, 2013), best practices for scientific software (Hunter-Zinck et al., 2021; Leprevost et al., 2014; Zhao et al., 2012), best software documenting practices (Karimzadeh & Hoffman, 2018; Lee, 2018), best practices for command-line software (Georgeson et al., 2019; Seemann, 2013), and best software testing practices (Aghamohammadi et al., 2021). We also filtered out criteria that were not required by FAIR4RS Principles such as open-source (Katz, Gruenpeter, et al., 2021). Therefore, our checklist is more comprehensive than existing checklists, and more focused on FAIR4RS Principles. Additionally, our checklist is more specific than the general FAIR4RS Principles and with a detailed evaluation guideline. We used multiple reviewers to mitigate subjective bias. We extracted related software from selected papers, and read their publicly available documentation, publications, and code repositories for a comprehensive FAIRness evaluation. The FAIR4RS criteria were synthesized from the literature and additional criteria were added via discussions among authors (XD, MAD, HY, DJL), and the criteria can also be used to guide future FAIR LC-HRMS metabolomics software development. Each criterion was assigned to one or more FAIR4RS categories based on a discussion between two bioinformaticians (XD and MAD) and one reproducibility expert (YH). During the evaluation, two reviewers (XD and FD) went through the official documentation, publications, and code repositories of the 61 included data processing software based on an agreed evaluation guideline. More importantly, our evaluation method leads to a meaningful result. The result indicates that findability decreases over time, which is expected since people usually try to publish first and then build up nicer documentation afterward when resources are available, which means it takes time after the release for findability to increase. Although we only evaluated non-commercial LC-HRMS metabolomics data processing software, the criteria may also be suitable for evaluating all types of software. To date, this is the first study to provide an implementation solution for using FAIR4RS Principles in the metabolomics field. However, the results of this study should be considered along with some limitations of our experimental design. First, our evaluation was based only on publicly available materials including documentation and code. Secondly, we only checked the software’s official documentation but did not check the software’s machine-readable metadata (e.g., XCMS has machine-readable metadata in the software ontology (Malone et al., 2014)), which might be added by experts other than the software’s authors. Additionally, we considered all evaluation criteria equally important without weighting them. Furthermore, we included only software that was mentioned in metabolomics papers or metabolomics software review papers published in 2016–2021, thus, the list of tools may not be completed and some relevant software published very recently might not be included, such as MS2DeepScore (F. Huber, van der Burg, et al., 2021) and MS2Query (Jonge et al., 2022). The controlled vocabulary and categorization of data processing functions shown in Table S2 were produced by looking through literature (Clasquin et al., 2012; De Vos et al., 2007; FillPeaks-Methods, n.d.; Libiseller et al., 2015; Liu et al., 2020; Mayer et al., 2013; Smith et al., 2006; Vitale et al., 2022; Zhou et al., 2012) and discussing internally, and we admit that such a categorization is never easy. So, our function categorization may not be perfect and recognized by the entire metabolomics community, and software authors may have a slightly different understanding of our categorization and descriptions when responding to our inquiry. For instance, one software author emailed us saying functions like mass precision enhancement (Huber et al., 2022) should be part of the data processing steps. Notably, our study only focuses on evaluating the FAIRness of software using relevant qualitative properties, but FAIRness is just one of many considerations when selecting the software for a research workflow. For example, some software authors responded to us by saying the speed of data processing, the capability to process large-scale data, and the efficiency of hardware usage are very important aspects (Gatto et al., 2021). We did not use real-world data to get results using the included software and make a comparison. Our main focus is the software itself, and we did not consider factors (e.g., the type of the mass spectrometer) that may affect the development of the software. Additionally, it is essential to assess the quantitative performance of software using annotated LC-HRMS dataset and library (Hao et al., 2018). The FAIRness of the LC-HRMS dataset, spectral library, and software for metabolite proximation are important for the FAIRness of the entire workflow. Therefore, a future review regarding studies and available datasets for such benchmarking would be a very useful resource for FAIR LC-HRMS metabolomics workflows. 5 Conclusion We presented a comprehensive FAIRness evaluation for existing non-commercial LC-HRMS metabolomics data processing software. We evaluated 61 qualified software with 47 criteria related to FAIR4RS. Our result indicated that no software had perfect FAIR4RS compliance (i.e., fulfilled 100% applicable criteria). The maximum of criteria fulfillment was 71.7%, meaning all evaluated software had considerable space for improvement. We also identified criteria that were poorly fulfilled for each category along with detailed strategies for improvement. We believe our study can serve as a guideline to create FAIR research software for LC-HRMS metabolomics data processing software. References Adusumilli, R., & Mallick, P. (2017). Data Conversion with ProteoWizard msConvert. Methods in Molecular Biology. (Clifton N J), 1550, 339–368. https://doi.org/10.1007/978-1-4939-6747-6_23. Article   CAS   Google Scholar   Aghamohammadi, A., Mirian-Hosseinabadi, S. H., & Jalali, S. (2021). Statement frequency coverage: a code coverage criterion for assessing test suite effectiveness. Information and Software Technology, 129, 106426. https://doi.org/10.1016/j.infsof.2020.106426. Article   Google Scholar   Agrawal, S., Kumar, S., Sehgal, R., George, S., Gupta, R., Poddar, S., Jha, A., & Pathak, S. (2019). El-MAVEN: A Fast, Robust, and User-Friendly Mass Spectrometry Data Processing Engine for Metabolomics. Methods in Molecular Biology (Clifton, N.J.), 1978, 301–321. https://doi.org/10.1007/978-1-4939-9236-2_19 Alonso, A., Julià, A., Beltran, A., Vinaixa, M., Díaz, M., Ibañez, L., Correig, X., & Marsal, S. (2011). AStream: an R package for annotating LC/MS metabolomic data. Bioinformatics, 27(9), 1339–1340. https://doi.org/10.1093/bioinformatics/btr138. Article   CAS   PubMed   Google Scholar   Analytica Chimica Acta | Journal | ScienceDirect.com by Elsevier. (n.d.). Retrieved September 16, from https://www.sciencedirect.com/journal/analytica-chimica-acta Barker, M., Chue Hong, N. P., Katz, D. S., Lamprecht, A. L., Martinez-Ortiz, C., Psomopoulos, F., Harrow, J., Castro, L. J., Gruenpeter, M., Martinez, P. A., & Honeyman, T. (2022). Introducing the FAIR principles for research software. Scientific Data, 9(1), https://doi.org/10.1038/s41597-022-01710-x. Berrios, D. C., Beheshti, A., & Costes, S. V. (n.d.). FAIRness and Usability for Open-access Omics Data Systems. 10. Broeckling, C. D., Afsar, F. A., Neumann, S., Ben-Hur, A., & Prenni, J. E. (2014). RAMClust: a Novel feature clustering method enables spectral-matching-based annotation for Metabolomics Data. Analytical Chemistry, 86(14), 6812–6817. https://doi.org/10.1021/ac501530d. Article   CAS   PubMed   Google Scholar   Brunius, C., Shi, L., & Landberg, R. (2016). Large-scale untargeted LC-MS metabolomics data correction using between-batch feature alignment and cluster-based within-batch signal intensity drift correction. Metabolomics: Official Journal of the Metabolomic Society, 12(11), 173. https://doi.org/10.1007/s11306-016-1124-4. Article   CAS   PubMed   Google Scholar   Bueschl, C., Kluger, B., Neumann, N. K. N., Doppler, M., Maschietto, V., Thallinger, G. G., Meng-Reiterer, J., Krska, R., & Schuhmacher, R. (2017). MetExtract II: a Software suite for stable isotope-assisted untargeted metabolomics. Analytical Chemistry, 89(17), 9518–9526. https://doi.org/10.1021/acs.analchem.7b02518. Article   CAS   PubMed   PubMed Central   Google Scholar   Cai, Y., Weng, K., Guo, Y., Peng, J., & Zhu, Z. J. (2015). An integrated targeted metabolomic platform for high-throughput metabolite profiling and automated data processing. Metabolomics, 11(6), 1575–1586. https://doi.org/10.1007/s11306-015-0809-4. Article   CAS   Google Scholar   Capellades, J., Navarro, M., Samino, S., Garcia-Ramirez, M., Hernandez, C., Simo, R., Vinaixa, M., & Yanes, O. (2016). geoRge: a computational Tool to detect the Presence of stable isotope labeling in LC/MS-Based untargeted metabolomics. Analytical Chemistry, 88(1), 621–628. https://doi.org/10.1021/acs.analchem.5b03628. Article   CAS   PubMed   Google Scholar   Chokkathukalam, A., Jankevics, A., Creek, D. J., Achcar, F., Barrett, M. P., & Breitling, R. (2013). mzMatch–ISO: an R tool for the annotation and relative quantification of isotope-labelled mass spectrometry data. Bioinformatics, 29(2), 281–283. https://doi.org/10.1093/bioinformatics/bts674. Article   CAS   PubMed   Google Scholar   Chong, J., & Xia, J. (2018). MetaboAnalystR: an R package for flexible and reproducible analysis of metabolomics data. Bioinformatics, 34(24), 4313–4314. https://doi.org/10.1093/bioinformatics/bty528. Article   CAS   PubMed   PubMed Central   Google Scholar   Chue Hong, N. P., Katz, D. S., Barker, M., Lamprecht, A. L., Martinez, C., Psomopoulos, F. E., Harrow, J., Castro, L. J., Gruenpeter, M., Martinez, P. A., Honeyman, T., Struck, A., Lee, A., Loewe, A., van Werkhoven, B., Jones, C., Garijo, D., Plomp, E., & Genova, F. (2022). … WG, R. F. FAIR Principles for Research Software (FAIR4RS Principles). https://doi.org/10.15497/RDA00068 Clasquin, M. F., Melamud, E., & Rabinowitz, J. D. (2012). LC-MS Data Processing with MAVEN: A Metabolomic Analysis and Visualization Engine. Current Protocols in Bioinformatics / Editoral Board, Andreas D. Baxevanis … et Al.], 0 14, Unit14.11. https://doi.org/10.1002/0471250953.bi1411s37 Considine, E. C., & Salek, R. M. (2019). A Tool to encourage Minimum Reporting Guideline Uptake for Data Analysis in Metabolomics. Metabolites, 9(3), E43. https://doi.org/10.3390/metabo9030043. Article   CAS   Google Scholar   Covidence—Better systematic review management. (n.d.). Covidence. Retrieved April 6, from https://www.covidence.org/ Creek, D. J., Jankevics, A., Burgess, K. E. V., Breitling, R., & Barrett, M. P. (2012). IDEOM: an Excel interface for analysis of LC-MS-based metabolomics data. Bioinformatics (Oxford England), 28(7), 1048–1049. https://doi.org/10.1093/bioinformatics/bts069. Article   CAS   PubMed   Google Scholar   Davidson, R. L., Weber, R. J. M., Liu, H., Sharma-Oates, A., & Viant, M. R. (2016). Galaxy-M: a Galaxy workflow for processing and analyzing direct infusion and liquid chromatography mass spectrometry-based metabolomics data. GigaScience, 5(1), 10. https://doi.org/10.1186/s13742-016-0115-8. Article   CAS   PubMed   PubMed Central   Google Scholar   De Livera, A. M., Olshansky, G., Simpson, J. A., & Creek, D. J. (2018). NormalizeMets: assessing, selecting and implementing statistical methods for normalizing metabolomics data. Metabolomics, 14(5), 54. https://doi.org/10.1007/s11306-018-1347-7. Article   CAS   PubMed   Google Scholar   De Vos, R. C., Moco, S., Lommen, A., Keurentjes, J. J., Bino, R. J., & Hall, R. D. (2007). Untargeted large-scale plant metabolomics using liquid chromatography coupled to mass spectrometry. Nature Protocols, 2(4), https://doi.org/10.1038/nprot.2007.95. Decan, A., Mens, T., Claes, M., & Grosjean, P. (2015). On the Development and Distribution of R Packages: An Empirical Analysis of the R Ecosystem. Proceedings of the 2015 European Conference on Software Architecture Workshops, 1–6. https://doi.org/10.1145/2797433.2797476 DeFelice, B. C., Mehta, S. S., Samra, S., Čajka, T., Wancewicz, B., Fahrmann, J. F., & Fiehn, O. (2017). Mass Spectral feature list optimizer (MS-FLO): a Tool to minimize false positive peak reports in untargeted liquid Chromatography–Mass Spectroscopy (LC-MS) data Processing. Analytical Chemistry, 89(6), 3250–3255. https://doi.org/10.1021/acs.analchem.6b04372. Article   CAS   PubMed   PubMed Central   Google Scholar   Del Carratore, F., Schmidt, K., Vinaixa, M., Hollywood, K. A., Greenland-Bews, C., Takano, E., Rogers, S., & Breitling, R. (2019). Integrated Probabilistic Annotation: a bayesian-based annotation method for metabolomic profiles integrating biochemical connections, isotope patterns, and Adduct Relationships. Analytical Chemistry, 91(20), 12799–12807. https://doi.org/10.1021/acs.analchem.9b02354. Article   CAS   PubMed   Google Scholar   Directorate-General for Research and Innovation (European Commission). (2018). Turning FAIR into reality: final report and action plan from the European Commission expert group on FAIR data. Publications Office of the European Union. https://doi.org/10.2777/1524. Du, X., Aristizabal-Henao, J. J., Garrett, T. J., Brochhausen, M., Hogan, W. R., & Lemas, D. J. (2022). A checklist for reproducible computational analysis in clinical Metabolomics Research. Metabolites, 12(1), https://doi.org/10.3390/metabo12010087. Dührkop, K., Fleischauer, M., Ludwig, M., Aksenov, A. A., Melnik, A. V., Meusel, M., Dorrestein, P. C., Rousu, J., & Böcker, S. (2019). SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nature Methods, 16(4), https://doi.org/10.1038/s41592-019-0344-8. Article 4. Fiehn, O., Sumner, L. W., Rhee, S. Y., Ward, J., Dickerson, J., Lange, B. M., Lane, G., Roessner, U., Last, R., & Nikolau, B. (2007). Minimum reporting standards for plant biology context information in metabolomic studies. Metabolomics, 3(3), 195–201. https://doi.org/10.1007/s11306-007-0068-0. Article   CAS   Google Scholar   fillPeaks-methods: Integrate areas of missing peaks in xcms: LC-MS and GC-MS Data Analysis. (n.d.). Retrieved April 6, from https://rdrr.io/bioc/xcms/man/fillPeaks-methods.html Fischer, D., Panse, C., & Laczko, E. (2022). cosmiq: Cosmiq - COmbining Single Masses Into Quantities (1.28.0). Bioconductor version: Release (3.14). https://doi.org/10.18129/B9.bioc.cosmiq Franceschi, P., Mylonas, R., Shahaf, N., Scholz, M., Arapitsas, P., Masuero, D., Weingart, G., Carlin, S., Vrhovsek, U., Mattivi, F., & Wehrens, R. (2014). MetaDB a Data Processing Workflow in untargeted MS-Based Metabolomics experiments. Frontiers in Bioengineering and Biotechnology, 2, 72. https://doi.org/10.3389/fbioe.2014.00072. Article   PubMed   PubMed Central   Google Scholar   Gatto, L., Gibb, S., & Rainer, J. (2021). MSnbase, efficient and elegant R-Based Processing and visualization of raw Mass Spectrometry Data. Journal of Proteome Research, 20(1), 1063–1069. https://doi.org/10.1021/acs.jproteome.0c00313. Article   CAS   PubMed   Google Scholar   Georgeson, P., Syme, A., Sloggett, C., Chung, J., Dashnow, H., Milton, M., Lonsdale, A., Powell, D., Seemann, T., & Pope, B. (2019). Bionitio: demonstrating and facilitating best practices for bioinformatics command-line software. GigaScience, 8, giz109. https://doi.org/10.1093/gigascience/giz109. Article   PubMed   PubMed Central   Google Scholar   Giacomoni, F., Le Corguillé, G., Monsoor, M., Landi, M., Pericard, P., Pétéra, M., Duperier, C., Tremblay-Franco, M., Martin, J. F., Jacob, D., Goulitquer, S., Thévenot, E. A., & Caron, C. (2015). Workflow4Metabolomics: a collaborative research infrastructure for computational metabolomics. Bioinformatics, 31(9), 1493–1495. https://doi.org/10.1093/bioinformatics/btu813. Article   CAS   PubMed   Google Scholar   Goodacre, R., Broadhurst, D., Smilde, A. K., Kristal, B. S., Baker, J. D., Beger, R., Bessant, C., Connor, S., Capuani, G., Craig, A., Ebbels, T., Kell, D. B., Manetti, C., Newton, J., Paternostro, G., Somorjai, R., Sjöström, M., Trygg, J., & Wulfert, F. (2007). Proposed minimum reporting standards for data analysis in metabolomics. Metabolomics, 3(3), 231–241. https://doi.org/10.1007/s11306-007-0081-3. Article   CAS   Google Scholar   Goodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8(341), 341ps12-341ps12. Guo, J., Shen, S., Xing, S., & Huan, T. (2021). DaDIA: Hybridizing Data-Dependent and Data-Independent Acquisition Modes for Generating High-Quality Metabolomic Data.Analytical Chemistry, 93(4),2669–2677. https://doi.org/10.1021/acs.analchem.0c05022 Hao, L., Wang, J., Page, D., Asthana, S., Zetterberg, H., Carlsson, C., Okonkwo, O. C., & Li, L. (2018). Comparative evaluation of MS-based Metabolomics Software and its application to preclinical Alzheimer’s Disease. Scientific Reports, 8(1), https://doi.org/10.1038/s41598-018-27031-x. Hasselbring, W., Carr, L., Hettrick, S., Packer, H., & Tiropanis, T. (2020). From FAIR research data toward FAIR and open research software. It - Information Technology, 62(1), 39–47. https://doi.org/10.1515/itit-2019-0040. Article   Google Scholar   Heil, B. J., Hoffman, M. M., Markowetz, F., Lee, S. I., Greene, C. S., & Hicks, S. C. (2021). Reproducibility standards for machine learning in the life sciences. Nature Methods, 18(10), 1132–1135. https://doi.org/10.1038/s41592-021-01256-7. Article   CAS   PubMed   PubMed Central   Google Scholar   Helmus, R., ter Laak, T. L., van Wezel, A. P., de Voogt, P., & Schymanski, E. L. (2021). patRoon: open source software platform for environmental mass spectrometry based non-target screening. Journal of Cheminformatics, 13(1), 1. https://doi.org/10.1186/s13321-020-00477-w. Article   CAS   PubMed   PubMed Central   Google Scholar   Huan, T., & Li, L. (2015a). Counting missing values in a metabolite-intensity data set for measuring the analytical performance of a metabolomics platform. Analytical Chemistry, 87(2), 1306–1313. https://doi.org/10.1021/ac5039994. Article   CAS   PubMed   Google Scholar   Huan, T., & Li, L. (2015b). Quantitative metabolome analysis based on Chromatographic Peak Reconstruction in Chemical isotope labeling liquid chromatography Mass Spectrometry. Analytical Chemistry, 87(14), 7011–7016. https://doi.org/10.1021/acs.analchem.5b01434. Article   CAS   PubMed   Google Scholar   Huang, X., Chen, Y. J., Cho, K., Nikolskiy, I., Crawford, P. A., & Patti, G. J. (2014). X13CMS: Global Tracking of Isotopic Labels in untargeted metabolomics. Analytical Chemistry, 86(3), 1632–1639. https://doi.org/10.1021/ac403384n. Article   CAS   PubMed   PubMed Central   Google Scholar   Huber, C., Nijssen, R., Mol, H., Philippe Antignac, J., Krauss, M., Brack, W., Wagner, K., Debrauwer, L., Vitale, M., Price, C. J., Klanova, E., Molina, J. G., Leon, B., Pardo, N., Fernández, O., Szigeti, S. F., Középesy, T., Šulc, S., Čupr, L., & Lommen, P., A (2022). A large scale multi-laboratory suspect screening of pesticide metabolites in human biomonitoring: from tentative annotations to verified occurrences. Environment International, 168, 107452. https://doi.org/10.1016/j.envint.2022.107452. Article   CAS   PubMed   Google Scholar   Huber, F., Ridder, L., Verhoeven, S., Spaaks, J. H., Diblen, F., Rogers, S., & van der Hooft, J. J. J. (2021). Spec2Vec: improved mass spectral similarity scoring through learning of structural relationships. PLOS Computational Biology, 17(2), e1008724. https://doi.org/10.1371/journal.pcbi.1008724. Article   CAS   PubMed   PubMed Central   Google Scholar   Huber, F., van der Burg, S., van der Hooft, J. J. J., & Ridder, L. (2021). MS2DeepScore: a novel deep learning similarity measure to compare tandem mass spectra. Journal of Cheminformatics, 13(1), 84. https://doi.org/10.1186/s13321-021-00558-4. Article   PubMed   PubMed Central   Google Scholar   Hughes, G., Cruickshank-Quinn, C., Reisdorph, R., Lutz, S., Petrache, I., Reisdorph, N., Bowler, R., & Kechris, K. (2014). MSPrep—Summarization, normalization and diagnostics for processing of mass spectrometry–based metabolomic data. Bioinformatics, 30(1), 133–134. https://doi.org/10.1093/bioinformatics/btt589. Article   CAS   PubMed   Google Scholar   Hunter-Zinck, H., de Siqueira, A. F., Vásquez, V. N., Barnes, R., & Martinez, C. C. (2021). Ten simple rules on writing clean and reliable open-source scientific software. PLOS Computational Biology, 17(11), e1009481. https://doi.org/10.1371/journal.pcbi.1009481. Article   CAS   PubMed   PubMed Central   Google Scholar   Ison, J., Ienasescu, H., Chmura, P., Rydza, E., Ménager, H., Kalaš, M., Schwämmle, V., Grüning, B., Beard, N., Lopez, R., Duvaud, S., Stockinger, H., Persson, B., Vařeková, R. S., Raček, T., Vondrášek, J., Peterson, H., Salumets, A., Jonassen, I., & Brunak, S. (2019). The bio.tools registry of software tools and data resources for the life sciences. Genome Biology, 20(1), 164. https://doi.org/10.1186/s13059-019-1772-6. Article   PubMed   PubMed Central   Google Scholar   Jaitly, N., Mayampurath, A., Littlefield, K., Adkins, J. N., Anderson, G. A., & Smith, R. D. (2009). Decon2LS: an open-source software package for automated processing and visualization of high resolution mass spectrometry data. Bmc Bioinformatics, 10(1), 87. https://doi.org/10.1186/1471-2105-10-87. Article   CAS   PubMed   PubMed Central   Google Scholar   Ji, H., Xu, Y., Lu, H., & Zhang, Z. (2019). Deep MS/MS-Aided structural-similarity scoring for unknown metabolite identification. Analytical Chemistry, 91(9), 5629–5637. https://doi.org/10.1021/acs.analchem.8b05405. Article   CAS   PubMed   Google Scholar   Ji, H., Zeng, F., Xu, Y., Lu, H., & Zhang, Z. (2017). KPIC2: an effective Framework for Mass Spectrometry-Based Metabolomics using pure Ion Chromatograms. Analytical Chemistry, 89(14), 7631–7640. https://doi.org/10.1021/acs.analchem.7b01547. Article   CAS   PubMed   Google Scholar   Jiménez, R. C., Kuzak, M., Alhamdoosh, M., Barker, M., Batut, B., Borg, M., Capella-Gutierrez, S., Chue Hong, N., Cook, M., Corpas, M., Flannery, M., Garcia, L., Gelpí, J. L., Gladman, S., Goble, C., González Ferreiro, M., Gonzalez-Beltran, A., Griffin, P. C., Grüning, B., & Crouch, S. (2017). Four simple recommendations to encourage best practices in research software. F1000Research, 6, ELIXIR-876. https://doi.org/10.12688/f1000research.11407.1 de Jonge, N. F., Louwen, J. R., Chekmeneva, E., Camuzeaux, S., Vermeir, F. J., Jansen, R. S., Huber, F., & van der Hooft, J. J. J. (2022). MS2Query: Reliable and Scalable MS2 Mass Spectral-based Analogue Search (p. 2022.07.22.501125). bioRxiv. https://doi.org/10.1101/2022.07.22.501125 Kantz, E. D., Tiwari, S., Watrous, J. D., Cheng, S., & Jain, M. (2019). Deep neural networks for classification of LC-MS spectral peaks. Analytical Chemistry, 91(19), 12407–12413. https://doi.org/10.1021/acs.analchem.9b02983. Article   CAS   PubMed   PubMed Central   Google Scholar   Karimzadeh, M., & Hoffman, M. M. (2018). Top considerations for creating bioinformatics software documentation. Briefings in Bioinformatics, 19(4), 693–699. https://doi.org/10.1093/bib/bbw134. Article   PubMed   Google Scholar   Kasalica, V., Schwämmle, V., Palmblad, M., Ison, J., & Lamprecht, A. L. (2021). APE in the Wild: Automated Exploration of Proteomics Workflows in the bio.tools Registry. Journal of Proteome Research, 20(4), 2157–2165. https://doi.org/10.1021/acs.jproteome.0c00983. Article   CAS   PubMed   PubMed Central   Google Scholar   Katz, D. S., Barker, M., Chue Hong, N. P., Castro, L. J., & Martinez, P. A. (2021, June 28). The FAIR4RS team: Working together to make research software FAIR. 2021 Collegeville Workshop on Scientific Software - Software Teams (Collegeville2021). Zenodo. https://doi.org/10.5281/zenodo.5037157 Katz, D. S., Gruenpeter, M., & Honeyman, T. (2021). Taking a fresh look at FAIR for research software. Patterns, 2(3), 100222. https://doi.org/10.1016/j.patter.2021.100222. Article   PubMed   PubMed Central   Google Scholar   Kuhl, C., Tautenhahn, R., Böttcher, C., Larson, T. R., & Neumann, S. (2012). CAMERA: an Integrated strategy for compound Spectra extraction and annotation of Liquid Chromatography/Mass Spectrometry Data Sets. Analytical Chemistry, 84(1), 283–289. https://doi.org/10.1021/ac202450g. Article   CAS   PubMed   Google Scholar   Kutuzova, S., Colaianni, P., Röst, H., Sachsenberg, T., Alka, O., Kohlbacher, O., Burla, B., Torta, F., Schrübbers, L., Kristensen, M., Nielsen, L., Herrgård, M. J., & McCloskey, D. (2020). SmartPeak automates targeted and quantitative Metabolomics Data Processing. Analytical Chemistry, 92(24), 15968–15974. https://doi.org/10.1021/acs.analchem.0c03421 Article   CAS   PubMed   Google Scholar   Lamprecht, A.-L., Garcia, L., Kuzak, M., Martinez, C., Arcila, R., Martin Del Pico,E., Dominguez Del Angel, V., van de Sandt, S., Ison, J., Martinez, P. A., McQuilton,P., Valencia, A., Harrow, J., Psomopoulos, F., Gelpi, J. L., Chue Hong, N., Goble,C., & Capella-Gutierrez, S. (2020). Towards FAIR principles for research software.Data Science, 3(1), 37–59. https://doi.org/10.3233/DS-190026 Lamprecht, A. L., Palmblad, M., Ison, J., Schwämmle, V., Manir, M. S. A., Altintas, I., Baker, C. J. O., Amor, A. B. H., Capella-Gutierrez, S., Charonyktakis, P., Crusoe, M. R., Gil, Y., Goble, C., Griffin, T. J., Groth, P., Ienasescu, H., Jagtap, P., Kalaš, M., Kasalica, V., & Wolstencroft, K. (2021). Perspectives on automated composition of workflows in the life sciences (10:897). F1000Research. https://doi.org/10.12688/f1000research.54159.1 Lee, B. D. (2018). Ten simple rules for documenting scientific software. PLOS Computational Biology, 14(12), e1006561. https://doi.org/10.1371/journal.pcbi.1006561. Article   CAS   PubMed   PubMed Central   Google Scholar   Leprevost, F. V., Barbosa, V. C., Francisco, E. L., Perez-Riverol, Y., & Carvalho, P. C. (2014). On best practices in the development of bioinformatics software. Frontiers in Genetics, 5, 199. https://doi.org/10.3389/fgene.2014.00199. Article   PubMed   PubMed Central   Google Scholar   Li, B., Tang, J., Yang, Q., Li, S., Cui, X., Li, Y., Chen, Y., Xue, W., Li, X., & Zhu, F. (2017). NOREVA: normalization and evaluation of MS-based metabolomics data. Nucleic Acids Research, 45(W1), W162–W170. https://doi.org/10.1093/nar/gkx449. Article   CAS   PubMed   PubMed Central   Google Scholar   Li, Z., Lu, Y., Guo, Y., Cao, H., Wang, Q., & Shui, W. (2018). Comprehensive evaluation of untargeted metabolomics data processing software in feature detection, quantification and discriminating marker selection. Analytica Chimica Acta, 1029, 50–57. https://doi.org/10.1016/j.aca.2018.05.001. Article   CAS   PubMed   Google Scholar   Liberati, A., Altman, D. G., Tetzlaff, J., Mulrow, C., Gøtzsche, P. C., Ioannidis, J. P. A., Clarke, M., Devereaux, P. J., Kleijnen, J., & Moher, D. (2009). The PRISMA statement for reporting systematic reviews and meta-analyses of studies that evaluate healthcare interventions: explanation and elaboration. Bmj, 339, b2700. https://doi.org/10.1136/bmj.b2700. Article   PubMed   PubMed Central   Google Scholar   Libiseller, G., Dvorzak, M., Kleb, U., Gander, E., Eisenberg, T., Madeo, F., Neumann, S., Trausinger, G., Sinner, F., Pieber, T., & Magnes, C. (2015). IPO: a tool for automated optimization of XCMS parameters. Bmc Bioinformatics, 16(1), 118. https://doi.org/10.1186/s12859-015-0562-8. Article   CAS   PubMed   PubMed Central   Google Scholar   Liggi, S., Hinz, C., Hall, Z., Santoru, M. L., Poddighe, S., Fjeldsted, J., Atzori, L., & Griffin, J. L. (2018). KniMet: a pipeline for the processing of chromatography–mass spectrometry metabolomics data. Metabolomics, 14(4), 52. https://doi.org/10.1007/s11306-018-1349-5. Article   CAS   PubMed   PubMed Central   Google Scholar   Liu, Q., Walker, D., Uppal, K., Liu, Z., Ma, C., Tran, V., Li, S., Jones, D. P., & Yu, T. (2020). Addressing the batch effect issue for LC/MS metabolomics data in data preprocessing. Scientific Reports, 10(1), 13856. https://doi.org/10.1038/s41598-020-70850-0. Article   CAS   PubMed   PubMed Central   Google Scholar   Lommen, A. (2009). MetAlign: Interface-Driven, Versatile Metabolomics Tool for Hyphenated full-scan Mass Spectrometry Data Preprocessing. Analytical Chemistry, 81(8), 3079–3086. https://doi.org/10.1021/ac900036d. Article   CAS   PubMed   Google Scholar   Loos, M. (2016). enviPick: Peak Picking for High Resolution Mass Spectrometry Data (1.5). https://CRAN.R-project.org/package=enviPick Mahieu, N. G., Spalding, J. L., & Patti, G. J. (2016). Warpgroup: increased precision of metabolomic data processing by consensus integration bound analysis. Bioinformatics, 32(2), 268–275. https://doi.org/10.1093/bioinformatics/btv564. Article   CAS   PubMed   Google Scholar   Malone, J., Brown, A., Lister, A. L., Ison, J., Hull, D., Parkinson, H., & Stevens, R. (2014). The Software Ontology (SWO): a resource for reproducibility in biomedical data analysis, curation and digital preservation. Journal of Biomedical Semantics, 5(1), 25. https://doi.org/10.1186/2041-1480-5-25. Article   PubMed   PubMed Central   Google Scholar   Mayer, G., Montecchi-Palazzi, L., Ovelleiro, D., Jones, A. R., Binz, P. A., Deutsch, E. W., Chambers, M., Kallhardt, M., Levander, F., Shofstahl, J., Orchard, S., Vizcaíno, J. A., Hermjakob, H., Stephan, C., Meyer, H. E., Eisenacher, M., & HUPO-PSI Group. (2013). &. The HUPO proteomics standards initiative- mass spectrometry controlled vocabulary. Database: The Journal of Biological Databases and Curation, 2013, bat009. https://doi.org/10.1093/database/bat009 Mayer, G., Müller, W., Schork, K., Uszkoreit, J., Weidemann, A., Wittig, U., Rey, M., Quast, C., Felden, J., Glöckner, F. O., Lange, M., Arend, D., Beier, S., Junker, A., Scholz, U., Schüler, D., Kestler, H. A., Wibberg, D., Pühler, A., & Turewicz, M. (2021). Implementing FAIR data management within the German Network for Bioinformatics infrastructure (de.NBI) exemplified by selected use cases. Briefings in Bioinformatics, 22(5), bbab010. https://doi.org/10.1093/bib/bbab010. Article   PubMed   PubMed Central   Google Scholar   Mendez, K. M., Pritchard, L., Reinke, S. N., & Broadhurst, D. I. (2019). Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing. Metabolomics, 15(10), 125. https://doi.org/10.1007/s11306-019-1588-0. Article   CAS   PubMed   PubMed Central   Google Scholar   Menke, J., Roelandse, M., Ozyurt, B., Martone, M., & Bandrowski, A. (2020). The rigor and transparency Index Quality Metric for assessing Biological and Medical Science Methods. IScience, 23(11), 101698. https://doi.org/10.1016/j.isci.2020.101698. Article   PubMed   PubMed Central   Google Scholar   Misra, B. B. (2018). New tools and resources in metabolomics: 2016–2017. ELECTROPHORESIS, 39(7), 909–923. https://doi.org/10.1002/elps.201700441. Article   CAS   PubMed   Google Scholar   Misra, B. B. (2021). New software tools, databases, and resources in metabolomics: updates from 2020. Metabolomics, 17(5), 49. https://doi.org/10.1007/s11306-021-01796-1. Article   CAS   PubMed   PubMed Central   Google Scholar   Misra, B. B., Fahrmann, J. F., & Grapov, D. (2017). Review of emerging metabolomic tools and resources: 2015–2016. ELECTROPHORESIS, 38(18), 2257–2274. https://doi.org/10.1002/elps.201700110. Article   CAS   PubMed   Google Scholar   Misra, B. B., & Mohapatra, S. (2019). Tools and resources for metabolomics research community: a 2017–2018 update. ELECTROPHORESIS, 40(2), 227–246. https://doi.org/10.1002/elps.201800428. Article   CAS   PubMed   Google Scholar   Müller, E., Huber, C. E., Brack, W., Krauss, M., & Schulze, T. (2020). Symbolic aggregate approximation improves gap filling in high-resolution Mass Spectrometry Data Processing. Analytical Chemistry, 92(15), 10425–10432. https://doi.org/10.1021/acs.analchem.0c00899. Article   CAS   PubMed   Google Scholar   Olivon, F., Elie, N., Grelier, G., Roussi, F., Litaudon, M., & Touboul, D. (2018). MetGem Software for the generation of Molecular Networks based on the t-SNE algorithm. Analytical Chemistry, 90(23), 13900–13908. https://doi.org/10.1021/acs.analchem.8b03099. Article   CAS   PubMed   Google Scholar   O’Shea, K., & Misra, B. B. (2020). Software tools, databases and resources in metabolomics: updates from 2018 to 2019. Metabolomics, 16(3), 36. https://doi.org/10.1007/s11306-020-01657-3. Article   CAS   PubMed   Google Scholar   Palarea-Albaladejo, J., Mclean, K., Wright, F., & Smith, D. G. E. (2018). MALDIrppa: quality control and robust analysis for mass spectrometry data. Bioinformatics, 34(3), 522–523. https://doi.org/10.1093/bioinformatics/btx628. Article   CAS   PubMed   Google Scholar   Palmblad, M., Lamprecht, A. L., Ison, J., & Schwämmle, V. (2019). Automated workflow composition in mass spectrometry-based proteomics. Bioinformatics, 35(4), 656–664. https://doi.org/10.1093/bioinformatics/bty646. Article   CAS   PubMed   Google Scholar   Pang, Z., Chong, J., Zhou, G., de Lima Morais, D. A., Chang, L., Barrette, M., Gauthier, C., Jacques, P., Li, S., & Xia, J. (2021). MetaboAnalyst 5.0: narrowing the gap between raw spectra and functional insights. Nucleic Acids Research, 49(W1), W388–W396. https://doi.org/10.1093/nar/gkab382. Article   CAS   PubMed   PubMed Central   Google Scholar   Pluskal, T., Castillo, S., Villar-Briones, A., & Orešič, M. (2010). MZmine 2: modular framework for processing, visualizing, and analyzing mass spectrometry-based molecular profile data. Bmc Bioinformatics, 11(1), 395. https://doi.org/10.1186/1471-2105-11-395. Article   CAS   PubMed   PubMed Central   Google Scholar   Protsyuk, I., Melnik, A. V., Nothias, L. F., Rappez, L., Phapale, P., Aksenov, A. A., Bouslimani, A., Ryazanov, S., Dorrestein, P. C., & Alexandrov, T. (2018). 3D molecular cartography using LC-MS facilitated by Optimus and ’ili software. Nature Protocols, 13(1), 134–154. https://doi.org/10.1038/nprot.2017.122. Article   CAS   PubMed   Google Scholar   Rainer, J., Vicini, A., Salzer, L., Stanstrup, J., Badia, J. M., Neumann, S., Stravs, M. A., Hernandes, V., Gatto, V., Gibb, L., S., & Witting, M. (2022). A modular and expandable ecosystem for Metabolomics Data Annotation in R. Metabolites, 12(2), https://doi.org/10.3390/metabo12020173. Article 2. Ram, K. (2013). Git can facilitate greater reproducibility and increased transparency in science. Source Code for Biology and Medicine, 8(1), 7. https://doi.org/10.1186/1751-0473-8-7. Article   PubMed   PubMed Central   Google Scholar   Referencing and citing content. (n.d.). GitHub Docs. Retrieved December 30, from https://ghdocs-prod.azurewebsites.net/en/repositories/archiving-a-github-repository/referencing-and-citing-content Review checklist—JOSS documentation. (n.d.). Retrieved April 28, from https://joss.readthedocs.io/en/latest/review_checklist.html RforMassSpectrometry. (n.d.). Retrieved January 14, from https://www.rformassspectrometry.org/ Ridder, L., van der Hooft, J. J. J., Verhoeven, S., de Vos, R. C. H., van Schaik, R., & Vervoort, J. (2012). Substructure-based annotation of high-resolution multistage MSn spectral trees. Rapid Communications in Mass Spectrometry, 26(20), 2461–2471. https://doi.org/10.1002/rcm.6364. Article   CAS   PubMed   Google Scholar   Rocca-Serra, P., & Sansone, S. A. (2019). Experiment design driven FAIRification of omics data matrices, an exemplar. Scientific Data, 6(1), https://doi.org/10.1038/s41597-019-0286-0. Romano, J. D., & Moore, J. H. (2020). Ten simple rules for writing a paper about scientific software. PLOS Computational Biology, 16(11), e1008390. https://doi.org/10.1371/journal.pcbi.1008390. Article   CAS   PubMed   PubMed Central   Google Scholar   Ross, D. H., Cho, J. H., Zhang, R., Hines, K. M., & Xu, L. (2020). LiPydomics: a Python Package for Comprehensive Prediction of lipid Collision Cross sections and Retention Times and Analysis of Ion Mobility-Mass Spectrometry-Based Lipidomics Data. Analytical Chemistry, 92(22), 14967–14975. https://doi.org/10.1021/acs.analchem.0c02560. Article   CAS   PubMed   PubMed Central   Google Scholar   Röst, H. L., Sachsenberg, T., Aiche, S., Bielow, C., Weisser, H., Aicheler, F., Andreotti, S., Ehrlich, H. C., Gutenbrunner, P., Kenar, E., Liang, X., Nahnsen, S., Nilse, L., Pfeuffer, J., Rosenberger, G., Rurik, M., Schmitt, U., Veit, J., Walzer, M., & Kohlbacher, O. (2016). OpenMS: a flexible open-source software platform for mass spectrometry data analysis. Nature Methods, 13(9), 741–748. https://doi.org/10.1038/nmeth.3959. Article   CAS   PubMed   Google Scholar   Ruttkies, C., Schymanski, E. L., Wolf, S., Hollender, J., & Neumann, S. (2016). MetFrag relaunched: incorporating strategies beyond in silico fragmentation. Journal of Cheminformatics, 8(1), 3. https://doi.org/10.1186/s13321-016-0115-9. Article   CAS   PubMed   PubMed Central   Google Scholar   Savoi, S., Arapitsas, P., Duchêne, É., Nikolantonaki, M., Ontañón, I., Carlin, S., Schwander, F., Gougeon, R. D., Ferreira, A. C. S., Theodoridis, G., Töpfer, R., Vrhovsek, U., Adam-Blondon, A. F., Pezzotti, M., & Mattivi, F. (2021). Grapevine and wine metabolomics-based guidelines for FAIR data and Metadata Management. Metabolites, 11(11), 757. https://doi.org/10.3390/metabo11110757. Article   CAS   PubMed   PubMed Central   Google Scholar   Schober, P., Boer, C., & Schwarte, L. A. (2018). Correlation coefficients: appropriate use and interpretation. Anesthesia & Analgesia, 126(5), 1763–1768. https://doi.org/10.1213/ANE.0000000000002864. Article   Google Scholar   Seemann, T. (2013). Ten recommendations for creating usable bioinformatics command line software. GigaScience, 2(1), 15. https://doi.org/10.1186/2047-217X-2-15. Article   PubMed   PubMed Central   Google Scholar   Senington, R., Pataki, B., & Wang, X. V. (2018). Using docker for factory system software management: experience report. Procedia CIRP, 72, 659–664. https://doi.org/10.1016/j.procir.2018.03.173. Article   Google Scholar   Shen, X., Wang, R., Xiong, X., Yin, Y., Cai, Y., Ma, Z., Liu, N., & Zhu, Z. J. (2019). Metabolic reaction network-based recursive metabolite annotation for untargeted metabolomics. Nature Communications, 10(1), https://doi.org/10.1038/s41467-019-09550-x. Smith, C. A., Want, E. J., O’Maille, G., Abagyan, R., & Siuzdak, G. (2006). Matching, and Identification. Analytical Chemistry, 78(3), 779–787. https://doi.org/10.1021/ac051437y. XCMS: Processing Mass Spectrometry Data for Metabolite Profiling Using Nonlinear Peak Alignment,. Snyder, M., Mias, G., Stanberry, L., & Kolker, E. (2014). Metadata Checklist for the Integrated Personal OMICS Study: Proteomics and Metabolomics experiments. OMICS: A Journal of Integrative Biology, 18(1), 81–85. https://doi.org/10.1089/omi.2013.0148. Article   CAS   PubMed   Google Scholar   Spicer, R., Salek, R. M., Moreno, P., Cañueto, D., & Steinbeck, C. (2017). Navigating freely-available software tools for metabolomics analysis. Metabolomics: Official Journal of the Metabolomic Society, 13(9), 106. https://doi.org/10.1007/s11306-017-1242-7. Article   CAS   PubMed   Google Scholar   Stanstrup, J., Broeckling, C. D., Helmus, R., Hoffmann, N., Mathé, E., Naake, T., Nicolotti, L., Peters, K., Rainer, J., Salek, R. M., Schulze, T., Schymanski, E. L., Stravs, M. A., Thévenot, E. A., Treutler, H., Weber, R. J. M., Willighagen, E., Witting, M., & Neumann, S. (2019). The metaRbolomics Toolbox in Bioconductor and beyond. Metabolites, 9(10), https://doi.org/10.3390/metabo9100200. Article 10. Sumner, L. W., Amberg, A., Barrett, D., Beale, M. H., Beger, R., Daykin, C. A., Fan, T. W. M., Fiehn, O., Goodacre, R., Griffin, J. L., Hankemeier, T., Hardy, N., Harnly, J., Higashi, R., Kopka, J., Lane, A. N., Lindon, J. C., Marriott, P., Nicholls, A. W., & Viant, M. R. (2007). Proposed minimum reporting standards for chemical analysis Chemical Analysis Working Group (CAWG) Metabolomics Standards Initiative (MSI). Metabolomics: Official Journal of the Metabolomic Society, 3(3), 211–221. https://doi.org/10.1007/s11306-007-0082-2. Article   CAS   PubMed   Google Scholar   Tautenhahn, R., Patti, G. J., Rinehart, D., & Siuzdak, G. (2012). XCMS Online: a web-based platform to process untargeted metabolomic data. Analytical Chemistry, 84(11), 5035–5039. https://doi.org/10.1021/ac300698c. Article   CAS   PubMed   PubMed Central   Google Scholar   Teo, G., Chew, W. S., Burla, B. J., Herr, D., Tai, E. S., Wenk, M. R., Torta, F., & Choi, H. (2020). MRMkit: Automated Data Processing for large-scale targeted Metabolomics Analysis. Analytical Chemistry, 92(20), 13677–13682. https://doi.org/10.1021/acs.analchem.0c03060. Article   CAS   PubMed   Google Scholar   Tsugawa, H., Cajka, T., Kind, T., Ma, Y., Higgins, B., Ikeda, K., Kanazawa, M., VanderGheynst, J., Fiehn, O., & Arita, M. (2015). MS-DIAL: Data-independent MS/MS deconvolution for comprehensive metabolome analysis. Nature Methods, 12(6), 523–526. https://doi.org/10.1038/nmeth.3393. Article   CAS   PubMed   PubMed Central   Google Scholar   Tsugawa, H., Kind, T., Nakabayashi, R., Yukihira, D., Tanaka, W., Cajka, T., Saito, K., Fiehn, O., & Arita, M. (2016). Hydrogen rearrangement rules: computational MS/MS fragmentation and structure elucidation using MS-FINDER Software. Analytical Chemistry, 88(16), 7946–7958. https://doi.org/10.1021/acs.analchem.6b00770. Article   CAS   PubMed   PubMed Central   Google Scholar   Uppal, K., Soltow, Q. A., Strobel, F. H., Pittard, W. S., Gernert, K. M., Yu, T., & Jones, D. P. (2013). xMSanalyzer: automated pipeline for improved feature detection and downstream analysis of large-scale, non-targeted metabolomics data. Bmc Bioinformatics, 14(1), 15. https://doi.org/10.1186/1471-2105-14-15. Article   PubMed   PubMed Central   Google Scholar   Uppal, K., Walker, D. I., & Jones, D. P. (2017). xMSannotator: an R Package for Network-Based annotation of high-resolution Metabolomics Data. Analytical Chemistry, 89(2), 1063–1067. https://doi.org/10.1021/acs.analchem.6b01214. Article   CAS   PubMed   PubMed Central   Google Scholar   van de Sandt, S., Nielsen, L. H., Ioannidis, A., Muench, A., Henneken, E., Accomazzi, A., Bigarella, C., Lopez, J. B. G., & Dallmeier-Tiessen, S. (2019). Practice meets Principle: Tracking Software and Data Citations to Zenodo DOIs (arXiv:1911.00295). arXiv. https://doi.org/10.48550/arXiv.1911.00295 Vesteghem, C., Brøndum, R. F., Sønderkær, M., Sommer, M., Schmitz, A., Bødker, J. S., Dybkær, K., El-Galaly, T. C., & Bøgsted, M. (2020). Implementing the FAIR Data Principles in precision oncology: review of supporting initiatives. Briefings in Bioinformatics, 21(3), 936–945. https://doi.org/10.1093/bib/bbz044. Article   CAS   PubMed   Google Scholar   Vitale, C. M., Lommen, A., Huber, C., Wagner, K., Garlito Molina, B., Nijssen, R., Price, E. J., Blokland, M., van Tricht, F., Mol, H. G. J., Krauss, M., Debrauwer, L., Pardo, O., Leon, N., Klanova, J., & Antignac, J. P. (2022). Harmonized Quality Assurance/Quality control provisions for nontargeted measurement of urinary pesticide biomarkers in the HBM4EU Multisite SPECIMEn Study. Analytical Chemistry, 94(22), 7833–7843. https://doi.org/10.1021/acs.analchem.2c00061. Article   CAS   PubMed   Google Scholar   Weber, R. J. M., & Viant, M. R. (2010). MI-Pack: increased confidence of metabolite identification in mass spectra by integrating accurate masses and metabolic pathways. Chemometrics and Intelligent Laboratory Systems, 104(1), 75–82. https://doi.org/10.1016/j.chemolab.2010.04.010. Article   CAS   Google Scholar   Wilkinson, M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J. W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., & Mons, B. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1), Article 1. https://doi.org/10.1038/sdata.2016.18 Wilkinson, M. D., Dumontier, M., Sansone, S. A., Bonino da Silva Santos, L. O., Prieto, M., Batista, D., McQuilton, P., Kuhn, T., Rocca-Serra, P., Crosas, M., & Schultes, E. (2019). Evaluating FAIR maturity through a scalable, automated, community-governed framework. Scientific Data, 6(1), https://doi.org/10.1038/s41597-019-0184-5. Wolf, M., Logan, J., Mehta, K., Jacobson, D., Cashman, M., Walker, A. M., Eisenhauer, G., Widener, P., & Cliff, A. (2021). Reusability First: Toward FAIR Workflows. 2021 IEEE International Conference on Cluster Computing (CLUSTER), 444–455. https://doi.org/10.1109/Cluster48925.2021.00053 Yu, T., Park, Y., Johnson, J. M., & Jones, D. P. (2009). ApLCMS—adaptive processing of high-resolution LC/MS data. Bioinformatics, 25(15), 1930–1936. https://doi.org/10.1093/bioinformatics/btp291. Article   CAS   PubMed   PubMed Central   Google Scholar   Zhang, X., Li, Q., Xu, Z., & Dou, J. (2020). Mass spectrometry-based metabolomics in health and medical science: a systematic review. RSC Advances, 10(6), 3092–3104. https://doi.org/10.1039/C9RA08985C. Article   CAS   PubMed   PubMed Central   Google Scholar   Zhao, J., Gómez-Pérez, J., Belhajjame, K., Klyne, G., García-Cuesta, E., Garrido, A., Hettne, K., Roos, M., Roure, D. D., & Goble, C. (2012). Why workflows break—Understanding and combating decay in Taverna workflows. 2012 IEEE 8th International Conference on E-Science. https://doi.org/10.1109/eScience.2012.6404482 Zheng, C. L., Ratnakar, V., Gil, Y., & McWeeney, S. K. (2015). Use of semantic workflows to enhance transparency and reproducibility in clinical omics. Genome Medicine, 7(1), 73. https://doi.org/10.1186/s13073-015-0202-y. Article   PubMed   PubMed Central   Google Scholar   Zhou, B., Xiao, J. F., Tuli, L., & Ressom, H. W. (2012). LC-MS-based metabolomics. Molecular BioSystems, 8(2), 470–481. https://doi.org/10.1039/c1mb05350g. Article   CAS   PubMed   Google Scholar   Zhou, R., Tseng, C. L., Huan, T., & Li, L. (2014). IsoMS: automated processing of LC-MS data generated by a chemical isotope labeling metabolomics platform. Analytical Chemistry, 86(10), 4675–4679. https://doi.org/10.1021/ac5009089. Article   CAS   PubMed   Google Scholar   Download references Acknowledgements The authors thank Biswapriya Misra, Ph.D., for his constructive remarks and useful suggestions for the study. The authors also sincerely thank Bailey Ballard and Jianming (Jennifer) Wang for their help in the process of title-abstract screening. We would like to express a special thank to all software authors that took the time out of their busy schedule to respond our emails and provide thoughtful feedback regarding the annotation of software functions. Funding Research reported in this publication was supported by the University of Florida Informatics Institute Fellowship Program. Research reported in this publication was also supported by Southeast Center for Integrated Metabolomics at the University of Florida, the National Institute of Diabetes and Digestive and Kidney Diseases (K01DK115632), the University of Florida Clinical and Translational Science Institute (UL1TR001427). The content is solely the responsibility of the authors and does not necessarily represent the official views the University of Florida Informatics Institute, Southeast Center for Integrated Metabolomics at the University of Florida, University of Florida Clinical and Translational Science Institute, or the National Institutes of Health. Author information Authors and Affiliations Department of Health Outcomes and Biomedical Informatics, University of Florida College of Medicine, Gainesville, FL, USA Xinsong Du, Farhad Dastmalchi, Matthew A. Diller, Mei Liu, William R. Hogan & Dominick J. Lemas Health Science Center Libraries, University of Florida, Florida, USA Hao Ye Department of Pathology, Immunology and Laboratory Medicine, College of Medicine, University of Florida, Florida, USA Timothy J. Garrett Department of Biomedical Informatics, College of Medicine, University of Arkansas for Medical Sciences, Little Rock, USA Mathias Brochhausen Department of Obstetrics and Gynecology, University of Florida College of Medicine, Florida, Gainesville, United States Dominick J. Lemas Center for Perinatal Outcomes Research, University of Florida College of Medicine, Gainesville, United States Dominick J. Lemas Contributions XD conceived the idea, designed the study, participated in the paper review, participated in software evaluation, participated in designing software evaluation criteria and assigning criteria to corresponding FAIR4RS categories as a major contributor, drafted the original version of the categorization and description of LC-HRMS metabolomics data processing steps, performed and programmed for all analysis and visualization, interpreted results, prepared the original draft, and revised the manuscript. FD participated in the paper review and software evaluation. HY provided literature search terms and databases, participated in designing software evaluation criteria and assigned criteria to corresponding FAIR4RS categories. TJL provided expertise regarding study design, revised the categorization and description of LC-HRMS metabolomics data processing steps, and revised the manuscript. MAD participated in designing software evaluation criteria and assigning criteria to corresponding FAIR4RS categories. ML provided expertise regarding statistical analysis. WRH revised the manuscript. MB revised the manuscript. DJL conceived the idea, provided expertise regarding study design, provided expertise regarding data analysis and visualization, interpreted results, and revised the manuscript. Corresponding author Correspondence to Dominick J. Lemas. Ethics declarations Conflict of interest The authors declare that they have no competing interests. Additional information Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Electronic supplementary material Below is the link to the electronic supplementary material. Supplementary Material 1 Supplementary Material 2 Supplementary Material 3 Supplementary Material 4 Rights and permissions Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. Reprints and permissions About this article Cite this article Du, X., Dastmalchi, F., Ye, H. et al. Evaluating LC-HRMS metabolomics data processing software using FAIR principles for research software. Metabolomics 19, 11 (2023). https://doi.org/10.1007/s11306-023-01974-3 Download citation Received 08 December 2022 Accepted 20 January 2023 Published 06 February 2023 DOI https://doi.org/10.1007/s11306-023-01974-3 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing initiative Keywords FAIR principles Metabolomics Research software Liquid chromatography-mass spectrometry Open science Reproducibility Use our pre-submission checklist Avoid common mistakes on your manuscript. Sections Figures References Abstract Introduction Methods Results Discussion Conclusion References Acknowledgements Funding Author information Ethics declarations Additional information Electronic supplementary material Rights and permissions About this article Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"

Paper 7:
- APA Citation: Zhou, N., Zhou, H., & Hoppe, D. (2023). Containerization for High Performance Computing Systems: Survey and Prospects. IEEE Transactions on Software Engineering, 49(4), 2722-2740.
  Main Objective: To provide a comprehensive overview of the state-of-the-art in containerization technologies and their use in cloud environments, and to identify key research gaps and propose new research questions and hypotheses for future work.
  Study Location: Unspecified
  Data Sources: Peer-reviewed journal articles, conference proceedings, technical reports
  Technologies Used: Containerization technologies, Cloud computing, Data processing, Machine learning
  Key Findings: Containerization technologies offer significant benefits for data processing and machine learning in cloud environments, including improved efficiency, scalability, and security. However, there are also a number of challenges that need to be addressed in order to use containers effectively in HPC systems. The authors of this paper identify several key research gaps and propose new research questions and hypotheses for future work.
  Extract 1: “Containerization not only enables customized environments on HPC systems, but also brings research reproducibility into practice. Containerised applications can become complex, e.g., thousands of separate containers may be required in production, and containers may require network isolation among each other for security reasons. Sophisticated strategies for container orchestration have been developed on Cloud or big-data clusters to meet such requirements. HPC systems, per contra, lack features of efficiency in container scheduling and management (e.g. load balancing and auto container scaling), and often provide no integrated support for environment provisioning (i.e., infrastructure, configurations and dependencies).
  Extract 2: “There have been numerous studies on containerisation and container orchestration on Cloud [2], [9], [10], [11], [12], [13], [14], [15], however, there is no comprehensive survey on these technologies and techniques for HPC systems existing as of yet. This article:
  Limitations: The paper does not have any major limitations. It provides a comprehensive overview of the state-of-the-art in containerization technologies and their use in cloud environments, and it identifies key research gaps and proposes new research questions and hypotheses for future work.
  Relevance Evaluation: The paper is highly relevant to the specific point in your literature review on leveraging containerization technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).
The paper provides a detailed overview of the state-of-the-art in containerization technologies and discusses the benefits and challenges of using containers for data processing and machine learning in cloud environments. Additionally, the authors of this paper conducted an extensive literature review and analysis of the state-of-the-art in containerization technologies and their use in cloud environments, which provides valuable insights for your own research.
On a scale from 0.0 to 1.0, I would give the paper a relevance score of 1.0.
  Relevance Score: 1.0
  Inline Citation: (Zhou et al., 2023)
  Explanation: Sure, here is the context you requested

**Explanation**

This paper is a comprehensive review of the current state and future potential of real-time, end-to-end automated irrigation management systems. It discusses the importance of automated irrigation management systems in addressing the global food challenge, evaluating the current state of the art, and examining the role of interoperability and standardization. The paper also highlights the challenges and proposes solutions for implementing real-time, automated irrigation systems, and identifies key research gaps and proposes new research questions and hypotheses for future work.

**Relevance Evaluation**

The paper is highly relevant to the specific point in your literature review on leveraging containerization technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). The paper provides a detailed overview of the state-of-the-art in containerization technologies and discusses the benefits and challenges of using containers for data processing and machine learning in cloud environments. Additionally, the authors of this paper conducted an extensive literature review and analysis of the state-of-the-art in containerization technologies and their use in cloud environments, which provides valuable insights for your own research.

**Relevance Score**

On a scale from 0.0 to 1.0, I would give the paper a relevance score of 1.0. The paper is highly relevant to the specific point in your literature review and provides valuable insights into the state-of-the-art in containerization technologies and their use in cloud environments.

**Extract 1**

**“Containerization not only enables customized environments on HPC systems, but also brings research reproducibility into practice. Containerised applications can become complex, e.g., thousands of separate containers may be required in production, and containers may require network isolation among each other for security reasons. Sophisticated strategies for container orchestration have been developed on Cloud or big-data clusters to meet such requirements. HPC systems, per contra, lack features of efficiency in container scheduling and management (e.g. load balancing and auto container scaling), and often provide no integrated support for environment provisioning (i.e., infrastructure, configurations and dependencies).**

**Relevance Evaluation**

This excerpt is highly relevant to the specific point in your literature review on leveraging containerization technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). The excerpt discusses the benefits of using containers for data processing and machine learning in cloud environments, as well as the challenges that need to be addressed in order to use containers effectively in HPC systems.

**Extract 2**

**“There have been numerous studies on containerisation and container orchestration on Cloud [2], [9], [10], [11], [12], [13], [14], [15], however, there is no comprehensive survey on these technologies and techniques for HPC systems existing as of yet. This article:**

**Relevance Evaluation**

This excerpt is highly relevant to the specific point in your literature review on leveraging containerization technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). The excerpt discusses the lack of comprehensive surveys on containerization technologies and techniques for HPC systems and highlights the need for such a survey.

**Limitations**

The paper does not have any major limitations. It provides a comprehensive overview of the state-of-the-art in containerization technologies and their use in cloud environments, and it identifies key research gaps and proposes new research questions and hypotheses for future work.

**Inline Citation**

(Zhou et al., 2023)

**APA Citation**

Zhou, N., Zhou, H., & Hoppe, D. (2023). Containerization for High Performance Computing Systems: Survey and Prospects. IEEE Transactions on Software Engineering, 49(4), 2722-2740.

**Study Location**

Unspecified

**Main Objective**

To provide a comprehensive overview of the state-of-the-art in containerization technologies and their use in cloud environments, and to identify key research gaps and propose new research questions and hypotheses for future work.

**Technologies Used**

Containerization technologies, Cloud computing, Data processing, Machine learning

**Data Sources**

Peer-reviewed journal articles, conference proceedings, technical reports

**Key Findings**

Containerization technologies offer significant benefits for data processing and machine learning in cloud environments, including improved efficiency, scalability, and security. However, there are also a number of challenges that need to be addressed in order to use containers effectively in HPC systems. The authors of this paper identify several key research gaps and propose new research questions and hypotheses for future work. 

**Note:** I have included some additional information in the response that is not explicitly stated in the paper, but that I believe is relevant and helpful for understanding the context of the paper.

I hope this is helpful! Let me know if you have any other questions.

 Full Text: >
"This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising purposes. To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Software... >Volume: 49 Issue: 4 Containerization for High Performance Computing Systems: Survey and Prospects Publisher: IEEE Cite This PDF Naweiluo Zhou; Huan Zhou; Dennis Hoppe All Authors 4 Cites in Papers 928 Full Text Views Abstract Document Sections I. Introduction II. Concepts and Technologies for Containerisation III. Container Engines and Runtimes for HPC Systems IV. Container Orchestration V. Research Challenges and Vision Show Full Outline Authors Figures References Citations Keywords Metrics Footnotes Abstract: Containers improve the efficiency in application deployment and thus have been widely utilised on Cloud and lately in High Performance Computing (HPC) environments. Containers encapsulate complex programs with their dependencies in isolated environments making applications more compatible and portable. Often HPC systems have higher security levels compared to Cloud systems, which restrict users’ ability to customise environments. Therefore, containers on HPC need to include a heavy package of libraries making their size relatively large. These libraries usually are specifically optimised for the hardware, which compromises portability of containers. Per contra , a Cloud container has smaller volume and is more portable. Furthermore, containers would benefit from orchestrators that facilitate deployment and management of containers at a large scale. Cloud systems in practice usually incorporate sophisticated container orchestration mechanisms as opposed to HPC systems. Nevertheless, some solutions to enable container orchestration on HPC systems have been proposed in state of the art. This paper gives a survey and taxonomy of efforts in both containerisation and its orchestration strategies on HPC systems. It highlights differences thereof between Cloud and HPC. Lastly, challenges are discussed and the potentials for research and engineering are envisioned. Published in: IEEE Transactions on Software Engineering ( Volume: 49, Issue: 4, 01 April 2023) Page(s): 2722 - 2740 Date of Publication: 14 December 2022 ISSN Information: DOI: 10.1109/TSE.2022.3229221 Publisher: IEEE Funding Agency: SECTION I. Introduction Containers have been widely adopted on Cloud systems. Applications together with their dependencies are encapsulated into containers [1], which can ensure environment compatibility and enable users to move and deploy programs easily among clusters. Containerisation is a virtualisation technology [2]. Rather than creating an entire operating system (called guest OS) on top of a host OS as in a Virtual Machine (VM), containers only share the host kernel, which makes containers more lightweight than VMs. Containers on Cloud are often dedicated to run micro-services [3] and one container mostly hosts one application or a part of it. High Performance Computing (HPC) systems are traditionally employed to perform large-scale financial, engineering and scientific simulations [4] that demand low latency (e.g., interconnect) and high throughput (e.g., the number of jobs completed over a specific time). To satisfy different user requirements, HPC systems normally provide predefined modules with specific software versions that users can switch by loading or unloading the modules with the desired packages [5]. This approach requires assistance of system administrators and therefore limits increasing user demands for environment customisation. On a multi-tenant environment as on HPC systems, especially HPC production systems, installation of new software packages on-demand by users is restricted, as it may alter the working environments of existing users and even raise security risks. Module-enabled software environments are also inconvenient for dynamic Artificial Intelligence (AI) software stacks [6]. Big Data Analytics hosted on Cloud are compute-intensive or data-intensive, mainly due to deployments of AI or Machine Learning (ML) applications, which demand extremely fast knowledge extraction in order to make rapid and accurate decisions. HPC-enabled AI can offer optimisation of supply chains, complex logics, manufacturing, simulation and underpin modelling to solve complex problems [7]. Typically, AI applications have sophisticated requirements of software stacks and configurations. Containerisation not only enables customised environments on HPC systems, but also brings research reproducibility into practice. Containerised applications can become complex, e.g., thousands of separate containers may be required in production, and containers may require network isolation among each other for security reasons. Sophisticated strategies for container orchestration [8] have been developed on Cloud or big-data clusters to meet such requirements. HPC systems, per contra, lack features of efficiency in container scheduling and management (e.g. load balancing and auto container scaling), and often provide no integrated support for environment provisioning (i.e., infrastructure, configurations and dependencies). There have been numerous studies on containerisation and container orchestration on Cloud [2], [9], [10], [11], [12], [13], [14], [15], however, there is no comprehensive survey on these technologies and techniques for HPC systems existing as of yet. This article: Investigates state-of-the-art works in containerisation on HPC systems and underscores their differences with respect to the Cloud; Introduces the representative orchestration frameworks on both HPC and Cloud environments, and highlights their feature differences; Gathers the related studies in the integration of container orchestration strategies on Cloud into HPC environments; Discusses the challenges and envisions the potential directions for research and engineering. The rest of the paper is organised as follows. First, Section II introduces the background on containerisation technologies and techniques. Key technologies of state-of-the-art container engines (Section III) and orchestration strategies (Section IV) are presented, and the feature differences thereof between HPC and Cloud systems are discussed. Next, Section V describes research challenges and the vision. Lastly, Section VI concludes this paper. SECTION II. Concepts and Technologies for Containerisation The main differences between containerisation technologies on Cloud and HPC systems are in terms of security and the types of workloads. The HPC applications tend to require more resources as to not only CPUs, but also the amount of memory and network speed. HPC communities have, therefore, developed sophisticated workload managers to leverage hardware resources and optimise application scheduling. Since the typical applications on Cloud differ significantly from those in HPC centres with respect to the sizes, execution time and requirements of the availability of hardware resources [16], the management systems on Cloud are evolved to include architectures different from those on HPC systems. Research and engineering on containerisation technologies and techniques for HPC systems can be classified into two broad categories: Container engines/runtimes; Container orchestration. In the first category, various architectures of container engines have been developed which vary in usage of namespaces (see Section II-A), image formats and programming languages. The research in the latter category is still in its primitive stage, which will be discussed in Section IV. A. Containerisation Concepts Containerisation is an OS-level virtualisation technology [17] that provides separation of application execution environments. A container is a runnable instance of an image that encapsulates a program together with its libraries, data, configuration files, etc. [1] in an isolated environment, hence it can ensure library compatibility and enables users to move and deploy programs easily among clusters. A container utilises the dependencies in its host kernel. The host merely needs to start a new process that is isolated from the host itself to boot a new container [18], thus making container start-up time comparable to that of a native application. In contrary, a traditional VM loads an entire guest kernel (simulated OS) into memory, which can occupy gigabytes of storage space on the host and requires a significant fraction of system resources to run. VMs are managed by hypervisor which is also known as Virtual Machine Monitor (VMM) that partitions and provisions VMs with hardware resources (e.g., CPU and memory). The hypervisor gives the hardware-level virtualisation [19], [20]. Fig. 1 highlights the architecture distinction of VMs and containers. It is worth noting that containers can also run inside VMs [21]. Besides portability, containers also enable reproducibility, i.e. once a program has been defined inside the container, its included working environment remains unchanged regardless of its running occurrences. Nevertheless, the shared kernel strategy presents an obvious pitfall: a Windows containerised application cannot execute on Unix kernels. Obviously, this should not become an impediment to its usage as Unix-like OS are often the preference for HPC systems. Fig. 1. Structure comparison of VMs and containers. On the VM side, the virtualisation layer often appears to be hypervisor while on the container side it is the container runtimes. Show All HPC applications are often highly optimised for processor architectures, interconnects, accelerators and other hardware aspects. Containerised applications, therefore, need to compromise between performance and portability. The studies have shown that containers can often achieve near-native performance [18], [22], [23], [24], [25], [26], [27] (see Section III-B). Linux has several namespaces [28] that isolate various kernel resources: mount (file system tree and mounts), PID (process ID), UTS (hostname and domain name), network (e.g., network devices, ports, routing tables and firewall rules), IPC (inter-process communication resources) and user. The last namespace is an unprivileged namespace that grants the unprivileged process access to traditional privileged functionalities under a safe context. More specifically, the user namespace allows to map user ID (UID) and group ID (GID) from hosts to containers, meaning that a user having UID 0 (root) inside a container can be mapped to a non-root ID (e.g., 100000) outside the container. Cgroups (Control Groups) is another namespace that is targeted to limit, isolate and measure resource consumption of processes. Cgroups is useful for a multi-tenant setting as excess resource consumption of certain users will be only adverse to themselves. One application of Linux namespaces is the implementation of containers, e.g., Docker, the most widely-used container engine, uses namespaces to provide the isolated workspace that is called container. When a container executes, Docker creates a set of namespaces for that container. B. Docker There are multiple techniques that realise the concept of containers. Docker is among the most popular ones [27]. After its appearance in 2013, various container solutions aimed for HPC have emerged [22]. Docker, initially based on LXC [29], is a container engine that supports multiple platforms, i.e., Linux, OSX and Windows. A Docker container image is composed of a readable/writable layer above a series of read-only layers. A new writable layer is added to the underlying layers when a new Docker container is created. All changes that are made to the running container, such as writing new files, modifying or deleting existing files, are written to this thin writable container layer. Docker adopts namespaces including Cgroups to provide resource isolation and resource limitation, respectively. Table I highlights the usage of namespaces with respect to Docker and a list of container engines targeted for HPC environments. TABLE I Linux Namespace Supports for HPC-Targeted Container Engines (Section III) and Docker in the Year of 2022 Docker provides network isolation and communication by creating three types of networks: host, bridge and none. The bridge network is the default Docker network. The Docker engine creates a subset or gateway to the bridged network. This software bridge allows Docker containers to communicate within the same bridged network; meanwhile, isolates the containers from a different bridged network. Containers in the same host can communicate via the default network by the host IP address. To communicate with the containers located on a different host, the host needs to allocate ports on its IP address. Managing ports brings overhead which can intensify at scale. Dynamically managing ports can solve this issue which is better handled by orchestration platforms as introduced in Section IV-B. Docker is widely adopted in Cloud where users often have root privileges. The root privilege is required to execute the Docker application and its Daemon process that provides the essential services. Originally running Docker with root permission brings some advantages to Cloud users. For instance, users can run their applications and alternative security modules to provide separation among different allocations [30]; users can also mount host filesystems to their containers. Root privilege can cause security issues. Therefore, the latest updates of Docker engine start to support rootless daemon and enable users to execute containers without root. Nevertheless, other security concerns still persist. For instance, usage of Unix socket can be changed to TCP socket which will grant an attacker a remote control to execute any containers in the privileged mode. Additionally, rootless Docker does not run out of box, system administrators need to carefully set the namespaces of hosts to separate resources and user groups in order to guarantee security. Hence HPC centres that typically have high security requirements are still reluctant to enable the Docker support on their systems. SECTION III. Container Engines and Runtimes for HPC Systems This section first reviews the state-of-the-art container engines/runtimes designed for HPC systems and compares the major differences with the mainstream Cloud container engine, i.e., Docker. Next, Section III-B shows the performance evaluation of the reviewed HPC container engines. A. State-of-The-Art Container Engines and Runtimes A list of representative container engines and runtimes for HPC systems is given in this section. They differ in functional extent and implementation, however, also hold some similarities. Tables I and II summarise the feature differences and similarities between Docker and a list of main HPC container engines. TABLE II Comparison of Docker With the List of Container Engines for HPC Systems 1) Shifter Shifter [31] is a prototypical implementation of container engine for HPC developed by NERSC. It utilises Docker for image building workflow. Once an image is built, users can submit it to an unprivileged gateway which injects configurations and binaries, flattens it to an ext4 file system image, and then compresses to squashfs images that are copied to a parallel filesystem on the nodes. In this way, Shifter insulates the network filesystem from image metadata traffic. Root permission of Docker is naturally deprived from Shifter that only grant user-level privileges. Existing directories can be also mounted inside Shifter image by passing certain flags. As an HPC container engine, Shifter supports MPICH that is an implementation of the Message Passing Interface (MPI) [32], [33] standard. To enable accelerator supports such as GPU without compromising container portability, Shifter runtime swaps the built-in GPU driver of a Shifter container with an ABI (Application Binary Interface) compatible version at the container start-up time. 2) Charliecloud Charliecloud [28] runs containers without privileged operations or daemons. Charlicloud can convert a Docker image into a tar file and unpacks it on the HPC nodes. Installation of Charliecloud does not require root permission. Such non-intrusive mechanisms are ideal for HPC systems. Charliecloud is considered to be secure against shenanigans, such as chroot escape, bypass of file and directory permission, privileged ports bound to all host IP addresses or UID set to an unmapped UID [15]. MPI is supported by Charliecloud. Injecting host files into images is used by Charliecloud to solve library compatibility issues, such as GPU libraries that may be tied to specific kernel versions. 3) Singularity Singularity is the most-widely used HPC container engine in academia and industry. Singularity [34] was specifically designed from the outset for HPC systems. Contrasting with Docker, it gives the following merits [23]: Running with user privileges and no daemon process. Only user privileges are required to execute Singularity applications. Acquisition of root permission is only necessary when users want to build or rebuild images, which can be performed on their own working computers. Unprivileged users can also build an image from a definition file with a few restrictions by ”fake root” in Singularity, however, some methods requiring to create block devices (e.g., /dev/null) may not always work correctly in this way; Seamless integration with HPC systems. Singularity natively supports GPU, MPI and InfiniBand [16]. No additional network configurations are expected in contrast with Docker containers; Portable via a single image file (SIF format). On the contrary, Docker is built up on top of layers of files. Two approaches are often used to execute MPI applications using Singularity, i.e., hybrid model and bind model. The former compiles MPI binaries, libraries and the MPI application into a Singularity image. The latter binds the container on a host location where the container utilises the MPI libraries and binaries on the host. The latter model has a smaller image size since it does not include compiled MPI libraries and binaries in the image. Utilising the host libraries is also beneficial to application performance, however, the version of MPI implementation that is used to compile the application inside the container must be compatible with the version available on the host. The hybrid model is recommended, as mounting storage volumes on the host often require privileged operations. Most Docker images can be converted to singularity images directly via simple command lines (e.g. docker save, singularity build). Singularity has quickly become the ipso facto standard container engine for HPC systems. 4) SARUS SARUS [35] is another container engine targeted for HPC systems. SARUS relies on runc1 to instantiate containers. runc is a CLI (Command-Line Interface) tool for spawning and running containers according to the OCI (Open Container Initiative) specification. Different from the aforementioned engines, the internal structure of SARUS is based on the OCI standard (see Section V-A2). As shown in Fig. 2, the CLI component takes the command lines which either invoke the image manager component or the runtime component. The latter instantiates and executes containers by creating a bundle that comprises a root filesystem directory and a JSON configuration file. The runtime component then calls runc that will spawn the container processes. It is worth noting that functionalities of SARUS can be extended by calling customised OCI hooks, e.g., MPI hook. Fig. 2. The internal structure of SARUS. OCI hooks include MPI hook. Show All 5) UDocker UDocker2 is a Python wrapper for the Docker container, which executes only simple Docker containers in user space without the acquisition of root privileges. UDocker provides a Docker-like CLI and only supports a subset of Docker commands, i.e., search, pull, import, export, load, save, create and run. It is worth noting that UDocker neither makes use of Docker nor requires its presence on the host. It executes containers by simply providing a chroot-like environment over the extracted container. 6) Other HPC Container Engines More and more HPC container engines are being developed, this section gives an overview of some that are targeted for special use cases. Podman [36] makes use of the user namespace to execute containers without privilege escalation. A Podman container image comprises layers of read-write files as Docker. It adopts the same runtime runc as in SARUS and Docker. The runtime crun, which is faster than runc, is also supported. A notable feature of Podman is as its name denotes: the concept of pod. A pod groups a set of containers that collectively implements a complex application to share namespaces and simplify communication. This feature enables the convergence with the Kubernetes [37] environment (Section IV-B1), however, requires advanced kernel features (e.g., version 2 Cgroups and user-space FUSE). These kernel features are not yet compatible with network filesystems to make full use of the rootless capabilities of Podman and consequently restrain its usage from HPC production systems [38]. Similar to UDocker, Socker [39] is a simple secure wrapper to run Docker in HPC environments, more specifically SLURM (Section IV-A2). It does not support the user namespace, however, it takes the resource limits imposed by SLURM. Enroot3 from NVIDIA can be considered as an enhanced unprivileged chroot. It removes much of the isolation that the other container engines normally provide but preserves filesystem separation. Enroot makes use of user and mount namespaces. B. Performance Evaluation for HPC Container Engines This section only selects the representative works as given in Table III, rather than exhausting the literature, to show the performance of containers that are specifically targeted for HPC systems in terms of CPU, memory, disk (I/O), network and GPU. Table VI lists the benchmarks utilised in these work. Overall, the container startup latency can be high on the Cloud. This startup overhead is caused by building containers from multiple image layers, setting read-write layers and monitoring containers [27]. An HPC container is composed of a single image or directory (with exception to Podman) and monitoring is performed by HPC systems. TABLE III Overview of the Related Work on Container Performance Evaluation in Terms of CPU, Memory, Disk, Network and GPU on HPC Systems TABLE IV The List of HPC Benchmarks Mentioned in Section III-B The work in [24], utilising the IMB [42] benchmark suite and HPCG [43] benchmarks, proved that little overhead of network bandwidth and CPU computing overhead is caused by Singularity when dynamically linking vendor MPI libraries in order to efficiently leverage advanced hardware resources. With the Cray MPI library, Singularity container achieved 99.4% efficiency of native bandwidth on a Cray XC [44] HPC testbed when running the IMB benchmark suite. However, the efficacy drastically drops to 39.5% with Intel MPI. Execution time evaluated with the HPCG benchmarks, indicated that the performance penalty caused by Singularity is negligible with Cray MPI, though the overhead can reach 18.1% with Intel MPI. The performance degradation with Intel MPI is mostly because of the vendor-tuned MPI library which does not leverage hardware resources from a different vendor, e.g., interconnect. Hu et al. [23] evaluated the Singularity performance in terms of CPU capacity, memory, network bandwidth and GPU with Linpack benchmarks [45] and four typical HPC applications (i.e., NAMD [46], VASP [47], WRF [48] and AMBER [49]). Singularity provides close to native performance on CPU, memory and network bandwidth. A slight overhead (4.135%) is shown on NVIDIA GPU. Muscianisi et al. [41] illustrated the performance impact of Singularity with the increasing number of GPU nodes. The evaluation was carried out on CINECA's GALILEO systems with TensorFlow [50] applications. The results again demonstrated that the container environments caused negligible performance overhead. The work by Hale et al. [18] presented the CPU performance of Shifter with HPGMG-FE (MPI implementation) benchmarks [51] on Cray XC30 (192 cores, 24 cores per compute node) where the performance margin between Shifter container and bare metal is unnoticeable. Comparison is also given for MPI with implementation in C++ and Python using a custom benchmark. The authors observed that it could take over 30 minutes to import the Python modules when running natively with 1,000 processes. Each process of a Python application imports modules from the filesystem on each node. Accesses to many small files on an HPC filesystem using many processes can be extremely slow comparing with the accesses to a few large files. The containerised benchmark has already included all the modules in its image that is mounted as a single file on each node, therefore, Shifter container outperforms the native execution in this case. Bahls [40] also evaluated the execution time of Shifter on Cray XC and Cray XE/XK systems exploiting Cray HSN (High Speed Network). Their results showed that Shifter gave comparable performance to bare metal. The study in [22] compared the performance of Shifter and Singularity against bare metal in terms of computation time using two biological use cases on three types of supercomputer CPU architectures: Intel Skylake, IBM Power9 and Arm-v8. Containerised applications can scale at the same rate as the bare-metal counterparts. However, the authors also observed that with a small number of MPI ranks, containers should be built as generic as possible, per contra, when it comes to a large number of cores, containers need to be tuned for the hosts. Without performance comparison with bare-metal applications, the work in [27] studied the CPU, memory, network and I/O performance of Charliecloud, Podman and Singularity. All the containers behave similarly with respect to the CPU and memory usage. Charliecloud and Singularity have comparable I/O performance. Charliecloud incurs large overhead on Lustre's MDS (Metadata Server) and OSS (Object Storage Server) due to its bare tree structure. Comparing with the structures of shared layers (as in Docker), this structure needs to access a large number of individual files from the image tree from Lustre. Consequently, it causes network overhead when data is transmitted from the client node over the network at container start-up time. Similarly, as Singularity is stored as a single file on Lustre, a large amount of data needs to be loaded at starting point resulting in a data transmission spike on network. SARUS has shown strong scaling capability on Cray XC systems with hybrid GPU and CPU nodes [35]. The performance difference between SARUS and bare metal is less than 0.5% up to 8 nodes and 6.2% up to 256 nodes. No specific metrics are given in terms of GPU, though GPU has been used as accelerators. C. Section Highlights Containers are introduced to HPC systems, as they enable environment customisation for users, which offers the solutions to application compatibility issues. This is particularly important on HPC systems that are typically inflexible for environment modifications. Notably, HPC container engines are designed to meet the high-security requirements on HPC systems. Multiple prevailing engines have been described in this section, they share some common features: Non-root privileges; Often can convert Docker images to their own image formats; Supports of MPI that are typical HPC applications; Use host network rather than pluggable network drivers. Yet differences exist in their image formats. Layered image format is seen in Docker (UDocker wraps Docker image layers to a local directory), which is executed by pulling the image layers that have not been previously downloaded on the host. HPC container images are stored in a single directory or file which can be transferred to the compute nodes easily avoiding the pulling operations that require network access. HPC container engines show various ways to incorporate well-tuned libraries targeting for the hosts in order to achieve optimised performance, e.g., OCI hooks (SARUS), injecting host files into images (Charliecloud). Section III-B aims to give examples that can provide general advices on how to build the container images to maximise performance. Clearly, performance loss can occur in certain cases which are summarised in the second column of Table III. SECTION IV. Container Orchestration Orchestration under the context herein means automated configuration, coordination and management of Cloud or HPC systems. In theory, HPC workload manager can be also addressed as orchestrator, however, this article takes the former term as it is the custom terminology that has been long-used and widely understood in the HPC area. The driving factors that push HPC workload managers and Cloud orchestrators to be developed in different directions can be multiple. This will be discussed at the end of this section (Section IV-D). However, first it is important to understand the mechanisms of HPC workload managers (Section IV-A) and Cloud orchestrators (Section IV-B). Mostly, container orchestration for HPC systems either relies on the orchestration strategies of the existing Cloud orchestrators or exploits the mechanisms of current HPC workload managers or software tools. This point will be depicted in Section IV-C. A. Workload Managers for HPC Systems Cloud aims to exploit economy of scale by consolidating applications into the same hardware [16] and the hardware resources can be easily extended based on user demands. In contrast, HPC centres have large-scale hardware resources available and reserve computing resources exclusively for users. Table V underscores the main differences between HPC workload managers and Cloud orchestrators. A typical HPC system is managed by a workload manager. A workload manager comprises a resource manager and a job scheduler. A resource manager [52] allocates resources (e.g., CPU and memory), schedules jobs and guarantees no interference from other user processes. A job scheduler determines the job priorities, enforces resource limits and dispatches jobs to available nodes [53]. TABLE V Comparison of HPC Workload Managers (Section IV-A) and Cloud Orchestrators (Section IV-2) HPC workload managers incorporate a big family, such as PBS [54], Spectrum LSF [55], Grid Engine [56], OAR [57] and Slurm [58]. Slurm and PBS are two main-stream workload managers. The workload managers shares some common features: a centralised scheduling system, a queuing system and static resource management mechanisms, which will be detailed in this section. 1) PBS PBS stands for Portable Batch System which includes three versions: OpenPBS, PBS Pro and TORQUE. OpenPBS is open-source and TORQUE is a fork of OpenPBS. PBS Pro is dual-licensed under an open-source and commercial license. The structure of a TORQUE-managed cluster consists of a head node and many compute nodes as illustrated in Fig. 3 where only three compute nodes are shown. The head node (coloured in blue in Fig. 3) controls the entire TORQUE system. A pbs_server daemon and a job scheduler daemon are located on the head node. The batch job is submitted to the head node (in some cases, the job is first submitted to a login node and then transferred to the head node). A node list that records the configured compute nodes is maintained on the head node. The architecture of this kind as shown in Fig 3 represents the fundamental cluster structure of main-stream HPC workload managers. The procedure of job submission on TORQUE is briefly described as follows: The job is submitted to the head node by the command qsub. A job is normally written in the format of a PBS script. A job ID is returned to the user as the standard output of qsub. Fig. 3. TORQUE structure. pbs_server , scheduler and pbs_mom are the daemons running on the nodes. Mother superior is the first node on the node list (on step4). Show All The job record, which incorporates a job ID and the job attributes, is generated and passed to pbs_server . pbs_server transfers the job record to the job scheduler daemon. The job scheduler daemon adds the job into a job queue and applies a scheduling algorithm to it (e.g., FIFO: First In First Out) which determines the job priority and its resource assignment. When the scheduler finds the list of nodes for the job, it returns the job information to pbs_server . The first node on this list becomes the mother superior and the rest are called sister MOMs or sister nodes. pbs_server allocates the resources and passes the job control as well as execution information to the pbs_mom daemon installed on the mom superior node instructing to launch the job on the assigned compute nodes. The pbs_mom daemons on the compute nodes manage the execution of jobs and monitor resource usage. pbs_mom will capture all the outputs and direct them to stdout and stderr which are written into the output and error files and are copied to the designated location when the job completes successfully. The job status (completed or terminated) will be passed to pbs_server by pbs_mom . The job information will be updated. In TORQUE, nodes are partitioned into different groups called queues . In each queue, the administrator sets limits for resources such as walltime and job size. This feature can be useful for job scheduling in a large HPC cluster where nodes are heterogeneous or certain nodes are reserved for special users. This feature is commonly seen in HPC workload managers. TORQUE has a default scheduler FIFO, and is often integrated with a more sophisticated job scheduler, such as Maui [59]. Maui is an open source job scheduler that provides advanced features such as dynamic job prioritisation, configurable parameters, extensive fair share capabilities and backfill scheduling. Maui functions in an iterative manner like most job schedulers. It starts a new iteration when one of the following conditions is met: (1) a job or resource state alters; (2) a reservation boundary event occurs; (3) an external command to resume scheduling is issued; (4) a configuration timer expires. In each iteration, Maui follows the below steps [60]: Obtain resource records from TORQUE; Fetch workload information from TORQUE; Update statistics; Refresh reservations; Select jobs that are eligible for priority scheduling; Prioritise eligible jobs; Schedule jobs by priority and create reservations; Backfill jobs. Despite an abundance of algorithms, only a few scheduling strategies are practically in use by job schedulers. Backfilling scheduling [61] allows jobs to take the reserved job slots if this action does not delay the start of other jobs having reserved the resources, thus allowing large parallel jobs to execute and avoiding resource underutilisation. Differently, Gang scheduling [62] attempts to take care of the situations when the runtime of a job is unknown, allowing smaller jobs to get fairer access to the resources. Both scheduling strategies are also seen in SLURM and backfilling can be also found in LSF. 2) SLURM The structure of a SLURM (Simple Linux Utility for Resource Management) [58] managed cluster is composed of one or two SLURM servers and many compute nodes. Its procedure of job submission is similar to that of TORQUE. Fig. 4 illustrates the structure of SLURM. Its server hosts the slurmctld daemon which is responsible for cluster resource and job management. SLURM servers and the corresponding slurmctld daemons can be deployed in an active/passive mode in order to provide services of high reliability for computing clusters. Each compute node hosts one instance of the slurmd daemon, which is responsible for job staging and execution. There are additional daemons, e.g., slurmdbd which allows to collect and record accounting information for multiple SLURM-managed clusters and slurmrestd that can be used to interact with SLURM through a REST API (RESTful Application Programming Interface). The SLURM resource list is held as a part of the slurm.conf file located on SLURM server nodes, which contains a list of nodes including features (e.g., CPU speed and model, amount of memory) and configured partitions (named queue in PBS) including partition names, list of associated nodes and job priority. Fig. 4. SLURM structure. Show All Both PBS and SLURM have little (if at all) dedicated supports for container workloads. Containers are only scheduled as conventional HPC workloads, e.g lacking of load-balancing supports. 3) Spectrum LSF IBM platform Load Sharing Facility (LSF), targeted for enterprises, is designed for distributed HPC deployments. LSF is based on the Utopia job scheduler [55] developed at the University of Toronto. Its Session Scheduler runs and manages short-duration batch jobs, which enables users to submit multiple tasks as a single LSF job, consequently reduces the number of job scheduling decisions. Session Scheduler can efficiently share resources regardless of job execution time and can make thousands of scheduling decisions per second. These capabilities create a focus on throughput which is often critical for HPC workloads. Fig. 5 illustrates the structure of LSF. Its license scheduler allows to make policies that control the way software licenses are shared among users within an organisation. Jobs are submitted via the command line interface, API or IBM platform application centre. Job submission carries similar procedure as in TORQUE. Fig. 5. Spectrum LSF structure. Show All LSF supports container workloads: Docker, Singularity and Shifter. LSF configures container runtime control in the application profile4 that is managed by the system administrator. Users do not need to consider which containers are used for their jobs, instead only need to submit their jobs to the application profile and LSF automatically manages the container runtime control. Section IV-C3 elaborates this feature in more details. B. Orchestration Frameworks on Cloud Cloud clusters often include orchestration mechanisms to coordinate tasks and hardware resources. Cloud has evolved mature orchestrators to manage containers efficiently. Container orchestrators can offer [11], [15], [37]: Resource limit control. Reserve a specific amount of CPUs and memory for a container, which restrains interference from other containers and provides information for scheduling decisions; Scheduling. It determines the policies that optimise the placement of containers on nodes; Load balancing. It distributes workloads among container instances; Health check. It verifies if a faulty container needs to be destroyed or replaced; Fault tolerance. It allows to maintain a desired number of containers; Auto-scaling. It automatically adds and removes containers. Additionally, a container orchestrator should also simplify networking, enable service discovery and support continuous deployment [63]. 1) Kubernetes Kubernetes originally developed by Google is among the most popular open-source container orchestrators, which has a rapidly growing community and ecosystem with numerous platforms being developed upon it. The architecture of Kubernetes comprises a master node and a set of worker nodes. Kubernetes runs containers inside pods that are scheduled to run either on master or worker nodes. A pod can include one or multiple containers. Kubernetes provides its services via deployments that are created by submission of yaml files. Inside a yaml file, users can specify services and computation to perform on the cluster. A user deployment can be performed either on the master node or the worker nodes. Kubernetes is based on a highly modular architecture which abstracts the underlying infrastructure and allows internal customisation, such as the deployment of software-defined networks or storage solutions. It also supports various big-data frameworks, such as Hadoop MapReduce [64], Spark [65] and Kafka [66]. Kubernetes incorporates a powerful set of tools to control the life cycle of applications, e.g., parameterised redeployment in case of failures and state management. Furthermore, it supports software-defined infrastructures5 [67] and resource disaggregation [68] by leveraging container-based deployments and particular drivers (e.g., Container Runtime Interface driver, Container Storage Interface driver and Container Network Interface driver) based on standardised interfaces. These interfaces enable the definition of abstractions for fine-grain control of computation, states and communication in multi-tenant Cloud environments along with optimal usage of the underlying hardware resources. Kubernetes incorporates a scheduling system that permits users to specify different schedulers for each job. The scheduling system makes the decisions based on two steps before the actual scheduling operations: Node filtering. The scheduler locates the node(s) that fit(s) the workload, e.g., a pod is specified with node affinity, therefore, only certain nodes can meet the affinity requirements or some nodes may not include enough CPU resources to serve the request. Normally the scheduler does not traverse the entire node list, instead it selects the one/ones detected first. Node priority calculation. The scheduler calculates a score for each node, and the highest scoring node will run that pod. Kubernetes has started being utilised to assist HPC systems in container orchestration (Section IV-C). 2) Docker Swarm Docker Swarm [69] is built for the Docker engine. It is a much simpler orchestrator comparing with Kubernetes, e.g., it offers less rich functionalities, limited customisations and extensions. Docker Swarm is hence lightweight and suitable for small workloads. In contrast, Kubernetes is heavyweight for individual developers who may only want to set up an orchestrator for simplistic applications and perform infrequent deployments. Nevertheless, Docker Swarm still has its own API, and provides filtering, scheduling and load-balancing. API is a strong feature commonly used in Cloud orchestrators, as it enables applications or services to talk to each other and provides connections with other orchestrators. The functionalities of Docker Swarm may be applied to perform container orchestration on HPC systems as detailed in Section IV-C3. 3) Apache Mesos and YARN Apache Mesos [70] is a cluster manager that provides efficient resource isolation and sharing across distributed applications or frameworks. Mesos removes the centralised scheduling model that would otherwise require to compute global schedules for all the tasks running on the different frameworks connected to Mesos. Instead, each framework on a Mesos cluster can define its own scheduling strategies. For instance, Mesos can be connected with MPI or Hadoop [71]. Mesos utilises a master process to manage slave daemons running on each node. A typical Mesos cluster includes 3 ∼ 5 masters with one acting as the leader and the rest on standby. The master controls scheduling across frameworks through resource offers that provide resource availability of the cluster to slaves. However, the master process only suggests the amount of resources that can be given to each framework according to the policies of organisations, e.g fair sharing. Each framework rules which resources or tasks to accept. Once a resource offer is accepted by a framework, the framework passes Mesos a description of the tasks. The slave comprises two components, i.e., a scheduler registered to the master to receive resources and an executor process to run tasks from the frameworks. Mesos is a non-monolithic scheduler which acts as an arbiter that allocates resources across multiple schedulers, resolves conflicts, and ensures fair distribution of resources. Apache YARN (Yet Another Resource Negotiator) [72] is a monolithic scheduler which was developed in the first place to schedule Hadoop jobs. YARN is designed for long-running batch jobs and is unsuitable for long-running services and short-lived interactive queries. Mesosphere Marathon6 is a container orchestration framework for Apache Mesos. Literature has seen the usage of Mesos together with Marathon in container orchestration on HPC systems as detailed in Section IV-C3. 4) Ansible Ansible [73] is a popular software orchestration tool. More specifically, it handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation and multi-node orchestration. The architecture of Ansible is simple and flexible, i.e., it does not require a special server or daemons running on the nodes. Configurations are set by playbooks that utilise yaml to describe the automation jobs, and connections to other nodes are via ssh. Nodes managed by Ansible are grouped into inventories that can be defined by users or drawn from different Cloud environments. Ansible is adopted by the SODALITE framework (Section IV-C4) as a key component to automatically build container images. 5) OpenStack OpenStack [74] is mostly deployed as infrastructure-as-a-service (IaaS)7 [75] on Cloud. It can be utilised to deploy and manage cloud-based infrastructures that support various use cases, such as web hosting, Big Data projects, software as a service (SaaS) [76] delivery and deployment of containers, VMs or bare-metal. It presents a scalable and highly adaptive open source architecture for Cloud solutions and helps to leverage hardware resources [77]. It also manages heterogeneous compute, storage and network resources. Together with its support of containers, container orchestrators such as Docker Swarm, Kubernetes and Mesos, Openstack enables the possibilities to quickly deploy, maintain, and upgrade complex and highly available infrastructures. OpenStack is also used in HPC communities to provide IaaS to end-users, enabling them to dynamically create isolated HPC environments. Academia and industry have developed a plethora of Cloud orchestrators. This article only reviews the ones that are mostly relevant to the HPC communities and the ones that have seen their usage in container orchestration for HPC systems, and the rest is out of the scope herein. C. Bridge Orchestration Strategies Between HPC and Cloud There are numerous works in literature [11], [78], [79], [80] on container orchestration for Cloud clusters, however, they are herein out of the scope. This section reviews the works that have been performed on the general issues of bridging the gap between conventional HPC and service-oriented infrastructures (Cloud). Overall, the state-of-the-art works on container orchestration for HPC systems fall into four categories as illustrated in Fig. 6. Added functionalities to HPC workload managers. It relies on workload managers for resource management and scheduling; meanwhile adopts additional software such as MPI for container orchestration. Fig. 6. The four types of container orchestration on HPC systems. Show All Connector between Cloud and HPC. Containers are scheduled from Cloud clusters to HPC clusters. This architecture isolates the HPC resources from Cloud so as to ensure HPC environment security; meanwhile offers application developments with flexible environments and powerful computing resources. Cohabitation. Workload managers and Cloud orchestrators co-exist on an HPC cluster, such as IBM LSF-Kubernetes. This gives a direction for the provision of HPC resources as services. In practice, the HPC workload managers and Cloud orchestrators do not coexist in one cluster. Meta-orchestration. An additional orchestrator is implemented on top of the Cloud orchestrator and HPC workload manager. There are pros and cons of the above four categories, which are outlined in Table VI. In addition, a research and engineering trend [12], [30], [81], [82], [83] is to move HPC applications to Cloud, as Cloud provides flexible and cost-effective services which are favoured by small-sized or middle-sized business. Beltre et al. [84] proposed to manage HPC applications by Kubernetes on a Cloud cluster with powerful computing resources and InfiniBand, which demonstrated comparable performance in containerised and bare-metal environments. The approach of this kind may be extended to HPC systems, however, remains unpractical for HPC centres to completely substitute their existing workload managers. TABLE VI A List of the Related Work on Container Orchestration for HPC Systems 1) Added Functionalities to WLM A potential research direction is to complement workload managers with container orchestration or make use of the existing HPC software stacks. Wofford et al. [85] simply adopt Open Runtime Environment (orted) reference implementation from Open MPI to orchestrate container launch suitable for arbitrary batch schedulers. Julian et al. [86] proposed their prototype for container orchestration in an HPC environment. A PBS-based HPC cluster can automatically scale up and down as load demands by launching Docker containers using the job scheduler Moab [98]. Three containers serve as the front-end system, scheduler (it runs PBS and Moab inside) and compute node (launches pbs_mom daemon, see Section IV-A1). More compute node containers are scheduled when there is no sufficient number of physical nodes. Unused containers are destroyed via external Python scripts when jobs complete. This approach may offer a solution for resource elasticity on HPC systems (Section V-B6). Similarly, an early study [87] described two models that can orchestrate Docker containers using an HPC workload manager. The former model launches a container to behave as one compute node which holds all assigned processes, whilst the latter boots a container per process by MPI launchers. The latter work seems to be outdated as to MPI applications which can be now automatically scaled with Singularity support. 2) Connector Between Cloud and HPC Cloud technologies are evolving to be able to support complex applications of HPC, Big Data and AI. Nevertheless, the applications with intensive computation and high inter-processor communication could not scale well, particularly due to the lack of low latency networks (e.g., InfiniBand) and the usage of network virtualisation for network isolation. A research and development trend is to converge HPC and Cloud in order to take advantage of the resource management and scheduling of both HPC and Cloud infrastructures with minimal intrusion to HPC environments. Furthermore, the software stack and workflows in Cloud and HPC are usually developed and maintained by different organisations and users with various goals and methodologies, hence a connector between HPC and Cloud systems would bridge the gap and solve compatibility problems. Zhou et al. [88], [89], [90], [91] described the design of a plugin named Torque-Operator that serves as the key component to its proposed hybrid architecture. The containerised AI applications are scheduled from the Kubernetes-managed Cloud cluster to the TORQUE-managed HPC cluster where the performance of the compute-intensive or data-demanding applications can be significantly enhanced. This approach is less intrusive to HPC systems, however, its architecture shows one drawback: the latency of the network bridging the Cloud and HPC clusters can be high, when a large amount of data needs to be transferred in-between. DKube8 is a commercial software that is able to execute a wide range of AI/ML components scheduled from Kubernetes to SLURM. The software comprises a Kubernetes plugin and a SLURM Plugin. The former is represented as a hub that runs MLOps (Machine Learning Operations) management and associated Kubernetes workloads, while the latter connects to SLURM. 3) Cohabitation Liu et al. [92] showed how to dynamically migrate computing resources between HPC and OpenStack clusters based on demands. At a higher level, IBM has demonstrated the ability to run Kubernetes pods on Spectrum LSF where LSF acts as a scheduler for Kubernetes. An additional Kubernetes scheduler daemon needs to be installed into the LSF cluster, which acts as a bridge between LSF and the Kuberentes server. Kubelet will execute and manage pod lifecycle on target nodes in the normal fashion. IBM released LSF connector to Kubernetes, which makes use of the core LSF scheduling technologies and Kubernetes API functionalities. Kubernetes needs to be installed in a subset of the LSF managed HPC cluster. This architecture allows users to run Kubernetes and HPC batch jobs on the same infrastructure. The LSF scheduler is packed into containers and users submit jobs via kubectl. The LSF scheduler listens to the Kubernetes API server and translates pod requests into jobs for the LSF scheduler. This approach can add additional heavy workloads to HPC systems, as Kubernetes relies deployments of services across clusters to perform load balancing, scheduling, auto scheduling, etc. Piras et al. [93] implemented a method that expanded Kubernetes clusters with HPC clusters through Grid Engine. Submission is performed by PBS jobs to launch Kubernetes jobs. Therefore, HPC nodes are added to Kubernetes clusters by installing Kubernetes core components (i.e., kubeadm and Kubelet) and Docker container engine. On HPC, especially HPC production systems in HPC centres, adding new software packages that require using root privileges can cause security risks and alter the working environments of current users. The security issues will be further elaborated in Section V-A4. Khan et al. [1] proposed to containerise HPC workloads and install Mesos and Marathon (Section IV-B3) on HPC clusters for resource management and container orchestration. Its orchestration system can obtain the appropriate resources satisfying the needs of requested services within defined Quality-of-Service (QoS) parameters, which is considered to be self-organised and self-managed meaning that users do not need to specifically request resource reservation. Nevertheless, this study has not shown insight into novel strategies of container orchestration for HPC systems. Wrede et al. [94] performed their experiments on HPC clusters using Docker Swarm as the container orchestrator for automatic node scaling and using C++ algorithmic skeleton library Muesli [99] for load balance. Its proposed working environment is targeted for Cloud clusters. Usage of Docker cannot be easily extended to HPC infrastructures especially to HPC production systems due to the security risks. 4) Meta-Orchestration Croupier [95] is a plugin implemented on Cloudify9 server that is located at a separate node in addition to the nodes that are managed by an HPC workload manager and a Cloud orchestrator. Croupier establishes a monitor to collect the status of every infrastructure and the operations (e.g., status of the HPC batch queue). Croupier together with Cloudify, can orchestrate batch applications in both HPC and Cloud environments. Similarly, Di Nitto et al. [96] presented the SODALITE10 framework by utilising XOpera11 to manage the application deployment in heterogeneous infrastructures. Colonnelli et al. [97] presented a proof-of-concept framework (i.e., Streamflow) to execute workflows on top of the hybrid architecture consisting of Kubernetes-managed Cloud and OCCAM [100] HPC cluster. D. Section Highlights HPC workload managers and Cloud orchestrators have distinct ways to manage clusters mainly because of their types of workloads and hardware resource availabilities. Table V summaries the differences of key features between HPC workload managers and Cloud Orchestrators. Typical HPC jobs are large workloads with long but ascertainable execution time and large throughput. HPC jobs are often submitted to a batch queue within a workload manager where jobs wait to be scheduled from minutes to days. Per contra, job requests can be granted immediately on Cloud as resources are available on demand. Batch-queuing is insufficient to satisfy the needs of Cloud communities: most of jobs are short in duration and the Cloud services are persistently long-running programs. Most of the HPC workload managers support Checkpointing that allows applications to save the execution states of a running job and restart the job from the checkpointing when a crash happens. This feature is critical for an HPC application with execution time typically from hours to months. Because it enables the application to recover from error states or resume from the state when it was previously terminated by the workload manager when its walltime limit had been reached or resource allocation had been exceeded. In contrary, jobs on Cloud, which are often micro-service programs, are usually relaunched in case of failures [101]. A container orchestrator offers an important property, i.e., container status monitoring. This is practical for long-running Cloud services, as it can monitor and replace unhealthy containers per desired configuration. HPC systems do not offer the equivalence of container pod which bundle performance monitoring services with the application itself as in Cloud systems [13]. Additionally, HPC workload managers often do not provide capabilities of application elasticity or necessary API at execution time, however, these capabilities are important for task migration and resource allocation changes at runtime on Cloud [102]. Section IV-C has reviewed the approaches to address the issues of container orchestration on HPC systems, which are summarised in Table VI. Overall, a container orchestrator on its own does not address all the requirements of HPC systems [3], as a result cannot replace existing workload managers in HPC centres. An HPC workload manager lacks micro-service support and deeply-integrated container management capabilities in which container orchestrators manifest their efficiency. SECTION V. Research Challenges and Vision The distinctions between Cloud and HPC clusters are diminishing, especially with the trend of HPC Clouds in industry [103]. HPC Cloud is becoming an alternative to on-premise HPC clusters for executing scientific applications and business analytics models [16]. Containerisation technologies help to ease the efforts of moving applications between Cloud and HPC. Nevertheless, not all applications are suitable for containerisation. For instance, in the typical HPC applications such as weather forecast or modelling of computational fluid dynamics, any virtualisation or high-latency networks can become the bottlenecks for performance. Containerisation in HPC still faces challenges of different folds (Section V-A). Interest in using containers on HPC systems is mainly due to the encapsulation and portability that yet may trade off with performance. In practice, containers deployed on HPC clusters often have large image size and as a result each HPC node can only host a few containers that are CPU-intensive and memory-demanding. In addition, implementation of AI frameworks such as TensorFlow and PyTorch [104] typically also have large container image size. Architecture of HPC containers should be able to easily integrate seamlessly with HPC workload managers. The research directions (Section V-B) which can be envisioned are not only to adapt the existing functionalities from Cloud to HPC, but to also explore the potentials of containerisation so as to improve the current HPC systems and applications. A. Challenges and Open Issues Although containerisation enables compatibility, portability and reproducibility, containerised environments still need to match the host architecture and exploit the underlying hardware. The challenges that containerisation faces on HPC systems are in three-fold: compatibility, security and performance. Some issues still remain as open questions. Table VII summarises the potential solutions to the research challenges and the open questions that will be discussed in this section. TABLE VII Overview of Research Challenges and Potential Solutions 1) Library Compatibility Issues Mapping container libraries and their dependencies to the host libraries can cause incompatibility. Glibc [105], which is an implementation of C standard library that provides core supports and interfaces to kernel features, can be a common library dependency. The version of Glibc on the host may be older or newer than the one in the container image, consequently introducing symbol mismatches. Additionally, when the container OS (e.g., Ubuntu 18.04) and the host OS are different (e.g., CentOS 7), it is likely that some kernel ABI are incompatible, which may lead to container crashes or abnormal behaviours. This issue can also occur to MPI applications. As a result users must either build an exact version of the host MPI or have the privilege to mount the host MPI dependency path into the container. A research direction to handle library mismatches between container images and hosts is to implement a container runtime library at a lower level. For instance, Nvidia implemented the library libnvidia-container12 that manages driver or library matching at container runtime, i.e., using a hook interface to inject and/or activate the correct library versions. However, the libnvidia-container library can be only applied to Nividia GPUs. A significant modification of this library code is likely to be needed in order to be adapted for other GPU suppliers. In practice, such a compatibility layer would also require supports from different HPC interconnect and accelerator vendors. 2) Compatibility Issues of Container Engines and Images Not all Docker images can be converted by HPC container engines to their own formats. Moreover, to reuse HPC container implementations between container engines, users need to learn different container command lines to build the corresponding images, which further complicates adoption of containers for HPC applications. This issue calls for container standardisation. OCI is a Linux foundation project that designs open standards for container image formats (a filesystem bundle or rootfs) and multiple data volume [106]. Some guidelines were proposed in [63], i.e., a container should be: Not bound to higher-level frameworks, e.g., an orchestration stack; Not tightly associated with any particular vendor or project; Portable across a wide variety of OSs, hardware, CPUs, clusters, etc. Unfortunately, this standard cannot guarantee that the runtime hooks built for one runtime can be used by another. For example, container privileges (e.g., mount host filesystems) assumed by one container runtime may not be translated to unprivileged runtimes (e.g., not all HPC centres have mount namespace enabled) [107]. 3) Kernel Optimisation In general, containers are forbidden by the host to install their own kernel modules for the purpose of application isolation [108]. This is a limitation for the applications requiring kernel customisation, because the kernels of their HPC hosts cannot be tuned and optimised. Shen et al. [108] proposed an Xcontainer to address this issue by tuning the Linux kernel into library OS that supports binary compatibility. This functionality is yet to be explored in HPC containers. 4) Security Issues Containers face three major threats [109]: Privilege Escalation. Attackers gain access to hosts and other containers by breaking out of their current containers. Denial-of-Service (DoS). An attack causes services to become inaccessible to users by disruption of a machine or network resources. Information Leak. Confidential details of other containers are leaked and utilised for further attacks. Multiple or many containers share a host kernel, therefore, one container may infect other containers. In this case, a container does not reduce attack surfaces, but rather brings multiple instances of attack surfaces. For example, starting from version V3.0, Singularity has added Cgroups support that allows users to limit the resources consumed by containers without the help from a batch scheduling system (e.g., TORQUE). This feature helps to prevent DoS attacks when a container seizes control of all available system resources which prohibits other containers from operating properly. Execution of HPC containers (including the Docker Engine starting from v19.03) does not require root privileges on the host. Containers in general adopt namespaces to isolate resources among users and map a root user inside a container to a non-root user on the host. The User namespace nevertheless is not a panacea to resolve all problems of resource isolation. User exposes code in the kernel to non-privileged users, which was previously limited to root users. A container environment is generated by users, and it is likely that some software inside a container may be embedded with security vulnerabilities. Root users inside a container may escalate their privileges via application level vulnerability. This can bring security issues to the kernel that does not account for mapped PIDs/GIDs. This issue can be addressed in two ways: (1) avoiding root processes inside HPC containers; (2) installing container engines with user permission instead of sudo installation. Security issues of the user namespace continue to be discovered even in the latest version of Linux kernels. Therefore, many HPC production centres have disabled the configuration of this namespace, which prevents usage of almost any state-of-the-art HPC containers. How to address the risks of using namespaces still remains an open question. 5) Performance Degradation GPU and accelerators often require customised or proprietary libraries that need to be bound to container images so as to leverage performance. This operation is at the cost of portability [107]. It is de facto standard to utilise the optimised MPI libraries for HPC interconnects, such as InfiniBand and Slingshot [110], and it is likely that the container performance degrades in a different HPC infrastructure [22] (see Section III-B). There is no simple solution to address this issue. Another example presented in [111] identified the performance loss due to increasing communication cost of MPI processes. This occurs when the number of containers (MPI processes running inside containers) rises on a single node, e.g., point to point communication (MPI_Irecv, MPI_Isend), polling of pending asynchronous messages (MPI_Test) and collective communication (MPI_Allreduce). B. Research and Engineering Opportunities Research studies should continue working on solutions to the open question identified in Section V-A. This section discusses current research and engineering directions that are interesting, yet still need further development. This section also identifies new research opportunities that yet need to be explored. The presentation of this section is arranged from short-term vision to long-term efforts. Table VIII summarises the potentials discovered in literature and the prospects given by the authors. TABLE VIII Future Directions of Research and Engineering 1) Containerisation of AI in HPC Model training of AI/DL applications can immensely benefit from the compute power (GPU or CPU), storage and security [112] of HPC clusters in addition to the superior GPU-aware scheduling and features of workflow automation provided by workload managers. The trained models are subsequently deployed on Cloud for scalability at low cost and on HPC for computation speed. Exploiting HPC infrastructures for ML/DL training is becoming a topic of increasing importance [113]. For example, Fraunhofer13 has developed the software framework Carme14 that combines established open source ML and Data Science tools with HPC backends. The execution environments of the tools are provided by predefined Singularity containers. AI applications are usually developed with high-level scripting languages or frameworks, e.g., TensorFlow and PyTorch, which often require connections to external systems to download a list of open-source software packages during execution. For instance, an AI application written in Python cannot be compiled into an executable that has included all the dependencies ready for execution as in C/C++. Therefore, the developers need flexibility to customise the execution environments. Since HPC environments, especially on HPC production systems, are often based on closed-source applications and their users have restricted account privileges and security restrictions [6], deployment of AI applications on HPC infrastructures is challenging. Besides the predefined module environments or virtual environments (such as Anaconda), containerisation can be an alternative candidate, which enables easy transition of AI workloads to HPC while fully taking advantage of HPC hardware and the optimised libraries of AI applications without compromising security. Huerta et al. [114] recommend three guidelines for containerisation of AI applications for HPC centres: Provide up-to-date documentation and tutorials to set up or launch containers. Maintain versatile and up-to-date base container images that users can clone and adapt, such as a container registry (see Section V-B2). Give instructions on installation or updates of software packages into containers. The AI program depends on distributed training software, such as Horovod [115], which then depends on system architecture and specific versions of software packages such as MPI. Increasing amount of new software frameworks are being developed using containerisation technologies to facilitate deployment of AI applications on HPC systems. Further research is still needed to improve scalability and enable out-of-box usage. 2) HPC Container Registry Container registry is a useful repository to provide pre-built container images that can be accessed easily either by public or private users by pulling images to the host directly. It is portable to deploy applications in this way on Cloud clusters. Accesses to external networks are often blocked in HPC centres, so users need to upload images onto the clusters manually. One solution is to set up a private registry within the HPC centres that offer pre-built images suitable for the targeted systems and architectures. A container registry is also a way to ensure container security. It is a good security practice to ensure that images executed on the HPC systems are signed and pulled from a trusted registry. Scanning vulnerabilities on the registry should be regularly performed. To simplify usage, the future work can enable HPC workload managers to boot the default containers on the compute nodes (by pulling images from the private registry) which match the environments with all the required libraries and configuration files of user login nodes where users implement their own workflows and submit their jobs. The jobs should be started without user awareness of the presence of containers and without additional user intervention. 3) Linux Namespace Guidelines The set of Linux namespaces used within an implementation depends on the policies of HPC centres [116]. HPC centres should provide clear instructions on the availabilities of namespaces. For example, different user groups may have different namespaces enabled or disabled. A minimal set of namespaces should be enabled for a general user group: mount and user, which are suitable for node-exclusive scheduling. PID and Cgroups should be provided to restrict resource usage and enforce process privacy, which are useful for shared-node scheduling. Advanced use cases may require additional sets of namespaces. When users submit the container jobs, workload managers can start the containers with appropriate namespaces enabled. 4) DevOps DevOps aims at integrating efforts of development (Dev) and operations (Ops) to automate fast software delivery while ensuring correctness and reliability [117], [118]. This concept is influential in Cloud Computing and has been widely adopted in industry, as DevOps tools minimise the overhead of managing a large amount of micro-services. In HPC environments, typical applications have large workloads, hence the usage of DevOps should concentrate on research reproducibility. Nevertheless, the off-the-shelf DevOps tools are not well fitted for HPC environments, e.g., the dependencies of MPI applications are too foreign for the state-of-the-art DevOps tools. A potential solution is to develop HPC-specific DevOps tools for the applications that are built and executed on on-premise clusters [16]. Unfortunately, HPC environments are known to be inflexible and typical HPC applications are optimised to leverage resources, thereby generation of DevOps workflows can be restricted and slow. Such obstacles can be overcome by containerisation, which may provision DevOps environments. For instance, Sampedro et al. [119] integrate Singularity with Jenkins [120] that brings CICD15 practices into HPC workflows. Jenkins is an open-source automation platform for building and deploying software, which has been applied at some HPC sites as a general-purpose automation tool. 5) Middleware System A middleware system, which bridges container building environments with HPC resource managers and schedulers, can be flexible. A middleware system can be either located on an HPC cluster or connect to it with secured authentication. The main task of the middleware is to perform job deployment, job management, data staging and generating non-root container environments [121]. Different container engines can be swiftly switched, optimisation mechanisms can be adapted to the targeted HPC systems and workflow engines [122] can be easily plugged in. Middleware systems can be a future research direction that provides a portable way to enable DevOps in HPC centres. 6) Resource Elasticity One major difference between resource management on HPC and Cloud is the elasticity [123], i.e., an HPC workload manager runs on a fixed set of hardware resources and the workloads of its jobs at any point can not exceed the resource capacity, while Cloud orchestrators can scale up automatically the hardware resources to satisfy user needs (e.g., AWS spot instances). Static reservation is a limitation for efficient resource usages on HPC systems [124]. One future direction of containerisation for HPC systems can work towards improvement of the elasticity of HPC infrastructure, which can be introduced to its workload manager. In [123], the authors presented a novel architecture that utilises Kubernetes to instantiate the containerised HPC workload manager. In this way, the HPC infrastructure is dynamically instantiated on demand and can be served as a single-tenant or multi-tenant environment. A complete containerised environments on HPC system may be impractical and much more exploration is still needed. 7) Moving Towards Minimal OS Containers may be utilised to partially substitute the current HPC software stack. Typical compute nodes on HPC clusters do not contain local storage (e.g., hardware disk), therefore lose states after reboots. The compute node boots via a staged approach [116]: (1) a kernel and initial RAM disk are loaded via a network device; (2) a root filesystem is mounted via the network. In a monolithic stateless system, modification of the software components often requires system rebooting to completely activate the functions of updates. Using containerised software packages on top of a minimal OS (base image) on the compute nodes, reduces the number of components in the kernel image, hence decreasing the frequency of node reboots. Furthermore, the base image of reduced size also simplifies the post-boot configurations that need to run in the OS image itself, consequently the node rebooting time is minimised. Additionally, when a failure occurs, a containerised service can be quickly replaced without affecting the entire system. Long-term research is required on HPC workload managers to control the software stack and workloads that are partially native and partially containerised. Moreover, it needs to explored whether containerisation of the entire OS on HPC systems is feasible. SECTION VI. Concluding Remarks This paper presents a survey and taxonomy for the state-of-the-art container engines and container orchestration strategies specifically for HPC systems. It underlines differences of containerisation on Cloud and HPC systems. The research and engineering challenges are also discussed and the opportunities are envisioned. HPC systems start to utilise containers as thereof reduce environment complexity. Efforts have been also made to ameliorate container security on HPC systems. This article identified three points to increase the security level: (1) set on-site container registry, (2) give Linux namespaces guidelines (3) and remove root privilege meanwhile avoid permission escalation. Ideally, HPC containers should require no pre-installation of container engines or installation can be performed without root privileges, which not only meets the HPC security requirements but also simplifies the container usability. Containers will continue to play a role in reducing the performance gap and deployment complexity between on-premise HPC clusters and public Clouds. Together with the advancement of low-latency networks and accelerators (e.g., GPUs, TPUs [125]), it may eventually reshape the two fields. Containerised workloads can be moved from HPC to Cloud so as to temporarily relieve the peak demands and can be also scheduled from Cloud to HPC in order to exploit the powerful hardware resources. The research and engineering trend are working towards implementation of the present container orchestrators within HPC clusters, which however still remains experimental. Many studies have been devoted to container orchestration on Cloud, however, it can be foreseen that the strategies will be eventually introduced to HPC workload managers. In the future, it can be presumed that containerisation will play an essential role in application development, improve resource elasticity and reduce complexity of HPC software stacks. ACKNOWLEDGMENTS The authors would like to thank Dr. Joseph Schuchart for proof-reading the contents. Authors Figures References Citations Keywords Metrics Footnotes More Like This Implementing a Hybrid Virtual Machine Monitor for Flexible and Efficient Security Mechanisms 2010 IEEE 16th Pacific Rim International Symposium on Dependable Computing Published: 2010 I/O for Virtual Machine Monitors: Security and Performance Issues IEEE Security & Privacy Published: 2008 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."

Paper 8:
- APA Citation: Kumar, S., Datta, S., Singh, V., Singh, S. K., & Sharma, R. (2024). Opportunities and challenges in data-centric AI. IEEE Access, 12, 33173-33189.
  Main Objective: This paper provides a comprehensive and critical analysis of the current state and future potential of real-time, end-to-end automated irrigation management systems.
  Study Location: Unspecified
  Data Sources: Literature review
  Technologies Used: Cloud computing, Machine learning, Containerization, Scalable computing
  Key Findings: - Containerization technologies provide efficient deployment and scaling of data processing and machine learning modules in cloud environments.
- Real-time data processing and inference can be achieved by deploying machine learning models in cloud environments using containerization strategies.
  Extract 1: Containerization technologies (e.g., Docker, Kubernetes) provide efficient deployment and scaling of data processing and machine learning modules in cloud environments, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).
  Extract 2: Real-time data processing and inference can be achieved by deploying machine learning models in cloud environments using containerization strategies.
  Limitations: None
  Relevance Evaluation: The paper is highly relevant to my point in the literature review, which focuses on leveraging containerization technologies to optimize the deployment and scaling of data processing and machine learning modules in cloud environments. The paper provides valuable insights into the benefits and implementation strategies of containerization, including automated deployment, resource optimization, and simplified management, making it an essential reference for my discussion.
  Relevance Score: 1.0
  Inline Citation: (Kumar et al., 2024)
  Explanation: The paper emphasizes the need for deploying machine learning modules in cloud environments for efficient data processing and inference, focusing on containerization strategies such as Docker and Kubernetes. Containerization involves packaging applications together with their dependencies into a standalone executable package that can be easily deployed and scaled across multiple machines. This approach enables efficient resource utilization, automated deployment, and simplified management of machine learning modules in the cloud for real-time data processing and inference.

 Full Text: >
"This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising purposes. To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 12 Opportunities and Challenges in Data-Centric AI Publisher: IEEE Cite This PDF Sushant Kumar; Sumit Datta; Vishakha Singh; Sanjay Kumar Singh; Ritesh Sharma All Authors 281 Full Text Views Open Access Comment(s) Under a Creative Commons License Abstract Document Sections I. Introduction II. AI Systems III. Model-Centric AI IV. Data-Centric AI V. Data Development Show Full Outline Authors Figures References Keywords Metrics Footnotes Abstract: Artificial intelligence (AI) systems are trained to solve complex problems and learn to perform specific tasks by using large volumes of data, such as prediction, classification, recognition, decision-making, etc. In the past three decades, AI research has focused mostly on the model-centric approach compared to the data-centric approach. In the model-centric approach, the focus is to improve the code or model architecture to enhance performance, whereas in data-centric AI, the focus is to improve the dataset to enhance performance. Data is food for AI. As a result, there has been a recent push in the AI community toward data-centric AI from model-centric AI. This paper provides a comprehensive and critical analysis of the current state of research in data-centric AI, presenting insights into the latest developments in this rapidly evolving field. By emphasizing the importance of data in AI, the paper identifies the key challenges and opportunities that must be addressed to improve the effectiveness of AI systems. Finally, this paper gives some recommendations for research opportunities in data-centric AI. General architecture of model-centric AI, data-centric AI, model data-centric AI Published in: IEEE Access ( Volume: 12) Page(s): 33173 - 33189 Date of Publication: 23 February 2024 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2024.3369417 Publisher: IEEE Funding Agency: CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation. SECTION I. Introduction Artificial intelligence (AI) models are used in almost all industries today, including automotive, agriculture, healthcare, financial, and semiconductor industries [1], [2], [3]. Earlier, the core of AI was machine learning (ML) algorithms. But when AI spread across many sectors and grew in demand, ML algorithms became a commodity and no longer the core of AI. Instead, the training data would drive greater performance in ML models and systems [4]. “Data is the new oil” was the phrase first used by the data scientist Clive Humby in 2006 [5]. People frequently believe that data is static, a significant factor in ML models becoming commonplace. The data literally means “that which is supplied,” which is true. ML entails downloading a readily available dataset and creating a model for most individuals. Afterward, efforts were directed to towards enhancing the model’s performance. We refer to data in real-time business as the result of the processes. Compared to actual model construction, maintaining and strengthening the deployed models accounts for a larger share of the cost and efficacy of many real-time ML systems. The main challenge for these applications is to improve the data quality [6]. In practical AI applications, 80% of the activities involve data cleansing and preparation. Therefore, an essential duty for an ML engineer to consider is assuring the data quality [7]. Data fluctuates significantly for higher-stakes AI. This data flow has a longer duration and various unfavorable effects. From data collection through model deployment, data movement can occur anywhere [8]. Conventional methods entirely dependent on code are overshadowed by modern-day AI systems, which consider both code and data. Practitioners strive to work on the model/code to improve the AI system instead of focusing more on the data part of the system [4]. However, improving the input data in actual applications is often preferable to experimenting with various algorithms. In the recent publications, 99% of papers have a model-centric focus, while only 1% are data-centric.1 It can be easily comprehend from the Table 1 that the performance improvements have primarily been achieved through a data-centric approach. It appears that the model-centric approach did not result in any significant improvements over the baseline. Despite this, it is likely that practitioners invested a considerable amount of time, possibly hundreds of hours, in developing and refining these algorithms. The case studies in AI, Computer Vision, and other fields showed how enhancing the data will result in a better model [9]. AI systems that are model-centric focus on how to alter the algorithm/code to improve performance. Data-centric AI’s prime focus is to systematically amend the data to achieve a model with better performance. Model-centric and data-centric methods may be balanced well to provide a robust AI solution [10]. The contribution of the work can be summarised as follows: Connecting various data-centric AI techniques and related topics together at a high level with a focus on recent and significant developments. A comprehensive review of data-centric AI techniques. Describe the challenges and solutions of the data-centric AI approaches. Finally, we also give a few recommendations for research opportunities in data-centric AI TABLE 1 Data-Centric AI vs Model-Centric AI The rest of this paper is organized as follows: Section II provides brief highlights of AI systems. An overview and limitations of model-centric AI are presented in Section III, while Section IV discusses the overview, mathematical model, advantages, and exploratory data analysis of data-centric AI. A summary of representative tasks and methods for training data development is provided in Section V. An overview of related work on data-centric AI is given in Section VI. The challenges, solutions, and research direction are outlined in Section VII. A discussion on the merits and limitations of the existing art, along with a few new perspectives, can be found in Section VIII. Finally, the concluding remarks of this paper are presented in Section IX. SECTION II. AI Systems Standard AI systems can be represented as: AI System=Algorithm+Data. (1) View Source For an optimal operation, an AI system requires a balance of both model-centric and data-centric approaches for effective solutions. Model-centric focuses on developing algorithms, while data-centric focuses on the quality and relevance of the data used to train these algorithms. Both are important and need to be considered together for successful AI implementation. The data-centric approach involves addressing the challenges of the completeness, relevance, consistency, and heterogeneity of the data. It is an iterative process that must be integrated with the model-centric approach to achieve an AI system that effectively leverages the code and data. These models can range from data classification and clustering to recommendation systems and natural language processing. AI algorithms such as deep learning, reinforcement learning, and decision trees can be used to process large amounts of data and make predictions or decisions based on that data. The use of AI in data-centric models helps to uncover hidden patterns, relationships, and insights in data, enabling organizations to make data-driven decisions and achieve better outcomes. The integration of AI in data-centric models has the potential to revolutionize various industries, including healthcare, finance, retail, and many others [3], [11], [12]. SECTION III. Model-Centric AI Model-centric methods often follow a structured sequence during production that involves scoping the project, data collection and augmentation, data storage, data cleaning, data visualization, visual analytics, model construction and training, and finally, model evaluation [13]. Fig. 1 shows the production workflow of the model-centric approach starting from data collection to model deployment. In these situations, the development and improvement of the model algorithms typically receive most of the attention, with data engineering being a one-time assignment. Modern approaches also tend to analyze biases and fairness, focusing on loss functions such as cross-entropy, errors in the mean square and mean absolute percentages, etc. In traditional ML production environments, a stronger emphasis is given to the code rather than the resultant trained model to improve its style, readability, and unambiguity. Since the models are to be trained with new data continuously, it necessitates a broader sense of the distinctions between a data scientist and ML engineer to corrode gradually, with all collaborators developing general expertise on skills and best practices for all the domains involved. Even though model training is essential, the development of automated machine learning (AutoML) systems has resulted in a progressive decline in the requisite amount of human intervention [14]. The model-centric design, also known as the application-centric, depending on the use case, typically necessitates minimal work on the side of the data engineer due to the conventional one-time nature of data acquisition and pre-processing. However, data collection and model training were to become a continuous process where subsequent semi-supervised models learn from the same data they collect. In that instance, the workforce would be directed towards constant labelling and augmentation of collected data, and a movement toward data-centric concepts would occur. FIGURE 1. Workflow in model-centric systems. Show All A. Limitations Model-centric AI is predicated on ML approaches that prioritize improving model architectures (algorithm/code) and the underlying hyper-parameters. In this method, data is created once and is maintained throughout the development of the AI system. Model-centric AI has been successful over the past few decades, yet it has some flaws, as shown in Fig. 2. It particularly thrives in organizations and sectors when there are customer platforms with millions of users free to depend on generic fixes. In these conditions, most consumers would be satisfied by a single AI system; nonetheless, outliers would be practically useless. Examples of such organizations and sectors include the advertising sector, where firms like Google, Baidu, Amazon, and Facebook can access vast amounts of data (sometimes in a standardized format), which they may use to build model-centric AI systems. Standardized solutions like those offered by a single AI system cannot be used in sectors like manufacturing, agriculture, or healthcare, where customized solutions are preferred versus one-size-fits-all recipes. Instead, they should conceive their strategy to ensure that their algorithms learns what it needs to learn from having complete data that includes all crucial cases and is labelled consistently. FIGURE 2. Shortcomings of model-centric systems. Show All The concept centers on the prevalence of extensive and dynamic datasets, particularly evident in the advertising sector. In such contexts, a ‘one-size-fits-all’ model consistently delivers satisfactory outcomes by mitigating the impact of outliers. However, this strategy proves impractical when dealing with small datasets. The flaw or limitation does not lie inherently within the model-centric approach itself but rather in recognizing that an effective AI application requires both a algorithm and a dataset. Consequently, any AI solution becomes “narrowly applicable” if the curation of the dataset does not align with the concurrent development of the model. This viewpoint, advocating the fusion of model-centric and data-centric approaches, is explicitly proposed in [15] as a crucial and optimal strategy to enhance AI systems. The suggestion is to refrain from a binary choice between deploying a model-centric or data-centric approach, emphasizing the necessity of synergy between the two for a more comprehensive and effective AI solution. Due to the rapid pace of innovation in today’s technology-driven world, AI models (algorithms) that are modelled may quickly become outdated and require retraining on new data, primarily because significant trends’ unforeseen or unexpected behaviour may outpace the model’s usefulness and relevance. Such model-centric AI’s capabilities are severely constrained in fields where the model itself is created to support ongoing scientific study or as a component of a more complex model. As a result, training data is produced using a smaller model based on the restricted findings and hypotheses. Typically, data collection is not practicable in these situations, or work involves limited data from expensive datasets. In such cases, the shortcomings of model-centric technologies become apparent since training a model-centered algorithm may result in biased findings, mainly if the initial model used to generate the data was built on incomplete knowledge or under a lack of domain expertise. With general trends in deep learning requiring a focus on large amounts of data, there is a general tendency to prefer reusing existing models to fit into our use cases with specific training data. Such a shift is perceived as a step towards the data-centric movement, where the model is fixed, with the only variable inputs being the training data and classifiers. SECTION IV. Data-Centric AI Data-centric AI is a data-engineering strategy that improves an AI system’s performance by methodically boosting the data quality used to train the underlying model under mutually exclusive yet collectively exhaustive methodologies and categories. Data-centric AI can improve the performance of AI models and services through augmentation, extrapolation, and interpolation. Data-centric AI can assist in making AI services more accurate and dependable by expanding the data that is accessible to them and enabling them to use it more efficiently [16]. With the help of training data from many sources, including synthetic data, public datasets, and private datasets, data-centric AI is created utilizing this innovative methodology. This strategy can lessen the time and effort needed to generate training data while also helping to increase the data quality. It can also help increase the effectiveness with which AI services use training data. Additionally, data-centric AI can process additional data sets because the data is personalized. This means that regardless of the magnitude of the dataset, data-centric AI can analyze and learn from it and make decent predictions. A. Mathematical Model The mathematical model of data-centric AI is a model that allows the system to learn from data and make predictions. This model can take various forms, such as decision trees, linear regression, neural networks, or deep learning models. The choice of model depends on the problem domain and the type of data available. The model is trained on a labeled dataset, where the input data is associated with the corresponding output or label. The training process involves minimizing a loss function that measures the difference between the predicted output and the actual label. The goal is to find the model parameters that minimize the loss function and generalize well to unseen data. Once the model is trained, it can be used to make predictions on new input data. The prediction process involves feeding the input data into the model, which outputs a predicted label or value. The prediction accuracy depends on the quality of the model and the amount and quality of the input data. Given a dataset D=( x 1 , y 1 ),( x 2 , y 2 ),…,( x n , y n ) consisting of n samples, where x i ϵX denotes the feature vector and y i ϵY denotes the corresponding label, the goal of data-centric AI is to learn a function f:XϵY that maps each input feature vector x to its corresponding output label y . Define a model function f(X;Θ) that takes the input data X and a set of learnable parameters Θ and outputs a prediction. This function f is learned by optimizing a loss function L(Y,f(X;Θ)) , which measures the discrepancy between the predicted outputs f( x i ) and the true labels y i for each sample in the dataset. Mathematically, finding the set of parameters Θ that minimizes the loss function can be expressed as: Θ ∗ = argmin Θ L(Y,f(X;Θ)) = argmin Θ 1 N ∑ i=1 N L( y i ,f( x i ;Θ)) (2) (3) View Source This optimization problem is typically solved using techniques from optimization theory, such as gradient descent or stochastic gradient descent. Where Θ ∗ is the optimal set of parameters that minimize the loss function. The optimization problem in the equation is typically solved using an iterative algorithm such as gradient descent. At each iteration t , the parameters Θ t are updated using the gradient of the loss function with respect to Θ : Θ t+1 = Θ t −α▽ΘL(Y,f(X; Θ t )) (4) View Source where α is the learning rate that determines the step size of the update, and ▽ΘL(Y,f(X; Θ t )) is the gradient of the loss function with respect to Θ at iteration t . The iterative process continues until convergence, i.e., until the change in the loss function between successive iterations falls below a certain threshold. Once the optimal set of parameters Θ ∗ is found, the model function f(X; Θ ∗ ) can be used to make predictions on new input data. Justifications: The model is based on the premise that the performance of an AI system is dependent on the quality of the data on which it is trained. This is reflected in the model through the use of a data-centric loss function, which prioritizes the reduction of errors in the training data over the complexity of the model. The mathematical justification for this approach can be found in the field of statistical learning theory, which provides a framework for understanding the relationship between the complexity of a model and its ability to generalize to new data. The theory suggests that simpler models are more likely to generalize well, as they are less likely to overfit the training data. The data-centric model also incorporates techniques such as regularization and early stopping to prevent overfitting and improve the generalization performance of the model. These techniques have been extensively studied and validated in the literature, providing further mathematical justification for their use in the model. Furthermore, the model emphasizes the importance of data pre-processing and feature engineering, which can have a significant impact on the performance of an AI system. This is supported by research in the field of ML, which has shown that carefully selecting and preprocessing features can improve the accuracy of a model. B. Advantages Data-centric AI is not confined to a particular data type, it can learn from text, images, audio, and video. A data-centric AI approach typically involves utilizing the proper labels, addressing any issues, removing noisy data inconsistencies, enhancing data through data augmentation and feature engineering, analyzing errors with the help of domain experts to determine the accuracy or error in data points, and so on [17], [18], [19]. Data-centric AI constantly evaluates the created AI model in conjunction with updated data. Typically, an AI model is trained on a dataset only once during the production stage, before the software development process can be finalized with the deployment of the model and its necessary features. However, the underlying model may encounter edge-case instances of data points that differ significantly from those encountered during the training phase. This is anticipated, as the workflow of data-centric AI involves continuous improvement of data, particularly in industries that cannot afford to have a large amount of data points, such as manufacturing, agriculture, and healthcare [20]. As a result, evaluating the model’s quality would also happen more regularly than only once. A model would be able to recognize, judge, and then answer appropriately to variational data distributions owing to the production systems’ capacity to offer rapid feedback. In fact, this ability gives data-centric approaches a competitive advantage over their model-centric counterparts. The advantages of data-centric systems are highlighted in Fig. 3. FIGURE 3. Advantages in data-centric systems. Show All The ever-arising challenges and problems in today’s world require continuous optimization and tuning of the model, along with simultaneous collection, processing, augmentation, and labelling of high-volume data. In cases of filter-list-based blocking/moderation of social media content, restriction of cyber-threats, fraud detection, and spam tracking, a shift from a data-driven or application-centric to a data-centric perspective focusing on labelling and cleaning of data is apparent, with the information being collected in high frequency, at an hourly basis or even faster. The boundaries between business and technology are vanishing, with tools and techniques like ML and DL requiring assistance from domain experts and consultants to modify inputs or generate better algorithms. DL-based approaches have become popular owing to the enormous scope and capability for collecting, storing, and processing Big Data, mainly due to their excellent performance with big data and technological advances [21]. In deep-learning-based applications, the distinguishing hierarchy and structure of features or parameters are learned from the data. At the same time, they are usually coded by a human domain expert in typical machine learning applications. Then, through algorithms such as gradient descent and backpropagation, the deep learning algorithm learns and fits itself for accuracy. This methodology allows for mimicking the human brain at a primitive level, allowing DL models to make predictions more precisely through a combination of weights, inputs, and biases. Since a massive volume of data is processed through multiple layers of neural networks, the aspect of clean, labelled information becomes vital, as the presence of dirty, repeated, inconsistent data can cause unnatural biases, failure in edge cases, erroneous predictions, the poor performance of the model, wasteful computations, etc. Most real-world datasets are noisy, unstructured, and unorganized, with several biases, outliers, missing values, repeated values, etc. C. Significance Model-centric AI assumes ML solutions that mainly focus on optimizing model architectures (algorithm/code) along with the underlying hyperparameters. Data within this approach is created only once and kept the same over the AI system’s development life cycle. Model-centric AI fails in many real-world scenarios where the data continuously improves, like in drug discovery, the drug-related data keeps generating as the new disease evolves. In such a situation, the model built using the model-centric approach fails as the data points it encounters are entirely different from those encountered during the training phase. The ignorance of updated data by model-centric AI also prohibits it from being generalized across datasets and also makes it susceptible to adversarial samples [13], [15]. In contrast, Data-centric AI considers ML solutions that emphasize continuous improvement of data and evaluation of the developed AI model in conjunction with data updates throughout the lifecycle of an AI project [22]. As a result, it can adapt to ever-changing real-world situations. D. Execution and Analysis Data preparation is a notable example of the tedious step of the ML lifecycle. Since data quality directly affects a model’s quality, it is also one of the most crucial processes. This section will discuss the significance of exploratory data analysis (EDA), data visualization, and other tools for preparing data for ML pipelines and identifying data quality problems [23]. Data scientists examine and glean essential insights from the data using EDA techniques. Additionally, efficient EDA dramatically benefits from the talents and subject expertise of data scientists in this area. To encourage more statisticians, particularly academics, to research a wide range of fascinating difficulties, we present the traditional yet current subject of data quality from a statistical perspective. The data quality landscape is discussed along with the research underpinnings in computer science, overall quality management, and statistics. The use of two case studies based on an EDA approach to data quality motivates a collection of research questions for statistics that cover theory, methodology, and software tools. Data visualization is a crucial EDA approach that uses visual elements like charts and graphs to make analysis simple and efficient [24]. When it comes to data quality profiling, visual EDA is very pertinent. With visual features like charts and graphs, data visualization is a crucial EDA technique that simplifies and streamlines analysis. Visual EDA is especially pertinent in the context of data quality profiling. To investigate and summarise multiple data sets, data scientists use exploratory data analysis (EDA), which typically employs data visualization tools. Figuring out how to alter data sources to obtain the required answers, makes it easier for data scientists to detect trends, spot anomalies, test hypotheses, or validate assumptions. EDA aids in comprehending the variables used in data collecting and how they relate. Typically, it is used to look into what information the data might reveal outside of the formal modelling or hypothesis testing assignment. It can also assist you in determining the suitability of the statistical methods you’re considering using for data analysis. John Tukey, an American mathematician, developed EDA methods, which are still extensively used in the data discovery process. EDA’s main objective is to help with data analysis before making any assumptions. It can help with identifying obvious errors, better-comprehending data patterns, identifying outliers or unexpected events, and identifying fascinating relationships between the variables. Data scientists can make sure their findings are accurate and pertinent to any targeted business objectives by using exploratory analysis. EDA assists stakeholders by making sure they are asking the right questions. EDA can help in answering inquiries about standard deviations, categorical notations, and confidence intervals. In [25], authors present unifying principles offered by the categorical notion of data and discuss the importance of these principles in data-centric AI transition. SECTION V. Data Development Approximately 45% of a data scientist’s time is spent on data preparation tasks including importing and cleaning data, according to an Anaconda survey of data scientists shown in Figure 4.2 The quality and quantity of training data are pivotal for machine learning model performance. A summary of representative tasks and methods for training data development are given in Table 2. TABLE 2 Representative Tasks and Methods for Training Data Development for Data-Centric AI Applications TABLE 3 A Brief Summary of Existing Literature in Data-Centric AI FIGURE 4. Time invested by data scientists in various phases. Show All A. Data Collection Data collection is a crucial step in AI as it forms the foundation for training machine learning models. It involves three main approaches: data acquisition, data labeling, and improving existing data and models. Data acquisition involves finding or generating new datasets, data labeling involves adding annotations to enable learning, and improving existing data and models involves modifying them to increase accuracy and usefulness. The quality and relevance of collected data directly impact the accuracy and usefulness of AI models. 1) Data Acquisition When there is a lack of data, data acquisition can be performed to find suitable datasets for training machine learning models. There are three approaches for data acquisition: data discovery, data augmentation, and data generation. Data discovery involves indexing and searching for datasets, data augmentation involves creating synthetic examples by distorting or combining labeled examples, and data generation involves creating datasets through crowdsourcing or synthetic data generation techniques. Data discovery refers to the process of identifying and locating relevant data sources to support decision-making and analysis. The goal is to make it easier for organizations to find and use the data they need, by indexing large datasets stored in data lakes [26] or searching for data on the web [27]. This can help organizations to improve the efficiency and effectiveness of their data-driven activities. The Goods system [67] and Google Dataset Search [28] are examples of data discovery tools that support searching for datasets. These tools have become more interactive, with systems like Juneau [68] providing interactive data search and management. Juneau uses similarity measures, provenance information, and schema information to find related tables, which is a critical task inefficiently joining or unifying tables in data lakes. LSH-based algorithms that perform set overlap search or unionable attribute retrieval have been proposed to solve this challenge [69]. Data augmentation is a popular technique in machine learning to generate new training data to increase the diversity of data used for training. Generative Adversarial Networks (GANs) [70], [71] are commonly used for this purpose. They consist of a generator and a discriminator, which are trained in an adversarial fashion to generate fake data that is similar to the real data. However, GANs have limitations and other techniques have been developed to complement them, such as policies [72] and AutoAugment [30], where transformations are applied to data by a controller. Another popular technique is Mixup [31], [73], [74], [75], [76], which mixes pairs of data points of different classes to create additional data that regularizes the model. Model patching [77] is another method that uses GANs to augment the data for specific subgroups of a class to improve the model’s accuracy. Another way to acquire new data is by generating it through crowdsourcing platforms such as Amazon Mechanical Turk [78], where people are paid to create or find data for specific tasks. Data can also be generated through simulators or generators specific to a certain domain, such as Hermoupolis [32] for mobility data or Crash to Not Crash [79] for driving data. Domain randomization [33], [80] is a technique that can be used to generate a variety of realistic data by varying the parameters of a simulator. GANs can also be used to generate new data, but they require a sufficient amount of real data for training. 2) Data Labeling Data labeling is the process of annotating or categorizing data to provide it with a label or class. This is necessary for training supervised machine learning models, where the model needs to learn to make predictions based on labeled data. Data labeling can be a time-consuming and labor-intensive process, but it is crucial for ensuring the quality and accuracy of the trained models. There are several approaches to data labeling, including manual labeling by human annotators, automated labeling using heuristics or pre-trained models, and crowdsourced labeling, where a large number of people can label the data through a platform. The choice of labeling method depends on the type and amount of data, the desired level of accuracy, and the budget and time constraints. Data labeling is a crucial step in training machine learning models, where the goal is to assign labels to examples in the dataset. The labeling process can be performed using various methods, including: Semi-supervised learning [34], where existing labels are used to predict the other labels. Crowdsourcing platforms like Amazon Mechanical Turk, where labelers are recruited to label the data. Domain experts, who provide labels based on their expertise but can be expensive. Active learning [81], which reduces the crowdsourcing cost by asking labelers to label uncertain examples that will improve model accuracy the most Weak supervision, where labels are generated semi-automatically, for example, by using external knowledge bases or data programming techniques like Snorkel [37], [38] or Snuba [39]. 3) Improving Existing Data Improving existing data and labels is a useful approach in scenarios where there are no relevant datasets available externally and collecting more data no longer increases the model’s precision. Re-labeling [40] is an effective approach to improve the quality of the labels and Can be accomplished through the collection of majority opinions on multiple labels per instance. Data validation, cleaning, and integration can also help improve the quality of existing data. B. Data Cleaning and Validation Data cleaning is an important step in preparing data for machine learning. However, simply fixing well-defined errors in the data may not necessarily lead to an improvement in the accuracy of the machine learning models [49], [82]. Improving the accuracy of the model and making the training process resistant to noise in the data is more effectively achieved through directly cleaning the data [83], [84]. A training process that is resistant to noise is deemed to be more effective than cleaning the data prior to model training [82]. Data with malicious intent, known as adversarial data noise, can also be present, and addressing it through cleaning is referred to as data sanitization. Incorporating AI ethics such as model fairness [85] is also an important consideration in data preparation as biased data can lead to discriminatory models. Platforms like TensorFlow Extended (TFX) [86], which specializes in machine learning, possess data validation [87] components that allow for the early detection of data errors. 1) Data Validation Data visualization is an effective way of validating data for machine learning [87]. Visualization tools like Facets [88] allow for quick checks on data to prevent errors downstream. Research has also been conducted on the automatic creation of visualizations, such as the SeeDB [41] framework that uses a deviation-based metric to determine interestingness. However, automatic generation can lead to false positive results, which is why there is also a body of research that concentrates on techniques for controlling false discoveries. Schema-based validation [86], [87] like TensorFlow Data Validation (TFDV) [43], [44] is widely used in practice. It creates a data schema from prior data sets, utilizing it to validate future datasets and informing users of any anomalies that are detected. Data validation systems are becoming increasingly equipped with additional capabilities like declarative data quality constraints, pipeline inspection, automatic error identification, and ease of usage. Deequ [45], [46] is a library that allows you to define data quality constraints in a declarative way, which are then transformed into unit tests. The mlinspect [47] library provides a way to inspect machine learning pipelines in a declarative manner. The most recent advancements in data validation systems encompass the automatic identification of error types, evaluation of the effects of errors on models, user-friendliness, and efficient validation with human involvement [89], [90], [91], [92]. 2) Data Cleaning Data cleaning involves eliminating errors in the data by meeting various integrity restrictions, including key constraints, domain constraints, referential integrity constraints, and functional dependencies. Data cleaning techniques have become sophisticated, with state-of-the-art techniques like HoloClean [48] that repair data using probabilistic inference, satisfying integrity constraints, checking value validity using external dictionaries, and using quantitative statistics. Exclusively focusing on data correction does not ensure optimal model accuracy. Clean data is not always a clear-cut concept and removing all possible errors is not always attainable. The CleanML [49] framework assesses different data cleaning methods to determine if they enhance model accuracy and reveals that data cleaning does not necessarily enhance machine learning models and could even have a detrimental impact. However, selecting the correct machine learning model can counterbalance the negative effects of data cleaning. There is no one-size-fits-all cleaning algorithm that is effective for all types of noise, and the selection of the algorithm depends on the type of noise. Care must be taken when utilizing data cleaning techniques that are not specifically designed for machine learning as they frequently have high-impact parameters that require proper tuning, similar to the tuning of machine learning hyperparameters. Advanced data cleaning techniques use probabilistic inference, external dictionaries, and statistical methods to repair data. However, it is not always clear if data cleaning improves model accuracy, as sometimes cleaning data may have a negative effect. There are data cleaning techniques designed specifically to improve model accuracy, such as ActiveClean [50], which iteratively cleans samples and updates the model, and TARS [51], which cleans labels to improve model accuracy. Recently, there has been a rise in systematic approaches to data cleaning for machine learning, including CPClean [93], which analyzes the impact of missing data on predictions, and a study that proposes solutions to tackle data quality issues in MLOps [94]. The most common challenges in data cleaning for machine learning include handling multimodal data and data that changes over time. 3) Data Sanitization Data poisoning is a serious issue where a small fraction of training data is altered with malicious intent to cause a machine learning model to fail. Data poisoning is becoming more sophisticated and hard to defend against [40], [95]. One approach to defending against data poisoning is data sanitization, which involves detecting and discarding poisonings using outlier detection. However, current data sanitization techniques have proven to be ineffective against carefully designed attacks [54], [96]. The field of data poisoning and sanitization is constantly evolving and requires further research. 4) Multimodal Data Integration Multimodal data integration refers to the process of combining data from multiple sources with different modalities (e.g. video streams, radar data, and time series) [97]. The integration of this data is important in machine learning, particularly in applications such as autonomous vehicles. Two primary techniques utilized in machine learning are alignment and co-learning. Alignment entails discovering connections between sub-components of instances that possess multiple modalities, while co-learning involves training a model based on one modality while leveraging information from another. Data integration is a vast field of research that has been explored for numerous years, however, not all techniques are applicable to machine learning [98]. SECTION VI. Related Work In recent times, there has been a remarkable shift in the importance of Data-Centric AI compared to Model-Centric AI. Industries are undergoing significant changes in their core architecture, becoming more focused on data. Many companies are adopting data-centric approaches to tackle their daily tasks. To address the challenges encountered in current workflow methodologies, where data engineering and model engineering are treated as separate cycles, a new division has emerged, introducing a range of tools such as CleanLab,7 LandingLens,8 Snorkel,9 AutoAlbument,10 HoloClean,11 Albumentations,12 and more. These tools support various production processes by identifying and resolving issues step by step. This progress has been made possible through structured thinking and dividing tasks into distinct yet comprehensive components. Several benchmarks like dcbench [99], ImageNet [100], and MLPerf [101] have been developed to evaluate and validate the effectiveness and quality of these tools based on parameters such as training data size, budget-restricted data cleaning, object detection, computer vision, and others. In a study by Chen et al. [102], the authors introduce the data-centric AI approach and discuss the CLIP Model for multimedia misogyny detection. Additionally, Motamedi et al. [103] propose a data-centric AI approach for enhancing data quality and suggest a solution based on Generative Adversarial Networks (GANs) to synthesize new data points. Their findings demonstrate that the optimized dataset generated by the pipeline improves accuracy while being significantly smaller than the baseline. RoboFlow [104], a data-centric cloud-based workflow management system, is revolutionizing the creation of AI-enhanced robots. By breaking down the development process into four building blocks and prioritizing data over conventional process-centric techniques, RoboFlow enables the creation of data-driven AI-enhanced robots. In the field of materials science [105], machine learning is facilitating the prediction of structural features and the discovery of new materials [106]. By utilizing data-centric machine learning methodologies, accurate predictions can be made with minimal theoretical data, improving computational efficiency. In the renewable energy sector, while most research focuses on model-centric predictive maintenance techniques, a shift towards data-centric approaches is gaining attention [107]. By adopting data-centric machine learning, the accuracy of climate change prediction and wind turbine understanding can be enhanced, leading to increased utilization of green energy [108], [109]. In a data-centric approach, Zeiser et al. [110] propose a method combining WGAN and encoder CNN for advanced process monitoring and prediction in real-world industrial applications. They highlight the importance of context-based data preparation and demonstrate that improving data quality has a significant impact on prediction accuracy. Safikhani et al. [111] argue that transformer-based language models can enhance automated occupation coding. By fine-tuning BERT and GPT3 on pre-labeled data and incorporating job titles and task descriptions, they achieve a 15.72 percentage point performance increase compared to existing methods. They also introduce a hierarchical classification system based on the KldB standard for encoding occupation details. Wang et al. [112] studied, a data-centric analysis of on-tree fruit detection using deep learning is conducted. They explore the challenges of fruit detection in precision agriculture and evaluate the impact of various data attributes on detection accuracy. They also investigate the relationship between dataset-derived similarity scores and expected accuracy, as well as the sufficiency of annotations for single-class fruit detection using different horticultural datasets. Chen and Chou [102], a system for detecting misogynous MEME images is presented using coherent visual and language features from the CLIP model and the data-centric AI principle. The system achieved a competitive ranking in the SemEval-2022 MAMI challenge, demonstrating the effectiveness of leveraging Transformer models and focusing on data rather than extensive model tweaking. The proposed approach utilizes logistic regression for binary predictions and acknowledges the importance of data quality and quantity. Although limitations are not explicitly mentioned, the method’s performance may be influenced by the available data and reliance on pre-trained models. Zhong et al. [114] proposed a data-centric approach to enhance the robustness of deep neural network (DNN) models against malicious perturbations. By focusing on dataset enhancement, their algorithm improves model performance against adversarial examples and common corruptions. The effectiveness of the approach is demonstrated through high rankings in a data-centric robust learning competition, highlighting the algorithm’s success across various DNN models. SECTION VII. Challenges and Opportunities Though data-centric AI shines as a promising advancement, its current limitations call for a more nuanced approach. We encourage synergistically leveraging both methods: utilizing data-centric practices for robust data engineering while refining models through conventional optimization techniques. This, as elegantly proposed in [15], treats data and model as intertwined gears driving exceptional results. This embrace of synergy acknowledges the current landscape and honors ethical research by recognizing the contributions of prior work. Fig. 5 illustrates the main challenges of the data-centric AI approach. Data-centric production faces challenges such as maintaining consistency, adequate volume retention after cleaning, quality of maintenance, the necessity of a proper data versioning system, etc. Amongst the various issues, one of the most significant is the loss of volume associated with cleaning and validating data. An AI model can only receive a massive amount of high-quality data if low-quality datasets are removed. FIGURE 5. Shortcomings of data-centric systems. Show All A. Challenges and Solutions The main challenges of data-centric artificial intelligence (AI) are given below: Data quality and quantity: One of the biggest challenges in data-centric AI is ensuring that the data used to train and test AI models is of high quality and sufficient quantity. Data may be incomplete, noisy, or biased, which can lead to poor performance or inaccurate predictions from AI models. Data preprocessing is a crucial step in the AI pipeline, but it can be time-consuming and computationally expensive. This includes cleaning, normalizing, and transforming data, as well as handling missing or corrupted data. Data labeling is also a time-consuming and costly task. Additionally, obtaining large amounts of high-quality data can be difficult and expensive, particularly in certain domains such as healthcare or finance. Handling uncertainty: AI systems often need to make decisions in uncertain and dynamic environments, where data is incomplete, noisy, or ambiguous. This requires developing methods for dealing with uncertainty, such as probabilistic and Bayesian methods. Real-time processing: Many data-centric AI systems need to analyze and make decisions based on real-time data streams, such as self-driving cars, financial trading, sensor data or social media feeds. This requires the ability to process data quickly and make predictions in near-real time, which can be challenging. There is a need for more efficient and effective algorithms for real-time data processing and analysis. Integration of multiple data sources: The ability to integrate and analyze multiple data sources, such as sensor data, social media data, and other types of unstructured data is a major challenge. This is because data from different sources may have different formats, structures, and levels of quality, which can make it difficult to combine and analyze them effectively. Generalization: AI models often struggle to generalize their predictions to new or unseen data, particularly when the data is significantly different from the training data [15]. This is a major concern in real-world applications, where the data is often noisy and unpredictable. Data Privacy and security: Data may contain sensitive or personal information that must be protected from unauthorized access or misuse. As AI systems are increasingly used to analyze sensitive data or personal information, such as medical records or financial data, there is a growing need to protect this data from unauthorized access or misuse. This includes developing methods for data anonymization and differential privacy are being developed to address these issues. Additionally, AI systems themselves may be vulnerable to attacks such as adversarial examples or model stealing. Explainability and interpretability: AI systems become more complex and autonomous, particularly those based on deep learning, it can be difficult for humans to understand how they are making decisions or accurate predictions. This makes it difficult to trust and validate the results and can limit the use of AI in sensitive applications such as healthcare, finance, or criminal justice. Researchers are working on developing methods for making AI systems more transparent and interpretable. Scalability and robustness: As the amount of data being generated and collected continues to grow, it becomes increasingly difficult to process and analyze all of this data in a timely and efficient manner. However, many existing AI systems are not well-suited for these types of major challenges, and research is needed to improve the scalability and robustness of AI systems, particularly in resource-constrained environments. Multi-modal and multi-sourced data: As the amount and diversity of data increases, it becomes more challenging to integrate and make sense of data from multiple sources, such as sensor data, social media data, and other types of unstructured data. Causality and counterfactual reasoning: AI systems are often used to make predictions or decisions, but it can be difficult to understand the causal relationships underlying those predictions or decisions. Additionally, it’s challenging for AI systems to reason about counterfactual scenarios, where the outcome of an event is different from what actually happened. Human-AI collaboration: As AI systems become more integrated into various domains, there is a need for more effective methods of collaboration between humans and AI systems. There are many challenges associated with human-AI interaction, such as trust, transparency, and explainability. This includes finding ways to make AI systems more transparent and explainable to humans, as well as developing methods for humans to provide feedback and guidance to AI systems. Bias and fairness: AI models can inadvertently introduce bias into their predictions, particularly if the data used to train them is not representative of the population. This can result in unfair or discriminatory decisions. This is a significant concern in applications such as healthcare, finance, and law enforcement. Researchers are working on developing methods for detecting and mitigating bias in AI systems. Integration with other technologies: AI systems often need to be integrated with other technologies, such as sensors, IoT, and robotics. This can be challenging as each technology has its specificities and constraints. Ethics and regulation: As AI systems become more sophisticated and prevalent, there are important ethical and regulatory challenges to consider, such as the impact of AI on jobs and society, the accountability of AI systems, and the potential misuse of AI. Therefore, a data-centric method frequently needs a more significant data volume than a model-centric one. This brings potential issues where cleaning might decrease the insufficient amount of collected data under technological or monetary constraints; for example, in scientific research, the model might be continuously tuned to generate and work on experimental data. Working in a constantly evolving setting can be challenging, primarily due to the changing algorithms and margins for errors. In many cases, rules-based algorithms result in a high rejection rate because the technology needs to differentiate between authentic defective components and acceptable levels of variation, especially in manufacturing and production-based industries. This forces a large percentage of human follow-up inspection, which raises costs and slows down production lines. Robustness is the model’s ability to preserve performance with a small amount of noise in the data, such that the training error rates are consistent with testing error rates. Fairness refers to the model’s tendency to defend against unnatural biases such that the model does not inadvertently introduce any biases. In the case of data quality issues, a focus is given to robust training algorithms when validation of noisy data is insufficient and/or to fairness when appropriate pre-processing levels are insufficient to remove biases from data. An AI system should be trained on the same data type on which it is to be tested and analyzed, including any edge-cases. Additionally, as part of quality control methods, properties of data records that are not causative features should be randomized during training. An AI model quickly loses accuracy without consistent data annotation. Unfortunately, it can be challenging to maintain a high level of consistency. However, this brings forth a significant challenge to data-centric AI, as it requires human annotation that is costlier than machine computation, especially in environments that emphasize maximum automation. B. Opportunities Several research opportunities can advance the Data-Centric AI field going forward. The main challenge for applying a data-centric approach in embedded ecosystem applications is the complexity. It is challenging to deploy data-centric AI on portable devices. In this context, one promising research direction is the Edge Impulse-based data-centric approach for a wide range of hardware targets [126]. Edge Impulse is a cloud-based ML operations (MLOps) platform like Microsoft Azureor [127] or Google VertexAI [128] for developing embedded and Tiny Machine Learning (TinyML) [129] systems. The researchers may conduct the Vivo experiments on data-centric Green AI [109]. They analyze how combining data-centric with Green AI techniques in real-world AI pipelines. These techniques may impact the energy efficiency and accuracy of AI models. The research opportunities in the field of data-centric artificial intelligence are given below: Developing methods for effectively and efficiently processing and analyzing large amounts of data, such as big data and real-time streaming data. Developing more efficient and accurate algorithms for data pre-processing, cleaning, and feature extraction. Improving the performance of machine learning models by developing new architectures and training techniques. Developing new methods for dealing with large and complex datasets, such as deep learning and reinforcement learning. Developing new approaches for dealing with unstructured and semi-structured data, such as natural language processing and image processing. Developing new methods for interpretability and explainability of AI models, Developing methods for integrating AI with other technologies, such as the Internet of Things (IoT) and edge computing. Developing new ways to use AI to analyze and make sense of data from multiple sources, such as sensor data, social media data, and financial data. Developing new approaches for dealing with privacy and security issues related to data-centric AI. Developing new ways to use AI to improve decision-making in various domains, such as healthcare, finance, and transportation. Researching on the ethical and societal impact of AI. Developing more efficient and effective algorithms for data processing and analysis, such as machine learning and deep learning techniques. Improving the scalability and robustness of AI systems to handle large and complex data sets. Developing new methods for integrating and analyzing multiple data sources, such as sensor data, social media data, and other types of unstructured data. Improving the performance and accuracy of machine learning and deep learning models through the use of advanced techniques such as transfer learning and ensemble learning. Developing new and innovative applications of AI in areas such as healthcare, finance, and transportation, to name a few. Exploring the use of AI for decision-making and problem-solving in complex and dynamic environments, such as in the areas of autonomous systems and smart cities. Understanding and addressing ethical and societal implications of AI, including issues related to bias, transparency, and accountability. Developing new methods for explainable AI (XAI) which can help to increase the transparency and interpretability of AI systems. Designing AI systems that can learn and adapt to changing environments and data distributions. Researching ways to make AI more robust and secure, such as against adversarial attacks. SECTION VIII. Discussion Initially, the model-centric AI approach appears more reasonable for modifying the model (algorithm/code) to enhance the system’s efficiency, as opposed to altering the data. However, when faced with the constraints of model-centric AI solutions, the use of data-centric approaches becomes pertinent, as discussed previously. Model-centric classification algorithms employ regularization techniques to adjust the algorithm or code in order to enhance model performance. Nevertheless, for real-time applications to function effectively, reliable data is essential, and this consideration should extend to the underlying model to the same degree. Therefore, it is important to view model-centric and data-centric approaches as complementary and not to dismiss the model-centric approach entirely due to its limitations in some specific organizations and industries. First, in the initial stages of problem-solving, it is crucial to emphasize not only understanding the properties and information of things but also how we interact with them. In the context of AI development, this implies a greater focus on designing and refining intelligent algorithms, as well as fine-tuning the hyperparameters of the underlying model and the code that implements the algorithms. Defining the dataset at this project stage is of utmost importance, as it serves as the foundation for comparing and categorizing the performance of models. Second, commencing the AI system design with a model-centric approach provides ample opportunities to gain hands-on experience in understanding real-world problems and exploring potential computational solutions, rather than the experience that might have been acquired through adopting a data-centric approach in research and industry. When attempting to solve a problem, it is often in our nature to start by taking action to make an impact before delving further into understanding the properties of items in our environment. Reinforcement learning, a recently well-publicized machine learning (ML) technique, is based on the idea that intelligent agents learn through interacting with their environments. Third, it would be challenging to relate the final models to real-world problems because they would have undergone a completely different development methodology. In the early phases of AI development, both academia and industry may have preferred the data-centric approach over the model-centric method. A general architecture of model-centric AI, data-centric AI, and model data-centric AI is shown in Fig. 6. Model-data-centric AI combines the benefits of both model-centric AI and data-centric AI, seeking to balance the refinement of models and the improvement of training data, recognizing that both aspects are crucial for achieving optimal results. The choice of approach should take into consideration the specific needs, available resources, and objectives of the AI project at hand. FIGURE 6. General architecture of model-centric AI, data-centric AI, model data-centric AI. Show All SECTION IX. Conclusion This paper presented a high-level review of the data-centric AI paradigm by connecting all relevant topics together to help academicians, researchers, and engineers manage the AI systems. Data-centric AI is capable of constituting a complementary paradigm to model-centric AI to build successful AI-based systems for real-world applications. Data-centric AI emphasizes the importance of data quality in the performance of AI systems, recognizing data as dynamic and constantly evolving, rather than just a static input for preprocessing. Improving data quality is a continuous process throughout the system’s life cycle. This article presents DCAI as a developing movement and proposes four principles to guide its efforts. The future of data-centric AI lies in establishing systematic processes for monitoring and enhancing data quality, which requires more focus on data supply, preparation, annotation, and integration into AI systems. Conflict of Interest The authors declare that there is no conflict of interest. ACKNOWLEDGMENT The authors express their gratitude to Manipal Academy of Higher Education, Manipal, for their essential role in facilitating the publication of this article. Authors Figures References Keywords Metrics Footnotes More Like This A Personalized Computational Model for Human-Like Automated Decision-Making IEEE Transactions on Automation Science and Engineering Published: 2022 Explainable Artificial Intelligence: Counterfactual Explanations for Risk-Based Decision-Making in Construction IEEE Transactions on Engineering Management Published: 2024 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."

Paper 9:
- APA Citation: Karamolegkos, P., Kiourtis, A., Karabetian, A., Voulgaris, K., Poulakis, Y., Mavrogiorgou, A., & Filippakis, M. (2023). MathBlock: Performing Complex Mathematical Operations on Synthetic Data. 2023 15th International Conference on Computer Research and Development (ICCRD), 1–8. https://doi.org/10.1109/ICCRD56364.2023.10080594
  Main Objective: To investigate the use of containerization technologies for scalable and autonomous deployment of data processing and machine learning modules in cloud environments for automated irrigation management systems.
  Study Location: Unspecified
  Data Sources: Not mentioned in the given context
  Technologies Used: Docker, Kubernetes
  Key Findings: Containerization technologies enable efficient deployment and scaling of data processing and machine learning modules. MathBlock provides a scalable solution for processing large datasets and executing complex mathematical operations. The combination of containerization technologies and cloud computing offers a flexible and efficient platform for developing and deploying automated irrigation management systems.
  Extract 1: "MathBlock showcases the power of containerization technologies in enabling the deployment and scaling of data processing and machine learning modules in cloud environments. It provides a scalable solution for processing large datasets and executing complex mathematical operations."
  Extract 2: "By combining containerization technologies with cloud computing, MathBlock offers a flexible and efficient platform for developing and deploying automated irrigation management systems. Its ability to seamlessly scale up and down based on demand makes it ideal for handling varying workloads and data volumes."
  Limitations: The paper does not present any major limitations of the proposed approach.
  Relevance Evaluation: The paper is highly relevant to the outline point, as it directly addresses the use of containerization technologies for scalable and autonomous deployment of data processing and machine learning modules in cloud environments. The paper provides a detailed analysis of the benefits and challenges of using containerization technologies in this context.
  Relevance Score: 0.95
  Inline Citation: (Karamolegkos et al., 2023)
  Explanation: The research paper delves into an end-to-end automated irrigation management system that integrates IoT and ML technologies. The paper focuses on leveraging containerization technologies, such as Docker and Kubernetes, for efficient deployment and scaling of data processing and machine learning modules in cloud environments.

 Full Text: >
"This website utilizes technologies such as cookies to enable essential site functionality, as well as for analytics, personalization, and targeted advertising purposes. To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards Authors Citations ADVANCED SEARCH Conferences >2023 15th International Confe... MathBlock: Performing Complex Mathematical Operations on Synthetic Data Publisher: IEEE Cite This PDF Panagiotis Karamolegkos; Athanasios Kiourtis; Andreas Karabetian; Konstantinos Voulgaris; Yannis Poulakis; Argyro Mavrogiorgou; Michael Filippakis All Authors 1 Cites in Paper 66 Full Text Views Abstract Document Sections I. Introduction II. Related Work III. Mathblock IV. Mathblock Experimentation V. Discussion and Concluding Remarks Authors Figures References Citations Keywords Metrics Abstract: The use of systems to calculate mathematical operations has facilitated people to automate processes in the corporate sector. Such systems lie behind everything from calculating the final amount on a grocery receipt, to complex mathematical operations involving finding behaviors in a business's customers. However, when a company or organization has a large amount of data on which to perform mathematical operations, the procedure becomes time-consuming, whereas to execute mathematical operations on entire datasets, one typically needs strong programming skills. In this paper, a service called MathBlock is analyzed that is able to be used as a language agnostic mathematical expression parser and executioner, on batch data. MathBlock consists of four types of functions, including arithmetic, comparison, logical, and statistical. To evaluate the applicability of MathBlock, an experiment is carried out on the mentioned service as a proof of concept. This experimentation uses batch and synthetic data, covering the domains of maritime and healthcare, with the aim of performing mathematical operations through MathBlock. The derived results showcase that MathBlock can assist users on their need to calculate and gather results for many different datasets. Overall, it can be clearly stated that through MathBlock the challenge of the need to perform arithmetic, logical, comparison and statistical operations on different datasets to get results in an automated manner is well addressed, whereas additional experimentation with datasets from multiple domains should take place in order to conclude to more concrete and reliable results. Published in: 2023 15th International Conference on Computer Research and Development (ICCRD) Date of Conference: 10-12 January 2023 Date Added to IEEE Xplore: 30 March 2023 ISBN Information: ISSN Information: DOI: 10.1109/ICCRD56364.2023.10080594 Publisher: IEEE Conference Location: Hangzhou, China SECTION I. Introduction The data analysis process is responsible for finding hidden patterns, rules, and information from a given set of data [1]. According to Statista [2], the total amount of data consumed globally has increased to 64.2 Zettabytes in 2020, 79 Zettabytes in 2021, and is expected to increase more than 180 Zettabytes by 2025. All this data is referred to as Big Data, which is defined as large volumes of data collected from various sources and in various formats [3]. Such data are widely known to obey to some specific characteristics (Vs of the data), which nowadays mainly refer to data Volume (i.e., data size), Variety (i.e., data format), Velocity (i.e., data production rate), Veracity (i.e., size of data authenticity), Validity (i.e., data validity), Volatility (i.e., time of data validation), and Value (i.e., data usefulness in terms of analysis) [3]. Although Big Data is essentially data, the traditional techniques and technologies of their analysis do not seem to perform efficiently, due to the complexity of the aforementioned characteristics. The three major motives for Big Data technology implementations are: (i) minimize hardware costs, (ii) check the value of Big Data before committing significant company resources, and (iii) reduce processing costs [4]. It is easily visible that companies are trying to handle these large amounts of data while minimizing their expenses. This is mostly possible due to the modern technologies and techniques that are built upon the field of Cloud Computing [5]. Big Data that are being able to be analyzed by a data analysis process are divided as (i) batch data deriving from ready-to-use datasets that require some processing or analytic activities, and (ii) streaming data deriving from live sources that are constantly streaming information [6]. Batch and streaming data can be either real data or synthetic data. As described in [7] synthetic data are data obtained from a generative process that learns the properties of the real data. More specifically, synthetic data are data that are trying to represent real data, without necessarily being real. Synthetic data are commonly used for training and testing of many data analysis processes [8], since through synthetic data more accurate Machine Learning (ML) models' training can be provided, leading to better decision-making systems and results of high-reliability and trustworthiness. Also, the use of systems to calculate mathematical operations has helped people to automate processes in the corporate sector. Such systems lie behind everything from calculating the final amount on a grocery receipt, to complex mathematical operations involving finding behaviors in a business's customers [9]. To support such systems and to give a language agnostic solution for the execution of mathematical expressions, some tools have been built, giving to their users the ability to compose and execute simple mathematical expressions, which are furtherly described in Section II. However, most of the time, these technologies require a human user to interact with them. Furthermore, the datasets must be handed one by one to the aforementioned systems, usually by human hand. Moreover, while these systems automate some of the methods for computing operations with datasets, they do not fully automate, and as a result, the entire process is semi-automated rather than entirely automated. To address the gaps of the aforementioned tools, in this paper, a service called MathBlock is being provided and analyzed. This service can be used as a language agnostic mathematical expression parser and executioner, on batch data. In simpler words, MathBlock is a program that can get as an input (i) many different datasets as batch data, (ii) a mathematical expression, as well as (iii) some needed metadata, to output a result dataset having values calculated based on the given expression. MathBlock can operate using different aspects of the mathematical and language agnostic systems mentioned, as described in the upcoming Sections. The rest of this paper is structured as follows. Section II presents a review of related research works. These are based on topics such as batch data, synthetic data, orchestration systems, mathematical operations systems, object storages, and workflow management systems. Section III covers the functionality of MathBlock, analyzing its different functions and operations. It also describes how it can be used by other users and services in addition to this analysis. Section IV covers the two experiments carried out in MathBlock as a proof of concept, considering the maritime and healthcare sector. In both experiments, synthetic data is employed, and MathBlock is used to test the reliability of the synthetic data based on the provided real data. Section V contains the paper's discussion and concluding remarks, referring to the outcomes of the experiments performed using MathBlock, as well as to its limitations and challenges. SECTION II. Related Work As already discussed, MathBlock has been implemented considering various modern tools and research works, related to its goals and implementation methods. These related works are analyzed in the following sub-sections. A. Batch Data As described, batch data derive from datasets that require some processing or analytic activities. These data can be collected from many different sources as they can originate from Big Data environments [6]. In the referenced research, different architectures for Big Data Batch Processing are explained and analyzed. An analysis of different environments and architectures is performed, using the MapReduce process, with the aim of producing results regarding the efficiency of each different architecture. B. Synthetic Data ML algorithms must be trained and validated using diverse datasets, including datasets with known patterns and distributions when creating models from sensor data [10]. Synthetic data can be created for initial testing and validation of novel machine learning techniques [11]. Researches [12] [13] are reviewing the concept of synthetic data. In the referenced paper [12], the authors are reviewing different methods of making synthetic data. With the advent of high dimensionality, accurate identification of relevant data features has become critical in real-world scenarios. The significance of feature selection in this context cannot be overstated, and various methods have been developed. Several synthetic datasets are used for this purpose, with the goal of evaluating the efficacy of feature selection algorithms in the presence of many irrelevant features, noise in the data, redundancy and interaction between characteristics, and a low sample-to-feature ratio. Research [13] is taking the discussion of synthetic data one step further. It is evaluating the safety of using synthetic data in terms of the protection of the real data involved in making the generated data. C. Orchestration Systems Orchestration systems are services for accessing processes that are in a queue and executing them one by one. The project called Diastema uses such a system to perform processes which are provided in a graph [14]. This graph is built by the users of the mentioned platform using a low code user interface. It then proceeds to the orchestration system which is responsible for accessing the constructed graph. When the graph is accessed, each of its nodes is executed. Each node is referred to as a ‘job’ of the node. Orchestration systems are not the only ones recommended for running processes as workflows. In addition to orchestration systems, there are also choreography systems. In [15] a comparison of these two is made, as well as the usage environments of each one is analyzed. Orchestration systems seem to be preferable to choreography systems, in environments where there is no waiting for a process to be executed outside of the system. MathBlock is a system in which all processes are predefined. This means that it does not expect anything to be completed outside of its own system. Therefore, the handling it performs on the graph given as input is ideal, as explained in Section III. D. Systems for Mathematical Operations Systems have been built which solve mathematical operations at the level of simple values up to entire matrices. Reference [16] shows a system which can perform mathematical operations involving integrals and differential equations. It does this by making use of devices, namely metadevices. It demonstrates that metadevices can perform generalized matrix inversions and serve as the foundation for the gradient descent approach for solving a wide range of issues. Finally, a general upper constraint on the solution convergence time demonstrates the potential of such metadevices for stationary iterative systems. Other tools that are able to handle complex mathematical operations are MatLab and Octave [17]. These tools are giving to their users the ability to compute data using traditional and ML practices. Also, programming languages such as Python can be used for analytic purposes as well [17]. E. Object Storages Many kinds of systems use object storages, with the purpose of managing various data or sets of data as files. Reference [18] is exploring the concept of object storages. In this survey, all the key differences and similarities of using a Network File System(NFS) and an object storage are mentioned. One of the object storages mentioned is MinIO. This study concludes that object storages are a practice that is a cloud native solution in contrast to NFS. However, although object storages are cloud native, they have fewer features than NFS. Project [19] is using object storages to collect JavaScript Object format Notation (JSON) files and save them as-is in a cloud infrastructure, proving the cloud-native use of the system called MinIO as an object storage. F. Workflow Management Systems Reference [19] is analyzing a serverless data pipeline for a cloud computing environment. The particular system that is mentioned uses tools like MinIO as object storage. Within the document, it is mentioned how the pipeline is executed. This method uses a choreography system to execute specific processes after being invoked by another process. The order of the execution of the processes is as follows: (i) acquire data from the source, (ii) compress data and send to the cloud, (iii) decompress the data, (iv) detect data type, (v) convert data into JSON, (vi) store the data in object storage, and (vii) send relevant notifications to the end users. Another cloud-based data pipeline platform is analyzed in [20]. Unlike the previous related work, this system uses an orchestrator. Therefore, each of its processes is executed when the previous running processes finish, as opposed to the invoking that was done in the previous project. The Amazon Web Service (AWS) environment is used for the purpose of performing analyzes on COVID-19 data. In the experimentation presented in the referred article, data from reliable sources are used as well as the user interface of the administrator and the researchers participating in the system is presented. The system ML4ProFlow in [21] can manage workflow task using graphs. The referred document depicts the continuous development of the mentioned system, as a framework that combines the following components: (i) it manages execution environments, (ii) it defines processing modules that prioritize reusability and cross-platform compatibility, and (iii) it includes benchmarking automation to facilitate developers in the implementation and analysis of modules and their combinations. These three framework components are given, and their usage is shown. G. Advancements Beyond the Related Work The MathBlock service combines all the literature developed above, going beyond the current state of the art analysis. MathBlock seems to innovate the way mathematical operations are managed by combining techniques related to managing processes on entire datasets. First, batch data is used, in order to use the data as datasets. These data, as well as their results, are stored in the form of files (objects) within an object storage. In order to extract results, but also to access each mathematical operation of an expression, algorithms are used that concern the mentioned Workflow Management Systems, their related algorithms, as well as the mentioned orchestration systems. Furthermore, MathBlock is able to be used with containerization technologies, so that it can be easily scaled to computing clusters [22]. Thus, MathBlock can also be provided in Big Data ecosystems [23]–[25], whereas providing this service is achievable using technologies such as Docker and Kubernetes [26]. To conclude, MathBlock has gone through experiments using synthetic data, to extract relevant results. SECTION III. Mathblock A. Brief Description of the Service MathBlock is a microservice that may be operated in Kubernetes environments using tools like Docker. The Hypertext Transfer Protocol (HTTP) is used to communicate with the service. MathBlock receives as input a JSON object with information about the execution that must be performed. The service can receive various datasets from an object storage as well as simple values on which to do mathematical operations. The imported JSON object contains all the information about the dataset and the values it should utilize. The JSON item can also be represented as a graph, which the service can access and utilize to conduct the given mathematical operations. MathBlock returns a single column from a created dataset. This column holds the outcomes of every mathematical operation done on each row of the imported dataset. Fig. 1 presents the described architecture of the service. Fig. 1. Mathblock architecture. Show All B. Mathematical Operations as a Graph A protocol has been created that must be followed in order to use the service. As previously stated, MathBlock communicates with users and other programs over HTTP. The HTTP request to MathBlock must include a JSON body with the attributes provided in Table I. Table I. Json body attributes There should be a JSON attribute corresponding to a unique identification, as indicated in Table I. This identification is used to grant the user of the service access to the execution's progress status. In more detail, during the execution of MathBlock, the status of the given execution is registered in a database. The status starts with the value “in progress” and ends when the execution is completed with the value “completed”. Thus, users of the service can make relevant HTTP calls whenever they wish and be informed about the status of the mathematical expression they wish to be executed. According to Table I, the user must additionally supply the locations where MathBlock may find the datasets to be utilized, as well as the locations where the user wants the result to be located. These locations must refer to paths within the object storage system that is being used. The most important part of the JSON object given to the service, is the last attribute shown in Table I. The “function” attribute is using a JSON object to specify what the service should do in its execution. Table II shows the attributes that the “function” JSON should include. Table II. Function attributes As indicated in Table II, “args” in each JSON Object must have two pieces of information as attributes. The first attribute, “arg_id” can take any integer value. The second attribute is either “feature” or “value”. If the property is “feature”, its value must be a string containing the name of the feature header that the service must use as input for the next imported path in the series, which is referred to Table I “inputs” attribute. In the instance when the property is “value”, the value must be a mathematical number or a Boolean value (true or false). If the attribute used is “value”, the value is still delivered as a string. The “expression” attribute of Table II gives to the service the expression to execute. To accomplish this, each JSON object listed in its value, should have the attributes showcased in Table III. Each JSON object of the “expression” list is called a MathBlock node. There are two types of nodes: (i) argument nodes, and (ii) operation nodes. Table III. Mathblock node attributes As shown in Table III, each MathBlock node has a unique identifier (in the “step” attribute) that may be used to reference other nodes via the “next” and “from” attributes. If a node does not originate from another, the “from” property must be set to zero (0). In addition, if a node has no following node to execute, the “next” attribute must be set to zero (0). In essence, nodes with no originating nodes are the MathBlock's inputs, whereas nodes with no next node represent the outcome. Finally, Table IV is showing the information that each MathBlock node includes in the attribute “info”. As shown, there must be a “kind” attribute. This attribute is referring to the type of the node. On the other hand, there can be only one of the attributes “arg_id” and “name”. If the node's type is “arg” then the attribute “arg_id” must be used. If the node's type is “operation” then the attribute “name” must be used. The operations allowed to be used as values for the attribute “name” are the following: (i) “addition”, (ii) “subtraction”, (iii) “division”, (iv) “multiplication”, (v) “logarithm”, (vi) “power”, (vii) “logical_and”, (viii) “logical_or”, (ix) “logical_not”, (x) “equal”, (xi) “not_equal”, (xii) “less_or_equal”, (xiii) “greater_or_equal”, (xiv) “greater_than”, (xv) “less_than”, (xvi) “mean_value”, (xvii) “variance_value”, and (xviii) “amount of”. Table IV. Node's information attributes C. Parsing of the Mathematical Operations Before explaining how each of the mathematical operations are executed by the MathBlock service, it is important to mention how the analyzed graph is parsed. In fact, each operation is performed as the graph is accessed, but the way that the operation is executed is completely separate to the parsing algorithm used. First, when MathBlock is called by a user or application, a new thread is built using the programming technique of multithreading. In this way it is possible to run many different imported graphs in a pseudo-parallel way (but in real parallel if a Kubernetes environment is used). After the execution status is given as “in progress”, the last step of preparation before the parsing of the graph is starting. This step is the initialization of a dictionary, which stores as keys all the “arg_id” mentioned above. The value of each key is a list with two values. The two values of the list are determined according to the type of argument: (i) in the case that the argument is a simple numeric or Boolean value, then the first position of the list is the literal “value”, and the second is the given value from the user, (ii) in the event that the argument is a column from a dataset that exists in the object storage, then the first value is the corresponding path within the object storage and the second value is the given feature that the user wishes to use as input. After the above, the graph is getting parsed by the service. In Fig. 2, the parsing processes are depicted using an activity diagram and the Unified Modeling Language (UML). Fig. 2. Parsing algorithm. Show All As represented in Fig. 2, the first step of the algorithm is the isolation of the expression given by the user. The service then makes a serial search for MathBlock nodes that do not come from other nodes. That is, in this step, all named starting nodes are searched. The graph within the algorithm acts as an inverted tree. The starting nodes are the leaves of the tree while the root of the tree is the result of the MathBlock service. Therefore, the result can also be called an ending node. The ending node is unique, unlike the starting nodes which are at least one. In the next step, an iterative process starts for each starting node. In each iteration loop, the recursion technique is used to access all subsequent nodes of the current node. A tree with one root, many leaves, and many branches will necessarily have its branches joined together. All the branches join and eventually end up in the trunk of the tree. Therefore, keeping the latter in mind, it is easy to establish that at some point two starting nodes will have some common next node in which the two theoretical branches are merging. MathBlock's parsing algorithm controls this condition so that it performs math operations only when all previous operations have been performed. So as MathBlock parses the graph, a loop terminates if it ends up merging with another branch of the tree whose execution is not complete. But a recursion loop will continue to access the graph in case the other branches it encounters have terminated their loop. So, it is certain that the last loop will be able to access all the nodes that are unions of other nodes. D. Execution of the Mathematical Operations When MathBlock parses a node, it performs the operation specified within it. In this sub-section, we will go through what each node does in detail. However, before proceeding with the required analysis, it is necessary to define how MathBlock stores the results. These outcomes are stored in the object storage, which uses MinIO. The outcome of an operation can be a complete portion of the expression used or even the ultimate result of a user call. The service ensures that every time it accesses a node, there is a corresponding object storage path that includes the information that the expression's next nodes must use. If this path already exists, it does not create a new one. When this path does not exist, it creates it in an ordered manner and stores a Comma-separated Values (CSV) file within it. There is only one column labeled as “result” in the aforementioned file, and its rows include all of the results for the presently completed node. As previously stated, there are two types of nodes. If a node is an argument type, MathBlock utilizes the attribute “arg_id” to look it up in the dictionary it created before beginning the graph parsing. After the service finds the argument, there are two possible outcomes, depending on whether the argument is a single value or a whole column of a dataset. If it is a basic value, the service simply generates an exported file that has only the value entered by the user in the first row. MathBlock marks this path as a “value path” after it saves it. If, on the other hand, the given input represents a feature of a dataset, MathBlock does not create a new path because it already exists. As indicated in the previous sub-section, the user provides the route as input when invoking MathBlock. In the two scenarios described, but also when the node is an operation, the service records the path for this node. This notation aids the following nodes in determining which results to act on. MathBlock can perform several operations, all of which have already been stated. Moving on, the generation of the exported files in the object storage is described. The analysis performed in the relevant operations only applies to the circumstance in which all the inputs to each operation are whole columns from prior results or user inputs. If all of an operation's inputs are simply values, the extracted result is the operation of the two values and the extracted file contains the result as a value. However, if one of an operation's inputs is a column and the remaining inputs contain at least one simple value, all the values are turned into columns. This conversion is based on the column that has the most rows in the specified columns. Simple values are translated into columns in which all rows have the same value as the input simple value. The size of the created column is the same as the size of the column with the most rows. The operation is then executed in the following manner, as it has now been translated into operation inputs that are only columns. When an operation has only columns as inputs, the service first retrieves the data from the object storage. It then organizes the data into arrays and begins processing them in memory. It is simple to compute the results. The outcomes are generated and placed in a new array based on the operation that was called. For example, if the supplied operation is addition, a loop is simply utilized to add the data by two. The procedure is applied for the other mathematical expressions as well. Finally, the new path is created within the object storage and the results are stored there. The execution of a statistical operation is the final case that can occur during the execution of MathBlock operations. These operations are the following: (i) “mean_value”, (ii) “variance_value”, and (iii) “amount_of’. They are known as statistical operations because they find statistical measures on a dataset feature, such as the mean value. MathBlock stores the result as a simple value in object storage after computing the information required to complete these operations. There are instances when an operation is unable to complete the tasks assigned to it. These instances are as follows: (i) a statistical operation cannot be calculated (e.g., the operation is to the “variance value” and there is only one row), (ii) different-sized column inputs are provided, and (iii) an attempt is made to execute an illegal operation (e.g., division by zero). In each of these scenarios, the service returns the result “None”. As a result, if an extracted value is “None”, it signifies that no action could be performed, and no meaningful result could be generated in its place. E. Construction of Functions Using Operations MathBlock enables its users to conduct a variety of operations on the datasets they wish to process or analyze. The first operations discovered to be useable by MathBlock users are the ones already described. They let the user to perform mathematical, comparative, logical, and statistical procedures. However, the various functions available to users do not end there. A user can create his/her own functions using the graph that MathBlock utilizes to conduct operations. These functions can employ all of MathBlock's primary operations several times. The outcomes of two such small functions are discussed in the following chapter. In this chapter. some examples of the functions that can be created are given. A user can find the number of values in a dataset that are more than a user-specified threshold by using the “greater than” and “amount of” operations. This can be accomplished by commencing with the “greater than” operation and specifying the value the user is looking for. This operation will produce boolean results with values of “true” or “false”. Then, using the “amount of”, one can identify and count all “true” values. Many algebraic identities can be established by employing the operations “power”, “addition” and “subtraction”. A user can generate the difference of squares, sum of squares, and the squared difference using these. Of course, this is still utilized in the cubic difference and other related identities. As it is clear, a user can create any function he/she desires. In the same manner that one can write operations he/she wants to do as functions on a piece of paper, one can work by constructing the MathBlock graph. As a result, the user's imagination serves as the limit. SECTION IV. Mathblock Experimentation A. Experimentation Method Experimentation was carried out on the MathBlock service to demonstrate how it works. More specifically, data from the maritime [27] and healthcare [28] sectors were utilized. The feature “speed” is utilized for Maritimes data and the feature “age” is used for healthcare data in the aforementioned experiment. Both approaches demonstrate how MathBlock can be used. As a result, it is easy to grasp how it works and when the service is valuable to a user. The aforementioned characteristics were used to generate synthetic data on them. The experiment employs MathBlock to determine whether the method used to generate the synthetic data approximates the real data. This is accomplished using mean values, as discussed in each experiment. Before going over each experiment, it is a good idea to go over how the synthetic data were generated. First, the features to be utilized were chosen, notably “age” and “speed”, as previously described. Following that, the mean value and standard deviation of the two specified features were discovered. The obtained values were then used to create their respective Normal Distributions. Then, as much random data as the data contained in each corresponding genuine characteristic was generated. The Maritimes data contained 38994 distinct rows, whereas the healthcare data contained 1575 distinct rows. Because the two datasets have a high number of records, the mean and variance values will be similar in both examples of fake data generation. The mean value of the created Maritimes data was close to zero. As a result, the random data contained negative values. These values were converted to their corresponding absolute positive values. In the Maritimes data, the mean will deviate by a percentage and should theoretically be much bigger if MathBlock functions properly. B. Maritimes Experimentation Table V displays a sample of the real data that was used for the Maritimes data testing. As previously stated, the “speed” feature was used. Table VI depicts a sample of the created synthetic column. After entering the data into the object storage, the graph that MathBlock will use was written. This graph uses the two object storage path inputs where the real and synthetic data were stored. The graph was given the inverted tree expression illustrated in Fig. 3. Fig. 3. Maritimes inverted tree expression. Show All More specifically, MathBlock was asked to calculate the mean values of the real and synthetic data. After obtaining these statistical metrics, the mean value of the synthetic data should be subtracted from the mean value of the real data. Because the last operation being the root of the tree, it will be the outcome of the experiment. Since several numbers in the specific synthetic data were utilized with their absolute value, the real mean is predicted to be lower than the synthetic one, and hence the result should be fairly negative. Table V. Real maritimes dataset Table VI. Synthetic maritimes dataset C. Healthcare Experimentation Table VII shows a sample of the real data utilized for healthcare data testing. The “age” feature was utilized, as previously indicated. Table VIII shows a sample of the synthetic column that was constructed. The graph that MathBlock will employ was written once the data was entered into the object storage. This graph makes advantage of the two object storage path inputs, which contain both the real and the synthetic datasets. The inverted tree expression shown in Fig. 4 was applied to the graph. Fig. 4. Healthcare inverted tree expression. Show All MathBlock was instructed to first subtract the values of the synthetic data from the real data. MathBlock is then asked to calculate the mean value of the newly computed column. The process used in this and earlier experiments from a mathematical point of view is the same. However, in this experiment, the use of a column operation is done as a proof of concept. Table VII. Real healthcare dataset Table VIII. Synthetic healthcare dataset D. Experimentation Results In the example of Maritimes data, Fig. 5 depicts the speed of each of the 200 first vessels in the dataset as a blue line and the speed reported by the synthetic data as an orange line. It is clear that the mean value of the synthetic data is higher. This experiment resulted with the negative number “-4.29” according to MathBlock. So, the outcome was as planned, and the service functioned flawlessly. The two mean values were initially recorded as simple values in the object storage and then simply subtracted. The second experiment appears to be more interesting. Fig. 6, graphically visualizes the first 200 magnitudes of the real ages with the blue line, while also displaying the magnitudes of the synthetic ages with the orange line. MathBlock first performed the subtraction of the two as requested by the received graph. The result of the subtraction is a whole column whose values are represented in Fig. 7. After that, the graph is asked to find the mean value of these new values. Specifically, the value placed in the object storage by the service is the number “0.16”. This number is very small, and it shows that the synthetic data was quite close to the real data. As a result, the answer of MathBlock is right in the second experiment as well. Fig. 5. Maritimes real and synthetic speeds. Show All Fig. 6. Healthcare real and synthetic ages. Show All Fig. 7. Healthcare age difference. Show All SECTION V. Discussion and Concluding Remarks This paper investigated the functionality of the MathBlock service. First, certain fundamental principles were presented to introduce the user to the topic being discussed. Following, some references to relevant studies on batch data, synthetic data, orchestration systems, mathematical operations systems, object storages, and workflow management systems were provided. MathBlock was then examined in terms of how to utilize it and how it functions. In addition, two experiments were performed as proof of concept utilizing synthetic data. Finally, several graphs were used to evaluate and discuss the outcomes. It is undeniable that MathBlock is an essential tool for anyone who needs to execute mathematical, logical, and comparison operations on various datasets. It can use a variety of features with no restrictions on the type of each one. It can also be implemented as a microservice in scalable systems due to its ability to be utilized by other apps and users over the HTTP protocol, being a novel service that can be used as a language-independent mathematical library. Furthermore, it can be used in low-code environments such as the previously stated [24], which can construct the explained graph automatically for the users. Such systems are going to take computer science to the next level by making technology and resources available globally via the cloud. The service under consideration has a diverse set of potential users. These users could be data analysts who want to run complex mathematical operations on their datasets. Furthermore, the service can be used by scientists from other fields who do not have special understanding of data analysis (e.g., students, healthcare practitioners, and marketing specialists). However, the service is not only able to be used by humans since MathBlock will be especially beneficial in systems that can automatically generate the graph given to them. Because the HTTP protocol is one of the most widely used, it facilitates experimentation and integration with other systems. However, it is necessary to mention the limitations of the service, which must be addressed. Three significant constraints have been identified: (i) If the input data contains values that cannot conduct an operation with another value, the outcome is “None”. In such a circumstance, a user may wish to be notified by the MathBlock system to make a change. (ii) The system uses arrays to manage the values of the columns it calculates. The use of arrays has the critical limitation that the size of memory used cannot exceed the memory accessible to the system. If this occurs, an error will occur, and the calculating procedure will be terminated. (iii) Even with the necessary documentation, constructing the graph is tough. It is advantageous to have a system that can automate the graph construction process. Nevertheless, MathBlock can be enhanced through several actions, which are also among our next steps. The ability to calculate mathematical expressions on streaming data [29] is the first and most significant feature that would be good to add to the service. This would allow such a technology to be used in far more flexible ways in sensor-enabled settings. It will also be beneficial to include more mathematical and statistical operations such as “max”, “min”, “sum”, and others. It would also offer an added value in the case that such a system could employ mathematical limits and integrals as well. Finally, it would make sense to include operations that are already used by MathBlock but are not immediately visible. For example, using the “power” function with the value “0.5”, a user can get the square root of any values in a dataset. This situation, however, does not arise to a man who is accustomed to representing the square root by its mathematical notation. As a result, adding the operation “root” would be a welcomed addition. ACKNOWLEDGMENT The research has been financed by the European Union and Greek national funds through the Operational Program Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH - CREATE - INNOVATE (project codes: RESPECT - T2EDK-03741, DIASTEMA - T2EDK-04612). Authors Figures References Citations Keywords Metrics More Like This Optimal assignment of research and development projects in a large company using an integer programming model IEEE Transactions on Engineering Management Published: 1965 A program of research on the research and development process IEEE Transactions on Engineering Management Published: 1964 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."

Paper 10:
- APA Citation: Solayman, H. E., & Qasha, R. P. (2023). On the Use of Container-based Virtualization for IoT Provisioning and Orchestration: A Survey. _International Journal of Computing Science and Mathematics (IJCSM)_, _18_(4).
  Main Objective: To analyze the use of containerization technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments.
  Study Location: Unspecified
  Data Sources: Literature review
  Technologies Used: Containerization technologies (e.g., Docker, Kubernetes)
  Key Findings: Container virtualisation is the preferred technique for IoT applications due to providing execution isolation, portability, lightweight deployment, and reduced design time as compared with hypervisor-based virtualisation. Containers can be used proficiently for provisioning and orchestrating IoT applications in different environments like Edge and Cloud Computing.
  Extract 1: "Container virtualisation became the preferred technique for IoT applications due to providing execution isolation, portability, lightweight deployment, and reduced design time as compared with hypervisor-based virtualisation."
  Extract 2: This survey presents a comprehensive study of provisioning and orchestrating the distributed IoT applications in different environments like Edge and Cloud Computing, and how containers can be used proficiently for provisioning and orchestrating IoT applications in these environments."
  Limitations: The study is limited to the use of containerization technologies for IoT applications. It does not consider other technologies that may be used for this purpose, such as serverless computing or function-as-a-service (FaaS) platforms.
  Relevance Evaluation: The paper is highly relevant to the point being made in the literature review, which focuses on the use of containerization strategies for scalable and autonomous deployment of data processing and machine learning modules in cloud environments. The paper provides a comprehensive overview of the use of containerization technologies for this purpose, including the benefits and challenges associated with this approach. It also provides a number of case studies that demonstrate the successful use of containerization technologies in real-world applications.
  Relevance Score: 0.9
  Inline Citation: (Solayman & Qasha, 2023)
  Explanation: This study's aim was to analyze the use of containerization technologies for efficient deployment and scaling of data processing and machine learning modules in cloud environments. The authors conducted a comprehensive survey of provisioning and orchestrating distributed IoT applications in different environments like Edge and Cloud Computing.

 Full Text: >
"Login Help Sitemap Home For Authors For Librarians Orders Inderscience Online News Home Full-text access for editors On the use of container-based virtualisation for IoT provisioning and orchestration: a survey by Haleema Essa Solayman; Rawaa Putros Qasha International Journal of Computing Science and Mathematics (IJCSM), Vol. 18, No. 4, 2023  Abstract: The Internet of Things (IoT) has great potential to be adopted by applications covering several smart domains, as it consists of a set of physically linked objects that can be accessed through the internet. Virtualisation techniques play an important role in the field of IoT, especially for provisioning and orchestrating IoT applications to overcome the heterogeneity and diversity of the IoT components and environments that host the applications. Recently, container virtualisation became the preferred technique for IoT applications due to providing execution isolation, portability, lightweight deployment, and reduced design time as compared with hypervisor-based virtualisation. This survey presents a comprehensive study of provisioning and orchestrating the distributed IoT applications in different environments like Edge and Cloud Computing, and how containers can be used proficiently for provisioning and orchestrating IoT applications in these environments. Online publication date: Tue, 28-Nov-2023 The full text of this article is only available to individual subscribers or to users at subscribing institutions.   Existing subscribers: Go to Inderscience Online Journals to access the Full Text of this article. Pay per view: If you are not a subscriber and you just want to read the full contents of this article, buy online access here. Complimentary Subscribers, Editors or Members of the Editorial Board of the International Journal of Computing Science and Mathematics (IJCSM): Login with your Inderscience username and password:     Username:        Password:          Forgotten your password?  Want to subscribe? A subscription gives you complete access to all articles in the current issue, as well as to all articles in the previous three years (where applicable). See our Orders page to subscribe. If you still need assistance, please email subs@inderscience.com    Keep up-to-date Our Blog Follow us on Twitter Visit us on Facebook Our Newsletter (subscribe for free) RSS Feeds New issue alerts Return to top Contact us About Inderscience OAI Repository Privacy and Cookies Statement Terms and Conditions Help Sitemap © 2024 Inderscience Enterprises Ltd."

</subsection_point_Point 1>

<previous_sections>

A systematic review of automated systems for real-time irrigation management

1. INTRODUCTION
The challenge of feeding a growing population with finite resources is becoming increasingly pressing. By 2050, the world population is expected to reach 9.7 billion, necessitating a 70% increase in food production (Falkenmark and Rockstrom, 2009). Irrigation plays a crucial role in enhancing crop yields and agricultural productivity to meet this growing demand. Studies have shown that irrigation can significantly increase crop water productivity, contributing to increased food production (Ali and Talukder, 2008; Playan and Mateos, 2005). However, water scarcity poses a significant challenge, with many regions facing water deficits and the need for improved water management practices (Falkenmark and Rockstrom, 2009). Optimizing irrigation schedules and doses based on crop requirements and environmental conditions is essential for maximizing yield and quality while minimizing water use (Zhang et al., 2024). The necessity of scalable water-efficient practices for increasing food demand cannot be overstated. Techniques such as regulated deficit irrigation, magnetically treated water, and the use of drought-tolerant crops like sorghum have shown promise in improving water productivity and ensuring food security (Mehmood et al., 2023; Putti et al., 2023; Hadebe et al., 2016). As the global food challenge intensifies, it is imperative to critically evaluate the current state and future potential of irrigation management systems to guide research, innovation, and implementation efforts towards fully autonomous, scalable solutions.

Despite the importance of irrigation in addressing the global food challenge, traditional irrigation management techniques, such as manual scheduling and timer-based systems, have significant limitations. These methods are often labor-intensive, inefficient, and less adaptable to changing conditions (Savin et al., 2023). Manual and timer-based scheduling can lead to high operational costs and inefficient water use (Raghavendra, Han, and Shin, 2023). The reliance on manual intervention and predetermined schedules limits their adaptability to changing environmental conditions, crop water requirements, and soil moisture levels (Kaptein et al., 2019). Sensor-based irrigation systems offer an alternative, enabling real-time adjustments based on soil water status measurements (Kaptein et al., 2019). However, the adoption of these systems in commercial settings has been limited, often requiring extensive input from researchers (Kim et al., 2014; Lea-Cox et al., 2018; Ristvey et al., 2018). The limitations of traditional irrigation management techniques highlight the need for scalable, automated solutions for greater efficiency in irrigation management. Automated systems that collect real-time data, analyze it, and make autonomous irrigation decisions can lead to improved water use efficiency and increased crop productivity (Champness et al., 2023; Wu et al., 2022). To fully understand the potential of automated systems, it is necessary to examine the automation of each part of the irrigation management pipeline and analyze the effectiveness and efficiency of integrated end-to-end solutions.

The emergence of smart irrigation management and IoT marks a significant shift from historical irrigation practices. Modern approaches rely on vast data and analysis algorithms, leveraging technologies such as remote sensing, sensor networks, weather data, and computational algorithms (Atanasov, 2023; Bellvert et al., 2023; Kumar et al., 2023). IoT plays a vital role in collecting vast amounts of data through sensors, data transmission, and tailored networks, enabling real-time monitoring and control of irrigation systems (Liakos, 2023; Zuckerman et al., 2024). These advancements in data collection and analysis have the potential to revolutionize irrigation management, allowing for more precise and efficient water use. However, challenges such as processing diverse data sources, data integration, and lack of integrated data analysis hamper the full benefit of IoT in irrigation management (Dave et al., 2023). The current fragmented approach in smart irrigation, focusing on individual components rather than the entire system, limits the potential for fully autonomous, real-time end-to-end irrigation management (Togneri et al., 2021). To address these challenges and fully realize the potential of smart irrigation management, there is a need for automating and integrating each section of the irrigation management pipeline, from sensor/weather data collection and transmission to processing, analysis, decision-making, and automated action (McKinion and Lemmon, 1985). This integration requires a thorough investigation of the role of interoperability and standardization in enabling seamless communication and compatibility between components within the automated irrigation management pipeline.

Machine learning (ML) plays a significant role in processing vast data, predicting plant stress, modeling climate effects, and optimizing irrigation in smart irrigation management systems. ML algorithms can analyze data collected from sensors and weather stations to determine optimal irrigation schedules (Vianny et al., 2022). However, the potential of ML is often constrained by manual steps, such as data interpretation, decision-making on irrigation timing and volume, and system adjustments. Automating ML integration to allow direct action from insights to irrigation execution, removing bottlenecks and achieving real-time adaptability, is crucial for fully autonomous irrigation management (Barzallo-Bertot et al., 2022). By integrating ML into automated systems, the irrigation management pipeline can become more seamless and efficient, enabling real-time decision-making and action based on data-driven insights. To achieve this level of automation and integration, it is essential to identify gaps and propose solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.

To achieve seamless integration across the automated irrigation management system, interoperability and standardization are critical. Interoperability allows different system components, such as sensors, actuators, and software, to communicate and exchange data effectively, while standardization ensures that data is represented in a consistent format (Santos et al., 2020). Standardized protocols and data formats are essential for achieving seamless integration and ensuring compatibility between components in real-time irrigation management systems (Robles et al., 2022; Hatzivasilis et al., 2018). Existing and emerging standards, such as OGC SensorThings API and ISO 11783, have applicability to real-time irrigation management systems (Hazra et al., 2021). However, challenges such as data quality, scalability, reliability, and security need to be addressed to fully realize the potential of interoperability and standardization in automated irrigation management systems (Hazra et al., 2021). Addressing these challenges is crucial for enabling the seamless integration of components within the automated irrigation management pipeline, which is essential for achieving fully autonomous, scalable irrigation management. A comprehensive evaluation of the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline is necessary to guide future research and implementation efforts.
The primary objective of this systematic review is to critically evaluate the current state and future potential of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies for enhancing agricultural water use efficiency and crop productivity.
Specific objectives include:
•	Examining the automation of each part of the irrigation management pipeline and the seamless integration of each section in the context of irrigation scheduling and management.
•	Analyzing the effectiveness and efficiency of integrated end-to-end automated irrigation systems.
•	Investigating the role of interoperability and standardization in enabling the integration of components within the automated irrigation management pipeline.
•	Identifying gaps and proposing solutions for seamless integration across the automated irrigation management system, aiming to achieve fully autonomous, scalable irrigation management.
By addressing these objectives, this systematic review aims to provide a comprehensive and critical evaluation of the current state and future potential of real-time, end-to-end automated irrigation management systems. Its intention is to guide future research, innovation, and implementation efforts to achieve fully autonomous, scalable irrigation management that can contribute to addressing the global food challenge.

2. REVIEW METHODOLOGY
•	Question-driven framework to guide the literature review of real-time, autonomous irrigation management systems
•	Key research questions posed, each with the motivation behind investigating them and a starting hypothesis to evaluate against the examined literature
•	Table presenting the major objectives, specific objectives, questions, motivations, and hypotheses
3. DATA COLLECTION TO CLOUD: AUTOMATION AND REAL-TIME PROCESSING
3.1. Irrigation management data
The success of automated irrigation management systems relies heavily on the collection, transmission, and analysis of various types of data. The most applicable data types for irrigation management include soil moisture, canopy temperature, weather data, and plant physiological parameters (Farooq et al., 2019; Li et al., 2019; Olivier et al., 2021; Evett et al., 2020). These data are typically collected from a range of sources, including in-field sensors, remote sensing platforms, weather stations, and manual measurements (Li et al., 2019; Karimi et al., 2018).
Soil moisture data is arguably the most critical type of data for irrigation management, as it directly reflects the water available to plants and can be used to determine the optimal timing and amount of irrigation (Olivier et al., 2021; Intrigliolo & Castel, 2006). Soil moisture sensors, such as tensiometers, capacitance probes, and time-domain reflectometry (TDR) sensors, can provide real-time measurements of soil water content at various depths (Farooq et al., 2019). These sensors can be deployed in a network configuration to capture spatial variability in soil moisture across a field (Karimi et al., 2018).
Canopy temperature data is another valuable type of data for irrigation management, as it can be used to assess plant water stress and adjust irrigation accordingly (Evett et al., 2020). Infrared thermometers and thermal cameras can be used to measure canopy temperature, which is influenced by factors such as air temperature, humidity, wind speed, and plant water status (Li et al., 2019). When plants experience water stress, they tend to close their stomata to reduce water loss, leading to an increase in canopy temperature (Evett et al., 2020). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
Weather data, including temperature, humidity, precipitation, wind speed, and solar radiation, are essential for predicting crop water requirements and scheduling irrigation events (Akilan & Baalamurugan, 2024). Weather stations equipped with various sensors can provide real-time measurements of these parameters, which can be used as inputs for crop water requirement models, such as the FAO-56 Penman-Monteith equation (Li et al., 2019). These models estimate crop evapotranspiration (ET) based on weather data and crop-specific coefficients, allowing for the calculation of irrigation requirements (Intrigliolo & Castel, 2006). By integrating weather data into automated irrigation systems, irrigation schedules can be dynamically adjusted based on changing environmental conditions, ensuring that crops receive the optimal amount of water at the right time (Akilan & Baalamurugan, 2024).
When collecting and utilizing these data types, several considerations must be taken into account, including the volume, frequency, format, and source of the data (Farooq et al., 2019). The volume of data generated by automated irrigation systems can be substantial, especially when high-resolution sensors are deployed at a large scale (Bastidas Pacheco et al., 2022). This necessitates the use of efficient data storage, processing, and transmission technologies to handle the data load (Farooq et al., 2019). The frequency of data collection is another important consideration, as it directly impacts the temporal resolution of the data and the ability to detect rapid changes in plant water status or environmental conditions (Bastidas Pacheco et al., 2022). Bastidas Pacheco et al. (2022) demonstrated that collecting full pulse resolution data from water meters provides more accurate estimates of event occurrence, timing, and features compared to aggregated temporal resolutions, highlighting the importance of selecting appropriate data collection frequencies to ensure the quality and usefulness of the data for irrigation management.
The format of the data is also crucial, as it determines the compatibility and interoperability of the data with various analysis tools and platforms (Farooq et al., 2019). Standardized data formats, such as JSON, XML, or CSV, can facilitate data exchange and integration between different components of the automated irrigation system (Zhang et al., 2023). The source of the data is another important consideration, as it can impact the reliability, accuracy, and spatial coverage of the data (Farooq et al., 2019). For example, in-field sensors provide highly localized measurements, while remote sensing platforms, such as satellites or drones, can provide data at larger spatial scales (Li et al., 2019). By combining data from multiple sources, automated irrigation systems can achieve a more comprehensive understanding of crop water requirements and optimize irrigation management accordingly (Farooq et al., 2019).
Data quality, accuracy, and reliability are paramount in irrigation management, as they directly impact the effectiveness of decision-making processes and the efficiency of water use (Gupta et al., 2020). Inaccurate or unreliable data can lead to suboptimal irrigation decisions, resulting in crop stress, yield losses, or wasted water resources (Ramli & Jabbar, 2022). Gupta et al. (2020) emphasized the critical importance of data security and privacy in smart farming systems, as the leakage of sensitive agricultural data can cause severe economic losses to farmers and compromise the integrity of the automated irrigation system. The authors also highlighted the need for robust authentication and secure communication protocols to prevent unauthorized access to smart farming systems and protect data in transit (Gupta et al., 2020).
Ramli and Jabbar (2022) addressed the challenges associated with implementing real-time, automated irrigation systems, including data quality, scalability, reliability, and security. They proposed solutions and best practices based on the analysis of case studies and real-world implementations, such as the use of redundant sensors, data validation techniques, and secure communication protocols (Ramli & Jabbar, 2022). The authors also emphasized the importance of regular maintenance and calibration of sensors to ensure the accuracy and reliability of the collected data (Ramli & Jabbar, 2022).
Researchers have investigated the use of data compression, aggregation, and filtering techniques to reduce bandwidth requirements and improve transmission efficiency in automated irrigation systems (Karim et al., 2023; Rady et al., 2020; Cui, 2023). Karim et al. (2023) explored the effectiveness of various data compression techniques, such as lossless and lossy compression algorithms, in reducing the size of data packets transmitted over wireless networks. The authors found that lossless compression techniques, such as Huffman coding and Lempel-Ziv-Welch (LZW), can significantly reduce data size without compromising data quality, while lossy compression techniques, such as JPEG and MP3, can further reduce data size by introducing acceptable levels of distortion (Karim et al., 2023).
Rady et al. (2020) developed a novel data compression algorithm specifically designed for irrigation data, which achieved significant compression ratios without compromising data quality. The authors demonstrated that their algorithm could reduce the amount of data transmitted over wireless networks, thereby improving the efficiency of the irrigation system and reducing costs (Rady et al., 2020). Cui (2023) investigated the use of data aggregation and filtering techniques to reduce the number of transmissions and save bandwidth in automated irrigation systems. The author proposed a data aggregation scheme that combines multiple sensor readings into a single value, such as the average soil moisture over a specified time interval, to reduce the frequency of data transmissions (Cui, 2023). Additionally, the author explored the use of data filtering techniques, such as Kalman filters and particle filters, to remove noise and outliers from sensor data, improving the accuracy and reliability of the transmitted information (Cui, 2023).
Data standardization and harmonization are crucial for facilitating seamless integration and interoperability between the various components of automated irrigation management systems (Zhang et al., 2023; Ermoliev et al., 2022). Zhang et al. (2023) developed a novel cyberinformatics technology called iCrop, which enables the in-season monitoring of crop-specific land cover across the contiguous United States. The authors highlighted the importance of data standardization and harmonization in the context of iCrop, as it allows for the efficient distribution of crop-specific land cover information based on the findable, accessible, interoperable, and reusable (FAIR) data principle (Zhang et al., 2023). By adopting standardized data formats and protocols, such as the Open Geospatial Consortium (OGC) standards, iCrop enables the seamless integration of various data sources and facilitates the interoperability of the system with other agricultural decision support tools (Zhang et al., 2023).
Ermoliev et al. (2022) proposed a linkage methodology for linking distributed sectoral/regional optimization models in a situation where private information is not available or cannot be shared by modeling teams. The authors emphasized the need for data standardization to enable decentralized cross-sectoral coordination and analysis, as it allows for the consistent representation and exchange of data between different models and stakeholders (Ermoliev et al., 2022). By adopting standardized data formats and interfaces, the proposed linkage methodology can facilitate the integration of various optimization models and support the development of comprehensive decision support systems for sustainable resource management (Ermoliev et al., 2022).
Metadata plays a vital role in providing context and enabling better data interpretation and decision-making in automated irrigation management systems (Jahanddideh-Tehrani et al., 2021). Metadata refers to the additional information that describes the characteristics, quality, and context of the primary data, such as the sensor type, calibration parameters, measurement units, and timestamp (Jahanddideh-Tehrani et al., 2021). Jahanddideh-Tehrani et al. (2021) highlighted the importance of metadata in water resources management, as it enables decision-makers to use the data to the best of its capabilities by understanding factors such as when water data was collected and what factors might have contributed to the measurements. The authors emphasized the need for standardized metadata formats and guidelines, such as the Dublin Core Metadata Initiative (DCMI) and the ISO 19115 standard, to ensure the consistency and interoperability of metadata across different water information systems (Jahanddideh-Tehrani et al., 2021).
In the context of automated irrigation management systems, metadata can provide valuable information about the data collection process, sensor performance, and environmental conditions that can aid in data interpretation and decision-making (Cota & Mamede, 2023). For example, metadata about the sensor type and calibration parameters can help assess the accuracy and reliability of the collected data, while metadata about the weather conditions and soil properties can provide context for interpreting the data and adjusting irrigation strategies accordingly (Cota & Mamede, 2023). By incorporating metadata into the data management and analysis pipeline of automated irrigation systems, decision-makers can make more informed and context-aware decisions, leading to improved water use efficiency and crop productivity (Jahanddideh-Tehrani et al., 2021).

3.2. Edge Computing and Fog Computing
Edge computing and fog computing have emerged as transformative technologies in the realm of real-time irrigation management systems, offering significant potential for improving efficiency, scalability, and reliability (Abdel Nasser et al., 2020; Tran et al., 2019). Edge computing refers to the practice of processing data near the edge of the network, close to the source of the data, while fog computing is a decentralized computing infrastructure that extends cloud computing capabilities to the network edge (Hassija et al., 2019). These technologies bring computation and analytics closer to the data source, reducing the need for data to travel to the cloud and enabling faster processing and decision-making (Hassija et al., 2019; Zhang et al., 2020).
The potential of edge computing and fog computing in real-time irrigation management is immense. Abdel Nasser et al. (2020) proposed a two-layer system for water demand prediction using automated meters and machine learning techniques, demonstrating the potential of edge computing in improving the efficiency and scalability of irrigation management. The system collects and aggregates data from distributed smart meters in the first layer, while the second layer uses LSTM neural networks to predict water demand for different regions of households. By leveraging edge computing, the system can achieve high accuracy in predicting water demand, which is essential for efficient irrigation management (Abdel Nasser et al., 2020).
Tran et al. (2019) conducted a comprehensive review of real-time, end-to-end automated irrigation management systems, highlighting the role of fog computing in addressing data transmission challenges and enabling seamless integration across the irrigation management pipeline. The authors emphasize that real-time, end-to-end automated irrigation management systems have the potential to significantly improve water efficiency, crop yields, and reduce labor costs. However, they also identify several challenges that need to be addressed, such as data quality, scalability, reliability, and security, which can be effectively tackled by implementing fog computing architectures (Tran et al., 2019).
Edge computing offers several benefits in real-time irrigation management systems, including reduced latency, real-time decision-making, and reduced reliance on cloud connectivity (Mishra, 2020; Zhang et al., 2020). By processing data closer to the source, edge computing enables faster response times and more efficient data handling (Mishra, 2020). Mishra (2020) highlights that edge computing reduces latency by processing data closer to the source, enabling real-time decision-making and lessening reliance on cloud connectivity by shifting processing to local or edge devices.
Zhang et al. (2020) explore the application of edge computing in agricultural settings, demonstrating its potential to improve the efficiency and accuracy of irrigation systems. The authors discuss how edge computing has prospects in various agricultural applications, such as pest identification, safety traceability of agricultural products, unmanned agricultural machinery, agricultural technology promotion, and intelligent management. They also emphasize that the emergence of edge computing models, such as fog computing, cloudlet, and mobile edge computing, has transformed the management and operation of farms (Zhang et al., 2020).
Fog computing plays a crucial role in distributing processing and storage across the network, enhancing the scalability and reliability of automated irrigation systems (Premkumar & Sigappi, 2022; Singh et al., 2022). Premkumar and Sigappi (2022) evaluate the current state of automated irrigation management systems and propose a hybrid machine learning approach for predicting soil moisture and managing irrigation. Their study emphasizes the potential of fog computing in distributing processing and storage across the network, improving the efficiency and scalability of irrigation systems. The proposed hybrid machine learning approach outperforms other machine learning algorithms in predicting soil moisture, demonstrating the effectiveness of fog computing in enhancing the performance of automated irrigation systems (Premkumar & Sigappi, 2022).
Singh et al. (2022) discuss the role of fog computing in distributing processing and storage across the network, enhancing scalability and reliability in agricultural management systems. The authors argue that by implementing fog computing, these systems can achieve faster data processing and response times, improving overall efficiency and effectiveness. They also highlight that fog computing can address the challenges faced by real-time data transmission in agricultural management systems, such as latency, bandwidth limitations, and data security (Singh et al., 2022).
The integration of edge and fog computing in real-time irrigation management systems is crucial for achieving fully automated, scalable, and reliable solutions. As the demand for autonomous irrigation management grows, these technologies will play a pivotal role in enabling faster decision-making, reduced latency, improved resource utilization, and seamless integration across the irrigation management pipeline (Tran et al., 2019; Zhang et al., 2020). By bringing computation and analytics closer to the data source and distributing processing and storage across the network, edge and fog computing can significantly enhance the efficiency and effectiveness of automated irrigation systems, contributing to the overall goal of addressing the global food challenge through optimized water resource management and increased agricultural productivity (Abdel Nasser et al., 2020; Premkumar & Sigappi, 2022; Singh et al., 2022).

3.3. Automation of Data Collection
The automation of data collection is a critical component in the development of real-time, end-to-end automated irrigation management systems that integrate IoT and machine learning technologies. It enables the efficient gathering of vital information about crop health, environmental conditions, and water requirements, which is essential for enhancing agricultural water use efficiency and crop productivity. Two key aspects of automated data collection are the use of advanced sensing technologies for non-invasive plant stress detection and the implementation of wireless sensor networks and energy-efficient communication protocols for large-scale, long-term data collection.
Advanced sensing technologies, such as hyperspectral imaging and thermal sensing, have emerged as powerful tools for non-invasive plant stress detection in automated irrigation management systems. These technologies provide valuable information about crop traits, enabling early and accurate detection of plant health issues (Triantafyllou et al., 2019). Triantafyllou et al. (2019) propose a comprehensive reference architecture model that incorporates advanced sensing technologies in the sensor layer for real-time plant stress detection, highlighting their importance in providing non-invasive plant stress detection. Similarly, Hossain et al. (2023) present a novel IoT-ML-Blockchain integrated framework for smart agricultural management that leverages advanced sensing technologies to optimize water use and improve crop yield, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
Hyperspectral imaging can capture subtle changes in plant physiology that are indicative of stress, while machine learning algorithms can be employed to extract meaningful patterns from the spectral data and classify different stress types (Araus et al., 2014). Thermal sensing can detect changes in canopy temperature, which is influenced by factors such as plant water status (Li et al., 2019). By monitoring canopy temperature and comparing it to reference values, automated irrigation systems can detect plant water stress and trigger irrigation events to maintain optimal plant health and productivity (Li et al., 2019).
The integration of advanced sensing technologies in automated irrigation management systems has the potential to revolutionize precision agriculture. Jiang et al. (2019) demonstrate the effectiveness of a deep learning-based model in accurately detecting leaf spot diseases, highlighting the importance of image augmentation and deep learning algorithms in enhancing the model's performance.
Wireless sensor networks (WSNs) and energy-efficient communication protocols have the potential to significantly improve the efficiency and reliability of data collection in large-scale, long-term irrigation systems. WSNs offer a cost-effective and scalable solution for real-time data collection in large-scale irrigation systems, providing remote monitoring and automated control capabilities (Mehdizadeh et al., 2020). Nishiura and Yamamoto (2021) propose a novel sensor network system that utilizes drones and wireless power transfer to autonomously collect environmental data from sensor nodes in vast agricultural fields, reducing operational costs and enhancing the efficiency of data collection. Similarly, Higashiura and Yamamoto (2021) introduce a network system that employs UAVs and LoRa communication to efficiently collect environmental data from sensor nodes distributed across large farmlands, optimizing data collection and reducing travel distance and time.
Energy-efficient communication protocols are crucial for ensuring reliable data transmission in challenging environmental conditions and extending the lifespan of sensor nodes (Mehdizadeh et al., 2020). Al-Ali et al. (2023) investigate the potential of WSNs and energy-efficient communication protocols for data collection in large-scale, long-term irrigation systems, discussing the challenges and opportunities of using these technologies to improve the efficiency and reliability of real-time data collection in irrigation management. Mehdizadeh et al. (2020) emphasize the need for careful consideration of factors such as data accuracy, energy consumption, and network reliability when designing effective WSNs for irrigation management, enabling timely irrigation decisions and improved crop yields.
The automation of data collection through the use of advanced sensing technologies and wireless sensor networks is essential for achieving fully autonomous, scalable irrigation management. By enabling non-invasive plant stress detection and large-scale, long-term data collection, these technologies contribute to the overall goal of optimizing water resource management and increasing agricultural productivity. The integration of these technologies in real-time, end-to-end automated irrigation management systems has the potential to enhance agricultural water use efficiency and crop productivity, ultimately contributing to the development of fully autonomous, scalable irrigation management solutions.

3.4: Real-Time Data Transmission Protocols and Technologies
Real-time data transmission is a critical component of automated irrigation management systems, as it enables the timely delivery of sensor data to the cloud for processing and decision-making. The exploration of suitable protocols and network architectures is essential for ensuring efficient and reliable data transmission in these systems, contributing to the overall goal of enhancing agricultural water use efficiency and crop productivity.
The Message Queuing Telemetry Transport (MQTT) protocol has emerged as a popular choice for real-time data transmission in IoT networks, including those used for automated irrigation management. MQTT is a lightweight, publish-subscribe protocol designed for constrained devices and low-bandwidth networks (Author, 2019). Its simplicity and low overhead make it well-suited for IoT applications where data transmission speed and energy efficiency are critical (Saranyadevi et al., 2022). MQTT provides three Quality of Service (QoS) levels, ensuring data reliability in real-time scenarios (Author, 2019). Chen et al. (2020) proposed novel algorithms to improve data exchange efficiency and handle rerouting in MQTT-based IoT networks for automated irrigation management systems. Their TBRouting algorithm efficiently finds the shortest paths for data transmission, while the Rerouting algorithm effectively handles the rerouting of topic-based session flows when a broker crashes. The combination of these algorithms can significantly improve the performance and reliability of automated irrigation management systems (Chen et al., 2020).
Client-server IoT networks, such as those based on MQTT, play a crucial role in real-time data transmission for automated irrigation management systems. In these networks, sensors and devices (clients) publish data to a central broker (server), which then distributes the data to subscribed clients (Verma et al., 2021). This architecture enables efficient data collection, processing, and dissemination, facilitating the integration of various components within the automated irrigation management pipeline. Verma et al. (2021) proposed an architecture for healthcare monitoring systems using IoT and communication protocols, which provides a comprehensive overview of existing approaches and highlights challenges and opportunities in the field. Although focused on healthcare, the insights from this study can be applied to automated irrigation management systems, emphasizing the importance of interoperability and standardization for seamless integration (Verma et al., 2021).
In addition to MQTT, other application layer protocols such as XMPP, CoAP, SOAP, and HTTP have been explored for real-time data transmission in IoT networks. Each protocol has its strengths and weaknesses, making them suitable for different applications and scenarios. XMPP (Extensible Messaging and Presence Protocol) is an open-standard protocol that supports real-time messaging, presence, and request-response services (Saint-Andre, 2011). CoAP (Constrained Application Protocol) is a specialized web transfer protocol designed for use with constrained nodes and networks in the Internet of Things (Shelby et al., 2014). SOAP (Simple Object Access Protocol) is a protocol for exchanging structured information in the implementation of web services, while HTTP (Hypertext Transfer Protocol) is the foundation of data communication for the World Wide Web (Fielding et al., 1999).
Motamedi and Villányi (2022) compared and evaluated wireless communication protocols for the implementation of smart irrigation systems in greenhouses, considering factors such as power consumption, range, reliability, and scalability. They found that ZigBee is the most suitable local communication protocol for greenhouse irrigation due to its large number of nodes and long range, while MQTT is the recommended messaging protocol for smart irrigation systems due to its TCP transport protocol and quality of service (QoS) options. GSM is a reliable and cost-effective global communication protocol for greenhouse irrigation, providing wide coverage and low cost (Motamedi & Villányi, 2022).
Syafarinda et al. (2018) investigated the use of the MQTT protocol in a precision agriculture system using a Wireless Sensor Network (WSN). They found that MQTT is suitable for use in IoT applications due to its lightweight, simple, and low bandwidth requirements. The average data transmission speed using the MQTT protocol was approximately 1 second, demonstrating its effectiveness for real-time data transmission in precision agriculture systems (Syafarinda et al., 2018).
The choice of application layer protocol for real-time irrigation management depends on factors such as data transmission speed, reliability, and energy efficiency. MQTT and RTPS (Real-Time Publish-Subscribe) are both suitable for real-time data transmission in IoT systems, but they have different strengths and weaknesses. MQTT is a better choice for applications that require low latency and high throughput, while RTPS is a better choice for applications that require high reliability and low latency (Sanchez-Iborra & Skarmeta, 2021). The exploration of MQTT and client-server IoT networks, along with the comparison of various application layer protocols, provides valuable insights into the suitability of these technologies for real-time data transmission in automated irrigation management systems.
In summary, real-time data transmission protocols and technologies play a vital role in the automation of irrigation management systems, enabling the efficient and reliable delivery of sensor data to the cloud for processing and decision-making. The exploration of MQTT and client-server IoT networks, along with the comparison of application layer protocols, highlights the importance of selecting suitable technologies based on factors such as data transmission speed, reliability, and energy efficiency. By leveraging these technologies, automated irrigation management systems can achieve seamless integration and contribute to the overall goal of enhancing agricultural water use efficiency and crop productivity.

3.5. Challenges and Solutions in Real-Time Data Transmission
Following the exploration of data collection, processing at the edge and fog, and automation in previous sections, we now turn to the critical aspect of real-time data transmission. While essential for automated irrigation management, this stage presents unique challenges that must be addressed to ensure system efficiency and reliability.
Obstacles in Real-Time Data Transmission
Agricultural environments present unique challenges for real-time data transmission, directly impacting the effectiveness of automated irrigation systems. Environmental factors can significantly disrupt wireless communication. Adverse weather conditions such as heavy rain, fog, and high winds can weaken or even block radio signals, leading to data loss and compromised system performance. Physical obstacles like trees, buildings, and uneven terrain further complicate signal propagation, creating reliability issues (Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). These environmental challenges necessitate robust communication protocols and network architectures that can ensure consistent and reliable data flow.
In addition to environmental factors, technical limitations also present significant obstacles. Large-scale agricultural operations often demand long-distance data transmission, which can be hindered by the limited range of certain wireless communication protocols. Network congestion, occurring when multiple sensors transmit data concurrently, can lead to delays and potential data loss, further complicating real-time decision-making (Hameed et al., 2020). To mitigate these issues, researchers have investigated the potential of cognitive radio networks (CRNs) and dynamic spectrum access (DSA) for optimizing spectrum utilization and reducing interference (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). CRNs enable devices to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency. DSA, on the other hand, facilitates the dynamic allocation of unused spectrum, enhancing spectrum utilization and reducing congestion.
Furthermore, data security and privacy are paramount concerns in real-time irrigation systems. The sensitive nature of agricultural data, such as crop yields and farm management practices, necessitates robust security measures to prevent unauthorized access and data breaches (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential to protect data integrity and ensure the trustworthiness of the system.
Investigating Data Optimization Techniques
To enhance the efficiency and reliability of real-time data transmission in automated irrigation systems, researchers have explored a range of data optimization techniques. Data compression techniques aim to reduce the size of data packets transmitted over the network, minimizing bandwidth requirements and improving transmission speed (Rady et al., 2020; Karim et al., 2023). Lossless compression algorithms, such as Huffman coding and LZW, preserve data integrity while effectively reducing data size, ensuring that no information is lost during transmission (Cui, 2023). Lossy compression algorithms, such as JPEG and MP3, offer higher compression ratios but introduce a controlled level of data loss, which may be acceptable for certain applications where some loss of precision is tolerable (Karim et al., 2023). The choice between lossless and lossy compression depends on the specific application and the trade-off between data size and accuracy.
Data aggregation techniques provide another effective approach to optimize data transmission. By aggregating multiple sensor readings into a single representative value, such as average soil moisture or temperature, the number of transmissions can be significantly reduced, conserving bandwidth and energy resources (Cui, 2023). This is particularly beneficial in large-scale irrigation systems where numerous sensors are deployed across vast areas, generating substantial amounts of data. Additionally, data filtering techniques play a crucial role in improving data quality and reliability. Kalman filters and particle filters can effectively remove noise and outliers from sensor data, ensuring that only accurate and relevant information is transmitted and used for decision-making (Cui, 2023). This is essential for preventing erroneous data from influencing irrigation decisions and potentially leading to suboptimal water management.
Sensor calibration, drift correction, and fault detection are essential for maintaining data accuracy and reliability (Dos Santos et al., 2023). Regular calibration ensures that sensors provide accurate measurements over time, while drift correction techniques account for gradual changes in sensor readings due to environmental factors or aging. Fault detection mechanisms can identify and address sensor malfunctions or anomalies, preventing erroneous data from influencing irrigation decisions and potentially harming crops or wasting water.
Addressing the Challenges
Effectively addressing the challenges in real-time data transmission requires a multifaceted approach that encompasses environmental, technical, and data-related considerations. Implementing robust and adaptive communication protocols is crucial for overcoming interference and signal degradation caused by weather conditions and physical obstacles. Selecting appropriate protocols, such as LoRa or ZigBee, with suitable range and penetration capabilities can ensure reliable data transmission in challenging agricultural environments (Motamedi & Villányi, 2022). Additionally, employing techniques like frequency hopping and error correction codes can further improve communication resilience and mitigate data loss.
Optimizing network architecture is another key consideration. Deploying a distributed network architecture with edge and fog computing capabilities can significantly enhance data processing and transmission efficiency (Abdel Nasser et al., 2020; Tran et al., 2019). Edge devices can perform initial data processing and aggregation tasks, reducing the amount of data transmitted to the cloud and minimizing latency, while fog nodes can provide additional processing power and storage closer to the data source, enhancing scalability and reliability. This distributed approach alleviates the burden on the central cloud server and allows for more responsive and efficient irrigation management.
Data optimization techniques play a vital role in reducing bandwidth requirements and improving transmission efficiency. The choice of data compression, aggregation, and filtering techniques should be tailored to the specific requirements of the irrigation system, considering factors such as data type, accuracy needs, and available bandwidth. By carefully selecting and implementing these techniques, the overall performance and effectiveness of real-time irrigation systems can be significantly enhanced, leading to more sustainable water management practices and improved agricultural productivity.
By addressing these challenges and implementing appropriate solutions, real-time data transmission can become a reliable and efficient component of automated irrigation systems, contributing to the overall goal of achieving sustainable and productive agriculture in the face of growing food demands and water scarcity.

3.6. IoT Network Architectures and Variable Rate Irrigation (VRI) for Real-Time Irrigation
Real-time irrigation management systems heavily rely on the efficient and reliable transmission of data from sensors and weather stations to the cloud for processing and decision-making. However, agricultural environments present unique challenges to wireless communication, including adverse weather conditions, physical obstacles, and the limitations of wireless technologies. These challenges necessitate robust and adaptive solutions to ensure the consistent and timely flow of data, enabling truly autonomous irrigation scheduling.
Environmental factors, such as heavy rain, fog, and strong winds, can significantly disrupt wireless communication by attenuating or even blocking radio signals, leading to data loss and compromised system performance (Ed-daoudi et al., 2023; Jukan et al., 2017; Yi & Ji, 2014; Zhang, Chang & Baoguo, 2018). Dense vegetation, buildings, and uneven terrain create further complications by causing multipath propagation and shadowing effects (Yim et al., 2018; Gautam and Pagay, 2020). The study by Yim et al. (2018) on LoRa networks in a tree farm environment exemplifies these challenges, revealing reduced communication range and data reliability compared to theoretical expectations. This underscores the need for carefully selecting and optimizing communication protocols and network parameters to ensure reliable data transmission in such environments.
The study by Guzinski et al. (2014a) using a modified TSEB model further highlights the importance of high-resolution data in accurately capturing the spatial and temporal dynamics of energy fluxes influenced by environmental factors. This emphasizes the need for advanced data acquisition and processing techniques that can effectively represent the complexities of agricultural settings.
The limitations of traditional wireless communication technologies, such as limited range and network congestion, pose additional challenges for large-scale agricultural operations. Long-distance data transmission can be hindered by range limitations, while network congestion arising from numerous sensors transmitting concurrently can lead to delays and data loss, hindering real-time decision-making (Hameed et al., 2020). Addressing these challenges requires the exploration of advanced networking technologies that can optimize spectrum utilization, mitigate interference, and improve reliability and efficiency.
Cognitive Radio Networks (CRNs) and Dynamic Spectrum Access (DSA) offer promising solutions for optimizing wireless communication in agricultural settings. CRNs empower devices with the ability to intelligently sense and adapt to the surrounding radio environment, dynamically adjusting transmission parameters to avoid interference and improve communication efficiency (Righi et al., 2017; Shafi et al., 2018; Trigka & Dritsas, 2022). Research has explored the potential of CRNs in predicting Radio Frequency (RF) power to avoid noisy channels and optimize spectrum utilization (Iliya et al., 2014; Iliya et al., 2014). These studies demonstrate the effectiveness of combining optimization algorithms with artificial neural networks (ANNs) to enhance the accuracy and generalization of RF power prediction, enabling CRNs to make informed decisions about channel selection and avoid interference.
DSA complements CRN technology by dynamically allocating unused spectrum, further enhancing spectrum utilization and reducing congestion (Shi et al., 2023). The numerical model developed by Shi et al. (2023) showcases the potential of CRNs and DSA for optimizing wireless communication in challenging environments.
The integration of CRNs and DSA into the IoT network architecture requires careful consideration of spectrum sensing techniques, network topology, and data security. Research on cooperative spectrum sensing suggests that distributed approaches, where sensor nodes collaborate and share information, can significantly improve the accuracy and efficiency of spectrum sensing, particularly in dynamic environments (Trigka and Dritsas, 2022; Khalid & Yu, 2019). This collaborative approach enables a more comprehensive understanding of the radio environment and facilitates the identification of available frequency bands for data transmission.
The choice of network topology also impacts the performance and scalability of CRN-based irrigation systems. Mesh networks, where sensor nodes are interconnected and relay data for each other, offer enhanced resilience and coverage compared to star topologies where nodes communicate directly with a central gateway (Akyildiz & Vuran, 2010). However, mesh networks can be more complex to manage and may introduce additional routing overhead. The trade-off between network resilience and complexity needs to be carefully evaluated to select the most appropriate topology for a specific agricultural setting.
Data security and privacy are paramount concerns in IoT-based irrigation systems due to the sensitive nature of agricultural data (Gupta et al., 2020). Implementing secure communication protocols, authentication mechanisms, and encryption techniques is essential for protecting data integrity and ensuring system trustworthiness. Research on secure spectrum leasing and resource allocation algorithms for CR-WSN-based irrigation systems has demonstrated the potential of these technologies for enhancing security and efficiency (Hassan, 2023; Afghah et al., 2018).
In conclusion, the development of effective and reliable real-time irrigation management systems requires a comprehensive approach that addresses the challenges of data transmission in agricultural environments. The integration of robust and adaptive communication protocols, optimized network architectures, and advanced networking technologies like CRNs and DSA, along with a focus on data security and privacy, can contribute significantly to achieving the goal of autonomous and efficient irrigation scheduling.
4. AUTOMATED DATA PROCESSING IN THE CLOUD
4.1. Data Quality and Preprocessing
Data quality is paramount in automated irrigation systems as it directly influences the effectiveness of decision-making and water use efficiency. Issues like missing values, inconsistencies, and outliers arising from sensor malfunctions, environmental interference, or network problems (Lv et al., 2023) can significantly impact the performance of machine learning models used for irrigation scheduling and management.
Real-time data cleaning techniques are essential for addressing these challenges. Kalman filtering proves particularly effective in handling missing values and correcting erroneous readings by recursively estimating the system's state based on previous measurements and current sensor data, taking into account noise and uncertainty (Kim et al., 2020). Moving average techniques, by averaging consecutive data points, provide a more stable representation of the underlying trend, filtering out short-term fluctuations (Chhetri, 2023). For outlier detection, adaptive thresholding methods offer a dynamic approach, adjusting thresholds based on the statistical properties of the data to effectively identify anomalies and minimize false positives (Bah et al., 2021). These techniques are crucial in maintaining the integrity of real-time data streams and ensuring the accuracy of subsequent analyses.
Adaptive data preprocessing is essential for managing the diversity of data sources and formats commonly found in irrigation systems. Data normalization techniques, such as min-max scaling or z-score normalization, ensure that all features contribute equally to the analysis by transforming data values to a common scale (Pradal et al., 2016). This is crucial for preventing features with larger values from dominating the analysis and ensuring that all features are given equal consideration. Similarly, feature scaling methods, like standardization or normalization, optimize the range of feature values to improve the performance and convergence of machine learning models (Tortorici et al., 2024). By scaling features to a similar range, the influence of outliers is reduced, and the model's ability to learn from the data is enhanced.
Data fusion techniques play a critical role in integrating information from diverse sources, creating a more comprehensive and reliable dataset for irrigation management. Dempster-Shafer theory, a generalization of probability theory, allows for the expression of both uncertainty and the degree of conflict in evidence, making it suitable for fusing uncertain and conflicting data from heterogeneous sources (Sadiq and Rodriguez, 2004). This is particularly relevant in irrigation systems where data from different sensors may provide slightly different or even contradictory information due to sensor variations or environmental factors. Bayesian inference offers another powerful framework for combining information from multiple sources, updating the probability of a hypothesis as new evidence becomes available. By applying these techniques, data from soil moisture sensors, canopy temperature sensors, weather stations, and other sources can be integrated to provide a holistic understanding of crop water requirements and environmental conditions, leading to more informed and accurate irrigation decisions.
The impact of data quality extends beyond model accuracy to the robustness of machine learning models under varying conditions. Robust models should maintain consistent performance even when faced with data inconsistencies or unexpected situations. Techniques like data augmentation and domain adaptation can enhance model robustness by exposing the model to a wider range of data variations during training. Data augmentation involves generating additional training data by applying transformations or introducing noise to existing data, making the model more resilient to noise and variations in the real-world data. Domain adaptation techniques aim to adapt a model trained on one domain (e.g., a specific crop or geographic location) to perform well on another domain with different data characteristics. This is particularly relevant in irrigation management, where models may need to be applied to different crops, soil types, or climatic conditions.
The choice of data cleaning, preprocessing, and fusion techniques should be carefully considered based on the specific characteristics of the irrigation system and the available data. By selecting and implementing appropriate techniques, the accuracy, reliability, and robustness of machine learning models can be significantly improved, leading to more efficient and sustainable irrigation management practices.
4.2. Scalable and Autonomous Deployment using Containerization Strategies
The transition from data collection and transmission to efficient data processing requires a robust infrastructure capable of handling diverse workloads and data volumes. Containerization technologies, specifically Docker and Kubernetes, offer a promising solution for deploying and scaling data processing and machine learning modules within cloud environments like AWS, Azure, and GCP (Vargas-Rojas et al., 2024; Rosendo et al., 2022; Solayman & Qasha, 2023). Docker provides a standardized way to package applications and their dependencies into self-contained units known as containers, ensuring consistent and reproducible execution across different platforms (Rosendo et al., 2022). Kubernetes, acting as a container orchestrator, manages their deployment, scaling, and networking across a cluster of machines (Rosendo et al., 2022). This combination presents several advantages for automated irrigation management systems.
Firstly, containerization facilitates efficient resource utilization and scalability. By encapsulating applications and their dependencies, containers enable the isolation of resources and prevent conflicts between different modules (Vargas-Rojas et al., 2024; Solayman & Qasha, 2023). This isolation allows for the efficient allocation of resources, such as CPU, memory, and storage, to each container based on its specific needs. Kubernetes further enhances scalability by allowing for the automatic scaling of containers based on real-time demand, ensuring the system can adapt to varying workloads and data volumes, preventing bottlenecks, and ensuring responsiveness to changing conditions (Karamolegkos et al., 2023).
Secondly, containerization promotes portability and reproducibility. By packaging applications and their dependencies into a single unit, containers make it easy to move and deploy them across different cloud environments without the need for environment-specific configurations (Rosendo et al., 2022; Solayman & Qasha, 2023). This portability simplifies the development and deployment process, reducing the time and effort required to set up and manage the system. Additionally, containers ensure reproducibility by providing a consistent execution environment, regardless of the underlying infrastructure. This eliminates variability and ensures that the system will behave consistently across different deployments (Zhou et al., 2023).
Optimizing container orchestration and resource allocation is crucial to minimizing latency and maximizing throughput in real-time data processing pipelines. Techniques like auto-scaling and dynamic resource allocation play a critical role in this context (Hethcoat et al., 2024; Werner and Tai, 2023; Kumar et al., 2024). Auto-scaling automatically adjusts the number of container instances based on real-time demand, ensuring that sufficient resources are available to handle peak workloads while avoiding over-provisioning during periods of low demand (Hethcoat et al., 2024; Kumar et al., 2024). Dynamic resource allocation enables the fine-grained adjustment of resources allocated to each container based on its specific needs and the current workload (Werner and Tai, 2023). This ensures efficient resource allocation and provides each container with the necessary resources to perform its tasks effectively.
Performance monitoring tools, such as Kubernetes Metrics Server and Prometheus, are essential for gaining insights into the performance of containers and the overall system (Hethcoat et al., 2024; Kuity & Peddoju, 2023). These tools provide valuable data on key performance indicators, such as CPU and memory usage, network traffic, and application-specific metrics. By monitoring this data, administrators can identify bottlenecks, optimize resource allocation strategies, and continuously improve system performance (Hethcoat et al., 2024). This data-driven approach ensures that automated irrigation management systems can operate efficiently and reliably.
By integrating containerization technologies with optimization techniques and performance monitoring, automated irrigation management systems achieve the scalability, autonomy, and efficiency required for effective real-time data processing and decision-making. This approach facilitates a seamless and responsive system that can adapt to changing conditions and contribute to the overall goal of optimizing water resource management and increasing agricultural productivity.
4.3. Deploying ML Models for Data Processing
•	Architectures and frameworks for deploying machine learning models on cloud platforms for real-time data processing and inference in irrigation management systems, such as: TensorFlow Serving, Apache MXNet Model Server, ONNX Runtime
•	Techniques for optimizing machine learning model performance and resource utilization in cloud environments, such as: Model compression (e.g., pruning, quantization), Hardware acceleration (e.g., GPU, TPU), Distributed training (e.g., Horovod, BytePS)
•	Integration of deployed machine learning models with other components of the automated irrigation management pipeline, such as data preprocessing, decision-making, and control systems, using protocols like: MQTT, CoAP, RESTful APIs
4.4. Online Learning in the Cloud
•	Application of online learning techniques for continuously updating and improving machine learning models based on incoming real-time data, using algorithms such as: Stochastic gradient descent (SGD), Passive-aggressive algorithms, Online random forests
•	Architectures and frameworks for implementing online learning in cloud-based irrigation management systems, such as: Apache Spark Streaming, Apache Flink, AWS Kinesis, leveraging serverless computing and stream processing paradigms
•	Strategies for balancing exploration and exploitation in online learning to adapt to changing environmental conditions and optimize irrigation decision-making, using techniques such as: Multi-armed bandits, Bayesian optimization, Reinforcement learning (e.g., Q-learning, SARSA)


5. GENERATING AND APPLYING IRRIGATION INSIGHTS 
5.1. Real-Time Generation of Actionable Irrigation Insights
•	Advanced predictive models, such as deep learning (e.g., LSTM, CNN) and ensemble methods (e.g., Random Forests), for precise, site-specific irrigation recommendations
•	Integration of IoT sensor data (e.g., soil moisture probes, weather stations) and cloud-based data sources (e.g., weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filtering) to enhance insight accuracy and resolution
•	Strategies for handling data heterogeneity, uncertainty, and quality issues in real-time insight generation, such as data preprocessing and outlier detection
•	Techniques for reducing computational complexity and latency, such as edge computing (e.g., fog computing), model compression (e.g., quantization), and hardware accelerators (e.g., GPUs)
5.2. Automated Application of Irrigation Insights
•	Architectures and protocols for seamless integration of ML-generated insights with IoT-enabled irrigation control systems, such as MQTT and CoAP for lightweight, real-time communication
•	Analysis of industry-leading products and services, such as smart irrigation controllers (e.g., Rachio) and cloud-based irrigation management platforms (e.g., CropX)
•	Strategies for ensuring reliability, security, and scalability of automated insight application, such as redundant communication channels and secure edge-to-cloud architectures
•	Case studies of successful implementations of closed-loop, autonomous irrigation systems in research and commercial settings, highlighting technologies used and benefits achieved

6. INTEGRATION, INTEROPERABILITY, AND STANDARDIZATION 
6.1. Interoperability and Standardization
•	Importance of interoperability and standardization in enabling seamless integration of automated irrigation components
•	Overview of existing and emerging standards for IoT devices, communication protocols, and data formats in precision agriculture (e.g., ISOBUS, agroXML, SensorML)
•	Role of standardization bodies and industry consortia in promoting interoperability (e.g., AgGateway, Open Ag Data Alliance, Agricultural Industry Electronics Foundation)
•	Challenges in adopting and implementing standards across diverse hardware and software platforms
•	Strategies for encouraging widespread adoption of standards and best practices for interoperability in automated irrigation systems
6.2. Integration with Existing Irrigation Infrastructure
•	Challenges and strategies for retrofitting legacy irrigation systems with IoT sensors, actuators, and communication devices
•	Hardware compatibility issues and solutions (e.g., adapters, modular designs)
•	Software and firmware updates to enable integration with automated decision-making systems
•	Data integration and normalization techniques for merging legacy and new data sources
•	Economic and practical considerations for transitioning from manual to automated irrigation management
•	Cost-benefit analysis of upgrading existing infrastructure vs. implementing new systems
•	Phased implementation approaches to minimize disruption and optimize resource allocation
•	Training and support requirements for farmers and irrigation managers adopting automated systems
•	Case studies and real-world examples of successful integration of automated irrigation with existing infrastructure
6.3. Integration with Other Precision Agriculture Technologies
•	Synergies between automated irrigation and complementary technologies
•	Remote sensing (satellite, UAV, and ground-based) for crop monitoring and evapotranspiration estimation
•	Soil moisture sensors and weather stations for real-time, localized data collection
•	Variable rate application systems for precise irrigation delivery based on crop requirements
•	Yield mapping and analytics for assessing the impact of automated irrigation on crop productivity
•	Architectures and frameworks for integrating diverse data sources and technologies into a unified precision agriculture ecosystem
•	Edge computing and fog computing paradigms for real-time data processing and decision-making
•	Cloud-based platforms for data storage, analysis, and visualization
•	API-driven approaches for modular integration of third-party services and applications
•	Challenges and solutions for ensuring data quality, consistency, and security across integrated precision agriculture systems
•	Data cleaning, preprocessing, and harmonization techniques
•	Blockchain and distributed ledger technologies for secure, tamper-proof data sharing and traceability
•	Access control and authentication mechanisms for protecting sensitive data and resources
•	Future trends and research directions in the integration of automated irrigation with advanced precision agriculture technologies (e.g., AI-driven crop modeling, robotics, and autonomous vehicles)
6.4. Cybersecurity Considerations for Integrated Automated Irrigation Systems
•	Unique security risks and vulnerabilities associated with IoT-based automated irrigation systems
•	Potential for unauthorized access, data tampering, and system manipulation
•	Implications of security breaches for crop health, water resource management, and farm productivity
•	Best practices and strategies for securing automated irrigation systems
•	Secure device provisioning and authentication (e.g., hardware security modules, certificates)
•	Encryption and secure communication protocols (e.g., TLS, DTLS)
•	Firmware and software updates to address emerging security threats
•	Network segmentation and access control to limit the impact of breaches
•	Role of cybersecurity standards and frameworks in guiding the development and deployment of secure automated irrigation systems (e.g., NIST CSF, IEC 62443)
•	Importance of user awareness, training, and incident response planning in maintaining the security of integrated automated irrigation systems

7. MONITORING AND ENSURING SYSTEM RELIABILITY
7.1. Resilience and Fault Tolerance in Automated Irrigation Systems
•	Strategies for ensuring robustness and reliability in the face of failures, disruptions, or unexpected events
•	Redundancy: Implementing redundant components, such as duplicate sensors (e.g., soil moisture sensors, weather stations), controllers (e.g., PLCs, microcontrollers), and communication channels (e.g., cellular, satellite, LoRaWAN) to maintain system functionality during component failures
•	Failover mechanisms: Designing seamless failover mechanisms that automatically switch to backup components or systems in case of primary system failure, such as hot-standby controllers or multi-path communication protocols (e.g., mesh networks, software-defined networking)
•	Self-healing capabilities: Incorporating AI-driven self-healing mechanisms that can detect, diagnose, and recover from faults without human intervention, using techniques like reinforcement learning, Bayesian networks, or self-organizing maps
•	The role of distributed architectures and edge computing in enhancing system resilience
•	Decentralizing critical functions and data processing to minimize the impact of single points of failure, using fog computing or multi-agent systems
•	Leveraging edge computing to enable localized decision-making and control, reducing dependence on cloud connectivity and improving response times, using technologies like Raspberry Pi, NVIDIA Jetson, or Intel NUC
•	Anomaly detection and predictive maintenance using AI techniques
•	Employing unsupervised learning algorithms (e.g., autoencoders, clustering) to detect anomalies in sensor data, system performance, and water usage patterns
•	Developing predictive maintenance models using techniques like long short-term memory (LSTM) networks, convolutional neural networks (CNNs), or gradient boosting machines (GBMs) to anticipate and prevent potential system failures based on historical data and real-time monitoring
7.2. Advanced Monitoring Techniques for Automated Irrigation Systems
•	Remote monitoring using IoT-enabled sensors and computer vision
•	Deploying a heterogeneous network of IoT sensors to collect real-time data on soil moisture (e.g., capacitive, tensiometric), temperature (e.g., thermocouples, thermistors), humidity (e.g., capacitive, resistive), and plant health (e.g., sap flow, leaf wetness)
•	Integrating high-resolution cameras (e.g., multispectral, hyperspectral) and computer vision algorithms for visual monitoring of crop growth, disease detection (e.g., using deep learning-based object detection and segmentation), and irrigation system performance (e.g., leak detection, sprinkler uniformity)
•	Transmitting sensor and camera data to cloud-based platforms (e.g., AWS IoT, Google Cloud IoT, Microsoft Azure IoT) for remote access and analysis using protocols like MQTT, CoAP, or AMQP
•	Innovative approaches for real-time system health assessment
•	Developing novel algorithms and metrics for evaluating the health and performance of automated irrigation systems, such as entropy-based measures, network resilience indices, or multi-criteria decision analysis (MCDA) frameworks
•	Combining data from multiple sources (e.g., sensors, weather forecasts, satellite imagery) using data fusion techniques (e.g., Kalman filters, Dempster-Shafer theory) to create a comprehensive view of system health
•	Employing advanced data visualization techniques (e.g., interactive dashboards, augmented reality) to present system health information in an intuitive and actionable format
7.3. Closed-Loop Control and Feedback Mechanisms
•	Exploring the concept of closed-loop control in autonomous irrigation systems
•	Implementing feedback loops that continuously monitor system performance and adjust irrigation schedules based on real-time data, using control techniques like proportional-integral-derivative (PID), model predictive control (MPC), or fuzzy logic control (FLC)
•	Integrating machine learning algorithms (e.g., reinforcement learning, genetic algorithms) to optimize closed-loop control strategies over time, adapting to changing environmental conditions and crop requirements
•	Designing effective feedback mechanisms for user interaction and system optimization
•	Providing user-friendly interfaces (e.g., mobile apps, web dashboards) for farmers to input preferences, constraints, and expert knowledge into the automated irrigation system, using techniques like participatory design or user-centered design
•	Incorporating user feedback and domain expertise to refine irrigation strategies and improve system performance
8. CASE STUDIES AND REAL-WORLD IMPLEMENTATIONS OF FULLY AUTONOMOUS IRRIGATION SYSTEMS
8.1. Fully Autonomous Irrigation Systems in Diverse Agricultural Settings
•	Row Crops: maize, wheat, soybean with real-time soil moisture monitoring and weather-based irrigation scheduling for fully automated precision irrigation
•	Orchards: citrus, apple, almond with plant health monitoring and precision water application for fully autonomous orchard management
•	Greenhouses: tomato, lettuce, herbs with automated drip irrigation and climate control integration for fully automated greenhouse operations
•	Urban Farming: rooftop gardens, vertical farms with IoT-enabled hydroponic systems and remote management for fully autonomous urban crop production
8.2. Integration of Advanced System Components for End-to-End Automation
•	Wireless sensor networks: soil moisture probes, weather stations, plant health monitoring cameras with low-power, long-range communication for fully automated data acquisition
•	Secure data transmission: LoRaWAN, NB-IoT, 5G, satellite communication for reliable, real-time data transfer from field to cloud in fully autonomous irrigation systems
•	Intelligent data processing: edge computing for local data filtering, cloud platforms for scalable storage and analysis, machine learning algorithms for predictive insights in fully automated irrigation management
•	Autonomous decision-making: advanced irrigation scheduling algorithms, precise valve control, closed-loop feedback systems for optimal water management in fully autonomous irrigation systems
8.3. Quantitative Performance Evaluation of Fully Automated Irrigation Systems
•	Water use efficiency: percent reduction in water consumption compared to conventional methods, improved water productivity (yield per unit of water) achieved through fully autonomous irrigation
•	Crop yield and quality improvements: percent increase in yield, enhanced crop uniformity, improved nutritional content attributed to fully automated precision irrigation
•	Labor and energy savings: quantified reduction in labor hours for irrigation management, decreased energy consumption for pumping due to optimized scheduling in fully autonomous systems
•	Economic viability: detailed return on investment analysis, payback period calculations, comprehensive cost-benefit analysis for fully autonomous irrigation management systems
8.4. Lessons Learned and Challenges Encountered in Deploying Autonomous Irrigation Systems
•	Technical challenges and solutions: ensuring reliable data transmission in remote locations, addressing interoperability issues between diverse system components, optimizing power consumption for extended battery life, adapting algorithms to local soil and weather conditions in fully autonomous irrigation systems
•	Operational and logistical hurdles: streamlining installation and maintenance procedures, providing effective user training, seamlessly integrating with existing farm management practices and legacy systems for fully automated irrigation management
•	Regulatory and socio-economic considerations: navigating complex water use regulations, addressing data privacy and security concerns, ensuring equitable access and affordability for smallholder farmers adopting fully autonomous irrigation technologies
8.5. Best Practices and Recommendations for Successful Implementation
•	Designing scalable, modular, and adaptable autonomous irrigation systems to accommodate future growth and changing requirements for fully automated water management
•	Prioritizing user-centered design principles and actively engaging stakeholders throughout the development and deployment process of fully autonomous irrigation solutions
•	Adopting open standards and communication protocols to enable seamless integration of system components and interoperability with third-party platforms in fully automated irrigation setups
•	Implementing robust data validation, filtering, and quality control mechanisms to ensure data integrity and reliability for decision-making in fully autonomous irrigation systems
•	Establishing clear data governance policies and security frameworks to protect sensitive information and maintain user trust in fully automated irrigation management
•	Developing intuitive user interfaces and decision support tools to facilitate easy adoption and effective use of fully autonomous irrigation systems
•	Collaborating with local extension services, agribusinesses, and technology providers for knowledge transfer, technical support, and continuous improvement of fully automated irrigation solutions
8.6. Synthesis of Case Studies and Implications for Autonomous Irrigation Adoption
•	Cross-case analysis of key performance indicators and critical success factors for fully autonomous irrigation scheduling systems in various contexts
•	Identification of common themes, challenges, and innovative solutions across diverse implementations of end-to-end fully automated irrigation management
•	Assessment of the potential for replicability and scaling of successful fully autonomous irrigation projects in different regions and farming systems
•	Implications for future research priorities, technology development roadmaps, and policy interventions to support widespread adoption of fully autonomous irrigation technologies

CONCLUSION/FUTURE DIRECTIONS AND UNANSWERED QUESTIONS
•	Summarize the key insights gained from the question-driven review, emphasizing how each section contributes to the overarching goal of achieving real-time, end-to-end automation in irrigation management
•	Based on the questions addressed, propose new research directions and unanswered questions
•	Identify key research gaps and propose concrete research questions and hypotheses for advancing the field of real-time, automated irrigation management
•	Highlight the need for collaborative research efforts across disciplines, such as computer science, agricultural engineering, and environmental science, to address the complex challenges of automated irrigation systems
•	Emphasize the need for further innovation and exploration in real-time, automated irrigation systems



</previous_sections>

</documents>
<instructions>


Use the information provided in the <documents> tags to write the next subsection of the research paper, following these steps:
1. Review the overall intention of the research paper, specified in the <review_intention> tag. Ensure the subsection you write aligns with and contributes to this overall goal.
2. Consider the specific intention for this subsection of the paper, stated in the <section_intention> tag. The content you write should fulfill this purpose. 
3. Use the title provided in the <subsection_title> tag as the heading for the subsection. 
4. Address each of the points specified in the </subsection_point_Point *> tags:
   a) Make a clear case for each point using the text provided in the "point" field.
   b) Support each point with evidence from the research papers listed in the corresponding "papers to support point" field.
   c) When citing a paper to support a point, include inline citations with the author name(s) and year, e.g. (Smith et al., 2020; Johnson and Lee, 2019; Brown, 2018). Cite all papers that strengthen or relate to the point being made.
   d) While making a point and citing the supporting papers, provide a brief explanation in your own words of how the cited papers support the point.
5. Ensure that both of the points from the <subsection_point> tags are fully addressed and supported by citations. Do not skip or combine any points.
6. After addressing the specified points, wrap up the subsection with a concluding sentence or two that ties the points together and relates them back to the <section_intention>.
7. Review the <Previous_sections> of the paper, and ensure that the new subsection you have written fits logically and coherently with the existing content. Add transition sentences as needed to improve the flow.
8. Proofread the subsection to ensure it is clear, concise, and free of grammatical and spelling errors. Maintain a formal academic tone and style consistent with the rest of the research paper.
9. Format the subsection using Markdown, including the subsection heading (using ## or the equivalent for the document), inline citations, and any other formatting needed for clarity and readability.
10. If any information is missing or unclear in the provided tags, simply do your best to write the subsection based on the available information. Do not add any information or make any points not supported by the provided content. Prioritize fully addressing the required points over hitting a specific word count.

The output should be a complete, well-organized, and properly cited subsection ready to be added to the research paper.

Begin your answer with a brief recap of the instructions stating what you will to optimize the quality of the answer. Clearly and briefly state the subsection you'll be working on and the points you'll be addressing. Then proceed to write the subsection following the instructions provided. 

Critical: 
- Do not include a conclusion or summary as the entry is in the middle of the document. Focus on addressing the points and supporting them with evidence from the provided papers. Ensure that the subsection is well-structured, coherent, and effectively contributes to the overall research paper.
- The subsection we are focusing on is: 4.4. Online Learning in the Cloud
- No need for sub-sub-sections. just provide paragraphs addressing each point. They should transition fluidly and narurally into each other.
- Ensure that the content is supported by the provided papers and that the citations are correctly formatted and placed within the text.
- Do not repeat content from the previous sections. Ensure that the information provided is new and relevant to the subsection being written.



</instructions>

