- analysis: '>'
  authors:
  - Schintke F.
  - Belhajjame K.
  - De Mecquenem N.
  - Frantz D.
  - Guarino V.E.
  - Hilbrich M.
  - Lehmann F.
  - Missier P.
  - Sattler R.
  - Sparka J.A.
  - Speckhard D.T.
  - Stolte H.
  - Vu A.D.
  - Leser U.
  citation_count: '0'
  description: Porting a scientific data analysis workflow (DAW) to a cluster infrastructure,
    a new software stack, or even only a new dataset with some notably different properties
    is often challenging. Despite the structured definition of the steps (tasks) and
    their interdependencies during a complex data analysis in the DAW specification,
    relevant assumptions may remain unspecified and implicit. Such hidden assumptions
    often lead to crashing tasks without a reasonable error message, poor performance
    in general, non-terminating executions, or silent wrong results of the DAW, to
    name only a few possible consequences. Searching for the causes of such errors
    and drawbacks in a distributed compute cluster managed by a complex infrastructure
    stack, where DAWs for large datasets typically are executed, can be tedious and
    time-consuming. We propose validity constraints (VCs) as a new concept for DAW
    languages to alleviate this situation. A VC is a constraint specifying logical
    conditions that must be fulfilled at certain times for DAW executions to be valid.
    When defined together with a DAW, VCs help to improve the portability, adaptability,
    and reusability of DAWs by making implicit assumptions explicit. Once specified,
    VCs can be controlled automatically by the DAW infrastructure, and violations
    can lead to meaningful error messages and graceful behavior (e.g., termination
    or invocation of repair mechanisms). We provide a broad list of possible VCs,
    classify them along multiple dimensions, and compare them to similar concepts
    one can find in related fields. We also provide a proof-of-concept implementation
    for the workflow system Nextflow.
  doi: 10.1016/j.future.2024.03.037
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. User stories 3. Fundamentals
    4. A formal definition of VCs for DAWs 5. Concrete validity constraints for DAWs
    6. Related work 7. Implementing VCs as contracts in Nextflow 8. Validity constraints
    specification approach 9. Conclusions CRediT authorship contribution statement
    Declaration of competing interest Acknowledgments Data availability References
    Vitae Show full outline Figures (6) Tables (4) Table Table Table 1 Table 2 Future
    Generation Computer Systems Volume 157, August 2024, Pages 82-97 Validity constraints
    for data analysis workflows Author links open overlay panel Florian Schintke a,
    Khalid Belhajjame b, Ninon De Mecquenem c, David Frantz d, Vanessa Emanuela Guarino
    c e, Marcus Hilbrich c, Fabian Lehmann c, Paolo Missier f, Rebecca Sattler c,
    Jan Arne Sparka c, Daniel T. Speckhard c g, Hermann Stolte c, Anh Duc Vu c, Ulf
    Leser c Show more Share Cite https://doi.org/10.1016/j.future.2024.03.037 Get
    rights and content Under a Creative Commons license open access Highlights • Portability
    and adaptability of scientific workflows suffer from hidden assumptions. • Validity
    constraints in workflow languages make hidden assumption explicit. • Validity
    constraints ensure integrity and enable conformance checking of scientific workflows.
    • Identify and discuss different classes and properties of validity constraints.
    Abstract Porting a scientific data analysis workflow (DAW) to a cluster infrastructure,
    a new software stack, or even only a new dataset with some notably different properties
    is often challenging. Despite the structured definition of the steps (tasks) and
    their interdependencies during a complex data analysis in the DAW specification,
    relevant assumptions may remain unspecified and implicit. Such hidden assumptions
    often lead to crashing tasks without a reasonable error message, poor performance
    in general, non-terminating executions, or silent wrong results of the DAW, to
    name only a few possible consequences. Searching for the causes of such errors
    and drawbacks in a distributed compute cluster managed by a complex infrastructure
    stack, where DAWs for large datasets typically are executed, can be tedious and
    time-consuming. We propose validity constraints (VCs) as a new concept for DAW
    languages to alleviate this situation. A VC is a constraint specifying logical
    conditions that must be fulfilled at certain times for DAW executions to be valid.
    When defined together with a DAW, VCs help to improve the portability, adaptability,
    and reusability of DAWs by making implicit assumptions explicit. Once specified,
    VCs can be controlled automatically by the DAW infrastructure, and violations
    can lead to meaningful error messages and graceful behavior (e.g., termination
    or invocation of repair mechanisms). We provide a broad list of possible VCs,
    classify them along multiple dimensions, and compare them to similar concepts
    one can find in related fields. We also provide a proof-of-concept implementation
    for the workflow system Nextflow. Previous article in issue Next article in issue
    Keywords Scientific workflow systemsWorkflow specification languagesValidity constraintsDependabilityIntegrity
    and conformance checking 1. Introduction Data analysis workflows (DAWs, or scientific
    workflows) are structured descriptions for scientific datasets’ scientific analysis
    [1], [2]. DAWs’ usage becomes increasingly popular in all scientific domains as
    datasets grow in size, analyses grow in complexity, and demands grow in terms
    of speed of development, the throughput of analyses, reusability by others, and
    reproducibility of results [3], [4], [5]. A DAW essentially is a program consisting
    of individual tasks (programs themselves) with their specific inputs and outputs
    and a specification of the dependencies between tasks. Executing a DAW means scheduling
    its tasks on the available computational infrastructure in an order compatible
    with the data dependencies under some optimization constraints, such as minimal
    time-to-finish [6], [7], [8]. When DAWs are applied for the analysis of large
    datasets and are executed on clusters of distributed compute nodes, managing their
    execution also involves resource management, coordination of distributed computations,
    and file handling [9]. Such distributed executions typically rely on the availability
    of an infrastructure stack consisting of several components such as (distributed)
    file systems, resource managers, container managers, and runtime monitoring tools
    [10]. The interfaces and functionality of these components are not standardized
    and vary substantially between different systems [10]. Therefore, DAW developers
    often optimize their code to the used infrastructure (e.g., to the number and
    memory sizes of available compute nodes) and to the particular datasets they wish
    to analyze (e.g., by hard-coding the number of data partitions for concurrent
    execution). Furthermore, many tasks of typical DAWs have been written by third
    parties, providing their specific functionality in a highly optimized manner,
    while others provide merely ‘glue’ code for data transformation and filtering
    between original tasks [11], [12]. As a result, real-life DAWs are rather brittle
    artifacts. They are tightly bound to the infrastructure used during development,
    suffer from intricacies of the programs they embrace, and only work flawless for
    a narrow range of inputs. Changes in any of these aspects that violate hard-coded,
    undocumented design choices quickly lead to unforeseen situations such as: unnecessarily
    slow DAW executions, underutilized resources, sudden runtime errors, straggler
    processes, meaningless log entries, non-terminating executions, overflows of buffers
    (memory, disk, log-space), etc. or, in the worst case, undetected faulty results
    [13]. The execution often stops with an arbitrary, undocumented, low-level error
    (‘file not found’, ‘core dumped’, ‘timeout’); even meaningful error messages are
    often difficult to trace back to the broken task as execution happens on multiple
    nodes and logs are distributed and created at different levels, ranging from OS
    to resource managers, workflow engine, and task implementations. While some of
    these problems also occur in other software-related situations, they are aggravated
    in DAWs due to their heavy reliance on external programs, generally very high
    resource requirements, long run times, and the complexity of coordinating distributed
    executions. Accordingly, reusing a DAW on another infrastructure or for input
    data with differing properties often requires time-consuming adaptations [14],
    [15]. In this work, we propose validity constraints (VCs) as a new primitive for
    DAW languages that help to improve this situation. A VC is a constraint that specifies
    a logical condition for a particular state or component of a DAW execution. When
    a VC evaluates to false (i.e., if the VC is ‘broken’), the DAW engine can issue
    a defined error message at a defined place. VCs may, for instance, control properties
    of the input and intermediate data files (e.g., minimal or maximal file sizes),
    of the runtime environment (e.g., minimal available memory or threads), or of
    the individual task executions (e.g., maximal execution time). We propose to specify
    VCs within the DAW specification, i.e., as first-class objects of the DAW program
    itself. Note that similar ideas have proven extremely useful in other fields (e.g.,
    integrity constraints in databases or contracts in software engineering), but
    an adaptation to the specific field of workflows is lacking. In the following,
    we motivate the idea of VCs for workflows based on a few exemplary user stories
    from different scientific domains (Section 2) and then first introduce a model
    for DAWs (Section 3) and then use this model to formally define general validity
    constraints (Section 4). We present a broad list of different concrete types of
    VCs (Section 5) and classify these along multiple dimensions, namely the time
    points when they need checking, the objects they affect, the actions they may
    trigger, and the infrastructure component that should handle them (Section 5.1).
    We relate VCs to similar concepts in other fields (Section 6.1), current workflow
    systems (Section 6.2) and discuss more general works of the scientific workflow
    community (Section 6.3). Furthermore, we sketch a prototypical implementation
    of explicit VCs in the state-of-the-art workflow engine Nextflow (Section 7).
    Throughout this work, we focus on simple DAWs performing batch processing and
    leave an extension to data analysis over streams (e.g., [16]) or to DAWs including
    cycles or conditionals for future work. 2. User stories We collected a small set
    of typical problems users from different application domains ran into when using
    and porting DAWs to another platform. Often, they stumbled over and had to solve
    validity constraints that were implicit and not explicit. 2.1. Bioinformatics
    In bioinformatics research, we often modify or rewrite workflows, which requires
    developing short workflows performing RNAseq data treatment. We have to check
    the overall results for their quality and reasonability, but we are also interested
    in the effects our modifications may cause on different infrastructures performance-wise.
    Workflow development—Empty and faulty files. During the development phase, errors
    can occur, and faulty files may be written. Such a situation does not necessarily
    interrupt the workflow directly because output files may exist. Workflow engines
    typically do not assess a task’s success by using the content of the output files.
    Identifying the wrongly behaving task in a distributed execution environment can
    become tedious and time-consuming for the user. For example, we once wanted to
    sort a file in the middle of a workflow but made a syntax error, which caused
    an empty output file. In this case, it would have been wonderful if we had a language
    with validity constraints that would help develop workflows and throw an error
    when an output file is empty, under a certain size, or does not contain specific
    characters. For such checks, the addition of monitoring tasks is necessary. Porting
    workflows to new infrastructures. We recently studied the impact of applying map-reduce
    on specific bioinformatics tools. While porting a workflow on a heterogeneous
    distributed infrastructure, we observed a severely reduced workflow runtime. It
    turned out that only a few nodes could run tasks that should all run in parallel.
    We found the memory of most of the nodes too small to run these tasks. As a result,
    we changed the biological model to one with smaller input references and recomputed
    several experiments. Fortunately, that was possible in this case. But it may not
    be an option for biologists studying a specific specie, for example. If their
    reference genome file is too big for the memory of the nodes of a cluster, they
    would need to set up their workflow on another infrastructure. As such experiments
    can take a long time (up to 40 h for treating only one sample), a way to know
    beforehand that the workflow cannot run (with the full degree of parallelism)
    on this infrastructure can save a lot of time and shared resources. Instead of
    executing a workflow by checking tasks’ resource demands only late, when the execution
    reaches them, a basic overall resource check to stop the workflow from the beginning
    (before it arrives at the task that breaks the workflow) would be preferable.
    “It would be wonderful if I had a language with validity constraints that would
    help me develop workflows or port them to a new infrastructure. I would have needed
    constraints that throw an error when an output file is empty, is under a certain
    size or does not contain specific characters such as those contained in a specific
    header. Additionally, some constraints that would stop the workflow from the beginning,
    before it arrives at the task that requires too much resources, would be very
    helpful”. – Ninon De Mecquenem The derived user requirements for validity constraints
    are: file must exist (R1); file is not empty or has a minimum size (R2); file
    has to contain (only) certain characters (R3); global pre-check for resource demands
    (R4). 2.2. Materials science The novel materials database laboratory (NOMAD) is
    a database that hosts hundreds of millions of material science simulation results,
    specifically density functional theory (DFT) simulations [17]. These results can
    be expensive to generate, sometimes taking several hundred GPU hours to compute
    [18]. As such, the community realized it is vital to share the results to avoid
    recomputation and to allow for the creation of large datasets for machine learning
    and data analysis applications [19], [20]. Users can upload data (input/output
    files) from different density functional theory simulation programs (e.g., VASP,
    exciting  FHI-aims) via the terminal or their browser [21], [22], [23]. The workflow
    to process uploaded data has a clear need for validity constraints. Each simulation
    program has an associated parser to parse the simulation input/output files that
    the user uploads [24], [25]. A common pain point is that these simulation programs
    get updated frequently, and the format of the output files change or new data
    fields are added. The NOMAD developers implemented VCs in the upload DAW to check
    for the existence of specific file names, extension types, and some properties
    of the input/output files that the parser expects [21]. For the DFT code exciting  for
    example, we expect files named INFO.out and input.xml files and particular key–value
    pairs, such as the ‘total energy’ key and its associated floating point value.
    This means our DAW first runs a validity constraint at the setup of the upload
    process to check that the required files exist. If these simulation files do not
    exist the DAW returns an error message to the user that the upload failed since
    no parsable files were found. The upload process also triggers a resource availability
    validity constraint that checks whether the user has used up the amount of storage
    space allocated to every user of NOMAD. If, however, the uploaded files satisfy
    the conditions above, the DAW then runs a resource validity constraint check.
    The parsing process can take considerable resources depending on the simulation
    settings and the DAW only executes the parsing once the container orchestration
    system, in this case Kubernetes, can allocate sufficient computing resources on
    the server. After parsing raw values from the uploaded files, a routine called
    the ‘normalizer’ is applied, which converts parsed values and simulation settings
    to standard units and standardized terms. For instance, two simulation programs
    might use different words for the same input parameter and NOMAD needs to standardize
    this (e.g., two different names for the same DFT functional). The normalizer also
    implements validity constraints on these parsed values to ensure they are reasonable.
    For instance, the normalizer checks that a parsed categorical property belongs
    to a list of expected values or that a floating point value is within a reasonable
    range (e.g., the band gap of the material must be non-negative). A visualization
    for the DAW for NOMAD can be seen in Fig. 1. When we have too many atoms in a
    unit cell of an input geometry, we observe another weakness of the DAW regarding
    the crystal structure classification, which is performed in the normalizer step.
    Large input files with many atoms in the unit cell are common in studies that
    investigate the effect of impurities on the electronic structure of crystalline
    materials [26]. Such a situation causes the crystal structure classifier to take
    a very large amount of computational resources. Currently, we use a timeout validity
    constraint that stops the classification if the classification takes too long.
    What might help, however, is to implement a validity constraint that decides whether
    to skip the crystal structure classification for unit cells where the number of
    atoms exceeds a certain threshold. This could avoid wasting resources on trying
    to classify systems that are very likely to trigger the timeout validity constraint
    during classification. Such a threshold could be determined using a logistic regression
    model trained on previous uploads and workflow executions. Alternatively, the
    threshold could be dependent on the resources available for computation. In this
    case, we envisage VCs that are chained together. Meaning, first we check if resources
    are somewhat limited and if so, we call a VC that checks if the number of atoms
    in the unit cell is too large. If this is true, it avoids the crystal structure
    classification all together. Download : Download high-res image (102KB) Download
    : Download full-size image Fig. 1. Overview of the NOMAD upload workflow. Users
    upload data from a specific DFT simulation code that is then parsed and normalized
    to make results from different DFT codes comparable. A further valuable addition
    to the NOMAD upload DAW would be a check for a metamorphic relation between input
    settings of the simulation and output results of the simulation to predict data
    quality [24]. Simulations uploaded to NOMAD usually come from different applications.
    They could be, for instance, super-high-precision ground state calculations (e.g.,
    using very large basis set sizes) or simulations to find heat and transport properties
    using cheap calculations (small basis set size), resulting in data of varying
    precision [27]. Current research has been on using machine learning models to
    add prediction intervals that act as error bars to the data results parsed by
    NOMAD [28]. Such annotations would help users better understand how precise and,
    therefore, how qualified results from NOMAD are for a particular use-case. For
    example, a formation energy calculated with high-precision settings would have
    small prediction intervals. It shows the NOMAD end user (e.g., an experimentalist)
    that this result does not need a recalculation. Many VCs are already integrated
    into the NOMAD DAW but we would like to add more as discussed above. A language
    for clearly presenting the VCs of the DAW would be useful, especially in a graphical
    form in a visual format. We believe this would help end-users and developers better
    understand the DAW especially when the upload process fails due to one of these
    constraints. Overall, we observe the demand for the following VCs from this user
    story: file must exist (R1); file with certain file extension must exist (R5);
    file content should fulfill certain properties (e.g., contain particular key–value
    pairs) (R6); disk quota check (R7); check of resource demands in advance (R8);
    reasonability checks on output data (e.g., value in certain range, positive, non-negative,
    negative, or from list of given values) (R9); tasks finish within a given timeout
    (R10); let workflows decide how to proceed based on the result of VCs (e.g., VC-dependent
    tasks and chained VCs) (R11); data quality checks with metamorphic relations (R12).
    2.3. Earth observation In the field of Earth Observation (EO), we typically work
    with large volumes of satellite images, often over large areas (from federal states
    to continents, sometimes even global) and across long time series (up to 40 years).
    Our workflows often consist of a preprocessing step to convert the ‘raw’ data
    into a more analysis-ready form. This preparation typically includes the detection
    of clouds and their shadows, eliminating atmospheric and other radiometric distortions,
    and often rearranging the data into so-called data cubes for improved data management
    and efficiency. This processing step usually runs on each individual satellite
    image separately, allowing image-based parallelism. Subsequently, workflows generally
    use map/reduce operations on spatial partitions of the data cube, or even sub-partitions
    that need specific parts of the images. Typically, all available data for some
    time period is reduced (averaged in the simplest case) to generate gap-free predictive
    features. The feature vectors at the locations of reference data are extracted,
    a machine learning model trained with some response variable (e.g., land cover
    labels or tree height values), and the model applied to the feature images to
    generate a wall-to-wall prediction of the respective response variable (i.e.,
    generating a map). A validation procedure typically follows. Different IT resources,
    such as CPUs, RAM, and I/O bandwidth, typically constrain various components of
    this generic workflow. Depending on the particular workflow, the analyzed data,
    user parameterization, as well as characteristics of the hardware, the limiting
    factor might be different each time. For example, a workflow that efficiently
    runs with Landsat data might fail when switching to Sentinel-2 data (more RAM
    needed) even when executed with the same parameterization on the same system.
    Another example would be a workflow that efficiently reads data but would quickly
    become input-limited when switching from an SSD- to an HDD-based platform, or
    another RAID configuration. In a noteworthy instance, we encountered an extreme
    worst-case scenario in which processes would sporadically become defunct while
    unzipping files from tape storage. As a result, our job would come to a complete
    halt after a certain period. Resolving this issue required a specific modification
    of our workflow, including the addition of a hard-coded timeout. Moreover, we
    had to first copy data from tape to warm storage before executing the workflow
    again. Consequently, a one-fits-all default parameterization is usually not feasible,
    and many user parameters may exist that can tweak the behavior of the workflow.
    For example, the FORCE software [29] includes parameters to fine-tune partition
    sizes, reduce the number of parallel processes, or to increase the multithreading
    to multiprocessing ratio when RAM becomes an issue. However, achieving optimal
    parameterization needs a deep understanding of the workflow, the underlying data,
    and their effects on system resources. Additionally, a solid understanding of
    the platform is essential for effective parameterization. Therefore, the presence
    of validity constraints capable of identifying common patterns of excessive resource
    usage, such as idle CPUs or high network latency in I/O-limited scenarios, or
    memory swapping leading to generic ‘killed’ messages in RAM-limited situations,
    would significantly aid in transferring workflows from one system to another.
    Additionally, it would facilitate a smoother onboarding process for new users,
    reducing the learning curve required. From this user story, we mainly derive monitoring
    validity constraints: warn on low CPU usage (R13); warn on high network latency
    (R14); warn on memory swapping (R15). 3. Fundamentals In this section, we formally
    define a DAW and its execution steps, sketch an abstract infrastructure for executing
    DAWs in a distributed system, and introduce scheduling as the process of executing
    a DAW on an infrastructure. Based on these models, we then define two types of
    general validity constraints (static and dynamic) as new first-class primitives
    for DAW specification languages, and use them to derive the concept of valid and
    correct DAW execution. Our DAW semantic is simple by intention; its purpose is
    to lay the grounds for the following sections, which will precisely define the
    connection between elements of a DAW and VCs and the impact that VCs may have
    on DAW execution. Conceptually, our semantics is similar to Petri-Nets [30] and
    dataflow languages [31]. Elaborated semantics of real workflow systems have been
    described elsewhere (e.g., [32], [33]); [34] gives a nice overview of different
    formal models in distributed computation. 3.1. A formal model of DAWs We define
    a logical DAW (see below for the distinction to physical DAWs) as follows. Definition
    3.1 Logical DAW A logical DAW is a directed acyclic graph (1) where is the set
    of tasks, is the set of dependencies between pairs of tasks, is a set of labels,
    is a function assigning labels to dependencies, is the start task, and is the
    end task. Intuitively, tasks are the programs to be executed for performing individual
    analysis steps, while dependencies model the data flow between tasks. The dependencies’
    label is an abstract representation of the specific data that is exchanged between
    two tasks. The start task does not depend on any other task and initiates the
    first steps of the analysis by sending the DAW’s input data to its dependent tasks.
    Similarly, the end task has no dependent tasks, and the labels of its incoming
    dependencies represent the results of the DAW. Note that having single start and
    end tasks does not restrict the model significantly as such tasks can easily be
    introduced as auxiliary, empty additional tasks in front of a group of initial
    tasks or behind a group of finishing tasks to join them to a single end task.
    Fig. 2 (upper part) shows a graphical representation of an example DAW consisting
    of six tasks plus start and end tasks; arcs represent dependencies. DAWs are executed
    by running their tasks in an order in which at all times all dependencies are
    satisfied. To formally define this semantics, we introduce the notation of the
    state of a DAW and, later, that of valid states. Definition 3.2 State of a DAW
    The state of a DAW is a function that assigns each task in the set to one of three
    possible states: (2) Here, means ‘executing’, means ‘finished’, means ‘open’,
    means ‘ready’. Definition 3.3 Valid States The state of a DAW is valid, iff the
    following conditions hold: (a) , (b) with : if , then , (c) : If with : , then
    , and (d) for all other . The initial state of a DAW is the state in which (1)
    the start task is finished: , (2) all tasks depending on are ready: , and (3)
    all other tasks have state ‘open’. Intuitively, these rules guarantee that: (a)
    the start task is always in the ‘finished’ state ; (b) a task is ‘ready’ ( ) only
    when all its predecessors ( with ) are ‘finished’ ( ); (c) any task with all its
    predecessors ‘finished’ ( ) has state ‘ready’ ( ), ‘executing’ ( ), or ‘finished’
    ( 3.2. DAW infrastructure and execution semantics Based on a DAW’s state, we next
    define the semantics of a DAW execution. Definition 3.4 Execution of a DAW An
    execution of DAW is a sequence of states such that (a) is the DAW’s initial state,
    (b) all , are valid, and (c) for all steps with , it holds that • If , then •
    If , then • If , then • If , then • There exists at least one where . We say that
    an execution of a DAW has executed when . Intuitively, the execution of a DAW
    progresses by iteratively executing tasks that are ready to run. During execution,
    they are in ; after execution, their state switches to ; tasks in state must first
    proceed to state before they can be executed. We make no assumptions regarding
    the order in which tasks that are ready to run at the same state are executed,
    nor do we assume only one task executes per execution step. But we do require
    at least one task to change its state between successive DAW states. Note that
    this change may be purely logical, by switching some task’s state from to . Download
    : Download high-res image (386KB) Download : Download full-size image Fig. 2.
    A logical (upper part) DAW and its physical counterpart (lower part). Logical
    DAWs are abstract objects. However, in real life, a DAW execution requires the
    start of programs representing workflow tasks on particular nodes of the available
    cluster and the management of the inputs and outputs of these programs. Fig. 3
    depicts a light architecture of the components involved in such a DAW execution.
    It encompasses the DAW specification in a proper DAW language and its compiler
    (comp.), the DAW engine steering the DAW execution (EE), a scheduler performing
    the task-to-node assignments (S), a resource manager and monitoring system controlling
    the resource assignment and task execution at the global and local level (RM,
    M), the individual nodes for executing tasks, and a distributed file system for
    data exchange between tasks (DFS).1 Clearly, many other architectures are possible,
    but for the sake of this work, such an idealized architecture suffices and allows
    later determining the responsible component to control a particular type of VC.
    With such an architecture in mind, we can now define a physical DAW. Download
    : Download high-res image (195KB) Download : Download full-size image Fig. 3.
    A simple DAW infrastructure architecture. Definition 3.5 Physical DAW Given a
    logical DAW and a set of compute nodes interconnected by a network, the physical
    DAW augments with a function that maps every task to a compute node. is an abstract
    representation of a compute cluster, whereas is an assignment (schedule) that
    maps tasks to nodes; in practice, this assignment is determined by the scheduler
    when a task’s state changes from to . The definition of a DAW execution can be
    naturally extended from logical DAWs to physical DAWs. Fig. 2 shows an example
    of the transition of a logical DAW to a physical DAW. In the physical DAW, each
    logical task is assigned to a node for execution, and each dependency is implemented
    as communication between nodes. 4. A formal definition of VCs for DAWs Having
    introduced logical and physical DAWs and their execution semantics, we can now
    define validity constraints (VCs) as logical formulas over the components of a
    DAW infrastructure and of a DAW execution, i.e., tasks, dependencies, executions,
    and schedules. Different VCs will address various properties of these components
    (see Section 5). Definition 4.1 Properties Let be a DAW and be a cluster, i.e.,
    a set of compute nodes. We model arbitrary properties of elements of , , and as
    property functions: is a function that assigns properties to tasks from , is a
    function that assigns properties to labels of dependencies from , and is a function
    that assigns properties to nodes from . We make no assumptions on the specific
    nature of such properties, such as data type or number of parameters they take.
    In Section 5, we will give a diverse list of concrete (static or dynamic) properties
    that we consider particularly useful for DAW management. 4.1. Validity constraints
    We discern two types of validity constraints: Static VCs address static properties,
    i.e., properties which always have the same value for a given property of a task/node/label,
    while dynamic VCs address dynamic properties, i.e., properties whose value may
    change during a DAW execution. Definition 4.2 Static VC Let be a DAW and be a
    cluster. Let , , and be their respective property functions. A static validity
    constraint is a Boolean formula whose atoms have any of the following forms (with
    being an arbitrary constant and and being a given particular property to compare
    with): for any task from , for the label of any dependency from , and for any
    node from , We call VCs of these three forms static because they are independent
    of a DAW’s execution. Intuitively, this implies that they must evaluate to the
    same value all the time before, during, and after a workflow’s execution. An example
    of a static VC would be the minimum size of main memory that must be available
    on a node on which a given task is about to be scheduled, or the availability
    of at least one node with a given minimum memory size within the cluster. The
    definition also captures conjunctions or disjunctions of atomic constraints; the
    order in which the individual atoms are checked in an implementation is not determined
    and leaves room for optimizations [35]. The second class of VCs are dynamic VCs,
    which constrain properties of tasks, dependencies, or nodes that may change during
    a DAW’s execution. For instance, executing a particular task in the middle of
    a DAW may require the existence (or ‘minimal size’ or ‘a certain format’) of a
    file that is created by previous steps in the very same DAW execution. Introducing
    such dynamic VCs requires first defining the scope of a step within an execution.
    Definition 4.3 Scope of Execution Steps Let be a DAW, an execution of , and a
    schedule for over a cluster . Let , , and be their respective property functions.
    Furthermore, for a step from , let be the set of tasks of that are executed in
    this step, i.e., whose state changes from to or from to ; let be the set of dependencies
    from which tasks in depend (incoming edges of all tasks executed in this step,
    each representing an input for a task); let be the set of dependencies outgoing
    from tasks in (outgoing edges, each representing an output of a task); and let
    be the node on which schedules task for execution. We call the tuple the scope
    of step . Definition 4.4 Dynamic VC A dynamic validity constraint over the scope
    of a step of a valid execution of a DAW is a Boolean formula whose atoms have
    of any of the following forms (with being an arbitrary constant, being a given
    particular property to compare with, and ): for a property of any task , for a
    property of node , for a property of a label with , and for a property of a label
    with . 4.2. Correct DAWs and correct DAW executions We defined VCs as logical
    constraints on the components (static) or steps (dynamic) of a DAW that evaluate
    to either true or false. However, we so far did not describe what consequences
    the evaluation of a such a constraint should have. Intuitively, VCs are intended
    as sensors, where an evaluation to ‘true’ implies that no issue was detected,
    while an evaluation to ‘false’ points to a concrete problem. To formally define
    this intuition, we next introduce the notion of correct DAW setups, where a DAW
    setup is a combination of a concrete DAW and a concrete cluster on which it should
    be executed, and correct DAW executions. Recall that we discerned static VCs,
    whose evaluation always returns the same result for a given combination of DAW
    and cluster, from dynamic VCs, which are defined over executions of DAWs. Definition
    4.5 Correct DAW Setup Let be a DAW, a cluster, i.e., a set of interconnected compute
    nodes, and a set of static VCs over , , and . We say that the tuple is correct
    with respect to when all constraints in evaluate to true. We will omit when it
    is clear from the context and simply say that is correct for . Definition 4.6
    Correct DAW Execution Let be a DAW, an execution of , a schedule for over a cluster
    , be a set of static VCs over and , and be a set of dynamic VCs over , and . We
    say that is correct if and only if: • is a correct setup for , and • All evaluate
    to true in all steps . Accordingly, an execution is not correct whenever either
    one of the static constraints is hurt or one of the dynamic VCs in at least one
    step of the execution. We call a step for which all hold a correct step; all other
    steps are called erroneous. Naturally, the first erroneous step is of particular
    importance, as usually (but not necessarily) DAW execution will stop at this point.
    Note that within a step of the execution, a given task’s state may change from
    to , from to , or from to (or may not change at all). This means that a dynamic
    constraint may affect (1) the start of a task (from to ), which corresponds to
    the definition of task pre-conditions; (2) the termination of a task (from to
    ), which corresponds to the definition of task post-conditions; or (3) a property
    of a task while it is executed (within state ), which corresponds to the definition
    of task runtime-conditions. This distinction has consequences for a practical
    implementation, because the latter case (3) must be achieved through continuous
    monitoring of state executions, while the former two cases (1) and (2) can be
    controlled during state changes, which correspond to defined points in the communication
    between the scheduler and the execution engine. Our notion of VCs clearly has
    limitations in terms of expressiveness, and we can envision several extensions.
    For instance, we only introduced VCs that affect single steps, single tasks, single
    dependencies, and single nodes. Thus, we have no notion for expressing constraints
    that, for instance, ensure (1) that two consecutive tasks in a workflow are scheduled
    on the same node (because we know of side effects not modeled in the DAW), or
    (2) that the total size of all intermediate files may not exceed a certain threshold
    (because there is a quota on available disk space). Both are realistic cases:
    (1) is a typical requirement emerging when tasks have to be integrated in a DAW
    that do not adhere to the fundamental assumption in scientific workflow systems
    that tasks only communicate through their input/output relationships. Such a demand
    is often solved by wrapping both tasks into one script, which, however, blurs
    dependencies and removes degrees-of-freedom for the scheduler. (2) is a common
    case in clusters shared between different groups to ensure a fair share of resources.
    In current systems, such quotas are controlled by the file system which would
    block write requests beyond this limit, which in turn very likely would cause
    the workflow engine to crash or the involved tasks to be aborted after their execution
    timeout, which may complicate finding the root cause. Extending our model of VCs
    to cover such cases would be worthwhile yet non-trivial. Regarding (1), our model
    of VCs would need to be extended by constructs that express conditions on pairs
    of tasks; a further generalization would allow selecting arbitrary subsets of
    tasks, possibly by some property (such as all tasks requiring a GPU), which, in
    turn, would need a model for annotating tasks and nodes. A more restrictive extension
    would allow only constraints on pairs of successive tasks, which would suffice
    for the example given. Similarly, a VC for example (2) would need to add constraints
    on groups of files. Allowing such extended forms of VCs would also impact their
    implementation. In their present form, VCs can be checked at clearly defined positions
    during a workflow execution; in contrast, a VC like (1) with arbitrary groups
    of tasks would require checking whenever a task of the group is started or finished,
    and a VC like (2) would require checking whenever an output file is generated
    or written to. We leave such extensions for future work. 5. Concrete validity
    constraints for DAWs After having defined DAWs, their components, VCs, and the
    formal relationships between DAWs and VCs in an abstract manner, we shall now
    introduce a broad collection of different concrete VCs. We do not aim for completeness
    but for a representative set that illustrates the spectrum of functionalities
    that can be covered when using VCs as first-class primitives for DAW languages.
    Naturally, one can envision further constraints up to arbitrary user-defined VCs,
    provided a proper specification language for them is defined. Note that none of
    the VCs we discuss is completely new; instead, many of them can be found either
    implicitly or explicitly in other research fields, such as integrity constraints
    in databases or pre-/post conditions in programming languages; we shall discuss
    these related lines of research in Section 6. We shall present VCs in three steps.
    We shall first list them grouped by the component of a DAW system they address,
    namely setup, task, file, or user-defined accompanied by an intuitive explanation
    and a classification into ‘static’ or ‘dynamic’. In Section 5.1, we distinguish
    several properties of VCs to enable a more fine-grained distinction. These properties
    will allow to systematically characterize VCs in Section 5.2. Based on related
    ideas in other fields, own experience in DAW development, and the user stories
    in Section 2, we consider the following VCs as particularly important. We group
    them according to the part of a DAW/infrastructure they primarily affect. Note
    that not all of them have the same level of abstractions; in some cases, we rather
    describe a type of VC than a concrete VC. For instance, we introduce a general
    VC for file properties instead of one distinct VC for every such property. Setup-related
    VCs This set of VCs are related to the particular combination of a DAW and the
    cluster it should be executed on. Two of them are intended to be controlled before
    the DAW execution starts and are thus static. The third is inherently dynamic.
    static resource availability: The nodes within a cluster must fulfill certain
    requirements in terms of available resources, such as minimal main memory, minimal
    number of allocated CPU hours, or availability of a GPU of a certain type. This
    VC could be defined with two different semantics: In at-least one node, at least
    one node of the cluster must fulfill the constraints; in all nodes, all nodes
    must do so. This VC addresses requirements R4, R7, and R8 of Section 2. file must
    exist: File must exist and must be accessible. Thus may, for instance, affect
    certain reference or metadata files, but can also be used to ensure availability
    of input files of the DAW. This VC could also be defined either in at-least one
    node or all nodes semantics; however, the latter is more common. This VC addresses
    requirement R1. infrastructure health: A node responds to requests from the DAW
    engine prior or during a DAW execution. Such constraints are often implemented
    with the help of a heartbeat-style infrastructure. Task-related VCs Task-related
    VCs describe properties of a concrete task of a DAW. Many of them can be defined
    either statically or dynamically. executable must exist: During execution, any
    concrete task must be scheduled on some node in the cluster. The program executing
    this task must be available on this node. Can be defined statically, which requires
    that all nodes in the cluster maintain executables of all tasks in the DAW, or
    dynamically, which allows for temporary installation (and subsequent deletion)
    of executables of tasks as part of their scheduling. dynamic resource availability:
    Before starting a task on a given node, certain requirements in terms of available
    resources must be fulfilled, such as minimal main memory, minimal number of allocated
    CPU hours, or availability of a GPU of a certain type. During task execution certain
    thresholds of resource usage have to be met. This VC addresses requirements R7,
    R8, and R13–15. configuration parameters: Parameters for execution of a task within
    a DAW must be valid, e.g., have a value within a certain range or of a certain
    format. Is typically defined dynamically as many arguments of tasks are created
    only at runtime, such as the names of input/output files. license valid: Some
    tasks might require a valid license to start. Can be defined statically (test
    for general availability of a valid license for all tasks in a DAW) or dynamically
    (test for concrete availability of a valid license as part of task scheduling).
    The latter is important then the number of possible concurrently running tasks
    is constrained by a volume contract. metamorphic relations: The relation of input
    and output of a task can be characterized by a reversible function. After executing
    a task, the concrete pair of input/output must have this relationship. This VC
    addresses requirement R12. tasks end within limits: The runtime of a particular
    task can be constrained by a VC on its maximal runtime. Such a constraint can
    help to identify stragglers. This VC addresses requirement R10. task ends correctly:
    The execution of a particular task must end with a predefined state or output
    message. File-related VCs File-related VCs control the management of files within
    the infrastructure. Using our definitions from Section 3, this also encompasses
    dependencies and hence data exchange between tasks. file must exist: File must
    exist and must be accessible before starting a task on a node. This VC addresses
    requirement R1. file properties: A file must fulfill certain criteria, such as
    file size, format, checksum over content, or creation time. Can be defined statically
    (properties of metadata files) or dynamically (properties of files generated during
    DAW execution). This VC addresses requirements R2, R3, R5, R6, and R9. folder
    exists: Certain folders must exist and must be readable before/after execution
    of a DAW/task. User-defined VCs In practical applications, DAW developers often
    find very specific cases of validity constraints, which cannot be captured in
    a pre-defined catalog as the one just presented. For such cases, systems should
    also foresee user-defined validity constraints, which implement arbitrary custom
    code. These must be executed before or after a workflow is run or a task is started,
    depending on the specific definition. Naturally, user-defined VCs cannot be categorized
    well as their custom code can perform arbitrary computation and access arbitrary
    data (within the execution environment of the controlled object). Therefore, we
    will not further describe user-defined VCs in the following sections. Note that
    our prototypical implementation presented in 7 also allows and uses user-defined
    VCs. Such VCs may help addressing elaborate variants of requirements R3, R5, R6,
    R9, R11, as well as R13–15. 5.1. Properties of VCs We so-far classified VCs only
    broadly into two classes based on the point in time when they can be checked in
    principle. However, there are many more dimensions by which VCs can be characterized.
    For instance, violations of VCs can have different levels of severity; while some
    must result in an immediate stop of the DAW execution, such as in the case when
    a task in the DAW requires an amount of main memory that none of the nodes of
    a cluster can provide, others might be interpreted rather as a warning, such as
    an improbable yet not impossible file size. Some VCs must be checked before a
    task starts, such as the available resources on the node it is scheduled on, some
    after a task ends, such as its result status, and a third class of VCs requires
    continuous control during task execution, for instance to ensure termination within
    a runtime limit. Table 1 provides six different properties (or dimensions) by
    which VCs can be characterized. These dimensions are mostly independent of each
    other and all have their own importance. For example, knowing whether a constraint
    is ‘hard’ or ‘soft’ is equivalent to knowing whether it expresses a mandatory
    requirement or not. The ‘affected object’ informs how to track the constraint
    and what might be affected if it is violated. Such a more fine-grained classification
    for VCs enables differentiating techniques and therefore enables a common shared
    understanding and objective discussion about VCs. Newly found VCs can be contrasted
    and grouped with other validity constraints using a given classification. Classifications
    can also help identifying new VCs, by looking for a VC that fulfills a certain
    combination of properties. Table 1. Dimensions by which VCs can be characterized.
    Dimension Description and possible values Severity Describes whether a VC must
    be fulfilled or not; non-mandatory VCs implement plausibility checks. Possible
    values: hard implies immediate stop of DAW execution; soft: issues a warning,
    for instance in the DAW log. Affected object Describes the type of object addressed
    by a VC. This dimension was used to group VC in the previous text. Possible values:
    setup, task, and file. Type Describes the type of a VC. Possible values: static;
    dynamic. See also Section 4.1. Time of check Describes the point in time when
    a VC should be checked. Possible values: before, : check before task starts on
    a given node, when state changes from to ; after, : check after a task has finished,
    when state changes from to ; during, : check periodically during task execution,
    i.e., while in state . Component Describes the component in the DAW architecture
    (see Section 3.2 and Fig. 3) which is responsible for controlling a VC. Possible
    values: execution engine EE; scheduler S; resource manager RM; monitoring M. Recoverable
    Describes whether the DAW system can try to recover from the error automatically.
    Possible values: yes (＋), no ( ), maybe ( ). 5.2. Formal characterization of VCs
    In this section, we introduce a classification for Validity Constraints for DAWs.
    First, we will explain why a classification is helpful in this context. Then,
    we present the properties alias dimensions we deem relevant to classify VCs and
    then classify the introduced VCs in Section 5 accordingly. Using the properties
    and dimensions of Table 1, we classify the selected VCs from Section 5 in Table
    2 and observe the following general trends and traits for the selected VCs. Most
    VCs are either hard constraints or can be both hard and soft constraints. This
    characteristic is most likely because there is less value in a VC that never indicates
    an error. There are two big groups regarding the time a VC is checked: many VCs
    are checkable either ‘before’ or ‘during’ the execution; few VCs are checkable
    ‘after’ the execution. This deviation is most likely caused by preconditions and
    invariants being more common than means to check postconditions. The most predominant
    component for checking VCs is the EE, which is tightly coupled to almost all of
    the dynamic VCs, as the EE is inherent to the execution. The time of check also
    correlates with the discreteness of the checks. If a VCs’ time of check is ‘before’
    or ‘after’, it is usually ‘discrete’. If the time of check is ‘during’, the VC
    is usually ‘continuous’. As we do not have enough examples of ‘triggered’ constraints,
    we cannot point out similar correlations for those. Many VCs are not limited to
    workflows but are also applicable in related fields, i.e., they represent more
    general constraints. Many constraint violations are ‘recoverable’ as violating
    them can be caused by spurious problems. Table 2. Validity constraints for workflows.
    6. Related work In the following, we first look at implicitly and explicitly defined
    concepts similar to VCs one can find in other research fields. This review also
    was an important source of input for Section 5. In Section 6.2, we provide a survey
    of validity checking mechanisms in selected current workflow languages or systems,
    namely the Common Workflow Language (CWL), Nextflow, Snakemake, Airflow, Spark,
    and Flink. Finally, Section 6.3 discusses prior (now essentially historical) work
    in VC-related concepts in scientific workflow research. 6.1. VCs in other fields
    of research First, we look at VCs in other fields of research. Database management
    systems. Relational databases use tables with attributes and values to thore their
    data, allowing queries through declarative languages like SQL [36]. They enforce
    integrity constraints (ICs) [37], which ensure specific properties of attribute
    values. These can be constraints on individual values (e.g., value-range constraints),
    constraints on all values of an attribute (e.g., unique constraint), and constraints
    relating values across attributes (e.g., foreign key constraint). In addition,
    user-defined constraints can be programmed using triggers for specific actions,
    such as insertion or deletion of a tuple. ICs in databases resemble validity constraints
    (VCs) in workflows, but there are key differences. ICs are defined over a persistent
    database and are enforced on every data change, ensuring consistency. In contrast,
    VCs for workflows operate in a transient process and must be managed alongside
    workflow execution. Some VCs prevent inconsistent states (pre-conditions), while
    others respond after inconsistencies occur (post-conditions). Database systems
    are monolithic, incorporating IC control, whereas workflow systems involve multiple
    independent components, making VC control more challenging (see Section 7). Model
    checking. Model checking is a technique for automatic formal verification of finite
    state systems. The model checking process can be divided into three main tasks
    [38]: Modeling: Convert a design (software, hardware, DAW) into a formalism accepted
    by a model-checking tool. Specification: State the properties a design must satisfy
    (i.e., some logical formalism, such as modal or temporal logics). Verification:
    Check if the model satisfies the specification (ideally completely automatic).
    Especially the second task (specification) and the first task (modeling) are related
    to VCs. During modeling, the DAW is translated into a Kripke transition system
    [38, Ch. 3], an automaton with states (tasks) and transitions. Each valid path
    in the Kripke transition represents a valid execution path in the DAW, thus ensuring
    that the necessary task execution order is respected. Temporal logic [38, Ch.
    2] is used, for example, to define VCs locally or globally in the specification.
    In other words, the constraints can be on the state (task) level, which can indicate
    that there exists a task along the path that fulfills a given condition or constraint.
    Furthermore, constraints can also be defined on the path-level, meaning that there
    exists a path generated from a state that holds true for a given condition [39].
    Business Process Management (BPM). BPM studies workflows in business-related areas
    to improve business process performance. There are different relevant perspectives
    to consider. The control-flow perspective models the ordering of activities and
    is often the backbone of BPM models. Organizational units, roles, authorizations,
    IT systems, and equipment are summarized and defined in the resource perspective.
    Furthermore, the data or artifact perspective deals with modeling decisions, data
    creation, forms, etc. The time perspective addresses task durations but also takes
    fixed task deadlines into account. Lastly, the function perspective describes
    activities and related applications [40]. Process models can use conditional events
    to define business rules. A conditional event allows a process instance to start
    or progress only when the corresponding business rule evaluates to true. When
    handling exceptions in BPM, validity constraints can be internal (caused inside
    the task) or external (caused by an external event) exceptions. Another constraint
    is the activity timeout, where an activity exceeds the predefined time for execution
    [41]. Software engineering and programming languages. Software engineering involves
    designing, implementing, and maintaining software systems. Data types play a crucial
    role by defining how different components can interact. Using inappropriate data
    types can lead to unspecified and likely invalid behavior, so enforcing type constraints
    is vital for software validity. Most programming languages have type checking.
    Dynamically typed languages like Python do this at runtime, while statically typed
    ones like Java do it at compile time. If a type constraint is violated, an error
    message is returned. Assertions and exceptions in programming languages check
    user-defined validity constraints at runtime. They are used for tasks like verifying
    the existence of a file or specifying correct behavior in software tests. Bertrand
    Meyer introduced Design-by-Contract in the Eiffel language [42], a methodology
    for designing reliable systems using assertions, preconditions, postconditions,
    and class invariants having its roots in Hoare Logic [43] for proving program
    correctness. Result checking [44], involves a separate program dedicated to verifying
    the correctness of results. This differs from software testing in that the checker
    must meet strict reliability and runtime requirements. The Rust programming language2
    [45] aims for safety by design, particularly for concurrent programs. It ensures
    memory safety using a borrow checker that validates memory references and controls
    access through its ownership system, preventing issues with multiple threads accessing
    the same variable. Service composition and interface constraints. The topic of
    automatic service composition is also the main focus of the book by Tan and Zhou
    [46]. It discusses, for example, the verification of service-based workflows,
    quality-of-service (QoS) aspects, deadlock detection, and dead path elimination.
    Based on interface descriptions they verify the automatic composability of workflows.
    As a foundation to analyze properties (e.g., deadlock detection) of DAWs Petri
    nets [30], -calculus [47], process algebra [48], or automata (linear temporal
    logic) [49] can be used. When services or tasks are not directly composable, one
    can look for mediator tasks that make interfaces compatible. In terms of validity
    constraints, the work mainly focuses on the validity of interfaces and data formats
    between tasks, which may be overcome with mediators. Machine Learning Operations
    (MLOps). MLOps involves the complete process of integrating machine learning models
    and pipelines into applications. For example, a video streaming service might
    use a model to suggest content or show relevant ads. The model integration involves
    reoccurring tasks, such as the curation, filtering, and preprocessing of datasets,
    plus the design, training, and validation of the models [50]. Ensuring model validity
    in MLOps focuses on robustness and prediction quality. While it is challenging
    to ensure correctness throughout the entire pipeline, checks can be made at input
    and output stages. This includes cross-checking output quality, detecting changes
    in input data distributions, and verifying infrastructure requirements. Machine
    learning pipelines are often designed for various infrastructures and complex
    technology stacks. Although platforms like TFX [50], MLFlow [51], or Kubeflow
    [52] support MLOps, there is no widely accepted standard to address fully automated
    validity checking. This is crucial due to the continuous changes in pipelines
    driven by real-world data, regulations, fairness, and safety concerns. Each change
    has the potential to introduce errors to the pipeline. Thus, the need for standardized
    quality assurance in MLOps is evident. 6.2. VCs in current workflow systems After
    this overview of validity constraints of different fields, we next describe the
    state-of-the-art in validity constraint definition and checking in actual systems.
    To this end, we look at a selection of current popular state-of-the-art workflow
    systems and examine if and how they support the application of validity constraints.
    Common Workflow Language (CWL). The CWL is an open standard that facilitates the
    description of command-line tool execution and workflow creation. It is still
    under active development [53]. The ways to define validity constraints are currently
    limited but subject to extension. So far, CWL supports a dynamic definition of
    resource requirements enabling the optimization of task scheduling and resource
    usage without manual intervention. Additionally, it allows the specification of
    software requirements. Both the resource and software requirements are expressed
    as hints. Workflow engines may consider or ignore these annotations as CWL is
    merely a workflow language and standard but does not provide a full-fledged execution
    engine other than a simple proof-of-concept runner. For better validation of workflow
    connections, it is recommended to use file format identifiers [54]. An extension
    currently under discussion is the addition of input value restrictions.3 Our proposed
    VCs are more general and conceptually go beyond the basic sanity checks that CWL
    currently offers. Nextflow. Nextflow is a workflow system that provides its own
    domain-specific language to compose user-provided tasks into workflows [55]. Although
    it is mainly used in the bioinformatics domain, Nextflow can be used to build
    workflows in any domain. Recently, the Nextflow developer team introduced their
    new language ‘DSL2’ to build Nextflow workflows. They point out that the next
    focus is to take advantage of the improved modularization capabilities of DSL2
    to support the testing and validation of process modules. We are not aware of
    any built-in functionality to define or check validity constraints of the workflows
    currently, though. Note that in Section 7 we will describe a prototype implementation
    of VC for Nextflow. Snakemake. Snakemake is a workflow management system that
    uses a Python-based language to define and execute workflows. Each rule in Snakemake
    specifies input and output files, along with any parameters or commands needed
    to produce the output from the input. The rules can be chained together to form
    a directed acyclic graph that represents the dependencies between the rules [56].
    While Snakemake ensures that each rule is well-defined and the workflow is reproducible,
    it does not, as far as we know, provide a formal mechanism for specifying validity
    constraints or checking the correctness of the workflow at runtime. Although,
    Snakemake supports a dry-run with the command line option ‘-n’ that can be used
    to check whether the workflow is defined properly and can also provide a rough
    estimate of the required computational time necessary to execute it. Furthermore,
    Snakemake checks for the existence of a task’s defined output files after its
    execution. For further checks, such as checking for them to be non-empty, users
    are advised to implement that by shell commands manually to provoke a non-zero
    exit status of the task.4 With VCs as an explicit concept, we can keep the main
    business logic separate from sanity checks and quality assurance. In a production
    environment, were everything runs smoothly without infrastructure or other changes,
    one then can consider to skip the actual checking of some VCs for performance
    reasons. Apache Airflow. Apache Airflow is a workflow management system created
    in 2014 by Airbnb [57]. Workflows in Airflow are created using the Python API.
    Airflow does not explicitly provide functionality targeted at checking the validity
    of workflows. Instead, they provide a best practices section in their documentation
    with a description of testing of airflow workflows. In this description, the authors
    suggest manually inserting customized checks into the workflow to ensure results
    are as expected. However, such a check is simply another user-defined task inside
    the workflow, and there are no specific airflow constructs to help build such
    checks or to react when checks fails. We discussed benefits of having VCs as a
    separate concept at the end of the previous paragraph and throughout the paper.
    Apache Spark. Apache Spark, which was started in 2009 at UC Berkeley, is a workflow
    engine for large-scale data analysis [58]. Spark workflows are defined via APIs
    in Java, Scala, Python, or R. Apache Spark does not seem to support validity constraints
    for their workflows. Therefore, users need to come up with their own validation
    schemes. Apache Flink. Apache Flink is a data analytics engine unifying batch
    and stream processing [59]. Akin to Spark, Apache Flink workflows are created
    using Java, Scala, or Python APIs. In a document for the nightly build of Apache
    v1.15, the Apache Flink team introduces a new non-stable minimum viable product
    named “Fine-Grained Resource Management”. This new feature will allow workflow
    developers to specify the resource requirements manually for each task. While
    this feature’s primary objective is to improve resource utilization, this may
    provide the possibility for resource-based validity constraints. Aside from that,
    Flink offers extensive support for local testing and validating workflows with
    constructs such as test harnesses and mini clusters. In summary, the concept of
    VCs seems not yet to be well established across the different workflow management
    systems. Nevertheless, the challenges of portability, productivity, and performance
    of workflows get increasing attention also in related fields such as high-performance
    computing [60]. 6.3. Previous work on VCs for scientific workflows Scientific
    Workflows are DAWs in the scientific data analysis domain (Section 1). Typically,
    they build on a Scientific Workflow Management System, which encompass workflow
    languages, execution engines, a form of resource management, and a form of data
    exchange; the later two components are often delegated to infrastructure components
    like shared file systems or resource managers. Over the years, many such systems
    were developed with differing features and capabilities [61]. Validity constraints—although
    they are a vital ingredient for portability, adaptability, and dependability as
    discussed in Section 1—often remain implicit and unchecked in these systems [3].
    Some research addresses only very specialized aspects, such as Rynge et al. who
    focus solely on detecting low-level data corruption (as hard VCs), for instance,
    after caused by network or hardware errors [62]. In the following, we discuss
    some prominent systems or perspectives from the viewpoint of validity constraints.
    Semantic workflows. Semantic workflows denotes a class of workflow languages that
    build on an elaborated, often domain-specific type system or ontology [63]. With
    this ontology, data that is to be exchanged between tasks are assigned a specialized
    type (such as “genomic reads from machine X” instead of the basic “set of strings”),
    tasks are assigned a type (such as “read mapper for genomic reads”), and the IO
    channels of tasks are assigned types. Types are arranged in a specialization hierarchy
    which allows inference regarding type compatibility or workflow planning [64].
    For instance, Lamprecht introduced a workflow language that allows for the definition
    of semantic constraints, leading to methodologies for model-guarded and model-driven
    workflow design [65]. Another example is the semantics-based ‘Wings’ approach
    to workflow development and workflow planning [66]. Types, i.e., semantically
    defined concepts, combined with compatibility checking are a form of validity
    constraints. The are usually defined statically and can be checked before workflow
    execution based on annotation of the workflow components. They operate on a different
    level than the VCs we defined. However, such systems are (yet) rarely used in
    practice because they require all data files and all tasks to be used in a workflow
    to be annotated with concepts from a consistent ontology. In quickly changing
    fields like scientific research where DAWs are often explorative, this requirement
    makes development cumbersome and inflexible. It also requires significant effort
    in community-driven ontology design and maintenance [67]. AWDL based workflows
    and data constraints. Qin and Fahringer [68] use the abstract workflow description
    language (AWDL), an XML-based language expressing workflows. AWDL allows describing
    the directed acyclic graph (DAG) of tasks with their conditions and execution
    properties (parallel, sequential, alternative paths), etc. Further, it allows
    specifying constraints for the runtime environment, optimization, and execution
    steering. The approach follows a UML-based workflow modeling and modularization.
    The specification of data representations and activity types with ontologies aims
    for automatic semantic workflow composition and automatic data conversion. AWDL
    supports simple properties and constraints per data port and task, such as read-only
    or read-write data, expected output size, memory usage, required CPU architecture,
    etc. It also supports constraints on the data distribution like “a task only needs
    the first index”, “a task can work on single items”, “a task needs a window of
    items”, etc. The typed data sources and sinks help the automatic composability
    of workflows and necessary data conversion tasks. Temporal constraints. Liu, Yang,
    and Chen discuss temporal constraints in scientific workflow systems [69]. They
    argue that fixed time constraints are often too strict, and their violation not
    necessarily indicates a failing (or otherwise wrong) workflow execution. Instead,
    they introduce probabilistic temporal constraints, e.g., 90% of tasks of class
    ‘A’ finish within 60 min. They distinguish the components of setting temporal
    constraints, monitoring temporal consistency, and handling temporal violations.
    Checkpoints can be used for re-execution and temporal checks. To overcome constraint
    violations they distinguish statistically recoverable temporal violations and
    statistically non-recoverable temporal violations. The former can be handled by
    doing nothing, or re-scheduling, and the latter by adding resources, stopping
    and restarting tasks or workflows, or workflow restructuring. Provenance and provenance
    validity constraints. The provenance of information produced by executing a workflow
    on some input dataset describes a graph of data derivations, from the inputs to
    the outputs, through each of the intermediate tasks, and potentially including
    accountability metadata, i.e., who has been responsible for the data and for the
    workflow specification. It has been suggested [70] that provenance graphs can
    help to check reproducibility as well as to validate the workflow’s correctness
    and its performance, though practical tools to achieve this are still lacking.
    When provenance is encoded using a standardized data model, such as the W3C’s
    PROV,5 it may be possible to express simple validity constraints on one workflow,
    for instance to assert that only inputs with specific attribution can be accepted.
    In the context of reproducibility, more complex constraints involving two or more
    workflow executions can also potentially be defined, for example to express the
    acceptable differences between the outputs obtained from two runs. The PROV data
    model is designed to facilitate interoperability of provenance information across
    different data providers and consumers. For this, PROV itself defines a formal
    system of validity constraints, denoted PROV-CONSTRAINTS.6 The constraints define
    structural and semantic properties of the graph, asserting for instance that “a
    data element cannot be used before it has been produced”, but also to enable inferences,
    for example “if a data element is generated by two activities” (for instance,
    two workflow tasks) “ and , then and must refer to the same activity” (uniqueness
    of data generation). This is important in the context of provenance interoperability,
    to ensure that semantic correctness of a provenance document received from a third
    party “does not tell an impossible story about the data”. Constraints have also
    been used to define ‘canonical forms’ for PROV graphs [71]. Note that here the
    constraints are all pre-defined, as opposed to user-defined, and as such they
    differ in their purpose and practical usage from the type of VCs considered in
    this paper. 7. Implementing VCs as contracts in Nextflow We implemented a prototype
    for a subset of the types of VCs defined in Section 5 in the popular workflow
    system Nextflow to validate our conceptual model in practice. Details of the implementation
    and its evaluation are available in [72], [73]. In brief, we added two new directives
    called ‘require’ and ‘promise’ into the Nextflow specification language, which
    allow us to insert code for dynamic, task- or file-related validity constraints
    specifying pre- or postconditions (but not runtime conditions) into task definitions.
    By incorporating VCs into the workflow definition language, we can leverage the
    existing language tools available in Nextflow, such as Groovy. This approach avoids
    a separate VC language with its own syntax and interpretation infrastructure,
    and VC writers do not have to learn another specification language [74]. Although
    VCs are written along the main business logic of a workflow, the code for VCs
    should not perform any essential tasks for the workflow, i.e., the workflow should
    run properly without the VCs being executed. The two newly added primitives are
    part of an extension to the DAW model borrowing the concept of contracts from
    software engineering, described in Section 6.1. This contract-based approach allows
    adding a contract to each task in a workflow. Such contracts manifest as sets
    of requirements and promises checked immediately before ( ) and after ( ) the
    task execution to ensure that the task runs in the appropriate environment and
    produces valid results. We added the primitives ‘require’ and ‘promise’ to Nextflow’s
    workflow definition language, so that code for these contracts can be incorporated
    into the task definition. These contracts are then executed alongside the tasks
    on the cluster as arbitrary bash scripts, thanks to Nextflow’s nature of compiling
    each task into a bash script. To facilitate the creation of these contracts, we
    introduced auxiliary constructs with an internal domain-specific language (DSL)
    [75] to Nextflow’s workflow definition language. Fig. 4 exemplary shows how to
    define VCs in the form of a contract for a Nextflow process. When Nextflow generates
    the command bash script for a process, it now places the code from the require
    block before the process code and the code from the promise block after the process
    code. The resulting program can be sent for execution on the cluster nodes. In
    this specific example, the process requires that all FASTA7 files should have
    lines that start with certain characters, such as . This is a simple check to
    verify the file format before data processing and makes the workflow more robust
    for better portability and adaptability. If the preceeding step of data generation
    is altered and then may use a binary encoding or compression for the data, for
    example, the VC safeguards the task and user from weird errors or unnoticed wrong
    analysis results. After processing the data, the task ensures that the process
    execution command did not encounter any errors and that the input files remain
    unmodified. These contracts are categorized as dynamic validity constraints because
    they are code that runs alongside the task they are defined in. In terms of categorizing
    VCs as described in Section 5, these implemented contracts belong to the task-related
    VCs. The task contracts can dynamically check node- and file-level properties,
    such as verifying that the current node has sufficient resources. However, they
    cannot check properties for all nodes. So, they cover the requirements R1–3, R5–9,
    and R12 of Section 2. The remaining open requirements belong either to global
    checks or continuous monitoring aspects, which fit not well into the contract-based,
    task-focused approach but need to be implemented with different means on another
    level. Download : Download high-res image (207KB) Download : Download full-size
    image Fig. 4. Listing of a Nextflow process with ‘require’ and ‘promise’ primitives
    to define VCs. We performed a case study in collaboration with domain scientists
    from the life sciences involving real-world DAWs from the bioinformatics domain,
    where scientists encountered several issues during development that caused delays
    which sometimes lasted several days [72], [73]. We enhanced these workflows with
    contracts to test the effectiveness and comprehensiveness of our contract-based
    approach to implementing validity constraints. This allowed us to identify common
    problems that arise during their execution and demonstrated that the specific
    notifications provided by broken contracts aid in debugging the DAWs. Our investigation
    focused on three main areas: (1) the impact of runtime overhead on each task,
    (2) the amount of computation time that could be saved by aborting the DAW early,
    and (3) how contracts enhance issue localization and explanation. Our practical
    use cases confirmed that the specification even of simple contracts are very effective
    in supporting the identification of issues in real-world DAWs, that they can save
    substantial compute time due to early aborts, and that the runtime overhead often
    is negligible, depending on the type of checks performed. The runtime is illustrated
    by Fig. 5, Fig. 6 in two exemplary workflows from bioinformatics we studied [72],
    [73]. The contract with the highest absolute costs resembles the one outlined
    in the listing above, which checks the input files to contain certain characters
    for the FASTP task. Nevertheless, it needs only about 3.3% of the actual task
    runtime but may save numerous hours of problem investigation by domain scientists
    based on the experience we gained in the case study. Download : Download high-res
    image (201KB) Download : Download full-size image Fig. 5. Exemplary STAR workflow
    from bioinformatics with task and contract execution times. The Cufflinks task
    actually failed. Download : Download high-res image (233KB) Download : Download
    full-size image Fig. 6. Exemplary Salmon workflow from bioinformatics with task
    and contract execution times. 8. Validity constraints specification approach Throughout
    this article, we have argued for the role of VCs as a means of ensuring the portability
    and sustainability of a data analysis workflow across different environments and
    contexts, ultimately saving time for users and developers who (re)use the workflow.
    However, an important question arises: How can VCs be specified? Typically, VCs
    are established during the final stages of workflow development, once the workflow
    has been designed, executed and is ready for sharing. It is the responsibility
    of the workflow developer to specify and incorporate VCs before sharing and publishing
    the workflow. However, VCs’ specification can be laborious and time consuming.
    Not only does the workflow developer need to consider the specific data and processing
    requirements within their own execution environment, but they also need to anticipate
    potential issues that may arise from variations in datasets, formats, parameters
    and environments. While this may seem like extra work for the workflow developer,
    it is crucial to ensuring the long-term viability of workflows and their reuse
    by others. Some constraints can be directly stipulated by the developer, such
    as required input and output files. Other types of VCs, e.g., setup- and task-related,
    however, can be tricky to specify without some investigative effort from the developer.
    In this section, we discuss a few sources of information that can be exploited
    to assist workflow developers in specifying (at least in part) VCs. Debugging.
    Workflows often require multiple debugging iterations before achieving successful
    execution. During this process, workflow developers execute the workflow using
    specific inputs, encounter failures, and then proceed to modify the workflow,
    input data, or environment parameters until a successful execution is achieved.
    Failed executions of workflows can serve as valuable sources for specifying VCs.
    If the workflow developers thoroughly document the reasons a workflow execution
    was deemed to fail, e.g., excessively quick or long task execution, empty output
    file, as well as identifying the specific causes, then such documentation can
    be exploited to harvest VCs. In particular, debugging is likely to point-out file-related
    causes such as missing input file or incorrect input file format, as well as some
    task-related causes, e.g., the existence of a given executable on a given node.
    Testing. While debugging can be effective in helping specify file-based VCs, it
    is less effective for setup- and task-related constraints. During debugging, the
    developer focuses on successfully executing the workflow with specific input files
    of interest. Just like software products, workflow debugging is complemented with
    testing to ensure that the workflow behaves as expected in various settings. Functional
    and non-functional tests can provide valuable information for identifying setup
    and task-related VCs. For instance, tests can involve varying the input files,
    their sizes, formats, and exploring different parameter configurations. These
    tests can help identify resource availability requirements (e.g., memory or number
    of cores required by the nodes or cluster) and configuration parameter constraints
    (e.g., valid range or format). It is important to note that this assumes the user
    has access to nodes with large capacity to identify task memory consumption for
    large input files or tasks with greedy processing. Workflow execution traces.
    If the workflow system has the capability to capture execution traces, also known
    as workflow provenance, then these traces can be collected during the debugging
    and testing phases of the workflow. These traces can serve as valuable resources
    for refining existing constraints, such as resource availability and configuration
    parameters, and even for identifying new constraints. Specifically, by analyzing
    workflow traces from multiple executions, it is possible to extract information
    such as the minimum memory requirement for a particular task. Moreover, execution
    traces can be leveraged to learn correlations or functions, which can be used,
    for example, to predict the memory needed by a node to perform a task correctly
    based on the size of the input files. This will enable the specification of dynamic
    VCs that are associated with the workflow task. Execution traces can also be utilized
    in specifying metamorphic relations. By mining the traces, it may become possible
    to learn functions that describe the relationships between input and output files
    for a given task. Furthermore, execution traces can be employed to set a maximum
    time limit for task execution. If a substantial number of execution traces is
    accessible, then it may be possible to acquire more advanced VCs. For instance,
    it may become feasible to predict the quality or precision of workflow results
    based on its input, as exemplified in a material science user story. If users
    annotate the quality of results obtained during tests and specify the inputs responsible
    for such quality, correlations can be explicitly drawn or even learned. VCs reuse
    across workflows. Developers often do not specify a workflow from scratch but
    reuse existing workflows as building blocks or modify and repurpose them. They
    may also use workflow tasks that have already been used in existing workflows.
    In such cases, some VCs associated with the reused workflow can be used in the
    context of the newly developed workflow. Task-related VCs, such as the availability
    of the executable, resource requirements, configuration parameters, license, and
    task limits, as well as metamorphic relations, can be reused in certain cases.
    However, it should be noted that the newly developed workflow may exhibit different
    features or requirements, which may necessitate adjustments to the reused constraints.
    For example, a task-end limit may be greater or lower depending on the properties
    of the input files used in the newly developed workflows. We have discussed, in
    this section, various sources of information that developers can utilize to draw
    VCs. From the above discussion, it becomes apparent that VCs’ specification should
    not be left until the end of workflow development, as suggested in the introduction
    of this section, but should, instead, be an integral part of the workflow development
    cycle, whereby VCs are specified and refined gradually during the debugging and
    testing operations. It is also worth noting that certain VCs are somewhat generic
    and apply to any data analysis workflow, making their specification straightforward.
    This is the case, for example, for constraints ensuring infrastructure health
    and the validity of task licenses. Workflow repair. The evaluation of our VCs
    can lead to three different actions: None, stopping an execution with a defined
    error message, or only issuing a warning into the workflow logs. However, one
    could also think of other possible reactions to a broken constraint. One intriguing
    idea is to try to automatically ‘repair’ the ongoing workflow execution based
    on the specific constraint [76]. For instance, a VC that fails because the minimal
    memory requirement of a task is not fulfilled on the foreseen node could also
    report its requirements to the scheduler to instruct it restarting the task on
    a more powerful node. A constraint testing the availability of a valid software
    license could also try to acquire such a license from on online repository; VCs
    failing due to ill-formatted input files could try reformatting these files. Such
    repairs probably are not feasible in all cases described in Table 2; for instance,
    a missing input file points to a problem that would be hart to cure without an
    in-depth evaluation of the workflow’s history. We consider an extension of VCs
    into rules with a more diverse universe of consequences than just ‘true’ or ‘false’
    as a particularly interesting route for future work. 9. Conclusions In this article,
    we introduced VCs as a means to make implicit assumptions in data analysis workflows
    explicit, allowing a workflow engine to perform fine-grained status checking and
    to take proper action if needed. We defined a formal model connecting VCs to the
    core elements of DAWs, namely tasks for computations, files for data exchanges,
    and nodes for execution. Based on this formal model, we introduced different types
    of VCs and classified them according to six dimensions. We extensively discussed
    related concepts in various fields of research to show that (a) VCs indeed are
    a vital and ubiquitous concept, but at the same time, (b) a unifying theory was
    missing, and (c) support for VCs should be considered as partial at best in production
    or research systems. We hope our work will help to improve this situation by making
    VCs an integral part of future DAW languages and systems. VCs can support debugging,
    save energy and time by an early failure of workflow executions, provide traceable
    warnings or error messages, and raise confidence in analysis results as they help
    making DAWs more reliable. Several extensions to our work are possible. We discussed
    sharing of VCs and their extension to workflow repair in Section 8. In certain
    situations, e.g., IoT, DAWs are increasingly often used to analyze data streams,
    which would pose specific requirements to validity checking and require fundamentally
    changing their semantics; for instance, the notion of failure would need to be
    revisited. One could also increase the expressiveness of VCs by allowing constraints
    that affect groups of tasks (e.g., the total memory of a group of tasks scheduled
    on a node may not exceed the overall memory of the node) or groups of files (e.g.,
    the files sent to different downstream tasks must be identical). VC checking could
    directly link to counter actions; for instance, breaking a constraint about necessary
    memory on a node could result in feedback to the scheduler and trigger a re-scheduling
    of affected tasks. Another idea worth exploring is an analysis of trace files
    of (failed) workflow executions to automatically find VCs by studying the common
    attributes of an execution leading to a certain behavior, similar to provenance
    patterns [77] or methods for failure predictions [78]. We leave such ideas for
    future work. CRediT authorship contribution statement Florian Schintke: Conceptualization,
    Project administration, Visualization, Writing – original draft, Writing – review
    & editing. Khalid Belhajjame: Writing – review & editing. Ninon De Mecquenem:
    Writing – original draft. David Frantz: Writing – original draft. Vanessa Emanuela
    Guarino: Writing – original draft. Marcus Hilbrich: Writing – original draft,
    Writing – review & editing. Fabian Lehmann: Writing – original draft. Paolo Missier:
    Writing – review & editing. Rebecca Sattler: Writing – original draft. Jan Arne
    Sparka: Writing – original draft. Daniel T. Speckhard: Writing – original draft.
    Hermann Stolte: Writing – original draft. Anh Duc Vu: Writing – original draft.
    Ulf Leser: Conceptualization, Funding acquisition, Methodology, Supervision, Writing
    – original draft, Writing – review & editing. Declaration of competing interest
    The authors declare the following financial interests/personal relationships which
    may be considered as potential competing interests: Florian Schintke, Ninon De
    Mecquenem, Marcus Hilbrich, Fabian Lehmann, David Frantz, Vanessa Emanuela Guarino,
    Rebecca Sattler, Jan Arne Sparka, Daniel T. Speckhard, Hermann Stolte, Anh Duc
    Vu, Ulf Leser report financial support was provided by German Research Foundation
    (CRC 1404 ‘FONDA’, project 414984028). Daniel T. Speckhard reports financial support
    was provided by IMPRS for Elementary Processes in Physical Chemistry. Daniel T.
    Speckhard reports financial support was provided by Max Planck Graduate Center
    for Quantum Materials. Acknowledgments This work was supported by the German Research
    Foundation (DFG) as CRC 1404, project 414984028. D.S. acknowledges support by
    the IMPRS for Elementary Processes in Physical Chemistry and support from the
    Max Planck Graduate Center for Quantum Materials . Data availability No data was
    used for the research described in the article. References [1] da Silva R.F.,
    et al. A characterization of workflow management systems for extreme-scale applications
    Future Gener. Comput. Syst., 75 (2017), pp. 228-238 Google Scholar [2] Liew C.S.,
    et al. Scientific workflows: Moving across paradigms ACM Comput. Surv., 49 (4)
    (2017), pp. 66:1-66:39 Google Scholar [3] Boulakia S.C., et al. Scientific workflows
    for computational reproducibility in the life sciences: Status, challenges and
    opportunities Future Gener. Comput. Syst., 75 (2017), pp. 284-298 Google Scholar
    [4] Janssen J., et al. Pyiron: An integrated development environment for computational
    materials science Comput. Mater. Sci., 163 (2019), pp. 24-36 View PDFView articleView
    in ScopusGoogle Scholar [5] Witt C., van Santen J., Leser U. Learning low-wastage
    memory allocations for scientific workflows at IceCube Int. Conf. on High Performance
    Computing & Simulation, HPCS, IEEE (2019), pp. 233-240 CrossRefView in ScopusGoogle
    Scholar [6] Sukhoroslov O.V. Toward efficient execution of data-intensive workflows
    J. Supercomput., 77 (8) (2021), pp. 7989-8012 CrossRefView in ScopusGoogle Scholar
    [7] Witt C., Wagner D., Leser U. Feedback-based resource allocation for batch
    scheduling of scientific workflows Int. Conf. on High Performance Computing &
    Simulation, HPCS, IEEE (2019), pp. 761-768 CrossRefView in ScopusGoogle Scholar
    [8] Yu J., Buyya R., Ramamohanarao K. Workflow scheduling algorithms for Grid
    computing Xhafa F., Abraham A. (Eds.), Metaheuristics for Scheduling in Distributed
    Computing Environments, Springer Berlin Heidelberg (2008), pp. 173-214 CrossRefView
    in ScopusGoogle Scholar [9] Hilbrich M., et al. A consolidated view on specification
    languages for data analysis workflows Margaria T., Steffen B. (Eds.), Leveraging
    Applications of Formal Methods, Verification and Validation. Software Engineering,
    ISoLA, Lecture Notes in Computer Science, Vol. 13702, Springer (2022), pp. 201-215
    CrossRefView in ScopusGoogle Scholar [10] da Silva R.F., et al. A community roadmap
    for scientific workflows research and development Workshop on Workflows in Support
    of Large-Scale Science, WORKS, IEEE (2021), pp. 81-90 CrossRefView in ScopusGoogle
    Scholar [11] Radetzki U., et al. Adapters, shims, and glue - service interoperability
    for in silico experiments Bioinformatics, 22 (9) (2006), pp. 1137-1143 CrossRefView
    in ScopusGoogle Scholar [12] Oinn T.M., et al. Taverna: lessons in creating a
    workflow environment for the life sciences Concurr. Comput. Pract. Exp., 18 (10)
    (2006), pp. 1067-1100 CrossRefView in ScopusGoogle Scholar [13] Kanwal S., et
    al. Investigating reproducibility and tracking provenance - A genomic workflow
    case study BMC Bioinform., 18 (1) (2017), p. 337 View in ScopusGoogle Scholar
    [14] Lehmann F., et al. FORCE on nextflow: Scalable analysis of earth observation
    data on commodity clusters Cong G., Ramanath M. (Eds.), 1st Int. Workshop on Complex
    Data Challenges in Earth Observation, CEUR Workshop Proceedings, vol. 3052, CEUR-WS.org
    (2021) Google Scholar [15] Schiefer C., et al. Portability of scientific workflows
    in NGS data analysis: A case study (2020) CoRR arXiv:2006.03104 Google Scholar
    [16] Affetti L., Margara A., Cugola G. FlowDB: Integrating stream processing and
    consistent state management Int. Conf. on Distributed and Event-Based Systems,
    DEBS, ACM (2017), pp. 134-145 CrossRefView in ScopusGoogle Scholar [17] Draxl
    C., Scheffler M. The NOMAD laboratory: from data sharing to artificial intelligence
    J. Phys.: Mater., 2 (3) (2019), Article 036001 CrossRefView in ScopusGoogle Scholar
    [18] Engel E., Dreizler R.M. Density functional theory Theoret. Math. Phys. (2011),
    pp. 351-399 Google Scholar [19] Scheffler M., et al. FAIR data enabling new horizons
    for materials research Nature, 604 (7907) (2022), pp. 635-642 CrossRefView in
    ScopusGoogle Scholar [20] Andersen C.W., et al. OPTIMADE, an API for exchanging
    materials data Sci. Data, 8 (1) (2021), p. 217 View in ScopusGoogle Scholar [21]
    Gulans A., et al. Exciting: a full-potential all-electron package implementing
    density-functional theory and many-body perturbation theory J. Phys.: Condens.
    Matter, 26 (36) (2014), Article 363202 CrossRefView in ScopusGoogle Scholar [22]
    Blum V., et al. The FHI-aims code: All-electron, ab initio materials simulations
    towards the exascale (2022) CoRR abs/2208.12335. arXiv:2208.12335 Google Scholar
    [23] Hafner J. Ab-initio simulations of materials using VASP: Density-functional
    theory and beyond J. Comput. Chem., 29 (13) (2008), pp. 2044-2078 CrossRefView
    in ScopusGoogle Scholar [24] Vogel T., et al. Challenges for verifying and validating
    scientific software in computational materials science Int. Workshop on Software
    Engineering for Science (SE4Science), IEEE (2019), pp. 25-32 CrossRefView in ScopusGoogle
    Scholar [25] Buccheri A., et al. Excitingtools: An exciting workflow tool J. Open
    Sour. Softw., 8 (85) (2023), p. 5148 CrossRefGoogle Scholar [26] Caruso F., Novko
    D., Draxl C. Photoemission signatures of nonequilibrium carrier dynamics from
    first principles Phys. Rev. B, 101 (3) (2020), Article 035128 View in ScopusGoogle
    Scholar [27] Carbogno C., et al. Numerical quality control for DFT-based materials
    databases Npj Comput. Mater., 8 (1) (2022), pp. 1-8 Google Scholar [28] Speckhard
    D.T., et al. Extrapolation to complete basis-set limit in density-functional theory
    by quantile random-forest models (2023) CoRR arXiv:2303.14760 Google Scholar [29]
    Frantz D. FORCE—Landsat + Sentinel-2 analysis ready data and beyond Remote Sens.,
    11 (9) (2019), p. 1124 CrossRefView in ScopusGoogle Scholar [30] Diaz M. Petri
    Nets: Fundamental Models, Verification and Applications John Wiley & Sons (2009)
    Google Scholar [31] Johnston W.M., Hanna J.R.P., Millar R.J. Advances in dataflow
    programming languages ACM Comput. Surv., 36 (1) (2004), pp. 1-34 View in ScopusGoogle
    Scholar [32] Sroka J., et al. A formal semantics for the Taverna 2 workflow model
    J. Comput. System Sci., 76 (6) (2010), pp. 490-508 View PDFView articleView in
    ScopusGoogle Scholar [33] Zinn D., et al. Scientific workflow design with data
    assembly lines Deelman E., Taylor I.J. (Eds.), Workshop on Workflows in Support
    of Large-Scale Science, WORKS, ACM (2009) Google Scholar [34] Lee E.A., Sangiovanni-Vincentelli
    A.L. A framework for comparing models of computation IEEE Trans. Comput. Aided
    Des. Integr. Circuits Syst., 17 (12) (1998), pp. 1217-1229 View in ScopusGoogle
    Scholar [35] Kastrati F., Moerkotte G. Generating optimal plans for Boolean expressions
    Int. Conf. on Data Engineering, ICDE, IEEE Computer Society (2018), pp. 1013-1024
    CrossRefView in ScopusGoogle Scholar [36] Garcia-Molina H. Database Systems: The
    Complete Book Pearson Education (2014) Google Scholar [37] Grefen P.W.P.J., Apers
    P.M.G. Integrity control in relational database systems - an overview Data Knowl.
    Eng., 10 (1993), pp. 187-223 View PDFView articleView in ScopusGoogle Scholar
    [38] Clarke E.M., Henzinger T.A., Veith H. Introduction to model checking Clarke
    E.M., et al. (Eds.), Handbook of Model Checking, Springer (2018), pp. 1-26 CrossRefView
    in ScopusGoogle Scholar [39] Beyer D., Gulwani S., Schmidt D.A. Combining model
    checking and data-flow analysis Clarke E.M., et al. (Eds.), Handbook of Model
    Checking, Springer (2018), pp. 493-540 CrossRefView in ScopusGoogle Scholar [40]
    van der Aalst W.M.P., Rosa M.L., Santoro F.M. Business process management - don’t
    forget to improve the process! Bus. Inf. Syst. Eng., 58 (1) (2016), pp. 1-6 View
    in ScopusGoogle Scholar [41] Dumas M., et al. Advanced process modeling Fundamentals
    of Business Process Management, Springer, Berlin, Heidelberg (2013), pp. 97-153
    CrossRefGoogle Scholar [42] Meyer B. Eiffel: A language and environment for software
    engineering J. Syst. Softw., 8 (3) (1988), pp. 199-246 View PDFView articleView
    in ScopusGoogle Scholar [43] Pratt V.R. Semantical considerations on Floyd-Hoare
    logic 17th Ann. Symp. on Foundations of Computer Science (Sfcs 1976), IEEE (1976),
    pp. 109-121 View in ScopusGoogle Scholar [44] Wasserman H., Blum M. Software reliability
    via run-time result-checking J. ACM, 44 (6) (1997), pp. 826-849 View in ScopusGoogle
    Scholar [45] Klabnik S., Nichols C. The Rust Programming Language (Covers Rust
    2018) No Starch Press (2019) Google Scholar [46] Tan W., Zhou M. Business and
    Scientific Workflows: A Web Service-Oriented Approach John Wiley & Sons, Ltd (2013)
    Google Scholar [47] Milner R. Communicating and Mobile Systems: the Pi-Calculus
    Cambridge University Press (1999) Google Scholar [48] Fokkink W.J. Introduction
    to Process Algebra Texts in Theoretical Computer Science. An EATCS Series, Springer
    (2000) Google Scholar [49] Baier C., Katoen J. Principles of model checking MIT
    Press (2008) Google Scholar [50] Baylor D., et al. TFX: A TensorFlow-based production-scale
    machine learning platform SIGKDD Int. Conf. on Knowledge Discovery and Data Mining,
    ACM (2017), pp. 1387-1395 CrossRefGoogle Scholar [51] Zaharia M., et al. Accelerating
    the machine learning lifecycle with MLflow IEEE Data Eng. Bull., 41 (4) (2018),
    pp. 39-45 Google Scholar [52] Bisong E. Kubeflow and Kubeflow pipelines Building
    Machine Learning and Deep Learning Models on Google Cloud Platform, Apress Berkeley,
    CA (2019), pp. 671-685 CrossRefGoogle Scholar [53] Amstutz P., et al. Common Workflow
    Language, v1.0 (2016) Google Scholar [54] Crusoe M.R., et al. Methods included:
    Standardizing computational reuse and portability with the common workflow language
    Commun. ACM, 65 (6) (2022), pp. 54-63 CrossRefView in ScopusGoogle Scholar [55]
    Di Tommaso P., et al. Nextflow enables reproducible computational workflows Nature
    Biotechnol., 35 (4) (2017), pp. 316-319 CrossRefView in ScopusGoogle Scholar [56]
    Köster J., Rahmann S. Snakemake – a scalable bioinformatics workflow engine Bioinformatics,
    34 (20) (2018), p. 3600 CrossRefView in ScopusGoogle Scholar [57] Harenslak B.P.,
    de Ruiter J. Data Pipelines With Apache Airflow Simon and Schuster (2021) Google
    Scholar [58] Zaharia M., et al. Apache Spark: a unified engine for big data processing
    Commun. ACM, 59 (11) (2016), pp. 56-65 CrossRefView in ScopusGoogle Scholar [59]
    Carbone P., et al. Apache Flink™: Stream and batch processing in a single engine
    IEEE Data Eng. Bull., 38 (4) (2015), pp. 28-38 Google Scholar [60] Ben-Nun T.,
    et al. Workflows are the new applications: Challenges in performance, portability,
    and productivity P3HPC@SC, IEEE (2020), pp. 57-69 CrossRefView in ScopusGoogle
    Scholar [61] Deelman E., et al. Workflows and e-science: An overview of workflow
    system features and capabilities Future Gener. Comput. Syst., 25 (5) (2009), pp.
    528-540 View PDFView articleView in ScopusGoogle Scholar [62] Rynge M., et al.
    Integrity protection for scientific workflow data: Motivation and initial experiences
    Furlani T.R. (Ed.), Pract. and Exp. in Advanced Research Computing on Rise of
    the Machines (Learning), PEARC, ACM (2019), pp. 17:1-17:8 Google Scholar [63]
    Gil Y., et al. Mind your metadata: Exploiting semantics for configuration, adaptation,
    and provenance in scientific workflows Aroyo L., et al. (Eds.), The Semantic Web
    - ISWC, Lecture Notes in Computer Science, Vol. 7032, Springer (2011), pp. 65-80
    CrossRefView in ScopusGoogle Scholar [64] Lamprecht A.L., et al. Perspectives
    on automated composition of workflows in the life sciences F1000Research, 10 (897)
    (2021) Google Scholar [65] Lamprecht A. User-Level Workflow Design - A Bioinformatics
    Perspective Lecture Notes in Computer Science, Springer (2013) Google Scholar
    [66] Gil Y., et al. Wings: Intelligent workflow-based design of computational
    experiments IEEE Intell. Syst., 26 (1) (2011), pp. 62-72 View in ScopusGoogle
    Scholar [67] Ison J.C., et al. EDAM: an ontology of bioinformatics operations,
    types of data and identifiers, topics and formats Bioinformatics, 29 (10) (2013),
    pp. 1325-1332 CrossRefView in ScopusGoogle Scholar [68] Qin J., Fahringer T. Scientific
    Workflows: Programming, Optimization, and Synthesis with ASKALON and AWDL Springer
    (2012) Google Scholar [69] Liu X., Yang Y., Chen J. Temporal QoS Management in
    Scientific Cloud Workflow Systems Elsevier (2012) Google Scholar [70] Deelman
    E., et al. The future of scientific workflows Int. J. High Perform. Comput. Appl.,
    32 (1) (2018), pp. 159-175 CrossRefView in ScopusGoogle Scholar [71] Moreau L.
    A canonical form for PROV documents and its application to equality, signature,
    and validation ACM Trans. Internet Technol., 17 (4) (2017), pp. 35:1-35:21 Google
    Scholar [72] Vu A.D., et al. Contract-driven design of scientific data analysis
    workflows E-Science 2023, IEEE (2023), pp. 1-10 CrossRefGoogle Scholar [73] Vu
    A.D., et al. Design by contract revisited in the context of scientific data analysis
    workflows E-Science 2023, IEEE (2023), pp. 1-2 CrossRefGoogle Scholar [74] Fähndrich
    M., Barnett M., Logozzo F. Embedded contract languages Shin S.Y., et al. (Eds.),
    Symp. on Applied Computing, SAC, ACM (2010), pp. 2103-2110 CrossRefView in ScopusGoogle
    Scholar [75] Fowler M. Domain-Specific Languages Addison-Wesley Professional (2010)
    Google Scholar [76] Goues C.L., Pradel M., Roychoudhury A. Automated program repair
    Commun. ACM, 62 (12) (2019), pp. 56-65 CrossRefGoogle Scholar [77] Miao Z., et
    al. Going beyond provenance: Explaining query answers with pattern-based counterbalances
    Proceedings of the 2019 International Conference on Management of Data, SIGMOD
    ’19, Association for Computing Machinery, New York, NY, USA (2019), pp. 485-502
    CrossRefView in ScopusGoogle Scholar [78] Salfner F., Lenk M., Malek M. A survey
    of online failure prediction methods ACM Comput. Surv., 42 (3) (2010), pp. 10:1-10:42
    Google Scholar Cited by (0) Florian Schintke heads the ‘Distributed Algorithms’
    research department at Zuse Institute Berlin (ZIB). He received his Diploma in
    computer science from the Technical University Berlin with distinction in 2000
    and then joined ZIB’s computer science group headed by Prof. Alexander Reinefeld
    as a research associate. He received his doctoral degree from Humboldt University
    Berlin in 2010. His current research interests include distributed algorithms
    and distributed data management in particular fault-tolerance, scalability, big
    data, resource management, modern networks, and scientific workflows. Khalid Belhajjame
    is an associate professor with HDR (habilitation to direct research) at the Parie
    Dauphine University, where he is a member of the LAMSADE research laboratory.
    His research focuses on information and knowledge management. In particular, he
    has made significant contributions in the areas of data preparation, data privacy
    and protection, eScience, scientific workflow management, provenance tracking
    and exploitation, and knowledge graphs. He is currently a member of the steering
    committee of the GDR MaDICS, a national research network for interdisciplinary
    data science research, co-leader of the Database Working Group for Remote Sensing
    Data in the Earth Science Informatics (IEEE GRSS), and section editor of the Elsevier
    MethodX journal. Ninon De Mecquenem is a Ph.D. student in Computer Science in
    Ulf Leser’s group at the Knowledge Management in Bioinformatics Lab at the Humboldt
    University in Berlin. She holds a bachelor’s degree in biology from the University
    of Perpignan and a master’s degree in bioinformatics from the University of Bordeaux.
    She worked as a data scientist in the private sector for two years before starting
    her PhD. She is now working on rewriting bioinformatics workflows for FONDA’s
    A2 group. David Frantz is an Assistant Professor at Trier University, where he
    leads the ’Geoinformatics - Spatial Data Science’ department. He earned his Diploma
    in Applied Environmental Sciences and obtained his Ph.D. in Remote Sensing from
    Trier University. From 2017 to 2021, he was a Postdoctoral Researcher at Patrick
    Hostert’s ’Earth Observation Lab’ at Humboldt-Universität zu Berlin, collaborating
    on interdisciplinary projects with social ecologists and computer scientists.
    His research focuses on translating raw Earth observation data into customized
    information to address a diverse array of environmental research and monitoring
    requirements. Vanessa Emanuela Guarino is a Ph.D. candidate affiliated with the
    Kainmüller Lab (Laboratory of Biomedical Image Analysis) at the Max-Delbrück-Center
    for Molecular Medicine Berlin and is currently funded by FONDA, a collaborative
    research center of the German Research Foundation (DFG). Her background ranges
    between financial engineering and statistics and her work focuses on principle
    ways to construct neural network architectures. In particular, her projects aim
    at analyzing and defining the mathematical foundations of deep learning, in order
    to render models invariant to small perturbations and aware of their degree of
    uncertainty. Marcus Hilbrich is the Scientific Coordinator of FONDA. FONDA aims
    to improve software usage, creation, and reuse by improving workflows. Doctorate
    engineer Marcus Hilbrich supports FONDA as a software engineer based on research
    on the management of software artifacts, e.g., based on ßMACH, a method to guide
    software artifact management. In addition, he provides knowledge of different
    computer science disciplines, like HPC, clouds, grids, security, performance modeling,
    management, and teaching. He supported a diverse set of projects in academics
    and industry at the Technische Universität Dresden, Universität Paderborn, Technische
    Universität Chemnitz, and currently Humboldt-Universität zu Berlin. Fabian Lehmann
    is a Ph.D. student in computer science and has been a member of Ulf Leser’s group
    at the Knowledge Management in Bioinformatics Lab at the Humboldt-Universität
    zu Berlin since 2020. He received his Bachelor’s and Master’s degrees from TU
    Berlin between 2015 and 2020 and is currently funded through FONDA, a Collaborative
    Research Center of the German Research Foundation (DFG). His research focuses
    on improving the execution of distributed workflows, with a particular emphasis
    on scheduling and data management. Paolo Missier is a Professor of Scalable Data
    Analytics with the School of Computing at Newcastle University, where he leads
    the Scalable Computing group, and a Fellow (2018–2023) of the Alan Turing Institute,
    UK’s National Institute for Data Science and Artificial Intelligence. His background
    is in data management and “e-science”, i.e., scalable workflow management in support
    of science. His notable contributions are in the area of data provenance, including
    the W3C PROV provenance model and provenance for data science. He has recently
    been focusing on methods for streamlining data engineering for Data Science and
    AI for Healthcare. He has been (2016–2023) Sr. Associate Editor for the ACM Journal
    on Data and Information Quality (JDIQ). Rebecca Sattler is currently a Ph.D. student
    affiliated with the DBIS (Database Systems and Information Management) group within
    the Department of Computer Science at Humboldt University of Berlin (HUB) and
    is currently funded through FONDA, a Collaborative Research Center of the German
    Research Foundation (DFG). In terms of her research focus, Rebecca’s interests
    encompass several areas within the field of computer science. Specifically, she
    is dedicated to the exploration of event query languages and the development of
    algorithms for event query discovery. Additionally, her work delves into the domain
    of event-based validation of workflow execution, showcasing her commitment to
    advancing knowledge and innovation in these critical areas of study. Jan Arne
    Sparka is a Ph.D. student in the department of Computer Science at Humboldt University
    of Berlin. Since 2020 he is part of the collaborative research center FONDA where
    his research focuses on improving the reliability of workflow systems using control
    operations. Daniel T. Speckhard is a Ph.D. student currently affiliated with the
    Max Planck Institute for Solid State Physics, Humboldt-Universität zu Berlin,
    and the NOMAD Laboratory at the Fritz Haber Institute of Max Planck Society and
    IRIS Adlershof. Their research focuses on developing statistical learning methods
    to predict solid material properties that bypass expensive density functional
    theory computations. Hermann Stolte is a Ph.D. student at the department of Computer
    Science at Humboldt University of Berlin and member of the Helmholtz Einstein
    international Berlin research school in data science (HEIBRiDS). His research
    is centered around uncertainty management in exploratory data analysis, with a
    particular focus on data-driven anomaly detection techniques in multi-wavelength
    astronomy using machine learning. Anh Duc Vu is a Ph.D. student in the department
    of Computer Science at Humboldt University of Berlin. Since 2020 he is part of
    the collaborative research center FONDA where his research focuses on the validity
    and trustworthiness of data centric systems using techniques from software testing
    and debugging. Ulf Leser is the chair of Knowledge Management in Bioinformatics
    at Humboldt-Universität zu Berlin. He is a computer scientist with long-standing
    experience in interdisciplinary research regarding managing and analyzing scientific
    data, especially in Biomedicine. His research focuses on data integration, infrastructures
    for large-scale scientific data analysis, biomedical text mining, and statistical
    Bioinformatics. His group develops novel algorithms in these fields and applies
    them in interdisciplinary research projects, especially in cancer research. 1
    Of course, other means of data exchange, such as in-memory channels, or mounting
    of remote file systems are possible as well. 2 https://www.rust-lang.org/. 3 https://github.com/common-workflow-language/common-workflow-language/issues/764.
    4 https://snakemake.readthedocs.io/en/v7.25.0/ FAQ. 5 https://www.w3.org/TR/prov-dm/.
    6 https://www.w3.org/TR/prov-constraints/. 7 https://blast.ncbi.nlm.nih.gov/doc/blast-topics/.
    © 2024 The Authors. Published by Elsevier B.V. Recommended articles Environmental
    impacts of earth observation data in the constellation and cloud computing era
    Science of The Total Environment, Volume 909, 2024, Article 168584 R. Wilkinson,
    …, K. Anderson View PDF Static analysis of Taverna workflows to predict provenance
    patterns Future Generation Computer Systems, Volume 75, 2017, pp. 310-329 Pinar
    Alper, …, Carole A. Goble View PDF Back to basics: Fast denoising iterative algorithm
    Signal Processing, Volume 221, 2024, Article 109482 Deborah Pereg View PDF Show
    3 more articles About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Future Generation Computer Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Validity constraints for data analysis workflows
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Werner S.
  - Tai S.
  citation_count: '0'
  description: Despite significant advances in data management systems in recent decades,
    the processing of big data at scale remains very challenging. While cloud computing
    has been well-accepted as a solution to address scalability needs, cloud configuration
    and operation complexity persist and often present themselves as entry barriers,
    especially for novice data analysts. Serverless computing and Function-as-a-Service
    (FaaS) platforms have been suggested to reduce such entry barriers by shifting
    configuration and operational responsibilities from the application developer
    to the FaaS platform provider. Naturally, “serverless data processing (SDP)”,
    that is, using FaaS for (big) data processing, has received increasing interest
    in recent years. However, FaaS platforms were never intended to support large
    data processing tasks primarily. SDP, therefore, manifests itself through workarounds
    and adaptations on the application level, addressing various quirks and limitations
    of the FaaS platforms in use for data processing needs. This, in turn, creates
    tensions between the platforms and the applications using them, again encouraging
    the constant (re-)design of both. Consequently, we present lessons learned from
    a series of application and platform re-designs that address these tensions, leading
    to the development of an SDP reference architecture and a platform instantiation
    and implementation thereof called CREW. Mitigating the tensions through the process
    of application platform co-design proves to reduce both entry barriers and costs
    significantly. In some experiments, CREW outperforms traditional, non-SDP big
    data processing frameworks by factors.
  doi: 10.1016/j.future.2024.01.029
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Serverless big data processing
    3. CREW – a next-generation serverless data processing system 4. Experiment-driven
    evaluation 5. Related work 6. Conclusion CRediT authorship contribution statement
    Declaration of competing interest Acknowledgments Appendix A. Systematic literature
    review Appendix B. Experiment setup Data availability References Vitae Show full
    outline Figures (7) Show 1 more figure Tables (5) Table 1 Table 2 Table A.2 Table
    B.3 Table B.4 Future Generation Computer Systems Volume 155, June 2024, Pages
    179-192 A reference architecture for serverless big data processing☆ Author links
    open overlay panel Sebastian Werner, Stefan Tai Show more Share Cite https://doi.org/10.1016/j.future.2024.01.029
    Get rights and content Under a Creative Commons license open access Highlights
    • A Reference Architecture for Serverless Big Data Processing. • Introduction
    and Definition of Tensions in Current Serverless Data Processing Architectures.
    • Implementation and Evaluation of a next-generation Serverless Data Processing
    System. Abstract Despite significant advances in data management systems in recent
    decades, the processing of big data at scale remains very challenging. While cloud
    computing has been well-accepted as a solution to address scalability needs, cloud
    configuration and operation complexity persist and often present themselves as
    entry barriers, especially for novice data analysts. Serverless computing and
    Function-as-a-Service (FaaS) platforms have been suggested to reduce such entry
    barriers by shifting configuration and operational responsibilities from the application
    developer to the FaaS platform provider. Naturally, “serverless data processing
    (SDP)”, that is, using FaaS for (big) data processing, has received increasing
    interest in recent years. However, FaaS platforms were never intended to support
    large data processing tasks primarily. SDP, therefore, manifests itself through
    workarounds and adaptations on the application level, addressing various quirks
    and limitations of the FaaS platforms in use for data processing needs. This,
    in turn, creates tensions between the platforms and the applications using them,
    again encouraging the constant (re-)design of both. Consequently, we present lessons
    learned from a series of application and platform re-designs that address these
    tensions, leading to the development of an SDP reference architecture and a platform
    instantiation and implementation thereof called CREW. Mitigating the tensions
    through the process of application platform co-design proves to reduce both entry
    barriers and costs significantly. In some experiments, CREW outperforms traditional,
    non-SDP big data processing frameworks by factors. Previous article in issue Next
    article in issue Keywords Serverless data processingApplication platform co-designServerless
    reference architectureFunction as a ServiceSoftware engineeringCloud computing
    1. Introduction Continuously and with ever-increasing speed, applications are
    being built on top of software platforms. The software platforms correspondingly
    must support many use cases, which creates a wide range of both features but also
    quirks and platform-specific constraints driving application architecture and
    changes. Application demands, in turn, force developers to seek platforms with
    minimal constraints and the features best suited to address their individual needs,
    thus incentivizing platforms to adapt to changing demands. Alternatively, changes
    in application demands may suggest the development of a new (variant of a) software
    platform better suited to address application needs with minimal adaptation costs
    for applications. Managing the duality of addressing these demands is the goal
    of application platform co-design [1] – an approach to address the design tensions
    in applications driving new platform designs and platforms driving new application
    designs. One area of particular interest are applications that must process large
    quantities of data. Over the last decades, it has become increasingly simple to
    generate and collect data [2], be it from extensive physics experiments, from
    IoT, or from logging and monitoring large distributed systems. While business
    and research collect data at an extraordinary pace, systems to analyze this data,
    however, must scale to handle the increasing volume, variety, and velocity of
    the data [3], [4], [5]. Consequently, industry actors and researchers must continually
    (re-) design and expand systems to store, evaluate, and process big data at an
    increasing scale. Notably, developers can choose between a magnitude of different
    environments and components to build their data processing systems for each application
    domain. However, these choices usually also come with varied configuration, management
    and usage complexity, creating increasingly high entry barriers for the average
    user. Contemporary approaches to bridge this complexity gap use cloud platforms,
    reducing the choice to either fully managed or partially self-managed data analytics
    platforms. For example, a user can elect to run Apache Spark [6], [7] in a fully
    managed manner, e.g., using Amazon EMR [8], and thereby relies on cloud providers
    for creating and owning large-scale data processing clusters. However, “distributed
    computing remains inaccessible to a large number of users” [9], as many tuning,
    sizing, programming, and operating challenges remain. One promising trend in this
    direction is the use of serverless platforms, e.g., Function-as-a-Service (FaaS)
    platforms to run large-scale data processing [10], reducing barriers to entry
    by sharply separating operational concerns between development matters and those
    related to usage [9], [11], [12], [13]. Here, all scaling and infrastructure decisions
    are made by the cloud provider. Applications built on such a platform can quickly
    respond to workload changes and scale even down to zero if no work arrives. Thus,
    industry actors and researchers have started to utilize FaaS to process data,
    as “ultimate elasticity and operational simplicity [...] in the context of data
    analytics sounds appealing” [14]. Data processing approaches that utilize FaaS
    obviate management and tuning of clusters and reduce the need for resource-based
    sizing and scaling decisions [10], [15], [16]. We argue that existing entry barriers,
    e.g., high configuration complexity and cost of ownership, are addressable FaaS
    approaches [17], [18]. At the same time, serverless solutions can outperform traditional
    big data processing systems in terms of cost and performance for exploratory data
    analysis [19]. Still, while generic FaaS platforms offer a simplified computing
    model, they do not specifically address data processing needs [12], [13], [14],
    [20], thereby creating design tensions between data processing applications and
    serverless platforms to address these needs. Consequently, in this paper, we ask
    the question: How can we co-design serverless data processing applications and
    platforms to reduce entry barriers and ease existing design tensions? Toward that
    end, we present the following contributions: • A reference architecture for implementing
    serverless data processing systems • CREW – a new serverless data processing framework
    and serverless platform that addresses tensions using application platform co-design
    • A demonstration and evaluation of CREW Note, while we present CREW as a new
    serverless data processing framework and platform, it is not intended as a production-ready
    system. Instead, we use CREW to demonstrate the feasibility of the application
    platform co-design approach to address design tensions we observed through our
    reference architecture. The remainder of this paper is structured as follows:
    First, in Section 2, we present the serverless data processing reference architecture
    and currently manifested tensions. Moreover, in Section 3, we introduce CREW,
    a new serverless data processing framework and platform that addresses these tensions.
    In Section 4, we evaluate CREW compared to Apache Spark. Lastly, we discuss related
    work in Section 5 and conclude this work. 2. Serverless big data processing In
    the following, we aim to identify “a family of similar systems and to standardize
    nomenclature”[21] for serverless data processing (SDP) using a literature study
    (see Appendix A). Moreover, we present common tensions found in existing serverless
    data processing architectures, propose a reference architecture for SDP and pinpoint
    the interactions within the reference architecture that are common sources of
    these tensions, providing an outlook on how to address these tensions in the next
    generation of serverless data processing systems. 2.1. Common serverless data
    processing systems Serverless data processing typically involves multiple, often
    interchangeable, platforms and services to address different functional and non-functional
    goals. Besides Function-as-a-Service (FaaS) offerings, fully managed storage services
    and fully managed orchestrators such as AWS StepFunctions are common. The main
    benefit of using serverless platforms instead of analytics engines with a traditional
    deployment model such as Apache Flink lies in the highly elastic, event-driven
    resource provisioning with minimal operational overhead offered through the serverless
    model. Consequently, any data processing task that benefits from ad-hoc computation,
    as well as, distributed batch data processing [9], [12], [13], [14], [22], [23],
    [24], [25] shows to benefit significantly. However, this creates a large number
    of “moving parts” that each require attentive configuration and management, which
    limits the ease of use and may create different entry barriers [9]. Hence, programming
    frameworks and tools have emerged to automate the generation, deployment and orchestration
    of serverless data processing applications. In Table 1, we present an overview
    of the most predominant frameworks discovered in a systematic literature review
    conducted from 2018–2022, along with the advancements they introduced. These advances
    all represent workarounds or approaches to deal with the tensions between applications
    and platforms discussed in this section. Note that some of the frameworks are
    not publicly available or no longer maintained, while a small number are still
    in development. 2.2. A reference architecture Based on a careful review of the
    architectures of the discovered frameworks (Table 1), we compiled a reference
    architecture for serverless data processing. Fig. 1 gives an initial high-level
    overview of the components, the relationships and the domains of responsibility
    of this reference architecture. It further highlights the different platforms
    (blue) that must interact with application components (gray), including interactions
    typically handled by the supporting processing framework (green). The components
    within this architecture are the most commonly modified parts of the original
    serverless model and thus represent the defining features of serverless data processing
    applications. We separate serverless data processing applications into four logical
    planes. Firstly, on the User Plane, data analysts define a processing job using,
    for example, query languages [26], map-reduce APIs [35], [36], or full-fledged
    Spark APIs [22]. Secondly, the Control Plane is responsible for translating, packaging,
    and deploying the serverless execution engine and managing the execution at runtime.
    Thirdly, the Execution Plane is responsible for realizing the analytics logic,
    usually packaged as multiple small functions or containers. Lastly, the Data Plane
    is responsible for storing and retrieving intermediate data and raw data for the
    processing job,1 as well as for enabling the observability for the control plane.
    Table 1. Overview of serverless data processing frameworks extracted through an
    SLR conducted between 2018–2022 (see Appendix A). Note that some of the frameworks
    are not publicly available or are no longer maintained. Name Release Year Last
    Activity Driver Computing Storage PyWren [9] 2016 2018 ✓ ✓ gg [12] 2017 2022 ✓
    ✓ ✓ Flint [22] N.A. N.A. ✓ ✓ Lambada [14] N.A. N.A. ✓ ✓ ✓ Starling [26] N.A. N.A.
    ✓ ✓ ✓ CRUCIAL [20] 2019 2021 ✓ ✓ ✓ Locus [23] 2016 N.A. ✓ Qubole [27] 2018 2019
    ✓ ✓ ✓ Opvis [28] N.A. N.A. ✓ ✓ MARLA [29] 2017 2021 ✓ ✓ ✓ Ooso [30] 2017 2017
    ✓ ✓ ✓ Wukong [24] 2019 2023 ✓ ✓ ✓ Berkley ML [31] N.A. N.A. ✓ ✓ SCAR [32] 2017
    N.A. ✓ ✓ ✓ CCoDaMiC [33] N.A. N.A. ✓ ✓ Corral [34] 2018 2021 ✓ ✓ ✓ Lithops [35]
    2018 2023 ✓ ✓ ✓ CREW 2020 2023 ✓ ✓ ✓ Empty Cell Download : Download high-res image
    (649KB) Download : Download full-size image Fig. 1. Generalized serverless data
    processing application reference architecture, showing how the different platforms
    (blue) interact with the application (gray) and the supporting processing framework
    (green). Including both platform- and application-side components that can influence
    overall processing qualities. In the following, we will briefly discuss these
    planes and how cross-cutting concerns within reveal tensions in the design of
    serverless data processing applications that framework developers must address.
    User plane. Within the User Plane, the data analyst is defining all the functionality,
    e.g., the tasks and configuration that the serverless data processing framework
    will use to perform the analytics job. Naturally, the way in which these functions
    and configurations are created is limited by the framework’s API and depends on
    the provided programming abstractions. Although platform details are often hidden
    from the data analyst, the underlying platform can still influence the means offered
    to define processing jobs. For example, writing code to run on AWS might need
    different considerations in terms of data sizes per function than writing code
    for OpenWhisk, as the limits of OpenWhisk can be tuned more than those for AWS.
    Moreover, it is up to the SDP framework to optimize and plan how the provided
    configurations and user defined functions (UDFs) are turned into a series of processing
    steps executed by serverless function runtimes. Control plane. The control plane
    typically contains a job driver, equivalent to a driver in traditional processing
    systems, responsible for translating a user-defined job description into something
    that can be executed. Thus, these drivers embody the core functionality of an
    SDP framework, translating UDFs into concrete deployment packages (functions)
    and providing execution plan (events) to run the job. Thus, the driver is strongly
    coupled to the Worker-Function, which receives the event and executes the corresponding
    task in accordance with the UDFs. This strong coupling also means that the driver
    has to observe event completion, error recovery, straggler mitigation and configuration
    optimizations. Consequently, the driver is a central component responsible for
    managing the interaction between applications and platforms and, thus, a central
    point of tension: T1 Tension from the mapping of application tasks (e.g., processing
    operations) to concrete deployment and configuration, as this is needed in a far
    more granular way than in other data processing systems. For example, platform-driven
    limitations on package sizes, runtimes, and memory can limit the possible functionality
    of provided code and, thus, the capability of processing tasks, which can lead
    to increased cold-starts [37] or increased overhead due as functions need to be
    split up. T2 Tension from the coarse configuring. For example, a function that
    is configured with too little memory can lead to faults, execution issues, or
    increases in management overhead, while a function with too much memory can lead
    to unnecessary costs. At the same time, the needed memory is often not known in
    advance, as it depends on the input data generated by the previous stage [38],
    [39]. T3 Tension from the scheduling and observation between the driver and execution
    platform. As, in many cases, the driver and the platform share responsibilities
    for scheduling the execution of the processing job, it is crucial to align both.
    Towards that end, the platform must provide consistent and timely feedback to
    the driver, and the driver should not overload the platform with status requests.
    Here, many workarounds are used such as the Job-Server and Workpulling-Functions
    [24], [28] which bypass the platforms resource optimizations, the Function-Chaining
    [14], [29] which add more complex fault mitigation needs [40] or approaches such
    as bootstraping [13] which introduced additional cost and overhead. Execution
    plane. The execution plane provides the processing power to perform each task
    that a driver generates. The goal of each serverless data processing application
    (SDPA) is to scale in response to the workload without the interaction of the
    data analyst. Naturally, the selected platform strongly influences the performance
    and cost [41] of an SDPA. Each platform presents unique limitations that a driver
    must work around [1], e.g., configuration-sensitivities [39], event generation
    support [1], observability [40] and acceleration [42]. Moreover, how worker functions
    are built also strongly influences system quality. For example, the use of pre-compiled
    binaries to execute specific operations leads to smaller deployment packages,
    which can reduce cold-starts [37], [43]. The capabilities and limitations of the
    serverless execution platform are thus a central point of tension for SDPA. Among
    them, the following tensions are of particular importance: T4 Tension from the
    atypical use of serverless systems for data processing, e.g., briefly running
    fleets of functions from a singular client.2 Which can significantly increase
    cold-start panelties [37], [44]. T5 Tensions from the stateless programming model,
    which leads to the need to store intermediate results in a separate storage system.
    Thus, data must always move between the storage system and the serverless platform,
    even for small results [45] Data plane. The data plane mainly consists of means
    to handle intermediate data (T5) and the bulk data that needs to be processed.
    For bulk data, object stores are most common [19]. Some approaches reuse the bulk
    data object store for intermediate results, while other approaches utilize fully
    managed message queues, in-memory databases, or purpose-built middleware for intermediate
    storage. However, all these approaches result in a large amount of data movement
    between the storage system and the serverless platform, leading to significant
    tensions: T6 Tension from the quickly shifting data access patterns (e.g., dynamic
    partitioning, shuffling and merging of data, and bulk access, as well as, line-wise
    or byte-wise access). Limiting how specialized a storage system can be for serverless
    data processing [46]. T7 Tension from the highly parallel nature of serverless
    processing. An SDPA might spawn thousands of functions that access the same object,
    each reading only very small parts. Thus, the storage system must be able to distribute
    the load adequately and in a more fine-grained way than for other use cases such
    as Spark [19]. T8 Tension from heavy usage of metadata queries. Differently from
    other applications, SDPA heavily misuses metadata queries to orchestrate the processing,
    e.g., to see if a series of functions is finished. Thus, the storage system must
    provide performant and consistent metadata to ensure smooth operation. At the
    same time, SDPA should limit the number of needed queries. For each of these tensions,
    we found manifold workarounds in the literature that aim to address platform limitations.
    However, we note that most of these workarounds solely address the symptoms of
    the tensions and not the underlying causes, namely missing serverless data processing
    features or storage systems features on the platform side. Features hopefully
    addressed in the next generation of serverless data processing. 2.3. Next-generation
    serverless data processing The model of event-driven scalability and other desirable
    properties of serverless platforms have fueled the development of many serverless
    data processing frameworks (see Table 1). However, a careful review of these frameworks
    also reveals a fundamental issue with current serverless platforms, such as AWS
    Lambda or Google Cloud Functions, which is that these platforms were never designed
    with data processing in mind. Thus, some features and behaviors of these platforms
    create challenges in using their full potential. Using the reference architecture
    (Fig. 1), we more closely identify these tensions within the boundaries between
    the application/platform and platform/platform. From our review of existing literature
    and source code, we see that so far, tensions have only been addressed through
    workarounds on the application level or by finding novel service compositions.
    However, we argue that these tensions must be addressed by both platforms and
    applications to realize this untapped potential. For example, bypassing the event
    system to invoke functions [24] to improve function utilization impacts the platform’s
    ability to predict workloads. Hence, it may improve performance but may impact
    stability. However, if the platform would offer a feature such as used in [24],
    these stability issues could be addressed. This dichotomy of where and how tensions
    are addressed also explains why some tensions, i.e., T5/T7/T8, are not discussed
    at all by current applications, limiting the potential and current qualities of
    SDP. Consequently, we argue that the next generation of serverless data processing
    systems must change both application architectures and serverless platforms (serverless
    compute platforms, storage systems and other composable services) to address these
    tensions. Showcasing the benefits of this application platform co-design [47]
    approach, we present CREW in Section 3. These re-designs also reveal potential
    future serverless platform features that could, with little impact on the current
    business model, be adapted by current serverless platforms. Moreover, we can demonstrate
    that these changes can lead to significant improvements in performance, cost,
    and reliability of serverless data processing applications and start to become
    competitive with traditional data processing systems like Apache Spark. However,
    we must point out that the concrete approach on how to best select appropriate
    re-designs is a complex engineering task [1], [47]. 3. CREW – a next-generation
    serverless data processing system In this section, we introduce and showcase CREW
    – Corral+Whisk: Rapid Elastic data processing with corral and OpenWhisk – A new
    serverless data processing system, combining both platform and application re-designs
    to resolve the most predominant tensions of data processing performance for ad-hoc
    tasks, following closely the reference architecture defined. For that, we first
    briefly introduce CREW, highlighting different re-design options we utilized to
    address the tensions. Then, we present the execution and processing steps that
    enable it to improve on state-of-the-art systems. We conclude this section by
    reviewing current shortcomings and open issues of CREW before presenting a comparative
    evaluation in the next section. However, we want to highlight that CREW is not
    aiming to replace existing, well-adopted solutions such as Lithops or Wukong but
    to demonstrate the potential of applying application platform co-design to serverless
    data processing. 3.1. Addressing tensions through re-designs Based on the reference
    architecture (see Fig. 1) and existing tensions in serverless data processing
    (see Section 2.3), we present a series of re-design options that address them
    through the augmentation of serverless data processing applications and serverless
    platforms. The presented re-designs are an excerpt of re-designs performed in
    the last few years by the authors and several students working on serverless data
    processing [47]. All re-designs were performed on the open-source serverless platform
    OpenWhisk [48] and the serverless data processing framework Corral [34]. For each
    of the following experimental re-designs, we studied several implementation options
    and evaluated the impact of using the re-designed systems for typical serverless
    workloads and data processing workloads, carefully selecting the most promising
    designs to present. We note that the selection of Corral as a starting point was
    mainly motivated by the excellent performance we observed in prior experiments
    [19]. Moreover, the existing architecture of Corral was elegant and simple, allowing
    us to quickly prototype augmentations. While evaluating all solutions presented
    in Table 1, we found that many were not actively maintained or had complex and
    historically grown architectures that would have made fast prototyping more difficult.3
    Similarly, we selected OpenWhisk, as it was one of the few available and, at the
    time, actively maintained open-source serverless platforms that exhibited most
    properties of a serverless system. While other options exist, many use Kubernetes
    as their sole orchestrator, while OpenWhisk also supports direct runc environments
    and custom orchestration, image loading, and caching. Thus, it functions as a
    good proxy compared to AWS Lambdas backend architecture [49], which gives us the
    confidence that the proposed platform re-designs could also be used in commercial
    settings. Batching. Addressing T4 through event batching yielded one of the most
    significant improvements by offering serverless data processing as a better means
    to send events. While serverless computation systems are often designed for web-facing
    tasks, they rarely offer the means to push correlated events at once. However,
    serverless data processing must simultaneously push hundreds or thousands of events
    to start massively parallel processing steps. Thus causing network overhead for
    spawning and observing executions. A common solution for such overheads is the
    batching of events into fewer messages to reduce traffic. However, this can hide
    the complexity of workloads from the platform in case these batches are only processed
    on the function level and also may require additional workarounds like bootstrapping
    [13]. After evaluating several options, we selected a platform-re-design, where
    we modified the OpenWhisk Invocation-API. Here, we introduced the option to send
    multiple invocation events in a new batch-event message. It is still up to the
    developer to choose the size of each batch and the time interval between sending
    batches, and the platform can still impose rate-limiting and throttling logic
    as before. The complexity of adding this feature in OpenWhisk is relatively low,
    as we only need to modify a single component in the controller. Lifecyclehooks.
    The nature of serverless systems implies that resources are often only provisioned
    in response to events. Thus, the cold-start of resources has been identified as
    one of the critical issues for serverless applications [37]. Furthermore, resources
    only execute code during the processing of events, thus requiring that all computations
    such as code management, monitoring, and logging can only happen during the response
    to events [50]. Due to this design, most serverless applications perform computations
    that do not directly contribute to a response, impacting cost and execution times
    (T7, T5). Based on the observation that some of these tasks, e.g., connecting
    to a database or logging, do not need to happen for every event or only need to
    happen for very special life cycle events, e.g., starting a runtime, we implemented
    an extension to the serverless execution model that allows developers to react
    to events outside the normal event-response cycle, expanding on the ideas of Hunhoff
    et al. [51]. Specifically, we define additional, developer-defined code execution
    points that run when a runtime is starting, paused, stopped, or responding to
    events (see Fig. 2). Download : Download high-res image (156KB) Download : Download
    full-size image Fig. 2. Overview of FaaS platform life-cycle (gray) and added
    life-cycle-hooks (orange) and typical function execution (blue). While generally
    beneficial, this change enables data processing applications to preload data on
    runtime creation or write data after job completion instead of doing so during
    every event. Moreover, we could also allow developers to react to execution failures,
    e.g., offering the developer strategies to influence the re-execution of the task.
    This simple yet versatile modification of the serverless programming model offers
    many opportunities to the developer to move code away from the critical execution
    path of a serverless data processing application. However, overly long-running
    life-cycle hooks can increase cold starts and also reserve resources of runtime
    hosts longer. Function hinting. Serverless data processing also creates unique
    demands on computing platforms (T4, T6), specifically, when it comes to the volatile
    number of invocations that are needed to perform processing jobs. Here, cold starts
    due to an unexpected increase in invocations, which can quickly lead to long waiting
    times. However, the number of invocations and computing demand can be predicted
    relatively accurately during the execution of each processing step. Thus, we implemented
    hinting, which allows the driver to communicate associations between running functions
    and future invocations, which can be used to speed up sequences of functions.
    Knowing which runtimes are needed next and in what relative volume, a scheduler
    can extend the pre-warming concept that many platforms use without user intervention.
    Here, we combined part of the life cycle hook implementation and allowed drivers
    to send upcoming execution hints to the platform, including anticipated duration
    per event. The platform would then trigger the freshen life-cycle-hook to perform
    a function warm-up or startup, depending on the state of the requested runtimes.
    For this, we expanded the OpenWhisk controller and the OpenWhisk controller to
    be able to prepare runtimes depending on received hints. Naturally, a production
    system should select careful limits on the type and size of hints to balance the
    ability to remain highly elastic and utilize all resources most effectively. Automatic
    re-configuration. We discussed the uniform, one-size-fits-all model of current
    serverless (FaaS) deployments (T2, T3). An application/framework developer has
    only a limited set of options to address the sizing problem [39]. However, prior
    work in using samples of runtime execution for different configuration variations
    [39] showed promising results in quickly finding suitable runtime configurations
    even for large function compositions, such as they are typical in serverless data
    processing. Here, the sizing problem can be even more drastic for very skewed
    data, where, depending on the data, some functions may be lacking resources while
    others are done far too quickly and cause higher costs than would be needed. Thus,
    we expanded the approach presented in CAT [39] by allowing the sizing of not only
    compositions but also the sizing of the same functions based on input mapping
    by implementing automatic re-configuration. This strategy utilizes two modifications
    of typical serverless data processing systems: (1) we deploy each function for
    the same step in a processing job multiple times to be able to manage skewed data
    by only sizing functions receiving more demanding inputs, and (2) we automate
    the observation of sizing data to quickly select appropriate sizes of all functions
    in a processing composition. The first part of this strategy is implemented on
    the driver side by dividing and deploying jobs across multiple deployments for
    each run. The second part of the strategy is implemented inside OpenWhisk. Here,
    we make use of the fact that a serverless platform, especially for data processing
    workloads, deploys many runtimes for the same function. Thus, we can size a portion
    of deployed runtimes differently to obtain the necessary samples for the CAT-Sizer
    [39]. This way, the serverless processing job quickly (within a few event executions)
    converges to a better-fitting configuration without developer interventions. In
    a production setting, a cloud provider may want to carefully select the number
    of sampling runtimes and also offer means to define the lower and upper bounds
    for runtimes to ensure that the sizeres’ dynamic optimization causes no degradation
    in performance. Feedbackapi. One platform-driven limitation of current serverless
    computing platforms is the way in which drivers can observe the execution functions.
    Essentially, a driver can either (1) perform synchronous invocations, which is
    infeasible at the typical scale of serverless data processing, (2) use asynchronous
    invocations and observe the completion by polling the serverless platform until
    the completion is observed, (3) remove the need for observation by letting functions
    directly call other functions or lastly use platform features such as orchestrates,
    e.g., AWS StepFunctions. Letting functions call others directly can quickly lead
    to run-away computations or make debugging significantly more difficult [40].
    Using the platform options may work in some cases, but it often is limited in
    the possible parallelism and can add significant costs. Thus, most existing frameworks
    use the polling method as it is the quickest and most effective option. However,
    polling also creates a lot of network overhead and stress on the serverless platform.
    To address this issue, we implemented a Feedback API that changes how the completion
    of invocation is communicated to applications (T1, T3). Specifically, the feedback
    API sends a message from the serverless system for each completed invocation (or
    group of invocations) using a callback address. This way, a driver can listen
    to the callbacks instead of implementing any polling. Moreover, allowing to group
    completions based on event type enables the significant reduction of network traffic
    between driver and platform. In a production system, we may need to enable means
    to abuse this system. For the tested serverless data processing applications,
    we could observe significant improvements, especially in combination with event
    batching and Hinting. Storageapi. Serverless data processing generates intermediate
    data that typically requires temporary storage and can be easily recreated. Since
    data objects are typically accessed by only a subset of functions, availability
    only needs to meet job requirements. The scalability of the storage system is
    crucial for effective data processing with parallel tasks. To balance the need
    for intermediate data and ingestion of raw data, many serverless data processing
    approaches utilize different data storage systems for raw and intermediate data
    [19]. However, finding a perfect match is challenging due to issues with data
    size, storage duration, and supported parallel reads (T6-T8). To address this
    challenge, we implemented a transparent StorageAPI, allowing the automatic selection
    of storage services based on runtime needs. The system supports Redis, NTFS, S3,
    and DynamoDB, enabling the driver to switch between them based on observations,
    such as the number and size of intermediate results generated. Future improvements
    could involve alternative algorithms for more backends and fine-grained decisions.
    Even with the simple rules implemented, significant improvements in execution
    speed and throughput were observed. Application aware function scheduling. In
    this design, we use the knowledge about the structure of a data processing application
    to aid task scheduling and pre-warming decisions made by a platform (T3, T4).
    Similar to the Hinting-Design, this strategy enables developers to communicate
    relationships between deployments and access patterns that can be used by a scheduler
    beforehand. We build this design based on a preliminary design by A. Fuerst [52],
    but instead of online predictions, we use two types of application information
    to aid the scheduling and especially the removal of runtimes in favor of others.
    In the first approach, we use information about the needed libraries within a
    deployment package to determine cold-start times. The assumption is that some
    libraries take significantly longer to load due to their size or functionality
    than others. Further, the presence of some libraries strongly correlates with
    initialization needs, such as connecting to databases before a runtime becomes
    truly operational [50]. We can also further include the use of starting and pausing
    life-cycle hooks in this equation, as these also indicate longer cold-start times.
    For the second approach, we focus on knowledge about the composition of a serverless
    application. By knowing how functions interact with each other, we can determine
    how likely a function will be called in the near future based on the last observed
    invocations. With this, long-running complex compositions can ensure that runtimes
    that are needed later are not removed prematurely. Here, we combine the hinting
    approach discussed before to ensure that we always prioritize runtimes that will
    be needed in the near future while removing runtimes that are quick to restart
    and also not needed by any currently running application. 3.2. Implementation
    Fig. 3 shows a high-level overview of CREW components, following the same reference
    architecture design presented in Fig. 1. CREW is divided into four distinct parts,
    see Fig. 3. Firstly, the core driver component, which is an extension of the Corral-Project
    [34]. While the basis of this code already existed, the CREW version contains
    about four times as many lines of code and extends the original functionality
    significantly, making it a fully unique software artifact. It was used as a convenient
    starting point but mainly served to inform early prototyping efforts. Secondly,
    the modified version of OpenWhisk [48] also includes significant code changes
    to the original4 in addition to significant changes to the runtime environments.
    Further, the Storage-API, a set of libraries and interfaces to interact with multiple
    cloud storage backends in the context of CREW without the need to know how to
    deploy and configure them, and the storage sidecar, a set of plugins that can
    dynamically be loaded and used by CREW to configure and deploy storage backends.
    In the following, we describe how each component is implemented, how the re-designs
    from the previous section are integrated into the respective components, and how
    the overall design enables ad-hoc serverless data processing. Download : Download
    high-res image (815KB) Download : Download full-size image Fig. 3. High-level
    component overview of CREW, highlighting all integrated augmentations in comparison
    to the baseline (green, bold). As a first step, a data analyst needs to implement
    a workflow using the map-reduce pattern, typically implemented as a single Golang
    file containing both the map and reduce function and the main function that initializes
    CREW to run the job. The job itself is driven by two things: a config file, which
    contains all the information to deploy all the functions and access the storage
    backend, and tunable values to drive the job. In the following, we describe the
    steps CREW performs to execute such a job on the example of TPC-H Query 6, a simple
    query to count items in a relation in the TPC-H benchmark. Once the main function
    starts, the following six steps are performed to perform the job: Validation First,
    CREW checks the configuration to validate that all the necessary access rights
    are present. In the case of our exemplary query, this includes validating the
    Minio credentials, the OpenWhisk credentials, and the Kubernetes credentials.
    Preparation Next, CREW queries the storage backend, evaluating based on the job
    definition which files are needed to start the job. This step informs the number
    of invocations needed to perform the job. Further, in this step, the StorageAPI
    feature is used to select an appropriate secondary storage backend. Using the
    meta-data information, we might also select parts of the requested data for preloading
    using the OnStart-LifeCycleHook feature. In the running example, that entails
    the discovery of the roughly 120 files needed to read the relation based on the
    total size. Here, a Redis database is selected with no preloading. Deployment
    Once the appropriate strategy for the job is selected, CREW deploys all needed
    components for the job. If we use a secondary storage backend, we instruct the
    storage sidecar to deploy that backend. In parallel, CREW compiles an executable
    version for the targeted serverless platform. In either case, the user-provided
    go file source code is compiled into a binary that can run on the target platform,
    including preloading operations. During compilation, we also inject all needed
    configurations to access the storage backends and other resources of the serverless
    composition. Lastly, after the successful compilation, we might also use the Hint
    feature to prewarm the first set of runtimes before the job is started. Furthermore,
    CREW might deploy more than one compiled binary to enable Automatic Re-configuration
    experiments during execution. In the example, this includes the deployment of
    Redis in the Kubernetes cluster using a Helm client. Further, based on the average
    file size and capability reported by the OpenWhisk platform, we anticipate 128
    parallel invocations. Accordingly, we use 128 as a hint. Execution Finally, the
    job is executed in a stage-by-stage manner by subdividing all input files across
    several invocation events. The number of invocation events is determined by the
    used storage backed, the size of the input data, the maximum concurrency of the
    target platform, and the goal definition in the job config, e.g., if either speed
    or cost should be maximized. Once the number of invocations is determined, the
    invocations are sent to the target platform either using the Batching feature
    or using the Invocation-API of the platform. After performing all invocations
    in a stage, CREW observes the storage backend and serverless platform to wait
    for the completion of a stage. Once the stage is completed, the next stage is
    started. Before starting the next stage, CREW might trigger some Re-configuration,
    send Hints to the platform, or switch the storage backend to a different one based
    on the input data for the next stage. In the case of TPC-H Query 6, we performed
    over 300 map invocations and a single reduce invocation. During the execution,
    CREW reconfigured the functions to use 512 MB of memory and reduced the number
    of parallel invocations to 64, as the throughput of the Minio backend was limited.
    Moreover, it utilized a single-server Redis instance to store the intermediate
    data. Completion Once all stages are completed, CREW can download the final results
    from the storage backend or provide the analyst with a link if the results are
    too large to download. CREW can also be tasked to trigger a post-process step,
    e.g., invocating a different serverless application to generate a set of plots
    from the resulting data. In the running example, the final result is a single
    value immediately downloaded and displayed. Termination Once the job is completed
    and all the results are collected, CREW terminates all deployed components if
    not otherwise specified in the config. Thus, the deployed database, deployed functions,
    and intermediate data are removed. In the case of the TPC-H Query 6 example, we
    remove the Redis deployment and remove the deployed functions. Each step might
    change based on the targeted platforms, supported platform features, and observations
    during the job execution. However, note that CREW is built to be self-contained.
    Thus, all resources needed to perform a processing job are created and removed
    for each run, embracing the ad-hoc nature of serverless data processing. 3.3.
    Limitations CREW already offers performance, cost and scalability advantages for
    ad-hoc data processing. However, CREW uses a rather rudimentary map/reduce interface.
    While this is a very powerful primitive, novice developers will need ample time
    to understand it. A more accessible interface, e.g., a query language or a higher-level
    abstraction such as a SPARK API, could significantly improve the ease of use.
    However, using the existing primitives, a query language could easily be implemented.
    Moreover, error recovery could be improved, in general, and is still an open issue
    in CREW; on the one hand, re-trying a single failed invocation will not significantly
    increase the cost or processing time of a job, but recovery of multiple errors
    will. In its current form, about 30% of CREW’s codebase is dedicated to error
    recovery. With the increasing complexity of the serverless platform, this percentage
    will increase. Here, a more transparent way to deal with errors on the driver
    and platform side could significantly improve code quality and overall performance.
    From the platform perspective, the current implementation still uses very little
    information on the global use of all functions. Consequently, both scheduling
    and load-balancing strategies could be improved to exploit locality and reduce
    throughput bottlenecks. Lastly, regarding error recovery, the current platform
    still offers little observability and troubleshooting capabilities. We investigate
    the integration of tracing into OpenWhisk [40], which reveals that such a feature
    is easily integrated and will not significantly impact the system. Overall, most
    of the platform modifications presented are not specific to OpenWhisk and could
    be applied to other serverless platforms, such as AWS Lambda. We can only assume,
    based on our observations on OpenWhisk, that both the platform impacts and application
    benefits would be similar; however, specific cost and resource models for other
    platforms might hinder adoption. 4. Experiment-driven evaluation In the following,
    we show an evaluation of CREW using the TPC-H benchmark on a comparison of the
    original Corral+Openwhisk, herein referred to as Baseline, and a Spark-Operator
    [53] all using the same four node Kubernetes cluster and a connected high-performance
    minio cluster. CREW will behave as described in the previous sections and select
    these designs based on the job requirements. This evaluation aims to demonstrate
    the benefits of the application platform co-design approach in addressing performance
    and cost issues in serverless data processing. Hence, we compare the performance
    of CREW to the baseline to showcase the improvements that are possible and we
    compare CREW to Spark to show that serverless data processing can be competitive
    with more established tools on the same resources, thus, closing the gap between
    serverless and established tools while gaining the benefits of the serverless
    operation model. Here, we must point out that Spark is a far more mature tool
    than CREW and has been optimized for many years, but Spark-Operators is a fairly
    recent development and is far closer to the serverless data processing model than
    the original Spark implementation. In this evaluation, we focus on the execution
    time, throughput and cost of the queries. When an organization performs queries
    such as in this evaluation on a regular and predictable basis, a dedicated cluster
    will be more cost-effective. However, for ad-hoc queries, where resources are
    only needed for a short time, and for less technical data analysis, serverless
    data processing is far more attractive [18], [19]. Thus, for all following experiments,
    we always tested the performance from a fully cold cluster to target ad-hoc use.
    4.1. Workload & protocol For the evaluation, we use the TPC-H benchmark [54] as
    a workload. TPC-H is a decision support benchmark that measures the performance
    of ad-hoc queries on large data sets, originally designed for relational databases.
    However, the benchmark is also used to evaluate other data processing systems,
    such as Apache Spark [53] and is also used in the serverless domain to evaluate
    serverless data processing systems [19], [55]. For the Baseline and CREW, we implemented
    the TPC-H queries in the map-reduce-paradigm, and for the Spark-Operator, we utilized
    the Spark-SQL library. For the evaluation, we utilize a subset of TPC-H queries
    (see Table B.3) with scaling factors 1 and 10. We limit the scaling factor to
    10 due to the bandwidth within the experiment environment; otherwise, it can become
    a limiting factor, especially for the baseline. We repeat each query five times
    without reusing any deployed resources other than the Minio storage in between
    runs. The order of query execution is randomized to remove any bias. For each
    query, the tools had to deploy all workers in an empty Kubernetes cluster as we
    were interested in the ad-hoc performance. For the Spark-Operator, we must manually
    size, scale, and configure the operator to fit the workload. However, the Spark-Operator
    does not allow using fractional vCPU in Kubernetes. Thus, the available CPU resources
    to the Spark Operator baseline were often higher than both the Baseline and CREW
    experiments. Towards that end, we sized the operator to match as close as possible
    to the maximum resource used by CREW for the same treatment. We also calculated
    the cost for executing each query by assuming the AWS Lambda pricing model for
    each runtime instance and fixed hourly cost for the Redis/Minio instances. 4.2.
    Results In the flowing, we first present the results of the TPC-H queries for
    the selected scaling factors in terms of execution latency and throughput in Fig.
    4 and Fig. 6 respectively. We then present selected results in detail, showcasing
    some of the differences introduced by using the different re-designs in CREW.
    Lastly, we present a summary table typical for the TPC-H benchmark as an extension
    to work presented in Werner et al. [19] in Table 2. In all results, we compare
    the baseline, an unmodified version of OpenWhisk and Corral, CREW and Spark, all
    using the same workload and hardware. Firstly, in Fig. 4, we observe that the
    baseline is well suited for simple queries and able to outperform the Spark-Operator
    in some runs, thus confirming the results of Werner et al. [19]. However, for
    larger data sets, the baseline performance significantly decreases, indicating
    that the baseline implementation suffers from a set of inefficiencies. Here, CREW
    improves the performance over the baseline significantly and, in most runs, even
    outperforms the Spark Operator. For all but the heavy broadcast queries, CREW
    and Spark performed similarly to each other, indicating that the inter-worker
    communication in Spark might be a bottleneck in the Experimental Environment5
    deployment. We examined these observations in more detail in Fig. 5. In the zoomed
    latency figure, we can see that CREW can start executions faster than both the
    baseline and Spark and distribute more work across available workers. For the
    baseline, we can see clear steps in the execution, a sign that the worker idles
    either due to IO operations or slow orchestration. The spark operator uses roughly
    the same amount of worker6 through the execution. Although in Q1, we can see a
    delay between the first stage in the job and the execution across all worker nodes,
    this delay only happens in some runs and is unrelated to cold-start effects, as
    we ensured a warm execution environment for each experiment. We saw such delays
    of 1–20 s in about 10% of the runs, significantly impacting the overall performance
    of the Spark approach. For CREW, the executions started fast and remained high,
    and gaps appeared for write-heavy operations, where the storage backend could
    not cope with incoming requests. However, these gaps are overall relatively small
    (between 0.5–2 s). We assume that deployments on a hyperscaler, e.g., AWS, would
    not suffer similar issues, other than that CREW improves significantly over the
    baseline. Download : Download high-res image (97KB) Download : Download full-size
    image Fig. 4. Aggregated mean job execution time for the TPC-H application workload
    using Spark-Operator, CREW and the baseline for scale factors 1 and 10. Looking
    into the contributing factors for this improvement, we firstly see a significantly
    higher throughput for CREW over the baseline, see Fig. 6. Note here that the maximum
    throughput for the used environment is about 10 Gbps. Since we calculated the
    throughput based on total data read/written during the execution time, we can
    see that CREW is able to operate at about half of this maximum capacity and manages
    to max out this capacity for simple queries with little random read/write access.
    For longer queries, the communication traffic within Kubernetes and OpenWhisk
    reduces the available bandwidth. For broadcast, heavy queries with lots of read
    and write operations (e.g., TPC-H Query 21), the Spark-Operator is able to operate
    at a higher throughput than the other two approaches, as Spark can also exploit
    node locality to share data, skipping the network, thus, having overall improved
    read/write throughput. Download : Download high-res image (263KB) Download : Download
    full-size image Fig. 5. In-depth execution distribution view of TPC-H Query 1
    and TPC-H Query 18 for the TPC-H application workload for scale factor 10 for
    a single run, showing initialization gaps and orchestration/scheduling delays.
    Note how the baseline performs most work at the end due to stragglers in prior
    stages and how Spark needed time to initialize the workers. However, due to the
    fixed worker size and available resources, Spark is not necessarily able to outperform
    CREW even with the higher throughput, as some tasks benefit from higher parallelism,
    contributing to the overall lower execution time of CREW. This higher parallelism
    can also be observed in Fig. 7. Here, we can see that CREW used almost the same
    number of tasks as Spark, but while in Spark, tasks shared a fixed number of workers,
    in CREW, each task runs in a different function, allowing for higher total parallelism.
    This performance difference is also due to the more flexible worker (function)
    allocation in CREW that can spawn a large number of small works at the beginning
    of queries and then uses fewer high-memory functions in later stages. The higher
    memory consumption of CREW in Table 2 for the smaller scaling factor also indicates
    this. However, we point out that the analyst or operator of both Spark and CREW
    can always configure the maximum memory that should be allocated by setting the
    maximum number of workers/functions and the memory per worker. Download : Download
    high-res image (102KB) Download : Download full-size image Fig. 6. Throughput
    overview for the TPC-H application workload using Spark-Operator, CREW and the
    serverless OpenWhisk baseline for scale factors 1 and 10, aggregated for Low,
    Medium, and High IO-intensive queries. We summarize the performance of Spark,
    CREW, and the baseline in Table 2. Here, we can first see that all three platforms
    are sensitive to the data volume (SF). This, however, is more an artifact of the
    used Exp-Env., as we could not scale the network to the storage backend. Download
    : Download high-res image (92KB) Download : Download full-size image Fig. 7. Runtime
    usage for the TPC-H application workload using Spark-Operator, CREW and the serverless
    OpenWhisk baseline for scale factors 1 and 10, aggregated for Low, Medium, and
    High IO-intensive queries, see Table B.3. Nevertheless, CREW can almost perform
    1000 queries per hour for SF = 1 and 130 for SF = 10, indicating that a better-scaled
    storage environment could archive similar performance for SF = 10 as for SF =
    1. We also see that CREW is the most expensive,7 primarily due to the extra costs
    of running a Redis instance. Note that at the same time, CREW never used more
    than two full vCPUs as each function never uses more than 40mili vCPUs for most
    operations, while Spark needed one vCPU per worker at a minimum + 1 vCPU for the
    Spark master. Thus, for On-Prem deployments, where vCPUs might be limited, using
    an approach such as CREW might be more beneficial as fewer CPU resources are committed.
    On the other hand, CREW will consume available memory more aggressively, as evident
    by the high memory usage for SF = 1. At the same time, Spark is very predictable
    as the works need to be preconfigured. However, we argue that the configuration
    limits still allow control of maximum memory consumption while utilizing the available
    resources more effectively. Table 2. Main TPC-H application workload results for
    Spark-Operator, CREW and OpenWhisk (Baseline) for scale factors 1 and 10, expressing
    theoretical queries per hour (QphH), cost per 1000 queries per Hour (cost/kQphH)
    and used resources for each scale factor. Platform Scale-factor QphH cost/kQphH
    [$]* Peak memory allocation[GB] vCPU IO [TB/h] Baseline 1 159 0.08 3 1 1.51 Crew
    1 992 2.50 15 1 5.56 Spark 1 133 0.68 6 2 3.07 Baseline 10 26 3.70 6 3 4.61 Crew
    10 130 6.51 32 2 16.30 Spark 10 32 5.51 32 15 14.33 4.3. Analysis The results
    of the evaluation show that serverless systems can be similarly performant as
    far more advanced platforms such as Apache Spark. The high scalability and flexible
    worker deployment of serverless data processing approaches, in combination with
    high-performance storage options available in cloud environments, makes platforms
    like CREW competitive. Some of the optimizations that Spark developed over time,
    e.g., sorting and partitioning strategies, could also be applied to CREW, likely
    improving the performance further. Concerning development, Spark is still far
    simpler, offering developers many abstraction libraries that let them use SQL
    or simple domain-specific languages (DSL) to define their data processing pipelines.
    However, deploying and operating Spark clusters, even using the simplification
    of Kubernetes, can quickly remove these advantages. A developer must pick the
    precise libraries, operator, and worker images to get a Spark operator to work,
    leading to much complexity. Using any additional libraries, such as the S3 connector,
    forced us to build a custom version of the spark operator and worker images. Further,
    we observed that the deployment, while eased, still required a lot of sizing-related
    tasks removed in the serverless data processing. Some of these issues are due
    to the high complexity of Sparks architecture, thus likely needing to be solved
    in the future. While approaches such as CREW can, over time, develop similar technical
    debt, the benefits of fully automated operational tasks will remain. With regards
    to the available programming abstractions, serverless data processing frameworks
    likely can, over time, adopt similar approaches as established tools such as Spark.
    Based on a detailed analysis of individual queries, we can see that CREW now is
    constrained mainly by the available bandwidth and application behavior, e.g.,
    how the queries are written and how data is shuffled and accessed. Thus, the serverless
    platform itself is not the limiting factor in the execution and performance of
    ad-hoc data processing. Instead, application logic and design now need improvements.
    Overall, serverless data processing proved to be highly performant for ad-hoc
    data processing, especially in on-prem deployments. With the right storage backends,
    scale and performance can be increased to fit the needs of each use-case, especially
    in cloud settings. The design of CREW enables highly elastic scaling, fitting
    the needs of each processing step to the data while allowing data analysts to
    control cost by setting fixed budgets. All modifications in the serverless platform
    (OpenWhisk) did not affect the stability or performance of other applications
    (tenants). Some modifications, such as batching or feedback, introduce increased
    resource demand on the platform side. However, the reduced time applications run
    due to these changes and the reduction in polling requests compensate for these
    resource demands. Still, further improvements both to the application- and platform-
    side can further improve the performance. 5. Related work Serverless computing
    is still an emerging field, with many opportunities and challenges [15]. Within
    the context of serverless, works by Jonas et al. [10] as well as Hellerstein et
    al. [56] and Castro et al. [57] present a foundational understanding of these
    opportunities and challenges. Indeed serverless data processing is especially
    affected by these challenges such as the need for “Serverless Ephemeral Storage
    and Serverless Durable Storage” [10], as well as the challenge that serverless
    functions “can only communicate through an autoscaling intermediary service” [56]
    but also provide specific application-platform re-designs that address these challenges
    in CREW through the FeedbackAPI and StorageAPI. Serverless data processing is
    one of these opportunities, which find wide interest in research and industry
    [9], [13], [16], [24], [25], [27], [36], [58], [59], [60], [61]. Consequently,
    several excellent frameworks and tools to perform serverless data processing (See
    Table 1) have been proposed to enable serverless data processing. Many of them
    found very novel and needed workarounds to address existing challenges not only
    in storage or communication but also for the unique tensions that serverless platforms
    present. Here, novel solutions to tackle increased effects of cold-starts [13]
    or novel solutions of task distribution [24] already present very helpful solutions.
    With CREW, we benefited from these many findings and created yet another serverless
    compute framework; however, rather uniquely, we asked the question if some of
    the platform-driven limitations are necessary and should remain. Thus, resolving
    these workarounds by proving platform-level features to reduce cold-starts (Hinting,
    LifeCycleHooks) and task distribution (Batching, FeedbackAPI). Additionally, many
    practitioners are looking into the limitations of existing serverless platforms
    [37], [62], [63], [64] with the aim to improve and adapt applications to fit the
    serverless model. Hence, these limitations all reveal current tensions in serverless
    platforms that can similarly be addressed at the platform level. While we specifically
    focused on the task of serverless data processing and therefore addressed challenges
    for improved scheduling and deployment sizing in CREW for this use-case, we argue
    that the practice of application platform co-design can be used to address other
    existing limitations. Moreover, several excellent approaches exist to deal with
    limitations in serverless data processing applications [65], [66], [67]. With
    CREW we strongly benefited from these approaches to further reduce the operational
    tasks a data analyst is faced when using cloud-based models. Indeed, we foresee
    that these approaches can and should also be integrated through application platform
    co-design [1] into both serverless platforms not only for the benefit of serverless
    data processing workloads but for serverless applications in general. 6. Conclusion
    Platform-driven limitations and application workloads in serverless data processing
    create significant tensions, resulting in performance and cost impacts. This paper
    presents an initial review of these tensions and proposes a set of re-design options
    to mitigate the cost and performance issues following the application platform
    co-design approach [47]. By expanding engineering efforts to the serverless platform,
    we effectively address these tensions and introduce CREW, an innovative serverless
    data processing framework and computing platform tailored for ad-hoc processing
    [1]. Our evaluation using the TPC-H benchmark demonstrates that CREW outperforms
    conventional serverless data processing systems and performs comparably to Spark
    Operators on the same resources, validating the approach to tackle arising cloud
    computing complexity by also looking at the platform side. Serverless data processing
    offers comparable performance to established methods while significantly reducing
    operational tasks for developers and data analysts. Therefore, it should be considered
    a viable alternative for big data processing. To meet the emerging application
    demand, platforms should adapt and re-design their limitations accordingly. Although
    our focus in this paper was on ad-hoc query processing, other trends in serverless
    computation, such as hardware resource abstraction, may enable serverless systems
    to tackle different processing tasks like machine learning model training and
    hyperparameter optimization [42]. Similar to the evolution of database management
    systems (DBMS) decades ago [68], we anticipate the divergence of commercial serverless
    platforms from the one-size-fits-all model. This will give rise to new platform
    variants that cater to specific application requirements. Moreover, as more fields
    require distributed, scalable, cost-effective, and on-demand solutions, the trend
    of delegating operational, management, and execution tasks to platforms will continue
    to grow. Consequently, there will be a need for diversified serverless platforms
    to accommodate these diverse application needs. Therefore, the development of
    new engineering methods to create such diversified application platforms becomes
    a crucial next step. CRediT authorship contribution statement Sebastian Werner:
    Writing – review & editing, Writing – original draft, Methodology, Formal analysis,
    Data curation, Conceptualization. Stefan Tai: Writing – review & editing, Supervision,
    Resources, Project administration, Funding acquisition. Declaration of competing
    interest The authors declare the following financial interests/personal relationships
    which may be considered as potential competing interests: Sebastian Werner reports
    financial support, article publishing charges, and travel were provided by TU
    Berlin University. Acknowledgments Funded by the European Union (TEADAL, 101070186).
    Views and opinions expressed are however those of the author(s) only and do not
    necessarily reflect those of the European Union. Neither the European Union nor
    the granting authority can be held responsible for them. Appendix A. Systematic
    literature review With the aim of extracting a reference architecture for serverless
    data processing, we performed a systematic literature review (SLR) of serverless
    data processing and serverless platforms. While many systematic literature reviews
    for serverless computing already exists [16], [58], [69], [70], [71], [72] none
    focuses on serverless data processing. For the review, we excluded publications
    published before 2014, as serverless computing was first publicized by the release
    of AWS Lambda in 2014 [10], only consider English peer-reviewed publications that
    were at least two pages in length, exclude all works published outside of scientific
    journals or conferences, and we exclude all secondary studies. However, since
    serverless computing research remains an emerging field, we do make an exception
    for pre-prints if these were already accepted at a conference or journal or by
    authors that published other peer-reviewed work before. In total, the searches
    yielded 484 candidates. After de-duplication and filtering, we had a total of
    323 candidates. From these, we identified a total of 92 as relevant for extraction.
    Within this search, we found 22 general-purpose serverless data processing frameworks
    and a total of 61 data processing applications. While some of these applications
    represent either one-time implementations or implementations of sub-problems,
    others are built to solve generalized data processing queries, map-reduce jobs,
    or generalized encoding/transcoding jobs. Further, we identified four main use
    cases, ranging from typical data analytics tasks such as batch and stream processing,
    machine learning tasks, data encoding/transcoding tasks, and operations automation,
    mostly built on AWS [10]. Table A.2 shows a list of highly relevant publications
    identified by the search. These publications can be separated into four groups
    of contributions in the field of serverless data processing: (1) frameworks that
    enable developers to utilize serverless systems, (2) use-case implementations,
    (3) assessment methods, and (4) contributions proposing new features to address
    platform problems. In terms of use cases for serverless data processing, we identified
    a total of 121, that can be broadly categorized into three groups, with some works
    presenting more than one. The majority of use-cases target: (1) batch and stream
    data processing tasks, consisting of scientific analysis or exploratory data analysis
    using map-reduce jobs, followed by (2) data encoding/transcoding tasks, such as
    video compression, and recently (3) machine learning tasks, such as inference
    workloads and even some model training. A more detailed executed literature study
    can be found in [47] and by reviewing the raw data in the repository.8 Table A.2.
    Excerpt of the search results depicting the most relevant findings. Mete-data
    Year Target platform Approach References Empty Cell AWS GCF MAF ICF/Whisk Other
    Empty Cell Jonas et al. [9] 2017 ✓ Framework Kim et al. [22] 2018 ✓ Framework
    Sampe et al. [13] 2018 ✓ Framework Klimovic et al. [73] 2018 ✓ Feature Lopex et
    al. [74] 2019 ✓ ✓ Framework Fouladi et al. [12] 2019 ✓ Framework Pons et al. [20]
    2019 ✓ ✓ ✓ Feature Perez et al. [75] 2019 ✓ Feature Carver et al. [24] 2019 ✓
    Framework Perez et al. [76] 2019 ✓ Framework Gimenez-Alventos et al. [29] 2019
    ✓ Assessment Müller et al. [14] 2020 ✓ Framework Perron et al. [26] 2020 ✓ Framework
    Goli et al. [77] 2020 ✓ Assessment Sanchez et al. [78] 2020 ✓ Feature Daw et al.
    [79] 2020 ✓ Feature Jain et al. [80] 2020 ✓ Framework Werner et al. [19] 2020
    ✓ Assessment Jarachanthan et al. [81] 2021 ✓ Framework Sampe et al. [35] 2021
    ✓ ✓ ✓ ✓ Framework Appendix B. Experiment setup For the evaluation of CREW, we
    use the following experiment environment and adhere to the recommendations of
    cloud service benchmarking [82] for measurement and evaluation practices. For
    evaluating OpenWhisk-based deployments, we use a Kubernetes-Cluster consisting
    of four worker nodes, a single master, and additional VMs to deploy auxiliary
    services in the same 10 Gbit network. We use Kubernetes in version 1.21.0 and
    base all changes to OpenWhisk on the commit d741c87. For the Spark workload, we
    used Spark-Operator version v1beta2-1.3.7-3.1.1. Specifically, we used an on-premises
    experiment environment (Exp-Env.) consisting of 6 machines; a detailed overview
    of the setup is depicted in Table B.4. For reproduction, any similarity-sized
    Kubernetes cluster should be sufficient. However, it is essential to provide at
    least a capacity for 128 parallel function runtimes9 in OpenWhisk and at least
    32 GB memory for the Minio deployment co-located in the same 10 Gbit network.
    Note that we needed to tune Minio to handle 10Gbit/s traffic for a sustained request
    traffic.10 We select a modified version of the decision support benchmark from
    the Transaction Processing Council for Ad-hoc/decision support (TPC-H) [83]. The
    ad-hoc nature of the queries fits well with the identified workloads, including
    mostly semi-structured data format seen as workloads during the literature review.
    The benchmark is well established with public comparable performance data on many
    different processing systems. Hence, this allows us to compare the performance
    of different serverless implementations with comparable processing systems such
    as Apache Spark. While the benchmark is targeted for Database systems, it has
    been used throughout research and industry to evaluate other data processing systems
    [19], [84], [85], [85], [86]. However, as the benchmark is originally designed
    to evaluate database systems, it also includes some database-specific workload
    elements, such as concurrent updates and modifications that are supposed to run
    in parallel to the processing workload to test the ability of a database to handle
    updates to the raw data as well as transaction management. However, these operations
    would mostly test the storage systems we use (Minio) instead of the serverless
    compute platforms and processing frameworks we are evaluating. Thus, we omit these
    concurrent updates from the benchmark, which is common in evaluating the performance
    of processing systems [19], [84], [85], [85], [86]. We implement all queries listed
    in Table B.3 using CREW in a program called corral-tpch.11 The implementation
    is largely compatible with the original version of Corral and the modified version
    presented, thus allowing for a comparison between an unmodified Corral version
    and CREW. We can trigger different design features and other CREW configurations
    through a config file. Table B.3. Used TPC-H queries, grouped by data volume and
    complexity. Empty Cell Tables Joins Groups Unions In-volume IO Out-volume Label
    Q01 1 1 Mid Low constant D1 Q06 1 Mid Low Q14 2 1 Mid Low Q15 1 1 Mid Low small,
    sparse D2 Q18 3 3 1 Mid High Q02 5 4 1 Low Mid scaling D3 Q11 3 2 1 Low Mid Q17
    2 1 1 High Mid Q21 4 5 1 High High Table B.4. Cluster overview of Exp-Env. connected
    by a 10 Gb/s network. Name CPU Model Cores Mem [GB] Client Xeon E3-1270 v6 4 64
    KMaster Xeon E5–2690 8 32 KNode1 Xeon E5–2690 8 32 Knode2 2x Xeon E5–2430 12 32
    Knode4 Xeon Silver 4114 10 16 Knode5 Xeon Silver 4114 10 16 Minio Xeon Silver
    4114 10 48 Data availability Links to the code and data are part of the manuscript.
    References [1] Werner S., Tai S. Application-Platform Co-design for Serverless
    Data Processing Hacid H., Kao O., Mecella M., Moha N., Paik H.y. (Eds.), Service-Oriented
    Computing, Springer International Publishing, Cham (2021), pp. 627-640 CrossRefView
    in ScopusGoogle Scholar [2] Fragkoulis M., Carbone P., Kalavri V., Katsifodimos
    A. A survey on the evolution of stream processing systems (2020) arXiv:2008.00842
    Google Scholar [3] Berghel H.L. Simplified integration of prolog with rdbms SIGMIS
    Database, 16 (1985), pp. 3-12, 10.1145/2147769.2147770 View in ScopusGoogle Scholar
    [4] Abouzeid A., Bajda-Pawlikowski K., Abadi D., Silberschatz A., Rasin A. Hadoopdb:
    An architectural hybrid of mapreduce and dbms technologies for analytical workloads
    Proc. VLDB Endow, 2 (2009), pp. 922-933, 10.14778/1687627.1687731 View in ScopusGoogle
    Scholar [5] Hahmann M., Hartmann C., Kegel L., Habich D., Lehner W. Big by blocks:
    Modular analytics it Inf. Technol., 58 (2016), pp. 176-185, 10.1515/itit-2016-0003
    View in ScopusGoogle Scholar [6] Apache Software Foundation Apache spark (2014)
    http://spark.apache.org/. (Online; Accessed 21 December 2022) Google Scholar [7]
    M. Zaharia, M. Chowdhury, M.J. Franklin, S. Shenker, I. Stoica, Spark: Cluster
    computing with working sets, in: 2nd USENIX Workshop on Hot Topics in Cloud Computing,
    HotCloud 10, 2010. Google Scholar [8] Amazon Web Services, Inc. AWS emr (2009)
    https://aws.amazon.com/emr/. (Online; Accessed 21 December 2022) Google Scholar
    [9] Jonas E., Pu Q., Venkataraman S., Stoica I., Recht B. Occupy the cloud: Distributed
    computing for the 99 percent Proceedings of the 2017 Symposium on Cloud Computing,
    ACM, New York, NY, USA (2017), pp. 445-451, 10.1145/3127479.3128601 View in ScopusGoogle
    Scholar [10] Jonas E., Schleier-Smith J., Sreekanti V., Tsai C.C., Khandelwal
    A., Pu Q., Shankar V., Carreira J., Krauth K., Yadwadkar N., Gonzales J.E., Popa
    R.A., Stoica I., Patterson D.A. Cloud programming simplified: A berkeley view
    on serverless computing (2019) URL: http://arxiv.org/abs/1812.03651 Google Scholar
    [11] Fox G.C., Ishakian V., Muthusamy V., Slominski A. Status of serverless computing
    and function-as-a-service(faas) in industry and research (2017) CoRR abs/1708.08028.
    URL: http://arxiv.org/abs/1708.08028 Google Scholar [12] Fouladi S., Romero F.,
    Iter D., Li Q., Chatterjee S., Kozyrakis C., Zaharia M., Winstein K. From laptop
    to lambda: Outsourcing everyday jobs to thousands of transient functional containers
    2019 USENIX Annual Technical Conference, USENIX, Renton, WA, USA (2019), pp. 475-488
    URL: https://www.usenix.org/conference/atc19/presentation/fouladi View in ScopusGoogle
    Scholar [13] Sampé G., Sánchez-Artigas P. Serverless data analytics in the IBM
    cloud Proceedings of the 19th International Middleware Conference Industry, ACM,
    New York, NY, USA (2018), pp. 1-8, 10.1145/3284028.3284029 View in ScopusGoogle
    Scholar [14] Müller I., Marroquın R., Alonso G. Lambada: Interactive data analytics
    on cold data using serverless cloud infrastructure Proceedings of the 2020 ACM
    SIGMOD International Conference on Management of Data, ACM, New York, NY, USA
    (2020), pp. 115-130, 10.1145/3318464.3389758 View in ScopusGoogle Scholar [15]
    Kuhlenkamp J., Werner S., Tai S. The IFS and buts of less is more: A serverless
    computing reality check 2020 IEEE International Conference on Cloud Engineering,
    IC2E, IEEE (2020), pp. 154-161 CrossRefView in ScopusGoogle Scholar [16] Leitner
    P., Wittern E., Spillner J., Hummer W. A mixed-method empirical study of function-as-a-service
    software development in industrial practice J. Syst. Softw., 149 (2018), pp. 340-359,
    10.1016/j.jss.2018.12.013 URL: http://www.sciencedirect.com/science/article/pii/S0164121218302735
    Google Scholar [17] Markl V. Mosaics in big data: Stratosphere, apache flink,
    and beyond Proceedings of the 12th ACM International Conference on Distributed
    and Event-Based Systems, Association for Computing Machinery, New York, NY, USA
    (2018), pp. 7-13, 10.1145/3210284.3214344 Google Scholar [18] Werner S., Kuhlenkamp
    J., Klems M., Müller S. Serverless big data processing using matrix multiplication
    as example Proceedings of the IEEE International Conference on Big Data, IEEE,
    Seattle, WA, USA (2018), pp. 358-365, 10.1109/BigData.2018.8622362 View in ScopusGoogle
    Scholar [19] Werner S., Girke R., Kuhlenkamp J. An evaluation of serverless data
    processing frameworks Proceedings of the 2020 Sixth International Workshop on
    Serverless Computing, ACM, New York, NY, USA (2020), pp. 19-24, 10.1145/3429880.3430095
    View in ScopusGoogle Scholar [20] Barcelona-Pons D., Sánchez-Artigas M., París
    G., Sutra P., García-López P. On the faas track: Building stateful distributed
    applications with serverless architectures Proceedings of the 20th International
    Middleware Conference, ACM, New York, NY, USA (2019), pp. 41-54, 10.1145/3361525.3361535
    View in ScopusGoogle Scholar [21] Klein J. Reference Architectures for Big Data
    Systems Carnegie Mellon University, Software Engineering Institute’s Insights
    (blog) (2017) URL: https://insights.sei.cmu.edu/blog/reference-architectures-for-big-data-systems/.
    (Online; Accessed 23 June 2023) Google Scholar [22] Kim Y., Lin J. Serverless
    data analytics with flint 11th International Conference on Cloud Computing, IEEE,
    New York, NY, USA (2018), pp. 451-455, 10.1109/CLOUD.2018.00063 View in ScopusGoogle
    Scholar [23] Pu Q., Venkataraman S., Stoica I. Shuffling, fast and slow: scalable
    analytics on serverless infrastructure 16th USENIX Symposium on Networked Systems
    Design and Implementation, USENIX Association, Boston, MA, USA (2019), pp. 193-206
    URL: https://www.usenix.org/conference/nsdi19/presentation/pu View in ScopusGoogle
    Scholar [24] Carver B., Zhang J., Wang A., Cheng Y. In search of a fast and efficient
    serverless dag engine 2019 IEEE/ACM Fourth International Parallel Data Systems
    Workshop, PDSW (2019), pp. 1-10, 10.1109/PDSW49588.2019.00005 View in ScopusGoogle
    Scholar [25] Sampe J., Garcia-Lopez P., Sanchez-Artigas M., Vernik G., Roca-Llaberia
    P., Arjona A. Toward multicloud access transparency in serverless computing IEEE
    Softw., 38 (2021), pp. 68-74, 10.1109/MS.2020.3029994 View in ScopusGoogle Scholar
    [26] Perron M., Castro Fernandez R., DeWitt D., Madden S. Starling: A scalable
    query engine on cloud functions Proceedings of the 2020 ACM SIGMOD International
    Conference on Management of Data, International Foundation for Autonomous Agents
    and Multiagent Systems, Richland, SC (2020), pp. 131-141, 10.1145/3318464.3380609
    View in ScopusGoogle Scholar [27] Qubole Apache spark on AWS lambda (2017) https://www.qubole.com/blog/spark-on-aws-lambda/.
    (Online; Accessed 21 December 2022) Google Scholar [28] Oliveira F., Suneja S.,
    Nadgowda S., Nagpurkar P., Isci C. Opvis: Extensible, cross-platform operational
    visibility and analytics for cloud Proceedings of the 18th ACM/IFIP/USENIX Middleware
    Conference: Industrial Track, Association for Computing Machinery, New York, NY,
    USA (2017), pp. 43-49, 10.1145/3154448.3154455 View in ScopusGoogle Scholar [29]
    Giménez-Alventosa V., Moltó G., Caballer M. A framework and a performance assessment
    for serverless mapreduce on aws lambda Future Gener. Comput. Syst., 97 (2019),
    pp. 259-274, 10.1016/j.future.2019.02.057 URL: https://www.sciencedirect.com/science/article/pii/S0167739X18325172
    View PDFView articleView in ScopusGoogle Scholar [30] Nahyl O. GitHub repository:
    Ooso (2017) https://github.com/d2si-oss/ooso. (Online; Accessed 21 December 2022)
    Google Scholar [31] J. Carreira, P. Fonseca, A. Tumanov, A. Zhang, R. Katz, A
    case for serverless machine learning, in: Workshop on Systems for ML and Open
    Source Software at NeurIPS, 2018. Google Scholar [32] Pérez A., Moltó G., Caballer
    M., Calatrava A. Serverless computing for container-based architectures Future
    Gener. Comput. Syst., 83 (2018), pp. 50-59 View PDFView articleView in ScopusGoogle
    Scholar [33] Dehury C.K., Srirama S.N., Chhetri T.R. Ccodamic: A framework for
    coherent coordination of data migration and computation platforms Future Gener.
    Comput. Syst., 109 (2020), pp. 1-16, 10.1016/j.future.2020.03.029 URL: https://www.sciencedirect.com/science/article/pii/S0167739X19330924
    View PDFView articleView in ScopusGoogle Scholar [34] Congdon B. GitHub repository:
    corral (2018) http://github.com/bcongdon/corral. (Online; Accessed 26 February
    2019) Google Scholar [35] Sampe J., Sanchez-Artigas M., Vernik G., Yehekzel I.,
    Garcia-Lopez P. Outsourcing data processing jobs with lithops IEEE Trans. Cloud
    Comput. (2021), p. 1, 10.1109/TCC.2021.3129000 View in ScopusGoogle Scholar [36]
    Congdon B. Introducing Corral: A serverless MapReduce framework (2018) http://benjamincongdon.me/blog/2018/05/02/Introducing-Corral-A-Serverless-MapReduce-Framework/.
    (Online; Accessed 26 February 2019) Google Scholar [37] Manner J., Endreß M.,
    Heckel T., Wirtz G. Cold start influencing factors in function as a service Proceedings
    of the 3rd International Workshop on Serverless Computing, IEEE, New York, NY,
    USA (2018), pp. 181-188, 10.1109/UCC-Companion.2018.00054 View in ScopusGoogle
    Scholar [38] Eismann S., Grohmann J., van Eyk E, Herbst N., Kounev S. Predicting
    the costs of serverless workflows ACM/SPEC International Conference on Performance
    Engineering, ACM, New York, NY, USA (2020), 10.1145/3358960.3379133 Google Scholar
    [39] Kuhlenkamp J., Werner S., Tran C.H., Tai S. Synthesizing configuration tactics
    for exercising hidden options in serverless systems International Conference on
    Advanced Information Systems Engineering, Springer (2022), pp. 36-44, 10.1007/978-3-031-07481-3_5
    URL: https://arxiv.org/abs/2205.15904 View in ScopusGoogle Scholar [40] Borges
    M.C., Werner S., Kilic A. FaaSter Troubleshooting - Evaluating Distributed Tracing
    Approaches for Serverless Applications 2021 IEEE International Conference on Cloud
    Engineering, IC2E, IEEE (2021) Google Scholar [41] Kuhlenkamp J., Werner S., Borges
    M.C., E. Tal K., Tai S. An evaluation of FAAS platforms as a foundation for serverless
    big data processing Conference on Utility and Cloud Computing, ACM, New York,
    NY, USA (2019), pp. 1-9, 10.1145/3344341.3368796 View in ScopusGoogle Scholar
    [42] Werner S., Schirmer T. Hardless: A Generalized Serverless Compute Architecture
    for Hardware Processing Accelerators 2022 IEEE International Conference on Cloud
    Engineering, IC2E (2022), pp. 79-84, 10.1109/IC2E55432.2022.00016 View in ScopusGoogle
    Scholar [43] Bermbach D., Karakaya A.S., Buchholz S. Using application knowledge
    to reduce cold starts in faas services 35th ACM/SIGAPP Symposium on Applied Computing,
    ACM, New York, NY, USA (2020), 10.1145/3341105.3373909 Google Scholar [44] Kuhlenkamp
    J., Werner S., Borges M.C., Ernst D. All but One: Faas Platform Elasticity Revisited
    SIGAPP Appl. Comput. Rev, 20 (2020), pp. 5-19, 10.1145/3429204.3429205 Google
    Scholar [45] Schirmer T., Scheuner J., Pfandzelter T., Bermbach D. Fusionize:
    Improving serverless application performance through feedback-driven function
    fusion (2022) arXiv:2204.11533 Google Scholar [46] Sampé J., Sánchez-Artigas M.,
    García-López P., París G. Data-driven serverless functions for object storage
    Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference, ACM, New York,
    NY, USA (2017), pp. 121-133, 10.1145/3135974.3135980 View in ScopusGoogle Scholar
    [47] Werner S. Serverless Data Processing: Application Platfrom Co-Design (Ph.D.
    thesis) TU Berlin (2023) Google Scholar [48] Apache Software Foundation Apache
    openwhisk (2018) http://openwhisk.incubator.apache.org. (Online; Accessed 15 October
    2017) Google Scholar [49] Agache A., Brooker M., Iordache A., Liguori A., Neugebauer
    R., Piwonka P., Popa D.M. Firecracker: Lightweight virtualization for serverless
    applications 17th USENIX Symposium on Networked Systems Design and Implementation,
    USENIX Association, Santa Clara, CA (2020), pp. 419-434 URL: https://www.usenix.org/conference/nsdi20/presentation/agache
    View in ScopusGoogle Scholar [50] Werner S., Kuhlenkamp J., Pallas F., Anders
    N., Mucaj N., Tsaplina O., Schmidt C., Yildirim K. Diminuendo! Tactics in support
    of faas migrations Paasivaara M., Kruchten P. (Eds.), Agile Processes in Software
    Engineering and Extreme Programming – Workshops, Springer International Publishing
    (2020), pp. 125-132, 10.1007/978-3-030-58858-8_13 View in ScopusGoogle Scholar
    [51] Hunhoff E., Irshad S., Thurimella V., Tariq A., Rozner E. Proactive serverless
    function resource management Proceedings of the 2020 Sixth International Workshop
    on Serverless Computing, Association for Computing Machinery, New York, NY, USA
    (2020), pp. 61-66, 10.1145/3429880.3430102 View in ScopusGoogle Scholar [52] Fuerst
    A., Sharma P. Faascache: Keeping serverless computing alive with greedy-dual caching
    Proceedings of the 26th ACM International Conference on Architectural Support
    for Programming Languages and Operating Systems, Association for Computing Machinery,
    New York, NY, USA (2021), pp. 386-400, 10.1145/3445814.3446757 View in ScopusGoogle
    Scholar [53] Foundation A.S. Apache spark on kubernetes (2021) https://spark.apache.org/docs/latest/running-on-kubernetes.html.
    (Online; Accessed 21 December 2022) Google Scholar [54] Counci T.P.P. TPC benchmark
    H standard specification revision 3.0.1 (2022) https://www.tpc.org/tpc_documents_current_versions/pdf/tpc-h_v3.0.1.pdf.
    (Online; Accessed 21 December 2022) Google Scholar [55] Spillner J. Transformation
    of python applications into function-as-a-service deployments (2017) URL: http://arxiv.org/abs/1705.08169.
    unpublished Google Scholar [56] Hellerstein J.M., Faleiro J.M., Gonzalez J.E.,
    Schleier-Smith V., Tumanov A., Wu C. Serverless computing: One step forward, two
    steps back CIDR 2019, 9th Biennial Conference on Innovative Data Systems Research,
    CIDR (2019) URL: http://cidrdb.org/cidr2019/papers/p119-hellerstein-cidr19.pdf
    Google Scholar [57] Castro P., Ishakian V., Muthusamy V., Slominski A. The rise
    of serverless computing Commun. ACM, 62 (2019), pp. 44-54, 10.1145/3368454 View
    in ScopusGoogle Scholar [58] J. Kuhlenkamp, Werner S. Benchmarking FaaS Platforms:
    Call for Community Participation Proceedings of the 3rd International Workshop
    on Serverless Computing, IEEE, Z̃rich, Switzerland (2018), pp. 189-194, 10.1109/UCC-Companion.2018.00055
    Google Scholar [59] Yussupov V., Breitenbücher U., Leymann F., Wurster M. A systematic
    mapping study on engineering function-as-a-service platforms and tools Proceedings
    of the 12th IEEE/ACM International Conference on Utility and Cloud Computing,
    ACM, New York, NY, USA (2019), pp. 229-240, 10.1145/3344341.3368803 View in ScopusGoogle
    Scholar [60] D. Taibi, N. E. Ioini, C. Pahl, J.R.S. Niederkofler, Patterns for
    serverless functions (function-as-a-service): A multivocal literature review,
    in: Proceedings of the 10th International Conference on Cloud Computing and Services
    Science, CLOSER, Research Gate. Preprint, 2020. Google Scholar [61] Scheuner J.,
    Leitner P. Function-as-a-service performance evaluation: A multivocal literature
    review J. Syst. Softw. (2020), Article 110708, 10.1016/j.jss.2020.110708 URL:
    http://www.sciencedirect.com/science/article/pii/S0164121220301527 View PDFView
    articleView in ScopusGoogle Scholar [62] Grambow M., Pfandzelter T., Burchard
    L., Schubert C., Zhao M., Bermbach D. Befaas: An application-centric benchmarking
    framework for faas platforms 2021 IEEE International Conference on Cloud Engineering,
    IC2E, IEEE (2021), pp. 1-8 CrossRefGoogle Scholar [63] Jackson D., Clynch G. An
    investigation of the impact of language runtime on the performance and cost of
    serverless functions Proceedings of the 3rd International Workshop on Serverless
    Computing, IEEE, New York, NY, USA (2018), pp. 154-160, 10.1109/UCC-Companion.2018.00050
    URL: http://ieeexplore.ieee.org/abstract/document/8605773 View in ScopusGoogle
    Scholar [64] van Eyk E., Scheuner J., Eismann S., Abad C.L., Iosup A. Beyond microbenchmarks:
    The spec-rg vision for a comprehensive serverless benchmark 2020 ACM/SPEC International
    Conference on Performance Engineering, ACM, New York, NY, USA (2020), pp. 197-208,
    10.1145/3375555.3384381 Google Scholar [65] P.G. López, Sánchez-Artigas M., Parıs
    G., Pons D.B., Ollobarren A.R., Pinto D.A. Comparison of faas orchestration systems
    Proceedings of the 3rd International Workshop on Serverless Computing, IEEE, New
    York, NY, USA (2018), pp. 148-153, 10.1109/UCC-Companion.2018.00049 Google Scholar
    [66] Gupta V., Carrano D., Yang Y., Shankar V., Courtade T., Ramchandran K. Serverless
    straggler mitigation using local error-correcting codes (2020) arXiv:2001.07490
    Google Scholar [67] Eismann S., Bui L., Grohmann J., Abad C.L., Herbst N., Kounev
    S. Sizeless: Predicting the optimal size of serverless functions (2021) Preprint
    Google Scholar [68] Stonebraker M., Cetintemel U. “One size fits all”: An idea
    whose time has come and gone 21st International Conference on Data Engineering,
    ICDE’05 (2005), pp. 2-11, 10.1109/ICDE.2005.1 View in ScopusGoogle Scholar [69]
    Eismann S., Scheuner J., Van Eyk E., Schwinger M., Grohmann J., Herbst N., Abad
    C., Iosup A. The state of serverless applications: Collection, characterization,
    and community consensus IEEE Trans. Softw. Eng. (2021), p. 1, 10.1109/TSE.2021.3113940
    Google Scholar [70] Hassan H.B., Barakat S.A., Sarhan Q.I. Survey on serverless
    computing J. Cloud Comput., 10 (2021), pp. 1-29 CrossRefGoogle Scholar [71] Yussupov
    V., Breitenbücher U., Leymann F., Wurster M. A systematic mapping study on engineering
    function-as-a-service platforms and tools 12th IEEE/ACM International Conference
    on Utility and Cloud Computing, ACM, New York, NY, USA (2019), pp. 229-240, 10.1145/3344341.3368803
    View in ScopusGoogle Scholar [72] Shafiei H., Khonsari A., Mousavi P. Serverless
    computing: A survey of opportunities, challenges and applications (2019) arXiv
    preprint arXiv:1911.01296 Google Scholar [73] Klimovic A., Wang Y., Stuedi P.,
    Trivedi A., Pfefferle J., Kozyrakis C. Pocket: Elastic ephemeral storage for serverless
    analytics 12th USENIX Conference on Operating Systems Design and Implementation,
    USENIX Association, Berkeley, CA, USA (2018), pp. 427-444 URL: http://dl.acm.org/citation.cfm?id=3291168.3291200
    Google Scholar [74] Garćıa-López P., Sańchez-Artigas M., Shillaker S., Pietzuch
    P., Breitgand D., Vernik G., Sutra P., Tarrant T., Ferrer A.J. Servermix: Tradeoffs
    and challenges of serverless data analytics (2019) arXiv:1907.11465 Google Scholar
    [75] Pérez A., Moltó G., Caballer M., Calatrava A. A programming model and middleware
    for high throughput serverless computing applications Proceedings of the 34th
    ACM/SIGAPP Symposium on Applied Computing, Association for Computing Machinery,
    New York, NY, USA (2019), pp. 106-113, 10.1145/3297280.3297292 View in ScopusGoogle
    Scholar [76] Pérez A., Risco S., Naranjo D.M., Caballer M., Moltó G. On-premises
    serverless computing for event-driven data processing applications 2019 IEEE 12th
    International Conference on Cloud Computing, CLOUD (2019), pp. 414-421, 10.1109/CLOUD.2019.00073
    View in ScopusGoogle Scholar [77] Goli A., Hajihassani O., Khazaei H., Ardakanian
    O., Rashidi M., Dauphinee T. Migrating from monolithic to serverless: A fintech
    case study Companion of the ACM/SPEC International Conference on Performance Engineering,
    ACM, New York, NY, USA (2020), p. 2025, 10.1145/3375555.3384380 Google Scholar
    [78] Sánchez-Artigas M. Eizaguirre G.T., Vernik G., Stuart L., García-López P.
    Primula: A practical shuffle/sort operator for serverless computing Proceedings
    of the 21st International Middleware Conference Industrial Track, Association
    for Computing Machinery, New York, NY, USA (2020), pp. 31-37, 10.1145/3429357.3430522
    Google Scholar [79] Daw N., Bellur U., Kulkarni P. Xanadu: Mitigating cascading
    cold starts in serverless function chain deployments Proceedings of the 21st International
    Middleware Conference, Association for Computing Machinery, New York, NY, USA
    (2020), pp. 356-370, 10.1145/3423211.3425690 View in ScopusGoogle Scholar [80]
    Jain A., Baarzi A.F., Kesidis G., Urgaonkar B., Alfares N., Kandemir M. Splitserve:
    Efficiently splitting apache spark jobs across FAAS and IAAS Proceedings of the
    21st International Middleware Conference, Association for Computing Machinery,
    New York, NY, USA (2020), pp. 236-250, 10.1145/3423211.3425695 View in ScopusGoogle
    Scholar [81] Jarachanthan J., Chen L., Xu F., Li B. Astra: Autonomous serverless
    analytics with cost-efficiency and QOS-awareness 2021 IEEE International Parallel
    and Distributed Processing Symposium, IPDPS (2021), pp. 756-765, 10.1109/IPDPS49936.2021.00085
    View in ScopusGoogle Scholar [82] Bermbach D., Wittern E., Tai S. Cloud Service
    Benchmarking: Measuring Quality of Cloud Services from a Client Perspective Springer
    International Publishing, Cham (2017) Google Scholar [83] Poess M., Floyd C. New
    tpc benchmarks for decision support and web commerce SIGMOD Rec, 29 (2000), pp.
    64-71, 10.1145/369275.369291 View in ScopusGoogle Scholar [84] M. Wawrzoniak,
    R. Müller, G. Alonso, Boxer: Data analytics on network-enabled serverless platforms,
    in: 11th Annual Conference on Innovative Data Systems Research, CIDR 2021, 2021.
    Google Scholar [85] D. Justen, Cost-efficiency and performance robustness in serverless
    data exchange, in: Proceedings of the 2022 International Conference on Management
    of Data, 2022, pp. 2506–2508. Google Scholar [86] T. Bodner, T. Pietz, L.J. Bollmeier,
    D. Ritter, Doppler: Understanding serverless query execution, in: Proceedings
    of the International Workshop on Big Data in Emergent Distributed Environments,
    2022, pp. 1–4. Google Scholar Cited by (0) Sebastian Werner is a senior researcher
    at the Information Systems Engineering chair at TU Berlin, Germany. He completed
    his Ph.D. in Computer Science in 2023 in the area of serverless technology and
    the design of distributed cloud-based applications. Sebastian has long experience
    working on European and national projects and is currently a work package leader
    in Horizon Europe Project TEADAL, focusing on accountability and trustworthiness
    in federated data lakes. His current research focuses on platform-based applications’
    sustainability, maintainability, and trustworthiness. Sebastian is passionate
    about addressing the challenges of the next generation of cloud-computing platforms
    in his research and teaching. Stefan Tai is Full Professor and Head of Chair Information
    Systems Engineering at TU Berlin, Germany (2014-present). Prior to that, he was
    a Full Professor at the Karlsruhe Institute of Technology (2007–2014) and a Research
    Staff Member at the IBM Thomas J.Watson Research Center in New York, USA (1999–2007).
    He also held concurrent posts as Director of research labs and is a member of
    corporate supervisory and advisory boards. He earned his Ph.D. in Computer Science
    from TU Berlin in 1999. Stefan’s research focuses on next-generation, platform-based
    distributed software systems that meet complex system qualities, providing for
    technology, social, and business innovation. Platforms of interest include cloud
    platforms and blockchain networks, and their meaningful combination and interplay.
    ☆ This document is in part published as Werner (2023). 1 Usually in the form of
    managed storage service, such as S3. 2 A different usage as than serving web requests,
    which may lead to increased and even unforeseen overhead on the platform side.
    3 Admittedly, CREW now also is one of those complex systems that would be difficult
    to augment. 4 additions in the codebase (excluding configurations and build files)
    5 See Appendix B. 6 For Spark, we considered each thread as a separate worker,
    even if multiple workers run in parallel on the same deployed worker node. 7 We
    calculated the prices using the AWS Lambda for all function executions and prices
    for m6a.4xlarge instances for the Minio/Redis instances as well as for the spark
    worker instances. Based on prices from April 2022.. 8 https://docs.google.com/spreadsheets/d/1ZIP712dKKmDOnd4oZ77pMg2YiVfn7-P2Q9ESEddTt0U.
    9 Each runtime needs, on average, 512 MB of memory. Thus, around 64 GB of free
    runtime memory after deploying OpenWhisk. 10 We observed that setting Minio to
    allow up to 100000 requests instead of the memory-based configuration works well
    in our experiments. 11 https://github.com/ISE-SMILE/corral-tpc-h . © 2024 The
    Author(s). Published by Elsevier B.V. Part of special issue Serverless computing
    for next-generation application development Edited by Adel N. Toosi, Alexandru
    Iosup, Bahman Javadi, Evgenia Smirni, Schahram Dustdar View special issue Recommended
    articles HyperFlow: A model of computation, programming approach and enactment
    engine for complex distributed workflows Future Generation Computer Systems, Volume
    55, 2016, pp. 147-162 Bartosz Balis View PDF TLR: Traffic-aware load-balanced
    routing for industrial IoT Internet of Things, Volume 25, 2024, Article 101093
    Abdeldjalil Tabouche, …, Abdelmalek Ghefrane Elaziz View PDF A fine-grained robust
    performance diagnosis framework for run-time cloud applications Future Generation
    Computer Systems, Volume 155, 2024, pp. 300-311 Ruyue Xin, …, Zhiming Zhao View
    PDF Show 3 more articles Article Metrics Captures Readers: 1 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Future Generation Computer Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A reference architecture for serverless big data processing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wang Y.
  - Gu H.W.
  - Yin X.L.
  - Geng T.
  - Long W.
  - Fu H.
  - She Y.
  citation_count: '0'
  description: 'Background: Food safety is an important public health issue, and deep
    learning (DL) algorithms can provide powerful tools and methods for food safety
    and authenticity detection. Compared with chemometric algorithms and traditional
    machine learning algorithms, the performances of DL algorithms are improved in
    many aspects. By learning and analyzing a large amount of data, DL models can
    improve the efficiency and accuracy of food safety and authenticity detection,
    helping to ensure the public health and safety. Scope and approach: This paper
    reviews some commonly used chemometric algorithms, traditional machine learning
    algorithms, and popular DL algorithms. Among them, special attentions are paid
    to convolutional neural network (CNN), fully convolutional network (FCN) and generative
    adversarial network (GAN). Moreover, the auxiliary effect of GAN on CNN is highlighted.
    Finally, this paper revisits recent applications of DL algorithms in the field
    of food safety and authenticity detection, and prospects the challenges and future
    directions of DL algorithms in this field. Key findings and conclusions: Although
    DL has made many achievements in the field of food safety and authenticity detection,
    there is still a great potential for development. For example, the data augmentation
    function of GAN can assist CNN to obtain more training samples, thus improving
    the recognition rate. In addition, multimodal neural network (MNN) or multimodal
    attention network (MAN) can be also used to achieve the fusion of data from different
    modalities to further improve the robustness and accuracy of DL algorithms.'
  doi: 10.1016/j.tifs.2024.104396
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Commonly
    used algorithms in food safety and authenticity detection 3. Application of deep
    learning in food safety and authenticity detection 4. Future prospects of deep
    learning in food safety and authenticity detection 5. Conclusions Credit author
    statement Declaration of competing interest Acknowledgements Data availability
    References Show full outline Figures (6) Tables (1) Table 1 Trends in Food Science
    & Technology Volume 146, April 2024, 104396 Deep leaning in food safety and authenticity
    detection: An integrative review and future prospects Author links open overlay
    panel Yan Wang a, Hui-Wen Gu a, Xiao-Li Yin a, Tao Geng a, Wanjun Long b, Haiyan
    Fu b, Yuanbin She c Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.tifs.2024.104396
    Get rights and content Highlights • Some popular DL algorithms in food safety
    and authenticity detection are introduced. • The improvements of DL algorithms
    to chemometric and ML algorithms are summarized. • Recent applications of DL algorithms
    in food safety and authenticity detection are reviewed. • Future prospects of
    DL algorithms in food safety and authenticity detection are discussed. Abstract
    Background Food safety is an important public health issue, and deep learning
    (DL) algorithms can provide powerful tools and methods for food safety and authenticity
    detection. Compared with chemometric algorithms and traditional machine learning
    algorithms, the performances of DL algorithms are improved in many aspects. By
    learning and analyzing a large amount of data, DL models can improve the efficiency
    and accuracy of food safety and authenticity detection, helping to ensure the
    public health and safety. Scope and approach This paper reviews some commonly
    used chemometric algorithms, traditional machine learning algorithms, and popular
    DL algorithms. Among them, special attentions are paid to convolutional neural
    network (CNN), fully convolutional network (FCN) and generative adversarial network
    (GAN). Moreover, the auxiliary effect of GAN on CNN is highlighted. Finally, this
    paper revisits recent applications of DL algorithms in the field of food safety
    and authenticity detection, and prospects the challenges and future directions
    of DL algorithms in this field. Key findings and conclusions Although DL has made
    many achievements in the field of food safety and authenticity detection, there
    is still a great potential for development. For example, the data augmentation
    function of GAN can assist CNN to obtain more training samples, thus improving
    the recognition rate. In addition, multimodal neural network (MNN) or multimodal
    attention network (MAN) can be also used to achieve the fusion of data from different
    modalities to further improve the robustness and accuracy of DL algorithms. Graphical
    abstract Download : Download high-res image (453KB) Download : Download full-size
    image Previous article in issue Next article in issue Keywords Food safety detectionFood
    authenticity detectionMachine learningDeep learningConvolutional neural networkChemometrics
    1. Introduction All human activities are inseparable from energy. As the main
    source of energy for people, food freshness (Wang & Teplitski, 2023), nutritional
    level (Wan et al., 2021), origin (Gokmen, 2023) and year (Yan et al., 2023a) are
    closely related to people''s health. However, driven by economic interests, illegal
    addition, shoddiness and adulteration often occur in the market, which is particularly
    serious in low-income countries (Gwenzi et al., 2023). In recent years, with the
    increase of people''s attention to food quality, food safety detection (Wang et
    al., 2022) and authenticity detection (Mialon et al., 2023) have become a hot
    issue in scientific world. On the one hand, food safety detection can identify
    microorganisms and other contaminants that may be present in food to ensure that
    products in the food supply chain meet safety standards. On the other hand, food
    authenticity detection is concerned with verifying the authenticity, quality,
    and traceability of food to prevent food fraud and the circulation of fake or
    substandard products. Therefore, food safety and authenticity detection is an
    important approach to protect consumers'' rights and interests, which can effectively
    prevent the occurrence of food safety accidents. In general, there are two main
    analytical strategies used for food safety and authenticity detection: targeted
    analysis and untargeted analysis (Amaral, 2021). Targeted analysis focuses on
    detecting a specific component or target substance with high degree of precision
    and specificity. However, it may miss those unknown components, and therefore,
    in such cases we usually turn to untargeted analysis. Compared to targeted analysis,
    untargeted analysis is a more comprehensive approach that aims to capture all
    possible components in analytical samples, including unknown components (Gao et
    al., 2019). Due to the wide variety of food and complex ingredients, a large amount
    of data will be inevitably generated from food samples in the detection process,
    especially in the untargeted detection process (Fisher et al., 2021). In the face
    of such a huge amount of data, how to extract the necessary effective information
    from the data has become a critical issue, and only relying on manual processing
    is no longer the optimal choice. With the breakthrough progress of information
    technology and computer hardware technology, various algorithms have been widely
    used in the field of data processing. Chemometric algorithms such as principal
    component analysis (PCA) (Greenacre et al., 2022), hierarchical cluster analysis
    (HCA) (Ran et al., 2022), partial least squares (PLS) (Mehmood & Ahmed, 2015),
    linear discriminant analysis (LDA) (Tharwat et al., 2017), and traditional machine
    learning algorithms such as support vector machine (SVM) (Cervantes et al., 2020),
    K-nearest neighbor (KNN) (Cunningham & Delany, 2021), decision tree (DT) (Costa
    & Pedreira, 2022), random forest (RF) (Biau & Scornet, 2016) have played an important
    role in mining regularities in complex data. Nevertheless, there are also some
    limitations in these algorithms. For example, in the aspect of feature extraction,
    some chemometric algorithms and traditional machine learning algorithms usually
    need to manually select and extract features, which largely depend on the knowledge
    and experience of domain experts. As a result, the feature selection and extraction
    process is easily limited by subjective factors, so that it cannot fully capture
    the complex patterns and information in the data. Consequently, deep learning
    (DL) (LeCun et al., 2015) has emerged as an new branch of machine learning to
    cope with these limitations. At present, DL algorithms have made significant achievements
    in many fields, such as tumor monitoring (Kanchanamala et al., 2023), disease
    classification (Jalehi & Albaker, 2023) and automatic driving (Hu et al., 2022).
    Common DL models include convolutional neural network (CNN) (Liu et al., 2021),
    fully convolutional network (FCN) (Shelhamer, Long, & Darrell, 2017), generative
    adversarial network (GAN) (Goodfellow et al., 2020), and so on. DL models automatically
    learn feature representations of data through multi-layer neural networks. Compared
    with traditional algorithms that need to manually select and extract features,
    DL models can learn higher-level and more abstract feature representations from
    raw data, which enables it to better capture complex patterns and relationships
    in data without relying on the prior knowledge of domain experts. In recent years,
    significant progress of DL algorithms has been also made in the field of food
    safety and authenticity detection. Through the training of a large number of data,
    DL algorithms can identify and classify different food ingredients, detect harmful
    substances or illegal additives, trace the origin and year of the food. As illustrated
    in Fig. 1, more and more articles regarding the application of DL algorithms in
    food field have been published over the last decade. Regrettably, there are little
    articles systematically summarizing the research progress of DL algorithms in
    the field of food safety and authenticity detection. Therefore, this paper focuses
    on the principle, algorithm process, recent applications, and future prospects
    of current popular DL algorithms in food safety and authenticity detection. The
    purpose of this review is to popularize and promote the research of DL algorithms
    in the field of food safety and authenticity detection, and we look forward to
    more exploration and progress in this field in the future. Download : Download
    high-res image (239KB) Download : Download full-size image Fig. 1. (a) Total number
    of articles regarding the application of DL algorithms in food field over the
    last decade; (b) top 5 countries contributing the most articles regarding the
    application of DL algorithms in food field over the last decade. The data was
    retrieved from Web of Science ranging from 2013-01-01 to 2023-12-31, and the search
    keywords were “Deep Learning” and “Food”. 2. Commonly used algorithms in food
    safety and authenticity detection 2.1. Chemometric algorithms 2.1.1. Principal
    component analysis PCA is an unsupervised analysis technique for multivariate
    data (Greenacre et al., 2022). Its core idea is to map high-dimensional data to
    low-dimensional space by linear transformation, while preserving the maximum variance
    information in the data, thereby revealing the main components of the data. In
    the field of food safety and authenticity detection, we often need to deal with
    a large number of complex data, including the content of various chemical components
    in food and similarity between samples. PCA is widely used in this kind of data
    analysis, which can reduce the dimension of the data and extract key features,
    which helps to deeply understand the internal structure of the data and the correlation
    between the samples (Buve et al., 2022). Therefore, PCA has rich cases in practical
    applications (de Araújo Gomes et al., 2023), such as mold detection in cheese
    (Farrugia et al., 2021) and adulteration detection in honey (Guellis et al., 2020).
    However, PCA still has some limitations in some aspects. For example, it is very
    sensitive to noise and outliers that are common in real-world data, and its robustness
    still needs to be improved (Liu et al., 2019). 2.1.2. Hierarchical cluster analysis
    HCA is a common unsupervised clustering method for multivariate data (Ran et al.,
    2022). The core idea is to construct a hierarchical clustering structure by gradually
    merging or splitting the data points, thus dividing the objects or samples in
    the data set into different clusters. HCA is usually divided into two main types:
    divisive clustering (DC) and agglomerative clustering (AC) (Roux, 2018). In the
    field of food safety and authenticity detection, it is usually necessary to conduct
    in-depth analysis and accurate classification of the relationship between samples
    in order to reasonably classify similar samples into the same category. This step
    is critical to understanding the intrinsic relationships between samples and to
    identifying potential group structures. By improving the similarity between objects
    in the same cluster and reducing the similarity between different clusters, HCA
    finally builds a clear tree-like hierarchy, which helps to show the clustering
    relationship between data points, thus providing researchers with a deep understanding
    of the data hierarchy (da Silva Torres et al., 2006). For this reason, HCA has
    accumulated a wealth of cases in practical applications (Medina et al., 2019),
    such as quality and authenticity detection of spices (Matsushita et al., 2018)
    and adulteration detection of butter (Taylan et al., 2020). However, when dealing
    with large-scale data sets, HCA will not only consume a lot of time, but also
    occupy a lot of resource space, which will limit its application in certain situations
    (Ran et al., 2022). 2.1.3. Partial least squares PLS is a supervised multiple
    regression technique, which aims to solve the modeling problems of multi-collinearity
    and high-dimensional data (Mehmood & Ahmed, 2015). The core idea is to build a
    prediction model by finding the maximum covariance between the input independent
    variable ( ) and the output dependent variable ( ). In the field of food safety
    and authenticity detection, it usually requires to accurately predict the properties
    of food samples, so that we can accurately control the quality of food. PLS can
    link multiple chemical components of food samples to their quality, nutrition
    or other properties, and accurately predict various components in food by considering
    multiple variables comprehensively, providing a reliable tool for quality control
    (Buve et al., 2022). Therefore, PLS has been widely used in food safety and authenticity
    detection (Zhu et al., 2021a), such as the detection of pesticide residues in
    apples and rice (Zhang et al., 2023) and the combination of discriminant analysis
    (DA) to detect the origin of avocado (Jimenez-Carvelo et al., 2021). However,
    with the increasing diversity of real-world data and the wide expansion of fields
    involved, various complex nonlinear problems have emerged. There is still much
    room for improvement in the fitting ability of PLS for these nonlinear problems
    (Zhu et al., 2017). 2.1.4. Linear discriminant analysis LDA is a classical supervised
    feature extraction technique (Tharwat et al., 2017). The core idea is to map high-dimensional
    data to low-dimensional space by linear transformation, so as to maximize inter-class
    scatter between different classes and minimize intra-class scatter within the
    same class, and finally realize effective classification and separation of data.
    In the field of food safety and authenticity detection, it frequently needs to
    accurately classify food samples into different categories for food quality control.
    As one of the most commonly used supervised algorithms in food identification,
    characterization and adulteration detection, LDA considers not only the distribution
    of data, but also the class information, which helps LDA to find those most discriminative
    linear discriminant features, and then realize the accurate classification of
    samples (Esteki et al., 2018). Therefore, LDA has rich cases in practical applications
    (Zhu et al., 2021a), including but not limited to detecting the oxidation degree
    of cooking oil (Xu et al., 2016) and detecting the adulteration of extra virgin
    olive oil (Georgouli et al., 2017). However, LDA assumes that the data distribution
    between categories is linearly separable, which means that if the data presents
    a complex nonlinear structure in high-dimensional space, LDA may not capture this
    structure well and will have poor performance (Li et al., 2023a). 2.2. Traditional
    machine learning algorithms 2.2.1. Support vector machine SVM is a kernel-based
    supervised learning algorithm, which is widely used in classification problems
    (Cervantes et al., 2020). The core idea of the algorithm is to use the kernel
    function to map the samples into a high-dimensional feature space, and build the
    optimal hyperplane in the space, transform the nonlinear problem into a linear
    problem, maximize the margin between different classes, so as to realize the classification
    task. SVM has long been favored in the field of food safety and authenticity detection
    because of its powerful classification ability and adaptability to high-dimensional
    data. Especially in the face of multi-variable data features (e.g., chemical composition
    and sensory attributes), SVM can improve the robustness and accuracy of the model
    by selecting appropriate kernel functions and adjusting hyperparameters (Saha
    & Manickavasagan, 2021). Therefore, SVM has rich cases in practical applications
    (Zhu et al., 2021a), such as the detection of fungal infections in cereals (Lu
    et al., 2020) and the identification of adulteration of cooked millet flour (Shao
    et al., 2018). However, there are some limitations to SVM. It was originally designed
    to solve binary classification problems, while food identification often involves
    the classification of multiple classes. Therefore, if SVM is to be applied to
    multi-classification problems, multiple binary classifiers need to be built, which
    increases the difficulty of optimization (Cervantes et al., 2020). 2.2.2. K-nearest
    neighbor KNN is an instance-based supervised learning algorithm that can be used
    in classification and regression problems (Cunningham & Delany, 2021). Its core
    idea is to classify a new sample into the class with the largest proportion of
    classes among its nearest K training samples by comparing the distance between
    the new sample and the samples in the training data. As a simple, easy to understand
    and implement algorithm, KNN is widely used for the classification and similarity
    analysis of samples in the field of food safety and authenticity detection. By
    comparing the feature vectors of the sample, the algorithm can find the K most
    similar neighbors to the target sample, so as to achieve effective classification
    (Lin et al., 2023). Therefore, KNN has accumulated rich cases in practical applications
    (Zhu et al., 2021a), such as detection of chili plant leaf diseases (Patil & Lad,
    2021) and the detection of chicken cold storage state (Mirzaee-Ghaleh et al.,
    2019). Nevertheless, when dealing with large-scale data and high-dimensional feature
    space, the computational complexity of KNN is huge, and it may face the challenge
    of computational efficiency and storage overhead. Therefore, the algorithm is
    more suitable for problems with small-scale data sets. 2.2.3. Decision tree DT
    algorithm is a supervised learning algorithm that has been used in data mining
    and machine learning (Costa & Pedreira, 2022). The core idea is to partition the
    dataset into different subsets, each corresponding to a DT node, so as to achieve
    accurate prediction or classification of the target variable. The construction
    process of DT aims to select the best split point by maximizing the information
    gain or minimizing the impurity. In the field of food safety and authenticity
    detection, DT has been widely used for feature selection, sample classification,
    and model interpretation. When faced with complex and variable food data, the
    decision tree automatically constructs a tree structure by considering the importance
    of each feature in the sample to capture the complex relationship between the
    samples (Saha & Manickavasagan, 2021). At the same time, this method helps researchers
    to track the classification decision process of samples in real time. Therefore,
    DT has accumulated rich cases in practical applications (Wang et al., 2022), including
    but not limited to the detection of harmful chemicals in food (van Asselt et al.,
    2018) and the evaluation of black tea quality (Ren et al., 2020). However, since
    DT tends to generate branches and leaf nodes in the training data that highly
    match each training sample, the problem of over-fitting becomes serious as the
    dataset gets larger, ultimately leading to its poor performance on unknown data
    (Kotsiantis, 2011). 2.2.4. Random forest RF is a powerful ensemble supervised
    learning technique (Biau & Scornet, 2016). The core idea is to build multiple
    DT models and increase the model diversity through random feature selection and
    sample sampling, so as to reduce the variance of the model and improve the generalization
    performance. Finally, RF combines the prediction results of these DTs by voting
    or averaging to obtain more accurate and robust prediction performance. Similar
    to DT, RF is also widely used in the field of food safety and authenticity detection
    for critical steps such as feature selection and sample classification. Compared
    to a single DT, RF effectively reduces the risk of overfitting, thus reasonably
    grouping similar samples into the same category, providing a highly reliable scheme
    for food detection (Saha & Manickavasagan, 2021). Accordingly, RF has accumulated
    a wealth of cases in practical applications (Wang et al., 2022), including but
    not limited to the determination of honeybee flower sources (Minaei et al., 2017)
    and the detection of fungal infections in strawberries (Siedliska et al., 2018).
    However, RF usually requires a well-defined set of features for training. When
    dealing with unstructured data, such as images, audio or video, researchers need
    to pre-process the original data before applying RF, which will significantly
    reduce the efficiency of research (Zhang et al., 2020). 2.3. Deep learning algorithms
    DL is an important branch of machine learning, and its core lies in the construction
    and training of algorithms based on artificial neural network models. With DL,
    researchers can perform high-level abstraction and complex pattern recognition
    on large-scale data. The core idea of DL is to gradually learn more abstract and
    high-level data representation through multiple layers of nonlinear transformation
    and feature extraction. These representations are able to capture the underlying
    structure and features in the data, which enables effective prediction and classification
    of unlabeled data. As summarized in Fig. 2, compared with chemometric algorithms
    and traditional machine learning algorithms, DL algorithms have several advantages
    that can compensate for their shortcomings. It is mainly manifested in the following
    aspects. (1) In the process of food safety and authenticity detection, the accuracy
    of data is affected by a variety of disturbances, including noise and outliers,
    which can be caused by instrumental errors, reagent contamination or laboratory
    operation errors. For this case, the multi-layer structure and parameter optimization
    process of deep neural networks can alleviate the impact of partial noise and
    outliers by learning a large amount of data. In addition, some auxiliary methods,
    including preprocessing techniques such as data cleaning (Fatima et al., 2017),
    the use of appropriate loss functions (Lohala et al., 2021) and regularization
    techniques (Nusrat & Jang, 2018), can also enhance the robustness of DL algorithms
    to noise and outliers. (2) The data obtained in the process of food safety and
    authenticity detection is often large-scale and high-dimensional, involving multiple
    features and parameters. For example, the use of mass spectrometer to analyze
    food samples may generate data containing thousands of mass spectral peaks, and
    each peak can be regarded as a feature, from which multiple spectra will constitute
    a high-dimensional data. In view of this situation, DL models have the ability
    to automatically learn abstract representations without relying on manually designed
    complex feature extractors. Through the nonlinear transformation and feature extraction
    of multiple hidden layers, DL models can effectively capture the latent structure
    and associations in the large-scale high-dimensional data (Min et al., 2023).
    (3) Food safety and authenticity detection usually involves nonlinear problems,
    which are caused by the nature and complexity of food. For example, determining
    whether a food is fresh may involve multiple factors, such as smell and color,
    and the relationship between these factors may be complex and non-linear. In this
    regard, DL models can learn abstract representations of data through multi-layer
    nonlinear transformations and feature extraction. A deep neural network consists
    of multiple layers of neurons, each of which introduces nonlinearities by introducing
    activation functions, thus enabling the network to learn and represent complex
    nonlinear relationships (Zheng et al., 2020). (4) Multi-classification is also
    very common in food safety and authenticity detection. For example, when distinguishing
    among different varieties or strains of foods, they may be classified into different
    categories such as fruits, vegetables or fish. At this time, softmax activation
    function and cross-entropy loss function are usually used to train and optimize
    DL models. By using the softmax activation function, the model can transform the
    output into a probability distribution to classify among multiple classes; while
    by maximizing the probability of the correct class, the model can classify accurately
    (Pan et al., 2017). (5) In food safety and authenticity detection, we may encounter
    the problem of overfitting. This phenomenon mainly stems from the relatively small
    size of the dataset. For over-fitting problems, DL models can solve them by data
    augmentation, where the main goal of data augmentation is to simulate the potential
    data variation and noise in the real-world by introducing randomness and diversity.
    This process helps to reduce the dependence of the model on specific data, which
    effectively prevents over-fitting (Shorten & Khoshgoftaar, 2019). (6) In the field
    of food safety and authenticity detection, it involves not only traditional structured
    data such as tabular data, but also a large number of unstructured data, such
    as photos or scanned images of food, as well as food labels, descriptions, comments
    and other texts. In this scenario, DL has a variety of models to choose from for
    such unstructured data problems. For image data, CNN can be used, while for text
    data, RNN and natural language processing (NLP) models are suitable choices (Zhang
    et al., 2020). Download : Download high-res image (1MB) Download : Download full-size
    image Fig. 2. Improvements of DL algorithms on the shortcomings of chemometric
    algorithms and traditional machine learning algorithms. 2.3.1. Convolutional neural
    network CNN is widely used to deal with supervised learning problems in the image
    domain. Through convolution operation and pooling operation, it can efficiently
    extract the features of input data, and realize complex feature representation
    and classification tasks by stacking multiple convolutional layers and fully connected
    layers. According to the dimension and type of input data, CNN can be subdivided
    into one-dimensional (1D-CNN), two-dimensional (2D-CNN), three-dimensional convolutional
    neural network (3D-CNN), and so on (Liu et al., 2021). Convolutional layers are
    the core components of CNN, which have the unique ability to capture local patterns
    and spatial structures in input data. In the process of food detection, the local
    join and weight sharing design of convolutional operations allows CNN to more
    effectively identify small changes in food samples. By stacking multiple convolutional
    layers, CNN can not only gradually extract low-level features in the original
    image, such as the shape and texture of the food, but also learn more abstract
    and high-level features, such as the structure and organization of food. The formula
    for the typical 2D convolution operation is as follows (Li et al., 2017): where
    represents the output value of a layer in the neural network; is the index of
    the layer; is the number of feature maps in the layer; and are the locations of
    the output values; denotes the activation function; is the index of the feature
    maps connected to the current feature map in layer , which indicates how many
    feature maps are connected to the current feature map in the previous layer; and
    are the height and width of the convolution kernel, respectively; is the weight
    parameter of the convolution kernel; is the output value of the previous layer
    neurons; and is the bias term. After convolution layers, pooling layers are often
    used to reduce the size of the feature map, reduce the data dimension, and extract
    key feature information. The pooling operation gradually extracts important features
    from the food image and finally passes these features to the fully connected layer
    to perform classification, regression, or other related tasks. Fig. 3 illustrates
    the flowchart of a common convolutional neural network. Download : Download high-res
    image (405KB) Download : Download full-size image Fig. 3. Flowchart of a common
    convolutional neural network. In recent years, a series of classical CNN architectures
    have emerged, including AlexNet, VGGNet, ResNet, and so on (Alzubaidi et al.,
    2021). They can alleviate the problem of vanishing gradients to some extent. In
    addition to the above three architectures, there are many other commonly used
    CNN architectures, such as Inception, MobileNet, Xception, and each architecture
    has its unique characteristics and application scenarios (Bhatt et al., 2021).
    The emergence and design ideas of these networks have had a profound impact on
    the fields of computer vision and DL. In the field of food safety and authenticity
    detection, through in-depth analysis and systematic training of food images, CNN
    is able to accurately identify counterfeit products (Wu et al., 2020), detect
    contaminants (Jiang et al., 2019), and trace key information such as the year
    (Hong et al., 2021) and place of origin of food (Yang et al., 2021a). This provides
    effective decision support for food detection personnel, thus greatly improving
    the efficiency and accuracy of food detection to ensure food quality and safety.
    2.3.2. Fully convolutional network FCN is widely used for semantic segmentation
    tasks in computer vision. It is able to segment the input image at pixel level
    so as to achieve semantic understanding of each pixel in the image. The core idea
    of FCN is to replace the traditional fully connected layer with a fully convolutional
    layer, so that the network can process input images of any size and output segmentation
    results of the corresponding size. This goal is achieved by adding transpose convolutional
    layers (also known as deconvolution layers) after the last few convolutional layers
    of the network. Transpose convolutional layers are able to reverse the convolution
    operation to up sample a low-resolution feature map to the same resolution as
    the original input image (Shelhamer, Long, & Darrell, 2017). Fig. 4 (a) shows
    the structure of FCN. Download : Download high-res image (249KB) Download : Download
    full-size image Fig. 4. The architectures of (a) fully convolutional network and
    (b) generative adversarial network. At present, although FCN is rarely applied
    in food safety and authenticity detection, it has presented good achievements
    in the field of medical image analysis. For example, Roth et al. (2018) successfully
    applied FCN to semantic segmentation of medical images in order to detect smaller
    blood vessels and organs more accurately. This application achieves a satisfactory
    recognition rate, which provides strong support for the research and practice
    in the field of food safety and authenticity detection. By training the FCN model,
    inspectors can effectively segment various information in food images and mark
    bad regions, so as to detect and locate possible food safety problems such as
    foreign bodies, mildew, and contamination. 2.3.3. Generative adversarial network
    GAN is widely used in image generation and inpainting, natural language processing,
    data augmentation and other fields. It consists of two main components, i.e. the
    generator and the discriminator, which compete with each other through an adversarial
    way to promote the training of the model and generate high-quality data samples
    (Goodfellow et al., 2020). The generator and discriminator are neural networks
    that start with separate configurations. The generator network can adopt various
    architectures such as CNN, FCN, and RNN. However, the discriminator network needs
    to contain fully connected layers and ends with a classifier. Fig. 4 (b) shows
    the structure of GAN. In the field of medicine, GAN has obtained remarkable achievements.
    With the continuous breakthrough of medical imaging technology, although the difficulty
    of early tumor detection is gradually reduced, insufficient data is still a serious
    problem. In order to cope with this challenge, Dhivya et al. (2020) used GAN to
    perform data augmentation on conventional datasets, and then combined with CNN
    to classify benign and malignant tumors, which has a significant performance improvement
    compared with directly using the original dataset for classification. Similarly,
    in order to improve the skin cancer detection technology, Sedigh et al. (2019)
    used GAN to enhance the main database, and the experimental results show that
    the model performance is improved by 18% after applying data augmentation. Inspired
    by the above applications, this method can be also transferred to the field of
    food safety and authenticity detection. In food detection, spectral images of
    food are often used for classification, but poor training results are often encountered
    due to insufficient data. In this sense, GAN can be used for data augmentation
    to generate more enhanced images to ensure the training results. It is worth mentioning
    that CNN is still the most commonly used DL algorithm in food safety and authenticity
    detection, and there are few direct studies on FCN and GAN. In addition to the
    three DL algorithms introduced in this paper, recurrent neural network (RNN) (Salehinejad
    et al., 2017) and stacked autoencoder (SAE) (Ng, 2011) also have some corresponding
    applications. Unfortunately, the application of GAN data augmentation strategy
    mentioned above in the field of food safety and authenticity detection is still
    in its infancy, so there is great potential for exploration. 3. Application of
    deep learning in food safety and authenticity detection With the development of
    economy and society, people have paid more attention to food quality than to basic
    food and clothing. Therefore, food safety and authenticity detection has become
    a hot research topic. Food safety detection can be divided into physical hazards
    detection, chemical hazards detection, and biological hazards detection according
    to the types of detected targets (Wang et al., 2022); while food authenticity
    detection can be divided into food adulteration detection and food traceability
    according to the types of counterfeiting (Mialon et al., 2023). Fig. 5 demonstrated
    the application of DL algorithms in the field of food safety and authenticity
    detection. Download : Download high-res image (2MB) Download : Download full-size
    image Fig. 5. Application of DL algorithms in the field of food safety and authenticity
    detection. 3.1. Food safety detection Among the three major categories of food
    safety detection, physical hazards mainly include foreign substances in food such
    as paper scraps and packaging materials; chemical hazards cover food additives,
    pesticide residues, heavy metal pollution and natural toxins; and biological hazards
    usually refer to microorganisms and pests. These factors may pose potential risks
    to the health of consumers, and therefore accurate detection of them is required.
    3.1.1. Physical hazards detection The presence of foreign bodies in food may bring
    a variety of hazards. If the small foreign body is eaten by mistake, it may scratch
    and puncture the digestive tract tissues such as esophagus and stomach, and then
    cause serious problems such as internal bleeding, pain and inflammation. Rong
    et al. (2019) adopted two different CNNs for automatic segmentation of walnut
    images and successfully detected different categories of foreign bodies including
    natural and man-made foreign bodies in walnuts. The results showed that in terms
    of image segmentation, they achieved 99.5% accuracy, while foreign object classification
    achieved 95% accuracy. In addition, on the basis of the original method, the same
    group proposed a segmentation method based on multi-scale residual FCN and a classification
    method based on CNN, which can automatically segment the image and detect foreign
    bodies of different sizes (Rong et al., 2020). The results showed that the proposed
    method can correctly segment 99.4% of the object regions in the test images. Moreover,
    it can correctly detect 96.5% and 100.0 % of the foreign objects in the validation
    and test images, respectively. For foreign bodies in wheat, Shen et al. (2021)
    proposed a fast detection method based on a combination of terahertz spectral
    imaging and CNN. First, they imaged wheat and its impurities using terahertz pseudo-color
    imaging to obtain the dataset. Next, they designed a new CNN called wheat-v2 to
    extract information from the data and perform classification. Finally, they compared
    the designed model with ResNet-v2_50 and ResNet-v2_101 under the same conditions.
    The results showed that the recognition rates of the designed wheat-v2 model on
    the validation sets Top_1 and Top_5 were 97.56% and 98.58%, respectively. These
    results are better than those of the traditional models, which proved that the
    proposed method can effectively identify impurities in wheat. 3.1.2. Chemical
    hazards detection The presence of pesticide residues in food may cause a variety
    of hazards (Wahab et al., 2022). Long-term ingestion of these residues can not
    only have chronic toxic effects on the human body, but may also trigger allergic
    reactions. Jiang et al. (2019) used a machine-vision-based segmentation algorithm
    and hyperspectral techniques to segment the foreground and background regions
    of the apple image, creating a region of interest (ROI) mask for the apple in
    the region with the largest roundness value. Four pesticides and one inactive
    control were used, and the hyperspectral regions of the corresponding sample images
    were extracted by acquiring different types of pesticide residues in the ROI mask.
    At the same time, in order to expand the data set, Gaussian white noise was added
    to the hyperspectral image of each apple, and the images were input into the AlexNet
    model with determined hyperparameters. The final recognition rate of test set
    reaches 99.09%, which is much higher than the traditional KNN algorithm. Zhu et
    al. (2021b) proposed a new method for the analysis of pesticide residues in tea
    by combining surface-enhanced Raman scattering (SERS) with 1D-CNN. They used a
    handheld Raman spectrometer to collect the SERS data and augmented the data by
    a data augmentation strategy. After that, the augmented data was fed into 1D-CNN
    for training. In order to evaluate the performance of 1D-CNN, four different models,
    namely PLS-DA, KNN, SVM and RF, were established and compared. The final results
    showed that 1D-CNN has higher recognition accuracy, which further verified that
    this method has a broad application prospect in the analysis of tea pesticide
    residues. Food additives are chemicals widely used in food processing and have
    many advantages. Proper use of additives can improve food quality, but excessive
    or improper use may cause harm to health. Nitrite is a common food additive, its
    main role is to maintain food color, extend shelf life and improve taste. However,
    excess nitrite may increase the risk of cancer. In order to solve this problem,
    Huang et al. (2022a) proposed a new method, which used polyacrylonitrile-based
    nitrite color sensors (PAN-NSS) to collect data and input these data into the
    deep convolutional neural network (DCNN) model for training. The predicted nitrite
    concentration of food samples such as beef and vegetables was compared with the
    maximum allowable value, so as to classify the qualified food and unqualified
    food. The accuracy of food classification reaches up to 91.33–100%, which exhibited
    good practical value. For the detection of another food additive antioxidants,
    Wu et al. (2023a) used two-dimensional correlation spectroscopy combined with
    CNN to realize the qualitative and quantitative analysis of antioxidants in edible
    oil. In qualitative analysis, the recognition rate of CNN reached 97.56%, which
    is significantly better than that of the PLS-DA model. In terms of quantitative
    analysis, three regression models, PLS, CNN and CNN combined PLS (CNN-PLS), were
    compared. The final results showed that the CNN model performed the best with
    the root mean square error for prediction (RMSEP) of 0.0232. The coefficient of
    determination for prediction ( ) is 0.9703, which proved that the method has great
    potential in the field of antioxidant detection in edible oil. Heavy metal pollution
    in food may cause harm to human health. These heavy metals such as lead, cadmium
    and mercury enter the food chain and long-term ingestion can lead to neurological
    and kidney problems and even cancer (Rehman et al., 2018). Therefore, it is essential
    to detect and control the content of heavy metals in food. Zhang et al. (2022a)
    used hyperspectral imaging (HSI) and chlorophyll fluorescence imaging (CHI-FI)
    techniques to identify rice plants under cadmium and copper stress. They fused
    the NIR spectral data extracted by HSI with the chlorophyll fluorescence kinetic
    curves extracted by CHI-FI, thus establishing a model for detecting the stress
    of harmful substances. In order to improve the recognition rate, a CNN model based
    on high-level fusion was proposed, and the highest recognition rate reached 97.7%.
    This study provides an effective method for plant stress phenotype analysis. Similarly,
    in order to detect cadmium content in lettuce, Xin et al. (2020) proposed a deep
    learning method of SAE combined with partial least squares support vector machine
    regression. By obtaining 1120 visible and near infrared hyperspectral images of
    lettuce leaf samples, the full area spectral data of lettuce leaf samples were
    collected, and finally different spectral preprocessing methods were used to obtain
    data sets for training. The final was 0.9487, RMSEP was 0.01049 mg kg−1, and RPD
    was 3.330. These results show that this method has great potential in the detection
    of heavy metal content in lettuce leaves. Natural toxins are found in all types
    of food and can cause serious health hazards if consumed without proper handling.
    Therefore, ensuring food safety requires the detection and control of natural
    toxins. Among the known mycotoxins, aflatoxins are the most dangerous to human
    health because they have the highest acute and chronic toxicity (Valencia-Quintana
    et al., 2020). Kılıç & İnner (2022) proposed a deep transfer learning based method
    for non-invasively detecting and classifying aflatoxin-contaminated dried figs
    by utilizing images captured under UV light. Four pre-trained networks, DenseNet,
    ResNet, VGG and InceptionNet, were used in the study for model training after
    fine-tuning. The results showed that the DenseNet169 model after fine-tuning exhibits
    the highest accuracy, with the training set accuracy of 98.57% and the test set
    accuracy of 97.50%. Also for aflatoxin, Gao et al. (2021) used 1D-CNN combined
    with hyperspectral imaging technology to successfully construct a high-performance
    model for predicting aflatoxin content in peanuts, corn and mixtures. Under the
    optimal hyperparameters, the 1D-CNN model achieves 96.35%, 92.11% and 94.64% accuracy
    on three different test sets, respectively. This study has positive implications
    for food processing detoxification. 3.1.3. Biological hazards detection Foodborne
    diseases (FBD) are recognized as the leading cause of morbidity and mortality
    worldwide, which belongs to an important group of diseases caused by pathogenic
    bacteria (Todd, 2020). Therefore, people are increasingly concerned about the
    foodborne pathogens contamination. Salmonella is a common bacterium found in the
    natural environment, especially in the digestive systems of animals. When food
    is contaminated by Salmonella, it will cause food poisoning. Zhang et al. (2022b)
    used surface-enhanced Raman spectroscopy (SERS) to detect Salmonella Typhimurium,
    Salmonella Enteritidis and Salmonella Paratyphoid in food and to distinguish between
    live and dead cells of all serotypes. The obtained SERS data were input into stacking-CNN
    for training and the final recognition rate reached 98.69%. Compared with traditional
    microbial methods, stacking-CNN not only has fast recognition speed, but also
    has low complexity. In a recent study, Yu et al. (2023) developed a CNN model
    for successfully analyzing the relationship between the Raman signals of pathogenic
    Vibrio microorganisms and purine metabolites based on label-free SERS technology.
    It achieved a recognition rate of 99.7% in the identification of six typical pathogenic
    Vibrio species within 15 min, which exhibits important application value. Feng
    et al. (2023) combined pseudo-targeted metabolomics with deep learning techniques
    to develop a novel deep semi-quantitative fingerprinting method for the identification
    of Listeria monocytogenes and its major serotypes. They used OPLS-DA to prescreen
    the features, and input the obtained feature data into ResNet for training. The
    prediction accuracies of the resulting model on L. monocytogenes at the serotype
    level and new strain verification set are greater than 99% and 97%, respectively,
    which is expected to be a powerful tool to rapidly and accurately identify pathogen.
    Crop pests have caused widespread harm to agricultural production and farmers’
    livelihoods, and even caused food safety problems. To solve this problem, Cheng
    et al. (2017) proposed a pest identification method based on computer vision technology
    and the deep learning model ResNet-101. They compared the proposed method with
    traditional machine learning algorithm SVM and BP neural networks as well as plain
    deep convolutional neural networks AlexNet. Experimental results showed that the
    recognition rate of the proposed method can reach 98.67% when classifying 10 types
    of crop pest images with complex backgrounds, which is significantly higher than
    those of the traditional machine learning algorithms and AlexNet. This study provides
    a powerful tool for farmers and agronomic scientists to detect insect pests more
    effectively, which in turn contributes to increased agricultural yields. Dai et
    al. (2023) proposed an image and text cross-modal feature fusion ITF-WPI model
    for identifying 17 pests common to wolfberry. They introduced the ODLS network
    constructed by the superposition of 1D-CNN and bidirectional LSTM, which made
    the network have stronger text feature extraction ability than other models. Compared
    with the classical SOTA model, lightweight SOTA model and advanced Transformer
    neural network synthesis, ITF-WPI has an accuracy of 97.98%, which is of great
    value for the research on wolfberry pest control and improving wolfberry yields.
    3.2. Food authenticity detection In the two major categories of food authenticity
    detection, food adulteration detection mainly aims at the problem of mixing low-cost
    or inferior raw materials into food to protect the rights and interests of consumers,
    while food traceability detection is to trace the origin or year of food to ensure
    the quality and credibility of food. 3.2.1. Food adulteration detection Sesame
    oil is a vegetable oil extracted from sesame seeds, and its main components are
    unsaturated fatty acids. In addition, it is also rich in vitamin E, phospholipids
    and other nutrients. Therefore, sesame oil has been widely used in cooking, food
    production, traditional Chinese medicine, beauty and skin care and other fields
    (Langyan et al., 2022). Its price is often higher than those of other edible oils,
    so criminals often mix low-priced oil into sesame oil for profiteering. In response
    to this chaos, Wu et al. (2020) developed a method to identify and quantify counterfeit
    sesame oil using 3D fluorescence and CNN. The pre-trained AlexNet was transferred
    to extract spectral features. Then, SVM and PLS models were established based
    on the extracted features to determine adulterated samples and adulterated levels.
    It was showed that the SVM model could identify the adulterated sesame oil with
    100% accuracy and the PLS model could predict the level of counterfeit sesame
    oil with RMSEPs less than 3%. In addition, the same group proposed a non-targeted
    method for botanical origin identification and adulteration quantification of
    honey based on Raman spectroscopy and CNN (Wu et al., 2022). The results showed
    that the recognition rate of the CNN classification model and chemometric algorithms
    for honey matrix identification is more than 99.76%. However, in terms of quantitative
    aspects, the coefficient of determination ( ) of CNN is higher than 0.95, and
    RMSEP is lower than 4.25, which is better than the chemometric algorithms. Saffon
    is a well-known spice in the food industry and it is one of the spices that are
    sometimes adulterated for gaining more economic profits. To solve this problem,
    Momeny et al. (2023) used a machine vision system based on deep learning to detect
    the authenticity and quality of saffon. A learning-to-augment incorporated Inception-v4
    convolutional neural network (LAII-v4 CNN) was developed for grading and fraud
    detection of saffron in images captured by smartphones. The performance of LAII-v4
    CNN was compared with regular CNN-based models and traditional classifiers such
    as SVM, DT and KNN. The results showed that LAII-v4 CNN with an accuracy of 99.5%
    has achieved the best performance. This finding provides a new method for detecting
    the adulteration of saffron. Milk powder is a powdered product made from fresh
    animal milk or plant raw materials after dehydration and processing. It is often
    used to supplement nutrition for infants and young children. If milk powder is
    adulterated, it will cause serious damage to the health of infants and young children.
    To solve this problem, Huang et al. (2022b) used laser induced breakdown spectroscopy
    (LIBS) in combination with CNN to classify adulterated milk powder, and compared
    it with traditional machine learning methods. It was demonstrated that the average
    accuracy of traditional machine learning methods is 93.9%, while the recognition
    rate of CNN can reach 97.8%. This result proved that LIBS combined with CNN is
    a simple and effective method for detecting the adulteration of milk powder. Mutton
    has become one of the most popular meats in the world because of its rich nutritional
    value and unique flavor. However, due to its increasing market demand and commercial
    value, its adulteration with low-priced meat often occurs. To address this problem,
    Zheng et al. (2021) proposed a method for classification and quantification of
    minced mutton adulteration with pork using thermal imaging and CNN. By tuning
    the hyperparameters, the accuracy of validation and test sets of the qualitative
    CNN model was 99.97% and 99.99%, respectively. In terms of quantitative CNN model,
    the and RMSE of validation and test sets were 0.9933, 0.0251 and 0.9933, 0.0252,
    respectively. These results indicated that the method has great application potential
    in the identification and quantification of mutton-adulteration. Furthermore,
    the same group proposed another method of recurrence plot transformed by spectrum
    combined with CNN for detecting mutton adulteration (Zhang et al., 2022c). To
    verify the effectiveness of the method, the authors used a batch of datasets containing
    pork-adulterated mutton and pure mutton. Results showed that the method could
    separate the adulterated mutton and pure mutton with accuracy more than 99.95%.
    For predicting the adulteration level of pork in mutton, the value of the regression
    model is more than 0.9479. The above results demonstrated the great potential
    of the proposed method in the identification and quantification of mutton adulteration.
    As the hot pot market is constantly developing and competitive, its adulteration
    behavior is also emerging in an endless stream. Some hot pot restaurants use cheap
    duck meat and fat to make fake beef or lamb slices, so as to seek huge profits.
    In view of this chaos, Liu et al. (2023a) proposed a rapid and non-destructive
    method for beef and lamb slice authentication based on the mobile phone photos
    and CNN. It takes a single image as input and uses the texture features of meat
    slices for identity verification. A lightweight convolutional neural network architecture
    named MTx-Net was developed for the authentication and the model accuracy on beef
    and lamb slices was 98.20% and 99.38%, respectively, which exhibits high practical
    value. 3.2.2. Food traceability detection Cumin and fennel are two common spices
    that are widely used in cooking to enhance flavor. However, cumin and fennel produced
    in different regions have significant differences in lipid, phenolic and protein
    content and essential oil composition, leading to large differences in prices
    of cumin and fennel from different regions. Therefore, to avoid the occurrence
    of food fraud, it is necessary to trace the geographical origin of cumin and fennel.
    Yang et al. (2021a) employed NIR spectroscopy coupled with traditional machine
    learning algorithms including PCA-based quadratic discriminant analysis (PCA-QDA)
    and PCA-based multilayer perceptron (PCA-MLP) and DL algorithms (CNN and GAN)
    to identify the origin of cumin and fennel. The spectral data of cumin and fennel
    collected from various regions were used for modeling analysis. The experimental
    results showed that GAN not only overcomes the problem of limited training samples
    in CNN but also solves the problem of unsatisfactory classification results in
    PCA-QDA and PCA-MLP models, which provides a new method for food origin traceability.
    As a common Chinese herbal medicine and food plant, wolfberry is widely recognized
    and endowed with various medicinal values in traditional herbology and Chinese
    medicine (Byambasuren et al., 2019). However, due to the differences in planting
    environment such as soil conditions and climatic factors, the quality and efficacy
    of wolfberry from different regions vary greatly. Dong et al. (2022) used two-dimensional
    correlation spectroscopy (2D-COS) of near-infrared hyperspectral images combined
    with CNN to identify the geographical origin of Chinese wolfberry. In addition,
    three other discrimination models including LDA, PLS-DA and SVM were also established
    for comparison. By using the optimized fusion dataset, the CNN model exhibited
    the best performance among the four models, with the accuracies of 100% and 97.71%
    for calibration and prediction set, respectively. It provides key technical supports
    for the development of Chinese wolfberry industry. Black pepper is a common spice
    that, in addition to providing a spicy taste and unique aroma, has a variety of
    other uses such as stimulating appetite, increasing digestive secretion, and promoting
    intestinal peristalsis. Moreover, it contains some antioxidants that may have
    health benefits. However, the quality and price of black pepper largely depend
    on its geographical origin (Liang et al., 2021). At present, the existing methods
    for detecting the origin of black pepper are not only expensive, but also time-consuming.
    Recently, Wang et al. (2023) proposed a new method for identifying the origin
    of black pepper by synergistically applying an E-tongue (ET), an E-nose (EN) and
    an E-eye (EE) in combination with DL algorithm. Specifically, they used ET, EN,
    and EE to collect data and utilized the Bayesian algorithm to globally optimize
    the hyperparameters of different CNN models. In addition, they introduced the
    channel attention mechanism (CAM) module to realize the feature-level fusion of
    the three kinds of signals, and the final recognition rate reached 99.71%. Compared
    with the use of single signal analysis, this method had better recognition accuracy.
    Black tea is one of the six kinds of tea in China, which is loved by the majority
    of tea lovers. However, with the increasing of storage time, the quality of black
    tea will gradually decrease. In the market, black tea that has been stored for
    a long time may be sold as fresh tea. To solve this problem, Hong et al. (2021)
    used near-infrared hyperspectral imaging combined with DL algorithms including
    CNN, LSTM, and CNN-LSTM to rapidly detect the storage year of black tea. Compared
    with conventional classification methods (LR and SVM), deep learning methods obtained
    better results, proving an efficient alternative for black tea quality inspection.
    Pericarpium Citri Reticulatae (PCR) refers to the outer skin of citrus fruits,
    such as oranges and grapefruit. They are usually orange-yellow or orange-red in
    appearance, while emitting a strong aroma and unique flavor. With the increase
    of storage age, the medicinal value of PCR also increases (Li et al., 2021a).
    However, due to their similar morphology, it is difficult to make an accurate
    distinction. In order to solve this problem, Liu et al. (2023b) used terahertz
    time-domain spectroscopy (THz-TDS) combined with CNN to identify PCR with different
    storage years, and compared it with traditional algorithms (PLS-DA, RF, and LS-SVM).
    The results showed that the CNN model shows the best classification performance
    with an accuracy of 95.63%, which is better than the traditional algorithms. In
    addition to the above detailed examples, the application of DL in the field of
    food safety and authenticity detection has rapidly increased in recent years.
    Table 1 summarizes some other representative applications of DL algorithms in
    this field. Table 1. Representative applications of deep learning algorithms in
    food safety and authenticity detection. Purpose Subject DL model Main result Reference
    Food safety detection (physical hazards) Two-dimensional convolutional neural
    networks and long short-term memory networks combined with hyperspectral imaging
    to identify foreign bodies in walnuts. CNN-LSTM The accuracy reaches 99.0%, better
    than PCA-KNN and SVM. Feng et al. (2022) Deep learning algorithms to detect foreign
    objects in the packaging process of food trays. ResNet-50 The accuracy is higher
    than 94%. Medus et al. (2021) Food safety detection (chemical hazards) One-dimensional
    convolutional neural network combined with RF algorithm to identify pesticide
    residues in black tea. 1D-CNN-RF The recognition rate of the test set reaches
    99.05%. Sun et al. (2022) Short-wave infrared hyperspectral imaging combined with
    one-dimensional convolutional neural network to identify pesticide residues on
    garlic chive leaves. 1D-CNN The test set recognition rate reaches 97.9%, which
    is higher than KNN, RF, SVM and other traditional algorithms. He et al. (2021)
    Ultraviolet spectroscopy combined with deep learning to classify food additives.
    1D-CNN The mean validation accuracy is 93.43% ± 2.01%. Potarniche et al. (2023)
    X-ray absorption spectroscopy combined with deep learning algorithms to detect
    beverage preservatives. DNN The recognition rate of DNN for preservative powder
    reaches 100%, which is better than SVM. Hu et al. (2018) Hyperspectral technology
    combined with deep learning algorithm to detect heavy metal content in lettuce
    Stack convolution autoencoder (SCAE) The of metal Cd is 0.9319, RMSEP is 0.04988
    mg kg−1, and RPD is 3.187; the of metal Pb is 0.9418, RMSEP is 0.04123 mg kg−1,
    and RPD is 3.214. Zhou et al. (2020) Detection of aflatoxin-contaminated figs
    by different wavelengths of light based on deep learning feature extraction. MobileNetV2,
    ResNet101V2, InceptionResNetV2 The accuracy of the detection of contaminated figs
    reached 100%, and the accuracy of the detection of uncontaminated figs reached
    92.3%. Kılıç et al. (2024) Food safety detection (biological hazards) Classification
    of foodborne bacteria by hyperspectral microscope imaging technology coupled with
    convolutional neural networks. 1D-CNN The accuracy is 90%, higher than KNN (81%)
    and SVM (81%). Kang et al. (2020) Raman spectroscopy combined with deep learning
    to analyze different genera of bacteria that may be present in food. CNN The recognition
    rate of CNN reaches 97%, which is higher than 95% for chemometrics. Dib et al.
    (2023) Identifying insects in crops such as rice, wheat, corn, soybeans, sugar
    cane, chickpeas, and potatoes. CNN (Inception-V3, Xception, MobileNet) The highest
    accuracy is 98.81%. Ayan et al. (2020) Deep convolutional neural network to classify
    images of crop pests. DCNN (Fine-tuning based on LeNet-5 and AlexNet networks)
    91% accuracy. Wang et al. (2017) Food authenticity detection (food adulteration)
    Identification and quantification of camellia oil adulteration by excitation-emission
    matrix fluorescence combined with deep learning algorithm. CNN The classification
    accuracy of external prediction set is 92.2% and mean-absolute quantitative errors
    are between 2.6% and 6.7%. Chen et al. (2023) Deep learning algorithm to detect
    turmeric powder adulteration. Improved CNN The accuracy reaches 99.36%, which
    is higher than other classifiers such as SVM and MLP. Jahanbakhshi et al. (2021)
    Detection of Atlantic salmon adulteration by convolutional neural networks combined
    with hyperspectral imaging. SNV-VCPA-IRIV-CNN combination model. The average ,
    RMSEP and RPD are 0.9839, 3.9926 and 8.0251, respectively. Li et al. (2023b) Detection
    of sugar adulteration in honey by convolutional neural networks combined with
    infrared spectroscopy. CNN The accuracy reaches 97.96%, better than LS-SVM and
    PLS-DA. Li et al. (2021b) Two-dimensional correlation spectroscopy combined with
    DL technology to achieve quantitative analysis of honey adulteration. Deep residual
    shrinkage networks (DRSN). The RMSEP of the two types of honey are 3.1166 and
    2.3188, respectively. Wu et al. (2023b) Laser-induced breakdown spectroscopy combined
    with Raman spectroscopy to obtain fish data, and then through the deep learning
    algorithm for fish adulteration detection. Low-level fusion CNN The classification
    accuracy reaches 98.2%, which is higher than that of SVM. Ren et al. (2023) Fast
    detection of adulteration of dairy products (butter) using deep learning method
    CRNN The accuracy rate reaches 82.66%. Iymen et al. (2020) A recurrent neural
    network with Internet of Things is used to detect milk adulteration RNN The accuracy
    rate reaches 92.31%. Nagamalla et al. (2022) Food authenticity detection (food
    traceability) Terahertz time-domain spectroscopy combined with CNN to identify
    citrus peels from different origins. 1D-CNN, 2D-CNN, Add-CNN The recognition rate
    of 1D-CNN is 82.99%, the recognition rate of 2D-CNN is 78.33%, and the recognition
    rate of Add-CNN is the best, reaching 86.17%. Pu et al. (2023) Electronic tongue
    combined with deep learning algorithm to classify wolfberry from different origins.
    CNN The classification accuracy reaches 98.27%. Yang et al. (2019) Deep convolutional
    neural network (DCNN) to identify the seal on the pig skin to trace the origin
    of the pork. DCNN The recognition rate reaches 92.59%, which is better than KNN.
    Song et al. (2019) Identification of wolfberry years by front-face excitation-emission
    matrix fluorescence spectra combined with deep learning. CNN (EEMnet) The accuracy
    is more than 98%. Yan et al. (2023b) The electronic tongue combined with deep
    learning algorithm was used to determine the storage time of Pu-erh tea. 1D-CNN
    The recognition rate of the test set reaches 98.8%. Yang et al. (2021b) 4. Future
    prospects of deep learning in food safety and authenticity detection Compared
    with chemometric algorithms and traditional machine learning algorithms, DL has
    a significant improvement. It has the characteristics of keeping pace with the
    times and inclusiveness. DL can not only improve the efficiency and performance
    of the training process by pre-training the network, but also combine with traditional
    algorithms. For example, the features extracted by the CNN convolutional layers
    can be fed to another classifier, such as an SVM (Wu et al., 2020), which increases
    the interpretability of the algorithm while preventing over-fitting. In recent
    years, in order to improve the convenience of researchers, a variety of deep learning
    frameworks have emerged in the market, such as TensorFlow (Abadi et al., 2016),
    PyTorch (Paszke et al., 2019), and Caffe (Jia et al., 2014). These frameworks
    provide a series of high-level application programming interfaces (API) and tools,
    which make the development and implementation of models easier and more efficient.
    In addition, deep learning frameworks also support parallel computing and distributed
    training, utilizing multiple GPU or multiple machines to accelerate the training
    process. This scalability makes it possible to handle large-scale datasets and
    complex models, and significantly improves the efficiency of training and inference.
    Nevertheless, DL algorithms also have encountered some challenges. First of all,
    despite many software aspects of support, hardware is still a non-negligible issue
    in DL. Large-scale models and complex network structures, high performance computing
    requirements, storage and memory requirements, and parallel computing requirements
    all make DL hardware expensive. Fortunately, with the development and popularity
    of modern technology, hardware prices have gradually decreased, and more affordable
    solutions have emerged. Besides, the rise of cloud computing platforms provides
    users with the option of using high performance hardware resources on demand,
    thus reducing the burden of purchasing expensive hardware (Bisong, 2019). Secondly,
    DL algorithms need a large number of high-quality labeled data for training in
    food safety and authenticity detection, but it is often difficult to obtain such
    a large number of high-quality labeled data. Moreover, due to the diversity and
    complexity of food samples, the problem of class imbalance or missing data may
    usually occur, which will affect the performance of the DL algorithms. At present,
    traditional data augmentation techniques (e.g., rotation, flipping and scaling)
    can expand the limited labeled data set and improve the coverage of data. In addition,
    GAN can also be used to generate realistic data samples, as described in Section
    2.3.3. Different from traditional data augmentation techniques, GAN can generate
    diverse data samples through the generator network, rather than just performing
    geometric or pixel-wise transformations on the original data. This is very valuable
    for data augmentation as it helps the model to better generalize to various data
    situations. Last but not least, DL algorithms also face the challenge of multi-modal
    data fusion in food safety and authenticity detection, that is, how to effectively
    combine multi-modal data (e.g., image, text, audio and video) from different sensors
    or data sources for comprehensive analysis and decision-making. Fortunately, a
    variety of multimodal fusion network structures have been proposed in the field
    of deep learning, such as multimodal neural network (MNN) or multimodal attention
    network (MAN) (Joshi et al., 2021). These networks have achieved remarkable results
    in other areas. Taking the multi-modal biometric identification system in the
    field of security as an example, the system uses many types of data such as face
    images, voice, and fingerprints. Compared with the single-modal framework, it
    achieves more accurate and reliable recognition (Natarajan & Shanthi, 2018). Similarly,
    cross-modal retrieval in multimedia achieves more accurate information retrieval
    by combining image and text data (Ji et al., 2020). These networks are able to
    learn modality weights and adaptively perform weighted fusion of features from
    different modalities, and therefore can be transferred to the field of food safety
    and authenticity detection to obtain a more comprehensive and consistent representation.
    5. Conclusions This paper reviews some popular DL algorithms including CNN, FCN,
    and GAN, and summarizes their applications in food safety and authenticity detection.
    Compared with chemometric algorithms and traditional machine learning algorithms,
    DL algorithms have certain fault tolerance ability to noise and outliers in data,
    and can better suppress the problem of over-fitting and deal with large-scale
    high-dimensional data problems, nonlinear problems, multi-classification problems
    and unstructured data problems. Nevertheless, DL also faces some challenges, such
    as high requirements for hardware, a large number of training samples, and difficulties
    in multi-modal data fusion. Fortunately, several strategies have been proposed
    to address these challenges. What''s more, we highlight an approach that uses
    GAN to generate new realistic data samples to aid CNN training, which, despite
    its application in other domains, holds great promise in food domain. With the
    advancement of DL and hardware technology, we have reason to believe that DL algorithms
    will play an increasingly important role in the field of food safety and authenticity
    detection. Credit author statement Yan Wang: Data curation, Investigation, Software,
    Visualization, Writing - original draft. Hui-Wen Gu: Conceptualization, Funding
    acquisition, Methodology, Writing - original draft. Xiao-Li Yin: Conceptualization,
    Funding acquisition, Validation, Writing - review & editing. Tao Geng: Formal
    analysis, Resources, Validation. Wanjun Long: Formal analysis, Writing - review
    & editing. Haiyan Fu: Conceptualization, Funding acquisition, Supervision, Writing
    - review & editing. Yuanbin She: Conceptualization, Project administration, Supervision,
    Writing - review & editing. Declaration of competing interest The authors declare
    that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgements
    The authors would like to acknowledge the financial supports from the National
    Natural Science Foundation of China (Grant Nos. 32122068, 32371501, 32272409 and
    32001790), and the Natural Science Foundation of Hubei Province (Grant No. 2022CFB161).
    Data availability Data will be made available on request. References Abadi et
    al., 2016 M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
    S. Ghemawat, G. Irving, M. Isard TensorFlow: A system for large-scale machine
    learning. 12th USENIX symposium on operating systems design and implementation
    (2016) Google Scholar Alzubaidi et al., 2021 L. Alzubaidi, J. Zhang, A.J. Humaidi,
    A. Al-Dujaili, Y. Duan, O. Al-Shamma, J. Santamaria, M.A. Fadhel, M. Al-Amidie,
    L. Farhan Review of deep learning: Concepts, CNN architectures, challenges, applications,
    future directions Journal Big Data, 8 (1) (2021), p. 53, 10.1186/s40537-021-00444-8
    View in ScopusGoogle Scholar Amaral, 2021 J.S. Amaral Target and non-target approaches
    for food authenticity and traceability Foods, 10 (1) (2021), p. 172, 10.3390/foods10010172
    View in ScopusGoogle Scholar Ayan, Erbay, & Varçın, 2020 E. Ayan, H. Erbay, F.
    Varçın Crop pest classification with a genetic algorithm-based weighted ensemble
    of deep convolutional neural networks Computers and Electronics in Agriculture,
    179 (2020), p. 105809, 10.1016/j.compag.2020.105809 View PDFView articleView in
    ScopusGoogle Scholar Bhatt et al., 2021 D. Bhatt, C. Patel, H. Talsania, J. Patel,
    R. Vaghela, S. Pandya, …, H. Ghayvat CNN variants for computer vision: History,
    architecture, application, challenges and future scope Electronics, 10 (20) (2021),
    p. 2470, 10.3390/electronics10202470 View in ScopusGoogle Scholar Biau and Scornet,
    2016 G. Biau, E. Scornet A random forest guided tour Test, 25 (2016), pp. 197-227,
    10.1007/s11749-016-0481-7 View in ScopusGoogle Scholar Bisong, 2019 E. Bisong
    Building machine learning and deep learning models on Google cloud platform Springer
    (2019) Google Scholar Buve et al., 2022 C. Buve, W. Saeys, M.A. Rasmussen, B.
    Neckebroeck, M. Hendrickx, T. Grauwet, A. Van Loey Application of multivariate
    data analysis for food quality investigations: An example-based review Food Research
    International, 151 (2022), Article 110878, 10.1016/j.foodres.2021.110878 View
    PDFView articleView in ScopusGoogle Scholar Byambasuren et al., 2019 S.-E. Byambasuren,
    J. Wang, G. Gaudel Medicinal value of wolfberry (Lycium barbarum L.) Journal of
    Medicine Plants Studies, 7 (4) (2019), pp. 90-97 Google Scholar Cervantes et al.,
    2020 J. Cervantes, F. Garcia-Lamont, L. Rodríguez-Mazahua, A. Lopez A comprehensive
    survey on support vector machine classification: Applications, challenges and
    trends Neurocomputing, 408 (2020), pp. 189-215, 10.1016/j.neucom.2019.10.118 View
    PDFView articleView in ScopusGoogle Scholar Chen et al., 2023 A.Q. Chen, H.L.
    Wu, T. Wang, X.Z. Wang, H.B. Sun, R.Q. Yu Intelligent analysis of excitation-emission
    matrix fluorescence fingerprint to identify and quantify adulteration in camellia
    oil based on machine learning Talanta, 251 (2023), Article 123733, 10.1016/j.talanta.2022.123733
    View PDFView articleView in ScopusGoogle Scholar Cheng et al., 2017 X. Cheng,
    Y. Zhang, Y. Chen, Y. Wu, Y. Yue Pest identification via deep residual learning
    in complex background Computers and Electronics in Agriculture, 141 (2017), pp.
    351-356, 10.1016/j.compag.2017.08.005 View PDFView articleView in ScopusGoogle
    Scholar Costa and Pedreira, 2022 V.G. Costa, C.E. Pedreira Recent advances in
    decision trees: An updated survey Artificial Intelligence Review, 56 (5) (2022),
    pp. 4765-4800, 10.1007/s10462-022-10275-5 Google Scholar Cunningham and Delany,
    2021 P. Cunningham, S.J. Delany K-nearest neighbour classifiers - a tutorial ACM
    Computing Surveys, 54 (6) (2021), pp. 1-25, 10.1145/3459665 View in ScopusGoogle
    Scholar da Silva Torres et al., 2006 E.A.F. da Silva Torres, M.L. Garbelotti,
    J.M. Moita Neto The application of hierarchical clusters analysis to the study
    of the composition of foods Food Chemistry, 99 (3) (2006), pp. 622-629, 10.1016/j.foodchem.2005.08.032
    View PDFView articleView in ScopusGoogle Scholar Dai, Fan, & Dewi, 2023 G. Dai,
    J. Fan, C. Dewi ITF-WPI: Image and text based cross-modal feature fusion model
    for wolfberry pest recognition Computers and Electronics in Agriculture, 212 (2023),
    p. 108129, 10.1016/j.compag.2023.108129 View PDFView articleView in ScopusGoogle
    Scholar de Araújo Gomes, Azcarate, Špánik, Khvalbota, & Goicoechea, 2023 A. de
    Araújo Gomes, S.M. Azcarate, I. Špánik, L. Khvalbota, H.C. Goicoechea Pattern
    recognition techniques in food quality and authenticity: A guide on how to process
    multivariate data in food analysis TrAC, Trends in Analytical Chemistry, 164 (2023),
    p. 117105, 10.1016/j.trac.2023.117105 View PDFView articleView in ScopusGoogle
    Scholar Dhivya et al., 2020 S.M. Dhivya, K. S, S. S, M. R GAN based data Augmentation
    for enhanced tumor classification 2020 4th international conference on computer,
    communication and signal processing (2020) Google Scholar Dib et al., 2023 O.H.
    Dib, A. Assaf, E. Grangé, J.F. Morin, C.B.Y. Cordella, G. Thouand Automatic recognition
    of food bacteria using Raman spectroscopy and chemometrics: A comparative study
    of multivariate models Vibrational Spectroscopy, 126 (2023), p. 103535, 10.1016/j.vibspec.2023.103535
    View PDFView articleView in ScopusGoogle Scholar Dong et al., 2022 F. Dong, J.
    Hao, R. Luo, Z. Zhang, S. Wang, K. Wu, M. Liu Identification of the proximate
    geographical origin of wolfberries by two-dimensional correlation spectroscopy
    combined with deep learning Computers and Electronics in Agriculture, 198 (2022),
    p. 107027, 10.1016/j.compag.2022.107027 View PDFView articleView in ScopusGoogle
    Scholar Esteki et al., 2018 M. Esteki, Z. Shahsavari, J. Simal-Gandara Use of
    spectroscopic methods in combination with linear discriminant analysis for authentication
    of food products Food Control, 91 (2018), pp. 100-112, 10.1016/j.foodcont.2018.03.031
    View PDFView articleView in ScopusGoogle Scholar Farrugia et al., 2021 J. Farrugia,
    S. Griffin, V.P. Valdramidis, K. Camilleri, O. Falzon Principal component analysis
    of hyperspectral data for early detection of mould in cheeselets Current Research
    in Food Science, 4 (2021), pp. 18-27, 10.1016/j.crfs.2020.12.003 View PDFView
    articleView in ScopusGoogle Scholar Fatima et al., 2017 A. Fatima, N. Nazir, M.G.
    Khan Data cleaning in data warehouse: A survey of data pre-processing techniques
    and tools International Journal of Information Technology and Computer Science,
    9 (3) (2017), pp. 50-61, 10.5815/ijitcs.2017.03.06 Google Scholar Feng et al.,
    2022 Z. Feng, W. Li, D. Cui Detection of endogenous foreign bodies in Chinese
    hickory nuts by hyperspectral spectral imaging at the pixel level International
    Journal of Agricultural and Biological Engineering, 15 (2) (2022), pp. 204-210,
    10.25165/j.ijabe.20221502.6881 View in ScopusGoogle Scholar Feng et al., 2023
    Y. Feng, X. Wei, M. Chen, H. Zhu, J. Zhang, Y. Zhang, L. Xue, L. Huang, G. Chen,
    Q. Gu, S. Wu, Y. Ding, Q. Wu Semiquantitative fingerprinting based on pseudotargeted
    metabolomics and deep learning for the identification of Listeria monocytogenes
    and its major serotypes Analytical Chemistry, 95 (15) (2023), pp. 6218-6226, 10.1021/acs.analchem.2c02554
    View in ScopusGoogle Scholar Fisher, Croley, & Knolhoff, 2021 C.M. Fisher, T.R.
    Croley, A.M. Knolhoff Data processing strategies for non-targeted analysis of
    foods using liquid chromatography/high-resolution mass spectrometry TrAC, Trends
    in Analytical Chemistry, 136 (2021), p. 116188, 10.1016/j.trac.2021.116188 View
    PDFView articleView in ScopusGoogle Scholar Gao et al., 2019 B. Gao, S.E. Holroyd,
    J.C. Moore, K. Laurvick, S.M. Gendel, Z. Xie Opportunities and challenges using
    non-targeted methods for food fraud detection Journal of Agricultural and Food
    Chemistry, 67 (31) (2019), pp. 8425-8430, 10.1021/acs.jafc.9b03085 View in ScopusGoogle
    Scholar Gao et al., 2021 J. Gao, L. Zhao, J. Li, L. Deng, J. Ni, Z. Han Aflatoxin
    rapid detection based on hyperspectral with 1D-convolution neural network in the
    pixel level Food Chemistry, 360 (2021), Article 129968, 10.1016/j.foodchem.2021.129968
    View PDFView articleView in ScopusGoogle Scholar Georgouli et al., 2017 K. Georgouli,
    J. Martinez Del Rincon, A. Koidis Continuous statistical modelling for rapid detection
    of adulteration of extra virgin olive oil using mid infrared and Raman spectroscopic
    data Food Chemistry, 217 (2017), pp. 735-742, 10.1016/j.foodchem.2016.09.011 View
    PDFView articleView in ScopusGoogle Scholar Gokmen, 2023 V. Gokmen Importance
    of food authentication and origin testing Food Chemistry X, 18 (2023), Article
    100708, 10.1016/j.fochx.2023.100708 View PDFView articleView in ScopusGoogle Scholar
    Goodfellow et al., 2020 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, …, Y. Bengio Generative adversarial networks Communications of the ACM,
    63 (3) (2020), pp. 139-144, 10.1145/3422622 View in ScopusGoogle Scholar Greenacre
    et al., 2022 M. Greenacre, P.J.F. Groenen, T. Hastie, A.I. D’Enza, A. Markos,
    E. Tuzhilina Principal component analysis Nature Reviews Methods Primers, 2 (1)
    (2022), p. 100, 10.1038/s43586-022-00184-w View in ScopusGoogle Scholar Guellis
    et al., 2020 C. Guellis, D.C. Valério, G.G. Bessegato, M. Boroski, J.C. Dragunski,
    C.A. Lindino Non-targeted method to detect honey adulteration: Combination of
    electrochemical and spectrophotometric responses with principal component analysis
    Journal of Food Composition and Analysis, 89 (2020), p. 103466, 10.1016/j.jfca.2020.103466
    View PDFView articleView in ScopusGoogle Scholar Gwenzi et al., 2023 W. Gwenzi,
    Z. Makuvara, J. Marumure, T.T. Simbanegavi, S.S. Mukonza, N. Chaukura Chicanery
    in the food supply chain! Food fraud, mitigation, and research needs in low-income
    countries Trends in Food Science & Technology, 136 (2023), pp. 194-223, 10.1016/j.tifs.2023.03.027
    View PDFView articleView in ScopusGoogle Scholar He et al., 2021 W. He, H. He,
    F. Wang, S. Wang, R. Lyu Non-destructive detection and recognition of pesticide
    residues on garlic chive (Allium tuberosum) leaves based on short wave infrared
    hyperspectral imaging and one-dimensional convolutional neural network Journal
    of Food Measurement and Characterization, 15 (5) (2021), pp. 4497-4507, 10.1007/s11694-021-01012-7
    View in ScopusGoogle Scholar Hu, Kong, Zhang, & Liu, 2022 J. Hu, H. Kong, Q. Zhang,
    R. Liu Enhancing scene understanding based on deep learning for end-to-end autonomous
    driving Engineering Applications of Artificial Intelligence, 116 (2022), p. 105474,
    10.1016/j.engappai.2022.105474 View PDFView articleView in ScopusGoogle Scholar
    Hu et al., 2018 W. Hu, S. Chen, Y. Li, Q. Wang, Z. Fang X-ray absorption spectrum
    combined with deep neural network for on-line detection of beverage preservatives
    Review of Scientific Instruments, 89 (10) (2018), Article 103108, 10.1063/1.5048281
    View in ScopusGoogle Scholar Hong, Zhang, Kong, Qi, & He, 2021 Z. Hong, C. Zhang,
    D. Kong, Z. Qi, Y. He Identification of storage years of black tea using near-infrared
    hyperspectral imaging with deep learning methods Infrared Physics & Technology,
    114 (2021), p. 103666, 10.1016/j.infrared.2021.103666 View PDFView articleView
    in ScopusGoogle Scholar Huang et al., 2022b W. Huang, L. Guo, W. Kou, D. Zhang,
    Z. Hu, F. Chen, …, W. Cheng Identification of adulterated milk powder based on
    convolutional neural network and laser-induced breakdown spectroscopy Microchemical
    Journal, 176 (2022), p. 107190, 10.1016/j.microc.2022.107190 View PDFView articleView
    in ScopusGoogle Scholar Huang et al., 2022 Z.J. Huang, J.Y. Luo, F.Y. Zheng, S.X.
    Li, F.J. Liu, L.X. Lin, Y.J. Huang, S. Man, G.X. Cao, X.G. Huang Long-term stable,
    high accuracy, and visual detection platform for In-field analysis of nitrite
    in food based on colorimetric test paper and deep convolutional neural networks
    Food Chemistry, 373 (Pt B) (2022), Article 131593, 10.1016/j.foodchem.2021.131593
    View PDFView articleView in ScopusGoogle Scholar Iymen, Tanriver, Hayirlioglu,
    & Ergen, 2020 G. Iymen, G. Tanriver, Y.Z. Hayirlioglu, O. Ergen Artificial intelligence-based
    identification of butter variations as a model study for detecting food adulteration
    Innovative Food Science & Emerging Technologies, 66 (2020), p. 102527, 10.1016/j.ifset.2020.102527
    View PDFView articleView in ScopusGoogle Scholar Jahanbakhshi et al., 2021 A.
    Jahanbakhshi, Y. Abbaspour-Gilandeh, K. Heidarbeigi, M. Momeny A novel method
    based on machine vision system and deep learning to detect fraud in turmeric powder
    Computers in Biology and Medicine, 136 (2021), Article 104728, 10.1016/j.compbiomed.2021.104728
    View PDFView articleView in ScopusGoogle Scholar Jalehi & Albaker, 2023 M.K. Jalehi,
    B.M. Albaker Highly accurate multiclass classification of respiratory system diseases
    from chest radiography images using deep transfer learning technique Biomedical
    Signal Processing and Control, 84 (2023), p. 104745, 10.1016/j.bspc.2023.104745
    View PDFView articleView in ScopusGoogle Scholar Ji et al., 2020 Z. Ji, H. Wang,
    J. Han, Y. Pang Sman: Stacked multimodal attention network for cross-modal image–text
    retrieval IEEE Transactions on Cybernetics, 52 (2) (2020), pp. 1086-1097, 10.1109/TCYB.2020.2985716
    Google Scholar Jia et al., 2014 Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,
    J. Long, R. Girshick, S. Guadarrama, T. Darrell Caffe: Convolutional architecture
    for fast feature embedding Proceedings of the 22nd ACM international conference
    on Multimedia (2014) Google Scholar Jiang et al., 2019 B. Jiang, J. He, S. Yang,
    H. Fu, T. Li, H. Song, D. He Fusion of machine vision technology and AlexNet-CNNs
    deep learning network for the detection of postharvest apple pesticide residues
    Artificial Intelligence in Agriculture, 1 (2019), pp. 1-8, 10.1016/j.aiia.2019.02.001
    View PDFView articleView in ScopusGoogle Scholar Jimenez-Carvelo et al., 2021
    A.M. Jimenez-Carvelo, S. Martin-Torres, F. Ortega-Gavilan, J. Camacho PLS-DA vs
    sparse PLS-DA in food traceability. A case study: Authentication of avocado samples
    Talanta, 224 (2021), Article 121904, 10.1016/j.talanta.2020.121904 View PDFView
    articleView in ScopusGoogle Scholar Joshi et al., 2021 G. Joshi, R. Walambe, K.
    Kotecha A review on explainability in multimodal deep neural nets IEEE Access,
    9 (2021), pp. 59800-59821, 10.1109/ACCESS.2021.3070212 View in ScopusGoogle Scholar
    Kanchanamala, Revathi, & Ananth, 2023 P. Kanchanamala, K.G. Revathi, M.B.J. Ananth
    Optimization-enabled hybrid deep learning for brain tumor detection and classification
    from MRI Biomedical Signal Processing and Control, 84 (2023), p. 104955, 10.1016/j.bspc.2023.104955
    View PDFView articleView in ScopusGoogle Scholar Kang et al., 2020 R. Kang, B.
    Park, M. Eady, Q. Ouyang, K. Chen Classification of foodborne bacteria using hyperspectral
    microscope imaging technology coupled with convolutional neural networks(double
    dagger) Applied Microbiology and Biotechnology, 104 (7) (2020), pp. 3157-3166,
    10.1007/s00253-020-10387-4 View in ScopusGoogle Scholar Kılıç & İnner, 2022 C.
    Kılıç, B. İnner A novel method for non-invasive detection of aflatoxin contaminated
    dried figs with deep transfer learning approach Ecological Informatics, 70 (2022),
    p. 101728, 10.1016/j.ecoinf.2022.101728 View PDFView articleView in ScopusGoogle
    Scholar Kılıç, Özer, & İnner, 2024 C. Kılıç, H. Özer, B. İnner Real-time detection
    of aflatoxin-contaminated dried figs using lights of different wavelengths by
    feature extraction with deep learning Food Control, 156 (2024), p. 110150, 10.1016/j.foodcont.2023.110150
    View PDFView articleView in ScopusGoogle Scholar Kotsiantis, 2011 S.B. Kotsiantis
    Decision trees: A recent overview Artificial Intelligence Review, 39 (4) (2011),
    pp. 261-283, 10.1007/s10462-011-9272-4 Google Scholar Langyan et al., 2022 S.
    Langyan, P. Yadava, S. Sharma, N.C. Gupta, R. Bansal, R. Yadav, S. Kalia, A. Kumar
    Food and nutraceutical functions of sesame oil: An underutilized crop for nutritional
    and health benefits Food Chemistry, 389 (2022), Article 132990, 10.1016/j.foodchem.2022.132990
    View PDFView articleView in ScopusGoogle Scholar LeCun et al., 2015 Y. LeCun,
    Y. Bengio, G. Hinton Deep learning Nature, 521 (7553) (2015), pp. 436-444, 10.1038/nature14539
    View in ScopusGoogle Scholar Li, Tang, Chen, Tian, & Zhong, 2023b P. Li, S. Tang,
    S. Chen, X. Tian, N. Zhong Hyperspectral imaging combined with convolutional neural
    network for accurately detecting adulteration in Atlantic salmon Food Control,
    147 (2023), p. 109573, 10.1016/j.foodcont.2022.109573 View PDFView articleView
    in ScopusGoogle Scholar Li et al., 2021a P. Li, X. Zhang, Y. Zheng, F. Yang, L.
    Jiang, X. Liu, …, Y. Shan A novel method for the nondestructive classification
    of different-age Citri Reticulatae Pericarpium based on data combination technique
    Food Sciences and Nutrition, 9 (2) (2021), pp. 943-951, 10.1002/fsn3.2059 View
    in ScopusGoogle Scholar Li et al., 2021b Q. Li, J. Zeng, L. Lin, J. Zhang, J.
    Zhu, L. Yao, …, Z. Wu Mid-infrared spectra feature extraction and visualization
    by convolutional neural network for sugar adulteration identification of honey
    and real-world application LWT, 140 (2021), p. 110856, 10.1016/j.lwt.2021.110856
    View PDFView articleView in ScopusGoogle Scholar Li et al., 2023a S. Li, H. Zhang,
    R. Ma, J. Zhou, J. Wen, B. Zhang Linear discriminant analysis with generalized
    kernel constraint for robust image classification Pattern Recognition, 136 (2023),
    p. 109196, 10.1016/j.patcog.2022.109196 View PDFView articleView in ScopusGoogle
    Scholar Li, Zhang, & Shen, 2017 Y. Li, H. Zhang, Q. Shen Spectral–spatial classification
    of hyperspectral imagery with 3D convolutional neural network Remote Sensing,
    9 (1) (2017), p. 67, 10.3390/rs9010067 View PDFView articleGoogle Scholar Liang
    et al., 2021 J. Liang, J. Sun, P. Chen, J. Frazier, V. Benefield, M. Zhang Chemical
    analysis and classification of black pepper (Piper nigrum L.) based on their country
    of origin using mass spectrometric methods and chemometrics Food Research International,
    140 (2021), Article 109877, 10.1016/j.foodres.2020.109877 View PDFView articleView
    in ScopusGoogle Scholar Lin et al., 2023 Y. Lin, J. Ma, Q. Wang, D.W. Sun Applications
    of machine learning techniques for enhancing nondestructive food quality and safety
    detection Critical Reviews in Food Science and Nutrition, 63 (12) (2023), pp.
    1649-1669, 10.1080/10408398.2022.2131725 View in ScopusGoogle Scholar Liu et al.,
    2023a D. Liu, Y. Ma, S. Yu, C. Zhang Image based beef and lamb slice authentication
    using convolutional neural networks Meat Science, 195 (2023), Article 108997,
    10.1016/j.meatsci.2022.108997 View PDFView articleView in ScopusGoogle Scholar
    Liu et al., 2019 Y. Liu, X. Gao, Q. Gao, L. Shao, J. Han Adaptive robust principal
    component analysis Neural Networks, 119 (2019), pp. 85-92, 10.1016/j.neunet.2019.07.015
    View PDFView articleGoogle Scholar Liu et al., 2023b Y. Liu, H. Pu, Q. Li, D.W.
    Sun Discrimination of Pericarpium Citri Reticulatae in different years using Terahertz
    Time-Domain spectroscopy combined with convolutional neural network Spectrochimica
    Acta Part A: Molecular and Biomolecular Spectroscopy, 286 (2023), Article 122035,
    10.1016/j.saa.2022.122035 View PDFView articleView in ScopusGoogle Scholar Liu
    et al., 2021 Y. Liu, H. Pu, D.-W. Sun Efficient extraction of deep image features
    using convolutional neural network (CNN) for applications in detecting and analysing
    complex food matrices Trends in Food Science & Technology, 113 (2021), pp. 193-204,
    10.1016/j.tifs.2021.04.042 View PDFView articleGoogle Scholar Lohala et al., 2021
    S. Lohala, A. Alsadoon, P.W.C. Prasad, R.S. Ali, A.J. Altaay A novel deep learning
    neural network for fast-food image classification and prediction using modified
    loss function Multimedia Tools and Applications, 80 (17) (2021), pp. 25453-25476,
    10.1007/s11042-021-10916-x View in ScopusGoogle Scholar Lu et al., 2020 Y. Lu,
    W. Wang, M. Huang, X. Ni, X. Chu, C. Li Evaluation and classification of five
    cereal fungi on culture medium using Visible/Near-Infrared (Vis/NIR) hyperspectral
    imaging Infrared Physics & Technology, 105 (2020), p. 103206, 10.1016/j.infrared.2020.103206
    View PDFView articleView in ScopusGoogle Scholar Matsushita et al., 2018 T. Matsushita,
    J.J. Zhao, N. Igura, M. Shimoda Authentication of commercial spices based on the
    similarities between gas chromatographic fingerprints Journal of the Science of
    Food and Agriculture, 98 (8) (2018), pp. 2989-3000, 10.1002/jsfa.8797 View in
    ScopusGoogle Scholar Medina et al., 2019 S. Medina, R. Perestrelo, P. Silva, J.A.M.
    Pereira, J.S. Câmara Current trends and recent advances on food authenticity technologies
    and chemometric approaches Trends in Food Science & Technology, 85 (2019), pp.
    163-176, 10.1016/j.tifs.2019.01.017 View PDFView articleView in ScopusGoogle Scholar
    Medus, Saban, Francés-Víllora, Bataller-Mompeán, & Rosado-Muñoz, 2021 L.D. Medus,
    M. Saban, J.V. Francés-Víllora, M. Bataller-Mompeán, A. Rosado-Muñoz Hyperspectral
    image classification using CNN: Application to industrial food packaging Food
    Control, 125 (2021), p. 107962, 10.1016/j.foodcont.2021.107962 View PDFView articleView
    in ScopusGoogle Scholar Mehmood and Ahmed, 2015 T. Mehmood, B. Ahmed The diversity
    in the applications of partial least squares: An overview Journal of Chemometrics,
    30 (1) (2015), pp. 4-17, 10.1002/cem.2762 Google Scholar Mialon et al., 2023 N.
    Mialon, B. Roig, E. Capodanno, A. Cadiere Untargeted metabolomic approaches in
    food authenticity: A review that showcases biomarkers Food Chemistry, 398 (2023),
    Article 133856, 10.1016/j.foodchem.2022.133856 View PDFView articleView in ScopusGoogle
    Scholar Min et al., 2023 W. Min, Z. Wang, Y. Liu, M. Luo, L. Kang, X. Wei, …,
    S. Jiang Large scale visual food recognition IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 45 (8) (2023), pp. 9932-9949, 10.1109/TPAMI.2023.3237871
    View in ScopusGoogle Scholar Minaei et al., 2017 S. Minaei, S. Shafiee, G. Polder,
    N. Moghadam-Charkari, S. van Ruth, M. Barzegar, J. Zahiri, M. Alewijn, P.M. Kuś
    VIS/NIR imaging application for honey floral origin determination Infrared Physics
    & Technology, 86 (2017), pp. 218-225, 10.1016/j.infrared.2017.09.001 View PDFView
    articleView in ScopusGoogle Scholar Mirzaee-Ghaleh et al., 2019 E. Mirzaee-Ghaleh,
    A. Taheri-Garavand, F. Ayari, J. Lozano Identification of fresh-chilled and frozen-thawed
    chicken meat and estimation of their shelf life using an E-nose machine coupled
    fuzzy KNN Food Analytical Methods, 13 (3) (2019), pp. 678-689, 10.1007/s12161-019-01682-6
    Google Scholar Momeny et al., 2023 M. Momeny, A.A. Neshat, A. Jahanbakhshi, M.
    Mahmoudi, Y. Ampatzidis, P. Radeva Grading and fraud detection of saffron via
    learning-to-augment incorporated Inception-v4 CNN Food Control, 147 (2023), p.
    109554, 10.1016/j.foodcont.2022.109554 View PDFView articleView in ScopusGoogle
    Scholar Nagamalla et al., 2022 V. Nagamalla, B.M. Kumar, N. Janu, A. Preetham,
    S.M. Parambil Gangadharan, M.A. Alqahtani, …, R. Khan Detection of adulteration
    in food using recurrent neural network with internet of things Journal of Food
    Quality, 2022 (2022), p. 6163649, 10.1155/2022/6163649 View in ScopusGoogle Scholar
    Natarajan and Shanthi, 2018 A. Natarajan, N. Shanthi A survey on multimodal biometrics
    authentication and template protection 2018 international conference on intelligent
    computing and communication for smart world (2018) Google Scholar Ng, 2011 A.
    Ng Sparse autoencoder CS294A Lecture Notes, 72 (2011), pp. 1-19 2011 Google Scholar
    Nusrat & Jang, 2018 I. Nusrat, S.-B. Jang A comparison of regularization techniques
    in deep neural networks Symmetry, 10 (11) (2018), p. 648, 10.3390/sym10110648
    View in ScopusGoogle Scholar Pan et al., 2017 L. Pan, S. Pouyanfar, H. Chen, J.
    Qin, S.-C. Chen Deepfood: Automatic multi-class classification of food ingredients
    using deep learning 2017 IEEE 3rd international conference on collaboration and
    internet computing (CIC) (2017) Google Scholar Paszke et al., 2019 A. Paszke,
    S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
    L. Antiga Pytorch: An imperative style, high-performance deep learning library
    Advances in Neural Information Processing Systems, 32 (2019), 10.48550/arXiv.1912.01703
    Google Scholar Patil and Lad, 2021 A. Patil, K. Lad Chili plant leaf disease detection
    using SVM and KNN classification Rising threats in expert applications and solutions
    (2021), pp. 223-231, 10.1007/978-981-15-6014-9_26 View in ScopusGoogle Scholar
    Potarniche, Sarosi, Terebes, Szolga, & Galatus, 2023 I.A. Potarniche, C. Sarosi,
    R.M. Terebes, L. Szolga, R. Galatus Classification of food additives using UV
    spectroscopy and one-dimensional convolutional neural network Sensors, 23 (17)
    (2023), p. 7517, 10.3390/s23177517 View in ScopusGoogle Scholar Pu et al., 2023
    H. Pu, J. Yu, D.W. Sun, Q. Wei, Q. Li Distinguishing pericarpium citri reticulatae
    of different origins using terahertz time-domain spectroscopy combined with convolutional
    neural networks Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy,
    299 (2023), Article 122771, 10.1016/j.saa.2023.122771 View PDFView articleView
    in ScopusGoogle Scholar Ran et al., 2022 X. Ran, Y. Xi, Y. Lu, X. Wang, Z. Lu
    Comprehensive survey on hierarchical clustering algorithms and the recent developments
    Artificial Intelligence Review, 56 (8) (2022), pp. 8219-8264, 10.1007/s10462-022-10366-3
    Google Scholar Rehman et al., 2018 K. Rehman, F. Fatima, I. Waheed, M.S.H. Akash
    Prevalence of exposure of heavy metals and their impact on health consequences
    Journal of Cellular Biochemistry, 119 (1) (2018), pp. 157-184, 10.1002/jcb.26234
    View in ScopusGoogle Scholar Ren et al., 2023 L. Ren, Y. Tian, X. Yang, Q. Wang,
    L. Wang, X. Geng, K. Wang, Z. Du, Y. Li, H. Lin Rapid identification of fish species
    by laser-induced breakdown spectroscopy and Raman spectroscopy coupled with machine
    learning methods Food Chemistry, 400 (2023), Article 134043, 10.1016/j.foodchem.2022.134043
    View PDFView articleView in ScopusGoogle Scholar Ren et al., 2020 G. Ren, Y. Wang,
    J. Ning, Z. Zhang Using near-infrared hyperspectral imaging with multiple decision
    tree methods to delineate black tea quality Spectrochimica Acta Part A: Molecular
    and Biomolecular Spectroscopy, 237 (2020), Article 118407, 10.1016/j.saa.2020.118407
    View PDFView articleView in ScopusGoogle Scholar Rong, Wang, Xie, Ying, & Zhang,
    2020 D. Rong, H. Wang, L. Xie, Y. Ying, Y. Zhang Impurity detection of juglans
    using deep learning and machine vision Computers and Electronics in Agriculture,
    178 (2020), p. 105764, 10.1016/j.compag.2020.105764 View PDFView articleView in
    ScopusGoogle Scholar Rong et al., 2019 D. Rong, L. Xie, Y. Ying Computer vision
    detection of foreign objects in walnuts using deep learning Computers and Electronics
    in Agriculture, 162 (2019), pp. 1001-1010, 10.1016/j.compag.2019.05.019 View PDFView
    articleView in ScopusGoogle Scholar Roth et al., 2018 H.R. Roth, H. Oda, X. Zhou,
    N. Shimizu, Y. Yang, Y. Hayashi, M. Oda, M. Fujiwara, K. Misawa, K. Mori An application
    of cascaded 3D fully convolutional networks for medical image segmentation Computerized
    Medical Imaging and Graphics, 66 (2018), pp. 90-99, 10.1016/j.compmedimag.2018.03.001
    View PDFView articleView in ScopusGoogle Scholar Roux, 2018 M. Roux A comparative
    study of divisive and agglomerative hierarchical clustering algorithms Journal
    of Classification, 35 (2) (2018), pp. 345-366, 10.1007/s00357-018-9259-9 View
    in ScopusGoogle Scholar Saha and Manickavasagan, 2021 D. Saha, A. Manickavasagan
    Machine learning techniques for analysis of hyperspectral images to determine
    quality of food products: A review Current Research in Food Science, 4 (2021),
    pp. 28-44, 10.1016/j.crfs.2021.01.002 View PDFView articleView in ScopusGoogle
    Scholar Salehinejad et al., 2017 H. Salehinejad, S. Sankar, J. Barfett, E. Colak,
    S. Valaee Recent advances in recurrent neural networks (2017), 10.48550/arXiv.1801.01078
    arXiv preprint arXiv:1801.01078 Google Scholar Sedigh et al., 2019 P. Sedigh,
    R. Sadeghian, M.T. Masouleh Generating synthetic medical images by using GAN to
    improve CNN performance in skin cancer classification 2019 7th international conference
    on robotics and mechatronics (2019) Google Scholar Shao et al., 2018 Y. Shao,
    G. Xuan, Z. Hu, X. Gao Identification of adulterated cooked millet flour with
    Hyperspectral Imaging Analysis IFAC-PapersOnLine, 51 (17) (2018), pp. 96-101,
    10.1016/j.ifacol.2018.08.068 View PDFView articleView in ScopusGoogle Scholar
    Shelhamer, Long, & Darrell, 2017 E. Shelhamer, J. Long, T. Darrell Fully convolutional
    networks for semantic segmentation IEEE Transactions on Pattern Analysis & Machine
    Intelligence, 39 (4) (2017), pp. 640-651, 10.1109/TPAMI.2016.2572683 View in ScopusGoogle
    Scholar Shen, Yin, Li, Zhao, & Li, 2021 Y. Shen, Y. Yin, B. Li, C. Zhao, G. Li
    Detection of impurities in wheat using terahertz spectral imaging and convolutional
    neural networks Computers and Electronics in Agriculture, 181 (2021), p. 105931,
    10.1016/j.compag.2020.105931 View PDFView articleView in ScopusGoogle Scholar
    Shorten and Khoshgoftaar, 2019 C. Shorten, T.M. Khoshgoftaar A survey on image
    data augmentation for deep learning Journal of Big Data, 6 (2019), Article 60,
    10.1186/s40537-019-0197-0 View in ScopusGoogle Scholar Siedliska et al., 2018
    A. Siedliska, P. Baranowski, M. Zubik, W. Mazurek, B. Sosnowska Detection of fungal
    infections in strawberry fruit by VNIR/SWIR hyperspectral imaging Postharvest
    Biology and Technology, 139 (2018), pp. 115-126, 10.1016/j.postharvbio.2018.01.018
    View PDFView articleView in ScopusGoogle Scholar Song et al., 2019 D. Song, C.
    Cai, Z. Peng Pork registration using skin image with deep neural network features
    Artificial intelligence and mobile services, AIMS (2019), pp. 39-53, 10.1007/978-3-030-23367-9_4
    2019 View in ScopusGoogle Scholar Sun et al., 2022 J. Sun, Y. Hu, Y. Zou, J. Geng,
    Y. Wu, R. Fan, Z. Kang Identification of pesticide residues on black tea by fluorescence
    hyperspectral technology combined with machine learning Food Science and Technology,
    42 (2022), Article e55822, 10.1590/fst.55822 View in ScopusGoogle Scholar Taylan
    et al., 2020 O. Taylan, N. Cebi, M. Tahsin Yilmaz, O. Sagdic, A.A. Bakhsh Detection
    of lard in butter using Raman spectroscopy combined with chemometrics Food Chemistry,
    332 (2020), Article 127344, 10.1016/j.foodchem.2020.127344 View PDFView articleView
    in ScopusGoogle Scholar Tharwat et al., 2017 A. Tharwat, T. Gaber, A. Ibrahim,
    A.E. Hassanien Linear discriminant analysis: A detailed tutorial AI Communications,
    30 (2) (2017), pp. 169-190, 10.3233/aic-170729 View in ScopusGoogle Scholar Todd,
    2020 E. Todd Food-borne disease prevention and risk assessment International Journal
    of Environmental Research and Public Health, 17 (14) (2020), p. 5129, 10.3390/ijerph17145129
    Google Scholar Valencia-Quintana et al., 2020 R. Valencia-Quintana, M. Milic,
    D. Jaksic, M. Segvic Klaric, M.G. Tenorio-Arvide, G.A. Perez-Flores, …, J. Sanchez-Alarcon
    Environment changes, aflatoxins, and health issues, a review International Journal
    of Environmental Research and Public Health, 17 (21) (2020), p. 7850, 10.3390/ijerph17217850
    Google Scholar van Asselt et al., 2018 E.D. van Asselt, M.Y. Noordam, M.G. Pikkemaat,
    F.O. Dorgelo Risk-based monitoring of chemical substances in food: Prioritization
    by decision trees Food Control, 93 (2018), pp. 112-120, 10.1016/j.foodcont.2018.06.001
    View PDFView articleView in ScopusGoogle Scholar Wahab et al., 2022 S. Wahab,
    K. Muzammil, N. Nasir, M.S. Khan, M.F. Ahmad, M. Khalid, …, A.M. Busayli Advancement
    and new trends in analysis of pesticide residues in food: A comprehensive review
    Plants, 11 (9) (2022), p. 1106, 10.3390/plants11091106 View in ScopusGoogle Scholar
    Wan et al., 2021 A.C.A. Wan, B.C.U. Tai, C. Du Food security and nutrition- a
    systematic approach Trends in Food Science & Technology, 109 (2021), pp. 738-745,
    10.1016/j.tifs.2020.12.024 View PDFView articleView in ScopusGoogle Scholar Wang
    et al., 2022 X. Wang, Y. Bouzembrak, A.O. Lansink, H.J. van der Fels-Klerx Application
    of machine learning to the monitoring and prediction of food safety: A review
    Comprehensive Reviews in Food Science and Food Safety, 21 (1) (2022), pp. 416-434,
    10.1111/1541-4337.12868 Google Scholar Wang and Teplitski, 2023 L. Wang, M. Teplitski
    Microbiological food safety considerations in shelf-life extension of fresh fruits
    and vegetables Current Opinion in Biotechnology, 80 (2023), Article 102895, 10.1016/j.copbio.2023.102895
    View PDFView articleView in ScopusGoogle Scholar Wang et al., 2017 R. Wang, J.
    Zhang, W. Dong, J. Yu, C. Xie, R. Li, …, H. Chen A crop pests image classification
    algorithm based on deep convolutional neural network TELKOMNIKA (Telecommunication
    Computing Electronics and Control), 15 (3) (2017), p. 5382, 10.12928/telkomnika.v15i3.5382
    Google Scholar Wang et al., 2023 S. Wang, Q. Zhang, C. Liu, Z. Wang, J. Gao, X.
    Yang, Y. Lan Synergetic application of an E-tongue, E-nose and E-eye combined
    with CNN models and an attention mechanism to detect the origin of black pepper
    Sensors and Actuators A: Physical, 357 (2023), p. 114417, 10.1016/j.sna.2023.114417
    View PDFView articleView in ScopusGoogle Scholar Wu et al., 2023b X. Wu, B. Xu,
    H. Luo, R. Ma, Z. Du, X. Zhang, …, Y. Zhang Adulteration quantification of cheap
    honey in high-quality Manuka honey by two-dimensional correlation spectroscopy
    combined with deep learning Food Control, 154 (2023), p. 110010, 10.1016/j.foodcont.2023.110010
    View PDFView articleView in ScopusGoogle Scholar Wu et al., 2022 X. Wu, B. Xu,
    R. Ma, S. Gao, Y. Niu, X. Zhang, …, Y. Zhang Botanical origin identification and
    adulteration quantification of honey based on Raman spectroscopy combined with
    convolutional neural network Vibrational Spectroscopy, 123 (2022), p. 103439,
    10.1016/j.vibspec.2022.103439 View PDFView articleView in ScopusGoogle Scholar
    Wu et al., 2023a X. Wu, B. Xu, Y. Niu, S. Gao, Z. Zhao, R. Ma, …, Y. Zhang Detection
    of antioxidants in edible oil by two-dimensional correlation spectroscopy combined
    with convolutional neural network Journal of Food Composition and Analysis, 119
    (2023), p. 105262, 10.1016/j.jfca.2023.105262 View PDFView articleView in ScopusGoogle
    Scholar Wu et al., 2020 X. Wu, Z. Zhao, R. Tian, Z. Shang, H. Liu Identification
    and quantification of counterfeit sesame oil by 3D fluorescence spectroscopy and
    convolutional neural network Food Chemistry, 311 (2020), Article 125882, 10.1016/j.foodchem.2019.125882
    View PDFView articleView in ScopusGoogle Scholar Xin et al., 2020 Z. Xin, S. Jun,
    T. Yan, C. Quansheng, W. Xiaohong, H. Yingying A deep learning based regression
    method on hyperspectral data for rapid prediction of cadmium residue in lettuce
    leaves Chemometrics and Intelligent Laboratory Systems, 200 (2020), p. 103996,
    10.1016/j.chemolab.2020.103996 View PDFView articleView in ScopusGoogle Scholar
    Xu et al., 2016 L. Xu, X. Yu, L. Liu, R. Zhang A novel method for qualitative
    analysis of edible oil oxidation using an electronic nose Food Chemistry, 202
    (2016), pp. 229-235, 10.1016/j.foodchem.2016.01.144 View PDFView articleView in
    ScopusGoogle Scholar Yan et al., 2023b X.Q. Yan, H.L. Wu, B. Wang, T. Wang, Y.
    Chen, A.Q. Chen, …, R.Q. Yu Front-face excitation-emission matrix fluorescence
    spectroscopy combined with interpretable deep learning for the rapid identification
    of the storage year of Ningxia wolfberry Spectrochimica Acta Part A: Molecular
    and Biomolecular Spectroscopy, 295 (2023), Article 122617, 10.1016/j.saa.2023.122617
    View PDFView articleView in ScopusGoogle Scholar Yan, Liu, Li, & Wang, 2023a Z.
    Yan, H. Liu, J. Li, Y. Wang Qualitative and quantitative analysis of Lanmaoa asiatica
    in different storage years based on FT-NIR combined with chemometrics Microchemical
    Journal, 189 (2023), p. 108580, 10.1016/j.microc.2023.108580 View PDFView articleView
    in ScopusGoogle Scholar Yang et al., 2021a B. Yang, C. Chen, F. Chen, C. Chen,
    J. Tang, R. Gao, X. Lv Identification of cumin and fennel from different regions
    based on generative adversarial networks and near infrared spectroscopy Spectrochimica
    Acta Part A: Molecular and Biomolecular Spectroscopy, 260 (2021), Article 119956,
    10.1016/j.saa.2021.119956 View PDFView articleView in ScopusGoogle Scholar Yang
    et al., 2021b Z. Yang, N. Miao, X. Zhang, Q. Li, Z. Wang, C. Li, …, Y. Lan Employment
    of an electronic tongue combined with deep learning and transfer learning for
    discriminating the storage time of Pu-erh tea Food Control, 121 (2021), p. 107608,
    10.1016/j.foodcont.2020.107608 View PDFView articleView in ScopusGoogle Scholar
    Yang et al., 2019 Z. Yang, Z. Wang, W. Yuan, C. Li, X. Jing, H. Han Classification
    of wolfberry from different geographical origins by using electronic tongue and
    deep learning algorithm IFAC-PapersOnLine, 52 (30) (2019), pp. 397-402, 10.1016/j.ifacol.2019.12.592
    View PDFView articleGoogle Scholar Yu et al., 2023 H. Yu, Z. Yang, S. Fu, Y. Zhang,
    R. Panneerselvamc, B. Li, …, J. Li Intelligent convolution neural network-assisted
    SERS to realize highly accurate identification of six pathogenic Vibrio Chemical
    Communications, 59 (38) (2023), pp. 5779-5782, 10.1039/d3cc01129a View articleView
    in ScopusGoogle Scholar Zhang et al., 2022b J. Zhang, J. Zhang, J. Ding, Q. Lin,
    G.M. Young, C. Jiang Rapid identification of live and dead Salmonella by surface-enhanced
    Raman spectroscopy combined with convolutional neural network Vibrational Spectroscopy,
    118 (2022), p. 103332, 10.1016/j.vibspec.2021.103332 View PDFView articleView
    in ScopusGoogle Scholar Zhang et al., 2023 Y. Zhang, L. Jiang, Y. Chen, Q. Zhang,
    C. Kang, D. Chen Detection of chlorpyrifos residue in apple and rice samples based
    on aptamer sensor: Improving quantitative accuracy with partial least squares
    model Microchemical Journal, 194 (2023), p. 109352, 10.1016/j.microc.2023.109352
    View PDFView articleView in ScopusGoogle Scholar Zhang et al., 2022a C. Zhang,
    L. Zhou, Q. Xiao, X. Bai, B. Wu, N. Wu, …, L. Feng End-to-End fusion of hyperspectral
    and chlorophyll fluorescence imaging to identify rice stresses Plant Phenomics,
    2022 (2022), Article 9851096, 10.34133/2022/9851096 View in ScopusGoogle Scholar
    Zhang et al., 2020 D. Zhang, C. Yin, J. Zeng, X. Yuan, P. Zhang Combining structured
    and unstructured data for predictive models: A deep learning approach BMC Medical
    Informatics and Decision Making, 20 (1) (2020), p. 280, 10.1186/s12911-020-01297-6
    View in ScopusGoogle Scholar Zhang, Zheng, Zhu, & Ma, 2022c Y. Zhang, M. Zheng,
    R. Zhu, R. Ma Adulteration discrimination and analysis of fresh and frozen-thawed
    minced adulterated mutton using hyperspectral images combined with recurrence
    plot and convolutional neural network Meat Science, 192 (2022), p. 108900, 10.1016/j.meatsci.2022.108900
    View PDFView articleView in ScopusGoogle Scholar Zheng, Zhang, Gu, Bai, & Zhu,
    2021 M. Zheng, Y. Zhang, J. Gu, Z. Bai, R. Zhu Classification and quantification
    of minced mutton adulteration with pork using thermal imaging and convolutional
    neural network Food Control, 126 (2021), p. 108044, 10.1016/j.foodcont.2021.108044
    View PDFView articleView in ScopusGoogle Scholar Zheng, Yang, Tian, Wang, & Wang,
    2020 Q. Zheng, M. Yang, X. Tian, X. Wang, D. Wang Rethinking the role of activation
    functions in deep convolutional neural networks for image classification Engineering
    Letters, 28 (1) (2020), pp. 80-92 View in ScopusGoogle Scholar Zhou et al., 2020
    X. Zhou, J. Sun, Y. Tian, B. Lu, Y. Hang, Q. Chen Hyperspectral technique combined
    with deep learning algorithm for detection of compound heavy metals in lettuce
    Food Chemistry, 321 (2020), Article 126503, 10.1016/j.foodchem.2020.126503 View
    PDFView articleView in ScopusGoogle Scholar Zhu et al., 2017 B. Zhu, Z.-S. Chen,
    Y.-L. He, L.-A. Yu A novel nonlinear functional expansion based PLS (FEPLS) and
    its soft sensor application Chemometrics and Intelligent Laboratory Systems, 161
    (2017), pp. 108-117, 10.1016/j.chemolab.2016.12.012 View PDFView articleView in
    ScopusGoogle Scholar Zhu et al., 2021b J. Zhu, A.S. Sharma, J. Xu, Y. Xu, T. Jiao,
    Q. Ouyang, …, Q. Chen Rapid on-site identification of pesticide residues in tea
    by one-dimensional convolutional neural network coupled with surface-enhanced
    Raman scattering Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy,
    246 (2021), Article 118994, 10.1016/j.saa.2020.118994 View PDFView articleView
    in ScopusGoogle Scholar Zhu et al., 2021a L. Zhu, P. Spachos, E. Pensini, K.N.
    Plataniotis Deep learning and machine vision for food processing: A survey Current
    Research in Food Science, 4 (2021), pp. 233-249, 10.1016/j.crfs.2021.03.009 View
    PDFView articleView in ScopusGoogle Scholar Cited by (0) View Abstract © 2024
    Elsevier Ltd. All rights reserved. Part of special issue Food authentication Edited
    by Dr. Xiaonan Lu, Professor Christopher Elliott, Dr. Hu Yaxi View special issue
    Recommended articles Bamboo shoot and its food applications in last decade: An
    undervalued edible resource from forest to feed future people Trends in Food Science
    & Technology, Volume 146, 2024, Article 104399 Yue Zhang, …, Guangjing Chen View
    PDF Recent advances in the formation and identification of nanoparticle protein
    coronas and their effects on the digestion and absorption of polyphenols Trends
    in Food Science & Technology, Volume 146, 2024, Article 104418 Zhiheng Zhang,
    …, Chao Qiu View PDF Challenges and opportunities in developing low glycemic index
    foods with white kidney bean α-amylase inhibitor Trends in Food Science & Technology,
    Volume 147, 2024, Article 104397 Tingting Liu, …, Haifeng Qian View PDF Show 3
    more articles Article Metrics Captures Readers: 3 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Trends in Food Science and Technology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Deep leaning in food safety and authenticity detection: An integrative review
    and future prospects'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sergio W.L.
  - Ströele V.
  - Dantas M.
  - Braga R.
  - Macedo D.D.
  citation_count: '0'
  description: In the evolving education landscape, institutions are transitioning
    to a hybrid model encompassing physical and virtual classes. In this scenario,
    as working hours and exhausting routines increase, individuals accumulate psychological
    and physical challenges over time. This change requires students and educators
    to adjust to changes in teaching and learning routines, leading to moments of
    stress and anxiety. Cultivating self-awareness about unhealthy habits emerges
    as a pragmatic strategy to address this issue. In this work, a comprehensive eHealth
    proposal is developed to provide individuals with pertinent health information,
    assisting educational agents (students and teachers) in identifying and managing
    these stressful instances during their daily activities. This proposal acquires,
    processes, and disseminates vital sign data using IoT devices, supported by a
    Fog Computing architecture for scalability and adaptability. IoT plays a pivotal
    role in this eHealth proposal, facilitating the seamless collection and transmission
    of real-time data from various devices. Connected wearables and sensors enable
    the continuous monitoring of vital signs, enhancing the accuracy and responsiveness
    of the system. Heart rate data from educational agents were collected for implementation,
    allowing the evaluation of system performance. Machine learning models were leveraged
    to discern behavioral profiles and predict possible irregularities in vital signs.
    The results confirm the system's ability to perform its intended functions, giving
    users quick and accurate insights into their evolving behavioral patterns. Integrating
    these research perspectives underscores the importance of adaptive systems in
    navigating the challenges of modern education environments. The incorporation
    of IoT technology not only enhances data collection but also opens avenues for
    real-time interventions and personalized feedback, ultimately contributing to
    a more proactive approach in addressing health-related concerns within the educational
    context.
  doi: 10.1016/j.iot.2023.101055
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related work 3. Architecture proposal
    4. Material and methods 5. Results 6. Discussion 7. Conclusions CRediT authorship
    contribution statement Declaration of competing interest Acknowledgment Data availability
    References Show full outline Figures (21) Show 15 more figures Tables (5) Table
    1 Table 2 Table 3 Table 4 Table 5 Internet of Things Volume 25, April 2024, 101055
    Enhancing well-being in modern education: A comprehensive eHealth proposal for
    managing stress and anxiety based on machine learning Author links open overlay
    panel Wagno Leão Sergio a, Victor Ströele a, Mário Dantas a, Regina Braga a, Douglas
    Dyllon Macedo b Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.iot.2023.101055
    Get rights and content Abstract In the evolving education landscape, institutions
    are transitioning to a hybrid model encompassing physical and virtual classes.
    In this scenario, as working hours and exhausting routines increase, individuals
    accumulate psychological and physical challenges over time. This change requires
    students and educators to adjust to changes in teaching and learning routines,
    leading to moments of stress and anxiety. Cultivating self-awareness about unhealthy
    habits emerges as a pragmatic strategy to address this issue. In this work, a
    comprehensive eHealth proposal is developed to provide individuals with pertinent
    health information, assisting educational agents (students and teachers) in identifying
    and managing these stressful instances during their daily activities. This proposal
    acquires, processes, and disseminates vital sign data using IoT devices, supported
    by a Fog Computing architecture for scalability and adaptability. IoT plays a
    pivotal role in this eHealth proposal, facilitating the seamless collection and
    transmission of real-time data from various devices. Connected wearables and sensors
    enable the continuous monitoring of vital signs, enhancing the accuracy and responsiveness
    of the system. Heart rate data from educational agents were collected for implementation,
    allowing the evaluation of system performance. Machine learning models were leveraged
    to discern behavioral profiles and predict possible irregularities in vital signs.
    The results confirm the system’s ability to perform its intended functions, giving
    users quick and accurate insights into their evolving behavioral patterns. Integrating
    these research perspectives underscores the importance of adaptive systems in
    navigating the challenges of modern education environments. The incorporation
    of IoT technology not only enhances data collection but also opens avenues for
    real-time interventions and personalized feedback, ultimately contributing to
    a more proactive approach in addressing health-related concerns within the educational
    context. Previous article in issue Next article in issue Keywords eHealthInternet
    of thingsMachine learningEducationFog–cloud computing 1. Introduction As our society
    grows, it brings changes to the population’s daily lives and how some professionals
    act and intensifies the demand for results on individuals. Changes concerning
    interaction, work performance, and activities are examples of changes [1], [2].
    However, some activities had a quick adaption that allowed them to be carried
    out safely [3]. This adaptation was also necessary for education, where teachers
    and students had to implement and adapt themselves to a fast digital transformation
    in the educational process (facilitated by technologies such as Zoom, Cisco Webex,
    Google Class, Panopto, etc.) [4], [5]. Since the COVID-19 pandemic, most higher
    education classes have come back from remote to face-to-face teaching. However,
    some educational institutions chose a hybrid model, in which students can follow
    classes in person (on-site) or remotely (online) [6], [7], [8], [9]. It is interesting
    to notice those effects on students and teachers who, despite still being able
    to have classes in a hybrid model, still need to adapt themselves to this new
    routine, impacting their mental health [10], [11], [12], [13]. Some studies point
    to the negative impact educational agents’ stress levels and anxiety can have
    on academic achievement [14], [15]. Positive thoughts and emotions can help students
    have better academic performance. On the other hand, negative emotions make knowledge
    difficult and harm the student during the teaching and learning process [16].
    Some research has already been put into focus and warning about the dangers of
    people’s lack of awareness regarding their health [17], [18]. In addition, it
    can be noted that the structures of companies and institutions are also gradually
    being updated to reduce such negative impacts on the people involved. Also, it
    has been observed that through self-awareness of the individual, he/she can improve
    his/her condition and adopt healthier daily practices [19]. Helping students and
    teachers monitor stress and anxiety levels can benefit their mental health and
    teaching and learning process. When analyzing possible efficient approaches to
    the issues presented, it is possible to observe a growing adoption of techniques
    where tools are created to support the individual. So that he can identify possible
    sources of problems, obtain relevant information about his health and behavior,
    and be assisted in decision-making. Such tools are often referred to as e-health
    applications. They can continuously monitor the user and use the data collected
    to generate meaningful answers. From a technological perspective, we can analyze,
    through IoT devices such as environment sensors, smart bands, and smartwatches,
    the signals captured to determine moments of stress and anxiety during teachers’
    and students’ day [20], [21]. The relationship between heart rate patterns and
    anxiety can be found in several studies over the years [22]. According to [23],
    there is a significant interest in IoT research within the healthcare community,
    including IoT healthcare applications, Machine Learning techniques, data analytics,
    among others. The authors identify important themes like fog computing, cloud–IoT
    integration, and cognitive smart healthcare. The review enhances scholars’ understanding
    of IoT research in healthcare, identifies knowledge gaps, and informs healthcare
    professionals about the latest developments and applications in the healthcare
    sector. Therefore, using IoT devices can be highly effective, non-invasive, and
    with great potential for analyzing education agents’ physical and mental health
    without crossing the boundaries between health and computing specialists. However,
    it is also important to note that a viable implementation of such applications
    requires high-quality criteria. The application must have a high level of service
    availability, efficiency in processing the data collected for the generation of
    the response, and security of access to education agents’ information that may
    be sensitive. Thus, it is necessary to consider several measures when developing
    applications in this specific field, in addition to modeling an architecture that
    can support the constant exchange of information and processing necessary for
    the proper support resources to be available to the education agent. There are
    several e-health applications, most of which seek to specialize in a specific
    task concerning monitoring the health of individuals in the observation environment
    [24]. Generally, sensors are used to collect the necessary data from users, and
    through them, some processing is carried out, resulting in information relevant
    to the stakeholders. These analyses then clarify or warn about the health of the
    education agent concerned. In addition, the systems that manage the collected
    data are mostly implemented based on a Cloud paradigm [25]. However, in cases
    where several users are using the system simultaneously, problems may occur regarding
    the processing speed and response of the infrastructure. Such data bottlenecks
    can occur due to the high volume of data sent by the various sensors inserted
    in the structure assembled and the long processing time that cloud services need
    to generate the appropriate results [26]. Such difficulties may reduce the applicability
    of the solutions, causing little interest in using these approaches. When considering
    how well the education agents will adopt the monitoring environment, it is important
    to consider the introduction of sensors that simultaneously do not interfere with
    the individual’s daily life and can collect relevant data. An important factor
    rarely considered in modeling e-health applications is how to perform the personalized
    delivery of the analyzed obtained for the monitored user. In most of the applications
    studied, the information generated was used to support the decision-making of
    health professionals. However, approaches where the user under observation also
    receives notifications about their activities can help them become aware of their
    health. Lack of attention and care towards health caused by extensive working
    hours or physically or mentally unstable daily activities can cause the education
    agent to acquire behaviors that are detrimental to their well-being. With this
    in mind, solutions such as e-health applications are created to provide the necessary
    support in monitoring the education agent’s health. However, solutions that require
    high scalability and low response time must adopt new architectures that support
    the volume of data constantly generated. In addition, it was observed that most
    systems in this category do not present the information acquired during monitoring
    in a personalized and relevant way to the user, taking into account only the clinical
    aspects of the individual. Some aspects that can bring a negative experience to
    the user during the use of monitoring systems were also analyzed, as well as applied
    data collection methods that need to offer freedom, comfort, and safety to those
    being monitored. With each of these points in mind, we can arrive at a question
    on the subject: Is it possible to use IoT devices and develop an e-Health systems
    capable of alerting educational agents about their behavior in a personalized,
    automatic, and effective way? The consideration of the problematic factors on
    the topic led to the question of the possibility of developing an architecture
    that could collect health data from educational agents under observation and generate
    meaningful analyses that could be delivered quickly and scalably. Such systems
    and services are increasingly improving teaching and learning agents’ process
    quality, by continuously monitoring their health status. In a complementary way,
    services in this field of study provide the academic community the ability to
    manage both their physical and mental conditions accurately and systematically,
    generating data that can be used to understand the current situation of a group
    of people. In addition, with the recent advancement in the Internet of Things
    technologies, there has been a sudden focus on solutions that offer the ability
    to use the devices around us to enhance every aspect of our lives. The capabilities
    of sensors for real-time monitoring are growing every day, in conjunction with
    edge computing devices, bringing even more possibilities for distributed and continuously
    running systems. Investigating the proposal of a system with these characteristics
    can offer educational agents an intuitive and easy way to understand their behavior,
    thus contributing to awareness regarding their health. This work proposes the
    development and experimentation of a scalable and adaptive computing system capable
    of supporting educational agents who need information about their health by collecting,
    processing, and visualizing their vital signs throughout their daily lives. The
    system has an event-driven architecture to identify stress and anxiety moments
    during an educational agent routine. The architecture uses smart band sensor data
    to understand behavioral variation and mental and physical problem signs. Therefore,
    as a contribution, we can point out the mix between the areas of computing in
    education and health that can bring great results to the teaching and learning
    process. This research involves a series of steps. The first involves designing
    an architecture model for IoT devices data collection, processing, and presentation.
    The second is to study and select the technologies available for collecting educational
    agents’ health data. The third aims to structure a model for processing the data
    obtained and transforming it into relevant information. The fourth aims to develop
    a system that presents the generated analyses to the user by developing the architecture
    to make the proposed system functional. Finally, in the fifth step, an experiment
    is carried out to evaluate the performance of the proposed solution. By executing
    this sequence of tasks, the work proposed in this paper could contribute to the
    field of eHealth, presenting a system capable of offering a quick, adaptive, and
    personalized response to users in the educational environment. These responses
    can be used to notify the individuals of possible instability in their state,
    thus creating the opportunity to take intervention actions on their own behavior,
    preventing possible worse situations. This work also combines multiple software
    infrastructures and architectural designs in a feasible and modular way to solve
    problems that appear frequently in systems with the same features as the one proposed
    in the article. The paper is organized as follows: First, the related work used
    to support this research, highlighting the process and concepts that contributed
    to our work. Then, the architecture designed is presented to accomplish the established
    goals, explaining its execution and deployment. After that, the system’s structure
    and technologies are described, with an explanation of the services implemented,
    processing flow, components, and deployment configurations, along with a description
    of the user behavior model’s management. Next, the experiments are described using
    the proposed systems and the results obtained. After that, a discussion about
    the results obtained is made, analyzing both the technical performance of the
    model and the quality of the system’s responses. Lastly, the conclusions about
    the work are made, highlighting the major ideas we could gather from this research
    and presenting possible approaches in future works. 2. Related work Disorders
    involving educational agents’ mental health in the educational environment are
    a very expressive topic nowadays and have been frequently discussed in previous
    research. Given the solution complexity, we searched for works that discuss ways
    to identify students’ and teachers’ stress and anxiety levels, machine learning
    techniques to recognize people’s behavior, and alternatives to process the large
    volume of data generated by wearable devices. An attempt was made, mainly, to
    find research that demonstrated the feasibility of systems of these categories
    and also pointed out the possible obstacles encountered when developing them.
    We seek to understand each work’s proposal and highlight their positive and negative
    points. In this way, we can better contextualize the effectiveness of the techniques
    and concepts selected for the development of the proposed system, in addition
    to being able to evaluate possible problems in its construction previously. An
    analysis in [27] establishes a significant relationship between student’s anxiety
    levels and academic performance. Similarly, [28] shows an association between
    stress and the performance of medical students. These correlations are of concern
    because if these problems are experienced for long periods, they may show signs
    of worsening in students’ mental health. In [29], the authors analyzed the effect
    of mindfulness meditation on controlling students’ anxiety. They selected data
    from several published articles and performed a meta-analysis, which showed that
    mindfulness meditation moderated anxiety levels. These studies collected data
    through forms, historical data, and interviews without using sensors to monitor
    and support students in real-time. In [30], an architecture is proposed to collect
    data generated by sensors that monitor equipment conditions in a factory in real-time.
    The data goes through a processing step, where information on possible abnormal
    conditions in one or more machines will be extracted. This information is then
    sent to the cloud to make automatic maintenance requests for equipment showing
    signs of malfunction. This article mainly analyzed the modeling of the architecture
    and its components to automate a process previously done slowly and expensively.
    Through the lambda architecture, the proposal presents a way to ensure that the
    system presents information quickly while performing complex task updates and
    deployment without interrupting the system. The paper presents a detailed description
    of the technologies selected for the architecture implementation and explains
    the data flow and its transformations. Therefore, the concepts presented could
    serve as a basis for modeling a similar architecture to monitor individuals’ vital
    signs. However, it is important to point out that the paper only defined the architecture,
    without the implementation and feasibility tests, an issue that differs from the
    research being developed in this work. Other studies focus on the use of devices
    to monitor students. In [31], a case study was conducted with 42 student volunteers
    to verify Heart Rate Variability (HRV) variation due to a real-life stressor.
    The authors indicate that it is possible to identify that the student is going
    through a moment of anxiety, but they do not present alternatives for students
    to get out of this state. Similarly, the work developed by [32] describes in detail
    an experiment carried out to detect volunteers’ mental stress through measures
    from Smartwatch during a series of cognitive tasks. The authors propose implementing
    a system that uses smartwatches to collect biological data from volunteers, such
    as heart rate, body temperature, and Galvanic Skin Response (GSR). The data is
    collected during specific activities and used to train a Machine Learning model
    to predict whether a volunteer is stressed. The results indicate the proposal’s
    feasibility and then conclude by suggesting possible integrations with mobile
    device applications for real-time monitoring of stress detection in the volunteer.
    The paper shows the feasibility of using sensors to collect the user’s vital data.
    Such feasibility is essential for developing this ongoing research since it is
    one of the major components of the proposed architecture. It also demonstrates
    the efficiency of using smartwatches to collect data from volunteers. Proving
    the effectiveness of this type of device is important, as this is one of the sensors
    that is intended to be introduced in our work. The research of [20] proposes a
    model that uses IoT devices for monitoring and managing students’ emotions. The
    authors present recommendations for students but do not implement a system considering
    all needed steps. [33] presents an experiment using wearable sensors to recognize
    students’ stress levels. These studies indicate that it is possible to monitor
    students’ anxiety levels using body sensors. However, they did not specify and
    develop a system that considers all the necessary steps for detecting stress and
    anxiety. So, developing an automated system considering all the steps from extraction
    to decision-making is a contribution of this research. In [23], [34], the authors
    analyze the current technological and social situation regarding the development
    and implementation of health systems based on the Internet of Things (IoT) and
    Artificial Intelligence (AI). The research was based on a literature review of
    recent studies on the use of eHealth systems, also exploring the constant evolution
    of technologies that enable IoT applications, along with an evaluation of solutions
    already on the market. The study discusses the growing opportunities arising from
    these technologies in the health field to improve people’s quality of life. The
    potential benefits of these applications for society are examined in detail, including
    impacts on costs, quality of service, and quality of life. Considering the stream
    data processing problem, [35] focused on identifying and reporting the daily activities
    of individuals at home in order to try to reduce the workload of caregivers and
    health professionals. Such an environment is then supervised by non-wearable sensors
    that collect a wealth of data. The authors proposed an event-driven framework
    to detect abnormal patterns in assisted environments. A Fog–Cloud paradigm and
    Lambda architecture [35], [36] were adopted to handle the volume of data and machine
    learning techniques were used to recognize their daily activities. The paper briefly
    details how to implement a fog–cloud approach, showing that processing part of
    the data beforehand improves the overall efficiency of the system. It also exposed
    how the Lambda architecture makes it possible to handle a large number of data
    while making this information available in real time. The work done by [37] aims
    to investigate the feasibility of detecting mental states of calm and stress using
    electroencephalogram (EEG) data to serve as input data for training different
    machine learning models. It is explained that, unlike other alternatives for monitoring
    brain signals, the EEG method has less invasive characteristics for patients,
    so its use is easy and less cumbersome. For the data classification, different
    machine learning algorithms were selected for the study, and the functioning of
    each algorithm was briefly explained, and finally, cross-validation methods were
    used to test the accuracy of each model. This study was particularly examined
    because it demonstrated a consistent methodology for data collection and pre-processing,
    feature extraction, and training and evaluation of different models. The high
    level of accuracy obtained by the process demonstrates the feasibility of using
    such a processing flow. Although we used other types of data in this paper, this
    technique can be changed or replaced to fit the definition of the problem being
    addressed. In [38], an investigation is carried out on mobile health apps, or
    mobile health (mHealth) apps, in order to study mainly their behaviors concerning
    sending notifications to users. The research aims to conduct experiments with
    several mHealth apps to see if such notifications impact user behavior. Regarding
    the level of occurrence of notifications, it was found that most of them occurred
    based on specific hours of the day, thus showing that the applications do not
    adapt to send based on the user’s context. It was also found that almost all apps
    had daily notifications. This work is relevant to this research because it can
    identify the main aspects that determine notifications’ impact on users’ adherence
    to health-oriented applications. Through the conclusions obtained in this work,
    we can elaborate on a notification methodology that can deal with all the factors
    commented on in this article. The collected data must be processed using pattern
    recognition techniques to recognize educational agents’ behavior. Several well-founded
    machine learning techniques in the literature have proven to be effective. For
    example, classification algorithms such as KNearest Neighbor (KNN) and Support
    Vector Machines (SVM) are used to identify the patterns in the stress degrees
    of the students [33], [37]. Similarly, [39] uses machine learning algorithms to
    determine the experiment participants’ different degrees of anxiety, depression,
    and stress. Performing data collection, processing, and visualization from multiple
    users simultaneously is challenging. For this reason, the solution presented in
    this paper employs an operation based on the Fog–Cloud paradigm [35], [40] and
    uses the strategies and concepts presented in the lambda architecture [30], [36].
    Such gimmicks have already proven to support the large data volume received from
    users and do not affect the processing and delivery of information [41]. We can
    see in the studies analyzed that the majority work do not fully addresses all
    of the selected subjects. This is reasonable, both because of the complexity of
    using each concept in a single proposal, and because each paper seeks to specialize
    in a specific field in its research. In addition, it can be seen that no concept
    was covered in some way by all the papers, with the concepts of eHealth, the use
    of sensors and data analysis having the greatest coverage among the papers. This
    shows the close relationship between these ideas in the research and their joint
    adoption to achieve the goal of obtaining relevant information about an individual’s
    health. In our work, we propose to use all the concepts investigated in order
    to develop a feasible solution to collect, process and distributed individual’s
    health data to help them to mitigate possible unstable situations, in a efficient
    and adaptive way. By doing that, we aim to contribute in the field of eHealth
    solutions to support educational agents. Through the studies presented in the
    selected works, we can identify some factors perceived as more relevant. Firstly,
    it is noticed that one of the most significant stages of the monitoring process
    is the stage of transforming the data to become suitable for creating a model
    of user behavior and each system implements architectures focusing on different
    elements depending on the desired result, being more speed of response, scalability,
    or automation of activities. Finally, we can see that most proposed solutions
    use Machine Learning models to establish a pattern in the collected data. Thus,
    these works have contributed to a more technical understanding of creating such
    systems and helped us identify possible difficulties in implementing such solutions.
    3. Architecture proposal Considering IoT devices data, such as wearable sensor
    data, we collect educational agents’ data in different scenarios and contexts.
    We provide a specific predictive model for each educational agent that enriches
    their profile and context information. With this, we highlight anxiety and stressful
    moments and recommend some action to revert these situations, such as doing physical
    activities to reduce tension in the long term. All types of activities and possible
    actions for recommendation are based on other research works [20], [42], [43].
    In order to create a system capable of collecting, processing, and presenting
    educational agents’ data, it is necessary to employ a robust and efficient architecture
    to support many individuals while providing up-to-date information in real-time.
    This way, some paradigms were selected, and a framework was designed. Fig. 1 presents
    an overview of the proposed architecture with its main layers and components.
    First, it was defined that the different components that make up the architecture
    are divided into different processing categories. Thus, the proposed architecture
    has three distinct computing layers: Edge, Fog, and Cloud. We decided for this
    computational structure to simultaneously avoid possible congestion problems and
    achieve high levels of scalability and modularity. The Edge layer is established
    in the architecture as the set of IoT devices that communicate with each other
    to be able to collect the necessary data. The Fog layer is inserted as a computational
    sub-core responsible for storing all the collected data and managing the necessary
    computing processes. Finally, the Cloud layer maintains the information generated
    and makes it available to the end user. In addition, the proposition of lambda
    architecture was found in the literature [36]. This concept was designed so that
    systems could perform the processing of large volumes of data simultaneously so
    results could be delivered in real-time. Following this principle, the architecture
    that has been modeled incorporates the elements present in the lambda architecture
    so that it can also present such capabilities. The lambda architecture is a model
    that implements a parallel processing flow through three different layers: speed
    layer, batch layer, and serving layer. The batch layer performs the system’s high
    processing cost and time computation. This way, the received data can be processed
    separately and stored to be used later. In the context of the proposed architecture,
    this is the layer for processing users’ vital signs, validating and using them
    as input data for training a machine learning model for predicting the educational
    agent’s behavior. The results are then generated and stored to be queried in the
    future. Thus, with each new batch of data that the system can extract at a given
    time from the recorded measurements, a new prediction model is generated for the
    individual whose data was collected so that the system can monitor changes in
    their behavior. The main function of the speed layer is to receive the data and
    send an appropriate response in the shortest possible time. It is represented
    in the proposal by the monitoring layer, which receives educational agents’ vital
    signs and uses the behavioral models already trained by the batch layer to produce
    a response during data collection. In the proposal presented here, the speed layer
    performs the irregularity detection calculations through the behavior models previously
    generated by the batch layer. The serving layer is specified, which makes the
    most up-to-date results always available for queries. The serving layer is composed
    of a server dedicated to the behavior information generated over time and an interface
    to warn the end user about possible anomalies in its state. After all the processing
    is done, the system generates a response that will be sent to the user. The final
    objective is for the user to receive a notification that can prompt the individual
    to perform some action to attenuate their current condition. In related investigations,
    we found that the individual can engage in different types of activities in order
    to stabilize themselves. Then, we could use the notifications created by the system
    to send possible suggestions of activities the user could perform. Download :
    Download high-res image (189KB) Download : Download full-size image Fig. 1. General
    System Architecture Model. 3.1. Behavior detection model In order to identify
    possible instability in the behavior of educational agents inserted in the system,
    it is necessary to have a strategy that can receive their vital signs and transform
    them into information useful for their monitoring. After an investigation in the
    literature, it was found that techniques that use Machine Learning algorithms
    can identify patterns in the behavior of the received data and make predictions.
    For this reason, such approaches were adopted to develop the educational agent
    behavior model. The flowchart in Fig. 2 describes the general process steps in
    generating user behavior profiles. As observed, when the data is received, it
    goes through a pre-processing before being stored. In addition, the algorithm
    periodically checks if there is enough data to start training the model. When
    there is, the training process of the machine learning model is started. It was
    defined that the models are built to predict the next values of the individual’s
    vital signs so that it is possible to compare whether the individual is in a moment
    of emotional instability. It is also planned to carry out experimentation to verify
    which Machine Learning technique is the most suitable for the data obtained through
    comparison tests of accuracy metrics. It can also be seen that both the model
    results and the trained model itself are stored. Store the model is necessary
    so that the educational agent’s behavior model is not recalculated and new predictions
    can be made anytime. Download : Download high-res image (220KB) Download : Download
    full-size image Fig. 2. Flowchart of the user behavior model training process.
    3.2. Deployment design After completing the modeling of the architecture, its
    components must be built and connected to a real infrastructure. For this, different
    services must be developed to obtain the desired functionalities. Fig. 3 illustrates
    how the required devices interact with each other within the proposed architecture.
    To collect the data, educational agents must use a SmartBand, which measures their
    vital signs, and an android application. [44], which manages the sensor with the
    rest of the services. This application was selected because it is an open-source
    project that can be changed and extended according to the needs presented. Download
    : Download high-res image (169KB) Download : Download full-size image Fig. 3.
    Implementation diagram of the proposed system. A measurement collection service
    has been established to organize the data collected from users. This service is
    responsible for communicating with the service that controls user data flow. In
    addition, the service stores the data in a Data Lake and verifies the credentials
    of users who wish to use the system. Next, we have the data flow management and
    behavior model services. These are responsible for implementing the logic for
    handling the data extracted from the measurement collection service and using
    the measurements obtained to generate and store an educational agent behavior
    model. Finally, we have the notification scheduling service, which receives the
    data processed by the previous services and registers them to be sent at specific
    times to the end user. Note that all communications between the services were
    defined as HTTP; this was thought to abstract their functions from the rest of
    the system and achieve a high level of modularity. So, these services can operate
    on different devices in possible later versions of the system. 3.3. Experimentation
    Once the proposed architecture is built and running, verifying whether it can
    help people identify situations where educational agents’ vital signs deviate
    from standard behavior is necessary. For this, it was planned to carry out an
    experiment simulating the execution of the system, with educational agents using
    the available functionalities. The experiment’s main purpose is to verify if it
    is possible to alert educational agents about their behavior in a personalized,
    automatic, and efficient way. 4. Material and methods Based on the hybrid educational
    environment where many students and teachers are participating nowadays, we must
    consider that there are several individuals to be monitored by wearable sensors,
    and the data generated by them must be used to warn each educational agent about
    their vital signs in real-time. In this scenario, many issues must be considered.
    First, we must evaluate how the system responsible for this task can manage the
    high volume of data generated every second; simultaneously, this system must also
    respond to the user in question. The system’s tolerance to failures in the hardware
    infrastructure and its scalability to the arrival of new data must also be evaluated.
    All these factors are recurrent in the Big Data area and led us to evaluate a
    fault and delay-tolerant architecture that can perform parallel processing with
    the data flow to which it is exposed, dealing both with extraction and generation
    of information and with the transmission of new data to the user in order to help
    him monitor his state. Download : Download high-res image (195KB) Download : Download
    full-size image Fig. 4. General System Architecture Model. The study carried out
    in previous works led us to use the lambda architecture as a basis for modeling
    our proposal due to its performance in solving the issues presented. As explained
    by [36], the lambda architecture has the ability to be linearly scalable and have
    a low delay time for both queries and updates, also dealing with human and hardware
    errors. In the architecture presented in this paper, the layers of the lambda
    architecture have been built as modular and independent services. So, general
    functionalities resemble the lambda architecture base layers but with certain
    characteristics specific to the problem to be solved. 4.1. Measurement collection
    service The first service to be defined is the measurement collection from IoT
    devices. This service performs the role of the lambda architecture’s extraction
    layer, and its responsibilities are always to be available to receive and store
    the measurements recorded by the educational agent sensors. This service comprises
    three main components: the educational agent(s) IoT devices, a Data Lake, and
    an interface that manages the measurements. In this context, the sensor devices
    are part of the Edge computing and Fog computing of the system, so they perform
    their own processing that is not taken to the downstream services. However, at
    the same time, they are the components that send the measurements to the other
    elements in this service. The Data Lake was used because, in our investigations,
    it proved to be a specific type of storage capable of receiving large amounts
    of data at high speed, maintaining its content continuously available. It is also
    important to note that the data inserted in the Data Lake is not properly validated
    or formatted, a disadvantage that is due to its high update speed. Such characteristics
    are highly desirable for the system in question, which must simultaneously handle
    a high volume of real-time measurements from different users. Finally, an interface
    manages the measures, connecting to the Data Lake and making its data standardized.
    So, the use of the service is abstracted concerning the other elements of the
    architecture. It can be observed that this particular component can suffer from
    a high level of requests, which can cause data congestion and possible delays
    in the system. Thus, this component was designed to be scalable vertically and
    horizontally to allocate more computational resources. At the same time, it is
    possible to create replicas of this component and distribute the demand for access
    to the system among them. 4.2. Data flow management service The data flow management
    service plays the role of the speed layer of the architecture. It is responsible
    for performing all the processing logic and distributing the data obtained automatically.
    This service makes all the information exchanges among the other elements of the
    architecture and is also the one that determines when a specific processing flow
    begins. The main functionality of this service is to extract a subset of measurements
    from the collection service, pre-process them, and send them to the model training
    service. The model training service generates a new behavior model for the educational
    agent and produces the information the scheduling service uses to send notifications.
    In this way, the data flow management service can update the educational agent’s
    data, regularly performing this process. Furthermore, to ensure no data congestion,
    this service implements only low latency and computational functions and queries.
    4.3. Behavior models service The behavioral model’s service plays the role of
    the batch layer of the architecture. Its main task is to perform the training
    of machine learning models that perform estimations about the vital signs of an
    individual. This service receives a dataset from an education agent to start a
    training routine of several machine learning models with different parameters
    to find the best match for that dataset. Among the generated models, the best-performing
    one is chosen and used to estimate the most recent educational agent measurements.
    These predictions are sent as results back to the streaming service and to the
    rest of the processing chain. In addition, after training the models, they are
    stored in a database, along with the execution metrics of each model, so that
    further analysis can be performed. It is important to note that each educational
    agent has a specific model since it seeks to identify each educational agent’s
    behavioral pattern. Thus, having a generic model does not make sense since each
    individual has a unique behavioral pattern. 4.3.1. Behavior model Machine learning
    models were designed so that the system would produce, over time, a pattern of
    behavior of the monitored user. In this way, when the system receives a vital
    sign that differs considerably from the prediction made by the trained model,
    an alert routine is triggered, sending a notification to those responsible. The
    data used as input to train the model are the minimum and maximum heart rate,
    the absolute interval between the minimum and maximum, and the heart rate increase
    calculated based on time intervals of measurements. The expected output value
    of the model is the vital sign that the individual should have at that moment.
    Furthermore, the dataset was divided into a training set, a test set, and a validation
    set so that the validation set is used to generate predictions regarding the individual’s
    vital signs. Different machine algorithms were tested in order to select the one
    that would gave us the best results. In Section 5 these details are explained
    in more details. 4.4. Notification scheduling service Finally, we have the notification
    scheduling service. This service performs the function of the service layer in
    the lambda architecture, and its responsibilities are to store the information
    processed by the previous services and to send notifications to educational agents
    at specified times. Three components are defined in this service: an interface
    for receiving data, a database for storing it, and a scheduler that, on specific
    dates and times, executes a routine for sending notifications to educational agents
    in order to alert them about something about their emotional state. First, this
    service receives the timestamp when educational agents’ behavior irregularities
    were detected. Thus, these moments (date and time) are stored and registered in
    the scheduler. Thus, at each registered time, the scheduler starts the function
    that sends a message to the end user, alerting them that a certain instability
    in their vital signs was detected at that given time. It is important to note
    that each service has been designed so that it can function independently. Son,
    it is possible to ensure a high modularity level and replicate certain services,
    thus increasing the vertical scalability of the system as demand increases. In
    addition, the presented architecture also uses a Fog–Cloud paradigm to reduce
    network traffic and the processing load of the server(s). Multiple configurations
    can be used to organize the services in the different computing levels. The services
    have been organized in this proposal as in Fig. 4. The collection service is located
    in the Edge, while the data flow and computation model services have been inserted
    in the Fog node. Finally, the scheduling service has been classified as a Cloud
    computation. Through the related work, this architecture fits adequately for the
    defined purpose. Multiple data sources can be incrementally configured so it does
    not congest the server and can also handle system connection failures from the
    batch and speed layers. In this way, the proposal architecture configures a great
    opportunity to help students and teachers. By sending suggestions to them, it
    is possible to reduce stress moments and these occurrences. Consequently, the
    proposal helps mental health and performance in the educational field since students
    tend to be more relaxed for exams, presentations, and daily academic activities.
    4.5. Solution development In order to confirm the applicability of the proposed
    architecture, a deployment scheme was planned. The services presented in the architecture
    were developed according to the desired characteristics and their specified functionalities.
    Therefore, this section describes the operation of the implemented system, explaining
    its behavior and flow of collected and generated data, its execution components,
    the tools used for its construction, and the configuration of the assembled infrastructure.
    4.5.1. Flow and behavior For the system to function properly, two main actions
    need to be performed: the registration of educational agent users in the system
    and the periodic retrieval of measurements from them. Fig. 5 shows a diagram of
    the data collection, processing, and distribution tasks and the artifacts generated
    along the process. It starts, in a parallel but not interdependent way, with the
    registration of an educational agent in the database and the request to add new
    data collected from this user’s sensors. It is noted that after this, the following
    tasks are initiated by the activation of a scheduled routine, which is executed
    at regular intervals. From this moment on, the process of extracting the data
    sent by the educational agent begins. In order for the system not to be overloaded
    and at the same time be able to generate expressive behavior models, it is previously
    checked if there is a sufficiently large number of measurements to be used. If
    not, the processing is terminated, and the routine is rescheduled later. With
    reasonable measurements available, the system will start transforming the extracted
    data into a standardized and valid version. This routine selects the measurement
    data to train a new behavior model for the user. During the first trials of the
    proposed system, different values for the flow execution limit were tested. In
    the end, it was defined that with less than thirty (30) valid measurements, the
    results were not expressive. This dataset is sent to the model manager, and the
    training task for the Machine Learning model is started there. It is important
    to highlight that, although a new model is being created, an extra number of older
    measurements from the educational agent are added to the dataset so that the models
    can have a consistent behavior throughout their iterations. This task generates
    several different artifacts, including the metrics obtained in each training performed,
    the specification of the model obtained, and a dataset of predictions made about
    the most recent measurements of the individual. This last artifact is sent back
    to the data flow management service to calculate the differences between the last
    measurements obtained and their predictions. Download : Download high-res image
    (320KB) Download : Download full-size image Fig. 5. Data flow diagram of the proposed
    system. The data flow management service then compares the two sets of measurements
    and calculates the divergence rate between the model’s estimated vital sign and
    the educational agent’s current vital sign. If this rate is higher than a stipulated
    percentage threshold, the time when such an unusual pattern occurs is recorded,
    which is done for each measurement. At the end of this process, a list of instability
    times is created for the educational agent and sent to the notification scheduling
    service. The scheduling service receives the specified times and records them
    in the database so that a routine is executed at each stipulated time to notify
    the educational agents, alerting them that instability has been detected. At the
    end of this process, once this flow of tasks is realized for an educational agent,
    the scheduling service can work continuously and independently, always alerting
    the users at the inferred times. Throughout several iterations of this processing,
    only the times of the notifications are updated. In this way, the system can always
    offer a response to the educational agent, and, at the same time, this response
    will follow the change in the individual’s behavior over time. 4.5.2. System components
    In order to implement the proposed system, it was necessary to select different
    technologies and define the relationships between the components created from
    them. Fig. 6 presents a component diagram that expresses the constitution of each
    service designed in the form of its components that have been developed. The measurement
    collection service is composed of an API REST developed using the Python framework,
    called Flask, and the Postgres database. The API is responsible for receiving
    and registering new educational agents’ measurements and managing these users.
    In addition, the API makes it possible to query batches of measurements for the
    other services by specifying a range of measurement dates to be analyzed. A Postgres
    database was selected for this service because, as already mentioned, it is a
    part of the system subject to a high volume of requests. Therefore, a robust enough
    database should be deployed to ensure the system’s robustness. Next is the data
    flow management service, which comprises an open-source data flow development,
    scheduling, and monitoring platform named Apache Airflow [45]. This component
    is one of the most important in the system, as it is the one that performs all
    the implemented processing control. In addition, it serves as a communication
    intermediary between the other services. It is through Airflow that the task stream
    is built, which is in the form of Directed Acyclic Graphs (DAGs). For each educational
    agent registered in the system, a DAG is created for processing their data during
    the monitoring period and executed daily so that new models and consistent predictions
    can be generated. It is also in this component that the execution conditions are
    made, as well as formatting and validations of the data received and sent. Apache
    Airflow was used in this system because of its easy prototyping and open-source
    characteristics. However, alternative tools can be used, such as Apache Kafka
    [46], Talend [47], Pentaho [48], and others. No comparisons were made regarding
    the efficiency of the tools, mainly because this work focuses on verifying if
    it is possible to create a system that can achieve the established objectives,
    regardless of which technology is being used. The behavior model service is composed
    of four components. First, a REST API was implemented in Flask for communication
    with other services. In addition, an open-source platform for managing lifecycles
    of machine learning models (MLflow) was used [49]. With this platform, it is possible
    to monitor the entire training of one or more models simultaneously while all
    models and results obtained are stored and identified over different versions.
    This tool was selected due to its ease of prototyping, deployment, and free availability.
    However, there are other tools for managing machine learning models, such as Amazon
    SageMaker [50], Azure Machine Learning [51], among many others. In order to train
    the models, the data collected from the user’s sensor needs to be preprocessed.
    When the data are sent to the model training process, they come in the format
    presented in Table 1. Download : Download high-res image (186KB) Download : Download
    full-size image Fig. 6. Component diagram of the proposed system. To capture the
    changes in the behavior of the user’s heart rate, we select every sequential pair
    of measures in the dataset base on the column Date, and calculate the minimum,
    maximum, heart rate increase and the difference between the maximum and minimum
    between the two measure, all based on the heart rate. The result of this preprocessing
    is shown in Table 2. Table 1. Table presenting how the sensor’s inputs are received
    in the system. Index Date Heart rate x YYYY-MM-DD H:M:S x We can use the columns
    Minimum, Maximum, Heart Rate Increase, and Min-Max Interval as our input for the
    model. The model can predict the individual’s heart rate after these measures,
    comparing it with the value in the column Heart Rate Final, and it is used to
    evaluate the model’s performance. Table 2. Table describing the input’s formats
    after the preprocessing step. Minimum Maximum Heart rate increase Min–max interval
    Heart rate final x x x x x To select the model that will be trained during the
    system’s execution, we used the same methodology carried out in [52] experiments.
    The tests were performed using different regression models, such as the Decision
    Tree Regressor, K-Nearest Neighbor Regressor, and Support Vector Machine Regressor.
    Due to our computational resource limits and worries about the system’s performance,
    we use generally less complex models with a relatively short response time. The
    algorithms were evaluated using the R-squared score, with values of 0.97204, 0.98256,
    and 0.9734, respectively. Based on this, the K-Nearest Neighbor regression algorithm
    was selected to implement the architecture. The tests were performed using all
    available data. R-squared was used as the decision metric because the main purpose
    was to determine which algorithm would generate the model that best fit the collected
    data’s space. Such a metric is particularly suitable for this task. In this component,
    a training set is executed with the K-Nearest Neighbor regression algorithm, each
    with different parameters, defining a search space for the best possible model
    configuration. This is done through a hyper-parameter optimization library in
    Python called Hyperopt. The available model parameters and their respective ranges
    are presented in the Table 3. The type of algorithm is related to how K-Nearest
    Neighbor will compute the nearest neighbors. The power parameter P is related
    to the parameter passed to compute the Minkowski metric [53]. When , it is equivalent
    to the Manhattan distance. With , the Euclidean distance is used. After all the
    models have been trained with the measurements obtained, the one that achieved
    the best metric concerning the validation set is selected, and this one is established
    as the new educational agent behavior model. Finally, to store the models and
    the execution data, SQLite databases were used. Thus, data can be analyzed anytime
    through the interface provided by MLflow. Finally, we have the components implemented
    in the notification scheduling service, which also comprises an API REST made
    in Flask for data transfer between services. In addition, a Python library for
    task scheduling named APScheduler (Advanced Python Scheduler) has been implemented.
    When the API receives a new list of notification times for an educational user,
    the APScheduler schedules a routine that assembles and sends an email at each
    listed time to the email address provided by the educational user. In addition,
    each routine and its activation date is entered into a SQLite database. This way,
    even if the service does not receive new notification schedules, it can continue
    working autonomously. Table 3. Parameters that can be optimized in the created
    machine learning models and their respective possible values. Algorithm type Number
    of leaf-nodes Number of neighbors Power parameter P Weight function [ball_tree,kd_tree,brute]
    [20,30,40] [2..6] [1.2] [distance, uniform] 4.6. Configurations For the system
    operation during the experimentation, some aspects and characteristics of the
    components’ execution were defined. The first is the logic of selecting time intervals
    of measurements for creating a dataset and, consequently, the training of a model.
    Fig. 7 summarizes how the set of measurements provided by the collection service
    are fragmented to train a machine-learning model. At the beginning of each processing
    run, the workflow manager selects a date range that is a parameter for requesting
    a set of measurements within this range. The dates are mainly calculated using
    the date of the last recorded measurement taken so that the extracted dataset
    has about 50% of the last measurements used in the previous processing. This ensures
    that the generated models remain consistent throughout the updates. In addition,
    the data is separated into three subsets: the training set, the test set, and
    the validation set. The training and test sets are the portion of the data used
    by the model service to train a new machine-learning model. This process does
    not use the validation set, so it represents the educational agent’s most recent
    measurements. It is used solely to generate predictions that serve as a comparison
    in identifying unusual patterns and analyzing model performance. Download : Download
    high-res image (117KB) Download : Download full-size image Fig. 7. Splitting the
    dataset by selecting the start and end dates. In the setup used in the experiment,
    processing takes place in a span of one week. Therefore, the oldest measurements
    correspond to about three days of the data used in the last measurement, and the
    new measurements represent about four days of this interval. In addition, it was
    defined that the validation set for the generation of predictions corresponds
    to 40% of the total available, and the remaining 60% are used for the training
    and test sets, each having 70% and 30% of the available data, respectively. Finally,
    for performance reasons, the number of simultaneous model training trials was
    limited to 10 executions per educational agent. There are other parameters of
    the systems that were also established. Regarding the limit of measurements required
    to run the flow, it was stipulated that if there were less than 30 valid and available
    measurements at that time, the processing would be stopped and rescheduled for
    a later date. Regarding the divergence limit for detecting unusual patterns, it
    was defined that if the predicted value had a difference of more than 30% of the
    actual measured value, the time of the unusual pattern would be recorded and sent
    to the notification escalation service. It is important to note that these values
    can be readjusted according to the system’s needs and users’ demands. 4.6.1. Deployment
    The environment used to deploy the developed system was a standalone server, with
    each service running simultaneously in different processes. The machine used for
    deployment has a 5-core processor and 8 GB of RAM. In addition, an Android1 application
    was used to connect to the Smartwatch and send the measurements, named DT78-App-Android
    [44]. 5. Results In order to carry out the established study, an experiment was
    set up consisting of running the architecture and sending real measurement data
    in a controlled manner so that it is possible to monitor how the system behaves
    in monitoring educational agents. It is also important to note that the data used
    for the experiment were previously collected from educational agents by the measurement
    collection service for three months, between 09/06/2022 and 11/06/2022. In total,
    1,767 measurements were collected and used in the studies. To facilitate the understanding
    of the results and discussions, we present data from one of the volunteers in
    this article. Therefore, a routine was implemented for sending batches of measurements
    to the collection service at periodic intervals. It was defined that the routine
    would send batches of measurements in spaces of one week at intervals of 9 min.
    In addition, the periodicity of execution of the processing flow was also set
    at 10 min. In this way, we ensured that a new set of measurements was available
    at each new flow start. After executing the entire process with this dataset,
    the results of each module were received, processed, and organized by the system
    to be analyzed, also considering meta-data, temporary files, and visualizations
    provided by the used tools. First, the general behavior of the collected measurements
    was observed. Fig. 8 shows a graph of the heart rate distribution among all possible
    values in beats per minute (bpm). It is possible to observe that the samples are
    concentrated between 60 and 80 bpm, with a higher density near 70 bpm. Observing
    a low occurrence of samples between 100 and 120 bpm is also possible, indicating
    some moments in which the user would have an increase in their beats. One of the
    first artifacts that the system produces is the directed acyclic graph of tasks
    (DAG) that is built from the code implemented with Apache Airflow. Fig. 9, Fig.
    10 show how the graph was generated in two different executions. In Fig. 9, the
    system identified insufficient measurements to train and generate a model when
    collecting the data stored in the measurement collection service. Thus, the processing
    tasks were canceled, represented by the pink border of each task, and the system
    moved on to the finalization task. In a subsequent run, the system obtained enough
    measurements for all tasks to be performed correctly, as shown in Fig. 10. Download
    : Download high-res image (143KB) Download : Download full-size image Fig. 8.
    Graph of the distribution of heart rate samples in beats per minute (bpm). The
    first information obtained from the system was the running duration. It was observed
    that the average duration of an execution was 45 s, with a maximum of 55 s and
    a minimum of 5 s. Fig. 11, Fig. 12 show the graphs of the time duration of each
    task in two different executions. The gray part of the bar represents that the
    task has been scheduled and is waiting to start. The black part of the bar represents
    the execution time of the tasks. Download : Download high-res image (102KB) Download
    : Download full-size image Fig. 9. Directed graph generated by Apache Airflow,
    illustrating one of the executions performed in which the processing tasks were
    canceled. Download : Download high-res image (112KB) Download : Download full-size
    image Fig. 10. Directed graph generated by Apache Airflow, illustrating one of
    the executions performed in which all the flow is performed normally. Similarly,
    Fig. 11 is from execution with insufficient measurements, and the following tasks
    were canceled. Fig. 12 illustrates the times of normal execution of the system.
    It is also important to note that the scales of the two graphs are not uniform.
    Similarly, Fig. 13 presents a graph of the cumulative completion time of each
    task over the executions. It shows that the training task of the model is the
    most time-consuming in the processing flow. Download : Download high-res image
    (78KB) Download : Download full-size image Fig. 11. Graph showing the scheduling
    and execution duration times for each task of a run that canceled processing.
    Download : Download high-res image (80KB) Download : Download full-size image
    Fig. 12. Graph showing the duration times of scheduling and execution of each
    task of a run with all tasks performed. Another metric extracted from the experiment
    was the amount of space used by the system and the size of the data generated
    during each flow. Table 4 shows each type of information generated in the experiment
    and their respective sizes in Megabytes (MB). All the data that the system components
    produced, the measurements sent periodically, and temporary data created due to
    transformations and formatting in the data passed between tasks were considered.
    Download : Download high-res image (276KB) Download : Download full-size image
    Fig. 13. Graph of the cumulative completion time of each task. It is noticed that
    after all the executions were performed, a total of 4.12MB of data and metadata
    per educational agent was generated. It is also noted that the metadata generated
    by Apache Airflow occupied most of the data produced by the system. Table 4. Table
    of data generated and their respective sizes considering all experiment executions.
    Generated data Size (MB) Machine Learning training data 0.360 Machine Learning
    model store data 1.1 Measures registered in the Data Lake 0.100 Managing data
    from Apache Airflow 0.940 Execution Metadata from the processing workflow 1.6
    Extra temporary data 0.020 Total 4.12 After the end of the experiment, the registered
    models were analyzed and their metadata studied. The Table 5 shows the parameters
    chosen by the Hyperopt of the best model of each run. We can observe that the
    algorithms ball_tree and kd_tree were the most used. In contrast, when the brute
    algorithm was used, the model obtained the worst performance among the executions.
    These results make sense, mainly if we take into account that both the ball_tree
    and kd_tree algorithms use the information of an aggregated distance for the inputs,
    represented as data points and use it to infer the closest points from each other,
    and at the same time excluding points that are too distant [54], [55]. As could
    be observed in Section 5, the heart rate of the user rarely changes abruptly,
    so similar measures would be more frequent, and this is a characteristic that
    makes these algorithms work better. This is also an advantage for us performance-wise
    because these algorithms are faster than the ball_tree. We can also observe that
    in the Number of Leaf-Nodes column, the specified values of 20, 30, and 40 were
    evenly selected over the executions. The number of leaf nodes does not necessarily
    affect the model’s accuracy but its performance. A larger number of leaf nodes
    can reduce the construction time of the node tree but also increase the memory
    needed to store it. It is also noticeable that there is no clear correlation between
    the number of neighbors selected and the errors calculated for the models, as
    runs 8 and 9 selected two neighbors and had the best results, while run 1 selected
    five neighbors and had a relatively similar result. In most executions, the system
    selected the Euclidean distance by the parameter , with the prediction weight
    function based on the distance. This selection correlates with the observed data
    if we think there is no reason to weigh any of the inputs used, as they are all
    derived from the same source. The heart rate and the preprocessing steps were
    also based on distance calculations. Regarding the performance data of the machine
    learning models, Fig. 14, Fig. 15 present the graphs of the average absolute error
    obtained by the best-trained model in each processing run. Fig. 14 first shows
    the average error for each run. It is noticeable that the models performed similarly
    in most of the training executions. In Fig. 15, the error is shown concerning
    the number of possible groups defined in the K-Nearest Neighbors regression algorithm.
    We can see that in most cases, the models that performed best in training defined
    2 or 4 groups. Table 5. Table with the selected parameters of the best model of
    each run of the processing flow and their respective average errors. Run Algorithm
    type Number of leaf-nodes Number of neighbors Power parameter P Weight function
    Mean absolute error 1 ball_tree 30 5 1 distance 1.093 2 kd_tree 20 4 1 distance
    1.463 3 ball_tree 30 3 2 distance 1.336 4 kd_tree 20 5 1 distance 1.369 5 brute
    40 5 2 distance 2.943 6 kd_tree 30 4 2 distance 1.676 7 kd_tree 40 4 2 distance
    1.449 8 ball_tree 40 2 2 distance 1.028 9 ball_tree 20 2 2 uniform 1.083 Important
    information generated by the system is the list of dates in which relevant differences
    were detected between the heart rate predicted by the machine learning model and
    the actual heart rate at that moment, i.e., unusual pattern detected in the educational
    agent’s heart rate frequency which can indicate a change in their emotional state.
    These data were extracted so that we can analyze how they behave. In Fig. 16,
    we have a graph of the divergence rate detected by the system concerning the hours
    in which unusual patterns were identified. We can see that most of the alerts
    happen between 10 a.m. and 11 p.m. Download : Download high-res image (124KB)
    Download : Download full-size image Fig. 14. Plot of the mean absolute error obtained
    by the models generated in each run. Download : Download high-res image (110KB)
    Download : Download full-size image Fig. 15. Graph of the mean absolute error
    obtained by the generated models in relation to the selected number of neighbors.
    Fig. 17 presents a similar graph but grouped by the days of the week. In this
    figure, we notice that most of the unusual patterns occur in greater intensity
    on Thursday. This can also be confirmed in the histogram of alerts in Fig. 18,
    where we see at the same time that there is a certain correlation between the
    number of unusual patterns detected and the intensity with which they occur. For
    example, there are fewer occurrences of unusual patterns on Monday, Tuesday, and
    Friday, while their occurrences are also relatively low. Download : Download high-res
    image (127KB) Download : Download full-size image Fig. 16. Plot of the divergence
    rates detected over the course of the executions against the times at which they
    occur. The gradient of the points also indicates the degree of divergence. Finally,
    the number of unusual patterns between executions of the processing flow was analyzed.
    Fig. 19 presents this information, showing that in the experiment, a maximum of
    5 alerts were detected in one execution, taking into account a data set of about
    one week. It is also noted that no significant unusual pattern was detected in
    one of the executions. Download : Download high-res image (122KB) Download : Download
    full-size image Fig. 17. Graph of the divergence rates detected over the course
    of the executions in relation to the times of day at which they occur. Download
    : Download high-res image (143KB) Download : Download full-size image Fig. 18.
    Graph of the number of divergences detected throughout the executions in relation
    to the days of the week on which they occur. Download : Download high-res image
    (153KB) Download : Download full-size image Fig. 19. Graph of the number of divergences
    detected in relation to the executions executed. 6. Discussion 6.1. Technological
    analysis The first factor to be analyzed in the results obtained is the duration
    time of the entire processing. We observed that, per educational agent, an average
    of 45 s was necessary to perform the entire dataset processing equivalent to a
    week of measurements. This means that, in a real deployment environment, where
    the execution of the processing flow would also happen in intervals of one week,
    this would be the average time for a single educational agent. However, we must
    consider that several students and teachers would simultaneously use the solution
    in a real environment. Thus, the processing time could increase considerably.
    Certain characteristics and measures were considered in the proposed architecture
    to avoid task congestion. In the results shown, we can observe that the task that
    fills most of the processing time is machine learning model training for heart
    rate prediction. In the experiment, the behavior model service was configured
    to be executed sequentially, so it was possible to observe and extract the data
    correctly. However, this service can be adjusted to manage the requests and perform
    the model training in parallel, thus reducing the response time and the overall
    duration of the execution. In addition, it is also possible to configure the data
    flow management service so that the number of concurrent executions is minimized.
    In this case, the schedules for the start of each user’s process are distributed
    in different periods. Thus, we can avoid that there is a high volume of simultaneous
    tasks in the system. It is also worth mentioning that, as seen in the experiment,
    some executions are canceled due to insufficient data. Such behavior would also
    contribute to better architectural performance on a large scale. According to
    the results, all executions generated a total of 4.12 MB of data per educational
    agent. As ten executions were performed per educational agent, we estimate that
    for a single processing execution for a user, an average of 412 KB would be generated.
    As the number of students and teachers increases, this value grows linearly, so
    we will have behavior as shown in Fig. 20. We can observe that, with 100 educational
    agents and a single execution performed for each one, the total space consumed
    reaches just over 40 MB. This is a relatively low value considering the computational
    resources used for the experiment. However, it is unlikely that this behavior
    will continue with the increase in the number of users. It was seen in the results
    that the metadata of the processing flow executions was one of the largest producers
    of information in the system, also estimating that in a single execution, this
    value would be 160 KB. If we estimate that this data increases at a higher rate
    as the number of users using the system increases, we would have a behavior similar
    to the one presented in Fig. 21, where we would have more than 40 GB generated
    with a single task execution for each user with 100 registered users. Download
    : Download high-res image (126KB) Download : Download full-size image Fig. 20.
    Linear projection plot of the size of space consumed by the system against the
    number of users. Some approaches can be taken to reduce data accumulation. The
    main one would be to perform periodic memory cleanups on the system, removing
    or archiving the oldest data. If we adopt such measures, the system could have
    a scalable storage rate. Download : Download high-res image (133KB) Download :
    Download full-size image Fig. 21. Linear projection plot of the size of space
    consumed by the system against the number of users, assuming that the processing
    metadata also increases linearly. It was noticed that the established services
    had relatively high operational independence when analyzing some design choices
    of the proposal deployment. For example, in one of the executions, the system
    did not detect any relevant irregularity in the user’s heart rate, so the escalation
    service did not schedule new notifications. Thus, this service could continue
    functioning in the same way as before. The same can also be observed for the measurement
    collection service. This works practically independently of the other services,
    so it can always be available for recording and consulting measurements. In addition,
    it was noticed that the collection service was the one that most received and
    sent requests, confirming the hypothesis that this is the part of the system that
    is most likely to have data congestion. Taking this into account, the components
    of this service could be easily replicated, and the demand received balanced between
    the replicas to decrease the system delay. These characteristics show that these
    services can continue to work even if the other elements of the architecture stop
    or are unavailable, showing the high level of availability of the architecture.
    Another advantageous feature of the implemented system is the ease with which
    new prediction models can be installed. As long as the model respects the communication
    interfaces between the components, almost any kind of prediction technique can
    be applied to the collected data and inserted into the system quickly. Moreover,
    with the support of the technologies used, it is possible to compare different
    implemented models, even between different users throughout the monitoring. 6.2.
    Unusual patterns analysis Let us look at the metrics from the regression models
    generated throughout the simulation; most obtained a similar average error, ranging
    between 1 and 2 bpm. However, it is also seen that in the execution performed,
    the model obtained a considerably higher error, reaching 3 bpm. In this way, the
    worst-generated model made predictions with an average of 3 bpm. Now, if we analyze
    this data based on what is shown in Fig. 14, Fig. 19, we see that the number of
    unusual pattern detections when this model is generated was the highest possible.
    Still, it is noted that in the previous execution, the number of detections was
    the same, and the model built by it obtained a relatively lower average error.
    Therefore, it is uncertain whether there is a correlation between the average
    prediction error of the model and the amount of alerts generated by the model.
    It is also worth noting that the similar performance values of the models are
    due to the regularity in the user’s heart rate, as seen in Fig. 8. Thus, we see
    that the models can capture the overall behavior of the user’s heart rate over
    different executions. Examining now the behavior of the unusual patterns detected
    by the system, we see in Fig. 16 that they occur mostly during the day, which
    makes sense if we consider the educational agent’s routine. In addition, the system
    could perceive certain abnormalities in the user’s heart rate later, between midnight
    and two in the morning. Such information could indicate that the individual has
    a problem with sleeping or even during sleeping. Looking at the unusual patterns
    over the days of the week, Fig. 17, Fig. 18 show that there is a certain tendency
    for the system to detect divergences in similar amounts on adjacent days of the
    week considering a specific volunteer. This is mainly due to the routine of activities
    performed by the individual examined over the weeks, and this volunteer confirmed
    this information. We can conclude that the system identified the educational agent’s
    behavior patterns. In addition, it was understood that the high number of alerts
    on Thursday made sense after being clarified by the volunteer that, throughout
    the experiment, Thursday was one of the days of the week with more activities
    to be performed, generating more anxiety in the volunteer. Therefore, the system
    could infer the moments when the user had irregularities in their heart rate.
    Considering the trained models and the environment setup, when the system receives
    new data from educational agents (body data), it recommends some action in case
    of unusual pattern detection. This study defined two types of notifications for
    students and teachers. One is related to immediate activities, and the other is
    preventive activities since both decrease stress levels. Hence, it is up to users
    to choose which one best suits the environment in which they find themselves.
    A sample of notification in this study occurred when the system recommended diaphragmatic
    breathing and Brain break activities when the models detected a high stress level.
    Also, the system detected persistent nervousness and stress within the volunteer’s
    routine, with frequent anxiety peaks. Therefore, preventive activities were recommended
    to make such heart rate hikes less and less recurrent. Another notification sampling
    occurred when the system detected the student’s high heart rate and recommended
    a paused breath. However, the student gave us feedback and said he had just climbed
    a staircase and went straight to class. In this case, the student returned to
    their normal heart state after rest, and the system did not need to send the recommendation.
    Therefore, it is interesting to emphasize and realize the vital role of location
    data in achieving the purpose of the idealized proposal. Through these, it is
    possible to promote a better accuracy of the moments of sending recommendations.
    Regarding our research question Is it possible to use IoT devices and develop
    an e-Health systems capable of alerting educational agents about their behavior
    in a personalized, automatic, and effective way We can state that the proposed
    solution can monitor each educational agent individually and automatically, generating
    personalized notifications. The performance obtained during the experiments and
    the feedback provided by the educational agents involved show that our solution
    can be used to monitor and help people in stressful moments. However, some limitations
    were observed, and we summarized them in the next section. Observing the system’s
    behavior, we can see that the notifications could help the education agents understand
    and change their daily activities. After investigating the literature, we found
    possible activities individuals can do to alleviate stress and anxiety moments.
    There are immediate activities (Brain Breaks, diaphragmatic breathing, progressive
    muscle relaxation, and positive self-talk) [42], and preventive activities (physical
    activities, meditation, outdoor relaxation, seeking medical help) [43]. So, depending
    on the student’s context, we can recommend certain actions to reduce these harmful
    states, contributing to the progress of their performance. 6.3. Limitations Some
    questions about the results found also need to be analyzed. First, we have to
    check for consistency in the system’s sending of notifications. We can assume
    that there are certain times when the user will not see the notification immediately,
    such as when they are sleeping. To solve this, we could add a feature allowing
    the user to define when he can receive notifications. Thus, notifications at unavailable
    times would be delayed to be sent correctly. Another issue evaluated during the
    experiment was the dispersion and scarcity of measurements at certain times, caused
    mainly by connectivity failures between the Smartwatch and the Android application,
    in addition to certain voluntary disconnections of the user due to external factors.
    It was noticed that such data omissions generate a high instability of system
    prediction, causing less coherent responses. This problem can be mitigated by
    preventing the system from generating models without valid measurements. Therefore,
    stable and regular monitoring is necessary for the system to provide reliable
    answers. 7. Conclusions The architecture proposed in this work has the task of
    helping educational agents in their daily lives to identify possible behaviors
    harmful to their physical and mental health. For this, it was planned to develop
    a e-health system based on lambda architecture. The system was designed in a Fog–Cloud
    computing structure to achieve greater modularity of its components and less data
    congestion. Having such a system built, an experiment to evaluate the system’s
    effectiveness in producing relevant results was conducted. Educational agents
    (students and teachers) were selected to wear IoT devices to monitor their vital
    signs and collect the data that served as input for processing their behavior
    profiles. Finally, an analysis of the information produced by the system was conducted,
    and it was concluded that it is indeed possible to have a greater understanding
    of the health of educational agents with the help of the proposed project. Several
    researches and works have been conducted to find different solutions for monitoring
    people’s health through their daily activities in different scenarios. In [52],
    [56], [57] investigations have been carried out to develop similar solutions,
    mainly in the field of education, to help students suffering from anxiety and
    depression problems under stressful conditions. In addition, there have also been
    efforts to design versions of equivalent architectures. In [58], a first definition
    of the architecture was established, whose main objective was to send behavior
    recommendations to the end user. In [59], a similar notification system was developed
    and implemented. These works contributed to the studies and tasks described in
    this paper. Through an experiment with people working in the academic environment
    (teachers and students), we answered the research question of this work: Is it
    possible to use IoT devices and develop an e-Health systems capable of alerting
    educational agents about their behavior in a personalized, automatic, and effective
    way. We highlight that it is possible to monitor educational agents using IoT
    devices data and alert them when a moment of stress and anxiety is detected. The
    results were promising, encouraging us to expand the solution. Some future directions
    are described in the next section. 7.1. Future works After the conclusions about
    the studies were made, some questions and possible experiments that can be investigated
    later were identified. One of the main points to be investigated later is the
    extraction of different types of user vital signs so that it is possible to understand
    their behavior throughout their daily activities better and offer greater data
    expressiveness for generating prediction models. Also, it was noticed that there
    is a certain difficulty in interpreting the system’s responses due to a need for
    more contextualization of the extracted data. Therefore, it is intended to extend
    the system to collect data from the context in which the user is located, such
    as their location. There are also plans to conduct experiments using different
    prediction models so that we can more consistently represent the educational agents’
    behavior and identify possible problems. Finally, it is intended to deploy the
    system in a more robust hardware configuration so that it is possible to conduct
    experiments with a larger number of educational agents and to stress the architecture
    to verify its efficiency. CRediT authorship contribution statement Wagno Leão
    Sergio: Writing – original draft, Software, Data curation. Victor Ströele: Writing
    – review & editing, Supervision, Methodology, Formal analysis, Data curation,
    Conceptualization. Mário Dantas: Writing – review & editing, Supervision, Methodology,
    Conceptualization. Regina Braga: Writing – review & editing, Supervision, Conceptualization.
    Douglas Dyllon Macedo: Writing – review & editing, Conceptualization. Declaration
    of competing interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Acknowledgment This work was partially funded by
    UFJF/Brazil, CAPES/Brazil, CNPq/Brazil (grant 435313/2018-5), (grant: 307194/2022-1)
    and FAPEMIG/Brazil (grant: APQ-02685-17). Data availability The dataset link is
    available in the text of the article. References [1] Hamdan K.M., Al-Bashaireh
    A.M., Zahran Z., Al-Daghestani A., Samira A.-H., Shaheen A.M. University students’
    interaction, internet self-efficacy, self-regulation and satisfaction with online
    education during pandemic crises of COVID-19 (SARS-CoV-2) Int. J. Educ. Manag.
    (2021) Google Scholar [2] Vaziri H., Casper W.J., Wayne J.H., Matthews R.A. Changes
    to the work–family interface during the COVID-19 pandemic: Examining predictors
    and implications using latent transition analysis J. Appl. Psychol., 105 (10)
    (2020), p. 1073 CrossRefView in ScopusGoogle Scholar [3] Carroll N., Conboy K.
    Normalising the “new normal”: Changing tech-driven work practices under pandemic
    time pressure Int. J. Inf. Manage., 55 (2020), Article 102186 View PDFView articleView
    in ScopusGoogle Scholar [4] Pakhomova T.O., Komova O.S., Belia V.V., Yivzhenko
    Y.V., Demidko E.V. Transformation of the pedagogical process in higher education
    during the quarantine Linguist. Cult. Rev., 5 (S2) (2021), pp. 215-230, 10.21744/lingcure.v5ns2.1341
    Google Scholar [5] Lima M.D.S., Maciel R.S.P. Practices and digital technological
    resources for remote education: an investigation of Brazilian professor’s profile
    Anais do XXXII Simpósio Brasileiro de Informática na Educação, SBIE 2021, Sociedade
    Brasileira de Computação - SBC (2021), 10.5753/sbie.2021.218676 Google Scholar
    [6] Bülow M.W. Designing synchronous hybrid learning spaces: Challenges and opportunities
    Understanding Teaching-Learning Practice, Springer International Publishing (2022),
    pp. 135-163, 10.1007/978-3-030-88520-5_9 Google Scholar [7] Leite D., Santos H.,
    Rodrigues A., Monteiro C., Maciel A. A hybrid learning approach for subjects on
    software development of automation systems, combining PBL, gamification and virtual
    reality Anais do XXXII Simpósio Brasileiro de Informática na Educação, SBIE 2021,
    Sociedade Brasileira de Computação - SBC (2021), 10.5753/sbie.2021.218455 Google
    Scholar [8] Kastornova V.A., Gerova N.V. Use of hybrid learning in school education
    in France 2021 1st International Conference on Technology Enhanced Learning in
    Higher Education, TELE, IEEE (2021), 10.1109/tele52840.2021.9482527 Google Scholar
    [9] Li Q., Li Z., Han J. A hybrid learning pedagogy for surmounting the challenges
    of the COVID-19 pandemic in the performing arts education Educ. Inf. Technol.,
    26 (6) (2021), pp. 7635-7655, 10.1007/s10639-021-10612-1 View in ScopusGoogle
    Scholar [10] Chaturvedi K., Vishwakarma D.K., Singh N. COVID-19 and its impact
    on education, social life and mental health of students: A survey Child. Youth
    Serv. Rev., 121 (2021), Article 105866, 10.1016/j.childyouth.2020.105866 View
    PDFView articleView in ScopusGoogle Scholar [11] Hall G., Laddu D.R., Phillips
    S.A., Lavie C.J., Arena R. A tale of two pandemics: How will COVID-19 and global
    trends in physical inactivity and sedentary behavior affect one another? Prog.
    Cardiovasc. Dis., 64 (2021), p. 108 View PDFView articleView in ScopusGoogle Scholar
    [12] Pitanga F.J.G., Beck C.C., Pitanga C.P.S. Physical activity and reducing
    sedentary behavior during the coronavirus pandemic Arq. Bras. Cardiol., 114 (2020),
    pp. 1058-1060 CrossRefView in ScopusGoogle Scholar [13] Souza A.P.d.S., Silva
    M.R.M., Silva A., Lira P., Silva J., Silva M., et al. Anxiety symptoms in university
    professors during the COVID-19 pandemic Health Sci. J., 14 (2020) Google Scholar
    [14] Pascoe M.C., Hetrick S.E., Parker A.G. The impact of stress on students in
    secondary school and higher education Int. J. Adolesc. Youth, 25 (1) (2019), pp.
    104-112, 10.1080/02673843.2019.1596823 Google Scholar [15] Gustems-Carnicer J.,
    Calderón C., Calderón-Garrido D. Stress, coping strategies and academic achievement
    in teacher education students Eur. J. Teach. Educ., 42 (3) (2019), pp. 375-390,
    10.1080/02619768.2019.1576629 View in ScopusGoogle Scholar [16] REIS H.M., ALVARES
    D., JAQUES P.A., ISOTANI S. A proposal of model of emotional regulation in intelligent
    learning environments Inform. Educ. (2021), 10.15388/infedu.2021.15 Google Scholar
    [17] Artazcoz L., Cortès I., Escribà-Agüir V., Cascant L., Villegas R. Understanding
    the relationship of long working hours with health status and health-related behaviours
    J. Epidemiol. Community Health, 63 (7) (2009), pp. 521-527 CrossRefView in ScopusGoogle
    Scholar [18] Prasad B., Thakur C. Chronic overworking: Cause extremely negative
    impact on health and quality of life Int. J. Adv. Microbiol. Health Res., 3 (1)
    (2019), p. 5 View in ScopusGoogle Scholar [19] Sutton A. Measuring the effects
    of self-awareness: Construction of the self-awareness outcomes questionnaire Eur.
    J. Psychol., 12 (4) (2016), p. 645 CrossRefView in ScopusGoogle Scholar [20] Silva
    G., Stroele V., Dantas M., Campos F. Hold Up: Modelo de Detecção e Controle de
    emoções em Ambientes Acadêmicos Brazilian Symposium on Computers in Education
    (Simpósio Brasileiro de Informática na Educação-SBIE), 30 (2019), p. 139 Google
    Scholar [21] Misra R., McKean M., West S., Russo T. Academic stress of college
    students: Comparison of student and faculty perceptions. Coll. Stud. J., 34 (2)
    (2000) Google Scholar [22] Gorman J.M., Sloan R.P. Heart rate variability in depressive
    and anxiety disorders Am. Heart J., 140 (4) (2000), pp. S77-S83 View PDFView articleView
    in ScopusGoogle Scholar [23] Rejeb A., Rejeb K., Treiblmaier H., Appolloni A.,
    Alghamdi S., Alhasawi Y., Iranmanesh M. The Internet of Things (IoT) in healthcare:
    Taking stock and moving forward Internet Things, 22 (2023), Article 100721, 10.1016/j.iot.2023.100721
    View PDFView articleView in ScopusGoogle Scholar [24] Norman C.D., Skinner H.A.
    Ehealth literacy: essential skills for consumer health in a networked world J.
    Med. Internet Res., 8 (2) (2006), Article e506 Google Scholar [25] Rimal B.P.,
    Choi E., Lumb I., et al. A taxonomy and survey of cloud computing systems 2009
    Fifth International Joint Conference on INC, IMS and IDC, IEEE (2009), pp. 44-51
    CrossRefView in ScopusGoogle Scholar [26] Dillon T., Wu C., Chang E., et al. Cloud
    computing: issues and challenges 2010 24th IEEE International Conference on Advanced
    Information Networking and Applications, IEEE (2010), pp. 27-33 CrossRefView in
    ScopusGoogle Scholar [27] Vitasari P., Wahab M.N.A., Othman A., Herawan T., Sinnadurai
    S.K. The relationship between study anxiety and academic performance among engineering
    students Procedia-Soc. Behav. Sci., 8 (2010), pp. 490-497 View PDFView articleView
    in ScopusGoogle Scholar [28] Sohail N. Stress and academic performance among medical
    students J. Coll. Phys. Surg. Pak., 23 (1) (2013), pp. 67-71 View in ScopusGoogle
    Scholar [29] Bamber M.D., Morpeth E. Effects of mindfulness meditation on college
    student anxiety: a meta-analysis Mindfulness, 10 (2) (2018), pp. 203-214, 10.1007/s12671-018-0965-5
    Google Scholar [30] Yamato Y., Kumazaki H., Fukumoto Y., et al. Proposal of lambda
    architecture adoption for real time predictive maintenance 2016 Fourth International
    Symposium on Computing and Networking, CANDAR (2016), pp. 713-715, 10.1109/CANDAR.2016.0130
    View in ScopusGoogle Scholar [31] Melillo P., Bracale M., Pecchia L. Nonlinear
    heart rate variability features for real-life stress detection. Case study: students
    under stress due to university examination BioMed. Eng. OnLine, 10 (1) (2011),
    p. 96, 10.1186/1475-925x-10-96 View in ScopusGoogle Scholar [32] Ciabattoni L.,
    Ferracuti F., Longhi S., Pepa L., Romeo L., Verdini F. Real-time mental stress
    detection based on smartwatch 2017 IEEE International Conference on Consumer Electronics,
    ICCE, IEEE (2017), pp. 110-111 CrossRefView in ScopusGoogle Scholar [33] Hasanbasic
    A., Spahic M., Bosnjic D., Mesic V., Jahic O., et al. Recognition of stress levels
    among students with wearable sensors 2019 18th International Symposium INFOTEH-JAHORINA,
    INFOTEH, IEEE (2019), pp. 1-4 CrossRefGoogle Scholar [34] MaksimovicVladimir M.,
    Vujovic V. Internet of things based E-health systems: Ideas, expectations and
    concerns Handbook of Large-Scale Distributed Computing in Smart Healthcare (2017),
    pp. 241-280 Google Scholar [35] Larcher L., Ströele V., Dantas M., Bauer M. Event-driven
    framework for detecting unusual patterns in aal environments 2020 IEEE 33rd International
    Symposium on Computer-Based Medical Systems, CBMS, IEEE (2020), pp. 309-314 CrossRefView
    in ScopusGoogle Scholar [36] Kiran M., Murphy P., Monga I., Dugan J., Baveja S.S.
    Lambda architecture for cost-effective batch and speed big data processing 2015
    IEEE International Conference on Big Data, Big Data, IEEE (2015), pp. 2785-2792
    CrossRefView in ScopusGoogle Scholar [37] Aditya S., Tibarewala D. Comparing ANN,
    LDA, QDA, KNN and SVM algorithms in classifying relaxed and stressful mental state
    from two-channel prefrontal EEG data Int. J. Artif. Intell. Soft Comput., 3 (2)
    (2012), pp. 143-164 CrossRefGoogle Scholar [38] J. Woodward, Y.P. Chen, K. Jurczyk,
    K.M. Ross, L. Anthony, J. Ruiz, A survey of notification designs in commercial
    mHealth apps, in: Extended Abstracts of the 2021 CHI Conference on Human Factors
    in Computing Systems, 2021, pp. 1–7. Google Scholar [39] Priya A., Garg S., Tigga
    N.P. Predicting anxiety, depression and stress in modern life using machine learning
    algorithms Procedia Comput. Sci., 167 (2020), pp. 1258-1267 View PDFView articleView
    in ScopusGoogle Scholar [40] Munir A., Kansakar P., Khan S.U., et al. IfcIoT:
    Integrated fog cloud IoT: A novel architectural paradigm for the future Internet
    of Things IEEE Consum. Electron. Mag., 6 (3) (2017), pp. 74-82 View in ScopusGoogle
    Scholar [41] Verma P., Sood S.K. A comprehensive framework for student stress
    monitoring in fog-cloud IoT environment: m-health perspective Med. Biol. Eng.
    Comput., 57 (1) (2018), pp. 231-244, 10.1007/s11517-018-1877-1 Google Scholar
    [42] Zaccaro A., Piarulli A., Laurino M., Garbella E., Menicucci D., Neri B.,
    Gemignani A. How breath-control can change your life: A systematic review on psycho-physiological
    correlates of slow breathing Front. Hum. Neurosci., 12 (2018), 10.3389/fnhum.2018.00353
    Google Scholar [43] Carter T., Pascoe M., Bastounis A., Morres I.D., Callaghan
    P., Parker A.G. The effect of physical activity on anxiety in children and young
    people: a systematic review and meta-analysis J. Affect. Disord., 285 (2021),
    pp. 10-21, 10.1016/j.jad.2021.02.026 View PDFView articleView in ScopusGoogle
    Scholar [44] Fbiego T. FBIEGO/DT78-app-android: alternative app for the DT78 smartwatch
    (2023) GitHub, URL https://github.com/fbiego/DT78-App-Android Google Scholar [45]
    Haines S. Workflow orchestration with apache airflow Modern Data Engineering with
    Apache Spark: A Hands-on Guide for Building Mission-Critical Streaming Applications,
    Springer (2022), pp. 255-295 CrossRefGoogle Scholar [46] Garg N. Apache Kafka
    Packt Publishing Birmingham, UK (2013) Google Scholar [47] Sreemathy J., Nisha
    S., RM G.P., et al. Data integration in ETL using TALEND 2020 6th International
    Conference on Advanced Computing and Communication Systems, ICACCS, IEEE (2020),
    pp. 1444-1448 CrossRefView in ScopusGoogle Scholar [48] Roldán M.C. Pentaho 3.2
    Data Integration: Beginner’s Guide Packt Publishing Ltd (2010) Google Scholar
    [49] Zaharia M., Chen A., Davidson A., Ghodsi A., Hong S.A., Konwinski A., Murching
    S., Nykodym T., Ogilvie P., Parkhe M., et al. Accelerating the machine learning
    lifecycle with MLflow IEEE Data Eng. Bull., 41 (4) (2018), pp. 39-45 Google Scholar
    [50] D. Nigenda, Z. Karnin, M.B. Zafar, R. Ramesha, A. Tan, M. Donini, K. Kenthapadi,
    Amazon sagemaker model monitor: A system for real-time insights into deployed
    machine learning models, in: Proceedings of the 28th ACM SIGKDD Conference on
    Knowledge Discovery and Data Mining, 2022, pp. 3671–3681. Google Scholar [51]
    Barnes J. Azure machine learning Microsoft Azure Essentials (first ed.), Microsoft
    (2015) Google Scholar [52] Leao W.S., Di iorio Silva G., Ströele V., Dantas M.,
    Campos F., Braga R., David J.M.N. A technological monitoring architecture for
    academics’ mental and physical health Anais do XXXIII Simpósio Brasileiro de Informática
    na Educação, SBC (2022), pp. 846-858 CrossRefGoogle Scholar [53] De Amorim R.C.,
    Mirkin B. Minkowski metric, feature weighting and anomalous cluster initializing
    in K-means clustering Pattern Recognit., 45 (3) (2012), pp. 1061-1075 View in
    ScopusGoogle Scholar [54] Bentley J.L. Multidimensional binary search trees used
    for associative searching Commun. ACM, 18 (9) (1975), pp. 509-517, 10.1145/361002.361007
    View in ScopusGoogle Scholar [55] Omohundro S.M. Five Balltree Construction Algorithms
    International Computer Science Institute Berkeley (1989) Google Scholar [56] Di
    iorio Silva G., Sergio W.L., Ströele V., Dantas M.A. A watchdog proposal to a
    personal e-health approach International Conference on Advanced Information Networking
    and Applications, Springer (2022), pp. 81-94 CrossRefView in ScopusGoogle Scholar
    [57] Leao W.S., Di iorio Silva G., Ströele V., Dantas M. Uma arquitetura fog-cloud
    para o monitoramento de sinais corporais Anais Estendidos Do XXII SimpÓSio Em
    Sistemas Computacionais de Alto Desempenho, SBC (2021), pp. 9-16 CrossRefGoogle
    Scholar [58] G. Di iorio Silva, W.L. Sergio, V. Ströele, M.A. Dantas, ASAP-Academic
    Support Aid Proposal for Student Recommendations, in: International Conference
    on Advanced Information Networking and Applications, AINA-2021, 2021, pp. 40–53.
    Google Scholar [59] Sergio W.L., di Iorio Silva G., Ströele V., Dantas M.A. An
    architecture proposal to support E-healthcare notifications International Conference
    on Advanced Information Networking and Applications, Springer (2023), pp. 157-170
    CrossRefView in ScopusGoogle Scholar Cited by (0) 1 In this work, we used an Android
    application, as the devices used were compatible. However, other mobile device
    platforms can be used without adapting the proposed architecture. View Abstract
    © 2024 Elsevier B.V. All rights reserved. Part of special issue Artificial Intelligence
    of Things in Education Edited by Santi Caballé, Nicola Capuano, Victor Ströele
    View special issue Recommended articles Human-in-the-loop machine learning: Reconceptualizing
    the role of the user in interactive approaches Internet of Things, Volume 25,
    2024, Article 101048 Oihane Gómez-Carmona, …, Javier García-Zubia View PDF Forecasting
    energy power consumption using federated learning in edge computing devices Internet
    of Things, Volume 25, 2024, Article 101050 Eduardo Montagner de Moraes Sarmento,
    …, Rodolfo da Silva Villaça View PDF Distributed soft video transmission based
    on hybrid digital and analog scheme Internet of Things, Volume 25, 2024, Article
    101011 Xuechen Chen, …, Xiaoheng Deng View PDF Show 3 more articles Article Metrics
    Captures Readers: 5 View details About ScienceDirect Remote access Shopping cart
    Advertise Contact and support Terms and conditions Privacy policy Cookies are
    used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply."'
  inline_citation: '>'
  journal: Internet of Things (Netherlands)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Enhancing well-being in modern education: A comprehensive eHealth proposal
    for managing stress and anxiety based on machine learning'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Risco S.
  - Alarcón C.
  - Langarita S.
  - Caballer M.
  - Moltó G.
  citation_count: '0'
  description: Serverless computing was a breakthrough in Cloud computing due to its
    high elasticity capabilities and fine-grained pay-per-use model offered by the
    main public Cloud providers. Meanwhile, open-source serverless platforms supporting
    the FaaS (Function as a Service) model allow users to take advantage of many of
    their benefits while operating on the on-premises platforms of organizations.
    This opens the possibility to deploy and exploit them on the different layers
    of the cloud-to-edge continuum, either on IoT (Internet of Things) devices located
    at the Edge (i.e. next to data acquisition devices), in on-premises clusters closer
    to the data sources (i.e. Fog computing) or directly on the Cloud. This paper
    presents two strategies to mitigate the overload that disparate data ingestion
    rates may cause in low-powered devices at the Edge or Fog layers. To this end,
    it is proposed to delegate and reschedule serverless jobs between the different
    layers of the cloud-to-edge continuum using an open-source platform for event-driven
    file processing. To demonstrate the performance of these strategies, a use case
    for fire detection is proposed that includes processing in the Fog via minified
    Kubernetes clusters located near the Edge, in the private Cloud via on-premises
    elastic clusters and, finally, in the public Cloud by using the AWS (Amazon Web
    Services) Lambda FaaS service. The results indicate that these strategies can
    mitigate overloads in use cases involving processing across the cloud-to-edge
    continuum by coordinating several layers of computing resources.
  doi: 10.1016/j.future.2023.12.015
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. Proposed
    architecture 4. Use case: Serverless fire detection across the cloud-to-edge continuum
    5. Conclusions and future work CRediT authorship contribution statement Declaration
    of competing interest Acknowledgements Data availability References Vitae Show
    full outline Figures (10) Show 4 more figures Future Generation Computer Systems
    Volume 153, April 2024, Pages 457-466 Rescheduling serverless workloads across
    the cloud-to-edge continuum Author links open overlay panel Sebastián Risco, Caterina
    Alarcón, Sergio Langarita, Miguel Caballer, Germán Moltó Show more Share Cite
    https://doi.org/10.1016/j.future.2023.12.015 Get rights and content Under a Creative
    Commons license open access Highlights • Two strategies are given to reschedule
    workloads in the cloud-to-edge continuum. • The open-source OSCAR serverless platform
    has been extended to implement them. • A use case on wildfire detection involving
    edge, cloud and FaaS is addressed. • The cloud-to-edge continuum better mitigates
    increased serverless workloads. Abstract Serverless computing was a breakthrough
    in Cloud computing due to its high elasticity capabilities and fine-grained pay-per-use
    model offered by the main public Cloud providers. Meanwhile, open-source serverless
    platforms supporting the FaaS (Function as a Service) model allow users to take
    advantage of many of their benefits while operating on the on-premises platforms
    of organizations. This opens the possibility to deploy and exploit them on the
    different layers of the cloud-to-edge continuum, either on IoT (Internet of Things)
    devices located at the Edge (i.e. next to data acquisition devices), in on-premises
    clusters closer to the data sources (i.e. Fog computing) or directly on the Cloud.
    This paper presents two strategies to mitigate the overload that disparate data
    ingestion rates may cause in low-powered devices at the Edge or Fog layers. To
    this end, it is proposed to delegate and reschedule serverless jobs between the
    different layers of the cloud-to-edge continuum using an open-source platform
    for event-driven file processing. To demonstrate the performance of these strategies,
    a use case for fire detection is proposed that includes processing in the Fog
    via minified Kubernetes clusters located near the Edge, in the private Cloud via
    on-premises elastic clusters and, finally, in the public Cloud by using the AWS
    (Amazon Web Services) Lambda FaaS service. The results indicate that these strategies
    can mitigate overloads in use cases involving processing across the cloud-to-edge
    continuum by coordinating several layers of computing resources. Previous article
    in issue Next article in issue Keywords Cloud computingCloud-to-edge continuumContainersFaaSKubernetesServerless
    computing 1. Introduction The cloud-to-edge continuum (or computing continuum)
    [1] encompasses a wide variety of components that may include low-powered devices
    with limited computer resources, on-premises servers with moderate resources,
    expensive high-performance computers and public cloud platforms. This is in line
    with the definition by the OpenFog Reference Architecture for Fog Computing, stating
    that it is a system-level architecture that distributes computing, storage, control
    and networking functions closer to the users along a continuum [2]. Indeed, the
    SPEC-RG reference architecture for the edge continuum [3] proposes an architecture
    for task offloading according to five computing models: Mist computing, edge computing,
    multi-access edge computing, fog computing and mobile cloud computing. Mist computing
    is sometimes used interchangeably with fog computing, even if some authors point
    to subtle differences [4]. This distributed computing paradigm extends cloud computing
    capacities into the edge of the network to bring computation closer to the data
    source and the end devices such as sensors and other IoT (Internet of Things)
    devices [5]. In this paradigm, the edge devices collect data that is locally processed
    at the edge of the network to the extent that it is possible due to the computing
    capacity constraints of such devices. Workload is offloaded into the Cloud when
    additional computing power is required, thus effectively using the cloud-to-edge
    continuum. This approach offers several benefits: • Reduced latency: By processing
    data locally, mist computing reduces the time it takes to transmit data to the
    cloud and receive a response. This is particularly important for real-time applications
    that require immediate decision-making. • Bandwidth optimization: Sending large
    volumes of data to the cloud can strain network bandwidth. Mist computing filters
    and processes data locally, reducing the amount of data that needs to be transmitted
    to the cloud. Only relevant or summarized data is sent, optimizing bandwidth usage.
    • Enhanced privacy and security: Some applications, such as those involving sensitive
    data or strict privacy requirements, can benefit from keeping data locally and
    reducing the need for data transfer over public networks. Mist computing allows
    sensitive data to be processed and analysed closer to its source, improving privacy
    and security. • Offline operation: In scenarios where intermittent connectivity
    to the cloud is common, mist computing enables devices to continue operating and
    processing data locally even when disconnected from the cloud. This ensures uninterrupted
    functionality and allows for offline data analysis if the computing capacity of
    the devices is not exceeded. However, the execution along the cloud-to-edge continuum
    involves several challenges that need to be addressed, as identified by the work
    of Mouradian et al. [6]. This work highlights “task scheduling” and “offloading
    and load redistribution” as key features for computing in scenarios related to
    fog computing. In this scenario, serverless has risen in recent years as an event-driven
    computing paradigm involving services where the service provider manages the underlying
    computational infrastructure entirely. This has paved the way for the surge of
    open-source serverless platforms to be deployed on on-premises resources that
    mimic this abstraction layer for the developers. These typically involve Container
    Orchestration Platforms, such as Kubernetes, which provide seamless resource allocation.
    This is the case of KNative [7], OpenFaaS [8] and, as addressed in this paper,
    OSCAR [9]. These platforms provide the required abstractions to execute functions
    or applications, packaged as Docker images, with dynamic provisioning of resources.
    To this aim, this work presents the following contributions: First, a novel approach
    for rescheduling workloads on a serverless platform that can run along the cloud-to-edge
    continuum. This attempts to mitigate the disparate workload distribution across
    the multiple layers of this continuum to profit from additional computing resources,
    especially when involving devices with constrained computing resources. Second,
    an implementation of the proposed approach is done in the OSCAR1 open-source serverless
    platform, together with an assessment of the functionality on a realistic use
    case on wildfire detection. To the best of the authors’ knowledge, this provides
    the first implementation of a job rescheduling system for serverless computing
    across the cloud-to-edge continuum, provided as a ready-to-use implementation
    in an existing open-source framework. The remainder of the paper is structured
    as follows. First, Section 2 discusses the related works. Next, Section 3 introduces
    an architecture to support job delegation and rescheduling across event-driven
    serverless platforms. Later, Section 4 introduces a use case on serverless fire
    detection along the cloud-to-edge continuum to assess the benefits of the proposed
    approach. Finally, Section 5 summarizes the main achievements and discusses future
    work. 2. Related work Several works in the state-of-the-art focus on the scheduling
    of serverless workloads. For example, the work by Zhang et al. [10] introduces
    the cost of execution as a requirement for scheduling serverless analytics tasks.
    They introduce a task scheduler that minimizes execution cost while being Pareto-optimal
    between cost and job completion time. Kaffes et al. [11] discuss the limitations
    of existing scheduling mechanisms for serverless platforms when considering the
    diverse requirements of applications in terms of burstiness, different execution
    times and statelessness. They propose a centralized and core-granular scheduler
    for serverless functions with a global view of the cluster resources. The usage
    of serverless computing along the cloud-to-edge continuum has also increased recently.
    This way, Rausch et al. [12] proposed a serverless platform for building and deploying
    edge AI applications, thus integrating concepts from AI lifecycle management into
    the serverless computing model. Based on OpenWhisk composer for workflow composition,
    they unveiled the lack of support for ARM-based architectures for OpenWhisk. The
    cloud-to-edge continuum embraces a diverse plethora of heterogeneous platforms
    and computer architectures. In this regard, the work by Jindal et al. [13] introduces
    an extension of the FaaS (Function as a Service) computing model to heterogeneous
    clusters and to support heterogeneous functions via a network of distributed heterogeneous
    platforms (Function Delivery Networks). They focus on SLO (Service Level Objective)
    requirements and energy efficiency, deploying functions on Edge platforms to reduce
    overall energy consumption. The authors use OpenWhisk, OpenFaaS and Google Cloud
    Functions. Sicari et al. [14] build on the concept of scientific workflows using
    the FaaS computational paradigm to create Serverless workflow-based applications
    based on a customized Domain-specific Language (DSL) to federate the Cloud-Fog-Edge
    layers to profit from each computing tier. This is exemplified in the open-source
    OpenWolf platform, a serverless workflow engine for native cloud-to-edge continuum,
    based on OpenFaaS, for function execution and Redis to store the workflow manifests
    and the execution information for the workflows. Smirnov et al. [15] introduce
    Apollo, an orchestration framework for serverless function compositions that can
    run across the cloud-to-edge continuum. The framework leverages data locality
    to perform cost and performance optimization. It also includes a decentralized
    orchestration approach where multiple instances can cooperatively orchestrate
    the application while balancing the workload between the spare resources. The
    work by Ferry et al. [16] introduce the SERVERLEss4I0T platform to perform the
    deployment and maintenance of applications over the cloud-to-edge continuum, but
    no open-source software is provided. Unlike previous works, our contribution provides
    an open-source implementation of the methods described in the paper to support
    job rescheduling and distribution among multiple service replicas that can execute
    along the cloud-to-edge continuum. An evaluation and assessment of the benefits
    of the implementation is done through a use case on wildfire detection run on
    disparate computing infrastructures on this continuum, involving serverless computing
    at the edge, on-premises clusters and public cloud infrastructures. Download :
    Download high-res image (470KB) Download : Download full-size image Fig. 1. Overall
    architecture of the OSCAR serverless platform. 3. Proposed architecture The work
    carried out is focused on the extension of the OSCAR [9], [17] platform, an open-source2
    framework for serverless data processing through container-based applications.
    OSCAR is a cloud-native framework that runs on the Kubernetes [18] container orchestration
    system to define serverless services for data processing. As shown in Fig. 1,
    it allows the scheduling of Kubernetes jobs for the asynchronous processing of
    files uploaded to a predefined bucket of the MinIO [19] storage system. These
    jobs are executed as containers, created out of user-defined Docker images, that
    run on an elastic Kubernetes cluster that can grow and shrink in terms of the
    number of nodes depending on the current workload and the limits defined at deployment
    time, [thanks to the CLUES3. Output files are likewise uploaded to MinIO so users
    can easily retrieve them or to any supported data storage systems such as Amazon
    S3, Onedata or dCache. OSCAR also supports the synchronous processing of invocations
    performed via HTTP requests. For this purpose, the platform is integrated with
    the Knative [7] Serving framework. However, this study focuses on the asynchronous
    feature of OSCAR, considering that it is more appropriate for compute-intensive
    batch tasks, such as inference processes using Artificial Intelligence/Machine
    Learning (AI/ML) models, as is the use case described in Section 4. OSCAR allows
    the definition of services via a web-based interface or through the Functions
    Definition Language (FDL)4 files using the command-line interface. An OSCAR service
    is mainly characterized by: • a Docker image available in a container image registry
    (e.g. Docker Hub or GitHub Container Registry) • A shell script that will be executed
    inside the container created out of the Docker image to perform the data processing
    on the customized execution environment provided by the Docker image. • A set
    of computing requirements for vCPUs, RAM and GPUs. • An input storage bucket that
    will trigger the execution of the OSCAR service and one or more output storage
    back-ends on which the output data generated by the service will be stored. These
    services can be run on an OSCAR cluster or in AWS Lambda via our development SCAR5
    [20]. AWS Lambda is a serverless computing service provided by Amazon Web Services
    (AWS) to support the Functions as a Service (FaaS) computing paradigm. It allows
    users to run code in response to certain events (file upload, HTTP request, etc.)
    without provisioning or managing servers, which is the responsibility of AWS.
    Its highly elastic features (up to 3000 parallel invocations) and fine-grained
    billing model (in 1 ms blocks) turned AWS Lambda into a popular option for developing
    microservices-based architectures. In turn, SCAR is an open-source tool that pioneered
    in 2017 the deployment of container-based applications in AWS Lambda when this
    service still had no native container support (introduced in late 2020). SCAR
    facilitates the execution of general-purpose applications in AWS Lambda, and it
    provides an automated delegation of jobs into AWS Batch, a managed service to
    provide automated elastic compute clusters as a service. This allows the use of
    AWS Lambda to execute spiky bursts of short jobs with moderated computing requirements
    (AWS Lambda invocations cannot run beyond 15 min or use more than 10 GiB of RAM)
    while delegating into AWS Batch jobs that require larger memory or specialized
    hardware, such as GPUs. The advantage of using a common Functions Definition Language
    is the ability to compose serverless workflows across the different layers of
    the cloud-to-edge continuum. For example, as described in our previous work by
    Risco et al. [21], workflows can be composed by services defined on OSCAR platforms
    configured on lightweight clusters (i.e. on ARM-based devices such as Raspberry
    Pi) located on the Edge or Fog, on OSCAR clusters in on-premises clouds or Lambda
    functions in the public Cloud. The main benefit of OSCAR is the ability to provide
    scalable event-driven computations upon file uploads to an object storage (or
    an HTTP-based invocation). OSCAR can run on multiple computer architectures (amd64
    and arm64) and container-based platforms (Kubernetes, K3s). It is also, integrated
    with SCAR for highly scalable cloud bursting into AWS Lambda. Therefore, for this
    reasons, it can be used to support serverless event-driven computing along the
    continuum and it has been the selected platform on which to develop our contributions.
    Further information about OSCAR is available in the work by Pérez et al. [17]
    . A well-known drawback of the cloud-to-edge continuum is the limited computational
    capacity at the edge. Usually, the devices employed have scarce computing resources,
    and this can represent a bottleneck in several use cases where the input data
    ingestion rate may fluctuate depending on external factors. The main goal of this
    contribution is to mitigate overload problems in these low-powered devices. Indeed,
    replication and distribution are features required to achieve high availability
    in a distributed system. Applying this approach in the cloud-to-edge continuum
    allows the use of resources from disparate computing infrastructures, coordinated
    by a distributed control plane that mediates access and resource distribution.
    Therefore, we introduce the ability to create replicas of serverless services
    for this work. An OSCAR cluster has the OSCAR Manager component (shown in Fig.
    1), which provides the entry point to trigger the execution of an OSCAR service.
    The cluster can be deployed on various computing infrastructures supported, such
    as Raspberry Pis, IaaS Clouds and public Clouds. The dynamic deployment on multiple
    Clouds is achieved thanks to the Infrastructure Manager (IM)6 [22], an [open-source7
    Infrastructure as Code (IaC) tool to provision and configure virtualized computing
    resources from multiple cloud back-ends. The dynamic deployment support of OSCAR
    clusters via the Infrastructure Manager allows users to self-deploy them on their
    preferred Cloud, where the user-defined OSCAR services are deployed to be triggered
    for scalable data-driven processing. An OSCAR service can have multiple replicas,
    each one potentially running on a different cluster with a similar configuration
    (but each service replica can specify a different number of computational resources).
    Each file upload to MinIO, or an asynchronous invocation to its REST API, triggers
    the creation of a job that is executed on the scalable Kubernetes cluster, which
    grows and shrinks depending on the number of jobs. In this scenario, it is important
    to support efficient strategies to distribute the workload among the available
    OSCAR service replicas to reduce the execution time. To this end, two strategies
    are proposed to reschedule jobs among OSCAR service replicas: Resource Manager,
    described in Section 3.1, and Rescheduler, described in Section 3.2. Furthermore,
    Section 3.3 defines the extension of the Functions Definition Language (FDL) used
    in SCAR and OSCAR to support this new functionality, as well as details the mechanism
    for delegating the events that trigger the execution of the jobs. 3.1. Resource
    manager Given the capabilities for resource discovery on the nodes of a Kubernetes
    cluster, a resource manager has been implemented in OSCAR to bypass job scheduling
    on a cluster that does not have available resources. For this purpose, the Kubernetes
    core API is used to obtain the status and resources available of all the active
    working nodes. If the resources available on a working node exceed those requested
    by an OSCAR service execution, the incoming job can be scheduled on the node.
    The availability of a working node to be scheduled is checked on a regular basis
    according to the periodicity specified on the environment variable , configurable
    by the user. As shown in Fig. 2, and highlighted by a dotted box, the lifecycle
    of the Resource Manager consists of periodically checking through the K8s API
    the available resources of each working node and caching them for the job handler
    to query. In turn, the job handler receives an event from a file upload on a MinIO
    bucket and checks the availability of resources. If there are no available resources
    in any of the working nodes of the cluster and the OSCAR service has a replica
    defined in its specification, it will delegate the event to the replica. The job
    handler will schedule the job in the current cluster only if resources are available.
    It is essential to mention that the Resource Manager is an optional feature in
    OSCAR and will only be activated if the Download : Download high-res image (266KB)
    Download : Download full-size image Fig. 2. Simplified diagram of the Resource
    Manager component. Download : Download high-res image (169KB) Download : Download
    full-size image Fig. 3. Simplified diagram of the Rescheduler component. configuration
    variable is enabled and replicas are defined for the active OSCAR service. 3.2.
    Rescheduler Although the Resource Manager prevents jobs from being scheduled once
    a cluster is overloaded, it is possible that during a peak of service invocations,
    the job scheduler allocates many jobs in the cluster before the resources available
    in the cluster are updated. These spikes can generate significant amounts of jobs
    queued in the Kubernetes scheduler for further processing as resources become
    available. To solve this situation, an additional mechanism named Rescheduler
    has been developed. The Rescheduler aims to mitigate cluster overloads and is
    in charge of checking the jobs in “Pending” status in the Kubernetes scheduler.
    For this purpose, it uses the Kubernetes core API to list the jobs scheduled in
    the system. It automatically filters them by their status and by several labels
    automatically defined by the OSCAR backend itself. Each OSCAR service can have
    its own threshold, which defines the maximum amount of time (in seconds) that
    a Kubernetes job from an invocation of an OSCAR service with replicas can be queued
    before delegating it. Therefore, the scheduled jobs are filtered by a label containing
    this information. Also, to figure out to which OSCAR cluster each job needs to
    be delegated, the jobs are filtered by another label that provides the service
    name. Fig. 3 shows how the Rescheduler periodically checks the cluster’s pending
    jobs that exceed the defined threshold. This interval is configurable through
    the RESCHEDULER_INTERVAL environment variable. It has a default value per cluster
    through the RESCHEDULER_THRESHOLD environment variable. However, as mentioned
    before, and detailed in Fig. 4, it can be configured for each service via the
    rescheduler_threshold parameter in the FDL. Jobs that exceed the defined threshold
    will be automatically delegated to a replica by the Rescheduler and, once scheduling
    is achieved on the replica, will be removed from the current cluster queue. Like
    the Resource Manager, the Rescheduler is an optional feature for OSCAR services
    and can be enabled or disabled through the RESCHEDULER_ENABLE environment variable.
    Furthermore, if a service does not have replicas in its definition, the OSCAR
    backend will not add the required labels for the Rescheduler to filter the jobs
    so they can remain in the Kubernetes scheduler queue as long as necessary until
    free resources are available. 3.3. Delegation mechanism To support the delegation
    of events to external clusters or endpoints, the Functions Definition Language
    (FDL) has been extended to include the concept of replicas, as introduced earlier.
    Multiple replicas can be defined for the same service, so if delegation fails
    on one replica, there are other replicas to which service invocation can be delegated.
    The definition of replicas can be done in the FDL through the replicas parameter,
    a list of OSCAR service replicas. A priority system has been implemented to choose
    the replica to delegate in the first place. Users can indicate each replica’s
    priority, with the number 0 as the highest priority and larger integers having
    a lower priority. As shown in Fig. 4, two different types of replicas can be specified.
    On the one hand, the “oscar” type of replicas are services defined in another
    OSCAR cluster. This requires to indicate the cluster identifier (cluster_id parameter)
    where such service is deployed, as well as its name. The OSCAR command-line interface
    (CLI)8 automatically embeds the access credentials to the clusters of the replicas
    in the configuration of the services so that users do not have to worry about
    managing them. On the other hand, the “endpoint” type of replicas support the
    delegation of events to HTTP endpoints, which will be sent via POST requests.
    Support for these endpoints makes it possible to use any FaaS service (such as
    AWS Lambda) where function invocation via REST APIs can be enabled. Thanks to
    this support, jobs can be rescheduled between OSCAR clusters, which can run on
    the edge, on-premises and public Clouds, and self-managed services in the public
    Cloud such as AWS Lambda functions, which can be exposed via HTTP APIs, using
    function URLs or via API Gateway, as done with the SCAR framework. Algorithm 1
    shows the simplified pseudocode of the delegation mechanism. The first step is
    to ensure that the list of replicas is sorted by priority to consequently wrap
    the original event that triggered the service, such as file upload to MinIO, by
    adding the identifier of the source cluster. This wrapping is necessary for the
    replica to know where the event comes from and, in this way, to download the input
    file, which usually comes from the MinIO storage provider of the source cluster.
    Then the algorithm proceeds as follows: if the replica type is “oscar”, it just
    checks that the cluster identifier is defined in the configuration (i.e. the cluster’s
    credentials exist under that identifier) and, consequently, the request is prepared
    with the replica configuration. In the case of “endpoint” type replicas, the HTTP
    headers defined by the user are added to the request. Finally, the request is
    sent, and the response is checked. If the response is valid, the algorithm finalizes;
    if not, it continues the loop to try to delegate to another replica in the list.
    Download : Download high-res image (321KB) Download : Download full-size image
    Download : Download high-res image (485KB) Download : Download full-size image
    Fig. 4. Support for replicas in the Functions Definition Language file. Regarding
    security, all jobs delegated to other OSCAR clusters are performed using authorization
    tokens obtained from the OSCAR configuration API via the basic auth credentials
    embedded in the services configuration. Moreover, different authorization mechanisms
    can be provided thanks to the support of user-defined custom headers in the “endpoint”
    replica type. In addition, all invocations support the HTTPS protocol, so the
    traffic between the client and server will be encrypted. Notice that this approach
    takes into account the peculiarities of event-driven serverless systems regarding
    the job delegation across replicas to avoid unnecessary data transfers and the
    ability to invoke remote HTTP endpoints as the entry point for public serverless
    services. To assess the benefits of this approach for automated serverless workload
    redistribution along the cloud-to-edge continuum, we carried out the use case
    described in the next section. 4. Use case: Serverless fire detection across the
    cloud-to-edge continuum Increased wildfires due to rising temperatures are one
    of the most alarming impacts of global warming [23]. Detecting fires in their
    early stages is essential to act quickly and minimize the damage caused to forests.
    However, it is not easy to anticipate these events. While they often correlate
    with several meteorological factors, external factors can also provoke them. Surveillance
    data analysis is an active field of research to prevent this type of situation.
    Advances in image processing and artificial intelligence enable the development
    of models capable of detecting fires from images taken from surveillance systems.
    This section proposes a use case for processing surveillance images across the
    cloud-to-edge continuum. For this purpose, an architecture is presented in which
    the data capture devices would be located at the Edge. These devices would be
    composed of thermal sensors capable of analysing different meteorological metrics
    such as temperature or relative humidity and cameras capable of obtaining images
    periodically. The information obtained by the thermal sensors will be used to
    detect the level of fire risk at a given time, thus increasing or decreasing the
    rate of obtaining the images to be processed. To process the images, Minified
    Kubernetes clusters (using the k3s [24] distribution) composed of Raspberry Pis
    located in the Fog, i.e. near the capture devices, will be used. Each cluster
    will be in charge of processing images from several cameras. In the experiment
    described in Section 4.1, a cluster in the Fog will process images from three
    cameras. Moreover, the Amazon SNS service will notify the firefighters in case
    of fire detection (see Fig. 5). Download : Download high-res image (368KB) Download
    : Download full-size image Fig. 5. Use case architecture for fire detection across
    the cloud-to-edge continuum. 4.1. Case study design To assess the new serverless
    job delegation mechanisms an experiment based on the use case described above
    has been designed. Although in a real scenario there would be multiple devices
    at the Edge to capture information, i.e. cameras with thermal sensors in a forest
    and multiple Fog clusters to process the data, in the experiment, we simulated
    the ingestion of images from only three cameras to a single Fog cluster. To highlight
    the influence of the delegation mechanisms, the on-premises cluster has been configured
    with a single working node to become overloaded quickly. However, it is essential
    to mention that in a real case, this cluster could have more nodes to process
    the jobs delegated from multiple Fog clusters. Moreover, OSCAR’s deployment can
    be configured to be elastic, i.e. the number of working nodes can be increased
    or decreased depending on the existing workload. The specifications of both the
    Fog and On-premises clusters are as follows: • Fog cluster: composed of four Raspberry
    Pi 4 model B, each with 4 GB of RAM and a Broadcom BCM2711, Quad-core Cortex-A72
    (ARM v8) 64-bit SoC @ 1.5 GHz. The Kubernetes minified distribution k3s has been
    used to deploy the components, running one node as the frontend, with the remaining
    three Raspberry Pi set as working nodes. • On-premises cluster: deployed on an
    OpenStack-based Cloud, whose underlying infrastructure is composed of 14 Intel
    Skylake Gold 6130 processors, with 14 cores each, 5.25 TB of RAM and 2 10GbE ports
    and 1 Infiniband port in each node. The virtualized Kubernetes-based OSCAR cluster
    is configured with one frontend node and one working node with eight vCPUs and
    32 GB of RAM each, dynamically deployed and configured using the Infrastructure
    Manager (IM). The fire detection service is based on the application9 from the
    study conducted by Thompson et al. [25], in which a compact convolutional neural
    network model for non-temporal real-time fire detection was developed and trained.
    The implementation consists of a simplified ShuffleNetV2 architecture for full-frame
    binary fire detection and an in-frame classification using superpixel segmentation.
    The application has been modified to provide a text file with the words “FIRE”
    or “NOT FIRE” as output. Meanwhile, the script employed for the service generates
    a compressed (zip) file with the text file and the image of the superpixel segmentation.
    Notifications, when a fire is detected, are sent via the Amazon SNS service [26],
    whose SDK (Software Development Kit) client has been included in the software
    container built for the service. AWS credentials can be specified in the services’
    definition so that notifications can be sent regardless of the cluster in which
    they are deployed. Fig. 4 shows the definition of the OSCAR services in FDL. After
    profiling the application, the OSCAR services were configured to 1 CPU and 1 GB
    of RAM for the jobs created when the service is invoked, both in the Fog and On-premises
    clusters. Therefore, the number of jobs that can be executed concurrently will
    be 9 in the Fog cluster and 7 in the On-premises cluster, since the services involved
    in the OSCAR control plane also use RAM from the underlying virtual infrastructure.
    Data ingestion was initially designed using Apache NiFi, a scalable tool for directed
    graphs of data routing, transformation, and system mediation logic, by creating
    a dataflow that controls the data ingestion into a MinIO bucket to trigger the
    OSCAR service. Since NiFi has no available processors to take pictures from the
    webcam, the GetWebCamera plugin was included.10 However, we found limitations
    in the data capture rate of this plugin. Therefore, we decided on the use case
    to emulate the data ingestion through a Python script that reproduces all the
    data flow. It gets the image from the virtual web camera and uploads it into the
    MinIO bucket. The ingestion rate has two phases with a duration of 30 min. The
    first phase ingests three images every 30 s. The second one has an ingestion rate
    of three images every 5 s. To validate the operation of the delegation mechanisms
    and to benchmark the performance of the developments, the experiment has been
    carried out in two different scenarios: • Scenario 1: There is the Fog cluster,
    to which the images that trigger the execution of the fire detection service are
    uploaded, and the On-premises cluster with the service configured as a replica.
    When the image ingestion rate increases, the Fog cluster will be overloaded and
    jobs will be delegated to the On-premises cluster. This scenario has been designed
    to exemplify the use case using on-premises resources, except the SNS service
    for fire notifications, so there is no need to rely on public Cloud serverless
    platforms (such as AWS Lambda). • Scenario 2: Same as the previous scenario but
    with the addition of a replica deployed as a function in AWS Lambda created through
    SCAR. The function has been made accessible via HTTP requests through the API
    Gateway service. Therefore, the FDL specifies an additional replica of type “endpoint”
    with 1 GB of RAM. This scenario has been developed to demonstrate how delegating
    jobs to higher levels of the cloud-to-edge continuum can be appropriate to profit
    from the scalability of managed serverless services, especially in time-constrained
    use cases. 4.2. Results and discussions This section presents the results obtained
    after conducting the previously described experiment for the two proposed scenarios.
    After running the experiment in both scenarios, the average processing time of
    the fire detection jobs on the three platforms used, i.e. Fog cluster, on-premises
    cluster and AWS Lambda, has been analysed. Fig. 6 shows that the Fog cluster is
    noticeably slower than the other platforms due to the lower computational capacity
    of the cluster’s lightweight devices (Raspberry Pis). Meanwhile, the on-premises
    cluster is the one that has offered the best performance, followed by AWS Lambda,
    in which the infrastructure is abstracted from the users, so it is not possible
    to know precisely the instance type used. AWS Lambda allocates computational power
    (e.g. CPU) proportionally to the amount of memory allocated (up to 10 GBs). For
    the sake of cost-effectiveness, the memory allocated to the Lambda function was
    only 1 GB, thus resulting in lower performance when compared to the execution
    in the on-premises cluster. The worst execution times for all three platforms
    correspond to the first runs when the software image has not yet been downloaded
    to the cluster nodes, in the case of OSCAR, and when the functions are not started
    in AWS Lambda (cold start). This cold start can be mitigated in OSCAR by pre-caching
    the Docker image in all the nodes of the Kubernetes cluster, a feature that can
    be activated in an OSCAR service via the image_prefetch parameter. The first phase
    of image ingestion resulted in 180 jobs being processed in the Fog cluster for
    both scenarios. In contrast, the second phase generated 1005 images in the first
    scenario and 1028 images in the second. The script employed to simulate the use
    case waits for the time indicated in the ingestion rate between file uploads but
    does not take into account the time incurred in uploading images. Therefore, if
    any image takes longer to be uploaded due to latency or bandwidth this may affect
    the total number of images uploaded in the experiment, as it has been the case.
    However, this does not affect the overall results of the experiment, whose main
    objective is to analyse the behaviour of the two job delegation mechanisms. Download
    : Download high-res image (118KB) Download : Download full-size image Fig. 6.
    Average execution time of the fire detection service on the three platforms employed.
    Download : Download high-res image (639KB) Download : Download full-size image
    Fig. 7. Number of scheduled jobs on the fog and the on-premises cluster, and the
    maximum number of jobs each cluster can execute simultaneously. Download : Download
    high-res image (657KB) Download : Download full-size image Fig. 8. Number of scheduled
    jobs on the fog, the on-premises cluster and AWS Lambda, and the maximum number
    of jobs each cluster can execute simultaneously. Since the ingestion rate in the
    first phase is three images every 30 s, all the jobs could be processed in the
    Fog cluster without the rescheduling mechanisms having to delegate any of them.
    The second phase, however, is where the behaviour of the delegation systems could
    be examined due to the large number of images to be processed: • In the scenario
    1, a total of 477 jobs have been delegated from the Fog cluster to the on-premises
    cluster, 455 of them delegated via the Resource Manager and 22 via the Rescheduler.
    Fig. 7 details the job scheduling of the second image ingestion phase for the
    first scenario. As can be seen, load peaks appear when the clusters become saturated.
    These spikes displayed above the lines of maximum parallel jobs for each cluster
    mean that the jobs cannot be processed and are kept in the queue until free resources
    are available. The peak that occurs at approximately the 1090th second in the
    on-premises cluster is worth mentioning, in which the cluster is fully saturated
    as many jobs are scheduled. • In the scenario 2, 538 jobs have been delegated
    from the Fog cluster to the on-premises cluster, 510 delegated by the Resource
    Manager and 28 by the Rescheduler. Likewise, the on-premises cluster has delegated
    85 jobs to AWS Lambda, 76 by the Resource Manager and 9 by the Rescheduler. As
    seen in Fig. 8, thanks to the delegation from the on-premises cluster to the public
    Cloud, the saturation of the on-premises cluster has almost disappeared. Unlike
    the previous scenario, most load peaks appear only in the Fog cluster. After analysing
    these results, it can be concluded that reducing the Resource Manager update interval
    could have further mitigated these workload spikes in the Fog cluster. Furthermore,
    an unusual behaviour was found after the experimentation: repeated output files
    were obtained in the second scenario. After analysing the results, it was discovered
    that the repeated files only appeared in some jobs delegated by the Rescheduler
    from the on-premises cluster to AWS Lambda. Due to the shorter processing time
    in this cluster and the default configuration of the Rescheduler, a non-negligible
    percentage of the jobs delegated to Lambda were also processed in the on-premises
    cluster. Remarkably, the Rescheduler has been configured in both OSCAR clusters
    (Fog and On-premises) with the default values, which are 15 s for the time interval
    between checking the jobs in pending state and 30 s for the threshold that indicates
    the maximum time a job can be queued. It is crucial to understand that these times
    are configurable and should ideally be adjusted according to the job processing
    time for each use case. Notice that this issue has caused an additional waste
    of computing resources. Still, it does not affect the main objective, which is
    to perform the automated delegation of computing when the workload exceeds a certain
    threshold along the cloud-to-edge continuum. To summarize, Fig. 9 shows the average
    time jobs have queued in the two scenarios. As it can be appreciated, in scenario
    2, this time has decreased notably. Indeed, AWS Lambda was introduced as an additional
    computing layer to offload workload executions from the on-premises cluster seamlessly.
    This significantly reduced the number of scheduled jobs in the on-premises cluster,
    as shown in Fig. 8, thus alleviating its workload. Download : Download high-res
    image (105KB) Download : Download full-size image Fig. 9. Average time that jobs
    have been queued for each scenario. This proves that combining serverless computing
    with strategies to delegate jobs to replicas along the different layers of the
    computing continuum can considerably benefit several use cases of near real-time
    processing where the workload may vary in a non-predictable way. This functionality
    has been implemented in the open-source OSCAR framework for the sake reproducibility
    and to facilitate user adoption when supporting cloud-to-edge computing scenarios
    based on serverless computing. The cost of delegating the execution of the 85
    jobs to AWS Lambda was subsumed in the free tier, which includes one million free
    requests per month and 400,000 GB-seconds of compute time per month. Without considering
    the free tier, the cost is estimated by the AWS Pricing Calculator to be 0,12
    $ in the North Virginia region. Notice that both scenarios included a delegation
    approach so that each OSCAR service could offload workload to a single replica
    located in an upper layer of the cloud-to-edge continuum (edge, fog and cloud).
    However, the implemented mechanism supports a set of replicas, thus being able
    to exploit additional potentially distributed resources from a layer before offloading
    into another layer. This facilitates the definition of more complex scenarios
    in which OSCAR service replicas can be simultaneously employed within layers of
    the cloud-to-edge continuum. 5. Conclusions and future work This paper has presented
    different strategies for delegating jobs on the OSCAR open-source serverless data-processing
    platform that runs on top of Kubernetes. To exemplify the operation of the two
    delegation mechanisms implemented, a use case was developed based on a pre-existing
    fire detection AI model and then adapted to the OSCAR platform. The experimentation
    carried out has allowed, in addition to testing the operation of the rescheduler
    and the resource manager, the benefits of delegating Serverless jobs to a different
    on-premises cluster, but also to FaaS services on public cloud providers, thus
    making use of the different layers of the cloud-to-edge continuum. The results
    indicate that such approach can be beneficial for several use cases where the
    workload is unpredictable, and relying only on edge processing devices can significantly
    limit the ability to handle information quickly. Future work involves fine-tuning
    the implementation of the Rescheduler component to minimize the execution of duplicate
    jobs. Also, adapting the Resource Manager mechanism to support additional workload
    scheduling systems on top of Kubernetes, such as Apache Yunikorn, is currently
    being used to limit the number of resources per service within an OSCAR cluster.
    In addition, we want to assess the effectiveness of the proposed strategies when
    including multiple replicas across the different layers of the edge-to-cloud continuum,
    including latency-aware algorithms to decide the delegated OSCAR service replica.
    Finally, we plan to introduce support for dynamically changing the replicas of
    an OSCAR service to reflect changes in the underlying infrastructure with the
    dynamic addition and removal of virtualized computing resources. CRediT authorship
    contribution statement Sebastián Risco: Conceptualization, Methodology, Software,
    Writing – original draft. Caterina Alarcón: Methodology, Software, Writing – review
    & editing. Sergio Langarita: Methodology, Software. Miguel Caballer: Methodology,
    Software, Writing – review & editing. Germán Moltó: Methodology, Writing – original
    draft, Writing – review & editing, Supervision, Funding acquisition. Declaration
    of competing interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Acknowledgements Grant PID2020-113126RB-I00 funded
    by MCIN/AEI/10.13039/501100011033. Project PDC2021-120844-I00 funded by MCIN/AEI/10.13039/501100011033
    and by the European Union NextGenerationEU/PRTR. This work was supported by the
    project AI-SPRINT “AI in Secure Privacy-Preserving Computing Continuum” that has
    received funding from the European Union’s Horizon 2020 Research and Innovation
    Programme under Grant 101016577. This work was also supported by the project AI4EOSC
    “Artificial Intelligence for the European Open Science Cloud” that has received
    funding from the European Union’s Horizon Europe Research and Innovation Programme
    under Grant 101058593. Data availability Data will be made available on request.
    References [1] Beckman P., Dongarra J., Ferrier N., Fox G., Moore T., Reed D.,
    Beck M. Harnessing the computing continuum for programming our world Fog Comput.
    (2020), pp. 215-230, 10.1002/9781119551713.ch7 View in ScopusGoogle Scholar [2]
    A. OpenFog Consortium Architecture Working Group P., et al. OpenFog reference
    architecture for fog computing OPFRA001, 20817 (2017), p. 162 Google Scholar [3]
    Jansen M., Al-Dulaimy A., Papadopoulos A.V., Trivedi A., Iosup A. The SPEC-RG
    reference architecture for the edge continuum (2022) URL http://arxiv.org/abs/2207.04159
    Google Scholar [4] Ketu S., Mishra P.K. Cloud, fog and mist computing in IoT:
    an indication of emerging opportunities IETE Tech. Rev., 39 (3) (2022), pp. 713-724
    CrossRefView in ScopusGoogle Scholar [5] Ghasempour A. Internet of things in smart
    grid: Architecture, applications, services, key technologies, and challenges Inventions,
    4 (1) (2019), p. 22 CrossRefView in ScopusGoogle Scholar [6] Mouradian C., Naboulsi
    D., Yangui S., Glitho R.H., Morrow M.J., Polakos P.A. A comprehensive survey on
    fog computing: State-of-the-art and research challenges IEEE Commun. Surv. Tutor.,
    20 (1) (2018), pp. 416-464, 10.1109/COMST.2017.2771153 View in ScopusGoogle Scholar
    [7] . Google, Knative, URL https://github.com/knative/. Google Scholar [8] A.
    Ellis, OpenFaaS, URL https://www.openfaas.com/. Google Scholar [9] . GRyCAP, OSCAR:
    Open source serverless computing for data-processing applications, URL https://oscar.grycap.net.
    Google Scholar [10] H. Zhang, Y. Tang, A. Khandelwal, J. Chen, I. Stoica, Caerus:{NIMBLE}
    Task Scheduling for Serverless Analytics, in: 18th USENIX Symposium on Networked
    Systems Design and Implementation, NSDI 21, 2021, pp. 653–669. Google Scholar
    [11] K. Kaffes, N.J. Yadwadkar, C. Kozyrakis, Centralized core-granular scheduling
    for serverless functions, in: Proceedings of the ACM Symposium on Cloud Computing,
    2019, pp. 158–164. Google Scholar [12] T. Rausch, W. Hummer, V. Muthusamy, A.
    Rashed, S. Dustdar, Towards a serverless platform for edge {AI}, in: 2nd USENIX
    Workshop on Hot Topics in Edge Computing, HotEdge 19, 2019. Google Scholar [13]
    Jindal A., Gerndt M., Chadha M., Podolskiy V., Chen P. Function delivery network:
    Extending serverless computing for heterogeneous platforms Softw. - Pract. Exp.,
    51 (2021), pp. 1936-1963, 10.1002/SPE.2966 URL https://onlinelibrary.wiley.com/doi/full/10.1002/spe.2966,
    https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2966, https://onlinelibrary.wiley.com/doi/10.1002/spe.2966
    View in ScopusGoogle Scholar [14] Sicari C., Carnevale L., Galletta A., Villari
    M. OpenWolf: A serverless workflow engine for native cloud-edge continuum 2022
    IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive
    Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf
    on Cyber Science and Technology Congress, DASC/PiCom/CBDCom/CyberSciTech, IEEE
    (2022), pp. 1-8 CrossRefGoogle Scholar [15] Smirnov F., Engelhardt C., Mittelberger
    J., Pourmohseni B., Fahringer T. Apollo: Towards an Efficient Distributed Orchestration
    of Serverless Function Compositions in the Cloud-Edge Continuum 9781450385640,
    dl.acm.org, Association for Computing Machinery (2021), 10.1145/3468737.3494103
    URL https://dl.acm.org/doi/abs/10.1145/3468737.3494103 Google Scholar [16] Ferry
    N., Dautov R., Song H. Towards a model-based serverless platform for the cloud-edge-IoT
    continuum Proceedings - 22nd IEEE/ACM International Symposium on Cluster, Cloud
    and Internet Computing, CCGrid 2022, 9781665499569, Institute of Electrical and
    Electronics Engineers Inc. (2022), pp. 851-858, 10.1109/CCGRID54584.2022.00101
    View in ScopusGoogle Scholar [17] Pérez A., Risco S., Naranjo D.M., Caballer M.,
    Moltó G. On-premises serverless computing for event-driven data processing applications
    2019 IEEE 12th International Conference on Cloud Computing, 2159-6182, CLOUD (2019),
    pp. 414-421, 10.1109/CLOUD.2019.00073 View in ScopusGoogle Scholar [18] . Kubernetes,
    Kubernetes, URL https://kubernetes.io/. Google Scholar [19] . MinIO, High performance,
    kubernetes native object storage, URL https://min.io/. Google Scholar [20] Pérez
    A., Moltó G., Caballer M., Calatrava A. Serverless computing for container-based
    architectures Future Gener. Comput. Syst., 83 (2018), pp. 50-59, 10.1016/j.future.2018.01.022
    URL https://linkinghub.elsevier.com/retrieve/pii/S0167739X17316485 View PDFView
    articleView in ScopusGoogle Scholar [21] Risco S., Moltó G., Naranjo D.M., Blanquer
    I. Serverless workflows for containerised applications in the cloud continuum
    J. Grid Comput., 19 (3) (2021), p. 30, 10.1007/s10723-021-09570-2 View in ScopusGoogle
    Scholar [22] Caballer M., Blanquer I., Moltó G., de Alfonso C. Dynamic management
    of virtual infrastructures J. Grid Comput., 13 (1) (2015), pp. 53-70, 10.1007/s10723-014-9296-5
    View in ScopusGoogle Scholar [23] Moritz M.A. Wildfires ignite debate on global
    warming Nature, 487 (7407) (2012), p. 273 CrossRefView in ScopusGoogle Scholar
    [24] . Cloud Native Computing Foundation, K3s, URL https://k3s.io/. Google Scholar
    [25] Thompson W., Bhowmik N., Breckon T. Efficient and compact convolutional neural
    network architectures for non-temporal real-time fire detection Proc. Int. Conf.
    Machine Learning Applications, IEEE (2020), pp. 136-141, 10.1109/ICMLA51294.2020.00030
    URL http://breckon.org/toby/publications/papers/thompson20fire.pdf View in ScopusGoogle
    Scholar [26] . Amazon Web Services, Push Notification Service - Amazon Simple
    Notification Service (SNS), URL https://aws.amazon.com/sns/. Google Scholar Cited
    by (0) Sebastián Risco received a B.Sc. degree in Computer Engineering from the
    Universitat Politècnica de València (UPV), Spain, in 2017. In 2017 he started
    his M.Sc. degree in Information Management. He joined the GRyCAP research group
    in 2018, while he worked on his Master’s Thesis. His research interests are focused
    on Serverless Computing, Cloud Computing and Container Orchestration Systems.
    Caterina Alarcón received a B.Sc. from Universitat Jaume I in 2020 and has a Master’s
    Degree in Cloud and High-Performance Computing by Universitat Politécnica de València.
    She has been a member of the GRyCAP research group at the Institute for Molecular
    Imaging (I3M) as a researcher since 2022. Sergio Langarita received a B.Sc. degree
    in Computer Science from Escuela Universitaria Politécnica de Teruel (EUPT) at
    the Universidad de Zaragoza (UNIZAR), Spain in 2021 and a Master’s degree in Big
    Data Analytics from the Universitat Politècnica de València (UPV), Spain, in 2023.
    Since 2022 he has been a member of the GRyCAP research group at the Institute
    for Molecular Imaging (I3M), developing serverless computing platforms. Miguel
    Caballer obtained a B.Sc. and M.Sc. degree in Computer Science from the Universitat
    Politècnica de València (UPV), Spain, in 2000 and 2012. He is member of the GRyCAP
    research group at the Institute of Instrumentation for Molecular Imaging (I3M)
    since 2001. He has participated in several European and National research projects
    about applying Parallel, Grid and Cloud computing techniques to several areas
    of engineering. Germán Moltó is Full Professor in Computer Science at the Universitat
    Politècnica de València (UPV), Spain. He has been a member of the GRyCAP research
    group at the Institute for Molecular Imaging (I3M) since 2002. He has participated
    with different responsibility roles in several European projects such as INDIGO-DataCloud,
    EOSC-HUB, DEEP HybridDataCloud, AI-SPRINT, AI4EOSC and InterTwin, and led national
    research projects related to cloud computing. His broad research interests are
    cloud computing and scientific computing. 1 OSCAR - https://oscar.grycap.net.
    2 OSCAR’s GitHub repository: https://github.com/grycap/oscar. 3 CLUES - https://github.com/grycap/clues]
    elasticity system. 4 FDL - https://docs.oscar.grycap.net/fdl/. 5 SCAR - http://github.com/grycap/scar.
    6 Infrastructure Manager (IM) - https://im.egi.eu. 7 IM’s GitHub repository -
    https://github.com/grycap/im]. 8 OSCAR CLI - https://github.com/grycap/oscar-cli.
    9 https://github.com/NeelBhowmik/efficient-compact-fire-detection-cnn. 10 https://github.com/tspannhw/GetWebCamera.
    © 2023 The Author(s). Published by Elsevier B.V. Part of special issue Serverless
    Computing in the Cloud-to-Edge Continuum Edited by Hao Wu, Carlo Puliafito, Omer
    Rana, Luiz Fernando Bittencourt View special issue Recommended articles Def-DReL:
    Towards a sustainable serverless functions deployment strategy for fog-cloud environments
    using deep reinforcement learning Applied Soft Computing, Volume 152, 2024, Article
    111179 Chinmaya Kumar Dehury, …, Satish Narayana Srirama View PDF Digital twin-assisted
    flexible slice admission control for 5G core network: A deep reinforcement learning
    approach Future Generation Computer Systems, Volume 153, 2024, pp. 467-476 Jiadai
    Wang, …, Jiajia Liu View PDF A novel verifiable chinese multi-keyword fuzzy rank
    searchable encryption scheme in cloud environments Future Generation Computer
    Systems, Volume 153, 2024, pp. 287-300 Mande Xie, …, Zhen Zhang View PDF Show
    3 more articles Article Metrics Captures Readers: 10 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Future Generation Computer Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Rescheduling serverless workloads across the cloud-to-edge continuum
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chen J.
  - Reitz J.
  - Richstein R.
  - Schröder K.U.
  - Roßmann J.
  citation_count: '0'
  description: Advancing digitalization is reaching the realm of lightweight construction
    and structural–mechanical components. Through the synergistic combination of distributed
    sensors and intelligent evaluation algorithms, traditional structures evolve into
    smart sensing systems. In this context, Structural Health Monitoring (SHM) plays
    a key role in managing potential risks to human safety and environmental integrity
    due to structural failures by providing analysis, localization, and records of
    the structure’s loading and damaging conditions. The establishment of networks
    between sensors and data-processing units via Internet of Things (IoT) technologies
    is an elementary prerequisite for the integration of SHM into smart sensing systems.
    However, this integrating of SHM faces significant restrictions due to scalability
    challenges of smart sensing systems and IoT-specific issues, including communication
    security and interoperability. To address the issue, this paper presents a comprehensive
    methodological framework aimed at facilitating the scalable integration of objects
    ranging from components via systems to clusters into SHM systems. Furthermore,
    we detail a prototypical implementation of the conceptually developed framework,
    demonstrating a structural component and its corresponding Digital Twin. Here,
    real-time capable deformation and strain-based monitoring of the structure are
    achieved, showcasing the practical applicability of the proposed framework.
  doi: 10.3390/info15030121
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all    Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Information All Article Types Advanced   Journals
    Information Volume 15 Issue 3 10.3390/info15030121 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Zahir M.
    Hussain Subscribe SciFeed Recommended Articles Related Info Link More by Authors
    Links Article Views 904 Table of Contents Abstract Introduction Related Work Concept
    The Digital Cantilever in the IoT-Based SHM System Discussion Conclusions Author
    Contributions Funding Institutional Review Board Statement Informed Consent Statement
    Data Availability Statement Conflicts of Interest Abbreviations References share
    Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessArticle IoT-Based SHM Using Digital Twins for Interoperable and Scalable
    Decentralized Smart Sensing Systems by Jiahang Chen 1,*, Jan Reitz 1, Rebecca
    Richstein 2, Kai-Uwe Schröder 2 and Jürgen Roßmann 1 1 Institute for Man-Machine
    Interaction, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany 2 Institute
    of Structural Mechanics and Lightweight Design, RWTH Aachen University, Wüllnerstraße
    7, 52062 Aachen, Germany * Author to whom correspondence should be addressed.
    Information 2024, 15(3), 121; https://doi.org/10.3390/info15030121 Submission
    received: 31 December 2023 / Revised: 9 February 2024 / Accepted: 16 February
    2024 / Published: 20 February 2024 (This article belongs to the Special Issue
    Intelligent Information Processing for Sensors and IoT Communications) Download
    keyboard_arrow_down     Browse Figures Review Reports Versions Notes Abstract
    Advancing digitalization is reaching the realm of lightweight construction and
    structural–mechanical components. Through the synergistic combination of distributed
    sensors and intelligent evaluation algorithms, traditional structures evolve into
    smart sensing systems. In this context, Structural Health Monitoring (SHM) plays
    a key role in managing potential risks to human safety and environmental integrity
    due to structural failures by providing analysis, localization, and records of
    the structure’s loading and damaging conditions. The establishment of networks
    between sensors and data-processing units via Internet of Things (IoT) technologies
    is an elementary prerequisite for the integration of SHM into smart sensing systems.
    However, this integrating of SHM faces significant restrictions due to scalability
    challenges of smart sensing systems and IoT-specific issues, including communication
    security and interoperability. To address the issue, this paper presents a comprehensive
    methodological framework aimed at facilitating the scalable integration of objects
    ranging from components via systems to clusters into SHM systems. Furthermore,
    we detail a prototypical implementation of the conceptually developed framework,
    demonstrating a structural component and its corresponding Digital Twin. Here,
    real-time capable deformation and strain-based monitoring of the structure are
    achieved, showcasing the practical applicability of the proposed framework. Keywords:
    IoT; smart sensing systems; structural health monitoring; digital twins 1. Introduction
    The primary function of lightweight structural components is to provide rigidity
    and strength while minimizing weight. Recent approaches have furthered their functionality
    by integrating distributed sensors and intelligent algorithms, evolving into smart
    sensing systems. These systems transform previously passive structural components
    into active components so that theycan digitally process data and exchange their
    state, for example, operational status, with each other. Presently, smart sensing
    systems are applied in various domains like water quality monitoring [1], generalized
    environmental monitoring [2], healthcare monitoring [3], and human motion disorders
    [4], showing their innovation in data collection and analysis. Back to the realm
    of lightweight structural components, typical structures that can be retrofitted
    to smart sensing systems include bridges [5], aircraft [6], or wind turbines [7].
    Such integrated systems, comprised of structural components, sensors, and data-processing
    systems, can potentially pose risks to human safety and environmental integrity
    due to system failures [8]. In response, Structural Health Monitoring (SHM) has
    emerged with the principal aim of analysis, localization, and recording of the
    loading and damaging conditions, enabling the prediction of systems’ remaining
    useful life [9]. The implementation of SHM necessitates the establishment of networks
    that connect sensors, digital models, evaluation algorithms, and users. Here,
    data acquisition, data processing, and data exchange are essential concerns. Traditional
    data acquisition systems use wires to connect sensors to a centralized server
    (e.g., database), where data processing and interpreting are carried out. Newer
    applications advocate so-called wireless sensing networks (WSN) to reduce costs
    related to installation and maintenance [10]. Overall, technological advancements
    and the widespread availability of wireless networks have shifted SHM from wire-based
    methods to real-time WSNs using wireless communication protocols [11]. Beyond
    WSNs, the Internet of Things (IoT) technologies are not limited to localized data
    processing and energy-efficient communication. They leverage a more complex and
    hierarchical architecture, integrating various devices, cloud-based services,
    and users to enable a seamless information flow across the global network. The
    integration of IoT paradigms drives more innovative solutions of communication
    in the field of SHM, enabling remote and continuous monitoring, as reflected in
    recent research [12,13,14,15]. Nevertheless, based on the observation of the current
    IoT-based SHM systems, we can generally identify the following issues. Using IoT
    technologies comes with significant challenges regarding communication security
    (e.g., related to confidentiality, integrity, and availability), data sovereignty
    (i.e., related to the control over the data), and exchange interoperability (e.g.,
    how can heterogeneous data and services be understood at the same level?). These
    considerations are often only an afterthought, but they are crucial to apply SHM
    in practice. Moreover, structures of smart sensing systems are becoming increasingly
    complex, requiring multiple measurement points or sensors even when monitoring
    hot-spot regions. Here, we identify the absence of a general approach to enable
    the monitoring and management of individual components and scaling to systems
    or clusters in the IoT. The center of the proposed scheme is Digital Twins (DTs)—virtual
    representations of physical assets. Until now, the concept of DTs is loosely defined,
    as there are diverse standards to interpret how a DT should look like and which
    functions it should have. Precisely because of this loose definition, DTs are
    highly expandable. This leads to the fact that the development of DTs is closely
    related to specific needs. In several current DT paradigms, DTs are associated
    with different security threats. Literature like [16,17] intents to classify these
    threats and shows the DT’s potential to ensure appropriate and trustworthy data
    exchange in a secured way. Meanwhile, DTs have gained traction in the digital
    transformation of various domains. This term is also relevant to structural engineering,
    as digitalization transforms how structures are designed, managed, and maintained.
    SHM applications, ranging from estimating component lifetime to maintenance scheduling,
    benefit from the incorporation of DTs, enhancing interoperability across diverse
    scales due to consistent and uniform modeling of DT’s structure and interface
    [18]. Studies focusing on the use DTs in SHM systems can be classified based on
    their application focus, like data analysis of the measured data [19,20,21], communication
    efficiency [22,23,24], and formulation of the integrated process [25] or the formalized
    modeling of DTs [26]. We find few publications that use DTs to address integration
    scalability in SHM. Based on these observations, we propose an organizational
    scheme to describe structural–mechanical objects from individual components to
    clusters based on hierarchical DTs. Additionally, we present the conceptual framework
    for integrating DTs into IoT-based SHM systems, addressing data security and interoperability.
    The remaining parts of the paper are structured as follows: Section 2 reviews
    published SHM applications with a focus on the employed networking infrastructures,
    as well as existing organizational schemes in SHM systems. In Section 3, we propose
    an organizational scheme of IoT-based SHM systems and the respective conceptual
    framework. Finally, a proof-of-concept implementation will be demonstrated in
    Section 4 to monitor the operational of an exemplary cantilever via an app in
    near real time. Section 5 discusses the extension possibilities of the prototypical
    application using our methodological approach. Section 6 concludes this paper.
    2. Related Work This section provides an overview of pivotal publications in the
    domains of Structural Health Monitoring (SHM) and the IoT. First, we focus on
    general, higher-level organizational schemes that structurally describe SHM systems,
    considering their IoT-based interconnections. The overall objective is to extract
    the SHM’s central requirements for interconnections through IoT infrastructures.
    Based on that, we review and compare IoT infrastructures concerning their suitability
    for SHM. 2.1. Organizational Schemes of SHM Systems A review of current publications
    reveals that IoT-based SHM systems tend to adopt layered architectures. Aguzzi
    et al. [27] subdivide their Web of Things platform for SHM application into four
    distinct layers: A monitoring layer directly connected to physical structures,
    an edge layer focusing on data acquisition and preprocessing, a data management
    layer addressing storage, aggregation, and visualization of acquired data, and
    a data analytics layer dedicated to interpreting the data for condition assessment
    and damage detection, localization, and prediction. Similar to this layered approach,
    Lamonaca et al. [28] present their SHM system as an aggregation of interconnected
    smart objects. They define a dual-layered architecture: a physical layer encompassing
    all sensors and actuators that comprise smart objects and a cyber part responsible
    for data processing and communication. The cyber part is further subdivided into
    functional layers targeting signal processing, event detection, and real-time
    applications. Similar layered architectures can also be found in other publications,
    e.g., [29,30]. The aforementioned organizational proposals focus on subdividing
    the data stream linearly from data acquisition via (central) evaluation to end-user
    visualization. The modeling granularity of physical objects remains unrefined.
    Physical objects—in this case, structures—can be increasingly complex to cover
    use cases ranging from basic elements to a complicated wind turbine. Hence, they
    demand a more detailed approach to represent their hierarchical and structural
    complexity accurately. Additionally, current proposals predominantly emphasize
    vertical communication along the data stream, i.e., traversing from the physical
    object (data acquisition) through a cloud-based analysis service (data preprocessing
    and aggregation) to the end user (data visualization). However, there is an oversight
    in horizontal communication—the interconnection of objects, regardless of the
    complexity. Enabling horizontal interconnections between objects is crucial, especially
    in complex SHM systems consisting of different structures and sensors. Here, individual
    objects should be able to autonomously manage their data and facilitate interfaces
    for interconnections. The use of IoT paradigms can help in creating organizational
    schemes for SHM applications in this respect. 2.2. The Role of DTs Aspects and
    concepts that shape the definition of DTs are diverse. In general, the summary
    of characteristics published by Jones et al. [31] is widely accepted. We perceive
    DTs as virtual representations of physical entities and are realized by aggregation
    of computation and communication technologies. In the digital world, DTs utilize
    their metadata to describe the basic physical structure, operational state, provided
    service functions, as well as access properties and interfaces. They are assigned
    a globally unique identity and equipped with communication endpoints so that peer-to-peer
    communication is possible. When it comes to IoT, we consider both the interconnection
    of DTs themselves as well as the interconnection of DTs with other IoT objects
    and end users. Hence, DTs are also regarded as communication nodes with a unique
    addressable and available identity in this decentralized landscape. In this context,
    the focus of DTs shifts to interconnection and holistic modeling [32], rather
    than other popular aspects like 3D modeling, product life cycle management, and
    so on. Interconnection seeks a seamless connection to collaborate on shared targets,
    which requires uniform interfaces and an identical understanding of the communication
    protocols used. Holistic modeling intends to formally and semantically describe
    DTs’ physical structure, functional composition, and other features using a standardized
    data model [33]. In this context, a specific example is the integration of a structure
    into a building information model [21]. In the domain of structure mechanics,
    a general framework for implementing DTs has not been established yet, as stated
    in [32]. 2.3. Dimensions of IoT-Based SHM Systems 2.3.1. Interoperability Interoperability
    in the IoT, as defined by Konduru et al. [34], refers to the ability of diverse
    connected objects to communicate at the same technical and semantic level. It
    is a crucial foundation for scalable and flexible IoT systems, enabling the integration
    of new objects and technologies without disrupting existing communication paradigms.
    Achieving interoperability requires semantics in both object structures and communication
    languages. In the context of SHM, Aguzzi et al. [27] utilize the Thing Description
    format of the W3C’s WoT specification to structurally describe SHM systems. Based
    on Thing Description, a semantic layer can be directly added to the meta model
    to realize interoperable data exchange. Similar ideas can be found in the publication
    from Gigli et al. [35], which proposes the semantic formalization of exchanged
    data. 2.3.2. Offline Capability The capability to continuously provide intended
    functions (e.g., data processing) is pivotal for long-term SHM systems [9]. This
    capability should be kept even in offline scenarios [30]. However, offline capability
    in wireless SHM systems presents challenges since it necessitates wireless transmission
    of entire structural response data sets, which has been proven to negatively impact
    the autonomy of wireless sensor nodes [36]. Investigating how IoT technologies
    and DTs are combined in SHM systems illustrates the feasibility of maintaining
    offline capabilities even in offline scenarios or disrupted network connections
    [30,37]. 2.3.3. Decentralized Data Collection and Centralized Data Analysis SHM
    processes, as interpreted by Farrar et al. [9], require a dynamic approach to
    data acquisition. Due to the inherent variability in structural geometries, damage
    manifestation can occur either locally within specific areas or more broadly across
    spatially distributed locations. Decentralized data acquisition is essential to
    address this variability and allow for precise monitoring of structural conditions.
    Moreover, it may be necessary to perform some data preprocessing on-site (e.g.,
    through edge computing) to ensure that the data being collected is of high quality
    [38]. Centralized data analysis is required to consolidate data collected from
    decentralized sources, especially when diverse sensor types from different vendors
    are being used. Integrating data into a unified analytical framework facilitates
    a comprehensive view of the structure’s condition and enhances the ability to
    identify issues [38]. Both decentralized data collection and centralized analysis
    underscore the need for an IoT infrastructure that supports seamless connectivity
    across various entities, such as smart sensors or cloud-based services. The combination
    of these two paradigms within an IoT infrastructure not only enhances the accuracy
    and efficiency of SHM systems but also ensures that data acquisition and processing
    are scalable and adaptable to the evolving structural conditions. 2.3.4. Flexibility
    and Scalability As stated in Section 2.3.3, SHM systems frequently employ various
    sensors to monitor different aspects of structures. As structures and their monitoring
    needs may change over time [9], these systems should be capable of expanding or
    contracting by adding or removing sensors, visualization units, and cloud-based
    services with ease. Additionally, SHM systems must be established with the flexibility
    to integrate new technologies, including novel communication standards and sensors,
    as they are introduced [39]. 2.3.5. Secure Communication In the IoT, open sharing
    of information is crucial for enhancing collaboration between different systems.
    However, this openness requires trust, which can be established by strong security
    properties of the underlying IoT infrastructure. Communication security is a crucial
    concern in the field of IoT-based SHM [15], which can typically be evaluated using
    the confidentiality, integrity, and availability (CIA) triad [40]. In general,
    a robust security concept that satisfies CIA depends on appropriate and comprehensive
    authentication and authorization methods, such as OpenID Connect [41] and access
    control [40]. They help systems to ensure that data are protected from unauthenticated
    and unauthorized access. To further strengthen the framework, an in-depth exploration
    of data privacy approaches is necessary. Privacy-preserving techniques, such as
    differential privacy and anonymization of sensor data, are dedicated to safeguarding
    user data against misuse. This is particularly significant for machine manufacturers,
    who would not allow measurement data from their potentially “damaged” components
    to be publicly available. Meanwhile, it ensures that SHM systems associated with
    IoT technologies are secure and resistant to manipulation or tampering. 2.4. Existing
    IoT Infrastructures for SHM Systems In light of the dimensions illustrated in
    Section 2.3, we select and analyze both open-source and commercial IoT infrastructures
    (for more, see [42]) which have been (or can be) used in IoT-based SHM systems,
    see Table 1 for a comparative overview. Bosch IoT Things [43] provides a versatile
    IoT infrastructure, facilitating the management of DTs for their IoT devices (assets).
    This infrastructure allows DTs to be equipped with various communication interfaces,
    enabling a bidirectional connection with their physical counterparts to manage
    asset data, obtain notifications on all relevant changes, and keep in a synchronous
    state with their assets. However, the modeling of DTs and the message utilized
    for communication is not the focus of Bosch IoT Things. This leads to interoperability
    issues when connected smart sensors are from different types or vendors. Although
    data acquisition is conducted in a decentralized manner, DTs and their measurements
    are forced to reside in the centralized cloud. However, data aggregation is still
    not centralized but dispersed within each DT aggregated in the same cloud. Due
    to the partial open-source nature, expanding the number of connected DTs involves
    the associated cost, consistently hindering flexibility and scalability. Security
    is considered from the transport layer to application-level access control, comprising
    device authentication via X.509 and application access control via OpenID Connect.
    Commercial IoT infrastructures like Siemens Mindsphere [44] and Microsoft Azure
    [45] predominately offer centralized cloud-based databases for data storage and
    management [46]. These infrastructures cater to a wide range of IoT applications,
    such as machine learning. However, their primary focus is placed on enhancing
    cross-company or cross-sector value chains. For example, the primary area of Siemens
    Mindsphere is providing functionalities and technologies for digital services
    in industrial manufacturing controlled by Siemens PLC. Hence, there remains the
    question of the general applicability of those commercial IoT infrastructures
    in SHM systems. In the context of interoperability, there are defined data models
    for DTs. The Digital Twin Definition Language (DTDL) [47] from Microsoft Azure
    provides a concept to model DTs with self-defined vocabulary and aspects. This
    language makes the hierarchical modeling of the SHM system more flexible. Towards
    security, there are implementations observed in different layers by those infrastructures
    [46]. ThingsBoard IoT is an open-source IoT infrastructure consisting of infrastructure
    components, databases, and gateways. This infrastructure can enable the out-of-the-box
    IoT cloud (i.e., plug-and-play cloud-based applications) or on-premises solution
    with different communication protocols [48]. It implies a flexible integration
    of, e.g., a cloud-based centralized data aggregation to gain an insight into heterogeneous
    data [49]. As for data acquisition, Ismail et al. [50] show the performance on
    the throughput of the platform, evaluated with REST and MQTT. Both results prove
    the data collection capability in decentralized scenarios. Moreover, this infrastructure
    allows users to add individual functionality as well as rules for diverse workflows.
    In the context of DTs’ integration, the system interprets the general lack of
    communication interoperability since it is necessary to define a conceptual data
    model for devices and communication protocol to interpret the meaning of exchanged
    messages. This aims to make the communication between devices understandable at
    the same technical and semantic level. From the perspective of IoT security, the
    IoT platform provides the options associated with device authorization flow by
    access token, x.509, and MQTT basic credentials. The Smart Systems Service Infrastructure
    (S3I, see [51,52]) is an open-source IoT infrastructure with a few centralized
    software services. This infrastructure allows the connected objects to authenticate
    and authorize themselves via S3I IdentityProvider, store and re-find their meta
    information, including properties and service functions via S3I Directory, and
    communicate end-to-end compliantly with each other via S3I Broker. Basically,
    the infrastructure is originally dedicated to forestry applications, enabling
    decentralized interconnection between Forestry 4.0 things (F4.0 things), consisting
    of physical assets and their DTs, software services, and apps. However, S3I’s
    distribution-oriented design (i.e., retaining as little central architecture as
    possible, allowing communication logic to be decentralized for execution) makes
    it possible to use S3I in other domains as well. In the S3I, things are not enforced
    to transmit and store their data to the centralized infrastructure; they only
    send metadata information to the S3I Directory so that things can be searched
    and discovered. As proposed by Chen et al. [53], the use of the S3I ensures security
    during communication since a comprehensive method is provided towards confidentiality,
    integrity, and availability, from OAuth 2.0-based authentication, authorization
    up to end-to-end encrypted communication via the S3I Broker. Moreover, the control
    over data is always kept since decentralized connected things only expose an interface
    to the outside, protected with fine-grained access control. Towards interoperability,
    the conceptual meta data model of the S3I Directory is delivered to allow the
    structure and content of F4.0 things to be mapped into the meta model. The S3I-B
    protocol specifies the predefined structure of S3I-B messages (including user
    messages, service messages, attribute messages, etc.) exchanged between different
    F4.0 Things [52]. Together with the forest modeling language 4.0 (ForestML 4.0,
    see [54]), decentralized F4.0 things (thus, also DTs) are described structurally
    and formally. Hence, technical interoperability during communication can be ensured.
    Table 1. Comparison of IoT Infrastructures used for SHM systems. 3. Concept Drawing
    upon the literature review in Section 2, we propose a decentralized SHM system
    methodology with the term DT as the central abstraction. DTs serve as a bridge
    connecting various stakeholders, services, and data, regardless of their technical
    implementations or locations, whether in edge devices, cloud environments, or
    a hybrid of both. This interconnectedness is crucial as it transforms isolated
    technical components into a cohesive, value-added network. The proposed system
    establishes a one-to-one relationship between DTs and physical objects, ranging
    from individual components to clusters. This granularity in representation not
    only refines the system’s monitoring capabilities but also reduces implementation
    redundancy, therefore enhancing the flexibility of the SHM system (Section 3.1).
    Interoperability is vital for the system’s functionality, necessitating the capacity
    of different DTs to coordinate with each other seamlessly. This cooperation is
    largely dependent on the DTs’ ability to understand each other, facilitated by
    standardized description structures and a common interaction language (Section
    3.2). Each DT is designed to support specific functionalities such as data preprocessing,
    storage, and analysis. The system’s unified interface and interoperability protocols
    ensure that these DTs can be integrated to address more complex monitoring tasks,
    meeting a diverse range of SHM criteria (Section 3.3). 3.1. Hierarchical Structure
    of the Proposed SHM System The proposed IoT-based SHM system is organized hierarchically
    and structured into layers of increasing abstraction. Each layer provides unique
    functionalities and insights. As illustrated through the example of wind power
    (see Figure 1), this system starts at the component level, including rotor blades,
    towers, and nacelles. These elements form wind turbines, which are then grouped
    into wind parks. Figure 1. Vision from physical objects: scales in IoT networks
    for SHM systems. 3.2. Data Model for IoT-Based SHM Systems The proposed system
    is comprised of a component level, a plant level, and a cluster level. At the
    component level, the system focuses on acquiring, processing, and interpreting
    sensor data. This level addresses specific questions, such as determining the
    next maintenance time for the nacelle or detecting deformations in blades exceeding
    certain thresholds. The plant level aggregates data from its components, offering
    a consolidated view of overall health and service requirements. Furthermore, the
    clustering of plants, e.g., clustering wind turbines into a wind park, represents
    an organizational level where data are aggregated from multiple plants. As we
    move up the hierarchy, the number of objects decreases, but the complexity increases.
    Although the number of clusters is smaller, they contain numerous systems, which
    in turn are made up of many components. Despite this complexity at higher levels,
    it remains manageable at the component level due to the separation of concerns
    into individual DTs. Thus, the component level is critical for direct interaction
    with sensors and raw structural data, forming the foundation of the SHM system.
    Higher levels, such as the plant or cluster levels, leverage these data, offering
    broader insights and facilitating interactions between components, plants, and
    the environment. The primary role of the cluster level is data consolidation,
    while models and detailed investigations are most effectively carried out at the
    plant level. This hierarchical structure ensures efficient data management and
    analysis across different levels of SHM systems. In decentralized IoT-based SHM
    systems, the necessity for a shared data model is paramount, especially to ensure
    both technical and semantic interoperability among diverse system components.
    A shared data model facilitates consistent communication, data exchange, and understanding
    across various elements within the system, regardless of their designs or functions.
    This is particularly crucial in systems where components from different vendors
    or with varying technical specifications must work cohesively. Our proposed data
    model is an augmentation of the established ML 4.0 data model [54], tailored specifically
    for the structural–mechanical domain. This data model is used to provide a comprehensive
    overview regarding the hierarchical physical structures, digital functionality,
    interfaces, and associated properties, as well as aggregates this information
    into a DT. We term this extension the mechanical modeling language 4.0 (mml40).
    Figure 2 illustrates this extension, highlighted in red, in juxtaposition with
    the original elements of the ML 4.0 model, depicted in gray. The central element
    is the (ml40::Thing), which represent independent units in the IoT. Each thing
    encompasses roles (ml40::Role) and features (ml40::Features), enabling a detailed
    and functional characterization, e.g., properties and services of each unit. In
    the mml40 extension, things in the field of SHM are further specialized into components
    (mml40::Component), plants (mml40::plant), and clusters (mml40::Cluster), therefore
    aligning the data model with the hierarchical structure of the proposed SHM system
    in Section 3.1. To add a dynamic aspect to this model, we introduce the concept
    of events to the data model. This allows the modeling of events to be autonomously
    published by things alongside the publishing conditions, event content, and meaning.
    Key features of these events include the topic (serving as the event identifier),
    schema (providing a formal event description), frequency, and a human-readable
    description. This aspect of the data model ensures that the system is not only
    structurally sound but also capable of real-time interaction and response to changing
    conditions within the SHM environment. Figure 2. The Proposed extension of ML
    4.0 [54] (gray) for the structural–mechanical domain (red). 3.3. Decentralized
    Communication and Security In our IoT-based SHM system, despite its hierarchical
    structure, communication with users and service invocation is streamlined through
    unified interfaces and message-based protocols. This design approach ensures that
    technical heterogeneity within the system is effectively addressed, making communication
    seamless and consistent across different system levels. Figure 3 depicts the proposed
    conceptual architecture of the IoT-based system. Interaction with individual objects
    within the system is secured through robust authentication and authorization mechanisms.
    Every interaction necessitates a valid authentication, authorization, and data
    encryption and signing process, guaranteeing the security of communications at
    the IoT level. To facilitate decentralized interactions within the system, we
    have selected the S3I as the preferred IoT infrastructure. The S3I Identity Provider
    offers an authentication service using OAuth 2.0. The outcome of this process
    is an access token representing the granted permissions, which can be used to
    access the S3I Services and decentralized interconnected DTs, services, and apps.
    An authorization system consisting of a policy engine and a policy model (e.g.,
    Role-based Access Control) can be employed to perform the authorization against
    requests and the associated access tokens. Figure 3. Conceptual architecture of
    the IoT-based SHM system with a communication infrastructure. For communication
    within the SHM system, a message-based communication protocol, the S3I-Broker
    (S3I-B) protocol, is employed. This protocol supports data encryption and signing
    using RFC 4880 [60]. This means that each message is encrypted (using the private
    key of the sender) and signed (using the public key of the receiver) before sending.
    Public keys are included in the information model, which is stored in the S3I
    Directory. This protocol encompasses various message types, including user messages
    for direct communications, attribute messages for querying data, and service messages
    for invoking services within the system. For instance, at the component level,
    potential service requests might include querying for overload events, load curves
    over a specified period ( 𝐹(𝑡);𝑡∈[ 𝑡 1 , 𝑡 2 ] ), sensor evaluations in specific
    regions ( 𝜀 𝑠 1 , 𝑠 2 (𝑡,𝑥);𝑡∈[ 𝑡 1 , 𝑡 2 ];𝑥∈[ 𝑥 3 , 𝑥 4 ] ), or the number of
    experienced load changes. Moving beyond the component level, the plant or cluster
    levels can also request services, such as temperature profiles over the system’s
    runtime or retrieval of reference or simulation data from stored databases. These
    outputs, along with user requests, are typically presented via a human–machine
    interface, such as an app. Event-Driven Communication in the SHM System We further
    extend the S3I-B protocol by an event system capable of realizing user-specified
    and event-driven communication. This system supports event exchange irrespective
    of the physical location of the things (in the cloud or on edge devices), underlining
    its flexibility and scalability. The S3I-B Event system is distinguished by its
    thing-centric design, granting each thing the autonomy to define event content,
    frequency, and triggering conditions. This level of customization enables things
    to tailor event management to their specific needs, with the entire process efficiently
    managed by the DT. The protocol is thus augmented with event messages, as depicted
    in Figure 4. We categorize events into two types: named events and custom events.
    Named events are predefined by the DT and described in the data model. Custom
    events are requested by subscribers and emitted based on user-specified rules,
    such as an attribute crossing a threshold. Each event is associated with a specific
    topic, forming the basis of networking and indicating the event’s focus. Figure
    4. The data model for S3I-B event messages in a UML class diagram. 4. The Digital
    Cantilever in the IoT-Based SHM System This section presents a prototypical application
    for a structural–mechanical cantilever beam connected with the IoT S3I infrastructure.
    We detail the technical implementation process, classifying the cantilever beam
    at the component level within our developed hierarchical infrastructure. Additionally,
    we provide a human–machine interface (HMI) for intuitive interaction with the
    monitored component. The choice of a cantilever beam as the base for our prototype
    is rooted in its fundamental nature. A cantilever beam is one of the primary structures
    frequently employed in the construction of cantilever-type components, including
    rotor blades, wings, and cranes. From a structural mechanics perspective, employing
    the cantilever beam allows us to investigate and analyze the foundational principles
    at its core. By isolating the beam as the smallest element, we gain a comprehensive
    understanding of its behavior and mechanical characteristics. Moreover, the proposed
    metadata model and unified communication interfaces not only facilitate this in-depth
    examination but also present an exciting opportunity for the expansion of these
    elemental structures. This expansion delivers possibilities for the development
    of more intricate and sophisticated structural configurations, demonstrating the
    versatility and adaptability inherent in our approach. 4.1. Setup The to-be-monitored
    component is a cantilever beam structure (see dimensions in Figure 5) that can
    be described based on the assumptions of the Bernoulli hypothesis [61]. A strain
    gauge is installed on the beam to measure the strain value. Its resistance varies
    with the beam’s deformation. We assume that the cantilever beam is a simple and
    homogeneous structural–mechanical component with a low complexity. Thus, we only
    consider the deformation along the x-axis. Advanced sensor configurations and
    applications are discussed further in Section 5. Figure 5. Physical Structure,
    the cantilever beam with one strain gauge (left) and its dimensions (right) with
    experimental parameters ℎ=2 , 𝑏=25 , 𝑑 𝑠 =100 , 𝑙=220 in mm. 4.2. Structural Model
    and Computation The initial prototype is built with only one sensor that can only
    determine one unknown variable in the system. However, the load introduction point
    and the magnitude of the applied force 𝐹 𝑟𝑒𝑎𝑙 are two independent effects that
    cannot be detected separately with one sensor. Therefore, an assumed load introduction
    point at the end of the beam and an equivalent force 𝐹 𝑒𝑞 are used for the calculation
    of the beam’s deformation in this case. This simplification is permissible for
    a phenomenological representation of the structural behavior (demonstrator) but
    should be replaced by an unambiguous determination of force and force application
    point for technical purposes, see Figure 5. Based on the assumptions for uniaxial
    bending, we derive the relationship between a strain value 𝜀 and the equivalent
    force 𝐹 𝑒𝑞 with Young’s modulus E, area moment of inertia 𝐼 𝑦 = 𝑏 ℎ 3 12 , cross-section
    geometry b, h, l and sensor position 𝑑 𝑠 , see Equation (1). 𝐸·𝜀(𝑥)=𝜎(𝑥)= 𝑀 𝑦
    (𝑥) 𝐼 𝑦 ℎ 2 = 𝐹 𝑟𝑒𝑎𝑙 (𝑙−Δ 𝑥 𝐹 −𝑥) 𝐼 𝑦 ℎ 2 = 𝐹 𝑒𝑞 (𝑙−𝑥) 𝐼 𝑦 ℎ 2 (1) The strain
    value 𝜀 𝑑𝑠 measured by the sensor corresponds to the strain at the position 𝑥=𝑙−
    𝑑 𝑠 . Therefore, the following applies for 𝜀 𝑑𝑠 =𝜀(𝑙− 𝑑 𝑠 ) , see Equation (2):
    𝐸·𝜀(𝑥)= 𝐹 𝑒𝑞 𝑑 𝑠 𝐼 𝑦 ℎ 2 (2) Rearranged and written for each time step t, it follows:
    𝐹 𝑒𝑞 (𝑡)= 2𝐸 𝐼 𝑦 ℎ 𝑑 𝑠 𝜀 𝑑𝑠 (𝑡) (3) Using the provided equivalent force 𝐹 𝑒𝑞 ,
    further information for the cantilever beam, for example, globally (approximated)
    deflection 𝑤(𝑥,𝑡) (see Equation (4)) and maximum stress 𝜎 𝑚𝑎𝑥 (see Equation (5))
    in the clamping, can be calculated. 𝑤(𝑥,𝑡)= 𝐹 𝑒𝑞 (𝑡) 6𝐸 𝐼 𝑦 (3𝑙 𝑥 2 − 𝑥 3 ) (4)
    𝜎 𝑚𝑎𝑥 (𝑡)=𝜎(𝑥=0,𝑡)= 𝐹 𝑒𝑞 (𝑡)𝑙ℎ 2 𝐼 𝑦 (5) In the chosen modeling approach, we tolerate
    inaccuracy in the calculated deflection (see Equation (4)), which represents the
    deformation of a cantilever for the load introduction point at the end of the
    beam. In reality, however, the beam does not have a cubic but a linear deflection
    curve from the actual force application point. The error only occurs in the range
    𝑥>𝑙− 𝑑 𝑠 and increases with increasing distance of the actual force application
    point from the end of the beam. The reduced complexity of the calculation approach
    makes it possible to capture the global behavior of the structure with little
    computational and sensing effort. In principle, the more detailed and local the
    modeling, the more complex the modeling must be, and the more computing power
    is required for each time step. An alternative method for real-time calculation
    of deformation of the cantilever beam structures is presented in [62]. 4.3. Communication
    Architecture The overall communication architecture is illustrated in Figure 6.
    By means of the S3I, the cantilever beam, its corresponding DT, a simulation service,
    and the visualization app are connected to enable situation-specific choreography.
    In the following, we present several hardware and software components involved
    in the architecture. Figure 6. The communication architecture to monitor the cantilever
    beam. 4.3.1. Physical Twin In the IoT, a physical twin (PT) is an entity that
    exists in the physical world. Here, the PT refers to the aluminum beam with one
    installed strain gauge, see Figure 5, measuring the strain value 𝜀 𝑑𝑠 (𝑡) at the
    given position 𝑑 𝑠 . 4.3.2. Digital Twin The DT in this application represents
    the cantilever beam, formally described using the extended ML 4.0. The formalization
    is considered to be a basis for interoperability. The DT is modeled in an object
    diagram, as interpreted in Figure 7. Figure 7. A ML 4.0-based data model of the
    cantilever beam. As assigned to the role mml40::Cantilever, this DT is associated
    with various value properties such as the strain value (mml40::Strain) measured
    by the strain gauge. The property (mml40::MaterialType) states the type of material
    used to construct the cantilever beam. The beam’s geometry is described by ml40::Dimensions.
    The numerical equations introduced in Section 4.2 are the base for the implemented
    software services that provide strain values (mml40::providesStrainData) and calculated
    deformation (providesDeformationData). Both services are integrated into the beam’s
    DT and, thus, can be called through the unified interfaces via the IoT, realizing
    a so-called passive DT. The composition object (ml40::Composite) implies a compositional
    relationship between the cantilever and the strain gauge that directly manages
    the strain value, which is represented using mml40::Strain. In addition to storing
    and representing the current status, the developed DT also hosts a calculation
    service dedicated to converting the measured strain values into the equivalent
    force and at the beam end and the deflection, using the introduced equations in
    Section 4.2. The use of the event system (introduced in Section 3.3) enables the
    realization of an active DT. Here, DT can \"recognize” each measurement of new
    strain value and notices all subscribers in the form of an S3I-B event message.
    The overall technical implementation is performed using the ML 4.0-based python
    framework [63], which provides the DT with a software runtime environment to ensure
    IoT connectivity. 4.3.3. Edge Device The DT is developed as an edge DT. This means
    that the DT “lives” in an edge device localized near the beam. This device provides
    the hardware runtime environment for the DT. The edge approach empowers SHM systems
    to maintain functionality even in an internet connection failure. Thanks to the
    DT’s modularization, reliable operation of sensors and data processing are separated
    from the Internet module, ensuring uninterrupted monitoring and analysis capabilities
    offline. In our application, we integrate all the needed hardware components into
    a compact box entitled DT Box, see Figure 6. The corn component of the DT Box
    is a Raspberry Pi 4, which acts as a computing unit. It is powered by a power
    supply module, which consists of a 4-pin interface and a DC/DC converter. This
    converter ensures that the box can be supplied with a stable, suitable voltage
    from a variety of input voltages from 24 V to 220 V. To measure the strain value,
    a voltage must be applied across the strain gauge. Here, a variable current (i.e.,
    an analog value) is generated. With an A/D converter, the current is converted
    to a digital value, which can subsequently be transferred directly to the Raspberry
    Pi for further processing via a serial communication protocol, I2C. We also installed
    an antenna connector in the DT Box for better Wi-Fi connectivity. 4.3.4. Simulation
    Services Simulation services often serve as a tool for the development, validation,
    and verification of algorithms. In SHMs, simulations aid in predicting the future
    behavior of physical structures and identifying potential issues before they become
    critical while considering cost-effectiveness. In this prototypical application,
    we introduce and deploy two simulation services. The first one refers to uniaxial
    load estimation. In the image, a downward force is applied at the end of the cantilever
    beam, resulting in deformation. In the first load estimation service, we specify
    the x-coordinate. Based on that, the simulation service estimates the load received
    at that point. The second service is concerned with maintenance estimation. Here,
    we simplify the whole process of estimation and give only the load applied to
    a specific x-coordinate. From this, the service estimates the remaining useful
    life of the cantilever beam under these conditions. Analogous to DTs, both services
    are so-called passive communications participants in the IoT-based SHM system,
    providing interfaces for interoperable service retrieval. 4.3.5. IoT Infrastructure
    (S3I) The use of the S3I necessitates the registration of identities. Concretely,
    the cantilever and the app are assigned a client ID and secret as credentials.
    Each user must register an S3I account to perform Single-Sign-On at the app. The
    result of the authentication and authorization process is an access token. The
    token must be provided for each interaction to ensure communication security.
    In ML 4.0, there are two mappings of the data model to JSON: (1) mapping on a
    metadata directory (stored in the S3I Directory) and (2) mapping on a runtime
    environment (stored in the edge device, i.e., DT Box). The use of the provided
    mappings delivers an overview of the thing both at a meta-level as well as a human-
    and machine-understandable level. In addition, we apply the event system, which
    allows the DT of the cantilever beam to perform near real-time conditional monitoring.
    For instance, the DT of the beam generates and proactively emits event messages
    (see Listing 1)—that incorporate the critical information about states—with a
    pre-configured time frequency. Listing 1. An example of an event message in json
    format that denotes the current operational values of the cantilever beam 4.3.6.
    User and App To provide a user-friendly interface to configure and monitor the
    cantilever beam, we built a Flutter-based app that also acts as a decentralized
    thing connected to the S3I, see Figure 8. Figure 8. An app for the intuitive interaction
    with the cantilever beam. Like the DT of the cantilever beam, the first step to
    use the app is authenticating and authorizing before a user needs to start data
    exchange with the cantilever beam. The interaction via S3I requires an S3I-compliant
    interface, which is directly integrated into the app. In our use case, the configuration
    parameters (e.g., geometry, sensor position, and material) can be transferred
    from the app to the cantilever using an S3I-B service request (typed as mml40::SetConfiguration).
    Accordingly, those parameters can also be retrieved with mml40::GetConfiguration
    as both are available services provided by the DT. To monitor the state change,
    the app subscribes to the events that are triggered by the DT of the cantilever
    beam if new measurements are available. After receiving these events, the app
    intends to visualize them in various diagrams. 5. Discussion Building on the established
    example, different variations are conceivable to extend the functionality and
    applicability of the proposed IoT-based SHM system. These variations include changes
    in sensor configuration, the type and the number of components, and the execution
    platforms. 5.1. Variations of Sensors An immediate enhancement involves adding
    additional strain gauges to the beam. For example, with an extra strain gauge
    along the y-axis of the beam, force and force application points can be determined
    simultaneously. In this context, the extension can be made from load monitoring
    to damage monitoring. Utilizing sensitive structural damage indicators, such as
    zero-strain trajectories [64], can significantly enhance monitoring capabilities,
    particularly for detecting cracks or other forms of structural damage. Here, a
    sensor setup as presented in [65] can be referenced. 5.2. Variations of Components
    In addition to the aforementioned cantilever example, an extension of the proposed
    IoT-based SHM system for other structural components is possible. This adaptability
    allows for monitoring larger structures like a construction crane or a wind turbine
    rotor blade. To effectively monitor large structures using a strain-based SHM
    approach, several distributed measuring points become essential, as damage typically
    affects the strain field locally. This requires a comprehensive network of sensors
    to capture these localized variations. In response to this, large structures integrate
    distributed sensors to form structurally and hierarchically more complex models,
    establishing systems at the plant layer. These systems operate as individual entities
    in the IoT, facilitating the centralized processing of distributed strain signals.
    An illustrative example is depicted in Figure 9, showcasing an emulated crane.
    The crane is a compositional aggregation of cantilever beams. Here, various virtual
    sensors are installed on the crane, delivering strain values during the operation
    of the crane. The crane’s DT centralized processes strain values, and thus, operational
    data such as deformation and force at the crane’s end are calculated. The access
    to the DT via the IoT infrastructure enables near real-time monitoring of the
    emulated crane with an app depicted in Figure 9. Similar ideas can be applied
    to retrofit more complex cantilever-typed systems, in which the basic element
    is always the cantilever beam. Overall, using the proposed approach not only enhances
    the overall availability of the health status of structures but also provides
    potential for efficient management of diverse components within a larger structural
    framework. Figure 9. An emulated crane (right) built in a 3D simulation software
    app (left), visualizing the simulated crane’s operational data. 5.3. Variations
    of Execution Platforms The implementation of DTs can be realized through different
    technical approaches. This flexibility allows for realizing DTs either (1) directly
    at an edge device that is near the assets (Edge DT), (2) to execute within a cloud
    service (Cloud DT), or (3) as a hybrid model in between, known as Fog DT. The
    diversity in execution platforms is intentionally unrestricted. Assets, which
    represent physical objects with limited computation and communication resources,
    always reside in their DTs in an edge device. Conversely, assets in the form of
    virtual objects, encompassing data sets and algorithms, deploy their DTs in the
    cloud. In addition to the presented application in Section 4, we also have the
    option to relocate the DT of the cantilever beam to a cloud service. In this case,
    we only need to integrate the interface on the cantilever’s side, ensuring the
    synchronization between both the physical cantilever and its cloud DT. This variation
    becomes particularly meaningful for applications requiring robust computing performance.
    Furthermore, we introduce the possibility of splitting the DT into two distinct
    parts. The first part resides at the edge device, which is responsible for collecting
    and preprocessing strain values. Meanwhile, the second part, encompassing the
    data model and services to calculate corresponding forces and displacements, is
    hosted in a cloud service. This dual-partitioning of the DT allows for a distributed
    approach, leveraging the strengths of both edge and cloud computing. Overall,
    Fog DTs serve as solutions for applications demanding a balance between real-time
    processing and computational complexity. 6. Conclusions This paper introduced
    a novel organizational scheme for IoT-based SHM systems, leveraging the capabilities
    of DTs. Grounded in the comprehensive literature review, our proposed framework
    integrates structural components into systematically organized clusters, enhancing
    the SHM process through IoT infrastructures. The cornerstones of the presented
    infrastructure are its flexibility and extensibility, enabling the adaptation
    of objects with varying scopes and scales. A basic element of our approach is
    decentralized networking via the S3I communication infrastructure. The S3I provides
    all communication participants with a globally unique identity and services for
    authentication and authorization, ensuring secure and interoperable data exchange
    as well as service calls. Moreover, the existing ML 4.0 modeling language is extended
    to encompass special properties and service functions required for SHM applications,
    therefore establishing a comprehensive data model that delineates the structure
    and content among physical assets and their DTs. This shared formal data model
    increases interoperability among participants and allows the addition of semantic
    information to further ease the integration of new components into SHM systems.
    To validate our framework, we implement a prototype using a mechanical system
    built with a simple cantilever beam. This demonstrator shows its capability to
    deliver live strain, force, and deformation data to an app in real time via its
    DT. This data access is only possible after users have successfully authenticated
    themselves and granted the appropriate authorization. The actual data exchange
    is mediated by the S3I Broker. This demonstration further illustrates the hierarchical
    concept, as the DT of the cantilever includes the information and algorithms to
    process raw sensor data and outputs meaningful monitoring and analysis data. Finally,
    the demonstrator shows the flexibility with which different software components
    can be deployed. In this example, the cantilever’s DT is being deployed on an
    edge device, while simulation services are deployed on a server, and the user
    interface is running locally on the user’s smartphone. This infrastructure can
    be extended to other structures or sensor configurations. With this, the functional
    test is completed. However, to validate the framework’s versatility and applicability
    in various contexts, additional case studies in various domains, such as bridges,
    high-rise buildings, and industrial equipment, are necessary. These case studies
    will help in establishing the system’s adaptability to different structural complexities
    and environmental conditions. In conclusion, our research indicates that SHM systems
    can effectively be transferred into IoT solutions using our organizational scheme.
    Author Contributions Conceptualization, J.C., J.R. (Jan Reitz) and R.R.; methodology,
    J.C, J.R. (Jan Reitz) and R.R.; software, J.C. and J.R. (Jan Reitz); validation,
    J.C. and J.R. (Jan Reitz); formal analysis, J.C., J.R. (Jan Reitz) and R.R.; investigation,
    J.C. and J.R. (Jan Reitz); resources, J.C. and R.R.; data curation, R.R.; writing—original
    draft preparation, J.C., J.R. (Jan Reitz) and R.R.; writing—review and editing,
    J.C. and J.R. (Jan Reitz); visualization, J.R. (Jan Reitz); supervision, K.-U.S.
    and J.R. (Jürgen Roßmann); project administration, K.-U.S. and J.R. (Jürgen Roßmann);
    funding acquisition, K.-U.S. and J.R. (Jürgen Roßmann) All authors have read and
    agreed to the published version of the manuscript. Funding This research was funded
    by the European Regional Development Fund (ERDF) and supported by the state of
    North Rhein–Westphalia under grant number EFRE-0200458 (EFRE.NRW project Kompetenzzentrum
    Wald und Holz 4.0). Institutional Review Board Statement Not applicable. Informed
    Consent Statement Not applicable. Data Availability Statement Data are contained
    within the article. Conflicts of Interest The authors declare no conflicts of
    interest. Abbreviations The following abbreviations are used in this manuscript:
    AD Analog Digital CIA Confidentiality, Integrity, and Availability DC Direct Current
    DT Digital Twin F4.0 Forestry 4.0 ForestML 4.0 Forest Modeling Language 4.0 JSON
    JavaScript Object Notation HMI Human–Machine Interface I2C Inter-Integrated Circuit
    IoT Internet of Things MQTT Message Queuing Telemetry Transport PT Physical Twin
    REST Representational State Transfer S3I Smart Systems Service Infrastructure
    SHM Structural Health Monitoring UML Unified Modeling Language WSN Wireless Sensor
    Network References Charef, A.; Ghauch, A.; Baussand, P.; Martin-Bouyer, M. Water
    quality monitoring using a smart sensing system. Measurement 2000, 28, 219–224.
    [Google Scholar] [CrossRef] Ullo, S.L.; Sinha, G.R. Advances in smart environment
    monitoring systems using IoT and sensors. Sensors 2020, 20, 3113. [Google Scholar]
    [CrossRef] An, B.W.; Shin, J.H.; Kim, S.Y.; Kim, J.; Ji, S.; Park, J.; Lee, Y.;
    Jang, J.; Park, Y.G.; Cho, E.; et al. Smart sensor systems for wearable electronic
    devices. Polymers 2017, 9, 303. [Google Scholar] [CrossRef] Lorenzi, P.; Rao,
    R.; Romano, G.; Kita, A.; Serpa, M.; Filesi, F.; Parisi, R.; Suppa, A.; Bologna,
    M.; Berardelli, A.; et al. Smart sensing systems for the detection of human motion
    disorders. Procedia Eng. 2015, 120, 324–327. [Google Scholar] [CrossRef] Chen,
    Z.; Zhou, X.; Wang, X.; Dong, L.; Qian, Y. Deployment of a Smart Structural Health
    Monitoring System for Long-Span Arch Bridges: A Review and a Case Study. Sensors
    2017, 17, 2151. [Google Scholar] [CrossRef] [PubMed] Güemes, A. SHM technologies
    and applications in aircraft structures. In Proceedings of the 5th International
    Symposium on NDT in Aerospace, Singapore, 13–15 November 2013; Volume 1315. [Google
    Scholar] Malekimoghadam, R.; Krause, S.; Czichon, S. A Critical Review on the
    Structural Health Monitoring Methods of the Composite Wind Turbine Blades. In
    Proceedings of the 1st International Conference on Structural Damage Modelling
    and Assessment, Ghent, Belgium, 4–5 August 2020; Springer: Singapore, 2021; pp.
    409–438. [Google Scholar] [CrossRef] Balageas, D.; Fritzen, C.P.; Güemes, A. Structural
    Health Monitoring; John Wiley & Sons: Hoboken, NJ, USA, 2010; Volume 90. [Google
    Scholar] Farrar, C.R.; Worden, K. An introduction to structural health monitoring.
    Philos. Trans. Ser. Math. Phys. Eng. Sci. 2007, 365, 303–315. [Google Scholar]
    [CrossRef] [PubMed] Noel, A.B.; Abdaoui, A.; Elfouly, T.; Ahmed, M.H.; Badawy,
    A.; Shehata, M.S. Structural Health Monitoring Using Wireless Sensor Networks:
    A Comprehensive Survey. IEEE Commun. Surv. Tutor. 2017, 19, 1403–1423. [Google
    Scholar] [CrossRef] Mishra, M.; Lourenço, P.B.; Ramana, G.V. Structural health
    monitoring of civil engineering structures by using the internet of things: A
    review. J. Build. Eng. 2022, 48, 103954. [Google Scholar] [CrossRef] Mahmud, M.A.;
    Bates, K.; Wood, T.; Abdelgawad, A.; Yelamarthi, K. A complete internet of things
    (IoT) platform for structural health monitoring (shm). In Proceedings of the 2018
    IEEE 4th World Forum on Internet of Things (WF-IoT), Singapore, 5–8 February 2018;
    pp. 275–279. [Google Scholar] Abdelgawad, A.; Yelamarthi, K. Internet of things
    (IoT) platform for structure health monitoring. Wirel. Commun. Mob. Comput. 2017,
    2017, 6560797. [Google Scholar] [CrossRef] Scuro, C.; Lamonaca, F.; Porzio, S.;
    Milani, G.; Olivito, R. Internet of Things (IoT) for masonry structural health
    monitoring (SHM): Overview and examples of innovative systems. Constr. Build.
    Mater. 2021, 290, 123092. [Google Scholar] [CrossRef] Kamal, M.; Mansoor, A. Structural
    Health Monitoring and IoT: Opportunities and Challenges. In Proceedings of the
    International Conference on Intelligence of Things, Hanoi, Vietnam, 17–19 August
    2022; pp. 3–15. [Google Scholar] Alcaraz, C.; Lopez, J. Digital twin: A comprehensive
    survey of security threats. IEEE Commun. Surv. Tutor. 2022, 24, 1475–1503. [Google
    Scholar] [CrossRef] Xu, H.; Wu, J.; Pan, Q.; Guan, X.; Guizani, M. A survey on
    digital twin for industrial internet of things: Applications, technologies and
    tools. IEEE Commun. Surv. Tutor. 2023, 25, 2569–2598. [Google Scholar] [CrossRef]
    Bado, M.F.; Tonelli, D.; Poli, F.; Zonta, D.; Casas, J.R. Digital twin for civil
    engineering systems: An exploratory review for distributed sensing updating. Sensors
    2022, 22, 3168. [Google Scholar] [CrossRef] [PubMed] Ye, C.; Butler, L.; Calka,
    B.; Iangurazov, M.; Lu, Q.; Gregory, A.; Girolami, M.; Middleton, C. A digital
    twin of bridges for structural health monitoring. In Proceedings of the 12th International
    Workshop on Structural Health Monitoring 2019, Standford, CA, USA, 10–12 September
    2019. [Google Scholar] Pillai, S.; Iyengar, V.; Pathak, P. Monitoring Structural
    Health Using Digital Twin. In Digital Twin Technology: Fundamentals and Applications;
    Scrivener Publishing: Beverly, MA, USA, 2022; pp. 125–139. [Google Scholar] Rainieri,
    C.; Rosati, I.; Cieri, L.; Fabbrocino, G. Development of the digital twin of a
    historical structure for SHM purposes. In Proceedings of the European Workshop
    on Structural Health Monitoring, Palermo, Italy, 4–7 July 2022; pp. 639–646. [Google
    Scholar] Gao, Y.; Li, H.; Xiong, G.; Song, H. AIoT-informed digital twin communication
    for bridge maintenance. Autom. Constr. 2023, 150, 104835. [Google Scholar] [CrossRef]
    Zhu, Y.C.; Wagg, D.; Cross, E.; Barthorpe, R. Real-time digital twin updating
    strategy based on structural health monitoring systems. In Model Validation and
    Uncertainty Quantification, Volume 3, Proceedings of the 38th IMAC, A Conference
    and Exposition on Structural Dynamics 2020, Austin, TX, USA, 13–16 February 2023;
    Springer: Berline/Heidelberg, Germany, 2020; pp. 55–64. [Google Scholar] Dang,
    H.V.; Tatipamula, M.; Nguyen, H.X. Cloud-based digital twinning for structural
    health monitoring using deep learning. IEEE Trans. Ind. Inform. 2021, 18, 3820–3830.
    [Google Scholar] [CrossRef] Wenner, M.; Meyer-Westphal, M.; Herbrand, M.; Ullerich,
    C. The concept of digital twin to revolutionise infrastructure maintenance: The
    pilot project smartBRIDGE Hamburg. In Proceedings of the 27th ITS World Congress,
    Hamburg, Germany, 11–15 October 2021; pp. 11–15. [Google Scholar] Chiachío, M.;
    Megía, M.; Chiachío, J.; Fernandez, J.; Jalón, M.L. Structural digital twin framework:
    Formulation and technology integration. Autom. Constr. 2022, 140, 104333. [Google
    Scholar] [CrossRef] Aguzzi, C.; Gigli, L.; Sciullo, L.; Trotta, A.; Zonzini, F.;
    De Marchi, L.; Di Felice, M.; Marzani, A.; Cinotti, T.S. MODRON: A Scalable and
    Interoperable Web of Things Platform for Structural Health Monitoring. In Proceedings
    of the 2021 IEEE 18th Annual Consumer Communications and Networking Conference
    (CCNC), Las Vegas, NV, USA, 9–12 January 2021; pp. 1–7. [Google Scholar] [CrossRef]
    Lamonaca, F.; Scuro, C.; Grimaldi, D.; Sante Olivito, R.; Sciammarella, P.F.;
    Carnì, D.L. A layered IoT-based architecture for a distributed structural health
    monitoring system System. Acta Imeko 2019, 8, 45. [Google Scholar] [CrossRef]
    Zonzini, F.; Aguzzi, C.; Gigli, L.; Sciullo, L.; Testoni, N.; de Marchi, L.; Di
    Felice, M.; Cinotti, T.S.; Mennuti, C.; Marzani, A. Structural Health Monitoring
    and Prognostic of Industrial Plants and Civil Structures: A Sensor to Cloud Architecture.
    IEEE Instrum. Meas. Mag. 2020, 23, 21–27. [Google Scholar] [CrossRef] Tokognon,
    C.A.; Gao, B.; Tian, G.Y.; Yan, Y. Structural health monitoring framework based
    on Internet of Things: A survey. IEEE Internet Things J. 2017, 4, 619–635. [Google
    Scholar] [CrossRef] Jones, D.; Snider, C.; Nassehi, A.; Yon, J.; Hicks, B. Characterising
    the Digital Twin: A systematic literature review. CIRP J. Manuf. Sci. Technol.
    2020, 29, 36–52. [Google Scholar] [CrossRef] Richstein, R.; Schröder, K.U. Characterizing
    the Digital Twin in Structural Mechanics. Designs 2024, 8, 8. [Google Scholar]
    [CrossRef] Tao, F.; Xiao, B.; Qi, Q.; Cheng, J.; Ji, P. Digital twin modeling.
    J. Manuf. Syst. 2022, 64, 372–389. [Google Scholar] [CrossRef] Konduru, V.R.;
    Bharamagoudra, M.R. Challenges and solutions of interoperability on IoT: How far
    have we come in resolving the IoT interoperability issues. In Proceedings of the
    2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon),
    Bengaluru, India, 17–19 August 2017; pp. 572–576. [Google Scholar] Gigli, L.;
    Sciullo, L.; Montori, F.; Marzani, A.; Di Felice, M. Blockchain and Web of Things
    for Structural Health Monitoring Applications: A Proof of Concept. In Proceedings
    of the 2022 IEEE 19th Annual Consumer Communications and Networking Conference
    (CCNC), Las Vegas, NV, USA, 8–11 January 2022; pp. 699–702. [Google Scholar] [CrossRef]
    Dragos, K.; Theiler, M.; Magalhães, F.; Moutinho, C.; Smarsly, K. On-board data
    synchronization in wireless structural health monitoring systems based on phase
    locking. Struct. Control. Health Monit. 2018, 25, e2248. [Google Scholar] [CrossRef]
    Sakr, M.; Sadhu, A. Visualization of structural health monitoring information
    using Internet-of-Things integrated with building information modeling. J. Infrastruct.
    Intell. Resil. 2023, 2, 100053. [Google Scholar] [CrossRef] Swartz, R.A. Decentralized
    algorithms for SHM over wireless and distributed smart sensor networks. In Earthquakes
    and Health Monitoring of Civil Structures; Springer: Dordrecht, The Netherlands,
    2013; pp. 109–131. [Google Scholar] Chang, F.K.; Markmiller, J.F.; Yang, J.; Kim,
    Y. Structural health monitoring. In System Health Management: With Aerospace Applications;
    John Wiley & Sons: Haboken, NJ, USA, 2011; pp. 419–428. [Google Scholar] Reshan,
    A.; Saleh, M. IoT-based Application of Information Security Triad. Int. J. Interact.
    Mob. Technol. 2021, 15, 61–76. [Google Scholar] [CrossRef] Chiranjeevi, S.; Manimegalai,
    R.; Saravanan, U. Program Architecture for Structural Health Monitoring of Pamban
    Bridge. In Computational Intelligence, Cyber Security, and Computational Models,
    Preceedings of the 5th International Conference, ICC3 2021, Coimbatore, India,
    16–18 December 2021; Springer: Berlin/Heidelberg, Germany, 2022; pp. 18–30. [Google
    Scholar] Toutsop, O.; Kornegay, K.; Smith, E. A comparative analyses of current
    IoT middleware platforms. In Proceedings of the 2021 8th International Conference
    on Future Internet of Things and Cloud (FiCloud), Virtual, 23–25 August 2021;
    pp. 413–420. [Google Scholar] Jung, S.; Ferber, S.; Cramer, I.; Bronner, W.; Wortmann,
    F. Bosch IoT Suite: Exploiting the Potential of Smart Connected Products. In Connected
    Business; Springer: Berlin/Heidelberg, Germany, 2021; pp. 267–282. [Google Scholar]
    Petrik, D.; Herzwurm, G. iIoT ecosystem development through boundary resources:
    A Siemens MindSphere case study. In Proceedings of the 2nd ACM SIGSOFT International
    Workshop on Software-Intensive Business: Start-Ups, Platforms, and Ecosystems,
    Tallinn, Estonia, 26 August 2019; pp. 1–6. [Google Scholar] Copeland, M.; Soh,
    J.; Puca, A.; Manning, M.; Gollob, D. Microsoft Azure; Apress: New York, NY, USA,
    2015; pp. 3–26. [Google Scholar] Malik, S.; Rouf, R.; Mazur, K.; Kontsos, A. The
    industry Internet of Things (IIoT) as a methodology for autonomous diagnostics
    in aerospace structural health monitoring. Aerospace 2020, 7, 64. [Google Scholar]
    [CrossRef] Nath, S.V.; Van Schalkwyk, P.; Isaacs, D. Building Industrial Digital
    Twins: Design, Develop, and Deploy Digital Twin Solutions for Real-World Industries
    Using Azure Digital Twins; Packt Publishing Ltd.: Birmingham, UK, 2021. [Google
    Scholar] De Paolis, L.T.; De Luca, V.; Paiano, R. Sensor data collection and analytics
    with thingsboard and spark streaming. In Proceedings of the 2018 IEEE Workshop
    on Environmental, Energy, and Structural Monitoring Systems (EESMS), Taranto,
    Italy, 21–22 June 2018; pp. 1–6. [Google Scholar] Tyagi, H.; Kumar, R. Cloud computing
    for iot. In Internet of Things (IoT) Concepts and Applications; Springer: Cham,
    Switzerland, 2020; pp. 25–41. [Google Scholar] Ismail, A.A.; Hamza, H.S.; Kotb,
    A.M. Performance evaluation of open source IoT platforms. In Proceedings of the
    2018 IEEE Global Conference on Internet of Things (GCIoT), Alexandria, Eqypt,
    5–7 December 2018; pp. 1–5. [Google Scholar] Chen, J.; Schluse, M.; Roßmann, J.
    Enabling a Secured Communication in Distributed IoT Using the Smart Systems Service
    Infrastructure. In Proceedings of the 2021 IEEE International Conference on Pervasive
    Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),
    Kassel, Germany, 22–26 March 2021; pp. 674–679. [Google Scholar] Hoppen, M. Smart
    Systems Service Infrastructure (S3I)—Design and Deployment of the Smart Systems
    Service Infrastructure (S3I) for Decentralized Networking in Forestry 4.0, A KWH4.0
    Position Paper. 2022. Available online: https://www.kwh40.de/wp-content/uploads/2022/02/KWH40-Standpunkt-S3I-EN.pdf
    (accessed on 26 December 2022). Chen, J.; Hoppen, M.; Böken, D.; Reitz, J.; Schluse,
    M.; Roßmann, J. Identity, Authentication and Authorization in Forestry 4.0 Using
    OAuth 2.0. In Proceedings of the 2022 3rd International Informatics and Software
    Engineering Conference (IISEC), Ankara, Turkey, 15–16 December 2022; pp. 1–6.
    [Google Scholar] Hoppen, M. Forest Modeling Language 4.0—Konzeption und Einsatz
    der Forest Modeling Language (fml40) zur Modellierung von Wald und Holz 4.0-Dingen.
    2020. Available online: https://www.kwh40.de/wp-content/uploads/2020/03/KWH40-Standpunkt-fml40-Version-1.0.pdf
    (accessed on 26 December 2022). Thiele, C.D.; Brötzmann, J.; Huyeng, T.J.; Rüppel,
    U.; Lorenzen, S.; Berthold, H.; Schneider, J. A Digital Twin as a framework for
    a machine learning based predictive maintenance system. In ECPPM 2021-eWork and
    eBusiness in Architecture, Engineering and Construction; CRC Press: Boca Raton,
    FL, USA, 2021; pp. 313–319. [Google Scholar] Peplinski, J.; Singh, P.; Sadhu,
    A. Real-Time Structural Inspection Using Augmented Reality. In Proceedings of
    the Canadian Society of Civil Engineering Annual Conference, Whistler, BC, Canada,
    25–28 May 2022; pp. 1045–1057. [Google Scholar] Lorusso, A.; Guida, D. IoT system
    for structural monitoring. In Proceedings of the International Conference “New
    Technologies, Development and Applications”, Sarajevo, Bosnia and Herzegovina,
    22–24 June 2022; pp. 599–606. [Google Scholar] Colace, F.; Elia, C.; Guida, C.G.;
    Lorusso, A.; Marongiu, F.; Santaniello, D. An IoT-based framework to protect cultural
    heritage buildings. In Proceedings of the 2021 IEEE International Conference on
    Smart Computing (SMARTCOMP), Irvine, CA, USA, 23–27 August 2021; pp. 377–382.
    [Google Scholar] de Alteriis, G.; Caputo, E.; Moriello, R.S.L. On the suitability
    of redundant accelerometers for the implementation of smart oscillation monitoring
    system: Preliminary assessment. Acta IMEKO 2023, 12, 1–9. [Google Scholar] [CrossRef]
    Callas, J.; Donnerhacke, L.; Finney, H.; Shaw, D.; Thayer, R. RFC 4880: OpenPGP
    Message Format. 2007. Available online: https://www.rfc-editor.org/rfc/rfc4880
    (accessed on 26 December 2022). Spura, C. Herleitung der E uler-B ernoulli-Balkentheorie.
    In Einführung in die Balkentheorie nach Timoshenko und Euler-Bernoulli; Springer:
    Wiesbaden, Germany, 2019. [Google Scholar] Krause, M.; Schröder, K.U.; Kaufmann,
    D.; Osterloh, T.; Roßmann, H.J. Coupling of rigid body dynamics with structural
    mechanics to include elastic deformations in a real-time capable holistic simulation
    for digital twins. In Proceedings of the 2018 European Simulation and Modelling
    Conference, ESM, Ghent, Belgium, 24–26 October 2018; pp. 77–81. [Google Scholar]
    Chen, J.; Roßmann, J. Enabling Digitalization in Forestry 4.0 Using ForestML 4.0-based
    Digital Twins. In Proceedings of the 2022 International Conference on Artificial
    Intelligence of Things (ICAIoT), Istanbul, Turkey, 29–30 December 2022; pp. 1–6.
    [Google Scholar] Schagerl, M.; Viechtbauer, C.; Schaberger, M. Optimal Placement
    of Fiber Optical Sensors along Zero-strain Trajectories to Detect Damages in Thin-walled
    Structures with Highest Sensitivity. In Proceedings of the Structural Health Monitoring
    2015, Stanford, CA, USA, 1–3 September 2015; Chang, F.K., Kopsaftopoulos, F.,
    Eds.; Destech Publications: Lancaster, PA, USA, 2015. [Google Scholar] [CrossRef]
    Richstein, R.; Schmid, S.; Schröder, K.U. Using SHM for the representation of
    structural components over their service life within digital twins. In European
    Workshop on Structural Health Monitoring; Rizzo, P., Milazzo, A., Eds.; Springer
    eBook Collection; Springer International Publishing and Imprint Springer: Cham,
    Switzerland, 2023; pp. 433–442. [Google Scholar] Disclaimer/Publisher’s Note:
    The statements, opinions and data contained in all publications are solely those
    of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s).
    MDPI and/or the editor(s) disclaim responsibility for any injury to people or
    property resulting from any ideas, methods, instructions or products referred
    to in the content.  © 2024 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Chen, J.; Reitz, J.; Richstein, R.; Schröder,
    K.-U.; Roßmann, J. IoT-Based SHM Using Digital Twins for Interoperable and Scalable
    Decentralized Smart Sensing Systems. Information 2024, 15, 121. https://doi.org/10.3390/info15030121
    AMA Style Chen J, Reitz J, Richstein R, Schröder K-U, Roßmann J. IoT-Based SHM
    Using Digital Twins for Interoperable and Scalable Decentralized Smart Sensing
    Systems. Information. 2024; 15(3):121. https://doi.org/10.3390/info15030121 Chicago/Turabian
    Style Chen, Jiahang, Jan Reitz, Rebecca Richstein, Kai-Uwe Schröder, and Jürgen
    Roßmann. 2024. \"IoT-Based SHM Using Digital Twins for Interoperable and Scalable
    Decentralized Smart Sensing Systems\" Information 15, no. 3: 121. https://doi.org/10.3390/info15030121
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 20. Feb 25. Feb 1. Mar 6. Mar
    11. Mar 16. Mar 21. Mar 26. Mar 31. Mar 5. Apr 0 1000 250 500 750 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Information, EISSN 2078-2489, Published by MDPI RSS
    Content Alert Further Information Article Processing Charges Pay an Invoice Open
    Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For
    Editors For Librarians For Publishers For Societies For Conference Organizers
    MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia
    JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive
    issue release notifications and newsletters from MDPI journals Select options
    Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer
    Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Information (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: IoT-Based SHM Using Digital Twins for Interoperable and Scalable Decentralized
    Smart Sensing Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Macías A.
  - Muñoz D.
  - Navarro E.
  - González P.
  citation_count: '1'
  description: 'Recent technological improvements have made it possible for pervasive
    computing intelligent environments, augmented by sensors and actuators, to offer
    services that support society''s aims for a wide variety of applications. This
    requires the fusion of data gathered from multiple sensors to convert them into
    information to obtain valuable knowledge. Poor implementation of data fusion hinders
    the appropriate actions from being taken and offering the appropriate support
    to users and environment needs, particularly relevant in the healthcare domain.
    Data fusion poses challenges that are mainly related to the quality of the data
    or data sources, the definition of a data fusion process and evaluating the data
    fusion carried out. There is also a lack of holistic engineering frameworks to
    address these challenges. These frameworks should be able to support automated
    methods of extracting knowledge from information, selecting algorithms and techniques,
    assessing information and evaluating information fusion systems in an automatic
    and standardized manner. This work proposes a holistic framework to improve data
    fusion in pervasive systems, addressing the issues identified by means of two
    processes: the first of which guides the design of the system architecture and
    focuses on data management. It is based on a previous proposal that integrated
    aspects of Data Fabric and Digital Twins to solve data management and data contextualization
    and representation issues, respectively. The extension of the previous proposal
    presented here was mainly defined by integrating aspects and techniques from different
    well-known multi-sensor data fusion models. The previous proposal identified high-level
    data processing activities and was intended to facilitate their traceability to
    components in the system architecture. However, the previously defined stages
    are not completely adequate in a data fusion process and the data processing tasks
    to be performed in each stage are not described in detail, especially in the data
    fusion stages. The second process of the framework deals with evaluating data
    fusion systems and is based on international standards to ensure the quality of
    the data fusion tasks performed by such systems. This process also offers guidelines
    for designing the architecture of an evaluation subsystem to automatically perform
    data fusion evaluation in runtime as part of the system. To illustrate the proposal,
    a system for preventing the spread of COVID-19 in nursing homes is described that
    was developed using the proposed guidelines It is also illustrated by a description
    of how the data fusion tasks it supports are evaluated by the proposed evaluation
    process. The overall evaluation of the data fusion performed by this system was
    considered satisfactory, which indicates that the proposal facilitates the design
    and development of data fusion systems and helps to achieve the necessary quality
    requirements.'
  doi: 10.1016/j.inffus.2023.102139
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Background 3. Related
    work 4. Data fabric and digital twins for an enhanced data fusion holistic framework
    5. Case study 6. Conclusions and future work Authors statement CRediT authorship
    contribution statement Declaration of Competing Interest Acknowledgements Data
    availability Bibliography Show full outline Cited by (1) Figures (10) Show 4 more
    figures Tables (8) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show all tables
    Information Fusion Volume 103, March 2024, 102139 Data fabric and digital twins:
    An integrated approach for data fusion design and evaluation of pervasive systems
    Author links open overlay panel Aurora Macías a, David Muñoz a, Elena Navarro
    a b, Pascual González a b Show more Share Cite https://doi.org/10.1016/j.inffus.2023.102139
    Get rights and content Under a Creative Commons license open access Highlights
    • Data diversity and heterogeneity may hinder the development of healthcare systems.
    • Data fabric is a context-independent option to improve the data lifecycle management.
    • Digital Twins solves the contextualization problems shown by data fabrics options.
    • The evaluation via a healthcare system shows the proposal is reliable and efficient.
    Abstract Recent technological improvements have made it possible for pervasive
    computing intelligent environments, augmented by sensors and actuators, to offer
    services that support society''s aims for a wide variety of applications. This
    requires the fusion of data gathered from multiple sensors to convert them into
    information to obtain valuable knowledge. Poor implementation of data fusion hinders
    the appropriate actions from being taken and offering the appropriate support
    to users and environment needs, particularly relevant in the healthcare domain.
    Data fusion poses challenges that are mainly related to the quality of the data
    or data sources, the definition of a data fusion process and evaluating the data
    fusion carried out. There is also a lack of holistic engineering frameworks to
    address these challenges. These frameworks should be able to support automated
    methods of extracting knowledge from information, selecting algorithms and techniques,
    assessing information and evaluating information fusion systems in an automatic
    and standardized manner. This work proposes a holistic framework to improve data
    fusion in pervasive systems, addressing the issues identified by means of two
    processes: the first of which guides the design of the system architecture and
    focuses on data management. It is based on a previous proposal that integrated
    aspects of Data Fabric and Digital Twins to solve data management and data contextualization
    and representation issues, respectively. The extension of the previous proposal
    presented here was mainly defined by integrating aspects and techniques from different
    well-known multi-sensor data fusion models. The previous proposal identified high-level
    data processing activities and was intended to facilitate their traceability to
    components in the system architecture. However, the previously defined stages
    are not completely adequate in a data fusion process and the data processing tasks
    to be performed in each stage are not described in detail, especially in the data
    fusion stages. The second process of the framework deals with evaluating data
    fusion systems and is based on international standards to ensure the quality of
    the data fusion tasks performed by such systems. This process also offers guidelines
    for designing the architecture of an evaluation subsystem to automatically perform
    data fusion evaluation in runtime as part of the system. To illustrate the proposal,
    a system for preventing the spread of COVID-19 in nursing homes is described that
    was developed using the proposed guidelines It is also illustrated by a description
    of how the data fusion tasks it supports are evaluated by the proposed evaluation
    process. The overall evaluation of the data fusion performed by this system was
    considered satisfactory, which indicates that the proposal facilitates the design
    and development of data fusion systems and helps to achieve the necessary quality
    requirements. Previous article in issue Next article in issue Keywords Data fabricData
    management lifecycleDigital twinsE-healthMultisensory environmentsPervasive computing
    1. Introduction Maximizing the benefits of any new initiative for society from
    social, economic, and territorial perspectives is greatly promoted by public institutions,
    such as the European Union. These initiatives include generating new knowledge
    and developing innovative solutions to promote eco-friendly production and consumption
    or to prevent, diagnose, monitor, treat and cure diseases [1]. Some computing
    paradigms such as ubiquitous [2] or pervasive computing [3] have discussed how
    information technology could be woven into everyday objects and settings under
    the Disappearing Computing vision [4] to offer new ways of supporting and enhancing
    people''s lives beyond the traditional support offered by conventional desktop
    computers [1]. Pervasive computing involves intelligent environments augmented
    by mobile, wearable, and environmental sensors in a diversity of computing devices
    that collect data from multiple sources. These environments are also populated
    with actuators that can unobtrusively react to the user''s needs and the environment,
    providing valuable computing services for a wide variety of applications [5].
    The service described requires the processing of data gathered from multiple sensors
    to turn these data into information and combining them to obtain valuable knowledge.
    This knowledge can be used to assess, for example, the activity being conducted
    by the user, the status of the environment in which it takes place, also the users’
    behaviour and/or physiological or even emotional state to appropriately respond
    to their needs [1]. As Ackoff [6] and Chen et al. [7] claim, the term data refers
    to symbols that represent the properties of objects and events, whereas information
    is data that have been processed to increase their usefulness. The difference
    between data and information is functional rather than structural, and both terms
    are frequently used interchangeably. Knowledge and understanding refer to the
    use of data and information to answer questions such as who, what, when, where,
    or how many. Knowledge is conveyed by instructions and answers how-to questions.
    Understanding is conveyed by the explanations and answers why questions. Healthcare
    is crucial in everyday life and concerns medical and public practices that frequently
    involve the use of devices [5]. The population in general and the elderly in particular
    demand multiple healthcare services such as medical examination services offered
    by physicians at clinics or outpatient services offered by specialists, nurses,
    etc. in hospitals, amongst others [8]. The demand for appropriate and efficient
    services is growing ever more rapidly [8] especially as a result of the COVID-19
    pandemic, which has caused millions of infections and deaths, especially amongst
    the elderly [9]. Medical models are evolving from evidence-based medicine to precision
    medicine focused on medication tuned to the person needs for the prevention of
    COVID-19 and other illnesses [8]. The advances in different research areas and
    technologies, such as computing hardware, the Internet of Things (IoT), Artificial
    Intelligence techniques, Big Data, communication technologies, cloud computing,
    and system architectures have greatly contributed to making the pervasive vision
    a reality and even to providing additional facilities such as those related to
    user mobility [5,1]. Technologies such as mobile internet [8], smartphones or
    smartwatches and their inbuilt sensing devices, like heart rate sensors, provide
    more opportunities to adopt technology in pervasive healthcare applications [5],
    while the availability of massive data sources and datasets have also helped to
    take advantage of the information generated and its fusion. The technique for
    handling multiple data sources is known as data or information fusion [5] and
    refers to the process of combining different data sources for using the combined
    information in a particular application [10]. Almost every information system
    now incorporates some level of data fusion as it can extract knowledge from raw
    data and improve data output quality [5]. This explains why it has been extensively
    used in pervasive healthcare applications [8,[11], [12], [13], [14]]. However,
    poor implementation of data fusion hinders many other key data processes, including
    data analytics, for taking appropriate actions and offering an appropriate support
    to users and the needs of the environment. Despite this, data fusion is often
    taken for granted and its implementation is delegated to low-level programmers
    or database administrators [10]. Several issues make data fusion a challenging
    task. Most of its unsolved issues and challenges are related to data or data source
    quality, the data fusion process itself and its evaluation [15]. Amongst the different
    aspects of data fusion quality, accuracy is the most difficult to address [10].
    Information systems in general have evolved and improved their efficiency to collect,
    process, store, analyse, and disseminate the enormous amount of heterogeneous
    data generated thanks to the adoption of approaches such as data fabric architectures
    [16]. Bearing in mind the recent shift from data-centric to data-driven approaches,
    the system architecture acquires a key role in addressing the above-mentioned
    challenges. However, designing a unique and generalized engineering framework
    has become a very complex task because of the constantly emerging problems regarding
    the context, quantity and quality of data, information, and knowledge. Different
    issues have also been identified that may impact the development of such frameworks,
    such as automated methods of extracting meaning from information; the selection
    of algorithms and techniques; the assessment of information and the evaluation
    of information fusion systems in an automatic and standardized manner, amongst
    others [15]. In this context, the aim of this work was therefore to develop a
    holistic framework to improve data fusion in pervasive systems to address the
    issues identified. The key contribution of this work is mainly the holistic data
    fusion framework with which to design the system architecture, the data fusion
    approach and the data fusion evaluation as part of the system under development.
    This framework includes two dimensions: • a process for guiding the design of
    the system architecture, known as EX-DLC, focusing on data management. This has
    been defined as an extension of the process described in [17] that integrates
    aspects of data fabric architectures [18] to deal with the management of heterogeneous
    data collected from different channels and sources. It also integrates aspects
    of Digital Twins (DT) [19] as the core to tackle the relevant challenge of information
    representation. EX-DLC now enhances that proposal, including specific aspects
    and techniques, from the framework described by Kokar et al. [28] and the process
    known as the Entity Based Data Fusion (EBDF) process [10]. We also analyse the
    way in which some of the properties associated with DT defined by Minerva et al.
    [19] are exploited by the data management process to facilitate the representativeness
    of the information and conduct specific fusion tasks. • a process for the evaluation
    of data fusion systems based on ISO/IEC 25040 international standards [20], ISO/IEC
    25012 [21], and ISO/IEC 25024 [61]. This process can be applied systematically
    and comprehensively to reduce errors and consequently the risks involved in the
    data fusion process, and to ensure the quality of the data fusion tasks performed
    by the system [10]. It also considers guidelines for designing the architecture
    of the evaluation subsystems that perform the evaluation automatically in runtime
    as part of the system at hand. The work also describes a case study related to
    the healthcare domain to show the applicability of the proposal. This domain was
    selected due to its relevance to society and because healthcare applications are
    considered high-risk applications in which the accuracy of the fusion must be
    very high [10]. A pervasive healthcare data fusion system, intended mainly to
    monitor and prevent the propagation of COVID-19 in nursing homes, was designed
    and implemented applying the proposed framework. Although a smart and pervasive
    healthcare environment could become really complex, a basic prototype is presented
    here for the sake of simplicity. In this prototype, data is collected from multiple
    sensors to monitor different entities in the real world, i.e. users, staff, and
    nursing facilities. Actions are taken based on the inferences obtained from the
    modelled input data. The standardized evaluation of the accuracy and completeness
    of the data fusion tasks performed by the system is carried out automatically
    in runtime by specific evaluation components integrated in the data fusion system
    as part of an additional subsystem. The document is structured as follows. Section
    2 summarizes some of the background regarding the basic principles of Data Fabric
    architectures and introduces DT and its main characteristics. Section 3 includes
    the most relevant related work on data fusion and the identified gaps in the existing
    literature. In Section 4, the different dimensions of the data fusion framework
    proposed is described: a data management process and a fusion evaluation process.
    Section 5 describes a case study as an example of how the proposal has been used
    to develop a system, describing its main features and the deployment of its data
    management and data fusion evaluation subsystems on a cloud platform. The specification
    and design of the evaluation plan for the system developed are explained in Section
    5.3, along with the automatic execution of the evaluation in runtime and the satisfactory
    results and conclusions. Finally, Section 6 details the conclusions drawn and
    our future work. 2. Background The evolution of healthcare services is generating
    a huge amount of data in smart and pervasive healthcare ecosystems from different
    sources and subsystems, such as real-time generated device data, electronic information
    systems, etc. [8]. To be useful, these data and information need to be fused and
    contextualized. The resulting new knowledge should have the appropriate format
    and quality to be analysed and used to obtain conclusions and improve or save
    people''s lives. However, failing to fuse the patients’ information can have severe,
    even life-threatening, implications and explains why healthcare data fusion applications
    are considered high-risk applications in which fusion accuracy must be very high
    [10]. Processing all the data required by healthcare applications usually involves
    the tasks outlined in Fig. 1, including: data collection and transmission; data
    integration encompassing data cleaning and fusion; and data storage and analysis
    [8]. This type of process combining different data integration and storage techniques
    without restriction to specific architecture archetypes led to the concept of
    Data Fabric [18] that offers important advantages for data processing. However,
    data fabric architectures that use data lakes show certain issues regarding the
    contextualization, representation, and modelling of the data, which, together,
    are also an important data fusion requirement. Digital Twins (DT) can address
    these challenges by unifying and contextualizing the data obtained from physical
    space by creating virtual models of the related entities. Download : Download
    high-res image (429KB) Download : Download full-size image Fig. 1. Cloud healthcare
    system framework (CloudDTH) main data processing tasks (extracted from [8]). Data
    fabric and DT can be used to address certain data fusion issues, although other
    open issues also still need to be tackled. The lack of a holistic framework that
    clearly identifies a data fusion model, the appropriate data fusion techniques
    to adopt and a systematic and standardized data fusion evaluation process are
    especially important as part of the holistic framework that can be automatically
    carried out in run-time as an integral part of the data fusion system. In the
    following Sections, the basic aspects of Data Fabric and Digital Twins are described,
    as both approaches are the main foundations of this proposal. Section 3 then provides
    a summary of the open issues that led to our proposal. 2.1. Data fabric According
    to Gartner [16], modern data management systems must be able to collect, integrate
    and deliver data to and from multiple sources and locations for a range of use
    cases. Data Fabric architecture can both integrate data and guide in what to add
    and improve. It can also address the extreme levels of diversity, distribution,
    scale and complexity in data assets that add tremendous complexity to the overall
    data integration and data management design. It is expected that by 2024 Data
    Fabric-based deployments will greatly improve the efficiency of data use while
    considerably reducing human-driven data management tasks. Data Fabric [18] can
    be seen as a design concept for attaining reusable and augmented data integration
    services, called data pipelines [37], and semantics for flexible and integrated
    data delivery. It refers to a unified data management architecture and the set
    of capabilities that provide consistent functionalities to conveniently connect
    data endpoints and enhance end-to-end data management activities. It does not
    promote specific pipeline steps or implementation paradigms [18], although it
    usually employs data lakes to store data without modifying them or having to first
    structure them, to solve certain Big-Data challenges, such as those related to
    data silos [38], amongst others. Data Fabric has been used as an effective method
    of data management in several fields, including healthcare and systems of different
    types on different cloud computing platforms [35,39]. As discussed in Section
    3, data fusion processes may be used in certain Data Fabric steps to extract higher-level
    information and to improve the reliability and reduce the uncertainty of the data
    or information obtained from multiple sources to improve its quality. However,
    data lakes by themselves cannot provide the extra content, such as structured
    information and new insights, i.e. the context required to turn data into the
    meaningful information needed to improve business operations [40]. To solve some
    of the problems aforementioned, Digital Twins (DT) is presented as an innovative
    solution because DT modelling capability can be seen as a form of data fusion.
    Some examples of Data Fabric architectures that integrate Digital Twins can be
    found in [41,8], while examples of adopting DT in the healthcare domain can be
    found in [8,42,42]. Although these works include descriptions of the system architecture,
    they do not provide a framework with the rationale for guiding its design. 2.2.
    Digital twins A Digital Twin (DT) [19] can be seen as a complete software representation
    of an entity (object, process, or person) whose main components are physical space,
    digital or virtual space, and information processing that connects them bidirectionally
    (see Fig. 2). DT facilitates the convergence of physical and the virtual spaces.
    A DT must be able to simulate, monitor, regulate and control the state of the
    physical space and its processes [43]. It emphasizes and contextualizes the received
    data by creating real-time virtual representations of physical systems, i.e. high-fidelity
    models [43] that are multi-physical and multi-scale [8]. DT thus combine data
    from multiple sources in a single place and unifies and contextualize such data
    [40]. Large cross-domain applications could benefit from DT capabilities for accurately
    representing a large aggregation of objects and transforming them into services
    [19]. Download : Download high-res image (169KB) Download : Download full-size
    image Fig. 2. Digital Twins: physical entity and virtual entities, and some common
    properties defined by Minerva et al. [19] (extracted from [44]). Minerva et al.
    [19] define the different properties that a DT must satisfy, some of which can
    be used to support certain data processing capabilities, as described below: •
    Representativeness and Contextualization: virtual models of entities will mimic
    the status and features of the real-world entities that are relevant to the context
    of use. Such status and features will depend on the specific location and moment
    in time of the entities in the real world. • Reflection: real-world entities will
    be timely and univocally represented using the values of the attributes, status
    and even behaviour of its virtual models. • Composability: virtual models of entities
    can be grouped into a composed model according to the relations of the composition
    amongst the corresponding entities in the real world and whether grouping these
    virtual models offers advantages to satisfy the requirements of the use case.
    • Entanglement: virtual models of entities must receive in real-time, or close
    to real-time, the information that represents the entities in the real world to
    make this information available to other applications and services. It should
    be mentioned that this entanglement should ideally be strong, i.e. the communication
    between entities and virtual models should be bidirectional and in real-time.
    • Memorization: since DTs encompass real-world entities and their virtual models,
    meaningful past and present data about such entities, as well as the context of
    when and where such data originated, will be stored for their later analysis.
    • Predictability: a virtual model of a real-world entity may be used to simulate
    its behaviour and interaction with other entities to determine the outcomes in
    a likely future or context. DT''s modelling capability resembles certain aspects
    discussed in Section 3, such as entity-based data fusion, feature-level data fusion
    or the multi-modelling approach, and thus can be exploited for fusing data to
    extract higher level information. Since the final purpose of data fusion is to
    obtain high-level information to obtain better conclusions, data fusion performed
    by DT enables the modelling and representation of high-fidelity physical entities
    to achieve this goal. DT also pose certain challenges that should be considered.
    The most important, from the data fusion point of view, is the need to improve
    modelling accuracy in a standardized manner [45]. Moreover, data are generally
    not fully trustworthy, and the data required for modelling are not always available
    or not necessarily available at the right time [19], being necessary to generate
    synthetic data through simulation [46]. Another important challenge is related
    to the DT architecture and its components: the modularization design principle
    needs to be explored to improve modelling efficiency [47]. 3. Related work Data
    fusion is a multidisciplinary research area borrowing ideas from many diverse
    fields, such as signal processing, information theory, statistical estimation
    and inference, and artificial intelligence [22]. Many definitions for data fusion,
    also called information fusion or data integration, exist in the literature [4,22,13,[23],
    [24], [25]]. Data fusion can be defined as “the study of efficient methods for
    automatically or semi-automatically transforming information from different sources
    and different points in time into a representation that provides effective support
    for human or automated decision making” [23,24]. Data fusion extracts knowledge
    from raw data and improves data output quality [5]. It could also encompass the
    amalgamation of (i) original data to produce an output, and (ii) both original
    and processed data to construct the more useful outputs required for decision-making
    procedures and control-related activities. Data fusion can also be used to perform
    more complex feature-based and decision-based amalgamations of different types
    of input data to produce a numerical output, a feature output, or a higher-level
    decision [25]. This explains why some authors use the term data fusion when data
    are obtained directly from sensors without any processing and use the term information
    fusion when data are processed [15]. Data fusion offers meaningful advantages
    in handling heterogeneous data and information [15] to achieve the following goals:
    (i) improve the reliability (accuracy and completeness) of the data or information
    [15,5], (ii) reduce uncertainty, (iii) increase information quality [15,25] when
    information is obtained from multiple sources [15], and (iv) extracting higher
    level information [5]. In the following, the work related to data fusion is further
    analysed, focusing on classification frameworks, techniques and methods, data
    fusion evaluation and the main data fusion challenges are summarized. 3.1. Data
    fusion classification and models Different surveys, such as [22,15], analyse and
    classify existing data fusion reference models and frameworks. Both surveys identify
    the JDL model [26] as one of the most widely used data fusion models that focuses
    on the abstraction level of the data manipulated rather than on their processing,
    although it is somewhat restrictive and especially tuned to military applications.
    A recent literature review [27] analyses the existing data fusion models for multi-sensor
    fusion, i.e. when different channels are used to facilitate multi-modality and
    multi-location. This review considers three different categories, Data-level fusion,
    Feature-level fusion and Decision-level fusion, as depicted in Fig. 3. The first
    focuses on fusing information from different homogeneous sources to improve the
    quality of the output. When heterogenous sources are to be fused, Feature-level
    fusion or Decision level fusion are used. The former creates a higher dimensional
    feature vector from the different sources, i.e. then used for classification or
    pattern recognition. The latter fuses data from heterogeneous sensors to make
    a final decision for the input for the next steps in the system. A well-known
    framework for multi-sensor fusion was proposed presented by Kokar et al. [28],
    which was defined to support all types of multi-source fusion, including data
    fusion, feature fusion, decision fusion, and fusion of relational information
    with a measurable and verifiable performance. Download : Download high-res image
    (330KB) Download : Download full-size image Fig. 3. Core categories of most common
    abstraction level-based (low-level) information fusion models (adapted from [22]).
    Khaleghi et al. [22] highlight the fact that research on high-level fusion tasks
    and methodologies are attracting the attention of the community, as the requirements
    for low-level fusion are already satisfied by the existing proposals. The Entity
    Based Data Fusion (EBDF) process [10] is a proposal for high-level multi-sensor
    fusion that integrates information on the same real-world entity from different
    sources when high accuracy is required. As can be seen in Fig. 4 (left), EBDF
    entails two steps, entity resolution (ER), to determine whether two records are
    referencing the same entity or different entities, and data rationalization (DR)
    to extract and reconcile conflicting or incomplete information being prepared
    for creating the final information product related to an entity. An information
    product is a set of data or a piece of information that represents an entity.
    Regarding the first step, even using a highly parallel and high-speed processing
    system, the ER process could take a long time to execute when many data comparisons
    are needed, so that ER is usually performed on subsets of the source data called
    blocks. The process for creating these blocks is called blocking. The ER process
    only compares entity references within the same block to reduce the number of
    comparisons; the smaller the blocks the faster the ER process will execute. This
    blocking process is usually executed before the ER process and can be considered
    as another step. DR may be both the final step of the production of the information
    product and a precursor step for further processing. Healthcare is one of the
    domains in which this proposal is clearly an advantage in avoiding life-threatening
    situations, such as fusing information from different patients. Download : Download
    high-res image (214KB) Download : Download full-size image Fig. 4. (Left) Two
    steps of the Entity-Based Data Fusion (EBDF) process: entity resolution (ER) and
    data rationalization (DR). (Right) Points in the EBDF process where errors may
    happen (adapted from [10]). S: Source data; B: Block, a subset of source data
    used to accelerate the ER subprocess execution. EBDF applies data fusion techniques
    at different points of the data cycle to create the final information product
    that represents an entity. It also identifies errors that may arise from the data
    fusion process and describes alternatives for addressing them. It can be considered
    a fair and comprehensive data fusion framework since it entails i) a data fusion
    process, the EBDF itself, as well as ii) a data quality management process (DQMP)
    focused on data accuracy, which is the most difficult data quality dimension to
    achieve. Despite being a powerful proposal, it still has certain weaknesses that
    may hinder its use, the most important being (a) partial satisfaction of data
    fusion aspects and requirements, since EBDF only considers two fusion steps apart
    from the optional blocking, and (b) the assessment of a single quality criteria,
    accuracy, as part of (c) an evaluation process that is not sufficiently detailed.
    The aforementioned frameworks and proposals focus on multi-sensor fusion. However,
    as indicated in [25], fusion may also be conducted by exploiting other sources
    such as forecasting or simulation, or even using other models that have different
    structures or inputs. The main reason behind these approaches is that no single
    model can perfectly satisfy all the demands of all the stages, phases and mechanisms
    of the use case being developed. It is therefore claimed in [25] that a multi-model
    approach is the only way to develop a final product that comprehensively and accurately
    supports the information representation and meets the expected requirements, removing
    or mitigating the errors caused by a single-model approach. Although there is
    a wide variety of data fusion models to build data fusion systems, general data
    fusion frameworks, functional models and methodologies must still be developed
    and/or enhanced [15]. 3.2. Data fusion techniques and methods According to Becerra
    et al. [15], once the architecture of a data fusion system has been designed,
    the appropriate data fusion techniques and methods to be used must be selected
    according to the task to be done and the system to be developed. Different publications,
    such as [5,15,25,27,10], identify the different data fusion techniques and methods
    that can be used. Abrahart and See [25] state that the most popular techniques
    for data fusion include low-level signal processing, traditional statistical methodologies,
    decision-making techniques and artificial intelligence technologies. Lau et al.
    [5] describe two groups of data fusion techniques, according to the different
    information enrichment obtained after data fusion. These groups are as follows:
    • common data fusion techniques such as data association, state estimation, and
    decision fusion. These techniques are used to fuse lower-level information to
    generate identical level of information. • techniques associated with data mining
    such as classification, prediction or regression, unsupervised Machine Learning,
    dimension reduction, statistical inference and analytics, and visualization. These
    techniques are used when simple input data from multiple sources are fused to
    generate higher level information enrichment. Qiu et al. [27] identify similar
    techniques but the authors group them according to the abstraction level as Data-level
    fusion, feature-level and decision-level fusion techniques. Qiu et al. [27] also
    state that different data fusion techniques are used for fusing features extracted
    from homogeneous and heterogeneous sensors, depending on the purpose of the fusion:
    (i) to reduce errors using methods based on probability and statistics;(ii) combine
    data to transform them into higher level data via machine learning algorithms;
    and (iii) solve space and time-dependant problems using automatic feature representation
    based on deep learning. Becerra et al. [15] state that data fusion techniques
    and methods can be grouped according to the level of the process they are applied
    to as: (i) sensor level, (ii) feature level, and (iii) score level. They also
    claim that these techniques and algorithms can be grouped according to the specific
    task to be performed, as (i) association, (ii) state estimation, and (iii) decision
    fusion. These techniques are used mainly in tracking processes. Talbur et al.
    [10] consider that most data fusion systems currently use one or two types of
    the following matching strategies: (i) deterministic matching based on “if-then”
    logic with Boolean operators “and/or” to make their decisions; (ii) probabilistic
    matching based on a model implementing a scoring method; or (iii) matching based
    on machine learning. The earliest data fusion systems used Boolean rules because
    they are easy to construct, explain and maintain, e.g. alignment is fairly straightforward
    when using “if-then” style rules. However, the Boolean rule strategy operates
    at the attribute level of the data, which can be considered its major weakness.
    As can be seen, there is a plethora of different data fusion alternatives, although
    it is still necessary to develop a holistic framework that guides the development
    of fusion systems. Data fusion implementation should attempt to capture all the
    meaningful aspects of the input data to be fused and consider the uncertain relationship
    between this data and the expected result of the data fusion process [25]. These
    aspects should be considered when selecting the techniques to adopt for every
    data fusion stage in the data fusion process. 3.3. Data fusion evaluation Like
    any other software system, the assessment of data fusion systems, closely related
    to the Information Quality (IQ) assessment [22], is based on certain quality criteria
    that they should satisfy. These criteria are selected according to the application
    domain and measured using specific metrics [15]. Different classifications of
    IQ criteria have been defined, the best known being the ISO/IEC 25012 standard
    [21], which defines the quality criteria that may be used to assess and improve
    these systems. The need to properly identify and assess these IQ criteria has
    led to some general data fusion frameworks to integrate them as an integral part
    of their definition to enhance their outcomes [15]. Evaluating the performance
    of the data fusion along with the reliability is considered one the biggest problems
    for data fusion systems [15], so that IQ has been introduced in the field of information
    fusion systems. Unfortunately, IQ has not been widely explored in the data fusion
    process due to the complexity of its assessment and is one of the most complex
    stages in information processing. As there are no standard and uniform metrics
    or comprehensive evaluation methods for information fusion systems, developing
    an IQ assessment for data fusion systems is a new and highly complex task. Despite
    the relevance of evaluation for data fusion systems, there are a limited number
    of proposals in this area. Some efforts have been made to evaluate the quality
    of the information generated by data fusion systems, such as [28,25,32]. However,
    most of these works simply identify certain criteria, metrics and measures without
    offering many details or precisely describing the tasks to be carried out, which
    may hinder their systematic use. Moreover, most evaluations are conducted considering
    information systems as black boxes, which turns the description of the quality
    assessment into a complex and difficult task. Some efforts have been made to improve
    the definitions of frameworks and methodologies for evaluating the quality of
    the information generated by data fusion systems. For instance, a set of guidelines
    has been collected in [29], such as the use of standard test collections and results,
    the exploitation of standard evaluation metrics, comparison with other fusion
    algorithms, etc. It is claimed that these guidelines should be considered for
    defining these frameworks to avoid the usual drawbacks of existing strategies,
    and despite these efforts, the existing frameworks still unfortunately have the
    aforementioned limitations. As the performance of a data fusion system depends
    mainly on two aspects (i) the quality of the input data, and (ii) the quality
    of the fusion method, its evaluation must also focus on these two aspects. Both
    must be jointly considered because a good IQ of the input data does not necessarily
    ensure the high fusion process IQ [15]. The quality of the data fusion methods
    is usually evaluated in terms of accuracy [30,31], data fusion performance [32,33],
    effectiveness [34,31], consistency [32], data fusion efficiency [34], and usefulness
    [31]. Accuracy is the most frequently evaluated quality attribute [15] and the
    most difficult to assess [6] because it can affect not only the source information
    but also the information fused. Different approaches, such as different statistical
    methods or techniques based on comparisons with different reference datasets have
    been exploited for its evaluation. It should be highlighted that accuracy is paramount
    in high-risk systems such as pervasive healthcare systems [5,10], which explains
    why it attracts the community''s attention. The EBDF proposal [10] is a framework
    that offers a Data Quality Management Process (DQMP) to reduce errors and also
    the risks associated with data fusion. DQMP considers the verification of accuracy,
    i) by the original source of information, which is often impractical because of
    the time and expense required, and ii) by comparing the information collected
    to a reference source known to be accurate, which may not exist or may not be
    ideal for the domain of interest. The paper claims that, due to the difficulties
    and problems related to verification, most data fusion processes normally rely
    on validation, which refers to the use of rules, usually Boolean, to check for
    outlier values or unexpected conditions in the source information. The failure
    of a validation rule indicates that the source information is inaccurate. Rules
    may test the quality of the values of individual fields for completeness, conformity
    to a required format, timeliness and other relevant data quality dimensions. Other
    rules may test the relationships between data items and, in relational models,
    to determine whether the relationships are present and correct. However, it should
    be noted that a source information passing the complete set of validation rules
    is not necessarily accurate, so that validation rules are usually considered as
    soft [10] because they impose less strict conditions than the original data fusion
    rules, i.e. a near match or partial match. Some evaluation processes for the assessment
    of IQ guided by frameworks and methodologies, for instance the one defined in
    [32], rely on the use of evaluation software tools and testbeds [22,36]. For example,
    the Boeing Fusion Performance Analysis (FPA) tool [35] is a software that enables
    the computation of technical performance measures (TPM). It is typically executed
    manually to carry out a post-run analysis of the input data, provided as a set
    of files. Although it is claimed that FPA is platform-independent and can be used
    for any fusion system, it is actually configured and used to evaluate the data
    fusion of the Boeing tracking system. The multisensor-multitarget tracking testbed
    [36] is another tool for the evaluation of large-scale distributed fusion systems
    for the surveillance of moving objects that is capable of handling multiple, heterogeneous
    sensors in a hierarchical architecture. The testbed is a scenario generator that
    can generate simulated data for multiple sensors as well as a tracker framework
    that integrates different tracking algorithms. Once the tracking systems are connected
    to the testbed, this automatically generates the different scenarios and evaluates
    the accuracy, completeness, timeliness, ambiguity, and continuity in real-time
    mode. This tool offers an interface to human operators to establish certain manual
    configurations. The main disadvantages of these tools are that both rely to a
    considerable extent on human intervention and cannot be used to assess the data
    fusion quality of applications in other domains. Several publications explicitly
    identify the different issues and challenges related to evaluating data fusion
    and how to properly measure an information systems’ performance. For example,
    in [22] it is stated that there is no standard and well-established evaluation
    framework to assess the performance of data fusion algorithms. They also argue
    that data fusion performance evaluation is frequently done in simulated or unrealistic
    test environments substantially different from real-use cases and are frequently
    based on idealized assumption(s), which makes it difficult to predict how a proposal
    would perform in real-life applications. Some of the major, and usually ignored,
    problems of fusion evaluation identified in [22] are, (i) that existing proposals
    do not use a ground truth, even though many performance measures require this
    knowledge, (ii) performance has different, possibly conflicting, dimensions that
    are difficult to capture in one comprehensive and unified measure, (iii) performance
    measures might need to be adapted over time or according to the given context,
    further standardized measures of performance applicable to the practical evaluation
    of data fusion systems being necessary. Some other open issues regarding IQ monitoring
    and controlling are the following [15]: (i) the need for an ontology of IQ criteria,
    (ii) the definition a unified quality metric based on IQ criteria, (iii) the definition
    of proposals for compensating a low IQ and evaluating usability and IQ quality,
    (iv) the identification of the effect of subjectivity on IQ and dependencies on
    the context, (v) the quality assessment of the system changes, (vi) the adequate
    selection of quality criteria, and (vii) the traceability of IQ assessment throughout
    the process stages. 3.4. Data fusion issues and challenges As Becerra et al. have
    stated [15], information systems have evolved and improved their efficiency to
    collect, process, store, analyse, and disseminate the enormous amount of heterogeneous
    data generated. Nevertheless, designing a unique and generalized engineering framework
    for these systems has become a very complex task because of the constantly emerging
    problems regarding the context, quantity and quality of data, information, and
    knowledge. Most of the unsolved issues and challenges in data fusion are related
    to the quality of the data or data sources, the data fusion process and evaluating
    the data fusion. The following data challenges are identified in [5]: • Data quality.
    Data source quality directly determines the quality of the fusion output. The
    specific data quality aspects to improve are data sensing coverage and data sensing
    longevity. Insufficient data coverage generates an unrepresentative result and
    often implies that more sensors need to be installed to expand the sensing coverage.
    Data sensing longevity refers to long-term data collection, which depends on the
    capacity of the physical sensors to run independently without the need for external
    power sources. • Data representation. A wide diversity of data sources frequently
    results in the incompatibility of data formats, especially when there is no standard.
    • Data fusion techniques. Extracting knowledge frequently involves the use of
    data mining techniques to fuse different data sources. Low level data fusion techniques
    have been well explored but current research focuses on machine learning due to
    its ability to handle high-dimensional data. As described in [22], most data fusion
    issues arise from the data to be fused as they may have outliers, spurious, conflicting
    and associated data. Other issues arise from the specific nature of the application''s
    environment. It is also claimed that the input data to the fusion system may be
    imperfect, correlated, inconsistent and/or be in disparate forms. This work also
    emphasizes that inherent data imperfection is the most challenging problem of
    data fusion systems and suggest the following strategies to address it: • exploit
    redundant data to alleviate effects such as imprecision and noise in the measurements
    and ambiguities and inconsistencies present in the environment. • treat highly
    conflicting data carefully to avoid producing counter-intuitive results. Scalability
    challenges are another issue related to data sources when exploiting wireless
    sensor networks. It must first be decided how processing should be conducted,
    since a centralized approach implies a large communication burden. While a decentralized
    approach alleviates this problem as each sensor processes the collected data locally,
    it also provides additional complexity for the management and development of distributed
    solutions. Wireless sensor networks may also cause potential collisions and transmissions
    of redundant data. As indicated in [22], many of these problems have been identified
    and investigated but no single data fusion approach has addressed all of them.
    To sum up, a process for evaluating the quality of the information generated by
    data fusion systems should satisfy the following aspects as described in Table
    1: Table 1. Evaluation aspects to be satisfied by a quality evaluating process
    that evaluates the quality of the information generated by data fusion systems.
    Identifier Evaluation aspect EA1 Define quality in a manner that all stakeholders,
    and especially users, can understand. EA2 Offer enough details of the different
    activities and carefully describe the tasks required to allow its systematic use.
    EA3 Allow the identification of certain standardized information quality criteria,
    metrics and measures that should be selected depending on the application domains.
    EA4 Include accuracy assessment as mandatory, since this quality criterium is
    paramount in data fusion systems, especially in high-risk systems such as data
    fusion systems in the healthcare domain. and is also the most difficult quality
    attribute to evaluate. EA5 Cover the evaluation of both (i) the quality of the
    input data, and (ii) quality of the fusion method, since good input data information
    quality does not necessarily ensure good information quality of the fusion process.
    EA6 Support the evaluation using different verification possibilities (such as
    comparison with the original information source or with a reference source known
    to be accurate) and validation (e.g. Boolean rules, statistical methods, machine
    learning and AI algorithms) according to requirements and restrictions. EA7 Support
    data fusion evaluation automatically on runtime. EA8 Support data fusion evaluation
    as part of the actual system, not in a separate testing environment. 4. Data fabric
    and digital twins for an enhanced data fusion holistic framework As mentioned
    in Section 3, a unique and holistic engineering framework is required for information
    fusion systems. The holistic framework should ideally encompass two main dimensions:
    (i) a data management process able to satisfy all the requirements of the stages
    and mechanisms of the pervasive computing system being developed, and (ii) a quality
    evaluation process of data fusion that considers the quality criteria and metrics
    selected for the system under development. In other words, the data management
    process should support a comprehensive and accurate representation of the information
    and offer a set of appropriate data fusion techniques which are selectable after
    designing the system architecture according to the tasks to be performed. The
    quality evaluation process should carefully describe the tasks to be done to facilitate
    its systematic use and be useful to detect and reduce errors and so also the risks
    associated with the data fusion stages, which was our aim in the present work.
    It should be noted that each dimension of this Enhanced X-DLC (EX-DLC) framework
    supports the characteristics identified in Section 3, especially those that required
    the use of Digital Twins (DT). The DT properties defined by Minerva et al. [19]
    used by EX-DLC, i.e. representativeness, contextualization, reflection, composability,
    entanglement, memorization, and predictability, will also be emphasized in the
    following sections. The proposed holistic framework integrates and adapts several
    appropriate aspects, including some from other proposals, to respond to the issues
    identified. The differences between our proposal and others are summarized in
    Table 2 in terms of the stages of their underlying data management cycle. Table
    2. Comparative amongst stages of studied proposals, X-DLC and EX-DLC. Proposals
    → CloudDTH Data Process D-CPSS Data Pipeline Model Context Toolkit MAPE-K Google
    Cloud Data Lifecycle Microsoft IoT Data Lifecycle AWS Data Lifecycle Management
    X-DLC EX-DLC Stages ↓ Evaluation planning ✓ Collection ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Ingestion
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Raw data evaluation ✓ Raw data storage ✓ ✓ ✓ ✓ ✓ ✓ (optional)
    ✓ Pre-processing ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Pre-processed data evaluation ✓ Pre-processed
    data storage ✓ ✓ ✓ Modelling / representation / simulation ✓ ✓ ✓ ✓ ✓ ✓ ✓ Modelled
    data evaluation ✓ Model history ✓ ✓ ✓ ✓ ✓ ✓ Analysis ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Inferred
    data evaluation ✓ ✓ Inferences history ✓ ✓ ✓ ✓ ✓ ✓ ✓ Visualization / actions (planning
    and execution) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Archive or retire ✓ ✓ (optional) ✓ (optional)
    4.1. Data management process based on data fabric and digital twins The goal of
    the process described here is to identify the features that need to be supported
    by a pervasive system to obtain data from the physical space, processing it and
    drawing conclusions for the virtual space to act over the physical space. Data
    fusion is required to conduct the data processing. The major data fusion objectives
    of this proposal are related to multi-source information processing, i.e. 1) to
    process the data related to all the attributes required for the fusion, 2) to
    increase data completeness, and 3) to extract higher-level information for making
    better high-level decisions. These decisions are usually related to entities that
    are generally users and other elements in the environment, so that fusing multi-sensor
    data to model and represent physical entities in a computing system can be considered
    paramount for data fusion. The need to represent physical entities also motivated
    the exploitation of Digital Twins. In addition to the data management process
    of healthcare data fusion applications supported by the CloudDTH architecture
    [8](see Section 2), other frameworks can be used to design IoT-based systems,
    such as D-CPSS [50], which guides the design of the system architecture according
    to a data process to facilitate the integration of the large amounts of data to
    be processed by these systems. Both processes decide the stages to be conducted,
    since the data are generated until they are consumed or retired, including one
    or more data fusion stages. Many systems supporting these data processes are deployed
    on cloud computing platforms [8,51] and are also developed using the native services
    offered by the cloud platforms. Public cloud platforms, such as Google Cloud [52],
    AWS [53], and Microsoft Azure [54] offer profuse details of the data processes
    integrated in reference architectures for IoT-based systems that carry out some
    type of data fusion tasks at some point. The challenges related to efficient data
    management are known to be shared with other types of systems with similar characteristics,
    namely intelligent, context-aware, or adaptive systems [55]. For example, the
    Data Pipeline Model [37] incorporates Machine Learning techniques for decision-making.
    Other proposals for context aware systems add extra features to the process, e.g.
    the Context Toolkit [56] parallelizes certain stages of the process. MAPE-K [57],
    defined for self-adaptive systems, makes the data handled by a system available
    due to its shared knowledge base. Table 2 shows the data management stages supported
    by the analysed proposals. A data process called X-DLC, based on the analysis
    of the stages of these proposals was defined in [17] inspired by the Data Fabric
    approach. As can be seen in Table 2, X-DLC includes most of the common stages,
    i.e. collection, ingestion, pre-processing, analysis, and visualization and/or
    actions, as well as others not quite so common but also identified by some of
    the data processes analysed. All the X-DLC stages were defined to be highly cohesive
    and offer a higher degree of Separation of Concerns than other proposals facilitating
    their traceability to the components in the system architecture. However, the
    X-DLC stages are not fully customized to a data fusion process and their data
    processing tasks (especially those for data fusion) are not described in detail.
    These problems led us to define Enhanced X-DLC (EX-DLC), which integrates features
    from other previously proposed data fusion models, including the well-known EBDF
    process and Kokar''s model (see Section 3). Data management stages supported by
    EX-DLC are also identified in Table 2. As EX-DLC must be traced to the architecture
    of the system to be developed, Fig. 5 shows these stages as system components
    and how they run through time. Executing the EX-DLC stages, running from left
    to right, does not require human intervention, as required in the literature on
    data fusion. The stages aligned vertically in Fig. 5 can be run in parallel to
    speed up the process, e.g. the analysis of the received measured values that have
    been fused and modelled along with the already stored historical fused data (Analysis)
    can be run in parallel with the storage of the values received in real-time (Model
    History). Fig. 5 also shows the stages in charge of carrying out certain data
    fusion tasks as well as the DT properties exploited in each stage. Download :
    Download high-res image (373KB) Download : Download full-size image Fig. 5. EX-DLC
    process. Stages exploiting the memorization DT property that do not perform data
    fusion are identified by *. The transmission process, included in the Collection
    stage, may be defined to support data processing for better data security, although
    none of the data fusion proposals or ours explicitly addresses data processing
    in this stage. As security is not the main focus of our work, the transmission
    process data process is not described in the following sections. Any errors related
    to the physical system or sensor malfunctions could affect the quality of the
    collected data and would also require data fusion support. However, the data fusion
    in the physical system is outside the scope of this work, so that few details
    will be provided on this issue. Although the case study described in Section 5
    to illustrate the use of the EX-DLC is related to the healthcare domain, EX-DLC
    can be used in other domains since its definition is based on generic proposals.
    The EX-DLC stages described below also identify the DT properties exploited and
    the data fusion aspects considered. Frequent errors that can arise during data
    fusion are also identified so that they can be properly addressed when they are
    detected during the evaluation. 4.1.1. Collection In this stage, the data related
    to properties of entities relevant to the system are obtained by sensor networks
    in the physical space. Multiple sensors and sources can be used to measure the
    same properties of a particular entity that may be a thing, person, place or process.
    The data collected in this stage are transmitted to the computing system that
    manages the virtual space (cyber-system), in which most of the data fusion processing
    occurs. It should be highlighted that the security of transmitted data should
    be guaranteed, so that a type of data processing such as encryption can be used
    [56,58]. According to [56], another appropriate data transformation or data fusion
    system can also be implemented before the transmission, should it be needed by
    the system under development, e.g. to detect physical errors or sensor malfunction
    and to correct the resulting erroneous information. Data fusion tasks carried
    out at this stage are outside the scope of this work, since the focus is on the
    virtual system and this subject will be addressed in future work. Certain errors
    may occur [10] in relation to the source information. Data quality defects may
    be in either (i) inconsistent formatting, (ii) erroneous information difficult
    to detect and correct, and (iii) challenges resulting from the use of semi-structured
    and unstructured sources, such as social media. These errors can be eliminated
    or reduced during pre-processing stage (see Section 4.1.4). 4.1.2. Ingestion The
    cyber-system receives the entities’ data collected through a component that acts
    as a gateway known as ingestion that routes messages from the devices in the physical
    space to the components implemented in the next stages and can also be used to
    filter and discard messages that include values outside the predefined valid ranges
    supported by the sensors. This prevents the cyber-system from receiving and processing
    erroneous data, e.g. due to sensor malfunctions. However, since this aspect is
    closely related to the physical system, it is outside the scope of the present
    study. Ingestion is partially supported by the entanglement DT property, as it
    is related to the communication between the physical and virtual spaces and the
    reception of data in real-time. This property will be further described in the
    context of the Modelling/Representation/Simulation stage. Ingestion in EX-DLC
    is based on the blocking strategy defined by the EBDF data fusion process [10]
    to facilitate the management of data belonging to the different entities of the
    system under development. When different types of entities are modelled, a different
    block is created for each type, after which filtering-based routing is conducted
    to determine the appropriate block in which to allocate the data received according
    to their entity type. Apart from filtering, normally based on Boolean rules, other
    low-level or sensor level data fusion techniques such as data association, etc.
    can be used for data fusion in this stage implementing the blocking strategy.
    The main errors that may be introduced in this stage [10] include placing various
    records with equivalent references in different blocks. This means that their
    references cannot be compared by data fusion, since it can only compare references
    in the same block, so that any records with equivalent references would not be
    linked and clustered together. Creating different blocks for the different kinds
    of entities helps to minimize this type of errors, as stated in Section 3.1. 4.1.3.
    Raw data storage The raw data received are temporarily stored in this stage. If
    a blocking strategy has been implemented as part of the ingestion, the data are
    stored in different blocks. Storage can be useful, as stated in [56], for traceability
    and detection problems, e.g. to detect transmission or ingestion problems. This
    stage is partially supported by the memorization DT property, which refers to
    the storage of meaningful past data used to create virtual models of physical
    entities and its generation context, as described in Section 2.2. 4.1.4. Pre-processing
    The data ingested into the cyber-system may be inconsistent even if they are related
    to the same entity. The purpose of the pre-processing stage is to reduce errors
    in the data received. In this stage, processes such as cleaning or transformation
    can be used to deal with data received in an unsuitable format in order to extract
    and reconcile conflicting or incomplete information, according to the application''s
    needs. These data transformation processes are also considered as data fusion.
    This stage adopts aspects from the Data Rationalization (DR) stage of the EBDF
    data fusion process and from Kokar''s model data level fusion. Low-level or sensor
    level data fusion techniques, such as denoising or feature extraction processes,
    Boolean rules, etc., are applied to the data received to obtain missing data or
    transform them into a suitable format. It should be highlighted that several data
    pre-processing stages can be carried out in sequence, each one followed by a data
    storage stage, according to the data fusion requirements of the system under development.
    The main errors that may be introduced in this stage [10] are as follows: • Errors
    in conflict resolution when two or more sources report different values for the
    same attribute. • Errors in value harmonization, i.e. errors in selecting the
    right value for an attribute from a set of values with the same meaning but reported
    in different ways (e.g. by different coding schemes). These errors can be reduced
    by adding appropriate sensor and measurement meta-data to the data received, as
    recommended in [56], which can be used to support the pre-processing tasks, e.g.
    sensor precision or accuracy meta-data can be considered in the first case, and
    the measurement unit as long with the value in the second case. 4.1.5. Pre-processed
    data storage The pre-processing stage''s data output is temporarily stored and
    this stage is partially supported by the memorization DT property, which, as described
    in Section 2.2, refers to the storage of meaningful past data from physical entities
    and its generation context. 4.1.6. Modelling, representation, and simulation of
    data The purpose of this stage is to combine data related to an entity to obtain
    high-level data that represents this entity. The values of the digital model that
    represents or simulates the state of the physical entity, i.e. its digital twin,
    are updated using the data received. This is also considered a type of data fusion.
    Amalgamating models can be used to more completely represent the information.
    In EX-DLC, the modelling of entities is based on the integration of the Entity
    Resolution (ER) and creation of Information Product (IP) stages of the EBDF data
    fusion process and on the Feature level DF stage of Kokar''s model. In order to
    combine high-level data representing an entity as a DT, feature level data fusion
    techniques, such as association, state estimation, simulation processes, Boolean
    rules, etc. are applied to the pre-processed data result of the previous stage.
    The challenges related to the contextualization, representation, and modelling
    of data is supported by the Representativeness, Contextualization, Reflection,
    Composability, and Entanglement DT properties detailed in Section 2.2. The importance
    of the data fusion performed in this stage goes beyond the representation itself.
    Data from the DT will be used as the input to further analysis stages to generate
    high-level decisions. For example, data representing a specific person, such as
    body temperature and heart rate, can be analysed and used to decide whether this
    person could be infected by the COVID-19 virus. DT will fusion and model the information
    received from the previous stage by default, even if it is erroneous. Data fusion
    techniques, such as Boolean rules, can be used as part of the DT logic to discard
    or correct out-of-range values. As described below, data management and data fusion
    evaluation processes should be responsible for controlling and ensuring the quality
    of the information at different points, including the stages prior to modelling.
    Of the different errors that can happen during this stage [10], the most frequent
    is the generation of many different records to model and describe the characteristics
    of the same entity. Errors on deciding whether the references of two data records
    should link to the same entity can also occur and reduce the accuracy of the data
    fusion process. 4.1.7. Model history Each update of the values of an entity''s
    virtual model is stored in this stage for use in subsequent stages. This stage
    is partially supported by the memorization DT property (see Section 2.2). The
    data stored in this stage are used as an input for the Analysis stage. 4.1.8.
    Analysis The analysis stage analyses the digital model and the historical data
    from the previous stages and infers new information with a higher level of abstraction
    to make decisions on the entities in the physical world. Obtaining this high-level
    information is considered a data fusion task. This stage is based on the Decision
    level DF stage of Kokar''s model, since a high-level decision is obtained from
    a set of hypotheses using the information extracted and modelled in the Modelling/Representation/Simulation
    stage. The high-level decisions are usually involved in achieving the application
    goals. To obtain a unique high-level decision, decision level data fusion techniques,
    such as statistical inference and analytics, complex inference methods based on
    artificial intelligence (AI) techniques like Machine Learning, Boolean rules,
    etc., are applied using modelled and/or historical data. The EX-DLC Analysis stage
    is directly supported by the Predictability DT property (see Section 2.2). It
    should be noted that several unique high-level decisions can be made in this stage,
    according to the application goals. For example, data representing a specific
    person, such as body temperature and heart rate, can be analysed and used to infer
    whether that person could be infected by the COVID-19 virus. Similarly and simultaneously,
    data representing a specific facility, such as temperature and humidity, can be
    analysed and used to decide whether the air quality of that facility is acceptable.
    Although not explicitly stated in [10], errors can also be introduced in this
    stage that jeopardize the accuracy of the inferences results. 4.1.9. Inferences
    history This stage is responsible for the storage of the inferences as historical
    records, exploiting the memorization DT property, which may be used in the Analysis
    stage. The DT property refers to the storage of meaningful past data of physical
    entities and its generation context, as described in Section 2.2. 4.1.10. Planning
    This stage identifies the actions that should be carried out according to the
    results of the Analysis stage. These actions are provided to the following stage
    for its execution. For instance, in the case study presented in Section 5, if
    it has been inferred that the distance between two people is less than 3 metres
    during the Analysis stage, then the action would be to warn these people about
    their proximity to each other. This information would then be forwarded to the
    next stage, along with the message recommending those involved to use a face masque
    to prevent the spread of COVID-19. 4.1.11. Execution The set of actions to be
    carried out, arranged in the Planning stage, are received and executed in this
    stage. The execution of certain actions may affect the physical environment according
    to the application requirements. In these cases, the execution is delegated to
    the appropriate actuators in the physical environment. In this proposal, this
    is achieved by taking advantage of the (near-)real-time bidirectional data link
    provided by DT. This stage is thus partially supported by the Digital Twins’ property
    known as entanglement, related to bidirectional communication between the physical
    and virtual spaces (see Section 2.2). 4.1.12. Archive/retire (optional) This stage
    is responsible for storing in a different setting or discarding, as appropriate,
    the data stored in other stages. This is important not only for the system performance,
    but also for optimizing the system costs, especially in cloud-based systems. It
    is also optional, as the results of the data fusion carried out as part of data
    management do not depend on its results. 4.2. Data fusion evaluation process based
    on international standards It is important to evaluate the information quality
    (IQ) of data fusion in analysing the performance of a data fusion system. Some
    proposals, such as the EBDF model, from which some aspects are incorporated in
    this proposal, include a data quality management process (DQMP). EBDF DQMP is
    focused on data accuracy, which is the most difficult data quality dimension of
    those to be addressed. However, EBDF has some flaws regarding this, so that our
    aim was to develop a comprehensive data fusion evaluation process based on palliating
    the DQMP''s flaws and satisfying the requirements given in Table 1. DQMP [10]
    outlines a process for ensuring and improving the data quality of data fusion
    systems that perform entity-based data fusion. It focuses on (i) evaluating a
    single quality criterium, accuracy, and even though most of the examples refer
    to applications in the healthcare domain, the applicability of the proposal was
    not shown by a use case. Furthermore, the (ii) quality management process is not
    sufficiently detailed which hinders its use in a systematic manner. To complement
    the DQMP and obtain a more detailed and standardized process, the international
    standard for Systems and software Quality Requirements and Evaluation (SQuaRE)
    ISO/IEC 25040 [20] was used to define this proposal due to its similarities with
    the DQMP process. This standard provides a detailed process description for evaluating
    software product quality and specifies the requirements for its application. This
    standard facilitates both the definition of the steps in the evaluation and identifies
    the information quality criteria and metrics that should be selected for different
    applications. Table 3 summarises the differences between the three quality processes
    based on the evaluation aspects considered as the ideal characteristics of a data
    fusion evaluation process in Table 1. It can be seen that the data fusion evaluation
    proposed as part of EX-DLC is the only one that satisfies all these aspects. Table
    3. Summary of differences between the quality processes based on aspects specified
    in Table 1. Empty Cell Quality process Evaluation aspect identifier DQMP ISO Evaluation
    process for EX-DLC EA1 Stakeholders understanding of Quality ✓ ✓ EA2 Systematic
    use ✓ ✓ EA3 Standardized information ✓ ✓ EA4 Assessment of accuracy ✓ ✓ EA5 Quality
    evaluation of input data and fusion method ✓ ✓ ✓ EA6 Evaluation using verification
    and validation ✓ ✓ ✓ EA7 Automatic Evaluation ✓ EA8 Evaluation in a real setting
    ✓ DQMP defines four components related to planning, control, assurance and improvement
    of data quality, for which the proposal vaguely enumerates some tasks to be carried
    out. These components and its tasks are detailed in Table 4. Most of the tasks
    resemble the tasks defined by ISO/IEC 25040 that are also given in Table 4 along
    with the equivalent tasks in both processes. As can be seen, the (i) Data Quality
    Planning component in the DQMP is equivalent to two tasks in ISO/IEC 25040, i.e.
    Establish the evaluation requirements and Specify the evaluation. The standard
    includes other tasks for the Design of the evaluation that has no equivalent in
    DQMP. The Execute evaluation activity of the standard corresponds with the components
    (ii) Data Quality Control and (iii) Data Quality Assurance in DQMP. Finally, the
    tasks defined for the activity Conclude evaluation of the standard have a similar
    goal to (iv) Data Quality Improvement of DQMP. Few tasks of one process have no
    clear equivalent to any task in the other process. The above indicates that both
    processes can be complementary due to their similarities, so that our proposal
    took the process defined by the standard as the base, since it is better documented
    than DQMP, while additional DQMP tasks are also considered. Table 4. Equivalent
    tasks and activities in the EBDF Data Quality Management Process (DQMP, the ISO/IEC
    25040 evaluation process, and the new data fusion evaluation process (selected
    task names include an identifier). DQMP components [59] Tasks ISO/IEC 25040 evaluation
    process activities [20] EX-DLC Data fusion evaluation process Data quality planning
    – 1a. Establish the purpose of the evaluation Establish the evaluation requirements
    Plan the evaluation Develop and manage quality requirements 1b. Obtain the software
    product quality requirements 1c. Identify the nature of the defects in final product
    that create risks – – 1d. Identify product parts to be included in the evaluation
    Develop and manage quality standards 1e. Define the stringency of the evaluation
    Determine acceptable levels for defects rates – 2a. Select quality measures Specify
    the evaluation Develop and manage quality policies 2b. Define decision criteria
    for quality measures 2c. Define decision criteria for evaluation – – 3a. Plan
    evaluation activities Design the evaluation Data quality control Product inspection
    by the established techniques of statistical process control 4a. Make measurements
    Execute the evaluation Execute the evaluation Data quality assurance Ensure the
    component parts of the product fall within expected tolerances 4b. Apply decision
    criteria for quality measures 4c. Apply decision criteria for evaluation Data
    quality improvement Error reporting and collection 5a. Review the evaluation results
    Conclude the evaluation Conclude the evaluation 5b. Create the evaluation report
    Root cause analysis 5c. Review quality evaluation and provide feedback to the
    organisation Implement process improvements – 5d. Perform disposition of evaluation
    data Based on the integration of the ISO/IEC 25040 and the DQMP processes, the
    tasks to be carried out for each activity are detailed below in greater detail
    than the standard to address the EA2 evaluation aspect (see Table 1). The first
    three activities defined by the ISO/IEC 25040 standard (Establish the evaluation
    requirements, Specify the evaluation, and Design the evaluation) refers to the
    planning activities common to both. We therefore consider these activities as
    part of a general planning activity labelled Plan the evaluation as depicted in
    Table 4. It should be noted that the redefined process will guide the evaluation
    of data fusion systems and address the other requirements and challenges identified
    in Table 1. 4.2.1. Plan the evaluation The sub-activities and tasks of this activity
    are detailed as follows. 4.2.1.1. Establish the evaluation requirements The first
    activity includes the following tasks: • 1a. Establish the purpose of the evaluation.
    Document the reason(s) to evaluate the quality of the data fusion system. The
    general reasons to evaluate data fusion systems are discussed in Section 3, for
    example “measurement of the performance of the data fusion processes performed
    by the developed system”. • 1b. Obtain the software product quality requirements
    by interviewing the system''s stakeholders. Other requirements may also be elicited
    from the bibliography of the application domain. These requirements must be specified
    by a quality model such as the ISO/IEC 25012 [21] standard, which was previously
    adopted for the evaluation of data fusion systems and consists of a set of data
    characteristics such as accuracy, completeness, consistency or credibility, amongst
    others. Since data fusion aims to ensure the accuracy and completeness of data
    and considering that accuracy is paramount and one of the most difficult quality
    attributes to assess, these two quality characteristics must always be evaluated.
    This facilitates addressing evaluation aspects EA3 and EA4 specified in Table
    1. According to ISO/IEC 25012, Accuracy is defined as “the degree to which data
    has attributes that correctly represent the true value of the intended attribute
    of a concept or event in a specific context of use”. It has two main aspects,
    Syntactic Accuracy and Semantic Accuracy, defined as the closeness of the data
    values to a set of values defined in a domain considered syntactically or semantically
    correct, respectively. Completeness represents the degree to which data associated
    with an entity have values for all the expected attributes and related entity
    instances in a specific context of use. In other words, it decides whether all
    the properties have values or if some are empty or null. The definitions of the
    information quality and its characteristics in the ISO/IEC 25012 can be used to
    address the evaluation aspect EA1 (see Table 1). • 1c. Identify the nature of
    the defects in the final product that create risks (from DQMP). In general, defects
    can affect (i) the input data and every (ii) fusion method executed as part of
    the system data management process (see Section 3.4). The data management process
    specified in Section 4.1 identifies the errors that can happen in each data fusion
    stage, i.e. Ingestion, Pre-processing, Modelling/Representation/Simulation; and
    Analysis as well as those in the Collection stage. Any other errors that can affect
    the operation of the system under study must also be identified. • 1d. Identify
    product parts to be included in the evaluation. The components of the data fusion
    system to be evaluated must be identified by using the results of the previous
    task. Fig. 6 shows when the evaluation must be carried out for the EX-DLC process,
    using Boolean rules, after every data fusion stage (see Section 4.1). In other
    words, the data fusion evaluation will be carried out after the Ingestion stage
    to assess the quality of (i) the raw data received and of the (ii) filtering method
    used for blocking; after each Pre-processing stage, to assess the quality of the
    different pre-processing methods; after the Modelling/Representation/Simulation
    stage, to assess the quality of the data fusion methods used to model entities;
    and after the Analysis stage, to assess the quality of the inferences. This helps
    to address the EA5 evaluation aspect given in Table 1. Download : Download high-res
    image (260KB) Download : Download full-size image Fig. 6. Overview of the EX-DLC
    holistic data fusion framework. Evaluation activities (white background) are integrated
    with the EX-DLC data management process when data fusion should be evaluated.
    The different data fusion evaluation methods and techniques that can be used at
    each stage are also identified. • 1e. Define the stringency of the evaluation.
    The goal is to provide the appropriate security to the data fusion system quality
    for its intended use and the purpose of the evaluation. This includes the expected
    evaluation levels, the evaluation techniques to be applied and the evaluation
    results to be achieved. The expected evaluation levels and results are defined
    according to the guidelines in ISO/IEC 25012. At least two acceptance levels should
    be defined for the evaluation results (see Table 7): unacceptable and acceptable.
    Acceptable is the minimum satisfactory level. The minimum quality acceptance levels
    for accuracy and completeness must be defined. The results for both characteristics
    must be acceptable to consider the evaluation acceptable. ISO/IEC 25012 specifies
    that the following stringency aspects must be used for the characteristics evaluated:
    type (inherent or system-dependant), previous documentation/specification required,
    measurement method, variables, formula, and scale. An example is given in Table
    7. 4.2.1.2. Specify the evaluation This activity specifies the modules and quality
    evaluation criteria and includes the following tasks: • 2a. Select the quality
    measures. Quality measures should be selected for every software quality evaluation
    requirement. The procedures should measure every software quality characteristic
    (or sub-characteristic) with sufficient accuracy to allow the criteria to be set
    and comparisons to be made. ISO/IEC 25024 [61] defines data quality metrics for
    quantitatively measuring the characteristics defined in ISO/IEC 25012. The metrics
    will thus be selected following this standard. • 2b. Define decision criteria
    for quality measures. Decision criteria are numerical thresholds or targets used
    to describe the level of confidence of each result. A decision criterion should
    be defined for individual metrics and can be set using benchmarks, historical
    data or other techniques. Since some works, such as [60], set the acceptable threshold
    at 75 % and since it is mandatory to evaluate accuracy and completeness, it is
    reasonable to establish that only results equal or above a threshold of 80 % will
    be considered as an acceptable measurement. Depending on the relevance of the
    attribute to measure, the value of its particular threshold could be higher. •
    2c. Define decision criteria for evaluation. This task prepares a procedure for
    summarizing different criteria for the quality characteristics and its output
    should be used as input for the software product quality assessment. Evaluation
    of the quality characteristics selected, such as accuracy and completeness and
    others, will be considered acceptable if the results for all the aspects of a
    particular product to evaluate are acceptable, i.e. if the EX-DLC stages of data
    fusion obtain a result equal or above the threshold defined in the previous task.
    The overall evaluation of the system can be summarized from the system''s particular
    requirements and application domain. 4.2.1.3. Design the evaluation This activity
    finally specifies the evaluation plan. • 3a. Plan evaluation activities. All the
    previously identified evaluation activities are scheduled as part of the evaluation
    plan, which should include the purpose of the evaluation, standards, the available
    software and hardware resources, the people involved (if applicable) as well as
    the evaluation methods (verification, validation, etc.) and tools as follows:
    • Evaluation techniques and methods must be selected according to the application,
    from the plethora of different possibilities shown in Fig. 6, and the stage of
    the process to evaluate. This addresses the evaluation aspect EA6 identified in
    Table 1. • Tools should ideally be software tools to enable an automated evaluation
    in run-time and minimizing human intervention. They should be integrated with
    the system''s real data as far as possible to address evaluation aspects EA7 and
    EA8, detailed in Table 1. As the data fusion evaluation tools support is limited,
    it is recommended to configure or develop the type of evaluation components integrated
    in the system architecture, e.g. as part of an evaluation subsystem. This is especially
    appropriate for highly demanding (high-risk) data fusion systems, e.g. data fusion
    applications in the healthcare domain. The development of the data fusion evaluation
    subsystem should be carried out at this point before continuing with the remaining
    data quality evaluation framework activities and considering the planning specified
    in the previous tasks. Based on the aspects defined in tasks 1c and 1d, Fig. 6
    shows how the evaluation process is integrated with the data management process,
    the stages of the evaluation process being identified by a white background. The
    first, Evaluation planning, is related to all the activities and tasks defined
    in Section 4.2.1. The other stages make up the data fusion evaluation subsystem.
    Each stage must be developed as a component that performs the data fusion evaluation
    processes included in task 1d, according to what has been defined in task 2a.
    The components must implement some of the methods and techniques summarized in
    Fig. 6 appropriate for the stage it is related to and the aims of the data fusion.
    These data fusion evaluation components must be executed automatically immediately
    after the execution of their evaluated data fusion component. Fig. 6 shows how
    both the data management process and the data fusion evaluation process can be
    integrated to form the EX-DLC holistic data fusion framework proposed in this
    work. 4.2.2. Execute the evaluation This activity consists in executing the evaluation
    and entails the following tasks: • 4a. Make measurements. The selected quality
    metrics are applied to the identified software product parts or components, according
    to the Evaluation Plan. As stated in the previous activity, every method selected
    to evaluate the quality of the data fusion stages of the system is executed in
    run-time by a software component that is part of the system. Measurement results
    can be recorded in the system logs, stored in a system database, or notified by
    any notification or messaging mechanism. • 4b. Apply the decision criteria for
    the quality measures. The decision criteria are applied to the measured values.
    The result may be given as the assertion to which every measurement converges,
    as defined in 2b. This task is conducted either as part of the operation of the
    evaluation software components or manually. • 4c. Apply the decision criteria
    for evaluation. The decision criteria are summarized for each characteristic.
    The result of the evaluation is given as the assertion to which the software product
    converges on quality requirements, as defined in 2c. This is carried out either
    by the evaluation software components or manually if no evaluation system has
    been developed. The set of decision criteria is summarised for all the sub-characteristics
    and characteristics as evidence of the level of the quality requirements met by
    the data fusion system and each of their data fusion stages. 4.2.3. Conclude the
    evaluation The final evaluation activity includes the tasks for reviewing the
    results, creating the final report, and providing feedback, as follows: • 5a.
    Review the evaluation results. The evaluator/s and the requester shall carry out
    a joint review of the evaluation results. • 5b. Create the evaluation report.
    This report should include the requirements of the evaluation, the results of
    the measures and analyses performed, any limitations or constraints, the evaluators,
    and their qualifications, etc. • 5c. Review quality evaluation and provide feedback
    to the organisation. This involves evaluating the results and validating the evaluation
    process, along with the actions applied. The feedback given by the evaluator should
    be used to improve the evaluation and the data fusion processes. • 5d. Perform
    disposition of evaluation data. If needed, all the evaluation data shall be disposed
    (returned, archived, or destroyed) as appropriate. 5. Case study This section
    describes a case study in which our data fusion holistic proposal was applied
    to address, amongst others, the issue regarding the lack of real system use cases.
    The first dimension of the framework was used to guide the design and development
    of the prototype of a smart and pervasive healthcare data fusion system for COVID-19
    prevention in nursing homes. After this, the data fusion evaluation of the prototype
    was designed, following the second dimension of the framework and the components
    of an evaluation subsystem were integrated into the prototype. The holistic framework
    was applied according to the following: 1 Data management process based on data
    fabric and digital twins (DT). a Design of the system architecture and its components
    based on EX-DLC considering that certain stages perform data fusion tasks and
    that DT properties aspects support some of the stages, especially modelling/representation/simulation
    (see Fig. 5). b For every component of the system architecture that should perform
    data fusion tasks, select the most appropriate data fusion methods or techniques
    from the different possibilities, according to the requirements and the corresponding
    EX-DLC data fusion stage (see Fig. 6). c Data fusion evaluation process based
    on international standards (see Table 4). 2 Plan the evaluation specifying the
    evaluation requirements and details, and the design of the evaluation subsystem
    based on (i) the EX-DLC evaluation steps and (ii) their data fusion evaluation
    methods and techniques (see Fig. 6). a Execute the evaluation automatically as
    part of the system operation. b Conclude the evaluation. A preliminary version
    of the prototype was described in [61]. This paper focuses on the data fusion
    carried out by the data management process and its evaluation, even though a smart
    and pervasive healthcare environment could become extremely complex, the current
    system is offered as a simple prototype for the sake of simplicity and to facilitate
    its applicability. We provide a summary of the prototype''s main characteristics
    in Section 5.1, its architecture designed according to EX-DLC and the DT properties
    that support the system, including the new data fusion evaluation subsystem (see
    Section 5.2), and the results of the data fusion quality evaluation (Section 5.3).
    5.2. System description The aim of the current prototype of a pervasive healthcare
    system is the proactive prevention of the spread of COVID-19 in nursing homes,
    considering that these centres have been seriously affected by this pandemic.
    For this, the design was based on the well-known prevention measures laid down
    by medical authorities worldwide, i.e. social distance, face masks, open spaces,
    unobstructed airflow, air quality, etc. The smart pervasive healthcare system
    prototype was developed for the nursing home “Las Hazas” in Hellín [62], Albacete,
    Spain. This nursing home can accommodate 200 users and has numerous common areas
    distributed over three floors. The maximum number of workers in a single shift
    is 50. To prevent the spread of COVID-19 in the nursing home, the prototype supports
    the functionalities given in Table 5. It also illustrates the data gathered and
    used to support each one. Table 5. System functionality and data used data. Functionality
    Data used Monitoring the location and physiological attributes of system users
    Heart rate, body temperature and location (longitude and latitude) Detection of
    close contacts between users Location (longitude and latitude) Notification as
    a reminder of the recommendation for proper use of face masks Location (longitude
    and latitude) Inference and notification of infections Heart rate, body temperature
    Measurement and analysis of the air quality and notification of alerts for low
    air quality and appropriate actions Concentration of CO2, ambient temperature,
    and relative humidity The inmates and staff wear smart bands for measuring their
    heart rate, body temperature and location (longitude and latitude). The staff
    take frequent readings of the inmates’ physiological signals by medical devices
    connected to the elderly''s mobile devices. Mobile devices transmit the data for
    individual users to the cyber-subsystem, which carries out the data fusion required
    following the processes defined in the proposal. In addition, the sensors installed
    in the common areas of the nursing home measure the CO2 concentration, ambient
    temperature and relative humidity. The different facilities have a dedicated device
    that receives the facility sensor signals and transmits them to the cyber-subsystem.
    A maintenance technician periodically measures the facility''s properties using
    more accurate sensors connected to the facility''s dedicated device. All users
    can see their physiological or location data in real time from an application
    installed on their smartphone, that is also used to provide notifications. The
    air quality data device in each room includes a display with these data in real
    time. Other screens installed at different points in the residence simulate a
    map giving the state of the facility''s air quality in real time. The system administrators
    have access to the data stored during the system''s execution. All the values
    are shown associated with the attribute and the measurement unit if applicable
    for easy interpretation. 5.3. System architecture and deployment This section
    describes the proposed holistic EX-DLC framework was used to design and develop
    a system that supports both the EX-DLC data management process and the data fusion
    quality evaluation process. The cloud platform chosen for the deployment of the
    prototype was Microsoft Azure [54], due to its suitability for supporting the
    development of IoT-based systems. Fig. 7 shows the system architecture, which
    uses different Azure services as components to support the different EX-DLC stages.
    The Azure services used are classified into different types based on their capabilities,
    as can be seen in Fig. 7. Azure Functions (Store raw data, Pre-process user data,
    etc.) follows the serverless approach to develop services that execute the core
    logic and were used to support different stages of the data fabric architecture,
    especially to implement the Boolean rules used in the data fusion stages. The
    Event Hubs service (Hub user, Hub distance, etc.) was also used to support different
    stages enabling the communication between the different services by acting as
    an event broker. The Application Insights service monitored the operation of the
    different services. Details of these services can be found in the Azure documentation.
    The areas of the diagram are coloured according to the EX-DLC stage that supports
    illustrating the data and the components’ execution flow. The stages and components
    of the automatic quality evaluation subsystem are also shown in Fig. 7 (white
    areas). The architecture and deployment of both subsystems are detailed in the
    following sections. Download : Download high-res image (1MB) Download : Download
    full-size image Fig. 7. Architecture components of the prototype implemented on
    Microsoft Azure and the corresponding stages in EX-DLC, including additional evaluation
    stages. 5.3.1. Data management subsystem The architecture of this subsystem was
    designed as a data fabric architecture that supports the EX-DLC data management
    process. The components that support the optional stage of archive/retire (in
    grey) are excluded from this prototype. The most suitable Azure native services
    were selected for the different stages according to the stage requirements and
    the capabilities offered by these services. Their costs were also considered,
    plus the credit limit of the Azure subscription. As can be seen in Fig. 7, the
    developed Data Management Subsystem is composed of two main subsystems as follows:
    • The physical and social subsystem is that of the nursing home''s real environment
    augmented by different devices, sensors and actuators for collecting data from
    multiple sources and unobtrusively reacting to the needs of users and facilities.
    The devices, sensors and actuators are related to a particular entity, so that
    this subsystem supports the first stage, i.e. data collection (see Section 4.1.1)
    from different sensors and devices and also the execution stage (see Section 4.1.11)
    if required. This subsystem was developed to fuse sensor data before being transmitted
    by a multi-source approach. For this, the different sensor data of an attribute
    are fused in the proxy device of the entity it belongs to, e.g. smartphones or
    edge device, depending on whether the type of entity is a user or a thing. These
    fused data are then sent to the cyber-subsystem by the proxy devices. This type
    of entity-based data fusion was implemented to ease the blocking strategy defined
    for the next stages in the cyber-subsystem. • The cyber-subsystem is the computing
    system that performs most of the data processing. Its main components support
    the data fusion processes, including a modelling stage supported by DT. This subsystem
    supports all the EX-DLC stages except the collection and execution stages. It
    was decided to employ Boolean rules for all the EX-DLC data fusion stages because
    of their simplicity and because all the data fusion operations required can be
    performed at the attribute level. Ingestion is the first stage of the cyber-subsystem
    (see Section 4.1.2). For its development, the IoT Hub service (IoT gateway) was
    used as a gateway to the cyber-subsystem to facilitate bidirectional communication
    with the physical subsystem entities, receiving the data from these and executing
    actions on the devices. This service also facilitates the use of the Boolean rules
    exploited to support the filtering capability necessary for the blocking strategy,
    i.e. different blocks are established for different entity types. This blocking
    enables determining the type of entity, user or facility of the data received,
    so that data of the two types of entities in this system are transmitted through
    different hubs, i.e. Hub user and Hub facility. After Ingestion, the CosmosDB
    service, a NoSQL database, is used to store the raw data received in the established
    blocks, based on their entity type (User raw data, Facility raw data) as part
    of the Raw Data Storage stage (see Section 4.1.3) After the raw data storage,
    a Pre-processing stage (see Section 4.1.4) is performed using different Azure
    Functions. Its goal is to increase the quality of the data that will be fused
    to update the models of the entities of the physical and social space. The attributes
    of an entity with multiple values are processed to generate a single value. The
    Pre-process User and Facility Data functions obtain this value from the sensor
    with the best accuracy, resolution and priority. These single values are then
    stored as part of the Pre-processed Data Storage stage (see Section 4.1.5) by
    means of the corresponding Store pre-processed User and Facility Data functions.
    They are also transmitted through the corresponding User and Facility pre-processed
    Hubs. After the pre-processing, as part of the Modelling/Representation/Simulation
    stage (see Section 4.1.6), the Azure Digital Twins service (Fig. 7, in yellow)
    is used to address the data contextualization problem associated with the data
    fabric. It also supports data fusion since entity data are managed by Azure Digital
    Twins instead of storing the data received in an unstructured way through data
    lakes. The Azure Digital Twins service receives the entities’ data collected by
    sensors in real time. This service is also used to fuse and update the virtual
    models that represent these entities as well as their different relations and
    interactions at a particular time. The Azure Digital Twins service is thus used
    to represent the physical world in the virtual world and connecting them to each
    other. This also facilitates that further analysis processes can make better decisions
    regarding the actions to be carried out to satisfy the users and environmental
    needs using the fused information. DT''s bidirectional communication capability
    is achieved in combination with the IoT Hub service. Fig. 7 shows that the Modelling/Representation/Simulation
    stage implementation encompasses the Digital Twins service, Event Hubs services
    as well as the following Azure Functions: • Model user data and Model facility
    data are responsible for creating, updating and deleting the different DT’ models
    of users and facilities, respectively, to represent the data received in real-time
    from those entities. • Get facility users is executed every time a user DT is
    updated and obtains all the users that share the facility at that time. • Calculate
    distance processes the list of users from the previous function and calculates
    the distance between the DT of the users by their location (longitude and latitude).
    • Update relations creates, updates and deletes relations between the DT of a
    facility and the user entities obtained in the previous function, depending on
    the distance between them. After this execution, the system model reflects the
    relation of proximity between all the DTs of the users affected by the last update.
    These functions, which are the services that perform the data fusion, implement
    the appropriate Boolean rules for their goal. The Digital Twins service also offers
    a tool for visualizing the entities’ DT in real-time (see Fig. 8). This tool creates
    a DT graph whose relationships show the users (UserXXXX) in a certain room (RoomYY)
    and those related to other users at a specific time. By clicking on each node
    of the graph, it is possible to access to the real-time values received from the
    selected DT. This graph clearly illustrates the ability of DT to contextualize
    data. Download : Download high-res image (444KB) Download : Download full-size
    image Fig. 8. Graph of DTs and their representation at a particular time. The
    relational Azure SQL and CosmosDB databases Users history, Facilities history
    and Relationships history are used for storing historical information of entities
    and their relations supporting the Model History stage (see Section 4.1.7). The
    DT service also offers a tool known as 3D Scenes for Azure Digital Twins to visualize
    the DTs within a 3D environment that represents the real space and then check
    their properties, facilitating the multi-scale and multi-physics modelling characteristic
    of DTs (see Section 2.2). Using this tool, a 3D interface was developed to improve
    user experience regarding other types of information visualization strategies.
    This interface shows in real-time the status of the air quality in any room by
    a colour scale (see Fig. 9). The interface also has a widget that shows the air
    quality parameters. If the air quality is not adequate, an alert, represented
    as an icon, is shown in the interface. The information shown in Fig. 9 can be
    seen as fused data related to all the nursing home facilities. It should be noted
    that data fused, modelled and presented in this way could be useful to nursing
    home staff, since they show the facilities that require ventilation or even evacuation.
    It can also be used by the elderly to indicate the facilities to avoid COVID-19
    infections. Download : Download high-res image (419KB) Download : Download full-size
    image Fig. 9. Qualitative 3D interface representing the DTs depicting the nursing
    home facilities. The data fused in the modelling stage are used during the Analysis
    stage (see Section 4.1.8) to satisfy the requirements and achieve the goals of
    the data management system. This stage is supported by four different Azure Functions
    (each one related to an inference) that are currently implemented as Boolean rules:
    • Analyse proximity determines whether the distance between different users is
    less than 3 metres to send them a reminder of the recommendation of using a face
    masque. • Analyse COVID-19 positive determines that a user is infected if the
    values of their body temperature and heart rate are above the thresholds laid
    down by the health authorities. The algorithm used is based on the evidence that
    for febrile cases of the disease, the heart rate increases 15 beats per minute
    for each degree Celsius that body temperature increases [63]. • Retrieve close
    contacts obtains a list of users that have been in contact with a user infected
    for at least 15 min in the last 5 days. • Analyse air quality infers poor air
    quality of a facility if some of its CO2, temperature and humidity attributes
    are out of the recommended range for nursing homes by the health authorities.
    The results of the inferences are stored in the Positive history, Poor air quality
    history, and Close contacts history databases as part of the Inferences History
    stage (see Section 4.1.9). The Planning stage (see Section 4.1.10) is run after
    the analysis has been conducted. For every situation to be managed, an Azure function
    was developed to plan the actions that should be taken according to the inferred
    results, i.e. Plan close contact, Plan no-distance contact action, etc. These
    actions are later run as part of the Execution stage (see Section 4.1.11) using
    the corresponding Azure Functions, i.e. Notify close contact, Notify put on masque,
    etc. Some of these actions simply send notifications or commands to the devices
    that act as the proxies of the entities, using the bidirectional communication
    capability offered by the IoT Hub service. It should be highlighted that the IoT
    Hub service records the different devices associated with the different users
    and facilities modelled by means of the DTs service and, thanks to the bidirectional
    communication link, physical entities and virtual entities can be synchronized
    almost in real time. The fast DT synchronization characteristic is paramount for
    COVID-19 prevention, i.e. the system''s main aim. Adopting other less complex
    approaches to store, query, show and notify information without support for real-time
    communications would not therefore be appropriate in this and similar cases. 5.3.2.
    Data fusion quality evaluation subsystem As depicted in Fig. 7, the components
    of the evaluation subsystem that carry out the evaluation of the data fusion stages
    were also implemented in Azure Functions. Evaluate raw data (for every entity
    type) evaluates the ingestion filter-based blocking quality. Evaluate pre-processed
    data (for every entity type) evaluates the quality of the pre-processing technique
    applied to the raw data, i.e. the selection of the best attribute value. Evaluate
    modelled data evaluates the quality of the entities’ relations created as part
    of the modelling. Finally, evaluate inferred positives, inferred proximity, etc.
    evaluate the quality of the corresponding analysis. All these Azure Functions
    services execute the Boolean rules defined in Table 6. In all cases, the data
    fusion evaluation component is executed automatically and immediately after the
    execution of their evaluated data fusion component, once the data fusion stage
    results have been stored. Table 6. Semantic and syntactic accuracy and completeness
    validation rules. Data fusion stage Characteristic Entity Attribute Rules to be
    applied Ingestion Accuracy (Semantic) Facility Device name Contains “Facility”
    User Device name Contains “User” Accuracy (Syntactic) Facility Entity type Data
    set is of type Facility User Entity type Data set is of type User Completeness
    Facility Device name Value not null User Device name Value not null Pre-processing
    Accuracy (Semantic) Facility CO2 level Value between 0 and 1200 Humidity Value
    between 20 and 95 Temperature Value between −30 and 50 User Heart rate Value higher
    than 0 Temperature Value between −30 and 50 Location Longitude value between −90
    and 90 and Latitude value between −180 and 180 Accuracy (Syntactic) Facility CO2
    level Integer Humidity Integer Temperature Integer User Heart rate Integer Temperature
    Decimal Location Decimal tuple Completeness Facility CO2 level, humidity, temperature
    Values not null User Temperature, heart rate Values not null Modelling Accuracy
    (Semantic) User Relationship (IsRelated is true and Distance value between 0 and
    3.00) or (IsRelated is false and Distance value not between 0 and 3.00) Accuracy
    (Syntactic) Relationship Tuple of Boolean and decimal Completeness Relationship
    Distance not null Analysis Accuracy (Semantic) Facility Air deficiency (Parameter
    is \"Temperature\" and Value >= 25 and Reason is \"High\") or (Parameter is \"Temperature\"
    and Value <= 23 and Reason is \"Low\") or (Parameter is \"Humidity\" and Value
    >= 70 and Reason is \"High\") or (Parameter is \"Humidity\" and Value <= 30 and
    Reason is \"Low\") or (Parameter is \"CO2Level\" and Value >= 500 and Reason is
    \"High\") User COVID-19 Positive (IsPositive is true and Temperature >= 37.5 and
    HeartRate >= 90) or (IsPositive is false and (Temperature < 37.5 or HeartRate
    < 90)) Proximity (State is “Far” and relation was deleted) or (State is “Far”
    and Distance > 3) or (State is “Near” and Distance <= 3) Close contact (IsCloseContact
    is true and Duration of contact >= 15′ and MinDistance <= 2) or (IsCloseContact
    is false and ((Duration of contact < 15′ or MinDistance > 2)) Accuracy (Syntactic)
    Facility Air deficiency Value is double and Reason is string User COVID-19 Positive
    IsPositive is Boolean Proximity State is string and Distance is Double Close contact
    Duration is TimeSpan Completeness Facility Air deficiency Value not null User
    COVID-19 Positive IsPositive not null Proximity Distance not null Close contact
    Duration, sourceId and TargetId not null 5.4. Case study data fusion evaluation
    As mentioned, the system supports well-known COVID-19 prevention measures that
    have been considered valid. This is why we consider that the system can help to
    prevent infections. Bearing that in mind and since our goal was not to evaluate
    the prevention system, but to show how the data fusion evaluation process can
    be applied in a real system, this Section does not describe an evaluation of the
    prototype but focuses on showing the application of the EX-DLC evaluation process
    for evaluating the data fusion carried out by the prototype, i.e. by the subsystem
    designed using the first EX-DLC dimensions. The data fusion carried out by the
    system described in Section 5.2 was thus evaluated following the evaluation process
    proposed here and defined in Section 4.2 to show the proposals’ applicability.
    The following sections describe how the activities of the data fusion evaluation
    process was conducted. 5.4.1. Plan the evaluation The work carried out in the
    context of the three sub-activities of the Plan the Evaluation activity is summarized
    in the following. 5.4.1.1. Establish the evaluation requirements The evaluation
    requirements are detailed in the context of the different tasks involved in this
    activity. • 1a. Determine the purpose of the evaluation. The purpose of the evaluation
    is the measurement of the degree of precision of the data fusion processes performed
    by the developed system. • 1b. Obtain the software product quality requirements.
    Since the data fusion system to be evaluated is a prototype and the purpose of
    the work is to show the applicability of the framework proposed, only the mandatory
    quality characteristics, accuracy and completeness, are considered as quality
    requirements of the system. • 1c. Identify nature of the defects in final product
    that create risks. Based on the data management process proposed in Section 4.1,
    we defined the four stages in which errors can be introduced: Ingestion, Pre-processing,
    Modelling/Representation/Simulation and Analysis. However, errors could also happen
    during the collection stage. • 1d Identify product parts to be included in the
    evaluation. Based on the data management process proposed in Section 4.1 and on
    the results of the previous task indicating the points at which errors may occur,
    we defined four points to include in the evaluation: the data management process
    stages of Ingestion, Pre-processing, Modelling/Representation/Simulation and Analysis,
    since they are responsible for performing data fusion tasks. • 1e. Defining the
    stringency of the evaluation. Two acceptance levels were established for the evaluation
    results: unacceptable and acceptable. 5.4.1.2. Specify the evaluation The evaluation
    is specified in the context of the different tasks included in this activity.
    • 2a. Selecting the quality measures to use in the evaluation. As indicated in
    Section 4.2.1, the metrics were selected from the standard ISO/IEC 25024. Since
    no additional quality requirements and characteristics were considered other than
    accuracy and completeness, the metrics semantic accuracy of data and syntactic
    accuracy for accuracy, and the metric attribute completeness for completeness
    were selected (see Table 7). Table 7. Acceptance levels for entity''s attributes
    and selected metrics for each quality characteristic. Characteristic Metric/s
    Measurement method Attribute Stage(s) Acceptance level Accuracy Semantic accuracy
    of data And Syntactic accuracy of data X: Ratio of attribute registries that fits
    semantic/syntactic rules X = A/B A = number of attribute registries that fits
    semantic/syntactic rules B = Total of registries Facility CO2 level Pre-processing
    X>=0.8 Temperature Pre-processing Humidity Pre-processing Air deficiency Analysis
    User Heart rate Pre-processing Body temperature Pre-processing Location Pre-processing
    Relationship Modelling X>=0.9 COVID-19 positive Analysis Proximity Analysis Close
    contact Analysis Completeness Attribute Completeness X: Ratio of required attribute
    registries that are non-null X = A/B A= number of required attribute registries
    that are non-null B = Total of registries Facility CO2 level Pre-processing X>=0.8
    Temperature Pre-processing Humidity Pre-processing Air deficiency Analysis User
    Heart rate Pre-processing Body temperature Pre-processing Location Pre-processing
    Relationship Modelling COVID-19 positive Analysis Proximity Analysis Close contact
    Analysis The real-world entities identified in the context of the use case described
    in Section 5 are the users (elderly and caregivers) and nursing home facilities.
    The selected metrics apply to the attributes of the user and the facility. After
    the Ingestion stage, it should be determined whether the different entities’ data
    have been redirected, using filtering, to the right block, according to the entity
    type (user or facility). The attributes to evaluate after the Pre-processing stage
    for the system described in Section 5 are CO2 level, humidity, and temperature
    for the facilities, and heart rate, body temperature, and location coordinates
    for the user. After the Modelling/Representation/Simulation stage, the characteristics
    of the user relationship attribute are evaluated. When the inferences have been
    made in the Analysis stage, the user attributes of proximity, positive in COVID-19
    and close contact are evaluated. In the case of facilities, the air deficiency
    attribute is also evaluated in this last stage. To measure the accuracy, it is
    necessary to establish the semantic and syntactic rules for each attribute. For
    the semantic rules, the working ranges provided by the manufacturers of the sensors
    were used. The location attribute is composed of longitude and latitude, Sensor
    precision is used for each coordinate. Inferred values such as air deficiency,
    positives in COVID-19, and close contacts are considered Boolean values. Proximity
    between users is made up of an attribute composed of a relation between users
    and the distance in meters. Syntactic rules consider the expected data type. Table
    6 shows these criteria in detail. • 2b. Defining the decision criteria for quality
    measures. Apart from the default threshold of 80 % to consider a measurement as
    acceptable established in Section 4.2.1, the acceptance level was raised to 90
    % for the attributes of positive in COVID-19 or close contacts, given their importance.
    Table 7 summarizes the acceptance level values for the selected metrics for the
    quality characteristics and attributes, along with the specified measurement method.
    It also includes the stage in which the different entities’ attributes are fused.
    • 2c. Define the decision criteria for evaluation. As mentioned in Section 4.2.1,
    every characteristic of every EX-DLC stage that performs data fusion is considered
    acceptably evaluated if its evaluation result is equal or above the threshold
    defined in the previous task. For the case study, the global evaluation of the
    system was not summarized in a pre-established manner, since we considered that
    the different data fusion components operated independently. Although the inputs
    of some of the components may depend on the outputs of the predecessors, good
    data fusion quality in one does not guarantee good data fusion quality in the
    others. Only the basic quality characteristics of accuracy and completeness were
    evaluated. 5.4.1.3. Design the evaluation As shown in Table 7, several Boolean
    rules were chosen to evaluate the different quality characteristics selected according
    to the different entity''s attributes fused in the developed system. Boolean rules
    are one of the data fusion evaluation techniques described in Section 3.2, and
    are easy to specify, understand, and implement. Since the fusion stages implemented
    in the developed system are based on Boolean rules, using them to validate the
    data fusion stages is the most logical and straightforward approach. However,
    as pointed out in Section 3.2, the Boolean rules for validation are ‘soft’ with
    respect to the original data fusion rules, i.e. they impose less strict conditions
    and only evaluate a near or partial match. The Boolean rules for validation have
    been implemented in the data fusion evaluation components included in Fig. 6.
    Overview of the EX-DLC holistic data fusion framework. Evaluation activities (white
    background) are integrated with the EX-DLC data management process when data fusion
    should be evaluated. The different data fusion evaluation methods and techniques
    that can be used at each stage are also identified.. Following the guidelines
    described in Section 4.2.1, these evaluation components are executed automatically
    in run-time immediately after their evaluated data fusion component. The results
    of the evaluation of the attributes range between 0 and 1. After the execution,
    the evaluation of numerical results is interpreted and summarized to determine
    whether or not they are acceptable, using the guide specified in Table 7, also
    whether the data fusion evaluation has been satisfactory according to what was
    defined in tasks 2b, and 2c and 1e (also in Section 4.2.1), respectively. The
    evaluation report is provided in the following section. 5.4.2. Evaluation execution
    The evaluation was conducted by carrying out load tests that followed the guidelines
    given by Microsoft [64]. We also considered the use limits of the Azure services
    deployed, their cost, and the credit available in the Azure subscription in which
    the prototype is currently deployed. A total of 250 users were simulated: 200
    being the maximum number of nursing home residents, plus 50 employees. The test
    lasted for 9 h, the period of the first daily shift in the nursing home, including
    the playful activities period. The different tasks in the evaluation execution
    activity are detailed in the following. • 4a. Make measurements. After starting
    the execution, the data fusion system is fed with pseudo-random data from the
    entities’ attributes collected by the different system sensors. The collection
    emulator is capable of generating invalid data (e.g. outliers) for certain attributes
    within a configurable random percentage range, which was between 0 and 10 % for
    this evaluation. Each time the components associated with the data fusion stages
    of Ingestion, Pe-processing, Modelling/Representation/Simulation and Analysis
    are executed, the related evaluation components (see Figs. 6 and 7) are executed
    after storing the data fusion results in the corresponding database. The measurement
    method based on the formula specified in Table 7 is applied to the output of every
    characteristic, i.e. the data fusion results of every data fusion component. The
    numerical results of the data fusion evaluation performed by every component are
    currently provided as system logs. For example, Fig. 10 shows the results of the
    evaluation of all the metrics after the pre-processing stage. The ''Formula''
    column shows the operation conducted that divides the number of data sets that
    passed the evaluation according to the corresponding Boolean rule by the total
    number of data sets evaluated for individual metrics. The result of this operation
    is shown in the ‘Result’ column. • 4b. Apply the decision criteria for quality
    measures. As can be seen from the images with the evaluation results included
    in the previous task, the results are 1 or near 1 in most cases, but always higher
    than the acceptance thresholds defined in Table 7. The corresponding measurements
    were interpreted as acceptable if the acceptance level threshold is achieved or
    surpassed, or as unacceptable in the opposite case. This was executed manually
    and the interpretation was specified as Measurement result in Table 8 in Section
    5.3.20. Table 8. Summary of evaluation results per measurement and data fusion
    stage quality characteristic. Data fusion stage Characteristic Entity Attribute
    Measurement numeric result Measurement result Characteristic result Ingestion
    Accuracy (Semantic) Facility Device name X = 34,945/34,945 = 1 Acceptable Acceptable
    User Device name X = 487,005/487,105 = 0,9998 Acceptable Accuracy (Syntactic)
    Facility Entity type X = 34,945/34,945 = 1 Acceptable User Entity type X = 487,105/487,105=
    1 Acceptable Completeness Facility Device name X = 34,945/34,945 = 1 Acceptable
    Acceptable User Device name X = 487,105/487,105 = 1 Acceptable Pre-processing
    Accuracy (Semantic) Facility CO2 level X = 28,543/30,242 = 0,9407 Acceptable Acceptable
    Humidity X = 29,779/30,366 = 0,9806 Acceptable Temperature X = 28,566/30,366 =
    0,9407 Acceptable User Heart rate X = 392,812/418,373 = 0,9389 Acceptable Temperature
    X = 366,557/418,373 = 0,8761 Acceptable Location X = 418,373/418,373 = 1 Acceptable
    Accuracy (Syntactic) Facility CO2 level X = 30,250/30,366 = 0,9961 Acceptable
    Humidity X = 30,366/30,366 = 1 Acceptable Temperature X = 30,366/30,366 = 1 Acceptable
    User Heart rate X = 418,373/418,373 = 1 Acceptable Temperature X = 418,373/418,373
    = 1 Acceptable Location X = 418,373/418,373 = 1 Acceptable Completeness Facility
    CO2 level, humidity, temperature X = 30,366/30,366 = 1 Acceptable Acceptable User
    Temperature, heart rate, location X = 418,373/418,373= 1 Acceptable Modelling
    Accuracy (Semantic) User Relation X = 718,303/745,692= 0,9632 Acceptable Acceptable
    Accuracy (Syntactic) User Relation X = 745,692/745,692 = 1 Acceptable Completeness
    User Relation X = 745,692/745,692 = 1 Acceptable Acceptable Analysis Accuracy
    (Semantic) Facility Air deficiency X = 34,793/35,162 = 0,9895 Acceptable Acceptable
    User COVID-19 Positive X = 309,065/309,066 = 0,9999 Acceptable Proximity X = 120,234/120,234
    = 1 Acceptable Close contact X = 27,003/27,003 = 1 Acceptable Accuracy (Syntactic)
    Facility Air deficiency X = 35,162/35,162= 1 Acceptable User COVID-19 Positive
    X = 309,066/309,066 = 1 Acceptable Proximity X = 120,234/120,234 = 1 Acceptable
    Close contact X = 27,003/27,003 = 1 Acceptable Completeness Facility Air deficiency
    X = 35,162/35,162 = 1 Acceptable Acceptable User COVID-19 Positive X = 309,066/309,066
    = 1 Acceptable Proximity X = 120,234/120,234 = 1 Acceptable Close contact X =
    27,003/27,003 = 1 Acceptable • 4c. Apply the decision criteria for evaluation.
    The summary of the decision criteria was carried out by grouping the different
    quality characteristic results per characteristic and data fusion stage. The corresponding
    characteristic results were interpreted as acceptable if the acceptance of all
    the sub-characteristics was interpreted as acceptable, or as unacceptable in the
    opposite case by a manual process. The interpretation was specified as Characteristic
    result in Table 8 in Section 5.3.20. Download : Download high-res image (749KB)
    Download : Download full-size image Fig. 10. Data fusion evaluation numerical
    results for the Pre-processing stage based on Table 7. 5.4.2.1. Evaluation results
    and conclusions The evaluation results and conclusions are given according to
    the different tasks involved in this activity: • 5a. Review the evaluation results.
    The evaluation results were reviewed and discussed by all the authors. • 5b. Create
    evaluation report. The entire Section 5.3.2 can be seen as the evaluation report.
    Table 8 constitutes a summary of the evaluation results per measurement and data
    fusion stage quality characteristic. As can be seen, the evaluation of the quality
    characteristics, accuracy and completeness, is acceptable for each data fusion
    stage performed by the prototype. From this it can be concluded that the evaluation
    of the different data fusion components of the prototype architecture was satisfactory,
    according to the criteria defined. The overall evaluation of the system of this
    prototype was not summarized according to a pre-established system as we considered
    that the different data fusion components operate independently although connected
    by inputs and outputs, and that the correct data fusion quality of one did not
    ensure good data fusion quality in the others. • 5c. Review quality evaluation
    and provide feedback to the organisation. From the satisfactory evaluation results,
    it can be concluded that the proposal facilitates the design and development of
    data fusion systems and helps to achieve the system''s quality requirements. Although
    the overall evaluation of the data fusion performed by the proposed system seems
    satisfactory, the evaluation was performed in a period of only nine hours to stay
    within the available Azure subscription credit and using simulated data. Furthermore,
    only the basic accuracy and completeness quality characteristics were evaluated.
    It can thus be considered that a more complete evaluation, including more quality
    characteristics in a longer period, etc. should be carried out to determine with
    greater certainty whether the entire data fusion system can be considered satisfactory.
    • 5d Performed disposition of evaluation data. The results of this evaluation
    can be made available and may be used as benchmark data for future evaluations,
    especially if new requirements are required. 6. Conclusions and future work Society
    currently aims to maximize the benefits of developing novel solutions in diverse
    fields such as healthcare to monitor and prevent diseases, etc. Due to recent
    technological advances, certain computing paradigms such as pervasive computing
    can offer enhanced personal support beyond the conventional support offered by
    desktop computers. The fusion of data collected by multiple sensors installed
    in the environment plays an important role in this pervasiveness. These environments
    are also populated with actuators that can react unobtrusively to the users and
    environment''s needs. Several issues make data fusion a challenging task, most
    of which are related to data or data source quality, the data fusion process and
    the evaluation of the data fusion. Of the different data fusion quality aspects,
    accuracy is the most difficult to address. The present work proposes the EX-DLC
    holistic framework to improve data fusion in pervasive systems by addressing the
    issues identified. It can be used to design the system architecture, the data
    fusion approach and the data fusion evaluation as part of the system, encompassing
    two dimensions: • a process for guiding the design of the system architecture
    focusing on the data management based on the process defined in [17]. It integrates
    aspects of Data Fabric and Digital Twins to address the relevant challenge of
    information representation of the different entities of the physical space in
    the virtual space. This process also includes aspects and techniques from different
    data fusion models. We also analysed the way in which some of the properties associated
    with DT are exploited by the data management process. • a process for the evaluation
    of data fusion systems based on international standards to ensure the quality
    of the data fusion tasks performed by the system. It also offers guidelines for
    designing the architecture of an evaluation subsystem that performs the evaluation
    automatically in runtime as part of the system. A system designed to prevent the
    spread of COVID in nursing homes was also developed using the proposal. The system
    monitors and, if necessary, controls, the activity, some physiological constants
    of the users and the staff and the air quality in the building. The developed
    system was configured to be deployed in a real Spanish nursing home and used DT
    to model the nursing home users, staff and facilities. A 3D interface was included
    in the system to show the state of the DTs that modelled the facilities in the
    virtual environment. The satisfactory evaluation results of the data fusion stages
    indicate state that the proposal facilitates the design and development of data
    fusion systems and helps to achieve the system’ quality requirements. The proposal
    here presented exploits DT as one of its main foundations, although it should
    be mentioned that some authors are against their use because of the high cost
    of their development. For instance, Barricelli et al. [48] state that the creation
    and application of a DT can be costly because the maintenance of the physical
    entity itself is quite expensive. Alternatively, a digital representation could
    be developed using fewer complex technologies such as a database, a set of equations
    or even a spreadsheet. The key added value that DT supports is the entanglement
    property. Its data link, ideally two-way, is what differentiates DT from similar
    concepts. As the Digital Twin Consortium points out [49], this link makes it possible
    for users to know the state of the entity by querying data, and, for actions communicated
    through the DT, to take effect in its physical counterpart, so that the physical
    entities and virtual entities are synchronized at a specified frequency, ideally
    in near real time and with high fidelity. During the development of the proposal
    and the system, different lines of future work were identified. One of these is
    to carry out a more detailed study of architectural proposals to improve the process
    based on Data Fabric architectures as well as its validation, developing different
    more complex case studies than the one presented. Regarding the data fusion, it
    would also be interesting to define the tasks performed as part of the Collection
    stage in a more detailed manner considering aspects as scalability, security,
    or sensors malfunction. It would be interesting to integrate an additional pre-processing
    stage for the generation of data when incomplete data are received from the Collection
    stage. Although the overall evaluation of the data fusion performed by the prototype
    was satisfactory, a more complete evaluation should be carried out to determine
    whether the evaluation of the entire data fusion system can be considered satisfactory
    in greater detail. This evaluation could measure more quality characteristics,
    be performed during a larger period of time, use real data, etc. The Boolean rules
    could also be replaced by other data fusion techniques as well as more complex
    evaluation techniques such as those based on AI technologies. A ground truth or
    benchmark data, such as those already stored in the database of the system developed,
    could be integrated into the evaluation system to support the verification of
    the data fusion conducted using AI technologies. We would also like to explore
    including context in the data fusion evaluation. Context is defined in [4] as
    metadata that contributes to the situation assessment of data fusion systems and
    provides important information. Including context would improve the information
    quality of the data fusion process and provide end-users with a better assessment
    of the situation to make better decisions. Some recent studies [15] are currently
    following this line of research. Bearing in mind that the application domain and
    goals influence the data fusion requirements, we plan to explore the system modelling
    also considering the Domain-Driven-Design-based approach for architecting Digital
    Twins proposed in [44]. It is also planned to add new functionalities to the prototype
    and deploy it on other cloud platforms. Authors statement All persons who meet
    authorship criteria are listed as authors, and all authors certify that they have
    participated sufficiently in the work to take public responsibility for the content,
    including participation in the concept, design, analysis, writing, or revision
    of the manuscript. Furthermore, each author certifies that this material or similar
    material has not been and will not be submitted to or published in any other publication.
    CRediT authorship contribution statement Aurora Macías: Conceptualization, Methodology,
    Validation, Investigation, Writing – original draft. David Muñoz: Conceptualization,
    Validation, Software, Writing – original draft. Elena Navarro: Supervision, Conceptualization,
    Methodology, Writing – review & editing, Project administration, Funding acquisition.
    Pascual González: Supervision, Conceptualization, Methodology, Writing – review
    & editing. Declaration of Competing Interest The authors declare that they have
    no known competing financial interests or personal relationships that could have
    appeared to influence the work reported in this paper. Acknowledgements This paper
    is part of the R+D+i projects PID2019-108915RB-I00 and PID2022-140907OB-I00 funded
    by MCIN/AEI/10.13039/501100011033. It is also funded by the University of Castilla-La
    Mancha and European Regional Development Fund 2022-GRIN-34436), as well as by
    MCINN with European Union Next Generation EU funds (PRTR-C17.I1). Data availability
    Data will be made available on request. Bibliography [1] A. Macías, E. Navarro
    Paradigms for the conceptualization of Cyber-Physical-Social-Thinking hyperspace:
    a thematic synthesis J. Ambient Intell. Smart Environ., 14 (4) (2022), pp. 285-316,
    10.3233/AIS-210492 View in ScopusGoogle Scholar [2] M. Weiser The computer for
    the 21st Century IEEE Pervasive Comput., 1 (2002), pp. 19-25, 10.1109/MPRV.2002.993141
    Google Scholar [3] M. Satyanarayanan Pervasive computing: vision and challenges
    IEEE Personal. Commun., 8 (2001), pp. 10-17, 10.1109/98.943998 View in ScopusGoogle
    Scholar [4] N. Streitz, A. Kameas, I. Mavrommati The Disappearing Computer: Interaction
    Design, System Infrastructures and Applications for Smart Environments, Lecture
    Notes in Computer Science Springer-Verlag, Berlin, Heidelberg (2007) Google Scholar
    [5] B.P.L. Lau, S.H. Marakkalage, Y. Zhou, N.U. Hassan, C. Yuen, M. Zhang, U.-X.
    Tan A survey of data fusion in smart city applications Inf. Fusion, 52 (2019),
    pp. 357-374, 10.1016/j.inffus.2019.05.004 View PDFView articleView in ScopusGoogle
    Scholar [6] R.L. Ackoff From data to wisdom J. Appl. Syst. Anal., 16 (1989), pp.
    3-9 Google Scholar [7] M. Chen, D. Ebert, H. Hagen, R.S. Laramee, R. van Liere,
    K.-L. Ma, W. Ribarsky, G. Scheuermann, D. Silver Data, Information, and Knowledge
    in Visualization IEEE Comput. Graph. Appl., 29 (2009), pp. 12-19, 10.1109/MCG.2009.6
    Google Scholar [8] Y. Liu, L. Zhang, Y. Yang, L. Zhou, L. Ren, F. Wang, R. Liu,
    Z. Pang, M.J. Deen A Novel cloud-based framework for the elderly healthcare services
    using digital twin IEEE Access, 7 (2019), pp. 49088-49101, 10.1109/ACCESS.2019.2909828
    View in ScopusGoogle Scholar [9] L.J. Basile, N. Carbonara, R. Pellegrino, U.
    Panniello Business intelligence in the healthcare industry: the utilization of
    a data-driven approach to support clinical decision making Technovation. (2022),
    Article 102482, 10.1016/j.technovation.2022.102482 Google Scholar [10] D, P.M.
    Talburt, John R. Pullen Evaluating and Improving Data Fusion Accuracy Bossé Éloi
    G.L., Rogova (Eds.), Information Quality in Information Fusion and Decision Making,
    Springer International Publishing, Cham (2019), pp. 295-326, 10.1007/978-3-030-03643-0_14
    Google Scholar [11] L. Yu, Y. Lu, X. Zhu Smart hospital based on internet of things
    J. Networks, 7 (2012), pp. 1654-1661 View in ScopusGoogle Scholar [12] A. Holzinger,
    C. Röcker, M. Ziefle From smart health to smart hospitals Holzinger A., Röcker
    C., Ziefle M. (Eds.), Smart Health: Open Problems and Future Challenges, Springer
    International Publishing, Cham (2015), pp. 1-20, 10.1007/978-3-319-16226-3_1 Google
    Scholar [13] A. Macías, E. Navarro, P. González A microservice-based framework
    for developing internet of things and people applications Proceedings (2019),
    p. 31, 10.3390/proceedings2019031085 Google Scholar [14] Md.M. Islam, S. Nooruddin,
    F. Karray, G. Muhammad Multi-level feature fusion for multimodal human activity
    recognition in Internet of Healthcare Things Inf. Fusion, 94 (2023), pp. 17-31,
    10.1016/j.inffus.2023.01.015 View PDFView articleView in ScopusGoogle Scholar
    [15] M.A. Becerra, C. Tobón, A.E. Castro-Ospina, D.H. Peluffo-Ordóñez Information
    quality assessment for data fusion systems Data (Basel)., 6 (2021), 10.3390/data6060060
    Google Scholar [16] Gartner Inc. and/or its affiliates Guide 4: what role data
    fabric will play in your data management Gartner''s Essential Guides for Effective
    Data Driven Decision Making (2023) https://www.gartner.com/en/information-technology/insights/data-and-analytics-essential-guides
    accessed April 13, 2023 Google Scholar [17] A. Macías, D. Muñoz, E. Navarro, P.
    González Digital twins-based data fabric architecture to enhance data management
    in intelligent healthcare ecosystems Bravo J., Ochoa S., Favela J. (Eds.), Proceedings
    of the International Conference on Ubiquitous Computing & Ambient Intelligence
    (UCAmI 2022), Springer International Publishing, Cham (2023), pp. 38-49 CrossRefView
    in ScopusGoogle Scholar [18] T. Priebe, S. Neumaier, S. Markus Finding Your Way
    Through the Jungle of Big Data Architectures CoRR (2022) abs/2201.0 Google Scholar
    [19] R. Minerva, G.M. Lee, N. Crespi Digital twin in the IoT context: a survey
    on technical features, scenarios, and architectural models Procceedings of the
    IEEE, 108 (2020), pp. 1785-1824, 10.1109/JPROC.2020.2998530 View in ScopusGoogle
    Scholar [20] ISO - ISO/IEC 25040:2011 Systems and software engineering — Systems
    and software Quality Requirements and Evaluation (SQuaRE) Evaluation Process (2023)
    n.d. https://www.iso.org/standard/35765.html accessed March 20, Google Scholar
    [21] ISO; IEC Software engineering — Software product Quality Requirements and
    Evaluation (SQuaRE) Data Quality Model (ISO/IEC 25012:2008(E)), ISO/IEC (2008)
    Google Scholar [22] B. Khaleghi, A. Khamis, F.O. Karray, S.N. Razavi Multisensor
    data fusion: a review of the state-of-the-art Inf. Fusion, 14 (2013), pp. 28-44,
    10.1016/j.inffus.2011.08.001 View PDFView articleView in ScopusGoogle Scholar
    [23] H. Boström, S.F. Andler, M. Brohede, R. Johansson, A. Karlsson, J. van Laere,
    L. Niklasson, M. Nilsson, A.S. Persson, T. Ziemke On the Definition of Information
    Fusion as a Field of Research, ISIF Perspectives On Information Fusion (2007)
    Google Scholar [24] R. Johansson, A. Karlsson, S.F. Andler, M. Brohede, J. van
    Laere, M.K. Nilsson, T. Ziemke On the definition and scope of information fusion
    as a field of research Perspectives (Montclair), 5 (2022), pp. 3-12 http://urn.kb.se/resolve?urn=urn:nbn:se:his:diva-21740
    Google Scholar [25] R.J. Abrahart, L. See Multi-model data fusion for river flow
    forecasting: an evaluation of six alternative methods based on two contrasting
    catchments Hydrol. Earth Syst. Sci., 6 (2002), pp. 655-670, 10.5194/hess-6-655-2002
    View in ScopusGoogle Scholar [26] F. White, Data fusion lexicon (1991). https://api.semanticscholar.org/CorpusID:60241292.
    Google Scholar [27] S. Qiu, H. Zhao, N. Jiang, Z. Wang, L. Liu, Y. An, H. Zhao,
    X. Miao, R. Liu, G. Fortino Multi-sensor information fusion based on machine learning
    for real applications in human activity recognition: state-of-the-art and research
    challenges Inf. Fusion, 80 (2022), pp. 241-265, 10.1016/j.inffus.2021.11.006 View
    PDFView articleView in ScopusGoogle Scholar [28] M.M. Kokar, J.A. Tomasik, J.
    Weyman Formalizing classes of information fusion systems Inf. Fusion, 5 (2004),
    pp. 189-202, 10.1016/j.inffus.2003.11.001 View PDFView articleView in ScopusGoogle
    Scholar [29] D. Lillis On the evaluation of data fusion for information retrieval
    ACM International Conference Proceeding Series (2020), pp. 54-57, 10.1145/3441501.3441506/ASSETS/HTML/IMAGES/FIRE20-4-FIG1.JPG
    View in ScopusGoogle Scholar [30] B. Zitová, J. Flusser Image registration methods:
    a survey Image Vis. Comput., 21 (2003), pp. 977-1000, 10.1016/S0262-8856(03)00137-9
    View PDFView articleGoogle Scholar [31] X. Zhou, W. Liang, K.I.K. Wang, H. Wang,
    L.T. Yang, Q. Jin Deep-learning-enhanced human activity recognition for internet
    of healthcare things IEEE Internet Things J., 7 (2020), pp. 6429-6438, 10.1109/JIOT.2020.2985082
    View in ScopusGoogle Scholar [32] S.G. Dastidar, K. Sambhoos, J. Llinas, C. Bowman
    Performance evaluation methods for data-fusion capable tactical platforms 2005
    7th International Conference on Information Fusion (2005) https://www.academia.edu/71967024/Performance_evaluation_methods_for_data_fusion_capable_tactical_platforms
    accessed March 19, 2023 Google Scholar [33] R.M. Pereira, D. Bertolini, L.O. Teixeira,
    C.N. Silla, Y.M.G. Costa COVID-19 identification in chest X-ray images on flat
    and hierarchical classification scenarios Comput. Methods Programs Biomed., 194
    (2020), Article 105532, 10.1016/J.CMPB.2020.105532 View PDFView articleView in
    ScopusGoogle Scholar [34] S.A. Kashinath, S.A. Mostafa, A. Mustapha, H. Mahdin,
    D. Lim, M.A. Mahmoud, M.A. Mohammed, B.A.S. Al-Rimy, M.F.M. Fudzee, T.J. Yang
    Review of data fusion methods for real-time and multi-sensor traffic flow analysis
    IEEE Access, 9 (2021), pp. 51258-51276, 10.1109/ACCESS.2021.3069770 View in ScopusGoogle
    Scholar [35] P. Jackson, J. Musiak, Boeing fusion performance analysis (FPA) tool,
    in: 2009: pp. 1444–1450. Google Scholar [36] D. Akselrod, R. Tharmarasa, T. Kirubarajan,
    Z. Ding, A.M. Ponsford Multisensor-multitarget tracking testbed IEEE Symposium
    on Computational Intelligence for Security and Defense Applications, CISDA 2009
    (2009), pp. 1-6, 10.1109/CISDA.2009.5356526 Google Scholar [37] A. Raj, J. Bosch,
    H.H. Olsson, T.J. Wang Modelling data pipelines 2020 46th Euromicro Conference
    on Software Engineering and Advanced Applications (SEAA) (2020), pp. 13-20, 10.1109/SEAA51224.2020.00014
    View in ScopusGoogle Scholar [38] D.M. Sundaram, M.S. Vidhya Data lakes-a new
    data repository for big data analytics workloads Int. J. Adv. Comput. Res., 7
    (2016), 10.26483/ijarcs.v7i5.2714 Google Scholar [39] M.M. Alvord, F. Lu, B. Du,
    C.-A. Chen, Big data fabric architecture: how big data and data management frameworks
    converge to bring a new generation of competitive advantage for enterprises, 2020.
    Google Scholar [40] B.R. May Digital Twins Can Help Turn Manufacturing Data into
    Insight Automation World (2022) https://www.automationworld.com/factory/iiot/article/22171856/digital-twins-can-help-turn-manufacturing-data-into-insight
    accessed July 1, 2022 Google Scholar [41] Microsoft Corp., Batch integration with
    Azure Data Factory for Azure Digital Twins, Docs.Microsoft.Com/En-Us/Azure/Architecture/Example-Scenario/Iot/Batch-Integration-Azure-Data-Factory-Digital-Twins.
    (2022). https://docs.microsoft.com/en-us/azure/architecture/example-scenario/iot/batch-integration-azure-data-factory-digital-tw
    (accessed July 1, 2022). Google Scholar [42] H, K.S. Jimenez Jaime Ibarra, Jahankhani
    Health care in the cyberspace: medical cyber-physical system and digital twin
    challenges A, H.-F.A, Farsi Maryam J.H., Daneshkhah (Eds.), Digital Twin Technologies
    and Smart Cities, Springer International Publishing, Cham (2020), pp. 79-92, 10.1007/978-3-030-18732-3_6
    Google Scholar [43] Y. Zheng, S. Yang, H. Cheng An application framework of digital
    twin and its case study J. Ambient Intell. Humaniz. Comput., 10 (2019), pp. 1141-1153,
    10.1007/s12652-018-0911-3 View in ScopusGoogle Scholar [44] A. Macías, E. Navarro,
    C.E. Cuesta, U. Zdun Architecting digital twins using a domain-driven design-based
    approach 2023 IEEE 20th International Conference on Software Architecture (ICSA)
    (2023), pp. 153-163, 10.1109/ICSA56044.2023.00022 View in ScopusGoogle Scholar
    [45] K.Y.H. Lim, P. Zheng, C.-H. Chen A state-of-the-art survey of Digital Twin:
    techniques, engineering product lifecycle management and business innovation perspectives
    J. Intell. Manuf., 31 (2020), pp. 1313-1337, 10.1007/s10845-019-01512-w View in
    ScopusGoogle Scholar [46] I. Errandonea, S. Beltrán, S. Arrizabalaga Digital Twin
    for maintenance: a literature review Comput. Ind., 123 (2020), Article 103316,
    10.1016/j.compind.2020.103316 View PDFView articleView in ScopusGoogle Scholar
    [47] C. Semeraro, M. Lezoche, H. Panetto, M. Dassisti Digital twin paradigm: a
    systematic literature review Comput. Ind., 130 (2021), Article 103469, 10.1016/j.compind.2021.103469
    View PDFView articleView in ScopusGoogle Scholar [48] B.R. Barricelli, E. Casiraghi,
    D. Fogli A survey on digital twin: definitions, characteristics, applications,
    and design implications IEEE Access, 7 (2019), pp. 167653-167671, 10.1109/ACCESS.2019.2953499
    Google Scholar [49] J. Niles, S. McClure, D. Pouliquen, Reality capture: a digital
    twin foundation, 2022. https://www.digitaltwinconsortium.org/wp-content/uploads/sites/3/2022/06/Reality-Capture-A-Digital-Twin-Foundation.pdf
    (accessed September 19, 2023). Google Scholar [50] B. Guo, Z. Yu, X. Zhou A data-centric
    framework for cyber-physical-social systems IT Prof., 17 (2015), pp. 4-7, 10.1109/MITP.2015.116
    View in ScopusGoogle Scholar [51] M.S. Hossain, G. Muhammad, A. Alamri Smart healthcare
    monitoring: a voice pathology detection paradigm for smart cities Multimed. Syst.,
    25 (2019), pp. 565-575, 10.1007/s00530-017-0561-x View in ScopusGoogle Scholar
    [52] Google Inc Google Cloud Computing Hosting Services & APIs | Google Cloud
    (2019) https://cloud.google.com/ accessed February 1, 2019 Google Scholar [53]
    Inc. Amazon.com Amazon Web Services (AWS) - Cloud Computing Services (2019) https://aws.amazon.com/?nc1=h_ls
    accessed March 20, 2019 Google Scholar [54] Microsoft Corp. Microsoft Azure Microsoft
    Azure (2018) https://azure.microsoft.com/ Google Scholar [55] A. Macías, E. Navarro
    An integrated approach for context-aware development Proceedings of the 12th European
    Conference on Software Architecture: Companion Proceedings, ACM, New York, NY,
    USA (2018), p. 47, 10.1145/3241403.3241452 :1–47:7 Google Scholar [56] A. Dey,
    G.D. Abowd, D. Salber A conceptual framework and a toolkit for supporting the
    rapid prototyping of context-aware applications Hum. Comput. Interact., 16 (2001),
    pp. 97-166, 10.1207/S15327051HCI16234_02 View in ScopusGoogle Scholar [57] D.G.D.
    La Iglesia, D. Weyns MAPE-K formal templates to rigorously design behaviors for
    self-adaptive systems ACM Trans. Auton. Adapt. Syst., 10 (2015), pp. 1-31, 10.1145/2724719
    Google Scholar [58] C. Lv, Q. Zhao Integration of data compression and cryptography:
    another way to increase the information security 21st International Conference
    on Advanced Information Networking and Applications Workshops (AINAW’07) (2007),
    pp. 543-547, 10.1109/AINAW.2007.208 View in ScopusGoogle Scholar [59] J.R. Talburt,
    D. Pullen, M. Penning, Evaluating and improving data fusion accuracy, (2019) 295–326.
    https://doi.org/10.1007/978-3-030-03643-0_14. Google Scholar [60] M. Peralta Ascue
    Calidad de datos en sistemas de gestión académica universitaria basado en ISO/IEC
    25012 Interfases (014) (2021), pp. 65-88 Google Scholar [61] D. Muñoz, A. Macías,
    E. Navarro Diseño e Implementación de un Sistema Ciber-físico-social para la Monitorización
    y Prevención de Infecciones por COVID-19 en Residencias de Ancianos XXVI J. Ingen.
    Ciencia Ingen. Serv. (JCIS) (2022) Google Scholar [62] Amavir, Residencia de ancianos
    Amavir Las Hazas Albacete | Amavir, Amavir. Residencias Para Mayores. (2022).
    https://www.amavir.es/residencia-de-ancianos-albacete-amavir-las-hazas/(accessed
    May 4, 2022). Google Scholar [63] I.A. Geisselmann, S.M. Torre, I.G. Molinero,
    E.M. Calahorro, S.L. Milla Formas atípicas de presentación de la enfermedad COVID-19
    observadas en atención primaria FMC, 27 (2020), p. 469, 10.1016/J.FMC.2020.06.002
    View PDFView articleView in ScopusGoogle Scholar [64] Microsoft, Microsoft azure
    well-architected framework - azure architecture center | Microsoft Docs, (2022).
    https://docs.microsoft.com/en-us/azure/architecture/framework/(accessed May 1,
    2022). Google Scholar Cited by (1) Research on establishment of digital-twin system
    for intelligent control of cutting tools sintering process driven by data-model
    combination 2024, Journal of Manufacturing Systems Show abstract © 2023 The Author(s).
    Published by Elsevier B.V. Part of special issue Information Fusion for Ubiquitous
    Computing Edited by Jesus Favela - Ensenada Center for Scientific Research and
    Higher Education, Ensenada, Mexico, Sergio F. Ochoa - University of Chile, Santiago,
    Chile View special issue Recommended articles Global-local fusion based on adversarial
    sample generation for image-text matching Information Fusion, Volume 103, 2024,
    Article 102084 Shichen Huang, …, Shuai Liu View PDF ViTMatte: Boosting image matting
    with pre-trained plain vision transformers Information Fusion, Volume 103, 2024,
    Article 102091 Jingfeng Yao, …, Baoyuan Wang View PDF Incomplete multi-view clustering
    via structure exploration and missing-view inference Information Fusion, Volume
    103, 2024, Article 102123 Ziyu Wang, …, Houbing Song View PDF Show 3 more articles
    Article Metrics Captures Readers: 10 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Information Fusion
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Data fabric and digital twins: An integrated approach for data fusion design
    and evaluation of pervasive systems'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sun X.
  - Ngueilbaye A.
  - Luo K.
  - Cai Y.
  - Wu D.
  - Huang J.Z.
  citation_count: '1'
  description: 'Basket analysis is a prevailing technique to help retailers uncover
    patterns and associations of sold products in customer shopping transactions.
    However, as the size of transaction databases grows, the traditional basket analysis
    techniques and systems become less effective because of two issues in the applications
    of the big data age: data scalability and flexibility to adapt different application
    tasks. This paper proposes a scalable distributed frequent itemset mining (ScaDistFIM)
    algorithm for basket analysis on big transaction data to solve these two problems.
    ScaDistFIM is performed in two stages. The first stage uses the FP-Growth algorithm
    to compute the local frequent itemsets from each random subset of the distributed
    transaction dataset, and all random subsets are computed in parallel. The second
    stage uses an approximation method to aggregate all local frequent itemsets to
    the final approximate set of frequent itemsets where the support values of the
    frequent itemsets are estimated. We further elaborate on implementing the ScaDistFIM
    algorithm and a flexible basket analysis system using Spark SQL queries to demonstrate
    the system''s flexibility in real applications. The experiment results on synthetic
    and real-world transaction datasets demonstrate that compared to the Spark FP-Growth
    algorithm, the ScaDistFIM algorithm can achieve time savings of at least 90% while
    ensuring nearly 100% accuracy. Hence, the ScaDistFIM algorithm exhibits superior
    scalability. On dataset GenD with 1 billion records, the ScaDistFIM algorithm
    requires only 360 s to achieve 100% precision and recall. In contrast, due to
    memory limitations, Spark FP-Growth cannot complete the computation task.'
  doi: 10.1016/j.ipm.2023.103577
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. Basket
    analysis on big data 4. A scalable distributed FIM algorithm 5. Basket analysis
    system in spark 6. Performance and scalability evaluation 7. Conclusions Declaration
    of competing interest Data availability References Show full outline Cited by
    (1) Figures (13) Show 7 more figures Tables (14) Table 1 Table 2 Table 3 Table
    4 Table 5 Table 6 Show all tables Information Processing & Management Volume 61,
    Issue 2, March 2024, 103577 A scalable and flexible basket analysis system for
    big transaction data in Spark☆ Author links open overlay panel Xudong Sun a b,
    Alladoumbaye Ngueilbaye a b, Kaijing Luo a b, Yongda Cai a b, Dingming Wu a b,
    Joshua Zhexue Huang a b c Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.ipm.2023.103577
    Get rights and content Under a Creative Commons license open access Highlights
    • Propose a non-MapReduce scalable distributed frequent itemset mining algorithm
    called ScaDistFIM. • Propose a flexible system based on ScaDistFIM to meet diverse
    business requirements in the era of big data. • Address the flexibility and scalability
    problems, to the extent that rapid analysis on the order of billions of data points
    is achievable. • Proposed algorithm and system theoretically do not encounter
    memory or computational bottleneck. • Proposed method ensures near 100% accuracy
    with less than 10% or even fewer computational time requirements. Abstract Basket
    analysis is a prevailing technique to help retailers uncover patterns and associations
    of sold products in customer shopping transactions. However, as the size of transaction
    databases grows, the traditional basket analysis techniques and systems become
    less effective because of two issues in the applications of the big data age:
    data scalability and flexibility to adapt different application tasks. This paper
    proposes a scalable distributed frequent itemset mining (ScaDistFIM) algorithm
    for basket analysis on big transaction data to solve these two problems. ScaDistFIM
    is performed in two stages. The first stage uses the FP-Growth algorithm to compute
    the local frequent itemsets from each random subset of the distributed transaction
    dataset, and all random subsets are computed in parallel. The second stage uses
    an approximation method to aggregate all local frequent itemsets to the final
    approximate set of frequent itemsets where the support values of the frequent
    itemsets are estimated. We further elaborate on implementing the ScaDistFIM algorithm
    and a flexible basket analysis system using Spark SQL queries to demonstrate the
    system’s flexibility in real applications. The experiment results on synthetic
    and real-world transaction datasets demonstrate that compared to the Spark FP-Growth
    algorithm, the ScaDistFIM algorithm can achieve time savings of at least 90% while
    ensuring nearly 100% accuracy. Hence, the ScaDistFIM algorithm exhibits superior
    scalability. On dataset GenD with 1 billion records, the ScaDistFIM algorithm
    requires only 360 s to achieve 100% precision and recall. In contrast, due to
    memory limitations, Spark FP-Growth cannot complete the computation task. Previous
    article in issue Next article in issue Keywords Big transaction dataFrequent itemset
    miningParallel and distributed computingBusiness basket analysisBasket analysis
    systems 1. Introduction Basket analysis is one of the key analytical tasks in
    retail business (Hossain et al., 2019, Long and Zhu, 2012), and its purpose is
    to discover associations between product items contained in a large number of
    customer shopping transactions (Agrawal et al., 1993, Han et al., 2012). The patterns
    or rules of associations can be used in various applications (Saputra, Rahayu,
    & Hariguna, 2023), e.g., product placement, product sales, product recommendation,
    inventory control, and procurement. Besides the retail industry, basket analysis
    is also used in Finance (Prajapati, Garg, & Chauhan, 2017), Logistics, E-commerce
    (Long & Zhu, 2012), and Bioinformatics (Delgado-Osuna et al., 2020, Naulaerts
    et al., 2015). Associations of product items are represented as frequent itemsets,
    where a frequent itemset is a set of product items that were frequently purchased
    together by many customers (Cheng, Chen, Lee, & Li, 2021). The process of finding
    product associations from customer shopping transactions is called frequent itemsets
    mining ( ). Let be a database containing transactions denoted as , and each transaction
    records a set of items purchased by a customer in a shopping activity. Formally,
    we denote as the set containing all distinct items in and as the set of items
    in a transaction. cannot be empty and is called a itemset if it contains distinct
    items. We denote as the power set of , i.e., the set of all itemsets for . Let
    be a itemset in and the number of transactions containing . is called the support
    of . is called a frequent itemset if is equal to or greater than the given minimal
    support threshold. algorithms for discovering frequent itemsets from transaction
    databases have been developed for over two decades (Luna et al., 2019, Shawkat
    et al., 2022), and the most representative ones are Apriori (Agrawal and Srikant,
    1994, Raj et al., 2020), Eclat (Raj and Ramesh, 2022, Singh et al., 2020), and
    FP-Growth (Shi et al., 2017, Yun et al., 2017). Two strategies are used in these
    algorithms. The first strategy is to search for all frequent itemsets directly
    from the transaction database, e.g., Apriori and Eclat. The drawback of this strategy
    is the step of generating the candidate set of frequent itemsets from the set
    of frequent itemsets. This step usually results in a large candidate set, which
    consumes a lot of memory and takes a long time to verify the true frequent itemsets
    from the transaction database (Wicaksono et al., 2020, Yoon and Lee, 2013). The
    second strategy is to convert the transaction database to a pattern tree and then
    search the tree for all frequent itemsets, e.g., FP-Growth and the fp-tree algorithm
    (Djenouri, Djenouri, Lin, & Belhadi, 2018). This strategy improves the computing
    efficiency because only two scans of the transaction database are required, but
    the pattern tree consumes more memory which limits its ability to process large
    databases. Parallel and distributed FIM algorithms have been developed with a
    goal to process large transaction datasets (Chon and Kim, 2018, Huang et al.,
    2021, Jiang and Meng, 2017, Li, Wang et al., 2008). The divide-and-conquer strategy
    is used to divide a large dataset into subsets and process each subset separately
    on each node of a cluster to generate the local result of the subset. All local
    results are then aggregated to the global result (Sun, He, Wu, & Huang, 2023).
    The popular distributed computing model used to implement parallel and distributed
    FIM algorithms is MapReduce. In this model, the mapper function computes the local
    result, and the reducer function computes the global result. However, MapReduce
    is computationally not efficient in executing iterative machine learning algorithms
    (Sun et al., 2023) due to the data communication overheads in the reducer stage
    (McCreadie, Macdonald, & Ounis, 2012). Therefore, current parallel and distributed
    FIM algorithms in the MapReduce paradigm (Agarwal et al., 2018, Fumarola and Malerba,
    2014, Zheng et al., 2018) are inefficient in processing big datasets with millions
    of transactions, which are frequently encountered in real-life applications (Ragaventhiran
    and Devi, 2020, Sun et al., 2023, Valiullin et al., 2021). For example, distributed
    Apriori and Eclat algorithms in MapReduce suffer the drawbacks of generation of
    candidate frequent itemsets, and of verification of true frequent itemsets in
    the iterations of multiple scans of the big dataset (Raj and Ramesh, 2022, Raj
    et al., 2021, Sreeyuktha and Geetha Reddy, 2019), and the big candidate set can
    easily use up the memory. Distributed FP-Growth algorithms require only two scans
    on the big dataset, but its data repartitioning stage on the item group-list is
    time-consuming, and the sizes of the data repartitions also affect the efficiency
    of the FP-Growth algorithm to mine the FP-trees on the nodes of the cluster. These
    drawbacks limit the data scalability of the distributed FP-Growth algorithms.
    In the big data age, basket analysis in business applications encounters two situations
    as follows: • The volume of transaction data in business grows much faster than
    the capacity of the basket analysis systems to handle adequately. The existing
    FIM algorithms, including the parallel and distributed implementations, are not
    scalable to big transaction data on the terabyte scale with billions of transactions.
    • The business requirements of basket analysis applications keep changing frequently.
    However, the current analytical process of basket analysis is not efficient and
    flexible to deal with varied analytical tasks. The user must always spend additional
    effort analyzing the results and discovering the business’s valuable knowledge.
    The reason is that business-relevant results are categorized with the settings
    on business-related attributes of product items, such as product taxonomy, price,
    location, departments, manager roles, calendars, etc., Vaishampayan, Singh, Hebasur,
    and Kute (2022) whereas the basket analysis systems based on the FIM algorithms
    cannot directly handle this kind of data. In this paper, we propose a new approach
    for basket analysis on big transaction data and a scalable and flexible system
    to meet different business needs in the big data age. In this new approach, we
    divide the current analytical process of basket analysis into two separate stages.
    The first stage is to use a FIM algorithm to convert a big transaction dataset
    to a set of base frequent itemsets named the BI table. For each transaction dataset,
    this stage is only performed once, so the efficiency of the basket analysis process
    is improved significantly. The second stage is to integrate some external data
    of product item attributes with the BI table to generate the extended BI tables
    for flexible basket analysis. The extended BI tables support flexible basket analysis
    tasks for different business needs. We use some business-relevant query examples
    to illustrate the application of this basket analysis system. In doing so, we
    showcase the scalability and flexibility advantages of our approach when compared
    to the state-of-the-art algorithms currently employed in basket analysis applications.
    The basket analysis examples also demonstrate the flexibility of this new approach
    in business applications. To improve the data scalability of the first stage,
    we propose ScaDistFIM, a new distributed FP-Growth algorithm that uses an approximate
    method to convert a big transaction dataset to an approximate set of frequent
    itemsets. The ScaDistFIM algorithm directly runs a sequential FP-Growth algorithm
    on a subset of the big transaction dataset on a node of a cluster to generate
    the local frequent itemsets, and all subsets of the transaction dataset are processed
    by the sequential FP-Growth algorithm in parallel to generate all local frequent
    itemsets. The local frequent itemsets are integrated by a global operation into
    the approximate set of frequent itemsets as the BI table. Since no data communication,
    ScaDistFIM runs much faster than the Spark Distributed FP-Growth algorithm, as
    shown in Section 6. The experiment results also show that ScaDistFIM can generate
    an accurate approximate set of frequent itemsets from a big transaction dataset
    and is scalable to a big dataset with one billion transactions. In summary, our
    work has achieved the following objectives in comparison to existing research:
    • Propose a scalable distributed FIM algorithm, ScaDistFIM, which addresses the
    memory bottleneck faced by existing FIM algorithms when dealing with large transaction
    datasets and exhibits theoretically unbounded scalability because it is a high-precision
    approximate method. • Present a flexible market basket analysis system based on
    the ScaDistFIM algorithm. It generates an extended BI table by integrating the
    BI table with external tables containing business-relevant attributes using SparkSQL
    and UDFs. These BI tables can produce many business-related association rules,
    thus demonstrating strong practicality. The paper is organized as follows. Related
    works are discussed in Section 2. Section 3 briefly reviews basket analysis in
    business, discusses the challenges on big data and presents a new approach to
    tackle the two challenging problems. Section 4 presents the ScaDistFIM algorithm
    and its implementation in Spark. Section 5 introduces the scalable and flexible
    basket analysis system in Spark. Section 6 presents experiment results on both
    synthetic and real world datasets to evaluate the performances of ScaDistFIM on
    big data. Finally, the paper is concluded in Section 7. 2. Related work In this
    section, the basic symbols and abbreviations of this paper are summarized in Table
    1 firstly. Then we briefly review some basket analysis applications in different
    areas and a few parallel and distributed FIM algorithms related to this work.
    Table 1. Notations used in this paper. Notation Description ScaDistFIM Scalable
    distributed frequent itemset mining algorithm FIM Frequent itemsets mining Transaction
    dataset Total number of transactions in The th transaction in Set of distinct
    items contained in Power set of A itemset which is subset of Total number of distinct
    items in Number of items in BI Base frequent itemset TDB Transaction database
    on file system The th partition of Support of (Number of transactions containing
    ) FI The set of frequent itemsets found from The set of frequent itemsets found
    from A frequent itemset in FI A frequent itemset in Relative support values of
    (a percentage number) A serial FIM algorithm Total number of partitions in Number
    of times a local frequent itemset appears in A judgment statement AFI Approximate
    frequent itemset ASE Average support error Number of true frequent itemsets in
    AFI Estimated support of the frequent itemset True support of the frequent itemset
    2.1. Basket analysis applications Besides retail business, basket analysis techniques
    based on FIM algorithms are also used in other areas. In Alawadh and Barnawi (2022),
    basket analysis techniques were used to analyze the dataset of 400 games played
    at FIVB between 2013 and 2016 and discover the rules of tactical trends in the
    game plays of Elite Beach Volleyball. In Shiokawa, Misawa, Date, and Kikuchi (2016),
    association rules were used to discover persons’ metabolic profiles based on their
    dietary intake. Transaction data on diet consumption were collected, and more
    than 5 million rules were discovered from the transaction data. 4000 rules were
    identified with high confidence, which can be used as guidelines to change human
    eating habits for good health. The work in Liew (2018) presents a case that basket
    analysis was used to generate clusters of eating patterns of kids at US schools.
    The clusters were used to make nutrition plans for different kids. In Meida, Rini,
    and Sukemi (2019), an application of basket analysis in hotels was presented,
    where a prominent hotel in Australia used basket analysis to boost sales and promotions
    with a result of the significant increase in sales. These examples demonstrate
    the diverse applications of basket analysis, but the flexibility of conducting
    basket analysis in real business environment was not concerned. 2.2. Parallel
    and distributed FIM algorithms Parallel and distributed FIM algorithms are the
    parallel versions of the sequential FIM algorithms, e.g., Apriori (Agrawal and
    Srikant, 1994, Raj et al., 2020), FP-Growth (Shi et al., 2017, Yun et al., 2017),
    and Eclat (Raj and Ramesh, 2022, Singh et al., 2020). Several excellent survey
    papers discuss the development of the FIM algorithms in different aspects (Agarwal
    et al., 2016, Aggarwal et al., 2014, Fournier-Viger et al., 2017, Gan et al.,
    2019). Here, we only briefly review some recent works on parallel and distributed
    FIM algorithms in MapReduce, which are closely related to our work in this paper.
    The parallel FIM algorithms on HPC (Pramudiono & Kitsuregawa, 2003), multi-cores
    and GPU (Djenouri et al., 2019, Jiang and Meng, 2017) are not discussed. Implementation
    of a distributed Apriori in Hadoop MapReduce is straightforward (Duong et al.,
    2017, Duong et al., 2018, Yimin et al., 2021). Like the sequential Apriori, the
    set of frequent itemsets is computed iteratively in multiple scans to the data
    file (Renjith, Sreekumar, & Jathavedan, 2020), which is time-consuming on Hadoop
    MapReduce platform due to I/O operations and data communications among the nodes.
    For big complex transaction datasets with a large number of unique items, the
    candidate frequent itemsets generation becomes a computation bottleneck (Raj et
    al., 2021, Riondato and Upfal, 2014). A lot of effort has been devoted to the
    parallel implementation of the FP-Growth algorithm (Pramudiono and Kitsuregawa,
    2003, Rochd et al., 2019). To reduce the I/O costs, Spark becomes preferable to
    Hadoop MapReduce in the implementation of distributed FIM algorithms (Fernandez-Basso
    et al., 2023, Li, Wang et al., 2008, Shi et al., 2017, Xun et al., 2021). Unlike
    Apriori, the FP-Growth algorithm is difficult to be implemented in a pipeline
    of mapper and reducer operations. Instead, a data repartition strategy is used
    to divide the transaction dataset into subsets, each containing a group of frequent
    1-itemsets. After repartition, each subset can be independently processed by the
    FP-Growth algorithm to generate frequent itemsets in the subset. Then, the sets
    of frequent itemsets in all subsets are aggregated to the final set of frequent
    itemsets. However, for a big distributed transaction dataset, the data repartition
    operation is time-consuming, and the sizes of the repartitioned subsets are not
    balanced, so the biggest subset takes a long time for the FP-Growth algorithm
    to process. Different methods were used to improve the partitioning process and
    balance the partitioning subsets (Li, Wang et al., 2008, Xun et al., 2016). However,
    the existing distributed FP-Growth algorithms are still not scalable to big transaction
    datasets in billion records. 3. Basket analysis on big data In this section, we
    briefly review basket analysis in retail and the challenges of scalability and
    flexibility in basket analysis applications in the big data age. We propose a
    new scalable and flexible approach for basket analysis on big transaction data.
    3.1. Basket analysis in retail Basket analysis is a technique widely used in retail
    to analyze what combinations of product items are frequently purchased together
    by customers. The product items purchased by a customer in one shopping activity
    are recorded in a shopping transaction record. Since customers use baskets or
    carts at the supermarkets to carry the product items they want to buy, a transaction
    record is metaphorically considered as a basket containing the product items chosen
    by the customer. Therefore, basket analysis is a data analysis task to find out
    the associations of the sold products in a large number of customer shopping transaction
    records. Basket analysis is fundamental in many business applications. For example,
    it can help predict the product items the new coming customers may buy and suggest
    marketing strategies (Liu, Huang, & Lin, 2018). Therefore, basket analysis is
    an important business endeavor in retail companies around the world (Alawadh and
    Barnawi, 2022, Patwary et al., 2021). As the transaction data grows at a rapid
    speed, basket analysis becomes a technically challenging task (Ünvan, 2021). A
    number of algorithms were developed to discover all baskets satisfying some given
    conditions. In data analysis, baskets are defined as itemsets and valuable itemsets
    are considered as those that occurred frequently in many transactions in the transaction
    dataset in question. The importance of an itemset is measured as support which
    is the number of transactions containing the itemset in the transaction dataset.
    An itemset whose support is equal to or larger than a given support threshold
    is called a frequent itemset. The first task of basket analysis is to find out
    all frequent itemsets from the given transaction database. However, frequent itemsets
    are not directly useful to businesses without other evaluations on business values,
    especially when the total number of frequent itemsets is very large given a large
    transaction database (Hedrick et al., 2022, Tatiana and Mikhail, 2018). Further
    analysis tasks are required to perform on the frequent itemsets to answer business-related
    queries. For example, if itemset “A, B” is a frequent itemset, a business-relevant
    query can be: “If a customer purchases product A, what is the probability that
    the customer will also purchase product B”. This query is called an association
    rule, written as . Finding all association rules from a given transaction dataset
    is called association rule mining (ARM) (Ünvan, 2021). By integrating more attribute
    data of product items to the frequent itemsets, such as the taxonomy, prices,
    shop floor locations, on sale items and inventory status of items, more business-relevant
    queries can be answered from the integrated data of frequent itemsets, such as
    the top 10 most valuable baskets in the food section, or the products with high
    prices sold together with the products on sale. This extensive basket analysis
    is valuable in several business applications, such as product localization, on
    sale promotion, inventory control, online recommendation, etc (Patron and Gomez,
    2020, Pradana et al., 2022, Vaishampayan et al., 2022). Indeed, other researchers
    have also proposed utility-oriented pattern mining approaches (Li, Yeh et al.,
    2008, Shen et al., 2002). For example, Gan et al. (2021) advocates a utility-oriented
    perspective to discover patterns with greater commercial significance. Download
    : Download high-res image (189KB) Download : Download full-size image Fig. 1.
    The analytical process of basket analysis in business. Fig. 1 illustrates the
    analytical process of basket analysis in business practice. The transaction dataset
    is extracted from the database TDB and loaded to the FIM algorithm. The FIM algorithm
    processes the transaction dataset and generates the set of frequent itemsets with
    respect to the given support threshold. The ARM algorithm processes the set of
    frequent itemsets and generates the set of association rules with respect to the
    given confidence threshold. The business user evaluates the association rules
    and identifies the business valuable ones for use in business process. This process
    repeats with different settings of support and confidence thresholds until business
    requirements are satisfied with the found association rules. 3.2. The challenges
    of basket analysis in big data age In real-world applications, the FIM algorithms,
    such as Apriori (Raj et al., 2020), Eclat (Raj and Ramesh, 2022, Singh et al.,
    2020), and FP-Growth (Shi et al., 2017, Yun et al., 2017), as well as their parallel
    and distributed implementations (Dhanabhakyam and Punithavalli, 2011, Yimin et
    al., 2021), are used to find frequent itemsets and association rules from companies’
    transaction databases. The analytical process for basket analysis is effective
    in business use if the transaction dataset to be processed is not large. However,
    when encountering a big transaction dataset, this process faces the following
    technical and operational handicaps in business practice: • Technically, the current
    FIM algorithms are not scalable to extremely big transaction datasets, e.g., in
    terabyte-scale with billions of transactions (Saggi & Jain, 2018).Obviously, it
    is impractical to conduct large-scale data mining on a single machine. Distributed
    frequent itemset mining (FIM) algorithms have, to some extent, improved the scalability
    of data mining. Nevertheless, they still encounter memory bottlenecks, as when
    dealing with exceedingly large transaction datasets, distributed clusters may
    not be able to accommodate all transaction data and the intermediate variables
    generated during computing. • The existing distributed FIM algorithm is inefficient.
    One reason is that there is still space for optimization in the design of distributed
    algorithms. Another main reason is that existing distributed algorithms mainly
    rely on the MapReduce paradigm. It divides the distributed FIM algorithm into
    multiple subtasks that can be executed in parallel, and then executes these distributed
    subtasks in sequence. This means frequent communication overhead between worker
    nodes and the master node. These overheads will also limit the efficiency of the
    distributed FIM algorithm. • Operationally, basket analysis tasks become diverse.
    The current process is not flexible to meet the diverse requirements of basket
    analysis in business. Repeatedly executing the FIM algorithm on the big transaction
    dataset wastes time and computing resources. Therefore, a flexible approach is
    required to facilitate basket analysis on big transaction datasets for diverse
    business tasks. To solve these problems in basket analysis, new FIM algorithms
    and basket analysis systems need to be developed. New FIM algorithms need to have
    excellent scalability and computational efficiency. The basket analysis system
    needs to provide more association rules with commercial value. 3.3. A new approach
    of basket analysis for big data Here, we propose a new approach for basket analysis
    on big transaction data. In this approach, the process of basket analysis in Fig.
    2 is divided into two stages. In the first stage, a algorithm is used to convert
    a big transaction dataset to a base set of frequent itemsets saved as a BI table.
    The BI table contains all frequent itemsets in the transaction dataset that meet
    the requirements of the diverse basket analysis tasks. In the second stage, the
    BI table is extended with other attribute data of the items to the extended BI
    tables to support diverse basket analysis tasks. Given the expanded BI tables,
    basket analysis tasks can be carried out using SQL queries. As shown in Fig. 2,
    the left box indicates the operation of the first stage that converts a big transaction
    dataset to a BI table by a FIM algorithm. This stage is performed once on each
    transaction dataset. The right box represents the operations in the second stage.
    These operations involve integrating the BI table with external attribute data
    of items to create extended BI tables. Subsequently, employing an SQL tool to
    conduct a variety of basket analysis on the extended BI tables, producing business-relevant
    results. In this new approach in Fig. 2, after the BI table is created in stage
    one, the basket analysis operations are performed on the extended BI tables using
    SQL queries. A data model is defined to specify the possible queries that can
    be performed in basket analysis. The data model can be enriched with more attribute
    data such as product taxonomy, product price, product shop floor locations, on
    sale product items, etc., to enable diverse basket analysis tasks and answer business-relevant
    queries, such as the top 10 most valuable baskets in the food section, or the
    products with high prices sold together with the products on sale. The second
    stage can be implemented in a database system in which these analytical tasks
    can be efficiently performed (Jashma Suresh, Dinesh Acharya, & Reddy, 2023). In
    this work, we use Spark system to implement the second stage which gains more
    performance in the distributed computing platform. The technical challenge in
    this approach is the first stage which requires an efficient and scalable FIM
    algorithm to convert extremely big transaction datasets in terabyte-scale to the
    BI tables. In the next section, we will introduce a new scalable distributed FIM
    algorithm for this purpose. Download : Download high-res image (215KB) Download
    : Download full-size image Fig. 2. The two stage process of the new approach for
    basket analysis. 4. A scalable distributed FIM algorithm In this section, we propose
    ScaDistFIM, a scalable distributed FIM algorithm that uses an approximate FIM
    method to compute the BI table from a big transaction dataset. We first introduce
    an approximate FIM method that uses the divide-and-conquer strategy. This method
    differs from the classic FIM algorithm (such as Apriori (Agarwal et al., 2018))
    that directly mines a complete set of frequent itemsets from the entire dataset.
    Instead, it finds approximate sets of frequent itemsets from large transaction
    datasets to gain data scalability. Afterward, a distributed FP-Growth for approximate
    FIM is described as the FIM method in ScaDistFIM. Furthermore, the algorithms
    of implementation of ScaDistFIM in Spark are represented. Finally, the complexity
    and scalability of the ScaDistFIM algorithms are discussed. 4.1. Approximate FIM
    method The classic FIM algorithms, such as Apriori (Raj et al., 2020), Eclat (Raj
    and Ramesh, 2022, Singh et al., 2020) and FP-Growth (Shi et al., 2017, Yun et
    al., 2017), compute the complete set of frequent itemsets satisfying the conditions
    of the minimal support and the maximal number of items from a transaction dataset.
    The parallel and distributed versions of these FIM algorithms, such as Jiang and
    Meng, 2017, Li, Wang et al., 2008, Zhou et al., 2010 and Xun et al. (2016), were
    also designed with the goal of computing the complete set of frequent itemsets
    from a distributed transaction dataset. However, in mining big transaction datasets,
    existing parallel and distributed FIM algorithms suffer two problems, i.e., they
    are inefficient and not scalable (Zhang et al., 2015) due to communication overheads
    and memory limits in the nodes of a computing cluster. To increase the efficiency
    and scalability of the distributed FIM process, a method was proposed in Valiullin
    et al. (2021) to use a divide-and-conquer strategy to compute an approximate set
    of frequent itemsets from a big transaction dataset, instead of the complete set
    of frequent itemsets from the same data set using the same minimal support threshold.
    The difference between the approximate set and the complete set is that the former
    contains some false positive frequent itemsets but misses some false negative
    frequent itemsets. A false positive frequent itemset is an itemset whose support
    value is slightly smaller than the minimal support threshold but which is identified
    as a frequent itemset by the approximate FIM method. A false negative frequent
    itemset is a true frequent itemset ignored by the approximate FIM method, even
    whose support value is slightly larger than the minimal support threshold. The
    approximate FIM method partitions a transaction dataset into subsets of equal
    sizes, and the subsets are made as random samples of the transaction dataset (Mahmud
    et al., 2023, Valiullin et al., 2021). Let be a partition of transaction dataset
    , and be the set of frequent itemsets found from with a FIM algorithm using a
    relative minimal support . Let be the set of frequent itemsets from subset using
    the same algorithm and . Assume is a frequent itemset in and is a frequent itemset
    in , i.e., and . Assume and are the relative support values of and , respectively.
    The random sample and the transaction dataset have the following property (Mahmud,
    Huang, Salloum, Emara, & Sadatdiynov, 2020) that if and are the same itemset,
    their relative support values are very close (Fan, Han, Wang, & Xie, 2022), i.e.,
    (1) Using this data partition property, we can compute the approximate set of
    frequent itemsets from using a FIM algorithm as follows. • Set the same relative
    minimal support threshold and the same maximal itemset for both subsets and the
    whole transaction dataset. • For each subset , use to compute the set of frequent
    itemsets, i.e., for ; • Use the popular vote method to determine the frequent
    itemsets in the approximate set as (2) where and (3) • For each , its support
    value is estimated as (4) where is the relative support value of frequent itemset
    . In Eq. (2), is indispensable. It signifies the degree to which the notion that
    “an itemset is a global frequent itemset” is endorsed by the tables of local frequent
    itemsets. The set of local frequent itemsets is discovered from independently
    and identically distributed data partitions. Consequently, embodies an application
    of the voting principle of “minority follows majority”. As evident from Eq. (4),
    when a frequent itemset is present in all local frequent itemset tables, the ScaDistFIM
    algorithm can compute the true support of that frequent itemset. When a local
    frequent itemset appears on the majority of data partitions, the ScaDistFIM algorithm
    can compute approximate support for that frequent itemset, and this approximate
    support is a little high. Clearly, for the majority of high-support frequent itemsets,
    Eq. (4) can calculate the exact support. For a small number of frequent itemsets
    with support values very close to the threshold, Eq. (4) will yield approximate
    support with a certain margin of error. Due to the similarity in distributions
    across data partitions, this error is entirely acceptable. Experiment results
    have shown that the error is typically around 1%. In summary, the main advantage
    of the approximate FIM method is that it uses the divide-and-conquer strategy
    to improve the data scalability of the FIM process. Partitioning a transaction
    dataset into subsets of random samples enables each subset to be processed efficiently
    by a FIM algorithm to generate local frequent itemsets. After all local frequent
    itemsets are discovered, the set of global frequent itemsets for the whole dataset
    can be efficiently identified with the method of Eqs. (2), (4). The disadvantage
    is that the final set of approximate frequent itemsets ( ) contains some false
    positive frequent itemsets and misses some true positive frequent itemsets. However,
    the random sample partition of the big transaction dataset makes the percentage
    of these itemsets minimal, and the set of approximate frequent itemsets can satisfy
    real application requirements. 4.2. Distributed FP-growth algorithm for approximate
    FIM In this work, we choose FP-Growth (Shawkat et al., 2022) as the FIM algorithm
    because of its efficiency. In this approximate method, since the set of local
    frequent itemsets in each subset is computed independently, the sequential FP-Growth
    algorithm is used to run in parallel to mine the local frequent itemsets on the
    distributed computing platform. Finally, we use Eqs. (2), (4) to compute the final
    approximate set of frequent itemsets. Fig. 3 shows the process of distributed
    computing for mining the approximate set of frequent itemsets using the sequential
    FP-Growth algorithm. This process consists of three main steps. • Step 1: A transaction
    dataset is partitioned into random subsets with a random sample partition algorithm
    (Wei et al., 2018). All these subsets have the property (1). • Step 2: Each subset
    of is loaded to a node or a virtual machine, and processed in parallel and independently,
    without data communication between nodes, by the sequential FP-Growth algorithm
    with the same support threshold. The outputs of this step are a set of frequent
    itemsets computed from , respectively. • Step 3: Use Eqs. (2), (4) to integrate
    into the final set of approximate frequent itemsets . From these steps, it is
    evident that the ScaDistFIM algorithm computes local frequent itemset tables on
    independently and identically distributed RSP data blocks. Then, it utilizes an
    approximate computing method to integrate these local frequent itemset tables
    into a global frequent itemset table. The computing from data blocks to local
    frequent itemset tables is performed sequentially within a single node, without
    inter-node communication. This process requires only a minimal amount of memory
    to store local frequent itemset tables. During this process, the primary computing
    constraints arise from the frequent itemset mining of individual RSP data blocks.
    It indicates that, even for immensely large transaction datasets, as long as individual
    RSP data blocks can be mined, the ScaDistFIM algorithm can be successfully executed.
    Moreover, the size of an individual RSP block is controllable and typically small.
    In contrast to other distributed FIM algorithms, it needs to execute a series
    of divided subtasks in sequence, and the variables between subtasks have mutual
    dependencies. The intermediate variable needs to be calculated on the entire dataset
    and stored in memory. They require simultaneous computing on the entire dataset.
    This implies that the size of the complete dataset determines the computing constraints.
    Hence, the ScaDistFIM algorithm effectively addresses memory bottlenecks and demonstrates
    exceptional computing efficiency. Download : Download high-res image (279KB) Download
    : Download full-size image Fig. 3. Three steps of the approximate distributed
    FP-Growth process. In summary, Comparing to other parallel and distributed FP-Growth
    algorithms (Fernandez-Basso et al., 2023), this distributed FP-Growth algorithm
    is much more efficient and scalable to big transaction data because of the following
    reasons: • If the transaction records of are randomly distributed (Salloum, Huang,
    & He, 2019), subsets with property (1) can be created by simply cutting . If the
    transaction records are not randomly ordered, a random sample partition algorithm
    (Wei et al., 2018) is used to generate subsets with property (1). For a big dataset,
    this step can take some time. With the algorithm in Wei et al. (2018), generating
    a random sample partition of a big dataset in 1 terabyte size takes less than
    1 h. However, this step is only performed once on a big transaction dataset. •
    Unlike other parallel and distributed FP-Growth algorithms, the second step is
    efficient. Its execution time is equivalent to the execution time of the sequential
    FP-Growth algorithm running on a single machine, because each sunset is much smaller
    than the local repartitioned datasets in the distributed FP-Growth algorithms.
    We can make the subsets smaller to increase the computing efficiency in this step.
    • Step 3 is obviously efficient because of its simple operation. This algorithm
    is scalable because after the big dataset is partitioned into random subsets,
    there will be no memory limit to generate . More subsets just require more virtual
    machines to process, and the subsets can also be processed in batches if their
    total number is larger than the maximal number of virtual machines available.
    4.3. ScaDistFIM algorithm in spark The above-mentioned distributed FP-Growth algorithm
    for approximate FIM is implemented in Spark, and the implementation is named ScaDistFIM
    Algorithm. The algorithm is executed in three steps: • Generate the random sample
    subsets of the input transaction dataset with operator toRSP and save the subsets
    in a distributed file in . • Compute the local sets of frequent itemsets from
    the subsets with operator logoFPGrowth. • Integrate the local sets of frequent
    itemsets into the global set of approximate frequent itemsets with operator PopularVote,
    and then save the final set in a BI table as a distributed file in . The operators
    toRSP, logoFPGrowth and PopularVote are internal operators implemented in Spark,
    and they can be called in Spark applications. Operator toRSP is an implementation
    of the random sample partition generation algorithm in Wei et al. (2018). The
    operator toRSP performs the first step to convert a distributed transaction data
    file to a set of random sample subsets, which are saved as a new distributed data
    file for computing the BI table. For each input transaction dataset, this operator
    is only performed once and in an offline manner. The second step is performed
    by operator logoFPGrowth. As mentioned above, as a sequential FP-Growth algorithm
    is run on each random subset independently to compute the set of local frequent
    itemsets, the Java code of the FP-Growth algorithm in Smile1 is chosen as the
    algorithm for this operator. For standardization of machine learning operators
    in operator names and syntax, we wrap the FP-Growth algorithm as operator logoFPGrowth
    in Algorithm 1. fpgrowth is the function name in the Java FP-Growth code in Smile.
    The fpgrowth function code is imported to Spark so it can be executed. candidateTable
    is an which stores all local frequent itemsets generated by logoFPGrowth. Here,
    the wrapper function dataWrapper() is used to convert partitionData to the format
    required by fpgrowth(). Download : Download high-res image (179KB) Download :
    Download full-size image Unlike the distributed FP-Growth algorithm in Spark,
    which rewrote the FP-Growth algorithm in MapReduce style in Scala, the operator
    logoFPGrowth executes the sequential FP-Growth algorithm in each executor to process
    one partition of the input data RDD, i.e., one subset of the transaction dataset,
    to generate all local frequent itemsets in the partition. The set of the local
    frequent itemsets is saved in a partition of the candidate frequent itemsets RDD.
    The operator logoFPGrowth is dispatched to local executors by operator . The sequential
    fpgrowth is executed in parallel and independently in the nodes of the cluster.
    The third step is performed by the operator PopularVote. This operator uses Eqs.
    (2), (4) to compute the final set of approximate frequent itemsets and saves it
    as a BI table. Algorithm 2 shows the implementation of the operator PopularVote
    in Spark. The input data to this operator is the candidateTable RDD that stores
    all local frequent itemsets from operator logoFPGrowth. This implementation calls
    Spark internal operators map, reduceByKey and filter to compute according to Eqs.
    (2), (4). Download : Download high-res image (387KB) Download : Download full-size
    image Fig. 4 illustrates the computation process of the operator PopularVote in
    MapReduce fashion. Initially, the locally computed candidate itemset table employs
    the map operator for a structural transformation aimed at tallying the vote count
    (columns 1 to 2). Then, the reduceByKey operator aggregates the multiple candidate
    itemset subsets into a singular table and groups them by itemsets (columns 2 to
    3). The summation of the vote values indicates the total support of all subsets
    containing the frequent itemsets, and the integers in the middle table of Fig.
    4 are the number of subsets containing the frequent itemsets. Subsequent utilization
    of the filter operator removes the itemsets occurring in a number of subsets smaller
    than the given threshold (columns 3 to 4). Ultimately, employing the map operator
    once again, the average support value of the remaining identified frequent itemsets
    is computed as the estimated support values of the frequent itemsets saved in
    the BI table (columns 4 to 5). Algorithm 3 shows the implementation of the algorithm
    in Spark. We assume that the input dataset is already in the form of random sample
    subsets generated by the toRSP operator. Therefore, the random sample subsets
    are directly loaded to the RDD transactionDataset in line 1. Then, operator logoFPGrowth
    is performed on transactionDataset to generate local frequent itemsets and save
    them in RDD candidateTableRDD. Finally, the operator PopularVote is called to
    process candidateTableRDD and generate the final set of approximate frequent itemsets
    saved in the BI table in RDD biTableRDD. Here, LO and GO are two general operators
    of the LOGO distributed computing framework (Sun et al., 2023). LO operator specifies
    the local operation stage on each partition of the RDD. The operator inside LO
    is executed on each partition of the RDD in parallel. The operator inside GO is
    executed on the RDD to compute the global result. Download : Download high-res
    image (184KB) Download : Download full-size image Download : Download high-res
    image (282KB) Download : Download full-size image Fig. 4. The detailed implementation
    steps of operator PopularVote to estimate the support values of the frequent itemsets
    in the BI table. The algorithm can be compiled as a standalone JAR package which
    can be executed in a Web interface. The business users can run it easily on different
    transaction datasets with different input parameters to generate BI tables for
    flexible basket analysis tasks to be discussed in the next section. 4.4. Efficiency
    and scalability As shown in Algorithm 3, starting from the distributed data file
    of random sample subsets, the Algorithm computes the BI table in two stages under
    the LOGO distributed computing framework (Sun et al., 2023). The efficiency of
    the LO stage depends on the efficiency of the node to execute the sequential FP-Growth
    algorithm on an RDD partition that stores a random sample subset of transactions.
    If memory is sufficient, the efficiency is mainly affected by the unique items
    and the minimal support threshold because a large unique item number and a small
    support threshold will result in a large FP-tree to process. We usually set a
    small support threshold to compute the local frequent itemsets for building a
    large BI table in order to satisfy different basket analysis tasks. For this purpose,
    we can reduce the size of random sample subsets or increase the minimal support
    threshold to improve the efficiency in the LO stage. The GO stage of the PopularVote
    operator only requires one shuffle operation on candidateTableRDD to compute the
    BI table so it can be executed efficiently. Therefore, the ScaDistFIM Algorithm
    in Spark scales to big data. 5. Basket analysis system in spark In this section,
    we present a basket analysis system implemented in Spark to enable users to conduct
    basket analysis tasks flexibly using Spark SQL. The data model is introduced first,
    which associates the BI table, generated by ScaDistFIM from a big transaction
    dataset, with other attribute data of items, such as price, class, shop floor
    location, inventory information, etc. Then, the system is presented with three
    function modules for the BI table generation, data integration and flexible basket
    analysis. Finally, we use some query examples to show the flexibility of the system
    in supporting basket analysis tasks. 5.1. Data model The table generated by the
    algorithm from a big transaction dataset only supports discoveries of important
    frequent itemsets and association rules. To extend the analysis tasks on the BI
    table, we can integrate more attribute data of items to the BI table to generate
    extended BI tables for diverse analysis on frequent itemsets to answer different
    business-related queries. For this purpose, we define a data model, as shown in
    Fig. 5, for integration of the BI table with other external data to generate extended
    BI tables for supporting diverse business basket analysis tasks. Typically, the
    external data are objectively existing entity tables. For instance, as shown in
    Fig. 5, it presents the field names of the external table. The data provider could
    provide these tables, which can be obtained directly. For example, a supermarket
    can provide product name(field item), product price (field price), as well as
    information regarding the product’s category and brand(field category and brand).
    The data model in Fig. 5 consists of three components, the BI table with the attributes
    of ¡itemsets, support¿, the external table of other attributes of items, and the
    extended BI tables with different attributes of frequent itemsets. The general
    key in this data model is the index of the frequent unique items. Its support
    is greater than the minimal threshold used to generate the BI table from the transaction
    dataset by ScaDistFIM. In different analysis tasks, new minimal thresholds for
    different business requirements can be specified to filter out the required frequent
    itemsets to reduce the BI table and speed up the query process. Through the general
    key, the extended BI tables can be generated by integrating the BI table with
    the external table. In practice, the external table with item attributes is extracted
    from the application system that manages the business transactions. What attributes
    are included in the external table depends on real business requirements for basket
    analysis tasks. In reality, the data model for flexible basket analysis can be
    implemented in a database system in which the extended BI tables can be generated
    by SQL queries. Many basket analysis tasks can also be conducted by SQL queries.
    However, some special analysis tasks, such as the top valuable baskets in some
    departments of a supermarket or the top association rules, cannot be easily calculated
    by SQL queries. Special algorithms may be required to run on the extended BI tables
    for these tasks. In this work, we present an implementation of the flexible basket
    analysis system in Spark. Download : Download high-res image (337KB) Download
    : Download full-size image Fig. 5. Data model for building extended BI tables
    to support flexible basket analysis. 5.2. Basket analysis system in spark Fig.
    6 shows the architecture of the basket analysis system implemented in Spark. The
    system has three functional modules: 1. Generation Module for generating the BI
    table from transaction data. 2. Integration Module for integrating the BI table
    with the external tables to generate extended BI tables. 3. Analysis Module for
    flexible basket analysis. The storage layer stores all related data on the disks
    of the nodes of the cluster in HDFS files, including the transaction datasets,
    the BI tables, the external tables, the extended BI tables and the results of
    basket analysis tasks. In practice, the transaction datasets and the external
    tables can be prepared outside Spark and loaded to the storage layer, or prepared
    with Spark. Download : Download high-res image (354KB) Download : Download full-size
    image Fig. 6. Flexible basket analysis system in Spark implementation. The engine
    layer illustrates three types of operations in three modules. After the transaction
    datasets and the external tables are in place, the user runs the ScaDistFIM algorithm
    in a Spark application on a transaction dataset to generate the BI table. For
    each transaction dataset, this operation is only required to perform once, and
    later operations for basket analysis on the transaction dataset are performed
    on the BI table and the corresponding external tables. Compared with the current
    methods, which repeatedly operate on transaction datasets and consume a lot of
    time, this system design saves a lot of computing time. After the BI table is
    generated, according to the data model and the requirements of basket analysis
    tasks, the user runs Spark SQL query applications to generate extended BI tables.
    These operations can be efficiently performed, and they are only performed once
    for specific basket analysis tasks. Finally, after the extended BI tables are
    available, the user writes Spark SQL applications to carry out basket analysis
    tasks. The user can also write Spark applications to perform complex basket analysis,
    such as the top k type of queries. For repeated queries, these applications can
    be performed in a Web interface for ordinary users to run them with different
    integrated files and parameters. 5.3. Basket analysis queries Spark SQL is an
    important application tool for structured data processing and analysis. It provides
    a high-level API called , for handling structured data. In the basket analysis
    system, after the BI table is generated, all later analysis operations are based
    on tabular structures. These table-based operations leverage Spark SQL to quickly
    and conveniently perform data querying and processing operations. Developers can
    use the simple and familiar SQL language for data operations while benefiting
    from Spark’s distributed computing capabilities and optimizer to enhance query
    performance. All integration operations can be accomplished using Spark SQL, making
    these operations scalable to datasets with millions of records. For example, data
    providers provide an external table. It contains multiple fields such as item,
    price, category, etc. Executing the query “select FI from ExtendedBITable where
    TotalValue(FI) 1000” in Spark SQL will return the frequent itemsets whose total
    values are greater than 1000. Here, BITable is an extended BI table that contains
    item prices. TotalValue(FI) is a UDF (User-Defined Function) to compute the total
    value of a frequent itemset. The UDF function is programmable and can retrieve
    the price of each item in the frequent itemset from the DataFrames of the external
    table. More query examples can be executed, such as: • “select FI from ExtendedBITable
    order by totalPrice desc limit 10” returns the top 10 baskets with the highest
    total price in the extended BI table where totalPrice is an attribute of frequent
    itemsets in ExtendedBITable. • “select sum(totalPrice) from ExtendedBITable where
    array_contains(itemset, ‘banana’)” returns the total value of all baskets containing
    item ‘banana’. These query examples show that the basket analysis system supports
    flexible basket analysis queries. 6. Performance and scalability evaluation In
    this section, we use experiment results to demonstrate the performance and scalability
    of the algorithm in computing the BI table from a big transaction dataset. The
    experiments were conducted on both real-life and synthetic transaction datasets.
    We first present the experiment settings, including the datasets, the cluster
    platform and Spark configuration to execute . Then, we show the evaluations of
    the experiment results on computing efficiency, result accuracy and data scalability.
    We compare the results of ScaDistFIM with Spark internal distributed FP-Growth
    algorithm which is a Spark implementation of the FP-Growth algorithm in Dahdouh,
    Dakkak, Oughdir, and Ibriz (2019). The comparison results show that is more efficient
    and much more scalable to big data than Spark FP-Growth. 6.1. Experiment settings
    Datasets. Table 2 lists the seven transaction datasets used in the experiments.
    The first six datasets were used to evaluate the computing efficiency of the ScaDistFIM
    algorithm and the accuracy of the approximate set of the frequent itemsets . The
    last big dataset was used to evaluate the scalability of the algorithm. The first
    four datasets are real-life transaction data representing the distributions of
    frequent itemsets in real-world applications. Kaggle, Kosarak, T10I4D100K and
    T40I10D100K are publicly accessible from the corresponding Websites. TH1 and TH2
    are also two real datasets from a supermarket in China. We open-source them after
    desensitization. Since there are no publicly available transaction datasets in
    terabyte scale, we generated the synthetic dataset GenD to test the scalability
    of the algorithm. Cluster Platform. The experiments were conducted on a cluster
    with 32 nodes. Among them, 24 nodes use Intel Xeon E5-2650@2.10 GHz CPUs with
    16 cores, 128 GB memory and 1 TB disk. The other 8 nodes use two Intel Xeon E5-2630@2.6
    GHz CPUs, each with 12 cores, 128 GB memory and 1 TB disk. The operating system
    is 64-bit Ubuntu 14.04. The main software tools installed are Spark-2.4.0, YARN
    3.0.0, Hadoop-2.6.0 and Scala-2.11.8. The maximal number of executors was set
    as 100, and the total available memory was 1.6 TB. Each executor was configured
    with 16 GB memory to facilitate the execution of ScaDistFIM. The reason is that
    the FP-Growth algorithm requires enough memory to build the FP-Tree and traverse
    the tree recursively to mine the set of local frequent itemsets. Table 2. Datasets
    and their characteristics. Dataset Number of transactions Number of items Size
    of data file Kagglea 3 214 874 49 688 67.67 MB Kosarakb 990 002 41 270 15.94 MB
    TH1c 2 657 818 824 38.15 MB TH2d 26 496 645 824 380.45 MB T10I4D100Ke 100 000
    870 1.37 MB T40I10D100Kf 100 000 942 4.96 MB GenDg 1000 27.7 276.9 GB a https://www.kaggle.com/competitions/instacart-market-basket-analysis/data.
    b https://www.kaggle.com/datasets/alexkever/th1forfim. c http://fimi.uantwerpen.be/data/kosarak.dat.
    d https://www.kaggle.com/datasets/alexkever/th2forfim. e http://fimi.uantwerpen.be/data/T10I4D100K.dat.
    f http://fimi.uantwerpen.be/data/T40I10D100K.dat. g This datasets was generated
    with a total of 10 equal differences. Spark Platform. Spark system was used as
    the software platform to implement this system. On the cluster, the files of transaction
    datasets, the BI tables and other related datasets are managed by HDFS (Sreeyuktha
    and Geetha Reddy, 2019, Yang et al., 2023). Internally, the Spark RDD data structures
    are used to hold the data blocks in the memory of the cluster where each random
    subset of transactions is stored in a partition of an RDD (Jain, Boyapati, Venkatesh,
    & Prakash, 2022). The ScaDistFIM algorithm is executed under the LOGO computing
    framework in two stages of the new non-MapReduce computing paradigm (Sun et al.,
    2023). The first stage executes operator logoFPGrowth in Algorithm 2 in the nodes
    of the cluster in parallel and saves the results of local frequent itemsets in
    candidateTableRDD. The second stage performs operator PopularVote on candidateTableRDD
    to generate the final set of approximate frequent itemsets which are saved in
    a file. Experiment Description. We select the open-source distributed FP-Growth
    algorithm from the Spark mllib(machine learning library) as the baseline. In the
    experimental environment, both Spark-FP-Growth and ScaDistFIM were configured
    with identical physical computing resources, including memory capacity, the number
    of cores, and distributed parameters (the number of executors and resource allocation
    per executor). We conducted a comparative speed experiment by measuring the time
    taken by both algorithms to mine frequent itemsets with the same cluster environment
    and support threshold. Spark-FP-Growth is an exact statistical algorithm capable
    of mining genuine statistical results. As a point of comparison, we calculated
    the accuracy of the results obtained by the ScaDistFIM algorithm. Specific metrics
    and formulas will be presented in Section 6.2. Table 2 provides open-source URLs
    for the experimental datasets. Furthermore, We have open-sourced the experimental
    code associated with this manuscript on GitHub2 so that other researchers can
    use and replicate it. 6.2. Evaluation measures Six measures are used to evaluate
    the experiment results as follows. 1. Time. Execution time in seconds measures
    the computing efficiency of the algorithm on a big dataset. 2. False Negatives
    (FN). The number of true frequent itemsets missed in the final set of approximate
    frequent itemsets by . 3. False Positives (FP). The number of false frequent itemsets
    that were found by as the true frequent itemsets. This value reflects the mining
    capability of . 4. Precision. (5) where is the set of true frequent itemsets and
    is the set of approximate frequent itemsets. 5. Recall. (6) 6. Average Support
    Error (ASE). This customized metric quantifies the deviation between the support
    of the mined frequent itemsets by ScaDistFIM and their true support. It is defined
    as follows: (7) where and are the estimated and true support values of the frequent
    itemset , respectively and is the number of true frequent itemsets in . Download
    : Download high-res image (816KB) Download : Download full-size image Fig. 7.
    Performance comparison on different numbers of partitions on the six datasets.
    6.3. Experiment results and comparisons 6.3.1. Efficiency The first six datasets
    in Table 2 are used to compare the efficiencies of the ScaDistFIM algorithm and
    the Spark FP-Growth algorithm. In this experiment, each dataset was processed
    10 times by each algorithm using the same relative support threshold, and different
    numbers of partitions in the RDD were used in each run. The minimal relative support
    thresholds used in the six datasets are 0.002, 0.005, 0.01, 0.01, 0.005 and 0.02,
    respectively. Fig. 7 shows the results of computing efficiencies of the two algorithms
    on the six datasets. The vertical axis is the execution time in seconds and the
    horizontal axis is the number of partitions used in the 10 runs. We can see that
    the ScaDistFIM algorithm ran much faster than the Spark FP-Growth algorithm, and
    its execution time decreases slightly as the number of partitions increases. The
    reason is that the transactions in each partition are reduced, and more executors
    are used to process smaller partitions in parallel. Download : Download high-res
    image (756KB) Download : Download full-size image Fig. 8. Performance comparison
    on different minimal relative support thresholds. The execution time of the Spark
    FP-Growth algorithm increases as the number of partitions increases. This is because
    the data repartitioning stage of the distributed FP-Growth in Spark implementation
    took more time to repartition transactions as the number of item groups in the
    group-list increased. This stage causes an increase in execution time when more
    partitions are used. However, the execution time of dataset TH2 shows a reverse
    trend. Compared to other datasets, this dataset is comparatively large with 2.6
    million transactions. When the number of partitions is small, the data size in
    each partition is big. It takes long time for FP-Growth to build the FP-Tree and
    traverse the tree to find all frequent itemsets. When the number of partitions
    increases, the data size in each partition is reduced so the execution time of
    FP-Growth to mine the frequent itemsets in each partition also decreases. Therefore,
    if memory is sufficient for big transaction datasets, more partitions and more
    efficient for the Spark FP-Growth algorithm. We fixed the number of partitions
    as 32, 10, 260, 100, 5 and 5, respectively for the six datasets and used ten different
    minimal relative support thresholds for each dataset to run each algorithm ten
    times and recorded the execution times. Fig. 8 shows the comparison results of
    the two algorithms on the six datasets. From these figures, we can see that the
    execution times of the algorithm on the six datasets are almost unchanged as the
    minimal relative support values increase. This is understandable because with
    our data RDD settings, the data size in each partition is not big, and the execution
    time of the LO stage does not change much as each computing node only uses the
    data partition on its node to compute the local frequent itemset table, and no
    communication between nodes is required. However, with the setting on the number
    of partitions for each dataset, the execution time of the Spark FP-Growth algorithm
    decreases as the minimal relative support threshold increases because a smaller
    FP-Tree is built from each partition as the minimal relative support threshold
    increases. These figures show that the algorithm is more efficient than the Spark
    FP-Growth algorithm because fewer data in each partition is processed by the sequential
    FP-Growth algorithm. With sufficient memory to run more executors, big datasets
    can be efficiently processed by the ScaDistFIM algorithm, whereas the repartitions
    in the Spark FP-Growth are often big for the FP-Growth algorithm to build a big
    complex FP-tree which causes out-of-memory errors. However, the algorithm produces
    an approximate set of frequent itemsets. In the next section, we will show how
    accurate can the approximate set of frequent itemsets be in comparison to the
    true set of frequent itemsets found by the Spark FP-Growth algorithm. Table 3.
    Kaggle. No. of True FP FN ASE partitions FIS FIS FIS 10 589 6 4 0.42 20 7 3 0.81
    30 7 2 1.19 40 8 1 1.49 50 8 1 1.88 60 10 1 2.25 70 10 0 2.47 80 11 0 2.76 90
    11 0 3.12 100 12 0 3.42 Table 4. Kosarak. No. of True FP FN ASE partitions FIS
    FIS FIS 10 1462 7 11 0.14 20 12 10 0.26 30 16 7 0.39 40 18 5 0.48 50 20 3 0.62
    60 28 2 0.68 70 32 1 0.79 80 40 1 0.83 90 45 0 1.01 100 48 0 1.01 Table 5. TH1.
    No. of True FP FN ASE partitions FIS FIS FIS 10 1757 11 4 0.67 20 12 4 1.41 30
    14 3 1.98 40 15 3 2.6 50 19 2 3.5 60 20 2 3.9 70 22 2 4.56 80 23 2 5.06 90 24
    2 5.4 100 27 1 6.46 Table 6. TH2. No. of True FP FN ASE partitions FIS FIS FIS
    10 1738 0 4 0.76 20 0 3 1.6 30 1 3 2.35 40 1 3 3.33 50 1 2 3.98 60 2 2 4.92 70
    3 1 5.5 80 4 1 6.59 90 4 1 7.5 100 4 1 7.67 Table 7. T10I4D100K. No. of True FP
    FN ASE partitions FIS FIS FIS 3 504 15 18 0.39 5 20 16 0.83 7 27 16 0.95 9 43
    12 1.25 11 45 9 1.69 13 48 9 1.52 15 55 8 1.94 17 56 7 2.22 19 60 5 1.94 21 95
    4 2.73 Table 8. T40I10D100K. No. of True FP FN ASE partitions FIS FIS FIS 3 1683
    37 38 0.35 5 37 37 1.01 7 42 35 1.31 9 46 27 1.8 11 52 24 2.17 13 53 21 2.48 15
    60 21 2.72 17 64 20 3.18 19 73 19 3.52 21 85 16 3.93 6.3.2. Accuracy This section
    compares the approximate sets of frequent itemsets found by the ScaDistFIM algorithm
    and the complete sets of frequent itemsets by the Spark-FP-Growth algorithms from
    the first six datasets in Table 2, and evaluates the accuracy of the results from
    the ScaDistFIM algorithm. Table 3, Table 4, Table 5, Table 6, Table 7, Table 8
    show the results of frequent itemsets by ScaDistFIM in comparison with the true
    frequent itemsets by Spark-FP-Growth from the six datasets with the minimal relative
    support thresholds 0.002, 0.005, 0.01, 0.01, 0.005 and 0.02, respectively. The
    results with 10 different numbers of partitions in running the algorithm are summarized
    in these six tables. The second column from the left gives the true number of
    frequent itemsets by Spark-FP-Growth. The third column FP FIS lists the number
    of false positive frequent itemsets which are not frequent itemsets but found
    by ScaDistFIM. The fourth column FN FIS lists the number of false negative frequent
    itemsets, i.e., those itemsets which are true frequent itemsets by the minimal
    relative support threshold but not found by ScaDistFIM. The last column ASE on
    the right lists the average difference between the true relative support values
    and the estimated relative support values of all true positive itemsets, i.e.,
    those itemsets that appear in both the true set of frequent itemsets by Spark-FP-Growth
    and the approximate set of frequent itemsets by ScaDistFIM. According to the relations
    among the columns True FIS, FP FIS, and FN FIS, the number of approximate frequent
    itemsets (AFIS) found by ScaDistFIM can be calculated as: (8) For example, AFIS
    591 frequent itemsets were found by ScaDistFIM when the Kaggle data is divided
    into 10 partitions and the relative support is set to 0.005. The results in Table
    3, Table 4, Table 5, Table 6, Table 7, Table 8 demonstrate that the ScaDistFIM
    algorithm accurately discovered the frequent itemsets in the six datasets. Compared
    with the number of true frequent itemsets by Spark-FP-Growth, the numbers of false
    negative and false positive frequent itemsets are very small. We can also observe
    from Table 3, Table 6 that the bigger the dataset, the more accurate the result
    by ScaDistFIM. Another observation is that the number of false positive frequent
    itemsets increases as the number of partitions increases so does the ASE value,
    but the number of false negative frequent itemsets decreases. The ASE values computed
    with Eq. (4) are over-estimated. However, their absolute values are very small
    which will not affect real applications. From these observations, we can conclude
    that ScaDistFIM is more suitable for discovering more accurate approximate set
    of frequent itemsets from big transaction datasets. The bigger the dataset, the
    more partitions we use. Table 9, Table 10, Table 11, Table 12, Table 13, Table
    14 show the changes in the results by ScaDistFIM as the minimal relative support
    threshold increases. From all six tables, we can observe that the number of true
    frequent itemsets decreases as the minimal relative support threshold increases
    on all datasets. Another observation is that there is no clear trend in the variation
    of ASE values, which means that the variation of support does not affect the accuracy
    of ScaDistFIM. In Table 13, Table 14, the number of false positive and false negative
    is obviously decreasing, which is mainly caused by the change of true FIS. Other
    observations are the same as those from Table 3, Table 4, Table 5, Table 6, Table
    7, Table 8. Table 9. Kaggle. Sup True FP FN ASE FIS FIS FIS 1.0 2111 52 0 1.21
    1.1 1787 42 2 1.25 1.2 1514 41 3 1.29 1.3 1289 29 3 1.26 1.4 1132 21 3 1.23 1.5
    1002 18 3 1.26 1.6 889 14 2 1.29 1.7 796 12 2 1.2 1.8 722 12 0 1.35 1.9 653 9
    2 1.37 Table 10. Kosarak. Sup True FP FN ASE FIS FIS FIS 5.0 1462 7 11 1.35 5.5
    1235 15 2 1.67 6.0 1019 11 5 1.75 6.5 832 18 3 1.63 7.0 695 13 1 1.92 7.5 594
    10 2 1.48 8.0 510 8 1 1.21 8.5 453 4 0 1.66 9.0 404 2 0 1.24 9.5 363 7 2 1.29
    Table 11. TH1. Sup True FP FN ASE FIS FIS FIS 1.00 1757 27 1 6.46 1.05 1552 18
    2 6.37 1.10 1392 17 3 7.02 1.15 1220 12 2 6.08 1.20 1115 8 0 6.59 1.25 1008 8
    2 6.69 1.30 910 7 0 6.98 1.35 830 7 0 7.27 1.40 750 6 0 6.76 1.45 686 2 0 6.77
    Table 12. TH2. Sup True FP FN ASE FIS FIS FIS 1.00 1738 2 0 1.93 1.05 1534 5 2
    1.8 1.10 1377 1 2 2.16 1.15 1201 3 0 1.55 1.20 1103 4 0 1.87 1.25 991 3 0 1.83
    1.30 899 4 0 1.9 1.35 813 5 1 1.94 1.40 740 3 0 1.92 1.45 676 4 1 1.94 Table 13.
    T10I4D100K. Sup True FP FN ASE FIS FIS FIS 4.0 1371 58 37 0.83 4.2 1146 83 39
    0.92 4.4 860 99 45 0.86 4.6 684 48 28 0.85 4.8 583 39 12 0.81 5.0 504 20 16 0.83
    5.2 441 19 19 0.99 5.4 393 18 16 1.15 5.6 342 18 16 1.17 5.8 286 23 5 1.09 Table
    14. T40I10D100K. Sup True FP FN ASE FIS FIS FIS 1.6 3930 81 158 0.98 1.7 3073
    63 85 0.94 1.8 2487 60 69 0.96 1.9 2047 38 44 1.02 2.0 1683 36 27 1.01 2.1 1386
    19 27 0.98 2.2 1155 16 21 1.05 2.3 969 28 13 1.05 2.4 803 28 18 1.07 2.5 676 20
    14 1.15 Fig. 9 shows the precisions and recalls of the true frequent itemsets
    discovered with ScaDistFIM from the six datasets with different numbers of partitions.
    We can see that the precision and recalls are very high in the four datasets,
    Kaggle, Kosarak, TH1, and TH2, all close to 100%. The precision decreases slightly
    as the number of partitions increases. This is because the number of false positive
    frequent itemsets increases which reduces the precision as shown in Eq. (5). However,
    the recalls increase as the number of partitions increases as shown in Eq. (6).
    The precision and recall for datasets T10I4D100K and T40I10D100K demonstrate tremendous
    changes as the number of partitions increases. This is because the number of transactions
    is only 100 thousand in these datasets, which is comparatively small in comparison
    with other datasets. When the datasets are divided into more partitions, for example,
    20 partitions, each partition only will be too small, e.g., 5000 transactions.
    The distributions of frequent itemsets in the 20 partitions will be more diverse,
    which affects the final result. This observation implies that ScaDistFIM is not
    suitable for small transaction datasets like T10I4D100K and T40I10D100K, which
    can be easily processed by the sequential FIM algorithms. Instead, ScaDistFIM
    is developed to process large-scale transaction datasets with a large number of
    transactions. Download : Download high-res image (476KB) Download : Download full-size
    image Fig. 9. Precision and recalls on different numbers of partitions. 6.3.3.
    Scalability The data scalability of the ScaDistFIM algorithm was evaluated with
    the last dataset GenD in Table 2. This dataset was generated with 1 billion transactions
    and 1000 unique items. We divided this dataset into ten datasets, each with 100
    million transactions. We ran the ScaDistFIM algorithm on these datasets ten times,
    starting with 100 million transactions on the first run and followed by the next
    run with 100 million transactions added to the dataset of the previous run until
    all transactions are used in the last run. The minimal relative support threshold
    was set as 0.01. The execution time in each run was recorded, and the recall and
    precision were calculated. Fig. 10(a) shows the execution times of the ten runs
    on the big dataset GenD and the corresponding recalls and precisions of these
    runs. We can see that the execution time increases slowly in a linear form as
    the size of data increases. We can see in Fig. 7 that the execution times of the
    ScaDistFIM algorithm on all six datasets are less than 20 s. From Fig. 10(a),
    we can see the execution time of processing 100 million transactions with ScaDistFIM
    is about 260 s, i.e., less than 5 min, whereas processing 1 billion transactions
    takes about 6 min. These results demonstrate that the ScaDistFIM algorithm is
    scalable to big transaction data in billions of transactions. The dataset GenD
    was generated with 1000 unique items. If more unique items were included, it would
    take longer execution time to complete, but the result still could be obtained.
    However, transaction datasets of such a big size are extremely difficult, if not
    impossible, to handle with existing FIM algorithms. This result clearly demonstrates
    the advantage of the ScaDistFIM algorithm in data scalability. Download : Download
    high-res image (382KB) Download : Download full-size image Fig. 10. Experiment
    results of ScaDistFIM on big dataset GenD. Fig. 10(b) shows the results of precision
    and recall calculated from the ten runs on the datasets GenD. As discussed before,
    since the transaction datasets are big, we can see that the precisions and recalls
    of all results are nearly 100%. This result further demonstrates that ScaDistFIM
    not only has efficiency and scalability advantages on big transaction datasets
    but also has the ability to generate very accurate results. 7. Conclusions This
    paper proposes a novel approach for conducting basket analysis on big transaction
    datasets. The primary benefit of this approach lies in its scalability, capable
    of handling transaction data containing billions of records, and its flexibility
    to perform a wide range of basket analysis tasks to fulfill diverse business requirements.
    To address the issue of data scalability, we proposed a distributed algorithm
    named ScaDistFIM. This algorithm can efficiently process one billion transactions
    within minutes to produce an approximate set of frequent itemsets. This algorithm
    can efficiently process one billion transactions within minutes to produce an
    approximate collection of frequent itemsets. ScaDistFIM operates within the Spark
    system on a cluster and utilizes an approximation technique for distributed mining
    frequent itemsets through FP-Growth, resulting in the accurate generation of frequent
    itemsets from big transaction datasets. Consequently, we have implemented a basket
    analysis system within the Spark framework. This system comprises three distinct
    modules, which segregate the generation of the base frequent itemsets from extended
    basket analysis operations. This division enhances the system’s performance when
    dealing with significant volumes of data and extends the flexibility of basket
    analysis to meet various business needs. Our forthcoming research will extend
    this approach by introducing a new FIM algorithm capable of computing approximate
    sets of frequent itemsets from extensive random samples of terabyte-scale transaction
    datasets. Additionally, we will explore novel methods for conducting basket analysis
    on geographically distributed transaction datasets stored across multiple data
    centers. Declaration of competing interest The authors declare that they have
    no known competing financial interests or personal relationships that could have
    appeared to influence the work reported in this paper. Data availability Data
    will be made available on request. References Agarwal et al., 2016 Agarwal R.,
    Singh S., Vats S. Implementation of an improved algorithm for frequent itemset
    mining using Hadoop 2016 international conference on computing, communication
    and automation (ICCCA), IEEE (2016), pp. 13-18 View in ScopusGoogle Scholar Agarwal
    et al., 2018 Agarwal R., Singh S., Vats S. Review of parallel apriori algorithm
    on MapReduce framework for performance enhancement Big data analytics: proceedings
    of CSI 2015, Springer (2018), pp. 403-411 CrossRefView in ScopusGoogle Scholar
    Aggarwal et al., 2014 Aggarwal C.C., Bhuiyan M., Hasan M.A. Frequent pattern mining
    algorithms: A survey Frequent pattern mining, Springer (2014), pp. 19-64 CrossRefView
    in ScopusGoogle Scholar Agrawal et al., 1993 Agrawal R., Imielinski T., Swami
    A.N. Mining association rules between sets of items in large databases Buneman
    P., Jajodia S. (Eds.), Proceedings of the 1993 ACM SIGMOD international conference
    on management of data, Washington, DC, USA, May 26-28, 1993, ACM Press (1993),
    pp. 207-216, 10.1145/170035.170072 Google Scholar Agrawal and Srikant, 1994 Agrawal
    R., Srikant R. Fast algorithms for mining association rules in large databases
    Bocca J.B., Jarke M., Zaniolo C. (Eds.), VLDB’94, proceedings of 20th international
    conference on very large data bases, September 12-15, 1994, Santiago de Chile,
    Chile, Morgan Kaufmann (1994), pp. 487-499 URL: http://www.vldb.org/conf/1994/P487.PDF
    View in ScopusGoogle Scholar Alawadh and Barnawi, 2022 Alawadh M.M., Barnawi A.M.
    A survey on methods and applications of intelligent market basket analysis based
    on association rule Journal on Big Data, 4 (1) (2022) Google Scholar Cheng et
    al., 2021 Cheng L., Chen K., Lee M., Li K. User-defined SWOT analysis - A change
    mining perspective on user-generated content Information Processing and Management,
    58 (5) (2021), Article 102613, 10.1016/j.ipm.2021.102613 View PDFView articleView
    in ScopusGoogle Scholar Chon and Kim, 2018 Chon K., Kim M. BIGMiner: a fast and
    scalable distributed frequent pattern miner for big data Cluster Computing, 21
    (3) (2018), pp. 1507-1520 CrossRefView in ScopusGoogle Scholar Dahdouh et al.,
    2019 Dahdouh K., Dakkak A., Oughdir L., Ibriz A. Large-scale e-learning recommender
    system based on spark and hadoop Journal of Big Data, 6 (1) (2019), pp. 1-23 Google
    Scholar Delgado-Osuna et al., 2020 Delgado-Osuna J.A., García-Martínez C., Barbadillo
    J.G., Ventura S. Heuristics for interesting class association rule mining a colorectal
    cancer database Information Processing and Management, 57 (3) (2020), Article
    102207, 10.1016/j.ipm.2020.102207 URL: https://doi.org/10.1016/j.ipm.2020.102207
    View PDFView articleView in ScopusGoogle Scholar Dhanabhakyam and Punithavalli,
    2011 Dhanabhakyam M., Punithavalli M. A survey on data mining algorithm for market
    basket analysis Global Journal of Computer Science and Technology, 11 (11) (2011),
    pp. 23-28 Google Scholar Djenouri et al., 2019 Djenouri Y., Djenouri D., Belhadi
    A., Cano A. Exploiting GPU and cluster parallelism in single scan frequent itemset
    mining Information Sciences, 496 (2019), pp. 363-377 View PDFView articleView
    in ScopusGoogle Scholar Djenouri et al., 2018 Djenouri Y., Djenouri D., Lin J.C.,
    Belhadi A. Frequent itemset mining in big data with effective single scan algorithms
    IEEE Access, 6 (2018), pp. 68013-68026 CrossRefView in ScopusGoogle Scholar Duong
    et al., 2017 Duong K., Bamha M., Giacometti A., Li D., Soulet A., Vrain C. MapFIM:
    Memory aware parallelized frequent itemset mining in very large datasets Benslimane
    D., Damiani E., Grosky W.I., Hameurlain A., Sheth A.P., Wagner R.R. (Eds.), Database
    and expert systems applications - 28th international conference, DEXA 2017, Lyon,
    France, August 28-31, 2017, proceedings, part i, Lecture Notes in Computer Science,
    Vol. 10438, Springer (2017), pp. 478-495, 10.1007/978-3-319-64468-4_36 View in
    ScopusGoogle Scholar Duong et al., 2018 Duong K.-C., Bamha M., Giacometti A.,
    Li D., Soulet A., Vrain C. Mapfim+: Memory aware parallelized frequent itemset
    mining in very large datasets Transactions on Large-Scale Data-and Knowledge-Centered
    Systems XXXIX: Special Issue on Database-and Expert-Systems Applications (2018),
    pp. 200-225 CrossRefView in ScopusGoogle Scholar Fan et al., 2022 Fan, W., Han,
    Z., Wang, Y., & Xie, M. (2022). Parallel Rule Discovery from Large Datasets by
    Sampling. In Proceedings of the 2022 international conference on management of
    data (pp. 384–398). Google Scholar Fernandez-Basso et al., 2023 Fernandez-Basso
    C., Ruiz M.D., Martin-Bautista M.J. New spark solutions for distributed frequent
    itemset and association rule mining algorithms Cluster Computing (2023), pp. 1-18
    Google Scholar Fournier-Viger et al., 2017 Fournier-Viger P., Lin J., Kiran R.,
    Koh Y., Thomas R. A survey of sequential pattern mining Data Science and Pattern
    Recognition, 1 (1) (2017), pp. 54-77 Google Scholar Fumarola and Malerba, 2014
    Fumarola F., Malerba D. A parallel algorithm for approximate frequent itemset
    mining using MapReduce 2014 international conference on high performance computing
    & simulation (HPCS), IEEE (2014), pp. 335-342 View in ScopusGoogle Scholar Gan
    et al., 2021 Gan W., Lin J.C., Fournier-Viger P., Chao H., Tseng V.S., Yu P.S.
    A survey of utility-oriented pattern mining IEEE Trans. Knowl. Data Eng., 33 (4)
    (2021), pp. 1306-1327, 10.1109/TKDE.2019.2942594 View in ScopusGoogle Scholar
    Gan et al., 2019 Gan W., Lin J.C., Fournier-Viger P., Chao H., Yu P.S. A survey
    of parallel sequential pattern mining ACM Transactions on Knowledge Discovery
    Data, 13 (3) (2019), pp. 25:1-25:34, 10.1145/3314107 Google Scholar Han et al.,
    2012 Han J., Kamber M., Pei J. 6-mining frequent patterns, associations, and correlations:
    Basic concepts and methods Data Mining: Concepts and Techniques (2012), pp. 243-278
    View PDFView articleGoogle Scholar Hedrick et al., 2022 Hedrick V.E., Farris A.R.,
    Houghtaling B., Mann G., Misyak S.A. Validity of a market basket assessment tool
    for use in supplemental nutrition assistance program education healthy retail
    initiatives Journal of Nutrition Education and Behavior, 54 (8) (2022), pp. 776-783
    View PDFView articleView in ScopusGoogle Scholar Hossain et al., 2019 Hossain
    M., Sattar A.S., Paul M.K. Market basket analysis using apriori and FP growth
    algorithm 2019 22nd international conference on computer and information technology
    (ICCIT), IEEE (2019), pp. 1-6 Google Scholar Huang et al., 2021 Huang P., Cheng
    W., Chen J., Chung W., Chen Y., Lin K.W. A distributed method for fast mining
    frequent patterns from big data IEEE Access, 9 (2021), pp. 135144-135159, 10.1109/ACCESS.2021.3115514
    View in ScopusGoogle Scholar Jain et al., 2022 Jain D.K., Boyapati P., Venkatesh
    J., Prakash M. An intelligent cognitive-inspired computing with big data analytics
    framework for sentiment analysis and classification Information Processing and
    Management, 59 (1) (2022), Article 102758, 10.1016/j.ipm.2021.102758 View PDFView
    articleView in ScopusGoogle Scholar Jashma Suresh et al., 2023 Jashma Suresh P.,
    Dinesh Acharya U., Reddy N.S. Mining frequent itemsets from transaction databases
    using hybrid switching framework Multimedia Tools and Applications (2023), pp.
    1-21 Google Scholar Jiang and Meng, 2017 Jiang, H., & Meng, H. (2017). A parallel
    fp-growth algorithm based on gpu. In Proceedings of IEEE 14th international conference
    on e-business engineering (pp. 97–102). Google Scholar Li, Wang et al., 2008 Li,
    H., Wang, Y., Zhang, D., Zhang, M., & Chang, E. Y. (2008). Pfp: parallel fp-growth
    for query recommendation. In Proceedings of the 2008 ACM conference on recommender
    systems (pp. 107–114). Google Scholar Li, Yeh et al., 2008 Li Y., Yeh J., Chang
    C. Isolated items discarding strategy for discovering high utility itemsets Data
    & Knowledge Engineering, 64 (1) (2008), pp. 198-217, 10.1016/J.DATAK.2007.06.009
    View PDFView articleView in ScopusGoogle Scholar Liew, 2018 Liew H.P. Dietary
    habits and physical activity: Results from cluster analysis and market basket
    analysis Nutrition and Health, 24 (2) (2018), pp. 83-92 CrossRefView in ScopusGoogle
    Scholar Liu et al., 2018 Liu D., Huang J., Lin C. Recommendation with social roles
    IEEE Access, 6 (2018), pp. 36420-36427, 10.1109/ACCESS.2018.2832185 View in ScopusGoogle
    Scholar Long and Zhu, 2012 Long S., Zhu W. Mining evolving association rules for
    e-business recommendation Journal of Shanghai Jiaotong University (Science), 17
    (2012), pp. 161-165 CrossRefView in ScopusGoogle Scholar Luna et al., 2019 Luna
    J.M., Fournier-Viger P., Ventura S. Frequent itemset mining: A 25 years review
    WIREs Data Mining Knowledge Discovery, 9 (6) (2019), 10.1002/widm.1329 Google
    Scholar Mahmud et al., 2023 Mahmud M.S., Huang J.Z., Ruby R., Ngueilbaye A., Wu
    K. Approximate clustering ensemble method for big data IEEE Transactions on Big
    Data (2023) Google Scholar Mahmud et al., 2020 Mahmud M.S., Huang J.Z., Salloum
    S., Emara T.Z., Sadatdiynov K. A survey of data partitioning and sampling methods
    to support big data analysis Big Data Mining and Analytics, 3 (2) (2020), pp.
    85-101 CrossRefView in ScopusGoogle Scholar McCreadie et al., 2012 McCreadie R.,
    Macdonald C., Ounis I. MapReduce indexing strategies: Studying scalability and
    efficiency Information Processing and Management, 48 (5) (2012), pp. 873-888,
    10.1016/j.ipm.2010.12.003 View PDFView articleView in ScopusGoogle Scholar Meida
    et al., 2019 Meida A., Rini D.P., Sukemi S. Pattern of E-marketplace customer
    shopping behavior using Tabu search and FP-growth algorithm Indonesian Journal
    of Electrical Engineering and Informatics (IJEEI), 7 (4) (2019), pp. 772-778 View
    in ScopusGoogle Scholar Naulaerts et al., 2015 Naulaerts S., Meysman P., Bittremieux
    W., Vu T., Berghe W.V., Goethals B., et al. A primer to frequent itemset mining
    for bioinformatics Briefings in Bioinformatics, 16 (2) (2015), pp. 216-231 CrossRefView
    in ScopusGoogle Scholar Patron and Gomez, 2020 Patron H., Gomez L. A market basket
    analysis of the US auto-repair industry Journal of Business Analytics, 3 (2) (2020),
    pp. 79-92 CrossRefView in ScopusGoogle Scholar Patwary et al., 2021 Patwary A.H.,
    Eshan M.T., Debnath P., Sattar A. Market basket analysis approach to machine learning
    2021 12th international conference on computing communication and networking technologies
    (ICCCNT), IEEE (2021), pp. 1-9 CrossRefGoogle Scholar Pradana et al., 2022 Pradana
    M.R., Syafrullah M., Irawan H., Chandra J.C., Solichin A. Market basket analysis
    using FP-growth algorithm on retail sales data 2022 9th international conference
    on electrical engineering, computer science and informatics (EECSI), IEEE (2022),
    pp. 86-89 CrossRefView in ScopusGoogle Scholar Prajapati et al., 2017 Prajapati
    D.J., Garg S., Chauhan N. Interesting association rule mining with consistent
    and inconsistent rule detection from big sales data in distributed environment
    Future Computing and Informatics Journal, 2 (1) (2017), pp. 19-30 View PDFView
    articleCrossRefGoogle Scholar Pramudiono and Kitsuregawa, 2003 Pramudiono I.,
    Kitsuregawa M. Parallel FP-growth on PC cluster Whang K., Jeon J., Shim K., Srivastava
    J. (Eds.), Advances in knowledge discovery and data mining, 7th Pacific-Asia conference,
    PAKDD 2003, Seoul, Korea, April 30 - May 2, 2003, proceedings, Lecture notes in
    computer science, Vol. 2637, Springer (2003), pp. 467-473, 10.1007/3-540-36175-8_47
    View in ScopusGoogle Scholar Ragaventhiran and Devi, 2020 Ragaventhiran J., Devi
    M.K.K. Map-optimize-reduce: CAN tree assisted FP-growth algorithm for clusters
    based FP mining on hadoop Future Generation Computer Systems, 103 (2020), pp.
    111-122 View PDFView articleView in ScopusGoogle Scholar Raj and Ramesh, 2022
    Raj S., Ramesh D. PartEclat: an improved eclat-based frequent itemset mining algorithm
    on spark clusters using partition technique Cluster Computing, 25 (6) (2022),
    pp. 4463-4480 CrossRefView in ScopusGoogle Scholar Raj et al., 2021 Raj S., Ramesh
    D., Sethi K.K. A Spark-based Apriori algorithm with reduced shuffle overhead The
    Journal of Supercomputing, 77 (1) (2021), pp. 133-151 CrossRefView in ScopusGoogle
    Scholar Raj et al., 2020 Raj S., Ramesh D., Sreenu M., Sethi K.K. EAFIM: efficient
    apriori-based frequent itemset mining algorithm on spark for big transactional
    data Knowledge and Information Systems, 62 (2020), pp. 3565-3583 CrossRefView
    in ScopusGoogle Scholar Renjith et al., 2020 Renjith S., Sreekumar A., Jathavedan
    M. An extensive study on the evolution of context-aware personalized travel recommender
    systems Information Processing and Management, 57 (1) (2020), 10.1016/j.ipm.2019.102078
    Google Scholar Riondato and Upfal, 2014 Riondato M., Upfal E. Efficient discovery
    of association rules and frequent itemsets through sampling with tight performance
    guarantees ACM Transactions on Knowledge Discovery Data, 8 (4) (2014), pp. 20:1-20:32,
    10.1145/2629586 Google Scholar Rochd et al., 2019 Rochd Y., Hafidi I., Ouartassi
    B. A review of scalable algorithms for frequent itemset mining for big data using
    Hadoop and Spark Lecture Notes in Real-Time Intelligent Systems (2019), pp. 90-99
    View in ScopusGoogle Scholar Saggi and Jain, 2018 Saggi M.K., Jain S. A survey
    towards an integration of big data analytics to big insights for value-creation
    Information Processingn and Management, 54 (5) (2018), pp. 758-790, 10.1016/j.ipm.2018.01.010
    View PDFView articleView in ScopusGoogle Scholar Salloum et al., 2019 Salloum
    S., Huang J., He Y. Random sample partition: A distributed data model for big
    data analysis IEEE Transactions on Industrial Informatics, 15 (11) (2019), pp.
    5846-5854 CrossRefView in ScopusGoogle Scholar Saputra et al., 2023 Saputra J.P.B.,
    Rahayu S.A., Hariguna T. Market basket analysis using FP-growth algorithm to design
    marketing strategy by determining consumer purchasing patterns Journal of Applied
    Data Sciences, 4 (1) (2023), pp. 38-49 View in ScopusGoogle Scholar Shawkat et
    al., 2022 Shawkat M., Badawi M., El-ghamrawy S., Arnous R., El-desoky A. An optimized
    FP-growth algorithm for discovery of association rules The Journal of Supercomputing
    (2022), pp. 1-28 Google Scholar Shen et al., 2002 Shen Y., Zhang Z., Yang Q. Objective-oriented
    utility-based association mining Proceedings of the 2002 IEEE international conference
    on data mining (ICDM 2002), 9-12 December 2002, Maebashi City, Japan, IEEE Computer
    Society (2002), pp. 426-433, 10.1109/ICDM.2002.1183938 View in ScopusGoogle Scholar
    Shi et al., 2017 Shi X., Chen S., Yang H. DFPS: Distributed FP-growth algorithm
    based on Spark 2017 IEEE 2nd advanced information technology, electronic and automation
    control conference (IAEAC), IEEE (2017), pp. 1725-1731 View in ScopusGoogle Scholar
    Shiokawa et al., 2016 Shiokawa Y., Misawa T., Date Y., Kikuchi J. Application
    of market basket analysis for the visualization of transaction data based on human
    lifestyle and spectroscopic measurements Analytical Chemistry, 88 (5) (2016),
    pp. 2714-2719 CrossRefView in ScopusGoogle Scholar Singh et al., 2020 Singh P.,
    Singh S., Mishra P., Garg R. RDD-Eclat: approaches to parallelize Eclat algorithm
    on spark RDD framework Second international conference on computer networks and
    communication technologies: ICCNCT 2019, Springer (2020), pp. 755-768 CrossRefView
    in ScopusGoogle Scholar Sreeyuktha and Geetha Reddy, 2019 Sreeyuktha H., Geetha
    Reddy J. Partitioning in apache spark Innovations in computer science and engineering:
    proceedings of the sixth ICICSE 2018, Springer (2019), pp. 493-498 CrossRefView
    in ScopusGoogle Scholar Sun et al., 2023 Sun X., He Y., Wu D., Huang J.Z. Survey
    of distributed computing frameworks for supporting big data analysis Big Data
    Mining and Analytics, 6 (2) (2023), pp. 154-169 CrossRefView in ScopusGoogle Scholar
    Tatiana and Mikhail, 2018 Tatiana K., Mikhail M. Market basket analysis of heterogeneous
    data sources for recommendation system improvement Procedia Computer Science,
    136 (2018), pp. 246-254 View PDFView articleGoogle Scholar Ünvan, 2021 Ünvan Y.A.
    Market basket analysis with association rules Communications in Statistics. Theory
    and Methods, 50 (7) (2021), pp. 1615-1628 CrossRefView in ScopusGoogle Scholar
    Vaishampayan et al., 2022 Vaishampayan S., Singh G., Hebasur V., Kute R. Market
    basket analysis recommender system using apriori algorithm High performance computing
    and networking: select proceedings of CHSN 2021, Springer (2022), pp. 461-472
    CrossRefView in ScopusGoogle Scholar Valiullin et al., 2021 Valiullin T., Huang
    Z.J., Wei C., Yin J., Wu D., Egorova I. A new approximate method for mining frequent
    itemsets from big data Computer Science and Information Systems, 18 (3) (2021),
    pp. 641-656 CrossRefView in ScopusGoogle Scholar Wei et al., 2018 Wei C., Salloum
    S., Emara T.Z., Zhang X., Huang J.Z., He Y. A two-stage data processing algorithm
    to generate random sample partitions for big data analysis CLOUD, Vol. 10967 (2018),
    pp. 347-364 CrossRefView in ScopusGoogle Scholar Wicaksono et al., 2020 Wicaksono
    D., Jambak M.I., Saputra D.M. The comparison of apriori algorithm with preprocessing
    and FP-growth algorithm for finding frequent data pattern in association rule
    Sriwijaya international conference on information technology and its applications
    (SICONIAN 2019), Atlantis Press (2020), pp. 315-319 View in ScopusGoogle Scholar
    Xun et al., 2016 Xun Y., Zhang J., Qin X., Zhao X. FiDoop-DP: Data partitioning
    in frequent itemset mining on hadoop clusters IEEE Transactions on Parallel and
    Distributed Systems, 28 (1) (2016), pp. 101-114 Google Scholar Xun et al., 2021
    Xun Y., Zhang J., Yang H., Qin X. HBPFP-DC: A parallel frequent itemset mining
    using Spark Parallel Computing, 101 (2021), Article 102738 View PDFView articleView
    in ScopusGoogle Scholar Yang et al., 2023 Yang S., Jin W., Yu Y., Hashim K.F.
    Optimized hadoop map reduce system for strong analytics of cloud big product data
    on amazon web service Information Processing and Management, 60 (3) (2023), Article
    103271, 10.1016/j.ipm.2023.103271 View PDFView articleView in ScopusGoogle Scholar
    Yimin et al., 2021 Yimin M., Junhao G., Mwakapesa D.S., Nanehkaran Y.A., Chi Z.,
    Xiaoheng D., et al. PFIMD: a parallel MapReduce-based algorithm for frequent itemset
    mining Multimedia Systems, 27 (4) (2021), pp. 709-722 CrossRefView in ScopusGoogle
    Scholar Yoon and Lee, 2013 Yoon Y., Lee G.G. Two scalable algorithms for associative
    text classification Information Processing and Management, 49 (2) (2013), pp.
    484-496, 10.1016/j.ipm.2012.09.003 View PDFView articleView in ScopusGoogle Scholar
    Yun et al., 2017 Yun U., Ryang H., Lee G., Fujita H. An efficient algorithm for
    mining high utility patterns from incremental databases with one database scan
    Knowledge-Based Systems, 124 (2017), pp. 188-206, 10.1016/j.knosys.2017.03.016
    View PDFView articleView in ScopusGoogle Scholar Zhang et al., 2015 Zhang F.,
    Liu M., Gui F., Shen W., Shami A., Ma Y. A distributed frequent itemset mining
    algorithm using Spark for Big Data analytics Cluster Computing, 18 (2015), pp.
    1493-1501 CrossRefView in ScopusGoogle Scholar Zheng et al., 2018 Zheng J., Deng
    X., Zhang H. A novel method to generate frequent itemsets in distributed environment
    2018 IEEE 37th international performance computing and communications conference
    (IPCCC), IEEE (2018), pp. 1-8 View PDFView articleGoogle Scholar Zhou et al.,
    2010 Zhou L., Zhong Z., Chang J., Li J., Huang J.Z., Feng S. Balanced parallel
    fp-growth with mapreduce 2010 IEEE youth conference on information, computing
    and telecommunications, IEEE (2010), pp. 243-246 View in ScopusGoogle Scholar
    Cited by (1) Non-MapReduce computing for intelligent big data analysis 2024, Engineering
    Applications of Artificial Intelligence Show abstract ☆ This research has been
    supported by the National Natural Science Foundation of China (61972261), and
    Key Basic Research Foundation of Shenzhen (JCYJ20220818100205012). 1 https://haifengl.github.io/association-rule.html.
    2 https://github.com/anonymous-userLOGO/ScaDistFIM. © 2023 The Author(s). Published
    by Elsevier Ltd. Recommended articles GPR-OPT: A Practical Gaussian optimization
    criterion for implicit recommender systems Information Processing & Management,
    Volume 61, Issue 1, 2024, Article 103525 Ting Bai, …, Jian-Yun Nie View PDF Screening
    through a broad pool: Towards better diversity for lexically constrained text
    generation Information Processing & Management, Volume 61, Issue 2, 2024, Article
    103602 Changsen Yuan, …, Qianwen Cao View PDF NSEP: Early fake news detection
    via news semantic environment perception Information Processing & Management,
    Volume 61, Issue 2, 2024, Article 103594 Xiaochang Fang, …, Huaxiang Zhang View
    PDF Show 3 more articles Article Metrics Citations Citation Indexes: 1 Captures
    Readers: 8 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Information Processing and Management
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A scalable and flexible basket analysis system for big transaction data in
    Spark
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Cob-Parro A.C.
  - Lalangui Y.
  - Lazcano R.
  citation_count: '0'
  description: As the global population is expected to reach 10 billion by 2050, the
    agricultural sector faces the challenge of achieving an increase of 60% in food
    production without using much more land. This paper explores the potential of
    Artificial Intelligence (AI) to bridge this “land gap” and mitigate the environmental
    implications of agricultural land use. Typically, the problem with using AI in
    such agricultural sectors is the need for more specific infrastructure to enable
    developers to design AI and ML engineers to deploy these AIs. It is, therefore,
    essential to develop dedicated infrastructures to apply AI models that optimize
    resource extraction in the agricultural sector. This article presents an infrastructure
    for the execution and development of AI-based models using open-source technology,
    and this infrastructure has been optimized and tuned for agricultural environments.
    By embracing the MLOps culture, the automation of AI model development processes
    is promoted, ensuring efficient workflows, fostering collaboration among multidisciplinary
    teams, and promoting the rapid deployment of AI-driven solutions adaptable to
    changing field conditions. The proposed architecture integrates state-of-the-art
    tools to cover the entire AI model lifecycle, enabling efficient workflows for
    data scientists and ML engineers. Considering the nature of the agricultural field,
    it also supports diverse IoT protocols, ensuring communication between sensors
    and AI models and running multiple AI models simultaneously, optimizing hardware
    resource utilization. Surveys specifically designed and conducted for this paper
    with professionals related to AI show promising results. These findings demonstrate
    that the proposed architecture helps close the gap between data scientists and
    ML engineers, easing the collaboration between them and simplifying their work
    through the whole AI model lifecycle.
  doi: 10.3390/agronomy14020259
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agronomy All Article Types Advanced   Journals
    Agronomy Volume 14 Issue 2 10.3390/agronomy14020259 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editors Sara Álvarez
    Sergio Vélez Subscribe SciFeed Recommended Articles Related Info Link More by
    Authors Links Article Views 1170 Table of Contents Abstract Introduction Materials
    and Methods Results Discussion Conclusions and Future Lines Author Contributions
    Funding Data Availability Statement Acknowledgments Conflicts of Interest Abbreviations
    References share Share announcement Help format_quote Cite question_answer Discuss
    in SciProfiles thumb_up Endorse textsms Comment first_page settings Order Article
    Reprints Open AccessArticle Fostering Agricultural Transformation through AI:
    An Open-Source AI Architecture Exploiting the MLOps Paradigm by Antonio Carlos
    Cob-Parro 1,2,*, Yerhard Lalangui 1 and Raquel Lazcano 1 1 Atos Research and Innovation,
    Albarracin 25, 28037 Madrid, Spain 2 University of Alcalá, Departament of Electronics,
    Ctra. Madrid-Barcelona, km. 33600, 28805 Alcalá de Henares, Spain * Author to
    whom correspondence should be addressed. Agronomy 2024, 14(2), 259; https://doi.org/10.3390/agronomy14020259
    Submission received: 13 December 2023 / Revised: 15 January 2024 / Accepted: 22
    January 2024 / Published: 25 January 2024 (This article belongs to the Special
    Issue Data-Driven Agriculture: Remote Sensing and Machine Learning for Sustainable
    Farming Practices) Download keyboard_arrow_down     Browse Figures Versions Notes
    Abstract As the global population is expected to reach 10 billion by 2050, the
    agricultural sector faces the challenge of achieving an increase of 60% in food
    production without using much more land. This paper explores the potential of
    Artificial Intelligence (AI) to bridge this “land gap” and mitigate the environmental
    implications of agricultural land use. Typically, the problem with using AI in
    such agricultural sectors is the need for more specific infrastructure to enable
    developers to design AI and ML engineers to deploy these AIs. It is, therefore,
    essential to develop dedicated infrastructures to apply AI models that optimize
    resource extraction in the agricultural sector. This article presents an infrastructure
    for the execution and development of AI-based models using open-source technology,
    and this infrastructure has been optimized and tuned for agricultural environments.
    By embracing the MLOps culture, the automation of AI model development processes
    is promoted, ensuring efficient workflows, fostering collaboration among multidisciplinary
    teams, and promoting the rapid deployment of AI-driven solutions adaptable to
    changing field conditions. The proposed architecture integrates state-of-the-art
    tools to cover the entire AI model lifecycle, enabling efficient workflows for
    data scientists and ML engineers. Considering the nature of the agricultural field,
    it also supports diverse IoT protocols, ensuring communication between sensors
    and AI models and running multiple AI models simultaneously, optimizing hardware
    resource utilization. Surveys specifically designed and conducted for this paper
    with professionals related to AI show promising results. These findings demonstrate
    that the proposed architecture helps close the gap between data scientists and
    ML engineers, easing the collaboration between them and simplifying their work
    through the whole AI model lifecycle. Keywords: Agriculture; Artificial Intelligence;
    Machine Learning; Deep Learning; Internet of Things; DevOps; MLOps 1. Introduction
    According to a United Nations (UN) report (https://www.un.org/development/desa/en/news/population/world-population-prospects-2019.html,
    accessed on 12 December 2023), the global population is expected to grow to almost
    10 billion people by 2050. Considering this estimation, the World Resources Institute
    (WRI) calculates that, to keep up with this demand, food production will have
    to be increased by at least 60% in the same period [1]. However, to cope with
    that need, the agricultural land that would be needed exceeds by far the land
    available as of today. This is a concept referred to by the WRI as land gap, and
    it has towering climate implications: using that much land for agricultural purposes
    would destroy vital ecosystems, which in turn would contribute even more to climate
    change, with food production being nowadays already responsible for almost 25%
    of global greenhouse gas emissions [1]. In recent years, many lines of research
    have started to explore the possibility of solving the land gap problem via the
    application of cutting-edge technologies, making agriculture itself evolve toward
    a new era, which has already been called, amongst others, smart farming, precision
    agriculture, or Agriculture 4.0 [2]. This field of study has gained a lot of momentum
    in the last few years since it is considered one of the key contributors toward
    the UN’s Sustainable Development Goals (https://sdgs.un.org/goals, accessed on
    12 December 2023) introduced in 2015 in the 2030 Agenda for Sustainable Development
    (https://sdgs.un.org/2030agenda, accessed on 12 December 2023). Undoubtedly, Artificial
    Intelligence (AI) is a pivotal technology in the realm of smart agriculture, as
    noted in the works [3,4]. Its integration has revolutionized this field, opening
    up new avenues to enhance both the quantity and quality of crop yields, as well
    as to automate processes. This paves the way for intelligent, autonomous systems
    that can learn and make informed decisions. For instance, the study in [5] highlights
    how AI can reduce chemical use by up to 90%, showcasing its efficacy in optimizing
    agricultural production processes, leading to more efficient farming, increased
    productivity, and reduced environmental impact. In addition, Shankar et al. (2020)
    [6] present insights on how AI-driven strategies can refine crop protection, bolstering
    sustainable agriculture. This research demonstrates the significant improvements
    AI can bring to crop management strategies. Further exploring AI’s role in environmentally
    conscious agriculture, the authors in [7] review precision chemical weed management
    strategies and propose a new CNN-based modular spot sprayer. This innovation is
    a testament to how AI can be applied to develop more precise and efficient weed
    control solutions, reducing the overall chemical footprint in farming. Furthermore,
    the work carried out in [8] delves into how AI can alleviate environmental challenges
    posed by agriculture. It provides a comprehensive look at how AI can be implemented
    for more efficient crop production and monitoring while minimizing ecological
    footprints. Complementing these findings, Visentin et al. (2023) [9] investigate
    a mixed-autonomous robotic platform for precise weed removal in both intra-row
    and inter-row settings. This development underscores the role of AI in precision
    agriculture, illustrating how robotic systems can be specialized for tasks like
    exact weed control, which helps in reducing chemical herbicide reliance and supports
    sustainable agriculture. Additionally, the deployment of collaborative smart robots,
    as detailed in the work [10], represents a significant advancement. In this case,
    a group of robots leverages AI to optimize harvesting routes, thereby boosting
    crop collection volumes. This not only exemplifies the increasing autonomy in
    agricultural systems but also their efficiency and ecological responsibility.
    The integration of AI in agriculture, as mentioned, seeks not only to enhance
    productivity, but also to ensure the welfare and efficiency of both the machinery
    and the workforce, emphasizing the potential of AI in aiding human workers rather
    than substituting them. Implementing AI in conjunction with UGVs, for example,
    to assist workers in optimizing fruit harvesting, or to accurately distribute
    phytosanitary products, highlights how well humans and machines can work together.
    Such synergies not only optimize agricultural processes but also ensure the protection
    of the environment and the sustainability of resources. This approach seeks a
    balance, where technology serves human efforts rather than replacing them, ensuring
    that the insights and expertise of human intervention remain an integral part
    of the process. However, this inclusion comes with its own share of challenges
    to overcome before starting to consider its extensive adoption within the agricultural
    domain. The enormous development and extension of AI in recent years is undeniable
    [11,12,13]. It has spread so much that it has become a revolution, being applied
    in almost every sector imaginable: embedded finances [14], business value [15],
    transport management [16], medicine [17], industry 4.0 [18], and, of course, agriculture
    [19,20,21]. Precisely, agriculture is one of the domains where AI is gaining much
    importance. The automation of processes through Machine Learning (ML) or Deep
    Learning (DL) based models is expected to allow for the substitution of humans
    by machines (UGVs, UAVs), to perform repetitive and costly tasks. This, in turn,
    is expected to increase the performance and efficiency of the task at hand. However,
    the application of AI in production or operational environments in general, and
    in the agriculture domain in particular, still faces many challenges. One of the
    main open issues in AI right now is not so much the creation of AI models themselves,
    but their deployment in production environments, their maintenance throughout
    their entire life cycles, and the management of the huge datasets that are usually
    involved. Rapid changes in models and data need continuous updates in production
    systems. Asset management, including model versions and data, should be autonomous
    and optimal as much as possible. The true challenge of AI integration lies in
    adapting to increasingly rapid changes, optimizing that integration for real-world
    scenarios, and maintaining an organized workflow to implement these measurements.
    Right now, there is not a single solution for this, but rather a plethora of tools
    that AI practitioners such as data scientists or ML engineers need to master before
    even getting to use them, let alone considering their usage in industrial environments.
    In the current environment, it is up to the data scientist/ML engineer to study
    the different tools and assess which ones are most suitable to build the whole
    workflow/model life cycle. This makes the learning curve a steep one and hinders
    the acceptance of AI in environments where it creates complete disruption, generating
    distrust in a technology that is seen as a black box, such as agriculture. To
    address these issues, a new paradigm known as Machine Learning Operations (MLOps)
    has emerged. Its objectives are twofold: (1) automating the process of building
    ML models and deploying them to production; and (2) maintaining and monitoring
    these models throughout their whole life cycle to detect potential issues which
    could compromise the AI model’s performance and automate a response [22,23,24],
    increasing the efficiency and scalability, as well as reducing the potential risks.
    By embracing MLOps culture, developers unlock the advantages of optimized workflows,
    automated AI model deployment, and effective collaboration, leading to increased
    productivity and robustness, faster development cycles, and better performance
    of their AI models. Therefore, the objective of this paper is to provide a solution
    for some of the aforementioned open challenges. We propose an open-source AI architecture
    based on the MLOps paradigm to reduce the complexity of developing and deploying
    AI models in agricultural contexts. The proposed architecture seeks to improve
    upon the state-of-the-art MLOps methods by implementing a functional and tested
    architecture that is used by several AI stakeholders. This solution aims to (1)
    minimize the learning curve associated with managing AI models without a centralized
    MLOps platform and (2) promote the acceptance of AI in agriculture by presenting
    an integrating approach to develop and deploy AI models, store datasets, and even
    gather data from different sources of information. It supports state-of-the-art
    IoT communication protocols such as Message Queuing Telemetry Transport (MQTT)
    [25] and Hypertext Transfer Protocol (HTTP) [26]. Hence, the main contributions
    of this paper are the following: An AI architecture using open-source technologies
    for creating and producing AI models is presented, covering the whole life cycle
    of the AI model, from its creation to its deployment and monitoring. The architecture
    builds a workflow made of state-of-the-art tools that enable data scientists and
    ML engineers to work more efficiently and rapidly, solving many problems in their
    day-to-day lives. The architecture supports the access through different types
    of IoT protocols, such as HTTP and MQTT, enabling ease of access and communication
    with diverse devices. The system is able to run different AI models at the same
    time, making optimal use of the hardware resources available in the cluster where
    the platform has been deployed. The rest of the document is organized as follows.
    First, in Section 2.1, the related work is analyzed and presented. Then, the proposed
    architecture with its different components, which is the main contribution of
    this paper, is described in Section 2.2. Insights gathered from key stakeholders
    who have already been exposed to the platform are summarized in Section 3. Finally,
    the main conclusions are extracted in Section 5. 2. Materials and Methods This
    section covers two essential elements of our research. Section 2.1 “Related Work”
    presents an in-depth analysis of existing studies, establishing the background
    and highlighting potential contributions of our research. Section 2.2 “Proposed
    Architecture” outlines our distinctive strategy, describing the technologies and
    methods applied to demonstrate both the novelty and practicality of our proposed
    solution. 2.1. Related Work The integration of AI and other advanced technologies
    into agriculture represents a significant paradigm shift in one of the most essential
    industries. This transformation is driven by the convergence of various technological
    innovations, including ML, computer vision, Internet of Things (IoT), and big
    data analytics. Together, these technologies are reshaping agricultural practices,
    enabling more efficient resource management, enhancing productivity, and contributing
    to sustainability. However, the adoption and implementation of these technologies
    present unique challenges and opportunities. This section provides a review of
    the current related work in AI and technology integration in agriculture, exploring
    key developments, challenges, and future prospects. 2.1.1. Artificial Intelligence
    in Agriculture Agriculture, one of the oldest and most vital industries, has been
    transformed by the integration of AI. Computer vision systems are now capable
    of detecting pests and diseases with remarkable accuracy [27]. Predictive algorithms
    have been developed to forecast weather patterns, enabling farmers to make informed
    decisions [28,29]. However, the implementation of AI in agriculture is not without
    challenges. Data variability, lack of standardized datasets, and resistance in
    traditional farming communities have slowed progress [30,31,32,33,34]. Collaborative
    efforts between AI experts and agronomists are essential to bridge this gap [35].
    In addition to developing algorithms and Artificial Intelligences to increase
    field productivity, improving the efficiency of agricultural practices is crucial.
    This perspective is vital in addressing climate change and the environmental challenges
    observed in the last decade. Therefore, the digitization and application of AI
    in agricultural settings introduce new challenges, including data management,
    the utilization of crop-specific technologies, and the need for new infrastructure
    to process these data [36]. Another critical area where Artificial Intelligence
    is extensively used to mitigate environmental implications is in water management,
    which is one of the most critical factors in crop cultivation. Intelligent water
    management, administered precisely and efficiently, ensures that crops are adequately
    hydrated to yield the maximum amount of food while conserving water usage [37].
    However, this raises the same dilemma as previously mentioned. It necessitates
    a network of sensors measuring various environmental parameters, communicating
    these parameters to an infrastructure for data collection and processing, ultimately
    determining the opening of water valves. To address this need, this article proposes
    a platform enabling interaction with the environment through the use of AI models.
    2.1.2. Open-Source Architectures and MLOps The open-source movement has democratized
    access to cutting-edge tools, with platforms such as TensorFlow [38] and PyTorch
    [39] leading the way. This democratization has led to the emergence of MLOps,
    a practice that seeks to standardize and automate the AI model lifecycle [40,41].
    In agriculture, MLOps enables rapid deployment of solutions, adapting to changing
    field conditions [42]. It also fosters collaboration among multidisciplinary teams,
    ensuring that models are developed and maintained efficiently [43,44]. The field
    of MLOps, now integral to AI development, has found a significant application
    in agriculture. MLOps are dedicated to streamlining and automating the complete
    AI model lifecycle, encompassing development, deployment, and ongoing maintenance.
    In agriculture, this translates to the ability to swiftly deploy AI-based solutions
    capable of adapting to ever-changing field conditions. Efficient model deployment
    holds immense value in agriculture, where real-time decision making is paramount
    for maximizing productivity and resource efficiency. For instance, computer vision
    algorithms for early disease detection can trigger rapid, precise responses, potentially
    averting significant crop losses [45]. Some studies have concentrated on the MLOps
    paradigm from a theoretical point of view [46], while others have presented various
    tools which could be applied in an MLOps platform [40,47]. Additionally, several
    studies have implemented portions of the overall solution [48,49]. However, there
    is a noticeable gap in the MLOps literature, providing a solution that integrates
    all these components, offering a first approach for a functional and tested MLOps
    architecture. 2.1.3. Smart Agriculture and Agriculture 4.0 Agriculture 4.0 is
    the next frontier, representing the convergence of digital and physical technologies
    in the agricultural sector [50,51]. IoT sensors are now used to monitor soil moisture
    and autonomously activate irrigation systems [52,53]. Robotics has found applications
    in tasks such as harvesting and weeding [54]. Big data analytics and AI are being
    used to optimize resource allocation and increase productivity [55,56,57]. However,
    interoperability between different devices and platforms remains a significant
    challenge, requiring further research and standardization [58,59]. In recent years,
    the concept of Agriculture 4.0, often referred to as “Smart Agriculture”, has
    gained substantial attention in the research and industry communities. This transformative
    approach harnesses the power of digital technologies to address various challenges
    in agriculture. One of the key aspects of Agriculture 4.0 is the utilization of
    IoT devices and sensors for real-time monitoring and control of agricultural processes.
    These devices collect data on soil conditions, weather patterns, crop health,
    and equipment status. With the help of AI algorithms, these data are processed
    to make informed decisions regarding irrigation, pest control, and crop management
    [60,61]. Furthermore, the integration of robotics in agriculture has marked a
    significant advancement. Autonomous drones and robotic systems are employed for
    tasks such as precision planting, harvesting, and weed control. These technologies
    not only enhance the efficiency of operations but also reduce the need for manual
    labor, addressing labor shortages in agriculture [62,63]. Moreover, the use of
    robotics contributes to minimizing the environmental impact by enabling targeted
    application of resources [64]. The role of big data analytics cannot be overstated
    in the context of Agriculture 4.0. Large volumes of data are generated from various
    sources, including sensors, satellites, and machinery. Advanced analytics techniques,
    such as Machine Learning and data mining, are applied to extract valuable insights
    from these data. These insights enable farmers and agricultural professionals
    to make data-driven decisions that optimize crop yield, resource utilization,
    and sustainability practices [55,56,57]. 2.1.4. User Experience in Agricultural
    Systems The integration of technology into agriculture must be user-centric. Systems
    must be adapted to work in remote areas with limited connectivity [65,66]. Interfaces
    have to be designed to be used without requiring deep technical knowledge [67].
    Training and support are crucial components to ensure that technological solutions
    are adopted and used effectively in the field [68]. Moreover, cultural and socioeconomic
    factors must be considered to create solutions that are truly aligned with the
    farmers needs [41,45]. Effective training and ongoing support are indispensable
    components of successful technology adoption in agriculture. Farmers and agricultural
    workers need proper training to harness the full potential of technology-enabled
    solutions [68]. This includes not only understanding how to operate the technology
    but also interpreting the data it provides. Support mechanisms, whether through
    local agricultural extension services or remote tech support, should be readily
    available to address issues and provide guidance as needed. Agricultural technology
    solutions should be designed with a deep understanding of the cultural and socioeconomic
    factors that influence farming practices. Different regions and communities have
    unique needs and preferences, which should be taken into account during the development
    of agricultural systems [45]. Solutions that align with local customs and preferences
    are more likely to be embraced by the farming community. 2.1.5. Recent Developments
    and Future Directions Recent developments in AI and technology have opened new
    paths for innovation in agriculture. Advanced weather prediction models are providing
    more accurate forecasts, allowing for better planning and resource management
    [29]. New methodologies are being developed to address data variability and standardization
    challenges [31]. Community engagement strategies are being explored to overcome
    resistance in traditional farming regions [34]. Collaborative AI development is
    fostering a more inclusive and efficient approach to technological innovation
    [44]. Edge computing, an emerging paradigm in agriculture, holds significant promise
    for enhancing real-time data processing and decision making at the farm level.
    By deploying edge devices equipped with AI capabilities, such as field-based sensors
    and edge servers, farmers can analyze data directly at its source. This approach
    reduces latency and allows for quicker responses to changing environmental conditions
    [61]. Edge computing is particularly beneficial for applications such as precision
    irrigation, where timely data insights can optimize water usage and crop health.
    As edge computing technologies continue to mature, their integration into agricultural
    systems is expected to grow, further improving farm efficiency and sustainability.
    In light of increasing climate variability and extreme weather events, resilience
    in agriculture has become a paramount concern. AI-driven solutions are aiding
    farmers in adapting to these challenges. Machine Learning models can analyze historical
    weather data and predict potential climate-related risks, enabling farmers to
    implement mitigation strategies and protect their crops [60]. Furthermore, AI-powered
    drones equipped with thermal imaging cameras can identify stress in crops caused
    by heatwaves or water shortages, allowing for targeted interventions [63]. Building
    climate-resilient agricultural systems through AI technologies is vital for ensuring
    food security in the face of a changing climate. The global agricultural community
    is increasingly recognizing the importance of collaboration and data sharing in
    addressing common challenges. Initiatives such as the Agricultural Model Intercomparison
    and Improvement Project (AgMIP) are bringing together researchers, farmers, and
    policymakers to share data and develop models that enhance agricultural sustainability
    [32]. AI plays a pivotal role in integrating diverse datasets and facilitating
    collaborative research efforts. Shared data resources, including crop performance
    data, weather data, and pest and disease monitoring data, empower stakeholders
    to make informed decisions and collectively work towards more resilient and sustainable
    agricultural practices. 2.1.6. Summary The landscape of AI in agriculture is rich
    and diverse, with notable developments in technology and user-centered approaches.
    As the industry advances towards the so-called agriculture 4.0, a comprehensive
    approach is essential, considering both technical capabilities and human needs.
    Open-source architectures, MLOps, smart agriculture technologies, and user experience
    design are key components of this evolving landscape. This article seeks to address
    this balance, offering the first open-source AI architecture that presents an
    integrated approach to cover the whole lifecycle of AI models. It facilitates
    the collaboration of data scientists and ML engineers and provides support for
    some of the most widely known IoT protocols, hence proving its differential value
    for the agricultural domain. 2.2. Proposed Architecture In response to the challenges
    faced by the agriculture industry to meet the growing global demand for food production
    while ensuring sustainability and the limitations of available agricultural land,
    innovative technological solutions have become imperative. This section proposes
    an architecture that aims to address these challenges by integrating AI in the
    agricultural landscape. By leveraging open-source technologies and embracing the
    principles of MLOps, the proposed architecture offers a comprehensive framework
    to address the complexities associated with the entire lifecycle of AI models,
    from development to deployment and maintenance. This section is organized as follows:
    Section 2.2.1 introduces the fundamental concepts of AI workflows, examines the
    professional profiles involved, and details the MLOps workflow. Section 2.2.2
    presents the architecture proposed in this paper and the technologies employed
    to shape it. Finally, Section 2.2.3, Section 2.2.4 and Section 2.2.5 delve into
    each technology used in designing the proposed MLOps architecture. 2.2.1. Fundamentals
    To create a new AI model, developers must follow some well-known sequential steps:
    preprocess and adapt the input data; design, train, and test the model; and finally,
    deploy the model into a production environment. Even though one could consider
    that the hardest parts are the first two steps (which usually conform the so-called
    development stage), it should be noted that, indeed, the last one is crucial,
    as it implies transitioning the model from a prototype to a real-world product
    (e.g., production stage). When starting to experiment with AI models, developers
    normally use local environments in their local machines. While this method may
    perform sufficiently well for small, local projects, it starts to be impractical
    when the models, data, and projects grow. Specifically, some of the main limitations
    of using local environments are Data management: the input dataset for an AI model
    is stored on the same local machine where the AI model is trained and tested.
    This makes it impossible to update the dataset in the case of new data being incorporated,
    as the memory consumption of that local machine would render the infrastructure
    inoperable. AI model versioning: tools that allow for the versioning of different
    trained models are not used, since this process normally involves exchanging considerable
    amounts of data. Teamwork: if a team is working on a local machine, the design
    of pipelines can only be performed and manipulated by one person. Production deployment:
    This process often involves additional considerations such as scalability, reliability,
    security, and monitoring. Furthermore, production deployment can hinder problem
    solving in a production environment, posing challenges when addressing errors
    efficiently and promptly. Therefore, in more complex production environments,
    additional techniques may become useful to simplify the management, deployment,
    and monitoring of software developments in a real-world setting. These techniques
    aim to ensure that the deployment process is efficient and robust, capable of
    handling the complexities and demands of a live environment. The term used for
    this culture or set of good practices is Development Operations or DevOps, which
    combines software architecture with software development, designing software environments
    that optimize the deployment of applications in high-stress environments. Additionally,
    in more places, security is taking an important place in DevOps culture, extending
    to the DevSecOps paradigm [69]. This shows a growing recognition of the key role
    of security in the development and operations processes, integrating it into every
    step of the software lifecycle to ensure more resilient and safer applications.
    When applied to ML, this paradigm is known as MLOps, primarily concentrating on
    the deployment of AI models in production. MLOps exhibits certain differences
    compared to the original DevOps paradigm, mainly stemming from the unique requirements
    of AI models, such as model and data versioning and management. These requirements
    include the need for training with large amounts of data and often require periodic
    retraining due to widely known issues, such as data drift. Consequently, methodologies
    and tools that confer agility to the platform must be employed to effectively
    support these requirements. With the increase in the complexity of the models
    to develop, the activities that developers must carry out in each of the stages
    mentioned before (development and production) also increase in complexity. In
    the development stage, developers iterate on several model hyperparameters, evaluate
    different algorithms, and improve the dataset quality. The result of this stage
    includes Training Code: Normally, a Jupyter Notebook [70] is used for experimentations,
    so it contains the code used for model training, hyperparameter tuning, and evaluation.
    Trained Model with Artifacts and Versioning: The developers produce a trained
    model along with its associated artifacts. These artifacts are versioned to ensure
    reproducibility and allow for easy tracking of model changes. Stored and versioned
    Dataset: The dataset used for training is also versioned to maintain a record
    of its evolution. In the production stage, developers focus on turning the Jupyter
    notebook into a production-ready system using ML pipelines, which bring several
    advantages, such as versioning, containerization, and collaboration. The steps
    involved in this stage are ML Pipelines: These pipelines automate key tasks, including
    data preprocessing, feature engineering, model (re)training, and evaluation. ML
    pipelines improve code modularity, facilitate version control, and enable collaboration
    among team members. Continuous Integration and Delivery (CI/CD) Pipeline: This
    pipeline includes steps such as formatting checks, execution of unit tests, and
    documentation verification. CI/CD ensures code quality, detects errors early,
    and allows for rapid iteration and deployment. Containerization and Deployment:
    In this step, the model, its dependencies, and the ML pipeline are encapsulated
    into a container, ensuring consistency across different environments. The containerized
    model is then deployed in the production infrastructure, ready to serve predictions.
    Considering the stages in which the AI generic workflow is divided and the complexity
    that each of them entails two specialized roles come about: data scientist and
    ML engineer. A Data scientist is responsible for gathering and preprocessing data,
    exploring and analyzing datasets, developing and training ML models, and evaluating
    their performance. Data scientists take the lead in the model development stage,
    experimenting with different algorithms, hyperparameters, and dataset improvements.
    An ML engineer specializes in implementing and operationalizing ML models in production
    environments. They bridge the gap between data science and software engineering,
    focusing on deploying, scaling, and maintaining ML systems. ML engineers work
    on developing robust ML pipelines, optimizing model performance, designing scalable
    architectures, and ensuring the reliability and scalability of production systems.
    They define formatting checks, unit tests, and documentation requirements to maintain
    code quality and ensure successful deployment. Although they have differentiated
    tasks, collaboration between data scientists and ML engineers is a key part of
    an AI model’s lifecycle. In terms of knowledge sharing, data scientists and ML
    engineers must regularly communicate and exchange insights, challenges, and discoveries.
    In production stages, ML engineers collaborate with data scientists to deploy
    the ML pipelines in the production infrastructure. They containerize the model
    and its dependencies for easy deployment and scalability. Both roles need to work
    closely to establish monitoring mechanisms for the deployed model, collecting
    real-time performance data and iterating on the model or making necessary updates
    based on the feedback. The challenge of collaborative work arises from the significant
    divergence in the technologies utilized by these two roles. The environments and
    procedural steps followed by each role exhibit notable disparities. This dichotomy
    often leads to data scientists and ML engineers operating in separate spheres,
    thereby sharing tools with limited efficiency (Figure 1). Hence, the objective
    of this endeavor is to create a framework based on an open-source architecture,
    capable of identifying synergies between these distinct roles. By fostering a
    more cohesive and collaborative approach, this framework aims to amplify productivity
    and performance, enabling seamless teamwork (Figure 2). Figure 1. Isolated working
    method for data scientists and ML engineers. Figure 2. Solution proposed in this
    paper. The proposed platform’s high-level architecture, as depicted in Figure
    3, highlights three key pipelines: a data management pipeline, a model development
    pipeline, and a model deployment in production pipeline. This design ensures a
    comprehensive framework that seamlessly integrates these essential aspects of
    AI model development and facilitates efficient collaboration between different
    AI stakeholders. Figure 3. Basic concepts MLOps. Data Management is a pivotal
    process responsible for the comprehensive handling of raw data, from its acquisition
    to its transformation into a standardized format. This process is composed of
    several distinct submodules, each with its specialized functions: Data Analysis
    Submodule: This module is tasked with assessing and integrating new data into
    the existing dataset. It maintains a connection with the logs from the Train,
    Test, and Fine-tuning Model submodule, offering insights into training performance
    and facilitating potential adjustments to the dataset format. As an example, Great
    Expectations tool [71] is an open-source Python-based library to ensure the reliability
    of data by asserting certain “expectations” or quality assessments on datasets.
    Data Versioning Submodule: Through meticulous documentation, areas for dataset
    improvement can be identified and the introduction of new dataset versions is
    registered, ensuring the maintenance of a dynamic dataset. As can be seen in the
    previous figure, DVC [72] is another open-source tool designed for versioning
    datasets, model weights, and intermediate files, enabling reproducibility and
    efficient data pipeline tracking. Data Storage Submodule: A plethora of technologies
    is available for data storage, such as MinIO, Amazon S3, PostgreSQL, etc. Notably,
    this submodule interacts directly with the Data Preprocessing submodule within
    the ML pipeline. The ML pipeline serves as the foundational element in the proposed
    architecture, encompassing the programming, training, and testing of the AI model.
    This pipeline is structured into three distinct submodules: Data Preprocessing:
    This submodule retrieves data from the database, undergoes cleaning operations,
    and restructures them to ensure the model’s optimal training. It is integral to
    the ML pipeline as it preprocesses data tailored for a specific AI model. Furthermore,
    the output of this submodule is interconnected with the Model Monitoring submodule
    in the Production Environment, ensuring the reference data remains updated. Train,
    Test, and Fine-tuning Model: This submodule ensures that the model’s accuracy,
    reliability, and performance are aligned with the desired outcomes. If data restructuring
    or modifications are required, a comprehensive Data Review is imperative, needing
    the invocation of the Data Analysis submodule again. Registry Model: This submodule
    is dedicated to the preservation of the highest-performing AI models which were
    trained in the previous submodule. MLFlow tool could be used as an example to
    register a trained model [73]. As a model registry tool, it centralizes model
    management, tracks versions, and facilitates lifecycle transitions. The model
    deemed superior is subsequently deployed in the production environment, bridging
    the Registry Model submodule with the Best Model in Production submodule in the
    Production Environment pipeline. To complete the lifecycle of both the AI model
    and data within a tangible system, the deployment into the production environment
    is crucial. Data Preprocessing in Production: Upon collection, raw data undergoes
    preprocessing via the Data Preprocessing submodule. These refined data are then
    fed to the Model Monitoring submodule, where its coherence is meticulously analyzed.
    Best Model in production: The processed data from the previous component is then
    channeled into the submodule dedicated to model inference. Model Monitoring and
    Alerts: The output data are further integrated into the Model Monitoring submodule
    for an in-depth analysis. Should the data received from the sensors and the model’s
    output diverge beyond the acceptable variance, the system triggers an alert, highlighting
    the potential need for training a novel model. 2.2.2. Technological Implementation
    As outlined in the previous section, this study presents a framework detailing
    the key functions an MLOps platform should include to overcome limitations commonly
    found in sequentially based programs. This section offers a detailed review of
    open-source tools that facilitate the effective design and implementation of this
    platform, particularly for its use in agriculture. Open-source technologies offer
    several key advantages for research and development. Firstly, they promote transparency,
    allowing researchers to access and inspect the source code, ensuring a high level
    of trust and credibility in the results. Secondly, they foster collaboration by
    encouraging a global community of developers to contribute, resulting in continuous
    improvement and innovation. Additionally, open-source tools are often cost-effective,
    making them accessible to a wider range of researchers and organizations. Lastly,
    they provide flexibility and customization options, enabling personalized solutions
    to specific research challenges. These benefits collectively enhance the efficiency,
    reliability, and impact of research projects, since the only cost associated to
    the usage of the proposed architecture would be associated to the underlying hardware
    infrastructure. For illustrative purposes, the work presented in here is supported
    by a server cluster with a total of 180 cores of varied types and 300 GB of RAM
    memory. Additionally, this cluster is equipped with two GPUs, specifically Nvidia
    T4 and A100. The utilization of this architecture in agricultural settings is
    justified for several reasons. First, these systems handle a large volume of inputs
    and outputs, generated by sensors in various agricultural tools. Due to this complexity,
    it is essential to have architectures that enable agile deployment and deactivation
    of tools, a function efficiently fulfilled by the proposed architecture, as will
    be explained further in the document. Secondly, when used in agricultural contexts,
    these systems often rely on the Internet of Things (IoT) paradigm [74], which
    normally involves a network of interconnected physical devices, each equipped
    with sensors, software, and connectivity options. These features not only facilitate
    data exchange but also enable interactions with the environment. In this context,
    the primary aim of the architecture is to seamlessly integrate these IoT agents
    in agricultural settings with existing production models. To achieve this, the
    platform uses well-known IoT protocols, such as MQTT (designed for device-to-device
    communication) and HTTP (used for data packet exchange in cloud environments).
    These protocols also contribute to the system’s communication redundancy and agility,
    ensuring that, in the event of a channel failure, devices can continue to communicate
    via alternative routes. This strategic integration allows data scientists and
    ML engineers to develop or fine-tune models more efficiently, as previously outlined
    in Section 1. The proposed architecture is built on an on-premise Kubernetes platform,
    which is a leading tool for container orchestration. This foundation provides
    strong stability and resilience for the included applications. Using Kubernetes’
    features allows the platform to scale, easily deploy applications, automate management,
    and recover from faults. Kubernetes serves as the fundamental building block of
    this solution, guiding the deployment and management of application containers.
    These containers, which host the different components of our architecture, are
    light and portable, ensuring consistency across various deployment environments
    and simplifying packaging processes. The architecture is illustrated in Figure
    4 which is composed of DevOps-based tools, such as Kubernetes, MQTT, CamelK, and
    state-of-the-art tools which support AI tasks, such as Kubeflow, MLFlow, and Kserve.
    Figure 4. Specialized MLOps Architecture for Agricultural Applications. It clarifies
    the key components and how they interact for maximum efficiency. As a result,
    the overall architecture, supported by Kubernetes, is designed to work smoothly
    with the open-source tools. This approach aligns with the concept displayed in
    Figure 3, with later sections delving into the specific role of each open-source
    tool within the MLOps framework. It is important to note that the code and setup
    instructions for the previously described architecture are openly accessible through
    the official repository of the FlexiGroBots European project [75], which aims
    to build a platform for intelligent automation of precision agriculture operations.
    For public interaction and model inference, a dedicated website is available at
    [76]. Moreover, the platform offers a centralized workspace exclusively for data
    scientists and ML engineers involved in the FlexiGroBots European project [77].
    This exclusive access to this common entrypoint allows for seamless AI model sharing,
    development, and collaboration among them. Subsequent subsections will delve deeper
    into the three pipelines/workflows previously outlined in earlier sections and
    depicted in Figure 3, providing a comprehensive overview of the architecture.
    2.2.3. Data Management The platform described in this document starts with a module
    called “Data Management”. As shown in Figure 3, this module’s scope encompasses
    the storage, versioning, and provision of data access. In the development and
    deployment of AI models, efficient storage and retrieval of large datasets and
    models is crucial. While Kubernetes offers native storage solutions, they come
    with limitations, particularly in terms of flexibility and mobility. To address
    these constraints, a specialized tool that enhances storage flexibility has been
    employed, allowing for dynamic adjustments as required by the system. To address
    this, the Kubernetes Network File System (NFS) Subdir External Provisioner is
    used. This tool allows for the creation of volumes by every component in the cluster
    using an NFS server and makes storage more flexible by dynamically setting up
    subdirectories on that server. In light of this dynamic storage management approach,
    MinIO is identified as the ideal open-source tool for handling data and versioning.
    Known for its high performance, MinIO is a distributed object storage system designed
    for handling large amounts of data and providing fast data access. It offers features
    such as scalability, fault tolerance, and specialized performance for object storage
    tasks. When combined with Kubeflow (which will be explained in the following subsection),
    MinIO acts as a robust storage layer capable of archiving models and datasets.
    Additionally, MinIO is user-friendly due to its easy-to-use interface and straightforward
    setup process, making it easy for users to quickly implement MinIO for their storage
    needs. The MinIO platform addresses the Data Management pipeline by establishing
    communication with other submodules of the platform via HTTP. Furthermore, it
    is capable of storing data using the bucket storage format. In addition, MinIO
    provides a system for versioning the buckets used for model training. 2.2.4. ML
    Pipeline In order to create the ML pipeline depicted in Figure 3 and leverage
    Kubernetes [78] technology as the foundation of the platform, a Kubeflow solution
    [79] was configured and deployed. Kubeflow provides a comprehensive suite of tools
    that enable data scientists and ML engineers to develop, train, deploy, and monitor
    ML models effectively. When installing Kubeflow, Istio [80] is also set up. Istio
    manages microservices, enhancing security and directing traffic between tools
    in the cluster. The adoption of this tool arises from the need to secure both
    the inputs and outputs of a system when utilizing open-source technologies, as
    illustrated in [81]. Since these tools typically lack a dedicated company to address
    bugs, the responsibility falls on the community. This underscores the importance
    of employing encrypted and secure systems, such as those facilitated by Istio.
    Thanks to this service mesh, we can implement TLS encryption in both external
    communications to the architecture and internal communications between the infrastructure’s
    internal modules, allowing us to secure both the internal and external connections
    with the architecture. This ensures smooth and secure communication between these
    tools which will be used by data scientists and ML engineers. By leveraging Kubeflow’s
    features, users can focus on building robust ML models while minimizing their
    involvement in underlying infrastructure complexities. Kubeflow encompasses various
    tools and components tailored to different stages of the ML workflow: For data
    preparation and task orchestration for training, Kubeflow Pipelines [82] offers
    a visual interface that facilitates the design and execution of data processing
    pipelines or automated model training. In terms of experimentation, development,
    and model training, Kubeflow integrates with popular ML frameworks such as TensorFlow
    [83], Pytorch [84], and scikit-learn [85]. Experiments and developments can be
    carried out by using its integrated Jupyter Notebook implementation. Jupyter Notebook
    is a web-based tool for creating, sharing, and executing files containing live
    code, visualizations, and explanatory text. These Jupyter Notebooks are managed
    as Docker [86] containers by Kubeflow, easing their deployment and versioning.
    Kubeflow provides tools such as Katib [87] for studying model hyperparameters
    during training. Regarding model serving and deployment, Kubeflow supports the
    deployment of models as scalable, production-ready services using Kubernetes [78]
    serving frameworks such as Tensorflow Serving [88] or Seldon Core [89] along with
    KServe [90]. As for monitoring and observability, users can monitor model performance,
    track key metrics, and set up alerts to ensure the models function as intended.
    As shown in Figure 5, a Machine Learning Operations (MLOps) process was set up
    using pipelines. This involves various stages of developing, using machine learning
    models in a Kubernetes environment, with Kubeflow playing a major role. This specific
    process is focused on agricultural applications, where it is used by agronomists.
    Figure 5. Kubeflow architecture. The first step is collecting data from different
    sources. These data are then preprocessed, which includes cleaning, normalizing,
    and extracting features to make it ready for training models. Following the preprocessing,
    the Kubeflow toolkit comes into play. Renowned for its robustness in managing
    machine learning workflows in Kubernetes, Kubeflow handles the dataset extraction.
    This step is pivotal in structuring the data into a format that is optimized for
    subsequent machine learning processes. Additionally, after the data processing,
    data analysis techniques for anomaly detection are employed, as demonstrated in
    the work cited [91], used to ensure the quality of time series-based datasets.
    This is to ensure the quality and balance of the data when training the models.
    Next, successful models are registered. This involves keeping track of different
    versions and their details. Then, these models are deployed and available for
    practical use, being, in this step, a key part of the project focused on agricultural
    engineering. As a result of this pipeline, several models have been integrated
    into an easy-to-use web platform [76] and are also accessible via the Kserve API-REST,
    allowing users to obtain information directly. Agronomists, the main users of
    this system, interact with the models via a web interface. They input real-world
    agricultural data and obtain back predictions for analysis. This use of the models
    shows how they can provide helpful insights for agricultural engineering. In conclusion,
    this MLOps process combines data processing, machine learning, and workflow management
    in Kubernetes. It is designed to meet the needs of agronomists, showing how machine
    learning can be applied effectively in agriculture. 2.2.5. Production Environment
    The architecture discussed in this paper has been designed to support the use
    of AI models after their development and deployment. This focus is particularly
    on key elements of MLOps, such as sensor interaction with the system and AI model
    deployment. It is important to note that sensors usually communicate via the HTTP
    protocol. Therefore, these types of data can be easily processed using KServe’s
    API. However, in the agricultural setting, sensors often come in arrays that generate
    large data streams. These arrays are managed using another protocol called MQTT,
    which is designed for real-time applications and is efficient in terms of bandwidth
    and computational resources. MQTT uses a publish–subscribe system, ensuring efficient
    and direct data exchange between data sources from sensors and AI models. To facilitate
    this, the architecture has been adapted to allow KServe to interact with the MQTT
    protocol. Two components have been added to the platform, enabling models hosted
    on KServe to collect sensor data, perform inferences, and return the results via
    MQTT or HTTP, depending on the project’s needs. To access MQTT messages in the
    broker, CamelK [92] is used, acting as an HTTP–MQTT connector. This allows data
    to be served to KServe modules, which can then redirect the information via HTTP
    or MQTT as needed. Given the nature of the agricultural field, another important
    challenge was anticipating and addressing potential communication and data management
    challenges. To ensure robust and efficient data handling, the presented architecture
    incorporates two methodologies. Firstly, asynchronous-type protocols, such as
    MQTT, have been used. This strategy serves a dual purpose: it not only distributes
    data loads efficiently across communication channels, but also fortifies the system’s
    overall resilience. In this protocol, when a broker receives a topic, it retains
    the information until a subsequent request is made, assuring that, upon system
    connection, the necessary data are dispatched promptly. Additionally, MQTT provides
    a unified communication channel for all sensors in the agricultural ecosystem,
    from drones to autonomous tractors, streamlining automated management processes.
    To complement this, as mentioned in Section 2.2.3, a storage system known as a
    data lake has been installed. Situated within the same cluster, this storage solution
    guarantees quick and direct access to crucial data for all managing components.
    3. Results The following section presents the results and findings obtained from
    the evaluation of the platform presented in this paper. Firstly, we have demonstrated
    practical applications of the graphical user interface, specifically designed
    to facilitate the accessibility of inference systems for individuals with non-technical
    backgrounds. Secondly, a thorough analysis was conducted to assess the performance,
    usability, and effectiveness of the proposed architecture. As part of this process,
    potential users of the platform have been surveyed regarding the usage of this
    solution. This evaluation aimed to validate the platform’s capabilities and its
    potential impact in addressing the challenges outlined in Section 1. The results
    provide valuable insights into the platform’s qualitative assessments, comparative
    analysis against existing solutions, scalability and efficiency, as well as real-world
    use cases. Additionally, limitations identified during the evaluation are discussed,
    and potential future directions for further improvement are suggested. Finally,
    the platform’s evaluation of its position within the broader scope of MLOps must
    be considered. This is particularly important as MLOps is rapidly evolving and
    becoming more prominent in fields such as agriculture. As a recent research study
    shows, MLOps primarily aims to streamline AI operations, improve collaboration,
    and facilitate the transition from AI model design to deployment [93]. The survey
    data, especially from AI professionals, supports the platform’s ability to effectively
    bridge the gap between data scientists and ML engineers, a sentiment also highlighted
    in [94]. The section is organized as follows: first, Section 3.1 presents the
    designed GUI for end-users to interact in a visual and simple way with inference
    systems. Second, Section 3.2 presents a summary of how different stakeholders
    can use the proposed architecture; then, Section 3.3 outlines the contents of
    the survey conducted, together with the insights extracted from them. 3.1. Web
    Platform In this work, a Graphical User Interface (GUI) (Figure 6) has been developed
    for individuals who lack a technical background in making queries to REST APIs
    or in deploying AI models. Therefore, in the GUI, four inference models have been
    deployed for use by Agricultural Engineers. In addition, this GUI is public [76]
    via internet. Figure 6. Web platform with agriculture tools. In this study, four
    distinct Artificial Intelligence models have been deployed for specific agricultural
    applications. These models represent cutting-edge integrations of AI in the realm
    of agriculture, addressing diverse challenges faced by the industry. Firstly,
    a model dedicated to the detection of tractors utilizing computer vision techniques
    has been implemented (top left corner in Figure 6). This model exemplifies the
    application of image recognition technologies in agricultural settings, enabling
    enhanced monitoring and management of farming equipment. Secondly, the project’s
    focus extends to the precise detection of Botrytis, a significant fungal disease
    affecting various crops (top right corner in Figure 6). Leveraging AI, a model
    that facilitates targeted spraying through advanced detection methods has been
    developed. This approach allows for precise application of fungicides, optimizing
    resource use and minimizing environmental impact. Thirdly, pest management is
    addressed via the deployment of an AI model capable of detecting and counting
    insects in traps (bottom left corner in Figure 6). This model facilitates the
    monitoring process, providing accurate, real-time data that is crucial for effective
    pest control strategies. Finally, a model for the detection and quantification
    of weeds has been introduced (bottom right corner in Figure 6). This model aids
    in the identification of unwanted flora, a key task for crop management and yield
    optimization. By accurately recognizing and counting weed species, this model
    supports more effective and environmentally conscious weed control practices.
    Collectively, these AI models demonstrate the potential of Artificial Intelligence
    to revolutionize various aspects of agricultural operations, offering innovative
    solutions to longstanding challenges in the field. 3.2. How to Use the Platform
    As introduced in Section 2.2, one of the main motivations of the work presented
    in this paper is to provide a comprehensive architecture that eases the collaboration
    between two clear and distinct professional profiles that have arisen in these
    last years: data scientists and ML engineers. Without this architecture, the collaboration
    between these roles is not efficient, which results in limited job parallelization.
    Typically, the ML engineer waits for the data scientist to finalize the model’s
    preparation and tuning. Then, the ML engineer must build the production infrastructure
    from scratch. If issues arise, adjustments by the data scientist are followed
    by reintegration by the ML engineer. These development and deployment processes
    are lengthier due to the required coordination between both roles, as depicted
    in Figure 7a. Figure 7. Comparative using standard and MLOps method. (a) Development
    and deployment without MLOps platform. (b) Development and deployment with MLOps
    platform. The architecture proposed in this paper (Figure 4) provides an ideal
    solution for this problem, empowering each role to excel in their respective stages.
    The proposed method delivers results in significantly shorter times compared to
    the traditional development methodology described above. In this process, collaboration
    between the data scientist and the ML engineer is crucial, defining the AI solution
    before development begins. Figure 7b illustrates an initial “agreement” step,
    outlining the pipeline components and their respective inputs and outputs. Once
    the data scientist is clear about their role and understands the ML engineer’s
    procedure, they can start designing AI solutions using Jupyter notebooks provided
    by Kubeflow. They also have the option to use Katib for hyperparameter tuning
    studies. Concurrently, to expedite the process, the ML engineer begins designing
    pipeline components, such as data collection and AI model deployment in KServe.
    Since this stage can also be executed with Kubeflow’s Jupyter notebooks, the ML
    engineer can integrate preliminary versions of the model into the full pipeline.
    Upon the data scientist’s model design completion, the ML engineer integrates
    the model, gathering the necessary weights for its pipeline implementation. During
    this phase, the ML engineer can also adjust the model’s input protocol, incorporating
    data from HTTP or MQTT. Thus, the ML engineer prepares the AI model for actual
    production deployment. The outcome is a comprehensive and efficient pipeline covering
    the entire process of development, testing, and deployment of an AI model. Finally,
    the proposed architecture allows data scientists and ML engineers to track the
    performance of the deployed model in real time by using the same platform. They
    can collect relevant metrics, monitor the model’s behavior, and make iterative
    improvements based on the feedback received. This feedback loop ensures continuous
    enhancement and adaptation of the model to changing requirements. 3.3. Conducted
    Interviews To assess the utility and effectiveness of the proposed platform, interviews
    were conducted with six data scientists and five ML engineers who are involved
    in diverse agricultural field activities. The interviewees possessed mid-level
    seniority and have been working in the machine learning field for the last 3–5
    years, giving a balanced perspective of experience and current industry practices
    to the study. Therefore, it has been determined that the responses provided by
    the respondents carry equal weight, as they all possess the same level of expertise.
    In Section 3.3.1, the contents and structure of the survey are presented. Afterwards,
    in Section 3.3.2, the results obtained during the surveys are presented in a pointed
    format from 1 to 5. Additionally, metrics such as the mean and standard deviation
    have also been extracted to statistically comprehend the results. 3.3.1. Contents
    and Structure of the Survey To evaluate the usability and adaptability of the
    platform in general, the surveyed professionals were invited to implement the
    platform in collaborative projects, aiming to understand its adaptability and
    performance in a real-world setting. One of the first aspects examined was the
    learning curve, measuring the time it took for professionals to become comfortable
    using the platform’s tools and architecture in an actual project. The choice to
    center this evaluation on interviews with AI professionals arises from the intention
    to ensure that the proposed platform addresses tangible challenges in real-world
    settings. The insights gained from these interviews offer a comprehensive view
    of the platform’s capabilities. This emphasizes its effectiveness in closing communication
    and operational gaps between data scientists and ML engineers [93]. This human-centric
    evaluation methodology highlights the commitment to utility and adaptability.
    Additionally, special attention was given to understanding how the platform would
    impact the team dynamics. Factors such as collaboration and efficient task division
    were evaluated to determine whether the platform facilitates or complicates these
    processes. This is crucial, as any tool, no matter how advanced, must effectively
    integrate into a team’s workflow to be genuinely useful. Another area of focus
    was the interaction between different professional profiles. The aim was to assess
    whether the platform simplifies communication and collaboration between data scientists
    and ML engineers, who often need to coordinate closely but from different technical
    perspectives. The study also explored the transition of projects from development
    to production. This is key to understanding the platform’s versatility and its
    applicability at different stages of a project’s lifecycle. In terms of data management,
    the user experience with the platform’s storage system (MinIO) was examined, especially
    concerning data handling and sharing. Lastly, the ease with which knowledge of
    the platform can be transferred to new users was evaluated, an aspect for the
    long-term sustainability of projects. In summary, the evaluation aimed to show
    how well the platform meets the needs and solves the challenges faced by both
    data scientists and ML engineers. 3.3.2. Metrics For a better understanding of
    the results and system distribution, in addition to the theoretical explanation,
    each interviewee was asked to provide a rating indicating the level of difficulty,
    ranging from 1 (most negative) to 5 (most positive). Each aspect mentioned in
    the previous section is considered one of the evaluated criteria. These ratings
    have been accumulated and organized into tables, where the \"criterion\" column
    displays the corresponding question, and the remaining table values store the
    accumulative scores given by the respondents. Furthermore, to provide more context
    and understand the statistical variation in these values, it has been decided
    to calculate the mean and standard deviation. Additionally, it has been assumed
    that the distribution is normal and confidence intervals have been constructed
    around the mean, where 70% aligns with the criterion [ 𝑥 ¯ −𝑧∗𝜎, 𝑥 ¯ +𝑧∗𝜎 ]. In
    Equations (1) and (2), the value n represents the number of elements in the dataset,
    and 𝑥 𝑖 represents the individual values in the dataset. 𝑥 ¯ = 1 𝑛 ∑ 𝑖=1 𝑛 𝑥 𝑖
    (1) 𝜎= 1 𝑛 ∑ 𝑖=1 𝑛 ( 𝑥 𝑖 − 𝑥 ¯ ) 2 − − − − − − − − − − − −  ⎷   (2) Following
    Table 1, the majority of the scores assigned to the data scientist team are above
    3. This suggests that the infrastructure is accessible to data scientist profiles.
    As observed in Table 2, the statistics for all criteria, specifically the mean,
    are above 4 out of 5, with a standard deviation of ±1. This demonstrates that
    the evaluated individuals feel comfortable and at ease with all the assessed criteria.
    Table 1. Data scientist evaluations. Table 2. Data scientist statistics. In Table
    3, the data reveals a noteworthy trend, with the majority of the scores awarded
    to the Machine Learning engineer team surpassing the 3-point mark, affirming the
    accessibility of the infrastructure for professionals specializing in Machine
    Learning. Based on the statistical analysis presented in Table 4, each criterion,
    particularly the mean scores, consistently register above the 4-point threshold
    out of 5. Accompanied by a standard deviation range of ±1, these findings affirm
    the participants’ pronounced comfort and proficiency across the spectrum of evaluated
    criteria. Table 3. Machine Learning engineers’ evaluations. Table 4. Machine Learning
    engineers’ statistics. 4. Discussion A summary of the key findings and conclusions
    extracted from the surveys conducted with the professionals aforementioned is
    provided in Table 5. In the following, the contents of the table are going to
    be analyzed in detail. Table 5. Summary of the answers to the survey. Both data
    scientists and ML engineers encountered a somehow steep initial learning curve
    when starting with MLOps tools, particularly with Kubeflow. However, this challenge
    was generally overcome after an initial period, leading to a more intuitive and
    user-friendly experience with the platform’s tools. It is important to mention
    that prior knowledge in related technologies, such as Kubernetes, was cited as
    beneficial for easing the learning process. In terms of collaboration and teamwork,
    the platform was universally seen as a facilitator. However, this benefit was
    not without its conditions. Both roles emphasized the need for a well-organized
    team and clear methodologies to fully leverage the platform’s capabilities. The
    structure of the platform allows for the division of tasks and parallel work streams,
    which can speed up project timelines and make the workflow more efficient. However,
    this requires the team to be well coordinated and possibly adhere to agile methodologies.
    Data management was another common area of agreement. The centralized data storage
    capabilities provided by MinIO were highly valued by both data scientists and
    ML engineers. This feature not only simplifies data management but also enhances
    collaboration by ensuring that all team members have access to the latest versions
    of datasets. When it comes to the handover of projects and the sharing of resources,
    the experiences were generally positive. The platform’s structure and the use
    of code for defining pipelines make it easier to transfer work between team members.
    However, the time required for handover could increase if the incoming member
    is not familiar with the platform’s technologies. Despite this, once the handover
    is complete, sharing data, models, and pipelines within the platform is relatively
    straightforward. In conclusion, both data scientists and ML engineers find significant
    value in using MLOps tools, despite facing different challenges and benefits.
    The common themes that emerged from the study include the importance of documentation,
    the initial learning curve, and the benefits of centralized data storage. 5. Conclusions
    and Future Lines In this study, the development of AI models and their deployment
    in real productive systems using traditional methods has proven to be complex.
    To address this challenge, two solutions have been proposed. First, a development
    method facilitates the natural development and deployment of models. Second, an
    MLOps-oriented architecture designed for the agricultural sector incorporates
    specific protocols (MQTT) and open-source technologies to develop and execute
    AI models, allowing for seamless communication with agricultural devices. This
    conclusions section summarizes the primary benefits and insights of the platform,
    highlighting its significance and potential impact. The platform provides access
    to high-performance infrastructure, enabling efficient utilization of computational
    power. This capability empowers users to tackle complex tasks and handle large
    datasets without being constrained by their local hardware limitations. The platform
    serves as a centralized entrypoint which enhances communication and collaboration
    among data scientists, ML engineers, and other stakeholders, such as domain experts
    and project managers. This unified platform facilitates the seamless sharing of
    code, documentation, and project updates, promoting efficient code review, feedback,
    and iteration cycles. The platform takes care of role and user management, eliminating
    the need for users to handle multiple credentials for various tools and services.
    This centralized approach simplifies user onboarding, access control, and overall
    security. The platform seamlessly manages the deployment and monitoring of ML
    models. This allows for timely issue detection and facilitates proactive maintenance
    and improvements. The platform incorporates centralized storage, streamlining
    the collaboration and sharing of datasets and ML artifacts. This centralized storage
    enables users to efficiently manage and access shared data, thereby accelerating
    the development process. Feedback from data scientists and ML engineers who have
    used the platform indicates its value for enhancing collaboration. However, they
    also mention that the initial steps can be somewhat challenging for new users.
    Despite this, they also highlight the need for structured and common methodologies
    to better organize resources within the platform and streamline day-to-day operations.
    The platform is tailored for agriculture, efficiently connecting with numerous
    sensors, compatible with MQTT, and built on adaptable open-source technologies,
    making it an ideal choice for diverse farming needs. The platform has specifically
    been designed considering the farmer’s needs, building it in such a way that the
    adoption of this architecture would only require a minimum hardware infrastructure
    to deploy the system. While this platform was originally developed for an agricultural
    environment, its modular and adaptable architecture allows for its application
    in diverse fields. With appropriate modifications, the platform could be extrapolated
    to cater to the specific needs and challenges of other sectors. As with any evolving
    technology, the platform presents opportunities for refinement and expansion.
    Considering the shifting nature of a brand-new paradigm such as MLOps and the
    feedback gathered from the user community, certain areas have been identified
    for further exploration and enhancement. Improving User Onboarding: Given the
    feedback regarding the initial challenges faced by new users, it becomes imperative
    to make the learning curve smoother. Interactive tutorials, context-sensitive
    help sections, and even AI-guided walkthroughs are potential solutions to better
    assist new users as they navigate and familiarize themselves with the platform.
    Model Monitoring: While the platform handles the entire lifecycle of ML models
    proficiently, there is a clear gap in terms of continuous model monitoring in
    a productive environment. The immediate next step, thus, is integrating well-established
    tools such as Grafana [95] and Prometheus [96], since they could provide real-time
    insights, performance metrics, and anomaly detection for deployed models. Although
    the platform efficiently manages the lifecycle of ML models, it lacks continuous
    model monitoring in a production environment. To address this, integrating tools
    like Grafana [95] and Prometheus [96] is essential for offering real-time insights
    and performance metrics, including anomaly detection for deployed models. More
    experimentation: The aim is to engage more data scientists and ML engineers on
    the platform to maximize its memory and computing capacities. This entails encouraging
    experts to perform resource-intensive tasks, challenging the system’s ability
    to handle extensive data processing and complex algorithms. By pushing the platform
    to its limits, it can showcase its scalability and effectiveness for demanding
    data science and machine learning tasks. Author Contributions Conceptualization,
    A.C.C.-P., Y.L. and R.L.; methodology, R.L.; software, A.C.C.-P. and Y.L.; validation,
    A.C.C.-P., Y.L. and R.L.; data curation, A.C.C.-P. and Y.L.; writing—original
    draft preparation, A.C.C.-P., Y.L. and R.L.; writing—review and editing, A.C.C.-P.,
    Y.L. and R.L.; supervision, R.L. All authors have read and agreed to the published
    version of the manuscript. Funding This work has been carried out in the scope
    of the H2020 FlexiGroBots project, which has been funded by the European Commission
    in the scope of its H2020 programme (contract number 101017111, https://flexigrobots-h2020.eu/,
    accessed on 1 January 2024). Data Availability Statement Code and setup instructions
    can be found at [75]. For public interaction and model inference, a dedicated
    website is available at [76]. Acknowledgments The authors would like to acknowledge
    the valuable help and contributions from all partners of the FlexiGroBots consortium,
    as well as the data scientists and ML engineers that participated in the survey.
    Conflicts of Interest The authors declare no conflicts of interest. Abbreviations
    The following abbreviations are used in this manuscript: AI Artificial Intelligence
    CD Continuous Delivery CI Continuous Integration DL Deep Learning HTTP Hypertext
    Transfer Protocol IoT Internet of Things MLOps Machine Learning Operations ML
    Machine Learning MQTT Message Queuing Telemetry Transport NFS Network File System
    UAV Unmanned Aerial Vehicles UGV Unmanned Ground Vehicles UN United Nations WRI
    World Resources Institute References Ranganathan, J.; Waite, R.; Searchinger,
    T.; Hanson, C. How to Sustainably Feed 10 Billion People by 2050, in 21 Charts.
    2018. Available online: https://www.wri.org/insights/how-sustainably-feed-10-billion-people-2050-21-charts
    (accessed on 12 December 2023). De Clercq, M.; Vats, A.; Biel, A. Agriculture
    4.0: The future of farming technology. In Proceedings of the World Government
    Summit, Dubai, United Arab Emirates, 29 December 2018; pp. 11–13. [Google Scholar]
    Wakchaure, M.; Patle, B.; Mahindrakar, A. Application of AI Techniques and Robotics
    in Agriculture: A Review. Artif. Intell. Life Sci. 2023, 3, 100057. [Google Scholar]
    [CrossRef] Eli-Chukwu, N.C. Applications of Artificial Intelligence in agriculture:
    A review. Eng. Technol. Appl. Sci. Res. 2019, 9, 4377–4383. [Google Scholar] [CrossRef]
    Elbasi, E.; Mostafa, N.; AlArnaout, Z.; Zreikat, A.I.; Cina, E.; Varghese, G.;
    Shdefat, A.; Topcu, A.E.; Abdelbaki, W.; Mathew, S. Artificial intelligence technology
    in the agricultural sector: A systematic literature review. IEEE Access 2022,
    11, 171–202. [Google Scholar] [CrossRef] Shankar, P.; Werner, N.; Selinger, S.;
    Janssen, O. Artificial Intelligence driven crop protection optimization for sustainable
    agriculture. In Proceedings of the 2020 IEEE/ITU International Conference on Artificial
    Intelligence for Good (AI4G), Geneva, Switzerland, 21–25 September 2020; pp. 1–6.
    [Google Scholar] Allmendinger, A.; Spaeth, M.; Saile, M.; Peteinatos, G.G.; Gerhards,
    R. Precision chemical weed management strategies: A review and a design of a new
    CNN-based modular spot sprayer. Agronomy 2022, 12, 1620. [Google Scholar] [CrossRef]
    Saxena, R.; Joshi, A.; Joshi, S.; Borkotoky, S.; Singh, K.; Rai, P.K.; Mueed,
    Z.; Sharma, R. The role of artificial Intelligence strategies to mitigate abiotic
    stress and climate change in crop production. In Visualization Techniques for
    Climate Change with Machine Learning and Artificial Intelligence; Elsevier: Amsterdam,
    The Netherlands, 2023; pp. 273–293. [Google Scholar] Visentin, F.; Cremasco, S.;
    Sozzi, M.; Signorini, L.; Signorini, M.; Marinello, F.; Muradore, R. A mixed-autonomous
    robotic platform for intra-row and inter-row weed removal for precision agriculture.
    Comput. Electron. Agric. 2023, 214, 108270. [Google Scholar] [CrossRef] Conejero,
    M.; Montes, H.; Andujar, D.; Bengochea-Guevara, J.; Rodríguez, E.; Ribeiro, A.
    Collaborative smart-robot for yield mapping and harvesting assistance. In Precision
    Agriculture’23; Wageningen Academic: Wageningen, The Netherlands, 2023; pp. 1067–1074.
    [Google Scholar] Talib, M.A.; Majzoub, S.; Nasir, Q.; Jamal, D. A systematic literature
    review on hardware implementation of Artificial Intelligence algorithms. J. Supercomput.
    2021, 77, 1897–1938. [Google Scholar] [CrossRef] Sufi, F. Algorithms in low-code-no-code
    for research applications: A practical review. Algorithms 2023, 16, 108. [Google
    Scholar] [CrossRef] Dogan, M.E.; Goru Dogan, T.; Bozkurt, A. The use of Artificial
    Intelligence (AI) in online learning and distance education processes: A systematic
    review of empirical studies. Appl. Sci. 2023, 13, 3056. [Google Scholar] [CrossRef]
    Mhlanga, D. Industry 4.0 in finance: The impact of Artificial Intelligence (ai)
    on digital financial inclusion. Int. J. Financ. Stud. 2020, 8, 45. [Google Scholar]
    [CrossRef] Enholm, I.M.; Papagiannidis, E.; Mikalef, P.; Krogstie, J. Artificial
    Intelligence and business value: A literature review. Inf. Syst. Front. 2022,
    24, 1709–1734. [Google Scholar] [CrossRef] Cob-Parro, A.C.; Losada-Gutiérrez,
    C.; Marrón-Romera, M.; Gardel-Vicente, A.; Bravo-Muñoz, I. Smart video surveillance
    system based on edge computing. Sensors 2021, 21, 2958. [Google Scholar] [CrossRef]
    Rajpurkar, P.; Chen, E.; Banerjee, O.; Topol, E.J. AI in health and medicine.
    Nat. Med. 2022, 28, 31–38. [Google Scholar] [CrossRef] [PubMed] Huang, Z.; Shen,
    Y.; Li, J.; Fey, M.; Brecher, C. A survey on AI-driven digital twins in industry
    4.0: Smart manufacturing and advanced robotics. Sensors 2021, 21, 6340. [Google
    Scholar] [CrossRef] Qazi, S.; Khawaja, B.A.; Farooq, Q.U. IoT-equipped and AI-enabled
    next generation smart agriculture: A critical review, current challenges and future
    trends. IEEE Access 2022, 10, 21219–21235. [Google Scholar] [CrossRef] Vincent,
    D.R.; Deepa, N.; Elavarasan, D.; Srinivasan, K.; Chauhdary, S.H.; Iwendi, C. Sensors
    driven AI-based agriculture recommendation model for assessing land suitability.
    Sensors 2019, 19, 3667. [Google Scholar] [CrossRef] Ben Ayed, R.; Hanana, M. Artificial
    Intelligence to improve the food and agriculture sector. J. Food Qual. 2021, 2021,
    5584754. [Google Scholar] [CrossRef] Alla, S.; Adari, S.K.; Alla, S.; Adari, S.K.
    What is mlops? In Beginning MLOps with MLFlow: Deploy Models in AWS SageMaker,
    Google Cloud, and Microsoft Azure; Apress: New York, NY, USA, 2021; pp. 79–124.
    [Google Scholar] John, M.M.; Olsson, H.H.; Bosch, J. Towards mlops: A framework
    and maturity model. In Proceedings of the 2021 47th Euromicro Conference on Software
    Engineering and Advanced Applications (SEAA), Palermo, Italy, 1–3 September 2021;
    pp. 1–8. [Google Scholar] Ruf, P.; Madan, M.; Reich, C.; Ould-Abdeslam, D. Demystifying
    mlops and presenting a recipe for the selection of open-source tools. Appl. Sci.
    2021, 11, 8861. [Google Scholar] [CrossRef] Hunkeler, U.; Truong, H.L.; Stanford-Clark,
    A. MQTT-S—A publish/subscribe protocol for Wireless Sensor Networks. In Proceedings
    of the 2008 3rd International Conference on Communication Systems Software and
    Middleware and Workshops (COMSWARE’08), Bangalore, India, 6–10 January 2008; pp.
    791–798. [Google Scholar] Berners-Lee, T.; Fielding, R.; Frystyk, H. Hypertext
    Transfer Protocol–HTTP/1.0. 1996. Available online: https://www.rfc-editor.org/rfc/rfc1945.html
    (accessed on 12 December 2023). Shankar, R.H.; Veeraraghavan, A.; Sivaraman, K.;
    Ramachandran, S.S. Application of UAV for pest, weeds and disease detection using
    open computer vision. In Proceedings of the 2018 International Conference on Smart
    Systems and Inventive Technology (ICSSIT), Tirunelveli, India, 13–14 December
    2018; pp. 287–292. [Google Scholar] Nalluri, S.; Ramasubbareddy, S.; Kannayaram,
    G. Weather prediction using clustering strategies in machine learning. J. Comput.
    Theor. Nanosci. 2019, 16, 1977–1981. [Google Scholar] [CrossRef] Ukhurebor, K.E.;
    Adetunji, C.O.; Olugbemi, O.T.; Nwankwo, W.; Olayinka, A.S.; Umezuruike, C.; Hefft,
    D.I. Precision agriculture: Weather forecasting for future farming. In AI, Edge
    and IoT-Based Smart Agriculture; Elsevier: Amsterdam, The Netherlands, 2022; pp.
    101–121. [Google Scholar] Kendler, S.; Aharoni, R.; Young, S.; Sela, H.; Kis-Papo,
    T.; Fahima, T.; Fishbain, B. Detection of crop diseases using enhanced variability
    imagery data and convolutional neural networks. Comput. Electron. Agric. 2022,
    193, 106732. [Google Scholar] [CrossRef] Karunathilake, E.; Le, A.T.; Heo, S.;
    Chung, Y.S.; Mansoor, S. The path to smart farming: Innovations and opportunities
    in precision agriculture. Agriculture 2023, 13, 1593. [Google Scholar] [CrossRef]
    Suryawanshi, Y.; Patil, K. Advancing agriculture through image-based datasets
    in plant science: A review. EPRA Int. J. Multidiscip. Res. (IJMR) 2023, 9, 233–236.
    [Google Scholar] Agarwal, S.; Hiran, D.; Kothari, H.; Singh, S. Leveraging data
    processing for optimizing organic farming practices. J. Data Acquis. Process.
    2023, 38, 3243. [Google Scholar] Rogelja, T.; Ludvig, A.; Weiss, G.; Prah, J.;
    Shannon, M.; Secco, L. Analyzing social innovation as a process in rural areas:
    Key dimensions and success factors for the revival of the traditional charcoal
    burning in Slovenia. J. Rural. Stud. 2023, 97, 517–533. [Google Scholar] [CrossRef]
    Khudyakova, E.; Shitikova, A.; Stepantsevich, M.N.; Grecheneva, A. Requirements
    of Modern Russian Agricultural Production for Digital Competencies of an Agricultural
    Specialist. Educ. Sci. 2023, 13, 203. [Google Scholar] [CrossRef] Garske, B.;
    Bau, A.; Ekardt, F. Digitalization and AI in European agriculture: A strategy
    for achieving climate and biodiversity targets? Sustainability 2021, 13, 4652.
    [Google Scholar] [CrossRef] Nova, K. AI-enabled water management systems: An analysis
    of system components and interdependencies for water conservation. Eig. Rev. Sci.
    Technol. 2023, 7, 105–124. [Google Scholar] Liang, P.; Tang, Y.; Zhang, X.; Bai,
    Y.; Su, T.; Lai, Z.; Qiao, L.; Li, D. A Survey on Auto-Parallelism of Large-Scale
    Deep Learning Training. IEEE Trans. Parallel Distrib. Syst. 2023, 34, 2377–2390.
    [Google Scholar] [CrossRef] Chheda, S.; Curtis, A.; Siegmann, E.; Chapman, B.
    Performance Study on CPU-based Machine Learning with PyTorch. In Proceedings of
    the HPC Asia 2023 Workshops, Singapore, 27 February–2 March 2023; pp. 24–34. [Google
    Scholar] Kreuzberger, D.; Kühl, N.; Hirschl, S. Machine learning operations (mlops):
    Overview, definition, and architecture. IEEE Access 2023, 11, 31866–31879. [Google
    Scholar] [CrossRef] Hassani, H.; Huang, X.; MacFeely, S. Enabling Digital Twins
    to Support the UN SDGs. Big Data Cogn. Comput. 2022, 6, 115. [Google Scholar]
    [CrossRef] Dharmaraj, V.; Vijayanand, C. Artificial Intelligence (AI) in agriculture.
    Int. J. Curr. Microbiol. Appl. Sci. 2018, 7, 2122–2128. [Google Scholar] [CrossRef]
    Pistor, N. Accelerating University-Industry Collaborations with MLOps: A Case
    Study about the Cooperation of Aimo and the Linnaeus University. Master’s Thesis,
    Linnaeus University, Växjö, Sweden, 2023. [Google Scholar] Yang, X.; Cao, D.;
    Chen, J.; Xiao, Z.; Daowd, A. AI and IoT-based collaborative business ecosystem:
    A case in Chinese fish farming industry. Int. J. Technol. Manag. 2020, 82, 151–171.
    [Google Scholar] [CrossRef] Lubua Dr, E.W. The influence of socioeconomic factors
    to the use of mobile phones in the agricultural sector of Tanzania. Afr. J. Inf.
    Syst. 2019, 11, 2. [Google Scholar] Kumara, I.; Arts, R.; Di Nucci, D.; Van Den
    Heuvel, W.J.; Tamburri, D.A. Requirements and Reference Architecture for MLOps:
    Insights from Industry. TechRxiv 2023. [Google Scholar] [CrossRef] Hewage, N.;
    Meedeniya, D. Machine Learning Operations: A Survey on MLOps Tool Support. arXiv
    2022, arXiv:2202.10169. [Google Scholar] Fujii, T.Y.; Hayashi, V.T.; Arakaki,
    R.; Ruggiero, W.V.; Bulla, R., Jr.; Hayashi, F.H.; Khalil, K.A. A digital twin
    architecture model applied with MLOps techniques to improve short-term energy
    consumption prediction. Machines 2021, 10, 23. [Google Scholar] [CrossRef] Zhou,
    Y.; Yu, Y.; Ding, B. Towards mlops: A case study of ml pipeline platform. In Proceedings
    of the 2020 International Conference on Artificial Intelligence and Computer Engineering
    (ICAICE), Beijing, China, 23–25 October 2020; pp. 494–500. [Google Scholar] Klerkx,
    L.; Jakku, E.; Labarthe, P. A review of social science on digital agriculture,
    smart farming and agriculture 4.0: New contributions and a future research agenda.
    NJAS-Wagening. J. Life Sci. 2019, 90, 100315. [Google Scholar] [CrossRef] Rose,
    D.C.; Chilvers, J. Agriculture 4.0: Broadening responsible innovation in an era
    of smart farming. Front. Sustain. Food Syst. 2018, 2, 87. [Google Scholar] [CrossRef]
    Placidi, P.; Gasperini, L.; Grassi, A.; Cecconi, M.; Scorzoni, A. Characterization
    of low-cost capacitive soil moisture sensors for IoT networks. Sensors 2020, 20,
    3585. [Google Scholar] [CrossRef] Farooq, M.S.; Riaz, S.; Abid, A.; Abid, K.;
    Naeem, M.A. A Survey on the Role of IoT in Agriculture for the Implementation
    of Smart Farming. IEEE Access 2019, 7, 156237–156271. [Google Scholar] [CrossRef]
    Morar, C.; Doroftei, I.; Doroftei, I.; Hagan, M. Robotic applications on agricultural
    industry. A review. In IOP Conference Series: Materials Science and Engineering;
    IOP Publishing: Bristol, UK, 2020; Volume 997, p. 012081. [Google Scholar] Kamilaris,
    A.; Kartakoullis, A.; Prenafeta-Boldú, F.X. A review on the practice of big data
    analysis in agriculture. Comput. Electron. Agric. 2017, 143, 23–37. [Google Scholar]
    [CrossRef] Akkem, Y.; Biswas, S.K.; Varanasi, A. Smart farming using Artificial
    Intelligence: A review. Eng. Appl. Artif. Intell. 2023, 120, 105899. [Google Scholar]
    [CrossRef] Siregar, R.R.A.; Seminar, K.B.; Wahjuni, S.; Santosa, E. Vertical farming
    perspectives in support of precision agriculture using Artificial Intelligence:
    A review. Computers 2022, 11, 135. [Google Scholar] [CrossRef] Araújo, S.O.; Peres,
    R.S.; Barata, J.; Lidon, F.; Ramalho, J.C. Characterising the agriculture 4.0
    landscape—Emerging trends, challenges and opportunities. Agronomy 2021, 11, 667.
    [Google Scholar] [CrossRef] Rejeb, A.; Abdollahi, A.; Rejeb, K.; Treiblmaier,
    H. Drones in agriculture: A review and bibliometric analysis. Comput. Electron.
    Agric. 2022, 198, 107017. [Google Scholar] [CrossRef] Palei, H. Artificial Intelligence
    in precision agriculture: A review. Arch. Comput. Methods Eng. 2021, 28, 1627–1647.
    [Google Scholar] Samui, P. Machine Learning Applications in Agriculture: A Comprehensive
    Review. Agriculture 2022, 12, 56. [Google Scholar] Bargoti, S.; Underwood, J.
    Deep Fruit Detection in Orchards. arXiv 2017, arXiv:1703.08236. [Google Scholar]
    Ma, L.; Liu, Y.; Zhang, X.; Ye, Y.; Yin, G.; Johnson, B.A. Deep Learning in remote
    sensing applications: A meta-analysis and review. ISPRS J. Photogramm. Remote
    Sens. 2019, 152, 166–177. [Google Scholar] [CrossRef] Prema, P.; Veeramani, A.;
    Sivakumar, T. Machine Learning Applications in Agriculture. J. Agric. Res. Technol.
    2022, 1, 126. [Google Scholar] [CrossRef] Sozzi, M.; Kayad, A.; Ferrari, G.; Zanchin,
    A.; Grigolato, S.; Marinello, F. Connectivity in rural areas: A case study on
    internet connection in the Italian agricultural areas. In Proceedings of the 2021
    IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor),
    Trento-Bolzano, Italy, 3–5 November 2021; pp. 466–470. [Google Scholar] Visconti,
    P.; de Fazio, R.; Velázquez, R.; Del-Valle-Soto, C.; Giannoccaro, N.I. Development
    of sensors-based agri-food traceability system remotely managed by a software
    platform for optimized farm management. Sensors 2020, 20, 3632. [Google Scholar]
    [CrossRef] Lee, M.; Wang, Y.R.; Huang, C.F. Design and development of a friendly
    user interface for building construction traceability system. Microsyst. Technol.
    2021, 27, 1773–1785. [Google Scholar] [CrossRef] Dhehibi, B.; Rudiger, U.; Moyo,
    H.P.; Dhraief, M.Z. Agricultural technology transfer preferences of smallholder
    farmers in Tunisia’s arid regions. Sustainability 2020, 12, 421. [Google Scholar]
    [CrossRef] Desai, R.; Nisha, T. Best practices for ensuring security in devops:
    A case study approach. In Journal of Physics: Conference Series; IOP Publishing:
    Bristol, UK, 2021; Volume 1964, p. 042045. [Google Scholar] Official Jupyter Web
    Site. Available online: https://jupyter.org/ (accessed on 19 June 2023). Official
    Great Expectations Tool Web Site. Available online: https://greatexpectations.io/
    (accessed on 19 June 2023). Official DVC Web Site. Available online: https://discuss.dvc.org/
    (accessed on 19 June 2023). Official MlFlow Web Site. Available online: https://mlflow.org/
    (accessed on 19 June 2023). Smith, J.; Doe, J. Challenges and Opportunities in
    Internet of Things (IoT): A Comprehensive Survey. J. IOT Res. 2021, 4, 25–60.
    [Google Scholar] Official FlexiGroBot Repository. Available online: https://github.com/FlexiGroBots-H2020/AI-platform
    (accessed on 19 June 2023). FlexiGroBots Entrypoint to Test Kserve Inference Model.
    Available online: https://web.platform.flexigrobots-h2020.eu/apps (accessed on
    19 June 2023). Official FlexiGroBot Kubeflow Access. Available online: https://kubeflow.flexigrobots-h2020.eu/
    (accessed on 19 June 2023). Official Kubernetes Web Site. Available online: https://kubernetes.io/
    (accessed on 19 June 2023). Official Kubeflow Web Site. Available online: https://www.kubeflow.org/
    (accessed on 19 June 2023). Official Istio Web Site. Available online: https://istio.io/
    (accessed on 19 June 2023). Ladisa, P.; Plate, H.; Martinez, M.; Barais, O. Sok:
    Taxonomy of attacks on open-source software supply chains. In Proceedings of the
    2023 IEEE Symposium on Security and Privacy (SP), San Francisco, CA, USA, 21–25
    May 2023; pp. 1509–1526. [Google Scholar] Official Kubeflow Pipeline Web Site.
    Available online: https://www.kubeflow.org/docs/components/pipelines/v1/introduction/
    (accessed on 19 June 2023). Official TensorFlow Web Site. Available online: https://www.tensorflow.org/
    (accessed on 19 June 2023). Official Pytorch Web Site. Available online: https://pytorch.org/
    (accessed on 19 June 2023). Official Sklearn Web Site. Available online: https://scikit-learn.org/stable/
    (accessed on 19 June 2023). Official Docker Web Site. Available online: https://www.docker.com/
    (accessed on 19 June 2023). Official Katib Web Site. Available online: https://www.kubeflow.org/docs/components/katib/overview/
    (accessed on 19 June 2023). Official TFX Web Site. Available online: https://www.tensorflow.org/tfx/guide/serving
    (accessed on 19 June 2023). Official Seldon Web Site. Available online: https://www.seldon.io/
    (accessed on 19 June 2023). Official Kserve Web Page. Available online: https://www.kubeflow.org/docs/external-add-ons/kserve/kserve/
    (accessed on 17 August 2023). Choi, K.; Yi, J.; Park, C.; Yoon, S. Deep Learning
    for anomaly detection in time-series data: Review, analysis, and guidelines. IEEE
    Access 2021, 9, 120043–120065. [Google Scholar] [CrossRef] Official Camel Web
    Site. Available online: https://camel.apache.org/ (accessed on 19 June 2023).
    Jahanshahi, H.; Alijani, Z.; Mihalache, S.F. Towards Sustainable Transportation:
    A Review of Fuzzy Decision Systems and Supply Chain Serviceability. Mathematics
    2023, 11, 1934. [Google Scholar] [CrossRef] Tomasiello, S.; Alijani, Z. Fuzzy-based
    approaches for agri-food supply chains: A mini-review. Soft Comput. 2021, 25,
    7479–7492. [Google Scholar] [CrossRef] Official Grafana Web Site. Available online:
    https://grafana.com/ (accessed on 19 June 2023). Official Prometheus Web Site.
    Available online: https://prometheus.io (accessed on 19 June 2023). Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2024 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Cob-Parro, A.C.; Lalangui, Y.; Lazcano, R. Fostering
    Agricultural Transformation through AI: An Open-Source AI Architecture Exploiting
    the MLOps Paradigm. Agronomy 2024, 14, 259. https://doi.org/10.3390/agronomy14020259
    AMA Style Cob-Parro AC, Lalangui Y, Lazcano R. Fostering Agricultural Transformation
    through AI: An Open-Source AI Architecture Exploiting the MLOps Paradigm. Agronomy.
    2024; 14(2):259. https://doi.org/10.3390/agronomy14020259 Chicago/Turabian Style
    Cob-Parro, Antonio Carlos, Yerhard Lalangui, and Raquel Lazcano. 2024. \"Fostering
    Agricultural Transformation through AI: An Open-Source AI Architecture Exploiting
    the MLOps Paradigm\" Agronomy 14, no. 2: 259. https://doi.org/10.3390/agronomy14020259
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 25. Jan 4. Feb 14. Feb 24.
    Feb 5. Mar 15. Mar 25. Mar 4. Apr 0 250 500 750 1000 1250 1500 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Agronomy, EISSN 2073-4395, Published by MDPI RSS Content
    Alert Further Information Article Processing Charges Pay an Invoice Open Access
    Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors
    For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives
    Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings
    Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release
    notifications and newsletters from MDPI journals Select options Subscribe © 1996-2024
    MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions
    Privacy Policy"'
  inline_citation: '>'
  journal: Agronomy
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Fostering Agricultural Transformation through AI: An Open-Source AI Architecture
    Exploiting the MLOps Paradigm'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Aldoseri A.
  - Al-Khalifa K.N.
  - Hamouda A.M.
  citation_count: '0'
  description: In an era defined by technological disruption, the integration of artificial
    intelligence (AI) into business processes is both strategic and challenging. As
    AI continues to disrupt and reshape industries and revolutionize business processes,
    organizations must take proactive steps to assess their readiness and capabilities
    to effectively leverage AI technologies. This research focuses on the assessment
    elements required to evaluate an organization’s current state in preparation for
    AI-based digital transformation. This research is based on a literature review
    and practical insights derived from extensive experience in industrial system
    engineering. This paper outlines the key assessment elements that organizations
    should consider to ensure successful and sustainable AI-based digital transformation.
    This emphasizes the need for a comprehensive approach to assess the organization’s
    data infrastructure, governance practices, and existing AI capabilities. Furthermore,
    the research work focuses on the evaluation of AI talent and skills within the
    organization, considering the significance of fostering an innovative culture
    and addressing change management challenges. The results of this study provide
    organizations with elements to assess their current state for AI-based digital
    transformation. By adopting and implementing the proposed guidelines, organizations
    can gain a holistic perspective of their current standing, identify strategic
    opportunities for AI integration, mitigate potential risks, and strategize a successful
    path forwards in the evolving landscape of AI-driven digital transformation.
  doi: 10.3390/asi7010014
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Applied System Innovation (ASI) All
    Article Types Advanced   Journals ASI Volume 7 Issue 1 10.3390/asi7010014 Submit
    to this Journal Review for this Journal Propose a Special Issue Article Menu Academic
    Editor Abdelkader Sbihi Subscribe SciFeed Recommended Articles Related Info Link
    More by Authors Links Article Views 1852 Table of Contents Abstract Introduction
    Materials and Methods Results Discussion Conclusions Author Contributions Funding
    Data Availability Statement Conflicts of Interest References share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Methodological Approach to Assessing the Current State of Organizations for AI-Based
    Digital Transformation by Abdulaziz Aldoseri , Khalifa N. Al-Khalifa and Abdel
    Magid Hamouda * Engineering Management Program, College of Engineering, Qatar
    University, Doha P.O. Box 2713, Qatar * Author to whom correspondence should be
    addressed. Appl. Syst. Innov. 2024, 7(1), 14; https://doi.org/10.3390/asi7010014
    Submission received: 5 October 2023 / Revised: 9 November 2023 / Accepted: 29
    January 2024 / Published: 8 February 2024 Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract In an era defined by technological disruption,
    the integration of artificial intelligence (AI) into business processes is both
    strategic and challenging. As AI continues to disrupt and reshape industries and
    revolutionize business processes, organizations must take proactive steps to assess
    their readiness and capabilities to effectively leverage AI technologies. This
    research focuses on the assessment elements required to evaluate an organization’s
    current state in preparation for AI-based digital transformation. This research
    is based on a literature review and practical insights derived from extensive
    experience in industrial system engineering. This paper outlines the key assessment
    elements that organizations should consider to ensure successful and sustainable
    AI-based digital transformation. This emphasizes the need for a comprehensive
    approach to assess the organization’s data infrastructure, governance practices,
    and existing AI capabilities. Furthermore, the research work focuses on the evaluation
    of AI talent and skills within the organization, considering the significance
    of fostering an innovative culture and addressing change management challenges.
    The results of this study provide organizations with elements to assess their
    current state for AI-based digital transformation. By adopting and implementing
    the proposed guidelines, organizations can gain a holistic perspective of their
    current standing, identify strategic opportunities for AI integration, mitigate
    potential risks, and strategize a successful path forwards in the evolving landscape
    of AI-driven digital transformation. Keywords: AI readiness assessment; business
    processes; data infrastructure 1. Introduction Artificial intelligence in the
    new world is not just a tool; it is the catalyst for a revolution. It is the dawn
    of a new era, where businesses do not just adapt—they thrive. This is the age
    of AI-driven metamorphosis, where the mundane becomes extraordinary and the extraordinary
    becomes the norm. In the contemporary landscape of global enterprises, the relentless
    pursuit of innovation is a cornerstone objective that drives organizations to
    strive for competitive supremacy and deliver unparalleled value to their stakeholders.
    Within this context, a transformative impetus emerges: the seamless integration
    of artificial intelligence (AI) into the heart of business operations. This convergence
    signifies more than mere technological advancement; it embodies a profound shift—an
    AI-based digital transformation [1] that reshapes the very fabric of how organizations
    function, decide, and engage with their stakeholders [2]. Imagine a realm where
    data speak volumes, where intricate patterns emerge effortlessly from vast data
    oceans, and where complex tasks are automated with finesse. This transformation
    is not just about embracing AI; it is about orchestrating a symphony of insights,
    automating intricate tasks, elevating customer experiences, and sparking the fires
    of innovation [3,4]. Digital transformation refers to the integration of digital
    technologies into all aspects of an organization, fundamentally changing how it
    operates and delivers value [4]. It involves rethinking business models, processes,
    products, and services to leverage the power of digital technologies, such as
    cloud computing, data analytics, the Internet of Things (IoT), and AI [5]. AI-based
    digital transformation takes this concept a step further by specifically focusing
    on harnessing the full potential of AI technologies to drive organizational change
    and achieve strategic objectives [6]. AI-based digital transformation represents
    a strategic approach that harnesses the power of AI to drive organizational change,
    deliver transformative outcomes, and revolutionize how organizations operate,
    make decisions, and interact with customers, leading to enhanced productivity,
    agility, and innovation [6]. By integrating AI technologies into business processes
    and systems, organizations can augment human capabilities, automate tasks, derive
    valuable insights from data, and make data-driven decisions [7]. This empowers
    organizations to streamline operations, enhance efficiency, optimize resource
    allocation, and create personalized experiences for their customers [8]. By leveraging
    AI capabilities, organizations can gain valuable insights from their data, automate
    repetitive tasks, and make more informed decisions [9]. This can lead to improved
    operational efficiency, increased productivity, enhanced customer experiences,
    and the ability to identify new business opportunities [10]. Moreover, AI-based
    digital transformation enables organizations to stay ahead of the curve, adapt
    to changing market dynamics, and drive innovation in their respective industries
    [11]. AI-based digital transformation presents organizations with unprecedented
    opportunities to revolutionize their operations, drive innovation, and deliver
    exceptional value to their stakeholders [12]. By leveraging AI technologies, organizations
    can unlock new insights, automate tasks, and make data-driven decisions [13].
    However, successful AI-based digital transformation requires a holistic and strategic
    approach that considers technological infrastructure, data management, talent
    acquisition, cultural readiness, ethical considerations, and change management
    [14]. With careful planning, the right tools and infrastructure, and strong leadership,
    organizations can embark on a transformative journey that propels them into the
    era of AI-driven innovation and competitive advantage [15]. However, successful
    AI-based digital transformation requires the careful consideration of strategy,
    data infrastructure, talent, ethics, change management, partnerships, infrastructure
    scalability, and continuous evaluation [16]. Implementing AI-based digital transformation
    is not without its challenges. Organizations should carefully plan and execute
    their transformation journeys to ensure success [17]. They must consider several
    key aspects, including technology infrastructure, data management, talent acquisition,
    cultural readiness, ethical considerations, and change management [18]. The integration
    of AI technologies requires a strategic and systematic approach that involves
    multiple stakeholders across the organization [14]. This paper delves into a fundamental
    phase of AI-based digital transformation: evaluating an organization’s existing
    state. The primary objective of this research is to offer a systematic and comprehensive
    approach for assessing an organization’s readiness for AI adoption. By thoroughly
    examining processes, current technology infrastructure, data management capabilities,
    and overall organizational preparedness for AI integration [19], this study aims
    to identify gaps and constraints that might impede the successful implementation
    of AI solutions [20]. Special attention will be given to evaluating data assets,
    data quality, and data accessibility, which are essential foundations for effective
    AI-based initiatives [21]. The critical nature of this initial assessment cannot
    be overstated. It not only sheds light on the organization’s strengths and weaknesses
    but also serves as the cornerstone for developing a strategic roadmap and efficiently
    allocating resources [22]. Through this meticulous evaluation, organizations can
    gain valuable insights into their technological infrastructure, data availability,
    organizational culture, talent pool, business processes, and regulatory considerations.
    Armed with this knowledge, organizations can make informed decisions, enabling
    them to embark on successful AI-driven digital transformation journey [23]. Significantly,
    the existing literature lacks a structured and holistic approach to assess an
    organization’s current status for AI-based digital transformation, leading to
    fragmented insights and a lack of standardized guidelines. Our research steps
    into this void with a clear objective: to develop a methodologically robust and
    all-encompassing framework. To address this gap, this study provides a definitive
    methodology, thereby bridging the existing void in scholarly discourse. This research
    not only outlines the importance of assessing an organization’s readiness for
    AI integration but also presents a robust framework that can guide practitioners
    and researchers in effectively conducting this critical assessment. Our study
    provides practitioners and researchers with a tailored framework that offers specific
    methods for evaluating processes, existing systems, data landscapes, and internal
    AI capabilities. Unlike generic guidelines, our approach delves into organizational
    intricacies, ensuring a nuanced assessment. By considering system compatibility
    and human expertise, our systematic methodology equips practitioners to effectively
    identify challenges and leverage strengths. Our framework serves as a beacon in
    AI-based digital transformation, guiding organizations with confidence and efficacy
    through modern technological integration. It illuminates a clear path for organizations,
    providing them with the tools they need to navigate the complexities of modern
    technological integration. By offering a structured and holistic approach, we
    contribute not only to academic knowledge but, more importantly, to practical
    advancements, enabling organizations to embrace AI technologies with confidence
    and efficacy. 2. Materials and Methods The research methodology followed in this
    work is described as “experience-driven” coupled with a literature review and
    represents a hybrid approach that marries practical, hands-on experience with
    a thorough exploration of existing academic knowledge. In this approach, our research
    draws extensively from the wealth of insights acquired through real-world experiences
    in the field of industrial system engineering. This practical knowledge is then
    complemented and contextualized by a comprehensive review of relevant academic
    literature. The “experience-driven” aspect of our research methodology suggests
    a focus on learning from actual situations, problem-solving in real-world industrial
    contexts, and understanding the intricacies of industrial system engineering through
    direct involvement and observation. This experiential knowledge serves as the
    foundation upon which our research is built, providing valuable insights that
    are often difficult to glean from purely theoretical perspectives. Simultaneously,
    our research incorporates a literature review component, indicating a meticulous
    examination of scholarly articles, research papers, and theoretical frameworks
    related to industrial system engineering. This literature review serves to situate
    our practical experiences within the broader theoretical landscape. This allows
    us to critically assess existing theories, identify gaps in the literature, and
    draw connections between academic concepts and real-world applications. By blending
    these 2 methodologies, our research is uniquely positioned to offer a holistic
    understanding of industrial system engineering. The synergy between hands-on experience
    and theoretical knowledge enhances the depth and richness of our research findings,
    allowing for a nuanced exploration of the subject matter. This hybrid approach
    not only strengthens the credibility of our research but also ensures its practical
    relevance, making it a valuable contribution to both academic scholarship and
    industrial practice. Experience-driven methodology is not solely reliant on the
    collective experiences of the authors but is substantiated by their significant
    and varied professional backgrounds. These experiences encompass working in different
    geographies and cultural settings, thereby providing a rich diversity of practical
    insights. This diverse experiential knowledge base was integral to the identification
    of key elements in AI-based digital transformation, offering a multifaceted perspective
    that enriches our research. The authors’ experiences were systematically reflected
    upon and recorded using a structured qualitative method, ensuring that subjective
    insights were critically evaluated. Complementing the experience-driven approach,
    the literature review was conducted with methodological rigor. This review followed
    an explicit protocol designed to ensure the comprehensive coverage and systematic
    analysis of the literature. We employed databases such as IEEE Xplore, Scopus,
    and the Web of Science to collect relevant publications. Specific inclusion and
    exclusion criteria were applied, guided by the Preferred Reporting Items of Systematic
    reviews and Meta-Analyses statement for reporting systematic reviews and meta-analyses,
    to ensure the relevance and quality of the literature included. Keywords and search
    strings were carefully chosen to encompass a broad range of topics within industrial
    system engineering and AI technologies, and searches were confined to peer-reviewed
    articles published within the last 3 years to ensure contemporary relevance. The
    selected references were chosen based on their citation index, relevance to the
    research questions, and the diversity of perspectives they offered. A thematic
    analysis was then conducted on the collated literature to identify, analyze, and
    report patterns (themes) within the data. This was essential to cross-reference
    the literature with the patterns observed in our practical experiences, thus ensuring
    a robust comparative analysis that bridges theory with practice. This approach
    can be summarized in the following diagram shown in Figure 1. Figure 1. Diagram
    of research methodology process. Assessing an organization’s current state for
    AI-based digital transformation is a complex endeavor that demands a systematic
    and multifaceted approach. This approach involves the integration of various methods
    and techniques tailored to the organization’s specific context and goals. By systematically
    integrating these methods and techniques, organizations can gain a holistic view
    of their current state in relation to AI adoption. In the development of our holistic
    guidelines for assessing an organization’s current state for AI-based digital
    transformation, we meticulously crafted a framework encompassing four essential
    components: current process, existing systems, data landscape, and AI capabilities.
    Each element plays a pivotal role in evaluating an organization’s readiness for
    AI integration, ensuring a comprehensive and informed approach to digital transformation.
    This comprehensive understanding forms the foundation for the informed decision
    making, strategic planning, and successful implementation of AI-based digital
    transformation initiatives. It is essential to provide a baseline to comprehend
    the organization’s readiness for transformation, ensuring the effective planning
    and execution of AI strategies [24,25]. Figure 2 depicts the elements of the current
    state assessment. It allows organizations to assess their AI readiness, thus enabling
    the strategic allocation of resources for AI initiatives [26]. Here, there is
    a deeper exploration of the systematic approach that encompasses diverse methods
    and techniques in terms of the elements of current state assessment: Figure 2.
    Element of current state assessment. 2.1. Current Processes Current processes
    are the business operations and workflows that form the backbone of an organization’s
    functioning. An in-depth understanding of current operations, workflows, and systems
    is essential for the successful application of AI. The work in [27] affirms that
    understanding the nuances of business operations and identifying areas that can
    benefit from AI are fundamental for enhancing operational efficiency. Understanding
    current processes requires at least the following: Process Documentation: The
    first step towards understanding the current processes is to have them thoroughly
    documented. This can be achieved by creating a process map or workflow diagram
    to visualize the steps involved, the roles accountable, the decisions made, and
    the systems used in each process. This documentation helps identify potential
    bottlenecks, inefficiencies, redundancies, and opportunities for process improvement
    or automation [26]. It outlines the sequential tasks required to perform the operational
    activities within a business. Once defined, these tasks provide an invaluable
    resource for understanding how work is carried out, who does it, the tools they
    use, and the outcomes they achieve [28]. It aids in identifying inefficient processes
    and potential areas for automation or improvement using AI technologies. Furthermore,
    it is essential for regulatory and compliance needs as well as in business process
    re-engineering, where the focus is on redesigning existing processes for greater
    efficiency or effectiveness [29]. The success of these AI-based tools heavily
    depends on the richness and quality of the data and how accurately it reflects
    the business processes. Process Performance Measurement: It is essential to have
    measures in place that accurately reflect the performance of the current processes.
    Key performance indicators (KPIs) should be defined for each process, capturing
    aspects such as the processing time, error rate, cost, and customer satisfaction.
    Tracking these metrics over time provides a performance baseline and can highlight
    areas where AI could add value [30]. This involves the identification and tracking
    of KPIs for individual processes, offering a quantifiable means to assess their
    operational performance and quality [30]. It can help identify areas of inefficiency,
    bottlenecks, or underperformance that could be addressed through automation, optimization,
    or redesign. Furthermore, this serves as a basis for the post-implementation comparison
    of AI solutions, making it possible to quantify the benefits of digital transformation
    in operational terms [31]. Various metrics may be employed. Common metrics include
    the cycle time, error rate, cost per transaction, process velocity, and customer
    satisfaction. It is increasingly common for businesses to use AI and data analytics
    tools for real-time process performance monitoring and analysis [32]. However,
    it is important to use relevant and balanced metrics to avoid creating unintended
    consequences. For instance, an overemphasis on speed might lead to a compromise
    in quality. Therefore, organizations must select metrics that provide a balanced
    view of the process performance [33]. Defining process KPIs is an essential part
    of the performance measurement and management. KPIs are quantifiable measurements
    that help organizations track their performance over time and achieve their strategic
    goals. Here, there are some steps to define process KPIs, as illustrated in Figure
    3 [34,35]: Figure 3. Defining process KPIs. Understand the process and define
    the purpose: The first step is to fully understand the process that the organization
    is tracking. The organization needs to know what the process involves, its objectives,
    and how it contributes to the overall business goals [36]. The purpose of the
    KPI should be clear. This could be to increase efficiency, improve quality, reduce
    cost, enhance customer satisfaction, or any other objective tied to the business
    strategy [35,36]. Identify metrics and KPI: Identify metrics that best represent
    the process’s performance and align with the organization’s objectives. These
    metrics can be quantitative (like processing time, cost, error rate) or qualitative
    (like customer satisfaction) [35,36]. They can also be made to be S.M.A.R.T: the
    KPIs should be specific, measurable, attainable, relevant, and time-bound. This
    means that each KPI should precisely state what it measures, be quantifiable,
    realistically achievable, have a clear link to the strategic objectives of the
    business, and be bound by a specific time frame [34]. Set targets: Decide on targets
    for each KPI. These should be challenging but achievable. The targets serve as
    a benchmark for assessing the process’s performance [35]. Regular review and communication:
    KPIs should be regularly reviewed to ensure they remain relevant and reflect any
    changes in business objectives or operating conditions. If a KPI is consistently
    being met, it may be time to set a more ambitious target. Conversely, if a KPI
    is consistently missed, it might be time to reassess whether the target is realistic
    [34,35]. Communicate the KPIs and their importance to all stakeholders, including
    employees. This helps ensure that everyone understands what KPIs are, why they
    are important, and how their role contributes to achieving them [34,35]. The process
    performance measurement, like process documentation, should be a continuous activity,
    with regular monitoring and updating to reflect the changes in the business environment,
    organizational objectives, or process redesign. Process Automation Opportunities:
    A careful analysis of the current processes can reveal repetitive tasks that are
    time-consuming and prone to human error, which are ideal candidates for automation.
    The use of AI can automate these tasks and improve the process efficiency. Moreover,
    AI can be applied to more complex tasks, such as decision making or pattern recognition,
    to further enhance the process effectiveness [37]. Identifying opportunities for
    process automation is a critical step in leveraging AI for digital transformation.
    Figure 4 illustrates the steps that can be used for business process automation
    as follows: Figure 4. Steps to automate business. Step 1: Identify the Process
    and Define the Goals The first step in automating a business process is to identify
    the processes that could and should be automated. Typically, processes that are
    repetitive, prone to human error, time-consuming, or important for compliance
    are good candidates for automation. By automating such processes, businesses can
    reduce the burden of mundane tasks on employees, leading to increased productivity
    and efficiency [38]. Determine what you want to achieve through automation. This
    could include improving efficiency, reducing errors, improving customer satisfaction,
    or other business objectives [39]. Step 2: Process Mapping Understand and document
    the existing process from start to end. This step involves outlining each stage
    of the process, identifying who is involved, and identifying the tools used. This
    provides a complete picture of the current process and helps identify areas of
    improvement [40]. Step 3: Identify Automation Opportunities and Choose the Right
    Tools Once the process has been mapped, identify which parts can be automated.
    It is important to consider which steps will yield the most benefit from automation
    because not all steps may be suitable or beneficial to automate [41]. Depending
    on the complexity of the process and business needs, the automation tool can vary.
    Tools can range from simple task automation software to more complex business
    process management (BPM) or robotic process automation (RPA) tools [42]. Step
    4: Design, Development, and Testing of the Automated Process Redesign the process
    by incorporating automation tools. Ensure that provisions are made for exceptions
    or error handling. It is crucial to have a clear process flow diagram that everyone
    can understand [43]. Once the process is designed, the next step is to build and
    test the automated process. This stage often involves IT professionals or consultants
    who have the skills to set up the automation and ensure that it works as expected
    [44]. Step 5: Training: Before fully implementing the automated process, ensure
    that all involved parties understand how it works and their role in it. They should
    know how to interact with the automation tool, how to manage exceptions, and who
    to contact if something goes wrong [45]. Step 6: Implementation, Monitoring, and
    Continuous Improvement After successful testing, roll out the automated process.
    It is often a good idea to do this gradually, starting with a pilot phase before
    full implementation [46]. After the process has been automated, it is important
    to monitor its performance to ensure that it meets its intended goals. Use the
    data from the automated process to identify areas for improvement and continually
    refine the process as needed [47]. The assessment of process automation opportunities
    needs to be carried out carefully, considering several factors. The complexity,
    frequency, and volume of the process are some of the aspects to be evaluated.
    It is also important to consider the cost, potential return on investment, and
    impact on customer service or other business functions [48]. Additionally, organizations
    should bear in mind that not all processes are suitable for automation. Processes
    that require human judgment, creativity, or complex decision making may not be
    ideal candidates for automation. Therefore, the assessment of automation opportunities
    should not only be based on the potential for efficiency gains but also on the
    strategic fit with organizational goals and values [49]. In recent years, AI-driven
    process mining tools have emerged as powerful aids for identifying automation
    opportunities. They can automatically analyze event logs from different IT systems
    to discover, monitor, and improve real-world business processes [50]. AI-powered
    technologies such as robotic process automation (RPA) [51] and intelligent process
    automation (IPA) [52] are increasingly being used to automate business processes.
    RPA involves the use of software robots or ‘bots’ to mimic human actions and perform
    repetitive tasks, whereas IPA incorporates machine learning and cognitive technology
    to automate and optimize more complex processes [53]. This assessment requires
    a multi-disciplinary approach that incorporates technical expertise, business
    acumen, and strategic thinking. Moreover, as AI technologies evolve and become
    more capable, the scope for automation is likely to increase, making the continuous
    re-evaluation of automation opportunities a necessity in the digital transformation
    journey. AI Alignment: Finally, current processes should be assessed for their
    alignment with potential AI capabilities. For instance, tasks involving large
    volumes of data or those requiring real-time decision making can particularly
    benefit from AI. This step requires a good understanding of both the business
    processes and the possibilities offered by AI [54]. AI alignment is a critical
    aspect of AI-based digital transformation that warrants careful attention and
    planning. It involves aligning AI applications and initiatives with the strategic
    objectives and values of the organization [55]. A successful digital transformation
    is not merely about implementing advanced technologies but also about leveraging
    these technologies to achieve business goals and create value [56]. Thus, assessing
    the alignment of AI initiatives with business strategy is essential to ensure
    that AI adoption is purposeful and effective. AI alignment involves several key
    dimensions that are essential for AI integration within an organization. Strategic
    alignment is crucial, as AI initiatives need to align with the organization’s
    strategic objectives to effectively contribute to their achievement. For instance,
    in an organization aiming to enhance customer service, this might involve implementing
    AI-powered chatbots or customer analytics systems [57]. Equally important is cultural
    alignment, where AI initiatives should resonate with the organization’s culture
    and values. This aspect emphasizes the importance of considering ethical implications,
    transparency, and the impact on employees. In an organization that values transparency,
    this would mean designing AI systems that are interpretable and explainable [58].
    Lastly, operational alignment ensures that AI initiatives are in sync with the
    organization’s operational needs and workflows. This involves smoothly integrating
    AI systems into existing processes and ensuring that the organization possesses
    the necessary infrastructure and skills to support these systems [59]. Each of
    these dimensions plays a vital role in the successful adoption and integration
    of AI within an organization. Assessing AI alignment is not a one-time activity
    but needs to be an ongoing process as business strategies, technologies, and market
    conditions evolve. Furthermore, AI alignment is not solely the responsibility
    of the IT department but should involve all key stakeholders, including business
    leaders, employees, and even customers [55]. AI alignment plays a key role in
    the success of AI-based digital transformation. Organizations should carefully
    consider and continuously monitor the alignment of AI initiatives with strategic
    objectives, cultural values, and operational needs. To achieve AI alignment, organizations
    should clearly define the goals and objectives for AI-based digital transformation,
    ensuring that they are aligned with the overall strategy of the organization.
    It is also important to establish ethical guidelines and principles for AI adoption
    and to develop processes to ensure that ethical considerations are integrated
    into AI system design and deployment. The organization should foster cross-functional
    collaboration among business, IT, data, and the ethics teams to ensure alignment
    across different organizational areas. The organization must continuously monitor
    and evaluate AI systems’ performance, impact, and alignment with the organization’s
    goals, adjusting and making improvements as needed. By prioritizing AI alignment
    throughout the digital transformation process, organizations can maximize the
    value and impact of AI technologies while ensuring ethical, responsible, and successful
    implementation. Hence, the current status assessment process can be summarized
    in the following diagram in Figure 5. Figure 5. The steps for current processes
    assessment. 2.2. Existing Systems Examining the existing technology infrastructure
    is critical to gauge an organization’s technological readiness for AI deployment
    [60]. This understanding can help identify the required changes in the IT infrastructure
    and guide the strategic selection of AI tools and technologies. It serves as the
    foundation for understanding the current technology landscape, which includes
    hardware, software, data storage, and processing systems. Figure 6 shows the Steps
    for existing systems assessment. Figure 6. Steps for existing systems assessment.
    System Identification: Identifying the existing systems within an organization
    is the first step toward AI-based digital transformation. An exhaustive list should
    include everything from customer relationship management (CRM), and enterprise
    resource planning (ERP) systems to specialized tools for inventory management,
    payroll, or content management. It is also crucial to account for informal and
    legacy systems that are still in use. Once all systems have been identified, the
    next step is categorization. Organize the systems and processes into functional
    categories such as customer management, finance, supply chain, operations, HR,
    marketing, and sales. Such categorization is pivotal for pinpointing areas where
    AI can deliver the most impact. Following categorization, evaluate each system’s
    suitability for AI integration. Factors to consider include data availability,
    system architecture, scalability, flexibility, and compatibility with AI technologies.
    An essential part of this process is engaging with users and stakeholders to identify
    the pain points and inefficiencies. Their insights are valuable for understanding
    how AI can resolve existing issues and improve the system performance. Concurrently,
    assessing the technical readiness for AI integration is necessary. This assessment
    should look at the system architecture, data format compatibility, and integration
    capabilities with AI frameworks and tools to determine whether system modifications
    or upgrades are needed. The scalability and interoperability of the systems must
    also be evaluated. Check whether the systems can handle increased demands from
    AI integration and scale accordingly to meet the AI’s evolving needs. Assess the
    ability of systems to communicate and exchange data with one another by checking
    for existing APIs, connectors, or frameworks that enable this integration. Finally,
    develop an implementation roadmap that details the sequence and timeline for AI
    integration, including the necessary steps, required resources, and milestones.
    Account for dependencies between systems and prioritize initiatives based on these
    factors. Remember that system identification is not a one-off task but a continuous
    part of the AI transformation journey. It is important to constantly evaluate
    and refine the system identification process, adapting the roadmap based on insights
    gained from ongoing implementation and feedback. Functional Analysis: For each
    system, conduct a detailed functional analysis. Understand the purpose each system
    serves, who uses it, what data it handles, and how it interacts with other systems.
    Document any known issues, limitations, or inefficiencies in the system. This
    analysis involves evaluating different functional areas within an organization
    to identify opportunities for AI integration and transformation. In enhancing
    various functional areas through AI, it is important to consider several key aspects:
    for customer experience, it is crucial to analyze customer-facing processes and
    touchpoints, identifying opportunities for AI-driven enhancements such as personalized
    recommendations, chatbots for customer support, sentiment analysis, and predictive
    modeling to improve satisfaction and engagement. In the realms of sales, marketing,
    and supply chain, evaluating processes to determine where AI can boost lead generation,
    customer segmentation, targeting, and campaign optimization is vital. The role
    of AI extends to pricing optimization, demand forecasting, customer behavior analysis,
    and recommendation engines for sales growth. In addition, in supply chain management,
    AI can streamline operations, enhance forecasting accuracy, optimize inventory
    management, and improve production planning, with applications in predictive maintenance
    and operational efficiency monitoring. For data and analytics and strategic decision
    making, it is essential to evaluate data management and analytics processes to
    establish a strong AI foundation, focusing on data governance, quality, integration,
    and infrastructure. The utility of AI in data discovery, cleansing, advanced analytics,
    and supporting data-driven decision making through predictive analytics, scenario
    modeling, and intelligent decision support systems is also significant. Lastly,
    in quality assurance and testing, identifying opportunities for AI integration
    is key, with applications in automated testing, anomaly detection, and quality
    control to enhance product or service quality, reduce defects, and boost testing
    efficiency, thereby facilitating continuous improvement and optimization efforts
    across all these functional areas. Technical Analysis: Performing a technical
    evaluation of the system is a critical step in understanding an organization’s
    readiness for AI implementation. This evaluation encompasses an examination of
    the system architecture, compatibility, scalability, security, and performance.
    It also involves assessing the age of the systems and their ability to support
    newer technologies, including AI. To gain a comprehensive understanding, this
    evaluation can be further divided into two main areas: hardware capabilities and
    the software environment, and network infrastructure and integration capabilities.
    In terms of hardware capabilities and the software environment, it is essential
    to evaluate the computing power and hardware infrastructure available within the
    organization. This involves assessing whether the existing hardware can meet the
    computational demands of AI algorithms and models. Factors such as processing
    speed, memory capacity, and parallel processing capabilities are crucial in this
    regard. Along with hardware, reviewing the software environment and tools currently
    used is vital. This review should identify whether the organization possesses
    the necessary software and development frameworks to support AI initiatives. It
    is also important to consider whether the organization’s software ecosystem is
    compatible with popular AI platforms, libraries, and frameworks. Evaluating the
    network infrastructure is another key aspect. This includes examining the bandwidth
    capacity and latency to determine whether the organization’s network can manage
    the increased data traffic that AI applications typically bring. Assessing the
    need for network upgrades or optimizations is crucial to ensure a smooth data
    transfer and communication between AI systems and data sources. Furthermore, assessing
    how well the organization’s existing technology infrastructure integrates with
    AI systems and tools is vital. This involves evaluating any limitations or challenges
    in integrating AI solutions with the organization’s current systems, databases,
    and applications and ensuring compatibility with APIs, data formats, and protocols
    for seamless data exchange. Data Evaluation: Since AI heavily relies on data,
    take a close look at the type and quality of data that each system handles. Evaluate
    the data structure, quality, availability, and relevance for potential AI use
    cases. This can be elaborated by evaluating data processing capabilities and storage
    architecture. This can be achieved by assessing the processing capabilities of
    an organization’s infrastructure in relation to AI workloads. Consider factors
    such as the ability to handle large-scale data processing, parallel processing,
    and distributed computing. Evaluate whether an organization’s infrastructure can
    efficiently handle the computational demands of AI algorithms and models. Evaluate
    the organization’s storage architecture in terms of scalability, performance,
    and data access. Consider whether the organization has a suitable storage solution,
    such as distributed file systems or object storage, that can handle the volume,
    variety, and velocity of data required for AI applications. Assess whether an
    organization’s storage architecture supports efficient data retrieval and processing.
    Integration with existing systems should be evaluated. This can be achieved by
    considering how well an organization’s current technology infrastructure integrates
    with existing systems, applications, and workflows. Assess the compatibility of
    the organization’s infrastructure with legacy systems and third-party applications
    that may need to interact with AI solutions. Evaluate whether there are any limitations
    or constraints in integrating AI with the organization’s existing technology stack.
    The organization should evaluate the real-time processing capabilities. Determine
    whether the organization’s infrastructure supports real-time data processing and
    analytics. Assess whether the organization has the necessary components, such
    as stream processing frameworks or event-driven architectures, to enable real-time
    decision making and AI-driven insights. Consider the ability to handle high-velocity
    data streams for real-time AI applications. Another important element is high
    availability and reliability. The organization should evaluate the availability
    and reliability of its infrastructure. It needs to consider whether the organization
    has redundant systems, failover mechanisms, or load balancing capabilities to
    ensure the high availability of AI applications. Assess whether the organization’s
    infrastructure can deliver the required uptime and reliability for critical AI-driven
    processes. Finally, the organization should evaluate whether it has the automation
    and orchestration capabilities to manage AI workflows and processes efficiently.
    Consider whether the organization has tools or platforms that enable workflow
    automation, job scheduling, and resource provisioning for AI tasks. Assess whether
    the organization can streamline the deployment and management of AI models and
    algorithms. Vendor Evaluation: If the systems are provided by external vendors,
    review the terms of these relationships. Assess the level of vendor support, maintenance,
    and potential integration with new AI technologies. Tools such as enterprise architecture
    software (like MEGA, BiZZdesign, or Software AG) can assist in documenting and
    visualizing the existing system landscape, making it easier to identify gaps,
    redundancies, and opportunities for improvement [61]. Challenges in this process
    may include resistance from staff accustomed to legacy systems, uncovering hidden
    or informal systems, and assessing poorly documented systems. Overcoming these
    challenges requires a systematic approach, stakeholder involvement, and sometimes
    expert assistance. Evaluating vendors for AI-based digital transformation is a
    crucial step to ensure that the organization selects the right partners who can
    support the organization’s goals. When evaluating vendors for AI-based digital
    transformation, organizations must systematically and thoroughly approach the
    process to ensure alignment with their specific needs and long-term objectives.
    Begin by clearly outlining the organization’s requirements and objectives and
    identifying the specific AI technologies, tools, or the solutions sought, along
    with the desired outcomes and key performance indicators (KPIs) to achieve. This
    clarity will guide the assessment of the technological capabilities and offerings
    of each vendor, examining the breadth and depth of their AI solutions, scalability,
    performance, compatibility with existing infrastructure, and support for the organization’s
    use cases. Furthermore, a vendor’s AI expertise and experience are critical. Evaluate
    their proficiency with AI technologies and track record in implementing AI projects,
    especially those akin to the organization’s industry and use cases. Delve into
    their knowledge of machine learning, data science, and relevant AI frameworks
    to ensure that they have the depth required for successful delivery. Integration
    capabilities are equally important. Assess the vendor’s ability to integrate AI
    solutions with the organization’s existing systems, applications, and data sources.
    This includes their expertise in data integration, API availability, and compatibility
    with the current technology stack. It is also necessary to consider the vendor’s
    customization abilities and scalability. They should be able to tailor their solutions
    to the organization’s unique requirements and workflows and scale solutions to
    accommodate the growth and evolving demands of AI initiatives. Implementation
    and support services must be scrutinized. Evaluate the vendor’s project management
    approach, training, onboarding programs, and extent of ongoing technical support.
    Comprehensive documentation, user training, and post-implementation support are
    crucial for the smooth transition and effective utilization of AI solutions. The
    vendor’s methodology for AI model development and deployment also demands attention.
    Consider their practices for model training, validation, deployment, explainability,
    interpretability, and ethical considerations. The potential for scalability and
    future growth should not be overlooked; assess the vendor’s infrastructure capacity,
    cloud integration capabilities, and vision for future AI advancements to support
    the organization’s long-term growth. Vendor support for change management and
    organizational readiness is a determinant of successful adoption. Evaluate their
    processes and ability to guide the organization through change management strategies,
    organizational restructuring, and cultural adaptation. Post-implementation support
    services, such as technical support, service-level agreements, and timely issue
    resolution, are also paramount. Finally, conduct a risk assessment to gauge the
    potential risks associated with each vendor, including their stability, financial
    health, data security, and privacy assurances. Compliance with industry regulations
    and standards, along with proper risk mitigation measures, should be confirmed.
    The total cost of ownership (TCO) is a decisive factor; hence, consider all costs,
    such as initial implementation, ongoing maintenance, licensing, and scaling or
    customization. A comprehensive cost–benefit analysis ensures that the vendor’s
    solutions meet the organization’s budget constraints and offer a favorable return
    on investment (ROI). 2.3. Data Landscape Data are the lifeblood of AI. Understanding
    the type of data that are collected, how they are stored and managed, and their
    use in decision making constitutes a crucial part of the current state assessment
    [62]. Data quality, governance, privacy, and security are the key considerations
    in this regard. The assessment of an organization’s data landscape is a complex
    process that encompasses several key facets, as outlined below. Data Quality:
    This involves an assessment of the data’s accuracy, integrity, timeliness, completeness,
    relevancy, and consistency. The poor data quality can lead to incorrect conclusions
    or faulty machine learning models [63]. Quality issues can arise from various
    sources, such as data entry errors, missing data, inconsistent data formats, or
    outdated information. Tools such as IBM’s InfoSphere Information Analyzer [64],
    Informatica Data Quality [65], and Talend Data Quality [66] can help assess and
    improve data quality. Data quality is a critical factor in the success of AI-based
    digital transformation initiatives. Here, are some key considerations for ensuring
    the data quality in AI-based digital transformation. First, establish robust data
    governance practices to ensure the data quality throughout its lifecycle. Define
    data ownership, responsibilities, and processes for data collection, storage,
    cleaning, and maintenance. Implement the data quality standards, data validation
    rules, and data access controls. Implement data cleaning and preprocessing techniques
    to address data quality issues. This may involve removing duplicate records, handling
    missing values, standardizing data formats, and correcting inconsistencies. Use
    data cleansing tools and algorithms to automate these processes where possible.
    Then, evaluate the context and relevance of data for AI applications. Ensure that
    the data used for training AI models are representative, unbiased, and relevant
    to the desired outcomes. Consider factors such as data source credibility, data
    sampling techniques, and the representativeness of data for the target population
    or problem domain. Define data quality metrics that align with the organization’s
    specific AI use cases and objectives. Establish key performance indicators (KPIs)
    to measure the data quality, such as accuracy, completeness, timeliness, consistency,
    and relevancy. Regularly monitor these metrics and establish thresholds for acceptable
    data quality. Finally, establish data monitoring and validation processes to continuously
    assess the data quality. Implement the data quality monitoring tools and techniques
    to identify anomalies, errors, and data inconsistencies. Regularly validate the
    data against predefined quality metrics and perform data audits to maintain high-quality
    data. Data Accessibility: It is not enough to have good-quality data; it must
    also be accessible to those who need it. This includes evaluating the existing
    data architecture, understanding where the data reside (on-premises or in the
    cloud), how these are stored (in databases, data warehouses, or data lakes), and
    how these can be accessed (APIs, SQL queries, etc.) [67]. Data accessibility is
    a critical aspect of AI-based digital transformation as it enables organizations
    to effectively leverage their data assets for AI initiatives. To ensure data accessibility,
    conduct a comprehensive inventory of the organization’s data assets; identify
    the types of data available, their sources, formats, and locations; and document
    the metadata, such as data definitions, data owners, and data access permissions.
    In addition, organizations should establish data accessibility governance processes
    to ensure compliance and adherence to data policies and regulations. Define data
    accessibility guidelines, data access approval processes, and data usage policies.
    Regularly monitor data access patterns, review access privileges, and update data
    accessibility policies as needed. It is vital to maintain the comprehensive documentation
    of data assets, including their source, transformation processes, and usage history.
    Document data lineage to track the origin and transformations applied to data.
    This documentation ensures transparency and enables users to understand the data’s
    context and reliability. Finally, ensure that data accessibility platforms and
    infrastructure can handle the performance and scalability requirements of AI-based
    digital transformation. Evaluate system performance, response times, and scalability
    under different data access scenarios. Scale resources as needed to accommodate
    increasing data accessibility demands. Data Governance: Data governance involves
    the management of the availability, usability, integrity, and security of the
    data. It encompasses the data policies, procedures, standards, roles, and responsibilities
    related to data management, data privacy, and compliance issues [68]. Tools such
    as Collibra [69] and Informatica Axon [70] can support data governance efforts.
    Data governance is crucial for AI-based digital transformation initiatives to
    ensure the availability, integrity, and privacy of data. Implementing data governance
    in AI-based digital transformation requires a multifaceted approach to managing
    data throughout its lifecycle. Begin by establishing a robust data governance
    framework that details policies, processes, roles, and responsibilities. This
    framework should articulate data governance objectives, delineate data stewardship
    roles, and establish cross-functional data governance committees tasked with overseeing
    data-related activities. Clarity in data ownership and accountability is key to
    assigning specific individuals or teams within the organization the responsibility
    for data management. Data stewards play a pivotal role in ensuring data quality,
    integrity, and compliance. To foster a culture of accountability for data management,
    it is important to encourage the organization-wide recognition of the value and
    importance of data. To maintain and improve the governance process, regular audits
    and reviews are essential. These evaluations should assess compliance with established
    data governance practices, determine their effectiveness, and identify areas that
    require improvement. Data governance assessments highlight adherence to policies
    and reveal gaps or non-compliance areas. The insights from these audits should
    inform the refinement of data governance processes and be applied to AI models
    and algorithms. Establish guidelines for AI model development, training data selection,
    model validation, and ongoing monitoring to ensure that ethical considerations
    and interpretability requirements are met while maintaining transparency and thorough
    documentation throughout the process. Metrics and reporting mechanisms play critical
    roles in data governance. It is important to define specific metrics that can
    measure the effectiveness of data governance initiatives. Establishing key performance
    indicators (KPIs) will help track data quality, compliance, accessibility, and
    the maturity of the data governance process. Develop a regular reporting framework
    to ensure the clear visibility of governance activities and their progress towards
    the set objectives. Recognize that implementing data governance is a change management
    initiative. It requires clear communication of its importance to all stakeholders,
    along with fostering a data-driven culture throughout the organization. Providing
    training and support to employees ensures that they understand and commit to data
    governance practices. It is also important to address any resistance to change
    by continuously communicating the benefits and positive impacts of a strong data
    governance strategy. Data Volume and Variety: The amount and types of data collected
    by the organization can influence the types of AI techniques that can be applied.
    Large volumes of data can support more complex models such as deep learning. The
    burgeoning volume of data presents several challenges that are critical to the
    successful implementation and functioning of AI systems, particularly in the realm
    of deep learning. The first challenge is the data deluge itself. With exponential
    data growth being a primary catalyst for AI, especially in deep learning, the
    resulting vast amounts of data necessitate robust solutions for storage, processing,
    and management. This ‘data deluge’ can quickly overwhelm existing infrastructures.
    Regarding storage, the sheer scale of data generated today demands more sophisticated
    and efficient storage solutions. Traditional storage architectures often fail
    to meet the scalability, performance, and cost-efficiency requirements of intensive
    AI workloads. Innovations such as non-volatile memory (NVM) and distributed storage
    systems are being investigated as potential solutions to these storage challenges.
    On the processing front, AI models and deep learning algorithms require substantial
    computational resources to process large datasets. This necessity has prompted
    an increased reliance on specialized hardware such as GPUs and TPUs, which are
    designed to accelerate AI training and inference. Alongside hardware solutions,
    there is a push towards developing new techniques such as model compression, cleaning,
    and quantization to make AI processing more efficient. Effective data management
    becomes crucial when dealing with voluminous data. It is essential for AI systems
    to implement robust strategies for data cleaning, preprocessing, labeling, and
    organizing. To simplify the labor-intensive process of data labeling, innovative
    approaches such as active learning, weak supervision, and transfer learning are
    being considered. Another challenge is data heterogeneity, where large datasets
    often comprise data from myriad sources, each potentially differing in format
    and structure, complicating integration and reconciliation efforts. Privacy and
    security concerns escalate with increasing data volume. More data heighten the
    risk of breaches and exposure, particularly to sensitive information. As data
    quantities grow, addressing these privacy and security issues becomes increasingly
    paramount. Furthermore, the presence of massive datasets does not inherently resolve
    the issues of bias and representativeness. Large data volumes can still harbor
    demographic, cultural, or other biases, potentially skewing AI model accuracy.
    Lastly, data access can be a significant hurdle. Organizations might have voluminous
    datasets at their disposal but are impeded from leveraging them due to legal or
    regulatory constraints. Ensuring appropriate permissions and licenses for data
    access and utilization is a non-trivial aspect that organizations must carefully
    navigate. Moreover, the variety of data—structured (like databases), semi-structured
    (like XML files), and unstructured (like text or images)—can also affect the choice
    of AI models and preprocessing techniques [71]. In the context of AI-based digital
    transformation, the volume and variety of data are crucial elements that organizations
    must address. A scalable infrastructure is fundamental for handling the volume
    and variety of data required for AI initiatives. Organizations should consider
    cloud-based solutions for flexibility and scalability, which are advantageous
    when adapting to growing data needs. The deployment of technologies such as distributed
    storage systems and parallel processing frameworks is also critical to efficiently
    manage large data volumes. An organization’s data storage and management capabilities
    must be evaluated to ensure that they can cope with the increased volume and variety
    of data. It is necessary to implement data management systems that can handle
    a range of data types, including structured, unstructured, and semi-structured
    data. Centralized storage solutions, such as data lakes or data warehouses, can
    be beneficial for efficient data retrieval and storage. To process and analyze
    these data, big data processing frameworks and analytics tools are indispensable.
    Technologies such as Apache Hadoop and Apache Spark, among other distributed computing
    platforms, enable the parallel processing and analysis of large datasets. Furthermore,
    leveraging machine learning and AI algorithms can extract valuable insights from
    varied data sources. Given the volume and diversity of data, automating data preparation
    processes is essential. Data preparation tools and technologies can streamline
    the ingestion, cleansing, and transformation of data. Automated data pipelines
    and workflows can significantly reduce manual efforts and ensure consistency in
    data preparation. Lastly, it is crucial to continuously monitor the volume and
    variety of data to ensure that the organization’s infrastructure and processes
    remain capable of meeting evolving requirements. Implementing monitoring mechanisms
    to detect shifts in data volume, variety, or data source patterns is necessary.
    Organizations must regularly assess and adapt their data management strategies
    to accommodate changes in data characteristics, ensuring ongoing alignment with
    the organization’s AI objectives. To effectively implement data governance in
    AI-based digital transformation, organizations must deeply understand the various
    elements of the data landscape. This understanding is crucial for assessing data
    readiness for AI, identifying gaps, and devising a strategy to address them. The
    use of data is a critical facet of this transformation, and several considerations
    are key to effectively using data in AI initiatives. Organizations should start
    by defining clear objectives for data usage in AI transformations and determining
    how data will contribute to specific business goals, process improvements, or
    innovations. Aligning data usage with strategic objectives ensures a focused and
    relevant application [72]. A data-driven decision-making culture within an organization
    is vital. Encouraging stakeholders to base their decisions on data and insights
    from AI models fosters trust in data and AI processes [73]. Identifying relevant
    data sources is also essential. An organization must assess both internal and
    external data sources, including structured and unstructured data, to ensure that
    they effectively contribute to AI outcomes [74]. Feature engineering is critical
    for transforming raw data into meaningful features that enhance the performance
    of the AI model. This involves applying domain knowledge and data analytics techniques
    to select and transform the most informative attributes [75]. Ethical considerations
    in data usage cannot be overstated. Organizations must adhere to privacy regulations
    and data protection policies, employing techniques such as anonymization and encryption
    to maintain data privacy throughout the AI lifecycle [76]. Measuring ROI and value
    from data usage in AI initiatives is necessary. This involves establishing KPIs
    that reflect the organization’s goals and tracking the impact of data-driven initiatives
    [77]. Predictive and prescriptive analytics are powerful tools for leveraging
    data. Using historical and real-time data, organizations can forecast future trends
    and behaviors and generate recommendations for optimizing business processes [78].
    Personalization enhances customer experience. By leveraging customer data, AI
    models can create personalized recommendations, targeted marketing, and customized
    offerings [79]. Data also plays a role in risk management and fraud detection.
    AI models can be used to identify potential risks or fraudulent patterns, with
    real-time monitoring systems in place to proactively address these issues [80].
    Lastly, continuous improvement and learning from data are imperative. Organizations
    should establish feedback mechanisms to continuously refine AI models and strategies,
    allowing for learning and adaptation to new data and business requirements [81].
    The data landscape assessment process can be summarized as shown diagram in Figure
    7. Figure 7. Steps for data landscape assessment. 2.4. AI Capabilities An organization’s
    AI capabilities, including the existing AI or machine learning initiatives, skills,
    tools, and infrastructure, need to be evaluated [26]. This step can provide insights
    into the organization’s capacity to embark on AI-based digital transformation.
    This assessment is multifaceted and encompasses the following key components:
    Existing AI Initiatives: Understanding the existing AI or machine learning projects
    in an organization provides critical insights into the organization’s experience
    with AI technologies and highlights recurring issues or challenges faced during
    these initiatives. Analyzing the scope, outcomes, and lessons learned from past
    and ongoing AI projects is invaluable for identifying potential pitfalls and adopting
    best practices in future projects [53]. When assessing existing AI initiatives
    in the context of digital transformation, it is important to evaluate the objectives
    of each project. Understanding the intended outcomes, such as improving operational
    efficiency, enhancing customer experience, or driving innovation, and aligning
    them with the overall AI strategy is crucial for assessing their relevance to
    the organization’s digital transformation goals [82]. In the realm of data use
    and model development, it is essential to analyze how data are being used in current
    AI initiatives. Assessing the types of data, their quality, volume, and variety,
    and evaluating the effectiveness of data preprocessing, feature engineering, and
    data integration techniques used in these projects is crucial. In addition, reviewing
    the development process of AI models within these initiatives is key. This review
    should include an assessment of the algorithms, techniques, frameworks used, level
    of automation, model selection, and hyperparameter tuning techniques employed.
    Evaluating the model performance, accuracy, and generalization capability, as
    well as the integration of AI models with existing systems or processes, is also
    important. This includes considering the scalability, reliability, and availability
    of the deployed models, their level of integration with other IT systems, such
    as CRM, ERP, or IoT platforms, and the efficiency of model monitoring and feedback
    loops for continuous improvement. Evaluating the impact and value generated by
    existing AI initiatives is another critical step. This involves assessing measurable
    outcomes, such as cost savings, revenue growth, or improved customer satisfaction,
    and analyzing the effectiveness of AI solutions in achieving the desired objectives
    and driving business value. Feedback from stakeholders and end-users regarding
    the perceived benefits and limitations of the initiatives should also be considered
    [83]. Finally, analyzing mechanisms for the continuous improvement and learning
    from existing AI initiatives is essential. This includes evaluating feedback loops,
    monitoring processes, adaptation strategies, and using user feedback, data-driven
    insights, and emerging technologies to refine and enhance existing AI solutions.
    The organizational impact of AI initiatives should be assessed, including the
    level of change management required to integrate AI solutions into existing processes,
    workflows, or organizational structures, considering the cultural shift, skill
    development, and organizational readiness for embracing AI-driven changes. Identifying
    challenges related to change management and planning for mitigating resistance
    or barriers is also an important aspect of this assessment [84,85]. Skills and
    Expertise: Assessing the technical skills and expertise within an organization
    is a fundamental aspect of preparing for AI-based digital transformation. This
    involves identifying staff members who possess skills in data science, machine
    learning, or related fields and pinpointing gaps in expertise that might require
    filling through recruitment, training, or external partnerships. This assessment
    is crucial for the effective planning and execution of AI initiatives because
    a lack of necessary skills can significantly hinder the success of these projects
    [86]. The first step is to identify the specific technical skills required for
    AI-based digital transformation initiatives. This includes skills in programing
    languages such as Python or R, understanding machine learning algorithms, statistical
    analysis, data manipulation, and data visualization [87,88]. A comprehensive list
    of relevant skills that align with the organization’s AI strategy should be created.
    Conducting an inventory of the current skills and expertise of the organization’s
    employees is essential to assess their proficiency in the identified technical
    areas and their experience in AI-related projects. This assessment can be conducted
    through various methods such as self-assessments, surveys, interviews, or performance
    evaluations. The next step involves conducting a skill gap analysis. This entails
    comparing the current skills inventory with the skills required for AI initiatives.
    Identifying these skill gaps is crucial to understanding where an organization’s
    capabilities fall short of its AI objectives [89]. Based on this analysis, developing
    training and upskilling programs is necessary to enhance the technical skills
    of employees. These could include workshops, online courses, or specialized training
    programs in areas relevant to AI, machine learning, and data science [90]. The
    third step is to evaluate the need for external expertise. If certain skills are
    lacking internally, it might be necessary to hire data scientists, AI specialists,
    or consultants who specialize in AI and data science. Collaborating with external
    partners, research institutions, or industry experts can provide access to additional
    technical skills and knowledge [91]. Finally, creating career development paths
    and growth opportunities for employees interested in AI and data science is essential.
    Providing mentorship programs, job rotations, or project assignments allows employees
    to apply and enhance their technical skills in AI initiatives. Supporting employees
    in gaining certifications or pursuing advanced degrees in relevant fields is also
    beneficial. Evaluating employees’ practical experience with AI technologies and
    tools, including their involvement in AI projects such as data preprocessing,
    model development, and deployment, is also important. Identifying individuals
    with hands-on experience in implementing AI solutions and working with real-world
    datasets is key to building a robust AI-capable workforce [92]. Tools and Infrastructure:
    Evaluating the current AI infrastructure is critical in determining whether an
    organization has the necessary hardware and software to support AI projects. This
    includes assessing data storage capacity, computing power, and networking capabilities.
    Understanding the available and suitable AI tools and platforms, such as cloud-based
    AI services and AI development tools, is also essential for effective strategic
    planning [93]. For AI-based digital transformation, having the right tools and
    infrastructure is crucial. In terms of data preprocessing and cleaning, tools
    such as pandas, scikit-learn, and Apache Spark are popular choices. These assist
    in handling missing data, outlier detection, data normalization, and feature scaling
    [94,95]. For machine learning and AI development, selecting appropriate tools
    and frameworks is vital. Python libraries such as TensorFlow, PyTorch, and scikit-learn
    offer a range of algorithms, models, and development frameworks for building,
    training, and deploying AI models [96]. For model training, deployment, and serving,
    tools that facilitate these processes, such as hyperparameter tuning, model evaluation,
    and comparison, are essential. Google Cloud AutoML, H2O.ai, or Microsoft Azure
    Machine Learning Studio are popular options [97,98,99]. Additionally, tools such
    as TensorFlow Serving, Amazon SageMaker, and Microsoft Azure ML Deployment help
    in deploying and serving AI models in production [100,101,102]. Automated machine
    learning (AutoML) tools such as Google Cloud AutoML, H2O.ai’s Driverless AI, or
    DataRobot automate the machine learning process from data preprocessing to model
    selection and tuning, even for those with limited expertise [103,104,105]. Data
    visualization and reporting tools such as Tableau, Power BI, or matplotlib/seaborn
    in Python are crucial for effectively communicating insights and results [106,107,108].
    For textual data, tools for natural language processing and text analytics, such
    as the Natural Language Toolkit (NLTK), spaCy, or Google Cloud NLP API, are important
    for tasks such as sentiment analysis or text classification [109,110,111]. In
    computer vision and image processing, tools such as OpenCV, TensorFlow’s Object
    Detection API, and Microsoft Azure Computer Vision enable tasks such as object
    detection or image segmentation [112,113,114]. Leveraging cloud infrastructure
    for AI-based digital transformation is essential. Cloud platforms such as AWS,
    GCP, and Microsoft Azure provide scalable solutions for data storage, model training,
    and deployment. Edge computing capabilities are also significant for real-time
    AI applications, with platforms such as NVIDIA Jetson, Intel Movidius, or Google
    Coral for edge AI deployments [115,116]. DevOps and MLOps practices streamline
    AI model development, deployment, and maintenance, using tools for version control,
    continuous integration, and model monitoring such as Git, Jenkins, Docker, or
    Kubeflow. Explainability and interpretability tools such as SHAP, Lime, and IBM
    AI Explainability 360 are crucial for understanding AI model decision making and
    addressing bias and transparency concerns [117,118]. Finally, automated data pipelines
    are key for efficient data movement from various sources to AI systems. Tools
    such as Apache Airflow, AWS Glue, and Google Cloud Dataflow are useful for this
    purpose [119,120,121]. Model versioning and management tools such as MLflow, DVC,
    and Git LFS help in tracking and managing different versions of AI models for
    reproducibility and traceability [122,123,124]. Culture and Leadership: The integration
    of AI within an organization is deeply influenced by its culture and leadership.
    A supportive leadership team and an organizational culture that values openness,
    collaboration, and innovation can significantly boost the success rate of AI projects.
    Conversely, resistance to change, lack of commitment from leadership, and poor
    collaboration can impede AI adoption [72]. Leadership buy-in and support are indispensable
    for AI-driven transformation. Leaders must not only advocate for AI but also communicate
    its significance and commit the necessary resources to its adoption. Their active
    participation affirms their commitment to AI initiatives [125]. AI-induced transformation
    is inherently a change management exercise. It is crucial to devise strategies
    that address employee apprehensions and the potential fear of job displacement.
    Cultivating an environment that welcomes change and advocates continuous learning
    is key to successful transformation [126]. Building a learning culture that encourages
    innovation and continuous improvement is another cornerstone. Providing ongoing
    training and development opportunities allows employees to enhance their AI competencies.
    A culture that motivates employees to explore new AI techniques and technologies
    can be very powerful. Moreover, promoting a culture where decisions are made on
    the basis of data is essential. Employees should be encouraged to use AI insights
    to inform their decisions with robust frameworks in place to support this approach
    at all organizational levels [127]. Investment in continuous leadership development
    tailored to AI is crucial. Leaders should be equipped with the necessary knowledge
    and skills to effectively steer and support AI initiatives. They should also have
    opportunities to keep abreast of the latest AI advancements and industry trends
    [128]. Finally, it is important to measure and celebrate the successes of AI initiatives.
    Establishing clear metrics to gauge the impact of AI on the organization and acknowledging
    the contributions of employees are important. Providing feedback and rewards for
    individual and team efforts can reinforce positive outcomes and support further
    AI integration efforts. The AI capability assessment process can be summarized
    in diagram shown in Figure 8. Figure 8. AI capability assessment. 3. Results Assessing
    an organization’s current status for AI-based digital transformation can be systematically
    approached through a framework structure. This framework, as shown in Figure 9,
    evaluates various aspects of the organization’s readiness and capabilities across
    several key dimensions: Figure 9. Elements of the current status assessment structure.
    The first dimension, current process, focuses on evaluating existing business
    processes for AI integration. It entails a detailed analysis of the organization’s
    current workflows to identify areas where AI can enhance efficiency. This examination
    delves into the intricacies of existing processes, uncovering inefficiencies,
    redundancies, and bottlenecks. Understanding these processes allows organizations
    to strategically integrate AI, streamlining operations, and improve productivity.
    Next, existing systems aim to assess the organization’s technology infrastructure
    and applications. This dimension scrutinizes the technological backbone of the
    organization, evaluating hardware, software, and the overall IT architecture.
    It considers factors such as compatibility, scalability, and integration capabilities.
    By identifying the strengths and weaknesses in the existing systems, organizations
    can make informed decisions about the necessary upgrades or modifications for
    smooth AI integration. The Data Landscape dimension analyses the quality, quantity,
    and accessibility of organizational data. It involves a thorough assessment of
    data sources, data quality, and data management practices. This analysis ensures
    that the data are reliable, accessible, and diverse enough to meet the requirements
    of AI algorithms. Understanding the data landscape is crucial for mitigating biases
    and enhancing the accuracy of AI-driven insights. AI capabilities is another critical
    dimension that focuses on evaluating an organization’s current AI-related knowledge
    and skills. It examines the existing talent pool, training programs, and partnerships
    with external AI experts. This assessment helps identify gaps in skills and knowledge,
    which are essential for planning targeted training programs and strategic collaborations.
    It ensures that the organization is equipped to effectively leverage AI technologies.
    By employing this structured evaluation framework, organizations can gain a comprehensive
    understanding of their readiness for AI integration. The insights from each dimension
    enable organizations to identify skill gaps, make strategic decisions about training
    and external expertise, and foster a culture of continuous learning. This thorough
    assessment forms a solid foundation for a successful and adaptive AI-based digital
    transformation journey. 4. Discussion Assessing an organization’s current state
    is a critical step in AI-based digital transformation. It provides valuable insights
    into the organization’s strengths, weaknesses, and readiness to adopt AI technologies.
    Through this assessment, organizations can identify gaps, develop a strategic
    roadmap, and effectively allocate resources [24]. By understanding the technological
    infrastructure, data availability, organizational culture, talent pool, business
    processes, and regulatory considerations, organizations can make informed decisions
    and embark on a successful AI-driven digital transformation journey [26]. The
    process starts with assessing organizational needs and objectives by understanding
    the organization’s strategic goals, challenges, and areas where AI can bring value.
    This leads to identifying the specific business processes, decision making, or
    customer experience aspects that could benefit from AI [27]. The assessment of
    the current state of an organization for AI-based digital transformation includes
    the evaluation of the potential for automation within the identified systems through
    AI [37]. It is based on the analysis of operational data to identify bottlenecks,
    inefficiencies, or areas for improvement in order to implement AI-driven process
    automation to streamline operations, reduce costs, and enhance productivity. This
    identification process may require you to engage system users and stakeholders.
    This involvement seeks their input and feedback on the systems’ strengths, weaknesses,
    and areas for improvement by understanding their requirements and expectations
    to ensure that AI integration aligns with their needs. In addition, assessing
    the involvement and engagement includes an evaluation of the collaboration and
    communication processes within the organization to identify opportunities for
    AI-driven enhancements in applications for natural language processing, virtual
    assistants, and knowledge management systems to facilitate efficient collaboration
    and communication. AI is a technology that heavily depends on data, which must
    be of high quality, in proper format, available, and accessible [63]. Therefore,
    it is essential to assess data quality, availability, and accessibility within
    each system. It is important to identify the data required for AI initiatives
    and assess whether they are captured, stored, and structured effectively. This
    must cover the consideration of data gaps, inconsistencies, and data integration
    challenges that need to be addressed for AI implementation. Therefore, it is essential
    to determine the data sources feeding into each system and the flow of data between
    systems [74]. This naturally leads to the identification of the critical data
    points and processes that contribute to the functioning of the systems, such as
    any data gaps or bottlenecks that need to be addressed to ensure comprehensive
    AI implementation. The assessment of the current state covers the capability of
    the organization to handle a huge growing amount of data. This requires the assessment
    of data compression and storage optimization within the organization. Advanced
    capability to implement data compression techniques to reduce the storage requirements
    for large volumes of data. Use compression algorithms such as gzip, zlib, or Snappy
    to compress data files without sacrificing data integrity. In addition, explore
    storage optimization techniques, such as data deduplication or data archival strategies,
    to efficiently manage and store large volumes of data. This may include data lake
    architecture that allows the storage of diverse data types in their raw form to
    provide a centralized repository for storing structured and unstructured data,
    enabling easy access for AI-based analysis and processing [67]. Leverage data
    lake architectures to support the variety and scalability of data. This allows
    data handling to be taken to a further step related to data integration and data
    fusion from multiple sources to create a comprehensive and holistic view to combine
    data from disparate systems or departments to gain insights that may not be apparent
    when analyzing individual data sources in isolation as a result of data integration
    techniques to ensure seamless data connectivity and interoperability. There are
    tools that facilitate data integration and extract, transform, load (ETL) processes
    [129]. These tools enable the seamless extraction of data from various sources,
    transformation into a usable format, and loading into the target systems. The
    organization may consider tools such as Apache Kafka [130], Talend [131], or Informatica
    [132] for efficient data integration. Implement alerts or triggers to notify stakeholders
    of any significant changes or deviations in data patterns. Humans cannot deal
    with data in any format by requiring data exploration and visualization techniques
    to gain insights from the data by using exploratory data analysis (EDA) [133]
    to understand the characteristics, patterns, and relationships within the data.
    It is practical and time saving to visualize data through interactive dashboards,
    charts, and graphs to effectively communicate insights to stakeholders. It provides
    deep insight into the use of data visualization and storytelling techniques to
    effectively communicate insights derived from data. Such a project requires an
    analysis of the compliance process and risk management activities to identify
    areas where AI can improve efficiency, accuracy, and risk assessment, and where
    AI applications for regulatory compliance monitoring, fraud detection, anomaly
    detection, and automated compliance reporting can be used across different functional
    areas. The assessment of the current state includes the evaluation of risk assessment
    and fraud detection processes to identify opportunities for AI applications to
    automate risk analysis, identify patterns of fraudulent activities, and enhance
    fraud detection capabilities. Furthermore, it must assess compliance and ethics
    processes to identify areas where AI can ensure adherence to regulatory requirements
    and ethical standards. This promotes AI applications for automated compliance
    monitoring, audit trail analysis, and ethical decision support systems. Therefore,
    the other dimension of AI-based digital transformation is related to expertise,
    knowledge transfer, and upskilling. The adoption of such technology requires promoting
    knowledge transfer and upskilling initiatives to empower employees with the skills
    and knowledge to effectively utilize data. Success can be maintained by providing
    training programs on data analysis, AI techniques, and data visualization tools
    and by fostering a data-literate workforce that can leverage data insights for
    informed decision making. This encourages risk taking and innovation in AI initiatives
    as it creates an environment where employees feel empowered to experiment, learn
    from failures, and propose new ideas to recognize and reward innovation and creative
    problem solving. This leads to the development of an agile and adaptive mindset
    to respond to the rapidly changing AI landscape, to encourage flexibility, agility,
    and adaptability in AI projects, and to foster an environment that embraces iteration,
    feedback, and continuous improvement. AI-based digital transformation requires
    continuous learning and adaptation to recognize that technical skills in AI and
    data science are continuously evolving. This requires encouraging employees to
    stay updated with the latest advancements, trends, and technologies in AI through
    participation in conferences, webinars, or industry events. Foster a culture of
    continuous learning and adaptation to keep pace with the fast-changing AI landscape.
    As technology continues to advance and high-level skills are needed, the issue
    of cross-functional collaboration arises to assess employees’ ability to collaborate
    across different functions and domains to identify individuals who can effectively
    communicate and work with stakeholders from various backgrounds, such as business
    teams, domain experts, or data scientists, to bridge the gap between technical
    expertise and business requirements. To keep up with technological advancements,
    employees should have updated industry knowledge and awareness. Hence, it is essential
    to evaluate employees’ knowledge of industry-specific AI applications, trends,
    and challenges to identify individuals who stay updated with the latest developments
    in AI within the organization’s industry, follow industry-specific AI use cases,
    and understand the unique considerations and opportunities for AI-based digital
    transformation in the organization’s sector. This takes employees to a higher
    level of continuous integration and deployment (CI/CD) [134] evaluation of their
    knowledge of CI/CD practices for AI model development and deployment. This indicates
    that they understand version control, automated testing, and continuous integration
    processes, which are crucial for maintaining code quality and ensuring the smooth
    deployment of AI models. Comprehensively covering AI skills among employees is
    vital, and this includes a variety of technological proficiencies. The expertise
    in natural language processing (NLP) techniques is crucial, particularly for organizations
    that handle textual data. Proficiency in deep learning techniques and neural networks,
    which form the backbone of many AI applications, should also be assessed among
    employees. For those dealing with image or video data, skills in computer vision
    are essential. Additionally, employee proficiency in big data technologies like
    Apache Hadoop or Apache Spark is important, as these technologies are commonly
    used for processing large volumes of data. Familiarity with cloud platforms such
    as Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure
    should be evaluated, considering their increasing importance in AI and data processing.
    Knowledge of DevOps practices and ML Ops (machine learning operations) principles
    is another critical area to assess, as these are increasingly relevant for efficient
    AI deployment and maintenance. Skills in data engineering, which include data
    integration, preprocessing, and pipeline development, are essential for managing
    the data lifecycle. Lastly, proficiency in data visualization tools and techniques
    should also be evaluated, as this skill is key in effectively communicating insights
    from data. However, AI-based digital transformation presents organizations with
    multifaceted challenges, inherent risks, and potential pitfalls that necessitate
    comprehensive analysis and strategic mitigation strategies. At the core of these
    challenges lies the intricate issue of data quality and availability. Organizations
    frequently encounter deficiencies and biases within datasets, which impede the
    accuracy and reliability of AI algorithms. In addition, a persistent scarcity
    of skilled professionals well versed in AI technologies amplifies the challenge,
    demanding extensive training initiatives and proactive talent acquisition efforts.
    The integration of AI systems with existing technologies poses a formidable challenge,
    requiring meticulous planning and implementation to ensure harmonious co-existence
    and minimal disruptions to ongoing operations. From a risk perspective, the vulnerability
    of AI systems to security breaches is a significant concern. Inadequately secured
    AI frameworks become susceptible targets for cyber-attacks, potentially leading
    to severe data breaches and the compromise of sensitive information. Furthermore,
    the ethical risk of algorithmic bias remains pronounced, with biased training
    data perpetuating societal prejudices and reinforcing discriminatory outcomes.
    An additional risk materializes in the form of over-reliance on AI technologies
    without adequate human oversight, particularly in sectors with high-stakes decision-making
    requirements, such as healthcare or finance, raising concerns about erroneous
    outcomes and potential ethical dilemmas. In tandem with these challenges and risks,
    organizations must navigate the potential pitfalls inherent to AI-based digital
    transformation. Unrealistic expectations often cloud the transformative potential
    of AI technologies, leading to disillusionment if capabilities are overestimated
    or inadequately understood. Vendor lock-in, in which organizations excessively
    rely on specific AI vendor ecosystems, can stifle flexibility and hinder innovation.
    Inadequate pre-deployment testing poses a significant pitfall because insufficiently
    tested AI applications are prone to unexpected failures, causing disruptions and
    eroding stakeholder trust. Lastly, the absence of continuous monitoring and adaptation
    strategies renders AI solutions susceptible to obsolescence, inefficiency, or
    ethical quandaries over time. Effectively addressing these challenges and mitigating
    associated risks and pitfalls necessitates a nuanced, adaptable approach grounded
    in rigorous academic inquiry. Organizations must prioritize the cultivation of
    high-quality, unbiased datasets, invest in continuous education and training initiatives,
    and implement robust cybersecurity protocols. Moreover, fostering a culture of
    innovation and ethical awareness is imperative to encourage interdisciplinary
    collaboration, knowledge dissemination, and ongoing discourse within the academic
    and professional community. Through these concerted efforts, organizations can
    navigate the complexities of AI-based digital transformation in an academically
    rigorous and ethically sound manner, thereby realizing the transformative potential
    while minimizing associated risks. To accommodate the future growth of the organization
    and greater usage and dependency on AI-based digital transformation, the organization
    has to assess the current state of the scalability and flexibility of existing
    AI initiatives to accommodate future growth and evolving business needs and assess
    the ability of AI solutions to handle increasing data volumes, user demands, or
    changing market dynamics. The organization may consider the extensibility and
    adaptability of the AI infrastructure, models, and algorithms to support scalability
    and flexibility. 5. Conclusions This research systematically explored and elucidated
    the key assessment elements crucial for the successful integration of AI technologies
    within organizational contexts. This study has contributed a comprehensive framework
    designed to guide organizations in their AI-based digital transformation endeavors.
    These elements not only draw from established theories but also emphasize their
    real-world impact. By meticulously conducting these assessments, organizations
    can gain profound insights into their existing strengths and areas that need improvement.
    Acknowledging the limitations inherent in any research, such as contextual variations
    and the rapidly evolving nature of AI technologies, our work provides a solid
    foundation for future academic inquiry and practical applications. As the landscape
    of AI-based technologies continues to evolve, our research points toward essential
    areas for further investigation. Future studies might explore specific industry
    applications, the impact of cultural and regulatory factors, and the development
    of adaptive frameworks capable of accommodating dynamic organizational needs.
    In the realm of practical applications, organizations are encouraged to embrace
    our guidelines as a starting point for their AI-based digital transformation initiatives.
    We emphasize the paramount importance of feedback mechanisms and continuous improvement
    strategies. By fostering a culture of iterative learning and adaptation, organizations
    can ensure the sustained relevance and effectiveness of their AI integration efforts.
    In the academic sphere, scholars are encouraged to delve deeper into the nuanced
    intersections of AI technology and organizational dynamics. It is important to
    mention that this research underscores the importance role of AI technologies
    in shaping the future of organizations. Through our comprehensive framework, we
    have illuminated the pathway for integrating AI within the organizational fabric,
    balancing both academic rigor and the practical exigencies of technological implementation.
    However, we acknowledge that integrating experiential insights, although valuable
    for capturing real-world complexities, introduces a subjective dimension to our
    research. This subjective lens is a limitation of the current study and indicates
    the qualitative nature of experience-driven research. Moreover, while our findings
    provide actionable guidelines, they are by their very nature influenced by the
    specific contexts from which our experiences arise. This underscores the necessity
    for future research to validate and extend these findings across diverse contexts
    and to critically assess the transferability of our framework to different organizational
    environments. Author Contributions Conceptualization, A.A., A.M.H. and K.N.A.-K.;
    writing—original draft preparation, A.A., A.M.H. and K.N.A.-K.; writing—review
    and editing, A.M.H. and A.A.; supervision, K.N.A.-K. and A.M.H. All authors have
    read and agreed to the published version of the manuscript. Funding Open Access
    funding provided by the Qatar National Library. Data Availability Statement The
    data presented in this study are available on request from the corresponding author.
    Conflicts of Interest The authors declare no conflicts of interest. References
    Gołąb-Andrzejak, E. AI-powered Digital Transformation: Tools, Benefits and Challenges
    for Marketers–Case Study of LPP. Procedia Comput. Sci. 2023, 219, 397–404. [Google
    Scholar] [CrossRef] Kim, K.; Kim, B. Decision-making model for reinforcing digital
    transformation strategies based on artificial intelligence technology. Information
    2022, 13, 253. [Google Scholar] [CrossRef] Gill, S.S.; Xu, M.; Ottaviani, C.;
    Patros, P.; Bahsoon, R.; Shaghaghi, A.; Golec, M.; Stankovski, V.; Wu, H.; Abraham,
    A.; et al. AI for next generation computing: Emerging trends and future directions.
    Internet Things 2022, 19, 100514. [Google Scholar] [CrossRef] Bogers, M.L.A.M.;
    Garud, R.; Thomas, L.D.W.; Tuertscher, P.; Yoo, Y. Digital innovation: Transforming
    research and practice. Innovation 2022, 24, 4–12. [Google Scholar] [CrossRef]
    Ancillai, C.; Sabatini, A.; Gatti, M.; Perna, A. Digital technology and business
    model innovation: A systematic literature review and future research agenda. Technol.
    Forecast. Soc. Change 2023, 188, 122307. [Google Scholar] [CrossRef] Jarrahi,
    M.H.; Kenyon, S.; Brown, A.; Donahue, C.; Wicher, C. Artificial intelligence:
    A strategy to harness its power through organizational learning. J. Bus. Strategy
    2023, 44, 126–135. [Google Scholar] [CrossRef] Rožman, M.; Oreški, D.; Tominc,
    P. Artificial Intelligence-Supported Reduction of Employees’ Workload to Increase
    the Company’s Performance in Today’s VUCA Environment. Sustainability 2023, 15,
    5019. [Google Scholar] [CrossRef] Raffey, M.A.; Gaikwad, S.B. The Impact of Artificial
    Intelligence on Business Operations: Investigating The Current State And Future
    Implications Of AI Technologies. J. Pharm. Negat. Results 2022, 5577–5580. [Google
    Scholar] [CrossRef] Mihai, F.; Aleca, O.E.; Gheorghe, M. Digital Transformation
    Based on AI Technologies in European Union Organizations. Electronics 2023, 12,
    2386. [Google Scholar] [CrossRef] Khanom, M.T. Business Strategies in The Age
    of Digital Transformation. J. Bus. 2023, 8, 28–35. [Google Scholar] Perifanis,
    N.-A.; Kitsios, F. Investigating the influence of artificial intelligence on business
    value in the digital era of strategy: A literature review. Information 2023, 14,
    85. [Google Scholar] [CrossRef] Fan, Q.; Ouppara, N. Surviving disruption and
    uncertainty through digital transformation: A case study on small to medium-sized
    enterprises (SME). In Moving Businesses Online and Embracing E-Commerce: Impact
    and Opportunities Caused by COVID-19; IGI Global: Beijing, China, 2022; pp. 1–22.
    [Google Scholar] Şişci, M.; Torkul, Y.E.; Selvi, I.H. Machine Learning as a Tool
    for Achieving Digital Transformation. Knowl. Manag. Digit. Transform. Power 2022,
    1, 55. [Google Scholar] O’Callaghan, M. Decision Intelligence: Human—Machine Integration
    for Decision-Making; CRC Press: Boca Raton, FL, USA, 2023. [Google Scholar] King,
    K. Using Artificial Intelligence in Marketing: How to Harness AI and Maintain
    the Competitive Edge; Kogan Page Publishers: London, UK, 2019. [Google Scholar]
    Brunetti, F.; Matt, D.T.; Bonfanti, A.; De Longhi, A.; Pedrini, G.; Orzes, G.
    Digital transformation challenges: Strategies emerging from a multi-stakeholder
    approach. TQM J. 2020, 32, 697–724. [Google Scholar] [CrossRef] Brock, J.K.U.;
    Von Wangenheim, F. Demystifying AI: What digital transformation leaders can teach
    you about realistic artificial intelligence. Calif. Manag. Rev. 2019, 16, 110–134.
    [Google Scholar] [CrossRef] Jan, J.; Weißert, M.; Wyrtki, K. Ready or not, AI
    comes—An interview study of organizational AI readiness factors. Bus. Inf. Syst.
    Eng. 2021, 63, 5–20. [Google Scholar] Davenport, T.H. The AI advantage: How to
    Put the Artificial Intelligence Revolution to Work; MIT Press: Cambridge, MA,
    USA, 2018. [Google Scholar] Kitsios, F.; Kamariotou, M. Artificial intelligence
    and business strategy toward digital transformation: A research agenda. Sustainability
    2021, 13, 2025. [Google Scholar] [CrossRef] Vidu, C.-M.; Pinzaru, F.; Mitan, A.
    What managers of SMEs in the CEE region should know about challenges of artificial
    intelligence’s adoption?—An introductive discussion Co menedżerowie MŚP w regionie
    Europy Środkowo-Wschodniej powinni wiedzieć o wyzwaniach związanych z wprowadzeniem.
    Nowocz. Syst. Zarządzania 2022, 17, 63–76. [Google Scholar] [CrossRef] Cayirtepe,
    Z.; Senel, F.C. The future of quality and accreditation surveys: Digital transformation
    and artificial intelligence. Int. J. Qual. Health Care 2022, 34, mzac025. [Google
    Scholar] [CrossRef] Ross, J.W.; Beath, C.M.; Mocker, M. Designing a digital organization.
    MIT Sloan Manag. Rev. 2018, 59, 57–65. [Google Scholar] Smith, D.R. Creation of
    a Unified Cloud Readiness Assessment Model to Improve Digital Transformation Strategy.
    Int. J. Data Sci. Anal. 2022, 8, 11. [Google Scholar] [CrossRef] Ahlberg, J.;
    Eriksson, C. To Measure Organizational Wellness with AI-A Future Competitive Advantage?
    Department of Business Administration, Lund University Press: Lund, UK, 2021.
    [Google Scholar] Davenport, T.; Guha, A.; Grewal, D.; Bressgott, T. How artificial
    intelligence will change the future of marketing. J. Acad. Mark. Sci. 2020, 48,
    24–42. [Google Scholar] [CrossRef] Bughin, J.; Chui, M.; Manyika, J. Artificial
    Intelligence: The Next Digital Frontier? McKinsey Global Institute: New York,
    NY, USA, 2018. [Google Scholar] Scheer, A.W. Architecture of Integrated Information
    Systems: Foundations of Enterprise Modeling; Springer: Berlin/Heidelberg, Germany,
    2018. [Google Scholar] Jaheer Mukthar, K.P.; Sivasubramanian, K.; Asis, E.H.R.;
    Guerra-Munoz, M.E. Redesigning and Reinvention of Retail Industry Through Artificial
    Intelligence (AI). In Future of Organizations and Work After the 4th Industrial
    Revolution: The Role of Artificial Intelligence, Big Data, Automation, and Robotics;
    Springer International Publishing: Cham, Switzerland, 2022; pp. 41–56. [Google
    Scholar] Melville, N.; Kraemer, K.; Gurbaxani, V. Review: Information technology
    and organizational performance: An integrative model of IT business value. MIS
    Q. 2018, 28, 283–322. [Google Scholar] [CrossRef] Jeston, J. Business Process
    Management: Practical Guidelines to Successful Implementations; Routledge: London,
    UK, 2018. [Google Scholar] Van Der Aalst, W.M.; Bichler, M.; Heinzl, A. Data-Driven
    Process Discovery and Analysis; Springer Nature: Berlin/Heidelberg, Germany, 2020.
    [Google Scholar] Wamba-Taguimdje, S.-L.; Wamba, S.F.; Kamdjoug, J.R.K.; Wanko,
    C.E.T. Influence of artificial intelligence (AI) on firm performance: The business
    value of AI-based transformation projects. Bus. Process Manag. J. 2020, 26, 1893–1924.
    [Google Scholar] [CrossRef] Bisbe, J.; Malagueño, R. How to design a successful
    KPI system. MIT Sloan Manag. Rev. 2018, 59, 45–51. [Google Scholar] Parmenter,
    D. Key Performance Indicators: Developing, Implementing, and Using Winning KPIs,
    4th ed.; John Wiley & Sons: Hoboken, NJ, USA, 2015. [Google Scholar] Bititci,
    U.S.; Carrie, A.S.; McDevitt, L.; Turner, T. Creating and managing value in collaborative
    networks. Int. J. Oper. Prod. Manag. 2017, 37, 87–102. [Google Scholar] [CrossRef]
    Lacity, M.; Willcocks, L. Nine keys to unlocking digital transformation in business
    operations. MIS Q. Exec. 2016, 15, 135–149. [Google Scholar] Fischer, T. Robotic
    Process Automation; Springer: Wiesbaden, Germany, 2018. [Google Scholar] Hammer,
    M.; Stanton, S. Reengineering the Corporation: A Manifesto for Business Revolution;
    Harper Business: New York, NY, USA, 2019. [Google Scholar] Dumas, M.; La Rosa,
    M.; Mendling, J.; Reijers, H.A. Fundamentals of Business Process Management; Springer:
    Berlin/Heidelberg, Germany, 2018. [Google Scholar] Lee, J.; Lapira, E.; Bagheri,
    B.; Kao, H.A. Recent advances and trends in predictive manufacturing systems in
    big data environment. Manuf. Lett. 2017, 11, 113–120. [Google Scholar] [CrossRef]
    Ramaswamy, R.; Gou, Y.; Wu, D.J.; Bush, D.; Grover, P. Organizing for digital
    innovation: The division of innovation labor between upstream and downstream teams.
    J. Manag. Inf. Syst. 2018, 35, 169–204. [Google Scholar] Smith, H.A.; Fingar,
    P. Business Process Management: The Third Wave; Meghan-Kiffer Press: Tampa, FL,
    USA, 2017. [Google Scholar] Lacity, M.; Willcocks, L.; Craig, A. Robotic process
    automation at Telefónica O2. MIS Q. Exec. 2018, 17, 99–108. [Google Scholar] Alavi,
    M.; Leidner, D.E. Knowledge management and knowledge management systems: Conceptual
    foundations and research issues. MIS Q. 2001, 25, 107–136. [Google Scholar] [CrossRef]
    Davenport, T.H.; Short, J.E. The new industrial engineering: Information technology
    and business process redesign. Sloan Manag. Rev. 2018, 29, 11–27. [Google Scholar]
    Power, D.J. Decision Support Systems: Concepts and Resources for Managers; Business
    Expert Press: New York, NY, USA, 2017. [Google Scholar] Willcocks, L.; Lacity,
    M.; Craig, A. Robotic Process Automation and Risk Mitigation: The Definitive Guide;
    SB Publishing, Ashford, UK, 2017. Schwartz, J. Workforce of the future: The competing
    forces shaping 2030. Strategy Leadersh 2019, 47, 16–22. [Google Scholar] Van Der
    Aalst, W. Process Mining: Data Science in Action; Springer: Berlin/Heidelberg,
    Germany, 2016. [Google Scholar] Martínez-Rojas, A.; Sánchez-Oliva, J.; López-Carnicer,
    J.M.; Jiménez-Ramírez, A. Airpa: An architecture to support the execution and
    maintenance of AI-powered RPA robots. In Proceedings of the International Conference
    on Business Process Management, Rome, Italy, 10–11 December 2021; Springer International
    Publishing: Cham, Switzerland, 2021; pp. 38–48. [Google Scholar] Kholiya, P.S.;
    Kapoor, A.; Rana, M.; Bhushan, M. Intelligent process automation: The future of
    digital transformation. In Proceedings of the 2021 10th International Conference
    on System Modeling & Advancement in Research Trends (SMART), Moradabad, India,
    10–11 December 2021; IEEE: New York, NY, USA, 2021; pp. 185–190. [Google Scholar]
    Davenport, T.H.; Ronanki, R. Artificial intelligence for the real-world. Harv.
    Bus. Rev. 2018, 96, 108–116. [Google Scholar] Brynjolfsson, E.; McAfee, A. The
    Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies;
    WW Norton & Company: New York, NY, USA, 2014. [Google Scholar] Kolbjørnsrud, V.
    How artificial intelligence will redefine management. Harv. Bus. Rev. 2018, 96,
    62–69. [Google Scholar] Brynjolfsson, E.; McAfee, A. The business of artificial
    intelligence. Harv. Bus. Rev. 2018, 96, 108–116. [Google Scholar] Nguyen, T.-M.;
    Malik, A. Cognitive processes, rewards and online knowledge sharing behavior:
    The moderating effect of organizational innovation. J. Knowl. Manag. 2020, 24,
    1241–1261. [Google Scholar] [CrossRef] Bryson, J.; Winfield, A. Standardizing
    ethical design for artificial intelligence and autonomous systems. Computer 2017,
    50, 116–119. [Google Scholar] [CrossRef] Du, S.; Xie, C. Paradoxes of artificial
    intelligence in consumer markets: Ethical challenges and opportunities. J. Bus.
    Res. 2021, 129, 961–974. [Google Scholar] [CrossRef] Raguseo, E. Big data technologies:
    An empirical investigation on their adoption, benefits and risks for companies.
    Int. J. Inf. Manag. 2018, 38, 187–195. [Google Scholar] [CrossRef] Guo, H.; Li,
    J.; Gao, S.; Smite, D. Boost the potential of EA: Essential practices. In Proceedings
    of the 23rd International Conference on Enterprise Information Systems, Online,
    26–28 April 2021; SciTePress: Setubal, Portugal, 2021. [Google Scholar] Kiron,
    D.; Unruh, G. Is Organization’s Business Ready for a Digital Future? MIT Sloan
    Manag. Rev. 2018, 59, 21–25. [Google Scholar] Aldoseri, A.; Khalifa; Al-Khalifa,
    N.; Hamouda, A.M. Re-Thinking Data Strategy and Integration for Artificial Intelligence:
    Concepts, Opportunities, and Challenges. Appl. Sci. 2023, 13, 7082. [Google Scholar]
    [CrossRef] IBM InfoSphere Information Analyzer. Available online: https://www.ibm.com/products/infosphere-information-analyzer
    (accessed on 24 September 2023). Informatic Data Quality. Available online: https://www.informatica.com/gb/products/dataquality/informatica-dataquality.html
    (accessed on 29 September 2023). Talend Data Quality. Available online: https://www.talend.com/products/data-quality/
    (accessed on 3 October 2023). Li, R.; Rao, J.; Wan, L. The digital economy, enterprise
    digital transformation, and enterprise innovation. Manag. Decis. Econ. 2022, 43,
    2875–2886. [Google Scholar] [CrossRef] Karkošková, S. Data governance model to
    enhance data quality in financial institutions. Inf. Syst. Manag. 2023, 40, 90–110.
    [Google Scholar] [CrossRef] Collibra Data Governance. Available online: https://www.collibra.com/us/en/products/datagovernance
    (accessed on 15 October 2023). AXON DATA GOVERNANCE. Available online: https://www.informatica.com/gb/products/dataquality/axon-data
    governance.html (accessed on 15 October 2023). Mahalle, P.N.; Hujare, P.P.; Shinde,
    G.R. Data Acquisition and Preparation. In Predictive Analytics for Mechanical
    Engineering: A Beginners Guide; Springer Nature: Singapore, 2023; pp. 11–38. [Google
    Scholar] Marchand, D.A.; Kettinger, W.J.; Rollins, J.D. Information orientation,
    business agility, and digital transformation. MIS Q. 2018, 42, 591–616. [Google
    Scholar] Gandomi, A.; Haider, M. Beyond the hype: Big data concepts, methods,
    and analytics. Int. J. Inf. Manag. 2015, 35, 137–144. [Google Scholar] [CrossRef]
    Almeida, F.; Bacao, F.; Santos, M.F. Data-driven innovation: Concepts, approaches,
    and empirical evidence. Inf. Syst. Manag. 2019, 36, 99–114. [Google Scholar] Kelleher,
    J.D.; Mac Namee, B.; D’Arcy, A. Fundamentals of Machine Learning for Predictive
    Data Analytics: Algorithms, Worked Examples, and Case Studies; MIT Press: Cambridge,
    MA, USA, 2015. [Google Scholar] Floridi, L.; Cowls, J.; Beltrametti, M.; Chatila,
    R.; Chazerand, P.; Dignum, V.; Luetge, C.; Madelin, R.; Pagallo, U.; Rossi, F.;
    et al. AI4People—An ethical framework for a good AI society: Opportunities, risks,
    principles, and recommendations. Minds Mach. 2018, 28, 689–707. [Google Scholar]
    [CrossRef] [PubMed] Marr, B. Artificial Intelligence in Practice: How 50 Successful
    Companies Used AI and Machine Learning to Solve Problems; Wiley: Hoboken, NJ,
    USA, 2019. [Google Scholar] Davenport, T.H. Process Innovation: Reengineering
    Work through Information Technology; Harvard Business Review Press: Brighton,
    MA, USA, 2013. [Google Scholar] Xiong, J.; Qin, G.; Liu, X.; Sun, X. Deep learning
    in personalized recommendation: A survey. Proc. IEEE 2019, 107, 15–37. [Google
    Scholar] Phua, C.; Lee, V.; Smith, K.; Gayler, R. A comprehensive survey of data
    mining-based fraud detection research. arXiv 2010, arXiv:1009.6119. [Google Scholar]
    Zhu, X.; Zheng, Y.; Zhang, Z.; Li, J.; Yu, P.S. Deep learning for online advertising:
    A comprehensive review. ACM SIGKDD Explor. Newsl. 2020, 22, 5–20. [Google Scholar]
    Najdawi, A.; Shaheen, A. Which Project Management Methodology is better for AI-Transformation
    and Innovation Projects? In Proceedings of the 2021 International Conference on
    Innovative Practices in Technology and Management (ICIPTM), Noida, India, 17–19
    February 2021; IEEE: New York, NY, USA, 2021; pp. 205–210. [Google Scholar] Holmström,
    J. From AI to digital transformation: The AI readiness framework. Bus. Horiz.
    2022, 65, 329–339. [Google Scholar] [CrossRef] Grebe, M.; Franke, M.R.; Heinzl,
    A. Artificial intelligence: How leading companies define use cases, scale-up utilization,
    and realize value. Inform. Spektrum 2023, 1–13. [Google Scholar] [CrossRef] Neumann,
    O.; Guirguis, K.; Steiner, R. Exploring artificial intelligence adoption in public
    organizations: A comparative case study. Public Manag. Rev. 2023, 1–28. [Google
    Scholar] [CrossRef] Bharadiya, J.P. A Comparative Study of Business Intelligence
    and Artificial Intelligence with Big Data Analytics. Am. J. Artif. Intell. 2023,
    7, 24. [Google Scholar] Voss, N.; Falcone, M.; Witherow, R.; Tenreiro, N.; Gans,
    H.; Camburn, M. Competency Modeling: An Essential Practice for the Future of Strategic
    Human Capital Management; Society for Industrial and Organizational Psychology,
    Inc.: Columbus, OH, USA, 2022. [Google Scholar] Li, B.; Qi, P.; Liu, B.; Di, S.;
    Liu, J.; Pei, J.; Yi, J.; Zhou, B. Trustworthy AI: From principles to practices.
    arXiv 2023, arXiv:2110.01167. [Google Scholar] [CrossRef] Zabala, C.; Javier,
    F. The Barriers for Implementing AI. In Grow Your Business with AI: A First Principles
    Approach for Scaling Artificial Intelligence in the Enterprise; Apress: Berkeley,
    CA, USA, 2023; pp. 85–110. [Google Scholar] La Torre, D.; Colapinto, C.; Durosini,
    I.; Triberti, S. Team formation for human-artificial intelligence collaboration
    in the workplace: A goal programing model to foster organizational change. IEEE
    Trans. Eng. Manag. 2021, 70, 1966–1976. [Google Scholar] [CrossRef] Heilig, T.;
    Scheer, I. Decision Intelligence: Transform Your Team and Organization with AI-Driven
    Decision-Making. John Wiley & Sons: Hoboken, NJ, USA, 2023. [Google Scholar] Bawany,
    S. Transforming the Next Generation Leaders: Developing Future Leaders for a Disruptive,
    Digital-Driven Era of the Fourth Industrial Revolution (Industry 4.0); Business
    Expert Press: New York, NY, USA, 2019. [Google Scholar] Priya, K.; Akshara, Y.A.S.;
    Venkatesh, J. Instinctive Data Analysis in Machine Learning and Summary Exhibitor.
    In Intelligent and Fuzzy Systems: Digital Acceleration and the New Normal—Proceedings
    of the INFUS 2022 Conference, July 19-21, Izmir, Turkey; Springer Nature: Berlin/Heidelberg,
    Germany, 2022; Volume 2. [Google Scholar] Rajamani, S.K.; Iyer, R.S. Machine Learning-Based
    Mobile Applications Using Python and Scikit-Learn. In Designing and Developing
    Innovative Mobile Applications; IGI Global: Beijing, China, 2023; pp. 282–306.
    [Google Scholar] Apache Spark. Available online: https://spark.apache.org/ (accessed
    on 16 December 2023). Liu, Y.H. Python Machine Learning by Example: Build Intelligent
    Systems Using Python, TensorFlow 2, PyTorch, and Scikit-Learn; Packt Publishing
    Ltd.: Birmingham, UK, 2020. [Google Scholar] Zeng, Y.; Zhang, J. A machine learning
    model for detecting invasive ductal carcinoma with Google Cloud AutoML Vision.
    Comput. Biol. Med. 2020, 122, 103861. [Google Scholar] [CrossRef] Daugėla, K.;
    Vaičiukynas, E. Real-Time Anomaly Detection for Distributed Systems Logs Using
    Apache Kafka and H2O.ai. In Proceedings of the International Conference on Information
    and Software Technologies, Kaunas, Lithuania, 13–15 October 2022; Springer International
    Publishing: Cham, Switzerland, 2022; pp. 33–42. [Google Scholar] Etaati, L.; Etaati,
    L. Azure machine learning studio. In Machine Learning with Microsoft Technologies:
    Selecting the Right Architecture and Tools for Your Project; Springer: Berlin/Heidelberg,
    Germany, 2019; pp. 201–223. [Google Scholar] TensorFlow. TensorFlow Serving Guide.
    Available online: https://www.tensorflow.org/tfx/guide/serving (accessed on 21
    December 2023). Amazon Web Services. Amazon SageMaker|Build, Train, and Deploy
    Machine Learning Models. Available online: https://aws.amazon.com/sagemaker/ (accessed
    on 21 December 2023). Klaffenbach, F.; Michalski, O.; Klein, M.; Wali, M.; Tanasseri,
    N.; Rai, R. Implementing Azure: Putting Modern DevOps to Use: Transform Your Software
    Deployment Process with Microsoft Azure; Packt Publishing Ltd.: Birmingham, UK,
    2019. [Google Scholar] Google Cloud. AutoML—Google Cloud. Available online: https://cloud.google.com/automl
    (accessed on 21 December 2023). H2O.ai. H2O Driverless AI—H2O.ai. Available online:
    https://h2o.ai/platform/ai-cloud/make/h2o-driverless-ai/ (accessed on 21 December
    2023). DataRobot. DataRobot: Augmented Machine Learning Platform. Available online:
    https://www.datarobot.com/ (accessed on 21 December 2023). Tableau. Tableau: Business
    Intelligence and Analytics Software. Available online: https://www.tableau.com/
    (accessed on 21 December 2023). Microsoft. Power BI|Interactive Data Visualization
    BI Tools. Available online: https://powerbi.microsoft.com/en-gb/ (accessed on
    21 December 2023). Seaborn. Seaborn: Statistical Data Visualization. Available
    online: https://seaborn.pydata.org/ (accessed on 21 December 2023). NLTK. Natural
    Language Toolkit—NLTK 3.6.5 Documentation. Available online: https://www.nltk.org/
    (accessed on 21 December 2023). spaCy. spaCy · Industrial-Strength Natural Language
    Processing in Python. Available online: https://spacy.io/ (accessed on 21 December
    2023). Google Cloud. Cloud Natural Language|Cloud Natural Language API|Google
    Cloud. Available online: https://console.cloud.google.com/apis/library/language.googleapis.com
    (accessed on 21 December 2023). OpenCV. Available online: https://www.opencv.ai/
    (accessed on 21 December 2023). TensorFlow. TensorFlow Object Detection API. Available
    online: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/
    (accessed on 21 December 2023). Azure. Computer Vision|Microsoft Azure. Available
    online: https://azure.microsoft.com/en-gb/products/cognitive-services/computervision
    (accessed on 21 December 2023). Run:AI. NVIDIA A100 GPU: Overview, Benchmarks,
    and Buying Guide|Run:AI. Available online: https://www.run.ai/guides/nvidia-a100/nvidia-jetson
    (accessed on 21 December 2023). Coral. Coral—Edge TPU—Google Coral. Available
    online: https://coral.ai/ (accessed on 21 December 2023). SHAP. SHAP (SHapley
    Additive exPlanations). Available online: https://shap.readthedocs.io/en/latest/
    (accessed on 21 December 2023). IBM. AI Explainability 360|IBM OSS. Available
    online: https://www.ibm.com/opensource/open/projects/ai-explainability/ (accessed
    on 21 December 2023). Apache. Apache Airflow. Available online: https://airflow.apache.org/
    (accessed on 21 December 2023). AWS. AWS Glue—Data Extraction, Transformation,
    and Loading (ETL)—Amazon Web Services. Available online: https://aws.amazon.com/glue/
    (accessed on 21 December 2023). Google Cloud. Cloud Dataflow: Stream & Batch Data
    Processing|Google Cloud. Available online: https://cloud.google.com/dataflow (accessed
    on 21 December 2023). MLflow. MLflow|An Open Platform to Manage the Machine Learning
    Lifecycle. Available online: https://mlflow.org/ (accessed on 21 December 2023).
    DVC. Data Version Control—DVC 2.0.17 Documentation. Available online: https://dvc.org/
    (accessed on 21 December 2023). Git Large File Storage. Git Large File Storage.
    Available online: https://git-lfs.com/ (accessed on 21 December 2023). Wright,
    B. ‘Happily ever After?’ Readiness for Change amongst Managers in Regard to the
    Adoption of AI within an International Bank. Ph.D. Dissertation, Edinburgh Napier
    University, Edinburgh, UK, 2022. [Google Scholar] Rawashdeh, A. The consequences
    of artificial intelligence: An investigation into the impact of AI on job displacement
    in accounting. J. Sci. Technol. Policy Manag. 2023. [Google Scholar] [CrossRef]
    Fountaine, T.; McCarthy, B.; Saleh, T. Building the AI-powered organization. Harv.
    Bus. Rev. 2019, 97, 62–73. [Google Scholar] Watson, G.J.; Desouza, K.C.; Ribiere,
    V.M.; Lindič, J. Will AI ever sit at the C-suite table? The future of senior leadership.
    Bus. Horiz. 2021, 64, 465–474. [Google Scholar] [CrossRef] Seenivasan, D. ETL
    (Extract, Transform, Load) Best Practices. Int. J. Comput. Trends Technol. 2023,
    71, 40–44. [Google Scholar] [CrossRef] Apache Kafka. Available online: https://kafka.apache.org/
    (accessed on 21 December 2023). Talend. Data Integration, Big Data, and Cloud
    Integration Platform|Talend. Available online: https://www.talend.com/ (accessed
    on 21 December 2023). Informatica. Informatica: Data Integration Leader for Big
    Data & Cloud Analytics. Available online: https://www.informatica.com/ (accessed
    on 21 December 2023). Milo, T.; Somech, A. Automating exploratory data analysis
    via machine learning: An overview. In Proceedings of the 2020 ACM SIGMOD International
    Conference on Management of Data, Portland, OR, USA, 14–19 June 2020; pp. 2617–2622.
    [Google Scholar] Singh, C.; Gaba, N.S.; Kaur, M.; Kaur, B. Comparison of different
    CI/CD tools integrated with cloud platform. In Proceedings of the 2019 9th International
    Conference on Cloud Computing, Data Science & Engineering (Confluence), Noida,
    India, 10–11 January 2019; IEEE: New York, NY, USA, 2019; pp. 7–12. [Google Scholar]
    Disclaimer/Publisher’s Note: The statements, opinions and data contained in all
    publications are solely those of the individual author(s) and contributor(s) and
    not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility
    for any injury to people or property resulting from any ideas, methods, instructions
    or products referred to in the content.  © 2024 by the authors. Licensee MDPI,
    Basel, Switzerland. This article is an open access article distributed under the
    terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Aldoseri, A.; Al-Khalifa, K.N.; Hamouda, A.M.
    Methodological Approach to Assessing the Current State of Organizations for AI-Based
    Digital Transformation. Appl. Syst. Innov. 2024, 7, 14. https://doi.org/10.3390/asi7010014
    AMA Style Aldoseri A, Al-Khalifa KN, Hamouda AM. Methodological Approach to Assessing
    the Current State of Organizations for AI-Based Digital Transformation. Applied
    System Innovation. 2024; 7(1):14. https://doi.org/10.3390/asi7010014 Chicago/Turabian
    Style Aldoseri, Abdulaziz, Khalifa N. Al-Khalifa, and Abdel Magid Hamouda. 2024.
    \"Methodological Approach to Assessing the Current State of Organizations for
    AI-Based Digital Transformation\" Applied System Innovation 7, no. 1: 14. https://doi.org/10.3390/asi7010014
    Article Metrics Citations No citations were found for this article, but you may
    check on Google Scholar Article Access Statistics Article access statistics Article
    Views 8. Feb 13. Feb 18. Feb 23. Feb 28. Feb 4. Mar 9. Mar 14. Mar 19. Mar 24.
    Mar 29. Mar 3. Apr 0 500 1000 1500 2000 2500 For more information on the journal
    statistics, click here. Multiple requests from the same IP address are counted
    as one view.   Appl. Syst. Innov., EISSN 2571-5577, Published by MDPI RSS Content
    Alert Further Information Article Processing Charges Pay an Invoice Open Access
    Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors
    For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives
    Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings
    Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release
    notifications and newsletters from MDPI journals Select options Subscribe © 1996-2024
    MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions
    Privacy Policy"'
  inline_citation: '>'
  journal: Applied System Innovation
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Methodological Approach to Assessing the Current State of Organizations for
    AI-Based Digital Transformation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Si Salem T.
  - Castellano G.
  - Neglia G.
  - Pianese F.
  - Araldo A.
  citation_count: '0'
  description: 'An increasing number of applications rely on complex inference tasks
    that are based on machine learning (ML). Currently, there are two options to run
    such tasks: either they are served directly by the end device (e.g., smartphones,
    IoT equipment, smart vehicles), or offloaded to a remote cloud. Both options may
    be unsatisfactory for many applications: local models may have inadequate accuracy,
    while the cloud may fail to meet delay constraints. In this paper, we present
    the novel idea of inference delivery networks (IDNs), networks of computing nodes
    that coordinate to satisfy ML inference requests achieving the best trade-off
    between latency and accuracy. IDNs bridge the dichotomy between device and cloud
    execution by integrating inference delivery at the various tiers of the infrastructure
    continuum (access, edge, regional data center, cloud). We propose a distributed
    dynamic policy for ML model allocation in an IDN by which each node dynamically
    updates its local set of inference models based on requests observed during the
    recent past plus limited information exchange with its neighboring nodes. Our
    policy offers strong performance guarantees in an adversarial setting and shows
    improvements over greedy heuristics with similar complexity in realistic scenarios.'
  doi: 10.1109/TNET.2023.3305922
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE/ACM Transactions on Netw...
    >Volume: 32 Issue: 1 Toward Inference Delivery Networks: Distributing Machine
    Learning With Optimality Guarantees Publisher: IEEE Cite This PDF Tareq Si Salem;
    Gabriele Castellano; Giovanni Neglia; Fabio Pianese; Andrea Araldo All Authors
    247 Full Text Views Abstract Document Sections I. Introduction II. Related Work
    III. Inference System Design IV. INFIDA Algorithm V. Theoretical Guarantees Show
    Full Outline Authors Figures References Keywords Metrics Media Footnotes Abstract:
    An increasing number of applications rely on complex inference tasks that are
    based on machine learning (ML). Currently, there are two options to run such tasks:
    either they are served directly by the end device (e.g., smartphones, IoT equipment,
    smart vehicles), or offloaded to a remote cloud. Both options may be unsatisfactory
    for many applications: local models may have inadequate accuracy, while the cloud
    may fail to meet delay constraints. In this paper, we present the novel idea of
    inference delivery networks (IDNs), networks of computing nodes that coordinate
    to satisfy ML inference requests achieving the best trade-off between latency
    and accuracy. IDNs bridge the dichotomy between device and cloud execution by
    integrating inference delivery at the various tiers of the infrastructure continuum
    (access, edge, regional data center, cloud). We propose a distributed dynamic
    policy for ML model allocation in an IDN by which each node dynamically updates
    its local set of inference models based on requests observed during the recent
    past plus limited information exchange with its neighboring nodes. Our policy
    offers strong performance guarantees in an adversarial setting and shows improvements
    over greedy heuristics with similar complexity in realistic scenarios. Published
    in: IEEE/ACM Transactions on Networking ( Volume: 32, Issue: 1, February 2024)
    Page(s): 859 - 873 Date of Publication: 31 August 2023 ISSN Information: DOI:
    10.1109/TNET.2023.3305922 Publisher: IEEE Funding Agency: SECTION I. Introduction
    Machine learning (ML) models are often trained to perform inference, that is to
    elaborate predictions based on input data. ML model training is a computationally
    and I/O intensive operation and its streamlining is the object of much research
    effort. Although inference does not involve complex iterative algorithms and is
    therefore generally assumed to be easy, it also presents fundamental challenges
    that are likely to become dominant as ML adoption increases [1]. In a future where
    AI systems are ubiquitously deployed and need to make timely and safe decisions
    in unpredictable environments, inference requests will have to be served in real-time
    and the aggregate rate of predictions needed to support a pervasive ecosystem
    of sensing devices will become overwhelming. Today, two deployment options for
    ML models are common: inferences can be served by the end devices (smartphones,
    IoT equipment, smart vehicles, etc.), where only simple models can run, or by
    a remote cloud infrastructure, where powerful “machine learning as a service”
    (MLaaS) solutions rely on sophisticated models and provide inferences at extremely
    high throughput. However, there exist applications for which both options may
    be unsuitable: local models may have inadequate accuracy, while the cloud may
    fail to meet delay constraints. As an example, popular applications such as recommendation
    systems, voice assistants, and ad-targeting, need to serve predictions from ML
    models in less than 200 ms. Future wireless services, such as connected and autonomous
    cars, industrial robotics, mobile gaming, augmented/virtual reality, have even
    stricter latency requirements, often below 10 ms and in the order of 1 ms for
    what is known as the tactile Internet [2]. In enabling such strict latency requirements,
    the advent of Edge Computing plays a key role, as it deployes computational resources
    at the edge of the network (base stations, access points, ad-hoc servers). However,
    edge resources have limited capacity in comparison to the cloud and need to be
    wisely used. Therefore, integrating ML inference in the continuum between end
    devices and the cloud—passing through edge servers and regional micro data-centers—will
    require complex resource orchestration. We believe that, to allocate resources
    properly, it will be crucial to study the trade-offs between accuracy, latency
    and resource-utilization, adapted to the requirements of the specific application.
    In fact, inference accuracy and, in general, resource efficiency increase toward
    the cloud, but so does communication latency. In this paper, we present the novel
    idea of inference delivery networks (IDN): networks of computing nodes that coordinate
    to satisfy inference requests achieving the best trade-off. An IDN may be deployed
    directly by the ML application provider, or by new IDN operators that offer their
    service to different ML applications, similarly to what happens for content delivery
    networks. The same inference task can be served by a set of heterogeneous models
    featuring diverse performance and resource requirements (e.g., different model
    architectures [3], multiple downsized versions of the same pre-trained model [4],
    different configurations and execution setups). Therefore, we study the novel
    problem of how to deploy the available ML models on the available IDN nodes, where
    a deployment strategy consists in two coupled decisions: (i) where to place models
    for serving a certain task and (ii) how to select their size/complexity among
    the available alternatives. In this paper, we first define a specific optimization
    problem for ML model allocation in IDNs. We characterize the complexity of such
    problem and then introduce INFIDA (INFerence Intelligent Distributed Allocation),
    a distributed dynamic allocation policy. Following this policy, each IDN node
    periodically updates its local allocation of inference models on the basis of
    the requests observed during the recent past and limited information exchange
    with its neighbors. The policy offers strong performance guarantees in an adversarial
    setting [5], that is a worst case scenario where the environment evolves in the
    most unfavorable way. Numerical experiments in realistic settings show that our
    policy outperforms heuristics with similar complexity. Our contributions are as
    follows: We present the novel idea of inference delivery networks (IDNs). We frame
    the allocation of ML models in IDNs as an (NP-hard) optimization problem that
    captures the trade-off between latency and accuracy, and study how this problem
    diverges from settings considered in previous works (Sec. III). We propose INFIDA,
    a distributed and dynamic allocation algorithm for IDNs (Sec. IV), and we show
    it provides strong guarantees in the adversarial setting, providing novel theoretical
    results in approximating budget-additive (submodular) set functions (Sec. V).
    We evaluate INFIDA in a realistic simulation scenario and compare its performance
    both with an offline greedy heuristic and with its online variant under different
    topologies and trade-off settings (Sec. VI). SECTION II. Related Work The problem
    of machine learning is often reduced to the training task, i.e., producing statistical
    models that can map input data to certain predictions. A considerable amount of
    existing works addresses the problem of model training: production systems such
    as Hadoop [6] and Spark [7] provide scalable platforms for analyzing large amount
    of data on centralized systems, and even the problem of distributing the training
    task over the Internet has been largely addressed recently by many works on federated
    learning [8], [9], [10], [11], [12], [13]. However, there is surprisingly less
    research on how to manage the deployment of ML models once they have been trained
    (inference provisioning). Most of the existing solutions on inference provisioning
    (e.g., Tensorflow Serving [14], Azure ML [15], and Cloud ML [16]) address the
    scenario where inference queries are served by a data center. Recent works [17],
    [18], [19], [20] propose improvements on performance and usability of such cloud
    inference systems. Clipper [17] provides a generalization of TensorFlow Serving
    [14] to enable the usage of different ML frameworks, such as Apache Spark MLLib
    [21], Scikit-Learn [22], and Caffe [23]. The auhtors of [18] propose a reinforcement
    learning scheduler to improve the system throughput. INFaaS [19] provides a real-time
    scheduling of incoming queries on available model variants, and scales deployed
    models based on load thresholds. Last, InferLine [20] extends Clipper to minimize
    the end-to-end latency of a processing pipeline, both periodically adjusting the
    models allocation and constantly monitoring and handling unexpected query spikes;
    the solution can be applied to any inference serving system that features a centralized
    queue of queries. All these solutions address the problem of inference provisioning
    in the scenario where the requests are served within a data center and are not
    suitable for a geographically distributed infrastructure where resources are grouped
    in small clusters and network latency is crucial (e.g., Edge Computing). For instance,
    none of the previous works consider the network delay between different compute
    nodes, it being negligible in a data center. For what concerns inference provisioning
    in constrained environments, fewer works exist. Some solutions attempt to adapt
    inference to the capabilities of mobile hardware platforms through the principle
    of model splitting, a technique that distributes a ML model by partitioning its
    execution across multiple discrete computing units. Model splitting was applied
    to accommodate the hardware constraints in multi-processor mobile devices [24],
    to share a workload among mobile devices attached at the same network edge [25],
    and to partially offload inferences to a remote cloud infrastructure [26], possibly
    coupled with early exit strategies [27] and conditional hierarchical distributed
    deployment [28]. Model splitting is orthogonal to our concerns and could be accounted
    for in an enhanced IDN scheme. There has been some work on ML model placement
    at the edge in the framework of what is called “AI on Edge” [29], but it considers
    a single intermediate tier between the edge device and the cloud, while we study
    general networks with nodes in the entire cloud-to-the-edge continuum. Our dynamic
    placement INFIDA algorithm could be applied also in this more particular setting,
    for example in the MODI platform [30]. The work closest to ours is [31], which
    proposes an online learning policy, with the premise of load balancing over a
    pool of edge devices while maximizing the overall accuracy. INFIDA has more general
    applicability, as we do not make any assumption on the network topology and also
    employ a more flexible cost model that takes into account network delay. Another
    related work in this framework is VideoEdge [32], which studies how to split the
    analytics pipeline across different computational clusters to maximize the average
    inference accuracy. Beside the focus on the specific video application, the paper
    does not propose any dynamic allocation placement algorithm. Even if the problem
    of inference provisioning is currently overlooked in the context of distributed
    systems, there exists a vast literature on the problem of content placement [33]
    where objects can be stored (cached) into different nodes in order to reduce the
    operational cost of content delivery. Content placement has been extended to the
    case of service caching (or placement), where an entire service can be offloaded
    onto nodes co-located with base-stations or mobile-micro clouds, engaging not
    only storage but also computational resources and energy [34], [35]. The similarities
    between this problem and inference provisioning inspired us in the design of Inference
    Delivery Networks. However, the two problems feature crucial differences. First,
    in a content delivery network a request for a given item may only be served by
    a server storing that specific item. Whereas, in IDNs, several models can provide
    an answer but the accuracy of the answer can be different [36]. Second, a key
    property of content placement is that the service cost always increases together
    with the path length, i.e., the distance between the request source and the node
    serving the request. This is not the case for inference delivery networks, as
    1) upstream models may be more accurate and 2) the same model at may feature different
    processing delays based on the serving node properties. This leads to a more complex
    cost function, as the first node receiving the request may not be the optimal
    one to serve it. Note that this difference was crucial in the design of our algorithm
    (see Fig. 3). Finally, multiple requests can simultaneously be processed by a
    given model, leading to additional considerations about requests load and serving
    capacities. Fig. 1. System overview: a network of compute nodes serves inference
    requests along predefined routing paths. A repository node at the end of each
    path ensures that requests are satisfied even when there are no suitable models
    on intermediate nodes. Show All Fig. 2. Example of pre-trained model catalog for
    the image classification task. Data from [61]. Show All Fig. 3. Necessity of partial
    synchronization in IDN among close-by computing nodes under the cost model in
    Eq. (6). Show All A similar trade-off between resource usage and perceived quality
    typically emerges in the context of video caching [37], [38], [39], [40], [41],
    [42], [43], where the same video can be cached into multiple network nodes at
    different qualities (or resolutions): the operator optimizes the user experience
    by jointly deciding the placement of videos and their quality. These works either
    maximize the video quality perceived by the user [37], [40], [41], minimize the
    download time [39], [43] and the backhaul traffic [38], or minimize a combined
    cost [42]. Although some of the models in these papers may be adapted to inference
    provisioning in IDNs, these works in general study static optimization problems
    under a known request process and consider simple network topologies: a single
    cache [38], [39], a pool of parallel caches [37], [43], bipartite networks [41],
    [42]. The only exception is [40], which considers an arbitrary topology and provides
    some online heuristics, but ignores the service latency, which is of paramount
    importance when placing interactive ML models (e.g., for applications like augmented
    reality or autonomous driving). Instead, we propose a dynamic policy that jointly
    optimizes inference quality and latency and provides strong performance guarantees
    without requiring any knowledge about the request process thanks to our adversarial
    setting (Sec. V). Adversarial analysis is typically studied through the lens of
    online convex optimization (OCO) [5]. OCO models can be tackled with well-understood
    learning algorithms [5], [44], [45]. However, the problem of optimizing the allocation
    of ML models in IDNs diverges from the template of OCO. In particular, the decision
    set and the cost functions are non-convex because the allocation decisions are
    not continuous; moreover, as we show in Appendix B, computing the optimal allocation
    is NP-hard contrarily to the OCO setting. In our work, we generalize findings
    from [46], [47], and [48], and provide novel results to approximate budget-additive
    (submodular) set functions [49], which are of independent interest beyond this
    work (e.g., for online advertising [50], [51], market equilibrium [52], [53]).
    Finally, ML model allocation in an IDN can also be considered as a particular
    instance of similarity caching [54], a general model where items and requests
    can be thought as embedded in a metric space: edge nodes can store a set of items,
    and the distance between a request and an item determines the quality of the matching
    between the two. Similarity caching was applied to a number of applications including
    content-based image retrieval [55], contextual advertising [56], object recognition
    [57], and recommender systems [58]. To the best of our knowledge, the literature
    on similarity caching has restricted itself to (i) a single cache (with the exception
    of [58], [59], [60]), and (ii) homogeneous items with identical resource requirements.
    A consequence is that in our setting similarity caching policies would only allocate
    models based on their accuracy, ignoring the trade-offs imposed by their resource
    requirements. Moreover, the literature on similarity caching ignores system throughput
    constraints, while we explicitly take into account that each model can only serve
    a bounded number of requests per second, according to its capacity. SECTION III.
    Inference System Design We consider a network of compute nodes, each capable of
    hosting some pre-trained ML models depending on its capabilities. Such ML models
    are used to serve inference requests for different classification or regression
    tasks.1 As shown in Fig. 1, requests are generated by end devices and routed over
    given serving paths (e.g., from edge to cloud nodes). The goal of the system is
    to optimize the allocation of ML models across the network so that the aggregate
    serving cost is minimized. Our system model is detailed below, and the notation
    used across the paper is summarized in Table I. TABLE I Notation Summary A. Compute
    Nodes and Models We represent the inference delivery network (IDN) as a weighted
    graph G(V,E) , where V is the set of compute nodes, and E represents their interconnections.
    Each node v∈V is capable of serving inference tasks that are requested from anywhere
    in the network (e.g., from end-users, vehicles, IoT devices). We denote by N={1,2,…,|N|}
    the set of tasks the system can serve (e.g., object detection, speech recognition,
    classification), and assume that each task i∈N can be served with different quality
    levels (e.g., different accuracy as illustrated in Fig. 2) and different resources’
    requirements by a set of suitable models M i . Each task is served by a separate
    set of models, i.e., M i ∩ M i ′ =∅,∀i, i ′ ∈N,i≠ i ′ . Catalog M i may encompass,
    for instance, independently trained models or shrunk versions of a high quality
    model generated through distillation [62], [63]. We denote by M= ∪ i∈N M i ={1,2,…,|M|}
    the catalog of all the available models. Finally, every model of the catalog may
    provide a different throughput (i.e., number of requests it can serve in a given
    time period), and therefore, support a different load (we formalize this in Sec.
    III-D). For each compute node v∈V , we denote by x v m ∈{0,1},for m∈M, (1) View
    Source the decision variable that indicates if model m∈M is deployed on node v
    .2 Therefore, x x v =[ x v m ] m∈M is the allocation vector on node v , and x
    x=[ x x v ] v∈V denotes the global allocation decision. We assume that the allocation
    of ML models at each node is constrained by a single resource dimension, potentially
    different at each node. A node could be, for instance, severely limited by the
    amount of available GPU memory, another by the maximum throughput in terms of
    instructions per second. The limiting resource determines the allocation budget
    b v ∈ R + at node v∈V . We also denote with s v m ∈ R + the size of model m∈M
    , i.e., the consumed amount of the limiting resource at node v .3 Therefore, budget
    constraints are expressed as ∑ m∈M x v m s v m ≤ b v ,∀v∈V. (2) View Source To
    every task i∈N , we associate a fixed set of repository nodes that always run
    one model capable of serving all the requests for task i (e.g., high-performance
    models deployed in large data centers). We call these models repository models
    and they are statically allocated. Repository models ensure requests are satisfied
    even when the rest of the network is not hosting any additional model. We discern
    repository models through constants ω v m ∈{0,1} , each indicating if model m
    is permanently deployed on node v . We assume that values ω v m are given as input.
    We call the vector ω ω=[ ω v m ] (v,m)∈V×M the minimal allocation. Note that the
    presence of repositories introduce the following constraints to the allocation
    vector: x v m ≥ ω v m ,∀v∈V,∀m∈M. (3) View Source The set of possible allocations
    at node v∈V is determined by the integrality constraints (1), budget constraints
    (2), and repository constraints (3), i.e., X v ≜{ x x v ∈{0,1 } M : x x v satisfiesEqs.(1)−(3)}.
    (4) View Source The set of possible global allocations is given as X≜ × v∈V X
    v . B. Inference Requests We assume that every node has a predefined routing path
    towards a suitable repository node for each task i∈N . Therefore, for a given
    request for task i , the routing path is a set of network nodes towards a repository
    node able to serve task i . Since we assume repository nodes are predefined, the
    routing path does not depend on the placement decisions (i.e., on the variables
    x v m ). Hence, a request always follows its predetermined path, but intermediate
    nodes that host suitable models can serve it directly instead of forwarding it
    all the way to the repository node. In such cases, the request would traverse
    just a portion of the path. A routing path p p of length | p p|=J is a sequence
    { p 1 , p 2 ,…, p J } of nodes p j ∈V such that edge ( p j , p j+1 )∈E for every
    j∈{1,2,…,J−1} . As in [46], we assume that paths are simple, i.e., they do not
    contain repeated nodes. A request is therefore characterized by the pair (i, p
    p) , where i is the task requested and p p is the routing path to be traversed.
    We call the pair (i, p p) the request type. We denote by R the set of all possible
    request types, and by R i all possible request types for tasks i . When a request
    for task i is propagated from node p 1 toward the associated repository node ν(
    p p)≜ p J , any intermediate node along the path that hosts a suitable model m∈
    M i can serve it. The actual serving strategy is described in Sec. III-E. C. Cost
    Model When serving a request of type ρ=(i, p p)∈R on node p j using model m ,
    the system experiences an inference cost that depends on the quality of the model
    (i.e., on inference inaccuracy) and the inference time.4 Additionally, the system
    experiences a network cost, due to using the path between p 1 and p j . Similarly
    to previous work [64], we can write the total cost of serving a request as C p
    j p p,m =f(( p 1 ,…, p j ),m). (5) View Source While our theoretical results hold
    under this very general cost model, in what follows—for the sake of concreteness—we
    refer to the following simpler model: C p j p p,m = ∑ j ′ =1 j−1 w p j ′ , p j
    ′ +1 + d p j m +α(1− a m ), (6) View Source where a m and d p j m are respectively
    the prediction accuracy (in a scale from 0 to 1) and the average inference delay
    of model m on node p j . Indeed, the same model may provide different inference
    delays, depending on the hardware capabilities of the node on which it is deployed,
    e.g., the type of GPU or TPU [65]. Parameter w v, v ′ ∈ R + is the (round-trip)
    latency of edge (v, v ′ )∈E . Parameter α weights the importance of accuracy w.r.t.
    the overall latency and can be set depending on the application. Note that seeking
    cost minimization along a serving path usually leads to a trade-off: while the
    network cost always increases with j , in a typical network the service cost d
    p j m +α(1− a m ) tends to decrease, as farther nodes (e.g., data centers) are
    better equipped and can run more accurate models (Fig. 2). We remark that models’
    sizes determine which allocations are feasible, but do not affect directly the
    service costs. As a consequence, even if we assume different limiting resources
    on different nodes (e.g., GPU, memory), we do not need to convert amounts of different
    resources to a common unit (e.g., a monetary cost). D. Request Load and Serving
    Capacity Let us assume that time is split in slots of equal duration. We consider
    a time horizon equal to T slots. At the beginning of a slot t , the system receives
    a batch of requests r r t =[ r t ρ ] ρ∈R , where r t ρ ∈N∪{0} denotes the number
    of requests of type ρ∈R . Model m∈M has maximum capacity L v m ∈N when deployed
    at node v∈V , i.e., it can serve at most L v m requests during one time slot t∈[T]
    , in absence of other requests for other models. We do not make specific assumptions
    on the time required to serve a request. We denote by l t,v ρ,m ∈N∪{0} the potential
    available capacity, defined as the maximum number of type- ρ requests node v can
    serve at time t through model m , under the current request load r r t and allocation
    vector x x v t . Formally, let load t,v m (ρ) denote the number of type- ρ requests
    served by model m at node v during the t -slot, then l t,v ρ,m ≜min ⎧ ⎩ ⎨ L v
    m − ∑ ρ ′ ∈R∖{ρ} load t,v m ( ρ ′ ), r t ρ ⎫ ⎭ ⎬ . (7) View Source The potential
    available capacity depends on the request arrival order and the scheduling discipline
    at node v . For instance, suppose that in time slot t , requests of two types
    ρ=(i, p p) and ρ ′ =(i, p p ′ ) arrive at node v . The arrival order and the node
    scheduling discipline may determine that many requests of type ρ ′ be served,
    which would leave a small l t,v ρ,m available for requests of type ρ . Or the
    opposite may happen. It is useful to define the potential available capacity also
    for models that are not currently deployed at the node, as l t,v ρ,m ≜min{ L v
    m , r t ρ } . The effective available capacity is then equal to l t,v ρ,m x v
    m . Our analysis in Sec. V considers a “pessimistic” scenario where an adversary
    selects both requests and available capacities for all models but the repository
    ones. This approach relieves us from the need to model system detailed operations,
    while our proposed algorithm (Sec. IV) benefits from strong guarantees in the
    adversarial setting. In what follows, we can then consider that values l t,v ρ,m
    are exogeneously determined. The vector of potential available capacities at time
    t∈[T] is denoted by l l t =[ l t,v ρ,m ] (ρ,m,v)∈ ⋃ i∈N R i × M i ×V . (8) View
    Source As we mentioned in Sec. III-A, any request of type ρ=(i, p p)∈R can always
    be served by the associated repository model at node ν( p p) . This requirement
    can be expressed as follows: ∑ ρ∈ R i r t ρ ≤ ∑ m∈ M i ω ν( p p) m L ν( p p) m
    ,∀i∈N. (9) View Source Thus, at any time t∈[T] the adversary can select a request
    batch r r t and potential available capacity l l t from the set A≜{ ( r r, l l)∈(N∪{0}
    ) R ×(N∪{0} ) ⋃ i∈N R i × M i ×V : ∑ ρ∈ R i r t ρ ≤ ∑ m∈ M i ω ν( p p) m L ν(
    p p) m , l v ρ,m ≤min{ L v m , r ρ }, ∀i∈N,v∈V,m∈M,ρ∈R}. (10) View Source Note
    the constraint on potential available capacities is looser than the definition
    in (7) corresponding to a more powerful adversary. E. Serving Model Given request
    ρ=(i, p p)∈R , let K ρ =| p p|| M i | denote the maximum number of models that
    may encounter along its serving path p p . We order the corresponding costs {
    C p j p p,m ,∀m∈ M i ,∀ p j ∈ p p} in increasing order and we denote by κ ρ (v,m)
    the rank of model m∈ M i allocated at node v within the order defined above.5
    If v∉ p p we have κ ρ (v,m)=∞ . If κ ρ (v,m)=k , then model m at node v has the
    k -th smallest cost to serve request ρ . We denote the model service cost, its
    potential available capacity, and its effective capacity as γ k ρ , λ k ρ ( l
    l t ) , and z k ρ ( l l t , x x) , respectively: γ k ρ = C v p p,m , λ k ρ ( l
    l t )= l t,v ρ,m , z k ρ ( l l t , x x)= x v m l t,v ρ,m . (11) View Source We
    assume the IDN serves requests as follows. Each request is forwarded along its
    serving path and served when it encounters a model with the smallest serving cost
    among those that are not yet saturated, i.e., that may still serve requests. Since
    models do not necessarily provide increasing costs along the path, this serving
    strategy requires that a node that runs a model m∈ M i and receives a request
    for task i , knows whether there are better alternatives for serving task i upstream
    or not. In the first case, it will forward the request along the path, otherwise
    it will serve it locally. We argue that, in a real system, this partial knowledge
    can be achieved with a limited number of control messages. In fact, if node v=
    p h hosts the model with the k -th cost for request (i, p p) , it only needs information
    about those models that (i) are located upstream on the serving path (i.e., on
    nodes p l ∈ p p with l>h ), and (ii) provide a cost smaller than γ k ρ . Since
    the cost increases with the network latency (as illustrated in Fig. 3), the number
    of models satisfying these criteria is small in practice.6 A node needs to propagate
    downstream a control message with the information about the requests it can serve
    and the corresponding costs. Nodes forwarding the control message progressively
    remove the information about the tasks they can serve with a smaller cost, until
    the control message payload is empty and the message can be dropped. Every node
    v∈V generates this control message whenever the available capacity of any of the
    local models in v changes. According to the presented serving strategy, the requests
    load is split among the currently available models giving priority to those that
    provide the smallest serving costs up to their saturation. In particular, model
    m with the k -th smallest cost will serve some requests of type ρ only if the
    less costly models have not been able to satisfy all of such requests (i.e., if
    ∑ k−1 k ′ =1 z k ′ ρ ( l l t , x x)< r t ρ ). If this is the case, model m will
    serve with cost γ k ρ at most z k ρ ( l l t , x x) requests (its effective available
    capacity) out of the r t ρ − ∑ k−1 k ′ =1 z k ′ ρ ( l l t , x x) requests still
    to be satisfied. The aggregate cost incurred by the system at time slot t is then
    given by C( r r t , l l t , x x) = ∑ ρ∈R ∑ k=1 K ρ γ k ρ ⋅min{ r t ρ − ∑ k ′ =1
    k−1 z k ′ ρ ( l l t , x x), z k ρ ( l l t , x x)} ⋅ 1 { ∑ k−1 k ′ =1 z k ′ ρ (
    l l t , x x)< r t ρ } . (12) View Source Note that we introduce the min{⋅⋅} operator,
    since the number of requests served by the k -th best model cannot exceed its
    effective capacity z k ρ ( l l t , x x) . We add the indicator function 1 {⋅}
    to indicate that the k -th best model does not serve any requests, in case better
    models (ranked from 1 to k−1 ) are able to satisfy all of them. F. Allocation
    Gain and Static Optimal Allocations We are interested in model allocations x x
    that minimize the aggregate cost (12), or, equivalently, that maximize the allocation
    gain defined as G( r r t , l l t , x x)=C( r r t , l l t , ω ω)−C( r r t , l l
    t , x x). (13) View Source The first term C( r r t , l l t , ω ω) on the right
    hand side is the service cost when only repository models are present in the network.
    Since intermediate nodes can help serving the requests at a reduced cost, C( r
    r t , l l t , ω ω) is an upper bound on the aggregate serving cost, and the allocation
    gain captures the cost reduction achieved by model allocation x x . The static
    model allocation problem can then be formulated as finding the model allocation
    x x ∗ that maximizes the time-averaged allocation gain over the time horizon T
    , i.e., x x ∗ = argmax x x∈X ( G T ( x x)≜ 1 T ∑ t=1 T G( r r t , l l t , x x)).
    (14) View Source This is a submodular maximization problem under multiple knapsack
    constraints. In our context, this intuitively means that the problem is characterized
    by a diminishing return property: adding a model m to any node v gives us a marginal
    gain that depends on the current allocation: the more the models already deployed
    in the current allocation, the less the marginal gain we get by the new m . We
    prove submodularity in Lemma A.2 in Appendix B. In Appendix B Theorem B.1, we
    prove that this problem is NP-hard even under cardinality constraints (i.e., the
    models have equal size) and a two nodes scenario. We demonstrate the hardness
    of the problem by a reduction of the similarity caching problem [54], [66], which
    is NP-hard; a result that follows from a reduction of the dominating set problem.
    It is known that submodular maximization problems cannot be approximated with
    a ratio better than (1−1/e) even under simpler cardinality constraints [67]. Under
    the multi-knapsack constraint, it is possible to solve the offline problem achieving
    a (1−1/e−ϵ) -approximation through a recent algorithm proposed in [68]. Let us
    consider a model allocation x x . Within time slot t , the k smallest cost models
    along a path p p that are suitable for request type ρ=(i, p p) can serve up to
    Z k ρ ( r r t , l l t , x x) requests, where Z k ρ ( r r t , l l t , x x) is defined
    as Z k ρ ( r r t , l l t , x x)≜min{ r t ρ , ∑ k ′ =1 k z k ′ ρ ( l l t , x x)}.
    (15) View Source The min{⋅⋅} operator denotes that we can never serve more than
    the number of requests r t ρ issued by users. Observe that, being the minimal
    allocation ω ω an input parameter not dependent on our decisions, Z k ρ ( r r
    t , l l t , ω ω) is a constant. Additionally, since the models allocated in x
    x always include those allocated in ω ω , we have Z k ρ ( r r t , l l t , x x)≥
    Z k ρ ( r r t , l l t , ω ω) . Using (15), we provide the following alternative
    formulation of the allocation gain. Lemma 3.1:The allocation gain (13) has the
    following equivalent expression: G( r r t , l l t , x x)= ∑ ρ∈R ∑ k=1 K ρ −1 (
    γ k+1 ρ − γ k ρ )              cost saving ( Z k ρ ( r r t , l l
    t , x x)− Z k ρ ( r r t , l l t , ω ω)).                  
                   additional requests (16) View Source We prove this
    lemma in Appendix C. This result tells us that the gain of a certain allocation
    x x can be expressed as a sum of several components. In particular, for each request
    type ρ , the k -th smallest cost model along the path contributes to the gain
    with a component (i) proportional to its cost saving γ k+1 ρ − γ k ρ with respect
    to the ( k+1 )-th smallest cost model and (ii) proportional to the amount of additional
    requests that the k -th smallest cost models in allocation x x can serve with
    respect to the minimal allocation ω ω . SECTION IV. INFIDA Algorithm In this section,
    we propose INFIDA, an online algorithm that can operate in a distributed fashion
    without requiring global knowledge of the allocation state and requests arrival.
    In Sec. V, we show that INFIDA generates dynamically allocations experiencing
    average costs that converge to a (1−1/e−ϵ) -approximation of the optimum, which
    matches the best approximation ratio achievable in polynomial time even in this
    online setting. A. Algorithm Overview On every node v∈V , INFIDA updates the allocation
    x x v ∈ X v ⊂{0,1 } |M| , by operating on a correspondent fractional state y y
    v ∈ Y v ⊂[0,1 ] |M| , and the fractional allocations satisfy the budget constraint
    in Eq. (2). Note that, if ∥ s s v ∥ 1 < b v for a node v∈V , we can always consider
    fractional allocations that consume entirely the allowed budget; otherwise, all
    the allocations are set to 1 (node v can store the whole catalog of models). Formally,
    if ∥ s s v ∥ 1 ≥ b v then Y v ≜{ y y v ∈[0,1 ] M : ∑ m∈M y v m s v m = b v ,∀v∈V};
    (17) View Source otherwise, for the corner case ∥ s s v ∥ 1 < b v , we have Y
    v ≜{[1 ] M } . Each variable y v m can be interpreted as the probability of hosting
    model m on node v , i.e., y v m =P[ x v m =1]=E[ x v m ] . We define G( r r t
    , l l t , y y) as in (13), replacing x x with y y . Note that G( r r t , l l t
    , y y) is a concave function of variable y y∈Y= × v∈V Y v (see Lemma F.1 in Appendix
    F). Within a time slot t , node v collects measurements from messages that have
    been routed through it (Sec. IV-B). At the end of every time slot, the node (i)
    computes its new fractional state y y v , and (ii) updates its local allocation
    x x v via randomized rounding (Sec. IV-C). INFIDA is summarized in Algorithm 1
    and detailed below. Algorithm 1 INFIDA Distributed Allocation on Node v 1: procedure
    INFIDA( y y v 1 = argmin y y v ∈ Y v ∩ D v Φ v ( y y v ) , x x v 1 =DEPROUND(
    y y v 1 ) , η∈ R + ) 2: for t=1,2,…,T do 3: Compute g g v t ∈ ∂ y y v G( r r t
    , l l t , y y t ) through (18). 4: y y ^ v t ←∇ Φ v ( y y v t )▹ Map state to
    the dual space 5: h h ^ v t+1 ← y y ^ v t +η g g v t ▹ Take gradient step in the
    dual space 6: h h v t+1 ← (∇ Φ v ) −1 ( h h ^ v t+1 )▹ Map dual state back to
    the primal space 7: y y v t+1 ← P Φ v Y v ∩ D v ( h h v t+1 )▹ Project new state
    onto the feasible region using Algorithm 2 8: x x v t+1 ←DEPROUND( y y v t+1 )▹
    Sample a discrete allocation 1) State Computation: The fractional state y y v
    is updated through an iterative procedure aiming to maximize G( r r t , l l t
    , y y) . This could be the standard gradient ascent method, which updates the
    fractional state at each node as y y v t+1 = y y v t +η g g v t , where η t ∈
    R + is the step size and g g v t is a subgradient of G( r r t , l l t , y y) with
    respect to y y v . In our work, we use a generalized version of the gradient method
    called Online Mirror Ascent (OMA) [69, Ch. 4]. OMA uses a function Φ v : D v →
    R + (mirror map) to map y y to a dual space before applying the gradient ascent
    method; then the obtained state is mapped back to the primal space (lines 3–5
    of Algorithm 1). OMA reduces to the classic gradient ascent method if Φ v is the
    squared Euclidean norm (in this case the primal space coincides with the dual
    one). Instead, we use the weighted negative entropy map Φ v ( y y v )= ∑ m∈M s
    v m y v m log( y v m ) , which is known to achieve better convergence rate in
    high dimensional spaces when each subgradient component is bounded.7 To compute
    a feasible fractional state y y v , we then perform a projection to the set Y
    v on node v (line 6 of Algorithm 1). We adapt the projection algorithm from [70]
    to obtain a negative entropy projection P Φ v Y v ∩ D v (⋅) . Our adaptation is
    described in Appendix D. 2) Allocation Update: Once the fractional state y y v
    has been updated, the final step of INFIDA is to determine a new random discrete
    allocation x x v and update the local models accordingly. The sampled allocation
    x x v should (i) comply with the budget constraint (2) on node v and (ii) be consistent
    with the fractional state, i.e., E[ x v m ]= y v m ∀m∈M . To this purpose, we
    use the DepRound [71] subroutine (line 7 of Algorithm 1). In the remainder of
    this section we detail how each node computes its contribution to the global subgradient,
    and the rounding strategy used to determine the discrete allocation. B. Subgradient
    Computation At the end of every time slot t , a subgradient g g t of the gain
    function in Eq. (16) at point y y t ∈Y is computed in a distributed fashion: each
    node v evaluates the (v,m) -th component of the subgradient for any m∈M as follows
    (see Appendix E): g v t,m = ∑ ρ∈R l t,v ρ,m ⋅( γ K ∗ ρ ( y y t ) ρ − C v p p,m
    )⋅ 1 { κ ρ (v,m)< K ∗ ρ ( y y t )} , (18) View Source where K ∗ ρ ( y y t ) is
    the order of the worst needed model, i.e., the model with the highest cost that
    is needed to serve all the r t ρ requests in the batch given the fractional state
    y y t . Formally, K ∗ ρ ( y y t )≜min{k∈[ K ρ −1]: ∑ k k ′ =1 z k ′ ρ ( l l t
    , y y t )≥ r t ρ } . For the sake of clarity assume that the mirror map is Euclidean,
    and then the dual and primal spaces collapse and y y ^ t = y y t . At each iteration,
    each component y t,v m of the fractional allocation vector is updated by adding
    a mass equal to the product of η>0 and the corresponding component of the subgradient
    (Algorithm 1, line 5). Observe that g v m,t is the sum of different contributions,
    one per each request type ρ . Thanks to the indicator function, only the terms
    of the request types that are served by model m on v contribute to g v m,t . This
    contribution is proportional to the potential available capacity of model m on
    node v and to the relative gain ( γ K ∗ ρ ( y y t ) ρ − C v p p,m ) , i.e., the
    cost reduction achieved when serving request type ρ with model m on v , rather
    than with the worst needed model. Then, gradient updates add more mass to the
    models that can contribute more to increase the gain. On the contrary, the projection
    step tends to remove the added mass from all components to satisfy the constraints.
    The overall effect is that fractional allocations of more (resp. less) useful
    models tend to increase (resp. decrease). The subgradient in Eq. (18) can be computed
    at each node using only information from the control messages collected at the
    end of the time slot t . The steps needed to compute the subgradient are as follows.
    At the end of the time slot, each node generates a control message for every received
    request type ρ=(i, p p) that is propagated along p p . The control message contains
    the quantity r t ρ ≥1 (the multiplicity of the request), and a cumulative counter
    Z initialized to zero. As the control message travels upstream, intermediate nodes
    add to Z the local values z k ρ ( l l t , y y t ) (fractional effective capacity
    in Eq. (11)). These values are added following increasing values of cost. This
    message is propagated until Z≥ r t ρ , that is until the message reaches the K
    ∗ ρ ( y y t ) -th model. Once the K ∗ ρ ( y y t ) -th model is detected, a control
    message is sent down in the opposite direction, containing the cost γ K ∗ ρ (
    y y t ) ρ of the last checked model. Every node v in the reverse direction reads
    the cost value from the control message and, for each model m∈ M i , computes
    the quantity h v m = l t,v ρ,m ⋅( γ K ∗ ρ ( y y t ) ρ − C v p p,m ). (19) View
    Source Node v can then compute g v t,m in Eq. (18) as follows g v t,m = ∑ m∈ M
    i h v m . View Source Note that the cost in Eq. (6) does not necessarily increase
    along the path. Therefore, a traversed node is not able to update directly the
    variable Z when there exist upstream nodes with lower cost. In this case, the
    node simply appends the information ( z k ρ ( l l t , y y t ), γ k ρ ) to the
    message, and lets upstream nodes to apply any pending update in the correct order.
    In our work, we assume to operate on a reliable communication channel. Nonetheless,
    we note that INFIDA is robust to noise ξ ξ t affecting the sub-gradient, as long
    as such noise is not biased, i.e., E[ ξ ξ t ]= 0 0 for every timeslot t [5, Theorem
    3.4]. C. State Rounding Once the new fractional state y y t+1 is computed, each
    node v independently draws a random set of models to store locally in such a way
    that E[ x x v t+1 ]= y y v t+1 . This sampling guarantees that the final allocation
    x x v t+1 satisfies constraint (2) in expectation. A naive approach is to draw
    each variable x v,t+1 m independently, but it leads to a large variance of the
    total size of the models selected, potentially exceeding by far the allocation
    budget at node v . To construct a suitable allocation we adopt the DepRound procedure
    from [71]. The procedure modifies the fractional state y y v t+1 iteratively:
    at each iteration, DepRound operates on two fractional variables y v,t+1 m , y
    v,t+1 m ′ so that at least one of them becomes integral and the aggregate size
    of the corresponding models s v m y v,t+1 m + s v m ′ y v,t+1 m ′ does not change.
    This operation is iterated until all variables related to node v are rounded except
    (at most) one, which we call residual fractional variable. This is done in O(|M|)
    steps. Note that, to satisfy E[ x x v t+1 ]= y y v t+1 , the residual fractional
    variable, say it y v,t+1 m ¯ , needs to be rounded. At this point x v,t+1 m ¯
    can be randomly drawn. Now the final allocation can exceed the budget bound b
    v by at most s m ¯ . These (slight) occasional violations of the constraint may
    not be a problem, e.g., at an edge server running multiple applications, where
    resources may be partially redistributed across different applications; they may
    be explicitly accounted for in the service level agreements. If the budget bound
    cannot be exceeded even temporarily, the node is not able to store the model m
    ¯ , but it may still exploits the residual free resources to deploy the model
    that provides the best marginal gain among those that fit the available budget.
    In practice, we expect the corresponding gain decrease to be negligible. SECTION
    V. Theoretical Guarantees We provide the optimality guarantees of our INFIDA algorithm
    in terms of the ψ -regret [72]. In our scenario, the ψ -regret is defined as the
    gain loss in comparison to the best static allocation in hindsight, i.e., x x
    ∗ ∈ argmax x x∈X ∑ T t=1 G( r r t , l l t , x x) , discounted by a factor ψ∈(0,1]
    . Formally, ψ− Regret T,X ≜ sup { r r t , l l t } T t=1 ∈ A T {ψ ∑ t=1 T G( r
    r t , l l t , x x ∗ )−E[ ∑ t=1 T G( r r t , l l t , x x t )]}, (20) View Source
    where allocations x x t are computed using INFIDA and the expectation is over
    the randomized choices of DEP ROUND. Note that, by taking the supremum over all
    request sequences and potential available capacities, we measure regret in an
    adversarial setting, i.e., against an adversary that selects, for every t∈[T]
    , vectors r r t and l l t to jeopardize the performance of our algorithm. Obviously,
    we do not expect such an adversary would exist in reality, but the adversarial
    analysis provides bounds on the behavior of INFIDA in the worst case. The adversarial
    analysis is a modeling technique to characterize system performance under highly
    volatile external parameters (e.g., the sequence of requests r r t ) or difficult
    to model system interactions (e.g., the available capacities l l t ). This technique
    has been recently successfully used to model caching problems (e.g., in [70],
    [73]). Our main result is the following (the full proof is in Appendix G): Theorem
    5.1:INFIDA has a sublinear (1−1/e) -regret w.r.t. the time horizon T , i.e., there
    exists a constant A such that: (1−1/e)− Regret T,X ≤A T − − √ , (21) View Source
    where A∝R L max Δ C . R , L max , and Δ C are upper bounds, respectively, on the
    total number of request types at any time slot, on the model capacities, and on
    the largest serving cost difference between serving at a repository node and at
    any other node. Proof:(sketch) We first prove that the expected gain of the randomly
    sampled allocations x x t is a (1−1/e) -approximation of the fractional gain.
    Then, we use online learning results [69] to bound the regret of Online Mirror
    Ascent schemes operating on a convex decision space and against concave gain functions
    picked by an adversary. The two results are combined to obtain an upper bound
    on the (1−1/e) -regret. We fully characterize the regret constant A in Appendix
    G. Note that this result holds over the integral domain (see Appendix F, Lemmas
    F.7–F.11), thus generalizing the approximation techniques in [46], [47], and [48]
    and providing a novel result in approximating budget-additive (submodular) set
    functions [49]. We observe that the regret bound depends crucially on the maximum
    number of request types R , maximum model capacity L max and maximum serving cost
    difference Δ C . When considering the cost model in Eq. (6), we can consider for
    Δ C the sum of the total latency of the heaviest path, the parameter α , and the
    largest inference delay. This result is intuitive: when these values are bigger,
    the adversary has a larger room to select values that can harm the performance
    of the system. As a direct consequence of Theorem 5.1, the expected time averaged
    (1−1/e) -regret of INFIDA can get arbitrarily close to zero for large time horizon.
    Hence, INFIDA achieves a time averaged expected gain that is a (1−1/e−ϵ) -approximation
    of the optimal time averaged static gain, for arbitrarily small ϵ . Observe that
    INFIDA computes a different x x t at every time slot. Intuitively, this allows
    it to “run after” the exogenous variation of the adversarial input { r r t , l
    l t } T t=1 ∈ A T . An alternative goal that can be achieved by INFIDA is to find
    a static allocation y y ¯ . In order to do so, we need to (i) run INFIDA for T
    ~ time-slots, (ii) based on the { y y t } T ~ t=1 computed by INFIDA, calculate
    x x ¯ (the exact calculation is in Proposition 1.1), (iii) deploy in the IDN the
    allocation x x ¯ and keep it static, in order to avoid switches. Obviously, we
    would like the quality of x x ¯ to be close to the best x x ∗ , defined in (14).
    The following proposition shows that the gain achieved with our x x ¯ is boundedly
    close to the optimum. Moreover, since (14) is NP-hard, there cannot exist better
    bounds than the one we achieve, assuming P≠NP [67]. Proposition 1.1 (Offline Solution):Replace
    in INFIDA the allocation gain G( r r t , l l t , y y) by G T ( y y) (defined in
    (14)). After T ~ iterations, let y y ¯ be the average fractional allocation y
    y ¯ = 1 T ~ ∑ T ~ t ~ =1 y y t , and x x ¯ the random state sampled from y y ¯
    using DEP ROUND. ∀ϵ>0 , for T ¯ large enough, x x ¯ satisfies E[ G T ( x x ¯ )]≥(1−
    1 e −ϵ) G T ( x x ∗ ), (22) View Source where x x ∗ = argmax x x∈X G T ( x x)
    . The proof is given in Appendix. H. SECTION VI. Experimental Results We evaluate
    INFIDA by simulating a realistic scenario based on the typical structure of ISP
    networks. We compare our solution with a greedy heuristic and its online variant
    (described below), as the greedy heuristic is known to achieve good performance
    in practice for submodular optimization [72]. Topology We simulate a hierarchical
    topology similar to [74] that spans between edge and cloud, with different capacities
    at each tier. We consider 5 tiers: base stations (tier 4), central offices (tiers
    3, 2), ISP data center (tier 1), a remote cloud (tier 0). We assume a hierarchical
    geographic distribution similar to LTE. We take the Round-Trip Time (RTT) across
    the different tiers as follows: tier 4 to tier 3 takes 6 ms, tier 3 to tier 2
    takes 6 ms, tier 2 to tier 1 takes 15 ms, and tier 1 to tier 0 takes 40 ms. We
    execute our experiments at two different scales: Network Topology I counts 24
    base stations and 36 nodes in total, while Network Topology II is a simpler 5-node
    scenario with 2 base stations. Processing Units: We take GPU memory of the computing
    nodes as the limiting budget. The node at tier 0 can store the entire models catalog.
    We simulate the performance of two different processing units: the computing nodes
    at tiers 0 and 1 are equipped with high-end GPUs (Titan RTX), and the remaining
    tiers 2–4 have mid-tier GPUs (GeForce GTX 980). The budget of each computing tier
    is given as follows: a tier-1 node has 16GB GPUs, a tier-2 node has 12GB GPUs,
    a tier-3 node has 8GB GPUs, and a tier-4 node has 4GB GPUs. Catalog and Requests:
    We simulate performance based on state-of-the-art pre-trained models and their
    pruned versions [75], [76], profiled for each simulated processing unit, for a
    total of 10 models (Table II). We consider a task catalog with |N|=20 different
    object detection tasks. We allow 3 duplicates per model; this gives | M i |=30
    alternative models per task i∈N . Note how, as model complexity decreases, the
    number of frames a GPU can process per second increases, and consequently the
    average inference delay decreases. TABLE II Catalog for Variants of YOLOv4 [75]
    Profiled on Two Different Processing Units. Accuracy Is for the MS COCO Dataset.
    Values for the Pruned Variants Are Adapted From [76] The time slot duration is
    set to 1 minute and requests arrive at a constant rate of 7,500 requests per second
    (rps), unless otherwise said. Each request type is assigned randomly to two base
    stations in tier 4. The corresponding task is selected according to two different
    popularity profiles: (i) in the Fixed Popularity Profile (Fig. 4a), a request
    is for task i with constant probability p(i)= (i+1 ) −1.2 ∑ i ′ ∈N ( i ′ +1 )
    −1.2 (a Zipf distribution with exponent 1.2), while (ii) in the Sliding Popularity
    Profile (Fig. 4b), the l -th consecutive request is for task i with probability
    p ~ (i,l)=p((i+5⌊l/W⌋)mod20) , that is, the popularity of the tasks changes through
    a cyclic shift of 5 tasks every 1 hour for a request rate of 7,500 rps ( W=2.7×
    10 7 ). Fig. 4. Popularity profiles of inference tasks for request rate 7,500
    rps. Each dot represents a request for a given task at given time. Therefore,
    popular tasks correspond to denser lines. In (b), popularity changes at fixed
    time intervals through a cyclic shift. At any moment the requests are i.i.d. and
    sampled from a Zipf distribution with exponent 1.2. The figure shows a random
    down-sample of 5,000 requests to emphasize the density difference across the different
    tasks. Show All Static Greedy: We adapt the static greedy (SG) heuristic from
    the cost-benefit greedy in [72]. SG operates in hindsight seeking maximization
    of the time averaged allocation gain over the whole time horizon T , as in Eq.
    (14). Starting from an empty allocation, this policy progressively allocates the
    model that provides the highest marginal gain normalized by size, among those
    that meet the budget constraints. This process is repeated until either the intermediate
    allocation is capable of serving all requests or none of the remaining valid allocations
    introduces a positive marginal gain. Online Load-Aware Greedy Heuristic: As INFIDA
    is the first online policy for ML models’ allocation in IDNs, there is no clear
    baseline to compare it with. We then propose an online heuristic based on SG,
    which we call online load-aware greedy (OLAG). A node v uses counters ϕ v m,ρ
    to keep track of the number of times a request ρ∈R is forwarded upstream but could
    have been served locally at a lower cost compared to the repository, i.e., using
    a model m∈M with positive gain that we denote by q v m,ρ . For every model m ,
    an importance weight is computed as w v m = 1 s v m 1 |R| ∑ ρ∈R q v m,ρ min{ ϕ
    v m,ρ , L v m } , where s v m is the size of model m and min{ ϕ v m,ρ , L v m
    } is the number of requests that could have been improved by m . At the end of
    a time slot, the node selects the model m ∗ with the highest importance while
    respecting the resource budget constraint, then subtracts the quantity min{ ϕ
    v m,ρ , L v m } from ϕ v m ∗ ,ρ and from all the ϕ v m ′ ,ρ : q v m ′ ,ρ < q v
    m ∗ ,ρ , i.e., models that provide a gain lower than m ∗ . This procedure is repeated
    until the resource budget of the node is consumed. Offline INFIDA: Motivated by
    Proposition 1.1, we implemented also an offline version of INFIDA that we call
    INFIDA OFFLINE , which maximizes the time-averaged gain (14) over the whole time
    horizon T . The potential available capacities are determined at runtime from
    the current allocations and request batches (rather than by an adversary). Performance
    Metrics: The performance of a policy P with the associated sequence of allocation
    decisions { x x t } T t=1 is evaluated in terms of the time-averaged gain normalized
    to the number of requests per time slot (NTAG): NTAG(P)= ∑ t=1 T 1 T ∥ r r t ∥
    1 G( r r t , l l t , x x t ). (23) View Source Moreover, we evaluate the update
    cost of a policy P with the associated sequence of allocation decisions { x x
    t } T t=1 by quantifying the total size of fetched models over T time slots. The
    update cost is reflected by the Time-Averaged Model Updates (MU) metric defined
    as: MU(P)≜ 1 T ∑ t=2 T ∑ (v,m)∈V×M s v m max{0, x v t,m − x v t−1,m }. (24) View
    Source A. Trade-off Between Latency and Accuracy We first evaluate how INFIDA
    adapts to different trade-offs between end-to-end latency and inference accuracy
    by varying the trade-off parameter α .8 Figure 5 shows the fractional allocation
    decision at each tier of the network topology for different values of α (remember
    that the smaller α the more importance is given to the latency rather than to
    inaccuracy, see Eq. (6)). Models are ordered horizontally by increasing accuracy
    with 3 potential replicas for each model, and only the models able to serve the
    most popular request are shown. Note that the tier-0 node acts as a repository
    and its allocation is fixed; moreover, in Fig 5 the repository node picks the
    second most accurate model because it provides the smallest combined cost in Eq.
    (6). Fig. 5. Fractional allocation decisions y v m of INFIDA on the various tiers
    of Network Topology I under Fixed Popularity Profile. We only show the allocations
    corresponding to the models capable to serve the most popular request. The model
    IDs are sorted by increasing accuracy. Show All For α=3 (Fig. 5a), INFIDA allocates
    a considerable amount of small models (which provide low accuracy) near the edge
    (tiers 1–3 and model IDs 0–18), as they can serve a larger number of requests
    compared to higher quality models with low inference delay. By giving more importance
    to the accuracy ( α=4 ) the system tends to deploy more accurate models and rarely
    allocates small models (Fig. 5b). For α=5 , the number of models deployed on lower
    tiers decreases, as the system allocates no small models in practice (model IDs
    0–20) and selects instead multiple replicas of the most accurate models (Fig.
    5c). Since higher quality models feature, in general, a lower serving capacity
    (Table II), Fig. 5c suggests that a significant number of requests is served in
    the cloud (Tier 0) for this value of α . Figure 6 shows the average experienced
    inaccuracy (inaccuracy is given by 100- mAP and mAP is the mean average precision)
    and latency for different values of α under Network Topology I and Fixed Popularity
    Profile. When accuracy is not important (i.e., α≈0 ), INFIDA effectively achieves
    very low end-to-end latency (few milliseconds) by prioritizing the deployment
    of small and inaccurate models near to the edge nodes. Noticeably, the trend in
    both curves (decreasing inaccuracy and increasing latency) suggests that, when
    higher accuracy is required, the system starts to prefer models deployed close
    to the cloud, leading to a sudden change in the trade-off and to a significant
    increase in latency. Fig. 6. Average latency (dashed line) and inaccuracy (solid
    line) costs experienced with INFIDA for different values of α under Network Topology
    I and Fixed Popularity Profile. Show All In Fig. 7 we show the normalized time-averaged
    gain of INFIDA compared to OLAG, SG, and INFIDA OFFLINE for different values of
    α under the Sliding Popularity Profile. Results are shown both for Network Topology
    I (Fig. 7a) and for Network Topology II (Fig. 7b). Fig. 7. NTAG of the different
    policies under Sliding Popularity Profile and network topologies: (a) Network
    Topology I, and (b) Network Topology II. Show All The plot shows that the gain
    decreases by increasing α . This is expected since the gain (13) is defined as
    the improvement w.r.t. the repository allocation (tier 0). Therefore, when the
    latency is not important, high accuracy models at tier 0 are preferred, and there
    is no much room for improvement (the optimal gain eventually tends to zero for
    α→+∞ ). Note that, in general, SG and INFIDA OFFLINE policies perform worse than
    their offline counterparts, as they pick a single allocation that is the best
    w.r.t. the whole sequence of requests. However, in the Sliding Popularity Profile
    (Fig. 4) the best decision changes periodically, and only the online policies
    have the ability to adapt to such change. Moreover, we observe that consistently
    INFIDA OFFLINE has better performance than SG: although both policies are offline,
    INFIDA OFFLINE manages to provide a better allocation. B. Trade-off Between Model
    Updates and Service Cost In this set of experiments, we evaluate how the frequency
    at which INFIDA updates the model allocation affects the update cost incurred
    by the system. Indeed, frequent updates could lead to massive migrations with
    an overhead on network bandwidth. As an evaluation metric, we measure the total
    size of fetched models averaged over time (see the performance metric in Eq. (24)).
    We introduce B that we call the refresh period, and we restrict INFIDA to only
    sample a physical allocation every B∈{4,8,32} time slots (line 8 in Algorithm
    1). Additionally, we experiment linear stretching of the refresh period B with
    initial period B init =1 and target period B target =32 in a stretching duration
    of Δt = 1H. We run this experiment under Network Topology I and Sliding Popularity
    Profile. We set the trade-off parameter α=1 . In particular, Figure 8a shows the
    update cost (MU) for different refresh periods, while Figure 8b shows the NTAG.
    Both plots include the performance of the OLAG heuristic. We observe that, by
    increasing the refresh period B , the system fetches a smaller number of models,
    and therefore the update cost decreases, at the expense of reactivity. This tradeoff
    was characterized formally in [66], wherein the regret is sublinear for B=Θ( T
    β ) for β∈[0,1 ), and the update costs are sublinear for β∈(0,1) . Nevertheless,
    even for large values of B INFIDA eventually exceeds OLAG in performance: this
    result is expected since the algorithm continues to learn on the fractional (virtual)
    states and only the physical allocations are delayed and eventually catch-up for
    a large time horizon. On the other hand, we observe that OLAG is relatively conservative
    in updating its allocation, as it quickly picks a sub-optimal allocation and rarely
    updates it. Fig. 8. (a) Models Updates (MU) and (b) NTAG of OLAG and INFIDA for
    different values of refresh period B∈{4,8,16} , and for a dynamic refresh period
    with initial value B init =1 , target value B target =32 and stretching duration
    Δt=60 (1H). The experiment is run under Network Topology I and Sliding Popularity
    Profile. Show All The previous observation motivates the use of a dynamic refresh
    period. By refreshing more frequently at the start we allow the physical allocation
    generated by INFIDA to catch up quickly with the fractional states as shown in
    Fig. 8b: a dynamic refresh period that stretches from B init =1 to B target =32
    attains much faster and more precise convergence. This is achieved at the expense
    of a high update cost at the start, which is, however, quickly dampened until
    it matches the same update cost of fixing the refresh period to B target . C.
    Scalability on Requests Load We show how the system performs under different requests
    loads. For this set of experiments, we set α=1 . Figure 9 compares the results
    for the different allocation policies. Fig. 9. NTAG of the different policies
    for different request rates under Network Topology I. Show All We notice that,
    being INFIDA OFFLINE and SG offline policies, they perform well when the popularity
    profile is static (Fig. 9a), but deteriorate under Sliding Popularity Profile.
    Notably, the performance degradation of INFIDA OFFLINE (≈8%) is considerably limited
    compared to SG (≈30%), which even gets worse when increasing the requests load.
    Figure 9 shows that, in general, INFIDA provides a higher gain compared to the
    OLAG heuristic. In particular, under Fixed Popularity Profile INFIDA manages to
    converge to the same NTAG provided by its offline counterpart (Fig. 9a), which
    is ≈10% better than the one provided by OLAG when the load is 7,083 rps. Additionally,
    OLAG’s performance deteriorates when the requests load increases from 7,083 rps
    to 10,000 rps. It is also noteworthy that OLAG visibly suffers from perturbed
    performance when the popularity of the tasks changes over time (Fig. 9b). On the
    other hand, results show the robustness of INFIDA against changing request loads
    and popularity: the algorithm preserves its performance in terms of normalized
    time-averaged gain for the analyzed request loads and under both Fixed Popularity
    Profile and Sliding Popularity Profile, always converging to the highest NTAG.
    Last, in Fig. 10 we evaluate separately the average latency and inaccuracy attained
    by the different policies using different values of α∈{0.5,1,2,3,4,5,6} under
    Fixed Popularity Profile and Network Topology II. We observe that INFIDA and its
    offline counterpart INFIDA OFFLINE consistently provide the lowest average inaccuracy
    and latency under both high request load (10,000 rps) and default request load
    (7,500 rps). INFIDA OFFLINE is run with hindsight and serves as a lower bound
    on the achievable latency and inaccuracy under a fixed popularity request process
    (as in Fig. 9a). Fig. 10. Average Latency vs. Average Inaccuracy obtained for
    different values of α∈{0.5,1,2,3,4,5,6} under Fixed Popularity Profile and Network
    Topology II. Show All SECTION VII. Conclusion In this paper, we introduced the
    idea of inference delivery networks (IDNs), networks of computing nodes that coordinate
    to satisfy inference requests in the continuum between Edge and Cloud. IDN nodes
    can serve inference requests with different levels of accuracy and end-to-end
    latency, based on their geographic location and processing capabilities. We formalized
    the NP-hard problem of allocating ML models on IDN nodes, capturing the trade-off
    between latency and accuracy. We proposed INFIDA, a dynamic ML model allocation
    algorithm that operates in a distributed fashion and provides strong guarantees
    in an adversarial setting. We evaluated INFIDA simulating the realistic scenario
    of an ISP network, and compared its performance under two different topologies
    with both an offline greedy heuristic and its online variant. Our results show
    that INFIDA adapts to different latency/accuracy trade-offs and scales well with
    the number of requests, outperforming the greedy policies in all the analyzed
    settings. ACKNOWLEDGMENT This work has been carried out in the framework of a
    common laboratory agreement between Inria and Nokia Bell Labs. Authors Figures
    References Keywords Metrics Media Footnotes More Like This A Survey of Machine
    Learning Applications for Energy-Efficient Resource Management in Cloud Computing
    Environments 2015 IEEE 14th International Conference on Machine Learning and Applications
    (ICMLA) Published: 2015 Resource Management in Cloud Computing Using Machine Learning:
    A Survey 2020 19th IEEE International Conference on Machine Learning and Applications
    (ICMLA) Published: 2020 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE/ACM Transactions on Networking
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Toward Inference Delivery Networks: Distributing Machine Learning with Optimality
    Guarantees'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Nkenyereye L.
  - Lee B.G.
  - Chung W.Y.
  citation_count: '0'
  description: 'The proliferation of wearable devices and personal smartphones has
    promoted smart mobile health (MH) technologies. The MH applications and services
    are extremely responsive to computation latency. Edge computing is a distinguished
    form of cloud computing that keeps data, applications, and computing power away
    from a centralized cloud network or data center. In this work, we design a containerized
    wearable edge AI inference framework. The cloud computing layer includes two cloud-based
    infrastructures: The Docker hub repository and the storage as service hosted by
    Amazon web service. The Docker containerized wearable inference is implemented
    after training a Deep Learning model on open data set from wearable sensors. At
    the edge layer, the Docker container enables virtual computing resources instantiated
    to process data collected locally closer to EC infrastructures. It is made up
    of a number of Docker container instances. The containerized edge inference provides
    data analysis framework (DAF) targeted to fulfill prerequisites on latency, and
    the availability of wearable-based edge applications such as MH applications.'
  doi: 10.1007/978-3-031-53830-8_28
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Intelligent Human Computer
    Interaction Conference paper Containerized Wearable Edge AI Inference Framework
    in Mobile Health Systems Conference paper First Online: 29 February 2024 pp 273–278
    Cite this conference paper Access provided by University of Nebraska-Lincoln Download
    book PDF Download book EPUB Intelligent Human Computer Interaction (IHCI 2023)
    Lionel Nkenyereye , Boon Giin Lee & Wan-Young Chung   Part of the book series:
    Lecture Notes in Computer Science ((LNCS,volume 14532)) Included in the following
    conference series: International Conference on Intelligent Human Computer Interaction
    54 Accesses Abstract The proliferation of wearable devices and personal smartphones
    has promoted smart mobile health (MH) technologies. The MH applications and services
    are extremely responsive to computation latency. Edge computing is a distinguished
    form of cloud computing that keeps data, applications, and computing power away
    from a centralized cloud network or data center. In this work, we design a containerized
    wearable edge AI inference framework. The cloud computing layer includes two cloud-based
    infrastructures: The Docker hub repository and the storage as service hosted by
    Amazon web service. The Docker containerized wearable inference is implemented
    after training a Deep Learning model on open data set from wearable sensors. At
    the edge layer, the Docker container enables virtual computing resources instantiated
    to process data collected locally closer to EC infrastructures. It is made up
    of a number of Docker container instances. The containerized edge inference provides
    data analysis framework (DAF) targeted to fulfill prerequisites on latency, and
    the availability of wearable-based edge applications such as MH applications.
    Keywords Wearable sensors AI inference Edge intelligence Activity recognition
    Data processing Deep Learning Docker Container Access provided by University of
    Nebraska-Lincoln. Download conference paper PDF Similar content being viewed by
    others Edge Computing Empowered Smart Healthcare: Monitoring and Diagnosis with
    Deep Learning Methods Article 21 February 2024 Fog Networking for Machine Health
    Prognosis: A Deep Learning Perspective Chapter © 2017 Personalized human activity
    recognition using deep learning and edge-cloud architecture Article 18 February
    2022 1 Introduction The proliferation of wearable devices and personal smartphones
    has promoted smart mobile health (MH) technologies. The MH system is the foundation
    stone of the Healthcare 3.0 [1]. The MH system employs wearable medical sensors,
    mobile computing, wireless communications, and networking technologies to regularly
    transmit many sensed data to MH processing platforms. These platforms are commonly
    located in the cloud to provide anywhere and anytime healthcare services, thus
    improving well-being and quality of life. Furthermore, artificial intelligence
    (AI) and machine learning (ML) have added smartness to MH applications to analyze
    MH data efficiently. The MH applications and services are extremely responsive
    to computation latency. For instance, the round trip time (RTT) to access MH cloud-based
    services is very long; thus, it increases latency. This limits the deployment
    of many time-critical MH applications. Thus, the deployment of computing infrastructures
    near where data are generated solves the issue of RTT. This computing infrastructure
    is designated edge computing (EC). EC is a distinguished form of cloud computing
    that keeps data, applications, and computing power away from a centralized cloud
    network or data center. In this work, we design a containerized wearable AI edge
    inference framework. It is made up of a number of Docker container services instantiated
    close to end-user wearable sensors. The containerized edge inference provides
    data analysis framework (DAF) targeted to fulfill prerequisites on latency, and
    the availability of wearable-based edge applications such as MH applications.
    2 Containerized Wearable Edge AI Inference Framework The MH system exhibits diversity
    in the devices it incorporates, spanning from wearables (For instance, sensors
    positioned on the individual’s chest, right wrist, left ankle, and cardiac sensors.)
    to video surveillance cameras and smartphones. Wearable devices are responsible
    for data collection. Within these devices, a subset is situated on the chest and
    is capable of providing two-lead electrocardiogram (ECG) measurements, which have
    the potential for basic heart monitoring. Each sensor can collect patient data,
    perform simple data processing, and send the information to the gateway (e.g.,
    a smart mobile phone). Just after data collection at the gateway, the mobile phone
    will send the data to the MH data management platform for storage and further
    analysis. Healthcare professionals (e.g., doctors, and nurses) observe and read
    patient data in an emergency. Clinical observation, diagnosis, and medical intervention
    are carried out. Only healthcare professionals who have authenticated credentials
    access patient data. Fig. 1. Containerized Wearable edge inference framework.
    Full size image The diagram in Fig. 1 showcases a wearable edge AI inference framework
    that utilizes containers. This framework streamlines two core components: the
    pre-trained AI model and the edge AI inference server. The pre-trained model and
    the inference Docker image are stored in the cloud, while Edge AI inference is
    a containerized service functioning on edge devices. The cloud computing layer
    comprises two cloud-based infrastructures: the Docker Hub repository and Amazon
    Web Services’ storage-as-a-service. The Docker containerized wearable inference
    is realized following the training of a deep learning model on an open dataset
    derived from wearable sensors. The MHEALTH dataset [2] encompasses data from 12
    physical activities recorded from ten subjects using four different inertial sensors.
    These sensors capture a total of 23 distinct signal types, which we’ll refer to
    as channels. Most of these channels pertain to body motion, with the exception
    of two that record electrodiagram signals from the chest. The participants are
    assigned the responsibility of carrying out 12 different activities, and the sensors
    on their bodies record data related to acceleration, rotational speed, and magnetic
    field orientation. This dataset serves as input for edge-based wearable data analytics,
    focusing on deep learning models for human activity recognition applications.
    Each log file contains 23 columns for each channel and an additional column for
    class, representing one of the 12 activities. On average, there are around 100,000
    rows for each subject. We employ the concept of MLOp pipeline [3]. The pipeline
    steps are detailed as follows: we divide the MHEALTH time series dataset into
    smaller chunks for classification as proposed in [4]. TensorFlow and Keras [4]
    are used for training neural networks. The long short-term memory (LSTM) [5] model
    was tested and showed better accuracy through the epochs. For the model to receive
    inference requests, with only Python (the programming language) wrapper available,
    the training process was written in Python using the Flask framework [4]. After
    ensuring that the model has almost 95 percent precision, it was saved on the cloud-based
    storage service. In our case, Amazon web service cloud object storage was used
    to save the pre-trained AI model. The pre-trained model is saved with HDF5. It
    is loaded while the pre-processing phase starts. Subsequently, the edge AI inference
    server was encapsulated into a container image and uploaded to our Docker repository
    account. The Docker repository name for our edge AI inference image is “\\(nkenye1982/mhealth\\_ai\\_service\\)”
    [6]. We named the AI edge inference server as virtual MH edge service (vMHES).
    At the edge layer, the Docker container enables virtual computing resources instantiated
    to process data collected locally closer to EC infrastructures. Although the computing
    resources orchestration is isolated, associating a tailored logical edge inference
    instance would enhance the end-to-end (E2E) performance. The deployment edge node
    fulfills the role of provisioning and instantiating the wearable sensor data processing.
    The cluster of worker nodes consists of one server (x64) and two Raspberry Pi
    3 (ARM). The server (x64) is used to determine the resource of each wearable sensor
    service performing data analytics on an x64 worker node. This device runs Ubuntu
    18.04 and has an AMD Opteron at 2 GHz and 4 GB RAM. The Raspberry Pi 3 is used
    to evaluate the provisioning of resources on an ARM device (armhf) to support
    edge wearable data processing service. This system operates using Ubuntu 20.4
    server on a Raspberry Pi with Kernel version 5.4. The hardware configuration consists
    of 2 GB of RAM and a quad-core Cortex-A72 processor running at 1.5 GHz. All assessments
    will be conducted on both x64 and armhf platforms. Different container runtimes
    (e.g., container runtime interface (CRI-O), Docker, and Containerd) were used
    [7]. To verify this, the orchestrator at the worker nodes is set up using Containerd,
    Docker, and CRI-O. Additionally, the average loading waiting time of the sensor
    data processing AI model to load TensorFlow/Keras APIs, dependencies, and Flask
    starting server was measured. Fig. 2. The average load on the CPU and the usage
    of memory within the edge cluster system while processing real-time dataset logs
    for DAF inference requests. Full size image Fig. 3. Allocating resources by retrieving
    a deep learning image based on mHealth from the Docker registry. Full size image
    3 Evaluation The experimental test plan uses Locust [8] to capture the containerized
    AI requests per second, i.e., the response time. The response time refers to how
    long the AI model (TensorFlow 2’s Keras API) orchestrated on the edge node takes
    to respond to a new prediction’s end-user request. The capacity and performance
    tests are necessary to demonstrate that a containerized DAF instance can successfully
    process AI models with satisfactory speed when numerous users simultaneously upload
    MH data. In Fig. 2, we can observe the computation resource load average within
    the container instance of the cluster. The CPU load of the active edge container
    is measured in three observations, reflecting the load over the last 1, 5, and
    15 min. It becomes worse as the number of user DAF inference requests rises. The
    findings indicate that as the volume of data and the number of users’ DAF inference
    requests increase, resource consumption deteriorates. Within the first few minutes,
    the CPU load average reaches 90% utilization, particularly on the sixth hour of
    experimentation. This signifies that the container instances became overloaded,
    leading to a high average load on the computing resources due to the increasing
    number of DAF inference requests. As shown in Fig. 3, when orchestrating the scheduling
    of edge resources by fetching vMHES from the Docker registry, the specific duration
    varies, typically taking about five minutes on x64 architecture compared to approximately
    seven minutes on ARM architecture. Interestingly, the CRI-O [7] demonstrates a
    significantly shorter scheduling time on x64, being twice as fast as on ARM. The
    outcomes reveal that Docker and Containerd [9] runtimes are notably less efficient
    in comparison to CRI-O. This is primarily due to Docker using Containerd as its
    underlying technology for running containers. The rationale behind CRI-O’s superior
    performance is its lightweight nature, making it an optimal runtime for Kubernetes
    (K8s). This enables Kubernetes to utilize any runtime that adheres to the Open
    Container Initiative (OCI) standards as the container runtime. 4 Conclusion The
    MH applications and services are extremely responsive to computation latency.
    For instance, the round trip time (RTT) to access MH cloud-based services is very
    long. The edge inference layer consists of the edge computing resources and is
    close to end-users’ wearable sensors. The deployment edge node fulfills the role
    of provisioning and instantiating the wearable sensor data processing. The results
    show the CPU load average in the first minutes is 90% utilization on the sixth
    hour of experimentation. This implies that the container instances experienced
    excessive load, with a high average usage of computing resources as the number
    of DAF inference requests continued to rise. Furthermore, Docker and Containerd
    runtimes exhibit significantly lower efficiency compared to CRI-O, primarily because
    Docker utilizes Containerd for container execution. References Yang, J., Zhou,
    J., Tao, G., Alrashoud, M., Mutib, K.N.A., Al-Hammadi, M.: Wearable 3.0: from
    smart clothing to wearable affective robot. IEEE Netw. 33(6), 8–14 (2019). https://doi.org/10.1109/MNET.001.1900059
    Article   Google Scholar   Banos, O., et al.: mHealthDroid: a novel framework
    for agile development of mobile health applications. In: Pecchia, L., Chen, L.L.,
    Nugent, C., Bravo, J. (eds.) IWAAL 2014. LNCS, vol. 8868, pp. 91–98. Springer,
    Cham (2014). https://doi.org/10.1007/978-3-319-13105-4_14 Chapter   Google Scholar   Niranjan,
    D.R., Mohana: Jenkins pipelines: a novel approach to machine learning operations
    (MLOps). In: 2022 International Conference on Edge Computing and Applications
    (ICECAA), Tamilnadu, India, pp. 1292–1297 (2022) Google Scholar   bhimmetoglu.
    A tutorial for datascience for classifying human activity from body motion and
    vital signs recordings. https://github.com/bhimmetoglu/datasciencecom-mhealth.
    Accessed 25 Sept 2022 Barut, O., Zhou, L., Luo, Y.: Multitask LSTM model for human
    activity recognition and intensity estimation using wearable sensor data. IEEE
    Internet Things J. 7(9), 8760–8768 (2020) Article   Google Scholar   DockerHub.
    Docker image for mobile Health pre-trained models and inference server. https://hub.docker.com/repositories/nkenye1982.
    Accessed 25 Apr 2023 Vaño, R., Lacalle, I., Sowiński, P., S-Julián, R., Palau,
    C.E.: Cloud-native workload orchestration at the edge: a deployment review and
    future directions. Sensors 23, 2215 (2023). https://doi.org/10.3390/s23042215
    Locust. An open source load testing tool. https://locust.io/. Accessed 25 July
    2023 Cloud Native Computing Foundation. An Industry-standard container runtime
    with an emphasis on simplicity, robusteness and portability. https://containerd.io/.
    Accessed 25 July 2023 Download references Acknowledgment This work was supported
    by the National Research Foundation of Korea (NRF) grant funded by the Korea government
    (MIST) (NRF-2019R1A2C1089139). Author information Authors and Affiliations BK21,
    Education and Research Group of AI Convergence, Pukyong National University, Busan,
    Republic of Korea Lionel Nkenyereye Nottingham Ningbo China Beacons of Excellence
    Research and Innovation Institute, School of Computer Science, University of Nottingham
    Ningbo China, Ningbo, Zhejiang, China Boon Giin Lee Department of Electronic Engineering,
    Pukyong National University, Busan, Republic of Korea Wan-Young Chung Corresponding
    author Correspondence to Wan-Young Chung . Editor information Editors and Affiliations
    Soongsil University, Seoul, Korea (Republic of) Bong Jun Choi Saint Louis University,
    St. Louis, MO, USA Dhananjay Singh Indian Institute of Information Technology,
    Allahabad, India Uma Shanker Tiwary Pukyong National University, Busan, Korea
    (Republic of) Wan-Young Chung Rights and permissions Reprints and permissions
    Copyright information © 2024 The Author(s), under exclusive license to Springer
    Nature Switzerland AG About this paper Cite this paper Nkenyereye, L., Lee, B.G.,
    Chung, WY. (2024). Containerized Wearable Edge AI Inference Framework in Mobile
    Health Systems. In: Choi, B.J., Singh, D., Tiwary, U.S., Chung, WY. (eds) Intelligent
    Human Computer Interaction. IHCI 2023. Lecture Notes in Computer Science, vol
    14532. Springer, Cham. https://doi.org/10.1007/978-3-031-53830-8_28 Download citation
    .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-53830-8_28 Published 29 February
    2024 Publisher Name Springer, Cham Print ISBN 978-3-031-53829-2 Online ISBN 978-3-031-53830-8
    eBook Packages Computer Science Computer Science (R0) Share this paper Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Publish
    with us Policies and ethics Sections Figures References Abstract Introduction
    Containerized Wearable Edge AI Inference Framework Evaluation Conclusion References
    Acknowledgment Author information Editor information Rights and permissions Copyright
    information About this paper Publish with us Discover content Journals A-Z Books
    A-Z Publish with us Publish your research Open access publishing Products and
    services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Computer Science (including subseries Lecture Notes in
    Artificial Intelligence and Lecture Notes in Bioinformatics)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Containerized Wearable Edge AI Inference Framework in Mobile Health Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Vargas-Rojas L.
  - Ting T.C.
  - Rainey K.M.
  - Reynolds M.
  - Wang D.R.
  citation_count: '0'
  description: Advancements in phenotyping technology have enabled plant science researchers
    to gather large volumes of information from their experiments, especially those
    that evaluate multiple genotypes. To fully leverage these complex and often heterogeneous
    data sets (i.e. those that differ in format and structure), scientists must invest
    considerable time in data processing, and data management has emerged as a considerable
    barrier for downstream application. Here, we propose a pipeline to enhance data
    collection, processing, and management from plant science studies comprising of
    two newly developed open-source programs. The first, called AgTC, is a series
    of programming functions that generates comma-separated values file templates
    to collect data in a standard format using either a lab-based computer or a mobile
    device. The second series of functions, AgETL, executes steps for an Extract-Transform-Load
    (ETL) data integration process where data are extracted from heterogeneously formatted
    files, transformed to meet standard criteria, and loaded into a database. There,
    data are stored and can be accessed for data analysis-related processes, including
    dynamic data visualization through web-based tools. Both AgTC and AgETL are flexible
    for application across plant science experiments without programming knowledge
    on the part of the domain scientist, and their functions are executed on Jupyter
    Notebook, a browser-based interactive development environment. Additionally, all
    parameters are easily customized from central configuration files written in the
    human-readable YAML format. Using three experiments from research laboratories
    in university and non-government organization (NGO) settings as test cases, we
    demonstrate the utility of AgTC and AgETL to streamline critical steps from data
    collection to analysis in the plant sciences.
  doi: 10.3389/fpls.2024.1265073
  full_citation: '>'
  full_text: '>

    "Top bar navigation About us All journals All articles Submit your research Search
    Login Frontiers in Plant Science Sections Articles Research Topics Editorial Board
    About journal Download article 545 Total views 135 Downloads View article impact
    View altmetric score Share on Edited by Jennifer Clarke University of Nebraska-Lincoln,
    United States Reviewed by Geng(Frank) Bai Department of Biological Systems Engineering,
    University of Nebraska - Lincoln, United States Alan Cleary National Center for
    Genome Resources, United States Table of contents Abstract 1 Introduction 2 Methods
    3 Results and discussion 4 Conclusion Data availability statement Author contributions
    Funding Acknowledgments Conflict of interest Publisher’s note Supplementary material
    References Supplemental data Export citation Check for updates People also looked
    at Physiological responses and transcriptomic analysis of StCPD gene overexpression
    in potato under salt stresses Xiangyan Zhou, Yanming Ma, Rong Miao, Caijuan Li,
    Ziliang Liu, Dan Zhang, Sijin Chen, Jiaqi Luo and Wenhui Tang Advances in Quercus
    ilex L. breeding: the CRISPR/Cas9 technology via ribonucleoproteins Vera Pavese,
    Andrea Moglia, Anna Maria Milani, Lorenzo Antonio Marino, Maria Teresa Martinez,
    Daniela Torello Marinoni, Roberto Botta and Elena Corredoira Quantifying physiological
    trait variation with automated hyperspectral imaging in rice To-Chia Ting, Augusto
    C. M. Souza, Rachel K. Imel, Carmela R. Guadagno, Chris Hoagland, Yang Yang and
    Diane R. Wang SpaTemHTP: A Data Analysis Pipeline for Efficient Processing and
    Utilization of Temporal High-Throughput Phenotyping Data Soumyashree Kar, Vincent
    Garin, Jana Kholová, Vincent Vadez, Surya S. Durbha, Ryokei Tanaka, Hiroyoshi
    Iwata, Milan O. Urban and J. Adinarayana Ontologies for increasing the FAIRness
    of plant research data Kathryn Dumschott, Hannah Dörpholz, Marie-Angélique Laporte,
    Dominik Brilhaus, Andrea Schrader, Björn Usadel, Steffen Neumann, Elizabeth Arnaud
    and Angela Kranz TECHNOLOGY AND CODE article Front. Plant Sci., 20 February 2024
    Sec. Technical Advances in Plant Science Volume 15 - 2024 | https://doi.org/10.3389/fpls.2024.1265073
    This article is part of the Research Topic Women in Plant Science - Linking Genome
    to Phenome View all 5 articles AgTC and AgETL: open-source tools to enhance data
    collection and management for plant science research Luis Vargas-Rojas1To-Chia
    Ting1Katherine M. Rainey1Matthew Reynolds2Diane R. Wang1* 1Department of Agronomy,
    Purdue University, West Lafayette, IN, United States 2Wheat Physiology Group,
    International Maize and Wheat Improvement Center (CIMMYT), Texcoco, Mexico Advancements
    in phenotyping technology have enabled plant science researchers to gather large
    volumes of information from their experiments, especially those that evaluate
    multiple genotypes. To fully leverage these complex and often heterogeneous data
    sets (i.e. those that differ in format and structure), scientists must invest
    considerable time in data processing, and data management has emerged as a considerable
    barrier for downstream application. Here, we propose a pipeline to enhance data
    collection, processing, and management from plant science studies comprising of
    two newly developed open-source programs. The first, called AgTC, is a series
    of programming functions that generates comma-separated values file templates
    to collect data in a standard format using either a lab-based computer or a mobile
    device. The second series of functions, AgETL, executes steps for an Extract-Transform-Load
    (ETL) data integration process where data are extracted from heterogeneously formatted
    files, transformed to meet standard criteria, and loaded into a database. There,
    data are stored and can be accessed for data analysis-related processes, including
    dynamic data visualization through web-based tools. Both AgTC and AgETL are flexible
    for application across plant science experiments without programming knowledge
    on the part of the domain scientist, and their functions are executed on Jupyter
    Notebook, a browser-based interactive development environment. Additionally, all
    parameters are easily customized from central configuration files written in the
    human-readable YAML format. Using three experiments from research laboratories
    in university and non-government organization (NGO) settings as test cases, we
    demonstrate the utility of AgTC and AgETL to streamline critical steps from data
    collection to analysis in the plant sciences. 1 Introduction As the cost of genotyping
    continues to decrease, acquiring and managing data associated with plant phenotypes
    and environmental conditions have emerged as considerable limiting factors in
    plant science research. In response, technological advancements in data acquisition
    have been able to greatly increase the volume of data that researchers are able
    to collect from experiments (Eitzinger, 2021; Machwitz et al., 2021). Despite
    improvement in increasing the throughput of measurements, new instrumentation
    has not entirely replaced traditional methods; rather, they are often used to
    complement the repertoire of conventional methodologies employed by research groups,
    especially for experiments carried out under field conditions (Coppens et al.,
    2017; Crain et al., 2022). For instance, standard methods used at the International
    Maize and Wheat Improvement Center (CIMMYT) for plant phenotyping in their applied
    crop research programs include all of the following: traditional observation-based
    methods, high-throughput and low-cost phenotyping tools, and highly specialized
    equipment (Reynolds et al., 2020). The situation is similar for university-based
    research labs, where new instruments and techniques are continuously being tested
    and adopted, but complementary ground-reference measurements are still retained
    (e.g., Ting et al., 2023). Given the diversity of measurements made by plant science
    research groups, labs currently experience several challenges related to the collection,
    processing, and management of data (e.g., protocols presented in Pask et al.,
    2012). First, many kinds of measurements are still recorded on paper. This is
    true not only for those collected by hand but also for measurements collected
    using electronic instruments that have limited memory, i.e. only storing a small
    number of observations. For example, the chlorophyll meter SPAD-502plus (Konica
    Minolta; Osaka, Japan) can save just 30 measurements in memory; for this reason,
    researchers still commonly record these data on paper (Mullan and Mullan, 2012).
    Newer versions of devices can sometimes enable greater data storage (e.g., the
    Chlorophyll Meter SPAD 502DL Plus with Data Logger can store up to 4,096 measurements).
    However, researchers often only have access to the older versions due to budget
    constraints that limit the upgrading of still-functional equipment. The second
    challenge concerns the heterogeneous nature of data files (i.e. those differing
    in format and structure), as measurements commonly collected in plant science
    research can originate from different instruments or methods. This creates issues
    in efficient data integration and management (Neveu et al., 2019). The final challenge
    lies in the storage and management of research data after they are integrated,
    which commonly rely on spreadsheet files on personal computers (Elsayed and Saleh,
    2018) or with file storage cloud services using non-standard naming conventions
    and nested directories. This creates potential issues for sharing data in standardized
    ways with version control. Overall, these observations likely indicate that the
    data landscape for experiments carried out in plant science research domains has
    become increasingly complex. Improving the data pipeline from collection and processing
    to storage and management would help enhance data interpretation to ultimately
    enable new discoveries. A data pipeline is a sequence of processes that begins
    with collection and includes extraction, transformation, aggregation, and validation,
    and is complete when data are loaded into a database for eventual analysis (Munappy
    et al., 2020). Even though data pipelines are designed to enhance research productivity,
    their successful implementation is often hindered by infrastructural and organizational
    challenges. Munappy et al. (2020) speculated that several human aspects underlie
    impediments to the adoption of these pipelines, including resistance to change,
    and development complexity. In plant science research, numerous commercial and
    open-source tools for improving various steps in the data pipeline have been developed.
    For example, software applications have been made available for phenotyping in
    breeding programs to improve data collection in the field. These include Field
    Book, an open-source Android application that enables direct data entry with a
    user-friendly interface using experimental information loaded by users via files
    known as field files (Rife and Poland, 2014); Phenobook, an open-source web application
    for collaborative research (Crescente et al., 2017); and AgroFIMS, an open-source
    web tool that was initially developed as an analytics platform for breeding, whose
    current version has been expanded for data collection (Devare et al., 2021). The
    Integrated Breeding Platform, another example, is a commercial data management
    service for plant breeding programs that provides software, support, and services
    for breeding data pipelines (Malosetti et al., 2016). Breedbase is a web-based
    application that allows management of phenotyping data, stores genotypic information,
    and can perform analyses related to genomic prediction (Morales et al., 2022).
    Enterprise Breeding System is an open-source software for breeding programs that
    enables management of germplasm trials and nurseries as well as data management
    and analysis (CGIAR Excellence in Breeding Platform, 2022). More recently, PhytoOracle
    was released to provide a suite of tools that integrates open-source distributed
    computing frameworks for processing lettuce and sorghum phenotypic traits from
    RGB, thermal, PSII chlorophyll fluorescence, and 3D laser scanner datasets (Gonzalez
    et al., 2023). For a comprehensive recent review of digital tools developed for
    field-based plant data collection and management, we refer the reader to Dipta
    et al. (2023). Despite the repertoire of software described, current tools have
    several potential barriers to adoption in the broader plant science research community:
    they may be (1) commercial platforms; (2) open source or freely available but
    specialized for breeding application; (3) freely available but indicate that specialized
    IT knowledge is required; or (4) advertised as freely available but not actually
    available upon investigation. To address these gaps, we describe the development
    of two generic tools, called AgTC and AgETL, to enhance data collection and management
    in plant science research (Figure 1). These are alternatives for research groups
    that may not have the budget for a licensed plant-research database software platform
    or may require additional specialized IT knowledge to implement available free
    options. AgTC and AgETL also address the need for data collection and management
    tools to enhance data pipelines for plant science experiments that have objectives
    different from those of plant breeding programs (for which many tools are already
    available). For instance, they can be used in experiments where physiological
    traits or environmental factors are sampled at different time points with either
    traditional or modern techniques. The new AgETL tool is also amenable to help
    standardize data tables resulting from phenomics pipelines for final storage.
    Importantly, both AgTC and AgETL were designed based on extensive first-hand experiences
    of the primary tool developer as a field-based researcher who led data collection
    campaigns. Together, the two tools aim to reduce the time consumed on data processing
    and improve data storage to make downstream data analysis more efficient and accessible.
    While both tools work independently, they have a similar structure consisting
    of (1) an ipynb file executed on Jupyter Notebook, (2) function files containing
    Python functions that execute the steps for each tool, and (3) configuration files
    that contain user-specified parameters to run the functions; these are written
    in YAML, a data serialization language whose human-readable format functions correctly
    with Python (Ben-Kiki et al., 2021). Here, we demonstrate the utility of AgTC
    and AgETL in ongoing experiments on soybean, rice, and wheat carried out at Purdue
    University and at CIMMYT. figure 1 Figure 1 AgTC and AgETL can support plant science
    research from data collection to analysis. (A) Elements in the white background
    represent a typical series of steps taken in field- and controlled environment-based
    plant science research and (B) elements with grey background show how the processes
    of AgTC and AgETL can fit into within this overall framework. The proposed steps
    are numbered as follows: (1) Comma-separated value (CSV) template files for collecting
    data are created and uploaded to a computer or mobile device, where (2a) data
    are entered under experimental settings. (2b) AgTC-derived templates are not needed
    when data are collected using instruments that contain their data storage systems.
    Data that have been collected using the template from AgTC (3a) or downloaded
    from scientific instruments (3b) are next extracted using AgETL Extract functions
    and transformed using AgETL’s Transform process into a standard format in a single
    CSV file (4). (5a) Data quality assurance/quality control (QA/QC) processes are
    carried out outside the AgETL pipeline. (5b) When QA/QC is complete, data are
    ready to be loaded into a PostgreSQL database using the Load process of AgETL.
    CSV files containing clean data can be used directly for analysis (6a) or analyzed
    after it is downloaded from the database (6b). 2 Methods In this section, we describe
    the objectives and structure of the AgTC and AgETL Python functions along with
    implementation details and options for deployment. Experimental details from three
    test cases where data were collected using AgTC-generated templates are provided.
    Definitions for key terms mentioned throughout are first outlined below. Database:
    A structured data collection stored and accessed electronically by a database
    management system (DBMS) such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server,
    or Oracle Database. One of the characteristics of a DBMS is that it maintains
    data integrity; for instance, it does not permit storing different types of data
    in the same field or column or storing duplicated records based on the primary
    key, and data will persist as complete, accurate, and reliable. Dataframe: A Python
    object where data are organized in tabular format (rows and columns). In contrast
    to a database that is stored on disk, a dataframe is not persistent because it
    is stored in memory (RAM). Database table: A database object where data are stored
    in tabular format as records (rows) and fields (columns). Primary key: Column
    of a database table that contains unique fields to enable the identification of
    single rows. 2.1 Agricultural data template creator AgTC aims to standardize data
    collection for experiments conducted in field or controlled environment conditions.
    Its output is a CSV template file containing tabular meta-data related to the
    target observation in separate columns, such as crop species, experiment name,
    treatment, measurement name, unit of measurement, season, or other temporal designation,
    and one column that contains a unique identifier for each observation. These columns
    in the template are generated via two procedures. The first group of columns describes
    the experiment and is generated using basic metadata that are contained in an
    input CSV file. This may include a list of the experimental units (i.e., plots
    or pots), replications, genotype names, and any other information related to the
    experimental design. This set of information should be unique for each experimental
    unit. The second group of columns is generated using the parameters specified
    in the user-modified configuration YAML file, which also serves as an input file
    to AgTC. In contrast with the first group of columns, these columns, which are
    created using the YAML configuration file parameters, are repeated in all rows
    on a sequence basis. For clarification, Figure 2 shows an example of an output
    template CSV created by AgTC and maps how information from input files is used
    to complete rows and columns in the output file. figure 2 Figure 2 Input and output
    files of AgTC. Information from the input.csv file (A) and parameters from the
    configuration.yml file (B) are used to generate the columns found in the output
    template file (C). All arguments (i.e., information passed into functions) for
    AgTC Python functions are taken directly from the user-specified configuration
    file; in this way, the user is able to add, delete, or modify parameters without
    the need to code in Python. The YAML configuration file is divided into six chunks
    of parameters known as block collections, where each block is identified with
    uppercase letters. A block collection may have keys, values, or sequences (Table
    1). Considering that the block collection content can be modified to write variable
    names and content of rows, this enables the user to use controlled vocabularies
    and ontologies (Arnaud et al., 2020) or namings of crops, traits, and variables
    as recommended by the FAIR (Findable, Accessible, Interoperable, and Reusable)
    data principles (Devare et al., 2021). Once all the parameters are specified,
    the main.ipynb file can be executed on Jupyter Notebook without requiring the
    user to modify any line code. Since the template output is a CSV file, it can
    be opened by any spreadsheet software independent of operating system on a computer
    or mobile device to enter the observation values. Alternatively, the CSV file
    can be directly uploaded as a field file in the Field Book Android application
    (Rife and Poland, 2014) to facilitate data collection on the experiment in situ.
    The template created by AgTC fulfills Field Book''s field file requirement of
    having a column containing unique observation identifiers and other columns that
    can be used to navigate within the application, such as plot number and treatment.
    table 1 Table 1 The block collections of the AgTC configuration file. 2.2 Agricultural
    data extract, transform, and load framework The objectives of the Extract, Transform,
    and Load (ETL) tool are to process CSV data files from different plant science
    experiments and aggregate them into a standard database table in a central repository.
    There, data are available to use for a variety of downstream analyses. The execution
    of functions in AgETL is divided into two Jupyter Notebook and configuration files.
    The first set of functions runs the Extract and Transform processes. This outputs
    a CSV file where the data from different source files have been aggregated and
    standardized into a single format. The second group of functions is used to load
    data into a single table in the database. The ETL functions are divided into these
    two separate groups (i.e., the Extract and Transform functions in one group and
    the Load functions in another) to enable users to carry out data quality control
    in between (Figure 1). Extract and Transform processes: The Extract and Transform
    steps aim to merge different data files into a single and standard dataframe containing
    all necessary information to run subsequent analyses without needing extra information.
    These individual data files may be ones that were generated using template files
    from AgTC or those that were downloaded directly from scientific instruments.
    Files require a CSV extension and a grid-like format of rows and columns, with
    the first row containing column header names. Parameters are specified in a series
    of collection blocks found in the configuration file (Table 2), however, only
    the parameters within the FILES_TO_PROCESS collection block are required. The
    other parameters depend on the specific transformations that the data need for
    formatting into the final standardized dataframe. Therefore, the user can leave
    them empty if the data do not require transformation. All measurement values from
    different files are moved to the same column in the standardized dataframe, and
    they are differentiated from each other, adding the name and units of the variable
    in two different columns (see Supplementary Figure 1). The rest of the transformations
    are used to standardize row and column values, which are helpful for data aggregation.
    In this step, any unnecessary columns are also dropped. A CSV file containing
    all processed data in a tabular structure is exported at the end of the process.
    table 2 Table 2 Block collections that are part of the two AgETL configuration
    files. Load processes: The objective of the Load processes is to upload the data
    into a database, with which users can interact to perform queries or carry out
    data analysis. Even if data reflect different species, experiments, seasons, variables,
    and sources, the Load functions are flexible to upload them in the same database
    table. This facilitates queries across experiments, researchers, species, etc.,
    later. Those functions can also run structured query language (SQL) statements
    and open the database connection using six chunks of parameters in the configuration
    file. Out of the block collections, only four are required to load a new dataframe
    into the database table (Table 2). The other block collections are only required
    the first time a new table is created in the database or when a new column is
    appended to the table. The database is created in PostgreSQL, a DBMS based on
    a client-server architecture with a variant of the standard SQL as the query language
    (Herzog, 1998). Three functions create SQL statements to interact with the database.
    One of them allows creating a table (sql_statement_create_table_if_not_exist),
    another one inserts a new column in a preexisting table (sql_statement_add_column_if_table_exists),
    and another enables loading of the data into the table (insert_dataframe_to_database).
    Finally, execute_sql_statement is the function that executes the SQL statements.
    Additional SQL statements and commands found in the Jupyter Notebook allow the
    user to make changes and queries in the database. Since the Load operations are
    independent of the Extract and Transform steps, the process is amenable to uploading
    additional data files (e.g., metadata) and enables users to perform any SQL actions.
    For instance, the user can create tables and load data following an object-relational
    database model, which facilitates using Minimum Information about Plant Phenotyping
    Experiments (MIAPPE) standards (Papoutsoglou et al., 2020). 2.3 Implementation
    and deployment options AgTC (source code available at https://github.com/DS4Ag/AgTC)
    and AgETL (source code available at https://github.com/DS4Ag/AgETL) are open-source
    and are available on GitHub. They can be executed under various versions of Jupyter.
    For example, they can run on a local server using a simple installation of JupyterLab
    or Jupyter Notebook, or they can run under environment management such as Conda,
    Mamba, or Pipenv. Another option to run these tools is on the cloud using a Jupyter
    Hub environment. The configuration file for the AgETL Load steps can establish
    a PostgreSQL database connection in a local host, such as installing it on a local
    development computer or using any of the standard service models of the database
    cloud service offered as Infrastructure as a Service, Platform as a Service, or
    Software as a Service (Jain and Mahajan, 2017). These options enable research
    labs to work with their institutional IT infrastructure to set up individual workflows
    or create their database using commercially available cloud services, including
    Amazon Web Services, Google Cloud Platform, or Microsoft Azure. 2.4 Test case
    1: soybean evaluation under field conditions (USA) As a first test case, we collected
    data on a soybean (Glycine max) experiment. This experiment evaluated 25 soybean
    genotypes with four repetitions and two treatments: early planting (planted on
    May 30, 2022) and late planting (planted on June 9, 2022). The trial was conducted
    at the Purdue University Agronomy Center for Research and Education (ACRE; 40°
    28′ 20.5″ N 86° 59′ 32.3″ W) in West Lafayette, Indiana, USA. Various plant traits,
    such as height, width, growth stages, and photographs of a fully expanded trifoliate
    leave for each plot, were collected in the field using the Field Book application
    after creating templates using AgTC. These measurement campaigns occurred five
    times throughout the crop cycle, starting at late vegetative stages (V6) and finishing
    at approximately the R6 stage (full seed). In addition to data collected directly
    in the field, AgTC-generated CSV template files were used to record trifoliate
    dry weights on a computer in a lab setting. During the same measurement campaigns,
    volumetric soil water content (using HydroSense II; Campbell Scientific; UT, USA)
    and Leaf Area Index (LAI) (using the LAI-2200C Plant Canopy Analyzer; LI-COR Inc.,
    NE, USA) were collected. In both these cases, data were initially stored in each
    of the devices’ internal memory. Thus, for soil moisture and LAI, there was no
    need to use the AgTC template. Finally, after the R8 growth stage (full maturity),
    plants were sampled and processed in the lab to obtain yield components (plants
    per meter, pods per plant, and seeds per pod). Data were entered into template
    files created by AgTC using a computer for these measurements. 2.5 Test case 2:
    wheat evaluation under field conditions (Mexico) For our second test case, we
    collected data on wheat (Triticum aestivum) in an experiment established at CIMMYT’s
    research station, Campo Experimental Norman E. Borlaug (CENEB), located near Ciudad
    Obregon, Sonora, Mexico (27° 23′ 46″ N, 109° 55′ 42″ W). The trial evaluated a
    panel of 14 wheat genotypes with three repetitions under three environments: well-watered
    (WW), drought (DR), and high temperature (HT). Data were collected throughout
    the 2022 and 2023 crop growth seasons. Seedling emergence occurred in early December
    for the WW and DR treatments in both seasons. However, the HT trial was planted
    only for the 2022 season, with an emergence date in early March (Supplementary
    Table 1). Direct and proximal sensing measurements were made during the two crop
    seasons. There were four direct measurement sampling campaigns throughout each
    cycle. The first occurred before sowing, and the second was 40 days after seedling
    emergence. The third and fourth sampling campaigns were scheduled based on each
    genotype’s specific growth stages (GS) (Zadoks et al., 1974), with the third carried
    out 12 days after heading (GS54) and the fourth at physiological maturity (GS87).
    Proximal sensing measurements were made every week from canopy closure to GS87.
    Template files created by AgTC were used to enter data for gravimetric soil water
    content, above-ground biomass, and yield components. Samples were first collected
    from the field and then processed in the laboratory. Growth stages, including
    seedling emergence (GS10), flag leaf sheath extending (GS41), GS54, and GS87,
    and one plant height measurement after GS87, were collected directly in the field
    using the AgTC template and the Field Book application. The proximal sensing data
    collected include chlorophyll content (using the SPAD-502 chlorophyll meter; Konica
    Minolta; Osaka, Japan), canopy temperature (using the Sixth Sense LT300 Infrared
    Thermometer; TTI Instruments; VT, USA), normalized difference vegetation index
    (using the GreenSeeker hand-held optical sensor; N-Tech Industries; CA, USA) and
    hyperspectral reflectance (using the ASD Field Spec 3; ASDInc., CO, USA). 2.6
    Test case 3: rice evaluation in a controlled-environment facility (USA) For our
    final test case, we collected data on cultivated Asian rice (Oryza sativa) in
    a growth chamber environment at Purdue University’s Ag Alumni Seed Phenotyping
    Facility (AAPF) (West Lafayette, Indiana, USA). Six genotypes were chosen based
    on their documented genetic information, constrained flowering dates, diverse
    geographical backgrounds, or potential genetic value (Rice Diversity Panels 1
    and 2 from the USDA-ARS Dale Bumper National Rice Research Center, Stuttgart,
    Arkansas, Genetic Stocks Oryza Collection (www.ars.usda.gov/GSOR)) and raised
    for 82 days during Summer and Fall of 2022. The facility has two large growth
    chambers (Conviron®, Winnipeg, Canada) with a weight-based automated irrigation
    system (Bosman Van Zaal, Aalsmeer, The Netherlands), and both chambers were leveraged
    in this experiment; one had CO2 concentration at 700 ppm (high CO2 chamber) and
    the other at 415 ppm (ambient CO2 chamber). The rice plants were grown in pots
    under two levels of CO2 and two levels of drought. Each treatment had two replications
    with a total of 48 plants. According to the timing of drought, the experimental
    period could be divided into three timepoints: before drought (42-47 DAS, timepoint
    1, TP1), during the mid of drought treatment (59-61 DAS, timepoint2 A, TP2-A),
    at the very end of drought (66-67 DAS, timepoint2 B, TP2-B) and upon recovering
    (77-82 DAS, timepoint3, TP3). In this test case, AgTC was utilized to create Field
    Book field files to (1) collect photographs for later calculation of Specific
    Leaf Area (SLA), similar to the soybean test case (during TP2-B and TP3) and (2)
    record leaf water potential (LWP, MPa) measurements at 0800, 1400, and 1800 hr
    during TP2-B. For both types of measurements, the youngest fully expanded leaf
    from one plant was selected for each observation. For LWP, we used the Model 1000
    pressure bomb (PMS Instrument Company, OR, USA). 3 Results and discussion This
    section proposes a workflow using AgTC and AgETL to support plant science experiments
    from data collection to analysis. We describe the results of implementing the
    entire data pipeline in the two field experiments and utilizing AgTC in all three
    experiments. 3.1 Proposed data pipeline We propose a pipeline for data collection
    and management using AgTC and AgETL. The first step is to create templates using
    AgTC. Users can then open the CSV output file to enter data in a standardized
    fashion (Figure 1). If data are acquired in the lab, as would be the case for
    any sample destructively collected in the field or greenhouse, this can be accomplished
    using spreadsheet software on a computer (Figure 3). Alternatively, the template
    can be used directly as a field file for the Field Book application to facilitate
    data collection on-site, such as for data collection in the field, greenhouse,
    or growth chamber. When data collection is carried out using instruments that
    come with their own internal data storage system (e.g., the LI-6800 Portable Photosynthesis
    System), users download resultant files directly. After data acquisition, AgETL
    functions Extract and Transform data files from different sources to standardize
    their formats. From there, researchers can run exploratory data analysis, such
    as data aggregation, visualization, outlier detection to perform data QC. After
    data are QC-ed, they are ready for downstream analyses, such as modeling. At this
    point, it is also advisable to load the QC-ed data into a database using AgETL’s
    Load functions. In addition to enhancing data analysis capabilities via interaction
    with data analysis dashboards, data are securely stored and easily accessible.
    figure 3 Figure 3 Application of AgTC in wheat experiments. Left: An example of
    using an AgTC-generated template in laboratory conditions. Here, wet and dry soil
    weights are measured (A) and entered into an AgTC template directly on the computer
    (B). Right: An example of using an AgTC-generated template in field conditions.
    In this example, wheat plant height is measured manually (C), and entered digitally
    in the field via the Field Book application that utilizes an AgTC-generated template
    (D). Below, we describe the application of AgTC and AgETL in several test cases.
    AgTC was utilized in experiments on soybean, rice, and wheat under field and controlled
    environmental conditions, while AgETL functions were utilized for generating soybean
    and wheat datasets. In addition to the specific test cases presented here, AgTC
    has been used on another soybean experiment carried out in Wanatah, Indiana, USA,
    in 2022 and two more wheat experiments in the same research center at CIMMYT in
    Mexico during the 2023 field season. 3.2 Application of AgTC Templates created
    using AgTC were used for data collection for 12 measurements across three species
    in three experimental settings (Table 3). We observed that the new tool enhanced
    data collection in at least two ways: (1) utilization reduced the total steps
    required for data collection, and (2) application of AgTC helped improve data
    file organization. table 3 Table 3 Overview of measurements collected using AgTC
    and/or processed by AgETL from three experimental test cases. Reduction in the
    number of steps for data collection. In the soybean and rice experimental test
    cases, SLA was measured at multiple timepoints (five for soybean and two for rice).
    SLA is computed as the ratio of leaf area and leaf dry weight. Conventionally,
    leaf area estimates are made using leaf scanners or image-based software, whereby
    image files are manually reamed utilizing labels found within each image itself
    prior to image processing. This renaming step is tedious and may be affected by
    human error. To improve on the conventional method for estimating SLA, AgTC was
    used to create one template per sampling campaign, which was uploaded as a field
    file in the Field Book application. Then, using the picture function of Field
    Book, a photo of the target leaf was taken using tablets in the field. The advantage
    of this system is that Field Book directly uses the values of the observation
    identifier column generated by AgTC as the names of the image files. Compared
    with our traditional method, this saves time by precluding the need to manually
    rename image files before extracting leaf area values using downstream software
    such as Easy Leaf Area (Easlon and Bloom, 2014). Another example where the use
    of a template generated from AgTC as a field file in the Field Book application
    helped decrease the number of steps in data collection was for wheat canopy temperature
    measurements in the field. The conventional method employed by the CIMMYT Wheat
    Physiology group requires that one researcher takes temperature readings from
    the crop canopy using a sensor device while another records these values on a
    paper field form. Then, values are transferred manually to a digital spreadsheet.
    When using an AgTC-generated template opened either in a mobile device or uploaded
    as a field file in Field Book, measurements are digitized directly in the field,
    enhancing the efficiency of data collection and potentially reducing human error
    involved in converting paper records into digital formats. Improvement of data
    file organization. For some measurements, AgTC did not minimize the overall number
    of steps involved in data collection but can improve data management. As an illustration,
    we describe the process for recording wheat biomass measurements. Conventionally,
    the data are obtained in a laboratory setting, where the samples are processed,
    and data are entered directly into electronic files. Using AgTC, the overall process
    does not change. However, AgTC automatically generates metadata columns and unique
    identifiers for each observation in the electronic template files without any
    need to manually copy and paste information across spreadsheets. Additionally,
    the standardized format for naming template files by AgTC facilitates file organization
    in storage directories. 3.3 Application of AgETL AgETL was tested successfully
    for processing data collected from soybean experiments of the 2022 summer season
    and for data collected from the wheat experiments of winter seasons 2021-2022
    and 2022-2023. The Extract and Transform functions were executed separately from
    the Load functions for these data. Resultant standardized dataframes were loaded
    into a PostgreSQL database using the Load process after the Extract and Transform
    processes. The main objective of the Extract and Transform processes of AgETL
    is to generate dataframes with a standard format and structure using heterogeneously
    formatted lab- or field-generated data. Extract and Transform functions that perform
    the extraction process first take in CSV files from the directory indicated on
    the configuration file. The next step compares each file’s column names to identify
    unique and duplicated names. The output of this execution, a list of similar and
    different column names, helps the user decide which transformations are needed
    to alter dataframe columns. In the following step, dataframes obtained from the
    extracted files are concatenated, resulting in a single dataframe where original
    columns with shared names are combined, and original columns with different names
    are retained separately. The resulting dataframe is then combined with other files
    containing additional relevant information (e.g., genotype names) if the user
    indicated these parameters in the configuration file. Subsequent transformation
    steps depend on the kinds of alterations the data needs to undergo quality assurance
    (QA) or enter the Load process. At this stage, trait column names can be unified.
    For example, ‘LAI’, ‘Leaf Area Index’, ‘lai’, and ‘leaf area index’ refer to the
    same trait yet would be treated differently by database management systems; a
    transformation to update column names is needed. This and other options for transformations
    are illustrated in Supplementary Figure 1 and include dropping undesired columns,
    creating new columns, updating row values, and updating primary key values. From
    our testing of Extract and Transform processes on the soybean and wheat field
    experiments, we found AgETL useful for several scenarios. Scenario 1: AgETL was
    applied on data files derived from templates generated using AgTC. For example,
    we simultaneously processed 42 different canopy temperature files from three treatments
    and two crop growth seasons in the wheat experiment. In this case, only transformations
    for updating column names were needed. Scenario 2: AgETL was used to process data
    files derived from lab-collected measurements. From the same wheat experiments,
    we implemented AgETL on ten biomass files that were collected from samples processed
    in the lab. After files were joined, multiple column names were updated to unify
    columns with the same meaning, one column was added to indicate the unit, and
    several extraneous columns used in intermediate stages of biomass estimation were
    dropped (e.g., bag dry weight). Scenario 3: AgETL was used to process data collected
    via sensors that had their own unique storage systems. For instance, in the soybean
    experiment, data files originated from hand-held instruments such as the Campbell
    HydroSense II and LAI2200C (Table 3); in this case, the Extract and Transform
    functions of AgETL were used to merge the files into a single dataframe and join
    genotype names from another data file. Standardized, uniformly formatted data
    files resulting from the execution of Extract and Transform functions were next
    uploaded into a PostgreSQL database server to test the utility of AgETL’s Load
    process. The first step is connecting with a PostgreSQL database (either a cloud-hosted
    instance or a local installation). The configuration file is flexible, allowing
    the user to write parameters for either of the two options. We successfully tested
    the database connection in three database scenarios: one was a local instance
    (Figure 4A), and the other two were commercial cloud service providers, i.e.,
    Database as a Service (DBaaS). The two cloud services were Cloud SQL (https://cloud.google.com),
    offered by Google Cloud Platform (Bisong, 2019), and the Railway PostgreSQL database
    service (https://railway.app), shown in Figure 4B. Instructions for establishing
    the database connection have been made available in the AgETL GitHub repository.
    One significant advantage of processing and managing data using AgETL was observed
    for canopy temperature and SPAD measurements collected weekly during wheat experiments.
    These data were processed and loaded immediately after the data were gathered.
    The dashboard automatically reflected updated information in its data visualization
    features because the database was connected with a real-time data visualization
    online interface. figure 4 Figure 4 The AgETL Load process facilitates the loading
    of clean data into databases. Shown here are data loaded (A) in a PostgreSQL localhost
    server and (B) in Railway, a commercial database cloud service provider. After
    establishing the database connection, users can create the first table by writing
    the names of the columns and the data types using the PostgreSQL names for native
    data types on the configuration file. Each table only needs to be created once,
    and it is possible to create many tables if needed. The recommendation is to create
    tables based on specific research goals. For instance, data from different experiments
    that will be used to calibrate crop models should be uploaded to the same table
    in the database. Once the table exists, the data from the files specified for
    the user are uploaded. Moreover, AgETL can add more columns when the table already
    exists. Additionally, the records in the database can be upgraded by loading the
    data file with the updated row and having the same primary key. Furthermore, AgETL
    gives the user access to the four standard SQL actions of database systems called
    CRUD, which comes from create, read, update, and delete (van den Brink et al.,
    2007; Krogh, 2018). For that reason, the Jupyter Notebook file has a section with
    SQL statements, called useful SQL statements, that allows the deletion of a column
    and a table. Finally, AgETL permits users to write and execute their own SQL statements,
    like SQL queries, directly from Jupiter Notebook without another extra configuration.
    We additionally tested the connection to the DBaaS using R (R Core Team, 2021)
    and Python, two widely used programming languages in data analysis for research
    (Fahad and Yahya, 2018; Rahmany et al., 2020). We successfully connected R with
    the two DBaaS providers using the R packages, DBI (R Special Interest Group on
    Databases (R-SIG-DB) et al., 2022), and odbc (Hester and Wickham, 2023). Finally,
    the RPostgreSQL R package was used to access the PostgreSQL database. For testing
    with Python, the PostgreSQL connection was successfully created using the Psycopg2
    library (https://www.psycopg.org). From our experience developing and testing
    AgTC and AgETL in the soybean, wheat, and field experiments, we have five suggestions
    to best leverage their use in small to mid-sized research groups: (1) Users who
    collect data should be the ones that create their own template files with AgTC,
    and (2) these templates should be saved on a centralized repository. The creation
    of multiple nested folders should be avoided since the template files can be sorted
    by their names, which are automatically generated in a structured, consistent,
    and meaningful way. For the Extract and Transform process using AgETL, it may
    be best to (3) appoint a single responsible person to handle these steps, while
    (4) individual researchers execute the QA/QC themselves on the files generated
    from the Extract and Transform steps, following and documenting steps appropriate
    for their own research project. Finally, (5) an appointed individual in the research
    group (or IT specialist collaborating with the research group) handles uploading
    clean, QA/QC-ed files to the database using AgETL’s Load process. This should
    occur on a regular basis that is sensible for the research group, e.g., after
    each field season, to keep the database up-to-date. 3.4 Areas for improvement
    We have identified several areas for future improvements to AgTC and AgETL. Even
    though AgTC templates allow the collection of multiple variables at the same time
    (e.g., as would occur if plant height and canopy width are collected with the
    same AgTC-generated template), multivariable files used as input in AgETL need
    to be processed as separate steps during the Extract and Transform stages mainly
    because the observation ID need to be updated. This was to make the configuration
    file simpler for the user, such that they do not need to specify many parameters.
    Streamlining these processing steps into a single step for multivariable files
    could be an improvement in an updated version of the tool. Additionally, documentation
    to implement ETL steps using functions on workflow management platforms such as
    Apache Airflow would help to automate AgETL. Finally, documentation to use both
    AgTC and AgETL in command-line mode could enable the implementation of these tools
    as part of a larger workflow, which may be particularly useful for moderate to
    large-sized laboratories. 4 Conclusion We have developed two tools to address
    observed challenges in data collection, processing, and management in plant science
    research called AgTC and AgETL. These tools are simple to use and do not require
    experience in programming. Additionally, they are adaptable for data collection
    and data processing in the field or the lab and are agnostic to crop species,
    experimental design, and scale. The templates generated by AgTC can be used for
    data collection in the field or the lab and can reduce the number of steps required
    for data collection and improve data file organization. AgETL enables the extraction
    of information from multiple files from diverse sources and measurements and merges
    them into the same data table. This tool facilitates loading and wrangling data
    on a local host or using a database cloud service, which can be readily managed
    by appointed individuals within research labs or by collaborating with institutional
    IT support. Finally, we have developed user documentation, with English and Spanish
    versions available in the README section of the AgTC and AgETL GitHub repositories.
    Data availability statement The source code for AgTC and AgETL can be found here:
    https://github.com/DS4Ag/AgETL, https://github.com/DS4Ag/AgTC. Author contributions
    LV-R: Conceptualization, Methodology, Software, Writing – original draft, Writing
    – review & editing. T-CT: Methodology, Writing – review & editing. KR: Resources,
    Writing – review & editing. MR: Resources, Funding acquisition, Project administration,
    Writing – review & editing. DW: Funding acquisition, Resources, Writing – review
    & editing, Conceptualization, Supervision, Writing – original draft. Funding The
    author(s) declare financial support was received for the research, authorship,
    and/or publication of this article. LV-R was supported by a CONACYT graduate fellowship
    from the Mexican government. Funding for the experimental test cases was provided
    by HedWIC #DFs-19-0000000013 to MR and USDA NIFA #2022-67013-36205 to DW. Acknowledgments
    The authors thank Abby Seybert and Ava Antic for help with soybean data collection;
    Makala Hammons and Sam Schafer for help with rice data collection; Lucia Nevescanin,
    and Pablo Rivera for help with wheat data collection. Eric Vince Seal planted
    the soybean trial. Conflict of interest The authors declare that the research
    was conducted in the absence of any commercial or financial relationships that
    could be construed as a potential conflict of interest. Publisher’s note All claims
    expressed in this article are solely those of the authors and do not necessarily
    represent those of their affiliated organizations, or those of the publisher,
    the editors and the reviewers. Any product that may be evaluated in this article,
    or claim that may be made by its manufacturer, is not guaranteed or endorsed by
    the publisher. Supplementary material The Supplementary Material for this article
    can be found online at: https://www.frontiersin.org/articles/10.3389/fpls.2024.1265073/full#supplementary-material
    References Arnaud, E., Laporte, M.-A., Kim, S., Aubert, C., Leonelli, S., Miro,
    B., et al. (2020). The ontologies community of practice: A CGIAR initiative for
    big data in agrifood systems. Patterns 1, 100105. doi: 10.1016/j.patter.2020.100105
    PubMed Abstract | CrossRef Full Text | Google Scholar Ben-Kiki, O., Evans, C.,
    döt Net, I. (2021) YAML Ain’t Markup Language (YAML™) revision 1.2.2. Available
    at: https://yaml.org/spec/1.2.2/. Google Scholar Bisong, E. (2019). An Overview
    of Google Cloud Platform Services. In Building Machine Learning and Deep Learning
    Models on Google Cloud Platform. Ed. Bisong, E. (Berkeley, CA: Apress), 7–10.
    doi: 10.1007/978-1-4842-4470-8_2 CrossRef Full Text | Google Scholar CGIAR Excellence
    in Breeding Platform (2022) The Enterprise Breeding System (EBS). Available at:
    https://excellenceinbreeding.org/toolbox/tools/enterprise-breeding-system-ebs.
    Google Scholar Coppens, F., Wuyts, N., Inzé, D., Dhondt, S. (2017). Unlocking
    the potential of plant phenotyping data through integration and data-driven approaches.
    Curr. Opin. Syst. Biol. 4, 58–63. doi: 10.1016/j.coisb.2017.07.002 PubMed Abstract
    | CrossRef Full Text | Google Scholar Crain, J., Wang, X., Evers, B., Poland,
    J. (2022). Evaluation of field-based single plant phenotyping for wheat breeding.
    Plant Phenome J. 5, e20045. doi: 10.1002/ppj2.20045 CrossRef Full Text | Google
    Scholar Crescente, J. M., Guidobaldi, F., Demichelis, M., Formica, M. B., Helguera,
    M., Vanzetti, L. S. (2017). Phenobook: an open source software for phenotypic
    data collection. GigaScience 6, giw019. doi: 10.1093/gigascience/giw019 CrossRef
    Full Text | Google Scholar Devare, M., Aubert, C., Benites Alfaro, O. E., Perez
    Masias, I. O., Laporte, M.-A. (2021). AgroFIMS: A tool to enable digital collection
    of standards-compliant FAIR data. Front. Sustain. Food Syst. 5. doi: 10.3389/fsufs.2021.726646
    CrossRef Full Text | Google Scholar Dipta, B., Sood, S., Devi, R., Bhardwaj, V.,
    Mangal, V., Thakur, A. K., et al. (2023). Digitalization of potato breeding program:
    Improving data collection and management. Heliyon 9, e12974. doi: 10.1016/j.heliyon.2023.e12974
    PubMed Abstract | CrossRef Full Text | Google Scholar Easlon, H. M., Bloom, A.
    J. (2014). Easy Leaf Area: Automated digital image analysis for rapid and accurate
    measurement of leaf area. Appl. Plant Sci. 2, 1400033. doi: 10.3732/APPS.1400033
    CrossRef Full Text | Google Scholar Eitzinger, A. (2021). Data collection smart
    and simple: evaluation and metanalysis of call data from studies applying the
    5Q approach. Front. Sustain. Food Syst. 5. doi: 10.3389/fsufs.2021.727058 CrossRef
    Full Text | Google Scholar Elsayed, A. M., Saleh, E. I. (2018). Research data
    management and sharing among researchers in Arab universities: An exploratory
    study. IFLA J. 44, 281–299. doi: 10.1177/0340035218785196 CrossRef Full Text |
    Google Scholar Fahad, S. K. A., Yahya, A. E. (2018). “Big data visualization:
    allotting by R and python with GUI tools,” in 2018 International Conference on
    Smart Computing and Electronic Enterprise (ICSCEE). (Shah Alam, Malaysia: IEEE)
    2018, 1–8. doi: 10.1109/ICSCEE.2018.8538413 CrossRef Full Text | Google Scholar
    Gonzalez, E. M., Zarei, A., Hendler, N., Simmons, T., Zarei, A., Demieville, J.,
    et al. (2023). PhytoOracle: Scalable, modular phenomics data processing pipelines.
    Front. Plant Sci. 14. doi: 10.3389/fpls.2023.1112973 CrossRef Full Text | Google
    Scholar Herzog, R. (1998). PostgreSQL–the linux of databases. Linux J. 46, 1–es.
    doi: 10.5555/327239.327240 CrossRef Full Text | Google Scholar Hester, J., Wickham,
    H. (2023) odbc: Connect to ODBC Compatible Databases (using the DBI Interface)
    (R package version 1.3.4.). Available at: https://CRAN.R-project.org/package=odbc.
    Google Scholar Jain, A., Mahajan, N. (2017). “Introduction to Database as a Service,”
    in The Cloud DBA-Oracle: Managing Oracle Database in the Cloud. Eds. Jain, A.,
    Mahajan, N. (Berkeley, CA: Apress), 11–22. Available at: doi: 10.1007/978-1-4842-2635-3_2
    CrossRef Full Text | Google Scholar Krogh, J. W. (2018). “SQL Tables,” in MySQL
    Connector/Python Revealed: SQL and NoSQL Data Storage Using MySQL for Python Programmers.
    Ed. Krogh, J. W. (Berkeley, CA: Apress), 371–401. Available at: doi: 10.1007/978-1-4842-3694-9_8
    CrossRef Full Text | Google Scholar Machwitz, M., Pieruschka, R., Berger, K.,
    Schlerf, M., Aasen, H., Fahrner, S., et al. (2021). Bridging the gap between remote
    sensing and plant phenotyping—Challenges and opportunities for the next generation
    of sustainable agriculture. Front. Plant Sci. 12. doi: 10.3389/fpls.2021.749374
    PubMed Abstract | CrossRef Full Text | Google Scholar Malosetti, M., Bustos-Korts,
    D., Boer, M. P., van Eeuwijk, F. A. (2016). Predicting responses in multiple environments:
    issues in relation to genotype × Environment interactions. Crop Sci. 56, 2210–2222.
    doi: 10.2135/cropsci2015.05.0311 CrossRef Full Text | Google Scholar Morales,
    N., Ogbonna, A. C., Ellerbrock, B. J., Bauchet, G. J., Tantikanjana, T., Tecle,
    I. Y., et al. (2022). Breedbase: a digital ecosystem for modern plant breeding.
    G3 Genes|Genomes|Genetics 12, jkac078. doi: 10.1093/g3journal/jkac078 CrossRef
    Full Text | Google Scholar Mullan, D., Mullan, D. (2012). “Chlorophyll content,”
    in Physiological Breeding II: A Field Guide to Wheat Phenotyping. Eds. Pask, A.,
    Pietragalla, J., Mullan, D., Reynolds, M. (Mexico: CIMMYT), 41–43. Available at:
    https://repository.cimmyt.org/bitstream/handle/10883/1288/96144.pdf Google Scholar
    Munappy, A. R., Bosch, J., Olsson, H. H. (2020). “Data Pipeline Management in
    Practice: Challenges and Opportunities,” in Product-Focused Software Process Improvement.
    Eds. Morisio, M., Torchiano, M., Jedlitschka, A. (Cham, Switzerland: Springer
    International Publishing), 168–184. Google Scholar Neveu, P., Tireau, A., Hilgert,
    N., Nègre, V., Mineau-Cesari, J., Brichet, N., et al. (2019). Dealing with multi-source
    and multi-scale information in plant phenomics: the ontology-driven Phenotyping
    Hybrid Information System. New Phytol. 221, 588–601. doi: 10.1111/nph.15385 PubMed
    Abstract | CrossRef Full Text | Google Scholar Papoutsoglou, E. A., Faria, D.,
    Arend, D., Arnaud, E., Athanasiadis, I. N., Chaves, I., et al. (2020). Enabling
    reusability of plant phenomic datasets with MIAPPE 1.1. New Phytol. 227, 260–273.
    doi: 10.1111/nph.16544 PubMed Abstract | CrossRef Full Text | Google Scholar Pask,
    A., Pietragalla, J., Mullan, D., Reynolds, M. P. (Eds.) (2012). Physiological
    breeding II: a field guide to wheat phenotyping (Mexico, D.F.: CIMMYT). Available
    at: https://repository.cimmyt.org/bitstream/handle/10883/1288/96144.pdf Google
    Scholar Rahmany, M., Mohd Zin, A., Sundararajan, E. A. (2020). Comparing tools
    provided by python and r for exploratory data analysis. Int. J. Inf. System Comput.
    Science(IJISCS) 4, 131–142. doi: 10.56327/ijiscs.v4i3.933 CrossRef Full Text |
    Google Scholar R Core Team (2021). R: A language and environment for statistical
    computing (Vienna, Austria: R Foundation for Statistical Computing). Available
    at: https://www.R-project.org/. Google Scholar Reynolds, M., Chapman, S., Crespo-Herrera,
    L., Molero, G., Mondal, S., Pequeno, D. N. L., et al. (2020). Breeder friendly
    phenotyping. Plant Sci. 295, 110396. doi: 10.1016/j.plantsci.2019.110396 PubMed
    Abstract | CrossRef Full Text | Google Scholar Rife, T. W., Poland, J. A. (2014).
    Field book: an open‐Source application for field data collection on android. Crop
    Sci. 54, 1624–1627. doi: 10.2135/cropsci2013.08.0579 CrossRef Full Text | Google
    Scholar R Special Interest Group on Databases (R-SIG-DB), Wickham, H., Müller,
    K. (2022) DBI: R Database Interface (R package version 1.1.3). Available at: https://CRAN.R-project.org/package=DBI.
    Google Scholar Ting, T.-C., Souza, A., Imel, R. K., Guadagno, C. R., Hoagland,
    C., Yang, Y., et al. (2023). Quantifying physiological trait variation with automated
    hyperspectral imaging in rice. Front. Plant Sci. 14. doi: 10.3389/fpls.2023.1229161
    CrossRef Full Text | Google Scholar van den Brink, H., van der Leek, R., Visser,
    J. (2007). “Quality assessment for embedded SQL,” in Seventh IEEE International
    Working Conference on Source Code Analysis and Manipulation (SCAM 2007). (Paris,
    France: IEEE), 163–170. doi: 10.1109/SCAM.2007.23 CrossRef Full Text | Google
    Scholar Zadoks, J. C., Chang, T. T., Konnzak, C. F. (1974). A decimal code for
    growth stages in cereals. Weed Res. 14, 415–42. doi: 10.1111/j.1365-3180.1974.tb01084.x
    CrossRef Full Text | Google Scholar Keywords: data pipeline, extract-transform-load,
    database, data aggregation, data processing, plant phenotyping Citation: Vargas-Rojas
    L, Ting T-C, Rainey KM, Reynolds M and Wang DR (2024) AgTC and AgETL: open-source
    tools to enhance data collection and management for plant science research. Front.
    Plant Sci. 15:1265073. doi: 10.3389/fpls.2024.1265073 Received: 21 July 2023;
    Accepted: 30 January 2024; Published: 21 February 2024. Edited by: Jennifer Clarke,
    University of Nebraska-Lincoln, United States Reviewed by: Alan Cleary, National
    Center for Genome Resources, United States Geng (Frank) Bai, University of Nebraska
    - Lincoln, United States Copyright © 2024 Vargas-Rojas, Ting, Rainey, Reynolds
    and Wang. This is an open-access article distributed under the terms of the Creative
    Commons Attribution License (CC BY). The use, distribution or reproduction in
    other forums is permitted, provided the original author(s) and the copyright owner(s)
    are credited and that the original publication in this journal is cited, in accordance
    with accepted academic practice. No use, distribution or reproduction is permitted
    which does not comply with these terms. *Correspondence: Diane R. Wang, drwang@purdue.edu
    Disclaimer: All claims expressed in this article are solely those of the authors
    and do not necessarily represent those of their affiliated organizations, or those
    of the publisher, the editors and the reviewers. Any product that may be evaluated
    in this article or claim that may be made by its manufacturer is not guaranteed
    or endorsed by the publisher. Footer Guidelines Author guidelines Editor guidelines
    Policies and publication ethics Fee policy Explore Articles Research Topics Journals
    Outreach Frontiers Forum Frontiers Policy Labs Frontiers for Young Minds Connect
    Help center Emails and alerts Contact us Submit Career opportunities Follow us
    © 2024 Frontiers Media S.A. All rights reserved Privacy policy | Terms and conditions
    We use cookies Our website uses cookies that are necessary for its operation and
    other cookies to track its performance or to improve and personalize our services.
    To manage or reject non-essential cookies, please click \"Cookies Settings\".
    For more information on how we use cookies, please see ourCookie Policy Cookies
    Settings Accept Cookies"'
  inline_citation: '>'
  journal: Frontiers in Plant Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'AgTC and AgETL: open-source tools to enhance data collection and management
    for plant science research'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhou K.
  - Li Y.
  - Han X.
  citation_count: '0'
  description: 'With the advent of the information age, data visualization technology
    has gradually shown its unique features in various information fields, and its
    importance has gradually been attached importance by various governments and commercial
    departments. In teaching in our country, most schools simply use office software
    to create pie chart, histogram or table to realize visualization. This kind of
    teaching creates a single chart that doesn’t change at all. The content of the
    data visualization course of many school courses is outdated, and the way of visualization
    is not in line with the needs of The Times, that is, the content has been criticized
    as abstract, language mechanization, format and public culture [1]. In order to
    analyze the teaching quality and evaluate and improve it, this paper processes
    and analyzes the data exported from the teaching administration system based on
    python, mainly from three aspects: data acquisition, data processing and data
    analysis. Firstly, the python crawler technology is used to obtain students’ grades,
    secondly, the invalid data is processed, and finally, the matplotlib library is
    used to visualize the processed data, and the learning status of students in the
    class is analyzed and evaluated by combining the obtained images. Through the
    data processing of this paper, it realizes the hiding of the student’s name, protects
    the privacy of the student, and uses the graph to intuitively reflect the student’s
    grade distribution, which makes the grade analysis more convenient.'
  doi: 10.1007/978-3-031-50580-5_4
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Multimedia
    Technology and Enhanced Learning ICMTEL 2023: Multimedia Technology and Enhanced
    Learning pp 42–52Cite as Home Multimedia Technology and Enhanced Learning Conference
    paper Visualization Techniques for Analyzing Learning Effects – Taking Python
    as an Example Keshuang Zhou, Yuyang Li & Xue Han  Conference paper First Online:
    21 February 2024 38 Accesses Part of the book series: Lecture Notes of the Institute
    for Computer Sciences, Social Informatics and Telecommunications Engineering ((LNICST,volume
    535)) Abstract With the advent of the information age, data visualization technology
    has gradually shown its unique features in various information fields, and its
    importance has gradually been attached importance by various governments and commercial
    departments. In teaching in our country, most schools simply use office software
    to create pie chart, histogram or table to realize visualization. This kind of
    teaching creates a single chart that doesn’t change at all. The content of the
    data visualization course of many school courses is outdated, and the way of visualization
    is not in line with the needs of The Times, that is, the content has been criticized
    as abstract, language mechanization, format and public culture [1]. In order to
    analyze the teaching quality and evaluate and improve it, this paper processes
    and analyzes the data exported from the teaching administration system based on
    python, mainly from three aspects: data acquisition, data processing and data
    analysis. Firstly, the python crawler technology is used to obtain students’ grades,
    secondly, the invalid data is processed, and finally, the matplotlib library is
    used to visualize the processed data, and the learning status of students in the
    class is analyzed and evaluated by combining the obtained images. Through the
    data processing of this paper, it realizes the hiding of the student’s name, protects
    the privacy of the student, and uses the graph to intuitively reflect the student’s
    grade distribution, which makes the grade analysis more convenient. Keywords Artificial
    Intelligence Python Visualization Word Cloud Access provided by University of
    Nebraska-Lincoln. Download conference paper PDF 1 Introduction Since ancient times,
    China has always attached great importance to education. With the rapid development
    of artificial intelligence in today’s society, the idea that science and technology
    are the primary productive forces is fully implemented, based on education. In
    the daily learning process of students, students’ performance in class, study
    time and grades of online courses, as well as the score statistics of students’
    final exams all need to deal with a large amount of data. Examination is an important
    link in the teaching process, and also an important means to reflect the teaching
    effect. The analysis of examination results can not only reflect students’ learning
    attitude, learning effect and effort level, but also reflect teachers’ learning
    attitude, teaching method, teaching effect and teaching management level to a
    certain extent. Achievement analysis is an important part of teaching process
    [2]. At present, the most commonly used tools are the PivotTable function of Excel
    or mode, mean value and variance in SPSS to generate visual charts so as to visually
    reflect the strengths and weaknesses of different subjects and daily learning
    performance of different students, providing reference for students to adjust
    their learning status in the future. It also provides a guarantee for teachers
    and parents to have a clearer understanding of students’ learning status. However,
    once faced with a large number of data, Excel and SPSS will inevitably make people
    a little confused, still need a more convenient way to carry out data visualization
    analysis. In the face of massive data, how to analyze and mine the connotation
    of data and better present useful information from the data has become the main
    object of current research. Data visualization is an important means of data analysis
    and information presentation, and has also become the main tool of data research
    [3]. Python is a commonly used programming language in big data. This paper uses
    Matplotlib [4] and Numpy [5] libraries in Python [6] and takes the final grades
    of this class as an example for data analysis. In order to protect personal privacy,
    the names of students have been processed when taking data in this paper. 2 Current
    Situation of Analysis for Learning Effect In the current teaching environment,
    how to let students have more time to study independently through teaching reform
    has become a more concerned issue. In the face of the increasing amount of data,
    the traditional paper analysis method is obviously unable to meet the demand.
    So how do you turn data into useful information? Through the use of visualization
    technology, we can use visual charts, graphs and other forms to visually display
    the performance information. Visual chart analysis of grades is an important reflection
    of students’ learning results in the learning process, reflecting students’ mastery
    of knowledge and learning ability in a period of time. In many cases, students’
    grades are not particularly ideal, so for them, performance analysis is particularly
    important. In this way, students can be helped to self-reflect, so as to find
    their own problems in learning and make targeted adjustments. In daily learning,
    students can learn the differences between their weak subjects and strong subjects
    through various channels. But much of this information is indirectly obtained
    by teachers, which obviously can’t meet the needs of students. So visualization
    allows students to have a more intuitive understanding of subjects they are weak
    in or interested in, It not only provides reference for students’ subsequent development
    of emphasis, but also provides feedback for teachers and schools’ teaching strategies
    and methods, playing an extremely important role. Traditional analysis of grades
    wastes time and results are not clear enough. For example, when the author was
    in high school, he relied on hand-drawn line charts of grades of each subject,
    which could only reflect the progress trend of the grades of a single subject,
    and also wasted students’ time. Moreover, some teachers will use the data perspective
    function of Excel and SPSS in statistics to integrate and analyze data and generate
    visual charts. The efficiency of these two methods in the face of a large number
    of data is also evident. The use of technical means, such as Python, can be used
    to visualize the data, so that students can see their own shortcomings intuitively.
    Through review and consolidation, students can better grasp the content, so that
    teachers and schools can quickly grasp students’ understanding of knowledge, so
    as to timely adjust the teaching plan and make relevant teaching plans, so that
    parents can understand the learning effect of students more clearly. Thus more
    comprehensive grasp of the students’ basic situation. 3 Overview of Tools 3.1
    Python Python is a widely used object-oriented, interpreted, cross-platform computer
    programming language. It is an open source, free, and powerful language that provides
    a rich and powerful standard library, as well as a large number of third-party
    libraries. Functions of third-party libraries involve artificial intelligence,
    data analysis and processing, machine learning, Web application development and
    other fields. With the development of these fields in recent years, Python has
    become more and more popular. It has the advantages of clear and simple, high
    development efficiency, portability, expansibility, embeddability and so on. The
    open source and free features also make this language get the support and promotion
    of many users, and become a new choice for the development of data processing
    system. It is an open-source programming language developed with the community-based
    model. It’s free to use, and since it’s open-source supports multiple platforms
    and can be run on any environment. For example, 2D and 3D information visualization
    optimization libraries Matplotlib, Seaborn and Pandas, Folium, Basemap, MapBox,
    GeoPlotlib, PyechartsMap, etc. Information visualization management library of
    social Service Network networkX, Wordcloud, a library for information visibility
    optimization of dictionaries and cloud images, and WordCloud from Pyecharts [7].
    3.2 Matplotlib Matplotlib is an open source data visualization toolkit that is
    the most popular drawing library in Python for developers, and it is comprehensive
    and versatile. Matplotlib is Python’s 2D drawing library. It generates publical-quality
    graphics in a variety of hardcopy formats and cross-platform interactive environments,
    mainly for generating drawings and other visualizations of two-dimensional data.
    When using this module, it takes a few lines of code to generate the required
    curve chart, histogram, scatter chart, line chart, pie chart, bar chart, box chart,
    spectrum chart and radar chart. Not only can you draw 2D graphics, you can draw
    3D graphics, you can also draw animations. 3.3 Numpy As the most important basic
    package of Python numerical computation, Numpy is an open source numerical computation
    extension of Python and a basic module of data analysis and Kaohsiung scientific
    computation. It can be used to store and process large matrices. It not only supports
    a large number of dimensional arrays and matrix operations, but also provides
    many advanced numerical programming tools, such as matrix data type, vector processing
    and precise operation library. Designed for strict digital processing. 4 Data
    Visualization Analysis 4.1 Concept Visualization is a theory, method and technology
    that uses computer graphics and image processing technology to convert data into
    graphics or images and display them on the screen, and carry out interactive processing.
    With the development of digital multimedia technology, visualization technology
    has been widely used in engineering, medicine and other fields. This technology
    converts relevant information such as data into graphic images for display, which
    helps users understand the interrelationship and development trend between a large
    number of abstract data, and improves users’ ability to observe things [8]. Visualization
    tools are widely used in the visualization of graphic images [9], human-computer
    interaction [10], scientific calculation visualization [11] and other fields.
    It involves many fields such as computer graphics, image processing and computer
    vision. For example, some line charts, pie charts and bar charts in Excel can
    visually display data in the form of charts to help people better understand the
    meaning of data and make decisions about the next step. This is visualization.
    The chart below, for example, is the simplest data analysis (Fig. 1). Fig. 1.
    Simple line graph Full size image Through data visualization technology, text
    is converted into graphs. The line chart can intuitively see the sales trend of
    one year. The chart shows that the sales volume changes first, then rises, then
    rises to the highest value, and finally declines steadily. 4.2 Principle At present,
    we are in an era of data and information explosion. No matter when and where we
    are, we will inevitably face the situation of actively or passively receiving
    news and feedback. The human eye has powerful pattern recognition capabilities,
    and more than half of the human brain function can be used to process and feed
    backvisual information [12]. Compared with boring words and numbers, the human
    brain can be more intuitive and more specific to recognize elements such as graphics,
    colors, and sizes, and can discover the information contained in the data for
    the first time from the data visualization graphics [13]. Data visualization refers
    to the presentation of a large amount of relevant data in the form of images and
    charts, such as word cloud map, radar map, percentage pie chart, etc., which can
    integrate data and visually display graphic images [14]. These charts can communicate
    and communicate data clearly and effectively, and can analyze data from multiple
    dimensions to draw deep relevant conclusions. With graphical means, key contents
    and features can be conveyed intuitively [15]. Word cloud map consists of frequently
    appearing words, which are cloud-like color graphics, used to display a large
    amount of text data. For example, in the topic discussion in the general course
    of learning, word clouds will be automatically generated when students post their
    discussions. Words with multiple repetitions are in the center with dark colors
    and large size [16]. This intuitive visual effect enables teachers to understand
    the learning effect of students by observing the word cloud. As shown in the picture
    below (Fig. 2): Fig. 2. Topic discussion word cloud map Full size image The implementation
    code is: 4.3 Steps of Data Visualization Analysis Any data that can be expressed
    in charts and images can be analyzed visually. First of all, a large amount of
    relevant data needs to be acquired. There are many methods of data acquisition,
    such as questionnaire survey and interview, and crawler and other methods are
    generally adopted to acquire some open data [16]. Secondly, data screening and
    processing are carried out. The data we acquire may be messy, complex and repetitive
    [17]. For such low-quality data, data should be discarded to ensure the effectiveness
    and reliability of the data. The last is data visualization. This step is to express
    the processed data information in the form of graphics and images. At present,
    people generally use Excel, Python, Matlab and other software technologies to
    realize data visualization analysis [18]. 5 Data Visualization Analysis by Python
    First of all, climb from the teaching administration system of the final subject
    scores of the class, calculate the personal score, and then integrate into the
    class score, and finally the class score for visual analysis. 5.1 Obtain Data
    Using the third party library requests this web crawler tool, crawl the student
    achievement information on the educational administration system, just need to
    give the website and crawl rules, you can get a lot of information on the website.
    Based on the analysis of the scores of some students in this class, this paper
    obtains the score data of the basic course of the final examination in the second
    semester of the 2022–2023 academic year, and makes a statistical analysis of the
    information of the students’ course scores. The key indicators mainly include
    the number of students taking the exam, the passing rate of the paper scores,
    the passing rate of the overall score, the passing rate of the paper scores and
    the passing rate of the overall score [19]. The code is as follows: The result
    information is beautified by BeautifulSoup and saved in a local file, or exported
    to an Excel workbook for processing [20]. 5.2 Data Processing Check the obtained
    data, and process invalid duplicate data, modify, replace, or delete it directly
    [21]. Such programs would also help to reduce error rates by alleviating the burden
    of manual data processing that hampers the processing of large-scale actigraphy
    data sets[i]. For example, the data redundancy processing code is as follows.
    5.3 Statistical Processing The results of each subject are counted to obtain the
    total results of the individual and the class. The weighted average function can
    be used. The average score, the variance, the highest score and the number of
    students who failed in each subject of the class were calculated according to
    the student achievement [22]. 5.4 Visualization Matplotib library is a third-party
    library in python language [23]. The most basic charts in daily work include bar
    charts, bar charts, line charts, pie charts, etc. Matplotlib module provides corresponding
    drawing functions for these charts [24]. The data used to draw the chart can be
    directly used in the code provided. The bar chart is taken as an example to draw
    the performance analysis chart below [25]. The code is as follows: Lines 2 and
    3 give the values for the x and y axes of the chart, line 4 uses the bar() function
    to draw the bar graph, and line 5 uses the show() function to show the graph (Fig.
    3). Fig. 3. Bar chart of student achievement Full size image After running, we
    can intuitively see the distribution of students’ grades. In actual operation,
    we can draw a variety of graphs about grades by modifying some parameters. Teachers
    can also set up charts of different situations according to the specific needs
    of analysis, such as individual subject or total score change table (line chart),
    the percentage of grade in the total number of students in the class (pie chart)
    and so on [26] (Figs. 4 and 5). Fig. 4. Line chart of student achievement Full
    size image Fig. 5. Pie chart of student achievement Full size image It can be
    seen that python visualization technology has the advantages of processing a large
    amount of data and high reuse rate [27]. When the original data is changed, information
    can be automatically updated, which is convenient for the use and observation
    of data statisticians. Visualization technology based on big data mining existing
    mass data, extract valuable information, applied to class performance analysis
    and university teaching management, has become a hot practice, can play a positive
    role in improving the teaching effect [28]. 6 Conclusion With the advent of the
    era of big data, the development of cloud computing and the continuous progress
    of big data storage technology, data analysis and visualization analysis are becoming
    more and more important [29]. This paper emphasizes its application in the field
    of education. The visual analysis of student achievement data can be generated
    flexibly by using Python according to the results of part of the final examination
    of the author’s class. Some data can be visualized as a whole, can also be used
    to show some time series data, and even can be used to show some distribution
    rules, it is also very convenient to use, can be directly through a code to call
    another code to achieve, for students, schools, parents in the next deployment
    of various aspects to provide convenient conditions. In order to make targeted
    adjustments to the next way of learning and teaching. It can be seen that achievement
    analysis will continue to be an indispensable part of education in the future
    [30]. At the same time, we also hope that relevant researchers and scholars can
    continue to in-depth study on the aspect of performance analysis, and apply it
    to more aspects in the field of education. Education information has become the
    mainstream, many schools have their own campus system, performance analysis system,
    so as to use information management to make school management more scientific
    and effective. As a result, data analysis and visualization are becoming mainstream.
    In the future, data visualization will become more popular, which can not only
    reduce labor costs, but also improve the efficiency of analysis, making analysis
    more simple and convenient for students. I believe that visual analysis technology
    will become more and more popular in the near future. However, Python also has
    some shortcomings, such as Python syntax is relatively simple, resulting in python
    is not easy to cooperate with other languages [31]. Moreover, python is also based
    on object-oriented language, so it is difficult to support some complex models,
    these problems need to be perfected in the future, so Python is a technology that
    can continue to expand the application. References Fabry, D.L., Higgs, J.R.: Barriers
    to the effective use of technology in education: current status. J. Educ. Comput.
    Res. 17(4), 385–395 (1997) Article   Google Scholar   Seidel, T., Shavelson, R.J.:
    Teaching effectiveness research in the past decade: the role of theory and research
    design in disentangling meta-analysis results. Rev. Educ. Res. 77(4), 454–499
    (2007) Article   Google Scholar   Few, S.: Eenie, Meenie, Minie, Moe: selecting
    the right graphyou’re your message (2004) Google Scholar   Bisong, E.: Matplotlib
    and seaborn. In: Building Machine Learning and Deep Learning Models on Google
    Cloud Platform: A Comprehensive Guide for Beginners, pp. 151–165 (2019) Google
    Scholar   Bressert, E.: SciPy and NumPy: an overview for developers (2012) Google
    Scholar   Kelly, S.: What Is Python? Python, PyGame and Raspberry Pi Game Development,
    pp. 3–5 (2016) Google Scholar   Diehl, S.: Software Visualization: Visualizing
    the Structure, Behaviour, and Evolution of Software, pp. 11–18. Springer, Heidelberg
    (2007). https://doi.org/10.1007/978-3-540-46505-8 Telea, A.C.: Data Visualization:
    Principles and Practice, pp. 91–102. CRC Press, Boca Raton (2007) Book   Google
    Scholar   Shahin, M., Liang, P., Babar, M.A.: A systematic review of software
    architecture visualization techniques. J. Syst. Softw. 94(5), 161–185 (2014).
    https://doi.org/10.1016/j.jss.2014.03.071 Article   Google Scholar   Drevelle,
    V., Nicola, J.: VIBes: a visualizer for intervals and boxes. Math. Comput. Sci.
    8(3–4), 563–572 (2014). https://doi.org/10.1007/s11786-014-0202-0 Article   Google
    Scholar   Allen, F., Gale, D.: Limited market participation and volatility of
    asset prices. J. Am. Econ. Rev. 984 (1994) Google Scholar   Cao, S., Zeng, Y.,
    Yang, S., et al.: Research on Python data visualization technology. J. Phys. Conf.
    Ser. 1757(1), 012122 (2021). IOP Publishing Google Scholar   Hammad, G., Reyt,
    M., Beliy, N., et al.: PyActigraphy: open-source python package for actigraphy
    data visualization and analysis. PLoS Comput. Biol. 17(10), e1009514 (2021) Article   Google
    Scholar   Dennis, D.R., Meredith, J.R.: An analysis of process industry production
    and inventory management systems. J. Oper. Manag. (2000) Google Scholar   Sambasivam,
    S., Theodosopoulos, N.: Advanced data clustering methods of mining web documents.
    Issues Inf. Sci. Inf. Technol. (2006) Google Scholar   Freitas, C.M.D.S., et al.:
    On evaluating information visualization techniques. In: Proceedings of the Working
    Conference on Advanced Visual Interfaces (2002) Google Scholar   Wehrend, S.,
    Lewis, C.: A problem-oriented classification of visualization techniques. In:
    Proceedings of the First IEEE Conference on Visualization: Visualization90. IEEE
    (1990) Google Scholar   Chi, Ed.H.: A taxonomy of visualization techniques using
    the data state reference model. In: IEEE Symposium on Information Visualization
    2000. INFOVIS 2000. Proceedings. IEEE (2000) Google Scholar   Klerkx, J., Verbert,
    K., Duval, E.: Enhancing learning with visualization techniques. In: Spector,
    J., Merrill, M., Elen, J., Bishop, M. (eds.) Handbook of Research on Educational
    Communications and Technology, pp. 791–807. Springer, New York (2014). https://doi.org/10.1007/978-1-4614-3185-5_64
    Cao, N., Cui, W.: Overview of text visualization techniques. In: Cao, N., Cui,
    W. (eds.) Introduction to Text Visualization. Atlantis Briefs in Artificial Intelligence,
    vol. 1, pp. 11–40. Atlantis Press, Paris (2016). https://doi.org/10.2991/978-94-6239-186-4_2
    Keim, D.A., Kriegel, H.-P.: Visualization techniques for mining large databases:
    a comparison. IEEE Trans. Knowl. Data Eng. 8(6), 923–938 (1996) Google Scholar   Kamat,
    V.R., et al.: Research in visualization techniques for field construction. J.
    Constr. Eng. Manag. 137(10), 853–862 (2011) Google Scholar   Al-Kodmany, K.: Using
    visualization techniques for enhancing public participation in planning and design:
    process, implementation, and evaluation. Landsc. Urban Plann. 45(1), 37–45 (1999)
    Google Scholar   Kucher, K., Kerren, A.: Text visualization techniques: taxonomy,
    visual survey, and community insights. In: 2015 IEEE Pacific visualization symposium
    (pacificVis). IEEE (2015) Google Scholar   White, S., Feiner, S.: SiteLens: situated
    visualization techniques for urban site visits. In: Proceedings of the SIGCHI
    Conference on Human Factors in Computing Systems (2009) Google Scholar   Tatu,
    A., et al.: Combining automated analysis and visualization techniques for effective
    exploration of high-dimensional data. In: 2009 IEEE Symposium on Visual Analytics
    Science and Technology. IEEE (2009) Google Scholar   Zammitto, V.: Visualization
    techniques in video games. Electron. Vis. Arts (EVA 2008), 267–276 (2008) Google
    Scholar   Vallat, R.: Pingouin: statistics in Python. J. Open Sour. Softw. 3(31),
    1026 (2018) Google Scholar   Sousa da Silva, A.W., Vranken, W.F.: ACPYPE-Antechamber
    python parser interface. BMC Res. Notes 5, 1–8 (2012) Google Scholar   Millman,
    K.J., Aivazis, M.: Python for scientists and engineers. Comput. Sci. Eng. 13(2),
    9–12 (2011) Google Scholar   Ari, N., Ustazhanov, M.: Matplotlib in Python. In:
    2014 11th International Conference on Electronics, Computer and Computation (ICECCO).
    IEEE (2014) Google Scholar   Download references Funding This paper is supported
    by the Fund for Philosophy and Social Sciences of Universities in Jiangsu Province,
    China, “Research on Integrated Education Based on the Decentralization of Blockchain
    Technology” (2019SJA0543). Author information Authors and Affiliations Nanjing
    Normal University of Special Education, Nanjing, 210000, Jiangsu, China Keshuang
    Zhou, Yuyang Li & Xue Han Corresponding author Correspondence to Xue Han . Editor
    information Editors and Affiliations Nanjing Normal University of Special Education,
    Nanjing, China Bing Wang Nanjing Normal University of Special Education, Nanjing,
    China Zuojin Hu Nanjing Normal University of Special Education, Nanjing, China
    Xianwei Jiang University of Leicester, Leicester, UK Yu-Dong Zhang Rights and
    permissions Reprints and permissions Copyright information © 2024 ICST Institute
    for Computer Sciences, Social Informatics and Telecommunications Engineering About
    this paper Cite this paper Zhou, K., Li, Y., Han, X. (2024). Visualization Techniques
    for Analyzing Learning Effects – Taking Python as an Example. In: Wang, B., Hu,
    Z., Jiang, X., Zhang, YD. (eds) Multimedia Technology and Enhanced Learning. ICMTEL
    2023. Lecture Notes of the Institute for Computer Sciences, Social Informatics
    and Telecommunications Engineering, vol 535. Springer, Cham. https://doi.org/10.1007/978-3-031-50580-5_4
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-50580-5_4
    Published 21 February 2024 Publisher Name Springer, Cham Print ISBN 978-3-031-50579-9
    Online ISBN 978-3-031-50580-5 eBook Packages Computer Science Computer Science
    (R0) Share this paper Anyone you share the following link with will be able to
    read this content: Get shareable link Provided by the Springer Nature SharedIt
    content-sharing initiative Publish with us Policies and ethics Download book PDF
    Download book EPUB Sections Figures References Abstract Introduction Current Situation
    of Analysis for Learning Effect Overview of Tools Data Visualization Analysis
    Data Visualization Analysis by Python Conclusion References Funding Author information
    Editor information Rights and permissions Copyright information About this paper
    Publish with us Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes of the Institute for Computer Sciences, Social-Informatics
    and Telecommunications Engineering, LNICST
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Visualization Techniques for Analyzing Learning Effects – Taking Python as
    an Example
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kumar S.
  - Datta S.
  - Singh V.
  - Singh S.K.
  - Sharma R.
  citation_count: '0'
  description: Artificial intelligence (AI) systems are trained to solve complex problems
    and learn to perform specific tasks by using large volumes of data, such as prediction,
    classification, recognition, decision-making, etc. In the past three decades,
    AI research has focused mostly on the model-centric approach compared to the data-centric
    approach. In the model-centric approach, the focus is to improve the code or model
    architecture to enhance performance, whereas in data-centric AI, the focus is
    to improve the dataset to enhance performance. Data is food for AI. As a result,
    there has been a recent push in the AI community toward data-centric AI from model-centric
    AI. This paper provides a comprehensive and critical analysis of the current state
    of research in data-centric AI, presenting insights into the latest developments
    in this rapidly evolving field. By emphasizing the importance of data in AI, the
    paper identifies the key challenges and opportunities that must be addressed to
    improve the effectiveness of AI systems. Finally, this paper gives some recommendations
    for research opportunities in data-centric AI.
  doi: 10.1109/ACCESS.2024.3369417
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 12 Opportunities
    and Challenges in Data-Centric AI Publisher: IEEE Cite This PDF Sushant Kumar;
    Sumit Datta; Vishakha Singh; Sanjay Kumar Singh; Ritesh Sharma All Authors 281
    Full Text Views Open Access Comment(s) Under a Creative Commons License Abstract
    Document Sections I. Introduction II. AI Systems III. Model-Centric AI IV. Data-Centric
    AI V. Data Development Show Full Outline Authors Figures References Keywords Metrics
    Footnotes Abstract: Artificial intelligence (AI) systems are trained to solve
    complex problems and learn to perform specific tasks by using large volumes of
    data, such as prediction, classification, recognition, decision-making, etc. In
    the past three decades, AI research has focused mostly on the model-centric approach
    compared to the data-centric approach. In the model-centric approach, the focus
    is to improve the code or model architecture to enhance performance, whereas in
    data-centric AI, the focus is to improve the dataset to enhance performance. Data
    is food for AI. As a result, there has been a recent push in the AI community
    toward data-centric AI from model-centric AI. This paper provides a comprehensive
    and critical analysis of the current state of research in data-centric AI, presenting
    insights into the latest developments in this rapidly evolving field. By emphasizing
    the importance of data in AI, the paper identifies the key challenges and opportunities
    that must be addressed to improve the effectiveness of AI systems. Finally, this
    paper gives some recommendations for research opportunities in data-centric AI.
    General architecture of model-centric AI, data-centric AI, model data-centric
    AI Published in: IEEE Access ( Volume: 12) Page(s): 33173 - 33189 Date of Publication:
    23 February 2024 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2024.3369417 Publisher:
    IEEE Funding Agency: CCBY - IEEE is not the copyright holder of this material.
    Please follow the instructions via https://creativecommons.org/licenses/by/4.0/
    to obtain full-text articles and stipulations in the API documentation. SECTION
    I. Introduction Artificial intelligence (AI) models are used in almost all industries
    today, including automotive, agriculture, healthcare, financial, and semiconductor
    industries [1], [2], [3]. Earlier, the core of AI was machine learning (ML) algorithms.
    But when AI spread across many sectors and grew in demand, ML algorithms became
    a commodity and no longer the core of AI. Instead, the training data would drive
    greater performance in ML models and systems [4]. “Data is the new oil” was the
    phrase first used by the data scientist Clive Humby in 2006 [5]. People frequently
    believe that data is static, a significant factor in ML models becoming commonplace.
    The data literally means “that which is supplied,” which is true. ML entails downloading
    a readily available dataset and creating a model for most individuals. Afterward,
    efforts were directed to towards enhancing the model’s performance. We refer to
    data in real-time business as the result of the processes. Compared to actual
    model construction, maintaining and strengthening the deployed models accounts
    for a larger share of the cost and efficacy of many real-time ML systems. The
    main challenge for these applications is to improve the data quality [6]. In practical
    AI applications, 80% of the activities involve data cleansing and preparation.
    Therefore, an essential duty for an ML engineer to consider is assuring the data
    quality [7]. Data fluctuates significantly for higher-stakes AI. This data flow
    has a longer duration and various unfavorable effects. From data collection through
    model deployment, data movement can occur anywhere [8]. Conventional methods entirely
    dependent on code are overshadowed by modern-day AI systems, which consider both
    code and data. Practitioners strive to work on the model/code to improve the AI
    system instead of focusing more on the data part of the system [4]. However, improving
    the input data in actual applications is often preferable to experimenting with
    various algorithms. In the recent publications, 99% of papers have a model-centric
    focus, while only 1% are data-centric.1 It can be easily comprehend from the Table
    1 that the performance improvements have primarily been achieved through a data-centric
    approach. It appears that the model-centric approach did not result in any significant
    improvements over the baseline. Despite this, it is likely that practitioners
    invested a considerable amount of time, possibly hundreds of hours, in developing
    and refining these algorithms. The case studies in AI, Computer Vision, and other
    fields showed how enhancing the data will result in a better model [9]. AI systems
    that are model-centric focus on how to alter the algorithm/code to improve performance.
    Data-centric AI’s prime focus is to systematically amend the data to achieve a
    model with better performance. Model-centric and data-centric methods may be balanced
    well to provide a robust AI solution [10]. The contribution of the work can be
    summarised as follows: Connecting various data-centric AI techniques and related
    topics together at a high level with a focus on recent and significant developments.
    A comprehensive review of data-centric AI techniques. Describe the challenges
    and solutions of the data-centric AI approaches. Finally, we also give a few recommendations
    for research opportunities in data-centric AI TABLE 1 Data-Centric AI vs Model-Centric
    AI The rest of this paper is organized as follows: Section II provides brief highlights
    of AI systems. An overview and limitations of model-centric AI are presented in
    Section III, while Section IV discusses the overview, mathematical model, advantages,
    and exploratory data analysis of data-centric AI. A summary of representative
    tasks and methods for training data development is provided in Section V. An overview
    of related work on data-centric AI is given in Section VI. The challenges, solutions,
    and research direction are outlined in Section VII. A discussion on the merits
    and limitations of the existing art, along with a few new perspectives, can be
    found in Section VIII. Finally, the concluding remarks of this paper are presented
    in Section IX. SECTION II. AI Systems Standard AI systems can be represented as:
    AI System=Algorithm+Data. (1) View Source For an optimal operation, an AI system
    requires a balance of both model-centric and data-centric approaches for effective
    solutions. Model-centric focuses on developing algorithms, while data-centric
    focuses on the quality and relevance of the data used to train these algorithms.
    Both are important and need to be considered together for successful AI implementation.
    The data-centric approach involves addressing the challenges of the completeness,
    relevance, consistency, and heterogeneity of the data. It is an iterative process
    that must be integrated with the model-centric approach to achieve an AI system
    that effectively leverages the code and data. These models can range from data
    classification and clustering to recommendation systems and natural language processing.
    AI algorithms such as deep learning, reinforcement learning, and decision trees
    can be used to process large amounts of data and make predictions or decisions
    based on that data. The use of AI in data-centric models helps to uncover hidden
    patterns, relationships, and insights in data, enabling organizations to make
    data-driven decisions and achieve better outcomes. The integration of AI in data-centric
    models has the potential to revolutionize various industries, including healthcare,
    finance, retail, and many others [3], [11], [12]. SECTION III. Model-Centric AI
    Model-centric methods often follow a structured sequence during production that
    involves scoping the project, data collection and augmentation, data storage,
    data cleaning, data visualization, visual analytics, model construction and training,
    and finally, model evaluation [13]. Fig. 1 shows the production workflow of the
    model-centric approach starting from data collection to model deployment. In these
    situations, the development and improvement of the model algorithms typically
    receive most of the attention, with data engineering being a one-time assignment.
    Modern approaches also tend to analyze biases and fairness, focusing on loss functions
    such as cross-entropy, errors in the mean square and mean absolute percentages,
    etc. In traditional ML production environments, a stronger emphasis is given to
    the code rather than the resultant trained model to improve its style, readability,
    and unambiguity. Since the models are to be trained with new data continuously,
    it necessitates a broader sense of the distinctions between a data scientist and
    ML engineer to corrode gradually, with all collaborators developing general expertise
    on skills and best practices for all the domains involved. Even though model training
    is essential, the development of automated machine learning (AutoML) systems has
    resulted in a progressive decline in the requisite amount of human intervention
    [14]. The model-centric design, also known as the application-centric, depending
    on the use case, typically necessitates minimal work on the side of the data engineer
    due to the conventional one-time nature of data acquisition and pre-processing.
    However, data collection and model training were to become a continuous process
    where subsequent semi-supervised models learn from the same data they collect.
    In that instance, the workforce would be directed towards constant labelling and
    augmentation of collected data, and a movement toward data-centric concepts would
    occur. FIGURE 1. Workflow in model-centric systems. Show All A. Limitations Model-centric
    AI is predicated on ML approaches that prioritize improving model architectures
    (algorithm/code) and the underlying hyper-parameters. In this method, data is
    created once and is maintained throughout the development of the AI system. Model-centric
    AI has been successful over the past few decades, yet it has some flaws, as shown
    in Fig. 2. It particularly thrives in organizations and sectors when there are
    customer platforms with millions of users free to depend on generic fixes. In
    these conditions, most consumers would be satisfied by a single AI system; nonetheless,
    outliers would be practically useless. Examples of such organizations and sectors
    include the advertising sector, where firms like Google, Baidu, Amazon, and Facebook
    can access vast amounts of data (sometimes in a standardized format), which they
    may use to build model-centric AI systems. Standardized solutions like those offered
    by a single AI system cannot be used in sectors like manufacturing, agriculture,
    or healthcare, where customized solutions are preferred versus one-size-fits-all
    recipes. Instead, they should conceive their strategy to ensure that their algorithms
    learns what it needs to learn from having complete data that includes all crucial
    cases and is labelled consistently. FIGURE 2. Shortcomings of model-centric systems.
    Show All The concept centers on the prevalence of extensive and dynamic datasets,
    particularly evident in the advertising sector. In such contexts, a ‘one-size-fits-all’
    model consistently delivers satisfactory outcomes by mitigating the impact of
    outliers. However, this strategy proves impractical when dealing with small datasets.
    The flaw or limitation does not lie inherently within the model-centric approach
    itself but rather in recognizing that an effective AI application requires both
    a algorithm and a dataset. Consequently, any AI solution becomes “narrowly applicable”
    if the curation of the dataset does not align with the concurrent development
    of the model. This viewpoint, advocating the fusion of model-centric and data-centric
    approaches, is explicitly proposed in [15] as a crucial and optimal strategy to
    enhance AI systems. The suggestion is to refrain from a binary choice between
    deploying a model-centric or data-centric approach, emphasizing the necessity
    of synergy between the two for a more comprehensive and effective AI solution.
    Due to the rapid pace of innovation in today’s technology-driven world, AI models
    (algorithms) that are modelled may quickly become outdated and require retraining
    on new data, primarily because significant trends’ unforeseen or unexpected behaviour
    may outpace the model’s usefulness and relevance. Such model-centric AI’s capabilities
    are severely constrained in fields where the model itself is created to support
    ongoing scientific study or as a component of a more complex model. As a result,
    training data is produced using a smaller model based on the restricted findings
    and hypotheses. Typically, data collection is not practicable in these situations,
    or work involves limited data from expensive datasets. In such cases, the shortcomings
    of model-centric technologies become apparent since training a model-centered
    algorithm may result in biased findings, mainly if the initial model used to generate
    the data was built on incomplete knowledge or under a lack of domain expertise.
    With general trends in deep learning requiring a focus on large amounts of data,
    there is a general tendency to prefer reusing existing models to fit into our
    use cases with specific training data. Such a shift is perceived as a step towards
    the data-centric movement, where the model is fixed, with the only variable inputs
    being the training data and classifiers. SECTION IV. Data-Centric AI Data-centric
    AI is a data-engineering strategy that improves an AI system’s performance by
    methodically boosting the data quality used to train the underlying model under
    mutually exclusive yet collectively exhaustive methodologies and categories. Data-centric
    AI can improve the performance of AI models and services through augmentation,
    extrapolation, and interpolation. Data-centric AI can assist in making AI services
    more accurate and dependable by expanding the data that is accessible to them
    and enabling them to use it more efficiently [16]. With the help of training data
    from many sources, including synthetic data, public datasets, and private datasets,
    data-centric AI is created utilizing this innovative methodology. This strategy
    can lessen the time and effort needed to generate training data while also helping
    to increase the data quality. It can also help increase the effectiveness with
    which AI services use training data. Additionally, data-centric AI can process
    additional data sets because the data is personalized. This means that regardless
    of the magnitude of the dataset, data-centric AI can analyze and learn from it
    and make decent predictions. A. Mathematical Model The mathematical model of data-centric
    AI is a model that allows the system to learn from data and make predictions.
    This model can take various forms, such as decision trees, linear regression,
    neural networks, or deep learning models. The choice of model depends on the problem
    domain and the type of data available. The model is trained on a labeled dataset,
    where the input data is associated with the corresponding output or label. The
    training process involves minimizing a loss function that measures the difference
    between the predicted output and the actual label. The goal is to find the model
    parameters that minimize the loss function and generalize well to unseen data.
    Once the model is trained, it can be used to make predictions on new input data.
    The prediction process involves feeding the input data into the model, which outputs
    a predicted label or value. The prediction accuracy depends on the quality of
    the model and the amount and quality of the input data. Given a dataset D=( x
    1 , y 1 ),( x 2 , y 2 ),…,( x n , y n ) consisting of n samples, where x i ϵX
    denotes the feature vector and y i ϵY denotes the corresponding label, the goal
    of data-centric AI is to learn a function f:XϵY that maps each input feature vector
    x to its corresponding output label y . Define a model function f(X;Θ) that takes
    the input data X and a set of learnable parameters Θ and outputs a prediction.
    This function f is learned by optimizing a loss function L(Y,f(X;Θ)) , which measures
    the discrepancy between the predicted outputs f( x i ) and the true labels y i
    for each sample in the dataset. Mathematically, finding the set of parameters
    Θ that minimizes the loss function can be expressed as: Θ ∗ = argmin Θ L(Y,f(X;Θ))
    = argmin Θ 1 N ∑ i=1 N L( y i ,f( x i ;Θ)) (2) (3) View Source This optimization
    problem is typically solved using techniques from optimization theory, such as
    gradient descent or stochastic gradient descent. Where Θ ∗ is the optimal set
    of parameters that minimize the loss function. The optimization problem in the
    equation is typically solved using an iterative algorithm such as gradient descent.
    At each iteration t , the parameters Θ t are updated using the gradient of the
    loss function with respect to Θ : Θ t+1 = Θ t −α▽ΘL(Y,f(X; Θ t )) (4) View Source
    where α is the learning rate that determines the step size of the update, and
    ▽ΘL(Y,f(X; Θ t )) is the gradient of the loss function with respect to Θ at iteration
    t . The iterative process continues until convergence, i.e., until the change
    in the loss function between successive iterations falls below a certain threshold.
    Once the optimal set of parameters Θ ∗ is found, the model function f(X; Θ ∗ )
    can be used to make predictions on new input data. Justifications: The model is
    based on the premise that the performance of an AI system is dependent on the
    quality of the data on which it is trained. This is reflected in the model through
    the use of a data-centric loss function, which prioritizes the reduction of errors
    in the training data over the complexity of the model. The mathematical justification
    for this approach can be found in the field of statistical learning theory, which
    provides a framework for understanding the relationship between the complexity
    of a model and its ability to generalize to new data. The theory suggests that
    simpler models are more likely to generalize well, as they are less likely to
    overfit the training data. The data-centric model also incorporates techniques
    such as regularization and early stopping to prevent overfitting and improve the
    generalization performance of the model. These techniques have been extensively
    studied and validated in the literature, providing further mathematical justification
    for their use in the model. Furthermore, the model emphasizes the importance of
    data pre-processing and feature engineering, which can have a significant impact
    on the performance of an AI system. This is supported by research in the field
    of ML, which has shown that carefully selecting and preprocessing features can
    improve the accuracy of a model. B. Advantages Data-centric AI is not confined
    to a particular data type, it can learn from text, images, audio, and video. A
    data-centric AI approach typically involves utilizing the proper labels, addressing
    any issues, removing noisy data inconsistencies, enhancing data through data augmentation
    and feature engineering, analyzing errors with the help of domain experts to determine
    the accuracy or error in data points, and so on [17], [18], [19]. Data-centric
    AI constantly evaluates the created AI model in conjunction with updated data.
    Typically, an AI model is trained on a dataset only once during the production
    stage, before the software development process can be finalized with the deployment
    of the model and its necessary features. However, the underlying model may encounter
    edge-case instances of data points that differ significantly from those encountered
    during the training phase. This is anticipated, as the workflow of data-centric
    AI involves continuous improvement of data, particularly in industries that cannot
    afford to have a large amount of data points, such as manufacturing, agriculture,
    and healthcare [20]. As a result, evaluating the model’s quality would also happen
    more regularly than only once. A model would be able to recognize, judge, and
    then answer appropriately to variational data distributions owing to the production
    systems’ capacity to offer rapid feedback. In fact, this ability gives data-centric
    approaches a competitive advantage over their model-centric counterparts. The
    advantages of data-centric systems are highlighted in Fig. 3. FIGURE 3. Advantages
    in data-centric systems. Show All The ever-arising challenges and problems in
    today’s world require continuous optimization and tuning of the model, along with
    simultaneous collection, processing, augmentation, and labelling of high-volume
    data. In cases of filter-list-based blocking/moderation of social media content,
    restriction of cyber-threats, fraud detection, and spam tracking, a shift from
    a data-driven or application-centric to a data-centric perspective focusing on
    labelling and cleaning of data is apparent, with the information being collected
    in high frequency, at an hourly basis or even faster. The boundaries between business
    and technology are vanishing, with tools and techniques like ML and DL requiring
    assistance from domain experts and consultants to modify inputs or generate better
    algorithms. DL-based approaches have become popular owing to the enormous scope
    and capability for collecting, storing, and processing Big Data, mainly due to
    their excellent performance with big data and technological advances [21]. In
    deep-learning-based applications, the distinguishing hierarchy and structure of
    features or parameters are learned from the data. At the same time, they are usually
    coded by a human domain expert in typical machine learning applications. Then,
    through algorithms such as gradient descent and backpropagation, the deep learning
    algorithm learns and fits itself for accuracy. This methodology allows for mimicking
    the human brain at a primitive level, allowing DL models to make predictions more
    precisely through a combination of weights, inputs, and biases. Since a massive
    volume of data is processed through multiple layers of neural networks, the aspect
    of clean, labelled information becomes vital, as the presence of dirty, repeated,
    inconsistent data can cause unnatural biases, failure in edge cases, erroneous
    predictions, the poor performance of the model, wasteful computations, etc. Most
    real-world datasets are noisy, unstructured, and unorganized, with several biases,
    outliers, missing values, repeated values, etc. C. Significance Model-centric
    AI assumes ML solutions that mainly focus on optimizing model architectures (algorithm/code)
    along with the underlying hyperparameters. Data within this approach is created
    only once and kept the same over the AI system’s development life cycle. Model-centric
    AI fails in many real-world scenarios where the data continuously improves, like
    in drug discovery, the drug-related data keeps generating as the new disease evolves.
    In such a situation, the model built using the model-centric approach fails as
    the data points it encounters are entirely different from those encountered during
    the training phase. The ignorance of updated data by model-centric AI also prohibits
    it from being generalized across datasets and also makes it susceptible to adversarial
    samples [13], [15]. In contrast, Data-centric AI considers ML solutions that emphasize
    continuous improvement of data and evaluation of the developed AI model in conjunction
    with data updates throughout the lifecycle of an AI project [22]. As a result,
    it can adapt to ever-changing real-world situations. D. Execution and Analysis
    Data preparation is a notable example of the tedious step of the ML lifecycle.
    Since data quality directly affects a model’s quality, it is also one of the most
    crucial processes. This section will discuss the significance of exploratory data
    analysis (EDA), data visualization, and other tools for preparing data for ML
    pipelines and identifying data quality problems [23]. Data scientists examine
    and glean essential insights from the data using EDA techniques. Additionally,
    efficient EDA dramatically benefits from the talents and subject expertise of
    data scientists in this area. To encourage more statisticians, particularly academics,
    to research a wide range of fascinating difficulties, we present the traditional
    yet current subject of data quality from a statistical perspective. The data quality
    landscape is discussed along with the research underpinnings in computer science,
    overall quality management, and statistics. The use of two case studies based
    on an EDA approach to data quality motivates a collection of research questions
    for statistics that cover theory, methodology, and software tools. Data visualization
    is a crucial EDA approach that uses visual elements like charts and graphs to
    make analysis simple and efficient [24]. When it comes to data quality profiling,
    visual EDA is very pertinent. With visual features like charts and graphs, data
    visualization is a crucial EDA technique that simplifies and streamlines analysis.
    Visual EDA is especially pertinent in the context of data quality profiling. To
    investigate and summarise multiple data sets, data scientists use exploratory
    data analysis (EDA), which typically employs data visualization tools. Figuring
    out how to alter data sources to obtain the required answers, makes it easier
    for data scientists to detect trends, spot anomalies, test hypotheses, or validate
    assumptions. EDA aids in comprehending the variables used in data collecting and
    how they relate. Typically, it is used to look into what information the data
    might reveal outside of the formal modelling or hypothesis testing assignment.
    It can also assist you in determining the suitability of the statistical methods
    you’re considering using for data analysis. John Tukey, an American mathematician,
    developed EDA methods, which are still extensively used in the data discovery
    process. EDA’s main objective is to help with data analysis before making any
    assumptions. It can help with identifying obvious errors, better-comprehending
    data patterns, identifying outliers or unexpected events, and identifying fascinating
    relationships between the variables. Data scientists can make sure their findings
    are accurate and pertinent to any targeted business objectives by using exploratory
    analysis. EDA assists stakeholders by making sure they are asking the right questions.
    EDA can help in answering inquiries about standard deviations, categorical notations,
    and confidence intervals. In [25], authors present unifying principles offered
    by the categorical notion of data and discuss the importance of these principles
    in data-centric AI transition. SECTION V. Data Development Approximately 45% of
    a data scientist’s time is spent on data preparation tasks including importing
    and cleaning data, according to an Anaconda survey of data scientists shown in
    Figure 4.2 The quality and quantity of training data are pivotal for machine learning
    model performance. A summary of representative tasks and methods for training
    data development are given in Table 2. TABLE 2 Representative Tasks and Methods
    for Training Data Development for Data-Centric AI Applications TABLE 3 A Brief
    Summary of Existing Literature in Data-Centric AI FIGURE 4. Time invested by data
    scientists in various phases. Show All A. Data Collection Data collection is a
    crucial step in AI as it forms the foundation for training machine learning models.
    It involves three main approaches: data acquisition, data labeling, and improving
    existing data and models. Data acquisition involves finding or generating new
    datasets, data labeling involves adding annotations to enable learning, and improving
    existing data and models involves modifying them to increase accuracy and usefulness.
    The quality and relevance of collected data directly impact the accuracy and usefulness
    of AI models. 1) Data Acquisition When there is a lack of data, data acquisition
    can be performed to find suitable datasets for training machine learning models.
    There are three approaches for data acquisition: data discovery, data augmentation,
    and data generation. Data discovery involves indexing and searching for datasets,
    data augmentation involves creating synthetic examples by distorting or combining
    labeled examples, and data generation involves creating datasets through crowdsourcing
    or synthetic data generation techniques. Data discovery refers to the process
    of identifying and locating relevant data sources to support decision-making and
    analysis. The goal is to make it easier for organizations to find and use the
    data they need, by indexing large datasets stored in data lakes [26] or searching
    for data on the web [27]. This can help organizations to improve the efficiency
    and effectiveness of their data-driven activities. The Goods system [67] and Google
    Dataset Search [28] are examples of data discovery tools that support searching
    for datasets. These tools have become more interactive, with systems like Juneau
    [68] providing interactive data search and management. Juneau uses similarity
    measures, provenance information, and schema information to find related tables,
    which is a critical task inefficiently joining or unifying tables in data lakes.
    LSH-based algorithms that perform set overlap search or unionable attribute retrieval
    have been proposed to solve this challenge [69]. Data augmentation is a popular
    technique in machine learning to generate new training data to increase the diversity
    of data used for training. Generative Adversarial Networks (GANs) [70], [71] are
    commonly used for this purpose. They consist of a generator and a discriminator,
    which are trained in an adversarial fashion to generate fake data that is similar
    to the real data. However, GANs have limitations and other techniques have been
    developed to complement them, such as policies [72] and AutoAugment [30], where
    transformations are applied to data by a controller. Another popular technique
    is Mixup [31], [73], [74], [75], [76], which mixes pairs of data points of different
    classes to create additional data that regularizes the model. Model patching [77]
    is another method that uses GANs to augment the data for specific subgroups of
    a class to improve the model’s accuracy. Another way to acquire new data is by
    generating it through crowdsourcing platforms such as Amazon Mechanical Turk [78],
    where people are paid to create or find data for specific tasks. Data can also
    be generated through simulators or generators specific to a certain domain, such
    as Hermoupolis [32] for mobility data or Crash to Not Crash [79] for driving data.
    Domain randomization [33], [80] is a technique that can be used to generate a
    variety of realistic data by varying the parameters of a simulator. GANs can also
    be used to generate new data, but they require a sufficient amount of real data
    for training. 2) Data Labeling Data labeling is the process of annotating or categorizing
    data to provide it with a label or class. This is necessary for training supervised
    machine learning models, where the model needs to learn to make predictions based
    on labeled data. Data labeling can be a time-consuming and labor-intensive process,
    but it is crucial for ensuring the quality and accuracy of the trained models.
    There are several approaches to data labeling, including manual labeling by human
    annotators, automated labeling using heuristics or pre-trained models, and crowdsourced
    labeling, where a large number of people can label the data through a platform.
    The choice of labeling method depends on the type and amount of data, the desired
    level of accuracy, and the budget and time constraints. Data labeling is a crucial
    step in training machine learning models, where the goal is to assign labels to
    examples in the dataset. The labeling process can be performed using various methods,
    including: Semi-supervised learning [34], where existing labels are used to predict
    the other labels. Crowdsourcing platforms like Amazon Mechanical Turk, where labelers
    are recruited to label the data. Domain experts, who provide labels based on their
    expertise but can be expensive. Active learning [81], which reduces the crowdsourcing
    cost by asking labelers to label uncertain examples that will improve model accuracy
    the most Weak supervision, where labels are generated semi-automatically, for
    example, by using external knowledge bases or data programming techniques like
    Snorkel [37], [38] or Snuba [39]. 3) Improving Existing Data Improving existing
    data and labels is a useful approach in scenarios where there are no relevant
    datasets available externally and collecting more data no longer increases the
    model’s precision. Re-labeling [40] is an effective approach to improve the quality
    of the labels and Can be accomplished through the collection of majority opinions
    on multiple labels per instance. Data validation, cleaning, and integration can
    also help improve the quality of existing data. B. Data Cleaning and Validation
    Data cleaning is an important step in preparing data for machine learning. However,
    simply fixing well-defined errors in the data may not necessarily lead to an improvement
    in the accuracy of the machine learning models [49], [82]. Improving the accuracy
    of the model and making the training process resistant to noise in the data is
    more effectively achieved through directly cleaning the data [83], [84]. A training
    process that is resistant to noise is deemed to be more effective than cleaning
    the data prior to model training [82]. Data with malicious intent, known as adversarial
    data noise, can also be present, and addressing it through cleaning is referred
    to as data sanitization. Incorporating AI ethics such as model fairness [85] is
    also an important consideration in data preparation as biased data can lead to
    discriminatory models. Platforms like TensorFlow Extended (TFX) [86], which specializes
    in machine learning, possess data validation [87] components that allow for the
    early detection of data errors. 1) Data Validation Data visualization is an effective
    way of validating data for machine learning [87]. Visualization tools like Facets
    [88] allow for quick checks on data to prevent errors downstream. Research has
    also been conducted on the automatic creation of visualizations, such as the SeeDB
    [41] framework that uses a deviation-based metric to determine interestingness.
    However, automatic generation can lead to false positive results, which is why
    there is also a body of research that concentrates on techniques for controlling
    false discoveries. Schema-based validation [86], [87] like TensorFlow Data Validation
    (TFDV) [43], [44] is widely used in practice. It creates a data schema from prior
    data sets, utilizing it to validate future datasets and informing users of any
    anomalies that are detected. Data validation systems are becoming increasingly
    equipped with additional capabilities like declarative data quality constraints,
    pipeline inspection, automatic error identification, and ease of usage. Deequ
    [45], [46] is a library that allows you to define data quality constraints in
    a declarative way, which are then transformed into unit tests. The mlinspect [47]
    library provides a way to inspect machine learning pipelines in a declarative
    manner. The most recent advancements in data validation systems encompass the
    automatic identification of error types, evaluation of the effects of errors on
    models, user-friendliness, and efficient validation with human involvement [89],
    [90], [91], [92]. 2) Data Cleaning Data cleaning involves eliminating errors in
    the data by meeting various integrity restrictions, including key constraints,
    domain constraints, referential integrity constraints, and functional dependencies.
    Data cleaning techniques have become sophisticated, with state-of-the-art techniques
    like HoloClean [48] that repair data using probabilistic inference, satisfying
    integrity constraints, checking value validity using external dictionaries, and
    using quantitative statistics. Exclusively focusing on data correction does not
    ensure optimal model accuracy. Clean data is not always a clear-cut concept and
    removing all possible errors is not always attainable. The CleanML [49] framework
    assesses different data cleaning methods to determine if they enhance model accuracy
    and reveals that data cleaning does not necessarily enhance machine learning models
    and could even have a detrimental impact. However, selecting the correct machine
    learning model can counterbalance the negative effects of data cleaning. There
    is no one-size-fits-all cleaning algorithm that is effective for all types of
    noise, and the selection of the algorithm depends on the type of noise. Care must
    be taken when utilizing data cleaning techniques that are not specifically designed
    for machine learning as they frequently have high-impact parameters that require
    proper tuning, similar to the tuning of machine learning hyperparameters. Advanced
    data cleaning techniques use probabilistic inference, external dictionaries, and
    statistical methods to repair data. However, it is not always clear if data cleaning
    improves model accuracy, as sometimes cleaning data may have a negative effect.
    There are data cleaning techniques designed specifically to improve model accuracy,
    such as ActiveClean [50], which iteratively cleans samples and updates the model,
    and TARS [51], which cleans labels to improve model accuracy. Recently, there
    has been a rise in systematic approaches to data cleaning for machine learning,
    including CPClean [93], which analyzes the impact of missing data on predictions,
    and a study that proposes solutions to tackle data quality issues in MLOps [94].
    The most common challenges in data cleaning for machine learning include handling
    multimodal data and data that changes over time. 3) Data Sanitization Data poisoning
    is a serious issue where a small fraction of training data is altered with malicious
    intent to cause a machine learning model to fail. Data poisoning is becoming more
    sophisticated and hard to defend against [40], [95]. One approach to defending
    against data poisoning is data sanitization, which involves detecting and discarding
    poisonings using outlier detection. However, current data sanitization techniques
    have proven to be ineffective against carefully designed attacks [54], [96]. The
    field of data poisoning and sanitization is constantly evolving and requires further
    research. 4) Multimodal Data Integration Multimodal data integration refers to
    the process of combining data from multiple sources with different modalities
    (e.g. video streams, radar data, and time series) [97]. The integration of this
    data is important in machine learning, particularly in applications such as autonomous
    vehicles. Two primary techniques utilized in machine learning are alignment and
    co-learning. Alignment entails discovering connections between sub-components
    of instances that possess multiple modalities, while co-learning involves training
    a model based on one modality while leveraging information from another. Data
    integration is a vast field of research that has been explored for numerous years,
    however, not all techniques are applicable to machine learning [98]. SECTION VI.
    Related Work In recent times, there has been a remarkable shift in the importance
    of Data-Centric AI compared to Model-Centric AI. Industries are undergoing significant
    changes in their core architecture, becoming more focused on data. Many companies
    are adopting data-centric approaches to tackle their daily tasks. To address the
    challenges encountered in current workflow methodologies, where data engineering
    and model engineering are treated as separate cycles, a new division has emerged,
    introducing a range of tools such as CleanLab,7 LandingLens,8 Snorkel,9 AutoAlbument,10
    HoloClean,11 Albumentations,12 and more. These tools support various production
    processes by identifying and resolving issues step by step. This progress has
    been made possible through structured thinking and dividing tasks into distinct
    yet comprehensive components. Several benchmarks like dcbench [99], ImageNet [100],
    and MLPerf [101] have been developed to evaluate and validate the effectiveness
    and quality of these tools based on parameters such as training data size, budget-restricted
    data cleaning, object detection, computer vision, and others. In a study by Chen
    et al. [102], the authors introduce the data-centric AI approach and discuss the
    CLIP Model for multimedia misogyny detection. Additionally, Motamedi et al. [103]
    propose a data-centric AI approach for enhancing data quality and suggest a solution
    based on Generative Adversarial Networks (GANs) to synthesize new data points.
    Their findings demonstrate that the optimized dataset generated by the pipeline
    improves accuracy while being significantly smaller than the baseline. RoboFlow
    [104], a data-centric cloud-based workflow management system, is revolutionizing
    the creation of AI-enhanced robots. By breaking down the development process into
    four building blocks and prioritizing data over conventional process-centric techniques,
    RoboFlow enables the creation of data-driven AI-enhanced robots. In the field
    of materials science [105], machine learning is facilitating the prediction of
    structural features and the discovery of new materials [106]. By utilizing data-centric
    machine learning methodologies, accurate predictions can be made with minimal
    theoretical data, improving computational efficiency. In the renewable energy
    sector, while most research focuses on model-centric predictive maintenance techniques,
    a shift towards data-centric approaches is gaining attention [107]. By adopting
    data-centric machine learning, the accuracy of climate change prediction and wind
    turbine understanding can be enhanced, leading to increased utilization of green
    energy [108], [109]. In a data-centric approach, Zeiser et al. [110] propose a
    method combining WGAN and encoder CNN for advanced process monitoring and prediction
    in real-world industrial applications. They highlight the importance of context-based
    data preparation and demonstrate that improving data quality has a significant
    impact on prediction accuracy. Safikhani et al. [111] argue that transformer-based
    language models can enhance automated occupation coding. By fine-tuning BERT and
    GPT3 on pre-labeled data and incorporating job titles and task descriptions, they
    achieve a 15.72 percentage point performance increase compared to existing methods.
    They also introduce a hierarchical classification system based on the KldB standard
    for encoding occupation details. Wang et al. [112] studied, a data-centric analysis
    of on-tree fruit detection using deep learning is conducted. They explore the
    challenges of fruit detection in precision agriculture and evaluate the impact
    of various data attributes on detection accuracy. They also investigate the relationship
    between dataset-derived similarity scores and expected accuracy, as well as the
    sufficiency of annotations for single-class fruit detection using different horticultural
    datasets. Chen and Chou [102], a system for detecting misogynous MEME images is
    presented using coherent visual and language features from the CLIP model and
    the data-centric AI principle. The system achieved a competitive ranking in the
    SemEval-2022 MAMI challenge, demonstrating the effectiveness of leveraging Transformer
    models and focusing on data rather than extensive model tweaking. The proposed
    approach utilizes logistic regression for binary predictions and acknowledges
    the importance of data quality and quantity. Although limitations are not explicitly
    mentioned, the method’s performance may be influenced by the available data and
    reliance on pre-trained models. Zhong et al. [114] proposed a data-centric approach
    to enhance the robustness of deep neural network (DNN) models against malicious
    perturbations. By focusing on dataset enhancement, their algorithm improves model
    performance against adversarial examples and common corruptions. The effectiveness
    of the approach is demonstrated through high rankings in a data-centric robust
    learning competition, highlighting the algorithm’s success across various DNN
    models. SECTION VII. Challenges and Opportunities Though data-centric AI shines
    as a promising advancement, its current limitations call for a more nuanced approach.
    We encourage synergistically leveraging both methods: utilizing data-centric practices
    for robust data engineering while refining models through conventional optimization
    techniques. This, as elegantly proposed in [15], treats data and model as intertwined
    gears driving exceptional results. This embrace of synergy acknowledges the current
    landscape and honors ethical research by recognizing the contributions of prior
    work. Fig. 5 illustrates the main challenges of the data-centric AI approach.
    Data-centric production faces challenges such as maintaining consistency, adequate
    volume retention after cleaning, quality of maintenance, the necessity of a proper
    data versioning system, etc. Amongst the various issues, one of the most significant
    is the loss of volume associated with cleaning and validating data. An AI model
    can only receive a massive amount of high-quality data if low-quality datasets
    are removed. FIGURE 5. Shortcomings of data-centric systems. Show All A. Challenges
    and Solutions The main challenges of data-centric artificial intelligence (AI)
    are given below: Data quality and quantity: One of the biggest challenges in data-centric
    AI is ensuring that the data used to train and test AI models is of high quality
    and sufficient quantity. Data may be incomplete, noisy, or biased, which can lead
    to poor performance or inaccurate predictions from AI models. Data preprocessing
    is a crucial step in the AI pipeline, but it can be time-consuming and computationally
    expensive. This includes cleaning, normalizing, and transforming data, as well
    as handling missing or corrupted data. Data labeling is also a time-consuming
    and costly task. Additionally, obtaining large amounts of high-quality data can
    be difficult and expensive, particularly in certain domains such as healthcare
    or finance. Handling uncertainty: AI systems often need to make decisions in uncertain
    and dynamic environments, where data is incomplete, noisy, or ambiguous. This
    requires developing methods for dealing with uncertainty, such as probabilistic
    and Bayesian methods. Real-time processing: Many data-centric AI systems need
    to analyze and make decisions based on real-time data streams, such as self-driving
    cars, financial trading, sensor data or social media feeds. This requires the
    ability to process data quickly and make predictions in near-real time, which
    can be challenging. There is a need for more efficient and effective algorithms
    for real-time data processing and analysis. Integration of multiple data sources:
    The ability to integrate and analyze multiple data sources, such as sensor data,
    social media data, and other types of unstructured data is a major challenge.
    This is because data from different sources may have different formats, structures,
    and levels of quality, which can make it difficult to combine and analyze them
    effectively. Generalization: AI models often struggle to generalize their predictions
    to new or unseen data, particularly when the data is significantly different from
    the training data [15]. This is a major concern in real-world applications, where
    the data is often noisy and unpredictable. Data Privacy and security: Data may
    contain sensitive or personal information that must be protected from unauthorized
    access or misuse. As AI systems are increasingly used to analyze sensitive data
    or personal information, such as medical records or financial data, there is a
    growing need to protect this data from unauthorized access or misuse. This includes
    developing methods for data anonymization and differential privacy are being developed
    to address these issues. Additionally, AI systems themselves may be vulnerable
    to attacks such as adversarial examples or model stealing. Explainability and
    interpretability: AI systems become more complex and autonomous, particularly
    those based on deep learning, it can be difficult for humans to understand how
    they are making decisions or accurate predictions. This makes it difficult to
    trust and validate the results and can limit the use of AI in sensitive applications
    such as healthcare, finance, or criminal justice. Researchers are working on developing
    methods for making AI systems more transparent and interpretable. Scalability
    and robustness: As the amount of data being generated and collected continues
    to grow, it becomes increasingly difficult to process and analyze all of this
    data in a timely and efficient manner. However, many existing AI systems are not
    well-suited for these types of major challenges, and research is needed to improve
    the scalability and robustness of AI systems, particularly in resource-constrained
    environments. Multi-modal and multi-sourced data: As the amount and diversity
    of data increases, it becomes more challenging to integrate and make sense of
    data from multiple sources, such as sensor data, social media data, and other
    types of unstructured data. Causality and counterfactual reasoning: AI systems
    are often used to make predictions or decisions, but it can be difficult to understand
    the causal relationships underlying those predictions or decisions. Additionally,
    it’s challenging for AI systems to reason about counterfactual scenarios, where
    the outcome of an event is different from what actually happened. Human-AI collaboration:
    As AI systems become more integrated into various domains, there is a need for
    more effective methods of collaboration between humans and AI systems. There are
    many challenges associated with human-AI interaction, such as trust, transparency,
    and explainability. This includes finding ways to make AI systems more transparent
    and explainable to humans, as well as developing methods for humans to provide
    feedback and guidance to AI systems. Bias and fairness: AI models can inadvertently
    introduce bias into their predictions, particularly if the data used to train
    them is not representative of the population. This can result in unfair or discriminatory
    decisions. This is a significant concern in applications such as healthcare, finance,
    and law enforcement. Researchers are working on developing methods for detecting
    and mitigating bias in AI systems. Integration with other technologies: AI systems
    often need to be integrated with other technologies, such as sensors, IoT, and
    robotics. This can be challenging as each technology has its specificities and
    constraints. Ethics and regulation: As AI systems become more sophisticated and
    prevalent, there are important ethical and regulatory challenges to consider,
    such as the impact of AI on jobs and society, the accountability of AI systems,
    and the potential misuse of AI. Therefore, a data-centric method frequently needs
    a more significant data volume than a model-centric one. This brings potential
    issues where cleaning might decrease the insufficient amount of collected data
    under technological or monetary constraints; for example, in scientific research,
    the model might be continuously tuned to generate and work on experimental data.
    Working in a constantly evolving setting can be challenging, primarily due to
    the changing algorithms and margins for errors. In many cases, rules-based algorithms
    result in a high rejection rate because the technology needs to differentiate
    between authentic defective components and acceptable levels of variation, especially
    in manufacturing and production-based industries. This forces a large percentage
    of human follow-up inspection, which raises costs and slows down production lines.
    Robustness is the model’s ability to preserve performance with a small amount
    of noise in the data, such that the training error rates are consistent with testing
    error rates. Fairness refers to the model’s tendency to defend against unnatural
    biases such that the model does not inadvertently introduce any biases. In the
    case of data quality issues, a focus is given to robust training algorithms when
    validation of noisy data is insufficient and/or to fairness when appropriate pre-processing
    levels are insufficient to remove biases from data. An AI system should be trained
    on the same data type on which it is to be tested and analyzed, including any
    edge-cases. Additionally, as part of quality control methods, properties of data
    records that are not causative features should be randomized during training.
    An AI model quickly loses accuracy without consistent data annotation. Unfortunately,
    it can be challenging to maintain a high level of consistency. However, this brings
    forth a significant challenge to data-centric AI, as it requires human annotation
    that is costlier than machine computation, especially in environments that emphasize
    maximum automation. B. Opportunities Several research opportunities can advance
    the Data-Centric AI field going forward. The main challenge for applying a data-centric
    approach in embedded ecosystem applications is the complexity. It is challenging
    to deploy data-centric AI on portable devices. In this context, one promising
    research direction is the Edge Impulse-based data-centric approach for a wide
    range of hardware targets [126]. Edge Impulse is a cloud-based ML operations (MLOps)
    platform like Microsoft Azureor [127] or Google VertexAI [128] for developing
    embedded and Tiny Machine Learning (TinyML) [129] systems. The researchers may
    conduct the Vivo experiments on data-centric Green AI [109]. They analyze how
    combining data-centric with Green AI techniques in real-world AI pipelines. These
    techniques may impact the energy efficiency and accuracy of AI models. The research
    opportunities in the field of data-centric artificial intelligence are given below:
    Developing methods for effectively and efficiently processing and analyzing large
    amounts of data, such as big data and real-time streaming data. Developing more
    efficient and accurate algorithms for data pre-processing, cleaning, and feature
    extraction. Improving the performance of machine learning models by developing
    new architectures and training techniques. Developing new methods for dealing
    with large and complex datasets, such as deep learning and reinforcement learning.
    Developing new approaches for dealing with unstructured and semi-structured data,
    such as natural language processing and image processing. Developing new methods
    for interpretability and explainability of AI models, Developing methods for integrating
    AI with other technologies, such as the Internet of Things (IoT) and edge computing.
    Developing new ways to use AI to analyze and make sense of data from multiple
    sources, such as sensor data, social media data, and financial data. Developing
    new approaches for dealing with privacy and security issues related to data-centric
    AI. Developing new ways to use AI to improve decision-making in various domains,
    such as healthcare, finance, and transportation. Researching on the ethical and
    societal impact of AI. Developing more efficient and effective algorithms for
    data processing and analysis, such as machine learning and deep learning techniques.
    Improving the scalability and robustness of AI systems to handle large and complex
    data sets. Developing new methods for integrating and analyzing multiple data
    sources, such as sensor data, social media data, and other types of unstructured
    data. Improving the performance and accuracy of machine learning and deep learning
    models through the use of advanced techniques such as transfer learning and ensemble
    learning. Developing new and innovative applications of AI in areas such as healthcare,
    finance, and transportation, to name a few. Exploring the use of AI for decision-making
    and problem-solving in complex and dynamic environments, such as in the areas
    of autonomous systems and smart cities. Understanding and addressing ethical and
    societal implications of AI, including issues related to bias, transparency, and
    accountability. Developing new methods for explainable AI (XAI) which can help
    to increase the transparency and interpretability of AI systems. Designing AI
    systems that can learn and adapt to changing environments and data distributions.
    Researching ways to make AI more robust and secure, such as against adversarial
    attacks. SECTION VIII. Discussion Initially, the model-centric AI approach appears
    more reasonable for modifying the model (algorithm/code) to enhance the system’s
    efficiency, as opposed to altering the data. However, when faced with the constraints
    of model-centric AI solutions, the use of data-centric approaches becomes pertinent,
    as discussed previously. Model-centric classification algorithms employ regularization
    techniques to adjust the algorithm or code in order to enhance model performance.
    Nevertheless, for real-time applications to function effectively, reliable data
    is essential, and this consideration should extend to the underlying model to
    the same degree. Therefore, it is important to view model-centric and data-centric
    approaches as complementary and not to dismiss the model-centric approach entirely
    due to its limitations in some specific organizations and industries. First, in
    the initial stages of problem-solving, it is crucial to emphasize not only understanding
    the properties and information of things but also how we interact with them. In
    the context of AI development, this implies a greater focus on designing and refining
    intelligent algorithms, as well as fine-tuning the hyperparameters of the underlying
    model and the code that implements the algorithms. Defining the dataset at this
    project stage is of utmost importance, as it serves as the foundation for comparing
    and categorizing the performance of models. Second, commencing the AI system design
    with a model-centric approach provides ample opportunities to gain hands-on experience
    in understanding real-world problems and exploring potential computational solutions,
    rather than the experience that might have been acquired through adopting a data-centric
    approach in research and industry. When attempting to solve a problem, it is often
    in our nature to start by taking action to make an impact before delving further
    into understanding the properties of items in our environment. Reinforcement learning,
    a recently well-publicized machine learning (ML) technique, is based on the idea
    that intelligent agents learn through interacting with their environments. Third,
    it would be challenging to relate the final models to real-world problems because
    they would have undergone a completely different development methodology. In the
    early phases of AI development, both academia and industry may have preferred
    the data-centric approach over the model-centric method. A general architecture
    of model-centric AI, data-centric AI, and model data-centric AI is shown in Fig.
    6. Model-data-centric AI combines the benefits of both model-centric AI and data-centric
    AI, seeking to balance the refinement of models and the improvement of training
    data, recognizing that both aspects are crucial for achieving optimal results.
    The choice of approach should take into consideration the specific needs, available
    resources, and objectives of the AI project at hand. FIGURE 6. General architecture
    of model-centric AI, data-centric AI, model data-centric AI. Show All SECTION
    IX. Conclusion This paper presented a high-level review of the data-centric AI
    paradigm by connecting all relevant topics together to help academicians, researchers,
    and engineers manage the AI systems. Data-centric AI is capable of constituting
    a complementary paradigm to model-centric AI to build successful AI-based systems
    for real-world applications. Data-centric AI emphasizes the importance of data
    quality in the performance of AI systems, recognizing data as dynamic and constantly
    evolving, rather than just a static input for preprocessing. Improving data quality
    is a continuous process throughout the system’s life cycle. This article presents
    DCAI as a developing movement and proposes four principles to guide its efforts.
    The future of data-centric AI lies in establishing systematic processes for monitoring
    and enhancing data quality, which requires more focus on data supply, preparation,
    annotation, and integration into AI systems. Conflict of Interest The authors
    declare that there is no conflict of interest. ACKNOWLEDGMENT The authors express
    their gratitude to Manipal Academy of Higher Education, Manipal, for their essential
    role in facilitating the publication of this article. Authors Figures References
    Keywords Metrics Footnotes More Like This A Personalized Computational Model for
    Human-Like Automated Decision-Making IEEE Transactions on Automation Science and
    Engineering Published: 2022 Explainable Artificial Intelligence: Counterfactual
    Explanations for Risk-Based Decision-Making in Construction IEEE Transactions
    on Engineering Management Published: 2024 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Opportunities and Challenges in Data-Centric AI
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hethcoat M.G.
  - Jain P.
  - Parisien M.A.
  - Skakun R.
  - Rogic L.
  - Whitman E.
  citation_count: '0'
  description: Climate-driven changes in fire regimes are expected across the pan-Arctic
    region. Trends in arctic fires are thought to be generally increasing; however,
    fire mapping across the region is far from comprehensive or systematic. We developed
    a new detection workflow and built a dataset of unrecorded tundra fires in Canada
    using Landsat data. We built a reference dataset of spectral indices from previously
    mapped fires in northern Canada to train a Random Forest model for detecting new
    fires between 1986 and 2022. In addition, we used time series information for
    each pixel to reduce false positives and narrow the large search space down to
    a finite set of regions that had experienced changes. We found 209 previously
    undetected fires in the Arctic and sub-Arctic regions, increasing the mapped burned
    area by approximately 30%. The median fire size was small, with roughly 3/4 of
    the fires being <100 ha in size. The majority of newly detected fires (69%) did
    not have satellite-derived hotspots associated with them. The dataset presented
    here is commission error-free and can be viewed as a reference dataset for future
    analyses. Moreover, future improvements and updates will leverage these data to
    improve the detection workflow outlined here, particularly for small and low-severity
    fires. These data can facilitate broader analyses that examine trends and environmental
    drivers of fire across the Arctic region. Such analyses could begin to untangle
    the mechanisms driving heterogeneous fire responses to climate observed across
    regions of the Circumpolar North.
  doi: 10.3390/rs16020230
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Remote Sensing All Article Types Advanced   Journals
    Remote Sensing Volume 16 Issue 2 10.3390/rs16020230 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Jungho Im
    Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links
    Article Views 812 Table of Contents Abstract Introduction Materials and Methods
    Results Discussion Limitations Conclusions Author Contributions Funding Data Availability
    Statement Acknowledgments Conflicts of Interest Appendix A Appendix B References
    share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessArticle Unrecorded Tundra Fires in Canada, 1986–2022 by Matthew G. Hethcoat
    1,*, Piyush Jain 1, Marc-André Parisien 1, Rob Skakun 1, Luka Rogic 2 and Ellen
    Whitman 1 1 Northern Forestry Centre, Canadian Forest Service, Natural Resources
    Canada, 5320 122 Street NW, Edmonton, AB T6H 3S5, Canada 2 Department of Electrical
    and Computer Engineering, University of British Columbia, 5500-2332 Main Mall,
    Vancouver, BC V6T 1Z4, Canada * Author to whom correspondence should be addressed.
    Remote Sens. 2024, 16(2), 230; https://doi.org/10.3390/rs16020230 Submission received:
    28 November 2023 / Revised: 20 December 2023 / Accepted: 4 January 2024 / Published:
    6 January 2024 (This article belongs to the Section Environmental Remote Sensing)
    Download keyboard_arrow_down     Browse Figures Versions Notes Abstract Climate-driven
    changes in fire regimes are expected across the pan-Arctic region. Trends in arctic
    fires are thought to be generally increasing; however, fire mapping across the
    region is far from comprehensive or systematic. We developed a new detection workflow
    and built a dataset of unrecorded tundra fires in Canada using Landsat data. We
    built a reference dataset of spectral indices from previously mapped fires in
    northern Canada to train a Random Forest model for detecting new fires between
    1986 and 2022. In addition, we used time series information for each pixel to
    reduce false positives and narrow the large search space down to a finite set
    of regions that had experienced changes. We found 209 previously undetected fires
    in the Arctic and sub-Arctic regions, increasing the mapped burned area by approximately
    30%. The median fire size was small, with roughly 3/4 of the fires being <100
    ha in size. The majority of newly detected fires (69%) did not have satellite-derived
    hotspots associated with them. The dataset presented here is commission error-free
    and can be viewed as a reference dataset for future analyses. Moreover, future
    improvements and updates will leverage these data to improve the detection workflow
    outlined here, particularly for small and low-severity fires. These data can facilitate
    broader analyses that examine trends and environmental drivers of fire across
    the Arctic region. Such analyses could begin to untangle the mechanisms driving
    heterogeneous fire responses to climate observed across regions of the Circumpolar
    North. Keywords: Arctic; Google Earth Engine; hotspots; lightning; MODIS; NBAC;
    Normalized Burn Ratio; Random Forest; Tasseled Cap; VIIRS 1. Introduction Empirical
    data and modeling indicate a growing likelihood of fire in the Arctic [1,2]. There
    are increasing trends in the length of the fire season [1,3], air temperature,
    fuel load and availability, and a host of fire-related abiotic factors in the
    Arctic and sub-Arctic zones [4,5,6]. While many northern regions are changing
    faster than those at lower- and mid-latitudes [2], global patterns of Arctic and
    tundra fire activity indicate substantial spatio-temporal variation in burning.
    Fires have increased in Alaska and Siberia [1,7], but a slight decreasing trend
    has been observed in the Northwest Territories of Canada [6,8]. The accurate spatio-temporal
    mapping of fires is fundamental to understanding changes in the burned area, fire
    severity, post-fire vegetation recovery, and for determining any climate- and
    human-driven changes in fire regimes globally. Furthermore, such maps offer an
    essential baseline from which to measure changes, particularly over periods in
    which rapid changes are forecasted. In recent decades, a suite of space-borne
    satellite sensors have facilitated large-scale fire monitoring and mapping, enabling
    a global-scale analyses of the patterns, trends, and drivers of wildfires [9,10].
    Indeed, since approximately 1979, beginning with the Advanced Very High Resolution
    Radiometer, satellites have acquired sub-daily observations of fire activity [11].
    However, many of the purpose-built fire products derived from thermal anomalies
    (referred to hereafter as hotspots) are spatially coarse and thus cannot detect
    small fires nor delineate perimeters effectively to produce detailed burned area
    estimates. The Landsat missions, with their increased spatial resolution (over
    Moderate Resolution Imaging Spectroradiometer (MODIS) or Visible Infrared Imaging
    Radiometer Suite (VIIRS) sensors) and global coverage, have enabled a fine-tuned
    understanding of burn severity and area burned [12]. Canada has two federally
    maintained national fire databases—the Canadian National Fire Database (CNFDB)
    and the National Burned Area Composite (NBAC)—that collect fire data from a variety
    of sources (e.g., provincial and territorial agencies, federal departments and
    agencies, satellite mapping techniques, etc.). The CNFDB is composed of fire maps
    of varying quality, produced by jurisdictional fire management agencies and compiled
    into a single national database [13]. The NBAC uses a consistent methodology to
    detect and map new fires, as well as refine the perimeters of historic fires that
    were not accurately delineated within the CNFDB [13,14]. Critically, data from
    the NBAC are used by the government of Canada to report on progress toward international
    commitments to monitor anthropogenic emissions and are regarded as the authoritative
    source on more recent fires in Canada [15,16]. Historically, fire suppression
    activity has been absent or minimal in the remote northern areas of Canada and,
    consequently, fires that have occurred outside of forested regions (i.e., north
    of the treeline) or far from inhabited areas have not been monitored or mapped
    [17]. Indeed, the territory of Nunavut does not have a fire management agency.
    Moreover, even newer methodological techniques to improve fire mapping in Canada
    have limited their study domain to exclude regions north of the treeline (e.g.,
    [13,18]). The omission is likely driven by assumptions about a lack of fire in
    the region and the fact that disturbance mapping in tundra environments is known
    to be extremely challenging with remotely sensed data [19]. Consequently, the
    NBAC and CNFDB do not represent a complete picture of fires in Canada and there
    is a clear gap in our understanding of fires in remote northern regions, particularly
    above the treeline. The primary objective of this study was to detect and map
    unrecorded tundra fires within Canada using Landsat data. There is a growing need
    to assess and understand potential changes in fire regimes within Canada, particularly
    in light of the 2023 fire season, where a record-breaking area of >15 M ha burned
    [20]. Given the lack of systematic fire monitoring outside of forested regions,
    we sought to advance the build-out and maintenance of a more comprehensive accounting
    of fires in northern Canada since 1986. 2. Materials and Methods 2.1. Study Area
    We focused on three ecozones in northern Canada [21] covering areas roughly north
    of the treeline—the Arctic Cordillera, Northern Arctic, and Southern Arctic ecozones
    (Figure 1). The study area stopped at approximately 74.5 degrees north (i.e.,
    in the Arctic Ocean north of Banks, Victoria, Prince of Wales, Somerset, and the
    Baffin Islands). The vegetation present across the region is diverse and is generally
    north of the treeline, consisting of a combination of herbaceous plants, shrubs,
    mosses, and lichens (see [22] for a more complete breakdown of vegetation characteristic
    and communities present across the various subzones within the region). Across
    the study area, systematic fire monitoring has historically been of low priority.
    Although there are many small communities throughout, it is very sparsely populated.
    Land use mainly consists of low-impact traditional and cultural activities, with
    few permanent roads or energy corridors; however, various mines operate across
    the region. Figure 1. Canadian ecozones and broad land cover types (from North
    American Land Change Monitoring System [23]) over the study area searched for
    tundra fires between 1986 and 2022. The Southern Arctic is outlined in dark blue,
    the Northern Arctic is outlined in pink, and the Arctic Cordillera is outlined
    in yellow. Needleleaf forests are dark green, shrublands are brown, grassland-lichen-moss
    are light green, barren-lichen-moss are medium green, water and wetlands are blue,
    barren ground is light grey, and snow is white. Provincial and territorial borders
    are in white, major lakes are light blue, and ecozones outside the study area
    are outlined in black. 2.2. Detecting Fires 2.2.1. Datasets, Satellite Imagery,
    and Initial Modeling We first sought to build a reference dataset of spectral
    fire indices (Table A1 and Table A2) for northern Canada to assist with fire detection.
    We sampled burned and unburned point locations from all NBAC fires that occurred
    within the study area and the next adjacent ecozones south (i.e., the northernmost
    fires available) between 2014 and 2020 (n = 253). Most NBAC fires had 25 burned
    pixels and 30 unburned pixels randomly sampled, with unburned locations coming
    from within a 1 km buffer surrounding fire perimeters. More unburned pixels were
    sampled in an attempt to balance out sampling, because burned areas were usually
    larger and therefore had a smaller chance of containing masked water bodies, which
    resulted in failed sampling. We sampled differenced fire indices, subtracting
    imagery 1-year post-fire from 1-year pre-fire. Some smaller fires could not fit
    25 burned and 30 unburned sample pixels (e.g., a fire less than two hectares has
    fewer than 25 pixels in the entire burn scar), and the final dataset contained
    11,056 points, of which 4989 (45%) were unburned and 6067 (55%) were burned. These
    data were then used to train and validate a Random Forest (RF) model for detecting
    fires in our study area (details below). We used Landsat (5, 7, 8, and 9) Collection
    2 surface reflectance data in Google Earth Engine (GEE) to calculate spectral
    indices associated with fire mapping and vegetation change [24]. Pixels covered
    by clouds, shadows, snow, and water were removed using the quality flags provided
    within the QA_PIXEL band in Landsat Collection 2. A summer-season mosaic was generated
    for each year using the median of all available scenes between 15 June and 15
    September. We removed permanent water bodies using the global water mask developed
    by the European Commission’s Joint Research Centre [25]. Preliminary model training
    and testing was conducted in a Google Colaboratory notebook [26] to optimize RF
    tuning parameters and select fire indices that accurately classified burned areas
    (Table A3 and Table A4). We used a suite of Scikit-learn [27] functions for model
    selection, feature reduction, and k-fold cross-validation to select a final RF
    model for efficient computation in GEE (Appendix B). The top RF model showed strong
    predictive performance during model validation (Table A4); however, during model
    deployment to the study area—a region not well represented in the training data—this
    model regularly predicted sparsely vegetated regions and atmospheric artifacts
    as having a high probability of being burned, despite having not experienced a
    fire (Figure 2). Through subsequent testing of candidate variables, we identified
    three metrics that maintained strong detection while greatly reducing spurious
    detections in our northern study area: Normalized Burn Ratio 2 (NBR2), Tasseled
    Cap Brightness (TCB), and Tasseled Cap Greenness (TCG, Figure 2, Table A4). This
    RF model was then used as part of the fire detection methodology detailed below.
    Figure 2. Comparison of Random Forest (RF) model predictions using the top-performing
    model during validation (a,c) versus the chosen model for the study area after
    further regional testing (b,d). Probability of being burned is displayed in shades
    of red, with darker red being higher probability of fire. The top images show
    higher incidences of false positives from insufficient cloud removal near the
    fires (circled in (a,c)). Noticeable banding present in imagery was a common artifact
    from the missing data in Landsat 7 scan-line corrector error images. 2.2.2. Fire
    Detection We used a three-step workflow to map fires (Figure 3). First, candidate
    fire detections—pixels that met a four-rule criteria (details below)—were exported
    from GEE. Next, candidate fires were visually inspected to confirm a change event
    was fire-related, as changes in hydrology, thaw slumps, mining activities, road
    expansions, phenological differences, etc. were frequently detected. Finally,
    we used R Statistical Computing Software, version 4.2.2 [28], to revisit confirmed
    fires and delineate a refined fire perimeter at 30-m resolution, utilizing the
    ‘rgee’ package [29]. This multi-stage approach was used to reduce the large search
    space down to a finite set of areas that had experienced change events and stay
    within the computation limits of GEE. Figure 3. Fire detection workflow used to
    locate and confirm Canadian tundra fires between 1986 and 2022. The first step
    in the multi-stage fire detection methodology was to identify candidate fires
    in GEE using a four-rule criteria. We used existing NBAC fire perimeters from
    within the study area to settle on the thresholds outlined below. The first criterion
    retained only pixels with ≥90% of the RF votes identified as being burned. A high
    RF threshold maintained detection and reduced false positives in sparsely vegetated
    areas and pixels with insufficient cloud removal. Despite a high RF threshold,
    many false positives remained and we incorporated two additional rules based on
    time series information. Criteria two and three compared post-fire imagery to
    a pre-fire median of up to three years. For criterion two, pixels were retained
    if there was a >50% decrease in post-fire NBR2, compared to a three-year pre-fire
    median. Inter-annual differences in phenology and apparent drought stress resulted
    in NBR2 drops > 30% in some years; we sought to account for these variations.
    The third criterion required the difference between the post-fire NBR2 and the
    three-year pre-fire median to be ≤−0.1. Again, inter-annual differences in seasonal
    mosaics resulted in NBR2 differences of ≅0 and we wanted to exclude those occurrences
    as well. Finally, the post-fire NBR values needed to be <0. Candidate fires that
    met all four criteria were exported as vectors from GEE at 90 m spatial resolution
    (Figure 3). This spatial resolution (i.e., 90 m rather than 30 m) was required
    in order to stay within the computation limits of GEE, given the size of the study
    area. The second step in the methodology required a visual inspection of each
    candidate fire ≥ four pixels in size (encompassing 50–100 candidate areas per
    year). Four pixels was chosen for a minimum size (approximately four hectares)
    because some atmospheric effects still satisfied the four criteria and resulted
    in isolated one- to two-pixel noise, particularly over snowy and alpine regions
    in the northeast of the study area. A single reviewer used a suite of custom GEE
    scripts to visualize a time series of imagery over each candidate fire to confirm
    a change event was associated with a fire. Examples of events that were not fire-related
    are widespread regional hydrological changes visible throughout the landscape,
    landslides or thaw slumps, and human footprint expansion through mining and road
    building. This manual validation step was preferred, rather than a fully automated
    workflow, because of the dynamic nature of the landscape—changes in hydrology,
    thaw slumps, mining activities, road expansion, etc., frequently satisfied the
    four-rule criteria. We finalized perimeters for confirmed fire events using the
    RF model from step one; however, this time retaining pixels with ≥50% of the votes
    as burned for perimeter mapping. The 50% threshold was chosen by comparing a range
    of thresholds against existing NBAC fire perimeters from within the study area.
    In general, fire perimeters were generated via RF prediction on differenced fire
    imagery, subtracting 1-year pre-fire imagery from 1-year post-fire imagery. However,
    some small or low-severity (n ≅ 20) fire perimeters were output with null geometries
    (i.e., no pixels had ≥ 50% votes as burned because of rapid vegetation recovery),
    and 1-year pre-fire imagery was subtracted from year-of-fire in those cases. 2.3.
    Validation We cross-referenced our dataset against two data sources: (1) existing
    fires within the NBAC dataset and (2) satellite-derived hotspots from MODIS Terra
    (MOD14A1), MODIS Aqua (MYD14A1), and VIIRS (VNP141A). Sixty-six fires within the
    study area were already in the NBAC database and we calculated the omission rate
    of known fires against our new dataset. When using hotspots, we searched for additional
    fires potentially missed by our methodology. We buffered each hotspot by 3 km
    and output any potential fire perimeters using the RF prediction, keeping pixels
    ≥ 50% class votes as burned. As Terra began acquiring data in 2000, Aqua in 2002,
    and VIIRS in 2012, hotspots were searched over this subset of the study period
    (i.e., 2000–2022). 3. Results We detected 206 new fires in our study area between
    1986 and 2022 using our workflow, with an additional 3 fires found during hotspot
    validation, for a total of 209 newly mapped fires. Thus, including the existing
    NBAC fires, the region had 275 documented fires over the study period (Figure
    4). In the combined dataset, the median number of fires in a given year was five
    and the median annual burned area was 219 hectares. The median fire size was 22.6
    hectares (bootstrapped 95% CI = 16.2–28.3 ha) and 210 of the 275 total fires were
    <100 ha in size. No fires were detected from any source in 2004 or 2005. In total,
    the new fires added approximately 12,622 ha (a 32.8% increase) to the previously
    known total for the region (38,470 ha). Figure 4. Distribution of (a) fire size
    in hectares, (b) number of fires per year, and (c) burned area per year within
    the study area from 1986 to 2022. The National Burned Area Composite (NBAC) data
    are in grey and the new fires detected in this study are in black, with the median
    fire size denoted by the vertical dashed lines (in (a)). Nearly half of the new
    fires were in the Arctic (n = 96; 45.9%), defined as latitudes above 66° N by
    the Arctic Monitoring and Assessment Programme (Figure 5). No new fires were found
    within the Arctic Cordillera ecozone, though a single fire from 2022 was already
    mapped in the NBAC (>3500 ha), inside Kuururjuaq National Park, QC, representing
    the lone fire in the region over the study period. The Northern Arctic ecozone
    had 41 fires, most generally southwest of Wager Bay, NU, except for 1 fire on
    Baffin Island in 2009 (25 ha) and another in the Qikiqtaaluk Region, NU in 2020
    (2 ha). The majority of newly detected fires occurred in the Southern Arctic ecozone,
    with 168, spread across three general regions: (1) the western edge of the ecoregion—forming
    a strip between Great Bear Lake and Tuktoyaktuk, NT; (2) northwest of Hudson Bay;
    and (3) northern Quebec (Figure 5). Figure 5. Spatial distribution of newly detected
    tundra fires that occurred between 1986 and 2022 (orange circles in (a), with
    individual fires north of 66°N shaded lighter orange. Fires previously mapped
    in the NBAC are black stars. Panels (b–d) are centered over each sub-region of
    fire clusters, showing the fire area as graduated point sizes and the year of
    burning as colors. Of the 66 fires already in the NBAC database, our methodology
    failed to detect 5 (i.e., an 7.6% omission rate), with areas of approximately
    9.4, 4.5, 3.4, 1.9, and 0.9 hectares. In terms of omission of the burned area,
    those five fires accounted for 0.05% of the total burned area previously mapped
    (approximately 20 ha out of 38,470). Using satellite-derived hotspots between
    2002 and 2022, we found three additional fires missed by our methodology, with
    areas of approximately 12, 8, and 8 hectares. The majority of the fires already
    in the NBAC database had hotspots associated with them (60.1%), reflecting one
    of the ways these fires have historically been detected and mapped; however, an
    additional 15 fires in the NBAC database did not have any hotspots and were likely
    opportunistically found due to their proximity to other fires targeted for mapping.
    In contrast, 69.5% of the newly detected fires with our method did not have any
    hotspots associated with them (Table 1). The median fire size was larger for fires
    that had hotspots (64.3 ha) compared to those without (13.4 ha) and all fires
    >~150 ha had hotspots. Table 1. Presence of thermal anomalies (i.e., hotspots)
    for fires within the study region, 2000–2022. 4. Discussion Satellite data are
    critical for monitoring fire dynamics globally, particularly in remote regions
    faced with rapid climate- and human-driven changes. We used Landsat imagery to
    generate a dataset of wildfires for the tundra region of Canada since 1986, increasing
    the known burned area by about 30%. In general, we did not find widespread, unaccounted-for
    burning across the study area. In an average year (i.e., median), the region—an
    area only slightly smaller than Argentina—had five fires and burned roughly 200
    ha. The approach outlined here combines elements, conceptually, from existing
    methods used to map fires in Canada [13,14,30]; however, some adjustments were
    made to tailor the detection approach to the study region. Specifically, our RF
    model (i.e., criterion 1) used bi-temporal Landsat imagery to detect changes in
    fire indices, akin to [13] and [14], but our approach used multiple indices (dNBR2,
    dTCB, and dTCG) and RF modeling as opposed to thresholding dNBR values. Similarly,
    changes in pixel value time series (i.e., criteria 2 and 3) are analogous to the
    breakpoint methods used by Hermosilla et al. [30] to detect fires, albeit much
    simplified to efficiently run in GEE. Moreover, we chose to use a multi-stage
    approach, whereby candidate fires were examined individually and re-mapped at
    a finer scale if confirmed, because attempts to produce a fully automated workflow
    (like the approaches referenced above) persistently included land-use and land-cover
    changes that were not fire-related (e.g., thaw slumps, road development, mine
    expansion, hydrologic changes). Thus, our methodological adjustments were focused
    on reducing false positives and producing a commission error-free dataset. Our
    RF model used three spectral indices less commonly used (dNBR2, dTCB, and dTCG)
    for fire mapping (but see [31,32,33,34]). During model development and training
    we assessed many of the spectral indices commonly used for mapping fire perimeters
    and characterizing burn severity. However, during model deployment across our
    study area, we found that two of the tasseled cap bands (brightness and greenness)
    and NBR2 performed well at reducing false positives (Figure 2). NBR2 is known
    to penetrate smoke and clouds effectively, because of the incorporation of longer
    wavelength bands in the calculation [34], and is likely one of the major contributions
    to the reduction in false positives observed (Figure 2). In addition, NBR2 has
    been shown to perform better at detecting surface fires, relative to NBR [35],
    and might explain why NBR performed well during model training but NBR2 performed
    better over the study area. That is, the majority of the training data came from
    northern boreal fires and were thus trained and validated against spectral characteristics
    of forest fires, whereas surface fires were more common across the study area.
    Tasseled cap indices (TCs), particularly brightness and greenness, have a long
    history in change detection applications across a variety of ecosystems [31,32],
    but are more often used to model post-fire recovery trajectories rather than map
    fire perimeters [36]. However, a number of studies from high latitudes have highlighted
    the usefulness of TCs for mapping fires [32,33,37]. TCs calculations decompose
    the visible, near-infrared, and short-wave infrared Landsat bands (n = 6) into
    three orthogonal bands and thus provide information about spectral changes across
    the full range of reflectance values (i.e., dimensionality reduction). Consequently,
    because TCs were not derived for a particular vegetation type, biome, or range
    of the electromagnetic spectrum measured using Landsat, they are likely sensitive
    to changes across diverse vegetation types present in tundra environments [32,37].
    For example, the spectral signatures of nonvascular plants (e.g., Bryophytes)
    and lichens can be significantly influenced by moisture content [19], with both
    photosynthetic activity and respiration tied to water availability. Given that
    this region lacks a strong reflectance signal from chlorophyll in tall vegetation,
    as relied on for fire mapping in more southern ecosystems, these alternate indices
    may be better suited to capture fire-induced change in low-vegetation and low-fuel
    environments. Our finding that the majority of fires in the study area did not
    have hotspots associated with them aligns with other recent studies that have
    highlighted the lack of a thermal signal for smaller, high-latitude fires [38,39].
    Moreover, it suggests that prior studies that have used hotspots to constrain
    their search for Arctic and/or tundra fires likely omitted many smaller fires
    [12,40,41]. The lack of reliable hotspots is precisely why we tailored our methodology
    to not rely on them as a data source. In mapping exercises, the omission of small
    fires (<100 ha) tends to be more problematic in regions where they account for
    a significant proportion of the total area burned [42]. Indeed, the 200+ additional
    fires we found amounted to an increased burned area of about 30%, whereas recent
    work in sub-Saharan Africa found small fires (<100 ha) added >80% to previously
    mapped totals [42]. The annual burned areas in the boreal and taiga regions of
    Canada tend to be dominated by a small number of large fires, with 90–95% of the
    burned area coming from 5–10% of fires annually [8,43,44]. Our dataset does not
    reflect this pattern as strongly, with >90% of the burned area coming from approximately
    23% of the fires (62/275) and >75% of fires being <100 ha in size (210/275). Moreover,
    these proportions are probably conservative, because we surely omitted some small
    fires in this initial study. Between 2012 and 2022—the study period during which
    both the VIIRS and MODIS sensors were operating in the region—we found an unexpected
    pattern in hotspot detections for the fires in our dataset (Table 2). In particular,
    the MODIS sensors were more likely to detect a fire, despite having a coarser
    spatial resolution than the VIIRS sensors (1 km versus 375 m). This was a somewhat
    surprising finding, as others have shown that the VIIRS I-Band offers an improvement
    over the MODIS sensors in detection efficiency for small fires in other high-latitude
    and low-biomass regions [45,46]. We initially thought this difference might be
    related to overpass time and, possibly, early detection of short, wind-driven
    burn events (as the Terra platform passes over the equator at approximately 10:30
    a.m. local time, followed by one of the VIIRS platforms at 12:40 p.m., then both
    the Aqua platform and the second VIIRS platform at approximately 1:30 p.m.). However,
    of the seven fires only seen by the MODIS sensors, the two smallest fires were
    detected only by the Aqua platform—the platform more temporally matched with the
    VIIRS sensors. Thus, it seems unlikely that overpass timing was the cause. Canada’s
    Arctic region is known to be extremely cloudy [47,48], and it is also possible
    that differences in cloud masking procedures between the two sensors resulted
    in altered detection efficiencies. Table 2. Summary of thermal anomalies detected
    by the MODIS and VIIRS satellite sensors between 2012 and 2022 (i.e., the study
    period during which both sensors were operational). All fire sizes are listed
    for MODIS and VIIRS, but only the smallest five detected by both sensors and the
    largest five neither sensor detected are listed. Superscripts a and t (for MODIS)
    indicate if the fire was exclusively detected by the Aqua or Terra platform, respectively
    (with no subscript representing detection by both platforms). An alternative explanation
    may be due to the higher fire radiative power (FRP) detected by the MODIS sensors
    at northern latitudes [49]. While FRP is not part of the detection algorithm on
    its own, it is calculated directly from brightness values (in Kelvin) detected
    by each sensor [49]. Thus, the relatively lower FRP (which is derived from brightness)
    observed by the VIIRS sensor, suggests it is possible that the VIIRS fire detection
    algorithm may be unintentionally calibrated in a manner that misses fires in this
    region. Specifically, the VIIRS and MODIS fire-detection algorithms are similar
    at their core—both relying on the MODIS C6 algorithm—but the VIIRS algorithm separates
    fire pixels from background pixels using only the fixed thresholds test (see [50]),
    while the MODIS algorithm uses the same fixed thresholds test but also incorporates
    a newer dynamic threshold test [49,50,51]. Indeed, a post-hoc examination of the
    archived near-real-time Fire Information for Resource Management System (FIRMS)
    Global VIIRS and MODIS data [52] over fires observed by both sensors found generally
    higher brightness values for the MODIS sensors and a consistent saturation from
    the VIIRS sensors (Figure 6a). Moreover, given brightness is constrained onboard
    VIIRS, there is a resulting disconnect in the relationship between brightness
    and FRP (Figure 6b). Figure 6. Brightness values recorded for VIIRS (black) and
    MODIS (grey) over tundra fires both sensors detected between 2012 and 2020 (a).
    Relationship between detected brightness and fire radiative power (b). We were
    unable to distinguish the cause of the fire ignitions (human or lightning caused)
    with the data available. Future analyses using data from the Canadian Lightning
    Detection Network [53] may enable some discrimination between human- and lightning-caused
    fires for the region. Fire occurrence in Arctic Canada is determined using a mix
    of top-down (weather) and bottom-up (fuel) controls, with weather being particularly
    important [54]. As anthropogenic climate change has created severe warming in
    the Arctic [55], there have been observations of increasing fuel availability
    from taller and more productive vegetation [56] and an increase in fire-conducive
    weather conditions [57]. With increasing fuel and worsening fire weather acting
    as dual positive feedbacks to fire in this region, our findings further highlight
    the importance of monitoring and continued research to better understand arctic
    fire regimes, where a few small fires could shift to become a much more substantial
    area burned, if thresholds are surpassed [58]. Although none have been reported
    in recent decades, the potential for large (e.g., >1000 ha) wildfires likely exists
    across large parts of the study area. Neighboring Alaska has experienced a number
    of significant tundra wildfires in recent decades, including one > 100,000 ha
    [59]. While the conditions conducive to tundra wildfires of this extent are unlikely
    in much of the Canadian tundra, a formal examination of wildfire potential in
    this biome represents an important future area of research. 5. Limitations While
    our results represent a step toward providing a more complete picture of burning
    over the preceding decades, we almost certainly missed some small fires, particularly
    in light of the frequency of smaller fires in the data (e.g., nearly 1/3 of fires
    were ≤10 ha; Figure 1a). Our method only examined candidate fires that were exported
    at 90 m pixel resolution and were at least 4 connected pixels in size. This resolution
    was initially determined with GEE computation limitations, but we continue to
    work toward the goal of using the native Landsat pixel size (i.e., 30 m) in future
    versions to more consistently identify smaller fires. The use of smaller pixels
    may help maintain the connectedness of burned patches, ensuring they are of sufficient
    size to be verified by the methodology. For example, a long, narrow fire might
    not have had four connected pixels, whereas a small circular one would be more
    likely to be connected. In addition, the patchiness of fire intensity would also
    influence whether four contiguous pixels met the criteria—even if more than four
    pixels fell within what would ultimately have formed the fire perimeter. Consequently,
    future versions will use native Landsat resolution and refine the criteria around
    the minimum connected pixels for examining candidate fires. We used a somewhat
    conservative study area and could have extended the study domain farther south
    and undoubtedly map more fires missing from the NBAC database. We opted to initially
    focus on the Arctic and sub-Arctic regions because these have been left out of
    recent advances in automated burn mapping for Canada [13,18,30]. The region’s
    exclusion has been, in part, the result of prior assumptions about a lack of fire
    in the region. In addition, disturbance mapping in the tundra is notoriously challenging
    with remotely sensed data [19] and is likely another reason the area has been
    omitted from so many mapping exercises over the years. The spectral signature
    of tundra environments is highly sensitive to species and community composition,
    inter- and intra-seasonal phenological events, and a host of abiotic factors [19,60].
    Collectively, this has made automated disturbance mapping for the region fraught
    with commission and omission errors and is precisely why we included a manual
    validation step in our initial workflow. There are certainly more fires that still
    need to be folded into the NBAC database; however, the dataset here can be used
    to build an improved workflow with training data from a broader area, now that
    a more robust dataset of fires exists for the region. 6. Conclusions The global
    Arctic and sub-Arctic are undergoing unprecedented changes. Continued warming
    has the potential to increase fire risk in the Arctic via increased lightning,
    increased human activity, drying of peatlands, migration of fuels (e.g., northern
    advance of the treeline), altered species composition, and more [1,3,4,61]. Yet,
    the region will likely see the continued thawing of permafrost, increased variability
    in rainfall, and shifting hydrology [4,62], and so the extent to which these myriad
    changes will interact and how burning will be impacted remains unknown. Moreover,
    given recent findings that tundra fire locations are more likely to become major
    methane sources [63], as a consequence of permafrost thaw, the climate implications
    of increased fire activity are vital to understand. Regardless of the pattern
    or trend in future burning, accurate historical mapping of Arctic and sub-Arctic
    fires is a critical first step to understand potential changes in fire regimes.
    The dataset presented here can be viewed as a reference or benchmark dataset for
    future analyses to characterize trends and drivers of fire across the region or
    to assess impacts to ecosystem functions and recovery. Such analyses could begin
    to untangle the mechanisms driving heterogeneous fire responses observed between
    Alaska, Canada, and Russia [3]. Author Contributions Conceptualization, P.J.,
    M.-A.P. and E.W.; methodology, M.G.H., P.J., M.-A.P. and E.W.; validation, M.G.H.,
    R.S., L.R. and E.W.; formal analysis, M.G.H., L.R. and E.W.; writing—original
    draft preparation, M.G.H.; writing—review and editing, M.G.H., P.J., M.-A.P.,
    R.S., L.R. and E.W.; visualization, M.G.H.; supervision, P.J., M.-A.P. and E.W.;
    project administration, E.W.; funding acquisition, P.J., M.-A.P. and E.W. All
    authors have read and agreed to the published version of the manuscript. Funding
    LR was supported by funding through Natural Resources Canada and the BC Student
    Co-op Program. Data Availability Statement The new fire perimeters will be incorporated
    into the NBAC dataset, which are openly available at https://cwfis.cfs.nrcan.gc.ca/datamart/metadata/nbac
    (accessed on 20 December 2023). Relevant code will be available at https://github.com/HethcoatMG/CADtundraFires
    (accessed on 20 December 2023). In addition, a Google Earth Engine App to produce
    vectors of candidate fires is available at https://mghethcoat.users.earthengine.app/view/tundrafirerf
    (accessed on 20 December 2023). Acknowledgments We would like to thank Jurjen
    van der Sluijs from the Government of Northwest Territories for early discussions
    around tundra fires and the Landsat Long-Term Change Detection dataset. We thank
    Matthew Coyle from the Government of Northwest Territories for helpful discussions
    around MODIS and VIIRS hotspot detections at high latitudes. In addition, we thank
    four anonymous reviewers for their comments and suggestions that helped improve
    the manuscript. Conflicts of Interest The authors declare no conflicts of interest.
    The funders had no role in the design of the study; in the collection, analyses,
    or interpretation of data; in the writing of the manuscript; or in the decision
    to publish the results. Appendix A Table A1. Landsat band abbreviations used in
    equations within Table A2.      Table A2. Equations for fire indices used in Random
    Forest models built for fire detection (double asterisk represents exponentiation).
    Tasseled Cap coefficients are from [64]. Appendix B Preliminary model training
    and testing was conducted in a Google Colaboratory notebook [26] to optimize RF
    tuning parameters and select fire indices that accurately classified burned areas.
    We explored different combinations of hyperparameters using a suite of tools from
    scikit-learn [27]. Initially, we ran 100 different permutations with RandomizedSearchCV
    to find hyperparameters. Next we used GridSearchCV to exhaustively search over
    a narrower range of hyperparameters identified by RandomizedSearchCV. We then
    used RFECV to find the optimal set of features via Recursive feature elimination
    with cross-validation. We performed a 5-fold cross-validation, using RepeatedStratifiedKFold,
    to test optimal hyperparameter and feature selection vs. the base RandomForestClassifier
    model with all variables. At each point, we compared model accuracy against the
    same model using only 100 trees to assess the accuracy of a model with shorter
    computation time (Table A3 and Table A4). Similarly, we compared the optimal model
    against a model that used the three best predictors, calculated using the gini
    criterion, to assess the accuracy of a model with an even shorter computation
    time (Table A4). Finally, after initial model deployment, persistent false positives
    were further reduced using a subset of indices (i.e., tundra variables in Table
    A4). Table A3. Model sets and hyperparameters used to detect fires in Random Forest
    models. Tuning parameters from only the highest performing model are listed and
    were used across all final model sets. Table A4. Average of 5-fold cross-validation
    from Random Forest models built in Google Colaboratory using randomly sampled
    burned and unburned points from 253 fires in northern Canada 2014–2020. References
    Descals, A.; Gaveau, D.L.A.; Verger, A.; Sheil, D.; Naito, D.; Peñuelas, J. Unprecedented
    Fire Activity above the Arctic Circle Linked to Rising Temperatures. Science 2022,
    378, 532–537. [Google Scholar] [CrossRef] [PubMed] Walsh, J.E.; Ballinger, T.J.;
    Euskirchen, E.S.; Hanna, E.; Mård, J.; Overland, J.E.; Tangen, H.; Vihma, T. Extreme
    Weather and Climate Events in Northern Areas: A Review. Earth-Sci. Rev. 2020,
    209, 103324. [Google Scholar] [CrossRef] McCarty, J.L.; Aalto, J.; Paunu, V.-V.;
    Arnold, S.R.; Eckhardt, S.; Klimont, Z.; Fain, J.J.; Evangeliou, N.; Venäläinen,
    A.; Tchebakova, N.M.; et al. Reviews and Syntheses: Arctic Fire Regimes and Emissions
    in the 21st Century. Biogeosciences 2021, 18, 5053–5083. [Google Scholar] [CrossRef]
    Berner, L.T.; Massey, R.; Jantz, P.; Forbes, B.C.; Macias-Fauria, M.; Myers-Smith,
    I.; Kumpula, T.; Gauthier, G.; Andreu-Hayles, L.; Gaglioti, B.V.; et al. Summer
    Warming Explains Widespread but Not Uniform Greening in the Arctic Tundra Biome.
    Nat. Commun. 2020, 11, 4621. [Google Scholar] [CrossRef] [PubMed] Leipe, S.C.;
    Carey, S.K. Rapid Shrub Expansion in a Subarctic Mountain Basin Revealed by Repeat
    Airborne LiDAR. Environ. Res. Commun. 2021, 3, 071001. [Google Scholar] [CrossRef]
    York, A.; Bhatt, U.S.; Gargulinski, E.; Grabinski, Z.; Jain, P.; Soja, A. Wildland
    Fire in High Northern Latitudes; Arctic Report Card, 2020, Thoman, R.L., Richter-Menge,
    J., Druckenmiller, M.L., Eds.; NOAA: Silver Spring, MD, USA, 2020. [Google Scholar]
    Kasischke, E.S.; Verbyla, D.L.; Rupp, T.S.; McGuire, A.D.; Murphy, K.A.; Jandt,
    R.; Barnes, J.L.; Hoy, E.E.; Duffy, P.A.; Calef, M.; et al. Alaska’s Changing
    Fire Regime—Implications for the Vulnerability of Its Boreal forests. Can. J.
    For. Res. 2010, 40, 1313–1324. [Google Scholar] [CrossRef] Hanes, C.C.; Wang,
    X.; Jain, P.; Parisien, M.-A.; Little, J.M.; Flannigan, M.D. Fire-Regime Changes
    in Canada over the Last Half Century. Can. J. For. Res. 2019, 49, 256–269. [Google
    Scholar] [CrossRef] Jones, M.W.; Abatzoglou, J.T.; Veraverbeke, S.; Andela, N.;
    Lasslop, G.; Forkel, M.; Smith, A.J.P.; Burton, C.; Betts, R.A.; van der Werf,
    G.R.; et al. Global and Regional Trends and Drivers of Fire Under Climate Change.
    Rev. Geophys. 2022, 60, e2020RG000726. [Google Scholar] [CrossRef] Tyukavina,
    A.; Potapov, P.; Hansen, M.C.; Pickens, A.H.; Stehman, S.V.; Turubanova, S.; Parker,
    D.; Zalles, V.; Lima, A.; Kommareddy, I.; et al. Global Trends of Forest Loss
    Due to Fire From 2001 to 2019. Front. Remote Sens. 2022, 3, 825190. [Google Scholar]
    [CrossRef] Wooster, M.J.; Roberts, G.J.; Giglio, L.; Roy, D.P.; Freeborn, P.H.;
    Boschetti, L.; Justice, C.; Ichoku, C.; Schroeder, W.; Davies, D.; et al. Satellite
    Remote Sensing of Active Fires: History and Current Status, Applications and Future
    Requirements. Remote Sens. Environ. 2021, 267, 112694. [Google Scholar] [CrossRef]
    Talucci, A.C.; Loranty, M.M.; Alexander, H.D. Siberian Taiga and Tundra Fire Regimes
    from 2001–2020. Environ. Res. Lett. 2022, 17, 025001. [Google Scholar] [CrossRef]
    Skakun, R.; Castilla, G.; Metsaranta, J.; Whitman, E.; Rodrigue, S.; Little, J.;
    Groenewegen, K.; Coyle, M. Extending the National Burned Area Composite Time Series
    of Wildfires in Canada. Remote Sens. 2022, 14, 3050. [Google Scholar] [CrossRef]
    Hall, R.J.; Skakun, R.S.; Metsaranta, J.M.; Landry, R.; Fraser, R.H.; Raymond,
    D.; Gartrell, M.; Decker, V.; Little, J.; Hall, R.J.; et al. Generating Annual
    Estimates of Forest Fire Disturbance in Canada: The National Burned Area Composite.
    Int. J. Wildland Fire 2020, 29, 878–891. [Google Scholar] [CrossRef] Kurz, W.A.;
    Apps, M.J. Developing Canada’s National Forest Carbon Monitoring, Accounting and
    Reporting System to Meet the Reporting Requirements of the Kyoto Protocol. Mitig.
    Adapt. Strateg. Glob. Chang. 2006, 11, 33–43. [Google Scholar] [CrossRef] Metsaranta,
    J.M.; Shaw, C.H.; Kurz, W.A.; Boisvenue, C.; Morken, S. Uncertainty of Inventory-Based
    Estimates of the Carbon Dynamics of Canada’s Managed Forest (1990–2014). Can.
    J. For. Res. 2017, 47, 1082–1094. [Google Scholar] [CrossRef] Tymstra, C.; Stocks,
    B.J.; Cai, X.; Flannigan, M.D. Wildfire Management in Canada: Review, Challenges
    and Opportunities. Prog. Disaster Sci. 2020, 5, 100045. [Google Scholar] [CrossRef]
    Hermosilla, T.; Wulder, M.A.; White, J.C.; Coops, N.C.; Hobart, G.W.; Campbell,
    L.B. Mass Data Processing of Time Series Landsat Imagery: Pixels to Data Products
    for Forest Monitoring. Int. J. Digit. Earth 2016, 9, 1035–1054. [Google Scholar]
    [CrossRef] Nelson, P.R.; Maguire, A.J.; Pierrat, Z.; Orcutt, E.L.; Yang, D.; Serbin,
    S.; Frost, G.V.; Macander, M.J.; Magney, T.S.; Thompson, D.R.; et al. Remote Sensing
    of Tundra Ecosystems Using High Spectral Resolution Reflectance: Opportunities
    and Challenges. J. Geophys. Res. Biogeosci. 2022, 127, e2021JG006697. [Google
    Scholar] [CrossRef] Canadian Interagency Forest Fire Centre (CIFFC). Available
    online: https://ciffc.ca/ (accessed on 20 October 2023). Ecological Stratification
    Working Group. A National Ecological Framework for Canada; Centre for Land and
    Biological Resources Research: Hull, QC, Canada, 1996. [Google Scholar] Walker,
    D.A.; Raynolds, M.K.; Daniëls, F.J.A.; Einarsson, E.; Elvebakk, A.; Gould, W.A.;
    Katenin, A.E.; Kholod, S.S.; Markon, C.J.; Melnikov, E.S.; et al. The Circumpolar
    Arctic Vegetation Map. J. Veg. Sci. 2005, 16, 267–282. [Google Scholar] [CrossRef]
    Natural Resources Canada 2020 Land Cover of Canada. Available online: https://open.canada.ca/data/en/dataset/ee1580ab-a23d-4f86-a09b-79763677eb47
    (accessed on 16 December 2023). Gorelick, N.; Hancher, M.; Dixon, M.; Ilyushchenko,
    S.; Thau, D.; Moore, R. Google Earth Engine: Planetary-Scale Geospatial Analysis
    for Everyone. Remote Sens. Environ. 2017, 202, 18–27. [Google Scholar] [CrossRef]
    Pekel, J.-F.; Cottam, A.; Gorelick, N.; Belward, A.S. High-Resolution Mapping
    of Global Surface Water and Its Long-Term Changes. Nature 2016, 540, 418–422.
    [Google Scholar] [CrossRef] [PubMed] Bisong, E. Google Colaboratory. In Building
    Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive
    Guide for Beginners; Bisong, E., Ed.; Apress: Berkeley, CA, USA, 2019; pp. 59–64.
    ISBN 978-1-4842-4470-8. [Google Scholar] Pedregosa, F.; Varoquaux, G.; Gramfort,
    A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.; Weiss,
    R.; Dubourg, V. Scikit-Learn: Machine Learning in Python. J. Mach. Learn. Res.
    2011, 12, 2825–2830. [Google Scholar] R Core Team. R: A Language and Environment
    for Statistical Computing; R Foundation for Statistical Computing: Vienna, Austria,
    2022. [Google Scholar] Aybar, C.; Qiusheng, W.; Bautista, L.; Yali, R.; Barja,
    A. rgee: An R package for interacting with Google Earth Engine. J. Open Source
    Softw. 2020, 5, 2272. [Google Scholar] [CrossRef] Hermosilla, T.; Wulder, M.A.;
    White, J.C.; Coops, N.C.; Hobart, G.W. An Integrated Landsat Time Series Protocol
    for Change Detection and Generation of Annual Gap-Free Surface Reflectance Composites.
    Remote Sens. Environ. 2015, 158, 220–234. [Google Scholar] [CrossRef] Healey,
    S.; Cohen, W.; Zhiqiang, Y.; Krankina, O. Comparison of Tasseled Cap-Based Landsat
    Data Structures for Use in Forest Disturbance Detection. Remote Sens. Environ.
    2005, 97, 301–310. [Google Scholar] [CrossRef] Fraser, R.H.; Olthof, I.; Kokelj,
    S.V.; Lantz, T.C.; Lacelle, D.; Brooker, A.; Wolfe, S.; Schwarz, S. Detecting
    Landscape Changes in High Latitude Environments Using Landsat Trend Analysis:
    1. Visualization. Remote Sens. 2014, 6, 11533–11557. [Google Scholar] [CrossRef]
    Loboda, T.V.; French, N.H.F.; Hight-Harf, C.; Jenkins, L.; Miller, M.E. Mapping
    Fire Extent and Burn Severity in Alaskan Tussock Tundra: An Analysis of the Spectral
    Response of Tundra Vegetation to Wildland Fire. Remote Sens. Environ. 2013, 134,
    194–209. [Google Scholar] [CrossRef] Trigg, S.; Flasse, S. An Evaluation of Different
    Bi-Spectral Spaces for Discriminating Burned Shrub-Savannah. Int. J. Remote Sens.
    2001, 22, 2641–2647. [Google Scholar] [CrossRef] French, N.H.F.; Graham, J.; Whitman,
    E.; Bourgeau-Chavez, L.L. Quantifying Surface Severity of the 2014 and 2015 Fires
    in the Great Slave Lake Area of Canada. Int. J. Wildland Fire 2020, 29, 892–906.
    [Google Scholar] [CrossRef] Marcos, B.; Gonçalves, J.; Alcaraz-Segura, D.; Cunha,
    M.; Honrado, J.P. Assessing the Resilience of Ecosystem Functioning to Wildfires
    Using Satellite-Derived Metrics of Post-Fire Trajectories. Remote Sens. Environ.
    2023, 286, 113441. [Google Scholar] [CrossRef] Olthof, I.; Fraser, R.H. Detecting
    Landscape Changes in High Latitude Environments Using Landsat Trend Analysis:
    2. Classification. Remote Sens. 2014, 6, 11558–11578. [Google Scholar] [CrossRef]
    Miller, E.A.; Jones, B.M.; Baughman, C.A.; Jandt, R.R.; Jenkins, J.L.; Yokel,
    D.A. Unrecorded Tundra Fires of the Arctic Slope, Alaska USA. Fire 2023, 6, 101.
    [Google Scholar] [CrossRef] Roteta, E.; Bastarrika, A.; Padilla, M.; Storm, T.;
    Chuvieco, E. Development of a Sentinel-2 Burned Area Algorithm: Generation of
    a Small Fire Database for Sub-Saharan Africa. Remote Sens. Environ. 2019, 222,
    1–17. [Google Scholar] [CrossRef] Zhang, Z.; Wang, L.; Xue, N.; Du, Z. Spatiotemporal
    Analysis of Active Fires in the Arctic Region during 2001–2019 and a Fire Risk
    Assessment Model. Fire 2021, 4, 57. [Google Scholar] [CrossRef] Lizundia-Loiola,
    J.; Franquesa, M.; Khairoun, A.; Chuvieco, E. Global Burned Area Mapping from
    Sentinel-3 Synergy and VIIRS Active Fires. Remote Sens. Environ. 2022, 282, 113298.
    [Google Scholar] [CrossRef] Ramo, R.; Roteta, E.; Bistinas, I.; van Wees, D.;
    Bastarrika, A.; Chuvieco, E.; van der Werf, G.R. African Burned Area and Fire
    Carbon Emissions Are Strongly Impacted by Small Fires Undetected by Coarse Resolution
    Satellite Data. Proc. Natl. Acad. Sci. USA 2021, 118, e2011160118. [Google Scholar]
    [CrossRef] Coogan, S.C.P.; Daniels, L.D.; Boychuk, D.; Burton, P.J.; Flannigan,
    M.D.; Gauthier, S.; Kafka, V.; Park, J.S.; Wotton, B.M. Fifty Years of Wildland
    Fire Science in Canada. Can. J. For. Res. 2021, 51, 283–302. [Google Scholar]
    [CrossRef] Stocks, B.J.; Mason, J.A.; Todd, J.B.; Bosch, E.M.; Wotton, B.M.; Amiro,
    B.D.; Flannigan, M.D.; Hirsch, K.G.; Logan, K.A.; Martell, D.L.; et al. Large
    Forest Fires in Canada, 1959–1997. J. Geophys. Res. Atmos. 2002, 107, FFR 5-1–FFR
    5-12. [Google Scholar] [CrossRef] Waigl, C.F.; Stuefer, M.; Prakash, A.; Ichoku,
    C. Detecting High and Low-Intensity Fires in Alaska Using VIIRS I-Band Data: An
    Improved Operational Approach for High Latitudes. Remote Sens. Environ. 2017,
    199, 389–400. [Google Scholar] [CrossRef] Fu, Y.; Li, R.; Wang, X.; Bergeron,
    Y.; Valeria, O.; Chavardès, R.D.; Wang, Y.; Hu, J. Fire Detection and Fire Radiative
    Power in Forests and Low-Biomass Lands in Northeast Asia: MODIS versus VIIRS Fire
    Products. Remote Sens. 2020, 12, 2870. [Google Scholar] [CrossRef] Ju, J.; Roy,
    D.P. The Availability of Cloud-Free Landsat ETM+ Data over the Conterminous United
    States and Globally. Remote Sens. Environ. 2008, 112, 1196–1211. [Google Scholar]
    [CrossRef] Comiso, J.C.; Hall, D.K. Climate Trends in the Arctic as Observed from
    Space. WIREs Clim. Chang. 2014, 5, 389–409. [Google Scholar] [CrossRef] [PubMed]
    Li, F.; Zhang, X.; Kondragunta, S.; Csiszar, I. Comparison of Fire Radiative Power
    Estimates From VIIRS and MODIS Observations. J. Geophys. Res. Atmos. 2018, 123,
    4545–4563. [Google Scholar] [CrossRef] Giglio, L.; Descloitres, J.; Justice, C.O.;
    Kaufman, Y.J. An Enhanced Contextual Fire Detection Algorithm for MODIS. Remote
    Sens. Environ. 2003, 87, 273–282. [Google Scholar] [CrossRef] Giglio, L.; Boschetti,
    L.; Roy, D.P.; Humber, M.L.; Justice, C.O. The Collection 6 MODIS Burned Area
    Mapping Algorithm and Product. Remote Sens. Environ. 2018, 217, 72–85. [Google
    Scholar] [CrossRef] [PubMed] NASA-FIRMS. Available online: https://firms.modaps.eosdis.nasa.gov/map/
    (accessed on 23 November 2023). Environment and Climate Change Canada Lightning
    Density Data. Available online: https://open.canada.ca/data/en/dataset/75dfb8cb-9efc-4c15-bcb5-7562f89517ce
    (accessed on 18 December 2023). Qu, Y.; Miralles, D.G.; Veraverbeke, S.; Vereecken,
    H.; Montzka, C. Wildfire Precursors Show Complementary Predictability in Different
    Timescales. Nat. Commun. 2023, 14, 6829. [Google Scholar] [CrossRef] [PubMed]
    Rantanen, M.; Karpechko, A.Y.; Lipponen, A.; Nordling, K.; Hyvärinen, O.; Ruosteenoja,
    K.; Vihma, T.; Laaksonen, A. The Arctic Has Warmed Nearly Four Times Faster than
    the Globe since 1979. Commun. Earth Environ. 2022, 3, 168. [Google Scholar] [CrossRef]
    Myers-Smith, I.H.; Kerby, J.T.; Phoenix, G.K.; Bjerke, J.W.; Epstein, H.E.; Assmann,
    J.J.; John, C.; Andreu-Hayles, L.; Angers-Blondin, S.; Beck, P.S.A.; et al. Complexity
    Revealed in the Greening of the Arctic. Nat. Clim. Chang. 2020, 10, 106–117. [Google
    Scholar] [CrossRef] Jain, P.; Castellanos-Acuna, D.; Coogan, S.C.P.; Abatzoglou,
    J.T.; Flannigan, M.D. Observed Increases in Extreme Fire Weather Driven by Atmospheric
    Humidity and Temperature. Nat. Clim. Chang. 2022, 12, 63–70. [Google Scholar]
    [CrossRef] Young, A.M.; Higuera, P.E.; Abatzoglou, J.T.; Duffy, P.A.; Hu, F.S.
    Consequences of Climatic Thresholds for Projecting Fire Activity and Ecological
    Change. Glob. Ecol. Biogeogr. 2019, 28, 521–532. [Google Scholar] [CrossRef] Mack,
    M.C.; Bret-Harte, M.S.; Hollingsworth, T.N.; Jandt, R.R.; Schuur, E.A.G.; Shaver,
    G.R.; Verbyla, D.L. Carbon Loss from an Unprecedented Arctic Tundra Wildfire.
    Nature 2011, 475, 489–492. [Google Scholar] [CrossRef] [PubMed] Stow, D.A.; Hope,
    A.; McGuire, D.; Verbyla, D.; Gamon, J.; Huemmrich, F.; Houston, S.; Racine, C.;
    Sturm, M.; Tape, K.; et al. Remote Sensing of Vegetation and Land-Cover Change
    in Arctic Tundra Ecosystems. Remote Sens. Environ. 2004, 89, 281–308. [Google
    Scholar] [CrossRef] Chen, Y.; Romps, D.M.; Seeley, J.T.; Veraverbeke, S.; Riley,
    W.J.; Mekonnen, Z.A.; Randerson, J.T. Future Increases in Arctic Lightning and
    Fire Risk for Permafrost Carbon. Nat. Clim. Chang. 2021, 11, 404–410. [Google
    Scholar] [CrossRef] Bintanja, R.; van der Wiel, K.; van der Linden, E.C.; Reusen,
    J.; Bogerd, L.; Krikken, F.; Selten, F.M. Strong Future Increases in Arctic Precipitation
    Variability Linked to Poleward Moisture Transport. Sci. Adv. 2020, 6, eaax6869.
    [Google Scholar] [CrossRef] Yoseph, E.; Hoy, E.; Elder, C.D.; Ludwig, S.M.; Thompson,
    D.R.; Miller, C.E. Tundra Fire Increases the Likelihood of Methane Hotspot Formation
    in the Yukon–Kuskokwim Delta, Alaska, USA. Environ. Res. Lett. 2023, 18, 104042.
    [Google Scholar] [CrossRef] DeVries, B.; Pratihast, A.K.; Verbesselt, J.; Kooistra,
    L.; Herold, M. Characterizing Forest Change Using Community-Based Monitoring Data
    and Landsat Time Series. PLoS ONE 2016, 11, e0147121. [Google Scholar] [CrossRef]
    Disclaimer/Publisher’s Note: The statements, opinions and data contained in all
    publications are solely those of the individual author(s) and contributor(s) and
    not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility
    for any injury to people or property resulting from any ideas, methods, instructions
    or products referred to in the content.  © 2024 by the authors. Licensee MDPI,
    Basel, Switzerland. This article is an open access article distributed under the
    terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Hethcoat, M.G.; Jain, P.; Parisien, M.-A.; Skakun,
    R.; Rogic, L.; Whitman, E. Unrecorded Tundra Fires in Canada, 1986–2022. Remote
    Sens. 2024, 16, 230. https://doi.org/10.3390/rs16020230 AMA Style Hethcoat MG,
    Jain P, Parisien M-A, Skakun R, Rogic L, Whitman E. Unrecorded Tundra Fires in
    Canada, 1986–2022. Remote Sensing. 2024; 16(2):230. https://doi.org/10.3390/rs16020230
    Chicago/Turabian Style Hethcoat, Matthew G., Piyush Jain, Marc-André Parisien,
    Rob Skakun, Luka Rogic, and Ellen Whitman. 2024. \"Unrecorded Tundra Fires in
    Canada, 1986–2022\" Remote Sensing 16, no. 2: 230. https://doi.org/10.3390/rs16020230
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 7. Jan 17. Jan 27. Jan 6. Feb
    16. Feb 26. Feb 7. Mar 17. Mar 27. Mar 0 1000 250 500 750 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Remote Sens., EISSN 2072-4292, Published by MDPI RSS
    Content Alert Further Information Article Processing Charges Pay an Invoice Open
    Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For
    Editors For Librarians For Publishers For Societies For Conference Organizers
    MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia
    JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive
    issue release notifications and newsletters from MDPI journals Select options
    Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer
    Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Remote Sensing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Unrecorded Tundra Fires in Canada, 1986–2022
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Venkatesh P.R.
  - Radha Krishna P.
  citation_count: '0'
  description: Distributed Computing platforms involve multiple processing systems
    connected through a network and support the parallel execution of applications.
    They enable huge computational power and data processing with a quick response
    time. Examples of use cases requiring distributed computing are stream processing,
    batch processing, and client-server models. Most of these use cases involve tasks
    executed in a sequence on different computers to arrive at the results. Numerous
    distributed computing algorithms have been suggested in the literature, focusing
    on efficiently utilizing compute nodes to handle tasks within a workflow on on-premises
    setups. Industries that previously relied on on-premises setups for big data processing
    are shifting to cloud environments offered by providers such as Azure, Amazon,
    and Google. This transition is driven by the convenience of Platform-as-a-Service
    offerings scuh as Batch Services, Hadoop, and Spark. These PaaS services, coupled
    with auto-provisioning and auto-scaling, reduce costs through a Pay-As-You-Go
    model. However, a significant challenge with cloud services is configuring them
    with only a single type of machine for performing all the tasks in the distributed
    workflow, although each task has diverse compute node requirements. To address
    this issue in this paper, we propose an Intelligent task scheduling framework
    that uses a classifier-based dynamic task scheduling approach to determine the
    best available node for each task. The proposed framework improves the overall
    performance of the distributed computing workflow by optimizing task allocation
    and utilization of resources. Although Azure Batch Service is used in this paper
    to illustrate the proposed framework, our approach can also be implemented on
    other PaaS distributed computing platforms.
  doi: 10.1007/978-3-031-50583-6_2
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Distributed
    Computing and Intelligent Technology ICDCIT 2024: Distributed Computing and Intelligent
    Technology pp 18–33Cite as Home Distributed Computing and Intelligent Technology
    Conference paper An Improved and Efficient Distributed Computing Framework with
    Intelligent Task Scheduling Pruthvi Raj Venkatesh & P. Radha Krishna   Conference
    paper First Online: 04 January 2024 154 Accesses Part of the book series: Lecture
    Notes in Computer Science ((LNCS,volume 14501)) Abstract Distributed Computing
    platforms involve multiple processing systems connected through a network and
    support the parallel execution of applications. They enable huge computational
    power and data processing with a quick response time. Examples of use cases requiring
    distributed computing are stream processing, batch processing, and client-server
    models. Most of these use cases involve tasks executed in a sequence on different
    computers to arrive at the results. Numerous distributed computing algorithms
    have been suggested in the literature, focusing on efficiently utilizing compute
    nodes to handle tasks within a workflow on on-premises setups. Industries that
    previously relied on on-premises setups for big data processing are shifting to
    cloud environments offered by providers such as Azure, Amazon, and Google. This
    transition is driven by the convenience of Platform-as-a-Service offerings scuh
    as Batch Services, Hadoop, and Spark. These PaaS services, coupled with auto-provisioning
    and auto-scaling, reduce costs through a Pay-As-You-Go model. However, a significant
    challenge with cloud services is configuring them with only a single type of machine
    for performing all the tasks in the distributed workflow, although each task has
    diverse compute node requirements. To address this issue in this paper, we propose
    an Intelligent task scheduling framework that uses a classifier-based dynamic
    task scheduling approach to determine the best available node for each task. The
    proposed framework improves the overall performance of the distributed computing
    workflow by optimizing task allocation and utilization of resources. Although
    Azure Batch Service is used in this paper to illustrate the proposed framework,
    our approach can also be implemented on other PaaS distributed computing platforms.
    Keywords Distributed Computing Azure Batch Decision Tree PaaS CSP Access provided
    by University of Nebraska-Lincoln. Download conference paper PDF 1 Introduction
    Cloud transformation and distributed computing are two major fields that organizations
    presently emphasize to attain high efficiency in processing large amounts of data.
    The use of cloud resources and distributed computing as a PaaS (Platform as a
    Service) service has significantly reduced the implementation cost because of
    the pay-as-you-go model and techniques such as auto-scaling to optimize resource
    utilization. While these techniques are useful in reducing costs, there is a necessity
    for job scheduling algorithms that are efficient and adaptable to mitigate the
    following challenges: 1. Diverse Computing Resource Demands: Distributed computing
    (DC) jobs involve various tasks such as data ingestion, processing, and computation,
    each with different resource needs. While some tasks can work well on low-resource
    machines, others require high-memory, multi-core nodes. Distributed computing
    PaaS services lack flexibility in dynamically selecting compute nodes based on
    task type. These services only allow node initialization at job creation, thus
    limiting node type diversity. This restriction means tasks must use the same node
    type, irrespective of their resource requirements. This inability to dynamically
    change node type forces platform administrators to use the most optimal node for
    all tasks thus increasing costs. Figure 1 shows that only one option can be selected
    in the “VM Size” dropdown. 2. Inflexible Autoscaling Parameters: Although autoscaling
    is a useful method for managing sudden increases in workload, it cannot be handled
    at the task level. Certain tasks may require a greater number of nodes, while
    others may require fewer nodes. Figure 1 shows an example of Azure Batch where
    the only option available for autoscaling during pool creation is to select the
    total number of nodes using the “Target Dedicated Nodes” field. The value can
    be static or dynamically changed (auto-scaling) based on the number of tasks in
    the job, processor, or memory. Below are some of the impacts due to the above
    limitations: 1. High Execution Cost: High costs arise in distributed job execution
    when low-compute tasks are assigned to high-compute machines. For instance, a
    web service call that consumes time can be executed on a low-compute machine.
    However, if this call is allocated to a high-compute VM, the cost of execution
    increases. 2. High Execution Time: To achieve cost optimization, the development
    team would prefer the most optimal compute node or Virtual Machine (VM) to perform
    all the tasks in the job pool. This cost optimization may lead to high execution
    time as high compute requirements tasks are executed on low-compute machines.
    Fig. 1. Configuration screen for adding pool in Azure Batch Full size image The
    Intelligent Task Scheduling (ITS) framework addresses the outlined constraints
    by using a decision tree classifier to determine the optimal compute node for
    a specific task and its corresponding job pool. For data transfer between tasks,
    the framework leverages Message Queue [1] for smaller data blocks, such as text
    messages and JSON objects, while the Blob service [1] is employed for larger blob
    objects, such as files, images, and videos. The main contributions of the paper
    are as follows: 1. Proposed a novel framework for dynamically allocating compute
    resources to the DC tasks called ITS 2. Provided a decision tree classifier to
    determine the node type of a task. This approach is extensible as more parameters
    can be added to the model depending on the task requirement or through incremental
    learning. 3. Developed a task-driven node pool to streamline the restricted autoscaling
    setup. The auto-scaling configuration at the pool level is utilized to flexibly
    adjust node quantities, enabling dynamic expansion or reduction. The rest of the
    paper is organized as follows. The related work is described in Sect. 2. Section
    3 discusses the basic components of the PaaS batch service. Section 4 presents
    the proposed approach. In Sect. 5, we present the implementation approach in the
    cloud. Section 5 discusses the experimental results. Section 6 concludes the paper.
    2 Related Work Researchers have done considerable work in algorithms that optimize
    the compute resource utilization time in a distributed computing platform. However,
    little work has been done on optimizing resource utilization in a PaaS environment.
    Chen et al. [2] proposed an autoencoder-based distributed clustering algorithm
    that helped cluster data from multiple datasets and combined the clustered data
    into a global representation. The approach highlights the challenges of handling
    huge and multiple datasets from different computing environments. Daniel et al.
    [3] proposed different distributed computing cloud services that can be used for
    machine learning in big data scenarios. Nadeem et al. [4] proposed a machine-learning
    ensemble method to predict execution time in distributed systems. The model takes
    various parameters, such as input and distributed system sizes, to predict workflow
    execution time. Sarnovsky and Olejnik [5] proposed an algorithm for improving
    the efficiency of text classification in a distributed environment. Ranjan [6]
    provided an in-depth analysis of cloud technologies focusing on streaming big
    data processing in data center clouds. Al-Kahani and Karim [7] provided an efficient
    distributed data analysis framework for big data that includes data processing
    at the data collecting nodes and the central server, in contrast to the common
    paradigm that provides for data processing only at the central server. This process
    was very efficient for handling stream data from diverse sources. Nirmeen et al.
    [8] proposed a new task scheduling algorithm called Sorted Nodes in Leveled DAG
    Division (SNLDD), which represents the tasks executing in a distributed platform
    in the form of Directed Acyclic Graph (DAG). Their approach divides DAG into levels
    and sorts the tasks in each level according to their computation size in descending
    order for allocating tasks to the available processors. Jahanshahi et al. [9]
    presented an algorithm based on learning automata as local search in the memetic
    algorithm for minimizing Makespan and communication costs while maximizing CPU
    utilization. Sriraman et al. [10] proposed an approach called SoftSKU that enables
    limited server CPU architectures to provide performance and energy efficiency
    over diverse microservices and avoid customizing a CPU SKU for each microservice.
    Pandey and Silakari [11] proposed different platforms, approaches, problems, datasets,
    and optimization approaches in distributed systems. The approaches in the literature
    primarily focus on a) optimizing source data organization for efficient processing,
    b) task allocation based on execution order to available resources, and c) utilizing
    cloud services for distributed computing. However, these methods do not address
    the limitations of PaaS DC services. Our proposed framework tackles the deficiencies
    of PaaS DC services and offers strategies for enhanced processor utilization.
    3 Batch Basic Concepts This section introduces the core batch service concepts
    provided by various cloud providers. Figure 2 illustrates the components of the
    batch service. 1. Batch Orchestration: Batch Service provides a comprehensive
    set of APIs for developers to efficiently create, manage, and control batch services.
    This API empowers developers to handle every aspect of a batch, encompassing pool
    creation, task allocation, task execution, and robust error handling. 2. Task:
    A task is a self-contained computing unit that takes input, executes operations
    and generates subsequent task results. Configured during batch service creation,
    tasks run scripts or executables, forming the core of a DC job which is a sequence
    of tasks working toward specific goals. Batch facilitates parallel execution of
    tasks via its service APIs. 3. Job Pool: A job pool is a collection of tasks.
    Any task that must be executed must be added to the job pool. The batch service
    orchestrates the execution of this task on any of the compute nodes available
    in the node pool. Fig. 2. Components of Batch Service Full size image 4. Node
    Pool: VMs or compute nodes in the job pool are managed by the batch service, overseeing
    their creation, task tracking, and provisioning. It offers both fixed VM numbers
    and dynamic auto-scaling based on criteria. In batch service, VMs are also known
    as compute nodes. 5. Batch Storage: Blob storage is created by the batch service
    to manage the internal working of the service. Batch storage is used for storing
    task execution logs and binaries. The batch service orchestrates the installation
    of these binaries on all the VMs in the node pool. 6. Start-Up Task: The Start-Up
    task is the first task executed on the VM provisioned in the Node Pool. It contains
    the command to download binaries from batch storage and install them on the provisioned
    VM. 7. Cloud Services: The VMs in the node pool have access to all the services
    provided by the CSP. The VM commonly accesses services such as blob storage or
    message queue as a common store to persist and retrieve sharable data among the
    various tasks executed in parallel. 4 Proposed Approach In this section, we describe
    the proposed approach that is used for scheduling tasks in a PaaS distributed
    computing environment. We use an example of document processing from an external
    source to explain the proposed approach. Document processing involves document
    download (Task t0), text extraction (Task t1), image extraction and optical character
    recognition (OCR)[12] (Task t2) for images present in the document, entity extraction
    [13] from OCR output (Task t3), text summarization of the text extracted (Task
    t4), and updating extracted information to the database (Task t5). 4.1 Initialization
    The first step in the proposed approach is to identify the different tasks involved.
    All the tasks follow a specific sequence of execution called workflow to arrive
    at the results. These workflows can be represented as a directed acyclic graph
    (DAG) [14]. The graph nodes represent the tasks t ∈ T where T is a set of n tasks
    in the workflow. The edge between the nodes e ∈ E represents the tasks’ execution
    or the message flow between the tasks. Figure 3 shows the DAG containing 6 tasks
    and 6 edges. The individual tasks are represented as ti ∈ T, and the edge between
    task ti and tj is represented as (ti, tj) ∈ E, which indicates that the tj can
    be started after ti is completed. It also indicates that ti sends a message to
    tj. The first task (t0) with no incoming edge is the starting task, and a task
    (t5) with no outgoing edge is called an exit task. It can be noted from Fig. 3
    that document download is the first task in the workflow. The downloaded file
    is sent simultaneously to text extraction and image extraction. The output of
    text extraction is sent for text summarization and the text output of image extraction
    and OCR is sent to entity extraction. Once both activities are completed the last
    task would be to store the extracted summarized text and the entities extracted
    into a single record in the database. A message mi,j ∈ M is sent between node
    ti and tj and it is associated with each edge (ti, tj). Here M is the set of all
    the messages exchanged between the nodes in the workflow. mi,j contains a set
    of attributes created by the task ti and sent to tj for further processing. A
    message mi,j comprises of {mindex, ti, md0, md1, md2,…., mdn} where mindex is
    a unique value created by the starting task to uniquely identify all the tasks
    in the complete workflow, ti is the reference to the source task and md(0 to n)
    include all message data attributes required to execute the task tj. Each task
    tj is associated with the PaaS queue service qj, created to store the message
    mi,j, which comes from the task ti. Each task is associated with a compute node
    attribute set ai = { ai1, ai2, ai3, …., ain} where aij represents the compute
    node properties required to execute task ti. Table 1 shows task attributes and
    their values for the tasks shown in Fig. 3. The attributes include. Fig. 3. DAG
    Task Processing Order Full size image Table 1. Task Attributes Full size table
    1. Avg Execution Time: Average time required to execute the task 2. Processor
    Requirement: The possible values are High, Medium, and Low 3. Memory Requirement:
    The possible values are High, Medium, and Low 4. External Dependency: Jobs that
    wait for external dependencies like web requests or API calls. 5. Operating System:
    The host operating system is required to perform the task. These attribute sets
    are gathered during the development phase of the project. It can be noted from
    Table 1 that tasks t2 (Image extraction and OCR) and t4 (Text summarization) require
    high memory, processor, and Linux systems, whereas the rest of the tasks can be
    executed on Windows machines. All the distinct attribute set ai are consolidated
    into an attribute set A = {a1,a2,a3,…., an}, used for classifier training. Table
    2 shows the distinct attribute set obtained from Table 1. Table 2. Distinct Attribute
    Set Full size table 4.2 Classifier Training and Compute Node Mapping In the second
    step, a decision tree classifier is trained by taking the distinct compute node
    attribute set A and mapping them to a compute node type ci ∈ C, where C = {c1,
    c2, c3,….,cn} is a set of all the compute node types provided by the CSP. Table
    3 shows the mapping between the attribute set and the compute node types. The
    decision tree classifier model takes task attributes A and generates the predictions
    C represented as P(A) = C. After the training, the model is used to create tuples
    (T, C). The tuple contains the elements (ti, ci), which indicates that task ti ∈ T
    requires predicted compute node ci ∈ C to execute. Table 3 shows the example of
    the task and compute node mapping generated from the model. Table 3. Task Compute
    Node Mapping Full size table 4.3 ITS Framework The source documents are represented
    by the set X = {1, 2, 3, …n}, where n is the total number of items in the source
    dataset. The ITS framework contains three separate flows that execute in parallel.
    Figure 4 shows the working of the ITS for the tasks shown in Fig. 3. 1. Job Initializer:
    Responsible for initiating the workflow’s first task by processing input data.
    Pseudocode 1 outlines the job initializer steps. It reads and extracts necessary
    details from the source data, creating messages in q0 for each item. In the example
    of Fig. 4, the Job Initializer processes files f0 to fn in the source data repository,
    generating messages in queue q0 containing the location details of the file. The
    first message for file f0 in queue q0 is represented using m(0)0 where (0) in
    parenthesis represents the file number similarly for file f1 it is m(1)0. Fig.
    4. ITS Execution and Data Flow Full size image 2. ITS: Responsible for scheduling
    the tasks in multiple job pools to ensure optimal utilization of resources at
    the task level. The ITS looks for messages in all the queues and schedules the
    tasks in the predicted job pool. Pseudocode 2 captures the steps in the ITS, which
    are explained below: a. ITS keeps monitoring the queues for any messages. In Fig
    4 the ITS is monitoring q0 to q5. b. For the first task in the workflow, messages
    m(f)0 are read from the queue q0 after it is populated from the Job Initializer.
    In Fig. 4 the ITS will read messages m(0)0 to m(n)0 from q0. c. For subsequent
    tasks message m(f)i,j is read from the queue qj populated from the task ti. In
    Fig. 4, the ITS reads messages m(0)0,1 to m(n)0,1 from q1 similarly from other
    queues such as m(0)1,2 will read from q2 and m(0)2,3 will be read from q3. d.
    ITS checks the DAG in Fig. 3 to find parents for tasks tj. If multiple parents
    exist, the queue qj is searched for message m(f)ij for all the parent task ti
    using the unique task identifier mindex, and parent tasks ti and merged before
    executing the task tj. In example Fig. 4 the tasks t0 to t4 have single parents
    so message m(f)0 is consumed by task t0, m(f)0,1 is consumed by task t1, m(f)2,3
    is consumed by task t3, and so on. In the case of q5, task t5 has parent t4 and
    t3 so the messages m(f)4,5 and m(f)3,5 are merged before executing t5. e. ITS
    identifies the best suitable VM Type required to run the task tj. f. ITS creates
    the task in the tj job pool. The message data(md) in the message are passed as
    parameters to task tj. 3. Task Executor: The Task Executor is responsible for
    executing and writing the output message back to the child task message queue
    for the next task execution. Pseudocode 3 captures the steps in the Task Executor.
    The flow involves consuming the parameters sent through message data md, executing
    the binaries associated with the task, and writing the results to the child task
    message queue. The following are the task executions that happen (see Fig. 4):
    a. t0 (Document Download) downloads the file after reading the external file location
    in the message queue q0. The task stores the file in a common location in the
    local store and populates the message m0,1 in q1 and m0,2 in q2 with the location
    of the local store in the message. b. t1 (Text Extraction) extracts the text from
    the document by reading the local file store location and populates the message
    q4 with the contents of the extracted text. c. t2 (Image Extraction and OCR) extracts
    all the images from the document performs an OCR to extract the text and populates
    the message q3 with the contents of the extracted text. d. t3 (Entity Extraction)
    extracts entities from the message received from t2 containing OCR text output
    and populates the message in q5 with the entities extracted. e. t4 (Text Summarization)
    summarizes the text output obtained from t1 and populates the message in q5 with
    the summarized text. f. ITS merges the message data from t2 containing entities
    extracted and t4 containing the summarized text and triggers t5. g. t5 (Database
    Update) updates the extracted information into the database. 5 Experimental Results
    5.1 Dataset Details We illustrate our approach for the Oil Industry domain to
    extract structured and unstructured data from images. The dataset was sourced
    from the BSEE website [15], an open repository of oil and gas industry data. The
    goal was to make images searchable based on text content and well-data attributes.
    The experiment involved processing 1000 images in Azure, involving tasks such
    as image download, classification, attribute extraction, OCR, NLP, and search
    index update. Figure 5 shows the image categories in the dataset. Fig. 5. Image
    Categories of Test Data Set Full size image 5.2 Azure Setup for the Experiment
    Figure 6 shows the experimental setup in Azure [1]. 1. Azure Storage: Azure blobs
    are used to store the images. 2. Processing Layer: Consists of Azure Batch and
    Scheduled Jobs. Azure Batch is a distributed computing PaaS platform provided
    by Azure and Schedule Jobs are services that run scripts on a schedule. They are
    configured to execute Job Initialization and ITS. 3. Search Layer: Consists of
    Azure cognitive search service that provides metadata and free text search from
    the extracted content. 4. ML Studio: Hosts the classification model that derives
    the VM size required for the task. 5. Forms Service: Used to extract structured
    data(attributes) from images. Figure 7 shows attributes such as well name, and
    lease name extracted from the forms services. 6. Custom Vision: Used for categorizing
    the images present in the source dataset, as shown in Fig. 5. 7. Storage Table:
    Used to store the log table containing the task compute node requirement. Fig.
    6. Experimental setup of Azure Batch Full size image Fig. 7. Structured Data Extraction
    Full size image 5.3 Experiment Steps The execution steps are: 1. Classifier Training:
    This step involved training the classifier model with training data containing
    the task resource requirements. Table 4 contains the training data with a compute
    node requirement column containing the Azure VM [1] size most suitable for running
    the task. Table 4. Classifier Training Data Full size table 2. Task Attribute
    Update: This step involved adding task attributes along with execution times into
    the storage table. Table 5 shows the entries in the Storage Table. Table 5. Task
    Attributes Full size table 3. Compute Node Prediction: Run the classification
    model against the entries in the table storage (Table 5) to determine the VM size
    required for running the tasks. Table 6 contains the compute node mapping obtained
    for each task in the Job. The entries in Table 6 are updated to the Storage Table
    for scheduling the tasks. Table 6. Task Compute Node Mapping Full size table 4.
    Run distributed Job using Azure Batch: The experiment involved creating two pools,
    Low-Cost Pool containing Standard_A4_v2 [16] (4 core, 8 GB RAM) VM and High-Cost
    Pool containing Standard_A8_v2 [16] (8 core, 16 GB RAM) VM. The number of machines
    used in the experiment was limited, considering the execution cost involved. The
    experiment involved three execution modes. a) Low Cost – High Execution Time Approach:
    In this mode, we allocated three Standard_A4_v2 VMs in the Low Compute Pool and
    allocated the task of extracting data from 1000 images. b) High Cost – Low Execution
    Time Approach: In this mode, we allocated three Standard_A8_v2 VMs in the High
    Compute Pool and allocated the task of extracting data from 1000 images. c) ITS
    Approach: In this mode, we allocated two Standard_A1_v2 VMs in the Low Compute
    Pool and a single Standard_A8_v2 VM in the High Compute VM pool. We used the classification
    model to predict the task job pool. The allocation of tasks to the pool depended
    on the output of the prediction model and the number of jobs in the pool. If the
    job pool length is less than the threshold set to 10 tasks, any job will be allocated
    to the respective pool. The OCR extraction task was primarily allocated to the
    high compute pool, whereas all the other tasks were allocated to the low compute
    pool. This allocation procedure ensures that no processor is idle during the data
    extraction. Table 7 shows the execution time in all three modes. There is an 8%
    decrease in execution time of the ITS Approach compared to the Low Cost- High
    Execution Time Approach and a total reduction of 68% in cost when the ITS Approach
    is compared with the High Cost – Low Execution Time Approach. The percentage reduction
    in time is calculated using the total execution time captured in Table 7. The
    total reduction in cost is obtained by multiplying the execution time with the
    unit price for VM usage from the Azure VM price sheet [16]. A similar experimental
    setup can be done on batch services provided by other CSPs such as AWS [17] and
    Google [18]. Table 7. Batch Execution Results Full size table 6 Conclusion Distributed
    systems are computing platforms that can be used to handle large amounts of data
    processing. However, they can be costly depending on the time it takes to complete
    a job. This paper introduces a new framework that optimizes both the execution
    time and cost associated with running data processing tasks on a massive scale.
    The suggested technique includes the dynamic identification of the compute nodes
    to execute the task based on the classification model’s output. This model can
    be trained to optimize execution cost and execution time or additionally, it can
    be easily retrained with new parameters to enhance the system’s flexibility in
    accommodating new rules. References Directory of Azure Cloud Services | Microsoft
    Azure. https://azure.microsoft.com/en-in/products/ Chen, C.-Y., Huang, J.-J.:
    Double deep autoencoder for heterogeneous distributed clustering. Information
    10(4), 144 (2019). https://doi.org/10.3390/info10040144 Pop, D., Iuhasz, G., Petcu,
    D.: Distributed platforms and cloud services: enabling machine learning for big
    data. In: Mahmood, Z. (ed.) Data Science and Big Data Computing, pp. 139–159.
    Springer, Cham (2016). https://doi.org/10.1007/978-3-319-31861-5_7 Chapter   Google
    Scholar   Nadeem, F., Alghazzawi, D., Mashat, A., Faqeeh, K., Almalaise, A.: Using
    machine learning ensemble methods to predict execution time of e-science workflows
    in heterogeneous distributed systems. IEEE Access 7, 25138–25149 (2019). https://doi.org/10.1109/ACCESS.2019.2899985
    Article   Google Scholar   Sarnovsky, M., Olejnik, M.: Improvement in the efficiency
    of a distributed multi-label text classification algorithm using infrastructure
    and task-related data. Informatics 6(12), 1–15 (2019). https://doi.org/10.3390/informatics6010012
    Article   Google Scholar   Ranjan, R.: Streaming big data processing in datacenter
    clouds, pp-78–83. IEEE Computer Society (2014) Google Scholar   Al-kahtani, M.S.,
    Karim, L.: An efficient distributed algorithm for big data processing. Arab. J.
    Sci. Eng. 42(8), 3149–3157 (2017). https://doi.org/10.1007/s13369-016-2405-y Article   Google
    Scholar   Bahnasawy, N.A., Omara, F., Koutb, M.A., Mosa, M.: Optimization procedure
    for algorithms of task scheduling in high performance heterogeneous distributed
    computing systems. Egypt. Inform. J. 12(3), 219–229 (2011). https://doi.org/10.1016/j.eij.2011.10.001.
    ISSN 1110-8665 Article   Google Scholar   Jahanshahi, M., Meybodi, M.R., Dehghan,
    M.: A new approach for task scheduling in distributed systems using learning automata.
    In: 2009 IEEE International Conference on Automation and Logistics, pp. 62–67
    (2009). https://doi.org/10.1109/ICAL.2009.5262978 Sriraman, A., Dhanotia, A.,
    Wenisch, T.F.: SoftSKU: optimizing server architectures for microservice diversity
    @scale. In: 2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture
    (ISCA), pp. 513–526 (2019) Google Scholar   Pandey, R., Silakari, S.: Investigations
    on optimizing performance of the distributed computing in heterogeneous environment
    using machine learning technique for large scale data set. Mater. Today: Proc.
    (2021). https://doi.org/10.1016/j.matpr.2021.07.089. ISSN 2214-7853 Optical character
    recognition. https://en.wikipedia.org/wiki/Optical_character_recognition Entity
    Extraction. https://en.wikipedia.org/wiki/Named-entity_recognition Directed acyclic
    graph – Wikipedia. https://en.wikipedia.org/wiki/Directed_acyclic_graph Scanned
    Well Files Query. https://www.data.bsee.gov/Other/DiscMediaStore/ScanWellFiles.aspx
    Pricing - Windows Virtual Machines | Microsoft Azure. https://azure.microsoft.com/en-in/pricing/details/virtual-machines/windows/
    Getting Started with AWS Batch - AWS Batch. https://docs.aws.amazon.com/batch/latest/userguide/Batch_GetStarted.html#first-run-step-2
    Batch service on Google Cloud. https://cloud.google.com/blog/products/compute/new-batch-service-processes-batch-jobs-on-google-cloud
    Download references Author information Authors and Affiliations Department of
    Computer Science and Engineering, National Institute of Technology, Warangal,
    Telangana, India Pruthvi Raj Venkatesh & P. Radha Krishna Corresponding author
    Correspondence to P. Radha Krishna . Editor information Editors and Affiliations
    University of Picardie Jules Verne, Amiens, France Stéphane Devismes Indian Institute
    of Technology Guwahati, Guwahati, India Partha Sarathi Mandal Indian Institute
    of Technology Guwahati, Guwahati, India V. Vijaya Saradhi Florida Agricultural
    and Mechanical University, Tallahassee, FL, USA Bhanu Prasad Indian Statistical
    Institute, Kolkata, India Anisur Rahaman Molla Kent State University, Kent, OH,
    USA Gokarna Sharma Rights and permissions Reprints and permissions Copyright information
    © 2024 The Author(s), under exclusive license to Springer Nature Switzerland AG
    About this paper Cite this paper Venkatesh, P.R., Radha Krishna, P. (2024). An
    Improved and Efficient Distributed Computing Framework with Intelligent Task Scheduling.
    In: Devismes, S., Mandal, P.S., Saradhi, V.V., Prasad, B., Molla, A.R., Sharma,
    G. (eds) Distributed Computing and Intelligent Technology. ICDCIT 2024. Lecture
    Notes in Computer Science, vol 14501. Springer, Cham. https://doi.org/10.1007/978-3-031-50583-6_2
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-50583-6_2
    Published 04 January 2024 Publisher Name Springer, Cham Print ISBN 978-3-031-50582-9
    Online ISBN 978-3-031-50583-6 eBook Packages Computer Science Computer Science
    (R0) Share this paper Anyone you share the following link with will be able to
    read this content: Get shareable link Provided by the Springer Nature SharedIt
    content-sharing initiative Publish with us Policies and ethics Download book PDF
    Download book EPUB Sections Figures References Abstract Introduction Related Work
    Batch Basic Concepts Proposed Approach Experimental Results Conclusion References
    Author information Editor information Rights and permissions Copyright information
    About this paper Publish with us Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Computer Science (including subseries Lecture Notes in
    Artificial Intelligence and Lecture Notes in Bioinformatics)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An Improved and Efficient Distributed Computing Framework with Intelligent
    Task Scheduling
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Henriques J.
  - Caldeira F.
  - Cruz T.
  - Simões P.
  citation_count: '0'
  description: The broadening dependency and reliance that modern societies have on
    essential services provided by Critical Infrastructures is increasing the relevance
    of their trustworthiness. However, Critical Infrastructures are attractive targets
    for cyberattacks, due to the potential for considerable impact, not just at the
    economic level but also in terms of physical damage and even loss of human life.
    Complementing traditional security mechanisms, forensics and compliance audit
    processes play an important role in ensuring Critical Infrastructure trustworthiness.
    Compliance auditing contributes to checking if security measures are in place
    and compliant with standards and internal policies. Forensics assist the investigation
    of past security incidents. Since these two areas significantly overlap, in terms
    of data sources, tools and techniques, they can be merged into unified Forensics
    and Compliance Auditing (FCA) frameworks. In this paper, we survey the latest
    developments, methodologies, challenges, and solutions addressing forensics and
    compliance auditing in the scope of Critical Infrastructure Protection. This survey
    focuses on relevant contributions, capable of tackling the requirements imposed
    by massively distributed and complex Industrial Automation and Control Systems,
    in terms of handling large volumes of heterogeneous data (that can be noisy, ambiguous,
    and redundant) for analytic purposes, with adequate performance and reliability.
    The achieved results produced a taxonomy in the field of FCA whose key categories
    denote the relevant topics in the literature. Also, the collected knowledge resulted
    in the establishment of a reference FCA architecture, proposed as a generic template
    for a converged platform. These results are intended to guide future research
    on forensics and compliance auditing for Critical Infrastructure Protection.
  doi: 10.1109/ACCESS.2023.3348552
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 12 A Survey
    on Forensics and Compliance Auditing for Critical Infrastructure Protection Publisher:
    IEEE Cite This PDF João Henriques; Filipe Caldeira; Tiago Cruz; Paulo Simões All
    Authors 569 Full Text Views Open Access Comment(s) Under a Creative Commons License
    Abstract Document Sections I. Introduction II. Cip and Iacs Security Landscape
    III. Forensics IV. Compliance Auditing V. Analytics for Cip Fca: the Road Ahead
    Show Full Outline Authors Figures References Keywords Metrics Abstract: The broadening
    dependency and reliance that modern societies have on essential services provided
    by Critical Infrastructures is increasing the relevance of their trustworthiness.
    However, Critical Infrastructures are attractive targets for cyberattacks, due
    to the potential for considerable impact, not just at the economic level but also
    in terms of physical damage and even loss of human life. Complementing traditional
    security mechanisms, forensics and compliance audit processes play an important
    role in ensuring Critical Infrastructure trustworthiness. Compliance auditing
    contributes to checking if security measures are in place and compliant with standards
    and internal policies. Forensics assist the investigation of past security incidents.
    Since these two areas significantly overlap, in terms of data sources, tools and
    techniques, they can be merged into unified Forensics and Compliance Auditing
    (FCA) frameworks. In this paper, we survey the latest developments, methodologies,
    challenges, and solutions addressing forensics and compliance auditing in the
    scope of Critical Infrastructure Protection. This survey focuses on relevant contributions,
    capable of tackling the requirements imposed by massively distributed and complex
    Industrial Automation and Control Systems, in terms of handling large volumes
    of heterogeneous data (that can be noisy, ambiguous, and redundant) for analytic
    purposes, with adequate performance and reliability. The achieved results produced
    a taxonomy in the field of FCA whose key categories denote the relevant topics
    in the literature. Also, the collected knowledge resulted in the establishment
    of a reference FCA architecture, proposed as a generic template for a converged
    platform. These results are intended to guide future research on forensics and
    compliance auditing for Critical Infrastructure Protection. A Reference Architecture
    for Forensics and Compliance Audit (FCA) Systems Published in: IEEE Access ( Volume:
    12) Page(s): 2409 - 2444 Date of Publication: 01 January 2024 Electronic ISSN:
    2169-3536 DOI: 10.1109/ACCESS.2023.3348552 Publisher: IEEE Funding Agency: SECTION
    I. Introduction Modern societies are increasingly dependent on essential products
    and services provided by Critical Infrastructures (CIs), supported by Industrial
    Automation and Control Systems (IACS) such as power plants, energy distribution
    networks, transportation systems, and manufacturing facilities. These IACS are
    becoming larger and more complex, due to the increasingly complex physical processes
    they manage and the increasing amount of (heterogeneous) data generated by a growing
    number of interconnected control and monitoring devices. These IACS are also heavily
    dependent on common IT systems whose security, management, and compliance must
    also be considered. This evolving scenario requires new strategies to improve
    the associated Critical Infrastructure Protection (CIP) frameworks. A. The challenge
    of protecting Critical Infrastructures Critical Infrastructures (CIs) provide
    a series of essential services which are key to ensure the security, societal
    and economical activities of a country, thus constituting an attractive target
    for cyber-attackers [1], [2]. Smart grids, water, oil, and gas distribution networks
    are becoming more complex due to the growing number of interconnected distributed
    devices, sensors, and actuators, often widely dispersed in the field, as well
    as the increasing amount of information exchanged among system components. Water-to-Wire
    generation, microgeneration, smart metering, oil, and gas distribution, or smart
    water management, among others, are pushing the boundaries of the classic Industrial
    Cyber Physical Systems (CPS) model, fostering a new generation of IACS and the
    Industry 4.0 paradigm [3]. Naturally, such developments have an impact on IACS
    cybersecurity requirements, due to a substantial increase in the scale and complexity
    of the protected infrastructure [4]. This increase in terms of interconnections
    has a direct impact in terms of the vulnerable attack surface, exposing the IACS
    to both traditional and new threats. For instance, according to IBM Managed Security
    Services data [5], attacks targeting IACS have increased over 110 percent in 2016.
    This is linked with the growing connectivity of industrial systems. Network-based
    attacks targeting Critical Infrastructure (CI) are also becoming a greater concern,
    as state-sponsored groups have become more active. Their activities comprise unauthorized
    access to government and corporate networks with the main purpose of gathering
    information, although they can be potentially disruptive for CIPs [6]. This trend
    is already a major concern, and is expected to further intensify in the future
    [7], [8]. Other IACS security threats come from their increasingly distributed
    nature, regarding both the physical processes under control, which have also become
    more widely dispersed and interconnected, and the associated control applications,
    which have also become increasingly distributed, for sake of scalability, elasticity,
    adaptability, resiliency, and fault-tolerance. Overall, this scenario makes it
    difficult to understand the nature of incidents and to assess their progression
    and threat profile. Moreover, defending against those threats is becoming increasingly
    difficult, requiring orchestrated and collaborative distributed detection, analysis,
    and reaction capabilities. Continuously capturing live data from a running IACS
    system, that has an intrinsic volatile nature, presents important challenges to
    forensics investigators. For instance, volatile data in physical memory contains
    information about the current state of the system, such as process information,
    open network connections and encryption keys. Another challenge comes from the
    amount of data to be collected, analyzed, and stored for detecting and profiling
    cyberattacks. According to IBM [9], the world produces over 2.5 quintillion bytes
    of data every day, and 80% of it is unstructured (and not analyzed). To improve
    decision making, enterprises are facing new challenges to collect a large amount
    of available data, retrieved from heterogeneous sources (including structured
    and unstructured data), and enriching it with the inclusion of additional contextualized
    data. In the specific scope of CIP, to face the tremendous growth of raw data
    being produced by sensors and process controllers, a Big Data approach is required
    to handle massive amounts of data in intensive online and offline processing flows.
    The growth of volume and heterogeneity of data sources, systems, workloads, and
    environment variability contributes to the complexity of data management. Traditional
    approaches, such as Relational Database Management Systems (RDBMS), might not
    be able to handle the deluge of industrial data they are experiencing, especially
    while addressing the need for improved performance, reliability, and user experience
    [10]. Gaining critical business insights by querying and analyzing such massive
    amounts of data is becoming a vital requirement [11]. B. The need for better Forensics
    and Compliance Auditing Security incidents trigger a series of reactive activities,
    such as blocking access to and quarantining compromised systems, assessing the
    impact of the breach, mitigating the damage, and conducting forensics investigations
    to identify exploited vulnerabilities, identify the attackers, and enhance future
    defensive actions. In 2020, it took an average of 207 days to identify a breach,
    and 280 days to contain it [12]. Such a scenario results from the current solutions
    demanding a multi-step process where security analysts goes to multiple systems
    to retrieve uncorrelated data and then correlate it manually. Moreover, the complexity,
    skillset, and costs required to deploy and operate those solutions present a significant
    number of obstacles to their adoption. Therefore, in addition to other security
    tools, such as specialized probes, intrusion detection platforms and firewalls,
    forensics tools are increasingly important for security professionals. Such tools
    provide the means to extract relevant insights and evidence from large volumes
    of heterogeneous data produced from the sources within the CI, which can be leveraged
    both for forensics and security analysis purposes. Auditing compliance with applicable
    laws, regulations, policies, and standards processes also contributes to increase
    CI trustworthiness. However, such auditing processes are complex, since they need
    to use of a large number of tools, protocols and standards to correlate and enforce
    the audit compliance policies that may help to prevent future incidents. Aggregating
    such tools in a unified platform can reduce the complexity, effort, and costs
    associated with investigating and connecting individual alerts to uncover potential
    threats. Such a proactive approach may avoid the disruption of operations and
    prevent evidence from being lost or corrupted. Moreover, it will also help deal
    with the evolving cyber threats affecting CIs, preparing the platforms for post-incident
    forensics analysis. Due to the considerable overlap of functionalities associated
    with security forensics and compliance audit processes, it makes sense to consider
    them as unified platforms, which in this paper we generically designate as FCA
    frameworks– even though many tools are applied only to one of these areas, they
    still share most requirements and technologies. In this work, we highlight the
    importance of both forensics and compliance auditing as high-priority topics for
    CIP. C. Scope and Contributions This paper surveys the research trends, challenges,
    and gaps in the field of FCA for Critical Infrastructures, exploring the most
    relevant approaches, methodologies, and technologies. To the best of our knowledge,
    this is the first survey specifically focused on FCA applied to the CIP domain.
    Based on the lessons learned from the survey, this paper also presents: a classification
    taxonomy for the different aspects and technologies related with FCA systems.
    a reference architecture for converged FCA systems, identifying their key functional
    blocks. and a discussion of potential future directions for research on the subject
    of FCA for CIP. As illustrated in Figure 1, the rest of the paper is organised
    as follows: FIGURE 1. Structure of the paper. Show All The background of CIP and
    IACS security are introduced in Section II. The next three sections survey related
    works: Section III for forensics, Section IV for compliance auditing, and Section
    V for modern analytics applied to FCA, in the era of Big Data, AI and ML. Based
    on the lessons learned from the survey, the next two sections are devoted to classification
    and architectural models: Section VI proposes a new taxonomy for FCA systems for
    CIP, and Section VII introduces a reference architecture for FCA systems. Finally,
    Section VIII discusses achieved results and identifies open issues, while Section
    IX concludes the paper. SECTION II. Cip and Iacs Security Landscape In this Section
    we provide an overview of CIP, with a more detailed perspective on IACS security–
    since most CIs are based on some sort of industrial control frameworks. The role
    of this Section is to provide the reader with a more detailed perspective on how
    such systems are currently managed, from a security perspective, so that the associated
    needs, in terms of forensics and compliance auditing, become more clear. First,
    we discuss the role of IACS and Supervisory Acquisition and Data Control (SCADA)
    systems in CIP. Next, we introduce related security frameworks from NIST, ISO/IEC
    and other standards development organizations. Next, we address IACS security,
    introduce the concept of Security Information and Event Management (SIEM), and
    discuss other security analytics frameworks. A. The role of IACS and SCADA Systems
    in CIP As already mentioned, a large number of CIs are based on large-scale IACS
    or Industrial Control Systems (ICS) which, traditionally, use SCADA systems to
    manage physical processes such as energy production and distribution, water and
    sewage treatment, traffic management and railways. These SCADA systems can be
    roughly defined as a set of systems of command and control networks that control
    the operational sequence of the underlying physical processes. A typical SCADA
    system controlling CIs generally includes a control center and several field sites
    [13]. These sites are often distributed over a wide geographical area. Field sites
    are equipped with devices such as Program Logic Controllers (PLC)s or Remote Terminal
    Units (RTU)s [14], that control the on-site machines and periodically send information
    about the state of the field equipment to the control center. SCADA communications
    use a wide range of protocols, such as DNP3, Modbus, PCOM, ProfiNet, DeviceNet,
    ControlNet or Common Industrial Protocol [15]. In the early days, SCADA systems
    did not incorporate cyber-security mechanisms, since they were significantly resource-constrained
    and designed to run in isolated networks. They consisted of simple I/O devices
    transmitting signals between master and remote terminal units. Currently, SCADA
    systems can communicate over Internet Protocol (IP) networks, enabling its connection
    to the corporate network or even directly to the Internet, to integrate SCADA
    data with external systems such as Enterprise Resource Planning and Business Process
    Management tools. This interconnection of SCADA systems with wider networks brings
    new threats for which they were not originally designed, making them much more
    vulnerable. Moreover, CIs such as smart grids and water distribution networks
    have become increasingly complex due to the number of interconnected distributed
    devices, sensors and actuators, often widely dispersed in the field, and the larger
    amount of information exchanged both within the control system and between the
    control system and external systems. As pointed out by Ahmed et al. [13], Cornelius
    and Fabro [16], and Eden et al. [17], the different nature of SCADA systems also
    raises important challenges in the application of forensics, when compared to
    traditional approaches. Those classic forensics methodologies potentially interfere
    with the IACS operation, since they may introduce latency and cause critical processes
    to fail. Another challenge arises from the use of resource-constrained devices
    such as RTU and PLC, which often lack the storage and processing capabilities
    required by forensics tools. Also, SCADA logs might be not suitable for forensic
    investigation, as they are geared towards process management, not cybersecurity.
    Nonetheless, there is still a general lack of SCADA-specific forensics tools.
    To prevent known and unknown attacks, including security vulnerabilities and threats,
    organizations are adopting a common set of defense solutions such as firewalls,
    antivirus, Intrusion Detection System (IDS)s, Intrusion Prevention System (IPS)s,
    and SIEM [18], [19]. Eden et al. [17] provided an overall forensic taxonomy of
    the SCADA system incident response model and discussed the development of forensic
    readiness within SCADA system investigations, including the challenges faced by
    the SCADA forensic investigator and suggested ways in which the process may be
    improved. van der Knijff [20] identified possible sources of evidence in the investigation
    process in CI. Some of them include engineering workstations, databases, historian,
    Human Machine Interface (HMI), application server, Field devices like PLC, RTU,
    Intelligent Electronic Devices (IED), firewall logs, web proxy cache, and ARP
    tables. B. Security Frameworks Several security frameworks incorporate a series
    of documented processes used to define the policies and procedures around the
    implementation and management of information security controls in an enterprise
    environment. These frameworks are a blueprint for building an information security
    program to manage risk and reduce vulnerabilities by applying a function for identifying,
    protecting, detecting, and responding to activities. These frameworks can help
    information security professionals to define and prioritize the tasks required
    to manage their organizations’ security. Examples of IT security frameworks include
    Control Objectives for Information and Related Technology (COBIT) [21], ISO 27000
    series [22], NIST Special Publications 800–53 [23], 800–171 [24], NIST Cybersecurity
    Framework for Improving Critical Infrastructure Cybersecurity [25] and HITRUST
    CSF. The HITRUST CSF represents a certifiable framework that provides a comprehensive,
    flexible, and efficient approach to regulatory/standards compliance and risk management
    [26]. NIST SP 800–53 is the standard required by United States (US) federal agencies
    but could also be used by any company to build a technology-specific information
    security plan [27]. NIST 800–171 [24] provides federal agencies with recommended
    security requirements for protecting the confidentiality of controlled unclassified
    information. The ISO/IEC 27000 series provide key information security frameworks
    applicable to any industry [22], [28]. For instance, ISO/IEC 27004:2016 provides
    guidelines supporting organizations in assessing security performance and effectiveness
    indicators [29] to fulfill the requirements of ISO/IEC 27001:2013, with ISO/IEC
    27005:2018 providing guidelines for information security risk management. ISO/IEC
    27037:2012 provides guidelines on the handling of digital evidence, including
    identification, collection, acquisition, and preservation of potential digital
    evidence [30]. ISO/IEC 27038:2014 covers the techniques for performing digital
    redaction on digital documents [31]. ISO/IEC 27042:2015 provides guidance on the
    analysis and interpretation of digital evidence keeping continuity, validity,
    reproducibility, and repeatability [32]. ISO/IEC 27050 represents a group of standards
    (27050-1 to 27050-3) addressing the discovery of Electronically Stored Information,
    a term coined to refer to forensic evidence in the form of digital data [33].
    Also within the ISO/IEC 27000 series, ISO/IEC 27041:2015 provides guidelines on
    how to make sure that the methodologies and processes used to investigate information
    security events are suitable [34]. ISO/IEC 27043:2015 includes guidance for common
    incident investigation techniques across numerous incident investigation scenarios
    utilizing digital evidence, based on idealized models [35]. ISO/IEC 27006:2015
    specifies requirements and guidance providing audit and certification of information
    security management systems [36]. ISO/IEC TS 27008:2019 provides guidance for
    evaluating the implementation and operation of information security controls,
    including their technical assessment, following an organization’s established
    information security requirements, including technical compliance [37]. ISO/IEC
    27040:2015 provides technical recommendations on how organizations can establish
    an appropriate level of risk mitigation by using a tried-and-true approach to
    data storage security strategy, design, documentation, and implementation. Moreover,
    there are other relevant standards within the ISO/IEC frameworks, such as ISO
    21043-1:2018 that introduces important terms and definitions in forensic sciences
    [38], also providing the requirements for the forensic process with a focus on
    the recognition, recording, collection, transport, and storage of potential forensic
    items [39]. Also, ISO/IEC 30121:2015 is a framework for helping organizations
    to be prepared for digital investigation processes [40]. Regarding forensics education
    and training, the ASTM standards are worth mentioning [41]. ASTM E2678 helps promoting
    computer forensics by developing model courses that are compatible with other
    forensic science programs. ASTM E2917 provides core standards for forensic science
    practitioners’ training, continuing education, and professional development, including
    training criteria for competency, training documentation and implementation, and
    continual professional development. ASTM E2916 takes computer forensics, image
    analysis, video analysis, forensic audio, and facial identification are just some
    of the phrases and definitions that are utilized in the study of digital and multimedia
    evidence. Deciding upon the applicable regulatory or standardisation frameworks
    an organization must comply with must consider several factors such as the type
    of industry or country-specific compliance requirements. For example, US traded
    companies may start by complying with Sarbanes-Oxley [42] and COBIT. In case the
    company needs information security capabilities the option is ISO 27000 certification.
    NIST SP 800–53 is the standard required by US federal agencies but could also
    be used by any company to build a technology-specific information security plan.
    The HITRUST CSF integrates well with healthcare software or hardware vendors looking
    to provide validation of the security of their products. NIST 800–94 [43], was
    introduced in 2007 highlighting the challenges in the detection accuracy, extensive
    tuning, blindspots, and performance limits. C. IACS Security Although the protection
    of CI is a topic not necessarily dependent on technology, this survey is driven
    by a technological approach focused on IACS protection. IACS incorporate Control
    Systems (CS) designed to manage and control physical processes, constituting one
    of the main targets for CIP activities. These CS can be defined as manual or automatic
    mechanisms used to manage dynamic processes by adjusting or maintaining physical
    quantities such as mass, temperature, or speed. CS are classified in two distinct
    categories: open- and closed-loop. Open-loop CS generate their output based on
    input only, while in a closed-loop the output is used as a feedback mechanism
    together with inputs to generate new output [20]. CS are generally used for monitoring
    and controlling industrial and infrastructure processes and dispersed assets supported
    by centralized data acquisition and supervisory control, often constituing a CPS.
    In the scope of the so-called essential services, these CPS are vital, often being
    highly interconnected and mutually dependent. 2010’s Stuxnet [44] and 2015’s BlackEnergy
    [45], [46] demonstrated that the so-called security by obscurity approach is no
    longer adequate for CIs. Stuxnet was the first known malware specifically designed
    to target automation systems, infecting between 50,000 to 100,000 computers worldwide.
    BlackEnergy was directly responsible for power outages for 250,000 customers in
    western Ukraine. Since then, many other attacks targeting IACS were recorded,
    such as Gauss, Havex, and Shamoon [47]. This situation has prompted the development
    of suitable mitigation mechanisms to deal with cyberthreats against IACS which
    may compromise integrity, information/control confidentiality or availability
    [48], such as unauthorised accesses, break-ins, penetration attempts, and other
    forms of abuse, to detect and secure the automation infrastructure perimeter from
    attacks [49]. Among these mechanisms, IDS provide the means to monitor the infrastructure,
    detecting security anomalies or suspicious behaviour by resorting to signature
    (rule-based) [50] or anomaly detection strategies [51]. Due to their nature, IDS
    often constitute one of the most relevant data sources for FCA purposes, detecting
    threats and recording incident-related valuable evidence for forensics analysis
    purposes, helping understand attacks and prevent them in the future. While the
    IDS concept was borrowed from the Information and Communication Technologies (ICT)
    world, its deployment in IACS must obey a specific set of restrictions calling
    for the development of domain-specific approaches [52]. As a result, several proposals
    for IACS IDS have been presented over the past years, covering several levels
    of the automation infrastructure, from the field-level, as it is the case for
    the Shadow Security Unit (SSU) PLC security monitor [53], to higher levels, as
    it is the case for Rosa et al. [3], which presented a distributed security framework
    for IACS. IDS systems can be classified according to their targets, as it is the
    case for Network Intrusion Detection System (NIDS) and Host Intrusion Detection
    System (HIDS) [54]. IPS are the natural counterpart for IDS, providing active
    response capabilities, with Intrusion Detection and Prevention System (IDPS) combining
    both detection and response capabilities [54]. However, it must be said that automatic
    reaction mechanisms are often avoided by CI operators, due to the risk of a knowledgeable
    attacker abusing them for its own purposes. Nevertheless, components such as IDS
    can not provide an encompassing level of protection for the infrastructure, a
    situation that requires the adoption of a structured approach capable of providing
    collection, analysis and storage for monitoring information coming from the entire
    IACS infrastructure. SIEM systems, which will be next presented, constitute one
    of the most popular approaches to consolidate diversified and relevant information,
    leveraging it for analytics purposes. D. Security Information Event Management
    SIEM systems are designed to collect and correlate security log data (record of
    events that occurred on a computer or network device) from a wide variety of sources
    within organizations, including security controls, operating systems, and network
    infrastructure, systems and applications. Their data sources include log data
    and network telemetry data from flows and packets. Typically, their blocks include
    source device, log collection, parsing normalization, rule engine, log storage,
    and event monitoring [55]. Once the SIEM has the log data, data are normalized
    and further analysis generates alerts when suspicious activity is detected. Moreover,
    SIEM provides reports on the request of administrators. Some SIEM products can
    also act to block malicious activity, for instance by running scripts (e.g. triggering
    reconfiguration of firewalls and other security controls). Forensic investigations
    will benefit from correlating the collected data with the information from the
    context, including assets, users, threats, and vulnerabilities. As stated by Gartner
    [56], SIEM technology provides Security Information Management, log management,
    analytics, compliance reporting, and Security Event Management. They provide real-time
    monitoring and incident management for security-related events from networks,
    security devices, systems, and applications. SIEM technology is typically deployed
    to support three primary use cases: advanced threat detection, basic security
    monitoring, and forensics and incident response. Forensics and incident response
    contributes with dashboards and visualization capabilities, as well as workflow
    and documentation support to enable effective incident identification, investigation
    and response. Basic security monitoring includes log management, compliance reporting,
    and basic real-time monitoring of selected security controls. In the case of advanced
    threat detection, it includes real-time monitoring and reporting of user activity,
    data access, and application activity, incorporation of threat intelligence, business
    context and ad hoc query capabilities. At the most basic level, a SIEM system
    can be supported by rules or employ a statistical correlation engine between event
    log entries. Pre-processing may happen at collectors, with only part of those
    events being moved to a centralized management component, reducing, in this way,
    the volume of information being communicated and stored. Notwithstanding, this
    approach can discard important events too early [57]. Sun et al. [58] presented
    an event-linked network model to query and organize big volumes of data. In this
    model, events are primary units in organizing the data, whereas links represent
    the association among them. This model is applied in Cloud or virtual-environment
    analysis, as a huge quantity of involved data, such as the case of the Internet
    service provider with SIEM solution having a huge quantity of data at centralized
    locations. In 2021, Gartner Magic Quadrant for SIEM identified the following market
    leaders: Exabeam, IBM, LogRhythm, Rapid7, Securonix, Splunk, Splunk, HPE, and
    Intel Security [56]. A survey of those solutions, including an analysis of external
    factors affecting the SIEM landscape in the mid and long-term, can be found in
    [55]. Authors concluded that SIEM systems are slowly converging with Big Data
    analytics tools. E. Other Security Analytics Platforms Active security and forensic
    capabilities are typically offered separately by different security systems [59].
    While SIEM have pushed for the development of complementary approaches for collecting
    and analyzing event data to identify and respond to advanced attacks, several
    operators have found them to be somehow limited due to reasons such as the lack
    of orchestration capabilities, prompting the emergence of a new generation of
    security analytic technologies. Next, we introduce some of those tools and technologies.
    Endpoint Detection and Response (EDR) is a complementary software to SIEM, extending
    detection and response capabilities by acting as an additional log source. According
    to Gartner [60], EDR is “a SaaS-based, vendor-specific, security threat detection
    and incident response tool that natively integrates multiple security products
    into a cohesive security operations system that unifies all licensed components.”
    EDR solutions record and store system-level endpoint behaviors, and include several
    data analytics techniques to detect suspicious system behavior, provide contextual
    information, block malicious activity, and provide remediation suggestions to
    restore affected systems [61]. While the primary incident response tools for security
    teams are EDR platforms, emerging Extended Detection and Response (XDR) products
    integrate a set of security products into a cohesive security incident detection
    and response platform. Gartner defines them in a category aggregating and correlating
    telemetry from different sources to synthesize and draw conclusions to enable
    automated response actions. In comparison to XDR and SIEM and Security Orchestration,
    Automation and Response (SOAR) tools, XDR offers a higher level of integration
    of their products at deployment, with a focus on threat detection and incident
    response use cases. Moreover, while a SIEM can be be delivered in a Software as
    a Service (SaaS) model, most XDR products are developed using new cloud-native
    architectures, making them an emerging alternative or complement to existing SIEM
    tools. Despite such advantages, some of the SIEM use cases, such as generic log
    storage or compliance, are not replaced by XDR solutions [56]. The combination
    of Elasticsearch, Logstash, and Kibana from Elastic Stack, OpenSOC, Apache Metron,
    and other tools leveraged with or natively using Big Data platforms like Hadoop
    offers data collection, management, and analytics capabilities. Some security
    analytics platforms are available [9], [62], [63], [64], [65], [66], and many
    open-source solutions have been developed supporting a wide spectrum of security-based
    analysis [67], [68]. OpenSOC, for instance, was one of the open-source platforms
    incorporating scalable security analysis tools and providing, in many cases, an
    alternative to the expensive commercial SIEM-frameworks. It provides real-time
    security analysis and data analytics. The OpenSOC framework also integrates a
    great part of the Apache stack, such as Hadoop [69], Kibana [70] and Elasticsearch
    [71] to store, index, and enrich data sources, including network traffic and application
    log data. Apache Metron [68] is another example of those platforms, and the successor
    of OpenSOC. It also provides a full-stack software infrastructure for the analysis
    and detection of network intrusions, zero-day attacks, and advanced persistent
    threats. IBM QRadar is another security platform able to scale up in terms of
    performance and storage. It is designed to monitor, correlate and store large
    volumes of data. It includes searching capabilities over the indexed data and
    also provides key capabilities such as risk management, vulnerability management,
    incident forensics, incident response, and application. It also includes incident
    forensics to enable visibility to the questions who, what, when, where, and how
    a security incident occurred [72]. There are examples of security platforms specifically
    designed for CIP, as it is the case for the platform proposed by Gonzalez-Granadillo
    et al. [73], where processes’ events are received from multiple sources affecting
    a water CI to be correlated to generate security alarms accordingly, indicating
    the presence of a threat or an attack in the monitored systems. Another example
    is [52] and [4], which presents an hierarchical two-level correlation architecture
    for electricity grids, which later evolved into a Big Data solution, presented
    in [3]. F. Summary This Section was not intended to provide an exhaustive overview
    of the field, but rather to provide an encompassing perspective about the specifics
    of the CIP and IACS domains from a security standpoint, providing the reader with
    broad knowledge about the problems, limitations and the solutions being used by
    CI operators. These concepts are key for understanding the next two sections,
    which will be devoted to discussing the functions and role of forensics and compliance
    auditing capabilities. SECTION III. Forensics Forensics refers to the application
    of science and technology to an investigation process to find out the facts in
    criminal or civil litigation. It comprises collecting evidence of the occurred
    facts, records and digital trails that can be legally used for criminal prosecution
    [74]. Based on this data, backward tracing can be used to reconstruct the chain
    of events that led to an incident, with forward tracing helping understand the
    repercussions of that event. Moreover, such procedures are often undertaken for
    reasons other than legal, such as root cause analysis of system failures or incorrect
    procedures, based on operational traces. This Section will start with the definition
    of what a Forensic Process is, followed by a description of the associated investigation
    processes and a definition of digital and network forensics. Next, a brief survey
    on digital forensics is provided, followed by a discussion of the impact of cloud
    computing on forensics processes, data privacy aspects, forensics readiness. Forensic
    schemas and interoperability formats are also discussed, together with query and
    visualization tools. This Section closes with an overview of CPS forensics and
    the impact of Internet of Things (IoT) and Industrial Internet of Things (IIoT)
    on the forensics domain. A. What is a Forensic Process? Overall, the definition
    of what constitutes a forensic process is mostly coherent across different literature,
    regulatory and/or standardisation sources. For instance, Rani and Geethakumari
    [75] defined computer forensics as the science allowing to identify, extract preserve,
    and describe the digital evidence stored in digital devices and networks that
    can be legally admissible in court for any cyber-crime or fraudulent act. The
    National Institute of Standards and Technology (NIST) [76] defined digital forensics
    or computer forensics as a scientific method to identify, collect, examine and
    analyze data, also comprising a systematic investigation process of crimes in
    which evidence can be retrieved from the media contents found in the associated
    digital device. Casey [6] defined digital forensic investigation as a complex
    and time-consuming activity in response to a cybersecurity incident or cybercrime
    that should answer these questions: what happened, when, where, how, and who is
    responsible. Attacks against ICS and SCADA systems, such as Stuxnet [44], Dragonfly
    [77] or Flame [78], highlighted the relevance of forensic investigations for post-mortem
    analysis. In many cases, this has prompted operators to design and implement defense
    and forensic readiness strategies, encompassing actions and procedures to provide
    the capabilities to diagnose incidents and support the identification and prosecution
    of attackers. Such capabilities can also be helpful to deal with harmful events
    such as natural disasters or hardware malfunctions, by providing the capabilities
    to analyse the underlying SCADA Information Technology (IT) system [13]. These
    approaches gain more significance as breaches in SCADA systems may cause dangerous
    consequences for both human life and the infrastructure, beyond significant monetary
    loss or service disruption [79], [80], [81]. While most cybersecurity tools are
    focused on detecting and monitoring, forensic tools are focused on collecting
    and recording traffic and events while, at the same time, providing feedback information
    to the security actors. Relevant operational events are monitored and recorded
    using a forensic approach akin to a system black box, providing the means to investigate
    and retrieve evidence. Also, it should be possible to trace the attack, prepare
    mitigation actions, adjust countermeasures, apply damage control policies or even
    recover from partial or total failure. B. The Forensic Investigation Process According
    to Hunt [59], the main purpose of intrusion analysis and collection of forensically
    sound data is to seek answers to the following questions: Who is responsible for
    the incoming intrusion or outgoing data transfer? What kind of equipment and services
    were involved? Were they able to do this because of limitations of incoming or
    outgoing security mechanisms? As illustrated in Figure 2, according to authors
    such as Whitman and Mattord [54], the forensic investigation process follows the
    basic methodology: FIGURE 2. Forensic investigation process. Show All Preparation,
    including the identification of the relevant items bringing value to evidence.
    Acquisition of evidence with preservation, without alteration or damage. Assure
    at every step the evidence is verifiably authentic and remains unchanged since
    the time it was seized. Evidence examination and analysis of the data without
    risking modification or unauthorized access. Report the findings to the proper
    authority and take the lessons learned. In this context, evidence may refer to
    a physical object or documented information about a past action that may help
    disclose the intent of a perpetrator [54], support an alibi [6] or provide legally
    admissible proof. It should be checked whether it was obtained legally as a result
    of a court order or by another order of an authorized institution or person. The
    forensic investigation process should be able to capture evidence before processes
    or services on the running system overwrite useful volatile data [13]. This may
    be justified for a wide array for scenarios such as disputed transactions, allegations
    of employee misconduct, presenting legal and regulatory compliance, negligence
    and breach-of-contract charge avoidance, assisting law enforcement investigations,
    meeting disclosure requirements in civil claims, or supporting insurance claims
    when a loss occurs. Digital evidence comprises the data stored or transmitted
    using computing means, which may be used for incident analysis and/or proof purposes.
    In the course of a forensic investigation, it should be assured that all available
    digital evidence is not only protected from deletion but also from modification
    without appropriate authorization [82], with all steps being recorded [83]. This
    is vital for integrity purposes, also protecting data from anti-forensics activities,
    which comprise the techniques aiming at hampering the forensics process, destroying
    or modifying any digital evidence [84]. For forensics applications, digital evidence
    integrity is a key property as its violation invalidates the admissibility of
    data for proof purposes. A cryptographic hash can be used to assess the integrity
    of the evidence, as well as the copies used along with the examinations and analysis
    results of compromised systems — this way, an examiner can rely on data he is
    working on, confident is exactly the one originally captured. A hash can be computed
    in the moment data is produced and used until the moment integrity is checked,
    allowing to detect abnormal situations, for instance, when an inconsistent data
    image does not accurately represent the state of the data acquisition [13]. Data
    provenance, which provides contextual information related to the origin of data,
    can support detailed explanations on how a specific state was reached, being included
    in evidence as a statement from the person carrying out the extraction. It specifies
    the source system, the acquired artifacts to denote the chain of custody as an
    audit trail of all activities, and a timestamp of data extraction [85]. Several
    approaches have been proposed to implement provenance tracking (e.g., ES3 [86],
    PASS [87], SPADE [88], Story Book [89], TREC [88]). Still in this scope, Zafar
    et al. [90] proposed a taxonomy of existing secure provenance schemes. Data provenance
    analysis can be used to extract host events into provenance graphs that represent
    the entire system execution and help causal analysis of system actions. Some of
    the recent works focused on fidelity [85], [91], [92], [93], [94], [95], while
    others focused instead on efficiency [83], [95], [96], [97], [98], [99], [100],
    [101]. Data provenance can also help reducing alert fatigue [102] and identifying
    intrusions [103], [104], [105]. The highly volatile nature of digital evidence
    implies that a careful integrity safeguarding approach should be followed. This
    chain of custody process intends to help preserving the integrity of the information,
    providing a non-repudiable chronological trace [106] detailing how evidence was
    acquired, processed/analyzed, handled, stored, and protected, to be presented
    as admissible evidence in court [107]. A chain of custody ensures the collected
    evidence is not modified along the investigation process and from the moment it
    was collected until it is presented [108]. Prayudi and Sn [109] provided an overview
    of the state of the art about challenges in the digital chain of custody. Cosic
    and Baca [110] presented a digital evidence management framework aiming to improve
    the chain of custody of digital evidence in all stages of the digital investigation
    process, supported by the use of SHA-2 hash function for the digital fingerprint
    of evidence. C. Digital and Network Forensics In 2008, the American Academy of
    Forensic Sciences (AAFS), one of the most widely recognized professional organizations
    for all established forensic disciplines, recognized forensic computer-related
    crime investigation as a legitimate area, for which a new Digital and Multimedia
    Sciences Section was allocated [6]. This enabled the development of a common ground
    for the forensic science community to share knowledge and address current challenges
    [111]. Digital forensics deal with evidence extraction, preservation, identification,
    documentation, and analysis using well-defined law enforcement procedures, establishing
    clear lines within the chain of custody. According to McKemmish [112], digital
    forensics can be broadly considered as having four stages, namely: identification,
    preservation, analysis, and presentation. Several methods have been proposed in
    the literature, aiming to formally reconstruct the sequence of events executed
    during the incident using proven methods [113]. However, the significant growth
    in the volume of data and the number of evidence items coming from a wide range
    of sources raises new challenges when conducting digital forensic investigations.
    Imaging, hashing, and carving are among the available techniques used by digital
    forensics investigations. Imaging consists of copying storage media to be examined
    as evidence. Such evidence can be compromised by modern Operating Systems (OS),
    due to the operations in the background on the file system, such as indexing or
    journal resolution [114]. Cryptographic hashing or signing is used to provide
    authenticity and integrity of files and other evidence. For instance, Afzaal et
    al. [115] presented an architecture aiming to overcome the limitations of the
    classic RSA algorithm to provide event integrity protection, allowing a group
    of n parties to participate in the digital signature process to enforce authenticity
    and non-repudiation. As for hashing techniques, while MD5 hashing was originally
    adopted by the forensics community [116], it was later superseeded by SHA-1 as
    a NIST federal standard, with a transition timeline towards SHA-2 or SHA-3 being
    announced in December 2022. Carving refers to the forensic tools to scan unused
    disk blocks to find and recover deleted data. Carving uses known header and footer
    signatures to combine the non-used nodes into the original deleted files. Mikus
    [117] conducted an analysis supported by the use of carving techniques. Recent
    advances in carving included recovering capabilities of fragmented files with
    more accuracy [118]. Within the digital forensics field, network forensics is
    concerned with monitoring network traffic to assess anomalies and attacks. To
    investigate such attacks, several data sources are available, including packet
    filters, firewalls, intrusion detection systems, honeypots, sinkholes, surveillance
    and vulnerability scanning systems [59]. Software Defined Networking (SDN) was
    also leveraged by Bates et al. [119] to deploy capture points over the network
    to have a holistic view of network activity, which can be used for forensics purposes.
    Reference [120] also discusses the challenges of executing network forensics investigations
    in virtual networking environments with tunneling and SDN. Nevertheless, one of
    the most important challenges in terms of network forensic has to do with the
    required data storage and computing capabilities [121]. For instance, even a moving
    window of some hours covering the duration of relevant real-time traffic may require
    a significant amount of storage from a computing cluster, something that may be
    aggravated in case of sustained attacks D. A Brief Survey on Digital Forensics
    There is a considerable corpus of related literature on digital forensics, whose
    focus is equally diverse. In this line, Casino et al. [122] reviewed several works
    in the field of digital forensics and identified their main topics and challenges.
    Regarding methodological aspects, Sommer [123] raised awareness of the challenges
    involved in gathering, analyzing, and presenting digital evidence among directors,
    managers, and their professional advisers, with Williams [124] providing direction
    to those who assist in the investigation of cyber security incidents and crimes,
    not just for law enforcement. van Baar et al. [125] reported benefits and performance
    on processing digital forensic investigations on a particular case involving collaboration
    between different actors. Regarding the subject of digital forensics frameworks
    and other architectural developments, Verma et al. [126] proposed a digital forensic
    framework that uses case information, case profile data and expert knowledge for
    automation of the digital forensic analysis process supported by Machine Learning
    (ML) for finding evidence. Hunt and Slay [59] advocate the need of a new forensic
    analysis approach requiring the implementation of forensic engines, supported
    by parallel processing while providing flexibility on customizing activities for
    the analysis of evidential data. Ahmadi-Assalemi et al. [127] presented a federated
    Blockchain model that achieves forensic-readiness by establishing a digital Chain-of-Custody
    and a collaborative environment to qualify as digital witness for post-incident
    investigations. Specifically on the CIP scope, Ahmed et al. [128] highlighted
    that forensic analysis for ICS is still in its early development stages, due to
    its specialized nature, together with the prevalence of proprietary and poorly
    documented protocols. Nevertheless, Kilpatrick et al. [129], [130] and Chandia
    et al. [131] proposed an architecture allowing to capture and analyse sensor data
    and control actions in a SCADA network (using agents located at strategic positions
    within the network to capture the local traffic and forward a relevant portion
    of packets, called a synopsis, to a data lake). Also, Elhoseny et al. [132] proposed
    a conceptual framework for automated and secure forensic investigation in modern
    complex SCADA networks, intentionally designed to comply with green computing
    requirements. Eden et al. [17] suggested deploying forensic hardware instrumentation
    connected to field device artefacts as a wrapper implemented at physical level,
    in order to improve the availability and recovery of information for cases where
    SCADA devices have restricted physical access.Valli [133] created a framework
    that produces forensically verified signatures for the Snort IDS for known and
    published vulnerabilities of SCADA, enabling investigators to trace exploits during
    analysis. There are also many works focused on identifying existing gaps and/or
    challenges, some which also proposing suitable solutions to address them. For
    instance, Huang [134] realized that the characteristics of big data complexity
    (e.g., volume and variety) make traditional data mining algorithms unsuitable
    to retrieve knowledge in forensics scenarios, something that Quick and Choo [135]
    also address, highlighting the challenges posed to digital forensic analysis (considering
    the ongoing growth in the volume of data seized and presented for analysis). These
    conclusions are reinforced by Koven et al. [136], who noticed a lack of suitable
    analysis tools for large datasets– despite the focus on email datasets, the findings
    are likely to be broadly applicable to other types of sources. Stelly and Roussev
    [137] presented the concept and prototype implementation of the first domain-specific
    language aimed at providing a practical and formal description of digital forensic
    investigations as a computation. Regarding causality analysis for attack investigation,
    several works have considered provenance graphs for tracking based on audit logs.
    Their approach is mainly related with the sub-topics based on causality, anomalies
    and learning analysis. As an example, Zipperle et al. [138] surveyed the literature
    on provenance-based IDS and proposed a taxonomy. Alsaheel et al. [139] proposed
    a framework to identify and reconstruct end-to-end cyber attack stories from unmodified
    systems and software audit logs. Kwon et al. [140] developed a model supported
    by causality-based inference for audit logging. Ma et al. [141] also proposed
    a provenance tracing system capable of alternating between logging and unit-level
    taint propagation, and event processing. Considering digital forensics performance
    and assessment, Ayers [142] proposed several metrics for measuring the efficacy
    and performance of forensic tools, such as speed, accuracy, completeness, reliability,
    and auditability. Roussev and Richard [143] discussed the need of distributed
    forensics approaches, highlighting the performance benefits inherent to distributed
    computing and proposing a distributed digital forensic tool to centralize data
    and distribute processing over multiple devices, with background preprocessing
    capabilities of multiple concurrent searches. Daubner et al. [144] presented research
    towards verification of forensic readiness in software development, with a focus
    on produced digital evidence. The topic of anti-forensics techniques and prevention
    is also addressed in the literature and has been the subject of research [145].
    As an example, Rekhis and Boudriga [113] developed and demonstrated an anti-forensics
    aware theoretical digital investigation approach, with Noura et al. [146] proposing
    a solution to prevent anti-forensics techniques targeting log availability and
    integrity (such as wiping and injection attacks), using encryption, fragmentation
    and authentication for data distribution across several storage nodes. E. Cloud
    Forensics The cloud computing paradigm, which shifts information from endpoint
    devices to a provider infrastructure [147], has become popular among many organizations
    due the potential cost and resource efficiencies it might entail, also offering
    several operational benefits for CI, including data redundancy, data availability
    and survivability when essential system components are isolated or lost [148].
    Its introduction raises new and substantially different challenges for forensics,
    since the target environment is no longer isolated and data is no longer acquired
    under the investigator’s control– thus, there is an evident need to go beyond
    traditional approaches [149]. In this scope, NIST identified 65 challenges of
    conducting digital investigation in cloud environments, also pinpointing existing
    technical gaps [150]. Forensics activities in the cloud present important challenges.
    Aspects such as the distribution of computing and storage (which have an impact
    in terms of increased attack surface), geographical storage dispersion across
    distinct jurisdictions with specific procedures and laws, privacy, or even the
    lack of norms on aspects such as Service Level Agreement (SLA) regulating the
    client and Cloud Service Provider (CSP), raise new complex challenges to forensics
    investigators [151]. Even if the CSP is compliant with the law enforcement agencies
    in its respective jurisdictions, cloud forensics may be a costly and time-consuming
    procedure [152], moreover considering how much storage may be used on the tenant
    storage pool. The increased need for forensic investigations involving cloud-based
    scenarios has prompted the emergence of cloud forensics [153], a hybrid approach
    encompassing remote, virtual, network, live, and large-scale operations, geared
    towards the generation of digital evidence from cloud environments. Moreover,
    cloud-based forensic architectures (which may still be used with private clouds)
    can be seen as an online solution to help remove any hardware dependency [154],
    [155]. This can enhance forensic experts and investigators activities with the
    tools and processes to be applied in the digital investigation of the collected
    evidence such as sorting, indexing, data recovery or bookmarking, among others.
    In this line, van Beek et al. [156] shared the lessons learned from providing
    Digital Forensics as a Service (DFaaS) implementations for almost 10 years, discussing
    the organizational, operational and development perspective, in a forensic and
    legal context. Zawoad et al. [157] presented an architecture for a secure cloud
    logging service, collecting information from different sources around the datacenter,
    both software (hypervisors) and hardware (network equipment), in order to create
    a complete landscape of the operations in a datacenter. Similarly, Zawoad et al.
    [158] proposed a Secure-Logging-as-a-Service (SecLaaS) to enhance forensic investigation
    in the cloud ecosystem that enables the acquisition of admissible log evidence
    in the cloud. A literature review revealed several cloud forensics framework proposals.
    Manral et al. [159] surveyed the cloud forensic literature published between January
    2007 and December 2018, categorized using a five-step forensic investigation process,
    and included a taxonomy of existing cloud forensic solutions as well. Ruan and
    Carthy [160] described the need for new forensic tools or to extend the existing
    digital forensic tools to make them fit into Cloud frameworks, also presenting
    a forensic tool for OpenStack Cloud which works through a daemon running in a
    compute node delivering network logs and the images of instances to the dashboard.
    Rani and Geethakumari [75] describe a snapshot-based approach to face the dynamic
    nature of Cloud in which the CSP takes a snapshot of a suspected Virtual Machine
    (VM) when an anomaly is found by an IDS, isolating it from the network and storing
    it in permanent storage. A similar approach was suggested by Hibshi et al. [161],
    which presents a study highlighting a number of usability points that need to
    be taken into consideration when designing and implementing digital forensics
    tools, also proposing an efficient approach to forensic investigation in the cloud
    using VM snapshots. Yu et al. [162] presented a framework for automated detection
    of anomalies in a cloud environment including a module for cloud forensics with
    learning capabilities embedded in the management layer of the cloud infrastructure.
    Patrascu and Patriciu [163] claimed there should be a revision of the classic
    network forensic principles, and a reorganization of well-known workflows, taking
    in consideration tools such as ML or large scale computing. A hypervisor-based
    approach has been considered for threat monitoring and forensic analysis in [164],
    where the hypervisor provides the means for examining VMs, by monitoring activities
    performed at a layer between the hardware and the virtual environment. The potential
    of this approach was demonstrated by Mishra et al. [165], which presented a taxonomy
    of hypervisor forensic tools and demonstrated how evidence that can be found in
    a VM, at the hypervisor and host system layers. Saibharath and Geethakumari [166]
    proposed a remote forensic evidence collection and pre-processing framework for
    cloud nodes that collects VM disk images, logs and network captures, pushed periodically
    into a Hadoop distributed file system. Huseinović and Ribić [167] evaluated the
    virtual machine memory dumps from Oracle VirtualBox and VMware VMs, with Cheng
    et al. [168] proposing a similar concept for a lightweight live memory forensic
    framework based on hardware virtualization that can build a virtualization environment
    on-the-fly. Also, Zhang et al. [169] and Guangqi et al. [170] proposed a KVM-based
    approach to acquire both data and VM meta-data, using the access and control privileges
    of a VM host to acquire VM-related information. In alternative to VMs, the combination
    of containers and microservices can help improving isolation between components
    in an cloud-bative application, with a reduced overhead. However, the topic of
    forensic investigation in containerized environments is a complex task raising
    new challenges [120], due to the fact that instances can be started and stopped
    on different systems, which results in an ongoing change in the structure of the
    network, as well as their shorter life span which implies that container instances
    may not be available anymore when a investigation process is triggered. Which
    such environments in mind, Sharma et al. [171] presented a deep learning approach
    for containerized application runtime stability analysis, and an intelligent publishing
    algorithm that can dynamically adjust the depth of process-level forensics published
    to a backend incident analysis repository. Stelly and Roussev [172] presented
    a scalable containerized framework for forensic computations. Other works have
    proposed procedures and standards for forensics activities in the cloud. Saibharath
    and Geethakumari [173] developed a framework for cloud forensics in OpenStack,
    according to the Infrastructure-as-a-Service model and using existing forensic
    tools, which is able to take live snapshots, image evidence, packet captures and
    log evidence. Banas [174] discussed the memory acquisition process to be placed
    in a kernel based virtual machine (KVM) storage and memory images in OpenStack
    without any CSP interaction in a self-service Cloud environment. The NIST Cloud
    Computing Forensic Science Working Group (NCC FSWG) [175] was established to research
    on Cloud forensic science challenges in the Cloud environment and to develop plans
    for measurements, standards and technology research to mitigate the challenges
    that cannot be handled with current technology and methods. Almulla et al. [152]
    proposed a forensic procedure based on the NIST model to examine private cloud
    VM snapshots, using existing digital forensic tools, being able to successfully
    acquire data without the need to transform the snapshot files. There is also an
    emerging line of work regarding the use of Blockchain for FCA purposes, providing
    a tamper-resistant ledger mechanism which matches the needs for non-reputiation
    and chain of custody purposes. In this scope, Liang et al. [176] proposed a decentralized
    and trusted cloud data provenance architecture using blockchain technology. Also,
    Awuson-David et al. [177] and Ahmadi-Assalemi et al. [127] presented Blockchain-enabled
    methodologies and frameworks for keeping a chain of custody of the digital forensic
    log evidence from the cloud ecosystem, to ensure trustworthiness, integrity, authenticity
    and non-repudiation. Finally, [178] proposed a cloud forensics taxonomy and denoted
    the trend towards the implementation of digital provenance assurance using blockchain
    technology. F. Data Privacy Protection in Digital Forensics Privacy can be defined
    as the right to control who has information about someone, including activity
    tracking [179]. Some of the concepts raised in privacy laws intend to establish
    limits restricting data use or its correlation from multiple sources, often mandating
    anonymization or removal of personal data from records [179]. Such an example
    is the General Data Protection Regulation (GDPR), introduced in 2016 to bring
    protection to personal data [180], making it mandatory to obtain consent on the
    use of personal data. The problem of balancing forensic investigation needs with
    privacy protection requirements is discussed by Aminnezhad et al. [181], with
    Dehghantanha and Franke [182] having established the foundations for the definition
    of privacy-respecting digital investigation as a new cross-disciplinary field
    of research, also reviewing the state of art in this field. Despite the large
    number of digital forensic models discussed in scientific literature, just a few
    of them are considering data privacy along the digital forensic investigation
    process, many of which are either tailored for specific environments or included
    as an independent module [126]. van Staden [183] proposed a framework to protect
    privacy in multi-user environments that are subject to post-incident forensics
    investigation, supported by profiling and filtering mechanisms. Law et al. [184]
    described a way to protect data privacy using encryption, proposing the introduction
    of simultaneous data encryption processes by email servers and indexing of related
    keywords, allowing an investigator to give a keyword input to the server owner,
    who has the encryption keys, to get back the emails that contain the keyword.
    Also regarding encryption-based approaches, Hou et al. [185] proposed a mechanism
    to protect data privacy on a third-party service provider’s storage center, using
    homomorphic and commutative encryption, with Hou et al. [186] describing a similar
    solution. As for identity or knowledge-based approaches, Shebaro and Crandall
    [187] used an identity-based encryption mechanism to carry out a network traffic
    data investigation in privacy preserving setting. Croft and Olivier [188] proposed
    a mechanism where data is divided into layers of sensitivity, placing less private
    data on lower layers, and highly private data on higher layers. In this schema,
    access to private information is controlled by initially restricting investigator
    access to the lower layers, requiring further proof to get access to higher-level
    information. G. Digital Forensic Readiness and Forensics-by-Design For many, the
    possibility of a security incident should be regarded as a certainty rather than
    a possibility [189]. In fact, when incidents happen, the priority is often restoring
    normal operational levels, instead of making an effort to collect and preserve
    as much forensics evidence as possible, eventually to be admitted to a court.
    The generalised approach is mostly reactive: first restore operational capacity,
    and then carry out investigations and seek evidence. As a result, evidence might
    be lost or rendered unsuitable as proof. Forensic readiness is a concept that
    contributes to minimise the aforementioned problems. It suggests taking proactive
    actions to capture evidence even before or during an incident and before investigations
    are started. This helps not only to save time and money, but also to mitigate
    potential incidents and ensure business continuity and compliance with minimal
    disruption and interruption of operations. Kruger and Venter [190] provided a
    systematic literature review to identify topics where digital forensic readiness
    is included. However, as denoted by Iqbal et al. [191], digital forensic readiness
    for CIP is still immature, judging by the lack of published research or industry
    reports. Forensic readiness comprises planning activities to collect, preserve,
    protect and analyze digital evidence to be effectively used [192]. It can also
    assist in fulfilling the increasing demand for the implementation of security
    practices addressing compliance with organizational policies and regulatory requirements,
    providing the means to deploy continuous monitoring and review processes supported
    by the already collected forensic data. This approach can help fill that gap,
    since even common standards such as the ISO 9001 series and regulatory frameworks
    for B2B relationships (e.g. supply chain risk management) do not account for best
    practices in the CI and IACS security domains. Forensic-by-design extends the
    concept of Digital Forensic Readiness. Similarly to Security-by-design, it advocates
    the integration of forensic requirements into the system’s design and development
    stages. Ab Rahman et al. [189] proposes a system and software engineering driven
    Forensic-by-design framework, with an emphasis on Cloud computing systems. Akilal
    and Kechadi [193] investigated the potential adoption of Forensic-by-design in
    cloud computing systems, with [194] and [195] suggesting the application of Forensic-by-design
    (FbD) strategy to enhance digital forensic readiness. Moreover, several proposals
    for implementing digital forensics readiness are documented in the literature.
    For instance, Daubner and Matulevičius [196] proposed the introduction of forensic
    readiness mechanisms within security risk management to refine specific requirements
    on forensic-ready software systems, by re-evaluating the taken security risk decisions
    with the aim of providing trustable data when the security measures fail. Elyas
    et al. [197] presented a digital forensic readiness framework through a series
    of expert focus groups to discuss the critical issues facing practitioners in
    achieving digital forensic readiness. Also, De Marco et al. [198] proposed a reference
    architecture for a Cloud forensic readiness system. Mouhtaropoulos et al. [199]
    classified forensic investigation frameworks to expose gaps in proactive forensics
    research and reviewed prominent information security incidents with regard to
    proactive forensics planning. On a more network-focused scope, Endicott-Popovsky
    et al. [200] proposed a framework for operationalizing network forensic readiness,
    with Ngobeni et al. [201] proposing a wireless forensic readiness model designed
    to help monitor, log, and preserve wireless network traffic for digital forensic
    investigations. Considering readiness maturity assessment, Ariffin and Ahmad [202]
    presented five indicators for the maturity and readiness of digital forensics,
    with Elyas et al. [203] describing an approach to identify the factors that contribute
    to digital forensic readiness and how these factors work together to achieve forensic
    readiness in an organization. Iqbal et al. [204] presented a study on the current
    support for forensic readiness of CI, highlighting the involved key challenges
    and providing a literature review on the subject. Also, Alenezi et al. [205] presented
    a framework to investigate the factors that facilitate the forensic readiness
    of organizations. H. Forensic Schemas and Interoperability In a general way, interoperability
    is concerned with making it possible for components or systems coming from different
    vendors to easily communicate and interact with each other. When investigation
    processes require evidence exchange between investigators, the use of different
    tools for the reconstruction of events or analytical purposes, the absence of
    standardised digital evidence formats can become a serious obstacle. Thus, it
    is particularly important to develop information interoperability mechanisms by
    means of common Forensic Schemas. A standardized approach for representing and
    sharing digital forensic information is also useful to help investigators collaborate
    when incidents involve different jurisdictions. Similar challenges were also recognized
    in traditional investigations of violent crime and led to the development of the
    US Federal Bureau of Investigation’s Violent Criminal Apprehension Program (ViCAP)
    and Royal Canadian Mounted Police’s Violent Crime Linkage System (ViCLAS) programs.
    These programs enabled the correlation of all the available information from unsolved
    violent crimes in disparate regions, trying to find links between them. There
    have been several schemas proposed in the past for representing digital forensic
    information, but these have not been widely adopted [206], [207], [208], [209],
    [210]. Also, Garfinkel [211] proposed a XML schema (DFXML) for easier interoperability
    between forensic extraction and visualization tools, primarily developed to represent
    the output from tools used to analyze storage media, including file system parsers,
    file carvers, and hash set generators. Casey et al. [212] conducted a review of
    digital forensic data schemas, including DFXML, also proposing the CybOX schema
    for handling forensic data. CybOX is an open-source, community-driven effort to
    develop a standardized representation of digital observations led by the US Department
    of Homeland Security (DHS) office of cyber-security and communications. The XML-based
    XIRAF system was created by the Netherlands Forensic Institute (NFI) to support
    digital forensic analysis, storing its data using a parent-child structure within
    a centralized database accepting structured output and searching tools [213].
    Bhoedjang et al. [214] described the second generation of this analysis system
    and outlined the complexity of importing different file types and analyzing and
    preprocessing files before storing them in databases. van Baar et al. [125] outlined
    the latest iteration of this system, which incorporates Cloud features. The Advanced
    Forensic Format (AFF4) has taken another approach for the representation of digital
    forensic information [215], [216], using the Resource Description Framework (RDF),
    a general purpose representational formalism for knowledge representation. Although
    the majority of digital forensic tools do not support AAF4, Google Rapid Response
    (GRR) uses the AFF4 data model to store information in a MongoDB database [217].
    The AFF4 data model is flexible. However, the use of RDF requires the adoption
    of a shared supporting ontology. While there is still no community consensus on
    such ontology to exchange digital forensic information, the ontology proposed
    by Casey et al. [212] could be used as a basis for such consensus. I. Visualization
    and Searching Tools When it comes to the forensics practitioner toolset, usability
    is a crucial aspect not to be disregarded [161]. Specifically, Osborne and Turnbull
    [218] pinpointed the importance and need for tools incorporating adequate visualization
    capabilities for digital forensic data, claiming that there is a lack of algorithms
    to identify relationships, normalize data, incorporate multiple data sources,
    and provide effective visualization methods, all of which are important to retrieve
    further insights from evidence. Following this same line of thought, Osborne et
    al. [219] highlight the importance of considering architectures incorporating
    familiar visualization tools and algorithms that could be able to include distinct
    data sources, normalizing and correlating data, later proposing a conceptual framework
    able to Explore, Investigate, and Correlate (EIC) [218]. Tassone et al. [220]
    also highlighted the importance of visualization in forensic tools, pointing out
    that many existing solutions where just simple layouts to search and display basic
    tabular data, also presenting a proof of concept including a database schema designed
    for third-party forensic data storage and visualization. Irfan et al. [221] describes
    a virtual cloud environment incorporating visualization capabilities designed
    to provide visibility for all security events, allowing to follow activities of
    cybercriminals, reproduce crude information identifying each respective incident,
    and execute proactive actions. Also, Aupetit et al. [222] presented a methodology
    and a tool for allowing the Internet Service Provider (ISP) to assess and visualize
    threats from an organization’s network traffic, allowing them to deal for instance
    with Distributed Reflective Denial of Service (DRDoS) events. Another example
    is provided by Setayeshfar et al. [223], which presents a graphical forensic analysis
    system for efficient loading, storing, processing, querying, and displaying of
    causal relations extracted from system events to support computer forensics. Tools
    such as the Elastic Stack have been widely adopted in industry and academia as
    a result of their capabilities and performance in terms of log handling. There
    are solutions for data visualization, including graph generation capabilities
    for analysis purposes, supported by frameworks such as Kibana [70], Grafana [224],
    and Prometheus (Prometheus.io), which retrieve data stored in indexed datastores
    like Elasticsearch [71]. Some of the tools built on ElasticStack are SOF ELK [225]
    and Plaso [226], that provide rich visualization and parsing capabilities. Despite
    their capacity for effective forensics and provenance tracking supported by queries,
    they lack information about the provenance models also don’t provide users with
    many query abilities beyond filtering. Moreover, it should be stressed that while
    these tools can be used for multiple use cases without the incorporation of analytic
    inference mechanisms, that’s not typically the case in cyber-security analytics
    or forensics [222]. J. Forensics Constraints for the CIP domain Homem [227] identified
    a series of general challenges regarding digital forensics processes, namely:
    the rising volume of heterogeneous digital evidence involved in investigations,
    the evidence-centricity of industry-standard tools, a deficiency in the availability
    of a highly-skilled workforce, and the great effort required by the largely manual
    and time-consuming activities involved in the overall process. Besides, CI operational
    environments add further constraints related to aspects such as complexity, systems
    interdependency, dependency on ICT and components provided by third parties, or
    the deployment of heterogeneous technologies [228]. Typically, forensic investigation
    can rely on live or dead evidence aquisition. While the latter is performed offline
    on static data after a system is shutdown, the former collects data from live
    systems, such as the contents of physical volatile memory, and non-volatile data,
    such as the data maintained in a storage system. While dead forensics corresponds
    to the most traditional approach, there was a increasing emphasis on live forensics
    processes over the past years, as it is the case for network traffic analysis.
    More specifically, in the case of SCADA systems, the forensic investigator cannot
    turn it off to capture and analyze data, because this kind of system is supposed
    to be continuously operational [229]– in such cases, live forensics is a suitable
    digital investigation methodology [230]. However, since continuous availability
    of SCADA systems is a mandatory requirement, forensic investigators should strive
    to be minimally intrusive, in order to reduce the risks in critical operations
    while aiming at a rapid response time, to preserve evidence that may be overwritten
    by runtime processes [231]. It is known that SCADA and IT systems exhibit different
    behaviours and possess different characteristics, often requiring for IDS and
    other security mechanisms to be configured according to with the domain of operation
    [4]. For instance, in a SCADA system, network traffic is more deterministic than
    in IT networks, in the sense that a system component communicates to other system
    components following established patterns, frequently with bounded time restrictions.
    Thus, administrators may impose a set of rules for security purposes, with any
    non-deterministic behavior flagged as an anomaly– e.g., an IDS might be configured
    to consider a specific communication pattern as normal [232]. Moreover, the same
    restrictions regarding live network trace capture can also apply to SCADA stations
    and other process control or monitoring systems. Any evidence collection tool
    or technique must avoid imposing overheads that might degrade the system response,
    interfere with operational indicators or expand the vulnerable attack surface.
    Overall, a simple rule must be kept in mind: live (or, for that matter, any other)
    forensics processes must be designed to adhere to the least overhead principle,
    in line with the recommendations from standards such as NIST SP800-82 [233], which
    clearly identify the risks associated with intrusive security procedures. K. IoT
    and Industrial IoT Forensics IoT can be defined as a system of networked smart
    devices that can be identified, named and addressed [234]. IoT is attracting great
    attention not only for consumer applications but also in the IACS domain, where
    they are usually designated as Industrial IoT. Naturally, the introduction of
    these technologies has increased the amount of generated, transported and processed
    data, as well as the number of forensically relevant events in consequence of
    the increasing number of available sensor devices [235]. Considering the emergence
    of IIoT, organisms such as NIST have defined guidelines [236], [237] to ensure
    that IIoT infrastructures rely on adequate safety, security, privacy, consistency,
    dependability, resiliency, reliability, interaction and coordination measures.
    However, it’s not always possible to apply traditional information security measures
    based on sophisticated encryption algorithms, multi-factor authentication, antivirus
    programs and firewalls (among others), due to the limited computational and energy
    resources of some sensor nodes [238], further reinforcing the need for the deployment
    of proper security monitoring and forensics capabilities. Stoyanova et al. [239]
    identified and discussed the main issues involved in the process of IoT-based
    investigations, particularly all legal, privacy and Cloud security challenges.
    They also provided an overview of the past and current theoretical models in the
    digital forensics and frameworks aiming to extract data in a privacy-preserving
    manner or secure the evidence integrity using decentralized blockchain-based solutions.
    Vendors such as Infineon, NXP, and STMicroelectronics prepared a position paper
    for ENISA [240], stating the IoT market failure for cyber-security and privacy,
    and claiming that there were “no level zero defined for the security and privacy
    of connected and smart devices,” no legal guidelines for IoT device and service
    trust, and no “precautionary requirements are in place”. This paper also predicts
    that attacks will get more risky and threatening due to the rise of IoT enabled
    cars, CI, and health applications. In the same line of thought, Chehri et al.
    [241] identified the trends, problems, and challenges of cybersecurity in smart
    grid CI in Big Data and Artificial Intelligence (AI). (I)IoT scenarios require
    the implementation of adequate forensic and compliance auditing approaches to
    improve security and privacy. In that regard, Yaqoob et al. [242] investigated
    studies on the topic of IoT forensics by analyzing their strengths and weaknesses.
    The authors categorize and classify the literature by devising a taxonomy based
    on forensics phases, enablers, networks, sources of evidence, investigation modes,
    forensics models, forensics layers, forensics tools, and forensics data processing.
    They also enumerate a few prominent use cases of IoT forensics and present the
    key requirements for enabling IoT forensics, identifying and discussing open research
    challenges as future research directions. L. Summary The purpose of this Section
    was to introduce and present a series of concepts and topics within the scope
    of digital forensics, with a view towards its application in the CIP domain. We
    started by conceptually introducing a definition of forensics activities, followed
    by a discussion about digital, network and cloud forensics, the latter constituting
    not only a challenge, but also an opportunity to implement innovative solutions
    tackling the issues of FCA. The implications of data privacy protection regulations
    in digital forensics activities were also discussed, followed by a review of the
    related subtopics of forensic readiness, interoperability, visualization and automation.
    Finally, we concluded with an overview of the current forensics constraints for
    the CIP domain. Figure 3 depicts how the key topics covered in this Section relate
    with each other, and Table 1 summarises the reviewed literature on these topics.
    While some topics are addressed from a more neutral perspective, it must be noted
    that this is due to the fact that many are still valid in the CIP domain. TABLE
    1 Reviewed works on forensics for CIP FIGURE 3. Key forensics concepts. Show All
    SECTION IV. Compliance Auditing An audit process represents a systematic, independent,
    formal, structured, and documented process, usually performed by a certified professional
    on behalf of stakeholders, aiming to verify if certain criteria match internal
    policies, external formal standards, and/or legal requirements [250]. Auditing
    practices help organizations meet such requirements, also providing due diligence,
    certification, and stakeholder security. Compliance auditing expertise is closely
    related to and frequently overlaps with forensic processes, since both often share
    data sources, tools, and techniques. This Section will delve into the topic of
    Compliance Auditing, with a view towards its applicability in CIP environments.
    Starting with an overview of the motivation and context, it will next review existing
    audit models and proposals and standards, concluding with a discussion about logging
    systems compliance for audit purposes. A. Motivation and Context Policy definition
    and enforcement are cornerstones of modern security practices. For instance, Yaacoub
    et al. [243] describes a series of policies encompassing aspects such as employee
    screening processes before recruitment, privilege suspension outside working hours,
    or additional activity monitoring for people in charge of sensitive tasks, which
    contribute to enhance the security posture of an organization. Compliance auditing
    checks whether workflows are compliant with organizational policies and rules–
    thus, each process or transaction may be checked to confirm whether it followed
    the applicable rules or policies. In case rules are violated, the auditor analyses
    relevant data to determine causes and recommends actions to prevent future deviations
    or non-compliance situations. Compliance audit frameworks can also help highlighting
    misconfigurations– for example, they can used for monitoring access security levels
    for individual and group accounts and help with detailed reports measuring the
    security progress. The compliance auditing process ends up with a report that
    includes the conclusions and additional information about requirements that have
    been met and non-compliance situations (if found). It can also highlight the implications
    and risks of non-compliance, suggesting corrective actions to prevent future occurrences
    [251]. As the surrounding environment evolves, infrastructure and service operators
    are often forced to adapt to an increasingly complex and constantly changing regulatory
    landscape. Thus, an organization aiming to implement specific regulatory or standardisation
    measures should depart from the identification of the entities with relevant technical
    and/or legal jurisdiction over its domain of activity. In this line, the GDPR
    [180] regulations constitute an example of a mandatory framework for privacy protection,
    which applies to organizations within the European Union (EU). Besides generic
    or sectorial standards, CI-specific regulations may also be imposed by organizations
    such as North American Electric Reliability Corporation Critical Infrastructure
    Protection (NERC-CIP) [252], which publishes a set of security guidelines, as
    it is the case for Electronic Security Perimeters (CIP-005) and System Security
    Requirements (CIP-007). B. Cybersecurity Audit Models Businesses are being increasingly
    pressured to undergo periodic audits and inspections as part of legal and regulatory
    compliance certification requirements. While such certifications processes are
    important to reinforce trust at the B2B and B2C levels, it should not be forgotten
    that their ultimate aim is to ensure that adequate preventive and reactive security
    mechanisms are implemented, as well as proper handling of sensitive data. Ultimately,
    it all comes to the establishment and maintenance of suitable levels of data confidentiality,
    integrity and availability within an organization, which may vary accordingly
    to the type of applications, data to be stored or processed (e.g., the case of
    sensitive healthcare data), or geographical location (e.g., regional requirements
    for data privacy and protection). From the industry standpoint, an organization
    may be required to comply with regulations such as Payment Card Industry Data
    Security Standard (PCIDSS) [253], Health Insurance Portability and Accountability
    Act (HIPAA) [254], Federal Information Security Modernization Act (FISMA) [255],
    GDPR, FedRamp, and SOC2. These are examples of compliance drivers prescribing
    the application security activities. The Institute of Internal Auditors (IIA)
    also provides guidance in the form of the International Professional Practices
    Framework Standard 2420 (Quality of Communications) [256], whose aim is to establish
    guidelines for objective, clear, concise, constructive, complete and timely reporting.
    Three different types of cybersecurity audits were described by Donaldson et al.
    [257]. The first category corresponds to threat audits targeting cyber threats,
    aiming to search for evidence in IT environments. The second one evaluates the
    cybersecurity controls mapped against frameworks, regulatory requirements, standards
    or a specific cyberthreat. The last one comprises validation assessments against
    cybersecurity controls measuring their effectiveness against designed and documented
    requirements. The assessment of access control policies is one of the aspects
    typically resorting to formal reasoning mechanisms to verify application control
    expressed at design time (for instance with eXtensible Access Control Markup Language,
    XACML) to dynamically enforce authorization by externalizing access controls.
    Fisler et al. [258] proposed Binary Decision Diagrams and custom algorithms to
    check access-control policies. Ahn et al. [259] used answer set programming (ASP)
    and leverage existing ASP reasoning models to conduct policy verification. Arkoudas
    et al. [260] proposed a Satisfiability Modulo Theory policy analysis framework.
    Sabillon et al. [261] proposed an audit model for conducting cybersecurity audits
    in organizations and nation-states. Agrawal et al. [262] introduced an auditing
    framework for determining whether a database system is adhering to its data disclosure
    policies by allowing users to formulate audit expressions to specify the data
    subject to disclosure review. Kaaniche et al. [263] proposed the usage of hierarchical
    ID-based encryption and signature schemes. Noura et al. [146] presented a security
    and protection audit that can be done by using an audit management system to collect
    and store logs in a distributed system. Bouet and Israël [264] presented a security
    assessment framework including an off-line tool enabling security and vulnerability
    audits of information systems to be used by system architects to assess the security
    of the system they are designing during the planning phase. The patent “Critical
    function monitoring and compliance auditing system” [265] describes a system and
    method for monitoring, auditing, and flagging compliance issues or other user-defined
    exceptions. Finally, Slapničar et al. [266] analyzed the effectiveness of internal
    audit of cybersecurity by developing a Cybersecurity Audit Index composed of three
    dimensions: planning, performing and reporting. In the scope of compliance auditing
    cloud computing platforms, Ullah et al. [267] proposed an architecture to build
    automated security compliance tools, focusing on auditing remote administration
    and on diagnosing port protection and clock synchronization. Also, Henze et al.
    [268] presented a practical approach enforcing data compliance in key-value-based
    Cloud storage systems. Doelitzscher [269] implemented an on-demand audit architecture
    for Infrastructure as a Service (IaaS) clouds, based on software agents for identifying
    anomalies for auditing purposes. Finally, there is also SecGuru, designed to audit
    Azure datacenter network policies [270]. Figure 4 summarizes the main regulations,
    security frameworks and auditing models applicable to this domain. FIGURE 4. Regulations,
    frameworks and auditing models. Show All C. Standards for Compliance Auditing
    The development of cyber Information Security Management Systems (ISMS) is guided
    by standards such as ISO/IEC 27001 and ISO/IEC 27002. As already mentioned, these
    standards cover the protection of an organization from cyber-attacks [271]. Domain-specific
    initiatives were also launched to develop and implement IACS standards to secure
    SCADA environments, including the ones from NIST, which presented the Special
    Publication 800–82 and International Electrotechnical Commission (IEC) 62443 [272],
    [273]. Another example is ISO/IEC 62443-1-1 (Security for industrial automation
    and CS: Terminology, concepts, and models), which constitutes an ongoing effort
    towards the improvement of cyber-security, robustness, and resilience design.
    The ISO/IEC 62443 series standard elements are arranged in four groups, namely:
    Policies and Procedures, System, and Component Requirements. The Policies and
    Procedures group is focused on the policies and procedures associated with IACS
    security, with the Systems group addressing the requirements at the system level.
    Systems and Component Requirements provide information about specific and detailed
    requirements associated with the development of IACS products [273]. The Japanese
    Information-Technology Promotion Agency (IPA) also implemented the Embedded Device
    Security Assurance Certification (EDSA) Program for provisioning SCADA devices
    [274]. Nevertheless, and despite these efforts from the academia and industry,
    there is still a lack of standards for compliance auditing techniques in Cloud
    domains [275]. D. Logging Systems Compliance Logs constitute key data sources
    to acquire visibility and obtain insights from the operational infrastructure
    processes, with log analysis being recognised as vital for collecting evidence
    and retrieving the necessary insights to understand the behaviour of a whole system,
    as well as its individual components, regardless of the deployment type. For instance,
    Amazon suggests the use of AWS CloudTrail and CloudWatch [276] for auditing purposes,
    as a web API offering logs and metrics data to their clients. Being important
    for administrators, developers and security operators alike (albeit for different
    reasons), log handling and processing components often have to comply with suitable
    availability, resiliency and continuous operation requirements– such systems should
    be sized and ready for possible high-demand situations where the overall system
    becomes unstable or overloaded, triggering a large number of events. It is important
    to rely on a logging system to acquire and deliver information, but also to intelligently
    process it using insight and analytics. A logging system should provide visibility
    over its behavior to enable correct predictions. From a security standpoint, log
    analysis must be reliable and accurate, especially in circumstances involving
    security incidents or critical situations. Thus, using a inadequate or non-compliant
    logging system may have several consequences, such as hampering monitoring, diagnosis
    or forensics procedures, up to the point of potentially voiding the possibility
    of gathering legally admissible evidence. E. Summary This Section presented the
    key definitions, topics, and related work about compliance auditing standards
    and regulations. Figure 5 depicts the relationship between the key topics that
    were addressed. Moreover, the related reviewed literature is summarised in Table
    2. TABLE 2 Reviewed literature on compliance auditing FIGURE 5. Key concepts of
    compliance auditing. Show All Together with Section III (Forensics), this Section
    also emphasises to which extent forensics procedures and requirements intersect
    with the regulatory frameworks and standards for compliance auditing, often with
    mutual benefit. This is one aspect among the many contact points that characterise
    the relationship between the FCA contexts, for which analytic procedures and tools
    constitute another cornerstone relationship, which will be further discussed in
    the next section. SECTION V. Analytics for Cip Fca: the Road Ahead Analytics corresponds
    to the set of activities focused on how to extract insights from data, correlating
    evidence to provide security-related capabilities to system administrators, security
    analysts, and network and application engineers. Analytics leverage FCA capabilities
    to improve CIP because they help identifying anomalies and their root cause and
    then extract evidence. This Section will delve into the benefits and challenges
    of modern analytics in the era of Big Data, AI and ML, starting with a motivation
    and following with a discussion about the impact of Big Data technologies on CIP.
    The intersection of Big Data technologies, AI and ML is also discussed, followed
    by the topic of Forensics Automation. This Section closes with a discussion of
    anomaly detection techniques for log analytics. A. Motivation and context Modern
    protected infrastructures are becoming increasingly complex, a situation for which
    CIP is no exception, with Industrial IoT infrastructures spreading on a massive
    scale, both geographically and in terms of components [278]. This has the side
    effect of generating considerable amounts of operational data and evidence, that
    cannot be properly handled by traditional analysis techniques. This poses a challenge
    to FCA, requiring the introduction of scalable techniques able to transport, store
    and process large amounts of data, thus calling for the adoption of Big Data techniques,
    designed to handle large amounts of data whose volume is beyond the ability of
    typical vertical approaches [279]. These circumstances also deem unfeasible to
    manually analyze large amounts of data, requiring practitioners to resort to automated
    techniques [280], often supported by AI-based techniques (with a particular focus
    on ML [241], a branch of AI geared towards automating pattern recognition or classification
    tasks to analyze vast amounts of data to predict or detect certain behaviors,
    which in the case of forensics, may consist of discovering or detecting malicious
    activity). These ML and information retrieval techniques have significantly improved
    in the last years, enabling the extraction of deeper insights from data [281],
    [282], with many of these analytic frameworks being able to perform effective
    and efficient data analysis supported by ML models implemented from a few lines
    of code, also supporting the automation of time-consuming tasks. B. The Impact
    of Big Data Technologies on CIP One of the most pressing issues when handling
    large data volumes is the implementation of efficient distributed storage and
    retrieval technologies. Big Data NoSQL databases address such challenges with
    technologies such as MongoDB, HyperTable, Cassandra, and Amazon Dynamo offering
    scalability and performance predictability that is suitable for storing and indexing
    real-time streams of big datasets [283]. Kalakanti et al. [10] evaluated different
    NoSQL datastores as a solution to the data and knowledge management challenges
    to meet the requirements of performance, reliability and scale imposed by the
    next generation of data historians as a central repository of SCADA systems. The
    need to deal with increasingly big data volumes also calls for an increase in
    the required amount of computational resources, which must be balanced with the
    need to contain query latencies within acceptable thresholds. To address this
    problem, Google developed the Google File System [284], as well as MapReduce [285],
    that was designed to address computational challenges. Several efforts were also
    made to have those technologies available as open source software, resulting in
    tools such as Apache Hadoop and the Hadoop File System [286]. As already explained,
    Big Data technologies are especially suitable for CIP and particularly IIoT, where
    large volumes of data are produced devices from distributed CPSs, for time series
    analysis. Specialized Time Series Management Systems (TSMS) have been developed
    to overcome the limitations of general purpose Database Management System (DBMS)
    for times series management [287]. For instance, Jensen et al. [287] surveyed
    the field of TSMSs developed bt the academy and the industry, and organized them
    into categories. Finally, Wang et al. [288] surveyed TSMSs in industrial and IoT
    fields addressing the new demand such as large amount and real-time analysis of
    industrial data. Big Data also poses significant challenges and stresses out privacy
    requirements, especially those related to privacy regulation emanated from the
    EU [289]. In that regard, Gartner predicted that by 2018, 50 percent of business
    ethics violations will be related with data [290]. C. Big Data Analytics in the
    Age of IA and ML In FCA applications, handling large volumes of data is only half
    of the equation, with analysis being the other half. Extracting insights and patterns
    from evidence calls for methods other than manual analysis, thus constituting
    a natural fit for AI and particularly ML techniques, something that was investigated
    by Brighi et al. [291], that tried to bridge these technologies with the substantive
    and procedural rules to be observed during investigation activities. Regarding
    forensics applications, Hoon et al. [292] reviewed the literature by addressing
    the challenges and opportunities of employing Big Data in Distributed denial-of-service
    (DDoS) forensics, implementing and comparing the performance of multiple supervised
    and unsupervised learning models, according to their efficiency and accuracy.
    They found that NaÏve Bayes, Gradient Boosting and Distributed Random Forest are
    the most suitable models for DDoS detection, due to their accuracy and time taken
    on training. As for network forensics, Yavanoglu and Aydos [293] reviewed the
    most commonly used datasets in AI and ML techniques, as primary tools for analyzing
    network traffic and detecting anomalies. Usman et al. [294] proposed a ML approach
    supported by Decision Tree algorithms to predict IP reputation in zero-day attacks,
    categorized via behavioral analysis to highlight forensic issues in big datasets.
    Wiyono and Cahyani [295] presented classification algorithms for network forensics
    based on the identification of network flows that could track suspected botnet
    activity in the infected network. Other tools presented by Hassan et al. [102],
    Setayeshfar et al. [223] implemented models based on AI to assist forensics experts
    in monitoring the system and detecting malicious behaviors based on known patterns–
    however, these tools are not designed for manual forensics tasks such as whole
    system provenance tracking, being often bound to a single proprietary data stream
    scheme. In the scope of Compliance Auditing, Moore and Childers [296] presented
    a ML solution to automatically generate program affinity policies that consider
    program behavior and the target machine. Similarly, Quiroz et al. [297] relied
    on unsupervised algorithms to capture the dynamic behavior of systems and the
    hidden relationship between the high-level business attribute space and the low-level
    monitoring space. Similarly, Pelaez et al. [298] used supervised models to capture
    dynamic behavior. Johansen et al. [299] proposed a mechanism for expressing and
    enforcing security policies for shared data expressed as stateful meta-code operations
    defined in scripting languages interposed in the filesystem. Gheibi et al. [300]
    reviewed the state of the art on the use of ML in self-adaptive systems based
    in the traditional Monitor-Analysis-Planning-Executing (MAPE) [301] feedback loop.
    Weyns et al. [302] also presented an approach combining MAPE and Control Theory
    to produce better adaptive systems. D. Forensics Automation Organizations often
    check whether their security and forensic controls are actually in place as intended
    using manual assessment procedures. Forensic processes are often no different,
    being typically time-consuming activities dependent on humans. From this perspective,
    the lack of qualified human skills and resources can hamper investigation and
    compliance auditing processes [291]. The use of technology to implement automated
    processes can streamline forensic investigation tasks fed by large volumes of
    data. The adoption of automation is therefore seen as an effective strategy to
    implement forensic processes while reducing the costs and operational errors resulting
    from human intervention [303], also constituting an emergent field of interest
    in the research community. Regarding the introduction of automated procedures,
    Hayes and Kyobe [304] reviewed the existing research in the field of cyber forensics,
    identifying current practices and associated challenges that could be tackled
    by the adoption of automation, as well as the relevant technology that could be
    leveraged to address such needs. Asquith and Horsman [303] provided an introductory
    discussion on robotic process automation, a form of service task automation that
    can improve efficiency in the field of forensics, with Moffitt et al. [305] discussing
    the automation of repetitive and manual rule-based tasks. From a more practical
    perspective, Verma et al. [126], [306] proposed a digital forensic framework that
    uses case information, case profile data, and expert knowledge for automation
    of the digital forensic analysis process supported by ML for finding evidence.
    Also, Patrascu and Patriciu [307] discussed the issues threatening CI systems
    and proposed an automated learning framework based on ML algorithms to protect
    such systems that, despite not being focused on forensics applications, can be
    leveraged for such purpose. Finally, recent contributions on the use of ML models
    supporting the automation of self-adaptive IT operations have been focusing on
    topics such as observability and AIOps [308], [309]– Notaro et al. [310] has compiled
    several contributions in this scope. E. Anomaly Detection from Log Data Sources
    An anomaly corresponds to an outlying observation that appears to deviate significantly
    from a nominal state or a statistical data distribution [244]. Anomalies are often
    classified into three types: point anomalies, contextual anomalies, and collective
    anomalies contexts [245]. Anomalies can be expressed by scores or labels [246].
    While anomaly detection techniques can be applied for all sorts of data sources,
    logs are of special importance for FCA applications, due to their almost pervasive
    and non-invasive nature, playing a vital role in case of a breach or incident
    analysis as they provide detailed information about activities. Nevertheless,
    the use of anomaly detection mechanisms using application and service log data
    for forensics and compliance auditing raises important challenges, due to factors
    such as the abundance of unstructured plain text contents and heterogeneous formats,
    redundant runtime information (which sometimes may change, as it is the case for
    certain IP addresses), and the existence of a significant amount of unbalanced
    data (a direct consequence of the prevalence of a normal operation mode). Moreover,
    with the increasing scale and complexity of distributed systems in the CI environment,
    monitoring, correlating and analysing logs is a time-consuming task that takes
    considerable effort, making it increasingly unfeasible to manually sort out trough
    evidence to detect anomalies. Event correlation can be also categorized into different
    categories: temporal, spatial, or hybrid, whose combined use allows to capture
    both local (subsystem level) or global (IACS level) abnormalities [248]. After
    anomalies have been identified, is important to take forensic efforts in the analysis
    to determine the root causes and collect evidence, which will help to elaborate
    on the definition and application of countermeasures. Some proposals have addressed
    the usage of log analysis as one of the input sources for anomaly detection. Chen
    and Li [311], for instance, proposed an improved version of an algorithm for detecting
    anomalies from audit data while updating the detection profile along with its
    execution. Clustering techniques, such as the k-means algorithm, are often used
    by intrusion detection systems for classifying normal or anomalous events, having
    also application in the forensics analysis field. For instance, Asif-Iqbal et
    al. [312] correlated logs from different sources, supported by clustering techniques,
    to identify and remove unneeded logs. Syarif et al. [313] compared five different
    clustering algorithms and identified those providing the highest detection accuracy,
    also concluding that those algorithms were not mature enough for practical applications.
    Hoglund et al. [314], as well as Hajamydeen et al. [315], classified events in
    two different stages supported by the same clustering algorithm. Münz et al. [316]
    applied the k-means clustering algorithm to feature datasets extracted from raw
    records, where training data are divided into clusters of time intervals for normal
    and anomalous traffic. Tian and Jianwen [317] improved traditional means clustering
    algorithm, to improve efficiency and accuracy when classifying data. Eslamnezhad
    and Varjani [318] proposed a detection algorithm to increase the quality of the
    clustering method based on a MinMax k-means algorithm, overcoming the low sensitivity
    to initial centers in the k-means algorithm. Ranjan and Sahoo [319] proposed a
    modified k-medoids clustering algorithm by presenting a new strategy to select
    the initial medoids, overcoming the means in anomaly intrusion detection and the
    dependency on initial centroids, number of clusters, and irrelevant clusters.
    Also, a k-nearest neighbor classifier for intrusion detection was explored by
    Liao and Vemuri [320]. Other authors adopted hybrid solutions for log analysis,
    combining the use of the k-means algorithm with other techniques for improving
    detection performance. They realized that despite the inherent complex structure
    and high computational cost, hybrid classifiers can contribute to improving accuracy.
    Mohammed et al. [321] proposed a clustering approach based on Fuzzy C-Means (FCM)
    and K-means algorithms to identify the evidential files and isolate the non-related
    files based on their metadata. Makanju et al. [322] took advantage of an integrated
    signature-based and anomaly-based approach to propose a framework based on frequent
    patterns. Varuna and Natesan [323] introduced a hybrid learning method integrating
    k-means clustering and Naive Bayes classification. Muda et al. [324] proposed
    k-means clustering and Naive Bayes classifiers in a hybrid learning approach by
    splitting instances into potential attacks and normal clusters. Hybrid approaches
    have indeed proven to be quite interesting. However, in general, they still take
    a considerable amount of time to generate models for particular datasets, aggravated
    by the growth patterns normally associated with log sources in production systems.
    Elbasiony et al. [325] used data mining techniques to build a hybrid framework
    for identifying network misuse and detecting intrusions through the use of random
    forests algorithm to detect misuses, with k-means as the clustering algorithm
    for unsupervised anomaly detection. Fu et al. [247] presented an algorithm to
    convert free-form text messages in log files to log keys without heavily relying
    on application-specific knowledge. Du et al. [326] proposed the use of a Long
    Short-Term Memory (LSTM) to model a system to automatically learn log patterns
    from normal execution, and detect anomalies when log patterns deviate from the
    model trained from log data under normal execution. Henriques et al. [249] proposed
    an integrated scalable framework for efficiently detecting anomalous events on
    large amounts of unlabeled data logs through the use of clustering and classification
    methods supported by a parallel computing approach. F. Summary This Section addressed
    the opportunities and challenges in the use of advanced analytics based on Big
    Data technologies, with AI and ML support, in the field of FCA. Figure 6 depicts
    how the key topics addressed in this Section are related. FIGURE 6. Analytics
    key concepts. Show All We surveyed the research in the field of advanced Big Data
    analytics taking into account the increased softwarization trend in terms of computing
    and network resource usage, as well as the benefits of leveraging advanced learning
    algorithms for improved automation. This has allowed to unveil a series of emerging
    development and evolution paths for FCA practices which are expected to have a
    profound change across the entire domain. Table 3 summarizes the relevant literature
    in Big Data for CIP. TABLE 3 Reviewed works related to big data-supported FCA
    SECTION VI. A Forensics and Compliance Auditing Taxonomy for Cip To the best of
    our knowledge, there is no specific taxonomy in the domain of FCA for CIP in the
    surveyed literature. To fill this gap, we devised a taxonomy covering the scopes
    as well as the functional and non-functional dimensions of the FCA practice, inspired
    by forensic investigation and compliance practices. The proposed taxonomy is depicted
    in Figure 7, being organised along seven major dimensions, inspired by the methodology
    proposed by [327]. These are the following: FIGURE 7. Proposed FCA taxonomy for
    CIP. Show All Critical Infrastructures: this dimension characterises the scope
    and environment to be protected, including SCADA and IACS core systems. Moreover,
    specific attacks targeting CIs, SIEM, and other security platforms and systems
    providing protection capabilities are also considered Governance: gathers the
    orientations that can support the decisions in the application of FCA processes.
    It comprises the investigation processes, guidelines, agencies, standards and
    regulations, training, directives, and existing specific security frameworks.
    Preparedness: this dimension comprises the proactive aspects that may be considered
    to safeguard, support and prepare in advance the execution of FCA processes. It
    encompasses readiness, forensic by design, forensic frameworks, anti-forensics,
    and auditing frameworks. Data Acquisition: this dimension deals with the challenges
    of gathering digital and network forensics covering aspects such as volume, live
    forensics and data provenance, while safeguarding the need to protect information
    about evidence. Evidence Identification: covers the models, algorithms and approaches
    helping to identify evidence and non-compliant events. It comprises IDS, detection
    techniques, causality, and learning, in this last case by using approaches supported
    by clustering and hybrid approaches algorithms. Reporting: this category covers
    communication and interoperability-related aspects, encompassing topics such as
    privacy concerns, visualization and searching, interoperability, and Chain of
    Custody. Deployment: this encompasses non-functional aspects, which relate to
    platform and infrastructure-related aspects, such as cloud computing, virtualization
    support, scalability, automation, and quality. This taxonomy aims at presenting
    FCA-related topics in a convenient way, using a set of criteria covering both
    functional and non-functional aspects while striving to provide a convenient organization
    for the most significant developments. SECTION VII. A Reference Architecture for
    Fca Systems As already mentioned, even though Forensics and Compliance Auditing
    are different activities, both in terms of purpose and expected outcomes, there
    is a considerable amount of proximity between them, since they often resort to
    the same data sources and similar information and context extraction techniques
    to gather and process evidence. This hints at the possibility of building both
    capabilities on top of a shared reference architecture, providing data acquisition,
    transport and processing pipelines, as well as persistence capabilities. In this
    section, we provide such a reference architecture, in order to better identify
    the various functional blocks typically found in FCA systems. It should be noted
    that this is an abstract architecture. Real-world FCA tools will usually map into
    subsets of this architecture. The main functional requirements to be met by FCA
    solutions include identifying, extracting, preserving and presenting digital evidence.
    Table 4 highlights how the architecture’s functional blocks typically required
    for Forensics operations and for Compliance Auditing activities largely overlap.
    TABLE 4 Relevance of functional blocks vs. Forensics and compliance auditing Figure
    8 presents proposed reference architecture. The first stage of FCA systems includes
    the collection of heterogeneous data from internal and external sources to be
    gathered into a single logical store. That data can include a vast amount of structured
    and unstructured heterogeneous data from a large number of sources widely dispersed
    across the CI, including those from the associated IACS and the ICT infrastructures.
    FIGURE 8. Reference architecture for FCA systems. Show All The second stage incorporates
    forensic analysis and third-party continuous auditing capabilities for the identification
    of post mortem security events, foreseeing, tracking, and tracing possible anomalies.
    Such objectives can be achieved by correlating the features retrieved from a seemingly
    disparate class of events that usually are not considered in terms of CI. Thus,
    beyond the forensics activities, the auditing layer checks compliance with standards,
    policies and rules. An example of such verifications is the cross-check of past
    system logs with the registration of physical access to remote facilities, to
    indirectly detect unauthorised accesses. Next, we discuss the key components of
    this architecture. A. Data Sources AND Data Ingestion The Ingesting Module acts
    as a set of probes capturing data from a large number of heterogeneous data sources
    from the surrounding environment, including applications such as Authentication
    Authorization and Accounting (AAA), ICT security logs (e.g., anti-virus, IDSs),
    internal personnel activities, physical access control logs (door switches and
    surveillance cameras), maintenance activities (physical and logical systems),
    interactions with third-parties (e.g., general documents, emails) and incident
    logs (e.g. ICT trouble tickets). Integration of third-party sources within the
    Ingesting Module is usually accomplished by using custom data adapter components.
    Such modules ingest data from IDSs, third-party applications or triggered alerts
    from monitoring processes, also including trust and reputation data, all of it
    being integrated using pull or push-based approaches. The ultimate goal of the
    Ingesting Module is to acquire, parse, enrich and normalize incoming data (which
    may be structured or unstructured, depending on its nature and sources) into a
    common format suitable to be stored in the Data Lake (DL) and later used for analysis
    purposes, while ensuring consistent timestamp synchronization across several sources
    in order not to compromise event timelines. This means that incoming raw data
    needs to be handled in a streamlined way, in order to optimize its transport,
    storage and processing, thus implying the deployment of data processing pipelines
    akin to Extract, Transform and Load (ETL) workflows. These Ingesting Module workflows,
    which may also include filtering, normalization, indexing, enrichment, and aggregation
    steps, must be capable of dealing with high volumes of heteregeneous data later
    to be fed into the DL, which constitutes the central repository component in the
    reference architecture. Persisted data from different sources (including enriched
    data) may be used for several purposes, such as training learning models or to
    feed visualization tools helping to identify threats. B. Data Lake The DL provides
    a repository to store data in different formats. This repository centralizes logs
    and other different sorts of data collected by the Ingesting Module (IM), to be
    made avaliable to FCA activities (and possibly other applications). Aditionally,
    the DL also persists the correlation and/or classification results of such data
    feeds, helping streamline higher-dimensional analytic procedures. The DL often
    assumes a distributed nature, to horizontally scale in order to fit increasing
    volumes of data and/or to increase the performance of data searching and correlating
    activities. It usually provides the automation capabilities to manage how indexes
    and queries are distributed across the cluster to accommodate large amounts of
    data and transactions, including support for automated scaling. This is important
    since high availability, resiliency, throughput, and low latency when querying
    large volumes of data are important non-functional requirements for the DL. The
    DL may also provide integration mechanisms to plug-in common authentication systems
    such as Active Directory, Lightweight Directory Access Protocol (LDAP), and Security
    Assertion Markup Language (SAML). C. Analytics After the data is captured and
    stored in the DL, the Analytics Module takes the responsibility for extracting
    relevant insights. Supported by state-of-the-art analytic methods, this module
    provides the capabilities to classify threats with potential impact on the systems’
    integrity, confidentiality, or availability. It starts by individually identifying
    unusual behaviors in past events, logged in computers or networks, correlating
    them in order to identify the compromised systems from the chain of events. For
    instance, this can be used to correlate the sequence of past executed shell commands
    with the list of files that have changed, to discover threats. The outcomes of
    this component also provide an important input to trigger automated rapid response
    actions. Within the Analytics Module, the use of ML techniques can help discover
    new behaviors and patterns to define and/or reveal the policies and business rules
    used to classify threats, from a vast amount and variety of data. Thus, it is
    expected that taking such a proactive approach to classify events in advance (before
    the forensic investigation has even started) may contribute to improve the readiness
    of forensic and compliance auditing processes. This is further reinforced by the
    fact that the resulting classified data will also be stored in the DL as input
    for further forensic analysis processes. The nature of its role requires Analytics
    Module to be flexible, allowing models to be updated “on the fly” between retraining,
    but also to offer a good performance/efficiency balance. The latter can be achieved
    by decoupling the training and classification processes and running in parallel,
    thus reducing the time devoted to event classification while increasing the chances
    of automatically recognizing new threats. Improving the time spent on training
    can also be achieved by dividing the dataset and even the model, assigning parts
    to different processes. Thus, even when the training model is too large, it can
    be trained in the background without disturbing the live system. Taking advantage
    of its scale-out properties, the reference Analytics Module architecture is designed
    to simultaneously train and run different models. Some of them can be used for
    training, while other ones can be used for classification purposes. Update or
    introduction of models into production after training should follow best practices,
    eventually pursuing a MLOps-like lifecycle management approach. D. Forensic Analysis
    Forensic analysis is a key step in the investigation process to identify the traces
    of malicious activity and extract evidence. Additionally, this may also encompass
    the establishment of a causality path between classified anomalies, oriented towards
    identifying the root cause and progression path of an incident. Forensic analysis
    capabilities can be leveraged by using ML models in the context of the Analytics
    Module. These can help forensic investigators efficiently find out the relevant
    events from large amounts of data, coming from diversified sources. Technically,
    evidence can be collected with queries entailing a set of rules to be run against
    the events previously stored in the DL. The adoption of a common standardized
    forensic schema assumes particular importance in collecting and exchanging relevant
    information or evidence between different entities and even jurisdictions, along
    the investigation chain. To ensure that evidence is legally admissible while safeguarding
    authenticity and integrity, schemas may adopt techniques such as cryptographic
    hashing. E. Audit Compliance The audit compliance component provides the capability
    to assess conformity with standard practices and defined policies, as part of
    an ongoing CIP strategy. Such standards may encompass regulatory requirements
    and/or industry guidelines that the infrastructure operator must comply with for
    certification, security and/or safety reasons. In case an audit trail is available,
    an expert can return to the source material to check the quality of the analysis
    and processing. Beyond the policies resulting from the need to comply with regulatory
    or standardization frameworks, organizations can establish custom rules based
    on their own internal processes and procedures, such as corporate laws, plans,
    and procedures. The Audit Compliance module takes business rules and regulatory
    policies to identify violations and trace the path of non-conforming events. This
    process assesses the compliance of the facts denoted by the ongoing events with
    the defined business rules and policies, providing an outcome that includes scores
    computed by quantifying the aspects regarding security and the level of risk.
    Both the Forensic Analysis and the Audit Compliance modules leverage the outcomes
    from correlating data at the Analytic component. F. Visualization and Dashboards
    Visualization capabilities are key for forensics activities, providing the means
    to display information in a manner that may evince the presence of suspicious
    or anomalous patterns. Such capabilities can be key to help understand and analyze
    specific domain datasets by applying histograms, scatter and box plots, tree maps,
    surface pots, parallel coordinate plots, and radar charts [347], [348]. This module
    is fed by the data persisted in the DL repository, which is used for analysis
    purposes. In a typical arrangement, dashboard panels are used to highlight a variety
    of indicators which may be directly generated from agent feeds, or as the result
    of enrichment (providing contextual information), aggregation or analytics/analysis
    sources. For instance, panels may provide information about the total number of
    received events, their variety, or a histogram depicting when events were received,
    just to name a few. Moreover, this data may be exported for integration with third-party
    tools. Visualization and Dashboards provide operators with suitable graphical
    tools to explore and analyze contextual information– such tools must provide querying
    and summarization capabilities adequate for dealing with large volumes of data
    in repositories, computing metrics and applying specific functions against some
    attributes. G. Business Policies and Rules Beyond the mandatory regulatory, legal
    and standardization frameworks, organizations often define specific procedural
    or workflow rules based on their own internal processes and needs, based on corporate
    laws, plans or roadmaps. A repository of CI business policies and rules may be
    used to support organizational-wide compliance assessment. If those events trigger
    some of the rules describing policies, then the associated alerts will also be
    triggered. Such rules can be tuned according to specific thresholds and can help
    prioritizing and score events. For example, a company policy may impose constraints
    on their employees on the use of resources, thus, any login attempt violating
    this rule should be reported. Physical access control is another example: alerts
    can be triggered when the doors in a given department or physical installation
    are opened out of the authorized period. Formally, those CI Business rules will
    assess the compliance of processes accordingly to the business norms. H. Monitoring
    A Monitoring component provides the capabilities to look at things as they happen,
    helping operators to identify anomalies from data. It can either trigger alerts
    or highlight information resulting from such a continuous assessment, matching
    CI audit compliance rules against persisted events in DL. Moreover, it will also
    check the level of trust and reputation risks to classify eventual threats and
    trigger alerts to the operators. Such a Monitoring component may also offer the
    ability to set up automatic response rulebooks or human-supported actions, as
    well as triggering alerts and notifications, providing information to help the
    operator become an effective link of a human-in-the-loop decision chain. Necessarily,
    models and rules used for alerting purposes must be fine tuned to provide adequate
    accuracy and low false positive rates. I. Real-Time Search A Real-Time Search
    component provides high-performance query capabilities from large amounts of stored
    data in the DL to support the extraction of relevant FCA information. The component
    is able to run queries against the indexed data in the DL. Because every second
    counts when looking up for quick responses, the process for indexing data can
    be executed in advance to improve the query performance. This component also includes
    an interface to integrate third-party components to run lookup actions for data.
    J. Orchestration The Orchestration component offers capabilities for managing
    and coordinating the different FCA components. Such capabilities comprise automation,
    self-healing, and service discovery. For cloud-native implementations, the use
    of containerized services integrated within a microservice architecture may improve
    scalability, eventually allowing for the deployment of multiple instances of the
    same architectural functional block to scale capabilities, as needed. A set of
    high-performance services requires a management and maintenance subsystem to coordinate
    the configuration settings supporting distributed FCA service synchronization
    and operation. While the capabilities of this framework are provided independently
    by different modules and components and made available to third parties through
    the use of an Application programming interface (API), the overall system can
    be depicted as being akin to an atomic structure. This paves the way to its possible
    provisioning and deployment in the form of a SaaS model. K. FCA Component Integration
    Figure 9 depicts the component integration view, providing a perspective that
    is complementary to the previous discussion. Here we can identify the ingestion
    stage, with the information coming from the several data sources being admitted
    by corresponding layer, where it can be also formatted and preprocessed with the
    help of the Ingestion core to perform filtering, normalization, indexing, enrichment,
    and aggregation. FIGURE 9. FCA component integration view. Show All Data streams
    can be persisted to a data lake (for batch processing purposes and forensics evidence
    persistence), also being sent over a fast path to feed stream processing mechanisms.
    The data lake can also store the results and outcomes of automatic and manual
    analytical processes. Next, the analytics layer provides the primitives and mechanisms
    for data analysis (by means of the Analytics engine), supporting the core FCA
    modules, as well as the monitoring, visualization and search components. Moreover,
    the results of FCA analysis tasks can potentially be used to update the Business
    Policies and Rules repository, which provides the knowledge base assisting the
    Analytic engine. Finally, the Orchestration layer performs a function that is
    orthogonal to the entire FCA framework, monitoring, managing and coordinating
    its components. L. A Cloud-native Platform as a Service The implementation of
    a framework designed according to the reference architecture hereby described
    can also benefit from adopting a Cloud-native architecture in which features are
    decoupled in microservices designed to improve scale-out capabilities, eventually
    hosted in containers. Taking this approach makes it possible to have wrap-up FCA
    solutions supporting a large number of customers. Providing independent external
    interfaces to the available functions of the cloud hosting or orchestration platform
    can provide instrumentation mechanisms for third parties, allowing them to tailor
    and deploy custom scenarios according to their needs. This approach increases
    the opportunity to integrate custom third-party solutions with the FCA, resorting
    to APIs or queuing mechanisms to enable effective integration between third-party
    applications and the FCA reference architecture. Such capabilities can enable
    the integration of specific policies and business rules, also providing the means
    to customize data sources, disable some components or extend their core capabilities,
    among other options. Moreover, this makes it possible, for instance, to customize
    solutions to integrate this reference architecture with solutions such as SIEM,
    SOAR, EDR and XDR. M. Platform Security Incorporating security in the FCA reference
    architecture allows to develop, deploy and operate each component safely, following
    the best practices in the field. It is also important to protect communication
    channels, providing secure inter-module integration. For this purpose, the adoption
    of Zero trust principles [351] may be a key design feature– while those principles
    are primarily focused on data and service protection, they can and should be expanded
    to include all enterprise assets and subjects. Complementary to active protection
    characteristics, Authentication and Authorization mechanisms should also be properly
    implemented and continuously assessed to check compliance with the defined access
    rules. N. Summary This Section proposed a reference architecture for FCA and its
    functional building blocks, according to the identified requirements, also detailing
    roles and interactions. Moreover, non-functional aspects comprising the implementation
    of Cloud-native Architecture, Platform as a Service, and Platform Security were
    also addressed, in order to demonstrate the plasticity of the proposed concept
    in terms of deployment and operational options. SECTION VIII. Discussion and Open
    Issues This Section discusses the findings of this survey and highlights the open
    issues and the research opportunities to be considered in the topic of FCA in
    the scope of CIP. A. Discussion When it comes to CIP, most literature references
    are focused on conventional cybersecurity prevention, detection and mitigation
    techniques. However, and given the considerable overlap of functionalities associated
    with security, forensics and compliance audit contexts, it makes sense to consider
    some proposals and technologies as candidates for application in FCA contexts.
    In fact, the lack of proper FCA capabilities within CIs may not be attributed
    to any sort of technological obstacles, but rather to a chronic lack of readiness.
    For instance, this can lead to situations where forensics procedures are undertaken
    on an as needed basis, long after incidents have occurred, in an offline basis.
    This can restrict the forensics process in a decisive way, hampering the establishment
    of a clear perspective about incidents, their root causes and implications. Moreover,
    and on the compliance auditing side, there is an ongoing trend requiring CI operators
    to comply with a growing body of standards and regulations while, at the same
    time, having to keep up with increasingly complex and interconnected infrastructures
    with a proliferation of control, sensory and endpoint devices. The implementation
    of adequate FCA mechanisms can assist in the prevention as well as in the mitigation
    of the potential consequences of incidents or adverse events, improving the CI
    resiliency. In fact, it is worth noticing that forensically reconstructing past
    events and highlighting disrupted compliance events in CI environments can make
    it possible to discover potential vulnerable vectors and hidden threats whose
    correction can be decisive to avoid future consequences. When it comes to FCA,
    proactivity is key. But operators need to understand the added value of adding
    such capabilities before committing to invest to adapt infrastructures, for instance
    to deploy and customize adaptor agents to extract the significant amounts of data
    living in silos (e.g. ICT systems surrounding the CI environment) into a single
    homogeneous coherent dataset, whose existence can help overcome the complexity
    arising from the use of a large number of forensic tools, protocols, and standards.
    With proper collection mechanisms in place, it becomes possible to correlate data
    by applying models, algorithms, architectures, and solutions to effectively classify
    and predict behaviors and extract evidence from large amounts of data or automatically
    support data-driven decision-making. Moreover, results from correlation can also
    help enforce auditing compliance on security policies, regulations, recommendations,
    applicable laws, and standards processes to increase the security and trust in
    CIs that may help to prevent future security incidents. Also, FCA need to keep
    up with times and adapt, as the trend towards resource consolidation also reaches
    CIs, with the adoption of virtualization technologies within private, public or
    hybrid clouds. For instance, while the adoption of a cloud-native setup with containers
    can bring significant challenges in terms of forensics integration, it can also
    provide net benefits in terms of management, monitoring, and control of FCA frameworks
    for CIP, providing elasticity to accommodate transient requirements from analysis
    processes. Another significant trend with impact in FCA processes is the emergence
    of IIoT and Big Data, which tend to go hand-in-hand in modern CIs, due to the
    considerable data handling requirements for massively distributed infrastructures.
    However, while such developments pose challenges to FCA solutions, it should also
    be noted that Big Data technologies also provide a technological basis enabling
    the development of sophisticated forensic data and evidence transport, processing
    and storage mechanisms that can take advantage of the elasticity of virtualization
    and cloud technologies. All the aforementioned aspects have been considered to
    devise a comprehensive and easy-to-deploy FCA framework template which was designed
    to be neutral from a deployment standpoint and decoupled from the end-user infrastructure
    to be protected. This reference design gathers the capabilities to collect and
    continuously monitor and correlate data from diversified data sources, being able
    to support decision-makers and forensics practicioners alike, also enabling the
    definition of responsive actions from large amounts of data. This approach can
    help track past events to perform evidence extraction and incident root cause
    analysis, also allowing to detect non-compliant events in near real-time, for
    example, from logs collected before, during, and after incidents. B. Open Issues
    This survey also identified a series of open issues and research gaps in terms
    of FCA capabilities for CIP. Probably one of the most important findings of this
    survey has to do with realising that, in most cases, existing security tools are
    missing the integration means for a full-stack FCA solution. This is due to the
    fact that many of these tools are not embracing open standards on maintaining
    an effective chain of custody or plug-and-play capabilities to increase their
    interoperability and reduce the need for collaborative work between tool owners
    and end-users. Also, many of these tools lack flexible FCA capabilities, decoupled
    from the applications they aim to protect (e.g. applied to 5G vertical applications
    taking advantage of cloud-native approaches). Other identified handicaps that
    equally affect SIEMs and forensics tools for CIP include: the absence of custom
    connectors and parsers for data source integration, incomplete data, lack of basic
    correlation rules, elemental storage capabilities, reliance on manual operation,
    basic reaction and reporting capabilities, limited data visualization, or deployment,
    and management complexity [55]. Other missing aspects comprise the lack of GDPR
    privacy compliance [352], as well as the absence of high-level security risk metrics.
    Also regarding metrics, there are no well-defined KPIs for FCA tools, for example
    to assess the Quality of Service (QoS) and Quality of Experience (QoE), reliability,
    availability, and resiliency. Also, the availability of open standards, languages,
    and data abstractions for sharing and exchanging evidence are key to enhance FCA
    tools and improve their interoperability while enhancing the processes devoted
    to discovering forensic evidence in an automated, effective, and efficient manner.
    That includes, for example, the adoption of open standards for sharing evidence
    and keeping an effective chain of custody. With the emergence of IIoT scenarios,
    the requirements to capture, transport and process of large volumes of data become
    more demanding. Thus, the lack of adequate computational and storage resources
    may impose limits on the application of FCA methodologies for gathering and analyzing
    data. Overcoming them is instrumental to achieve a near real-time data correlation
    latency from multiple physical sources, also enabling the deployment of effective
    alerting mechanisms for non-compliance incidents. Equally important is the lack
    of automated and dynamic orchestration capabilities, adaptation systems, and tools
    supporting FCA activities and managing their entire life cycle, which are key
    for implementing efficient and resource-effective FCA capabilities. Another key
    concern in FCA activities is their eventual impact on performance and efficiency
    on systems being secured, such as in the case of collecting large amounts of data
    for forensic purposes and preserving data privacy [306]. For instance, in systems
    with specific determinism and real-time requirements, special care must be taken
    to avoid imposing any kind of undesirable overhead or creating potential points
    of failure. SECTION IX. Conclusion This work highlights the importance of considering
    both forensics and compliance auditing (FCA) as high-priority topics for CIP,
    contributing with guidance in the design and implementation of security processes
    by considering policies, standards, guidelines and procedures and evidence analysis
    techniques. For this purpose, we surveyed the latest developments, methodologies,
    challenges, and solutions addressing FCA in the scope of CIP, focusing on contributions
    capable of tackling the requirements imposed by massively distributed and complex
    IACS which handle large volumes of heterogeneous, noisy, redundant and even ambiguous
    data, for analytic purposes. We started by highlighting the need for addressing
    modern security challenges and requirements to improve the security of CI by considering
    FCA capabilities. With that in mind, a survey of the the relevant literature was
    undertaken, focused on the intertwined topics that may stress the benefits and
    value brought by FCA approaches. From this survey it was also noticed the lack
    of specific FCA approaches and taxonomies for CIP. One of the reasons for this
    relates to the misleading perception that CIP requirements for FCA may be fulfilled
    resorting to generic solutions with integrator customisation. However, that may
    prove difficult due to the domain-specific standardisation and regulatory frameworks
    which often deviate from more generic recommendations, due to the limitations
    imposed by the often rigorous CI service continuity, reliability, security and
    safety requirements. Moreover, aspects such as broad heterogeneity of data sources,
    and the geographic and administrative dispersion of the CIs, also preclude a straightforward
    application of mainstream solutions. The surveyed literature resulted in a taxonomy
    gathering the major identified categories, such as CIs governance, preparedness,
    data acquisition, evidence identification, reporting, and data. Together with
    the lessons learned from the literature analysis, this taxonomy was instrumental
    to help identify the most relevant FCA capabilities, resulting in the identification
    of a series of key functional blocks later organized as part of a reference FCA
    architecture template, designed to provide a strong foundation to support the
    implementation of future solutions aiming to protect CIs. Authors Figures References
    Keywords Metrics More Like This Security-Gateway for SCADA-Systems in Critical
    Infrastructures 2022 International Conference on Applied Electronics (AE) Published:
    2022 Using integrated system theory approach to assess security for SCADA systems
    cyber security for critical infrastructures: A pilot study 2014 11th International
    Conference on Fuzzy Systems and Knowledge Discovery (FSKD) Published: 2014 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Survey on Forensics and Compliance Auditing for Critical Infrastructure
    Protection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wang Q.
  - Zhang J.
  - Liu Z.
  - Duan Y.
  - Li C.
  citation_count: '0'
  description: With the development of sequencing technology and the dramatic drop
    in sequencing cost, the functions of noncoding genes are being characterized in
    a wide variety of fields (e.g. biomedicine). Enhancers are noncoding DNA elements
    with vital transcription regulation functions. Tens of thousands of enhancers
    have been identified in the human genome; however, the location, function, target
    genes and regulatory mechanisms of most enhancers have not been elucidated thus
    far. As high-throughput sequencing techniques have leapt forwards, omics approaches
    have been extensively employed in enhancer research. Multidimensional genomic
    data integration enables the full exploration of the data and provides novel perspectives
    for screening, identification and characterization of the function and regulatory
    mechanisms of unknown enhancers. However, multidimensional genomic data are still
    difficult to integrate genome wide due to complex varieties, massive amounts,
    high rarity, etc. To facilitate the appropriate methods for studying enhancers
    with high efficacy, we delineate the principles, data processing modes and progress
    of various omics approaches to study enhancers and summarize the applications
    of traditional machine learning and deep learning in multi-omics integration in
    the enhancer field. In addition, the challenges encountered during the integration
    of multiple omics data are addressed. Overall, this review provides a comprehensive
    foundation for enhancer analysis.
  doi: 10.1093/bib/bbad442
  full_citation: '>'
  full_text: '>

    "Advertisement Journals Books Issues Submit Alerts About Briefings in Bioinformatics
    This issue                      Bioinformatics and Computational Biology Books
    Journals Oxford Academic                                   Advanced Search Volume
    25 Issue 1 January 2024 Article Contents Abstract INTRODUCTION THE APPLICATION
    OF DIFFERENT OMICS DATA IN ENHANCER RESEARCH ENHANCER DATABASE MULTI-OMICS INTEGRATION
    METHOD CHALLENGES IN MULTI-OMICS APPROACHES CONCLUSION ACKNOWLEDGEMENTS FUNDING
    ETHICS APPROVAL AND CONSENT TO PARTICIPATE CONSENT FOR PUBLICATION References
    < Previous Next > JOURNAL ARTICLE Integrative approaches based on genomic techniques
    in the functional studies on enhancers Qilin Wang, Junyou Zhang, Zhaoshuo Liu,
    Yingying Duan, Chunyan Li Briefings in Bioinformatics, Volume 25, Issue 1, January
    2024, bbad442, https://doi.org/10.1093/bib/bbad442 Published: 02 December 2023
    Article history PDF Split View Cite Permissions Share Abstract With the development
    of sequencing technology and the dramatic drop in sequencing cost, the functions
    of noncoding genes are being characterized in a wide variety of fields (e.g. biomedicine).
    Enhancers are noncoding DNA elements with vital transcription regulation functions.
    Tens of thousands of enhancers have been identified in the human genome; however,
    the location, function, target genes and regulatory mechanisms of most enhancers
    have not been elucidated thus far. As high-throughput sequencing techniques have
    leapt forwards, omics approaches have been extensively employed in enhancer research.
    Multidimensional genomic data integration enables the full exploration of the
    data and provides novel perspectives for screening, identification and characterization
    of the function and regulatory mechanisms of unknown enhancers. However, multidimensional
    genomic data are still difficult to integrate genome wide due to complex varieties,
    massive amounts, high rarity, etc. To facilitate the appropriate methods for studying
    enhancers with high efficacy, we delineate the principles, data processing modes
    and progress of various omics approaches to study enhancers and summarize the
    applications of traditional machine learning and deep learning in multi-omics
    integration in the enhancer field. In addition, the challenges encountered during
    the integration of multiple omics data are addressed. Overall, this review provides
    a comprehensive foundation for enhancer analysis. enhancer, multi-omics, high-throughput
    data analysis, data integration, machine learning Issue Section: Review INTRODUCTION
    The maintenance of transcriptional homeostasis is crucial to the development and
    growth of living things [1]. Transcriptional homeostasis is dependent on the interactions
    between transcription factors (TFs) and cis-regulatory elements (e.g. promoters
    and enhancers) [2]. In the 1980s, enhancers were first discovered in simian virus
    40 (SV40) [3]. Subsequently, researchers have gradually characterized different
    types of enhancers, and various techniques have been developed to predict and
    study the function of enhancers (Figure 1). In 2004, Benjamin predicted enhancers
    in the Drosophila genome based on sequence conservation, initiating the application
    of bioinformatics methods in enhancer research [4]. The eRNA and Super-enhancer
    were discovered in 2010 and 2013, respectively, further enriching the understanding
    of enhancers [5, 6]. In 2013, the introduction of CRISPR (Clustered Regularly
    Interspaced Short Palindromic Repeats)-Cas9 technology greatly accelerated the
    validation of enhancer functions [7]. Since 2016, with the rapid development of
    the machine learning field, scientists have gradually adopted neural network technology
    into enhancer research and have developed numerous models and softwares to study
    enhancers [8, 9]. The development of omics technology has brought a breakthrough
    in enhancer research. The dissection of the underlying transcriptional regulation
    of enhancers provides new insights into the complexity of transcriptional regulation.
    Figure 1 Open in new tabDownload slide Timeline of enhancer research. Figure 2
    Open in new tabDownload slide Challenges in the study of enhancers by omics methods.
    Figure 3 Open in new tabDownload slide Applications of different omics methods
    and CRISPR gene editing technology in enhancer research. Currently, omics approaches
    for enhancer research focus on four key questions (Figure 2). (i) How are enhancers
    identified? (ii) What induces the changes in enhancer activity? (iii) How do enhancers
    interact with their targets in the complicated 3D structure of the genome? (iv)
    What regulates the production and function of eRNA (enhancer RNA)? Since multiple
    biological processes are involved in the above four questions, genomic sequencing
    data provide opportunities to study such complicated issues by genomic sequencing
    data integration. Enhancer-associated sequencing technologies can be divided into
    four categories: genomics, epigenomics, transcriptomics and gene-editing technology
    (Figure 3). Genomics focuses on gene sequences and genomic structures of enhancers.
    Specifically, the former explores enhancers through genome variation-phenotype
    correlation and gene sequence conservation, while the latter identifies potential
    enhancers and target genes under the 3D structure of the genome [10–13]. Epigenomics
    identifies enhancers from the perspectives of chromatin spatial information, DNA
    interaction and modification, and RNA secondary structure [14–16]. Since active
    enhancers are transcribed into eRNAs, the transcriptome is widely applied to characterize
    enhancers and enhancer-target pairs based on expression correlation [5, 17–19].
    STARR-seq (self-transcribingactive regulatory region sequencing) is specifically
    designed to evaluate enhancer activity [20, 21]. CRISPR gene editing technology
    has been applied prevalently to knock out/down genes or enhancers. In addition,
    CRISPR gene editing technology has been developed to conduct large-scale parallel
    screening of enhancers followed by sequencing [22]. The centermost circle represents
    the transcriptional regulation by enhancers on the target genes; the inner circle
    presents a variety of techniques, particularly sequencing; the middle layer presents
    molecular information acquired from each technique and the outermost displays
    the classification. Genomic approaches are shown in blue, epigenomic approaches
    are represented by pink, transcriptomic approaches are shown in yellow and gene
    editing technology is represented by green. Although single omics data mining
    can initially screen out enhancers or genes correlated with a specific disease
    or phenotype, single omics analysis is subject to significant limitations, e.g.
    inadequate interpretation of data in a single dimension and insufficient depletion
    of signal noise [23]. Multi-omics analysis has the advantage of diversity and
    is systematic, making it more conducive to clarifying the underlying mechanisms
    of enhancers [24–27]. The integration of multi-omic data provides more reliable
    results and dramatically reduces the false-positive rate [28]. Over the past decade,
    various multi-omics analysis methods have been developed. However, there are still
    many challenges, such as the accuracy variation among different omics data, missing
    values, and computational and storage costs [29]. This review will discuss the
    data characteristics of different omics in enhancer research, the methods of multi-omics
    data analysis and the challenges in multi-omics research. THE APPLICATION OF DIFFERENT
    OMICS DATA IN ENHANCER RESEARCH The widespread application of sequencing technologies
    has provided a wealth of molecular information in the enhancer field (Table 1).
    To better illustrate the sources and applications of different types of molecular
    information, we categorized omics approaches from the perspective of the research
    subject: genomics, epigenomics, transcriptomics and CRISPR editing technologies
    (Figure 3). Table 1Key questions about enhancers are addressed by different molecular
    information Omics methods Molecular information Identification Activity Structure
    eRNA Genomics DNA sequence √ √ Genomics 3D structure √ √ √ Epigenomics Chromatin
    accessibility √ √ Epigenomics DNA methylation √ √ Epigenomics DNA–Protein interaction
    √ √ Epigenomics DNA–DNA interaction √ √ √ Epigenomics DNA–RNA interaction √ √
    √ Epigenomics RNA secondary structure √ √ √ Transcriptomics Gene expression √
    √ Transcriptomics Enhancer expression √ √ √ Transcriptomics Enhancer activity
    √ Gene editing technology Gene/enhancer activation √ √ √ Open in new tab Genomics
    Driven by the progress of sequencing technology and the decline in sequencing
    cost, large-scale population genome sequencing has been initiated in many countries,
    and the amount of data has grown exponentially [30]. SNPs (single nucleotide polymorphisms),
    SVs (structural variations), CNVs (copy number variations), InDels (insertions–deletions)
    and other molecular information can be obtained using WGS (whole genome sequencing),
    WES (whole exon sequencing), WGRS (whole genome resequencing) and other genomic
    sequencing techniques [31]. Researchers have developed GWAS (genome-wide association
    study) analysis and eQTL (expression quantitative trait loci) analysis methods
    to study the relationship between SNPs or CNVs and phenotypes. The GWAS method
    can analyze millions of SNPs in the genome simultaneously, which has the advantages
    of high efficiency and wide coverage [32]. eQTL is used to study the relationship
    between gene expression level and genotype [33]. Young group summarized the results
    of 1675 GWAS and found 5303 SNPs associated with various diseases. The majority
    of SNPs are in noncoding regions (93%), and among these, 64% of the loci are enriched
    in enhancer regions [34]. However, the GWAS method has limitations, such as the
    inability to identify complex traits, the inability to assess rare genetic variants
    and the uncertainty of gene-phenotype associations [32]. Compared with GWAS analysis,
    eQTL is advantageous in exploring gene expression regulation mechanisms and gene-phenotype
    associations. Since eQTL information can determine genetic variants associated
    with gene expression levels, it can more accurately identify potential enhancer
    elements [35, 36]. By introducing eQTLs from the 1000 Genomes Project, Chen et
    al. identified 65 pairs of cancer-specific enhancer genes [36]. Chignon et al.
    conducted a colocalization analysis of enhancer–promoter locations with tissue
    eQTL locations associated with genetic coronary artery disease, evaluating the
    importance of genetic variability in the disease [35]. In both studies, the approach
    to integrate enhancers with eQTLs was a location-based approach [35, 36]. Transcription
    regulation is closely correlated with the 3D conformation of chromatin, which
    is an alternative perspective for studying enhancers [37]. Hi-C (high-throughput
    chromosome conformation capture) has been the most extensively performed approach
    for 3D genome sequencing at the genome-wide level, with the advantages of wide
    coverage, high accuracy and more complete sequence positioning [38]. Since Hi-C
    data provide comprehensive information on chromatin interactions, they are used
    to determine the binding of enhancers to target genes in physical space [37].
    The Hi-C derivative technologies include ChIA-PET (chromatin interaction analysis
    based on paired-end-tag sequencing) [39], HiChIP (in situ Hi-C followed by chromatin
    immunoprecipitation) [40] and PLAC-seq (proximity ligation-assisted ChIP-seq)
    [41]. These techniques can detect specific protein-mediated chromatin loops at
    high resolution, which are also used in enhancer analysis. However, for most tissues
    and cell lines, the Hi-C and Hi-C derivative technologies have the disadvantage
    of insufficient resolution [42]. The exponential growth in data volume and depth
    brings new analytical challenges as well [43]. Epigenomics Epigenetics (e.g. DNA
    methylation, RNA modification, RNA secondary structure, histone modifications,
    etc.) refers to a type of regulatory mechanism on phenotypic properties by regulating
    gene transcription or translation processes without changing the DNA sequence
    [44, 45]. In enhancer studies, epigenomics methods can be divided into four categories
    according to the differences in research objects: chromatin accessibility, DNA
    modification, DNA interaction and RNA interaction (Figure 3). Chromatin accessibility
    The open state of eukaryotic chromatin is considered as a prerequisite for transcription.
    Four sequencing techniques have been developed to identify chromatin regions in
    the open state: DNase-seq (DNaseI sequencing), MNase-seq (micrococcal nuclease
    digestion and sequencing), FAIRE-seq (formaldehyde-assisted isolation of regulatory
    elements) and ATAC-seq (assay for targeting accessible chromatin with high-throughput
    sequencing) [46–48] (Figure 3). DNase-seq and MNase-seq are both genome sequencing
    techniques based on enzymatic digestion to determine chromatin accessibility.
    DNase-seq combines nonspecific endonuclease DNase I (Deoxyribonuclease I) to obtain
    DNA sequences between nucleosomes, whereas MNase-seq obtains DNA sequences wrapped
    around nucleosomes using micrococcal nuclease (MNase). Consistently, these techniques
    are used to identify active enhancers by high chromatin accessibility. However,
    both DNase I and MNase enzymes have sequence preferences, resulting in uneven
    signal distribution and false-negatives [49–52]. FAIRE-seq uses the difference
    in the solubility between DNA with or without nucleosome wrapping in phenol and
    chloroform. The DNA in nucleosome-free regions is determined by sequencing the
    DNA in the aqueous phase. FAIRE-seq overcomes the sequence preference of MNase
    and DNase I, but the low signal-to-noise ratio and the high background signal
    make FAIRE-seq data difficult to interpret [47, 53]. As the main sequencing technology
    for open chromatin so far, ATAC-seq employs the modified Tn5 transposase to randomly
    insert designed DNA sequences with adapter sequences into the open chromosomal
    regions. Fragmentation by Tn5 transposase and ligation with adapters are performed
    simultaneously, such that the sequencing library preparation process is notably
    simplified [54]. ATAC-seq has good repeatability, strong consistency and significant
    signals, and as few as 500 cells are needed, although mitochondrial contamination
    is inevitable [55]. Innovations in single-cell genomic technologies make it possible
    to map regulomes in individual cells. The single-cell ATAC-seq (scATAC-seq) and
    single-cell DNase-seq (scDNase-seq) are two technologies for analyzing open chromatin
    in single cells. By adding barcode sequences to each cell, scientists are able
    to examine heterogeneous samples at cellular resolution [56]. In enhancer studies,
    chromatin accessibility analysis methods are often employed to screen potential
    active enhancers. For example, Chen and Liang hypothesized a negative correlation
    between enhancer activity and the strength of nucleosome binding. To validate
    the hypothesis, they integrated enhancer position information and MNase-seq data
    from 29 different tissues/cell types and observed a reduction in nucleosome signals
    on the eRNA loci compared with the flanking sequences across all 29 tissue types.
    Integrating these findings with RNA-seq data to determine the eRNA expression
    level, they identified ~200 000 new eRNA loci [57]. Through in-depth analysis
    of single-cell RNA sequencing (scRNA-seq) and scATAC-seq data from mouse embryonic
    spinal cord, an enhancer regulatory network algorithm, called eNET, successfully
    identified enhancers crucial to the development of spinal cord neurons [58]. DNA
    modification In most cancer types, the proportion of DNA methylation in the enhancer
    region is negatively correlated with its activity [59]. Yu’s group developed Guide
    Positioning Sequencing technology. By harnessing the 3′ → 5′ exonuclease and 5′ →
    3′ polymerase activities of T4 DNA polymerase, methylcytosines were introduced
    into the 3′ end of each DNA fragment. Following bisulfite treatment, the 3′ read
    of each DNA fragment serves as a guide to determine the DNA methylation status
    of the paired 5′ read [60]. The approach improves the efficiency and accuracy
    of the mapping rate, and there is no sequence preference in methylation detection
    [60]. By comparing the changes in DNA methylation and H3K27ac histone modification
    between normal liver and two liver cancer cell lines (97 L and LM3), they discovered
    that the DNA methylation levels were increased, the H3K27ac peaks were lost in
    5 liver-specific enhancer regions, and the expression of target genes was silenced
    in liver cancer cells. Therefore, they concluded that aberrant DNA methylation
    pattern in enhancer regions may alter the activity of enhancers, resulting in
    alterations in the expression of target genes. DNA interaction The interactions
    between DNA and other molecules, such as proteins, DNA and RNA, modify the structure
    or binding affinity of DNA. These processes can also result in functional alterations
    in enhancers. Given the crucial role of DNA interactions in biological processes,
    a multitude of sequencing technologies have been devised for their study. Based
    on the different interaction partners, we will introduce various omics technologies
    from three perspectives: DNA–protein interactions, DNA–DNA interactions and DNA–RNA
    interactions. DNA–protein interactions To characterize the interaction between
    DNA and protein, ChIP-seq (Chromatin Immunoprecipitation sequencing), FiTAc-seq
    (fixed-tissue ChIP-seq for H3K27ac profiling) and CUT&Tag (cleavage under targets
    and tagmentation) have been prevalently performed [14–16, 61–64]. ChIP-seq, first
    developed in 2007, has become one of the most prevalent methods for identifying
    the binding sites of TFs on DNA and DNA interacting with certain histone modifications
    to study epigenetic mechanisms [65]. However, the prolonged exposure of clinical
    specimens to formalin results in excessive chemical cross-linking, which limits
    the isolation of soluble chromatin. Therefore, the signal intensity of ChIP-seq
    analysis for FFPE (formalin fixation and paraffin embedding) samples is low, and
    the resolution is poor [63]. Therefore, FiT-seq and FiTAc-seq were developed to
    obtain high-quality information on the signal distribution of H3K4me1 and H3K27ac
    for FFPE samples [63, 64]. Another disadvantage of ChIP-seq is the low peak signal,
    high background noise and sometimes uneven distribution of target DNA fragments
    [66]. To address the issues with ChIP-seq, in 2019, Kaya-Okur et al. developed
    CUT&Tag technology, which requires fewer cells, with a minimum of 60 cells, and
    which has library construction steps that are simplified by removing the steps
    of formaldehyde crosslinking and ultrasonic interruption [67]. CUT&Tag has the
    advantages of lower background noise, higher reading accuracy and better data
    repeatability [68]. In general, there are two main directions to study enhancers
    using sequencing data: to screen active enhancers by detecting enriched histone
    modifications (H3K4me1 and H3K27ac) and to characterize enhancer-target pairs
    by binding to certain transcription activators or coactivators [69, 70]. DNA–DNA
    interactions Extrachromosomal circular DNA (eccDNA) is a circular and double-stranded
    molecule in the nucleus that is independent of chromosomal DNA (chrDNA). These
    eccDNAs can vary greatly in size, ranging from tens to millions of base pairs
    [71]. eccDNA interferes with the replication and expression of genes by interacting
    with chrDNA [71]. In addition, eccDNA functions as a mobile enhancer to increase
    the transcription of genome-wide target genes [72]. At present, canonical DNA
    sequencing can indirectly predict eccDNA through sequence information, while Circle-seq
    is specifically designed to detect eccDNA [73]. DNA–RNA interactions Increasing
    evidence suggests that nascent RNAs mediate the chromosomal interaction between
    promoters and enhancers several mega-bases away in linear distance. GRID-seq (global
    RNA interactions with DNA by deep sequencing) is a technique for unbiased detection
    of DNA–RNA interactions at the genome scale [74]. GRID-seq is complementary to
    Hi-C in studying 3D chromatin architecture [75, 76]. However, GRID-seq requires
    rather deep sequencing to generate a robust contact map, which limits its application
    [74, 75]. RNA–RNA interactions RNA molecules in the cell nucleus form secondary
    structures via intramolecular base pairing to exert their biological functions.
    For example, eRNA and promoter upstream antisense RNAs (also known as promoter
    upstream transcripts, PROMPTs) form enhancer–promoter loops to activate transcription
    [77]. Thus, deciphering the higher-order structure of RNA is crucial for understanding
    the underlying mechanisms [77, 78]. RIC-seq can accurately capture the secondary
    structure of RNA and identify RNA–RNA interactions through chimeric sequences.
    In HeLa cells, 31 genes were predicted as target genes of 7 enhancers by RIC-seq
    (RNA in situ conformation sequencing). Locked nucleic acid and antisense oligonucleotides
    were used to knock down the 7 enhancers, and the expression of 27 predicted target
    genes was decreased accordingly. Therefore, the prediction accuracy for target
    genes for enhancers was >85% based on RIC-seq. RIC-seq will be helpful to study
    the regulatory role of eRNA in promoter activity. Transcriptomics The transcriptome
    refers to the collection of all RNAs transcribed in a specific tissue or cell
    at a certain stage [79]. The most extensively employed transcriptome detection
    methods comprise microarray, RNA-seq, scRNA-seq, spatial transcriptome sequencing
    and other derivative techniques [17]. RNA-seq, the most prevalent transcriptome
    sequencing technology, represents low background noise, accurate quantification
    and higher resolution of differentially expressed genes, and has a much lower
    limit of detection than a standard whole genome microarray [18, 19]. However,
    in model organisms, microarrays are reliable and more cost effective than RNA-seq
    [80]. To address the different cell states within a sample, single-cell transcriptomics
    was developed in 2009. Nowadays, many single-cell transcriptome platforms have
    emerged, such as 10X Genomics, BD Rhapsody, Fluidigm C1, etc. Among these platforms,
    the 10X Genomics single-cell transcript platform is the most commonly used due
    to its high-throughput and efficiency in capturing 100–80 000 cells (per chip)
    [81]. The scRNA-seq and spatial transcriptome sequencing endow expression information
    with high accuracy and specificity at single-cell resolution, whereas the steep
    price and the complexities in data analysis hinder their prevalence [82, 83].
    In addition to conventional RNA-seq, STARR-seq has been applied to detect enhancer
    activity [84]. STARR-seq is a massively parallel reporter assay that identifies
    transcriptional enhancers based on their activity across the genome and quantitatively
    assesses their activity [84]. Transcriptome sequencing (RNA-sequencing) has been
    performed to study the expression and genomic alterations of enhancers. First,
    the transcriptome provides expression information for both target genes and enhancers.
    Compared with mRNAs, eRNAs have the characteristics of instability and low expression
    level, and most eRNAs do not contain polyA tails [85, 86]. Most RNA-seq studies
    utilize oligo-dT enrichment to capture polyA-tailed RNAs, which results in low
    detection efficiency for eRNAs. scRNA-seq and spatial transcriptomics sequencing,
    with relatively low depth, have not yet been performed to obtain eRNA expression.
    In addition to RNA-seq, GRO-seq (global nuclear run-on sequencing), PRO-seq (precision
    nuclear run-on sequencing), CAGE-seq (cap analysis of gene expression by deep
    sequencing) and other RNA-seq-derived techniques have been employed to capture
    eRNAs [87–91]. CRISPR gene editing technology The integration of gene editing
    techniques and second-generation sequencing technology implements genome-wide
    parallel screenings for enhancers regulating a specific phenotype. CRISPR/Cas9
    technology has been applied in enhancer screening, functional verification and
    target gene identification [22]. Various CRISPR-derived techniques for high-throughput
    screening of enhancers have been developed, such as CRISPRi-FlowFISH. CRISPRi-FlowFISH
    integrates CRISPRi with RNA fluorescence in situ hybridization (FISH) technology.
    The main principle is that gRNA guides KRAB-dCas9 to bind to a specific nucleotide
    sequence and inhibit the transcription of the sequence 200–500 bp near the gRNA.
    Subsequently, RNA FISH has been used to quantitatively label single cells based
    on the expression level of a gene of interest. When an enhancer is targeted by
    gRNA, CRISPRi-FlowFISH can quantify the effect of the enhancer on the target gene(s)
    [92]. Furthermore, Perturb-seq (also referred to as CRISP-seq and CROP-seq) integrates
    multiplexed CRISPR-mediated gene inactivation with scRNA-seq to comprehensively
    evaluate gene expression phenotypes for each perturbation. By designing sgRNAs
    (single guide RNAs) for enhancers, Perturb-seq enables simultaneous quantitative
    measurement of enhancer expression in many cells and a wealth of phenotypic information
    and greatly improves screening efficiency [93]. ENHANCER DATABASE With the continuous
    growth of genomic data and experimental results, the enhancer databases have become
    an essential resource to study enhancers efficiently. Currently, there are more
    than a dozen databases that are widely used (Table 2). VISTA enhancer browser,
    DiseaseEnhancers and ENdb are three databases collecting enhancers experimentally
    validated [94–96]. So far, the VISTA enhancer browser (https://enhancer.lbl.gov/)
    has collected 1699 human or mouse noncoding elements with enhancer activity assessed
    in transgenic mice [94]. The DiseaseEnhancer (https://github.com/shijianasdf/DiseaseEnhancer/tree/master)
    database has collected 1059 experimentally validated disease-related enhancers
    from 167 human diseases based on literature [95]. And the ENdb (https://bio.liclab.net/ENdb/index.php)
    database is a manually curated enhancer database for human and mouse from 1590
    published literatures, with 713 experimentally validated enhancers and their related
    information, including target genes, TFs, diseases and functions [96]. Cancer-specific
    enhancers are one of the hot topics in enhancer research. CancerEnD (https://webs.iiitd.edu.in/raghava/cancerend/)
    has conducted on 18 different cancer types by RNA expression data from TCGA, providing
    8599 enhancers of 8063 cancer samples [97]. CenhANCER (http://cenhancer.chenzxlab.cn/)
    has collected H3K27ac ChIP-seq data from 49 cancer types, and predicts >57 million
    enhancers [98]. The TCEA database (https://bioinformatics.mdanderson.org/Supplements/Super_Enhancer/TCEA_website/)
    has collected TCGA and GTEx RNA-seq data and provides the downloadable eRNA expression
    data that has been calculated [57]. In addition to cancer-specific enhancers,
    tissue-specific and disease-related enhancer research is another key issue in
    the enhancer field. The GeneHancer (http://www.genecards.org/) database uses an
    integration algorithm to eliminate redundancy and identifies >434 000 tissue-specific
    enhancers from multiple data sources [25]. Mutations on the DNA sequences of enhancers
    may cause diseases by affecting target gene expression. HACER (http://bioinfo.vanderbilt.edu/AE/HACER/)
    utilizes GWAS information on disease-related genetic variation sites to link enhancers
    to diseases [99]. In addition to enhancers in human, enhancer research in mouse
    and other mammals gains increasing attention, as well. Fantom5 (https://fantom.gsc.riken.jp/5/)
    and RAEdb (http://www.computationalbiology.cn/RAEdb/index.php) have predicted
    different enhancers in humans and mouse, respectively, through CAGE-seq and STARR-seq
    methods [91, 100]. EnhancerAtlas2.0 (http://www.enhanceratlas.org/) collected
    data from 12 different tissue samples and predicted enhancers for 9 different
    mammalian species, greatly expanding the scope of enhancer research [101]. Based
    on the evolutionary conservation on enhancer between species, studies on enhancers
    and enhancer-gene interactions were performed in other model organisms. scEnhancer
    (http://enhanceratlas.net/scenhancer/), the first database to annotate enhancers
    at the single-cell level, covering 14 527 776 enhancers from 1196 906 single cells
    in human, mouse and Drosophila. Table 2Comparison of commonly used enhancer databases
    Database Species Enhancers eRNA Specificity Experimental result CancerEnD Human
    168 464 No Cancer 0 CenhANCER Human >57 000 000 No Cancer 0 DiseaseEnhancer Human
    1059 No Disease 1059 ENdb Human/Mouse 713 No Disease 713 EnhancerAtlas2.0 9 species
    13 494 603 No None 0 Fantom5 Human/Mouse 65 359 Yes None 0 GeneHancer Human 434
    139 Yes None 0 HACER Human 1676 284 No Disease 0 RAEdb Human/Mouse >500 000 No
    None 0 scEnhancer 3 species 14 527 776 No None 0 TCEA Human >300 000 Yes Cancer
    0 VISTA Human/Mouse 3321 No None 1699 Open in new tab Figure 4 Open in new tabDownload
    slide Classification of multi-omics integration methods in enhancer research.
    MULTI-OMICS INTEGRATION METHOD In recent years, the development of mathematics,
    statistics and computational science has laid the foundation for the integration
    of multi-omics analysis. At present, multi-omics integration methods can be divided
    into two categories based on whether neural networks are used: traditional machine
    learning models, which have the advantages of strong interpretability of algorithms
    and lower requirements for computing resources; and deep learning models using
    neural networks, which can capture complex relationships in data due to their
    powerful nonlinear fitting capabilities (Figure 4) [102–104]. The key factors
    in determining the two methods include data volume, computational resources and
    feature numbers. Neural networks require a large volume of data (at least thousands
    of samples) to avoid overfitting and abundant computational resources (hardware,
    software, etc.) [105]. Compared with traditional machine learning, one advantage
    of neural networks is that they do not require a large amount of manual labeling,
    and only simple data preprocessing is required for computation [106]. When choosing
    a method, researchers should weigh the characteristics of the issue to resolve.
    Traditional machine learning models Traditional machine learning models are algorithms
    that use statistics, linear algebra and optimization algorithms to extract information
    from existing data to build predictive models. The classical machine learning
    methods, such as logistic regression, random forests and naive Bayes, are used
    to predict and classify unknown data. Based on whether manually annotated labels
    are required for data, it can be divided into three types of learning: unsupervised,
    semi-supervised and supervised (Table 3). Table 3Model classification for the
    prediction of enhancer Traditional machine learning Algorithms Tool name Model
    Unsupervised Distance ABC Distance-based Unsupervised Correlation ELMER Pearson
    correlation Unsupervised Correlation CISMAPPER Pearson correlation Semi-supervised
    Regression McEnhancer Logistic regression Semi-supervised Classifier DPHM Bayesian
    model Supervised Regression JEME Regression-based methods Supervised Regression
    FENRIR Elastic net logistic regression Supervised Classifier FOCS Linear regression
    Supervised Classifier IM-PET Random forest Supervised Classifier PETModule Random
    forest Supervised Classifier RIPPLE Random forests Supervised Classifier TargetFinder
    Gradient tree boosting Open in new tab Unsupervised learning Unsupervised learning
    is an analytical approach that eliminates the need for prelabeled training data.
    The main objective of unsupervised learning is to unveil hidden patterns and establish
    new connections between variables within a dataset [107]. In the study of enhancers,
    unsupervised learning methods can be divided into distance-based methods and correlation-based
    methods [108]. The earliest method used to predict enhancer target genes was the
    distance-based method, which relies on the genomic proximity between enhancers
    and genes. This approach assumes that enhancers tend to regulate nearby genes
    in the genome [109, 110]. However, the accuracy is not high, the variation range
    is large and the false discovery rate (FDR) is ~40–73% [111]. Even when RNA expression
    data have been used to screen the enhancer’s regulatory gene, its accuracy has
    remained low, with FDR values ranging from 53 to 77% [112]. Furthermore, the distance-based
    method overlooks distal regulatory interactions and the situation in which multiple
    enhancers target the same promoter [5]. Therefore, the distance-based method is
    generally used as a baseline [109]. For example, the ABC model predicts cell type-specific
    enhancer-target pairs based on the distance between enhancers and genes, the frequency
    of chromosomal contact between enhancers and promoters (by Hi-C data analysis),
    and enhancer activity (by DNase-seq and H3K27ac ChIP-seq) [92]. Developed from
    distance-based methods, correlation-based methods combine the correlation of features
    (e.g. histone modifications, DHS signals of enhancers and promoters, and gene
    transcription levels) to increase the prediction accuracy, such as ELMER and CisMapper
    [113–116]. ELMER identifies transcriptional targets by correlating methylation-affected
    enhancers with the expression of nearby genes. A nonparametric U test was used
    to examine the correlation degree of enhancer methylation and expression data
    (RNA-seq) with 10 genes upstream and 10 genes downstream of each enhancer, and
    all enhancer-gene pairs with P < 0.001 were retained [115]. CisMapper predicts
    enhancer-target pairs by calculating the Pearson correlation coefficient between
    the log of gene expression and the log of the histone signal at the TF-binding
    site within 500 kb upstream of the gene TSS [116]. CisMapper is more accurate
    than simple distance-based methods, with an average accuracy improvement of 2.7
    times [116]. Semi-supervised and supervised learning Semi-supervised learning
    uses algorithms that cover both unlabeled and labeled data for training, which
    is preferred when there is not enough labeled dataset available for supervised
    learning [117]. Compared with supervised learning, semi-supervised learning can
    reduce overfitting and improve the robustness of the model [117]. Supervised learning
    depends on high confidence positive and negative labeled training datasets (enhancers
    and non-enhancers, respectively). The model is usually trained to maximize the
    distinction between case and control sets [118]. Dependent on the algorithm applied
    in the model, semi-supervised and supervised learning can be divided into regression-based
    methods and classifier-training [109]. Regression-based methods (e.g. McEnhancer,
    JEME, FENRIR and FOCS) integrate enhancer and promoter features or gene expression
    to identify the regulatory relationship between enhancers and target genes [119–122].
    McEnhancer uses a semi-supervised logistic regression model to calculate the probability
    of TFs binding to promoters and enhancers, and predicts the binding strength between
    genes and enhancers, with a prediction accuracy of 73–98% [122]. The merged regulation
    by multiple enhancers is considered in JEME, and sample-specific information is
    integrated as well to predict gene regulatory networks [121]. FENRIR integrates
    thousands of different epigenetic and functional genomics datasets to infer tissue-specific
    functional relationships between enhancers in 140 different human tissues and
    cell types [119]. FOCS is a statistical framework that utilizes eRNA as a marker
    of enhancer activity and determines enhancer–promoter interactions correlated
    with transcriptional activity based on information about chromatin epigenetic
    modifications [120]. The classifier training method uses experimentally identified
    enhancer–promoter interactions as the gold standard set. By learning the sequence
    and epigenetic modification features of the standard, a classifier can be trained
    to predict whether a given enhancer–promoter pair has an interaction or not [109].
    DPHM, as a semi-supervised Bayesian model, predicts target genes of 47 enhancers
    in mice using Nkx2-5 ChIP-seq data [123]. IM-PET tool, using the random forest
    classifier algorithm, predicts the association between enhancers and promoters
    by collecting a large amount of enhancer feature data (epigenetic modification
    data, TF expression data, enhancer conservation data, etc.) [124]. The tool has
    high predictive accuracy, with an FDR reduced to ~1%, and the predicted distance
    between enhancers and target genes is also extended to 2 Mb [124]. PETModule,
    RIPPLE, TargetFinder, EAGLE and EPIP are algorithms that adopt supervised learning
    methods to predict enhancer-target gene interactions. Although they use different
    classification features, they all present good prediction performance on different
    datasets [112, 125–128] (Table 3). Deep learning models Since 2016, scientists
    have gradually begun to use neural network technology to study enhancers [8, 9].
    Many studies have shown that neural networks have significant advantages in enhancer
    research, such as being able to predict across different cell types, thereby reducing
    computational and time costs [129, 130]. Convolutional neural networks (CNNs)
    have become the widely used algorithm in enhancer research, and various models
    such as DNABERT [131], iEnhancer-GAN [132], GC-MERGE [133], GraphReg [134], EPIVAN
    [130] and DeepTACT [135] have been proposed and optimized (Figure 3). DNABERT,
    as a novel pre-trained bidirectional encoder representation, can reveal the potential
    associations between different cis-DNA by learning DNA sequence information [131].
    iEnhancer-GAN integrate word embeddings and sequence generation adversarial networks
    to predict the binding strength of enhancer-target gene interactions [132]. DeepTACT
    applies a bootstrapping deep-learning model to integrate genome sequence and chromatin
    accessibility data to predict enhancer–promoter interactions [135]. GC-MERGE is
    a graph-based deep learning framework that decodes Hi-C map through graph convolutional
    networks to capture the potential genomic spatial structure. It models the epigenetic
    modification signals and DNA sequence information to predict the target genes
    regulated by distant enhancers [133]. GraphReg model uses CNN layers to learn
    1D features of enhancer-target gene (epigenomic data, genomic DNA sequence, etc.),
    and then constructs different enhancer-target genes into a whole through iterative
    methods on 3D genomic maps (such as HiChIP, Hi-C, etc.) by using graph attention
    networks (GAT) [134]. Compared with linear CNN models (such as Epi-CNN, Seq-CNN,
    etc.), the GraphReg model requires less sample size and has higher accuracy in
    prediction [134]. Graph-based methods (GC-MERGE and GraphReg) have advantages
    in handling complex relationships, robustness and data utilization compared with
    traditional machine learning methods. Compared with linear CNN models, graph-based
    methods have the advantages of strong interpretability, fast calculation efficiency
    and high accuracy in predicting long-distance enhancer-target genes [133, 134].
    With more and more researchers focusing on graph theory and deep learning techniques
    in bioinformatics, graph-based methods will provide more powerful tools for analyzing
    enhancer-target gene networks [136]. Figure 5 Open in new tabDownload slide Challenges
    and resolution strategies for multi-omics integration methods in enhancer research.
    In addition to CNN, architectures based on deep neural networks (DNN) are used
    to learn enhancer features as well. For example, EP-DNN uses p300 binding sites
    as markers for enhancers, and TSS and random non-DHS sites as markers for non-enhancers
    to perform training. The prediction accuracy of EP-DNN is 91.6%, exceeding the
    accuracy of DEEP-ENCODE (85.3%) and RFECS (85.5%) [129]. ES-ARCNN is a computational
    model for predicting the enhancers strength. To train ES-ARCNN, researchers applied
    two data augmentation tricks (i.e. reverse complement and shift) to improve the
    model’s predictive performance [137]. Enformer, as a developed enhancer prediction
    model based on the transformer, can integrate information of remote interactions
    in the genome (up to 100 kb away) [138]. By calculating the contribution scores
    of gene input gradients and attention weights, Enformer can identify the enhancer
    sequences that are most predictive of specific gene expression [138]. Although
    deep learning has outperformed many traditional computer methods in enhancer prediction
    applications, the problems of overparameterization and limited model performance
    still exist, and its interpretability lags behind traditional statistical methods.
    Continuous development of new deep learning methods is expected to achieve elegant
    applications in enhancer research. CHALLENGES IN MULTI-OMICS APPROACHES In recent
    years, there has been an increasing number of studies on enhancers by multi-omics
    approaches. However, there are still challenges in the application of multi-omics
    approaches to study enhancers, either due to a lack of sufficient attention or
    limited solutions. We have summarized the five major challenges in enhancer research,
    and provided some possible methods to overcome these challenges (Figure 5). The
    accuracy variation between different omics data Multi-omics data from different
    sources are often heterogeneous, with divergence in signal-to-noise ratios and
    significant differences in accuracy [139]. For instance, genome sequencing has
    a higher coverage than RNA-seq; transcriptomics and ChIP-seq use different quantification
    methods (the former uses RPM or count values, while the latter quantifies based
    on peak areas), resulting in different data ranges and distributions [140]. Currently,
    increasing the number of samples and improving experimental design can improve
    the statistical power of different omics analyses. However, according to MultiPower
    software, in the estimation of sample size required to achieve specific statistical
    power in different omics, DNA-seq and ChIP-seq require close to more than double
    the sample size of RNA-seq samples to achieve the same statistical effect [141].
    Therefore, it is inefficient and expensive to improve accuracy only by increasing
    sequencing samples. Instead, one can consider balancing sample sizes through undersampling
    [141]. In addition, it can also evaluate the performance of machine learning by
    using standardized metrics to choose the optimal sample size, or by adopting techniques
    (such as regularization, bagging, cross-validation) to balance bias-variance trade-offs
    [142, 143]. Missing value imputation in multi-omics data Data may be missing due
    to experimental random errors or inherent technical defects (e.g. low coverage
    in repetitive regions) during sequencing [144]. Consequently, some unmatched data
    have to be excluded during data integration, limiting the power for detection
    in the genome. Surprisingly, the problem of processing missing values is often
    treated as a data preprocessing step, and some scientists do not believe that
    it will have any impact on the outcomes of subsequent statistical analyses. Instead,
    the distribution characteristics of the multi-omics data should be reassessed
    in the analysis process, and sensitivity analysis should be performed to assess
    the impact of missing value inputs on the downstream analysis [29]. Imputation
    methods have the potential to correct missing values by leveraging the correlations
    within omics data and utilizing partially measured data from other omics datasets
    to impute missing values. MOFA analyzes the latent space across omics types to
    impute missing samples, and MultiBaC creates a multivariate predictive model of
    the incomplete omics types as a function of a shared omics modality [145, 146].
    However, these two methods can create data structures that violate the assumption
    of independence and subsequently lead to unreliable analysis [29, 147, 148]. Therefore,
    the missing values across different data resources affirm the reliability and
    applicability of multi-omics analysis, and a better solution is urgently needed.
    Liew et al. compared 19 different missing value completion algorithms and found
    that the choice of algorithm should be assessed from an application-driven viewpoint,
    and validation of the imputation data is an important step in evaluating the performance
    of any input algorithm [149]. There is no one optimal imputation algorithm for
    all type of data, so it is necessary to choose an appropriate imputation algorithm
    according to the characteristics of the data [149]. Evaluation of model performance
    Currently, the computational models for integrating multi-omics data in biology
    possess various characteristics (e.g. accuracy, speed, complexity and computational
    cost), and it is crucial to select the most suitable algorithm for multi-omics
    analysis [130]. Some supervised approaches are subjected to overfitting from inappropriate
    cross-validation policies, while certain approaches are limited by training label
    uncertainties [150]. In supervised learning, an incorrect label definition can
    lead to inaccurate prediction results. Consequently, it is essential to enlist
    biological expertise to properly define the labels [107]. On the other hand, models,
    such as the ABC model and eNet model, predict the functions of thousands of enhancers.
    However, most of these enhancers have not been experimentally validated, making
    it difficult to determine the accuracy of model prediction. Performance metrics
    used commonly for this purpose include the F1 score (Harmonic Mean of Precision
    and Recall), the area under the receiver operating characteristic curve and the
    area under the precision recall curve. But due to the diversity of the principles
    and standard definitions of prediction, it is difficult to systematically evaluate
    the performance of all available computational methods [109]. Interpretability
    of multi-omics approaches Interpretability is about the extent to which a cause
    and effect can be observed within a system [151]. Factors affect the interpretability
    of a model, including data, model architecture and algorithms [152, 153]. To improve
    the interpretability of a model integrating the multi-omics data, several approaches
    have been developed from different perspectives of the affecting factors mentioned
    above. (i) Human-labeled data can improve the interpretability of a model. For
    example, GenNet improves the interpretability of genotype data by constructing
    explainable neural networks that use prior biological knowledge to label the data
    [154]. (ii) Simplifying the network architecture can increase the interpretation
    [153]. For example, ExplaiNN uses a large series of simple neural networks, each
    of which learns different TF binding profiles. As a result, it becomes easier
    to understand the prediction results of each TF, thereby improving the overall
    interpretability [153]. The HEAP model uses the weights of the first convolutional
    layer to capture important enhancer features to build a deep network model [152].
    Adopting explainable artificial intelligence (XAI) architecture is another approach
    [155]. With this kind of architecture, researchers can have a clearer understanding
    of the degree of causal relationship between input signals and output results.
    Therefore, some teams have utilized XAI to identify enhancers based on the epigenetic
    feature signals of different histone groups, and discovered the connection between
    the enrichment of different histone modifications and the activity of enhancers
    [155]. (iii) From the algorithmic perspective, using interpretable algorithms
    (such as clustering, SHAP, etc.) can improve the interpretability of a model [152,
    153]. In summary, although there are many methods available currently to improve
    the interpretability of models, overly pursuing interpretability may lead to a
    decline in model performance [156]. The existing interpretability methods often
    target specific model architectures or data types only [29]. Therefore, new strategies
    need to be continuously developed to improve the explanation ability of models.
    Computational and storage costs Multi-omics analysis incurs costs for computation
    and data storage [157]. Most integrated algorithms require high computational
    power and considerable storage capacity to store logs, results and analyses [140].
    How to store multi-omics datasets to facilitate the reuse of existing research
    datasets is another challenge. High-performance computing infrastructure, cloud
    computing solutions and advanced statistical methods are all effective ways to
    reduce computing and storage costs. The general principle is FAIR, which stands
    for findable, accessible, interoperable and reusable [158]. However, many multi-omics
    data storage platforms (such as Figshare, Zenodo or Lifebit) do not support data
    retrieval and query [29]. Numerous computing models have been deployed on specialized
    graphics processing units and cloud computing platforms over the past few years
    (such as Microsoft Azure [159]), which is one of the ways to address the issues
    mentioned above [160, 161]. The five yellow rectangles in the figure represent
    the five major challenges encountered in enhancer research, while the blue squares
    represent different resolution strategies to address these challenges. CONCLUSION
    Enhancers are crucial regulatory elements in gene transcription, and the application
    of omics techniques speeds up the elucidation of the role and mechanisms of enhancers
    in gene regulation. In this review, we first summarized the current issues encountered
    in enhancer research. Next, we discussed the application and limitations of four
    types of omics technologies (genomics, epigenomics, transcriptomics and CRISPR
    gene editing). With the increasing availability of larger, high-quality datasets
    paralleled by the development of new omics technologies, the demand for ideas
    and methods for multi-omics analysis will continue to grow. Using machine learning
    to integrate and analyze high-dimensional and multi-omics data can effectively
    improve the accuracy of the enhancer prediction model. Furthermore, novel algorithms
    can be utilized to extract new information from existing data. However, the application
    of omics technology in the field of enhancer research is still challenging. Despite
    the widespread heterogeneity and divergent quality of multi-omics data, both data
    quality and quantity are being improved with the increasing application of sequencing
    techniques. In addition, statistical methods are continuously evolving to address
    the current challenges. In summary, we believe that with the development of omics
    technologies and statistics, multi-omics techniques will have greater value in
    enhancer research. Key Points At present, there are four basic problems in enhancer
    research: identification, activity, structure and eRNA. Genomics, epigenomics,
    transcriptomics and CRISPR-gene editing technology have been widely used in enhancer
    research. Multi-omics integration methods in enhancer research are divided into
    traditional machine learning and deep learning methods. ACKNOWLEDGEMENTS We deeply
    thank Yong Zhang and Weiwei Zhai for their comments and helpful suggestions during
    the manuscript preparation. FUNDING This work was supported by grants from the
    National Natural Science Foundation of China (32270610, 31801094 and 82072499
    to C.L.) and the Fundamental Research Funds for the Central Universities (YWF-21-BJ-J-T105
    to C.L.). ETHICS APPROVAL AND CONSENT TO PARTICIPATE Not applicable. CONSENT FOR
    PUBLICATION Not applicable. Qilin Wang is a PhD candidate at Beihang University,
    China. His research interests are bioinformatics, machine learning and data mining.
    Junyou Zhang is a PhD candidate at Beihang University, China. His research interests
    focus on cancer genomics. Zhaoshuo Liu is a master candidate at Beihang University,
    China. His research interests are data mining, computational biology and user
    modeling. Yingying Duan is a master candidate at Beihang University, China. Her
    research interests focus on cancer genomics. Chunyan Li is an associate professor
    at Beihang University, China. Her research interests are functional genomics on
    cancer and osteoporosis. References 1. Ohler   U, Wassarman   DA. Promoting developmental
    transcription. Development  2010;137:15–26. Google Scholar CrossrefPubMed WorldCat   2.
    Peng   Y, Zhang   Y. Enhancer and super-enhancer: positive regulators in gene
    transcription. Animal Model Exp Med  2018;1:169–79. Google Scholar CrossrefPubMed
    WorldCat   3. Thomas   HF, Buecker   C. What is an enhancer?  Bioessays  2023;45:e2300044.
    Google Scholar CrossrefPubMed WorldCat   4. Berman   BP, Pfeiffer   BD, Laverty   TR,
    et al.   Computational identification of developmental enhancers: conservation
    and function of transcription factor binding-site clusters in Drosophila melanogaster
    and Drosophila pseudoobscura. Genome Biol  2004;5:R61. Google Scholar CrossrefPubMed
    WorldCat   5. Ye   R, Cao   C, Xue   Y. Enhancer RNA: biogenesis, function, and
    regulation. Essays Biochem  2020;64:883–94. Google Scholar PubMed WorldCat   6.
    Agrawal   P, Rao   S. Super-enhancers and CTCF in early embryonic cell fate decisions.
    Front Cell Dev Biol  2021;9:653669. Google Scholar CrossrefPubMed WorldCat   7.
    Corradin   O, Scacheri   PC. Enhancer variants: evaluating functions in common
    disease. Genome Med  2014;6:85. Google Scholar CrossrefPubMed WorldCat   8. Liu   F,
    Li   H, Ren   C, et al.   PEDLA: predicting enhancers with a deep learning-based
    algorithmic framework. Sci Rep  2016;6:28517. Google Scholar CrossrefPubMed WorldCat   9.
    Yang   B, Liu   F, Ren   C, et al.   BiRen: predicting enhancers with a deep-learning-based
    model using the DNA sequence alone. Bioinformatics  2017;33:1930–6. Google Scholar
    CrossrefPubMed WorldCat   10. Bosse   Y, Amos   CI. A decade of GWAS results in
    lung cancer. Cancer Epidemiol Biomarkers Prev  2018;27:363–79. Google Scholar
    CrossrefPubMed WorldCat   11. Wainberg   M, Sinnott-Armstrong   N, Mancuso   N,
    et al.   Opportunities and challenges for transcriptome-wide association studies.
    Nat Genet  2019;51:592–9. Google Scholar CrossrefPubMed WorldCat   12. Cano-Gamez   E,
    Trynka   G. From GWAS to function: using functional genomics to identify the mechanisms
    underlying complex diseases. Front Genet  2020;11:424. Google Scholar CrossrefPubMed
    WorldCat   13. Dozmorov   MG, Tyc   KM, Sheffield   NC, et al.   Chromatin conformation
    capture (Hi-C) sequencing of patient-derived xenografts: analysis guidelines.
    Gigascience  2021;10:10. Google Scholar Crossref WorldCat   14. Nakato   R, Sakata   T.
    Methods for ChIP-seq analysis: a practical workflow and advanced applications.
    Methods  2021;187:44–53. Google Scholar CrossrefPubMed WorldCat   15. Song   L,
    Crawford   GE. DNase-seq: a high-resolution technique for mapping active gene
    regulatory elements across the genome from mammalian cells. Cold Spring Harb Protoc  2010;
    2010(2):pdb prot5384. Google Scholar WorldCat   16. Ocampo   J, Cui   F, Zhurkin   VB,
    Clark   DJ. The proto-chromatosome: a fundamental subunit of chromatin?  Nucleus  2016;7:382–7.
    Google Scholar CrossrefPubMed WorldCat   17. van  Dijk   EL, Jaszczyszyn   Y,
    Naquin   D, Thermes   C. The third revolution in sequencing technology. Trends
    Genet  2018;34:666–81. Google Scholar CrossrefPubMed WorldCat   18. Orgaz   JL,
    Crosas-Molist   E, Sadok   A, et al.   Myosin II reactivation and cytoskeletal
    remodeling as a hallmark and a vulnerability in melanoma therapy resistance. Cancer
    Cell  2020;37:85–103.e9. Google Scholar CrossrefPubMed WorldCat   19. Conesa   A,
    Madrigal   P, Tarazona   S, et al.   A survey of best practices for RNA-seq data
    analysis. Genome Biol  2016;17:13. Google Scholar CrossrefPubMed WorldCat   20.
    Neumayr   C, Pagani   M, Stark   A, et al.   STARR-seq and UMI-STARR-seq: assessing
    enhancer activities for genome-wide-, high-, and low-complexity candidate libraries.
    Curr Protoc Mol Biol  2019;128:e105. Google Scholar CrossrefPubMed WorldCat   21.
    Tian   W, Huang   X, Ouyang   X. Genome-wide prediction of activating regulatory
    elements in rice by combining STARR-seq with FACS. Plant Biotechnol J  2022;20:2284–97.
    Google Scholar CrossrefPubMed WorldCat   22. Lu   T, Yang   B, Wang   R, et al.   Xenotransplantation:
    current status in preclinical research. Front Immunol  2019;10:3060. Google Scholar
    CrossrefPubMed WorldCat   23. Subramanian   I, Verma   S, Kumar   S, et al.   Multi-omics
    data integration, interpretation, and its application. Bioinform Biol Insights  2020;14:1177932219899051.
    Google Scholar CrossrefPubMed WorldCat   24. Kleftogiannis   D, Kalnis   P, Bajic   VB.
    Progress and challenges in bioinformatics approaches for enhancer identification.
    Brief Bioinform  2016;17:967–79. Google Scholar CrossrefPubMed WorldCat   25.
    Fishilevich   S, Nudel   R, Rappaport   N, et al.   GeneHancer: genome-wide integration
    of enhancers and target genes in GeneCards. Database (Oxford)  2017;2017:bax028.
    Google Scholar WorldCat   26. Tsai   A, Alves   MR, Crocker   J. Multi-enhancer
    transcriptional hubs confer phenotypic robustness. Elife  2019;8:e45325. Google
    Scholar CrossrefPubMed WorldCat   27. Ribeiro   DM, Rubinacci   S, Ramisch   A,
    et al.   The molecular basis, genetic control and pleiotropic effects of local
    gene co-expression. Nat Commun  2021;12:4842. Google Scholar CrossrefPubMed WorldCat   28.
    Wörheide   MA, Krumsiek   J, Kastenmüller   G, et al.   Multi-omics integration
    in biomedical research – a metabolomics-centric review. Anal Chim Acta  2021;1141:144–62.
    Google Scholar CrossrefPubMed WorldCat   29. Tarazona   S, Arzalluz-Luque   A,
    Conesa   A. Undisclosed, unmet and neglected challenges in multi-omics studies.
    Nat Comput Sci  2021;1:395–402. Google Scholar Crossref WorldCat   30. Investigators   GPP,
    Smedley   D, Smith   KR, et al.   100,000 genomes pilot on rare-disease diagnosis
    in health care - preliminary report. N Engl J Med  2021;385:1868–80. Google Scholar
    PubMed WorldCat   31. Nakagawa   H, Fujita   M. Whole genome sequencing analysis
    for cancer genomics and precision medicine. Cancer Sci  2018;109:513–22. Google
    Scholar CrossrefPubMed WorldCat   32. Tam   V, Patel   N, Turcotte   M, et al.   Benefits
    and limitations of genome-wide association studies. Nat Rev Genet  2019;20:467–84.
    Google Scholar CrossrefPubMed WorldCat   33. Gilad   Y, Rifkin   SA, Pritchard   JK.
    Revealing the architecture of gene regulation: the promise of eQTL studies. Trends
    Genet  2008;24:408–15. Google Scholar CrossrefPubMed WorldCat   34. Hnisz   D,
    Abraham   BJ, Lee   TI, et al.   Super-enhancers in the control of cell identity
    and disease. Cell  2013;155:934–47. Google Scholar CrossrefPubMed WorldCat   35.
    Chignon   A, Mathieu   S, Rufiange   A, et al.   Enhancer promoter interactome
    and Mendelian randomization identify network of druggable vascular genes in coronary
    artery disease. Hum Genomics  2022;16:8. Google Scholar CrossrefPubMed WorldCat   36.
    Chen   H, Li   C, Peng   X, et al.   A pan-cancer analysis of enhancer expression
    in nearly 9000 patient samples. Cell  2018;173:386–399.e312. Google Scholar CrossrefPubMed
    WorldCat   37. Mohanta   TK, Mishra   AK, Al-Harrasi   A. The 3D genome: from
    structure to function. Int J Mol Sci  2021;22:11585. Google Scholar CrossrefPubMed
    WorldCat   38. Lafontaine   DL, Yang   L, Dekker   J, et al.   Hi-C 3.0: improved
    protocol for genome-wide chromosome conformation capture. Curr Protoc  2021;1:e198.
    Google Scholar CrossrefPubMed WorldCat   39. Vardaxis   I, Drablos   F, Rye   MB,
    et al.   MACPET: model-based analysis for ChIA-PET. Biostatistics  2020;21:625–39.
    Google Scholar CrossrefPubMed WorldCat   40. Mumbach   MR, Rubin   AJ, Flynn   RA,
    et al.   HiChIP: efficient and sensitive analysis of protein-directed genome architecture.
    Nat Methods  2016;13:919–22. Google Scholar CrossrefPubMed WorldCat   41. Rosen   JD,
    Yang   Y, Abnousi   A, et al.   HPRep: quantifying reproducibility in HiChIP and
    PLAC-Seq datasets. Curr Issues Mol Biol  2021;43:1156–70. Google Scholar CrossrefPubMed
    WorldCat   42. Zhang   Y, An   L, Xu   J, et al.   Enhancing Hi-C data resolution
    with deep convolutional neural network HiCPlus. Nat Commun  2018;9:750. Google
    Scholar CrossrefPubMed WorldCat   43. Rao   SS, Huntley   MH, Durand   NC, et
    al.   A 3D map of the human genome at kilobase resolution reveals principles of
    chromatin looping. Cell  2014;159:1665–80. Google Scholar CrossrefPubMed WorldCat   44.
    Wang   KC, Chang   HY. Epigenomics: technologies and applications. Circ Res  2018;122:1191–9.
    Google Scholar CrossrefPubMed WorldCat   45. Wilson   PC, Ledru   N, Humphreys   BD.
    Epigenomics and the kidney. Curr Opin Nephrol Hypertens  2020;29:280–5. Google
    Scholar CrossrefPubMed WorldCat   46. Klemm   SL, Shipony   Z, Greenleaf   WJ.
    Chromatin accessibility and the regulatory epigenome. Nat Rev Genet  2019;20:207–20.
    Google Scholar CrossrefPubMed WorldCat   47. Song   L, Zhang   Z, Grasfeder   LL,
    et al.   Open chromatin defined by DNaseI and FAIRE identifies regulatory elements
    that shape cell-type identity. Genome Res  2011;21:1757–67. Google Scholar CrossrefPubMed
    WorldCat   48. Buenrostro   JD, Wu   B, Chang   HY, et al.   ATAC-seq: a method
    for assaying chromatin accessibility genome-wide. Curr Protoc Mol Biol  2015;109:21.29.1–29.
    Google Scholar WorldCat   49. Chen   A, Chen   D, Chen   Y. Advances of DNase-seq
    for mapping active gene regulatory elements across the genome in animals. Gene  2018;667:83–94.
    Google Scholar CrossrefPubMed WorldCat   50. Liu   Y, Fu   L, Kaufmann   K, et
    al.   A practical guide for DNase-seq data analysis: from data management to common
    applications. Brief Bioinform  2019;20:1865–77. Google Scholar CrossrefPubMed
    WorldCat   51. Kong   S, Lu   Y, Tan   S, et al.   Nucleosome-omics: a perspective
    on the epigenetic code and 3D genome landscape. Genes (Basel)  2022;13:1114. Google
    Scholar CrossrefPubMed WorldCat   52. Chereji   RV, Bryson   TD, Henikoff   S.
    Quantitative MNase-seq accurately maps nucleosome occupancy levels. Genome Biol  2019;20:198.
    Google Scholar CrossrefPubMed WorldCat   53. Seuter   S, Neme   A, Carlberg   C.
    Monitoring genome-wide chromatin accessibility by formaldehyde-assisted isolation
    of regulatory elements sequencing (FAIRE-seq). Epigenetics Methods  2020;353–69.
    Google Scholar WorldCat   54. Buenrostro   JD, Giresi   PG, Zaba   LC, et al.   Transposition
    of native chromatin for fast and sensitive epigenomic profiling of open chromatin,
    DNA-binding proteins and nucleosome position. Nat Methods  2013;10:1213–8. Google
    Scholar CrossrefPubMed WorldCat   55. Jia   G, Preussner   J, Chen   X, et al.   Single
    cell RNA-seq and ATAC-seq analysis of cardiac progenitor cell transition states
    and lineage settlement. Nat Commun  2018;9:4877. Google Scholar CrossrefPubMed
    WorldCat   56. Ji   Z, Zhou   W, Hou   W, et al.   Single-cell ATAC-seq signal
    extraction and enhancement with SCATE. Genome Biol  2020;21:161. Google Scholar
    CrossrefPubMed WorldCat   57. Chen   H, Liang   H. A high-resolution map of human
    enhancer RNA loci characterizes super-enhancer activities in cancer. Cancer Cell  2020;38:701–715.e705.
    Google Scholar CrossrefPubMed WorldCat   58. Hong   D, Lin   H, Liu   L, et al.   Complexity
    of enhancer networks predicts cell identity and disease genes revealed by single-cell
    multi-omics analysis. Brief Bioinform  2023;24(1):bbac508. Google Scholar WorldCat   59.
    Clermont   P-L, Parolia   A, Liu1   HH, et al.   DNA methylation at enhancer regions:
    novel avenues for epigenetic biomarker development. IMR Press. 2016;21(2):430–46.
    60. Li   J, Li   Y, Li   W, et al.   Guide positioning sequencing identifies aberrant
    DNA methylation patterns that alter cell identity and tumor-immune surveillance
    networks. Genome Res  2019;29:270–80. Google Scholar CrossrefPubMed WorldCat   61.
    Park   PJ. ChIP-seq: advantages and challenges of a maturing technology. Nat Rev
    Genet  2009;10:669–80. Google Scholar CrossrefPubMed WorldCat   62. Nakao   K,
    Miyaaki   H, Ichikawa   T. Antitumor function of microRNA-122 against hepatocellular
    carcinoma. J Gastroenterol  2014;49:589–93. Google Scholar CrossrefPubMed WorldCat   63.
    Cejas   P, Li   L, O''Neill   NK, et al.   Chromatin immunoprecipitation from
    fixed clinical tissues reveals tumor-specific enhancer profiles. Nat Med  2016;22:685–91.
    Google Scholar CrossrefPubMed WorldCat   64. Font-Tello   A, Kesten   N, Xie   Y,
    et al.   FiTAc-seq: fixed-tissue ChIP-seq for H3K27ac profiling and super-enhancer
    analysis of FFPE tissues. Nat Protoc  2020;15:2503–18. Google Scholar CrossrefPubMed
    WorldCat   65. Pareek   CS, Smoczynski   R, Tretyn   A. Sequencing technologies
    and genome sequencing. J Appl Genet  2011;52:413–35. Google Scholar CrossrefPubMed
    WorldCat   66. Mundade   R, Ozer   HG, Wei   H, et al.   Role of ChIP-seq in the
    discovery of transcription factor binding sites, differential gene regulation
    mechanism, epigenetic marks and beyond. Cell Cycle  2014;13:2847–52. Google Scholar
    CrossrefPubMed WorldCat   67. Kaya-Okur   HS, Wu   SJ, Codomo   CA, et al.   CUT&Tag
    for efficient epigenomic profiling of small samples and single cells. Nat Commun  2019;10:1930.
    Google Scholar CrossrefPubMed WorldCat   68. Kaya-Okur   HS, Janssens   DH, Henikoff   JG,
    et al.   Efficient low-cost chromatin profiling with CUT&Tag. Nat Protoc  2020;15:3264–83.
    Google Scholar CrossrefPubMed WorldCat   69. Li   QL, Lin   X, Yu   YL, et al.   Genome-wide
    profiling in colorectal cancer identifies PHF19 and TBC1D16 as oncogenic super
    enhancers. Nat Commun  2021;12:6407. Google Scholar CrossrefPubMed WorldCat   70.
    Cheung   K, Barter   MJ, Falk   J, et al.   Histone ChIP-Seq identifies differential
    enhancer usage during chondrogenesis as critical for defining cell-type specificity.
    FASEB J  2020;34:5317–31. Google Scholar CrossrefPubMed WorldCat   71. Zuo   S,
    Yi   Y, Wang   C, et al.   Extrachromosomal circular DNA (eccDNA): from chaos
    to function. Front Cell Dev Biol  2021;9:792555. Google Scholar CrossrefPubMed
    WorldCat   72. Zhu   Y, Gujar   AD, Wong   CH, et al.   Oncogenic extrachromosomal
    DNA functions as mobile enhancers to globally amplify chromosomal transcription.
    Cancer Cell  2021;39:694–707.e697. Google Scholar CrossrefPubMed WorldCat   73.
    Møller   HD. Circle-Seq: isolation and sequencing of chromosome-derived circular
    DNA elements in cells. Methods Mol Biol  2020;2119:165–81. Google Scholar CrossrefPubMed
    WorldCat   74. Zhou   B, Li   X, Luo   D, et al.   GRID-seq for comprehensive
    analysis of global RNA-chromatin interactions. Nat Protoc  2019;14:2036–68. Google
    Scholar CrossrefPubMed WorldCat   75. Li   X, Zhou   B, Chen   L, et al.   GRID-seq
    reveals the global RNA-chromatin interactome. Nat Biotechnol  2017;35:940–50.
    Google Scholar CrossrefPubMed WorldCat   76. Li   J, Xiang   Y, Zhang   L, et
    al.   Enhancer-promoter interaction maps provide insights into skeletal muscle-related
    traits in pig genome. BMC Biol  2022;20:136. Google Scholar CrossrefPubMed WorldCat   77.
    Cai   Z, Cao   C, Ji   L, et al.   RIC-seq for global in situ profiling of RNA-RNA
    spatial interactions. Nature  2020;582:432–7. Google Scholar CrossrefPubMed WorldCat   78.
    Kim   TK, Shiekhattar   R. Architectural and functional commonalities between
    enhancers and promoters. Cell  2015;162:948–59. Google Scholar CrossrefPubMed
    WorldCat   79. Wang   Z, Gerstein   M, Snyder   M. RNA-Seq: a revolutionary tool
    for transcriptomics. Nat Rev Genet  2009;10:57–63. Google Scholar CrossrefPubMed
    WorldCat   80. Mantione   KJ, Kream   RM, Kuzelova   H, et al.   Comparing bioinformatic
    gene expression profiling methods: microarray and RNA-Seq. Med Sci Monit Basic
    Res  2014;20:138–42. Google Scholar PubMed WorldCat   81. Jovic   D, Liang   X,
    Zeng   H, et al.   Single-cell RNA sequencing technologies and applications: a
    brief overview. Clin Transl Med  2022;12:e694. Google Scholar CrossrefPubMed WorldCat   82.
    Saliba   AE, Westermann   AJ, Gorski   SA, et al.   Single-cell RNA-seq: advances
    and future challenges. Nucleic Acids Res  2014;42:8845–60. Google Scholar CrossrefPubMed
    WorldCat   83. Moses   L, Pachter   L. Museum of spatial transcriptomics. Nat
    Methods  2022;19:534–46. Google Scholar CrossrefPubMed WorldCat   84. Muerdter   F,
    Boryn   LM, Arnold   CD. STARR-seq - principles and applications. Genomics  2015;106:145–50.
    Google Scholar CrossrefPubMed WorldCat   85. Goldstein   I, Hager   GL. Dynamic
    enhancer function in the chromatin context. Wiley Interdiscip Rev Syst Biol Med  2018;10(1):10.1002.
    Google Scholar WorldCat   86. Andersson   R, Refsing Andersen   P, Valen   E,
    et al.   Nuclear stability and transcriptional directionality separate functionally
    distinct RNA species. Nat Commun  2014;5:5336. Google Scholar CrossrefPubMed WorldCat   87.
    Lee   JH, Xiong   F, Li   W. Enhancer RNAs in cancer: regulation, mechanisms and
    therapeutic potential. RNA Biol  2020;17:1550–9. Google Scholar CrossrefPubMed
    WorldCat   88. Hah   N, Kraus   WL. Hormone-regulated transcriptomes: lessons
    learned from estrogen signaling pathways in breast cancer cells. Mol Cell Endocrinol  2014;382:652–64.
    Google Scholar CrossrefPubMed WorldCat   89. Murakawa   Y, Yoshihara   M, Kawaji   H,
    et al.   Enhanced identification of transcriptional enhancers provides mechanistic
    insights into diseases. Trends Genet  2016;32:76–88. Google Scholar CrossrefPubMed
    WorldCat   90. Consortium F, the RP, CLST, et al.   A promoter-level mammalian
    expression atlas. Nature  2014;507:462–70. CrossrefPubMed WorldCat   91. Andersson   R,
    Gebhard   C, Miguel-Escalada   I, et al.   An atlas of active enhancers across
    human cell types and tissues. Nature  2014;507:455–61. Google Scholar CrossrefPubMed
    WorldCat   92. Fulco   CP, Nasser   J, Jones   TR, et al.   Activity-by-contact
    model of enhancer-promoter regulation from thousands of CRISPR perturbations.
    Nat Genet  2019;51:1664–9. Google Scholar CrossrefPubMed WorldCat   93. Dixit   A,
    Parnas   O, Li   B, et al.   Perturb-Seq: dissecting molecular circuits with scalable
    single-cell RNA profiling of pooled genetic screens. Cell  2016;167:1853–1866.e1817.
    Google Scholar CrossrefPubMed WorldCat   94. Visel   A, Minovitsky   S, Dubchak   I,
    et al.   VISTA enhancer browser--a database of tissue-specific human enhancers.
    Nucleic Acids Res  2007;35:D88–92. Google Scholar CrossrefPubMed WorldCat   95.
    Zhang   G, Shi   J, Zhu   S, et al.   DiseaseEnhancer: a resource of human disease-associated
    enhancer catalog. Nucleic Acids Res  2017;46:D78–84. Google Scholar Crossref WorldCat   96.
    Bai   X, Shi   S, Ai   B, et al.   ENdb: a manually curated database of experimentally
    supported enhancers for human and mouse. Nucleic Acids Res  2019;48:D51–7. Google
    Scholar WorldCat   97. Kumar   R, Lathwal   A, Kumar   V, et al.   CancerEnD:
    a database of cancer associated enhancers. Genomics  2020;112:3696–702. Google
    Scholar CrossrefPubMed WorldCat   98. Luo   Z-H, Shi   M-W, Zhang   Y, et al.   CenhANCER:
    a comprehensive cancer enhancer database for primary tissues and cell lines. Database  2023;2023:baad022.
    Google Scholar WorldCat   99. Wang   J, Dai   X, Berry   LD, et al.   HACER: an
    atlas of human active enhancers to interpret regulatory variants. Nucleic Acids
    Res  2019;47:D106–12. Google Scholar CrossrefPubMed WorldCat   100. Cai   Z, Cui   Y,
    Tan   Z, et al.   RAEdb: a database of enhancers identified by high-throughput
    reporter assays. Database (Oxford)  2019;2019:bay140. Google Scholar WorldCat   101.
    Gao   T, Qian   J. EnhancerAtlas 2.0: an updated resource with enhancer annotation
    in 586 tissue/cell types across nine species. Nucleic Acids Res  2020;48:D58–64.
    Google Scholar PubMed WorldCat   102. Tang   L, Hill   MC, Wang   J, et al.   Predicting
    unrecognized enhancer-mediated genome topology by an ensemble machine learning
    model. Genome Res  2020;30:1835–45. Google Scholar CrossrefPubMed WorldCat   103.
    Cai   Z, Poulos   RC, Liu   J, et al.   Machine learning for multi-omics data
    integration in cancer. iScience  2022;25:103798. Google Scholar CrossrefPubMed
    WorldCat   104. Chen   Z, Zhang   J, Liu   J, et al.   DECODE: a deep-learning
    framework for condensing enhancers and refining boundaries with large-scale functional
    assays. Bioinformatics  2021;37:i280–8. Google Scholar CrossrefPubMed WorldCat   105.
    Miotto   R, Wang   F, Wang   S, et al.   Deep learning for healthcare: review,
    opportunities and challenges. Brief Bioinform  2018;19:1236–46. Google Scholar
    CrossrefPubMed WorldCat   106. Ahmad   F, Mahmood   A, Muhmood   T. Machine learning-integrated
    omics for the risk and safety assessment of nanomaterials. Biomater Sci  2021;9:1598–608.
    Google Scholar CrossrefPubMed WorldCat   107. Correa-Aguila   R, Alonso-Pupo   N,
    Hernández-Rodríguez   EW. Multi-omics data integration approaches for precision
    oncology. Mol Omics  2022;18:469–79. Google Scholar CrossrefPubMed WorldCat   108.
    Xu   H, Zhang   S, Yi   X, et al.   Exploring 3D chromatin contacts in gene regulation:
    the evolution of approaches for the identification of functional enhancer-promoter
    interaction. Comput Struct Biotechnol J  2020;18:558–70. Google Scholar CrossrefPubMed
    WorldCat   109. Tao   H, Li   H, Xu   K, et al.   Computational methods for the
    prediction of chromatin interaction and organization using sequence and epigenomic
    profiles. Brief Bioinform  2021;22(5):bbaa405. Google Scholar WorldCat   110.
    Popay   TM, Dixon   JR. Coming full circle: on the origin and evolution of the
    looping model for enhancer-promoter communication. J Biol Chem  2022;298:102117.
    Google Scholar CrossrefPubMed WorldCat   111. Malin   J, Aniba   MR, Hannenhalli   S.
    Enhancer networks revealed by correlated DNAse hypersensitivity states of enhancers.
    Nucleic Acids Res  2013;41:6828–38. Google Scholar CrossrefPubMed WorldCat   112.
    Whalen   S, Truty   RM, Pollard   KS. Enhancer-promoter interactions are encoded
    by complex genomic signatures on looping chromatin. Nat Genet  2016;48:488–96.
    Google Scholar CrossrefPubMed WorldCat   113. Ernst   J, Kheradpour   P, Mikkelsen   TS,
    et al.   Mapping and analysis of chromatin state dynamics in nine human cell types.
    Nature  2011;473:43–9. Google Scholar CrossrefPubMed WorldCat   114. Corces   MR,
    Granja   JM, Shams   S, et al.   The chromatin accessibility landscape of primary
    human cancers. Science  2018;362:eaav1898. Google Scholar CrossrefPubMed WorldCat   115.
    Yao   L, Shen   H, Laird   PW, et al.   Inferring regulatory element landscapes
    and transcription factor networks from cancer methylomes. Genome Biol  2015;16:105.
    Google Scholar CrossrefPubMed WorldCat   116. O''Connor   T, Bodén   M, Bailey   TL.
    CisMapper: predicting regulatory interactions from transcription factor ChIP-seq
    data. Nucleic Acids Res  2017;45:e19. Google Scholar PubMed WorldCat   117. Huska   MR,
    Ramisch   A, Vingron   M, et al.   Predicting enhancers using a small subset of
    high confidence examples and co-training. PeerJ Preprints 2016;4:e2407v1. 118.
    Greene   CS, Tan   J, Ung   M, et al.   Big data bioinformatics. J Cell Physiol  2014;229:1896–900.
    Google Scholar CrossrefPubMed WorldCat   119. Chen   X, Zhou   J, Zhang   R, et
    al.   Tissue-specific enhancer functional networks for associating distal regulatory
    regions to disease. Cell Systems  2021;12:353–362.e356. Google Scholar CrossrefPubMed
    WorldCat   120. Hait   TA, Amar   D, Shamir   R, et al.   FOCS: a novel method
    for analyzing enhancer and gene activity patterns infers an extensive enhancer-promoter
    map. Genome Biol  2018;19:56. Google Scholar CrossrefPubMed WorldCat   121. Cao   Q,
    Anyansi   C, Hu   X, et al.   Reconstruction of enhancer-target networks in 935
    samples of human primary cells, tissues and cell lines. Nat Genet  2017;49:1428–36.
    Google Scholar CrossrefPubMed WorldCat   122. Hafez   D, Karabacak   A, Krueger   S,
    et al.   McEnhancer: predicting gene expression via semi-supervised assignment
    of enhancers to target genes. Genome Biol  2017;18:199. Google Scholar CrossrefPubMed
    WorldCat   123. Mehdi   TF, Singh   G, Mitchell   JA, et al.   Variational infinite
    heterogeneous mixture model for semi-supervised clustering of heart enhancers.
    Bioinformatics  2019;35:3232–9. Google Scholar CrossrefPubMed WorldCat   124.
    He   B, Chen   C, Teng   L, et al.   Global view of enhancer-promoter interactome
    in human cells. Proc Natl Acad Sci U S A  2014;111:E2191–9. Google Scholar PubMed
    WorldCat   125. Zhao   C, Li   X, Hu   H. PETModule: a motif module based approach
    for enhancer target gene prediction. Sci Rep  2016;6:30043. Google Scholar CrossrefPubMed
    WorldCat   126. Roy   S, Siahpirani   AF, Chasman   D, et al.   A predictive modeling
    approach for cell line-specific long-range regulatory interactions. Nucleic Acids
    Res  2015;43:8694–712. Google Scholar CrossrefPubMed WorldCat   127. Gao   T,
    Qian   J. EAGLE: an algorithm that utilizes a small number of genomic features
    to predict tissue/cell type-specific enhancer-gene interactions. PLoS Comput Biol  2019;15:e1007436.
    Google Scholar CrossrefPubMed WorldCat   128. Talukder   A, Saadat   S, Li   X,
    et al.   EPIP: a novel approach for condition-specific enhancer-promoter interaction
    prediction. Bioinformatics  2019;35:3877–83. Google Scholar CrossrefPubMed WorldCat   129.
    Kim   SG, Harwani   M, Grama   A, et al.   EP-DNN: a deep neural network-based
    global enhancer prediction algorithm. Sci Rep  2016;6:38433. Google Scholar CrossrefPubMed
    WorldCat   130. Hong   Z, Zeng   X, Wei   L, et al.   Identifying enhancer-promoter
    interactions with neural network based on pre-trained DNA vectors and attention
    mechanism. Bioinformatics  2020;36:1037–43. Google Scholar CrossrefPubMed WorldCat   131.
    Ji   Y, Zhou   Z, Liu   H, et al.   DNABERT: pre-trained bidirectional encoder
    representations from transformers model for DNA-language in genome. Bioinformatics  2021;37:2112–20.
    Google Scholar CrossrefPubMed WorldCat   132. Yang   R, Wu   F, Zhang   C, et
    al.   iEnhancer-GAN: a deep learning framework in combination with word embedding
    and sequence generative adversarial net to identify enhancers and their strength.
    Int J Mol Sci  2021;22(7):3589. Google Scholar WorldCat   133. Bigness   J, Loinaz   X,
    Patel   S, et al.   Integrating long-range regulatory interactions to predict
    gene expression using graph convolutional networks. J Comput Biol  2022;29:409–24.
    Google Scholar CrossrefPubMed WorldCat   134. Zhao   M, Ma   L, Jia   X, et al.   GraphReg:
    dynamical point cloud registration with geometry-aware graph signal processing.
    IEEE Trans Image Process  2022;31:7449–64. Google Scholar CrossrefPubMed WorldCat   135.
    Li   W, Wong   WH, Jiang   R. DeepTACT: predicting 3D chromatin contacts via bootstrapping
    deep learning. Nucleic Acids Res  2019;47:e60. Google Scholar CrossrefPubMed WorldCat   136.
    Xiao   S, Lin   H, Wang   C, et al.   Graph neural networks with multiple prior
    knowledge for multi-omics data analysis. IEEE J Biomed Health Inform  2023;27:4591–600.
    Google Scholar CrossrefPubMed WorldCat   137. Zhang   T-H, Flores   M, Huang   Y.
    ES-ARCNN: predicting enhancer strength by using data augmentation and residual
    convolutional neural network. Anal Biochem  2021;618:114120. Google Scholar CrossrefPubMed
    WorldCat   138. Avsec   Ž, Agarwal   V, Visentin   D, et al.   Effective gene
    expression prediction from sequence by integrating long-range interactions. Nat
    Methods  2021;18:1196–203. Google Scholar CrossrefPubMed WorldCat   139. Bersanelli   M,
    Mosca   E, Remondini   D, et al.   Methods for the integration of multi-omics
    data: mathematical aspects. BMC Bioinformatics  2016;17(Suppl 2):15. Google Scholar
    PubMed WorldCat   140. Reel   PS, Reel   S, Pearson   E, et al.   Using machine
    learning approaches for multi-omics data analysis: a review. Biotechnol Adv  2021;49:107739.
    Google Scholar CrossrefPubMed WorldCat   141. Tarazona   S, Balzano-Nogueira   L,
    Gómez-Cabrero   D, et al.   Harmonization of quality metrics and power calculation
    in multi-omic studies. Nat Commun  2020;11:3092. Google Scholar CrossrefPubMed
    WorldCat   142. Jeni   LA, Cohn   JF, Torre   FDL. Facing imbalanced data--recommendations
    for the use of performance metrics. In: 2013 Humaine Association Conference on
    Affective Computing and Intelligent Interaction. 2013, p. 245–51. 143. Siebert   U,
    Rochau   U, Claxton   K. When is enough evidence enough? - Using systematic decision
    analysis and value-of-information analysis to determine the need for further evidence.
    Z Evid Fortbild Qual Gesundhwes  2013;107:575–84. Google Scholar CrossrefPubMed
    WorldCat   144. Chen   L, Liu   P, Evans   TC, Jr, et al.   DNA damage is a pervasive
    cause of sequencing errors, directly confounding variant identification. Science  2017;355:752–6.
    Google Scholar CrossrefPubMed WorldCat   145. Argelaguet   R, Velten   B, Arnol   D,
    et al.   Multi-omics factor analysis—a framework for unsupervised integration
    of multi-omics data sets. Mol Syst Biol  2018;14:e8124. Google Scholar CrossrefPubMed
    WorldCat   146. Ugidos   M, Tarazona   S, Prats-Montalbán   JM, et al.   MultiBaC:
    a strategy to remove batch effects between different omic data types. Stat Methods
    Med Res  2020;29:2851–64. Google Scholar CrossrefPubMed WorldCat   147. Voillet   V,
    Besse   P, Liaubet   L, et al.   Handling missing rows in multi-omics data integration:
    multiple imputation in multiple factor analysis framework. BMC Bioinformatics  2016;17:402.
    Google Scholar CrossrefPubMed WorldCat   148. Conesa   A, Beck   S. Making multi-omics
    data accessible to researchers. Sci Data  2019;6:251. Google Scholar CrossrefPubMed
    WorldCat   149. Liew   AW-C, Law   N-F, Yan   H. Missing value imputation for
    gene expression data: computational techniques to recover missing data from available
    information. Brief Bioinform  2010;12:498–513. Google Scholar CrossrefPubMed WorldCat   150.
    McCabe   SD, Lin   DY, Love   MI. Consistency and overfitting of multi-omics methods
    on experimental data. Brief Bioinform  2020;21:1277–84. Google Scholar CrossrefPubMed
    WorldCat   151. Lipton   ZC. The mythos of model interpretability: in machine
    learning, the concept of interpretability is both important and slippery. Queue  2018;16:31–57.
    Google Scholar Crossref WorldCat   152. Liu   Y, Wang   Z, Yuan   H, et al.   HEAP:
    a task adaptive-based explainable deep learning framework for enhancer activity
    prediction. Brief Bioinform  2023;24(5):bbad286. Google Scholar WorldCat   153.
    Smith   GD, Ching   WH, Cornejo-Páramo   P, et al.   Decoding enhancer complexity
    with machine learning and high-throughput discovery. Genome Biol  2023;24:116.
    Google Scholar CrossrefPubMed WorldCat   154. van  Hilten   A, Kushner   SA, Kayser   M,
    et al.   GenNet framework: interpretable deep learning for predicting phenotypes
    from genetic data. Commun Biol  2021;4:1094. Google Scholar CrossrefPubMed WorldCat   155.
    Wolfe   JC, Mikheeva   LA, Hagras   H, et al.   An explainable artificial intelligence
    approach for decoding the enhancer histone modifications code and identification
    of novel enhancers in Drosophila. Genome Biol  2021;22:308. Google Scholar CrossrefPubMed
    WorldCat   156. McDermid   JA, Jia   Y, Porter   Z, et al.   Artificial intelligence
    explainability: the technical and ethical dimensions. Philos Trans A Math Phys
    Eng Sci  2021;379:20200363. Google Scholar PubMed WorldCat   157. Herrmann   M,
    Probst   P, Hornung   R, et al.   Large-scale benchmark study of survival prediction
    methods using multi-omics data. Brief Bioinform  2021;22:bbaa167. Google Scholar
    CrossrefPubMed WorldCat   158. Caspi   R, Billington   R, Fulcher   CA, et al.   The
    MetaCyc database of metabolic pathways and enzymes. Nucleic Acids Res  2018;46:D633–9.
    Google Scholar CrossrefPubMed WorldCat   159. Copeland   M, Soh   J, Puca   A,
    et al.   Microsoft Azure and cloud computing. In: Copeland   M, Soh   J, Puca   A  et
    al.  (eds). Microsoft Azure: Planning, Deploying, and Managing Your Data Center
    in the Cloud. Berkeley, CA: Apress, 2015, 3–26. Google Scholar Crossref Google
    Preview WorldCat COPAC  160. Schmidhuber   J. Deep learning in neural networks:
    an overview. Neural Netw  2015;61:85–117. Google Scholar CrossrefPubMed WorldCat   161.
    Armbrust   M, Fox   A, Griffith   R, et al.   A view of cloud computing. Commun
    ACM  2010;53:50–8. Google Scholar Crossref WorldCat   © The Author(s) 2023. Published
    by Oxford University Press. This is an Open Access article distributed under the
    terms of the Creative Commons Attribution Non-Commercial License (https://creativecommons.org/licenses/by-nc/4.0/),
    which permits non-commercial re-use, distribution, and reproduction in any medium,
    provided the original work is properly cited. For commercial re-use, please contact
    journals.permissions@oup.com Advertisement CITATIONS 0 VIEWS 1,257 ALTMETRIC More
    metrics information Email alerts Article activity alert Advance article alerts
    New issue alert In progress issue alert Receive exclusive offers and updates from
    Oxford Academic Recommended Advances in bulk and single-cell multi-omics approaches
    for systems biology and precision medicine Yunjin Li et al., Briefings in Bioinformatics,
    2021 From single- to multi-omics: future research trends in medicinal plants Lifang
    Yang et al., Briefings in Bioinformatics, 2023 Bibliometric review of ATAC-Seq
    and its application in gene expression Liheng Luo et al., Briefings in Bioinformatics,
    2022 Applications of integrative OMICs approaches to gene regulation studies Jing
    Qin et al., Quantitative Biology, 2016 Single-Cell Genomics Carmela Paolillo et
    al., Clinical Chemistry, 2019 Informing disease modelling with brain-relevant
    functional genomic annotations Regina H Reynolds et al., Brain, 2019 Powered by
    Citing articles via Google Scholar Latest Most Read Most Cited Integrated modeling
    of protein and RNA ConvNeXt-MHC: improving MHC–peptide affinity prediction by
    structure-derived degenerate coding and the ConvNeXt model Multilevel superposition
    for deciphering the conformational variability of protein ensembles PCAO2: an
    ontology for integration of prostate cancer associated genotypic, phenotypic and
    lifestyle data Graphormer supervised de novo protein design method and function
    validation More from Oxford Academic Bioinformatics and Computational Biology
    Biological Sciences Science and Mathematics Books Journals About Briefings in
    Bioinformatics Editorial Board Author Guidelines Facebook Twitter Purchase Recommend
    to your Library Advertising and Corporate Services Journals Career Network Online
    ISSN 1477-4054 Copyright © 2024 Oxford University Press About Oxford Academic
    Publish journals with us University press partners What we publish New features  Authoring
    Open access Purchasing Institutional account management Rights and permissions
    Get help with access Accessibility Contact us Advertising Media enquiries Oxford
    University Press News Oxford Languages University of Oxford Oxford University
    Press is a department of the University of Oxford. It furthers the University''s
    objective of excellence in research, scholarship, and education by publishing
    worldwide Copyright © 2024 Oxford University Press Cookie settings Cookie policy
    Privacy policy Legal notice Oxford University Press uses cookies to enhance your
    experience on our website. By selecting ‘accept all’ you are agreeing to our use
    of cookies. You can change your cookie settings at any time. More information
    can be found in our Cookie Policy. Cookie settings Accept all"'
  inline_citation: '>'
  journal: Briefings in Bioinformatics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Integrative approaches based on genomic techniques in the functional studies
    on enhancers
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Murthy D.H.R.
  - Mohan K.G.
  - Augustine J.
  - Patra G.K.
  citation_count: '0'
  description: IoT is mainly used to forward the data to blockchain enabled networks
    to avert the tampering of sensitive data. This enhances the reliability and scalability
    of the IoT based services. Meanwhile, the advancement of technologies might affect
    the blockcahin system and hence the transaction rate per second and the scalability
    got reduced. In concern with this, we propose a novel Shard technology along with
    PBFT Blockchain. This enhances the throughput along with mitigated latency. In
    addition to this we have developed decentralized student database using the IPFS
    and estimated the transaction time and compared those results with the deployment
    of Sharding technique and PBFT technique. The Black Hole Optimization (BHO) algorithm
    improves the throughput per second and thus improvize the scalability and also
    reduces the tradeoff between the scalability and delay. Simulation outcomes outlined
    the system that deployed with the sharding and PBFT techniques improvise the scalability
    and reliability of the system.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: International Journal of Intelligent Systems and Applications in Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An Approach to Improvise Blockchain Scalability Using Sharding and PBFT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kumar P.
  - Paul R.K.
  - Roy H.S.
  - Yeasin M.
  - 'Ajit '
  - Paul A.K.
  citation_count: '0'
  description: Advancements in high-throughput technologies, genomics, transcriptomics,
    and metabolomics play an important role in obtaining biological information about
    living organisms. The field of computational biology and bioinformatics has experienced
    significant growth with the advent of high-throughput sequencing technologies
    and other high-throughput techniques. The resulting large amounts of data present
    both opportunities and challenges for data analysis. Big data analysis has become
    essential for extracting meaningful insights from the massive amount of data.
    In this chapter, we provide an overview of the current status of big data analysis
    in computational biology and bioinformatics. We discuss the various aspects of
    big data analysis, including data acquisition, storage, processing, and analysis.
    We also highlight some of the challenges and opportunities of big data analysis
    in this area of research. Despite the challenges, big data analysis presents significant
    opportunities like development of efficient and fast computing algorithms for
    advancing our understanding of biological processes, identifying novel biomarkers
    for breeding research and developments, predicting disease, and identifying potential
    drug targets for drug development programs.
  doi: 10.1007/978-1-0716-3461-5_11
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Reverse Engineering of Regulatory
    Networks pp 181–197Cite as Home Reverse Engineering of Regulatory Networks Protocol
    Big Data Analysis in Computational Biology and Bioinformatics Prakash Kumar, Ranjit
    Kumar Paul, Himadri Shekhar Roy, Md. Yeasin, Ajit & Amrit Kumar Paul  Protocol
    First Online: 07 October 2023 521 Accesses Part of the book series: Methods in
    Molecular Biology ((MIMB,volume 2719)) Abstract Advancements in high-throughput
    technologies, genomics, transcriptomics, and metabolomics play an important role
    in obtaining biological information about living organisms. The field of computational
    biology and bioinformatics has experienced significant growth with the advent
    of high-throughput sequencing technologies and other high-throughput techniques.
    The resulting large amounts of data present both opportunities and challenges
    for data analysis. Big data analysis has become essential for extracting meaningful
    insights from the massive amount of data. In this chapter, we provide an overview
    of the current status of big data analysis in computational biology and bioinformatics.
    We discuss the various aspects of big data analysis, including data acquisition,
    storage, processing, and analysis. We also highlight some of the challenges and
    opportunities of big data analysis in this area of research. Despite the challenges,
    big data analysis presents significant opportunities like development of efficient
    and fast computing algorithms for advancing our understanding of biological processes,
    identifying novel biomarkers for breeding research and developments, predicting
    disease, and identifying potential drug targets for drug development programs.
    Key words Computational biology Bash shell script Hadoop Python R language Big
    data Machine learning Access provided by University of Nebraska-Lincoln. Download
    protocol PDF Springer Nature is developing a new tool to find and evaluate Protocols.
    Learn more 1 Introduction The introduction of high-throughput technologies has
    accelerated biological study at the micro level. With the rapid growth of data
    generation in biology and genomics, the use of big data analysis techniques has
    become essential for extracting meaningful insights from the massive amount of
    data. In computational biology and bioinformatics, big data analysis is widely
    used for the analysis of genomics, transcriptomics, proteomics, and other high-throughput
    data. This chapter aims to provide an overview of the current state of big data
    analysis in computational biology and bioinformatics. It will cover various aspects
    of big data analysis, including data acquisition, storage, processing, and analysis.
    The chapter will also highlight some of the challenges and opportunities of big
    data analysis in this field. The overview of the role of big data used in computational
    biology and bioinformatics is described in Fig. 1. Fig. 1 Overview of role of
    big data used in computational biology and bioinformatics Full size image 1.1
    Data Acquisition and Storage Large volumes of data from genome, transcriptome,
    or proteome sequencing have been employed in bioinformatics. Data acquisition
    and storage are critical components of big data analysis in computational biology
    and bioinformatics. The amount of genetic data has grown rapidly since the advent
    of high-throughput sequencing methods. Similarly, the advent of high-throughput
    proteomics and transcriptomics methods has resulted in an exponential growth in
    data generation. To deal with this vast volume of data, different databases for
    storing and exchanging biological data have been established. The most popular
    databases include the National Center for Biotechnology Information (NCBI), the
    European Bioinformatics Institute (EBI), and the DNA Data Bank of Japan (DDBJ).
    Despite these databases, there are others that are important, such as Uniprot
    for protein sequence and functional information: the world’s most comprehensive
    and openly available database, Ensembl for vertebrate genome sequence variation
    and transcriptional regulatory studies, The Encyclopedia of DNA Elements (ENCODE)
    for functional elements in the human genome, RefSeq, Online Mendelian Inheritance
    in Man (OMIM): hereditary disorders, and others. 1.2 Data Processing and Analysis
    The processing and analysis of big data in computational biology and bioinformatics
    involve several steps, including quality control, preprocessing, assembly, mapping,
    alignment, and statistical analysis. The following are some of the most commonly
    used tools and techniques for big data analysis in this field. Quality Control:
    Quality control is essential for ensuring that the data is reliable and accurate.
    Tools such as FastQC and Trimmomatic are used for quality control and data filtering.
    Preprocessing: Preprocessing involves preparing the data for downstream analysis.
    This includes data normalization, feature selection, and dimensionality reduction.
    Tools such as DESeq2 and EdgeR are commonly used for preprocessing RNA-seq data.
    Alignment: Alignment is the process of mapping reads to a reference genome or
    transcriptome. This is a crucial step in the analysis of high-throughput sequencing
    data. Popular alignment tools include Bowtie, HISAT2, and STAR. Statistical Analysis:
    Statistical analysis is used to identify differentially expressed genes, mutations,
    or other genomic variations. Popular statistical analysis tools include DESeq2,
    Limma, and edgeR. 1.3 Tools to Handling Big Data Analysis in Computational Biology
    and Bioinformatics There are several tools available for handling big data analysis
    in computational biology and bioinformatics. Some of the most commonly used tools
    are: Linux shell scripts are commonly used in computational biology and bioinformatics
    to handle large data analysis tasks. Here are some of the basic steps that a typical
    shell script for Big Data analysis in computational biology and bioinformatics
    might include: Data preprocessing: The first step in any big data analysis is
    to preprocess the data to ensure that it is in a usable format. NumPy, SciPy,
    Pandas, Keras, SciKit-Learn, and others might involve tasks such as cleaning the
    data, removing duplicates, filtering out irrelevant information, and converting
    file formats. Some commonly used tools for data preprocessing in computational
    biology and bioinformatics include awk and sed. Data analysis: Once the data has
    been preprocessed, the next step is to perform the actual analysis. This might
    involve tasks such as aligning sequences, identifying genomic variants, performing
    gene expression analysis, or clustering data. There are many tools available for
    performing these tasks, including Bowtie, BWA, SAMtools, GATK, R(shiny, caret,
    dplyr, etc.), and Python (TensorFlow, NumPy, SciPy, Pandas, Keras, SciKit-Learn,
    and others). Data visualization: After the analysis is complete, it is often helpful
    to visualize the results in a meaningful way. This might involve creating plots,
    graphs, or heatmaps to help identify patterns or relationships in the data. Some
    commonly used tools for data visualization in computational biology and bioinformatics
    include R (ggplot2, RGL, Hmisc, etc.), Python (Matplotlib, Plotly, Seaborn, etc.),
    and gnuplot. Data storage: It is important to store the data in a way that is
    easily accessible and can be shared with others. This might involve storing the
    data in a database, a file system, or a cloud-based storage service. Some commonly
    used tools for data storage in computational biology and bioinformatics include
    MySQL, MongoDB, and OrientDB. 1.4 Linux Shell Scripts A typical shell script for
    Big Data analysis in computational biology and bioinformatics might include a
    series of commands that perform these steps in sequence, with each step building
    on the results of the previous step. The exact commands used will depend on the
    specific data being analyzed and the goals of the analysis, but the general process
    outlined above provides a useful framework for designing such a script. Handling
    big data in computational biology and bioinformatics often involves using specialized
    software tools and libraries that are designed to handle large datasets efficiently.
    Various roles of Ubuntu shell script used in big data bioinformatics and computational
    biology are described in Fig. 2. Fig. 2 Overview of the role of Ubuntu shell script
    used in big data bioinformatics and computational biology Full size image Here’s
    an example Ubuntu shell script that uses some tools to perform big data analysis
    in bioinformatics. The grep command used with various options to searching some
    terms present in the big data file like -i: Perform a case-insensitive search,
    -r: Search for the term recursively in all files under a directory, -n: Display
    the line numbers of the matching lines, -v: Invert the search and display all
    lines that do not match the search term, -w: Match only whole words (not partial
    matches), etc. AWK is a powerful command-line tool for text processing and data
    manipulation, for example, if you want to print the first column of data in the
    specified file then use “awk ''{ print $1 }'' input_file_name”. The basic outline
    for a shell script that could be used for big data analysis in bioinformatics
    using Ubuntu is given below. First, Install necessary tools: Begin by installing
    the necessary tools and software for bioinformatics analysis. These may include
    packages like BLAST, Bowtie, BWA, or other bioinformatics tools depending on your
    specific needs. Second, Download and preprocess data: Download the relevant bioinformatics
    data, which could be gene expression data, genomic sequencing data, or other types
    of data depending on the analysis. Preprocessing steps may include data cleaning,
    filtering, and normalization to ensure the data is in a usable format. Third,
    Data analysis: Depending on the research question, a range of analysis tools can
    be used. For example, clustering algorithms like k-means or hierarchical clustering
    could be used to group genes based on similarity in expression patterns. Differential
    gene expression analysis could be performed to identify genes that are differentially
    expressed in different conditions. Other analyses may include gene ontology enrichment
    analysis or pathway analysis to understand the biological processes underlying
    the data. Fourth, Visualize results: Once the analysis is complete, visualize
    the results to help better understand the data. There are several tools available
    for this purpose, including R, Python, or specialized bioinformatics visualization
    tools like Cytoscape. Finally, save the results in a format that is easy to share
    and use in future analyses. This could include creating a report or a publication-ready
    figure. # Install necessary tools code “sudo apt-get install bowtie” for installation
    any tools/modules # Download and preprocess data from RefSeq Proteins data of
    Human wget https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_protein.faa.gz
    tar -zxvf GRCh38_latest_protein.faa.gz #A gene expression data file typically
    contains information about the expression levels of different genes in various
    biological samples. The file might contain the following columns: SampleID: A
    unique identifier for each sample, GeneSymbol: The official symbol for each gene,
    GeneID: The official identifier for each gene, ExpressionValue: The expression
    value for each gene in each sample. Example is given in Table 1. Table 1 Gene
    Expression Data table save as gene_expression_data.csv Full size table It then
    performs two types of analysis on the data. The first analysis calculates the
    mean and variance of the gene expression levels for each gene, as well as identifies
    which genes have significantly different expression levels between two groups
    of samples. It uses the tail, awk, cut, and system commands to extract and analyze
    the relevant data. In this example, we have two reads, each represented by four
    lines. The first read has an identifier of ‘@SEQ_ID_1’ (the first line starts
    with the ‘@’ character, followed by a unique identifier for the read), followed
    by the second line contains the actual nucleotide sequence of the read, the third
    line starts with the ‘+’ character and may contain optional information about
    the read, and the fourth line contains the quality scores for each base in the
    read, in ASCII format. The second read has an identifier of ‘@SEQ_ID_2’, followed
    by its own sequence and quality scores (Table 2). #Command line to convert FASTQ
    file to FASTA in ubuntu cat fastq_file.fq | paste - - - - | cut -f 1,2 | sed ''s/^@/>/''
    | tr \"\\t\"\"\\n\"> fasta_file_name.fa The second analysis calculates the mean
    and maximum lengths of the DNA sequences in the FASTA file (Table 3). It uses
    the grep, awk, sort, and tail commands to extract and analyze the relevant data.
    # Perform sequence analysis: “cat sequences.fasta | head” >Seq1 CCTTTATCTAATCTTTGGAGCATGAGCTGGCATAGTTTTCTTTATAGTAATACCAATCATGATCGGTGGTTTCGGAAACTGACTAGTCCCACTCATAAT
    >Seq2 GGTAGGTACCGCCCTAAGNCTCCTAATCCGAGCAGACTTTGACCCAGCAGGAGGAGGAGACCCAGTACTATACCAGCACCTATTCTGATTCTTAAAAAA
    # Average sequence length grep -v \">\" sequences.fasta | awk ''{sum+=length($0)}
    END {print sum/NR}'' 99 # Maximum sequence length grep -v \">\" sequences.fasta|
    awk ''{print length($0)}'' | sort -n | tail -1 Table 2 Example of fastq file save
    as sequences.fasta Full size table This code assumes that the DNA sequences are
    stored in a FASTA file called sequences.fasta. Table 3 Example of fasta file save
    as sequences.fasta Full size table 1.5 Hadoop Apache Hadoop is an open-source
    platform for storing and processing big datasets ranging in size from gigabytes
    to petabytes. Hadoop Distributed File System (HDFS) is a framework that allows
    for the distributed processing of large datasets across clusters of computers.
    It can be used to store, manage, and analyze big data in bioinformatics. Instead
    of storing and processing data on a single huge computer, Hadoop enables for the
    clustering of numerous computers to analyze massive datasets in parallel more
    quickly. 1.5.1 Hadoop Modules (a) MapReduce: A set of rules that enables programs
    to do parallel computing. The map task changes input datasets into key value pairs.
    Reduce tasks consume the map task’s output to aggregate it and produce the required
    result. (b) HDFS: It outperforms traditional file systems in terms of data performance,
    as well as fault tolerance and native support for huge datasets. (c) YARN (Yet
    Another Resource Negotiator): Its role is to manage and oversee cluster nodes,
    ensuring efficient utilization of resources, and effectively issuing commands
    to handle jobs and tasks. (d) Hadoop Common: Provides a set of shared Java libraries
    that may be utilized by various modules. 1.5.2 Hadoop’s Working Mechanism Data
    is collected in various forms and placed in the Hadoop cluster using an API and
    connect to the NameNode. The NameNode keeps track of the file directory structure
    and makes “chunks” for each file, which is duplicated among DataNodes. To query
    the data, supply a MapReduce job composed of several maps and reduce processes
    that execute against the data in HDFS distributed among the DataNodes. Map tasks
    are done on each node against the specified input files, and reducers are run
    to aggregate and organize the final output. The Hadoop ecosystem has evolved significantly
    over the years. Today’s Hadoop ecosystem contains a wide range of tools and applications
    for collecting, storing, processing, analyzing, and managing large amounts of
    data. 1.6 R-Programming Language R is a popular open-source programming language
    used for statistical computing and graphics. It provides a wide range of packages
    for analyzing and visualizing large-scale biological datasets. R is a popular
    programming language for data analysis and visualization in computational biology
    and bioinformatics. Role of R-software used in big data bioinformatics and computational
    biology is described in Fig. 3. Fig. 3 Overview of the role of R-software used
    in big data bioinformatics and computational biology Full size image Here are
    some examples of R code that can be used for Big Data analysis in this field:
    1.6.1 Data Preprocessing (a) Reading and cleaning data: R can be used to read
    and import large datasets in various formats such as CSV, Excel, and TSV. The
    following code imports a CSV file called “input_file_name.csv” into R: # Read
    data from a CSV file data <- read.csv(\"input_file_name.csv\", header = TRUE)
    # Clean data by removing NAs and duplicates cleaned_data <- na.omit(data) cleaned_data
    <- unique(cleaned_data) (b) Filtering data: # Filter data to keep only rows where
    gene expression is above a certain threshold value filtered_data <- data[data$expression
    > 10,] 1.6.2 Data Analysis R has many built-in functions and libraries that can
    be used for data analysis. The following code calculates the mean, median, and
    standard deviation of a column in a data frame: mean_value <- mean(data$column)
    median_value <- median(data$column) sd_value <- sd(data$column) (a) Gene expression
    analysis using DESeq2 package: # Load DESeq2 package library(DESeq2) # Create
    a DESeqDataSet object dds <- DESeqDataSetFromMatrix(countData = counts, colData
    = metadata, design = ~ condition) # Run differential expression analysis dds <-
    DESeq(dds) # Get differentially expressed genes results <- results(dds) (b) Clustering
    data: # Create a distance matrix dist_mat <- dist(data, method = \"euclidean\")
    # Cluster the data using hierarchical clustering hclust_res <- hclust(dist_mat)
    # Visualize the clustering using a dendrogram plot(hclust_res) (c) Machine learning
    : R provides several libraries for machine learning such as caret and mlr. The
    following code trains a linear regression model using the caret library: library(caret)
    model <- train(column_to_predict ~ ., data = data, method = \"lm\") 1.6.3 Data
    Visualization R provides several powerful libraries such as ggplot2 for data visualization.
    The following code creates a scatterplot using ggplot2: library(ggplot2) ggplot(data,
    aes(x=column1, y=column2)) + geom_point() (a) Creating a heatmap: # Load pheatmap
    package library(pheatmap) # Create a heatmap of gene expression data pheatmap(data,
    scale = \"row\") (b) Creating a scatterplot: # Create a scatterplot of gene expression
    data plot(data$gene1, data$gene2, xlab = \"Gene 1 expression\", ylab = \"Gene
    2 expression\") 1.6.4 Data Storage # Save cleaned data to a CSV file write.csv(cleaned_data,
    \"cleaned_data.csv\") The following code loads a dataset of gene expression data
    and performs differential gene expression analysis using Bioconductor: These are
    just a few examples of the many tasks that can be performed using R for Big Data
    analysis in computational biology and bioinformatics. The specific commands used
    will depend on the data being analyzed and the goals of the analysis. 1.7 Python
    Programming Language Today is the era of Python, which is utilized by many large
    corporations such as Google for task automation, Amazon for task automation and
    machine learning, Facebook, Netflix for creating fresh content to watch, and Instagram.
    Python is a general-purpose programming language that is widely used in bioinformatics
    for handling large-scale data analysis. Python provides several libraries, such
    as Pandas, Numpy, and Scikit-learn, which are widely used for data manipulation
    and machine learning. Handling big data in computational biology and bioinformatics
    typically involves using specialized software tools and libraries that are designed
    to handle large datasets efficiently. Some commonly used tools and libraries for
    this purpose include NumPy, Pandas, BioPython, and SciPy. Importance of programming
    used in big data bioinformatics and computational biology is described in Fig.
    4. Fig. 4 Why Python programming used in big data bioinformatics and computational
    biology? Full size image Here’s an example code that uses some of these tools
    to perform big data analysis: import numpy as np import pandas as pd from Bio
    import SeqIO from scipy.stats import ttest_ind # Load data gene_expression_data
    = pd.read_csv(''gene_expression_data.csv'') fasta_sequences = SeqIO.parse(''sequences.fasta'',
    ''fasta'') # Perform gene expression analysis mean_expression = gene_expression_data.mean()
    variance_expression = gene_expression_data.var() significant_genes = gene_expression_data.loc[:,
    (ttest_ind(gene_expression_data.iloc[:, :10], gene_expression_data.iloc[:, 10:],
    equal_var=False) [1] < 0.05)] # Perform sequence analysis sequence_lengths = [len(sequence.seq)
    for sequence in fasta_sequences] mean_length = np.mean(sequence_lengths) max_length
    = np.max(sequence_lengths) # Output results print(''Mean expression:'', mean_expression)
    print(''Variance expression:'', variance_expression) print(''Significant genes:'',
    significant_genes) print(''Mean sequence length:'', mean_length) print(''Max sequence
    length:'', max_length) This code assumes that the gene expression data is stored
    in a CSV file called gene_expression_data.csv, and that the DNA sequences are
    stored in a FASTA file called sequences.fasta. It first loads these datasets using
    Pandas and BioPython, respectively. It then performs two types of analysis on
    the data. The first analysis calculates the mean and variance of the gene expression
    levels for each gene, as well as identifies which genes have significantly different
    expression levels between two groups of samples. The second analysis calculates
    the mean and maximum lengths of the DNA sequences in the FASTA file. Galaxy Galaxy
    is a web-based platform for data-intensive biomedical research that provides a
    user-friendly interface for performing data analysis on large-scale biological
    datasets. Galaxy tracks and manages the source of information automatically and
    supports capturing the context and purpose of computational procedures that allow
    users to present a comprehensive computational analysis. BioPython It is a collection
    of Python tools for computational biology and bioinformatics research. This is
    an excellent library for learning Python for bioinformatics because it includes
    documentation, sequence alignment, and source code. GATK The Genome Analysis Toolkit
    (GATK) is a software package developed by the Broad Institute that provides tools
    for variant discovery and genotyping. It is widely used for processing large-scale
    genomic data. The toolbox includes a wide range of tools, with a major emphasis
    on variant finding and genotyping and a significant emphasis on data quality assurance.
    Its strong design, powerful processing engine, and high-performance computing
    capabilities enable it to handle projects of any scale. These are some of the
    most commonly used tools for handling big data analysis in computational biology
    and bioinformatics. 2 Review of Literature Big data has revolutionized the field
    of bioinformatics by providing powerful tools for processing and analyzing large-scale
    biological data sets. This review of literature focuses on the application of
    big data in bioinformatics, covering topics such as genomics, systems biology,
    computational biology, and personalized medicine. Big data has become increasingly
    prevalent in the field of computational biology, providing powerful tools for
    data processing and analysis. This review of literature covers the recent advances
    and challenges in the application of big data analytics in computational biology.
    Several studies have discussed the challenges and opportunities for using big
    data in computational biology. Ching et al. [1] discussed the opportunities and
    obstacles for deep learning in biology and medicine, Yang et al. [2] reviewed
    algorithms, methods, and applications of big data analytics in bioinformatics,
    while Yang et al. [3] reviewed big data analytics in biology, emphasizing the
    computational challenges. The integration of machine learning and artificial intelligence
    has been crucial in the application of big data analytics in computational biology.
    Big data analytics has also played an essential role in understanding complex
    biological systems. Kim and Kim [4] discussed the application of big data analysis
    in systems biology, emphasizing the integration of different data types for better
    understanding of complex biological systems. Ljosa et al. [5] compared different
    methods for image-based profiling of cellular morphological responses to small-molecule
    treatment. Big data analytics has been instrumental in personalized medicine.
    Khan et al. [6] reviewed big data analytics in healthcare and bioinformatics,
    emphasizing the role of big data in personalized healthcare. Chen et al. [7] highlighted
    the role of big data analytics in genomics and again Chen et al. [8] reviewed
    the role of big data in computational biology, discussing the potential for personalized
    medicine. Several studies have emphasized the need for efficient tools to process
    and analyze big data in genomics, while Peng and Tang [9] discussed the use of
    big data analytics in cancer research. Sharma and Menon [10] reviewed the application
    of big data analytics in genomics, emphasizing the role of machine learning and
    deep learning in the field. The integration of big data in systems biology has
    also gained attention. A reviewed the integration of big omics data for better
    human health [11]. Nobile and Sealfon [12] discussed the use of big data and machine
    learning in neuroscience. In computational biology, several studies have focused
    on the application of big data analytics in the field. Alqurashi and Mavromatis
    [13] reviewed the use of big data analytics in computational biology and discussed
    the role of big data in computational biology. Leung et al. [14] reviewed machine
    learning in genomic medicine, emphasizing the computational challenges and available
    data sets. Personalized medicine has also benefited from the application of big
    data analytics. Machine learning is a type of artificial intelligence that enables
    computers to learn and improve from experience without being explicitly programmed.
    This is achieved through the use of algorithms that can identify patterns and
    relationships in large volumes of data, and then use this knowledge to make predictions
    or take actions [15]. Python is a popular programming language for data science
    and has many libraries that can be used for natural language processing (NLP),
    data visualization, and network analysis. Some examples of Python libraries commonly
    used for these tasks include Matplotlib, and NetworkX for visualization and graph
    theory respectively. These libraries may be used to extract and analyze metadata
    from scientific publications, such as author names, publication dates, and keywords.
    NLP approaches may also be used to analyze the content of publications, such as
    determining the major subjects or sentiment of the text. The findings of these
    studies can then be shown in relevant and intelligible ways using visualization
    tools such as word clouds or interactive graphs. Python libraries can be extremely
    useful for analyzing scientific publications and metadata, and can help researchers
    to gain insights and make discoveries more efficiently [16]. A review article
    on the use of big data analytics tools and techniques can provide valuable insights
    into crop development and agricultural trends, enabling farmers and policymakers
    to make better decisions about land use, crop management, and other important
    aspects of agriculture [17]. A significant advancements made in the application
    of omics and clinical health data to personalized medicine has been discussed
    and a review written on Big data in biomedicine [18]. A review on big data in
    healthcare and discussed big data management, analysis, and interpretation that
    is efficient can alter the status quo by opening up new paths for modern healthcare
    [19]. Cloud computing, a paradigm in which users rent computers and storage space
    from various data servers or data centers, is gaining appeal in genomics research.
    How cloud computing is utilized in genomics for research and large-scale collaborations,
    and also recommended that its elasticity, repeatability, and privacy properties
    make it perfect for large-scale reanalysis of publicly available archived data,
    including data with privacy protection [20]. Recently, Google Colaboratory or
    Google Colab is a research initiative for developing machine learning models using
    powerful hardware like Graphical Processing Units (GPUs) and Tensor Processing
    Units (TPUs). Google Colab is available for free and provides an interactive Jupyter
    notebook environment [21]. The literature reviewed in this study highlights the
    crucial role of big data analytics in bioinformatics, providing powerful tools
    for processing and analyzing large-scale biological data sets. The application
    of big data in genomics, systems biology, computational biology, and personalized
    medicine has significantly improved our understanding of biological processes
    and has the potential to advance disease diagnosis, treatment, and prevention.
    3 Challenges and Opportunities The analysis of big data in computational biology
    and bioinformatics presents several challenges and opportunities. Some challenges
    and opportunities are described below: Data integration: Data integration from
    several biological databases or resources is a key challenge in the area of big
    data research. Developing algorithms that can incorporate data from several web
    resources can aid in our understanding of biological processes. Data standardization:
    Various statistical analyses, such as hypothesis testing, ANOVA, T-test, F-test,
    and other parametric tests need certain assumptions, such as data having a normal
    distribution. The absence of normalized/standardized data formats is a major challenge
    in huge data processing in computational biology and bioinformatics. Before analyzing
    the data, it must first be transformed into a standardized data form. The development
    of standard data formats and researchers can facilitate data interchange and collaboration.
    Data privacy and security: Protecting the privacy and security of sensitive biological
    data is a serious concern in the area of big data analysis. It is vital to further
    research in the area by creating secure and encrypted data exchange platforms.
    Despite these difficulties, large data analysis in computational biology and bioinformatics
    offers great prospects for furthering our understanding of biological processes.
    In the area of big data analysis, high processing power machines are necessary,
    as are efficient rapid algorithms that consume very little time and memory. This
    necessitates research and development in computer and statistical sciences. 4
    Conclusion Nowadays, big data analysis in computational biology and bioinformatics
    is an important area of research and development. With high-throughput technology
    accelerating the development of biological data, it has become important to create
    new methods, algorithms, and tools to store, process, retrieve, statistical analysis,
    and visualize these massive and complicated datasets. Large amounts of biological
    data, such as sequencing data in genomics, have the potential to lead to significant
    improvements in our knowledge of biological systems and disease research. It still
    presents considerable challenges that necessitate multidisciplinary collaboration
    among biologists, computer scientists, statisticians, and other experts. With
    further research and development, we may be able to fulfill the full potential
    of big data analysis in the areas of computational biology and bioinformatics
    with more research and development, resulting in new insights and successes in
    these domains. Big data analysis has rapidly grown into an important part of computational
    biology and bioinformatics. With so much sequencing data being generated in genomics,
    transcriptomics, and proteomics, it is vital to develop efficient data collection,
    storage, processing, and analysis tools and algorithms. Although massive data
    analysis presents a number of challenges, it gives an excellent chance to further
    our understanding of biological processes. References Ching T, Himmelstein DS,
    Beaulieu-Jones BK, Kalinin AA, Do BT, Way GP, Alipanahi B (2018) Opportunities
    and obstacles for deep learning in biology and medicine. J R Soc Interface 15(141):20170387
    Article   Google Scholar   Yang J, Zhang X, Liu S (2017) Big data analytics in
    bioinformatics: algorithms, methods, and applications. Int J Data Min Bioinform
    17(2):105–124 Google Scholar   Yang CT, Kristiani E, Leong YK, Chang JS (2023)
    Big data and machine learning driven bioprocessing – recent trends and critical
    analysis. Bioresour Technol 372:128625 Article   Google Scholar   Kim JH, Kim
    S (2019) Big data analysis in systems biology. J Microbiol Biotechnol 29(2):171–180
    Google Scholar   Ljosa V, Caie PD, Ter Horst R, Sokolnicki KL, Jenkins EL, Daya
    S, Carpenter AE (2013) Comparison of methods for image-based profiling of cellular
    morphological responses to small-molecule treatment. J Biomol Screen 18(10):1321–1329
    Article   Google Scholar   Khan S, Khan HU, Nazir S (2022) Systematic analysis
    of healthcare big data analytics for efficient care and disease diagnosing. Sci
    Rep 12:22377 Article   Google Scholar   Chen Y, Palakal M, Zhou B (2017) Big data
    analytics in genomics. J Biomed Inform 67:1–3 Google Scholar   Chen Y, Liu Y,
    Yu C (2019) Big data in computational biology: a review. Biomed Pharmacother 110:524–532
    Google Scholar   Peng Y, Tang H (2016) Big data analytics in cancer research.
    Curr Pharmacol Rep 2(6):305–313 Google Scholar   Sharma A, Menon R (2019) Big
    data analytics in genomics: a review. Genomics 111(1):43–50 Google Scholar   Karczewski,
    Konrad J, Snyder MP (2018) Integrative omics for health and disease. Nat Rev Genet
    19(5):299–310 Google Scholar   Nobile MS, Sealfon SC (2017) Big data and machine
    learning in neuroscience. J Neuroimmune Pharmacol 12(1):1–2 Google Scholar   Alqurashi
    M, Mavromatis C (2018) Big data analytics in computational biology: a review.
    Curr Bioinforma 13(5):452–462 Google Scholar   Leung MK, Delong A, Alipanahi B,
    Frey BJ (2015) Machine learning in genomic medicine: a review of computational
    problems and data sets. Proc IEEE 104(1):176–197 Article   Google Scholar   Chen
    L, Li S, Bai Q, Yang J, Jiang S, Miao Y (2021) Review of image classification
    algorithms based on convolutional neural networks. Remote Sens 13(22):4712 Article   Google
    Scholar   Heldens S, Sclocco A, Dreuning H, Werkhoven BV, Hijma P, Maassen J,
    Nieuwpoort RV (2022) litstudy: a Python package for literature reviews. SoftwareX
    20:Article 101207 Article   Google Scholar   Kumar P, Kumar A, Panwar S, Dash
    S, Sinha K, Chaudhary VK, Ray M (2018) Role of big data in agriculture – a statistical
    perspective. Ann Agric Res 39(2):210–215 Google Scholar   Costa FF (2014) Big
    data in biomedicine. Drug Discov Today 19(4):433–440 Article   Google Scholar   Das
    S, Chaudhuri S, Chatterjee R (2017) Big data analytics in healthcare and bioinformatics:
    a survey of the literature. J Biomed Inform 71:93–108 Google Scholar   Langmead
    B, Nellore A (2018) Cloud computing for genomic data analysis and collaboration.
    Nat Rev Genet 19(4):208–219 Article   Google Scholar   Bisong E (2019) Google
    colaboratory. In: Building machine learning and deep learning models on Google
    cloud platform. Apress, Berkeley Chapter   Google Scholar   Download references
    Author information Authors and Affiliations ICAR-Indian Agricultural Statistics
    Research Institute, Pusa, New Delhi, India Prakash Kumar, Ranjit Kumar Paul, Himadri
    Shekhar Roy, Md. Yeasin,  Ajit & Amrit Kumar Paul Editor information Editors and
    Affiliations Department of Electronics and Communication Engineering, Jalpaiguri
    Govt. Engineering College, Jalpaiguri, West Bengal, India Sudip Mandal Rights
    and permissions Reprints and permissions Copyright information © 2024 The Author(s),
    under exclusive license to Springer Science+Business Media, LLC, part of Springer
    Nature About this protocol Cite this protocol Kumar, P., Paul, R.K., Roy, H.S.,
    Yeasin, M., Ajit, Paul, A.K. (2024). Big Data Analysis in Computational Biology
    and Bioinformatics. In: Mandal, S. (eds) Reverse Engineering of Regulatory Networks.
    Methods in Molecular Biology, vol 2719. Humana, New York, NY. https://doi.org/10.1007/978-1-0716-3461-5_11
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-1-0716-3461-5_11
    Published 07 October 2023 Publisher Name Humana, New York, NY Print ISBN 978-1-0716-3460-8
    Online ISBN 978-1-0716-3461-5 eBook Packages Springer Protocols Publish with us
    Policies and ethics Download protocol PDF Download protocol EPUB Sections Figures
    References Abstract Introduction Review of Literature Challenges and Opportunities
    Conclusion References Author information Editor information Rights and permissions
    Copyright information About this protocol Publish with us Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Methods in Molecular Biology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Big Data Analysis in Computational Biology and Bioinformatics
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Arizpe-Gómez P.
  - Harms K.
  - Janitzky K.
  - Witt K.
  - Hein A.
  citation_count: '0'
  description: 'Background: Gait feature analysis plays an important role in diagnosing
    and monitoring diseases that compromise motor function. This article presents
    the results of a study, which was aimed at assessing the accuracy and precision
    of computer-aided gait feature analysis performed with a system based on Microsoft®
    Azure™ Kinect™ Cameras (AzureKinect). Research question: Can an AzureKinect-based
    system measure basic gait parameters with sufficient accuracy for motor status
    assessments? Methods: The presented AzureKinect-based system was evaluated by
    measuring the step length (SL), cadence, and velocity, which are important gait
    features, of both healthy participants and participants with a neurological motor
    impairment (total number of participants: N = 24). The GAITRite® system, which
    is an established gold standard for gait analysis, was used as the ground truth.
    Results: The results show that the AzureKinect-based system can provide measurements
    of average SL, cadence, and velocity. A comparison with the ground truth revealed
    a mean absolute error (MAE) of 1.74 cm in SL, 4.6 cm/s in gait velocity and 6.3
    steps/min for cadence. Pearson''s correlation coefficients range from r = 0.8
    to r = 0.99, demonstrating a very high correlation between the measurements of
    the AzureKinect system and the ground truth. Significance: The AzureKinect-based
    system is able to measure basic gait parameters with sufficient accuracy. This
    is a first step towards a comprehensive self-measuring marker-less camera-based
    kinematic analysis that could be performed at home or in general medical practices.'
  doi: 10.1016/j.bspc.2023.105352
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Materials
    and methods 3. Results 4. Discussion 5. Conclusion CRediT authorship contribution
    statement Declaration of Competing Interest Acknowledgement Appendix A. Supplementary
    material Data availability References Show full outline Figures (13) Show 7 more
    figures Tables (2) Table 1 Table 2 Biomedical Signal Processing and Control Volume
    87, Part A, January 2024, 105352 Towards automated self-administered motor status
    assessment: Validation of a depth camera system for gait feature analysis Author
    links open overlay panel Pedro Arizpe-Gómez a 1, Kirsten Harms a 2, Kathrin Janitzky
    b, Karsten Witt b c 3, Andreas Hein a d 4 Show more Add to Mendeley Share Cite
    https://doi.org/10.1016/j.bspc.2023.105352 Get rights and content Under a Creative
    Commons license open access Highlights • This paper presents a prototypical system
    based on RGB-D cameras and compares it to a gold standard for gait analysis. •
    The new system uses 3 interconnected RGB-D cameras and BodyTracking software to
    calculate gait parameters. • Analyzed variability within parameters of healthy
    gait and gait impaired by a neurological disorder. • Significant gait parameters
    (step length, cadence and velocity) could be measured with sufficient accuracy.
    • Important step towards comprehensive self-administered camera-based kinetic
    analysis. Abstract Background Gait feature analysis plays an important role in
    diagnosing and monitoring diseases that compromise motor function. This article
    presents the results of a study, which was aimed at assessing the accuracy and
    precision of computer-aided gait feature analysis performed with a system based
    on Microsoft® Azure™ Kinect™ Cameras (AzureKinect). Research question Can an AzureKinect-based
    system measure basic gait parameters with sufficient accuracy for motor status
    assessments? Methods The presented AzureKinect-based system was evaluated by measuring
    the step length (SL), cadence, and velocity, which are important gait features,
    of both healthy participants and participants with a neurological motor impairment
    (total number of participants: N = 24). The GAITRite® system, which is an established
    gold standard for gait analysis, was used as the ground truth. Results The results
    show that the AzureKinect-based system can provide measurements of average SL,
    cadence, and velocity. A comparison with the ground truth revealed a mean absolute
    error (MAE) of 1.74 cm in SL, 4.6 cm/s in gait velocity and 6.3 steps/min for
    cadence. Pearson’s correlation coefficients range from r = 0.8 to r = 0.99, demonstrating
    a very high correlation between the measurements of the AzureKinect system and
    the ground truth. Significance The AzureKinect-based system is able to measure
    basic gait parameters with sufficient accuracy. This is a first step towards a
    comprehensive self-measuring marker-less camera-based kinematic analysis that
    could be performed at home or in general medical practices. Graphical abstract
    Download : Download high-res image (177KB) Download : Download full-size image
    Previous article in issue Next article in issue Keywords Gait analysisGAITRiteRGB-D
    cameraMicrosoft kinectReliability analysisDepth sensor 1. Introduction Mobility
    has a major impact on quality of life. Gait disturbances are a hallmark of aging,
    musculoskeletal changes, and the majority of neurological diseases such as neuropathies,
    stroke, multiple sclerosis, and Parkinson’s disease. Moreover, the detection of
    impaired spatial and temporal parameters of gait enables the prediction of falls
    and functional decline in elderly people [1], [2]. Movement analysis systems based
    on RGB-D (Red, Green, Blue and Depth) cameras combine several advantageous properties.
    They can be installed at small local medical practices and even in patients’ homes,
    can be used in less advanced technical settings and are easy to handle. If the
    data recordings and automated kinematic analysis are accurate, such systems could
    provide medical personnel with comprehensive information. This could lead to better
    detection of gait disturbances, targeted treatment decisions and therefore may
    improve the patients’ quality of life. Step length, cadence, and gait velocity
    are the clinical most important gait parameters and therefore these parameters
    are the focus of this validation study. These parameters are important regarding
    the natural course of aging [3], the differentiation to pathological processes
    [4], change after therapeutic procedures [5], [6] and are part of the general
    geriatric assessment [7]. In addition, these variables are useful to compare the
    new 3D camera system with the established GAITRite system, reporting the values
    with a favorable accuracy. Several studies have already proposed the use of depth
    cameras, such as the first and second versions of the Microsoft® Kinect™ (KinectV1
    and KinectV2, respectively), to record and analyze gait parameters in the context
    of health services (see [8]). Baldewijns et al. compared the KinectV1 system with
    the GAITRite walkway and reported overall acceptable correlation coefficients
    of 0.94 for average step length (SL) and 0.75 for average step time per walk [9].
    However, the authors report problems (i) that arise from the small field of view
    (hereafter FOV) of about 2.8 m of the KinectV1 camera, (ii) with defining the
    exact SL for single steps on the basis of the kinetics of the center of mass and
    (iii) regarding a dropout rate of one third among all study participants due to
    technical and software problems associated with the KinectV1 RGB-D camera system.
    In a further study, the KinectV1 was compared with the marker based Vicon® [10]
    motion capture system demonstrating a satisfactory correspondence between both
    systems regarding the gait speed variability, a risk factor for falls in elderly
    people [11]. Both studies demonstrated the feasibility of analyzing basic gait
    parameters using a 3D consumer camera system. However, both studies also showed
    limitations such as problematic automated motion detection in a small FOV. The
    Neurokinect system [12] is a portable, inexpensive, and marker-less solution for
    gait analysis. However, this solution is based on older sensors, originally developed
    for pure entertainment, whose reliability and accuracy were not sufficient for
    some gait parameters [13]. To overcome the mentioned limitations, the present
    study addresses the question whether a high definition RGB-D consumer camera can
    accurately measure basic parameters of gait (such as SL, cadence, and velocity)
    on an 8.84 m GAITRite walkway. Compared to previous works, the proposed system
    incorporates three main improvements: The use of the Microsoft® Azure™ Kinect™
    [14] (Microsoft Corporation, WA, USA, hereafter AzureKinect) which offers significantly
    higher depth resolution and improved body pose estimation accuracy [15] compared
    to KinectV1 and KinectV2. The utilization of 3 interconnected cameras [16] for
    a complete capture of the GAITRite walkway. The inclusion of healthy participants
    and participants suffering from neurological gait disorders to increase the variability
    of gait parameters. In addition, the selected approach compares the use of the
    coordinates of the more reliable estimation of the pelvis position for gait parameter
    identification to the use of the noisy estimation [15] of the feet coordinates.
    The aim of this study is to validate a proof-of-concept system for movement analysis.
    The main purpose is to elicit whether movement features can, in principle, be
    derived from the 3D coordinate data provided by the AzureKinect-based system.
    2. Materials and methods The deviations between the proposed system and the ground
    truth are determined both for normal gait patterns of healthy participants and
    for pathological gait patterns of the study participants with motor impairments.
    The study was approved by the local ethics committee. All participants gave their
    consent to participate in the study in written form. The study was registered
    (DRKS00020921). 2.1. Study design All study participants were presented with the
    same task [17]. They were asked to walk over the GAITRite walkway in one direction,
    turn 180 degrees (outside of the walkway), walk back in the opposite direction,
    turn again and so forth. This way every participant made 10 passes over the walkway
    − 5 in each direction. 2.2. Experimental setup The ground truth was provided by
    the GAITRite system, which is a certified medical device that computes several
    relevant gait features in 2-D. The AzureKinect SDK provides 3-D coordinates of
    the estimated positions of 32 body parts, including the feet [18]. Both the GAITRite
    and three AzureKinects were installed in a long corridor in a Neurology Clinic,
    as schematically depicted in Fig. 1a. This figure shows the position of the GAITRite
    within the setup, as well as the positions of the AzureKinects. It also shows
    the top view of the operating ranges of all cameras, the FOV of each camera and
    the distance between the cameras and the GAITRite operating area. Fig. 1b shows
    the side view of the operating ranges of all cameras and their FOV. Download :
    Download high-res image (298KB) Download : Download full-size image Fig. 1a. Schematic
    overview of experimental setup: Top view (except for the cube, the laptop and
    the person walking. These icons are in side perspective for clarity). The orange/brown
    rectangle represents the mat, the transparent rectangles with numbers represent
    the cameras, the grey rectangles represent the plastic covers for the sensors
    of the mat, the rounded triangles represent the operating ranges of the cameras
    (to depict their overlapping). Download : Download high-res image (110KB) Download
    : Download full-size image Fig. 1b. Representation of the operating ranges of
    the camera used in the experimental setup (schematic, side view). The rectangles
    represent the camera, and the triangles represent the operating ranges. Fig. 1c
    (including AzureKinect diagram [18]) illustrates the local frame of reference
    for each element of the setup (AzureKinects, ArUCo cube and GAITRite walkway).
    The coordinate transformation will be discussed in section 2.3.1. Download : Download
    high-res image (168KB) Download : Download full-size image Fig. 1c. Overview of
    frames of references for the GAITRite, the extrinsic calibration cube (world coordinates)
    and the camera. While walking over the GAITRite walkway, the subjects were recorded
    by three Azure Kinect cameras. The cameras were positioned around the GAITRite
    walkway in such a way that their respective FOVs overlapped, i.e., it was made
    sure that every movement of the subjects was recorded by at least one camera,
    while ensuring that the movements of the participants were not disrupted. The
    AzureKinect 0 and 1 were each connected to a small computer (INTEL NUC7I5BNK with
    Intel Core i5 processor) and AzureKinect 2 was connected to a laptop (Fujitsu
    E744 Intel Core i5 processor) running a program to store the camera data as MKV
    files for offline processing with the AzureKinect body tracking (BT) SDK version
    1.1.0. The GAITRite system was connected to the same laptop as the AzureKinect
    2. The cameras were synchronized in a daisy-chain configuration and the GAITRite
    system was included at the end of the chain. For a detailed technical description
    of the study design, see [17]. 2.3. Camera calibration The three AzureKinects
    provided 3D point clouds of the respective scenes they had recorded. Using this
    information, it was possible to perform an extrinsic calibration of the cameras
    (step 1, Fig. 2). Download : Download high-res image (414KB) Download : Download
    full-size image Fig. 2. Overview of the data processing steps. The Step Length
    Method refers to the method of extracting the basic gait parameters from the analysis
    of the distance between the feet coordinates. The Pelvis Method refers to the
    method of extracting the basic gait parameters from the analysis of the elevation
    of the pelvis. The initial plan was to use ArUCo markers for extrinsic calibration.
    These are binary square fiducial markers that are commonly used for camera pose
    estimation [19], [20]. However, the detection was unreliable and error prone,
    probably due to the poor lighting of the setup, as described in our previous work
    [17] and relatively large distances between cameras and markers. The calibration
    had to be done for every patient and every camera, since the setup had to be mounted
    and unmounted repeatedly. Because of this, a new offline calibration method had
    to be used for this study. The Iterative Closest Point (ICP) point cloud registration
    was not an option because of the sparse presence of points in the overlapping
    areas of the operating ranges of the cameras. Therefore, a semi-automatic extrinsic
    calibration system for multi-RGBD-cameras using arbitrary axis-aligned edges was
    developed. It was necessary to use manually selected axis-aligned vectors (see
    Fig. 3), which were then automatically processed for the transformations of the
    extrinsic camera calibration. This is a similar approach to Open3D’s “registration
    using user correspondences” [21]. Download : Download high-res image (650KB) Download
    : Download full-size image Fig. 3. Axis-Aligned Vectors for Calibration. Note:
    RGB images are displayed for clarity, but the points were selected on the colored
    point clouds. 2.3.1. Semi-automatic extrinsic calibration system for multi-RBGD-cameras
    using arbitrary axis-aligned edges The calibration uses the long edges of the
    mat and the long edges of the walls, which are perpendicular and easy to find,
    to set the calibration axes. Let denote the participant number and the camera
    number. Let, and be such unit vectors for participant and camera . Furthermore
    let be a function that calculates the rotation matrix. between unit vector and
    unit vector . This function is given by: where is the identity matrix in and is
    the skew-symmetric cross product matrix of . Then the rotation matrix between
    the coordinate system of camera and the world coordinate system for each participant
    can be calculated as follows: Since the three axes are orthogonal, the method
    was tested by verifying that The translation vector can be calculated from the
    vector with the necessary adjustment: (since the cube’s sides were 13 cm long).
    Finally, the homogeneous transformation matrix. between the coordinate system
    of camera and the world coordinate system for each participant can be described
    as: where . This yielded the transformation information necessary to transform
    the BT-joint coordinate data of all three cameras into a common world coordinate
    system (step 2, Fig. 2). 2.4. Computation of gait features Once the positions
    of the participants’ body parts (called Body Tracking joints or BT joints in the
    context of the AzureKinect SDK) within the world coordinate system were known,
    interesting gait features could be computed from this data. For this, only BT
    joints detected with a reported confidence level of 2 or more were considered
    (AzureKinect BT SDK assigns a confidence value from 0 to 4 to every detected BT
    joint). Fig. 2 shows an overview of the data processing steps that were performed
    to compute the gait features SL, cadence, and velocity. One clinically relevant
    feature is the SL, which can be determined by looking at the maxima in feet distance
    along the line of progression (step 3a, Fig. 2). As the -axis of the world coordinate
    system runs in parallel to the GAITRite mat, the distance in y-direction (hereafter
    y-distance) of the two foot-BT-joints over time was computed for each participant
    and the maxima were detected algorithmically [22]. As the ranges of the three
    AzureKinects overlapped there was also an overlap in the feet distance data, i.e.,
    some maxima were recorded more than once (see Fig. 4a). The redundantly recorded
    maxima were fused, i.e., detected maxima between which there was a time delta
    of less than 0.15 s were handled as a single maximum. Download : Download high-res
    image (695KB) Download : Download full-size image Fig. 4. Traces of feet y-distance
    and pelvis elevation for the first walk of participant HP-1. The data from camera
    0 is shown in green, the data from camera 1 in orange and the data from camera
    2 in blue. Detected maxima are marked in the respective color. Those that were
    redundantly detected are fused and the maxima that resulted from that fusion process
    are marked in semi-transparent cyan. In this context, we noticed that the cameras
    tend to detect smaller step lengths at the edges of their recording range. This
    “distortion” appears only when the AzureKinect BT does not provide enough data
    points with sufficient confidence to calculate a step properly. Because of this
    distortion, the maximum with the higher value was chosen for computation of SL.
    A manual confirmation followed afterwards. Even after these measures, some false
    detections due to noise remained, e.g., the third cyan-circled maximum in Fig.
    4a. These were dealt with in step 4 (of Fig. 2). The clustering algorithm used
    was DBSCAN (Density-Based Spatial Clustering for Applications with Noise) implemented
    by the Python module scikit-learn [24], [25]. Three important parameters of this
    algorithm are: • metric - the metric that is used for calculating the distance
    between two data points. • epsilon - the maximum distance between two data points
    to be considered as neighbors. • min_samples - the minimum number of data points
    that have to be in the neighborhood of a given point for that point to be considered
    a core point. Regarding the metric, two data points were considered “infinitely
    far apart”, if the time delta between them was less than 0.4 s or greater than
    1.6 s. The reasoning behind this was that data points with a time delta that is
    too small do not signify independent steps, while data points with a time delta
    that is too great cannot belong to the same walk over the GAITRite mat. It is
    also consistent with typical cadence of healthy adults being ca. 2 steps/s [31]
    and the assumption that persons with a neurological disease walk more slowly.
    If this time constraint was fulfilled by two data points, their distance was computed
    as the Euclidean distance with regard to the two dimensions time measured in [s]
    and step length or pelvis elevation measured in [cm]. For the step length clustering,
    epsilon was varied between values of 5 and 15 in order to detect the expected
    number of 10 walk clusters. Generally, epsilon was chosen as low as possible and
    it had to be set to a higher value for participants with a Parkinson’s disease
    (PD) diagnosis, because of a greater variability in step length. In case of the
    pelvis elevation, a value of eight was determined for epsilon in order to lead
    to the detection of all walk clusters. min_samples was set to two for all participants
    because a walk, by nature, consists of single consecutive steps, so no greater
    number of data points can be expected to be in the neighborhood of a core point.
    Fig. 5a shows an example of the clustering of feet y-distance maxima. A number
    of outliers (black) were detected and removed. Among those outliers are the first
    three steps detected in the first walk (leftmost cluster). Download : Download
    high-res image (304KB) Download : Download full-size image Fig. 5. Result of DBSCAN
    clustering for the feet y-distance – from the SLM (a) and PEM (b) maxima data.
    Shown data represent the step length in cm in relation to the time of the recording.
    This depicts the detected steps of participant HP-1 using the SLM and PEM. The
    colors of the points represent their correspondence to each of the ten walks the
    participant performed. Each DBSCAN cluster represents a walk and is depicted in
    a different color. Outliers are depicted as small black points. The clustering
    of the pelvis elevation maxima revealed very few outliers, and none were detected
    for one participant (see Fig. 5b). The gait features cadence, and velocity were
    computed via two different methods: In step 5a (of Fig. 2), the SL data was used
    and in step 5b it was based on the pelvis elevation data. All values were first
    determined for each individual walk (i.e., value cluster) and finally the mean
    was calculated. 2.4.1. Computation via step length data Let be the timestamps
    of a cluster and the respective SLs. The first timestamp signifies the end of
    the first registered step, because that is the point, when the y-distance between
    the feet is highest. The start time of that first step is unknown. Therefore,
    to compute the total distance walked between (which is also the start time of
    the second step) and , only the SLs must be added. The velocity is therefore determined
    via the following formula: Between and n steps take place. The cadence is computed
    as: 2.4.2. Computation via pelvis elevation data Let be the timestamps of a cluster
    and the -positions of the pelvis at those timestamps. Because the -axis of the
    world coordinate system runs parallel to the line of progression of the participants
    on the GAITRite mat, the total distance walked can be determined via . The velocity
    is therefore determined via the following formula: Between and n steps take place.
    The cadence is computed as: In step 5a (of Fig. 2) finally, the results were compared
    to the ground truth provided by the GAITRite system. 3. Results The study originally
    included 26 participants. 15 of these had been diagnosed with a movement disorder
    (14 were diagnosed with PD and 1 was diagnosed with hereditary spastic spinal
    paralysis) and 11 participants had no movement disorder diagnosis at the time
    of data capturing. The data of two participants with a movement disorder (including
    the participant with spinal paralysis) could not be evaluated because of data
    file corruption (see section 4). A complete overview of the demographic data of
    each evaluated participant can be found in Table 1. Table 1. Demographic data
    for all evaluated study participants. Participant Gender Age [years] Height [cm]
    Weight [kg] Left Leg Length [cm] Right Leg Length [cm] HP-1 F 37 168 50 88 86
    HP-2 M 38 190 110 97 97 HP-3 M 30 177 67 93 94 HP-4 F 29 173 70 94 95 HP-5 F 27
    165 74 86 86.5 HP-6 M 20 173 66 86 87 HP-7 M 31 183 105 94 93 HP-8 F 24 168 65
    87 90 HP-9 F 38 172 65 93 94 HP-10 F 52 171 73 92 94 HP-11 M 45 198 152 96 96
    PPD-1 F 79 156 62 89 89 PPD-2 M 64 179 88 94 94 PPD-3 M 70 180 75 98 99 PPD-4
    F 48 168 97 94 94 PPD-5 M 73 176 73 92 93 PPD-6 M 77 175 77 99 98 PPD-8 F 74 160
    74 84 83 PPD-9 F 71 168 71 85 85 PPD-10 M 69 176 69 94 93 PPD-12 F 48 168 97 94
    94 PPD-13 F 49 159 49 77 78 PPD-14 M 78 177 78 95 93 PPD-15 F 67 156 67 84 85
    mean µ 51.6 172.3 78.1 91.0 91.3 std σ 19.2 9.7 21.4 5.3 5.1 The averages of the
    gait features SL, cadence, and velocity were computed for 11 healthy participants
    (HP) and 13 participants with a neurological disorder (PPD). However, some of
    the recorded files were unreadable or corrupted, so that for two participants
    (PPD-7, PPD-11) there was no usable data and the computed values for the participants
    PPD-1 - PPD-6, PPD-8 and PPD-10 are based on the data of only 2 AzureKinects.
    In most cases, the AzureKinect-based system underestimates the mean SL. Fig. 6
    A shows the results of the SL computation for all participants. Download : Download
    high-res image (851KB) Download : Download full-size image Fig. 6. A: Average
    step length of all healthy participants (HP) and participants with a Parkinson''s
    disease diagnosis (PPD). B: Velocity values for healthy participants (HP) and
    participants with a Parkinson''s disease diagnosis (PPD). C: Cadence values for
    healthy participants (HP) and participants with a Parkinson''s disease diagnosis
    (PPD). D: Mean absolute error of computed step lengths. Orange: Ground truth provided
    by the GAITRite system. E: Mean absolute errors of computed velocities. Blue:
    Values computed via step length data. F: Mean absolute errors of computed cadence
    values. Green: Values computed via pelvis elevation data. The average deviation
    between means is 2.6 %. The greatest deviation is found for participant PPD-5
    with −7.6 %%, the smallest for participant HP-2 with + 0.2 %. For all but three
    participants (HP-4, HP-5, and PPD-5) the deviation is less than 5 %. The MAE is
    1.74 cm (see Fig. 6 D). The standard deviation (std) is greater for the values
    determined by the AzureKinect-based system in all cases. While the average std
    provided by the GAITRite system is 3.1 cm, it is 4.9 cm in case of the AzureKinect-based
    system. The velocity values computed using the SL data are smaller than the values
    given by the ground truth in almost all cases, except for PPD-14. In case of the
    pelvis elevation data, there are overestimations as well as underestimations.
    Fig. 6 B shows the velocity results for all participants. The MAE of the step
    length method (SLM) is greater than the average error of the pelvis elevation
    method (PEM), with values of 8.6 % or 10.6 cm/s and 3.7 % or 4.6 cm/s, respectively
    (see Fig. 6 E). For most participants (except HP-1, PPD-5, PPD-8, PPD-13), the
    error of the PEM is less than 5 %. Fig. 7 A shows the Bland-Altman plot for the
    step length (SL) value pairs of all participants. The mean difference is 1.42
    cm. The 95 % limits of agreement lie at −2.02 cm/s and 4.87 cm. All but one value
    lie within these limits. Download : Download high-res image (491KB) Download :
    Download full-size image Fig. 7. Bland-Altman Plots. Fig. 7 B shows the Bland-Altman
    plot for the velocity (VEL) value pairs of all participants. The mean difference
    is −2.97 cm/s. The 95 % limits of agreement lie at −14.56 cm/s and 8.61 cm/s.
    All values lie within these limits. The cadence values computed using the SL data
    are smaller than the values given by the ground truth in most cases (except HP-8,
    HP-9 and PPD-14). PEM shows some underestimations as well as overestimations.
    Fig. 6 C shows the cadence results for all participants. The average error of
    PEM is smaller than the average error of SLM, with values of 5.7 % or 6.3 steps/minute
    and 7.2 % or 8.1 steps/minute, respectively (see Fig. 6 F). PEM produces an error
    greater than 5 % in 10 out of 24 participants. For SLM this is the case for 17
    of the participants. Fig. 7 C shows the Bland-Altman plot for the cadence (CAD)
    value pairs of all participants computed via the PEM. The mean difference is 3.81
    steps/min. The 95 % limits of agreement lie at −10.86 steps/min and 18.49 steps/min.
    All but one value lie within these limits. Pearson''s correlation coefficients
    (PCC) between the results of the AzureKinect and GAITRite system were calculated
    for the means of step length, velocity and cadence (see Table 2). Looking at all
    participants, the PCC values indicate a very strong correlation5 in all cases,
    with PCC values greater than 0.950 for step length and velocity. The correlation
    of the cadence results is slightly smaller, with 0.854 for the values determined
    via the step length method and 0.817 for the values determined via the pelvis
    elevation method. Table 2. PCC values for the gait features step length, velocity
    and cadence. Gait Feature PCC Healthy Participants PCC Participants with PD PCC
    All Participants Step Length 0.958 0.992 0.993 Velocity via Step Length Method
    0.975 0.970 0.977 Velocity via Pelvis Method 0.965 0.970 0.983 Cadence via Step
    Length Method 0.920 0.672 0.854 Cadence via Pelvis Method 0.793 0.827 0.817 4.
    Discussion This study evaluates the validity of a gait analysis system based on
    AzureKinect’s consumer RGB-D camera in comparison to the GAITRite Walkway. This
    was done with regard to the accuracy and precision of key indicators of healthy
    and pathological gait. Data of 24 participants were evaluated, of which 13 had
    a neurodegenerative motor disease. A comparison between both techniques revealed
    an MAE of 1.74 cm in SL, 4.6 cm/s difference in gait velocity and 6.3 steps/min
    for cadence. PCCs range from to , demonstrating a very high correlation between
    both recording techniques. Student’s t-test indicates statistical significance
    with for SL, for velocity and for cadence. According to [18], the std of the random
    error of the AzureKinects is less than or equal to 17 mm and the typical systematic
    error is less than 11 mm + 0.1 % of the distance without multipath interference,
    which in our case means that the systematic error is less than 17 mm for all our
    measurements. The MAE (1.74 cm) of the method used for SL approximation is smaller
    than the average error seen in other works, e.g., Steinert et al. [27], who compared
    the KinectV2 to the GAITRite and obtained error values of 1.99 cm and 2.59 cm
    for the left and right SL, respectively. Besides using a newer camera version,
    for the current study, a GAITRite walkway with an active length of 7.93 m was
    used and walks were recorded with three AzureKinects simultaneously. This study
    design might have enabled the recording of more steps compared to the [27] study.
    It is conceivable that a longer measuring path leads to more data and results
    that are more accurate. The difference between the mean SL of patients with PD
    and healthy elderly people in [28] is 10.66 cm, which is a much higher value than
    the average SL error of our AzureKinect-based system (1.74 cm). Therefore, it
    seems realistic that such a system could be used to differentiate between the
    gait of healthy individuals and that of individuals affected by a neurological
    disorder. The MAE of gait velocity, which is 10.6 cm /s for SLM and 4.6 cm /s
    for PEM, is comparable with similar works. [27] Obtained a mean difference of
    4.01 cm/s at preferred speed and 5.76 cm/s at fast speed. However, this degree
    of accuracy was not reached with regard to the gait feature cadence. Regarding
    the MAE, [27] achieve significantly more accurate results with an error value
    of −1.16 steps/minute compared to 6.3 steps/minute (PEM) and 8.1 steps/minute
    (SLM) of the system used in this study. The detection of the feet BT-joints was
    noisier and more unreliable than the detection of the pelvis BT-joint, as was
    also observed by [15]. This improvement in detection stability propagates to the
    calculation of cadence, and velocity. PEM offered an improvement in the MAE of
    gait velocity of 56 % and an improvement of 22 % in the MAE of gait cadence, compared
    to SLM. The more accurate results of the PEM can be explained by the fact that
    the pelvis BT-joint is located at the root of the tree that represents the BT-joint
    positions of the AzureKinect Body Tracking. Because of this, the estimated pelvis
    coordinates are much stabler than the noisier readings of the extremities, like
    feet and hands. This can also be seen in the data collected by Albert et al. [15].
    As described in subsection 2.3 the initially planned extrinsic calibration method
    was unsuccessful and therefore, a new offline calibration method had to be used
    for this study. Using a 3D consumer camera system and further data processing
    and data analysis comes with several potential error sources. These errors can
    be disentangled into different factors such as. I. the distance from the camera
    to the object, II. camera to object angle (see also [29]). III. error of the camera
    system per se, IV. system deficiencies in targeting the intended region of interest
    (target detection, e.g., point or heel of the foot, hip) and further error sources
    such as. V. shoe color. VI. insufficient ambient lighting. In our analysis all,
    these error sources were grouped together given the fact that we are not able
    to dissociate them from one another. We simply compare the results computed based
    on the camera data to the ground truth. Therefore, our analysis encompasses all
    errors, irrespectively of their source and nature. This strategy in the validation
    process might be an advantage, because the measured and processed data as a whole
    were analyzed against the golden standard of the ground truth. It also holds some
    disadvantages, because we were not able to report all variables needed to make
    the study reproducible, (e.g., the color of the shoes worn by the participants
    and the ambient lighting are not systematically reported). One of the goals of
    this validation study was to investigate whether BT data provided by the AzureKinect
    system can be used directly for step detection. Regarding data processing, a fusion
    of redundantly detected steps and a clustering with noise detection was performed
    in order to avoid faulty step detection (see section 2.4). An alternative would
    be to model the y-distance traces via sinusoidal functions (since the movement
    of the pelvis follows a sinusoidal path in the vertical direction during walking
    [23]) or to apply filtering techniques (like a fourth order Butterworth filter).
    This could lead to smoother traces. However, this approach was not implemented
    here to avoid over-filtering, since the (closed source), BT software already considers
    movement continuity. Nevertheless, this and other approaches, including machine
    learning methods should be investigated in future works. In order to identify
    individual walks in the data, a detection of clusters was performed both for the
    feet y-distance and the pelvis elevation maxima (see section 2.4), using the algorithm
    DBSCAN. This algorithm is non-deterministic regarding the labeling of clusters
    if the dataset is permuted [24]. However, this is not relevant in our case since
    labels did not play a role in the interpretation of the data. Another non-deterministic
    aspect of DBSCAN is the fact that border points between clusters may be reachable
    from more than one cluster and are randomly assigned to the first cluster processed
    by the algorithm [24]. In our case, however, one of the dimensions we are clustering
    in is time and there is a natural break between each of the walks over the GAITRite
    mat. During this time, the participants turn around outside the mat, so no steps
    are detected. There were therefore no border points in our data (see reasoning
    regarding “infinitely far apart” points in section 2.4). Some external factors
    might have influenced the accuracy of the BT. Loose, dark, and opaque footwear,
    like boots, could lead to diminished accuracy in the readings, while tighter,
    lighter and smaller footwear is tracked more reliably. This effect might have
    led to a relatively poor detection of the foot BT-joints in HP-4, which resulted
    in a large step length error. Lighting conditions might also influence the accuracy
    of the pose estimation. Some of the captures, especially from camera 0, were probably
    affected by the sunlight that came from the side through an adjacent window. This
    could have caused an unreliable BT-joint estimation. Furthermore, the orientation
    of the participants with respect to the camera also had repercussions on the accuracy
    of the BT, similar to [29]. The segments where the participants walked towards
    a camera had a much better accuracy and stability than the segments where the
    participants walked orthogonally to the direction of view of a camera (so that
    the camera mostly saw them laterally) or away from the camera. Future studies
    could take different directions. On the one hand, the setup of this study could
    be significantly improved by using more cameras along the walkway, so that every
    participant could be captured by at least two cameras at every point. This would
    be possible by putting one camera on each end and three further cameras on each
    side of the walkway. This would offer better camera perspectives to enhance the
    amount and quality of data obtained by the body tracking and offer a broader data
    basis for comparison with the GAITRite. On the other hand, it would be worth evaluating
    the informative value of the data provided by only two cameras. These should cover
    a sufficient range for gait feature analysis, while simplifying the setup for
    future in-home use. Finally, it is worth mentioning that all participants were
    physically able to complete the study, which attests to its feasibility. 5. Conclusion
    The results of the presented system for automatic gait feature extraction based
    on the body pose estimation data provided by the AzureKinect-based system are,
    on average, largely consistent with the measurements offered by the GAITRite system.
    This is a promising finding regarding the possibility of enabling motor status
    assessments at home or at general practices. AzureKinects are easier to set up,
    much more portable and compact than other widespread gait analysis systems and
    could complement other approaches (like the Unsupervised Sensor System [30]) to
    achieve a holistic assessment with limited spatial resources. For further studies,
    it would be worthwhile to analyze further gait parameters as well as extending
    the movement analysis to the middle and upper body parts to cover the entire movement
    spectrum. Moreover, it would be recommendable to investigate both: a setup with
    more cameras, that would potentially provide better Body Tracking quality in a
    general practice, and a minimal setup with just two cameras for in-home use. This
    way, it could be possible, in the long term, to characterize and quantify movement
    disorders in a marker-less way. CRediT authorship contribution statement Pedro
    Arizpe-Gómez: Conceptualization, Methodology, Software, Validation, Formal analysis,
    Investigation, Resources, Data curation, Visualization. Kirsten Harms: Conceptualization,
    Software, Validation, Formal analysis, Investigation, Visualization. Kathrin Janitzky:
    Conceptualization, Methodology, Validation, Investigation, Resources. Karsten
    Witt: Conceptualization, Methodology, Validation, Formal analysis, Investigation,
    Resources. Andreas Hein: Conceptualization, Methodology, Software, Validation,
    Formal analysis, Resources, Supervision, Project administration, Funding acquisition.
    Declaration of Competing Interest The authors declare the following financial
    interests/personal relationships which may be considered as potential competing
    interests: Pedro Fernando Arizpe-Gomez reports financial support, administrative
    support, and article publishing charges were provided by Carl von Ossietzky University
    of Oldenburg nevertheless, this is no longer true, since the funding for OA came
    directly from the BMBF. Karsten Witt reports financial support was provided by
    German Research Foundation. Andreas Hein reports financial support was provided
    by German Research Foundation. Acknowledgement This research and development project
    was funded by the German Federal Ministry of Education and Research (BMBF) within
    the “The Future of Value Creation – Research on Production, Services and Work”
    program (funding number 02K20D130) and managed by the Project Management Agency
    Karlsruhe (PTKA). The author is responsible for the content of this publication.
    This research was also funded by the Deutsche Forschungsgemeinschaft (DFG, German
    Research Foundation), DFG Graduate School 2783 (Principal investigators A.H. and
    K.W). Appendix A. Supplementary material The following are the Supplementary data
    to this article: Download : Download high-res image (405KB) Download : Download
    full-size image Supplementary figure 1. Download : Download high-res image (439KB)
    Download : Download full-size image Supplementary figure 2. Download : Download
    high-res image (441KB) Download : Download full-size image Supplementary figure
    3. Data availability The authors do not have permission to share data. References
    [1] A. Rodríguez-Molinero, A. Herrero-Larrea, A. Miñarro, L. Narvaiza, C. Gálvez-Barrón,
    N. Gonzalo León, E. Valldosera, E. de Mingo, O. Macho, D. Aivar, E. Pinzón, A.
    Alba, J. Passarelli, N. Stasi, R.A. Valverde, L. Kruse, E. Felipe, I. Collado,
    J.B. Sabater The spatial parameters of gait and their association with falls,
    functional decline and death in older adults: a prospective study Sci. Rep., 9
    (1) (2019), 10.1038/s41598-019-45113-2 Google Scholar [2] B.S. Moreira, R.F. Sampaio,
    R.N. Kirkwood Spatiotemporal gait parameters and recurrent falls in community-dwelling
    elderly women: a prospective study Braz. J. Phys. Ther., 19 (2014), pp. 61-69,
    10.1590/bjpt-rbf.2014.0067 View in ScopusGoogle Scholar [3] N.M. Peel, S.S. Kuys,
    K. Klein Gait speed as a measure in geriatric assessment in clinical settings:
    a systematic review J. Gerontol. A Biol. Sci. Med. Sci., 68 (2013), pp. 39-46,
    10.1093/gerona/gls174 View in ScopusGoogle Scholar [4] H. Stolze, S. Klebe, G.
    Petersen, J. Raethjen, R. Wenzelburger, K. Witt, et al. Typical features of cerebellar
    ataxic gait J. Neurol. Neurosurg. Psychiatry, 73 (2002), pp. 310-312, 10.1136/jnnp.73.3.310
    View in ScopusGoogle Scholar [5] A. Bishnoi, M. Shankar, R. Lee, Y. Hu, M.E. Hernandez,
    Effects of therapeutic intervention on spatiotemporal gait parameters in adults
    with neurological disorder: systematic review and meta-analysis, Arch. Phys. Med.
    Rehabil. 2022:S0003-9993(22)00485-3. 10.1016/j.apmr.2022.06.003. Google Scholar
    [6] C. Lei, K. Sunzi, F. Dai, X. Liu, Y. Wang, B. Zhang, et al. Effects of virtual
    reality rehabilitation training on gait and balance in patients with Parkinson’s
    disease: A systematic review PloS One, 14 (2019), Article e0224819, 10.1371/journal.pone.0224819
    View in ScopusGoogle Scholar [7] C.N. Scanaill, B.R. Greene, E.P. Doheny, K. O’Donovan,
    T. O’Shea, A.D. O’Donovan, et al. Clinical Gait assessment of older adults using
    open platform tools, in: Annu Int Conf IEEE Eng Med Biol Soc IEEE Eng Med Biol
    Soc Annu Int Conf 2011;2011:462–5. 10.1109/IEMBS.2011.6090065. Google Scholar
    [8] S. Springer, G. Yogev Seligmann Validity of the kinect for gait assessment:
    A focused review Sens Switz, 16 (2) (2016), p. 194 CrossRefGoogle Scholar [9]
    G. Baldewijns, G. Verheyden, B. Vanrumste, T. Croonenborghs, Validation of the
    kinect for gait analysis using the GAITRite walkway, in: 2014 36th Annu. Int.
    Conf. IEEE Eng. Med. Biol. Soc. EMBC 2014, vol. 2014, IEEE; 2014, p. 5920–3. 10.1109/EMBC.2014.6944976.
    Google Scholar [10] Vicon Motion Systems Limited. Vicon Nexus Product Guide 2015:1–200.
    https://documentation.vicon.com/nexus/v2.2/Nexus1_8Guide.pdf. Google Scholar [11]
    E.E. Stone, M. Skubic, Passive in-home measurement of stride-to-stride gait variability
    comparing vision and Kinect sensing, in: 2011 Annu. Int. Conf. IEEE Eng. Med.
    Biol. Soc., IEEE; 2011, p. 6491–4. 10.1109/IEMBS.2011.6091602. Google Scholar
    [12] J.P.S. Cunha, A.P. Rocha, H.M.P. Choupina, J.M. Fernandes, M.J. Rosas, R.
    Vaz, et al. A novel portable, low-cost kinect-based system for motion analysis
    in neurological diseases, in: Proc Annu Int Conf IEEE Eng Med Biol Soc EMBS 2016;2016-Octob:2339–42.
    10.1109/EMBC.2016.7591199. Google Scholar [13] X. Xu, R.W. McGorry, L.S. Chou,
    J. hua Lin, C chi Chang, Accuracy of the Microsoft KinectTM for measuring gait
    parameters during treadmill walking, Gait Posture 42 (2015) 145–51. 10.1016/j.gaitpost.2015.05.002.
    Google Scholar [14] C.S. Bamji, S. Mehta, B. Thompson, T. Elkhatib, S. Wurster,
    O. Akkaya, et al. IMpixel 65nm BSI 320MHz demodulated TOF Image sensor with 3μm
    global shutter pixels and analog binning Dig. Tech. Pap. - IEEE Int. Solid-State
    Circuits Conf, 61 (2018), pp. 94-96, 10.1109/ISSCC.2018.8310200 View in ScopusGoogle
    Scholar [15] J.A. Albert, V. Owolabi, A. Gebel, C.M. Brahms, U. Granacher, B.
    Arnrich Evaluation of the pose tracking performance of the azure kinect and kinect
    v2 for gait analysis in comparison with a gold standard: A pilot study Sensors,
    20 (2020), p. 5104, 10.3390/s20185104 Google Scholar [16] O. Lohmann, T. Luhmann,
    A. Hein, Skeleton timed up and go, in: 2012 IEEE Int. Conf. Bioinforma. Biomed.,
    IEEE; 2012, p. 1–5. 10.1109/BIBM.2012.6392610. Google Scholar [17] P. Arizpe-Gomez,
    K. Harms, S. Fudickar, K. Janitzky, W. Karsten, A. Hein, Preliminary Viability
    Test of a 3-D-Consumer-Camera-Based System for Automatic Gait Feature Detection
    in People with and without Parkinson’s Disease, in: 2020 IEEE Int. Conf. Healthc.
    Inform. ICHI, IEEE; 2020, p. 1–10. 10.1109/ICHI48887.2020.9374363. Google Scholar
    [18] Corporation M. Azure Kinect DK documentation 2019. Google Scholar [19] S.
    Garrido-Jurado, R. Muñoz-Salinas, F.J. Madrid-Cuevas, R. Medina-Carnicer Generation
    of fiducial marker dictionaries using Mixed Integer Linear Programming Pattern
    Recogn., 51 (2016), pp. 481-491, 10.1016/j.patcog.2015.09.023 View PDFView articleView
    in ScopusGoogle Scholar [20] F.J. Romero-Ramirez, R. Muñoz-Salinas, R. Medina-Carnicer
    Speeded up detection of squared fiducial markers Image Vis. Comput., 76 (2018),
    pp. 38-47, 10.1016/j.imavis.2018.05.004 View PDFView articleView in ScopusGoogle
    Scholar [21] Q.-Y. Zhou, J. Park, V. Koltu,n Open3D: A modern library for 3D data
    processing. ArXiv Prepr ArXiv180109847 2018. 10.48550/arXiv.1801.09847. Google
    Scholar [22] P. Virtanen, R. Gommers, T.E. Oliphant, M. Haberland, T. Reddy, D.
    Cournapeau, et al. SciPy 1.0: Fundamental Algorithms for Scientific Computing
    in Python Nat. Methods, 17 (2020), pp. 261-272, 10.1038/s41592-019-0686-2 View
    in ScopusGoogle Scholar [23] C.L. Lewis, N.M. Laudicina, A. Khuu, K.L. Loverro
    The Human Pelvis: Variation in Structure and Function During Gait Anat. Rec. Hoboken,
    4 (2017), pp. 633-642, 10.1002/ar.23552 View in ScopusGoogle Scholar [24] E. Schubert,
    J. Sander, M. Ester, H.P. Kriegel, X. Xu, DBSCAN Revisited, Revisited: Why and
    How You Should (Still) Use DBSCAN. ACM Trans Database Syst 2017;42. 10.1145/3068335.
    Google Scholar [25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
    O. Grisel, et al. Scikit-learn: Machine Learning in Python J. Mach. Learn. Res.,
    12 (2011), pp. 2825-2830, 10.5555/1953048.2078195 Google Scholar [26] J.D. Evans,
    Straightforward statistics for the behavioral sciences. Thomson Brooks/Cole Publishing
    Co; 1996. Google Scholar [27] A. Steinert, I. Sattler, K. Otte, H. Röhling, S.
    Mansow-Model, U. Müller-Werdan Using new camera-based technologies for gait analysis
    in older adults in comparison to the established GAITRite system Sensors, 20 (2020),
    p. 125, 10.3390/s20010125 View in ScopusGoogle Scholar [28] Y.-R. Yang, Y.-Y.
    Lee, S.-J. Cheng, P.-Y. Lin, R.-Y. Wang Relationships between gait and dynamic
    balance in early Parkinson’s disease Gait Posture, 27 (2008), pp. 611-615, 10.1016/j.gaitpost.2007.08.003
    View PDFView articleView in ScopusGoogle Scholar [29] L.-F. Yeung, Z. Yang, K.-C.-C.
    Cheng, D. Du, R.-K.-Y. Tong Effects of camera viewing angles on tracking kinematic
    gait patterns using Azure Kinect, Kinect v2 and Orbbec Astra Pro v2 Gait Posture,
    87 (2021), pp. 19-26, 10.1016/j.gaitpost.2021.04.005 View PDFView articleGoogle
    Scholar [30] S. Fudickar, S. Hellmers, S. Lau, R. Diekmann, J.M. Bauer, A. Hein
    Measurement System for Unsupervised Standardized Assessment of Timed “Up & Go”
    and Five Times Sit to Stand Test in the Community—A Validity Study Sensors, 20
    (2020), p. 2824, 10.3390/s20102824 View in ScopusGoogle Scholar [31] T.K. Uchida,
    Delp SL. Biomechanics of Movement – The Science of Sports, Robotics and Rehabilitation.
    The MIT Press 2020. Google Scholar Cited by (0) 1 ORCID: 0000-0002-2853-2055 2
    ORCID: 0000-0002-8637-2942 3 ORCID: 0000-0002-9384-3474 4 ORCID: 0000-0001-8846-2282
    5 Interpretation of PCC values as suggested by Evans[26]: ● 0.00–0.19 – very weak
    ● 0.20–0.39 – weak ● 0.40–0.59 – moderate ● 0.60–0.79 – strong ● 0.80–1.00 – very
    strong. © 2023 The Authors. Published by Elsevier Ltd. Recommended articles Diagnosis
    of Community-Acquired pneumonia in children using photoplethysmography and Machine
    learning-based classifier Biomedical Signal Processing and Control, Volume 87,
    Part A, 2024, Article 105367 Kehkashan Kanwal, …, Aisha Ghazal Qurashi View PDF
    An objective approach to identifying individual atrial fibrillation triggers:
    A simulation study Biomedical Signal Processing and Control, Volume 87, Part A,
    2024, Article 105369 Vilma Pluščiauskaitė, …, Andrius Petrėnas View PDF ReliefF
    based feature selection and Gradient Squirrel search Algorithm enabled Deep Maxout
    Network for detection of heart disease Biomedical Signal Processing and Control,
    Volume 87, Part A, 2024, Article 105446 Balasubramaniam S, …, K Satheesh Kumar
    View PDF Show 3 more articles Article Metrics Captures Readers: 2 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Biomedical Signal Processing and Control
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Towards automated self-administered motor status assessment: Validation
    of a depth camera system for gait feature analysis'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chen Q.
  - Chen Z.
  - Zhang K.
  - Wang X.S.
  citation_count: '0'
  description: With the ever-increasing data volume and application diversity, a modern
    data analytics job is generally built as a workflow consisting of multiple tasks.
    For either specific functionalities or higher performance, tasks in a workflow
    may need to be deployed on different data processing platforms. This article proposes
    CLIC, a highly extensible system for efficient cross-platform data analytics.
    To leverage the advantage of diverse platforms while alleviating development efforts,
    we propose an embedding-based operator encoding scheme and a Graph Convolutional
    Network model for efficient platform selection. Aiming at flexibly integrating
    new operators and platforms, CLIC is designed with a highly extensible system
    architecture that decouples the core functionalities from backend platforms. Experiments
    show that CLIC can significantly improve the performance of modern data analysis
    workflows with fast platform selection.
  doi: 10.1109/TPDS.2023.3298038
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account
    Personal Sign In Browse My Settings Help Institutional Sign In All Books Conferences
    Courses Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals
    & Magazines >IEEE Transactions on Parallel... >Volume: 35 Issue: 1 CLIC: An Extensible
    and Efficient Cross-Platform Data Analytics System Publisher: IEEE Cite This PDF
    Qixiang Chen; Zhijun Chen; Kai Zhang; X. Sean Wang All Authors 228 Full Text Views
    Abstract Document Sections I. Introduction II. Background and Motivation III.
    System Overview IV. GCN-Based Platform Selection V. System Design and Implementation
    Show Full Outline Authors Figures References Keywords Metrics Footnotes Abstract:
    With the ever-increasing data volume and application diversity, a modern data
    analytics job is generally built as a workflow consisting of multiple tasks. For
    either specific functionalities or higher performance, tasks in a workflow may
    need to be deployed on different data processing platforms. This article proposes
    CLIC, a highly extensible system for efficient cross-platform data analytics.
    To leverage the advantage of diverse platforms while alleviating development efforts,
    we propose an embedding-based operator encoding scheme and a Graph Convolutional
    Network model for efficient platform selection. Aiming at flexibly integrating
    new operators and platforms, CLIC is designed with a highly extensible system
    architecture that decouples the core functionalities from backend platforms. Experiments
    show that CLIC can significantly improve the performance of modern data analysis
    workflows with fast platform selection. Published in: IEEE Transactions on Parallel
    and Distributed Systems ( Volume: 35, Issue: 1, January 2024) Page(s): 34 - 45
    Date of Publication: 24 July 2023 ISSN Information: DOI: 10.1109/TPDS.2023.3298038
    Publisher: IEEE I. Introduction Recent progresses in Big Data and artificial intelligence
    have significantly enhanced and energized data analytics. With diverse goals in
    performance and programming efficiency, data analytics platforms targeting different
    domains are constantly emerging. For instance, tasks such as data cleaning and
    filtering are generally performed on a DBMS or Big Data platforms like Spark [1];
    tasks applied on the graph structure such as relationship discovery are performed
    by GraphX or Giraph [2]; deep learning models are built and trained by platforms
    like Tensorflow [3] and Pytorch [4], to name a few. Besides the functionalities,
    a data processing platform may deliver much higher performance for specific tasks
    due to the specialized system design. Therefore, an efficient modern data analytics
    workflow is usually built as a sequence of tasks that are separately executed
    on multiple platforms to leverage their advantages [5], [6], [7]. Sign in to Continue
    Reading Authors Figures References Keywords Metrics Footnotes More Like This Quantum
    Machine Learning with Quantum Topological Data Analysis 2023 IEEE International
    Conference on Quantum Computing and Engineering (QCE) Published: 2023 Impact of
    Labeling Noise on Machine Learning: A Cost-aware Empirical Study 2022 21st IEEE
    International Conference on Machine Learning and Applications (ICMLA) Published:
    2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Parallel and Distributed Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'CLIC: An Extensible and Efficient Cross-Platform Data Analytics System'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mahjour B.
  - McGrath A.
  - Outlaw A.
  - Zhao R.
  - Zhang C.
  - Cernak T.
  citation_count: '0'
  description: Data science is becoming a mainstay in research. Despite this, very
    few STEM graduates matriculate with basic formal training in programming. The
    current lesson plan was developed to introduce undergraduates studying chemistry
    or biology to chemoinformatics and data science in medicinal chemistry. The objective
    of this lesson plan is to introduce students to common techniques used in analyzing
    medicinal chemistry data sets, such as visualizing chemical space, filtering to
    molecules that observe the Lipinski rules of drug-likeness, and principal component
    analysis. The content provided in this lesson plan is intended to serve as a tutorial-based
    reference for aspiring researchers. The lesson plan is split into two three-hour
    class sessions, each with an introductory slide deck, Python notebook consisting
    of several modules, and lab report template. During this activity, students learned
    to parse medicinal chemistry data sets with Python, perform simple machine learning
    analyses, and develop interactive graphs. During each session, students complete
    the Python notebook protocol and fill out a lab report template after a short
    lecture. By the end of the lesson plan, students were able to generate and manipulate
    various plots of chemical space and they reported having increased confidence
    in their understanding of chemistry, Python, and data science.
  doi: 10.1021/acs.jchemed.3c00357
  full_citation: '>'
  full_text: '>

    "ACS ACS Publications C&EN CAS Access provided byUNIV OF NEBRASKA - LINCOLN Log
    In Pair your account to your Institution Click the pair button to affiliate your
    institution with your personal account PAIR Turn off this notification more info...
    My Activity Publications ADVERTISEMENT RETURN TO ISSUEPREVACTIVITYNEXT Interactive
    Python Notebook Modules for Chemoinformatics in Medicinal Chemistry Babak Mahjour
    , Andrew McGrath , Andrew Outlaw , Ruheng Zhao , Charles Zhang , and Tim Cernak*
    Cite this: J. Chem. Educ. 2023, 100, 12, 4895–4902 Publication Date:November 28,
    2023 https://doi.org/10.1021/acs.jchemed.3c00357 Copyright © 2023 American Chemical
    Society and Division of Chemical Education, Inc. Request reuse permissions Article
    Views 1377 Altmetric 12 Citations - LEARN ABOUT THESE METRICS Share Add to Export
    RIS PDF (3 MB) Supporting Info (2)» Supporting Information SUBJECTS:Color,Genetics,Medicinal
    chemistry,Partition coefficient,Students Journal of Chemical Education Abstract
    Data science is becoming a mainstay in research. Despite this, very few STEM graduates
    matriculate with basic formal training in programming. The current lesson plan
    was developed to introduce undergraduates studying chemistry or biology to chemoinformatics
    and data science in medicinal chemistry. The objective of this lesson plan is
    to introduce students to common techniques used in analyzing medicinal chemistry
    data sets, such as visualizing chemical space, filtering to molecules that observe
    the Lipinski rules of drug-likeness, and principal component analysis. The content
    provided in this lesson plan is intended to serve as a tutorial-based reference
    for aspiring researchers. The lesson plan is split into two three-hour class sessions,
    each with an introductory slide deck, Python notebook consisting of several modules,
    and lab report template. During this activity, students learned to parse medicinal
    chemistry data sets with Python, perform simple machine learning analyses, and
    develop interactive graphs. During each session, students complete the Python
    notebook protocol and fill out a lab report template after a short lecture. By
    the end of the lesson plan, students were able to generate and manipulate various
    plots of chemical space and they reported having increased confidence in their
    understanding of chemistry, Python, and data science. This publication is licensed
    under the terms of your institutional subscription. Request reuse permissions.
    KEYWORDS:Upper-Division Undergraduate Python Chemoinformatics Computer-Based Learning
    Medicinal Chemistry Introduction ARTICLE SECTIONSJump To Chemoinformatics is the
    use of computational informatics techniques to solve problems in chemistry. These
    in silico methods can be used to transform data into information and aid in drug
    discovery. Recently, a rise in computational power and increased availability
    of software tools have made chemoinformatics an invaluable tool for research.
    Meanwhile, there has been recent interest in teaching young scientists how to
    work at the interface of physical science and data science. (1−6) In previous
    works, lesson plans have been developed incorporating programming into physical
    chemistry, (1,3) general chemistry, (4) analytical chemistry, (5,7) bioinformatics,
    (6) and other topics. (8−16) This paper extends the previous works through a lesson
    plan that introduces students to the basics of chemoinformatics in medicinal chemistry
    with the most popular scripting language, Python. Specifically, the exercises
    included in this activity are based on traditional techniques used by medicinal
    chemists to visualize and analyze chemical space. Objectives ARTICLE SECTIONSJump
    To The purpose of this experiment is to introduce fundamental chemoinformatics
    using Python through medicinal chemistry-based exercises. The modules teach the
    following: how to load compiled medicinal chemistry data sets that are suitable
    for sharing and analysis how to visualize chemical space in a multitude of ways
    how to filter chemical data sets based on drug-like properties how to validate
    medicinal chemistry principles using data visualization how to simplify multidimensional
    physicochemical properties of drugs using principal component analyses (PCA) Structure
    and Content ARTICLE SECTIONSJump To In this text, “lesson plan” refers to the
    entirety of the work and consists of two “class sessions”. Each “class session”
    consists of a brief lecture, to be administered at the beginning of the class
    session, a Python notebook consisting of several “modules” to be completed by
    students, and a lab report to be completed and submitted for a grade. As mentioned,
    this lesson plan is executed over two separate three-hour class sessions, supervised
    by one or two graduate student teaching assistants with some familiarity with
    Python. Students participating in this activity are expected to have familiarity
    with basic concepts of medicinal chemistry, such as how structures impact drug-like
    properties. Classes typically comprised 20–40 students. Each class session consists
    of a brief slide deck and lecture introducing the basics of Python and its capabilities,
    presented to the students, followed by an interactive Python notebook, composed
    of multiple modules. Additionally, there is a preliminary 90-minute lecture given
    in a classroom setting introducing chemoinformatic and drug design concepts such
    as chemical space, partition coefficient (LogP), Lipinski rules, molecular fingerprints,
    chemical diversity, and qualitative structure activity relationships. In this
    lecture students learn how drugs are classified by the “Lipinski rule of five”
    and how commercial drugs may (Figures 1 and 2) or may not (Figures 1, 3, and 4)
    meet these guidelines. Alongside this preliminary lecture are two recitation exercises,
    where students review and summarize papers from the literature describing the
    quantitative estimate of drug-likedness (QED) score (17) and the central nervous
    system multiparameter optimization (CNS-MPO) tool (18)─chemoinformatic concepts
    that are applied in the subsequent laboratories. Figure 1 Figure 1. Medicinal
    chemistry concepts taught in a preliminary lecture in a classroom setting before
    the lab. (A) Examples of drugs passing the Lipinski rules. Radar plots display
    the physicochemical properties of the adjacent molecule. Values within the green
    concentric circle are within the rule of five threshold (except for the QED score).
    (B) Examples of drugs failing the Lipinski rules. Some properties break through
    the rule of five threshold shown in the radar plots. (C) Scatter plot of molecular
    weight (MW) versus partition coefficient (LogP, a dimensionless quantity representing
    a molecule’s distribution between hydrophobic and aqueous environments) for the
    drugs shown, where each point is sized by the QED. The green boundary demarcates
    the “Lipinski rule of five”. HBA = hydrogen bond acceptor; HBD = hydrogen bond
    donor. Figure 2 Figure 2. (A) A self-contained code template generates several
    plots and is provided to students. Several lines are deactivated using the “#”
    symbol, as indicated by the teal text. Lines of code can be reactivated by removing
    the leading “#”, resulting in the code producing a different visual output when
    executed. “asdf” and “qwer” represent the data set for the following plots. (B)
    The plot generated by the code in panel A when run as is. The three plotted points
    are defined in the code template, connecting the code to its visual output. Later
    in the lesson plan, code is introduced to include axis labels. (C) The plot generated
    when line 8 of panel A is activated, resulting in larger points. (D) The plot
    generated when line 9 is also activated, resulting in different point colors.
    (E) The plot generated when line 11 is activated, creating the same scatter plot
    with a blue background. (F) A potential plot generated after students are instructed
    to include an additional parameter to the scatter function. In this case, the
    shapes of the points are changed. Figure 3 Figure 3. Students are provided with
    the code to import any tabular json file. The utility of the package Pandas is
    used in reading tables programmatically. The “alldrugsprops.json” file is provided
    to students and provided with a template to import the data into Python, as seen
    in the input block. Students then inspect the contents of the file with Python,
    revealing a data file of over 9,000 drugs and their properties downloaded from
    DrugBank. Figure 4 Figure 4. Four chemoinformatic experiments run by students
    during the first session. In each graph, over 9,000 drugs are graphed onto scatter
    plots using calculated properties of the drug. Students learn to change the axes
    and colors of the plot, exposing them to a strategy to rapidly investigate chemical
    space and generate reports in Python. (A) Plot of LogP (a dimensionless quantity
    representing a molecule’s distribution between hydrophobic and aqueous environments)
    against the number of hydrogen bond donors (HBD), colored by the number of aromatic
    bonds (AROM). (B) Plot of fraction sp3 (FSP3, the ratio of sp3 hybridized atoms
    to all atoms) against the number of rotatable bonds (ROTB), colored by the number
    of hydrogen bond acceptors (HBA). (C) Plot of LogP against aLogP (a similar quantity
    to LogP representing a molecule’s lipophilic nature but calculated differently
    with a focus on the molecules constituent atoms), colored by quantitated estimate
    of druglikedness (QED). (D) Plot of formal charge (FC) against FSP3, colored by
    number of hydrogen bond donors. These notebooks are written in Google Colaboratory
    (Colab), (19) an easily accessible online Python environment that executes code
    on the cloud for free, based on the popular Jupyter software. (20) Colab has several
    important data science packages preinstalled, including Pandas, (21) Numpy, (22)
    SciPy, (23) and Matplotlib (24)─Python add-ons that simplify the manipulation
    and visualization of data. Its primary advantages here are allowing a fast and
    simple way for new students to get started coding, as it is agnostic of computer
    and operating system and requires no technical setup. Code is separated into blocks
    called “cells”, which execute independently of each other. The two Colab notebooks
    are organized by modules, each with specific objectives and corresponding questions
    in the notebook’s respective lab report, and walk the students through various
    exercises to meet the teaching objectives. Each notebook is to be completed alongside
    the provided lab report templates consisting of module-specific questions and
    discussion items. Graduate student teaching assistants provide guidance and feedback
    to students during the class. Class Session 1: Introduction to Colab, Python,
    and Chemoinformatics ARTICLE SECTIONSJump To In the first class session, an initial
    slide deck is presented by the graduate student teaching assistants explaining
    the increasing popularity of scripting languages and their use in industry and
    academia. Several examples of data visualizations generated by Python are shared
    with the students (Figure S1). Python as a scripting language is then formally
    introduced,introduced as well as Colab. Students are shown how to execute print
    (“hello world”) and are encouraged to log into Colab from their computer and run
    a line of code. Lists and dictionaries are introduced as two basic data structures.
    If statements and for loops are introduced through an example of their use in
    filtering a list of dictionaries. Finally, it is shown how the Python package
    Pandas (21) can be used to load tabulated data sets from comma-separated value
    (CSV) or JavaScript Object Notation (JSON) files. Students then complete the first
    Python notebook and its corresponding lab report template after the brief (∼15
    minute) lecture in the remaining class time with graduate student supervision.
    The notebook given in the initial class session introduces students to Google
    Colab, plotting in Python, and basic chemoinformatic concepts. The objectives
    of this notebook are to learn basic Python coding and to quickly load and plot
    chemoinformatic data from spreadsheets or other data formats. The module also
    exemplifies how to customize plots generated in Python. The first module walks
    through basic plotting in Python. The students receive a code template (Figure
    2A, Figure S2), which generates a plot when executed in the notebook. Several
    lines have been “commented out”, and thus deactivated, as indicated by lines containing
    green text and beginning with a #. Students are instructed to run the script,
    note what happens, then reactivate a line of code (by deleting the # symbol),
    and run the script again. This approach is intended to build familiarity with
    Colab and to exemplify how certain lines of code affect the script’s output. As
    with the code template, most Python scripts begin with a list of package imports,
    lines of code that allow prewritten code to be used succinctly in the current
    script and always contain the keyword import. For instance, to avoid writing the
    code to render plots from scratch, Python is instructed to import the Pyplot module
    from the package Matplotlib, a well maintained and versatile graphing software.
    As seen in the first line of the code template, a package is imported through
    specific syntax: import <package_name> or import <package_name> as <alias>. In
    the first syntax, future references to the package must use the exact package
    name written in the import statement. In the second syntax, the as keyword instructs
    Python to allow the coder to refer to the package as plt, a commonly used short-hand
    alias for the Pyplot module of Matplotlib that represents the package in code.
    At this point onward in the code, functions (reusable blocks of code that perform
    a specific task or set of tasks when executed) provided by the imported package
    can be utilized by “calling” them, which is done by appending parentheses ( )
    to the function name─for example, plt.subplots( ), where the period between plt
    and subplots indicates that subplots is a function defined in the module plt (the
    short-hand alias given for the Pyplot module of Matplotlib). Variables are pointers
    to objects (data stored in computer memory) that are named by the user and can
    reference data or the results of previously written code in other files or installed
    packages. Variables are subsequently referred to in code by their given name.
    In the following two lines, two lists are instantiated. Lists are Python objects
    that store data in a fixed order, similar to a row in a spreadsheet, and can be
    referred to by variables. The first list is referred to by a variable named asdf
    and stores the integers 1, 2, and 3, in that order. The second list is named qwer
    and stores the integers 3, 5, and 6 in that order. In the following line of code,
    the variables fig and ax are instantiated and assigned to the output of the function
    plt.subplots( ). The response of this function when called is captured in the
    variables fig and ax, as named by the user, and are later referred to in the code
    to draw and save a plot. The portion of the response that is stored in fig is
    described as “the top-level container for all the plot elements” in its documentation.
    (25) In essence, the variable fig now represents the digital data structure, stored
    in the computer’s random access memory, that will eventually render into a visual
    image. Similarly, ax represents an abstract object that is used in the code to
    draw plots. The scatter( ) function, found in the object represented by the variable
    ax, can be provided data to generate a scatter plot. As with before, the function
    is called by appending parentheses to the name of the function, ax.scatter( ).
    Data is “passed” to the function as input through what is written between the
    parentheses of the function call, otherwise known as the “parameters” of the function.
    In the case of the code template, four lines of code are written in-between the
    parentheses, each representing a parameter to the scatter function. Syntactically,
    parameters must be separated by commas, and new lines between parameters are optional
    but encouraged for legibility. The first parameter is passed in line six, where
    it is specified to the function that the x axis data are equal to the list of
    data that is represented by the variable asdf. In the following line, the y data
    are set to qwer. The final two lines of parameters are commented out. The scatter
    function requires both x axis data and y axis data to be provided. The remaining
    parameters are optional. The final line of code is plt.show( ), which instructs
    the notebook to render the plot. Thus, running the code as given generates the
    scatter plot shown in Figure 2B. By uncommenting the line # s = 275, students
    reactivate the size parameter of the scatter plot function. Rerunning the code
    now generates the plot shown in Figure 2C. Reactivating the color parameter creates
    the plot in Figure 2D, which changes the color of the markers in the plot. Students
    then change the background color of the plot by using the set_facecolor( ) function
    provided by ax. By uncommenting the line of code, a hexadecimal color code is
    passed as a parameter to the function, and the plot of Figure 2E is produced.
    In the module, students are provided the documentation to the scatter function
    (26) and are asked to add an additional parameter to change the shape of the points,
    generating a plot such as Figure 2F. This simple plotting exercise is designed
    to give students at a minimum a new skill of plotting: while few students in the
    class are likely to go on to a career in data science, nearly all of them will
    need to plot data in many diverse settings, and this learning module complements
    what they would generally perform in the Microsoft Excel software. To expand on
    module one, the second module guides the students through creating a plot of a
    large data set. The data file is a json file and contains a list of dictionaries
    (similar to a list, but in Python dictionaries, data is stored via key:value pairs,
    where the value is retrieved by passing the key to the Python dictionary). Each
    Python dictionary in the list contains information regarding a drug in DrugBank,
    (27) containing its SMILES string (“Simplified Molecular Input Line Entry System”,
    a text encoding of molecules readable by the machine commonly used to store chemicals
    in a data set) (28) as well as precalculated physicochemical properties. Students
    are instructed to upload their data file into Colab (Figure S3). With the provided
    code, students are then instructed to import the data contained in the given file
    using the json package and to convert the data into a DataFrame, a data structure
    representation provided by the Pandas package, allowing for spreadsheet-like manipulation
    of the data set (Figure S4). Using a for loop, ten dictionaries from the json
    file are added to a list, and the pd.DataFrame( ) function is used to store the
    data as a DataFrame. Students then print the contents of the DataFrame and inspect
    the contents of the json file (Figure 3, Figure S5). Finally, another graphing
    template is provided, and the students must execute it to plot the drugs by their
    properties in an x–y scatter plot. In this template, a for loop is used to iterate
    through the DataFrame and store the LogP, polar surface area (PSA), and QED of
    each drug into respective initialized lists. Then, the scatter function is used
    to plot the drugs on a graph with LogP on the x-axis and PSA on the y-axis (Figure
    S6). Each point is then colored by QED using the color parameter (c). This exercise
    helps to visualize the Lipinski properties and concepts of drug-likeness, while
    visualizing a large chemical space. In the third module, the student uses the
    code provided in module 2 to generate another plot that visualizes the uploaded
    DrugBank data but with a different color than before. In this module, additional
    physicochemical properties are introduced. Common parameters of druglikedness
    are included in the data set such as LogP, PSA, number of aromatic rings, number
    of hydrogen bond donors (HBD), and number of hydrogen bond acceptors (HBA). The
    full list of included properties is listed in the Supporting Information. Concepts
    such as variables (user specified names that represent values stored in the computer’s
    memory) and f-strings (a feature in Python that allows values to be embedded in
    text) are explicitly introduced here. The user can set the color variable to a
    column header from the DataFrame. Through a function parameter, the color of the
    plot is automatically updated, and the title of the output figure is modified
    through a separate function. In the final module, students are asked to swap the
    data represented by the axes of the plots, which can be easily performed by switching
    the DataFrame header referenced in the code. The student also has the option to
    investigate a third dimension by modifying the color of the plotted points. In
    the culmination of this notebook, students are now comparing trends of physicochemical
    properties by modifying the x-axis, y-axis, and color attributes. Several plots
    generated by students are showcased in Figure 4. This notebook concludes with
    the following questions, which the students answer in their lab report: 1 What
    are the Lipinski rules? 2 Write code for filtering to drugs that pass all the
    Lipinski rules 3 Suggest a research question that you could ask of the DrugBank
    data set These discussion questions evaluate student understanding of medicinal
    chemistry concepts and their ability to manipulate data sets using basic Python
    code. Given the importance of the online Python community, it is expected and
    encouraged that students will utilize Internet searches to assist in the writing
    of filter code to answer the second question. This emphasizes the importance of
    independent problem solving when encountering unfamiliar bugs or errors when coding.
    Filters can be written manually with if statements and for loops, as explained
    in the preliminary presentation, or by using a function included in the Pandas
    package. Class Session 2: Principal Component Analysis ARTICLE SECTIONSJump To
    The second class session begins with another short lecture. In this lecture, the
    concept of machine-readable molecular representations is introduced through SMILES.
    Box plots are shown as a way to visualize molecular data sets via distributions
    of Lipinski (29) and other relevant medicinal chemistry properties. Finally, histograms
    and principal component analyses (PCA) are introduced. The second notebook and
    its lab report are then completed after the lecture. The notebook provided with
    this class session utilizes a data set from the COVID Moonshot project. (30) Inhibition
    data against the SARS-COV-2 main protease (MPro) alongside precalculated physicochemical
    properties and SMILES data for various inhibitors are included in the provided
    CSV file. The learning objectives of this notebook are to filter out unusable
    data, use data visualization to validate medicinal chemistry principles, perform
    basic statistical analyses, and simplify multidimensional data using PCA. In the
    first module, students are provided with the Moonshot CSV and are instructed to
    load and inspect the CSV using the Pandas package (Figure S7, Figure S8). Some
    lines are missing data, and the concept of cleaning data sets is introduced. Using
    Pandas, entries without IC50 values are then filtered, and students are asked
    to record the number of remaining molecules in their report. In the second module,
    the students are asked to validate GlaxoSmithKline’s (GSK) Solubility Forecast
    Index (SFI), (31) which predicts aqueous solubility based on a compound’s polarity
    (logP or logD) and number of aromatic rings (AROM), using the filtered data set
    from the previous module (Figure S9). The students are directed to use the template,
    plotting code to show how the solubility is affected by the number of aromatic
    rings and logP (Figure 5). In this case, the template script is missing lines
    of code, requiring students to fill in the missing portions themselves (Figure
    S10). This requires students to correctly plot certain properties from the data
    set, which is possible given an understanding of the components in the index and
    the script template, directly building on the concepts of the first lab exercise.
    By this point, students should be comfortable loading data sets into Python and
    creating and styling plots based on column headers and have growing familiarity
    with visualizing and exploring chemical space via relevant properties. Figure
    5 Figure 5. (A) The template plotting code is incrementally improved until it
    can be used effectively to make manuscript-ready graphics. This code creates a
    scatter plot of the Moonshot compounds provided in the CSV, where the x-axis is
    the number of aromatic rings (AROM) and the y-axis is logP. (B) By coloring the
    points by the compound’s reported experimental solubility, a trend is revealed
    where compounds with fewer aromatic rings and lower logP are more soluble. Validating
    GSK’s Solubility Forecast Index is a simple experiment to allow students to build
    confidence in their ability to manipulate and analyze data sets. Also in this
    module, students are instructed to make box plots of various properties encoded
    in the data file, providing an alternative visualization of the data set. The
    code template is provided, and students are asked to analyze and modify the code
    to reshape the grid of box plots. Students are encouraged to improve the plots
    aesthetically and to practice modifying the data that are plotted by changing
    the variables. Here the ability to plot multiple distributions into the same figure
    using Matplotlib’s subplots function is explicitly introduced. As molecules can
    have many descriptors, multiplexed figures are powerful in evaluating medicinal
    chemistry data sets. In the third module, the multidimensional scaling PCA is
    performed on the data set. The utility of a PCA is explained to the students within
    the lecture slides as a method to combine multiple features of a data point into
    fewer features, while the math is omitted, as it is beyond the scope of this medicinal
    chemistry course. Students are instructed to create a matrix containing the Lipinski
    physicochemical properties for each entry in the data set, resulting in a DataFrame
    with five columns and a row for each compound. Using the provided template code
    (Figure S11), the students feed their matrix into scikit-learn’s PCA decomposition
    algorithm, where the matrix is reduced to two dimensions. The resultant data are
    then plotted and colored by a property. Students are subsequently instructed to
    filter out non-Lipinski compliant molecules, rerun the dimensionality reduction,
    and compare the final graphs (Figure 6). To complete this objective, students
    must apply what they have learned in the previous modules or in the lecture notes
    to write a for loop and use if statements to collect data points that comply with
    the Lipinski rules. Alternatively, students are encouraged to search for Pandas
    documentation and to perform the data filtration using a Pandas function. In this
    module students begin to become familiarized with dimensionality reduction and
    the identification of features that lead to data set variance. Figure 6 Figure
    6. By the end of the modules, students have implemented a data filter and a principal
    component analysis on a data set of SARS-COV-2 main protease inhibitors. (A) The
    PCA before the data filter is applied. Points represent DrugBank compounds and
    are colored by their molecular weight. By changing the color of the points, the
    correlation of the reduced features can be mapped to specific features of the
    data set. In this instance, the molecular weight tracks with the first principal
    component, increasing as the x value increases. (B) The PCA plot after the Lipinski
    filter is applied. Outliers are removed by the filter, and the distribution of
    molecular weights in the first principal component becomes wider. The direction
    of the color gradient has changed as the sign of the principal components is arbitrarily
    assigned. In the final module, students use the filtered data set and are introduced
    to a new package that allows for the creation of interactive plots. Using the
    template code, students use the package plotly to generate an interactive PCA
    that displays SMILES and other information for each plotted entry. Plotly works
    similarly to the Matplotlib package but instead produces an interactive scatter
    plot, where data points can be hovered over with the cursor to inspect additional
    information. Students are asked to engage with the interactive scatter plot, record
    several of molecules from different clusters, and answer a question about sampled
    molecules from various clusters (Figure S12). Participants ARTICLE SECTIONSJump
    To The participants in this lesson plan were students enrolled in a senior level
    undergraduate medicinal chemistry course. Through informal in-class surveys, it
    was found that nearly all participants had little to no previous coding experience
    and that these modules were their first introduction to a hands-on coding activity.
    This lesson plan was developed and conducted over five years of students, in an
    in-person and remote format. In the first two years, only the first module was
    taught. Over 100 students have participated in the class. Implementation ARTICLE
    SECTIONSJump To This lesson plan was given to undergraduate classes of pharmaceutical
    science majors in mid-semester of their senior year, after several units introducing
    basic medicinal chemistry concepts are completed. In this case, each class consisted
    of around 30 students and was supervised by two graduate student teaching assistants.
    As mentioned, the lesson plan is split into two three-hour class sessions. At
    the beginning of each class session, graduate student teaching assistants present
    a lecture and slide deck with basic coding and Python concepts before the students
    begin working on the notebook corresponding to the class session (notebook 1 for
    the first class, notebook 2 for the second class). The initial implementation
    of this activity was done without Colab, with tutorials provided to install Python
    on each student’s personal computers. The diverse operating systems and security
    settings of each individual student’s laptop required additional installation
    guidance and troubleshooting for a handful of students each year. With the introduction
    of Colab to the academic community, the onboarding process for the activity was
    greatly simplified, as it became guaranteed any student could complete the activities
    on their own computer agnostic of hardware or software. During the class sessions,
    graduate student teaching assistants are available for troubleshooting and questions.
    In this implementation, the most common problem students encountered was syntax
    issues in their code. While the templates and instructions provided were meant
    to minimize syntax errors, it was beneficial for the graduate student teaching
    assistants to have familiarity with common Python error messages to quickly identify
    the source of the bug and to direct the students to correct the invalid syntax.
    Students are permitted to work in groups to complete the exercises, and most students
    were able to independently complete all of the exercises without supervision or
    additional guidance from an instructor. Assessment of Effectiveness ARTICLE SECTIONSJump
    To As of 2023, nearly all senior undergraduates who have enrolled in the class
    have reported having little to no experience in coding or programming prior to
    taking this lab exercise. Throughout the lesson plan, students submitted plots
    they had generated to the graduate student teaching assistants alongside their
    code to be graded for accuracy. By the end of the two classes, each student was
    able to successfully use the code to generate various plots of chemical space
    that were correctly labeled. All students received a passing grade on their submitted
    lab reports, which were assessed for competency by diverse (graduate student)
    teaching assistants, with students able to produce plots from text prompts and
    no provided template after completing this exercise. Furthermore, after the lab
    sessions, students from the last two years’ labs were asked to complete the following
    five-question survey: 1. This exercise improved my understanding of chemistry
    2. This exercise improved my understanding of Python 3. This exercise improved
    my understanding of chemical space 4. This exercise improved my data science 5.
    I enjoyed this exercise Based on the responses received and the high marks earned
    by the students on their lab reports, it is concluded that the current state of
    the module is effective in improving the understanding of basic data science and
    informatics in medicinal chemistry for most students (Figure 7). Figure 7 Figure
    7. Bar charts showing student feedback on the lesson plan collected after the
    lab. Questions were intended to gauge the student’s perspective on their learning
    experience. In the first year, students felt strongly that their understanding
    of Python, chemical space, and data science was improved. In the second year,
    students felt strongly that their understanding of data science improved but were
    neutral on other learning objectives. The difference between the two years may
    be accounted for by the level of involvement the graduate student teaching assistance
    had in developing the course. In the first year, the class was taught by the writer
    of the lesson plan; the second year was taught by assistants who were given the
    lesson plan to teach. Summary ARTICLE SECTIONSJump To A lesson plan to teach undergraduates
    the basics of data science in medicinal chemistry was developed and validated
    over several semesters. Over the course of two lab sessions, students are introduced
    to Python, Google Colaboratory, and several Python packages. Students learn these
    tools through guided, interactive modules that begin at learning how to use Colab
    and end with developing a program that reads abstractable data sets and generates
    user-interactive data analytics through Python. An assessment reveals that the
    lesson plan seems to be effective in improving students’ familiarity with modern
    chemoinformatic tools and concepts. It is postulated that the lesson plan may
    be effective for students even earlier in their careers and can be completed by
    anyone with computer and Internet access. Supporting Information ARTICLE SECTIONSJump
    To The Supporting Information is available at https://pubs.acs.org/doi/10.1021/acs.jchemed.3c00357.
    All lesson plan details including the introductory slide decks, Colab notebooks,
    and lab report templates are provided, as well as figure components and code,
    in the associated GitHub repository (https://github.com/cernaklab/jchemed-chemoinformatics).
    Protocols and report templates are provided in the Supporting Information (PDF,
    DOCX) ed3c00357_si_001.pdf (567.47 kb) ed3c00357_si_002.docx (994.83 kb) Terms
    & Conditions Most electronic Supporting Information files are available without
    a subscription to ACS Web Editions. Such files may be downloaded by article for
    research use (if there is a public use license linked to the relevant article,
    that license may permit other uses). Permission may be obtained from ACS for other
    uses through requests via the RightsLink permission system: http://pubs.acs.org/page/copyright/permissions.html.
    Author Information ARTICLE SECTIONSJump To Corresponding Author Tim Cernak - Department
    of Medicinal Chemistry, College of Pharmacy, University of Michigan, Ann Arbor,
    Michigan 48109, United States;  Department of Chemistry, University of Michigan,
    Ann Arbor, Michigan 48109, United States;  https://orcid.org/0000-0001-5407-0643;  Email:
    tcernak@med.umich.edu Authors Babak Mahjour - Department of Medicinal Chemistry,
    College of Pharmacy, University of Michigan, Ann Arbor, Michigan 48109, United
    States;  https://orcid.org/0000-0002-8225-6514 Andrew McGrath - Department of
    Medicinal Chemistry, College of Pharmacy, University of Michigan, Ann Arbor, Michigan
    48109, United States;  https://orcid.org/0000-0001-9275-0017 Andrew Outlaw - Department
    of Medicinal Chemistry, College of Pharmacy, University of Michigan, Ann Arbor,
    Michigan 48109, United States Ruheng Zhao - Department of Medicinal Chemistry,
    College of Pharmacy, University of Michigan, Ann Arbor, Michigan 48109, United
    States Charles Zhang - Department of Medicinal Chemistry, College of Pharmacy,
    University of Michigan, Ann Arbor, Michigan 48109, United States Notes The authors
    declare no competing financial interest. Acknowledgments ARTICLE SECTIONSJump
    To The University of Michigan College of Pharmacy is thanked for instructional
    funds and guidance. B.M. thanks the American Chemical Society Medicinal Chemistry
    Division for an ACS MEDI Fellowship. T.C. thanks the National Science Foundation
    (CHE-2236215) for funding. References ARTICLE SECTIONSJump To This article references
    31 other publications. 1Bravenec, A. D.; Ward, K. D. Interactive Python Notebooks
    for Physical Chemistry. J. Chem. Educ. 2023, 100 (2), 933– 940,  DOI: 10.1021/acs.jchemed.2c00665
    Google Scholar 2Lafuente, D.; Cohen, B.; Fiorini, G.; García, A. A.; Bringas,
    M.; Morzan, E.; Onna, D. A Gentle introduction to machine learning for chemists:
    an undergraduate workshop using python notebooks for visualization, data processing,
    analysis, and modeling. J. Chem. Educ. 2021, 98 (9), 2892– 2898,  DOI: 10.1021/acs.jchemed.1c00142
    Google Scholar 3van Staveren, M. Integrating Python into a Physical Chemistry
    Lab. J. Chem. Educ. 2022, 99 (7), 2604– 2609,  DOI: 10.1021/acs.jchemed.2c00193
    Google Scholar 4Weiss, C. J. A creative commons textbook for teaching scientific
    computing to chemistry students with python and Jupyter notebooks. J. Chem. Educ.
    2021, 98 (2), 489– 494,  DOI: 10.1021/acs.jchemed.0c01071 Google Scholar 5Menke,
    E. J. Series of Jupyter Notebooks Using Python for an Analytical Chemistry Course.
    J. Chem. Educ. 2020, 97 (10), 3899– 3903,  DOI: 10.1021/acs.jchemed.9b01131 Google
    Scholar 6Gupta, Y. M.; Kirana, S. N.; Homchan, S.; Tanasarnpaiboon, S. Teaching
    Python programming for bioinformatics with Jupyter notebook in the Post-COVID-19
    era. Biochem. Mol. Biol. 2023, 51, 537,  DOI: 10.1002/bmb.21746 Google Scholar
    7De Haan, D. O.; Schafer, J. A.; Gillette, E. I. Using a Modular Approach to Introduce
    Python Coding to Support Existing Course Learning Outcomes in a Lower Division
    Analytical Chemistry Course. J. Chem. Educ. 2021, 98 (10), 3245– 3250,  DOI: 10.1021/acs.jchemed.1c00456
    Google Scholar 8Dickson-Karn, N. M.; Orosz, S. Implementation of a Python Program
    to Simulate Sampling. J. Chem. Educ. 2021, 98 (10), 3251– 3257,  DOI: 10.1021/acs.jchemed.1c00597
    Google Scholar 9Green, M.; Chen, X. Data Functionalization for Gas Chromatography
    in Python. J. Chem. Educ. 2020, 97 (4), 1172– 1175,  DOI: 10.1021/acs.jchemed.9b00818
    Google Scholar 10Srnec, M. N.; Upadhyay, S.; Madura, J. D. A Python Program for
    Solving Schrödinger’s Equation in Undergraduate Physical Chemistry. J. Chem. Educ.
    2017, 94 (6), 813– 815,  DOI: 10.1021/acs.jchemed.7b00003 Google Scholar 11Wallum,
    A.; Liu, Z.; Lee, J.; Chatterjee, S.; Tauzin, L.; Barr, C. D.; Browne, A.; Landes,
    C. F.; Nicely, A. L.; Gruebele, M. An Instrument Assembly and Data Science Lab
    for Early Undergraduate Education. J. Chem. Educ. 2023, 100 (5), 1866– 1876,  DOI:
    10.1021/acs.jchemed.2c01072 Google Scholar 12Weiss, C. J. Scientific Computing
    for Chemists: An Undergraduate Course in Simulations, Data Processing, and Visualization.
    J. Chem. Educ. 2017, 94 (5), 592– 597,  DOI: 10.1021/acs.jchemed.7b00078 Google
    Scholar 13Chen, E.; Asta, M. Using Jupyter Tools to Design an Interactive Textbook
    to Guide Undergraduate Research in Materials Informatics. J. Chem. Educ. 2022,
    99 (10), 3601– 3606,  DOI: 10.1021/acs.jchemed.2c00640 Google Scholar 14Thrall,
    E. S.; Lee, S. E.; Schrier, J.; Zhao, Y. Machine Learning for Functional Group
    Identification in Vibrational Spectroscopy: A Pedagogical Lab for Undergraduate
    Chemistry Students. J. Chem. Educ. 2021, 98 (10), 3269– 3276,  DOI: 10.1021/acs.jchemed.1c00693
    Google Scholar 15Konkol, J. A.; Tsilomelekis, G. Porchlight: An Accessible and
    Interactive Aid in Preprocessing of Spectral Data. J. Chem. Educ. 2023, 100 (3),
    1326– 1332,  DOI: 10.1021/acs.jchemed.2c00812 Google Scholar 16Engelberger, F.;
    Galaz-Davison, P.; Bravo, G.; Rivera, M.; Ramírez-Sarmiento, C. A. Developing
    and Implementing Cloud-Based Tutorials That Combine Bioinformatics Software, Interactive
    Coding, and Visualization Exercises for Distance Learning on Structural Bioinformatics.
    J. Chem. Educ. 2021, 98 (5), 1801– 1807,  DOI: 10.1021/acs.jchemed.1c00022 Google
    Scholar 17Bickerton, G. R.; Paolini, G. V.; Besnard, J.; Muresan, S.; Hopkins,
    A. L. Quantifying the chemical beauty of drugs. Nat. Chem. 2012, 4 (2), 90– 98,  DOI:
    10.1038/nchem.1243 Google Scholar 18Wager, T. T.; Hou, X.; Verhoest, P. R.; Villalobos,
    A. Central Nervous System Multiparameter Optimization Desirability: Application
    in Drug Discovery. Biochem. Mol. Biol. 2016, 7 (6), 767– 775,  DOI: 10.1021/acschemneuro.6b00029
    Google Scholar 19Bisong, E. Google Colaboratory. Building Machine Learning and
    Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners;
    Apress: Berkeley, CA, 2019; pp 59 64. DOI: 10.1007/978-1-4842-4470-8_7 Google
    Scholar 20Kluyver, T.; Ragan-Kelley, B.; Pérez, F.; Granger, B. E.; Bussonnier,
    M.; Frederic, J.; Kelley, K.; Hamrick, J. B.; Grout, J.; Corlay, S. Jupyter Notebooks-a
    publishing format for reproducible computational workflows. Positioning and Power
    in Academic Publishing: Players, Agents and Agendas; IOS Press, 2016; pp 98– 90
    DOI: 10.3233/978-1-61499-649-1-87 Google Scholar 21McKinney, W. Data structures
    for statistical computing in python. Proc. Python Sci. Conf. 2010, 445, 51– 56,  DOI:
    10.25080/Majora-92bf1922-00a Google Scholar 22Harris, C. R.; Millman, K. J.; van
    der Walt, S. J.; Gommers, R.; Virtanen, P.; Cournapeau, D.; Wieser, E.; Taylor,
    J.; Berg, S.; Smith, N. J.; Kern, R.; Picus, M.; Hoyer, S.; van Kerkwijk, M. H.;
    Brett, M.; Haldane, A.; del Río, J. F.; Wiebe, M.; Peterson, P.; Gérard-Marchant,
    P.; Sheppard, K.; Reddy, T.; Weckesser, W.; Abbasi, H.; Gohlke, C.; Oliphant,
    T. E. Array programming with NumPy. Nature 2020, 585 (7825), 357– 362,  DOI: 10.1038/s41586-020-2649-2
    Google Scholar 23Virtanen, P.; Gommers, R.; Oliphant, T. E.; Haberland, M.; Reddy,
    T.; Cournapeau, D.; Burovski, E.; Peterson, P.; Weckesser, W.; Bright, J. SciPy
    1.0: fundamental algorithms for scientific computing in Python. Nat. Methods 2020,
    17 (3), 261– 272,  DOI: 10.1038/s41592-019-0686-2 Google Scholar 24Hunter, J.
    D. Matplotlib: A 2D Graphics Environment. Comput. Sci. Eng. 2007, 9 (3), 90– 95,  DOI:
    10.1109/MCSE.2007.55 Google Scholar 25 Matplotlib. Matplotlib Figure Documentation.
    https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure (accessed
    Jan 2023). Google Scholar 26 Matplotlib. Matplotlib Scatter Documentation. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html
    (accessed Jan 2023). Google Scholar 27Wishart, D.; Knox, C.; Guo, A.; Shrivastava,
    S.; Hassanali, M.; Stothard, P.; Chang, Z.; Woolsey, J. Drugbank: a comprehensive
    resource for in silico drug discovery and exploration. Nucleic Acids Res. 2006,
    34, D668– 672,  DOI: 10.1093/nar/gkj067 Google Scholar 28Weininger, D. SMILES,
    a chemical language and information system. 1. Introduction to methodology and
    encoding rules. J. Chem. Inf. Comput. Sci. 1988, 28 (1), 31– 36,  DOI: 10.1021/ci00057a005
    Google Scholar 29Lipinski, C. A. Lead- and drug-like compounds: the rule-of-five
    revolution. Drug Discovery Today Technol. 2004, 1 (4), 337– 341,  DOI: 10.1016/j.ddtec.2004.11.007
    Google Scholar 30Boby, M. L. Open science discovery of potent noncovalent SARS-CoV-2
    main protease inhibitors. Science 2023, 382, eabo7201  DOI: 10.1126/science.abo7201
    Google Scholar 31Hill, A. P.; Young, R. J. Getting physical in drug discovery:
    a contemporary perspective on solubility and hydrophobicity. Drug Discovery Today
    2010, 15 (15–16), 648– 655,  DOI: 10.1016/j.drudis.2010.05.016 Google Scholar
    Cited By ARTICLE SECTIONS Jump To This article has not yet been cited by other
    publications. Download PDF close the sidebar. Abstract Figure 1 Figure 1. Medicinal
    chemistry concepts taught in a preliminary lecture in a classroom setting before
    the lab. (A) Examples of drugs passing the Lipinski rules. Radar plots display
    the physicochemical properties of the adjacent molecule. Values within the green
    concentric circle are within the rule of five threshold (except for the QED score).
    (B) Examples of drugs failing the Lipinski rules. Some properties break through
    the rule of five threshold shown in the radar plots. (C) Scatter plot of molecular
    weight (MW) versus partition coefficient (LogP, a dimensionless quantity representing
    a molecule’s distribution between hydrophobic and aqueous environments) for the
    drugs shown, where each point is sized by the QED. The green boundary demarcates
    the “Lipinski rule of five”. HBA = hydrogen bond acceptor; HBD = hydrogen bond
    donor. Figure 2 Figure 2. (A) A self-contained code template generates several
    plots and is provided to students. Several lines are deactivated using the “#”
    symbol, as indicated by the teal text. Lines of code can be reactivated by removing
    the leading “#”, resulting in the code producing a different visual output when
    executed. “asdf” and “qwer” represent the data set for the following plots. (B)
    The plot generated by the code in panel A when run as is. The three plotted points
    are defined in the code template, connecting the code to its visual output. Later
    in the lesson plan, code is introduced to include axis labels. (C) The plot generated
    when line 8 of panel A is activated, resulting in larger points. (D) The plot
    generated when line 9 is also activated, resulting in different point colors.
    (E) The plot generated when line 11 is activated, creating the same scatter plot
    with a blue background. (F) A potential plot generated after students are instructed
    to include an additional parameter to the scatter function. In this case, the
    shapes of the points are changed. Figure 3 Figure 3. Students are provided with
    the code to import any tabular json file. The utility of the package Pandas is
    used in reading tables programmatically. The “alldrugsprops.json” file is provided
    to students and provided with a template to import the data into Python, as seen
    in the input block. Students then inspect the contents of the file with Python,
    revealing a data file of over 9,000 drugs and their properties downloaded from
    DrugBank. Figure 4 Figure 4. Four chemoinformatic experiments run by students
    during the first session. In each graph, over 9,000 drugs are graphed onto scatter
    plots using calculated properties of the drug. Students learn to change the axes
    and colors of the plot, exposing them to a strategy to rapidly investigate chemical
    space and generate reports in Python. (A) Plot of LogP (a dimensionless quantity
    representing a molecule’s distribution between hydrophobic and aqueous environments)
    against the number of hydrogen bond donors (HBD), colored by the number of aromatic
    bonds (AROM). (B) Plot of fraction sp3 (FSP3, the ratio of sp3 hybridized atoms
    to all atoms) against the number of rotatable bonds (ROTB), colored by the number
    of hydrogen bond acceptors (HBA). (C) Plot of LogP against aLogP (a similar quantity
    to LogP representing a molecule’s lipophilic nature but calculated differently
    with a focus on the molecules constituent atoms), colored by quantitated estimate
    of druglikedness (QED). (D) Plot of formal charge (FC) against FSP3, colored by
    number of hydrogen bond donors. Figure 5 Figure 5. (A) The template plotting code
    is incrementally improved until it can be used effectively to make manuscript-ready
    graphics. This code creates a scatter plot of the Moonshot compounds provided
    in the CSV, where the x-axis is the number of aromatic rings (AROM) and the y-axis
    is logP. (B) By coloring the points by the compound’s reported experimental solubility,
    a trend is revealed where compounds with fewer aromatic rings and lower logP are
    more soluble. Validating GSK’s Solubility Forecast Index is a simple experiment
    to allow students to build confidence in their ability to manipulate and analyze
    data sets. Figure 6 Figure 6. By the end of the modules, students have implemented
    a data filter and a principal component analysis on a data set of SARS-COV-2 main
    protease inhibitors. (A) The PCA before the data filter is applied. Points represent
    DrugBank compounds and are colored by their molecular weight. By changing the
    color of the points, the correlation of the reduced features can be mapped to
    specific features of the data set. In this instance, the molecular weight tracks
    with the first principal component, increasing as the x value increases. (B) The
    PCA plot after the Lipinski filter is applied. Outliers are removed by the filter,
    and the distribution of molecular weights in the first principal component becomes
    wider. The direction of the color gradient has changed as the sign of the principal
    components is arbitrarily assigned. Figure 7 Figure 7. Bar charts showing student
    feedback on the lesson plan collected after the lab. Questions were intended to
    gauge the student’s perspective on their learning experience. In the first year,
    students felt strongly that their understanding of Python, chemical space, and
    data science was improved. In the second year, students felt strongly that their
    understanding of data science improved but were neutral on other learning objectives.
    The difference between the two years may be accounted for by the level of involvement
    the graduate student teaching assistance had in developing the course. In the
    first year, the class was taught by the writer of the lesson plan; the second
    year was taught by assistants who were given the lesson plan to teach.  Partners
    1155 Sixteenth Street N.W. Washington, DC 20036 Copyright © 2024 American Chemical
    Society About About ACS Publications ACS & Open Access ACS Membership ACS Publications
    Blog Resources and Information Journals A-Z Books and Reference Advertising Media
    Kit Institutional Sales ACS Publishing Center Privacy Policy Terms of Use Support
    & Contact Help Live Chat FAQ Connect with ACS Publications This website uses cookies
    to improve your user experience. By continuing to use the site, you are accepting
    our use of cookies. Read the ACS privacy policy. CONTINUE"'
  inline_citation: '>'
  journal: Journal of Chemical Education
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Interactive Python Notebook Modules for Chemoinformatics in Medicinal Chemistry
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Guo S.
  - Sun M.
  - Mao X.
  citation_count: '0'
  description: Vast amounts of heterogeneous data on marine observations have been
    accumulated due to the rapid development of ocean observation technology. Several
    state-of-art methods are proposed to manage the emerging Internet of Things (IoT)
    sensor data. However, the use of an inefficient data management strategy during
    the data storage process can lead to missing metadata; thus, part of the sensor
    data cannot be indexed and utilized (i.e., ‘data swamp’). Researchers have focused
    on optimizing storage procedures to prevent such disasters, but few have attempted
    to restore the missing metadata. In this study, we propose an AI-based algorithm
    to reconstruct the metadata of heterogeneous marine data in data swamps to solve
    the above problems. First, a MapReduce algorithm is proposed to preprocess raw
    marine data and extract its feature tensors in parallel. Second, load the feature
    tensors are loaded into a machine learning algorithm and clustering operation
    is implemented. The similarities between the incoming data and the trained clustering
    results in terms of clustering results are also calculated. Finally, metadata
    reconstruction is performed based on existing marine observation data processing
    results. The experiments are designed using existing datasets obtained from ocean
    observing systems, thus verifying the effectiveness of the algorithms. The results
    demonstrate the excellent performance of our proposed algorithm for the metadata
    reconstruction of heterogenous marine observation data.
  doi: 10.1007/s11802-023-5430-y
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Ocean University of China
    Article A Metadata Reconstruction Algorithm Based on Heterogeneous Sensor Data
    for Marine Observations Published: 28 November 2023 Volume 22, pages 1541–1550,
    (2023) Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Journal of Ocean University of China Aims and scope Submit manuscript Shuai Guo,
    Meng Sun & Xiaodong Mao  32 Accesses Explore all metrics Abstract Vast amounts
    of heterogeneous data on marine observations have been accumulated due to the
    rapid development of ocean observation technology. Several state-of-art methods
    are proposed to manage the emerging Internet of Things (IoT) sensor data. However,
    the use of an inefficient data management strategy during the data storage process
    can lead to missing metadata; thus, part of the sensor data cannot be indexed
    and utilized (i.e., ‘data swamp’). Researchers have focused on optimizing storage
    procedures to prevent such disasters, but few have attempted to restore the missing
    metadata. In this study, we propose an AI-based algorithm to reconstruct the metadata
    of heterogeneous marine data in data swamps to solve the above problems. First,
    a MapReduce algorithm is proposed to preprocess raw marine data and extract its
    feature tensors in parallel. Second, load the feature tensors are loaded into
    a machine learning algorithm and clustering operation is implemented. The similarities
    between the incoming data and the trained clustering results in terms of clustering
    results are also calculated. Finally, metadata reconstruction is performed based
    on existing marine observation data processing results. The experiments are designed
    using existing datasets obtained from ocean observing systems, thus verifying
    the effectiveness of the algorithms. The results demonstrate the excellent performance
    of our proposed algorithm for the metadata reconstruction of heterogenous marine
    observation data. Article PDF Similar content being viewed by others An acquisition,
    curation and management workflow for sustainable, terabyte-scale marine image
    analysis Article Open access 28 August 2018 Multi-source and heterogeneous marine
    hydrometeorology spatio-temporal data analysis with machine learning: a survey
    Article 08 July 2022 Scalable big earth observation data mining algorithms: a
    review Article 11 August 2023 References Alex, R., and Alessandro, L., 2014. Clustering
    by fast search and find of density peaks. Science, 344 (6191): 1492–1496, DOI:
    https://doi.org/10.1126/science.1242072. Article   Google Scholar   Alrehamy,
    H., and Walker, C., 2015. Personal data lake with data gravity pull. 2015 IEEE
    Fifth International Conference on Big Data and Cloud Computing. Dalian, 160–167.
    Atzori, L., Iera, A., and Morabito, G., 2010. The Internet of Things: A survey.
    Computer Networks, 54 (15): 2787–2805, https://doi.org/10.1016/j.comnet.2010.05.010.
    Article   Google Scholar   Bandyopadhyay, D., and Sen, J., 2011. Internet of Things:
    Applications and challenges in technology and standardization. Wireless Personal
    Communications, 58: 49–69, https://doi.org/10.1007/s11277-011-0288-5. Article   Google
    Scholar   Bettini, C., Wang, X. S., Jajodia, S., and Lin, J. L., 1998. Discovering
    frequent event patterns with multiple granularities in time sequences. IEEE Transactions
    on Knowledge and Data Engineering, 10 (2): 222–237, DOI: https://doi.org/10.1109/69.683754.
    Article   Google Scholar   Datta, S. K., and Bonnet, C., 2015. Internet of Things
    and M2M communication as enablers of smart city initiatives. 2015 9th International
    Conference on Next Generation Mobile Application. Cambridge, 393–398. Evans, D.,
    2011. The Internet of Things: How the next evolution of the internet is changing
    everything. The CISCO White Paper, 1: 1–11. Google Scholar   Golov, N., and Ronnback,
    L., 2015. Big data normalization for massively parallel processing database. Advances
    in Conceptual Modeling: ER 2015 Workshops AHA, CMS, EMoV, Mo-BID, MORE-BI, MReBA,
    QMMQ, and SCME. Stockholm, Sweden, 154–163. Gubbi, J., Buyya, R., Marusic, S.,
    and Palaniswami, M., 2013. Internet of Things (IoT): A vision, architectural elements,
    and future directions. Future Generation Computer Systems, 29 (7): 1645–1660,
    https://doi.org/10.1016/j.future.2013.01.010. Article   Google Scholar   Hai,
    R., Geisler, S., and Quix, C., 2016. Constance: An intelligent data lake system.
    Proceedings of the 2016 International Conference on Management of Data. San Francisco,
    2097–2100. Halevy, A. Y., Korn, F., Noy, N. F., Olston, C., Polyzotis, N., Roy,
    S., et al., 2016. Managing Google’s data lake: An overview of the Goods system.
    IEEE Data Engineering Bulletin, 39 (3): 5–14. Google Scholar   Hartigan, J. A.,
    and Wong, M. A., 1979. Algorithm AS 136: A K-means clustering algorithm. Journal
    of the Royal Statistical Society, Series C (Applied Statistics), 28 (1): 100–108,
    https://doi.org/10.2307/2346830. Google Scholar   Hu, C. Q., Pu, Y., Yang, F.,
    Zhao, R., Alrawais, A., and Xiang, T., 2020. Secure and efficient data collection
    and storage of IoT in smart ocean. IEEE Internet of Things Journal, 7 (10): 9980–9994,
    DOI: https://doi.org/10.1109/JIOT.2020.2988733. Article   Google Scholar   Jackson,
    K. R., Ramakrishnan, L., Muriki, K., Canon, S., Cholia, S., Shalf, J., et al.,
    2010. Performance analysis of high performance computing applications on the Amazon
    Web Services Cloud. 2010 IEEE Second International Conference on Cloud Computing
    Technology and Science. Indianapolis, IN, 159–168. Jiang, F., Ma, J., Wang, B.,
    Shen, F., and Yuan, L., 2021. Ocean observation data prediction for Argo data
    quality control using deep bidirectional LSTM network. Security and Communication
    Networks, 2021: 1–11, https://doi.org/10.1155/2021/5665386. Google Scholar   Ma,
    H., and Chen, B., 2016. An authentication protocol based on quantum key distribution
    using decoy-state method for heterogeneous IoT. Wireless Personal Communications,
    91: 1335–1344, https://doi.org/10.1007/s11277-016-3531-2. Article   Google Scholar   Mangiameli,
    P., Chen, S. K., and West, D., 1996. A comparison of SOM neural network and hierarchical
    clustering methods. European Journal of Operational Research, 93 (2): 402–417,
    https://doi.org/10.1016/0377-2217(96)00038-0. Article   Google Scholar   McCrory,
    D., 2010. Data gravity in the clouds. https://datagravitas.com/2010/12/07/data-gravity-in-the-clouds/.
    Miorandi, D., Sicari, S., Pellegrini, F. D., and Chlamtac, I., 2012. Internet
    of Things: Vision, applications and research challenges. Ad Hoc Networks, 10 (7):
    1497–1516, https://doi.org/10.1016/j.adhoc.2012.02.016. Article   Google Scholar   Naumann,
    F., 2014. Data profiling revisited. ACM SIGMOD Record, 42 (4): 40–49, DOI: https://doi.org/10.1145/2590989.2590995.
    Article   Google Scholar   O’Leary, D. E., 2014. Embedding AI and crowdsourcing
    in the big data lake. IEEE Intelligent Systems, 29 (5): 70–73, DOI: https://doi.org/10.1109/MIS.2014.82.
    Article   Google Scholar   Qiu, Z., Hu, N., Guo, Z., Qiu, L., Shuai, G., and Xi,
    W., 2016. IoT sensing parameters adaptive matching algorithm. International Conference
    on Big Data Computing and Communications. Shenyang, 198–211. Satija, D., Bagchi,
    M., and Martínez-Ávila, D., 2020. Metadata management and application. Library
    Herald, 58 (4): 84–107, DOI: https://doi.org/10.5958/0976-2469.2020.00030.2. Article   Google
    Scholar   Sawadogo, P. N., and Darmont, J., 2020. On data lake architectures and
    metadata management. Journal of Intelligent Information Systems, 56 (1): 97–120,
    DOI: https://doi.org/10.1007/s10844-020-00608-7. Article   Google Scholar   Terrizzano,
    I. G., Schwarz, P. M., and Colino, J. E., 2015. Data wrangling: The challenging
    Yourney from the wild to the lake. 2015 7th Biennial Conference on Innovative
    Data Systems Research. Asilomar, California. Vassiliadis, P., Simitsis, A., and
    Skiadopoulos, S., 2002. Conceptual modeling for ETL processes. Proceedings of
    the 5th ACM International Workshop on Data Warehousing and OLAP. McLean Virginia,
    USA, 14–21. Wu, Y., Zhu, Y., and Li, B., 2014. Trajectory improves data delivery
    in urban vehicular networks. IEEE Transactions on Parallel and Distributed Systems,
    25 (4): 1089–1100, DOI: https://doi.org/10.1109/TPDS.2013.118. Article   Google
    Scholar   Xu, G., Shi, Y., Sun, X., and Shen, W., 2019. Internet of Things in
    marine environment monitoring: A review. Sensors, 19 (7): 1717, DOI: https://doi.org/10.3390/s19071711.
    Article   Google Scholar   Xu, H., 2020. Classification and storage method of
    marine multi-source transmission data under cloud computing. Journal of Coastal
    Research, 115: 84–86, DOI: https://doi.org/10.2112/JCR-SI115-025.1. Article   Google
    Scholar   You, L. I., and Liu, D. B., 2005. A method for automatic schema matching
    using characteristic of data distribution. Computer Science, 32 (11): 85–86. Google
    Scholar   Download references Acknowledgement The study is supported by the Shandong
    Province Natural Science Foundation (No. ZR2020QF028). Author information Authors
    and Affiliations College of Science, Qingdao University of Technology, Qingdao,
    266520, China Shuai Guo College of Information and Control Engineering, Qingdao
    University of Technology, Qingdao, 266520, China Meng Sun & Xiaodong Mao Corresponding
    author Correspondence to Shuai Guo. Rights and permissions Reprints and permissions
    About this article Cite this article Guo, S., Sun, M. & Mao, X. A Metadata Reconstruction
    Algorithm Based on Heterogeneous Sensor Data for Marine Observations. J. Ocean
    Univ. China 22, 1541–1550 (2023). https://doi.org/10.1007/s11802-023-5430-y Download
    citation Received 20 May 2022 Revised 09 October 2022 Accepted 27 February 2023
    Published 28 November 2023 Issue Date December 2023 DOI https://doi.org/10.1007/s11802-023-5430-y
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Key words Internet of Things (IoT) sensor data data swamp metadata
    reconstruction Use our pre-submission checklist Avoid common mistakes on your
    manuscript. Sections References Abstract Article PDF References Acknowledgement
    Author information Rights and permissions About this article Advertisement Discover
    content Journals A-Z Books A-Z Publish with us Publish your research Open access
    publishing Products and services Our products Librarians Societies Partners and
    advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress
    Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Ocean University of China
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Metadata Reconstruction Algorithm Based on Heterogeneous Sensor Data for
    Marine Observations
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
