- analysis: '>'
  authors:
  - Rabie O.B.J.
  - Selvarajan S.
  - Hasanin T.
  - Alshareef A.M.
  - Yogesh C.K.
  - Uddin M.
  citation_count: '1'
  description: The Internet of Things (IoT) is extensively used in modern-day life,
    such as in smart homes, intelligent transportation, etc. However, the present
    security measures cannot fully protect the IoT due to its vulnerability to malicious
    assaults. Intrusion detection can protect IoT devices from the most harmful attacks
    as a security tool. Nevertheless, the time and detection efficiencies of conventional
    intrusion detection methods need to be more accurate. The main contribution of
    this paper is to develop a simple as well as intelligent security framework for
    protecting IoT from cyber-attacks. For this purpose, a combination of Decisive
    Red Fox (DRF) Optimization and Descriptive Back Propagated Radial Basis Function
    (DBRF) classification are developed in the proposed work. The novelty of this
    work is, a recently developed DRF optimization methodology incorporated with the
    machine learning algorithm is utilized for maximizing the security level of IoT
    systems. First, the data preprocessing and normalization operations are performed
    to generate the balanced IoT dataset for improving the detection accuracy of classification.
    Then, the DRF optimization algorithm is applied to optimally tune the features
    required for accurate intrusion detection and classification. It also supports
    increasing the training speed and reducing the error rate of the classifier. Moreover,
    the DBRF classification model is deployed to categorize the normal and attacking
    data flows using optimized features. Here, the proposed DRF-DBRF security model's
    performance is validated and tested using five different and popular IoT benchmarking
    datasets. Finally, the results are compared with the previous anomaly detection
    approaches by using various evaluation parameters.
  doi: 10.1038/s41598-024-51154-z
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    03 January 2024 A novel IoT intrusion detection framework using Decisive Red Fox
    optimization and descriptive back propagated radial basis function models Osama
    Bassam J. Rabie, Shitharth Selvarajan, Tawfiq Hasanin, Abdulrhman M. Alshareef,
    C. K. Yogesh & Mueen Uddin  Scientific Reports  14, Article number: 386 (2024)
    Cite this article 844 Accesses 1 Citations Metrics Abstract The Internet of Things
    (IoT) is extensively used in modern-day life, such as in smart homes, intelligent
    transportation, etc. However, the present security measures cannot fully protect
    the IoT due to its vulnerability to malicious assaults. Intrusion detection can
    protect IoT devices from the most harmful attacks as a security tool. Nevertheless,
    the time and detection efficiencies of conventional intrusion detection methods
    need to be more accurate. The main contribution of this paper is to develop a
    simple as well as intelligent security framework for protecting IoT from cyber-attacks.
    For this purpose, a combination of Decisive Red Fox (DRF) Optimization and Descriptive
    Back Propagated Radial Basis Function (DBRF) classification are developed in the
    proposed work. The novelty of this work is, a recently developed DRF optimization
    methodology incorporated with the machine learning algorithm is utilized for maximizing
    the security level of IoT systems. First, the data preprocessing and normalization
    operations are performed to generate the balanced IoT dataset for improving the
    detection accuracy of classification. Then, the DRF optimization algorithm is
    applied to optimally tune the features required for accurate intrusion detection
    and classification. It also supports increasing the training speed and reducing
    the error rate of the classifier. Moreover, the DBRF classification model is deployed
    to categorize the normal and attacking data flows using optimized features. Here,
    the proposed DRF-DBRF security model''s performance is validated and tested using
    five different and popular IoT benchmarking datasets. Finally, the results are
    compared with the previous anomaly detection approaches by using various evaluation
    parameters. Similar content being viewed by others Firefly algorithm based WSN-IoT
    security enhancement with machine learning for intrusion detection Article Open
    access 02 January 2024 A balanced communication-avoiding support vector machine
    decision tree method for smart intrusion detection systems Article Open access
    05 June 2023 Robust genetic machine learning ensemble model for intrusion detection
    in network traffic Article Open access 11 October 2023 Introduction Internet of
    Things (IoT) has recently drawn increased attention because of its innovative
    uses and support for various industries, including industrial applications, healthcare,
    transportation, ambient intelligence1, etc. IoT offers a vast range of applications
    and services but also confronts serious security risks and assaults. Since the
    IoT is a heterogeneous environment, traditional security techniques are not supported
    by its interoperability mechanism2. IoT security is improved in other ways, such
    as data authentication, secrecy, and access controls3. However, IoT networks are
    susceptible to numerous assaults that try to disrupt the web, even with these
    defenses. A separate module must therefore ensure the security of the IoT network.
    One such idea is the intrusion detection system (IDS)4,5, which is already utilized
    in wireless networks. Also, it helps to secure the network from assaults and other
    vulnerabilities by improving the IDS features of wireless networks. Specifically,
    the IDS6,7,8 is treated as the essential element in enhancing the cybersecurity
    of IoT networks, which is also highly suited for both fog and cloud platforms.
    Moreover, it uses the internet and real-time applications to offer users an efficient
    and convenient environment. Therefore, before deploying an IDS9,10, it is essential
    to analyze the security challenges in the network. Some of the significant properties
    used to ensure the security of IoT networks are as follows: data confidentiality,
    authentication, integrity, availability, and authorization11. The three primary
    functional mechanisms that most existing IDS12,13,14,15,16 use are as follows:
    Sources of information When determining if an intrusion has occurred, sources
    of information such as incoming packets or data are considered. Characterization
    The required method determines when the events gathered suggest that intrusions
    are happening or have already happened. The most popular analysis techniques are
    misuse detection and anomaly detection. Reaction When the system notices an intrusion,
    it sends a response. There are two types of reaction measures: active and passive.
    A functional response measure occurs when the system takes action on its own,
    whereas a passive response measure sends its findings to the administrator, who
    may take action based on these reports. Also, various machine learning and deep
    learning17,18 based AI mechanisms are used in the traditional works for developing
    an effective IDS. Machine learning is a kind of artificial intelligence that systematically
    uses algorithms to discover the underlying connections between data and information.
    It is categorized into the types of supervised learning, unsupervised learning,
    and reinforced learning. Similarly, the deep learning techniques19 are also increasingly
    used nowadays, which is an extended version of machine learning. However, the
    conventional classification methodologies4 face the challenges associated to the
    factors of increased time consumption, overfitting, reduced processing speed,
    high false positives, and difficulty in understanding. The IoT delivers innovative
    features and services to a large number of consumers, hence enhancing their lifestyles.
    Most IoT devices and objects don’t require a lot of capacity. The IoT has a limited
    amount of available storage and transmission capacity. As a result, clouds are
    used to store a vast amount of confidential documents. This increases the availability
    and accessibility of the services supplied while lowering the expenses and effort.
    This technology enables the users to access the applications and services at anywhere
    & anytime, which creates a significant challenges to the data security. Moreover,
    some other factors such as cost, performance, data scalability and availability
    are also considered as the IoT related challenges. Since, there is no standard
    format or protocol for the data transmission, storage, maintenance and etc in
    IoT, when it is dealing with vast amount of data. Some of these issues usually
    take the form of network anomalies, like a deviation from normal network action.
    The IoT devices are becoming more prevalent in today''s world, yet the cloud has
    significant restrictions as listed below: More energy consumption Increased network
    bandwidth consumption High latency or delay Outage of internet High maintenance
    cost due to an unwanted data storage Minimal control over the applications or
    data Security breaches Due to the IoT features such as flexible data sharing and
    constant connectivity, there are a number of cybersecurity problems have been
    created with this development. To resolve this problem, many IDS are developed
    for assuring IoT security, which showed their effectiveness in mitigating cyber-threats.
    Specifically, the deep learning algorithms are increasingly used in the existing
    works for enhancing the attack detection rate in an IoT networks. However, the
    existing deep learning techniques are highly complex to interpret, and their prediction
    decisions are very difficult to understand by the cybersecurity experts. As a
    result, the corresponding users are unable to both understand and trust the decisions
    made by DL models and to optimize their own actions in light of those decisions.
    Therefore, the proposed work motivates to develop an efficient and highly secured
    IDS framework for IoT security. The main purpose of this research article to design
    and develop a novel IoT based intrusion detection framework for maximizing the
    security with lower computational burden. It also intends to maintain an improved
    detection performance and results while accurately predicting the type of intrusion
    from the large/huge dimensional intrusion datasets. For accomplishing these objectives,
    the different kinds of mining techniques including preprocessing, DRF based feature
    selection, and DBRF based classification are implemented in this study. The major
    research contributions of this paper are as follows: In order to generate a balanced
    dataset that will increase the detection rate and precision of IDS, data preprocessing
    is carried out, which includes handling of NaN values, the extraction of categorical
    features, and the identification of missing fields. A Decisive Red Fox (DRF) optimization
    approach is used to extract the pertinent features from the balanced IoT datasets,
    which improves the classifier''s training process. The use of a Descriptive Back
    Propagated Radial Basis Function (DBRF) classification method allows the identification
    and categorization of intrusions in IoT systems based on the features of data.
    To validate and compare the results of proposed DRF-DBRF security framework, various
    evaluation indicators as well as the popular IoT IDS datasets are utilized in
    this work. The remaining sections of this article are divided into the following
    categories: The traditional approaches to enhancing the security of IoT networks
    are reviewed in “Related works” section. Additionally, it verifies the benefits
    and drawbacks of each mechanism in light of the effectiveness and outcomes of
    its attack detection. The suggested DRF-DBRF methodology is fully explained in
    “Methods” section together with the overall work flow and algorithms. Additionally,
    “Results” section compares and validates the performance and outcomes of the proposed
    technique using a variety of performance indicators. Finally, “Conclusion” section
    summarizes the entire work together with the conclusions and future scope. Related
    works The comprehensive literature review of the IDS frameworks currently in use
    for enhancing the security of IoT networks is presented in this part. Furthermore,
    it examines each model''s benefits and drawbacks in context of its effectiveness
    and reliability in detection. Gu et al.20 utilized a Convolutional Neural Network
    (CNN) mechanism for developing an accurate IDS framework to ensure the security
    of IoT networks. Here, the Kitsune network attack database has been utilized to
    implement this system, which comprises the different types of network attacks.
    The CNN has the ability to automatically recognize the data packets for ensuring
    a secured end-to-end communication in IoT systems. However, the CNN model requires
    a lot of training data to predict an accurate results, and it has a reduced learning
    speed. Alsoufi et al.21 presented a comprehensive literature review to examine
    various deep learning techniques for designing an effective anomaly detection
    system. Also, it intends to increase the detection accuracy, and minimize the
    false alarm rate by solving the security problems in the IoT networks. Here, the
    11 different types of attack datasets have been utilized to validate the system
    model using various parameters. According to this survey, it is observed that
    developing a lightweight anomaly detection mechanism could be highly beneficial
    for the IoT systems. According to this study work, it is noted that the majority
    of deep learning mechanisms facing challenges in high computational complexity
    while training samples for classification, increased time consumption for both
    training and testing operations, and overfitting outcomes. Mishra et al.22 presented
    a comprehensive literature review to analyze the security challenges, vulnerabilities,
    and attacks in the IoT networks. The authors of this paper intend to conduct a
    multi-fold survey for analyzing the security issues in the IoT layers. Typically,
    ensuring the parameters such as interoperability, connectivity, and standardization
    were considered as the major security challenges of IoT networks, which is graphically
    represented in Fig. 1. Figure 1 Challenges in IoT systems. Full size image The
    main focus of this paper is to study the different types of DDoS attacks with
    their mitigation strategies. Here, the various types such as volumetric attack,
    protocol based attack, and application layer attack are discussed with the goal
    of attacker and the preventive solutions. As its name implies, a DDoS attack aims
    to overload a target and stop services from functioning. IoT devices are highly
    suited for the DDoS attack because it needs a lot of devices to initiate an attack.
    Also, the users will typically not be aware that a device is compromised. The
    suggested work only focused on detecting DDoS attacks from the network, since
    some of the modern attacks could degrade the performance of wireless networks
    in present days. Fatani et al.23 utilized an aquila optimization technique integrated
    with the deep learning mode for developing an efficient IDS for IoT systems. Here,
    the CNN algorithm was utilized for extracting the relevant features from the given
    attack datasets. Then, the binary aquila optimization algorithm was deployed for
    choosing the optimal features with increased classification accuracy. Finally,
    the ML classification algorithm was deployed to categorize the type of attacks
    according to the reduced features. However, the suggested optimization technique
    having the specific drawbacks of local optimum, lower searching efficiency, and
    increased time for finding optimal solutions. Abd-Elaziz et al.24 developed a
    new capuchin search algorithm incorporated with the deep learning model for detecting
    intrusions from cloud-IoT systems. The purpose of this paper is to implement a
    new feature selection based deep learning algorithm for assuring the security
    of IoT systems. Here, various and recent Cloud-IoT datasets have been utilized
    to validate the performance of the suggested mechanism. The outcomes of this analysis
    depict that the suggested technique provides a competitive performance for all
    datasets utilized in this work. Nevertheless, the suggested deep learning algorithm
    requires lot of training samples to predict the accurate results. Aslam et al.25
    introduced an adaptive machine learning based security methodology for protecting
    SDN from cyber-attacks. Here, an adaptive multi-layered feed forward mechanism
    is deployed to accurately spot the DDoS attacks by analyzing the features of the
    network traffic. Moreover, this framework provides an increased accuracy with
    low false alarm rate. But, it failed to focus some of the modern attacks or vulnerabilities
    that degrade the security of SDN. Smys et al.26 introduced a hybrid IDS for protecting
    IoT system against network vulnerabilities and harmful intrusions. The motive
    of this work was to guarantee the properties of data confidentiality, integrity,
    availability, authorization, and authentication for IoT security. Typically, the
    three different types of security schemes were used for IoT networks, which includes
    placement strategy, detection strategy, and validation strategy. In this work,
    the LSTM-RNN model was used to detect the network anomaly with improved performance.
    Moreover, this framework comprises the working stages of log file generation,
    feature extraction, encoring, matrix formation, classification, and intrusion
    categorization. However, the suggested methodology was not more suitable for handling
    the complex network datasets, which could be the major limitation of this work.
    Almiani et al.27 implemented a Deep Recurrent Neural Network (DRNN) for increasing
    the security of IoT networks. It encompasses the major operations of feature reduction,
    data normalization, over sampling, and intrusion detection. In the suggested framework,
    the common mining operations including sampling, normalization, feature elimination,
    and intrusion identification processes are performed. For classification, the
    DRNN technique is implemented here, which follows some complex mathematical models
    to accurately predict the type of intrusion. Hence, it may be difficult to understand
    the classification operations of the suggested technique. Verma et al.28 deployed
    an ensemble of machine learning classifiers for detecting intrusions from the
    IoT networks. It includes Random Forest (RF), Gradient Boosted Machine (GBM),
    Extreme Gradient Boost (EGB), Extremely Randomized Trees (ERT), Classification
    & Regression Trees (CART), and Multi-Layer Perceptron (MLP). Consequently, various
    benchmarking datasets have been used to validate the performance of these classifiers.
    Based on this investigation, it is identified that the CART outperforms the other
    machine learning models with improved attack detection accuracy. Yet, it follows
    some complex mathematical modeling for attack prediction and classification. Anthi
    et al.29 developed a three layered IDS framework using a supervised learning methodology
    for protecting IoT networks. This framework comprises the following operations:
    IoT device behavior analysis Malicious packet identification Attack class categorization
    Specifically, the authors intend to design and develop a lightweight security
    framework for detecting cyber-attacks in the smart home IoT networks. The advantages
    of this framework were increased attack detection accuracy, better efficacy, easy
    deployment, and reduced overfitting. However, the time required for training and
    testing the features while classifying the type of data need to be reduced. Al-Hadhrami
    et al.30 introduced a real time dataset generation framework for spotting intrusions
    in the IoT networks. In this work, the problems and limitations associated to
    the existing IDS datasets have been discussed. Moreover, the key components involved
    in this framework were capturing medium, data aggregation, feature extraction,
    and queuing unit. Benkhelifa et al.31 presented a critical review to protect the
    IoT networks against the network intrusions. The purpose of this paper was to
    develop a highly secure and robust IDS framework for analyzing the malicious behavior
    of nodes. The different types of detection methodologies reviews in this work
    were anomaly detection models, specification based detection methods, and hybrid
    detection models. Qureshi et al.32 introduced a heuristic based detection mechanisms
    for protecting IoT networks, which includes the modules of data preprocessing,
    classifier training and testing. During dataset processing, the attribute selection,
    one hot encoding, and normalization operations were performed to improve the training
    and testing processes. Moreover, it accurately predict the normal and attacking
    data traffic flows based on the features training features. Due to the increased
    dimensionality of features, the overall attack detection accuracy and efficiency
    of classification have been affected. Kumar et al.33 introduced a Unified IDS
    framework for strengthening the security IoT networks against four different types
    of attacks such as exploit, DoS, probe and generic. Here, the dataset clustering
    was performed at the initial stage for analyzing the behavior of attacks. Then,
    the rule generation and integration operations were performed to extract the relevant
    features for classifier training and testing. This framework is not capable of
    handling huge datasets with low time and computational complexity. This part presented
    the related works that review and outline intrusion detection strategies utilizing
    machine learning/deep learning algorithms in the IoT network by emphasizing their
    key contributions. In several studies, the topics of IoT security, privacy, and
    intrusion detection are addressed. Although several research studies34 on intrusion
    detection systems in IoT applications are still in the development phase. The
    study indicates that much of the existing research work faces several challenges
    while ensuring security in IoT. Hence, it is most important to resolve the following
    problems for developing an effective IDS: computational burden, increased amount
    of time for prediction, inability to handle a vast amount of data, and high false
    positives. As a result, the proposed study aims to create an intelligent and efficient
    IDS framework for enhancing IoT security against dangerous network intrusions.
    Methods This section provides the complete explanation for the proposed security
    model used to protect IoT systems. The IoT technologies are anticipated to provide
    a new level of communication with the use of smart devices, which can improve
    regular chores and enable smart decisions based on sensed data. The original contribution
    of the proposed work is to develop an intelligent IoT intrusion detection framework
    with the use of advanced DRF and DBRF techniques. By using the combination of
    these methodologies, the overall performance and efficacy of the intrusion detection
    system is greatly improved with high accuracy, lower training and testing time.
    Moreover, this eliminates the need of complex mathematical calculations for preprocessing,
    feature optimization, and classification operations. In order to determine its
    efficacy and superiority, the most recent and huge dimensional IoT intrusion datasets
    are taken into account for performance validation and assessment. The sensitive
    data collected by the IoT must be protected from assaults and privacy concerns.
    Moreover, the IoT security is a hotly debated topic in both academia and business
    in present days. In fact, attacks to IoT products and services could result in
    security breaches and information leakage. The purpose of this work is to design
    an IDS framework using machine learning technique, with the goal of detecting
    attempts to exploit IoT systems and to mitigate hostile occurrences. The original
    contribution of this work is to develop a highly efficient and accurate IDS framework
    for securing the IoT networks by using a novel data mining methodologies. For
    accomplishing this objective, a novel Decisive Red Fox optimization (DRF) and
    Descriptive Back Propagated-Radial Basis Function (DBRF) network classification
    models are deployed, which helps to strengthen the security of IoT networks. The
    overall work flow of the proposed system is shown in Fig. 2, which comprises the
    following operations: Data preprocessing & normalization Decisive Red Fox (DRF)
    optimization based feature selection Descriptive Back Propagated-Radial Basis
    Function (DBRF) network based classification Attack identification and categorization
    Performance evaluation Figure 2 Workflow model of the proposed security framework.
    Full size image Here, the popular IoT IDS datasets such as IoTID-20, NetFlow-BoT-IoT-v2,
    NF-ToN-IoT-v2, NSL-KDD, UNSW-NB 15 datasets have been used for system implementation.
    The raw network datasets are noisy, which holds some irrelevant attributes, and
    missing fields. As a result, it affects intrusion detection and classification
    performance and outcomes. Thus, the data normalization and preprocessing operations
    are performed in this framework, which holds the operations of handling Not a
    Number (NaN) values, handling categorical values, and missing values. In the proposed
    work, the imbalanced dataset is handled by using the random over-sampler to preprocess
    the incoming data, handling missing values, categorical features, NaN values,
    and unbalanced datasets. Data cleansing, visualization, feature engineering, and
    vectorization are typically done as part of the dataset preprocessing procedure.
    To extract data from the data collection, all of these methods have been applied.
    Two sets of these characteristic vectors have been generated, one for training
    and the other for testing, with 80:20 proportion between the two sets. An unbalanced
    dataset, missing values, categorical features, and NaN value handling are the
    four processes used in the proposed work to deal with the incoming data. Here,
    the NaN value handling is mainly performed to highly increase the accuracy of
    intrusion recognition and classification. After successfully handling NaN values,
    the next step in handling categorical features is processing those characteristics.
    This stage involves handling categorical data before it is fed into artificial
    intelligence learning models. Following that, the non-random missing values and
    the random missing values are handled. Randomly missing values are those that
    are absent from a subset of the data. Finally, the imbalanced data is balanced
    with complete attributes or information with the aid of random over sampler. Following
    preprocessing, the data is fed into the DRF feature selection algorithm, which
    retrieves features out of the dataset. The DBRF classification approach is used
    to classify the features and divide the data into attack and non-attack groups.
    Consequently, the DRF optimization model is used to select the most pertinent
    and advantageous features, hence enhancing the classifier''s training speed and
    detection rate. The data flow is then classified as either an attacker or a normal
    flow based on an optimum collection of attributes using the DBRF classification
    model. The primary advantages of using the proposed DRF-DBRF IDS framework are
    increased training speed, minimal time consumption, reduced overfitting, accurate
    detection rate, and easy to deploy. Balanced dataset is referred to as the preprocessed
    or the normalized dataset that is used for subsequent intrusion detection operations.
    This dataset has the normalized attribute information, no missing values, and
    redundant information. By using the DRF algorithm, the most required subset of
    features are selected with its best optimum solution, which helps to train the
    classifier with reduced dimensionality of features. In the proposed study, there
    are 5 distinct and different intrusion datasets have been used for intrusion detection,
    and each of which having increased number of features or attributes. These are
    eliminated by optimally picking some selective attributes according to the best
    optimum solution obtained from the DRF technique. After feature reduction, the
    selected subset of features are passed to the DBRF classifier for training and
    testing operations. Based on this process, the accurate label is predicted as
    whether normal or attacker with high accuracy. In the proposed work, there are
    5 distinct IoT intrusion datasets are used for system implementation and we are
    not combining these datasets together. Here, each dataset is separately used as
    the input for intrusion detection and classification. Preprocessing and normalization
    The original IoT datasets are preprocessed at first for normalizing the attributes
    before classification, which holds the operations of NaN values handling, categorical
    feature extraction, and identification of missing fields. Then, it produces the
    balanced and normalized dataset as the output for further operations. The data
    is first preprocessed, which involves dealing with NAN values, categorical characteristics,
    unbalanced datasets, and missing values that can happen both unintentionally and
    purposefully. The data is then processed further afterwards this process. Preprocessing
    helps to gain better quality data while also lowering the challenges that come
    with the data, which impedes the flow of data traffic. The abbreviation NaN, which
    stands for \"Not a Number,\" is one of the most frequently used symbols to denote
    a missing value in data when dealing with NaN numbers. The input data for an attack
    detection system must be free of NaN values in order to increase the accuracy
    of attack detection. After successfully managing NaN values, handling categorical
    characteristics is the next step for handling categorical features. Before categorical
    data is fed into the machine learning models, which is the final step, it must
    be processed in this stage. Machine learning models are unable to operate effectively
    with data that is saved in the texture format because they are regarded as mathematical
    models. Both randomly generated and non-randomly generated missing values are
    handled in the next phase of the missing value handling operation. Randomly missing
    values are those that are absent from certain subsamples of data. When data is
    absent but still has a defined structure, it''s referred to as missing values.
    During this process, the operations such as NaN values handling, categorical attributes
    handling, and missing values handling at both random and not at random are performed.
    If the estimated ratio of both attack and non-attack samples are same, the features
    are directly extracted from the dataset for balancing; otherwise, the random over
    sampler is used to handle the imbalance information for producing the balanced
    dataset. The preprocessing phase handles both missing values that are not random
    and missing values that are missing at random. Missing values at random are those
    values that are absent from some subsamples of the data, which are identified
    when the missing data has a certain structure. Here, the NaN handling is performed
    to find out the missing values in the given data, which helps to increase the
    accuracy of intrusion detection. It is computed by using the following equation:
    $$ DS_{N}^{{handling{ }\\left( {NaN} \\right)}} = {\\Phi }_{{NaN_{handling} }}
    \\left( {DS_{N} } \\right) $$ (1) where \\(DS_{N}\\) indicates the input data,
    \\({\\Phi }_{{NaN_{handling} }}\\) represents the model used to handle the NaN
    values, and \\(DS_{N}^{{handling{ }\\left( {NaN} \\right)}}\\) indicates that
    is acquired after processing NaN values. Consequently, the categorical feature
    handling is performed NaN handling, since it is processed before being fed into
    the classification stage. The features are obtained by using the following models:
    $$ DS_{N}^{{handling{ }\\left( {CF} \\right)}} = \\varrho_{CF\\_handling} \\left(
    {DS_{N} } \\right) $$ (2) where \\(\\varrho_{CF\\_handling}\\) indicates the model
    used to handle the categorical data, \\(DS_{N}^{{handling{ }\\left( {CF} \\right)}}\\)
    is the output data retrieved after category processing. Moreover, the missing
    values are identified and handled for generating the normalized dataset. Missing
    values at random are those values that are absent from some subsamples of the
    data. Missing values—as opposed to missing data—are identified when the missing
    data has a certain structure. The missing values are identified by using the following
    equation; $$ DS_{N}^{{handling\\left( {Miss{ }Value} \\right)}} = \\delta_{{handling
    - missvalue{ }}}^{{\\left( {R,{ }NR} \\right)}} \\left( {DS_{N} } \\right) $$
    (3) where \\(\\delta_{{handling - missvalue{ }}}^{{\\left( {R,{ }NR} \\right)}}\\)
    represents the method used to handle the missing values, and \\(DS_{N}^{{handling\\left(
    {Miss{ }Value} \\right)}}\\) is the output data obtained after handling missing
    values. Moreover, the preprocessed dataset is generated in the following form:
    $$ DS_{N}^{PD} = \\left\\{ {DS_{1} ,DS_{2} ,{ }DS_{3} \\ldots DS_{N} } \\right\\}
    $$ (4) where \\(DS_{N}^{PD}\\) denotes the preprocessed dataset, and N indicates
    the total number of data. The balanced and imbalanced dataset is obtained based
    on the ratio of attacking and non-attacking samples by using the following equation:
    $$ DS_{N}^{PD} = \\left\\{ {\\begin{array}{*{20}l} {DS_{N}^{B} } \\hfill & {if\\;\\left(
    {X\\left( {DS_{N}^{PD} } \\right) = Y\\left( {DS_{N}^{PD} } \\right)} \\right)}
    \\hfill \\\\ {DS_{N}^{IB} } \\hfill & {if\\;\\left( {X\\left( {DS_{N}^{PD} } \\right)
    \\ne Y\\left( {DS_{N}^{PD} } \\right)} \\right)} \\hfill \\\\ \\end{array} } \\right.
    $$ (5) where \\(DS_{N}^{B}\\) represents the balanced dataset, \\(DS_{N}^{IB}\\)
    denotes the imbalanced dataset, X and Y indicates the attacking and non-attacking
    data respectively. The balanced data from the collected information is added to
    the subsequent phase, while the imbalanced data is dealt with by a random over
    sampler. Here, an imbalanced dataset is handled by using a random oversampler
    to balance the data. By arbitrarily repeating instances from the minority class
    and applying them to the training input, the random oversampler creates balanced
    data by using the following equation: $$ DS_{N}^{IB} \\mathop{\\longrightarrow}\\limits^{Oversampling}DS_{N}^{B}
    . $$ (6) Finally, the balanced dataset is obtained after oversampling, which can
    be used for further optimization and classification processes. Decisive Red Fox
    (DRF) optimization After obtaining the balanced dataset from the previous stage,
    the DRF optimization algorithm is applied to choose the optimal features for improving
    the training speed and accuracy of intrusion detection. In the traditional IDS
    frameworks, various meta-heuristic optimization models are developed for increasing
    the security of networks. For instance, the Mayfly Optimization (MO), Greedy Swarm
    Optimization (GSO), Fruitfly Optimization (FO), and Spider Monkey Optimization
    (SMO) are the recently developed models used for network security. However, it
    has the key problems associated to the factors of complex computational operations,
    overfitting, reduced convergence rate, and slow in process. Typically, the Dragon
    Fly Algorithm (DFA), Moth Flame Optimization (MFO), Harris Hawks Optimization
    (HHO), Firefly Algorithm (FA), Flower Pollination Algorithm (FPA), Whale Optimization
    Algorithm (WO), and Ant Lion Optimization (ALO) are some of the recently developed
    nature inspired/bio-inspired optimization techniques. These algorithms are extensively
    used in many security applications for solving the complex optimization problems.
    Among others, the DRF is one of the most recently developed optimization algorithm,
    and it has enormous benefits comparing to other techniques. It includes low computational
    complexity, avoids stacking of the algorithm during optimization, fast convergence,
    and reduced local optimum. Also, the DRF35 is not specifically used in the IoT-IDS
    security applications. Therefore, the proposed work intends to use this algorithm
    for optimizing the features of dataset based on the best optimal solution. Moreover,
    this optimization process helps to simplify the process of classification with
    increased attack detection rate. This optimization algorithm can optimally tune
    the parameters of the balanced IoT dataset. Generally, the foxes are omnivorous,
    small- to medium-sized mammals that is a member of a number of Canidae genera;
    because of their sharp noses, thick tails, long, thin legs, and slim limbs. Also,
    the foxes can be differentiated from other members of their family, or giant dogs.
    The DRF is a new meta-heuristic optimization algorithm that draws inspiration
    from the red foxes'' hunting habits. When hunting, the red fox approaches the
    target gradually while it hides in the bushes, and then the animal is suddenly
    attacked. This algorithm incorporates both the exploitation and exploration capabilities
    like other meta-heuristics models. In this algorithm, the parameter initialization
    is performed based on the generation of random individuals as represented in below:
    $$ P = \\left[ {p_{0} ,{ }p_{1} \\ldots p_{n - 1} } \\right] $$ (7) $$ \\left(
    P \\right)^{i} = \\left[ {\\left( {p_{0} } \\right)^{i} ,{ }\\left( {p_{1} } \\right)^{i}
    \\ldots \\left( {p_{n - 1} } \\right)^{i} } \\right] $$ (8) where i indicates
    the number of populations in the searching space. Then, the optimum solution is
    achieved in the searching space by using the global optimal function. Here, the
    Euclidean distance is applied to obtain the optimum solution by using the following
    model: $$ E\\left( {\\left( {\\left( P \\right)^{i} } \\right)^{k} ,\\left( {P_{best}
    } \\right)^{k} } \\right) = \\sqrt {\\left( {\\left( P \\right)^{i} } \\right)^{k}
    - \\left( {P_{best} } \\right)^{k} } $$ (9) where k indicates the number of iterations,
    \\(P_{best}\\) is the best optimum, and \\(E\\left( . \\right)\\) indicates the
    Euclidean distance. Consequently, the optimum solution is used to migrate all
    candidates as shown in below: $$ \\left( {\\left( P \\right)^{i} } \\right)^{k}
    = \\left( {\\left( P \\right)^{i} } \\right)^{k} + rsign{ }\\left( {\\left( {P_{best}
    } \\right)^{k} - \\left( {\\left( P \\right)^{i} } \\right)^{k} } \\right) $$
    (10) where \\(r\\) denotes the random number in the range of 0 to 1, which is
    a randomly chosen scaling hyperparameter that is set once per an iteration for
    the entire population. After moving to the best place, if the values of fitness
    at their new positions are higher, individuals stay there; otherwise, they migrate
    back to their original positions. This illustrates how family members return home
    after an expedition and teach the others where to hunt. The family members follow
    the explorers’ directions. If there was a chance of finding food, they would stay
    to hunt; otherwise, they would return home “empty-handed”. In each DRF cycle,
    these operations stand in for proposed global searches. Moreover, the candidates’
    new location should offer a suitable option; otherwise, the prior location would
    still exist. The red fox approaches the prey to observe it, which is characterized
    as the use of the DRF modelled by assuming a random number \\(\\omega\\) between
    [0, 1]: $$ \\left\\{ {\\begin{array}{*{20}l} {Move\\;forward} \\hfill & {if,\\omega
    > 3/4{ }} \\hfill \\\\ {Stay\\;hidden} \\hfill & {if,\\omega > 3/4} \\hfill \\\\
    \\end{array} } \\right. $$ (11) $$ \\omega = \\left\\{ {\\begin{array}{*{20}l}
    {h \\times \\frac{{{\\text{sin}}\\left( {\\delta_{0} } \\right)}}{{\\delta_{0}
    }}} \\hfill & {if\\;\\delta_{0} \\ne 0} \\hfill \\\\ \\tau \\hfill & {if\\;\\delta_{0}
    = 0} \\hfill \\\\ \\end{array} } \\right. $$ (12) where h is the random number
    in the range of [0, 0.2], \\(\\delta_{0}\\) is also a random number lies in the
    range of [0, 2 \\(\\pi\\)] that is considered as the fox observation angle, and
    \\(\\tau\\) denotes the random value in the range of 0 to 1. The following system
    of equations for spatial coordinates are used to model motions for the population
    of individuals. $$ \\left\\{ {\\begin{array}{*{20}l} {p_{0}^{new} = h \\times
    \\omega \\times \\cos \\left( {\\delta_{1} } \\right) + p_{0}^{actual} } \\hfill
    \\\\ {p_{1}^{new} = h \\times \\omega \\times \\sin \\left( {\\delta_{1} } \\right)
    + h \\times \\omega \\times \\cos \\left( {\\delta_{2} } \\right) + p_{1}^{actual}
    } \\hfill \\\\ {p_{1}^{new} = h \\times \\omega \\times \\sin \\left( {\\delta_{1}
    } \\right) + h \\times \\omega \\times \\sin \\left( {\\delta_{2} } \\right) +
    h \\times \\omega \\times \\cos \\left( {\\delta_{3} } \\right) + p_{2}^{actual}
    } \\hfill \\\\ \\vdots \\hfill \\\\ {p_{n - 1}^{new} = h \\times \\omega \\times
    \\mathop \\sum \\limits_{t = 1}^{n - 2} \\sin \\left( {\\delta_{1} } \\right)
    + h \\times \\omega \\times \\cos \\left( {\\delta_{n - 1} } \\right) + p_{n -
    2}^{actual} } \\hfill \\\\ {p_{n - 1}^{new} = h \\times \\omega \\times \\sin
    \\left( {\\delta_{1} } \\right) + h \\times \\omega \\times \\sin \\left( {\\delta_{2}
    } \\right) + \\ldots + h \\times \\omega \\times \\sin \\left( {\\delta_{n - 1}
    } \\right) + p_{n - a}^{actual} } \\hfill \\\\ \\end{array} } \\right. $$ (13)
    In order to maintain a fixed size of the population, the population''s worst members
    were eliminated, and many new members were added. Subsequently, two optimal members
    are identified at iteration k, and their center is estimated as follows: $$ C_{e}^{k}
    = \\frac{1}{2}\\left( {P\\left( 1 \\right)} \\right)^{k} - \\left( {P\\left( 2
    \\right)} \\right)^{k} $$ (14) here a random parameter \\(\\varphi\\) between
    (0 and 1) is used for each iteration that specifies replacements in the iteration
    in accordance with the following model: $$ \\left\\{ {\\begin{array}{*{20}l} {new\\;nomadic\\;individual}
    \\hfill & {if,\\;\\varphi > 0.45} \\hfill \\\\ {reproduction} \\hfill & {if,\\;\\varphi
    \\le 0.45} \\hfill \\\\ \\end{array} } \\right. $$ (15) Based on this process,
    the random locations are updated in the searching space, and the new members are
    added by using the following model: $$ \\left( {P^{rp} } \\right)^{k} = \\frac{\\varphi
    }{2}\\left( {P\\left( 1 \\right)} \\right)^{k} - \\left( {P\\left( 2 \\right)}
    \\right)^{k} $$ (16) By using this function, the reproduced individual is obtained,
    and the best \\(P_{best}\\) is returned as the output. This function can be used
    to optimally select the features for training the data samples of the classifier.
    Descriptive back propagated: radial basis function (DBRF) network classification
    After feature optimization, the DBRF network classification model is implemented
    to categorize the data flow as whether normal or intrusion. In the traditional
    works, various machine learning and deep learning based classification techniques
    are implemented to increase the security of IoT networks by protecting it from
    the harmful intrusions. For instance, the Logistic Regression (LR), Decision Tree
    (DT), eXtreme Gradient Boost (XGB), Convolutional Neural Network (CNN), and ensemble
    learning models are extensively used in many network security applications. However,
    it has the major problems of inaccurate prediction if the sample is too sample,
    overlapping, higher training time, and unstability36,37,38. Therefore, the proposed
    work motivates to develop a new classification model, named as, DBRF for increasing
    the security of IoT networks. The proposed DBRF39 provides enormous benefits such
    as simple design, high adaptation, great input noise tolerance, and online learning
    capability. Also, a robust networking systems can be designed extremely well owing
    to the characteristics of DBRF networks. It is a kind of learning model that distributes
    the input space among local kernels. A portion of these locally tailored kernel
    units are engaged for each input data point, depending on where in the input space
    it appears. It appears as though these local units have assigned each of them
    a portion of the input area to manage. The concept of locality itself suggests
    the requirement for a distance function that gauges how similar provided input
    data with dimensionality is to the center of each kernel unit. The Euclidean distance
    is computed between the input data and center for estimating the response function
    of the classifier. The concept behind employing such local models is that we define
    a basis function for each of these clusters if we presume that there are groups
    of data points in the training data. According to the non-linearity function,
    the DBRF can accurately predict the data into the corresponding class. Moreover,
    the hyperbolic function and error function are computed in this model during the
    training phase. Due to the intrinsic ability of the radial basis function network
    model to learn the underlying distribution of training data, the DBRF classifier
    is employed here. In this model, the Gaussian function \\(G_{f}\\) is estimated
    by using the input data and its center as shown in below: $$ G_{f} = exp\\left[
    { - \\frac{{\\left| {\\left| {D - q_{x} } \\right|} \\right|^{2} }}{{2\\sigma^{2}
    }}} \\right] $$ (17) where D indicates the input data, \\(q_{x}\\) is the center
    of kernel unit, and \\(\\sigma\\) denotes the standard deviation. Following the
    discovery of these cluster centers and spreads, the output of the response function
    is considered as the input to a perceptron as shown in below: $$ b = f\\left(
    {\\mathop \\sum \\limits_{x = 1}^{X} \\omega_{x} G_{f} + \\omega_{0} } \\right)
    $$ (18) where \\(f\\left( . \\right)\\) denotes the non-linearity function, X
    indicates the number of basis functions, \\(\\omega_{x}\\) represents the weight
    value associated to the unit x, and \\(\\omega_{0}\\) is the bias value. After
    that, the hyperbolic tanh function is applied to reduce the error rate at the
    time of training. Then, the function is computed as follows: $$ \\varepsilon =
    \\frac{1}{2}\\left( {k - b} \\right)^{2} $$ (19) $$ b = \\tanh \\left( m \\right)
    $$ (20) $$ m = \\mathop \\sum \\limits_{x = 1}^{X} \\omega_{x} G_{f} + \\omega_{0}
    . $$ (21) Consequently, the learning rate rule updation is performed, and the
    output class label is predicted as shown in below: $$ OC\\left( Y \\right) = \\left\\{
    {\\begin{array}{*{20}l} {Normal} \\hfill & {if,b \\ge \\left( {\\overline{b} -
    \\sigma_{B} } \\right)} \\hfill \\\\ {Intrusion} \\hfill & {if,b < (\\overline{b}
    - \\sigma_{B} } \\hfill \\\\ \\end{array} } \\right.. $$ (22) By using this model,
    the normal and intrusion classes are accurately predicted from the given IoT datasets.
    The primary benefits of using the proposed DRF-DBRF IoT security framework are
    as follows: Increased speed of training Accurate intrusion detection rate Easy
    to implement and understand Computational efficient Reduced overall time consumption
    Results This section validates the performance and results of the proposed DRF-DBRF
    security model by using various evaluation parameters. In this system, the most
    popular and different IoT benchmarking datasets are used to validate the system,
    which includes IoTID-20, NetFlow-IoT-v2, ToN-IoT, NSL-KDD, UNSW-NB 15. Moreover,
    the obtained results are compared with some of the baseline IoT IDS security frameworks
    for proving the superiority of the proposed model. The parameters used to assess
    the results are computed by using the following equations: $$ Accuracy = { }\\frac{TrP
    + TrN}{{TrP + TrN + FaP + FaN}} \\times 100{\\text{\\% }} $$ (23) $$ Precision
    = { }\\frac{TrP}{{TrP + FaP}} \\times 100{\\text{\\% }} $$ (24) $$ F1{\\text{-}}score
    = { }\\frac{2 \\times Pre \\times Sen}{{Pre + Sen}} \\times 100{\\text{\\% }}
    $$ (25) $$ Recall = { }\\frac{TrP}{{TrP + FaN}} \\times 100{\\text{\\% }} $$ (26)
    $$ Sensitivity = { }\\frac{TrP}{{TrP + FaN}} \\times 100{\\text{\\% }} $$ (27)
    $$ Specificity = { }\\frac{TrN}{{TrN + FaP}} \\times 100{\\text{\\% }} $$ (28)
    where TrP—true positive, TrN—true negative, FaP—false positive, and FaN—false
    negative. The list of datasets used to validate the system model are presented
    in Table 1. Table 1 List of IoT datasets used in this study. Full size table The
    dataset descriptions are provided for all these datasets with the number of samples
    and attacking classes in Tables 2, 3, 4, 5 and 6. These IoT datasets are extensively
    used in many network application systems for increasing the security of IoT networks.
    To assess the overall performance and intrusion detection efficiency of the proposed
    DRF-DBRF security model, these 6 different types of IoT datasets have been used
    in this work. Table 2 Dataset description of IoTID20. Full size table Table 3
    Dataset description of Netflow-ToN-IoT. Full size table Table 4 Dataset description
    of Netflow-BoT-IoT. Full size table Table 5 Dataset description for UNSW-NB-15
    dataset. Full size table Table 6 Dataset description for NSL-KDD. Full size table
    Table 7 and Fig. 3 compares the classification accuracy of the traditional and
    proposed AI based detection methodologies used for IoT security by using IoT-IDS20
    dataset40. Typically, the classification accuracy is one of the most prominent
    measure used to determine the overall intrusion detection rate of IDS framework.
    Consequently, the training time and accuracy (%) of the existing and proposed
    intrusion detection mechanisms are validated by using the IoT-IDS 20 dataset as
    shown in Table 8 and Fig. 4. In general, the increased classifier’s training time
    indicates the reduced performance of the detection system. Hence, the training
    time of classifier must be reduced to the maximum. According to the results, it
    is analyzed that the proposed DRF-DBRF technique outperforms the other classification
    approaches with increased classification accuracy and training time. Due to the
    utilization of DRF algorithm, the training speed and effectiveness of the classifier
    is highly improved. Table 7 Classification accuracy of IoT-ID20 dataset. Full
    size table Figure 3 Classification accuracy using IoTID-20 dataset. Full size
    image Table 8 Training time and accuracy analysis using IoT-ID20 dataset. Full
    size table Figure 4 Comparative analysis based on training time and accuracy using
    IoT-ID20 dataset. Full size image Table 7 compares the classification accuracy
    of the proposed DRF-DSRF mechanisms with that of the traditional NN, DT, LR, NB,
    and one-hot encoding models using the IoT-ID 20 dataset. Using data samples from
    the IoT-ID20 dataset, this assessment compares some of the most popular and widely
    utilized machine learning approaches with the proposed DRF-DBRF model. One of
    the most crucial and crucial parameters used to verify the attack detection effectiveness
    of the classifier is classification accuracy. The suggested DRF-DBRF model surpasses
    the other current machine learning algorithms with higher classification accuracy,
    as shown by the prediction results. The primary factor in the proposed framework''s
    enhanced performance is the use of the DRF optimization technique, which lowers
    the dimensionality of features prior to classification. Similar to this, Table
    8 compares and validates the training times and training accuracy of the proposed
    and standard models. Techniques including linear SVM, quadratic SVM, LDA, KNN,
    QDA, MLP, LSTM, AE, and DT are taken into account for this comparative analysis.
    In this work, the aforementioned classification algorithms are contrasted in order
    to assess the training performance of the suggested DBRF classification strategy.
    Final results show that the suggested DBRF outperforms traditional methods with
    higher training accuracy and shorter training times. The suggested DRF optimization
    technique extracts the pertinent subset of features from the provided intrusion
    dataset, speeding up the classifier''s training process and reducing the overall
    training time with high accuracy. Table 9 and Fig. 5 compares the precision conventional41
    and proposed security methodologies by using Netflow-BoT-IoT-v2 dataset. Typically,
    precision is the metric that assesses a performance of the model by determining
    how frequently the model''s forecast is accurate when it correctly foresees an
    occurrence. Consequently, Tables 10 and 11 compares the recall and f-measure values
    of existing and proposed IDS methodologies by using NetFlow-BoT-IoT-v2 dataset.
    Then, its corresponding graphical illustrations are represented in Figs. 6 and
    7. The recall is also termed as detection rate/true positive rate, which is an
    indicator of how well the machine learning model detected the occurrences of True
    Positives. Moreover, it validates that how well the model recognizes pertinent
    facts. Moreover, the total accuracy of classifier is determined based on the trade-off
    between recall and precision, which considers both false positives and false negatives.
    According to the improved values of these parameters, the overall detection efficacy
    of the classifier is determined. For class-wise evaluation of the classifier''s
    output, these criteria are helpful. The harmonic mean of recall and precision
    is the F score, if the maximum value of 1, which denotes the perfect precision
    and recall, and a minimum value of 0 can occur when either precision or recall
    is zero. The F score is also more useful criteria than accuracy in classes with
    unequal distribution. Based on the overall analysis, it is observed that the proposed
    DRF-DBRF outperforms the other approaches with increased precision, recall, and
    f1-score values. Due to the proper dataset balancing and attributes tuning, the
    training of classifier is highly improved, which helps to improve these parameters.
    Table 9 Precision using NetFlow-BoT-IoT-v2. Full size table Figure 5 Precision
    analysis using NetFlow-BoT-IoT-v2. Full size image Table 10 Recall using NetFlow-BoT-IoT-v2.
    Full size table Table 11 F1-score using NetFlow-BoT-IoT-v2. Full size table Figure
    6 Comparative analysis based on recall using NetFlow-BoT-IoT-v2. Full size image
    Figure 7 Comparative analysis based on f1-score using NetFlow-BoT-IoT-v2. Full
    size image Figure 8 and Table 12 compares the accuracy, recall, and f1-score of
    the existing and proposed classification methodologies by using ToN-IoT dataset.
    In Table 12, the conventional approaches including, Gini Impurity based Weighted
    Random Forest (GIWRF) integrated with Decision Tree (DT) and Random Forest (RF)
    are considered into account for comparison. In order to analyze the intrusion
    detection efficacy and competence of the proposed security model, various IoT
    datasets are considered in this study during evaluation. The observed results
    also indicate that the combination of DRF-DBRF outperforms the other existing
    models with increased accuracy, recall, and f1-score values. Figure 8 Performance
    evaluation using ToN-IoT dataset. Full size image Table 12 Comparative analysis
    based on ToN-IoT dataset. Full size table Tables 13 and 14 presents the overall
    comparative analysis of the existing42 and proposed anomaly detection methodologies
    by using the UNSW-NB 15 and ToN-IoT datasets respectively. Then, its corresponding
    graphical evaluations are depicted in Figs. 9 and 10. The proposed solution once
    more outperforms the other methods in terms of high performance values. The methods
    are compared in this experiment based on how well they can predict the actual
    classes of dataset records. The overall performance findings on this dataset further
    support the proposed approach''s superiority over previous classification approaches.
    Table 13 Overall comparative analysis using UNSW-NB 15 dataset. Full size table
    Table 14 Overall comparative analysis using ToN-IoT dataset. Full size table Figure
    9 Overall performance analysis based on UNSW-NB15 dataset. Full size image Figure
    10 Overall performance analysis based on ToN-IoT dataset. Full size image Table
    15 and Fig. 11 compares the Attack Detection Rate (ADR) of previous and proposed
    anomaly detection methodologies by using the NSL-KDD dataset. In comparison to
    the existing study, models created using the proposed approach are more capable
    of effectively classifying the legitimate and malicious data flows from the given
    IoT datasets. Table 15 Comparative analysis of ADR between existing and proposed
    IDS frameworks. Full size table Figure 11 Attack detection rate using NSL-KDD
    dataset. Full size image Typically, the computational complexity is one of the
    most essential parameter used to assess the overall efficacy of the algorithm.
    Moreover, the computational cost of optimization can be computed according to
    the parameters of population size, dimension of the problem, and number of iterations
    required to reach the optimal solution. In this security framework, the proposed
    DRF mechanism requires the maximum of 100 number of iterations with the population
    size of 30. Table 16 validates the computational time of the existing15 and proposed
    optimization based IDS methodologies using different IoT-IDS datasets. The existing
    techniques considered in this evaluation are Harris Hawks Optimization (HHO),
    Gorilla Troop Optimizer-Binary Swarm Algorithm (GTO-BSA), and Hunger Game Search
    (HGS). The results demonstrates that the proposed DBRF classifier outperforms
    the other classifiers with the inclusion of DRF optimization algorithm. In addition,
    Table 17 validates and compares the accuracy of some of the most extensively classification
    approaches in the intrusion detection systems with the proposed DRF-DBRF approach.
    For this assessment, all of five datasets used in this study are considered into
    account for comparison. The findings state that the proposed DRF-DBRF outperforms
    the other classification approaches with increased accuracy. Table 16 Computational
    time analysis. Full size table Table 17 Accuracy of conventional and proposed
    classifiers using all intrusion datasets. Full size table Conclusion This paper
    presents an enhanced DRF-DBRF classification model addressing the intrusion detection
    problems in the IoT systems. Initially, the data normalization is performed with
    the operations of NaN values handling, categorical feature extraction, and missing
    field identification. The NaN processing is carried out in this case to identify
    the missing values in the supplied data, which contributes to improving the precision
    of intrusion detection. Since the categorical feature is processed before being
    fed into the classification stage, the handling of the categorical feature is
    conducted NaN handling. Additionally, the missing values are located and dealt
    with in order to create the standardized dataset. Here, the DRF optimization approach
    is used to extract the pertinent features from the balanced IoT datasets, which
    speeds up the classifier''s training process. When compared to the optimization
    techniques, the primary reasons of using the DRF algorithms are as follows: increased
    convergence rate, training speed, and reduced overfitting. Based on the optimized
    characteristics, the DBRF classification process is used to identify and classify
    the type of intrusions. The radial basis function network model’*s inherent capacity
    to understand the underlying distribution of training data is the reason the DBRF
    classifier is used in this context. The normal and intrusion classes are correctly
    predicted from the provided IoT datasets based on the learning rule update. Moreover,
    the performance of the proposed DRF-DBRF model is validated and tested by using
    five different datasets, and the estimated results are compared with the recent
    anomaly detection approaches. From the overall observed results, it is analyzed
    that the combination of DRF-DBRF overwhelms the other anomaly detection techniques
    with increased precision (99%), accuracy (99.2%), recall (99%), and f1-score (98.9%).
    Moreover, the results are highly superior to the existing techniques, which shows
    the improved performance and competence of the proposed model. In future, the
    present work can be extended by implementing the IDS framework to the IoT integrated
    smart application systems. Data availability The data that support the findings
    of this study are available from the corresponding author, upon reasonable request.
    References Ellappan, V. et al. Sliding principal component and dynamic reward
    reinforcement learning based IIoT attack detection. Sci. Rep. 13, 20843. https://doi.org/10.1038/s41598-023-46746-0
    (2023). Article   ADS   CAS   PubMed Central   PubMed   Google Scholar   Selvarajan,
    S. et al. An artificial intelligence lightweight blockchain security model for
    security and privacy in IIoT systems. J Cloud Comput 12, 12–38 (2023). Article   Google
    Scholar   Prasanth, S. K., Shitharth, S., PraveenKumar, B., Subedha, V. & Sangeetha,
    K. Optimal feature selection based on evolutionary algorithm for intrusion detection.
    SN Comput. Sci. https://doi.org/10.1007/s42979-022-01325-4 (2022). Article   Google
    Scholar   Saif, S., Das, P., Biswas, S., Khari, M. & Shanmuganathan, V. HIIDS:
    Hybrid intelligent intrusion detection system empowered with machine learning
    and metaheuristic algorithms for application in IoT based healthcare. Microprocess.
    Microsyst. 104622 (2022). Shitharth, S., Kshirsagar, P. R., Balachandran, P. K.,
    Alyoubi, K. H. & Khadidos, A. O. An Innovative Perceptual Pigeon Galvanized Optimization
    (PPGO) Based Likelihood Naïve Bayes (LNB) classification approach for network
    intrusion detection system. IEEE Access 10, 46424–46441. https://doi.org/10.1109/ACCESS.2022.3171660
    (2022). Article   Google Scholar   Shitharth, S. et al. Development of edge computing
    and classification using the internet of things with incremental learning for
    object detection. Internet Things https://doi.org/10.1016/j.iot.2023.100852 (2023).
    Article   Google Scholar   Mohammad, G. B. et al. Mechanism of internet of things
    (IoT) integrated with radio frequency identification (RFID) technology for healthcare
    system. Math. Probl. Eng. https://doi.org/10.1155/2022/4167700 (2022). Article   Google
    Scholar   Shitharth, S., Satheesh, N., Kumar, B. P. & Sangeetha, K. Architectural
    Wireless Networks Solutions and Security Issues 247–265 (Springer, 2021). Book   Google
    Scholar   Saba, T., Rehman, A., Sadad, T., Kolivand, H. & Bahaj, S. A. Anomaly-based
    intrusion detection system for IoT networks through deep learning model. Comput.
    Electr. Eng. 99, 107810 (2022). Article   Google Scholar   Mehedi, S. T., Anwar,
    A., Rahman, Z., Ahmed, K. & Rafiqul, I. Dependable intrusion detection system
    for IoT: A deep transfer learning-based approach. IEEE Trans. Ind. Inform. (2022).
    Tharewal, S. et al. Intrusion detection system for industrial Internet of Things
    based on deep reinforcement learning. Wirel. Commun. Mob. Comput. 2022 (2022).
    Selvarajan, S. et al. SCBC: Smart city monitoring with blockchain using Internet
    of Things for and neuro fuzzy procedures. Math. Biosci. Eng. 20(12), 20828–20851.
    https://doi.org/10.3934/mbe.2023922 (2023). Article   MathSciNet   PubMed   Google
    Scholar   Yadav, N., Pande, S., Khamparia, A. & Gupta, D. Intrusion detection
    system on IoT with 5G network using deep learning. Wirel. Commun. Mob. Comput.
    2022 (2022). Rabie, O. B. J. et al. A full privacy-preserving distributed batch-based
    certificate-less aggregate signature authentication scheme for healthcare wearable
    wireless medical sensor networks (HWMSNs). Int. J. Inf. Secur. https://doi.org/10.1007/s10207-023-00748-1
    (2023). Article   Google Scholar   Shitharth, S., Manoharan, H., Shankar, A.,
    Alsowail, R. A. & Pandiaraj, S. Federated learning optimization: A computational
    blockchain process with offloading analysis to enhance security. Egypt. Inform.
    J. 24(4), 100406. https://doi.org/10.1016/j.eij.2023.100406 (2023). Article   Google
    Scholar   Dahou, A. et al. Intrusion detection system for IoT based on deep learning
    and modified reptile search algorithm. Comput. Intell. Neurosci. 2022 (2022).
    Sarhan, M., Layeghy, S., Moustafa, N., Gallagher, M. & Portmann, M. Feature extraction
    for machine learning-based intrusion detection in IoT networks. Digit. Commun.
    Netw. (2022). Tsimenidis, S., Lagkas, T. & Rantos, K. Deep learning in IoT intrusion
    detection. J. Netw. Syst. Manag. 30, 1–40 (2022). Article   Google Scholar   Mahadik,
    S., Pawar, P. M. & Muthalagu, R. Efficient intelligent intrusion detection system
    for heterogeneous internet of things (HetIoT). J. Netw. Syst. Manag. 31, 1–27
    (2023). Article   Google Scholar   Gu, Z., Nazir, S., Hong, C. & Khan, S. Convolution
    neural network-based higher accurate intrusion identification system for the network
    security and communication. Secur. Commun. Netw. 2020 (2020). Alsoufi, M. A. et
    al. Anomaly-based intrusion detection systems in IoT using deep learning: A systematic
    literature review. Appl. Sci. 11, 8383 (2021). Article   CAS   Google Scholar   Mishra,
    N. & Pandya, S. Internet of things applications, security challenges, attacks,
    intrusion detection, and future visions: A systematic review. IEEE Access 9, 59353–59377
    (2021). Article   Google Scholar   Fatani, A., Dahou, A., Al-Qaness, M. A., Lu,
    S. & Elaziz, M. A. Advanced feature extraction and selection approach using deep
    learning and Aquila optimizer for IoT intrusion detection system. Sensors 22,
    140 (2021). Article   ADS   PubMed Central   PubMed   Google Scholar   Abd Elaziz,
    M., Al-qaness, M. A., Dahou, A., Ibrahim, R. A. & Abd El-Latif, A. A. Intrusion
    detection approach for cloud and IoT environments using deep learning and Capuchin
    Search Algorithm. Adv. Eng. Softw. 176, 103402 (2023). Article   Google Scholar   Aslam,
    M. et al. Adaptive machine learning based distributed denial-of-services attacks
    detection and mitigation system for SDN-enabled iot. Sensors 22, 2697 (2022).
    Article   ADS   PubMed Central   PubMed   Google Scholar   Smys, S., Basar, A.
    & Wang, H. Hybrid intrusion detection system for internet of things (IoT). J.
    ISMAC 2, 190–199 (2020). Article   Google Scholar   Almiani, M., AbuGhazleh, A.,
    Al-Rahayfeh, A., Atiewi, S. & Razaque, A. Deep recurrent neural network for IoT
    intrusion detection system. Simul. Model. Pract. Theory 101, 102031 (2020). Article   Google
    Scholar   Verma, A. & Ranga, V. Machine learning based intrusion detection systems
    for IoT applications. Wirel. Pers. Commun. 111, 2287–2310 (2020). Article   Google
    Scholar   Anthi, E., Williams, L., Słowińska, M., Theodorakopoulos, G. & Burnap,
    P. A supervised intrusion detection system for smart home IoT devices. IEEE Internet
    Things J. 6, 9042–9053 (2019). Article   Google Scholar   Al-Hadhrami, Y. & Hussain,
    F. K. Real time dataset generation framework for intrusion detection systems in
    IoT. Future Gener. Comput. Syst. 108, 414–423 (2020). Article   Google Scholar   Benkhelifa,
    E., Welsh, T. & Hamouda, W. A critical review of practices and challenges in intrusion
    detection systems for IoT: Toward universal and resilient systems. IEEE Commun.
    Surv. Tutor. 20, 3496–3509 (2018). Article   Google Scholar   Qureshi, A. U. H.,
    Larijani, H., Ahmad, J. & Mtetwa, N. Intelligent Computing-Proceedings of the
    Computing Conference 86–98 (Springer, 2019). Google Scholar   Kumar, V., Das,
    A. K. & Sinha, D. UIDS: A unified intrusion detection system for IoT environment.
    Evol. Intell. 14, 47–59 (2021). Article   Google Scholar   Padmaja, M. et al.
    Grow of artificial intelligence to challenge security in IoT application. Wirel.
    Pers. Commun. 1–17 (2021). Połap, D. & Woźniak, M. Red fox optimization algorithm.
    Expert Syst. Appl. 166, 114107 (2021). Article   Google Scholar   Liu, J., Gao,
    Y. & Hu, F. A fast network intrusion detection system using adaptive synthetic
    oversampling and LightGBM. Comput. Secur. 106, 102289 (2021). Article   Google
    Scholar   Liu, L., Wang, P., Lin, J. & Liu, L. Intrusion detection of imbalanced
    network traffic based on machine learning and deep learning. IEEE Access 9, 7550–7563
    (2020). Article   Google Scholar   Seth, S., Singh, G. & Kaur Chahal, K. A novel
    time efficient learning-based approach for smart intrusion detection system. J.
    Big Data 8, 1–28 (2021). Article   Google Scholar   Deng, Y. et al. New methods
    based on back propagation (BP) and radial basis function (RBF) artificial neural
    networks (ANNs) for predicting the occurrence of haloketones in tap water. Sci.
    Total Environ. 772, 145534 (2021). Article   ADS   CAS   PubMed   Google Scholar   Dat-Thinh,
    N., Xuan-Ninh, H. & Kim-Hung, L. MidSiot: A multistage intrusion detection system
    for internet of things. Wirel. Commun. Mob. Comput. 2022, 9173291. https://doi.org/10.1155/2022/9173291
    (2022). Article   Google Scholar   Awad, M., Fraihat, S., Salameh, K. & Al Redhaei,
    A. Examining the suitability of NetFlow features in detecting IoT network intrusions.
    Sensors 22, 6164 (2022). Article   ADS   PubMed Central   PubMed   Google Scholar   Disha,
    R. A. & Waheed, S. Performance analysis of machine learning models for intrusion
    detection system using Gini Impurity-based Weighted Random Forest (GIWRF) feature
    selection technique. Cybersecurity 5, 1. https://doi.org/10.1186/s42400-021-00103-8
    (2022). Article   Google Scholar   Download references Funding This project was
    funded by the Deanship of Scientific Research (DR) at King Abdulaziz University
    (KAU), Jeddah, Saudi Arabia has funded this Project, under Grant No. (RG-9-611-43).
    The authors, therefore, acknowledge with thanks DSR for technical and financial
    support. Author information Authors and Affiliations Department of Information
    Systems, Faculty of Computing and Information Technology, King Abdulaziz University,
    Jeddah, Kingdom of Saudi Arabia Osama Bassam J. Rabie, Tawfiq Hasanin & Abdulrhman
    M. Alshareef Cybersecurity Center, King Abdulaziz University, Jeddah, Kingdom
    of Saudi Arabia Osama Bassam J. Rabie School of Built Environment, Engineering
    and Computing, Leeds Beckett University, Leeds, LS1 3HE, UK Shitharth Selvarajan
    Department of Computer Science, Kebri Dehar University, Kebri Dehar, Ethiopia
    Shitharth Selvarajan School of Computer Science and Engineering, ViT Chennai Campus,
    Chennai, India C. K. Yogesh College of Computing and IT, University of Doha for
    Science and Technology, 24449, Doha, Qatar Mueen Uddin Contributions O.B.J.R.:
    Conceptualization, project administration, supervision, writing—original draft,
    writing—review and editing; S.S.: Conceptualization, project administration, supervision,
    writing—original draft, writing—review and editing; T.H.: Data curation, methodology,
    software; A.M.A.: Data curation, resources, software; revision and editing; C.K.Y.
    and M.U. Corresponding author Correspondence to Shitharth Selvarajan. Ethics declarations
    Competing interests The authors declare no competing interests. Additional information
    Publisher''s note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Open Access This article is licensed under a Creative Commons Attribution 4.0
    International License, which permits use, sharing, adaptation, distribution and
    reproduction in any medium or format, as long as you give appropriate credit to
    the original author(s) and the source, provide a link to the Creative Commons
    licence, and indicate if changes were made. The images or other third party material
    in this article are included in the article''s Creative Commons licence, unless
    indicated otherwise in a credit line to the material. If material is not included
    in the article''s Creative Commons licence and your intended use is not permitted
    by statutory regulation or exceeds the permitted use, you will need to obtain
    permission directly from the copyright holder. To view a copy of this licence,
    visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions About
    this article Cite this article Rabie, O.B.J., Selvarajan, S., Hasanin, T. et al.
    A novel IoT intrusion detection framework using Decisive Red Fox optimization
    and descriptive back propagated radial basis function models. Sci Rep 14, 386
    (2024). https://doi.org/10.1038/s41598-024-51154-z Download citation Received
    03 June 2023 Accepted 01 January 2024 Published 03 January 2024 DOI https://doi.org/10.1038/s41598-024-51154-z
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Subjects Electrical and electronic engineering Engineering Comments
    By submitting a comment you agree to abide by our Terms and Community Guidelines.
    If you find something abusive or that does not comply with our terms or guidelines
    please flag it as inappropriate. Download PDF Sections Figures References Abstract
    Introduction Related works Methods Results Conclusion Data availability References
    Funding Author information Ethics declarations Additional information Rights and
    permissions About this article Comments Advertisement Scientific Reports (Sci
    Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press releases Press
    office Contact us Discover content Journals A-Z Articles by subject Protocol Exchange
    Nature Index Publishing policies Nature portfolio policies Open access Author
    & Researcher services Reprints & permissions Research data Language editing Scientific
    editing Nature Masterclasses Research Solutions Libraries & institutions Librarian
    service & tools Librarian portal Open research Recommend to library Advertising
    & partnerships Advertising Partnerships & Services Media kits Branded content
    Professional development Nature Careers Nature Conferences Regional websites Nature
    Africa Nature China Nature India Nature Italy Nature Japan Nature Korea Nature
    Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies
    Legal notice Accessibility statement Terms & Conditions Your US state privacy
    rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A novel IoT intrusion detection framework using Decisive Red Fox optimization
    and descriptive back propagated radial basis function models
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Eid A.M.
  - Soudan B.
  - Nasif A.B.
  - Injadat M.N.
  citation_count: '0'
  description: This study investigates the effectiveness of six prominent machine
    learning models—random forest, decision trees, K-nearest neighbor, logistic regression,
    support vector machines, and Naïve Bayes—for intrusion detection systems in industrial
    Internet of Things environments. The evaluation encompasses the effects of data
    preprocessing techniques, including feature engineering, data normalization, recoding,
    and missing data mitigation. Furthermore, the research delves into dataset balancing,
    examining the effects of six different techniques on model performance. The investigations
    are conducted using the domain-specific WUSTL-IIOT-2021 dataset, which captures
    the unique characteristics of IIoT data. The study also investigates multi-class
    attack identification utilizing an innovative SMOTE-based multi-class balancing
    approach to tackle dataset imbalances. The results indicate that data preprocessing
    and intelligent dataset balancing produce consistent enhancements in the classification
    performance of the selected models across binary and multi-classification tasks.
    Random forest emerges as the standout algorithm, delivering consistently high
    performance with computational efficiency.
  doi: 10.1007/s00521-024-09439-x
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Neural Computing and Applications
    Article Comparative study of ML models for IIoT intrusion detection: impact of
    data preprocessing and balancing Original Article Published: 11 February 2024
    Volume 36, pages 6955–6972, (2024) Cite this article Download PDF Access provided
    by University of Nebraska-Lincoln Neural Computing and Applications Aims and scope
    Submit manuscript Abdulrahman Mahmoud Eid , Bassel Soudan , Ali Bou Nasif & MohammadNoor
    Injadat   256 Accesses 1 Altmetric Explore all metrics Abstract This study investigates
    the effectiveness of six prominent machine learning models—random forest, decision
    trees, K-nearest neighbor, logistic regression, support vector machines, and Naïve
    Bayes—for intrusion detection systems in industrial Internet of Things environments.
    The evaluation encompasses the effects of data preprocessing techniques, including
    feature engineering, data normalization, recoding, and missing data mitigation.
    Furthermore, the research delves into dataset balancing, examining the effects
    of six different techniques on model performance. The investigations are conducted
    using the domain-specific WUSTL-IIOT-2021 dataset, which captures the unique characteristics
    of IIoT data. The study also investigates multi-class attack identification utilizing
    an innovative SMOTE-based multi-class balancing approach to tackle dataset imbalances.
    The results indicate that data preprocessing and intelligent dataset balancing
    produce consistent enhancements in the classification performance of the selected
    models across binary and multi-classification tasks. Random forest emerges as
    the standout algorithm, delivering consistently high performance with computational
    efficiency. Similar content being viewed by others Intrusion detection based on
    ensemble learning for big data classification Article 07 November 2023 A Two-Stage
    Classifier Approach for Network Intrusion Detection Chapter © 2018 Evaluating
    the impact of filter-based feature selection in intrusion detection systems Article
    17 October 2023 1 Introduction Industrial Internet of Things (IIoT) networks play
    a crucial role in interconnecting diverse components of industrial control systems
    (ICS), enabling efficient monitoring of industrial processes and equipment. However,
    IIoT networks differ significantly from conventional IT networks [1, 2]. These
    disparities need to be thoroughly acknowledged when tackling the security challenges
    in IIoT networks. The unique behaviors and attributes of these two network types
    underscore the importance of tailored solutions that cater to the distinctive
    requirements of IIoT environments. The proliferation of IIoT implementations across
    diverse industries has facilitated seamless communication between interconnected
    devices and systems [3, 4]. Nonetheless, this heightened connectivity has concurrently
    rendered IIoT networks more susceptible to cyberattacks [5,6,7]. In response,
    organizations have turned to the development of intrusion detection systems (IDSs)
    as a means to bolster the security of their IIoT networks [8,9,10,11]. These systems
    play a vital role in identifying potential cyber threats and enabling prompt responses
    to safeguard the integrity of IIoT networks. 1.1 Related work In recent years,
    there has been growing interest in the literature in exploring the application
    of different ML algorithms for the development of IDS systems for mitigating attacks
    on IIoT networks [12]. Support vector machine model (SVM) was assessed for its
    potential in detecting compromises within an industrial system [13]. The researchers
    simulated a reconnaissance attack, where a command injection exploit was targeted
    at the ICS controller. The algorithm was implemented using MATLAB and validated
    on a simulated test bed employing the Tennessee Eastman (TE) dataset. While the
    researchers highlighted their model’s real-time anomaly detection capabilities,
    the specific detection accuracy of their implementation was not explicitly reported.
    Another study integrated the J48 and I Bayes approaches in the development of
    an IDS for IIoT networks [14]. The researchers used the labeled remote terminal
    unit (RTU) telemetry data from the gas pipeline system at Mississippi State University’s
    Critical Infrastructure Protection Center (CIPC) as the dataset for training and
    evaluation. Experimental results achieved an accuracy of 99.5% for the specific-attack-labeled
    cases. However, no other performance metrics were investigated. Vulfin et al.
    conducted a study focusing on detecting reconnaissance attacks on ICS systems
    [15]. They evaluated a number of ML algorithms such as random forest (RF), logistic
    regression (LR), and multilayer perceptron (MLP) using the WUSTL-IIOT-2018 dataset.
    Their proposed models achieved F1-scores ranging between 91 and 95% across different
    scenarios. A study was undertaken to assess the effectiveness of diverse ML algorithms
    in detecting compromises triggered by command and injection attacks [16]. The
    researchers evaluated the I Bayes, RF, J48, non-nested generalized exemplars,
    SVM, and OneR algorithms. The models were trained and evaluated using the Mississippi
    State University’s RTU dataset. The researchers reported that RF excelled in all
    classes, perfectly classifying normal responses, and achieving recall/precision
    values better than 75% for five response injection classes. Researchers explored
    the use of SVM for identifying network traffic abnormalities [17]. Their investigation
    employed simulations based on the IEEE 118 bus network, effectively distinguishing
    between normal and fault scenarios. They reported that their IDS was able to maintain
    the system free from cyberattacks throughout the study. However, their highest-performing
    model could not achieve more than 95% accuracy. Unfortunately, the researchers
    did not consider any other performance metrics. Similarly, an anomaly-based IDS
    leveraging the SVM algorithm was assessed for intrusion detection in electric
    grid traffic, utilizing solely two attributes, “data rate” and “packet size” [18].
    The training dataset was collected from a SCADA system during regular operation.
    The study reported a classification accuracy of 98.8%. However, it is essential
    to note that their training dataset lacked threat-related data, and the performance
    evaluation was based on accuracy exclusively. Song et al. evaluated the ability
    of an IDS to detect availability compromises in an IIoT network [19]. The proposed
    IDS employed the extra trees classifier, RF, and K-nearest neighbor (KNN) algorithms.
    The models were trained using the WUSTL-IIOT-2018 dataset and validated using
    the cybersecurity Modbus ICS dataset. They reported that extra trees classifier
    and RF models achieved accuracies of 99.0%. Zolanvari et al. in [20] evaluated
    multiple ML algorithms such as RF, decision trees (DT), KNN, LR, SVM, artificial
    neural networks (ANNs), and Naïve Bayes (NB) for the development of an IDS. Their
    models were trained and evaluated using the recently developed WUSTL-IIOT-2021
    dataset [21]. The authors utilized several performance metrics to evaluate the
    efficacy of the proposed IDS. They reported that RF achieved the highest performance
    with an accuracy score of 99.99% and a Matthews correlation coefficient (MCC)
    score of 96.81%. Conversely, NB achieved 97.48% accuracy but only 24.4% MCC. They
    attributed the low performance of NB to the significant impact of dataset imbalance.
    1.2 Gap analysis for existing works Tables 1 and 2 present a comprehensive assessment
    of the literature related to the development of IDS systems for IIoT networks
    considering various aspects such as the ML models employed, and the datasets utilized
    for evaluation purposes. It can be seen that the collective literature suffers
    from the following gaps: 1. Limited comparative analysis of the ML algorithms
    Table 1 shows that each of the previous works concentrated on the evaluation of
    either one or a very limited number of models and that there is a deficiency in
    comparative analysis of the different algorithms. Such a comparative analysis
    would provide valuable insight into the strengths and weaknesses of the individual
    algorithms and would allow identification of the most effective ML algorithms
    for developing robust and accurate IDS models. 2. Limited dataset relevance Table
    2 shows that many of the examined works rely on outdated, non-domain-specific,
    private, or simulated datasets for training and evaluation purposes. While these
    models often demonstrate high accuracy within the limitations of their datasets,
    their efficacy in detecting novel cyberattack scenarios remains questionable.
    The lack of up-to-date and representative datasets poses a significant challenge
    in developing robust and reliable IDS models for IIoT systems. 3. Absence of consideration
    for data preprocessing it is noted that the majority of the reviewed works did
    not incorporate consideration for data preprocessing. It is well known that the
    efficacy of ML models is markedly influenced by the quality of input data furnished
    to the model [22, 23]. Hence, it is crucial to consider data preprocessing in
    the development of the IDS model to increase detection accuracy. 4. Absence of
    consideration for multi-class identification of attacks it is observed that the
    IDS systems proposed in the literature are mainly used for binary classification
    between normal and abnormal behavior. Very little work has been done on detection
    and classification of multiple attack scenarios. Table 1 Categorization of the
    IDS systems in the literature based on ML model Full size table Table 2 Categorization
    of the IDS systems in the literature based on dataset Full size table In light
    of the aforementioned research gaps, this work endeavors to perform a comprehensive
    comparison between the six ML algorithms commonly used in the existing literature
    for the implementation of IDS systems for IIoT networks. This work will also investigate
    utilization of the recently developed WUSTL-IIOT-2021 dataset since it is more
    specific for the domain of IIoT networks. Furthermore, this work will evaluate
    the effect of data preparation strategies on the performance of the models from
    the literature. Finally, this work will investigate the performance of these techniques
    for multi-classification between different intrusion scenarios. In particular,
    this work extends beyond the work of reference [20], which used the default WUSTL-IIOT-2021
    dataset without considering the effects of dataset balancing and also did not
    consider the possibility of discriminating the attack type through multi-class
    classification. 1.3 Contributions The contributions of this work can be summarized
    into the following points: 1. Exploring the effectiveness of various dataset balancing
    techniques by assessing their impact on the performance of IDS models in detecting
    cyberattacks in IIoT networks. 2. Evaluating the performance of the leading six
    ML algorithms commonly used in intrusion detection research for IIoT using the
    domain-specific WUSTL-IIoT-2021. Dataset. 3. Evaluating the comparative performance
    of these six ML algorithms under varying conditions involving dataset normalization
    and balancing techniques. 4. Implementing a novel multi-class dataset balancing
    and investigating the effectiveness of the leading models from the literature
    in multi-classification of attacks on an IIoT network. 2 Background This section
    will introduce some of the technical concepts that will be needed for the rest
    of the discussion. In particular, this section will give a brief introduction
    to the different ML algorithms used in the literature as well as an introduction
    to techniques used for balancing classes within a training dataset. 2.1 ML models
    commonly used for developing IDS systems for IIoT networks The summary in Table
    3 shows that researchers have opted to use mainly six ML algorithms for the development
    of IDS systems for IIoT networks [24]. A brief introduction and the main characteristics
    of these algorithm in relevance to intrusion detection in IIoT networks is summarized
    in Table 3. Table 3 The highlights of the chosen ML models for IIoT IDS Full size
    table 2.2 Data balancing Class imbalance is a common challenge affecting the performance
    of machine learning models where one class exhibits a higher attribute rate compared
    to others. This imbalance can significantly impact the accuracy of the model,
    particularly in classification operations. Optimal performance of most algorithms
    is observed when there is balanced representation across classes, aiming to minimize
    false rates and improve accuracy. However, when faced with imbalanced data, the
    model may excel in predicting the majority class but struggle with the minority
    class, which often holds greater significance. To address this issue, various
    algorithms have been developed for improving the class distribution within a dataset,
    as shown in Fig. 1. The following subsections will briefly introduce each of these
    techniques. Fig. 1 Techniques for handling imbalanced data Full size image 2.2.1
    Cost-sensitive learning (CSL) Cost-sensitive learning (CSL) as a means of inducing
    cost-sensitive trees, showcasing its practicality and ease of integration with
    existing ML algorithms [25]. The core principle involves assigning distinct weights
    to the minority classes, enabling the ML model to prioritize their importance.
    CSL is particularly useful in the context of IDS for IIoT due to its ability to
    address the inherent class imbalance that often characterizes intrusion detection
    datasets, thereby improving the accuracy of detecting critical cyber threats.
    2.2.2 Data level preprocessing Data level preprocessing involves manipulating
    the dataset to achieve a balanced distribution. As depicted in Fig. 1, this can
    be achieved by either oversampling the minority classes or under-sampling the
    majority classes. 2.2.2.1 Under-sampling Under-sampling involves randomly removing
    instances from the majority class until a balanced distribution is achieved. While
    under-sampling effectively improves class distribution and helps address the challenges
    posed by skewed datasets, it may lead to information loss due to the removal of
    data points. Careful consideration of the trade-offs between improved balance
    and potential loss of data patterns is essential when applying under-sampling
    techniques to enhance the performance of IDS models in IIoT networks. 2.2.2.2
    Over-sampling On the other hand, over-sampling operates by increasing the representation
    of the minority class. This can be achieved through attribute duplication or synthetic
    minority oversampling technique (SMOTE). In attribute duplication, attributes
    from the minority class are replicated to match the number of instances in the
    majority class. This essentially involves creating copies of existing data points
    from the minority class. While this method may help balance class distribution,
    it carries the risk of introducing redundancy and potentially overfitting the
    model to the minority class, which can lead to reduced generalization capabilities.
    SMOTE on the other hand addresses imbalanced class distribution by generating
    synthetic attributes for the minority class [46]. Synthetic samples are inserted
    along line segments connecting each minority class instance with its K-nearest
    neighbors [25]. The newly created instances contribute to a more balanced class
    distribution while also introducing diversity into the dataset. SMOTE can help
    alleviate the risk of overfitting and improve the ability of the model to recognize
    complex decision boundaries, thus enhancing its generalization performance. Both
    over-sampling techniques can be advantageous in addressing class imbalance, but
    as with under-sampling, careful consideration is needed to avoid potential drawbacks.
    Over-sampling may increase the risk of overfitting, and synthetic instances generated
    by SMOTE should reflect meaningful patterns present in the data to ensure the
    model’s improved performance on unseen samples. 2.2.3 Ensemble learning 2.2.3.1
    K-means clustering Cluster-based majority under-sampling was developed for choosing
    a subset from the majority class that is representative [26]. As compared to random
    under-sampling, cluster-based under-sampling can efficiently prevent loss of crucial
    majority class information. This technique uses cluster centroids as reduced data
    points. Therefore, instead of directly sampling from the data, it generates artificial
    samples that represent the majority class. 2.2.3.2 Bagging and pasting In [27],
    researchers introduced the concept of bootstrap aggregating to construct ensembles,
    whereby multiple classifiers are trained using bootstrapped copies of the original
    training dataset. This involves generating new datasets by randomly selecting
    instances from the initial dataset. 3 Methodology The diagram in Fig. 2 demonstrates
    the main step of the implementation and evaluation methodology for this work.
    First, multiple data cleaning steps were applied to the dataset to enhance its
    quality and reliability. Subsequently, categorical features were encoded using
    one-hot encoding enabling their conversion into a suitable representation for
    subsequent analysis. Simultaneously, the data types of numerical features were
    standardized to achieve uniformity across the dataset. Following these transformations,
    both categorical and numerical features underwent a normalization process, thereby
    facilitating consistent and comparable scaling of the data. Fig. 2 Flowchart for
    the evaluation methodology Full size image Upon completion of the data preprocessing
    stage, the dataset was partitioned into an 80% training set and a 20% testing
    set. To address class imbalance and mitigate bias, six balancing algorithms were
    applied to the training set, while keeping the testing set intact. Subsequently,
    six ML models were constructed using the training set, and their performance was
    evaluated using the testing set. To assess the effectiveness of the proposed model,
    various evaluation metrics were calculated based on the prediction results of
    the various ML models. The following subsections will discuss the steps of the
    methodology in detail. But, first a discussion of the selected dataset and its
    characteristics. 3.1 Dataset The selection of the WUSTL-IIoT-2021 dataset for
    this study is underpinned by several compelling justifications. Firstly, this
    dataset is specifically curated for IIoT networks. It was designed to mimic real-world
    industrial operations encompassing a wide range of network activities encountered
    in industrial environments [20]. This contextual relevance ensures that the dataset
    captures the challenges, intricacies, and real-world nuances of IIoT environments.
    Moreover, the WUSTL-IIoT-2021 dataset consists of normal and attack traffic, covering
    four distinct attack scenarios: DoS, reconnaissance, command injection, and backdoor
    attacks. This diversity facilitates a holistic evaluation of the performance across
    a spectrum of scenarios. Furthermore, the availability of ground truth labels
    for the dataset provides a solid foundation for evaluating and benchmarking the
    performance. The presence of labeled instances allows quantifying metrics like
    accuracy, precision, recall, and F1-score. Table 4 illustrates the distribution
    of the dataset across the different classes, while Fig. 3 presents the binary
    distribution of the dataset between normal and attack instances. All records from
    the various attacks were grouped under “Malicious.” Table 4 Distribution of traffic
    in the WUSTL-IIoT-2021 dataset Full size table Fig. 3 Binary distribution of the
    WUSTL-IIOT-2021 dataset (normal vs. malicious) Full size image 3.2 Data preprocessing
    and cleaning Effective data preprocessing and cleaning are pivotal in ML-based
    modeling. As the quality of the produced model relies heavily on the quality of
    the training data. To ensure optimal dataset preparation, it is essential to go
    through a number of data cleaning and transformation processes [28]. 3.2.1 Data
    cleaning Data cleaning involves organizing, editing, and refining data to ensure
    consistency and analysis readiness. It eliminates inaccurate or irrelevant data,
    enabling effective interpretation and analysis [29]. A three-step approach was
    implemented for data cleaning: 3.2.1.1 Remove irrelevant data and feature selection
    To ensure better model generalization and eliminate irrelevant data, specific
    features including “StartTime,” “LastTime,” “SrcAddr,” “DstAddr,” “sIpId,” and
    “dIpId” were removed from the dataset. These features uniquely identified the
    attacks, potentially compromising the model’s ability to handle unseen data. Additionally,
    non-essential elements such as hashtags, URLs, emoticons, HTML tags, and dates
    were eliminated during the data cleaning process [20]. 3.2.1.2 Remove duplicated
    records Eliminating duplicates is essential to achieve balanced outcomes, as they
    can increase storage needs, hinder analysis, and bias the results of the ML models.
    3.2.1.3 Handling missing data Given the abundance of records available in the
    dataset, all records with missing data were dropped from the dataset. 3.2.2 Data
    transformation Data transformation is performed to optimize outcomes. In IIoT
    intrusion detection, data transformation involves two main steps. 3.2.2.1 Data
    type conversion The dataset contained two main types of features: categorical
    and numerical. It became necessary to modify the representation of these features
    to establish consistent representation and facilitate efficient computation and
    analysis. The following modifications were made to the representation of the different
    types of features: a. Categorical features were encoded using one-hot encoding
    to convert them into a numerical format suitable for normalization and model training.
    The process involves representing each unique category as a binary vector, where
    only one element corresponding to the category is marked as a 1, while all other
    elements are set to 0. This transformation ensures that the categorical information
    is maintained and does not introduce ordinal relationships between categories.
    After this encoding step, categorical features can be treated in a similar manner
    to numerical features. b. Numerical values (numerical features and the encoded
    categorical features) were all unified to an “Int64” variable size, ensuring uniformity
    and consistency in the representation of the data. 3.2.2.2 Data normalization
    Normalization is the process of rescaling numerical attributes to a common range,
    typically centered between 0 and 1. In this study, we employed Min–Max normalization,
    which offers several advantages, to normalize all attributes. This normalization
    technique ensures that the features are scaled proportionally and preserves the
    relationships between the data points, contributing to improved accuracy. 3.3
    Training and testing procedure To ensure randomness, a shuffling process was applied
    to the dataset before it was split into the separate training and testing datasets.
    The training dataset comprised 80% of the original data, while the testing dataset
    comprised the remaining 20%. This partitioning strategy allowed for an unbiased
    evaluation of the model’s performance on unseen data, enabling the assessment
    of its ability to generalize and make accurate predictions. 3.4 Evaluation metrics
    The performance evaluation of the six proposed models in this study encompassed
    a comprehensive set of five evaluation metrics: accuracy, recall, precision, F1-score,
    and MCC. These metrics were selected to provide a thorough assessment of each
    model’s effectiveness and the performance of the underlying ML algorithms employed.
    Moreover, the evaluation process included the measurement of training and testing
    times for the multi-classification IDS. By considering these different figures
    of merit, a detailed and precise evaluation of each model was obtained, facilitating
    a comprehensive comparison of their performance. This evaluation framework enables
    the identification of the most suitable ML algorithm for a given model, based
    on the specific objectives and priorities established for the system. One of the
    most significant indicators of performance for ML models is the confusion matrix
    which reflects the model’s achievement in correctly classifying records. The details
    in Table 5 show the definition of the terms for the confusion table that are used
    in our work. Table 5 Definition of the confusion matrix parameters Full size table
    The goal for our IDS is to reduce the false negative (FN) and increase the true-negative
    (TN) classifications. The definition for these terms is described as follows:
    True negatives (TNs): the number of legitimate packets that have been classified
    successfully as normal. True positives (TPs): the number of abnormal or malicious
    packets that have been successfully classified as an attack. False positive (FP):
    the number of non-attack packets that have been wrongly classified as an attack.
    False negative (FN): the number of anomalous packets that were wrongly classified
    as legitimate packets. Additionally, we used the following figures of merit to
    evaluate the performance of the different ML models: Accuracy: accurately predicted
    samples as a proportion of total predictions, it is calculated based on the expression
    in Eq. (1): $$Accuracy = \\frac{TP + TN}{{{\\text{TP}} + {\\text{TN}} + {\\text{FP}}
    + {\\text{FN}}}}$$ (1) Recall: the percentage of malicious traffic that has been
    correctly classified as an attack. It is also known as sensitivity or true positive
    rate (TPR). It is calculated based on the expression in Eq. (2): $$Recall = TPR
    = \\frac{TP}{{TP + FN}}$$ (2) Precision: the percentage of relevant results. It
    is mainly used when the FP is a priority, and it can be calculated based on the
    expression in Eq. (3): $${\\text{Precision}} = \\frac{TP}{{TP + FP}}$$ (3) F1-score:
    the harmonic mean between precision and recall. It is used when both recall and
    precision are a priority. It is calculated based on the expression in Eq. (4):
    $${\\text{F}}1 - {\\text{Score}} = \\frac{2 TP}{{2 TP + FP + FN}}$$ (4) Matthews
    correlation coefficient (MCC): this statistical metric gauges the quality of classification
    by illustrating the agreement between observed and expected values. The MCC is
    a robust measure that yields a high score when the predictions consistently exhibit
    strong performance across all facets of the confusion matrix [30, 31]. It will
    be utilized to comprehensively assess the model’s performance in binary classification.
    It is calculated based on the expression in Eq. (5): $$MCC = \\frac{{\\left( {TP*
    TN} \\right) - \\left( {FP* FN} \\right)}}{{\\sqrt {\\left( {TP + FP} \\right)*
    \\left( {TP + FN} \\right)* \\left( {TN + FN} \\right)} }}$$ (5) While a high
    accuracy rate may be expected in an imbalanced dataset where most samples are
    legitimate traffic, it does not necessarily indicate a strong model. Thus, accuracy
    alone is not a reliable measure in the presence of dataset imbalances. MCC on
    the other hand is a reliable statistical measure that accounts for the model’s
    performance across all areas of the confusion matrix, providing a more comprehensive
    evaluation of effectiveness. 4 Results and discussion The aim of this work is
    to investigate the effect of data normalization and dataset balancing on the performance
    of the leading ML models used for detecting malicious intrusions in IIoT networks
    in the literature. The diagram in Fig. 2 showed the operational flow of the evaluations
    conducted in this work, and the following subsections will present a discussion
    of the results. 4.1 Effect of data normalization The first experiment conducted
    was to evaluate the effect of data normalization on the performance of the six
    ML models used in the literature. The models were implemented, and their performance
    was evaluated using the default un-normalized dataset, and the dataset after normalization
    as described in Sect. 3.2.2. The chart in Fig. 4 shows the MCC results for the
    different models as reported in reference work [20]. These results will be used
    as the benchmark since they represent the latest IDS built using the WUSTL-IIOT-2021
    dataset. Fig. 4 MCC results of the six ML models from the literature [20] Full
    size image 4.1.1 Comparative performance of our implementation on the un-normalized
    dataset The dataset was first taken through the cleaning steps described in Sect.
    3.2.1 before it was partitioned into the training and testing datasets as described
    in Sect. 3.3. Then, the six ML models were implemented using Python, and trained
    on the un-normalized dataset. The results in Fig. 5 provide a comparison between
    the results of our implementation and the results from the benchmark. It is to
    be noted that the results of our implementation are based on the un-normalized
    dataset, while the results from the benchmark are based on a normalized dataset.
    This should explain the reduction in performance for SVM and LR in our implementation
    since they exhibit suboptimal performance on un-normalized datasets. It is observed
    that RF, DT, and KNN achieved a high classification accuracy (with MCC levels
    around 99%), while SVM, LR, and NB did not perform as well. Notably, our implementation
    yielded higher classification accuracy for RF, DT, KNN, and NB compared to the
    benchmark implementation. The performance of SVM, LR, and NB will be closely monitored
    to assess potential improvements and enhancements in their performance. To provide
    a comprehensive evaluation, all five performance metrics were calculated and are
    presented in Table 6. The findings demonstrate the performance consistency of
    the six ML algorithms across different evaluation metrics, ensuring a comprehensive
    assessment of their effectiveness in intrusion detection. Fig. 5 MCC results of
    the six ML models from the benchmark and our implementation using the WUSTL-IIOT-2021
    dataset without normalization Full size image Table 6 Performance of ML algorithms
    on the un-normalized WUSTL-IIOT-2021 dataset Full size table The superior performance
    of our implementation in comparison to the reference can be attributed to meticulous
    data preprocessing steps, including feature reduction, duplicate removal, and
    handling of missing data. Tailored data transformations were applied, such as
    one-hot encoding for categorical data and direct transformation of numerical features
    into integers. These measures optimized the dataset for the implementation and
    evaluation of the six selected ML algorithms. 4.1.2 Effect of normalization “MinMaxScaler”
    technique was applied to the dataset, then the same six ML algorithms were employed
    to construct multiple IDS models. Figure 6 presents a comparison of the MCC results
    obtained from the benchmark, our implementation without dataset normalization,
    and our implementation after dataset normalization. The results highlight notable
    improvements in the performance of SVM and LR, both reaching an accuracy level
    better than 99% after normalization. However, the performance of the NB algorithm
    only exhibited a marginal increase compared to the un-normalized dataset, suggesting
    its limited ability to handle imbalanced datasets effectively. Table 7 presents
    the five performance metrics for our implementations using the normalized dataset.
    Fig. 6 MCC results of the six ML models from the benchmark and our implementation
    with and without normalization Full size image Table 7 Performance of ML algorithms
    on the normalized WUSTL-IIOT-2021 dataset Full size table The results show that
    the performance of all ML models improved after the application of data normalization.
    Particularly, the performance of SVM and LR improved significantly across all
    metrics. Accordingly, it was decided to utilize the MinMaxScaler technique for
    dataset normalization in all further experiments and model development. 4.2 Effect
    of different balancing techniques on the performance of LR algorithm The severe
    class imbalance observed in the WUSTL-IIOT-2021 dataset, as highlighted in Table
    4, poses a challenge to the learning process and can lead to biased machine learning
    models. To address this issue, we conducted a comprehensive investigation into
    the effects of various balancing techniques on the performance of the IDS. Among
    the ML models, LR was selected for its simplicity and interpretability, which
    makes it a suitable candidate for initial investigations into the dataset’s class
    imbalance and the effects of various balancing techniques. The evaluated balancing
    techniques include cost-sensitive learning (CSL), random under-sampling, random
    over-sampling, SMOTE, and bagging and pasting. Unfortunately, the K-means clustering
    technique could not be evaluated due to the dataset’s large size, rendering it
    impractical. It is to be noted that the primary objective of an IDS is to accurately
    detect security breaches in real-time, with minimal overlooked threats. Therefore,
    it is paramount to keep false negatives (undetected malicious traffic) at a minimum.
    Consequently, the recall and MCC performance metrics become critical determinants
    in identifying the most effective technique. Therefore, these metrics were used
    to evaluate the relative performance of the models with the application of the
    different balancing techniques. The results presented in Fig. 7 show that all
    balancing techniques produced an improvement in the recall of the model. Additionally,
    SMOTE seems to have an advantage over the other techniques as summarized in the
    following points: 1. Effective MCC: SMOTE achieves the highest MCC score of 99.37%.
    This indicates that SMOTE is highly successful in detecting malicious traffic
    and minimizing false negatives. 2. Resource efficiency: unlike random under-sampling
    and bagging and pasting, SMOTE does not sacrifice data samples or impose excessive
    computational costs. It generates artificial data points that are marginally different
    from the original data, allowing for efficient utilization of available resources.
    3. Scalability and accuracy: SMOTE’s ability to generate synthetic data without
    introducing duplicates makes it an ideal balancing algorithm for classification
    tasks that prioritize accuracy and scalability. It strikes a balance between maintaining
    data diversity and avoiding overfitting, resulting in improved overall model performance.
    Fig. 7 Comparison of balancing techniques based on recall and MCC score Full size
    image These findings highlight the effectiveness of SMOTE in achieving a balance
    between accurate classification, efficient resource utilization, and scalability
    compared to other balancing techniques. Therefore, it was decided to use SMOTE
    in all further experiments. 4.3 Effect of SMOTE on the performance of the ML models
    from the literature An experiment was conducted to assess the impact of SMOTE
    balancing on the performance of the six ML models from the literature in binary
    intrusion detection (normal versus attack). The WUSTL-IIoT-2021 dataset, which
    has already been normalized, was split into a training dataset (randomly selected
    80% of the samples) and a testing dataset (remaining 20% of the samples). The
    training dataset (comprising of 838,903 samples) underwent SMOTE balancing to
    achieve a balance between normal and malicious samples (as outlined by Table 8).
    On the other hand, the testing dataset (comprising of the remaining 209,718 samples)
    was left unbalanced to represent the expected traffic in an IIoT network under
    attack. Table 8 Distribution of samples in the training dataset balanced for binary
    classification using SMOTE Full size table Figure 8 illustrates the percentage
    distribution of the dataset among the various classes following the application
    of the SMOTE balancing technique. Since the first model involves binary classification,
    all classes were consolidated into two categories: “Normal” and “Attack,” with
    an equal number of attributes for each class. Fig. 8 Distribution of the training
    dataset after applying SMOTE for binary classification Full size image All six
    ML models were trained using the balanced training dataset and subsequently evaluated
    using the unbalanced testing dataset. The charts in Fig. 9 present the impact
    of SMOTE balancing on the evaluation metrics of the different algorithms. The
    charts demonstrate that most algorithms either maintained or exhibited improved
    performance with the application of SMOTE. Fig. 9 Effect of SMOTE balancing on
    the performance of binary classification using ML models from the literature Full
    size image Notably, the NB algorithm demonstrated significant improvement across
    all metrics as a result of SMOTE balancing. This enhancement can be attributed
    to the algorithm’s reliance on the assumption of feature independence when calculating
    class probabilities. In cases where the dataset is imbalanced, NB may exhibit
    a bias toward the majority class. However, by applying SMOTE to the dataset, synthetic
    samples are generated specifically for the minority class, which enables NB to
    capture a more comprehensive representation of the minority class, leading to
    improved accuracy in classifying instances from this class. On the other hand,
    it is worth noting that the LR algorithm displayed a slight decrease in performance
    across various metrics when SMOTE balancing was applied. This can be attributed
    to the potential introduction of noise or outliers through the synthetic samples,
    which may disrupt the LR model’s assumed linear relationships. Table 9 summarizes
    the improvement in the MCC score for the different ML algorithms after SMOTE was
    applied to the dataset. The results demonstrate the remarkable improvement in
    the performance of the NB and LR algorithms. Table 9 MCC results for binary classification
    using the different ML algorithms Full size table 4.4 Performance of the six ML
    algorithms in multi-classification of intrusion attacks The preceding experiments
    showcased that the proposed combination of normalizing and balancing the dataset
    enhances the abilities of the models in determining whether the network is under
    attack or behaving normally. However, once it is established that an attack is
    underway, it becomes imperative to identify the type of attack so that the operators
    can tailor their response appropriately. Accordingly, an experiment was conducted
    to evaluate the performance of the ML models in a multi-class categorization of
    the type of attack to which the network is being subjected. To the best of our
    knowledge, no existing work has addressed the challenge of balancing multi-attack
    IIoT datasets or developed a multi-classification IDS specifically designed for
    IIoT environments. The six ML algorithms from the literature were used to develop
    multi-classification IDS models for identifying the various attacks present in
    the WUSTL-IIoT-2021 dataset. The dataset underwent the same data preprocessing,
    cleaning, feature selection, and normalization procedures applied in the earlier
    binary classification experiments. In the absence of comparable benchmarks within
    existing research, the performance of these models will be assessed against each
    other to establish their relative effectiveness. The following subsections will
    present the results using the unbalanced versus SMOTE-balanced datasets. 4.4.1
    Performance of the ML algorithms in multi-classification using the unbalanced
    dataset A portion of the default unbalanced WUSTL-IIoT-2021 dataset was extracted
    to create the training dataset for the multi-classification models, and their
    performance was evaluated using the performance metrics discussed in Sect. 3.4.
    Additionally, training and testing times were documented as an additional figure
    of merit for the different models. In general, the results in Table 10 show that
    the ML algorithms produced reasonably consistent performance across the various
    evaluation metrics indicating reasonable effectiveness in detecting the different
    attack scenarios present in the dataset. Notably, RF and DT exhibited good resilience
    even though the dataset is severely unbalanced, while the performance of KNN,
    SVM, and LR is slightly lagging indicating possible room for improvement through
    dataset balancing. As anticipated from binary classification, NB exhibited relatively
    lower performance compared to the other models. A comparison of the processing
    time requirement seems to indicate that DT requires the least training and testing
    times among the models that produced good classification performance. Table 10
    Multi-classification performance of ML algorithms using unbalanced dataset Full
    size table 4.4.2 Effect of SMOTE balancing on performance of ML algorithms for
    multi-classification The WUSTL-IIoT-2021 dataset comprises of five classes, four
    types of cyberattacks along with normal traffic. Accordingly, SOMTE was used to
    balance the training dataset to ensure equal representation of the classes. The
    minority classes (attack samples) underwent SMOTE-based up-sampling until their
    representation in the training dataset became the same as the majority class (normal
    traffic), as shown in Table 11. Table 11 Distribution of samples in the training
    dataset balanced for multi-classification using SMOTE Full size table The results
    presented in Table 12 show the multi-classification performance of the six ML
    algorithms with the use of the SMOTE-balanced training dataset using the different
    measures. Table 12 Multi-classification performance of ML algorithms using SMOTE-balanced
    dataset Full size table Figure 10 shows a comparison between the multi-classification
    performance of the different models before and after applying SMOTE balancing.
    The figure shows that RF produced identically high performance in both the unbalanced
    and balanced scenarios, indicating its capability to handle class imbalance effectively.
    Furthermore, Table 12 shows that it exhibited the same execution efficiency, as
    there was no significant increase in either training or testing times. Similarly,
    DT demonstrated the same outstanding performance in both scenarios. However, it
    is important to note that DT exhibited an increased training and testing time
    with the balanced dataset, which can be attributed to the significantly larger
    training dataset. Fig. 10 Effect of SMOTE balancing on the performance of multi-classification
    using ML models from the literature Full size image KNN on the other hand showed
    significant performance improvement after the application of balancing. However,
    this improvement comes at the cost of significant increase in testing time. Similarly,
    SVM showed impressive performance improvement, but at the expense of multiple
    orders of magnitude increase in both training and testing time. LR also witnessed
    improvements in all evaluation metrics with a relatively small increase in training
    time (from one minute to 3.5 min), and testing time (from one second to five seconds).
    Lastly, NB, which initially showed limitations in handling imbalanced datasets,
    demonstrated noticeable improvements in all metrics after the dataset was balanced,
    but with slight increases in both training and testing time. In general, it can
    be concluded from these results that, with the application of dataset normalization
    and balancing using SMOTE, all six of the ML models from the literature produce
    reasonable performance in detecting and correctly classifying the different attacks
    present in the domain-specific WUSTL-IIoT-2021 dataset. Overall, KNN and SVM produced
    the best results based on the different figures of merit. However, at severe training
    and testing time penalties. On the other hand, DT and LR produced highly competitive
    performance, approaching 99% in all metrics, albeit at the expense of a tenfold
    increase in training and testing durations. Notably, RF stands clearly as the
    superior algorithm amongst these six, consistently delivering robust performance
    even in the face of the significantly expanded dataset, while maintaining computational
    efficiency unaffected by the dataset’s expansion. Applying optimizations to some
    of these techniques would further improve their performance [32]. 5 Conclusion
    This work presents a comprehensive quantitative analysis of the performance of
    RF, DT, KNN, LR, SVM, and NB in detecting intrusions in IIoT networks. These models
    are chosen due to their extensive use in relevant literature. This work also investigated
    the effect of data preprocessing, namely feature engineering, data normalization,
    recoding, and mitigation of missing data on the performance of these models. The
    results reveal a substantial improvement in intrusion detection accuracy across
    all models following the application of the data preprocessing steps. This improvement
    ranged from 3% for the RF model to 260% for NB compared to the baseline from the
    literature, based on the MCC evaluation metric. In addition, the work performed
    extensive analysis on the effect of six different dataset balancing techniques
    on the performance of the models, especially with the use of the domain-specific
    WUSTL-IIOT-2021 dataset. The results showed that balancing the dataset using SMOTE
    led to the highest binary classification effectiveness between normal and attack
    traffic. The effectiveness of the different models increased to the range of 95.4%
    (for NB) to 99.98% (for RD and DT). Furthermore, this work implemented multi-class
    identification between the different attack types present in the dataset. A novel
    implementation of multi-class balancing using the SMOTE technique was used to
    counter the substantial imbalance in the dataset against some attack types. The
    results showed that RF, DT, and LR produced superior multi-class attack identification
    performance approaching 99.99% accuracy, while maintaining a very robust computation
    time. The other models produced highly competitive performance, however at the
    expense of a significant increase in training and testing times due to the considerably
    larger training dataset. In general, it can be concluded that there was a performance
    improvement for all six ML models from the literature with the application of
    dataset normalization and balancing using SMOTE. However, for some models, this
    improvement came at considerable training and testing time penalties. Notably,
    RF stands clearly as the superior algorithm as it produces consistently high performance
    for both binary- and multi-classification while maintaining robust computational
    efficiency. In conclusion, this study has shown that it is possible to achieve
    very significant accuracies in intrusion detection for IIoT networks. The study
    has outlined some limitations, thereby suggesting promising areas for future research.
    These encompass diverse dataset exploration, alternative ML algorithm evaluation,
    enhanced feature engineering, model optimization, investigation of real-time implementations,
    and addressing interpretability challenges. Future work should also emphasize
    scalability and adaptability to accommodate evolving IIoT networks and emerging
    threat scenarios. Data availability This study relies on a publicly available
    dataset (WUSTL-IIOT-2021). This dataset is available from the following reference—Zolanvari,
    Μ., Gupta, L., Khan, K. Μ., & Jain, R. (2021), WUSTL-IIOT-2O2l Dataset for IIoT
    Cybersecurity Research, Washington University in St. Louis, USA. References Stouffer
    K, Pillitteri V, Lightman S, et al (2015) Guide to industrial control systems
    (ICS) security NIST special publication 800–82 revision 2, pp 1–157 Smadi AA,
    Ajao BT, Johnson BK et al (2021) A comprehensive survey on cyber-physical smart
    grid testbed architectures: requirements and challenges. Electronics 10:1043.
    https://doi.org/10.3390/electronics10091043 Article   Google Scholar   Bonetto
    R, Sychev I, Zhdanenko O, et al (2020) Smart grids for smarter cities. In: 2020
    IEEE 17th annual consumer communications and networking conference (CCNC). https://doi.org/10.1109/CCNC46108.2020.9045309
    Attar H (2023) Joint IoT/ML platforms for smart societies and environments: a
    review on multimodal information-based learning for safety and security. J Data
    Inf Qual. https://doi.org/10.1145/3603713 Article   Google Scholar   Calabretta
    M, Pecori R, Vecchio M, Veltri L (2018) MQTT-AUTH: a token-based solution to endow
    MQTT with authentication and authorization capabilities. J Commun Softw Syst 14:320–331.
    https://doi.org/10.24138/jcomss.v14i4.604 Article   Google Scholar   Calabretta
    M, Pecori R, Veltri L (2018) A token-based protocol for securing MQTT communications.
    In: Proceedings of the 26th international conference on software, telecommunications
    and computer networks, SoftCOM 2018, pp 373–378. https://doi.org/10.23919/SOFTCOM.2018.8555834
    Nti IK, Adekoya AF, Narko-Boateng O, Somanathan AR (2022) Stacknet based decision
    fusion classifier for network intrusion detection. Int Arab J Inf Technol 19:478–490.
    https://doi.org/10.34028/iajit/19/3A/8 Article   Google Scholar   Abdul Rahman
    Al-chikh Omar A, Soudan B, Ala’ Altaweel (2023) A comprehensive survey on detection
    of sinkhole attack in routing over low power and Lossy network for internet of
    things. Internet Things (Netherlands). https://doi.org/10.1016/j.iot.2023.100750
    Article   Google Scholar   Samara G, Aljaidi M, Alazaidah R, et al (2023) A comprehensive
    review of machine learning-based intrusion detection techniques for IoT networks.
    In: Artificial intelligence, Internet of Things, and society 5.0. pp 465–473 Manderna
    A, Kumar S, Dohare U et al (2023) Vehicular Network Intrusion Detection Using
    a Cascaded Deep Learning Approach with Multi-Variant Metaheuristic. Sensors 23:8772.
    https://doi.org/10.3390/s23218772 Article   ADS   PubMed   PubMed Central   Google
    Scholar   Alamleh A, Albahri OS, Zaidan AA et al (2023) Federated Learning for
    IoMT Applications: A Standardization and Benchmarking Framework of Intrusion Detection
    Systems. IEEE J Biomed Heal Informatics 27:878–887. https://doi.org/10.1109/JBHI.2022.3167256
    Article   Google Scholar   Surakhi O, García A, Jamoos M, Alkhanafseh M (2022)
    The Intrusion detection system by deep learning methods: issues and challenges.
    Int Arab J Inf Technol 19:501–513. https://doi.org/10.34028/iajit/19/3A/10 Article   Google
    Scholar   Keliris A, Salehghaffari H, Cairl B, et al (2016) Machine learning-based
    defense against process-aware attacks on industrial control systems. In: Proceedings
    of 2016 IEEE international test conference (ITC), pp 1–10. https://doi.org/10.1109/TEST.2016.7805855
    Ullah I, Mahmoud QH (2017) A hybrid model for anomaly-based intrusion detection
    in SCADA networks. In: Proceedings of 2017 IEEE international conference on big
    data (big data), pp 2160–2167. https://doi.org/10.1109/BigData.2017.8258164 Vulfin
    AM, Vasilyev VI, Kuharev SN et al (2021) Algorithms for detecting network attacks
    in an enterprise industrial network based on data mining algorithms. J Phys Conf
    Ser. https://doi.org/10.1088/1742-6596/2001/1/012004 Article   Google Scholar   Beaver
    JM, Borges-Hink RC, Buckner MA (2013) An evaluation of machine learning methods
    to detect malicious SCADA communications. In: Proceedings of 2013 12th international
    conference on machine learning and applications ICMLA, vol 2, pp 54–59. https://doi.org/10.1109/ICMLA.2013.105
    Zhang Y, Ilić MD, Tonguz OK (2011) Mitigating blackouts via smart relays: a machine
    learning approach. Proc IEEE 99:94–118. https://doi.org/10.1109/JPROC.2010.2072970
    Article   Google Scholar   Maglaras LA, Jiang J (2014) Intrusion detection in
    SCADA systems using machine learning techniques. In: Proceedings of 2014 science
    and information conference, pp 626–631. https://doi.org/10.1109/SAI.2014.6918252
    Song Y, Luo W, Li J, et al (2021) SDN-based Industrial Internet Security Gateway.
    In: 2021 International conference on security, pattern analysis, and cybernetics
    (SPAC), pp 238–243. https://doi.org/10.1109/SPAC53836.2021.9539961 Zolanvari M,
    Teixeira MA, Gupta L et al (2019) Machine learning-based network vulnerability
    analysis of industrial Internet of Things. IEEE Internet Things J 6:6822–6834.
    https://doi.org/10.1109/JIOT.2019.2912022 Article   Google Scholar   Teixeira
    MA, Gupta L, Khan KM, Machine RJ (2021) WUSTL-IIOT-2021 dataset for IIoT cybersecurity
    research. Washington University, St. Louis Google Scholar   Siebert J, Joeckel
    L, Heidrich J et al (2022) Construction of a quality model for machine learning
    systems. Softw Qual J 30:307–335. https://doi.org/10.1007/s11219-021-09557-y Article   Google
    Scholar   Sarker IH (2021) Deep learning: a comprehensive overview on techniques,
    taxonomy, applications and research directions. SN Comput Sci. https://doi.org/10.1007/s42979-021-00815-1
    Article   PubMed   PubMed Central   Google Scholar   Eid AM, Nassif AB, Soudan
    B, Injadat MN (2023) IIoT network intrusion detection using machine learning.
    In: 2023 6th International conference on intelligent robotics and control engineering
    (IRCE). IEEE, pp 196–201 Ting KM (1998) Inducing cost-sensitive trees via instance
    weighting. Lect Notes Comput Sci (Subser Lect Notes Artif Intell Lect Notes Bioinf)
    1510:139–147. https://doi.org/10.1007/bfb0094814 Article   Google Scholar   Zhang
    YP, Zhang LN, Wang YC (2010) Cluster-based majority under-sampling approaches
    for class imbalance learning. In: Proceedings of 2010 2nd IEEE international conference
    on information and financial engineering, pp 400–404. https://doi.org/10.1109/ICIFE.2010.5609385
    Richman R, Wuthrich MV (2020) Nagging predictors. SSRN Electron J. https://doi.org/10.2139/ssrn.3627163
    Article   Google Scholar   Mesevage TG (2021) Data cleaning steps and process
    to prep your data for success. MonkeyLearn, Montevideo Google Scholar   Tableau
    (2022) Data cleaning: definition, benefits, and how-to. Tableau, Mountain View
    Google Scholar   Chicco D, Jurman G (2020) The advantages of the Matthews correlation
    coefficient (MCC) over F1 score and accuracy in binary classification evaluation.
    BMC Genomics. https://doi.org/10.1186/s12864-019-6413-7 Article   PubMed   PubMed
    Central   Google Scholar   Chicco D, Jurman G (2023) The Matthews correlation
    coefficient (MCC) should replace the ROC AUC as the standard metric for assessing
    binary classification. BioData Min. https://doi.org/10.1186/s13040-023-00322-4
    Article   PubMed   PubMed Central   Google Scholar   Khafajeh H (2020) An efficient
    intrusion detection approach using light gradient boosting. J Theor Appl Inf Technol
    98:825–835 Google Scholar   Download references Author information Authors and
    Affiliations Department of Computer Engineering, College of Computing and Informatics,
    University of Sharjah, Sharjah, UAE Abdulrahman Mahmoud Eid, Bassel Soudan & Ali
    Bou Nasif Department of Data Science and AI, Faculty of Information Technology,
    Zarqa University, Zarqa, Jordan MohammadNoor Injadat Corresponding author Correspondence
    to Bassel Soudan. Ethics declarations Conflict of interest The authors declare
    that they have no conflict of interest. Ethical approval The authors would like
    to convey their thanks and appreciation to the “University of Sharjah” for supporting
    this work. Informed consent This study does not involve any experiments on animals.
    Additional information Publisher''s Note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    Rights and permissions Springer Nature or its licensor (e.g. a society or other
    partner) holds exclusive rights to this article under a publishing agreement with
    the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Eid, A.M., Soudan, B., Nasif, A.B. et al. Comparative study of ML
    models for IIoT intrusion detection: impact of data preprocessing and balancing.
    Neural Comput & Applic 36, 6955–6972 (2024). https://doi.org/10.1007/s00521-024-09439-x
    Download citation Received 10 October 2023 Accepted 15 January 2024 Published
    11 February 2024 Issue Date May 2024 DOI https://doi.org/10.1007/s00521-024-09439-x
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Industrial Internet of Things Intrusion detection Machine
    learning algorithms Data normalization Dataset balancing Use our pre-submission
    checklist Avoid common mistakes on your manuscript. Sections Figures References
    Abstract Introduction Background Methodology Results and discussion Conclusion
    Data availability References Author information Ethics declarations Additional
    information Rights and permissions About this article Advertisement Discover content
    Journals A-Z Books A-Z Publish with us Publish your research Open access publishing
    Products and services Our products Librarians Societies Partners and advertisers
    Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy
    choices/Manage cookies Your US state privacy rights Accessibility statement Terms
    and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic
    Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024
    Springer Nature"'
  inline_citation: '>'
  journal: Neural Computing and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Comparative study of ML models for IIoT intrusion detection: impact of data
    preprocessing and balancing'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Xu S.
  - Xia H.
  - Liu P.
  - Zhang R.
  - Chi H.
  - Gao W.
  citation_count: '0'
  description: Federated learning (FL) is a critical technology for implementing time-critical
    computing systems in the Internet of Things (IoT). It allows for continuous updates
    to machine learning (ML) models across IoT devices. However, the vulnerability
    of ML models and the complexity of IoT pose significant threats to device data
    security and privacy, affecting the robustness of time-critical computing systems
    constructed through FL. Recent research on FL data protection has made progress,
    but challenges remain in balancing privacy protection with model availability.
    For example, cryptography-based defense schemes increase time overhead in time-critical
    computing systems, while differential privacy negatively impacts system performance.
    This paper proposes the FL properties modification scheme (FLPM) for data preprocessing
    to resist property inference attacks and data poisoning attacks. FLPM modifies
    training data properties using algorithms for property separation, selection,
    and control based on continuous latent variables. While this sacrifices a small
    amount of classification accuracy, it significantly improves data protection capabilities.
    Detailed experimental results demonstrate that FLPM successfully separates and
    controls image property vectors. In the FL classification task, the property modification
    data achieve a precision of 94.44%. This scheme effectively prevents property
    inference attacks and data poisoning attacks. FLPM can reduce the AUC score for
    property inference attacks from 0.94 to 0.56 and reduce the success rate of data
    poisoning attacks to 5.13%, 7.07%, and 4.60%.
  doi: 10.1016/j.future.2023.12.030
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Background
    and threat model 3. Federated learning property modification 4. Experiment 5.
    Conclusion CRediT authorship contribution statement Declaration of competing interest
    Acknowledgments Data availability References Vitae Show full outline Figures (12)
    Show 6 more figures Tables (4) Table 1 Table 2 Table 3 Table 4 Future Generation
    Computer Systems Volume 154, May 2024, Pages 151-159 FLPM: A property modification
    scheme for data protection in federated learning Author links open overlay panel
    Shuo Xu a, Hui Xia a, Peishun Liu a, Rui Zhang a, Hao Chi a, Wei Gao b Show more
    Add to Mendeley Share Cite https://doi.org/10.1016/j.future.2023.12.030 Get rights
    and content Highlights • Data security and privacy challenges arise in time-critical
    computing systems. • Protecting data security and privacy by modifying data properties.
    • Improved data protection at the sacrifice of a small amount of model accuracy.
    • Property inference attacks and data poisoning attacks are mitigated in experiments.
    Abstract Federated learning (FL) is a critical technology for implementing time-critical
    computing systems in the Internet of Things (IoT). It allows for continuous updates
    to machine learning (ML) models across IoT devices. However, the vulnerability
    of ML models and the complexity of IoT pose significant threats to device data
    security and privacy, affecting the robustness of time-critical computing systems
    constructed through FL. Recent research on FL data protection has made progress,
    but challenges remain in balancing privacy protection with model availability.
    For example, cryptography-based defense schemes increase time overhead in time-critical
    computing systems, while differential privacy negatively impacts system performance.
    This paper proposes the FL properties modification scheme (FLPM) for data preprocessing
    to resist property inference attacks and data poisoning attacks. FLPM modifies
    training data properties using algorithms for property separation, selection,
    and control based on continuous latent variables. While this sacrifices a small
    amount of classification accuracy, it significantly improves data protection capabilities.
    Detailed experimental results demonstrate that FLPM successfully separates and
    controls image property vectors. In the FL classification task, the property modification
    data achieve a precision of 94.44%. This scheme effectively prevents property
    inference attacks and data poisoning attacks. FLPM can reduce the AUC score for
    property inference attacks from 0.94 to 0.56 and reduce the success rate of data
    poisoning attacks to 5.13%, 7.07%, and 4.60%. Graphical abstract Download : Download
    high-res image (459KB) Download : Download full-size image Previous article in
    issue Next article in issue Keywords Federated learningVariational autoencoderPrivacy
    protectionProperty inference attackData poisoning attack 1. Introduction Federated
    learning (FL) [1] presents an efficient solution for constructing machine learning
    (ML) applications tailored to the vast array of sensors in the Internet of Things
    (IoT) [2]. In particular, FL has an important role in time-critical computing
    systems for IoT, as it can leverage sensor data in real-time for model training
    and aggregation. For example, many urban environments often suffer from data silos
    due to fragmented information collected by sensors embedded in infrastructure
    and vehicles. FL can address the problem by swiftly aggregating ML models in smart
    cities [3]. However, employing sensor data in FL confronts security and privacy
    challenges [4] arising from the intricate and heterogeneous IoT environment. Malicious
    attackers can exploit IoT devices to propagate poisoned data within time-critical
    computing systems, compromising FL models. Attackers may also conduct privacy
    attacks to extract sensitive information from other devices in FL. Consequently,
    this has raised concerns about data security and privacy preservation in FL. Currently,
    there are two main approaches to data protection measures in FL: differential
    privacy and homomorphic encryption. (1) Differential privacy achieves confidentiality
    by introducing controlled noise into the local models of FL, thereby obscuring
    device-specific information [5], [6]. (2) Homomorphic encryption facilitates cryptographic
    computations on ciphertexts, circumventing the need for decryption [7], [8]. It
    prevents FL insider threats from attempting to illegally access sensitive information
    embedded in models [9]. Nonetheless, one of the critical challenges lies in balancing
    the trade-off between computational time overhead and the efficacy of these privacy-preserving
    schemes [10]. In the study of data privacy, property inference attacks [11], [12],
    [13], [14], [15] can infer whether a device contains data with a specific property
    through an ML model. In data security, the misuse of poisoned data by devices
    can also lead to corruption of FL models [16], [17], [18]. Therefore, our research
    seeks robust defense mechanisms against both privacy and security attacks. The
    idea of this paper is inspired by two concepts: big data desensitization [19]
    and machine learning preprocessing techniques [20]. Central to our study is the
    applicability of variational autoencoders (VAEs) [21] for altering privacy-sensitive
    attributes within data. Given that images usually contain rich properties [22],
    such as expression, gender, and background information in face images. These very
    properties could potentially harbor malicious backdoors and become targets for
    property inference attacks. However, completing an ML classification task typically
    requires only a subset of these properties [23]. By strategically modifying or
    obfuscating non-task properties, we can preserve classification performance while
    providing privacy defenses. This paper proposes federated learning property modification
    (FLPM), a data protection scheme for complex data. This scheme lies in three algorithms:
    first, a property separation algorithm that leverages an enhanced VAE with class
    activation mapping. Second, a property selection algorithm guided by distribution
    constraints specific to non-task properties. Third, a property control algorithm
    based on latent variable continuity. Rather than utilizing raw data, FLPM employs
    reconstructed datasets in FL tasks to safeguard clients privacy effectively. Our
    thorough evaluation demonstrates the scheme’s robustness against several types
    of attacks. In summary, the contributions of this paper are as follows. (1) This
    paper proposes the FLPM employing property statistics and distribution laws to
    control the expression of privacy properties in the data preprocessing stage.
    (2) This scheme dramatically reduces the success rate of property inference attacks.
    Moreover, this paper has enhanced FLPM’s capability with single property dataset
    by incorporating unsupervised learning algorithms, thereby expanding the scheme’s
    application scenarios. (3) By reconstructing input datasets, FLPM can dramatically
    improve FL’s resistance to data poisoning attacks by excluding anomalous information
    from the data. It avoids poisoned data from entering the FL training process and
    enhances the security of the global model construction. (4) Detailed experimental
    results show that the modification of the property effectively separates, selects,
    and controls data properties. The accuracy of the global model trained with property
    modified data in FL is 94.44%. Meanwhile, the AUC score for the property inference
    attack is reduced to 0.56 and prevents data poisoning attacks. FLPM guarantees
    privacy without significantly degrading the accuracy of classification and increasing
    time overhead. The subsequent sections of this paper are organized as follows:
    Section 2 briefly introduces the background and potential threats in FL. Section
    3 provides a detailed analysis of the FLPM scheme. Section 4 demonstrates the
    effectiveness of the FLPM scheme through experiments. Finally, this paper is concluded
    and introduces the following research directions. 2. Background and threat model
    2.1. Federated learning In contrast to centralized deep learning, FL solves the
    problem of data islands by which each device with a dataset independently trains
    a local model. The server then aggregates the device model parameters to construct
    the global model [24]. Assume that there are devices involved in FL. In the th
    communication round, devices upload local model parameters . The server aggregates
    all uploaded as a global model and distributes them to all devices using the method
    shown in Eq. (1). (1) The represents the global model parameters of the communication
    round . The global model gradually converges through the iterations as above and
    finally reaches the target accuracy or is terminated by the communication rounds
    set. For its effectiveness, the FedAvg algorithm [1] has been validated by various
    datasets such as Federated EMNIST, CIFAR-100, Shakespeare, and StackOverflow [25].
    2.2. Threat model In property inference attacks within FL, attackers seek to deduce
    sensitive properties from model updates of FL. These incidental properties could
    reveal private information about the data used by participant devices during training.
    To achieve this, attackers construct a composite loss function, denoted as , which
    they apply to local models. Eq. (2) combines two distinct objectives: represents
    the loss associated with the main task of FL (e.g., gender classification from
    facial images), while quantifies the loss relating to inferring target properties
    (e.g., expression or age) that are not intended for disclosure. The balancing
    coefficient serves to weigh the two loss functions, as formalized in Eq. (2).
    (2) Data poisoning attacks in FL refer to the intentional manipulation of training
    data by attackers, with the aim of disrupting or manipulating the training process
    and final results of a model. Attackers may inject malicious samples into the
    training data, containing misleading or harmful features, in order to alter or
    disrupt the training process and performance of the model. The objectives of data
    poisoning attacks can vary, such as tampering with the model’s prediction results
    to generate incorrect outputs, manipulating the inference process of the model
    to favor the attacker’s interests, or stealing and leaking sensitive information
    from other devices. Download : Download high-res image (475KB) Download : Download
    full-size image Fig. 1. Framework of FLPM scheme. 3. Federated learning property
    modification 3.1. Scheme framework This paper proposes FLPM for protecting the
    privacy of FL device data. FLPM scheme is divided into three main parts: property
    separation, property selection, and property control. Firstly, The scheme uses
    property separation to extract non-task properties from the data. Second, the
    property selection algorithm filters the appropriate properties from the many
    non-task properties. Finally, the property control algorithm controls the variation
    of properties to protect the privacy of the data. The scheme particularly considers
    the property modification of single property data and proposes adaptive changes
    for each module to enhance the generalization of the scheme. The scheme is deployed
    on devices and based on a general assumption that the model is in a trusted execution
    environment where an attacker cannot modify the execution process of the scheme.
    Privacy-preserving schemes often introduce significant time overheads in FL. To
    address this challenge, FLPM integrates VAE-based algorithms for initial data
    preprocessing. This innovative approach requires only a single modification of
    the training data ain the initial round, unlike homomorphic encryption methods
    that impose recurring time costs due to the need for encryption and decryption
    in every communication round. Consequently, FLPM leverages this modified dataset
    throughout subsequent rounds of model training, thus streamlining the process
    significantly. Moreover, when compared with schemes such as NOSnoop [26] which
    defends against property inference attacks by modifying gradients in each epoch,
    FLPM eliminates the need for continuous training of defense mechanisms. For the
    VAE model used by FLPM, the server pre-trains the VAE model using the auxiliary
    dataset . It should be noted that dataset does not require additional label information
    to collect relevant data from the Internet quickly. The VAE model is deployed
    from the server to the clients using transfer learning techniques. Following a
    few epochs of finetuning, it becomes capable of modifying the properties of the
    images. The optimization objective of Eq. (3) is to achieve the best parameters
    of the VAE model under the condition that the difference between the images generated
    by the VAE model and the device images is as small as possible. (3) Fig. 1 shows
    the FLPM framework. The blue region represents the VAE deployed to reconstruct
    images within devices. The green region illustrates enhancements to the VAE achieved
    through integration with a class activation map, optimizing main task feature
    discernment. The red region is the separation and selection of properties for
    different datasets. The data is preprocessed with the above method and then integrated
    tightly for FL local models to protect private information by modifying the non-task
    properties in the training set, such as the hair color and smile. 3.2. Scheme
    details 3.2.1. Property separation based on class activation map The properties
    associated with the FL task should be defined as those contained in the key regions
    where the local model classifies images. These properties are remembered within
    the model parameters during training of the local model [27], and later aggregated
    into the global model, providing the conditions for privacy leakage attacks. Therefore,
    the scheme needs to improve the VAE to modify the properties in critical regions
    that are not relevant to the classification task. Meanwhile, the complex backgrounds
    in the training images often contain much private information that may reveal
    properties such as the time of taking, geographical location, etc. So other information
    in the image also needs to be weakened to reduce the risk of possible privacy
    leakage. The scheme is based on Grad-CAM++ [28] to obtain the importance of each
    region of images. It is calculated as shown in Eq. (4) by multiplying the weight
    with the last convolutional layer feature map to generate the class activation
    map. is the number of the last convolutional layer. (4) After obtaining the class
    activation map , the scheme redesigns the VAE decoder to focus more on the reconstruction
    of essential regions while reducing the details of non-important regions. The
    loss function in vanilla VAE contains reconstruction loss and Kullback–Leibler
    (KL) loss, so the scheme guides the image generation by adding the class activation
    map information during the reconstruction calculation. The original reconstruction
    loss is calculated as the MSE between the reconstructed images and the original
    images, as shown in Eq. (5), where is the spatial location of the pixel and is
    the number of pixels in the images. (5) The class activation map is used as masks
    for the original and reconstructed images to compute the reconstruction loss,
    as shown in Eq. (6). The mask matrix value is between 0 and 1. The critical regions
    have values closer to 1 so that the features of those regions are better preserved
    in the generated images, conversely, the information of those regions is ignored.
    (6) Then, the improved VAE isolate specific properties within images to form discrete
    property vectors. Each identified property is assigned a binary value,1 for presence
    (positive sample) or 0 for absence (negative sample), indicating whether the image
    encompasses that particular property. For clarity in discussion, we denote property
    (label) relevant to the FL classification task as . Non-task properties are denoted
    as , , , . The VAE effectively disentangles these non-task-related property vectors
    from the input data, transforming them into structured latent variables as shown
    in Eq. (7). (7) where is one of the non-task properties and . Divide all data
    into two groups according to the value of , and generate positive and negative
    vectors of samples by encoder of VAE respectively. The difference between the
    two vectors is the property vectors , which must be separated. 3.2.2. Property
    selection based on non-task property distribution constraints Download : Download
    high-res image (583KB) Download : Download full-size image For the multiple properties
    generated after separation, the scheme needs to select the appropriate non-task
    properties for modification to ensure that the accuracy of the FL task is not
    significantly degraded. The scheme proposes a property selection method based
    on image properties’ statistics and distribution law, as shown in Eq. (8). (8)
    where is the value of the property to be selected from the device dataset . If
    is 1, the data is a positive sample. If is 0, that means the data is a negative
    sample. is the threshold of the lower value. The function is when the expression
    inside is true, taking the value of 1 otherwise 0. The principle of selecting
    properties is to choose non-task properties whose property distribution is independent
    of the task properties. Such a selection allows property control to minimize damage
    to task properties. Eq. (8) requires that a non-task property can be selected
    when it is more evenly distributed in the sample. It is well known that using
    unbalanced classes of samples can reduce the accuracy of the model in deep learning.
    Therefore, Eq. (8) constrains the independence of the non-task properties from
    the task property and the distribution of the non-task properties in samples.
    The FLPM algorithm is shown in Algorithm 1. 3.2.3. Property control based on latent
    variable continuity After obtaining the appropriate property vectors , the scheme
    will modify the properties of the device data to protect the privacy of the data.
    At this point, the calculation is shown in Eq. (9) implemented taking advantage
    of the feature that latent variables are continuous. Firstly, data is input into
    VAE encoding to obtain the latent variable . Then, it is combined with the vectors
    in the set of properties . Finally, the calculated latent variable is decoded
    using the VAE model to obtain the data after the modified properties. The variable
    can affect the performance of the properties represented by vectors in . A positive
    strengthen these properties, yielding a more characteristic expression in . Conversely,
    a negative diminishes them, resulting in less pronounced property within . The
    size of can control the performance of corresponding features of the property.
    (9) 3.2.4. Property modification based on single property data In a single property
    dataset, each data is associated with exactly one property (label) . For example,
    a handwritten alphabet dataset only contains the labels identifying each letter
    class. No more properties of the dataset are directly available. This limitation
    indicates that FLPM must be improved when applied on such datasets. Property separation:
    To facilitate property separation in single property data, FLPM leverages non-task
    properties through a VAE-based unsupervised clustering approach that generates
    pseudo property labels. First, FLPM employ a VAE encoder to transform image data
    into latent variables (see Eq. (10)). Subsequently, calculate the distance between
    the current and each center of mass , where represents the center of mass of negative
    samples and represents the center of mass of positive samples. Finally, pseudo
    labels of non-task properties are assigned to the data according to the clusters
    they are classified in, and the center of mass is used as vectors of non-task
    properties . (10) Property selection: The scheme also requires the selection of
    properties separated from single property data, and we change the property constraint
    Eq. (8) proposed in the previous section to fit two clusters Eq. (11), where the
    pseudo-label is determined by the closest clustering center or of the th data.
    The constraint’s target is to ensure the selection of more evenly distributed
    properties among the positive and negative samples of the dataset. (11) Property
    control: When modifying data properties, we consider that the classification generated
    by unsupervised clustering may still differ from the true classification of the
    data properties. The center of mass generated by clustering is not necessarily
    located at the center of the data distribution to avoid corruption of some data
    far from the center of mass due to property modification. Therefore, is no longer
    chosen randomly in the property control flow of the scheme but is calculated through
    Eq. (12). (12) Calculate the distance between the latent variable of the current
    data and the center of mass of the target cluster and the distance between the
    center of mass of the current data and the center of mass of the target cluster
    . Then, the reconstructed calculation is changed to the process shown in Eq. (13),
    which optimizes the addition of the clustering feature to the target in . The
    unsupervised FLPM for single property data is shown as Algorithm 2. (13) Download
    : Download high-res image (518KB) Download : Download full-size image 4. Experiment
    This section describes the datasets and settings used in the experiments in Section
    4.1. The results of experiment for each module in FLPM and the effectiveness of
    the defense against privacy leakage for non-task properties are shown in Section
    4.2. The effect of property modification on a single property dataset is shown
    in Section 4.3. Finally, the effectiveness of the defense of this scheme against
    backdoor attacks is shown in Section 4.4. 4.1. Datasets and settings We conducted
    experiments using two datasets: CelebA [29] and EMNIST [30], to train FL classification
    models. To enhance FLPM’s performance, we pre-trained two VAE models using auxiliary
    datasets: IMDB-WIKI [31] and MNIST [32]. It is important to note that there was
    no overlap between the training and auxiliary datasets, although they shared some
    features. For example, both MNIST and EMNIST are handwritten characters, and both
    CelebA and IMDB-WIKI contain face images. Utilizing similar datasets for obtaining
    pre-trained models is a prevalent practice in deep learning. The CelebA dataset
    contains 202599 face images. Each image contains 40 properties, including the
    person’s hair, gender, face shape, decoration, etc. We resized the images in the
    CelebA dataset to 64 × 64 pixels as training and test dataset. The EMNIST dataset
    contains all handwritten letters and numbers, and only the part of the dataset
    with handwritten letters was used in the experiment and divided into 26 categories
    according to the type of letters. IMDB-WIKI offers an extensive collection of
    524 230 facial images sourced from IMDB and Wikipedia. MINST is a dataset of 70
    000 images containing handwritten digits. In FL, the number of local training
    epoch is set to three in the experiment. Local models used ResNet-50 and Adam
    optimizer with learning rate 0.001. Considering the realistic possibility of device
    dropouts caused by network issues, there exists a range of 0% to 20% of devices
    that may not participate in FL training in each communication round. For the CelebA
    dataset, we set up 50 devices, and the task of FL is to classify the gender. Based
    on the original partition of the EMNIST dataset, seven FL devices were used to
    train according to the settings of the above parameters. 4.2. Property modification
    experiments and defense effects In FL, we believe that the computational and storage
    resources on devices are much less than those on the server. Furthermore, training
    a VAE model with good performance requires many training data and a long computation
    time. Therefore, we trained the VAE model on the server and sent it to the devices.
    The devices only need to train a small number of epochs of VAE model with private
    data. After training with the IMDB-WIKI dataset, the VAE model has a clear reconstruction
    effect. Fig. 2 shows the comparison between the original images and VAE generated
    images, row 1 and row 3 are the original images, and row 2 and row 4 are the generated
    images. It can be seen that the VAE reconstructs the basic features of the person’s
    face. The improved VAE based on the class activation map in the scheme makes the
    irrelevant task properties in the images blurred while maintaining the original
    features of the person’s face. It does not visually affect the classification
    of the person’s gender, so the improved VAE is feasible for devices data processing.
    Download : Download high-res image (468KB) Download : Download full-size image
    Fig. 2. Original images and VAE generated images. Download : Download high-res
    image (232KB) Download : Download full-size image Fig. 3. Visualization of the
    average properties of positive and negative samples. There are 40 properties in
    the CelebA dataset, which need to be selected according to the constraints proposed
    above. The Black_Hair, Smiling, and Straight_Hair properties were chosen from
    the selection results. The results of the constraint calculations are shown in
    Table 1, where the properties are ranked as Smiling, Black_Hair, and Straight_Hair
    in descending order of threshold value. The next step will extract these three
    property vectors, and the experimental results are shown in Fig. 3. These three
    rows correspond to the Smiling, Black_Hair, and Straight_Hair properties. The
    four images in each row correspond to the average vectors visualization results
    for positive and negative samples of female images properties and positive and
    negative samples of male images properties. Gender is the main property used for
    the FL classification task, and in order to reduce the interference of the training
    data on the classification accuracy after processing, the sample vectors of non-task
    properties are calculated separately for female and male images. After comparing
    the reconstruction effects of different properties, it can be found that the properties
    satisfying a smaller constraint threshold have better property modifiability.
    Table 1. The calculation results of property constraints. Threshold Smiling Black_Hair
    Straight_Hair 7261 105 655 118 155 9577 71 533 74 663 16 838 34 122 43 492 After
    obtaining the positive and negative sample vectors of the images, the difference
    between the two is the property vectors used in the property modification. When
    the property variables are added to the latent variables generated by the VAE
    model, the corresponding properties can be added to the reconstructed data. Fig.
    4 shows the training images modified by the properties. Each column corresponds
    to the original images of female and male, modified Black_Hair property images,
    modified Smiling property images, modified Straight_Hair property images, modified
    Black_Hair Smiling property images, modified Black_Hair, Smiling and Straight_Hair
    property images. It can be seen that the property vectors have an excellent modification
    effect on the original images, and the result of a single property modification
    shows a negative correlation with the threshold value of the constraint in the
    previous section, i.e., the smaller the threshold value, the better the modification
    effect. Multiple property modifications achieve the superposition of properties
    on the images with the feature that latent variables have continuity. To validate
    the usability of the scheme, experiments were carried out on devices to generate
    the above five classes of property modification data to participate in the FL
    gender classification task. Compared to the classification accuracy of the original
    data in the FL task, the experimental results are shown in Fig. 5. The classification
    accuracy of the original data is 97.12%, and the classification accuracy of the
    modified data for the five categories of properties is 92.92%, 93.21%, 93.37%,
    92.47%, and 94.44%, which can still maintain a high accuracy. FL with paillier
    homomorphic encryption (HE) exhibits a delayed convergence compared to FLPM. FL
    using differential privacy (DP) yields lower classification accuracy and an unstable
    training process. Download : Download high-res image (458KB) Download : Download
    full-size image Fig. 4. Celeba dataset property modified images. Download : Download
    high-res image (345KB) Download : Download full-size image Fig. 5. Federated learning
    accuracy of CelebA dataset. The experiment employed two distinct interpretability
    techniques: LIME [33] and SmoothGrad [34], to assess the images reconstruction
    of FLPM. In Fig. 6, the first and third columns show the original data, while
    the second and fourth columns show the reconstructed data using FLPM. Specifically,
    LIME is utilized to identify critical regions within images that influence classification
    decisions. These regions are highlighted in red in Fig. 6(a). It is evident that
    FLPM’s property separation approach, based on the class activation map method,
    reduces the model’s focus on task-irrelevant areas during image classification
    tasks. This effectively mitigates poisoning attacks due to the embedding of triggers
    in task-irrelevant regions of the data. Additionally, SmoothGrad creates sensitivity
    maps by aggregating variations in model gradients caused by introducing noise
    to input images. The resultant images, marked by green areas as seen in Fig. 6(b),
    point essential features used for classification. By comparing original and reconstructed
    data in Fig. 6(b), it becomes apparent that FLPM modifies properties that alter
    the distribution of key points in sensitivity maps, thereby increasing the difficulty
    of property inference attacks. Download : Download high-res image (277KB) Download
    : Download full-size image Fig. 6. The effect explanation for FLPM. The target
    of property inference is to reveal the properties of training data from other
    devices on the attacker-controlled device. The experiments verify whether the
    property modified data can correctly recover the corresponding properties by constructing
    a property classifier. Table 2 shows the classification accuracy of the property
    classifier for 10 benign devices for Smiling, Black_Hair, and Straight_Hair properties
    when 0.1. First, the AUC score of the property classifier for the original images
    of all three properties in the table is greater than 0.98, while there is a significant
    decrease in the AUC score of the property modified data. It indicates that the
    property modification has taken effect and that the attacker cannot successfully
    discriminate the non-task properties of the devices’ private data. Then, properties
    with smaller thresholds of constraints are more suitable for their modification
    to protect privacy, where Smiling with a minor threshold is already close to the
    probability of random guessing on the data. Table 2. The AUC score of property
    inference attack after FLPM. Property 0 1 2 3 4 5 6 7 8 9 Smiling 0.56 0.56 0.56
    0.56 0.55 0.56 0.56 0.55 0.55 0.54 Black_Hair 0.65 0.62 0.65 0.65 0.64 0.63 0.66
    0.65 0.66 0.61 Straight_Hair 0.72 0.71 0.72 0.70 0.71 0.70 0.72 0.71 0.71 0.73
    Table 3. The comparison of FLPM and NOSnoop schemes. Property No defense S Q(0)
    Q(0.5) Q(0.9) FLPM Empty Cell ACC AUC ACC AUC ACC AUC ACC AUC ACC AUC ACC AUC
    Smiling 93.21 0.94 76.45 0.53 83.58 0.75 81.43 0.68 77.52 0.58 92.92 0.56 Black_Hair
    92.92 0.97 77.24 0.64 86.17 0.79 82.65 0.72 78.39 0.65 93.21 0.65 Straight_Hair
    93.37 0.98 76.60 0.63 83.95 0.78 82.58 0.76 80.4 0.69 93.37 0.71 The comparison
    results between no defense, FLPM and NOSnoop scheme [26] are shown in Table 3,
    where ACC represents the accuracy of the global model, and the lower the AUC score
    close to 0.5 indicates the better defense effect. When defense is not used, the
    AUC scores for the property inference attack against the Smiling, Black_Hair,
    and Straight_Hair properties are all above 0.94. After using FLPM with the parameter
    0.1, there is a significant decrease in the AUC score. It indicates that the property
    modification has taken effect and that the attacker cannot successfully discriminate
    the non-task properties of the devices’ private data. S, Q(0), Q(0.5), and Q(0.9)
    represent NOSnoop schemes with different parameter settings. It is clear that
    the defense effect of FLPM is slightly worse than S, but in most cases it is better
    than Q(0), Q(0.5), and Q(0.9). Furthermore, the effect of FLPM on the global model
    accuracy is minimal and achieves the balance between model accuracy and defense
    effectiveness. 4.3. Single property data property modification In the property
    modification on the EMNIST dataset, the experiment’s target is to achieve the
    separation of non-task properties of letters in images with the help of clustering
    algorithms. Since the dataset is simple, image reconstruction can be performed
    using only vanilla VAE [21]. The devices quickly completed the VAE training in
    50 epochs by using the VAE pre-trained by MNIST dataset with the help of the server.
    The VAE encoder downscaled the input data to 512-dimensional vectors and subsequently
    clustered these vectors into two classes with the help of Kmeans [35] under the
    constraint of Eq. (11). The visualization of the center of mass for clustering
    is shown in Fig. 7. The figure shows that the two clustered masses of the same
    letter show apparent differences, and the difference between uppercase and lowercase
    letters can be clearly identified. The scheme separates the uppercase and lowercase
    properties of the letters. Then, these center of mass vectors is used to reconstruct
    latent variables. After adopting the property modification for single property
    data in the scheme, the target properties are well reflected in the images while
    producing significant differences from the original images, and some of the reconstructed
    results are shown in Fig. 8. All uppercase letters in the figure are from the
    original dataset. Lowercase letters are generated from uppercase letters by the
    property modification scheme. We conducted experiments on FL alphabet classification
    to assess how modifications aimed at enhancing data privacy affect performance.
    Our results revealed that while unmodified datasets achieved a classification
    accuracy of 94.45%, datasets with modified properties exhibited a slightly lower
    accuracy of 92.95%. This marginal reduction of 1.5% is attributable to the inherent
    trade-offs required to safeguard privacy. Nonetheless, FLPM successfully achieves
    a balance between privacy protection and practical utility, a notable improvement
    over traditional differential privacy methods applied in FL scenarios. Table 4.
    The comparison of defense schemes for data poisoning attacks. Method Visible Invisible
    DBA Empty Cell ACC ASR ACC ASR ACC ASR No defense 92.18 95.12 94.39 91.86 92.84
    92.65 FLIP 81.67 44.68 77.84 42.37 79.23 40.82 SP-4 84.72 53.28 83.55 49.73 82.36
    52.62 AE 81.46 9.62 84.36 13.52 82.43 11.78 NC 91.32 10.58 90.95 21.83 88.35 16.75
    DS 87.14 4.54 89.58 10.34 89.02 12.75 FLPM 93.25 5.13 93.41 7.07 93.57 4.60 Download
    : Download high-res image (89KB) Download : Download full-size image Fig. 7. Visualization
    of the center of mass for clustering. Download : Download high-res image (156KB)
    Download : Download full-size image Fig. 8. Images of the property modification
    of the EMNIST dataset. 4.4. Effectiveness of defense against data poisoning attacks
    FLPM can effectively mitigate data poisoning attacks. These attacks typically
    involve injecting malicious data into a dataset, with the intent to skew models’
    predictions. FLPM thwarts such attacks by leveraging VAE, which play a crucial
    role in sanitizing input data. The defense mechanism operates on two fronts. First,
    during VAE encoding, it distills the essence of image data into a set of latent
    variables which similar to compressing an image while striving to retain its core
    content. In this step, any extraneous information (including harmful elements
    introduced by a poisoning attack) is likely identified as noise and removed due
    to its irrelevance to the main features of the image. Second, in the decoding
    phase, VAE reconstruct images from their latent representations. It inherently
    smooths over gaps and inconsistencies in the feature space caused by poisoned
    data. This process effectively repairs images, restoring normality by ensuring
    continuity and removing anomalous values. Through this dual-action filtering and
    repairing process facilitated by VAE in FLPM frameworks, models are less susceptible
    to being mislead by corrupted inputs. The experiments use visible and invisible
    triggers in backdoor attacks to construct poisoned data, respectively. First,
    we generate visible triggers effective against FL according to badnets [16], and
    poisoning images and VAE reconstructed images are shown in Fig. 9. The scheme
    adds triggers of size 15 × 15 to different locations in images and masks facial
    regions as much as possible. It can be seen that the triggers (i.e., abnormal
    regions in images) in the property modified images have been corrected by VAE.
    Although there is some impact on the sharpness of the images, the property modified
    images can still be classified into the correct category. Second, using the invisible
    trigger generation scheme proposed in [17], we generated the poisoned data as
    shown in the fourth row of Fig. 9. It is important to note that the invisibility
    here is for the human visual difficulty to distinguish. Fig. 9(e) shows the property
    modification images that the background and face are much cleaner compared to
    the poisoned data. The defense effectiveness of FLPM, FLIP [20], SP-4 [20], Auto-Encoder
    (AE) [36], Neural Cleanse (NC) [37], and DeepSweep (DS) [38] schemes are shown
    in Table 4. The experiment carried out a data poisoning attack on FL by using
    visible and invisible triggers to construct poisoned data, based on the backdoor
    attack framework proposed by E. Bagdasaryan et al. [39]. Furthermore, a distributed
    data poisoning attack was carried out using DBA [40]. ACC and ASR represent the
    accuracy of the global model and the attack success rate of attacks. From the
    table, we can see that the FLIP and SP-4 schemes are poor for defending against
    attacks. The AE scheme achieves good defense results, but the quality of its reconstructed
    images is worse than FLPM, therefore leading to a significant accuracy decrease.
    FLPM can better balance model accuracy with defense effectiveness compared to
    the NC and DS schemes. Download : Download high-res image (683KB) Download : Download
    full-size image Fig. 9. FLPM defense data poisoning attacks. 5. Conclusion This
    paper proposes a VAE-based property modification scheme (FLPM) in the context
    of the growing security issues of FL. FLPM contains property separation, selection,
    and control algorithms to modify specific properties of data. By striking a balance
    between privacy and practicality, FLPM effectively protects device data privacy
    and model security. Detailed experimental results demonstrate that FLPM introduces
    a marginal increase in training time while preserving model accuracy. The modified
    properties are clearly visible in the processed images produced by FLPM. Furthermore,
    the scheme achieves an accuracy of 94.44% for data with three modified properties
    in FL while successfully defending against property inference attacks and data
    poisoning attacks. In future work, we aim to analyze observed phenomena from these
    experiments to propose enhanced mechanisms for protecting against a wider range
    of attacks in FL. CRediT authorship contribution statement Shuo Xu: Conceptualization,
    Methodology, Validation, Writing – original draft. Hui Xia: Conceptualization,
    Funding acquisition, Supervision. Peishun Liu: Funding acquisition, Writing –
    review & editing. Rui Zhang: Data curation, Resources. Hao Chi: Formal analysis,
    Investigation. Wei Gao: Funding acquisition, Writing – review & editing. Declaration
    of competing interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Acknowledgments This research is supported by the
    National Natural Science Foundation of China (NSFC) [grant numbers 62172377, 61872205],
    the Shandong Provincial Natural Science Foundation, China [grant number ZR2019MF018],
    and the Startup Research Foundation for Distinguished Scholars, China [grant number
    202112016]. Data availability Data will be made available on request. References
    [1] McMahan B., Moore E., Ramage D., Hampson S., y Arcas B.A. Communication-efficient
    learning of deep networks from decentralized data Proceedings of the 20th International
    Conference on Artificial Intelligence and Statistics, AISTATS, 54, PMLR (2017),
    pp. 1273-1282 View in ScopusGoogle Scholar [2] Li Z., Chen Z., Wei X., Gao S.,
    Ren C. HPFL-CN: communication-efficient hierarchical personalized federated edge
    learning via complex network feature clustering 19th Annual IEEE International
    Conference on Sensing, Communication, and Networking, SECON, IEEE (2022), pp.
    325-333 CrossRefView in ScopusGoogle Scholar [3] Zhao J., Wang R. Fedmix: A sybil
    attack detection system considering cross-layer information fusion and privacy
    protection 19th Annual IEEE International Conference on Sensing, Communication,
    and Networking, SECON, IEEE (2022), pp. 199-207 View in ScopusGoogle Scholar [4]
    Barroso N.R., Jiménez-López D., Luzón M.V., Herrera F., Martínez-Cámara E. Survey
    on federated learning threats: Concepts, taxonomy on attacks and defences, experimental
    study and challenges Inf. Fusion, 90 (2023), pp. 148-173 Google Scholar [5] Shokri
    R., Shmatikov V. Privacy-preserving deep learning Proceedings of the 22nd ACM
    SIGSAC Conference on Computer and Communications Security, ACM (2015), pp. 1310-1321
    CrossRefView in ScopusGoogle Scholar [6] Liu W., Cheng J., Wang X., Lu X., Yin
    J. Hybrid differential privacy based federated learning for internet of things
    J. Syst. Archit., 124 (2022), Article 102418 View PDFView articleView in ScopusGoogle
    Scholar [7] Rivest R.L., Adleman L.M., Dertouzos M.L. On data banks and privacy
    homomorphisms Found. Secure Compuation (1978) Google Scholar [8] Zheng X., Cai
    Z. Privacy-preserved data sharing towards multiple parties in industrial iots
    IEEE J. Sel. Areas Commun., 38 (5) (2020), pp. 968-979 CrossRefView in ScopusGoogle
    Scholar [9] Liu Y., Kang Y., Xing C., Chen T., Yang Q. A secure federated transfer
    learning framework IEEE Intell. Syst., 35 (4) (2020), pp. 70-82 CrossRefGoogle
    Scholar [10] Jayaraman B., Evans D. When relaxations go bad: differentially-private
    machine learning (2019) CoRR abs/1902.08874 Google Scholar [11] Melis L., Song
    C., Cristofaro E.D., Shmatikov V. Exploiting unintended feature leakage in collaborative
    learning 2019 IEEE Symposium on Security and Privacy, SP, IEEE (2019), pp. 691-706
    CrossRefView in ScopusGoogle Scholar [12] Gopinath D., Converse H., Pasareanu
    C.S., Taly A. Property inference for deep neural networks 34th IEEE/ACM International
    Conference on Automated Software Engineering, ASE, IEEE (2019), pp. 797-809 CrossRefView
    in ScopusGoogle Scholar [13] Ganju K., Wang Q., Yang W., Gunter C.A., Borisov
    N. Property inference attacks on fully connected neural networks using permutation
    invariant representations Proceedings of the 2018 ACM SIGSAC Conference on Computer
    and Communications Security, CCS, ACM (2018), pp. 619-633 CrossRefView in ScopusGoogle
    Scholar [14] Pasquini D., Ateniese G., Bernaschi M. Unleashing the tiger: Inference
    attacks on split learning CCS ’21: 2021 ACM SIGSAC Conference on Computer and
    Communications Security, ACM (2021), pp. 2113-2129 CrossRefView in ScopusGoogle
    Scholar [15] Tayyab M., Marjani M., Jhanjhi N.Z., Hashem I.A.T., Usmani R.S.A.,
    Qamar F. A comprehensive review on deep learning algorithms: Security and privacy
    issues Comput. Secur., 131 (2023), Article 103297 View PDFView articleView in
    ScopusGoogle Scholar [16] Gu T., Dolan-Gavitt B., Garg S. Badnets: Identifying
    vulnerabilities in the machine learning model supply chain (2017) CoRR abs/1708.06733
    Google Scholar [17] Saha A., Subramanya A., Pirsiavash H. Hidden trigger backdoor
    attacks The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,
    the Thirty-Second Innovative Applications of Artificial Intelligence Conference,
    AAAI Press (2020), pp. 11957-11965 CrossRefView in ScopusGoogle Scholar [18] Liu
    T., Hu X., Shu T. Assisting backdoor federated learning with whole population
    knowledge alignment in mobile edge computing 19th Annual IEEE International Conference
    on Sensing, Communication, and Networking, SECON, IEEE (2022), pp. 416-424 CrossRefView
    in ScopusGoogle Scholar [19] Cai Z., He Z., Guan X., Li Y. Collective data-sanitization
    for preventing sensitive information inference attacks in social networks IEEE
    Trans. Dependable Secur. Comput., 15 (4) (2018), pp. 577-590 View in ScopusGoogle
    Scholar [20] Li Y., Zhai T., Jiang Y., Li Z., Xia S. Backdoor attack in the physical
    world (2021) CoRR abs/2104.02361 Google Scholar [21] Kingma D.P., Welling M. Auto-encoding
    variational bayes 2nd International Conference on Learning Representations, ICLR
    (2014) Google Scholar [22] Zheng X., Guo Y., Huang H., Li Y., He R. A survey of
    deep facial attribute analysis Int. J. Comput. Vis., 128 (8) (2020), pp. 2002-2034
    CrossRefView in ScopusGoogle Scholar [23] Russakovsky O., Fei-Fei L. Attribute
    learning in large-scale datasets Trends and Topics in Computer Vision - ECCV 2010
    Workshops, Vol. 6553, Springer (2010), pp. 1-14 Google Scholar [24] Kairouz P.,
    McMahan H.B., Avent B., Bellet A., Bennis M. Advances and open problems in federated
    learning Found. Trends Mach. Learn., 14 (1–2) (2021), pp. 1-210 CrossRefView in
    ScopusGoogle Scholar [25] He C., Li S., So J., Zhang M., Wang H. Fedml: A research
    library and benchmark for federated machine learning (2020) CoRR abs/2007.13518
    Google Scholar [26] Ma X., Li B., Jiang Q., Chen Y., Gao S. Nosnoop: An effective
    collaborative meta-learning scheme against property inference attack IEEE Internet
    Things J., 9 (9) (2022), pp. 6778-6789 CrossRefView in ScopusGoogle Scholar [27]
    Ye J., Maddi A., Murakonda S.K., Bindschaedler V., Shokri R. Enhanced membership
    inference attacks against machine learning models Proceedings of the 2022 ACM
    SIGSAC Conference on Computer and Communications Security, CCS, ACM (2022), pp.
    3093-3106 CrossRefView in ScopusGoogle Scholar [28] Chattopadhyay A., Sarkar A.,
    Howlader P., Balasubramanian V.N. Grad-cam++: Generalized gradient-based visual
    explanations for deep convolutional networks 2018 IEEE Winter Conference on Applications
    of Computer Vision, WACV, IEEE Computer Society (2018), pp. 839-847 Google Scholar
    [29] Liu Z., Luo P., Wang X., Tang X. Deep learning face attributes in the wild
    2015 IEEE International Conference on Computer Vision, ICCV, IEEE Computer Society
    (2015), pp. 3730-3738 CrossRefView in ScopusGoogle Scholar [30] Cohen G., Afshar
    S., Tapson J., van Schaik A. EMNIST: extending MNIST to handwritten letters 2017
    International Joint Conference on Neural Networks, IJCNN, IEEE (2017), pp. 2921-2926
    CrossRefView in ScopusGoogle Scholar [31] Rothe R., Timofte R., Gool L.V. Deep
    expectation of real and apparent age from a single image without facial landmarks
    Int. J. Comput. Vis., 126 (2–4) (2018), pp. 144-157 CrossRefView in ScopusGoogle
    Scholar [32] LeCun Y., Bottou L., Bengio Y., Haffner P. Gradient-based learning
    applied to document recognition Proc. IEEE, 86 (11) (1998), pp. 2278-2324 Google
    Scholar [33] Ribeiro M.T., Singh S., Guestrin C. Why should I trust you?: Explaining
    the predictions of any classifier Proceedings of the 22nd ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
    13-17, 2016, ACM (2016), pp. 1135-1144 CrossRefView in ScopusGoogle Scholar [34]
    Smilkov D., Thorat N., Kim B., Viégas F.B., Wattenberg M. Smoothgrad: removing
    noise by adding noise (2017) CoRR abs/1706.03825 Google Scholar [35] J. MacQueen,
    Some methods for classification and analysis of multivariate observations, in:
    Proc. 5th Berkeley Symposium on Math. Stat. and Prob., 1965, p. 281. Google Scholar
    [36] Liu Y., Xie Y., Srivastava A., trojans Neural. Neural trojans 2017 IEEE International
    Conference on Computer Design, ICCD, IEEE Computer Society (2017), pp. 45-48 Google
    Scholar [37] Wang B., Yao Y., Shan S., Li H., Viswanath B., Zheng H., Zhao B.Y.
    Neural cleanse: Identifying and mitigating backdoor attacks in neural networks
    2019 IEEE Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA,
    May 19-23, 2019, IEEE (2019), pp. 707-723 CrossRefGoogle Scholar [38] Qiu H.,
    Zeng Y., Guo S., Zhang T., Qiu M. Deepsweep: An evaluation framework for mitigating
    DNN backdoor attacks using data augmentation ASIA CCS ’21: ACM Asia Conference
    on Computer and Communications Security, ACM (2021), pp. 363-377 CrossRefView
    in ScopusGoogle Scholar [39] Bagdasaryan E., Veit A., Hua Y., Estrin D., Shmatikov
    V. How to backdoor federated learning The 23rd International Conference on Artificial
    Intelligence and Statistics, AISTATS 2020 26-28 2020, Online [Palermo, Sicily,
    Italy], Proceedings of Machine Learning Research, vol. 108, PMLR (2020), pp. 2938-2948
    View in ScopusGoogle Scholar [40] Xie C., Huang K., Chen P., Li B. DBA: distributed
    backdoor attacks against federated learning 8th International Conference on Learning
    Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net
    (2020) Google Scholar Cited by (0) Shuo Xu received the B.S. degree in Computer
    Science and Technology from Qingdao University, in 2016. He is currently pursuing
    a Ph.D. in System and Network Security with Ocean University of China, Qingdao,
    China. His current research interests include federated learning and privacy protection.
    Hui Xia received his Ph.D. degree in computer science at School of Computer Science
    and Technology, Shandong University, China, in June 2013. From July 2013 to June
    2020, he worked as a lecturer and associate professor at College of Computer Science
    and Technology, Qingdao University, China. Since July 2020, he is currently a
    Full Professor and a Ph.D. Supervisor at the College of Computer Science and Technology,
    Ocean University of China. He was a visiting scholar at the Department of Computer
    Science, The George Washington University, U.S. (20172018). His current research
    interests focus on Wireless self-organizing networks, IoT security, AI security,
    Crowdsourcing computing, Privacy protection, Federated learning and Edge computing.
    He has published over 43 scientific papers, and his research is sponsored by the
    Natural Science Foundation of China (NSFC) under Grant Nos. 62172377 and 61872205,
    and the Shandong Provincial Natural Science Foundation under Grant No. ZR2019MF018.
    He is a member of the ACM and the IEEE. Peishun Liu is an Associate Professor
    in the Faculty of Information Science and Engineering at Ocean University of China.
    His research interests include network & information security and artificial intelligence.
    Rui Zhang received the M.S. degree in network security from Qingdao University,
    in 2018. She is currently pursuing a Ph.D. in Artificial Intelligence at Ocean
    University of China, Qingdao, China. Her current research interests include artificial
    intelligence and privacy protection. Hao Chi received the Bachelor’s degree in
    Computer Science and Technology from the School of Qingdao University, Shandong,
    China, in June 2021. He was awarded the title of Outstanding Graduate of Qingdao
    University in 2021. He was admitted to Ocean University of China in September
    2021. His research interests include Federated Learning, Transfer Learning and
    information security, and IoT security. Wei Gao, Tai Shan Scholar, Deputy Director
    of Shandong Acoustical Society, and Level One Expert of China Aviation Industry
    Corporation. He received his Ph.D. degree in the ocean Information detection and
    processing at Department of Ocean Technology, Ocean University of China. From
    July 2008 to December 2015, he worked as the Deputy Chief Engineer of Products
    and Head of Research and Development Department at the 710th Institute of China
    Shipbuilding Industry Corporation (CSIC). From January 2016 to December 2018,
    he worked as the Deputy Chief Engineer and Chief Engineer of Models at Aviation
    Electronics Company, China Aviation Industry Corporation. Since January 2019,
    he is currently a Full Professor and a Ph.D. Supervisor at the College of Ocean
    Technology, Ocean University of China. His current research interests focus on
    underwater acoustics, underwater signal processing, multi-physics field coupling,
    and the application of deep learning in underwater acoustics engineering. In 2021,
    he was honored as an outstanding science and technology worker in Shandong Province.
    View Abstract © 2023 Elsevier B.V. All rights reserved. Part of special issue
    Artificial Intelligence for Time-critical Computing Systems Edited by Xiaomin
    Chen, Zhiming Zhao, Long Cheng View special issue Recommended articles Efficient
    topic partitioning of Apache Kafka for high-reliability real-time data streaming
    applications Future Generation Computer Systems, Volume 154, 2024, pp. 173-188
    Theofanis P. Raptis, …, Andrea Passarella View PDF Load balancing for heterogeneous
    serverless edge computing: A performance-driven and empirical approach Future
    Generation Computer Systems, Volume 154, 2024, pp. 266-280 Mohammad Sadegh Aslanpour,
    …, Mohsen Amini Salehi View PDF FaaS for IoT: Evolving Serverless towards Deviceless
    in I/Oclouds Future Generation Computer Systems, Volume 154, 2024, pp. 189-205
    Giovanni Merlino, …, Antonio Puliafito View PDF Show 3 more articles Article Metrics
    Captures Readers: 4 View details About ScienceDirect Remote access Shopping cart
    Advertise Contact and support Terms and conditions Privacy policy Cookies are
    used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply."'
  inline_citation: '>'
  journal: Future Generation Computer Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'FLPM: A property modification scheme for data protection in federated learning'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Arnau Muñoz L.
  - Berná Martínez J.V.
  - Maciá Pérez F.
  - Lorenzo Fonseca I.
  citation_count: '0'
  description: The inclusion of IoT in digital platforms is very common nowadays due
    to the ease of deployment, low power consumption and low cost. It is also common
    to use heterogeneous IoT devices of ad-hoc or commercial development, using private
    or third-party network infrastructures. This scenario makes it difficult to detect
    invalid packets from malfunctioning devices, from sensors to application servers.
    These invalid packets generate low quality or erroneous data, which negatively
    influence the services that use them. For this reason, we need to create procedures
    and mechanisms to ensure the quality of the data obtained from IoT infrastructures,
    regardless of the type of infrastructure and the control we have over them, so
    that the systems that use this data can be reliable. In this work we propose the
    development of an Anomaly Detection System for IoT infrastructures based on Machine
    Learning using unsupervised learning. We validate the proposal by implementing
    it on the IoT infrastructure of the University of Alicante, which has a multiple
    sensing system and uses third-party services, over a campus of one million square
    meters. The contribution of this work has been the generation of an anomaly detection
    system capable of revealing incidents in IoT infrastructures, without knowing
    details about the infrastructures or devices, through the analysis of data in
    real time. This proposal allows to discard from the IoT data flow all those packets
    that are suspected to be anomalous to ensure a high quality of information to
    the tools that consume IoT data.
  doi: 10.1016/j.iot.2024.101095
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Background and related work 3. Proposed
    solution 4. Implementation and analysis of results 5. Conclusions Declaration
    of competing interest Acknowledgement Data availability References Show full outline
    Figures (12) Show 6 more figures Tables (2) Table 1 Table 2 Internet of Things
    Volume 25, April 2024, 101095 Anomaly detection system for data quality assurance
    in IoT infrastructures based on machine learning Author links open overlay panel
    Lucia Arnau Muñoz, José Vicente Berná Martínez, Francisco Maciá Pérez, Iren Lorenzo
    Fonseca Show more Share Cite https://doi.org/10.1016/j.iot.2024.101095 Get rights
    and content Under a Creative Commons license open access Abstract The inclusion
    of IoT in digital platforms is very common nowadays due to the ease of deployment,
    low power consumption and low cost. It is also common to use heterogeneous IoT
    devices of ad-hoc or commercial development, using private or third-party network
    infrastructures. This scenario makes it difficult to detect invalid packets from
    malfunctioning devices, from sensors to application servers. These invalid packets
    generate low quality or erroneous data, which negatively influence the services
    that use them. For this reason, we need to create procedures and mechanisms to
    ensure the quality of the data obtained from IoT infrastructures, regardless of
    the type of infrastructure and the control we have over them, so that the systems
    that use this data can be reliable. In this work we propose the development of
    an Anomaly Detection System for IoT infrastructures based on Machine Learning
    using unsupervised learning. We validate the proposal by implementing it on the
    IoT infrastructure of the University of Alicante, which has a multiple sensing
    system and uses third-party services, over a campus of one million square meters.
    The contribution of this work has been the generation of an anomaly detection
    system capable of revealing incidents in IoT infrastructures, without knowing
    details about the infrastructures or devices, through the analysis of data in
    real time. This proposal allows to discard from the IoT data flow all those packets
    that are suspected to be anomalous to ensure a high quality of information to
    the tools that consume IoT data. Previous article in issue Next article in issue
    Keywords Internet of thingsAnomaly detectionMachine learningIsolation forest 1.
    Introduction IoT infrastructures are now widely deployed in our society. They
    were first driven mainly by the needs of large companies to control certain processes,
    speed up and improve their efficiency, or reduce the occurrence of errors [1].
    Subsequently, due to their use in Smart City environments, for any type of use,
    such as air quality monitoring, traffic monitoring, waste management or citizen
    safety [2]. Nowadays, they have become common sensor systems in Digital Twin platforms
    [3], where a realistic representation of the controlled world requires knowledge
    of the real-time state of the system. It is precisely in these latter environments,
    where data quality is crucial to achieve the objectives. These systems also generate
    new challenges and problems, some of them concerning the proper functioning of
    the infrastructures themselves. These types of problems related to monitoring
    anomalies in the performance of infrastructures have been widely addressed from
    the perspective of traditional TCP/IP ethernet networks, using for example network
    intrusion detection systems (IDS), which is just a subtype of anomaly detection
    systems (ADS) [4] but focused on the use of infrastructures. Such systems are
    capable of monitoring events occurring in the infrastructure, detecting anomalies
    of various nature, and generating an automated and controlled processing of these
    events to ensure the proper functioning of the system [5]. The strength of these
    systems lies in the use of highly standardised and accepted protocols, such as
    TCP/IP, and services whose behaviour is tightly controlled, such as HTTP, FTP,
    STMP, etc. However, in IoT networks we find a large number of technologies, encodings,
    packaging, applications and in general much more heterogeneous systems where we
    cannot analyse headers as is done in traditional IDSs over Ethernet networks [6].
    In addition, in IoT environments it is common to use infrastructures based on
    Open Source initiatives, such as TTN [7], and therefore the data-emitting devices
    cross intermediate networks that are beyond the control of the administrator of
    the IoT sensors that emit data, so there is even metadata over which there is
    no control or access. It is also very common to find that, in our control system
    platform, Smart City platform or digital twin, the information that is being integrated
    comes from different subsystems, manufactured by different companies, with different
    technologies and different natures. In order for these tools to work as expected,
    it is necessary to ensure that the data with which we are going to feed them are
    valid and have an adequate degree of quality, discarding anything that may come
    from an element of the infrastructure that exhibits a malfunction. Likewise, it
    is necessary to do so even when we do not have details about the infrastructure
    that originates the data. That is why, to ensure the quality of the data generated
    by the IoT infrastructure, it is necessary to conceive mechanisms and procedures
    to be applied on these IoT data flows, of which the types of incidents that can
    be detected are unknown. That is, to create an Anomaly Detection System (ADS)
    that is capable of working on information generated by the IoT, detecting everything
    that is out of the expected normality. For an ADS to work correctly and efficiently
    when detecting possible threats or anomalous elements, it is necessary to follow
    a series of processes [8]: data collection to obtain the parameters of the packet
    to be analysed; generation of rules and algorithms for the definition of anomalies;
    execution of filters and analysis using the rules and algorithms on the collected
    packets; and detection and treatment of the detected anomalies. Only the systematisation
    of the processes to be carried out can ensure a good result, and, in addition,
    these processes must be adapted to the scope of the problem to be addressed, since
    the search for anomalies or outliers requires techniques and strategies coupled
    to the environment [9]. If we look at some of the contributions that review the
    different techniques for the creation of anomaly detection systems [10], we can
    see that the systematic treatment of the data is one of the first tasks to be
    completed before moving on to the selection and training of the Machine Learning
    (ML) algorithms. Once the data adequacy process has been completed, it is essential
    to select an algorithm in accordance with the objective to be achieved, so that
    the resulting trained model can correctly ensure the operation of the infrastructure.
    This work proposes the development of an ADS on IoT infrastructures based on Machine
    Learning, so that the system can be sufficiently generic to cover a wide range
    of platforms and at the same time be able to detect anomalies in the infrastructures.
    The objective is not to diagnose the infrastructures, to know the exact incident
    that is occurring, but to be able, on real-time traffic, to classify the data
    packets we receive as valid or invalid, regardless of their content or origin.
    In this way, a filter can be generated prior to the injection of this data on
    the applications that use it and prevent incorrect information from contaminating
    the services. The paper proposes the design and development of an ADS system based
    on Machine Learning and also to validate its operation, it is instantiated on
    the Smart City platform of the University of Alicante, in which several IoT systems
    coexist, thus generating the previous data analysis that ensures the quality of
    the data before being used by the platform. The motivation to propose, develop
    and implement this model in our platform comes from the need to control a large
    infrastructure, in which different interconnected sensorization areas coexist,
    in which being aware of each anomaly that may occur is a complex and laborious
    work for the people responsible for its management. The rest of the work is organised
    into the following sections: Section 2 contains a preliminary study of the techniques
    and problems related to the project objectives; Section 3 shows the design of
    the ADS proposal and the processes and algorithms involved; Section 4 carries
    out the instantiation of the system on a real platform, the Smart University platform
    of the University of Alicante; Section 5 finally draws the main conclusions of
    the work and sets out the lines of future work. 2. Background and related work
    With the increasing evolution of cities and the lifestyle of the citizens living
    in them, IoT environments are evolving faster and faster, mainly due to the number
    of devices in these environments, using different communication protocols, great
    diversity of data to be sent, and varied formats and packaging for transmission.
    This is why the level of complexity for their control also increases along with
    the evolution of these systems, and with it, the need to develop solutions that
    can guarantee their security, efficiency, and reliability in these heterogeneous
    systems [11]. This complexity has derived in a need to expand research to find
    feasible solutions to the control of these systems, and over the last few years,
    the use of Machine Learning based applications has been chosen, so that they can
    be applied to IoT environments, since they are already implemented in many different
    areas of our daily life, such as medicine, in research or applications such as
    cancer detection [12]. Moreover, within the new difficulties that these environments
    already present when controlling them, even with the use of artificial intelligence,
    we must consider the aforementioned heterogeneity of the sensorisation devices,
    both in terms of the values they emit and the rest of the parameters they present,
    including their semantics and syntax [13], since they are still systems with a
    dynamic nature. This is why developing an automated model for this type of detection
    is a challenge to say the least, since the data for training cannot always be
    labelled, as required in some Machine Learning algorithms, as it can be very difficult
    to categorise them for this purpose. In addition, the data often contains noise
    and other types of values that can interfere with the reliability of the data,
    thus resulting in false anomalies [14]. Even today, even with the information
    available to address this problem, there is still no definitive solution due to
    the diversity of these environments, coupled with the lack of standardisation
    in the IoT domain. Although today''s IoT environments are still very much in flux,
    AI-based techniques have managed to provide a new approach from which to work
    and continue to gain knowledge. In AI, there are different valid techniques for
    anomaly detection, which already take into account this diversity of the system,
    both in the data sent and in the device that sends it [15], with approaches focused
    on Machine Learning to adjust to these changes in the data, or other techniques
    used in the transformation of the data itself, such as its normalisation before
    the application of the algorithm [16]. Some of the most outstanding algorithms,
    mainly due to their great adaptive capacity, are Neural Networks [17], Support
    Vector Machines (SVM) [18], or Random Forest [19], working satisfactorily in systems
    where the data used have a heterogeneous and uncategorized nature [20]. Some examples
    of their use are SPAM detection [21], mainly developed using supervised learning
    techniques, such as collaborative approaches, or content-based models [22] or
    the detection of malicious URLS [23], using techniques such as Decision Trees
    or Random Forest [24], mainly due to the large amount of features that must be
    extracted for this type of detections. Within the field of anomaly detection systems
    development, algorithms based on the creation of random trees and decision trees
    show high efficiency and accuracy in detecting such anomalies, testing these algorithms
    with different types of datasets [25]. In the context of IoT, the Isolation Forest
    [26] algorithm has been one of the most prominent ones currently, due to its effectiveness
    in such varied, heterogeneous, and dynamic environments, as well as presenting
    good results when working with a large amount of data volumes, such as data broadcasts
    from IoT devices. Some of the reasons for its effectiveness is that it does not
    require a lot of training data when developing the model, and its decision making
    returns a concrete yes or no result, identifying the anomaly or not [27]. An example
    of the use of this technique can be found in the detection of fraudulent banking
    transactions [28]. However, these proposals tend to focus detection on already
    identified anomalies and have not been applied to open and uncertain scenarios.
    Table 1 show a comparative summary of the analysed techniques. Table 1. Comparative
    summary of the main techniques used for anomaly detection. Algorithm Type Description
    Typical IoT applications Advantages Disadvantages Random Forest Supervised Set
    of decision trees. Classification and regression Good performance and handling
    characteristics. Requires adjustment of hyperparameters. Neural Networks Supervised
    Model inspired by the human brain. Pattern recognition Ability to learn complex
    relationships. Requires large data sets and computation SVM Supervised Finds a
    hyperplane that maximizes margin. Anomaly detection Efficient in high-dimensional
    spaces. Requires adjustment of hyperparameters. Isolation Forest Not Supervised
    Based on construction of random trees. Anomaly detection Efficient and scalable.
    Sensitive to noise. 3. Proposed solution For the development of our proposal,
    we will use a sequence of independent processes that form the framework, as shown
    in Fig. 1. The aim of this framework is to provide a generic procedure that can
    be extrapolated to any IoT infrastructure. Download : Download high-res image
    (216KB) Download : Download full-size image Fig. 1. Framework for general anomaly
    detection in IoT. The first process is responsible for collecting the dataset
    from the IoT infrastructure and performing the pre-analysis of the data. In this
    process, data capture from IoT data channels is performed and stored in persistence
    systems, and meticulous observation of the data is performed to know the types
    of data captured. The following process performs a processing of the data in such
    a way that invalid or incomplete data is cleaned, feature engineering is carried
    out to determine which of all the data received are necessary for the study, the
    construction of processable data vectors is performed and finally a data representation
    is made in order to be able to observe the distribution of the dataset. This obtained
    dataset is divided into three subsets with a ratio of 70–20–10. The first subset
    will be used for training, the second will be used for testing the training, and
    the third for validating the testing and therefore the trained model. Through
    these processes of testing and validation, or cross-validation, we can ensure
    that the trained model meets its objectives. Finally, the last process will be
    the evaluation of the model through its implementation in a real scenario. 3.1.
    Data collection and pre-analysis To develop our proposal, we used a dataset collected
    directly from the IoT infrastructures of the University of Alicante. The use of
    open-source datasets such as those provided by Kaggle [29] could allow us to follow
    strategies and proposals from other authors and compare results more easily, but
    it did not provide us with a realistic context, nor would it be useful to apply
    it to our infrastructures. It was therefore decided to generate our own dataset.
    For this purpose, a packet collector was developed using Message Queuing Telemetry
    Transport (MQTT) to connect to the IoT management system based on TheThingStack
    [30]. A dataset of approximately 320,000 packets has been collected, the equivalent
    of one day of sensing on the platform. Each packet consists of 117 features. This
    dataset is made up of sensor packages from different sources: • Climatology: sensorisation
    data on environment such as outdoor temperature, relative humidity, light level,
    UV rays, rain. • Quality&Comfort: sensorisation related to indoor spaces of buildings
    such as temperature, humidity, air quality, CO2 concentration, suspended particles,
    presence of harmful gases, etc. • Consumption: information related to infrastructure
    consumption, water, electricity, and gas consumption. • Production: sensorisation
    of the energy generation of photovoltaic plants. • Recharging electric vehicles:
    data on the use and consumption of electric vehicle charging stations. • WIFI:
    data on the real-time use of the campus wireless network infrastructures. • Luminaire:
    data concerning lighting systems including outdoor streetlights, signage, monument
    courtesy lights, etc. • Others: other sensorisations, where data received that
    do not belong to any previous collection will be dumped. In addition, the amount
    of data generated by each sensor group is different. Fig 2 shows the distribution
    of data per set. Download : Download high-res image (132KB) Download : Download
    full-size image Fig. 2. Percentage of data of the total collected by type of sensorisation.
    In the pre-analysis, all fields in each packet were inspected and from the initial
    117, only 47 fields were selected. All fields that are unique tokens, additional
    timestamps or unique identifiers are eliminated. These are fields that do not
    correlate or do not provide useful information in principle. 3.2. Processing The
    use of Machine Learning techniques requires the ability to explore the dataset,
    i.e., it must be able to undergo classification techniques. This involves processing
    the dataset to perform several operations. First, null or invalid values must
    be dealt with, cleaning the dataset of these records to avoid contamination. Unexpected
    values are also analysed in order to treat them and transform them into expected
    values. The aim is to have a dataset that can be processed by the algorithms.
    On the set of 47 selected features, a study of the entropy of the data has been
    carried out through correlation, to decide which ones are determinant in the Machine
    Learning algorithms. Fig. 3 shows the result of the study. As can be seen, there
    are many variables that, although important from a technical point of view, such
    as the timestamp of the data emission, do not contribute value within the correlation,
    because they are fields that will always grow. Download : Download high-res image
    (773KB) Download : Download full-size image Fig. 3. Results of the correlation
    study. Through the correlation matrix it was decided to finally select 18 characteristics,
    which are shown in Table 2. Table 2. List of attributes to be used for algorithm
    training. Attributes Description received_at The date and time the data packet
    was received at the TTN server. uplink_message.session_key_id The uplink message
    session key identifier. This key is used in data encryption. uplink_message.frm_payload
    The raw data field of the uplink message. uplink_message.decoded_payload.bytes
    The decoded data contained in the uplink message, expressed in bytes. gateway_ids.gateway_id
    The unique identifier of the gateway that received the message. time The date
    and time of the message timestamp A timestamp indicating when the message was
    received, usually in Unix timestamp format. rssi Received Signal Strength Indicator
    - Received Signal Strength Indicator, which measures the strength of the signal
    received from the device. channel_rssi The signal strength on the specific channel
    on which the message was received. snr Signal-to-Noise Ratio - The signal-to-noise
    ratio, which indicates the quality of the received signal. location.latitude The
    geographic latitude of the location of the gateway that received the message.
    location.longitude The geographic longitude of the location of the gateway that
    received the message. location.altitude The geographic altitude of the location
    of the gateway that received the message. uplink_token A token associated with
    the uplink message. channel_index The index of the channel used to transmit the
    message. uplink_message.settings.data_ rate.lora.spreading_factor The spreading
    factor used in the LoRa modulation of the message. uplink_message.received_at
    The date and time the uplink message was received. uplink_message.consumed_airtime
    The airtime consumed by the uplink message in the network. Regarding the decision
    factors for feature selection, we have focused on two determining elements, the
    numerical factor obtained in the correlation study, from the perspective of \"Pearson''s
    Correlation\" [31], and also on the know-how of the technical staff, since the
    experience gained from daily work with IoT traffic helps us to know which attributes
    can contribute to indicate infrastructure failures. The elements that were finally
    selected have correlation indexes from 0.1 to 0.99, indicating different levels
    of correlation between the different values. If we look at the central part of
    Fig. 3, we realize that most of the values present fall within this range of correlation
    values between each other. After this first indicator, we move on to the experience
    factor of the technical staff. During data transmission, there are essential values
    to control such as the noise signal, the received signal strength indicator, the
    byte packet sent, or the gateway that received the packet, amongst others. For
    this reason, these attributes will be taken into account in the analysis regardless
    of their correlation factor. The discarded values are fundamentally unique values,
    credentials to be exact, which are repeated during data sending, providing information
    such as the cluster in which the device is located, or the fixed port through
    which it works, so it did not provide us with useful information for the detection
    of anomalies in our system. Finally, another important operation is the transformation
    of categorical values to nominal values. In the dataset there are many columns
    of data indicating identifiers of network elements, services, or application.
    One Hot Encoding labelling is used for this purpose, converting the text data
    into feature vectors that can be further processed. 3.3. Theoretical considerations
    After the analysis of viable AI techniques in our context, it was determined that
    the Isolation Forest algorithm can generate the most correct results. This algorithm
    belongs to the most widely used unsupervised algorithms for anomaly detection
    and provides great flexibility in training as it does not need to label the data
    as valid or invalid from the beginning. This type of algorithm works on the detection
    of erroneous and unlabelled values within the datasets, also called outliers or
    anomalies. When training and implementing this type of algorithms, it has been
    demonstrated in different studies its efficient performance when dealing with
    large volumes of data, in addition to presenting a linear time complexity with
    a very low memory cost [32]. In our work, we will define outliers as: \"an observation
    that, being atypical and/or erroneous, deviates decidedly from the general behaviour
    of the experimental data with respect to the criteria that should be analysed
    about it\" [33]. The methodology employed by the Isolation Forest algorithm is
    based on the detection of outliers by using decision trees to isolate outliers
    from the rest of the data. To do this, a feature is selected and a random split
    between the minimum and maximum value is performed, repeating this process until
    all possible data splits are performed, or a specified limit on the number of
    splits is reached. The number of divisions needed to isolate a data item will
    be smaller for an outlier, while for normal values the number of divisions will
    be larger, since the algorithm attributes to each division an \"anomaly score\",
    calculated as the average of the number of subdivisions needed to isolate the
    outlier. The \"anomaly score\" is a value calculated using the following formula
    (1): (1) The parameters of which are: • h(x): is the average depth of constructed
    trees. • c(n): is the average height to find a node in one of the trees. • n:
    size of the dataset. • s: if the value obtained is close to 1, it is generally
    an anomaly, while if the value of s is less than 0.5, it is a correct value. The
    term E(h(x)) in formula (1) is calculated as: (2) where t is a tree, c(|lt(x)|)
    is a normalization factor needed when t is not fully grown (which estimates the
    average tree depth that can be constructed from lt(x)) and ht(x) = |Pt(x)| with
    Pt(x) being the path of x, the set of nodes visited by x from the root to the
    leaf containing x. From formula (1) it can be inferred that the score of an object
    x is proportional to the inverse of the average length of its path in the forest:
    if x ends in very deep leaves of the trees, its score will be quite low (close
    to 0), if on the contrary its path ends very soon the score will be high (close
    to 1) [34]. The reason for using this algorithm over other existing algorithms
    is mainly because it is an easily scalable algorithm for use on large datasets.
    In addition, it works well when features that may initially be irrelevant are
    included, as multi-modal datasets. This is the case for IoT infrastructures, where
    the cohesion or internal correlation between the data being sent is unknown, and
    we simply acquire a dataset with its corresponding parameters and want to detect
    outliers. In terms of implementation for model building, we must consider the
    basic elements with which we can train and subsequently improve the accuracy of
    the result: • \"contamination\", the amount of overall data that we expect to
    be considered outliers, indicates the estimated proportion of outliers that the
    dataset possesses. Based on this value, the limit by which the values are classified
    as anomalous or normal is set. • “n_estimators\", this value represents the number
    of isolation trees to be used to construct the Isolation Forest itself. Using
    higher estimator values can improve the accuracy for detection, but should always
    be appropriate to the dataset used, as it also results in increased training time.
    Varying the value of this parameter will serve to adjust the final performance
    of the model. • “max_samples\", number of observations used to train each tree;
    serves to control the maximum number of samples to be used to train each tree
    generated. You can use the value of auto which implies that all the samples in
    the dataset will be used, however, if you have a dataset that is too large, this
    value will have to be readjusted. • “max_features\", which indicates the maximum
    number of features to be used when splitting each node of the tree. By setting
    the value to 1, we specify that all available features will be used for each split.
    This parameter can be used as such in this case since many columns have been filtered
    during the data fitting process, however, if the dataset has many different columns,
    it can be adjusted in a way that improves performance and avoids overfitting.
    4. Implementation and analysis of results 4.1. Experimental setup The experimentation
    was carried out on an HP Pro-SFF 400 G9, with Windows 11 Pro 64-bit operating
    system, Intel® Core I7–12,700 CPU up to 4.9 GHz and 12 cores, 32GB RAM, and NVIDIA
    Quadro T400 graphics card. For the development of the algorithm, use was made
    of different libraries. NumPy [35] was used to carry out the relevant numerical
    arrays to process data, and Pandas [36] was used for the analysis and manipulation
    of structured data, providing the DataFrame format required for the algorithm.
    As for the implementation of the algorithm, the scikit-learn library [37] was
    used for its development, specifically the modules: sklearn.ensemble.IsolationForest,
    as the final algorithm for anomaly detection; sklearn.model_selection.train_test_split,
    to split the dataset for the training, testing and validation phases; sklearn.impute.SimpleImputer,
    mainly used for handling null values in the dataset, so that missing values are
    filled in a certain way; and finally, sklearn.preprocessing.RobustScaler, whose
    function is to scale features in a dataset, being very useful in datasets that
    may contain outliers. For the implementation of our model in particular, the following
    parameters were used in the configuration of the Isolation Forest algorithm. •
    contamination=float(0.1667). In the case of the training dataset, it is estimated
    that 16.67 % of the data contains an erroneous value that must be detected, so
    the training is adjusted to this contamination rate. • n_estimators=100 • max_samples=''auto
    • max_features=1.0 The choice of parameters depends on several issues and may
    condition the effectiveness of the model. The parameter \"contamination\" depends
    on the known number of erroneous packets, this value is known since the erroneous
    packets are introduced in a random but controlled way. The value of \"max_samples\"
    is set to \"auto\" to take all the data in the set, and the value of \"max_features\"
    is set to \"1″ to use all the properties of the dataset, since we have previously
    selected the significant properties. To set the value of \"n_estimators\", an
    empirical study was carried out for which: a reduced dataset was generated from
    the original one; and the Isolation Forest algorithm was tested by varying only
    the value of n_estimators until reaching the minimum value above which detection
    was no longer improved. Once the training is finished, we can visualise the results
    obtained for each field of the dataset. In order to have a graph for each field,
    we consider as a common element the \"time\" field (the timestamp of the packet),
    so the following graphs correspond to the time field, and another parameter of
    the dataset, such as snr, uplink_message.senssion_key_id,channel_index,uplink_message.setittings.data_rate.lora.spreading_factor,uplink_message.consumed_airtime
    or uplink_message.decode_payload.bytes. The following Fig. 4, Fig. 5, Fig. 6,
    Fig. 7, Fig. 8, Fig. 9 is the result of the training with some variables where
    the outliers can be seen. Download : Download high-res image (233KB) Download
    : Download full-size image Fig. 4. Result of the detection on the snr column in
    relation to time. Download : Download high-res image (244KB) Download : Download
    full-size image Fig. 5. Detection result on the uplink_message_session_key_id
    column in relation to time. Download : Download high-res image (208KB) Download
    : Download full-size image Fig. 6. Detection result on the channel_index column
    in relation to time. Download : Download high-res image (180KB) Download : Download
    full-size image Fig. 7. Result of the detection on the uplink_message_settings_data_rate.lora.spreading_factor
    column in relation to time. Download : Download high-res image (185KB) Download
    : Download full-size image Fig. 8. Result of the detection on the uplink_message.consumed_airtime
    column in relation to time. Download : Download high-res image (188KB) Download
    : Download full-size image Fig. 9. Result of the detection on the uplink_message.decoded_payload.bytes
    column in relation to time. 4.2. Analysis of results and evaluation The marking
    of outliers requires a subsequent analysis and evaluation, to validate whether
    or not errors are really being detected. To this end, a sampling has been carried
    out on the set of outliers, selecting several dozens to analyse them in detail
    and find out the causes of their cataloguing as outliers, and therefore anomalous
    value within the packet flow. The detailed analysis has identified that most of
    the anomalous packets are due to three types of malfunctions. 4.2.1. Drop in signal
    strength One of the first anomalies detected by the system was a sudden change
    in the signal strength of the sensors. This issue in a device that is stable in
    its power was unusual, so the system flagged an anomaly at that instant, and after
    analysis of this detection, it was possible to locate that there had been a change
    in the position of the antenna that caused a shielding of the signal. Fig. 10
    shows an instant in which this sudden change in signal power detected by the algorithm
    can be seen. Download : Download high-res image (197KB) Download : Download full-size
    image Fig. 10. Sudden change of signal power. 4.2.2. Detection of unusual gateways
    Another anomaly detected by the algorithm is the appearance of gateways that do
    not belong to the organisation''s infrastructure. This can occur because the Lora
    technology used has a large range and by using The Things Networks as the base
    application, there are more open-source antennas in the project nearby Fig. 11.
    Being something out of the ordinary, two possibilities are established, the probe
    has changed location and with it the gateways around it, or a new gateway has
    been activated within its range, without being one of the ones they have configured
    from the Smart University group. After analysing the packet following the detection
    of the anomaly, using the TTN Mapper tool [38] it was concluded that one of the
    devices had not only been moved from a room, but also taken out of the university
    area, without anyone notifying of this fact. Download : Download high-res image
    (466KB) Download : Download full-size image Fig. 11. Location of the gateway detected
    from TTN Mapper. 4.2.3. Receiving messages without a data packet Finally, one
    of the most important anomalies detected is when a device stops sending data within
    the packet. This is the case when a sensor is not working properly, but the board
    to which it is connected to the sensor, together with its LoRa antenna, are in
    good condition, so that the packets arrive at the gateway, but there is no coded
    message inside. Fig. 12 shows an example of one of the devices that exhibited
    this behaviour. Download : Download high-res image (399KB) Download : Download
    full-size image Fig. 12. Sample of the data sending flow until its sudden cut-off.
    As can be seen in the image, from approximately 13:38, the sensor stopped emitting
    quantitative values, whether they were C02, GPS, or other sensors. In the IoT
    platform of the University of Alicante, the data could not be visualised, while
    the package had been processed. This implies that data is being received by the
    platform, but does not actually exist, which can lead to errors and contradictory
    actions in the IoT platform. 5. Conclusions Several interesting results have been
    generated through this project. Firstly, a model for anomaly detection based on
    Machine Learning on IoT infrastructure has been proposed. One of the most outstanding
    characteristics of IoT infrastructures is precisely not having control over them,
    using services and even open intermediate infrastructures (as in our case TTN).
    This means that we are faced with the ignorance of information, which in traditional
    network infrastructure is very useful for detecting anomalies, such as IPs, MACs
    or protocols. Our model aims to perform broad-spectrum detections, i.e., to detect
    packets that are not correct, remove them from the data ingestion flow in the
    applications that use the IoT, and then analyse them and extract the anomaly.
    An analysis of the characteristics of the IoT data packets has also been carried
    out, choosing the parameters to be considered for anomaly analysis after the correlation
    study, together with our own experience gained from working with the incoming
    packets on the TTN platform. This has allowed us to focus and narrow down the
    dimension in which these unexpected values can be detected, a factor that has
    helped to improve the quality of the anomalies detected and to optimise the training
    time. Based on the model developed, an implementation has been carried out on
    the campus of the University of Alicante, based on IoT infrastructure, The Things
    Network, which uses the open-source software The Thing Stack as a base, which
    is widely used by the IoT community. This instance of the algorithm has allowed
    to implement an anomaly detector on the real IoT platform of the University of
    Alicante, so that it has been possible to detect possible failures in the infrastructure
    and unexpected situations in terms of the behaviour of the sensors, has allowed
    to take measures in this regard, being an important support point for the overall
    monitoring of the project. In other words, the proposal is valid, it is useful,
    and it is up and running. And it can be used by the TTN community. The proposed
    work has some limitations. The most important one is the resources needed to train
    the algorithm, since it is necessary to perform the learning process with large
    volumes of data, including a large variety of packets, originating from the correct
    operation of the infrastructure to quickly detect anomalies. This affects us especially
    when there are permanent changes in the infrastructures since it is necessary
    to retrain the algorithm with this new, but unknown, configuration. If retraining
    is not performed, the new infrastructure elements would generate packets that
    would be flagged as anomalies by the ADS. Another limitation of the proposal is
    that the anomaly is not classified, which requires further analysis by the administrator
    after detection. As for possible improvements and future work, we want to implement
    different Machine Learning modules for anomaly detection and identification. First,
    we will look for the comparison of different algorithms that allow us to detect
    more situations of anomalies, to avoid that incorrect packets can \"sneak in\".
    We are even considering the possibility of having several algorithms processing
    in parallel and generating an aggregation of their results. This first step should
    serve to solve the problem of the resources needed for training. And, secondly,
    once we can discriminate packets and put them in a state of observation, other
    specialised algorithms can be used to generate a cataloguing that identifies the
    specific anomaly, such as: transmission failures, device subtraction, device malfunction
    or the implantation of unexpected devices (a very common problem in IoT, which
    appear to be sensors of unknown origin). Then, chaining anomaly detection systems
    with anomaly identification can help to have scalable, heterogeneous systems where
    there is uncertainty due to the lack of knowledge of uncontrolled intermediate
    IoT infrastructures. Being able to obtain a library of trained modules for different
    infrastructures would facilitate the construction of more efficient systems as
    it would only incorporate the necessary modules, and could even automate the system
    to search, amongst a possible collection of analysis and identification modules,
    those that best fit the infrastructure or circumstances. Declaration of competing
    interest The authors declare the following financial interests/personal relationships
    which may be considered as potential competing interests: Jose Vicente Berna Martinez
    reports financial support was provided by University of Alicante. Acknowledgement
    This project has been funded by the UAIND22–01B project \"Adaptive control of
    urban supply systems\" of the University of Alicante. Data availability The data
    that has been used is confidential. References [1] G. Eason, B. Noble, I.N. Sneddon
    On certain integrals of Lipschitz-Hankel type involving products of Bessel functions
    Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Sci., 247 (935) (1955), pp. 529-551
    Google Scholar [2] T.H. Nasution, M.A. Muchtar, A. Simon Designing an IoT-based
    air quality monitoring system Proceedings of the IOP Conference Series: Materials
    Science and Engineering, 648, IOP Publishing (2019), Article 012037 CrossRefView
    in ScopusGoogle Scholar [3] R. Minerva, G.M. Lee, N. Crespi Digital twin in the
    IoT context: a survey on technical features, scenarios, and architectural models
    Proc. IEEE, 108 (10) (2020), pp. 1785-1824 CrossRefView in ScopusGoogle Scholar
    [4] J.J. Davis, A.J. Clark Data preprocessing for anomaly based network intrusion
    detection: a review Comput. Secur., 30 (6–7) (2011), pp. 353-375 View PDFView
    articleView in ScopusGoogle Scholar [5] T.J. Veasey, S.J. Dodson Anomaly detection
    in application performance monitoring data Int. J. Mach. Learn. Comput., 4 (2)
    (2014), p. 120 CrossRefGoogle Scholar [6] K. Xu, Y. Qu, K. Yang A tutorial on
    the internet of things: from a heterogeneous network integration perspective IEEE
    Netw., 30 (2) (2016), pp. 102-108 View in ScopusGoogle Scholar [7] Blenn, N.,
    & Kuipers, F. (2017). LoRaWAN in the wild: measurements from the things network.
    arXiv preprint arXiv:1706.03086. Google Scholar [8] Z. Ahmad, A. Shahid Khan,
    C. Wai Shiang, J. Abdullah, F. Ahmad Network intrusion detection system: a systematic
    study of machine learning and deep learning approaches Trans. Emerg. Telecommun.
    Technol., 32 (1) (2021), p. e4150 View in ScopusGoogle Scholar [9] Fernández Oliva,
    A., Maciá Pérez, F., Berna-Martinez, J.V., & Abreu Ortega, M. A Meth-od non-deterministic
    and computationally viable for detecting outliers in large datasets. (2020). Google
    Scholar [10] T. Gu, A. Abhishek, H. Fu, H. Zhang, D. Basu, P. Mohapatra Towards
    learning-automation IoT attack detection through reinforcement learning Proceedings
    of the 2020 IEEE 21st International Symposium on\" A World of Wireless, Mobile
    and Multimedia Networks\"(WoWMoM), IEEE (2020), pp. 88-97 CrossRefView in ScopusGoogle
    Scholar [11] C. Perera, A. Zaslavsky, P. Christen, D. Georgakopoulos Context aware
    computing for the internet of things: a survey IEEE Commun. Surv. Tutor., 16 (1)
    (2014), pp. 414-454 View in ScopusGoogle Scholar [12] M.M. Hasan, M.M. Islam,
    I.I. Zarif, M. Hashem Attack and anomaly detection in IoT sensors in IoT sites
    using machine learning approaches Internet Things, 7 (2019), Article 100059, 10.1016/j.iot.2019.100059
    View PDFView articleView in ScopusGoogle Scholar [13] A. Ukil, S. Bandyoapdhyay,
    C. Puri, A. Pal IoT healthcare analytics: the importance of anomaly detection
    Proceedings of the 2016 IEEE 30th International Conference on Advanced Information
    Networking and Applications (AINA), IEEE (2016), pp. 994-997 View in ScopusGoogle
    Scholar [14] A. Chatterjee, B.S. Ahmed IoT anomaly detection methods and applications:
    a survey Internet Things, 19 (2022), Article 100568 View PDFView articleView in
    ScopusGoogle Scholar [15] H.F. Nweke, Y.W. Teh, M.A. Al-Garadi, U.R. Alo Deep
    learning algorithms for human activity recognition using mobile and wearable sensor
    networks: state of the art and research challenges Expert. Syst. Appl., 105 (2018),
    pp. 233-261 View PDFView articleView in ScopusGoogle Scholar [16] L. Erhan, M.
    Ndubuaku, M. Di Mauro, W. Song, M. Chen, G. Fortino, A. Liotta Smart anomaly detection
    in sensor systems: a multi-perspective review Inf. Fusion, 67 (2021), pp. 64-79
    View PDFView articleView in ScopusGoogle Scholar [17] J.E. Albuquerque Filho,
    L.C. Brandão, B.J. Fernandes, A.M Maciel A Review of Neural Networks For Anomaly
    Detection IEEE Access (2022) Google Scholar [18] M. Hosseinzadeh, A.M. Rahmani,
    B. Vo, M. Bidaki, M. Masdari, M. Zangakani Improving security using SVM-based
    anomaly detection: issues and challenges Soft Comput., 25 (2021), pp. 3195-3223
    CrossRefView in ScopusGoogle Scholar [19] R. Primartha, B.A. Tama Anomaly detection
    using random forest: a performance revisited Proceedings of the 2017 International
    Conference on Data and Software Engineering (ICoDSE), IEEE (2017), pp. 1-6 View
    in ScopusGoogle Scholar [20] V. Chandola, A. Banerjee, V. Kumar Anomaly detection:
    a survey ACM Comput. Surv. (CSUR), 41 (3) (2009), pp. 1-58 CrossRefGoogle Scholar
    [21] I. Cid, L.R. Janeiro, J.R. Méndez, D. Glez-Peña, F. Fdez-Riverola The impact
    of noise in spam filtering: a case study Advances in Data Mining. Medical Applications,
    E-Commerce, Marketing, and Theoretical Aspects: 8th Industrial Conference, ICDM
    2008 Leipzig, Germany, July 16-18, 2008, Proceedings, 8, Springer Berlin Heidelberg
    (2008), pp. 228-241 CrossRefView in ScopusGoogle Scholar [22] O.Z. Maimon, L.
    Rokach Data Mining With Decision Trees: Theory and Applications, 81, World scientific
    (2014) Google Scholar [23] V. Vundavalli, F. Barsha, M. Masum, H. Shahriar, H.
    Haddad Malicious URL detection using supervised machine learning techniques Proceedings
    of the 13th International Conference on Security of Information and Networks (2020),
    pp. 1-6 CrossRefGoogle Scholar [24] B. Janet, R.J.A. Kumar Malicious URL detection:
    a comparative study Proceedings of the 2021 International Conference on Artificial
    Intelligence and Smart Systems (ICAIS), IEEE (2021), pp. 1147-1151 Google Scholar
    [25] M. Douiba, S. Benkirane, A. Guezzaz, M. Azrour An improved anomaly detection
    model for IoT security using decision tree and gradient boosting J. Supercomput.,
    79 (3) (2023), pp. 3392-3411 CrossRefView in ScopusGoogle Scholar [26] J. Lesouple,
    C. Baudoin, M. Spigai, J.Y. Tourneret Generalized isolation forest for anomaly
    detection Pattern Recognit. Lett., 149 (2021), pp. 109-119 View PDFView articleGoogle
    Scholar [27] F.T. Liu, K.M. Ting, Z.H. Zhou Isolation forest Proceedings of the
    2008 8th IEEE International Conference on Data Mining, IEEE (2008), pp. 413-422
    View in ScopusGoogle Scholar [28] S. Dhankhad, E. Mohammed, B. Far Supervised
    machine learning algorithms for credit card fraudulent transaction detection:
    a comparative study Proceedings of the 2018 IEEE International Conference on Information
    Reuse and Integration (IRI), IEEE (2018), pp. 122-125 CrossRefView in ScopusGoogle
    Scholar [29] DS2OS traffic traces, Kaggle. https://www.kaggle.com/francoisxa/ds2ostraffictraces.
    Accessed September 2023. Google Scholar [30] The Things Industries. Platform For
    LoraWAN Networks Server. www.thethingsindustries.com. Accessed September 2023.
    Google Scholar [31] I. Jebli, F.Z. Belouadha, M.I. Kabbaj, A. Tilioua Prediction
    of solar energy guided by Pearson correlation using machine learning Energy, 224
    (2021), Article 120109 View PDFView articleView in ScopusGoogle Scholar [32] M.
    Mohy-eddine, A. Guezzaz, S. Benkirane, M. Azrour An effective intrusion detection
    approach based on ensemble learning for IIoT edge computing J. Comput. Virol.
    Hacking Techn. (2022), pp. 1-13 Google Scholar [33] D.M. Hawkins Identification
    of Outliers, 11, Chapman and Hall, London (1980) Google Scholar [34] A. Mensi,
    M. Bicego A novel anomaly score for isolation forests Image Analysis and Processing–ICIAP
    2019: 20th International Conference, Trento, Italy, September 9–13, 2019, Proceedings,
    Part I, 20, Springer International Publishing (2019), pp. 152-163 CrossRefView
    in ScopusGoogle Scholar [35] NumPy. Package For Scientific Computing With Python.
    https://numpy.org/. Accessed September 2023. Google Scholar [36] Pandas. Open
    Source Data Analysis and Manipulation Tool. https://pandas.pydata.org/. Accessed
    September 2023. Google Scholar [37] F. Pedregosa, G. Varoquaux, A. Gramfort, V.
    Michel, B. Thirion, O. Grisel, É. Duchesnay Scikit-learn: machine learning in
    Python J. Mach. Learn. Res., 12 (2011), pp. 2825-2830 Google Scholar [38] TTN
    Mapper. Tool to Map Coverage of The Things Networks. https://ttnmapper.org/. Accessed
    September 2023. Google Scholar Cited by (0) © 2024 The Authors. Published by Elsevier
    B.V. Recommended articles Detecting cyberthreats in Metaverse learning platforms
    using an explainable DNN Internet of Things, Volume 25, 2024, Article 101046 Ebuka
    Chinaechetam Nkoro, …, Dong-Seong Kim View PDF Are perfect transcripts necessary
    when we analyze classroom dialogue using AIoT? Internet of Things, Volume 25,
    2024, Article 101105 Deliang Wang, Gaowei Chen View PDF Advancing 6G-IoT networks:
    Willow catkin packet transmission scheduling with AI and bayesian game-theoretic
    approach-based resource allocation. Internet of Things, Volume 25, 2024, Article
    101119 Ali. M. A. Ibrahim, …, Wail M. Idress View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 825821490 Captures Readers: 8 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Internet of Things (Netherlands)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Anomaly detection system for data quality assurance in IoT infrastructures
    based on machine learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ahmed M.M.
  - Hassanien E.E.
  - Hassanien A.E.
  citation_count: '0'
  description: Poultry farming is crucial to feeding the world's growing population.
    Birds' abnormal behaviour can harm the birds, and disease detection relies on
    poultry behaviour. Integrating Internet of Things (IoT) technology into poultry
    farming can revolutionize the way to monitor and manage poultry health. Feeding,
    preening, and dustbathing are poultry's daily routines. In response to the problem
    of detecting correct poultry behaviour and health status, this paper proposes
    a smart poultry monitoring system that leverages IoT sensors to detect and monitor
    chicken behaviour in poultry farms and provides valuable information to industry
    stakeholders for management decisions and individual poultry health status. The
    phases of the proposed system are data preprocessing, feature extraction, feature
    selection, and detection of poultry behaviour via different classification algorithms.
    An optimized synthetic minority over-sampling technique (SMOTE) via an artificial
    hummingbird algorithm (AHA) is applied to solve the data imbalance problem. The
    experimental results show that an optimized SMOTE obtains better accuracy with
    97 % than other algorithms. Further, to attain accuracy in predicting poultry
    behaviours, Random Forest (RF) achieves superiority compared to other machine
    learning algorithms with an accuracy of 98 %.
  doi: 10.1016/j.iot.2023.101010
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Literature review 3. Preliminaries
    4. The proposed smart poultry monitoring system (SPMS) 5. Performance evaluation
    measures 6. Experiments results and discussion 7. Conclusion and future work Ethics
    approval Consent to participate Consent to publish Funding CRediT authorship contribution
    statement Declaration of Competing Interest Data availability References Show
    full outline Figures (24) Show 18 more figures Tables (11) Table 1 Table 2 Table
    3 Table 4 Table 5 Table 6 Show all tables Internet of Things Volume 25, April
    2024, 101010 A smart IoT-based monitoring system in poultry farms using chicken
    behavioural analysis Author links open overlay panel Mohammed Mostafa Ahmed a
    d, Ehab Ezat Hassanien b, Aboul Ella Hassanien c d Show more Share Cite https://doi.org/10.1016/j.iot.2023.101010
    Get rights and content Abstract Poultry farming is crucial to feeding the world''s
    growing population. Birds'' abnormal behaviour can harm the birds, and disease
    detection relies on poultry behaviour. Integrating Internet of Things (IoT) technology
    into poultry farming can revolutionize the way to monitor and manage poultry health.
    Feeding, preening, and dustbathing are poultry''s daily routines. In response
    to the problem of detecting correct poultry behaviour and health status, this
    paper proposes a smart poultry monitoring system that leverages IoT sensors to
    detect and monitor chicken behaviour in poultry farms and provides valuable information
    to industry stakeholders for management decisions and individual poultry health
    status. The phases of the proposed system are data preprocessing, feature extraction,
    feature selection, and detection of poultry behaviour via different classification
    algorithms. An optimized synthetic minority over-sampling technique (SMOTE) via
    an artificial hummingbird algorithm (AHA) is applied to solve the data imbalance
    problem. The experimental results show that an optimized SMOTE obtains better
    accuracy with 97 % than other algorithms. Further, to attain accuracy in predicting
    poultry behaviours, Random Forest (RF) achieves superiority compared to other
    machine learning algorithms with an accuracy of 98 %. Previous article in issue
    Next article in issue Keywords Internet of ThingsPoultry behaviourClassificationImbalanced
    datasetHealth of poultryArtificial hummingbird optimization algorithm 1. Introduction
    Nowadays, some diseases are particularly important because of the significant
    economic liabilities that aggravate poultry production. But there is also a fair
    amount of risk involved in the chicken farming industry [1], as shown by the sheer
    number of chicken farming companies that have failed because of various difficulties
    [2]. Precision animal and poultry farming relies on the monitoring of animal data
    and a reliable decision-making aid. Numerous things, including financial difficulties,
    illnesses, poor livestock management, and other things, can cause this phenomenon.
    In the chicken farming industry, problems are often caused by mistakes and a lack
    of care in how chickens are managed and cared for. Chickens are an important part
    of achieving the worldwide need for low-fat, high-protein foods. For centuries,
    the poultry industry has been trying to meet the growing demand for chicken and
    eggs. Consumers'' worries about the chicken they buy are on the rise, along with
    the industry''s response to rising demand. When it comes to the health and safety
    of the consumer and the quality of poultry products, animal welfare is of paramount
    importance. The health of poultry products is always enhanced by good animal welfare
    because it decreases the prevalence of disease in poultry chickens. Chickens are
    notorious for spreading disease among themselves and across entire farms, which
    has led to significant economic losses in the poultry sector [3]. Reducing losses
    and slowing the spread of disease in poultry flocks through early detection of
    sick birds by labelling or classifying individuals based on their behaviour. Poultry
    farming has attracted an increasing number of researchers in recent years [4].
    Using the Internet of Things (IoT) and analysis of images, as well as other technologies
    that assist farmers in poultry health administration and surveillance to track
    and maintain manufacturing information in real-time, other researchers create
    online platforms and employ smart sensors [5]. The process of extracting information
    entails analysing the statistical properties of the raw data before defining an
    extensive hierarchy of the features extracted for upcoming analyses of novel/unknown
    instances. But things like the shape and size of the data, the loudness and unstructured
    data, attribute values loss, imbalanced data, and redundant data tend to make
    it harder for data mining processes to find patterns that make sense. By leveraging
    interconnected devices and sensors, IoT enables real-time monitoring of animal
    behaviors, offering insights into movement, interactions, and health status [6]
    employed IoT sensors to track pet cat behaviors, revolutionizing our understanding
    of their habits and ecological dynamics. Despite the numerous methods available,
    data can be classified in various ways and there are problems that affect its
    accuracy and efficiency. One of the most important problems that affect classification
    is the imbalance of data, which consequently affects the extraction of accurate
    features. Imbalanced datasets are characterized by a significant disparity in
    the number of samples among different classes [7]. The performance of machine
    learning (ML) classifiers suffers as a result of this difficult problem of uneven
    class distribution. As a result, a vast literature has focused on enhance the
    performance of machine learning classifiers to handle imbalanced datasets [8].
    Other factors besides class imbalance should be considered when classifying imbalanced
    data, such as sample size and class overlap. There are multiple approaches for
    oversampling imbalanced datasets that increase the minority class into the majority
    class, such as the Synthetic Minority Oversampling Technique (SMOTE) [9]. SMOTE
    uses interpolation to create additional minority class examples in regions close
    to the existing ones. In spite of its potential to increase minority class accuracy,
    this method often introduces noisy instances and overfitting issues because it
    ignores the distribution of neighbouring samples. Sampling techniques are required
    for processing imbalanced datasets. There are three main methods for balancing
    the distribution of negative and positive classes; they are positive instances,
    negative instances, and combination sampling between positive and negative instances.
    Data mining techniques [10] can be applied to the series data that is extracted
    from these IoT sensors to gain insight into the behaviour of animals. There is
    frequently a significant amount of imbalance when a decision-making process is
    applied to isolate a special circumstance. The issue of imbalance has received
    more focus in recent years. Many practical contexts feature imbalanced data sets,
    including identifying dishonest phone users, spotting oil spills in satellite
    radar images, teaching word pronunciation, categorizing texts, spotting fake phone
    calls, and carrying out information retrieval and filtering tasks. The classification
    of imbalanced data has attracted a lot of attention. Already in the review [11],
    over 500 papers were gathered and explored in terms of applied methods and actual
    outcomes on data sets. Most publications, highly specialized workshops, and conferences
    have increased dramatically since then. There is ongoing research into better
    ways to categorise imbalanced data. There are a few ways to handle imbalanced
    datasets, including under-sampling the data in the majority class, over-sampling
    the minority class, or a hybrid of the two. The main contributions of this paper
    are summarized as follows: • Design a smart poultry monitoring system that detects
    and monitors the behaviours of the chickens in poultry farms. • Handling an imbalanced
    dataset using an optimized SMOTE via the artificial humming optimization algorithm
    • Prediction of the poultry''s behaviours via different machine learning techniques.
    • Detection of poultry disease status via the classification of sick and healthy
    chickens on a poultry farm according to poultry behaviours. • Selection of the
    best features from extracted new features using feature selection-based optimization.
    • Provides a performance validation comparison between various algorithms. The
    paper is organized as follows. Section 2 discusses other studies that are related
    to this paper. The preliminaries, like the essential concepts for SMOTE and the
    standard optimization algorithm for the artificial hummingbird, are provided in
    Section 3. Section 4 describes the details of the proposed poultry system. The
    measures of performance evaluation are discussed in Section 5. The experiment
    results are discussed in Section 6. Section 7 discusses the paper''s conclusion
    as well as future work. 2. Literature review Traditional classification algorithms
    suppose a balanced training dataset, but they struggle to identify minority classes.
    Traditional classification algorithms perform poorly on class-imbalanced datasets.
    On the other hand, since the uncommon classes typically represent intriguing concepts,
    it is typically most crucial to identify the minority classes. Furthermore, gathering
    these minority class examples can be costly or difficult. The uneven distribution
    in the datasets is what gives rise to the issue of class imbalance. As a result,
    it makes sense to think about rebalancing through the data space sampling to lessen
    the impact of class imbalance. One benefit of such a solution is that it can be
    used regardless of the classifier, so it can also be used as a pre-sampling technique.
    Under-sampling and over-sampling are the two fundamental concepts that are at
    the core of resampling methods, which are used to achieve a more equitable class
    distribution. The terms \"random under-sampling\" (RUS) and \"random over-sampling\"
    (ROS) refer to the elimination of representative samples from the majority classes
    and the systematic duplication of examples from the minority classes, respectively.
    The most prominent over-sampling method is SMOTE [12], which has garnered a lot
    of interest in recent years. A hybrid method was proposed for pre-processing imbalanced
    datasets by [13] for generating new samples using SMOTE and rough set theory.
    They noticed excellent average results from the experimental findings. Similarly,
    [14] introduced the Majority Weighted Minority Oversampling Technique (MWMOTE)
    to address issues with unbalanced learning. The method first identifies samples
    from the minority class that are challenging to learn and gives them weights based
    on their Euclidean distance from the closest majority samples. The SMOTE algorithm
    was utilized by [15] to resample the data, which they paired with a collection
    of extreme learning machines, a feature selection ensemble, and a decision tree
    (DT) to develop an original mechanism for forecasting the future to predict the
    two-class imbalance. A new semi-supervised adversarial generative adversarial
    network with spectral normalisation was proposed by [16]. (SN-SSCGAN). The idea
    behind it is to use minority fault samples that have been partially labelled to
    rebalance the dataset, it is necessary to produce additional samples with a comparable
    spread. To obtain a time-frequency matrix and to address these issues, a pre-process
    such as smooth the vibration signal using the wavelet transform is implemented,
    second, adversarial training is used to achieve Nash equilibrium on the partially
    labelled time-frequency fault data before generating data with a similar distribution.
    Random under-sampling (RUS) and the AdaBoost algorithm are two sampling techniques
    that have been used extensively in research that builds on ensemble learning [17].
    Because of the unpredictability of the random sampling technique, instances may
    occasionally not be representative, making it difficult to see how the model can
    be improved. [18] proposed the SMOTE Boost algorithm for building a balanced and
    high-quality dataset. In [19], authors use Internet of Things (IoT) sensors to
    gather information about waste features like waste bin size, waste size, and smell
    in the bin to alert truck drivers, waste management, and authorities. In addition
    to solving the problem of missing data and choose best optimal path for waste
    truck from disposal center to suitable waste bins in many different places in
    the city have embedded devices. An intelligent method was proposed by [20] who
    implemented it to detect and classify chickens based on their vocalization and
    using fisher discriminate analysis (FDA) using signals detection to sort healthy
    chickens from sick ones. [21] Detect method for avian influenza according to the
    sound (noise) analysis of poultry via SVM. By examining the poultry chickens''
    feces, it is possible to keep track of their behaviour and make an early infection
    diagnosis [22]. A managed lighting environment and an IR camera have been used
    to determine the number of chickens in poultry [23]. [24] proposed an IoT platform
    that allows for the real-time analysis of each hen''s egg production, enabling
    the substitute of chicken whose egg production falls below a predetermined level
    to achieve the overall target egg yield rate. The health, cleanliness, and growth
    of the poultry chickens were determined by tracking their [25]. One of the factors
    contributing to poor poultry welfare is lameness [26], and when lameness is detected
    early, farmers and veterinarians can take preventative measures. Table 1 summarizes
    the related work including the data sets type, used techniques, the main contribution,
    and limitations. The majority of the existing literature focuses on predicting
    diseases in poultry, and other research deals with imbalanced databases. But it
    did not consider the imbalance of data when detecting behaviours and diseases
    in poultry. Most of the previous research largely lacked an effective strategy
    for boosting the real-world effectiveness of a model trained with an imbalanced
    dataset when detecting diseases in poultry. Table 1. The relevant literature summaries.
    Ref Dataset focus Techniques Contributions Limitations [12] Imbalanced dataset
    SMOTE • Solving imbalanced dataset Determining the k value of k-neighbors by randomly
    inside the SMOTE algorithm [13] Imbalanced dataset hybrid method • The construction
    of new samples for datasets Evaluation includes only synthetic instances falling
    within the minority class''s lower approximation will be used. [14] Imbalanced
    dataset MWMOTE • produces synthetic samples using weighted, enlightening minority
    class samples. The quantity of synthetic samples generated was negligible. [15]
    Imbalanced dataset SMOTE- ELM -DT • Re-size the imbalanced dataset. Dimensionality
    reduction can be performed after the dataset has been preprocessed. [16] Imbalanced
    dataset SN-SSCGAN • Creating new samples with the same distribution using minority
    fault samples, and enhancing the fault diagnosis model''s capacity for generalization
    Only use flow distribution data that can be found in the bearing deterioration
    process. [17] Imbalanced dataset RUSBoost • Examines RUSBoost and SMOTEBoost''s
    Effectiveness. Increasing training time and Complexity. [18] Imbalanced dataset
    SMOTEBoost • Enhancing the minority classes’ prediction. Need to increase the
    number of comparisons until it shows efficiency and needs automatic determination
    of the amount of SMOTE [19] Frequencies of chicken Peak The audio recording procedures
    • Monitoring the growth of chickens Because the humming sound vibrations are interconnected,
    the poultry industry cannot profit from it. [20] Analysis of vocal sound Fisher
    Discriminate Analysis (FDA) • Diagnosis of Disease It is challenging to implement
    in large poultry farms because vocal analysis is challenging due to the vocal
    vibration overlapping [21] Vibrations of Sound Support vector machines (SVM) •
    Detection of flu in poultry Determine the presence of flu in chickens at large
    poultry farms [22] Images of faces from chickens monitoring Campylobacter control
    • Diseases Detection and Monitoring Abnormal Feeding Small-scale study of abnormal
    feeding behaviour in chickens. [23] Chickens Images A lighting preference test
    system uses the weight method • Feeder Crowd Surveillance Keeping the lighting
    conditions ideal for viewing camera images. [24] Data extracted from sensors attached
    to the poultry body Smart nest box based on RFID • Controlling the production
    of eggs. Only allows for tracking the chickens inside and outside the nest. [25]
    Data extracted from sensors attached to the poultry body Automated position monitoring
    • Tacking of poultry''s whereabouts Only describes the tracking method. [26] Data
    extracted from sensors attached to the poultry body Decision tree (DT) • Prediction
    of Lameness for poultry through walking speed, acceleration, genetic strain, and
    sex. Need to compare with multiple other algorithms to prove that it is the best
    method 3. Preliminaries 3.1. SMOTE for balanced data Synthetic Minority Over-sampling
    Technique (SMOTE) [27] is regarded as an improved version of the randomized oversampling
    technique that increases the classifier''s validation dataset generalization capability
    and lowers the likelihood of overfitting. By adding a synthetic minority class
    to the original dataset between the minority class and its nearest neighbours
    using random linear interpolation, it focuses on enhancing the dataset''s imbalance.
    The imbalanced data issue is ameliorated by SMOTE. The specific concept is put
    into practice as follows: Identity data set X''s k closest neighbours in the minor
    class, choose n samples at random, and record them as Xi. Finally, interpolation
    defines the new sample Xnew as defined in Eq. (1): (1) Where rand represents a
    random number uniformly distributed within the range of (0, 1). But there are
    some issues with the SMOTE algorithm Fig. 1. The choice of the value for k is
    unrelated to the selection of the nearest neighbors. Additionally, replicating
    the distribution of the original data is challenging due to potential repeatability
    issues and the presence of noisy, indistinct boundaries between the classes of
    positive and negative in the artificially generated samples from the minor class
    samples located at the periphery. Download : Download high-res image (224KB) Download
    : Download full-size image Fig. 1. Chart for generating synthetic data. 3.2. Artificial
    hummingbird optimization algorithm (AHA) A brand-new bio-inspired algorithm is
    introduced the Artificial Hummingbird Algorithm (AHA) has been proposed by [28]
    to address global optimization problems. The world''s smallest bird and most beautiful
    creature. To the extent that brain size correlates with overall intelligence,
    hummingbirds would rank at the top of the animals. Hummingbirds have a top speed
    45 km/h. While some of the smallest seeds can reach speeds of over 80 beats per
    second, the largest seeds can emerge in the air at a rate of about 12 beats per
    second. Hummingbirds must forage to maintain their metabolism, and nectar tends
    to make up a majority of their nutrition. AHA consists of three components as
    follows: ■ Sources of Food: the decision of hummingbird on which flower to visit
    next for food depends on several factors, including the quality and content of
    flower nectar, and the refill rate of the flower nectar, and the hummingbird''s
    previous frequency of visits to the flower. ■ Hummingbirds: Each species of hummingbird
    has its unique food source, and the hummingbird and its source of the food are
    always in the same relative location. When a hummingbird finds a reliable source
    of nectar, it will tell the other members of its population where to find it and
    how often it will be refilled. ■ Visit Table: For each food source, data on the
    visit level is kept, which indicates how long it has been since the same hummingbird
    last visited that particular source. Initialization phase: Hummingbirds in a population
    of n are distributed at random among n food sources using Eq. (2). (2) The upper
    and lower boundaries are denoted as Low and Up respectively, while xi ''s representation
    of the location of the ith food source. The visit table is illustrated by Eq.
    (3). (3) Foraging with a guide: Every hummingbird possesses an inherent inclination
    to visit the food source containing the highest volume of nectar. Consequently,
    a desirable food source should exhibit a high rate of nectar refilling and a longer
    duration between visits by a particular hummingbird. To facilitate guided foraging
    behaviour, a hummingbird is permitted to identify food sources based on their
    visit levels and subsequently choose the one with the highest nectar-refilling
    rate as its target source. Once identified, the hummingbird can then direct its
    flight towards the desired food source. Three types of flight maneuvers, namely
    omnidirectional, diagonal, and axial flights, are employed for foraging purposes,
    as outlined from Eqs. (4) to (6). (4) (5) (6) The function generates a random
    number ranging from 1 to d, while randperm(k) produces a random permutation of
    numbers ranging from 1 to k. Additionally, r1 represents a random number that
    falls within the range of 0–1. A. Foraging on the frontier: After a hummingbird
    finish consuming all the nectar from its target food source, it prefers to forage
    for food in new locations rather than revisit old ones. Consequently, Hummingbirds
    are able to move freely between different parts of their territory using Eqs.
    (7) & (8). (7) (8) The factor of territorial, denoted as b, has a normal distribution,
    which describes it by N(0,1). Where 0 is mentioned as the mean and 1 is the standard
    deviation. B. Foraging migration: If the food supply in a hummingbird''s regular
    feeding spot dwindles, it may move to a nearby flower bed, the hummingbird typically
    undertakes a migration to a farther food source for feeding purposes. Hummingbird
    migration behaviour can be modelled using Eq. (9) by first determining the source
    with the lowest nectar-refilling rate and then simulating the bird''s transition
    to a new source that is generated randomly. (9) xwor represents the food source
    within the population that possesses the poorest nectar-refilling rate. 4. The
    proposed smart poultry monitoring system (SPMS) The utilization of wearable sensing
    devices enables the precise observation of individual chickens, as they are tracked
    and monitored for a specific period of time. Rapid trends in sensing technology
    have reduced the cost of such devices, making sensor-driven data analysis a more
    viable option. More recent research has also helped to enhance data collection
    techniques for livestock and poultry farming by making use of wearable sensing
    devices [29]. This paper proposes a smart poultry monitoring system to classify
    poultry behaviours that affect the status of poultry (sick or healthy). Fig. 2
    depicts the overall architecture of the proposed system. Download : Download high-res
    image (575KB) Download : Download full-size image Fig. 2. The general architecture
    of the proposed system. 4.1. Dataset description phase The process of obtaining
    the desired input data from various resources is known as the dataset collection
    phase. Data size and data characteristics were all considered during the data
    collection process. By placing wearable accelerometers over individual chickens,
    the research [30,31] created a dataset of poultry. The data stores three-axis
    chicken data, which depicts the behaviours of poultry chickens like dustbathing,
    preening, and pecking. Both healthy and sick chickens were included in the dataset
    of these activities. The external parasites covering the chickens'' skin were
    purposefully embossed by the researchers, stressing the animals. The data generated
    from the sensor, which is attached to the chicken''s back to collect data, can
    be used to record a wide range of natural behaviours. The Activity AX3 sensor
    is a tri-axial accelerometer, measuring static and dynamic acceleration on three
    orthogonal axes (X, Y, and Z) and weighing only 11 g. The X, Y, and Z axes were
    all oriented dorsoventrally, laterally, and anteriorly-posteriorly, respectively.
    The sensor is set up with a 100 Hz sampling frequency and +/− 8 g sensitivity,
    allowing for two weeks of continuous data collection on a single charge. The following
    actions are hypothesised to be related to poultry health based on a survey of
    the available literature [32]. The 24 unique poultry chickens distributed among
    four different flocks make up the 20-week dataset. The dataset includes the daily
    averages for each chicken''s pecking, preening, and dustbathing in six different
    flocks. The available poultry behaviour dataset attributes are shown in Table
    2 which describes raw data X, Y, and Z axes of the accelerometer which describes
    the chicken behaviours cases. Table 2. Description of tri-axial accelerometer
    raw data. Raw data Description X-axis Is a horizontal axis that runs from left
    to right. Y-axis Is a vertical axis that runs from the bottom. Z-axis Is a third
    axis that runs perpendicular to the X-axis and Y-axis. Class Describes the behaviour
    (Peaking/Feeding, preening, and dustbathing) of poultry according to three axes.
    Table 3 describes the three labelled behaviour classes. Fig. 3 demonstrates pre-processing
    of raw data are created from the input labelled data (pecking, preening, and dustbathing).
    The labelled dataset was used to feed the model and then used to validate it.
    Table 3. Description of the three behaviour classes monitored. Class id Behaviour
    Description 2 Pecking The frequency for bringing the beak to the ground 3 Preening
    The frequency of preening of the feathers by the beak. 4 Dustbathing The frequency
    of bird which is in a sitting or lying position with feathers raised in a vertical
    wing-shake. 0 No Behaviour —— Download : Download high-res image (500KB) Download
    : Download full-size image Fig. 3. Visualization of tri-axial accelerometer data
    before preprocessing. 4.2. Data preprocessing phase The data from an accelerometer
    comprises readings from three float numeric axes (x, y, z). All the identified
    exercises are confined within the range of [−8, 8] for each axis. Subsequently,
    these readings were employed to identify three behaviours: pecking/feeding, preening,
    and dustbathing. The size of dataset is 176,154 records before data cleaning.
    The dataset size after cleaning and deleting all the duplicate data is 99,998
    records. To prepare the data for the classification process, it will go through
    several processes. Data cleaning (removing zeros values) and data duplication
    are the processes involved. According to the techniques employed during the classification
    model, the raw data is formatted into the necessary system. In essence, the decision
    class is included at the end of the entire column, and the data is presented using
    an r * c matrix, where r and c represent several rows and columns, respectively.
    The best advantage of pre-processing is that it helps to minimize data redundancy
    and improve overall dataset organization. To improve classification results, perform
    a feature selection task to extract a subset of pertinent features from the dataset.
    This task also helps eliminate the possibility of incorrect training by removing
    redundant features and noise. Fig. 4 demonstrates tri-axial accelerometer data
    after removing null values, zero classes, and duplicate records. Download : Download
    high-res image (693KB) Download : Download full-size image Fig. 4. Visualization
    of tri-axial accelerometer data after pre-processing. 4.3. Feature extraction
    phase A systematic evaluation was conducted to determine the usefulness and significant
    features for distinguishing between various activities by employing different
    classification models and ranking their importance. Classification models were
    used to identify the relative importance of the seventeen features. The process
    of feature extraction was carried out via MATLAB (2019a). There are 17 instant
    characteristics through 3D accelerations (X, Y, Z), Single Magnitude Area (SMA),
    Average Intensity (AI), Movement Variation (MV), Energy, and Entropy were calculated
    via formulas listed in Table 4, where i is the record index, were chosen according
    to the length of time each behaviour lasted. For instance, feeding/pecking typically
    take very little time, about one peck per second, when compared to the time it
    takes a broiler to preening, which is roughly 3 s for 4 steps. This resulted in
    a total number of features, all of which were incorporated into the model training
    process. Table 4. Formulations for motion characteristics of broilers utilizing
    triaxial acceleration data. Feature Formulas Acceleration of X axis Acceleration
    of Y axis Acceleration of Z axis Average X-axis (Ax) Average Y-axis (Ay) Average
    Z-axis (Az) Movement Variation (MV) MV =  Signal Magnitude Area (SMA) SMA = Average
    Intensity (AI) AI =  Where MI(t) = Entropy EN =   Where Energy E =   Where Maximum
    X (MaxX) The highest acceleration along the X-axis. Maximum Y (MaxY) The highest
    acceleration along the Y-axis Maximum Z (MaxZ) The highest acceleration along
    the Z-axis Minimum X (MinX) The lowest acceleration along the X-axis. Minimum
    Y (MinY) The lowest acceleration along the Y-axis Minimum Z (MinZ) The lowest
    acceleration along the Z-axis 4.4. Feature selection phase Various classification
    models were employed systematically to evaluate the utility of features and identify
    the most significant ones for distinguishing between different activities. In
    the field of data science, it is essential to determine the most relevant features
    within a dataset for training learning algorithms [33]. Some features may not
    contribute to improving the performance of the learning algorithm and can even
    degrade its accuracy. Consequently, feature selection (FS) algorithms are utilized
    to identify a subset of features that can enhance the machine learning model''s
    performance. FS algorithms also help prevent overfitting and expedite the training
    process of the learning algorithm. Furthermore, knowledge about the selected features
    can provide valuable insights into the datasets. 4.4.1. Fitness function The hummingbird
    swarm consists of individual birds, each representing a solution for feature selection.
    These birds are encoded using real numbers, as demonstrated in Eq. (10). Each
    solution, denoted as X, is comprised of n real numbers, n denotes the overall
    features of the dataset being analysed. The xi dimension of X corresponds to the
    decision of selecting or not selecting a particular feature. To form a subset
    of features, a decoding process must be performed on the bird''s position. This
    allows for the conversion of the bird''s position into a subset of selected features.
    (10) (11) The subset of features obtained after decoding the d-dimensionality
    of each solution is represented by Sd in Eq. (11). The value of xd in the d-dimension
    determines whether Sd is selected as 0 or 1. If xd corresponds to a selected feature,
    then Sd is set to 1. Conversely, if xd corresponds to a non-selected feature,
    then Sd is set to 0. Feature selection aims to identify an optimal combination
    of features that optimize the accuracy of classification while reducing the number
    of selected features. While it involves finding a suitable combination of features,
    the primary objective is to optimize the classification accuracy. The fitness
    function is designed to maximize the accuracy of classification for the test sets
    while maintaining the minimum number of selected features. Eq. (12) demonstrates
    the simultaneous consideration of these two aspects in the fitness function. (12)
    Where, constant value ∝, between 0 and 1, is utilized to control the relative
    significance of classification accuracy versus the number of selected features.
    As ∝ increases, classification accuracy is given greater weight. The accuracy
    of the classifier for bird i is represented by ACC(i), while the number of features
    selected by bird i is denoted as ObjSum(i). The total number of features in the
    dataset is represented by Total Features. 4.5. An optimized SMOTE for solving
    the data imbalance problem This stage is considered one of the most important
    stages that solves the data imbalance problem that significantly and tangibly
    affects the results related to poultry management. Incorporate (Artificial Humming
    Algorithm) AHA''s optimization process with DT and RF to prevent local convergence.
    Additionally, the proposed hybrid metaheuristic will serve as the foundation for
    an algorithm that optimizes hyperparameters to determine the optimum values of
    N and k for balancing class distribution of training data over parameter range.
    The values of the two most important parameters are optimized with the swarm algorithm
    in Fig. 5, which depicts the optimization of the SMOTE algorithm. Every iteration
    the search agents (such as hummingbird in AHA) move, the new approach is expected
    to locate the optimal combination of K and S through a classifier decision tree
    to produce the best Kappa and accuracy performance. Then, using iterative processing
    to determine the optimal K and S, compare the performance measures with the conditions
    to enhance the Kappa, the accuracy values, and the imbalanced ratio (min/maj).
    Download : Download high-res image (323KB) Download : Download full-size image
    Fig. 5. Flowchart of the proposed balancing dataset with optimized SMOTE. Each
    step interval for the S and K parameters in the computation is different. The
    proportion of the majority class to the minority class has the highest value for
    S. S must have a minimum value of 1 %. If there are two possible labels for the
    target class in the dataset, The majority class has 1000 instances, while the
    minority class has only 10. This implies that the minority class sample must grow
    by at least 20 times and up to 100,000 times, Kmin = 2, and Kmax = 10. As a result
    of the testing dataset''s consistency, Kappa [34] is regarded as a substitute
    measure of computing performance of classification. The classifier model''s reliability
    is represented by kappa. The accuracy is more credible because the Kappa value
    is higher. The range of Kappa values (also known as simply Kappa) is from −1 to
    1. While this is going on, three Kappa thresholds are used to determine the credibility
    of classification accuracy: The first measure in Kappa is considered to have strong
    consistency and high accuracy when Kappa is greater than or equal to 0.75. Second,
    if Kappa is greater than or equal to 0.4 and less than or equal to 0.75, the accuracy''s
    confidence level is generally considered to be high. The third threshold is that
    accuracy is astounding if Kappa is less than 0.4. When the observation in one
    class is higher than the observation in the other two classes, there is a class
    imbalance. This dataset is to detect poultry behaviours. As you can see in Fig.
    6, pecking behaviour is around 85.14 % when compared with preening and dustbathing
    at 14.31 % and 0.54 %, respectively. Therefore, we want the preening and dustbathing
    data to be balanced with the chicken pecking data so that the data can be analysed,
    and behaviour predicted efficiently. Download : Download high-res image (265KB)
    Download : Download full-size image Fig. 6. Imbalanced analysis dataset for poultry
    behaviours. 4.6. Dictionary for poultry behaviours To attempt the automatic construction
    of a dictionary behaviours or query-templates, our training/testing data as shown
    in Fig. 7. A dictionary''s list of query-templates (behaviours) is presented in
    Column1, Column2, …, Columnn; each query has a class (Column1.class), a threshold
    value is shown in Column2, and Column3 depicts an axis (X or Y or Z) property,
    in addition to the query data points. We need a threshold that defines our acceptable
    rate of false positives and false negatives for each class. Download : Download
    high-res image (657KB) Download : Download full-size image Fig. 7. Tri-axial accelerometer
    for chicken time series. In theory, a single behaviour could have multiple possible
    instantiations, just as the number seven has two different but equivalent written
    forms (the traditional \"7\" and the middle line \"7\"). A polymorphic dictionary
    describes this type of dictionary. From what we''ve seen in the chickens, we''ve
    deduced that each behaviour has only one correct execution. However, expanding
    this code to work with any polymorphic dictionary is so simple that it''s been
    left out for the sake of brevity. A dictionary of chicken behaviours can be constructed
    that searches annotated regions for motifs of highly conserved sequences [35].
    Think about the corresponding difficulty in the discrete string space to get a
    feel for this. We count, categories, and time-stamp certain behaviours as they
    occur in the time series data stream. The data stream is filtered using a sliding
    window. If the sequence meets the matching criteria for a query-template (behaviour)
    in the dictionary, it is associated with that behaviour and given a timestamp.
    The effectiveness of feeding/pecking, preening, and dustbathing behaviours are
    all assessed and reported. Out-of-sample evaluation is the only objective of the
    test dataset after the training dataset has been used to construct the dictionary
    of behaviours. Perhaps the most recognizable behaviour in chickens is the act
    of pecking/feeding. Dictionary has discovered the query-template in Fig. 8 to
    analyse pecking/feeding behaviour in a training dataset, where matching subsequences
    along the (left) X-axis and (right) Z-axis need to be identified, which incorporates
    subsequences that match those in the training dataset. Subsequence matches outside
    of regions annotated as feeding/pecking behaviour are considered false positives
    (FP), but those within these regions are considered true positives (TP). The query-template
    and corresponding subsequences from the training dataset for preening behaviour
    are shown in Fig. 9. Download : Download high-res image (326KB) Download : Download
    full-size image Fig. 8. A query template for pecking/feeding behaviour. Download
    : Download high-res image (241KB) Download : Download full-size image Fig. 9.
    A query template for preening behaviour. Fig. 10, Fig. 11 demonstrate how challenging
    it can be to detect dustbathing behaviour, as it was infrequent in both the training
    and test datasets. Only two occurrences of the green colour of dustbathing were
    present in the training dataset, and only one was found in the test dataset. These
    figures display the query template and matching subsequences regarding behaviour
    involving dustbathing in the test dataset, with the X-axis and Z-axis shown on
    the left and right, respectively. Download : Download high-res image (222KB) Download
    : Download full-size image Fig. 10. A query template for dustbathing behaviour.
    Download : Download high-res image (289KB) Download : Download full-size image
    Fig. 11. Matching sub-sequences for dustbathing behaviour. 4.7. Classification
    of poultry behaviours phase Behaviour, which is evident through actions and posture,
    is widely recognized as one of the most indicators that are crucial, most widely
    used, and most easily understood indicators of animal welfare and health, even
    more so than stress and output. Therefore, enhancing animal welfare and detecting
    sick chickens early necessitates real-time, automatic, and nondestructive monitoring
    of poultry behaviour. The following behaviours are hypothesized to be related
    to poultry health. When feeding or pecking, the bird lowers its beak to the ground
    and picks up a tidbit of food. Feather care routines that involve the beak include
    preening and dustbathing. Ectoparasite-infected birds, in particular, are thought
    to engage in increased preening/dustbathing. The various algorithms are used to
    categories pecking, preening, and dustbathing. Our analysis of the behaviour of
    broiler chickens was informed by the results of the preliminary behavioural posture
    pre-processing. Feeding/pecking, preening, and dustbathing are the three behaviours
    we assign to the test data. The chickens'' daily routine typically consists of
    them strolling around and chowing down. When prompted, such as by noise in the
    environment, they will take off running and only stop when they are fatigued.
    Additionally, there is a phenomenon where sick chickens lie down, and this behaviour
    warrants related study [36]. Preening is a common behaviour as well, accounting
    for about 11 % of a bird''s daily time. Chickens'' actions are assumed to be unrelated
    to one another; for instance, we don''t think chickens would run around the yard
    while munching on some food. In Section 6.1 we can see how the algorithm for classifying
    chicken behaviour recognition in this paper. The classification models of poultry
    behaviour were implemented via MATLAB (2019a) using Machine Learning Toolbox.
    Classification algorithms examined in this study are KNN, Decision Tree, Random
    Forest, Logistic Regression, Gaussian Naïve Bayes, and AdaBoost. 4.8. Diagnosis
    of poultry disease phase Seven prediction models were investigated for their ability
    to predict the poultry'' health. Table 3 provides a brief overview of the dataset
    that was used for this study. The dataset was chosen because it represents a comprehensive
    set of broiler behaviours, all of which are necessary for proper analysis: feeding/pecking,
    preening, and dustbathing. Real-world datasets were used to train the machine
    learning classification models. This paper proposes two techniques, AHA-DT and
    AHA-RF, and conducts experiments with supervised learning algorithms including
    k-Nearest Neighbor, Decision Tree, Random Forest, Naive Bayes, and Support Vector
    Machine to improve the classification of the poultry according to extracted data
    from the poultry dataset. Fig. 12 demonstrates the standard Decision Tree classifier,
    which contains X, Y, and Z that represent three axis values which predict poultry
    behaviours pecking, preening, and dustbathing, respectively. Download : Download
    high-res image (390KB) Download : Download full-size image Fig. 12. Standard decision
    tree for poultry dataset classification. 5. Performance evaluation measures The
    performance measures in the proposed system include multiple measures: measures
    for evaluating the balance of data and other measures for evaluating the best
    classification of the proposed methods. All measures are based on the following
    confusion matrix; refer to Fig. 13. When evaluating learning algorithms on a dataset
    quantitatively, the confusion matrix is a useful component. Important comparison
    parameters are computed using an empirical method. It also includes the crucial
    metrics defined by Eqs. (13)–(23): • True Positive (TP): indicates that the event
    was correctly predicted. • True Negative (TN): indicates that the system has correctly
    predicted that no event will occur. • False Positive (FP): represents a system''s
    incorrect prediction of an event. • False Negative (FN): represents a system''s
    incorrect prediction of no event. Download : Download high-res image (64KB) Download
    : Download full-size image Fig. 13. Confusion matrix. Accuracy: measures the proportion
    of samples that are correctly classified into all samples Eq. (13). (13) F-measure:
    It is defined as a metric for evaluating an algorithm''s effectiveness and is
    computed by combining the recall and precision metrics. Calculating F-measure
    using Eq. (14): (14) where the values of precision and recall are calculated using
    Eqs. (15) & (16), respectively. (15) (16) Area Under Curve (AUC): It is the plot
    that evaluates the classification model. The classification effect of the model
    is better with a higher AUC value. Classifiers with a value closer to 1 have faster
    convergence, while those with a value closer to 0 have sub-optimal performance
    Eq. (17). (17) G-value calculates using recall and precision average which Classification
    models perform better at classifying imbalanced data when the G-value is high
    Eq. (18). (18) OOBerror: is the average percentage of incorrect predictions made
    by each decision tree. where ntree is the number of decision trees. The more accurate
    a model''s classification is, the lower the acceptable OOBerror Eq. (19). (19)
    Kappa value: can also be called Cohen''s Kappa Eq. (20) which be determined by
    using a confusion matrix that lists the percentages of each class''s TP, FP, and
    TN which clarifies p0 indicates the overall accuracy and pc is a metric for evaluating
    the degree to which predicted and actual class values. (20) where and are given
    as respectively Eqs. (21) & (22) (21) (22) Imbalanced Ratio (IR): is used to calculate
    the percentage of imbalanced data value where the total number of points for the
    minority class denoted by while the total number of points for the majority class
    is denoted by . Whenever the percentage of imbalanced data is small, it indicates
    the efficiency and balance of the data Eq. (23). (23) The most important key metrics
    to track the changes and results for solving imbalanced datasets are accuracy,
    Kappa, and the imbalance ratio (Min/Maj). 6. Experiments results and discussion
    The findings of the experiments are discussed in this section along with the phases
    that were tested and the proposed system. The same computer contains the following
    specific settings: Hardware specification consists of Core (TM) i7-4500, 2.40
    GHz, 16 G, and 1 TB that express Processor, Frequency, RAM, and Hard Disk respectively.
    Software specification contains Windows 11, and MATLAB R2019a that express operating
    system and programming language, respectively to extract and analyse the results.
    6.1. Experiments of feature selection Assuming a population size of 100, the fitness
    function employs an α value of 0.8. The dataset undergoes 20 independent experiments,
    with a maximum of 500 iterations set as the limit. To test the classification
    accuracy of the selection scheme for each bird, the KNN classifier (with K = 7)
    is utilized. The hyperparameter configurations for each algorithm are provided
    in Table 5. The information of dataset is composed of 20 poultry features, more
    than 99,998 instances, and two classes which are described if the poultry is sick
    or healthy. Table 5. Hyperparameter settings. Algorithm Parameters AHA r [0, 1]
    WOA a = 2–0, a2 = −1 to −2 PSO w = 0.729, c1 = c2 = 1.49445 GA Crossover = 0.7,
    Mutation = 0.25 The experimental results of the AHA algorithm for feature selection
    and three other comparison algorithms on a dataset are presented in Table 5. AHA
    is the algorithm with the highest mean classification accuracy and the AHA algorithm
    outperformed the other algorithms on test datasets, and its mean accuracy is better
    than that of CSO, PSO, and GA algorithms in feature selection. The worst mean
    accuracy in feature selection was observed for the GA algorithm. However, the
    dataset with better mean accuracy on full features saw an improvement of less
    than 10 %. The experimental outcomes prove the AHA algorithm is more effective.
    Table 6 displays the average features and standard deviation in dimensions for
    the four algorithms following feature selection for the dataset. An intuitive
    observation shows that the AHA and CSO algorithms have a significant dimensionality
    reduction effect compared to the GA and PSO algorithms, and the dimensional standard
    deviation is low, indicating relatively high stability of the algorithms. The
    experimental results provide direct evidence that the AHA algorithm is highly
    effective, resulting in better classification accuracy on datasets and a significant
    reduction in the number of unnecessary features. Table 6. The mean, standard deviation
    (Std), and mean accuracy (MACC) after performing feature selection with various
    algorithms. Empty Cell AHA CSO PSO GA Dataset Mean Std MACC Mean Std MACC Mean
    Std MACC Mean Std MACC Poultry Dataset 5.85 0.65 0.982 5.85 0.65 0.921 6.00 1.34
    0.893 6.20 1.78 0.843 6.2. Experiments of handling imbalanced dataset The study''s
    experiments involve the utilization of both real-world and artificial datasets.
    An optimized SMOTE results in the creation of the synthetic dataset which is used
    to supplement the movement data of the poultry chickens and are a significant
    contribution to the proposed study for classifying poultry behaviours. The original
    dataset''s sparsity is provided by the proposed generated synthetic dataset SMOTE-AHA-DT
    and SMOTE-AHA-RF to suggest a predictive behaviour of poultry farms for instantaneous
    chicken disease diagnosis, the experiment developed machine learning classification
    models. The results of the proposed optimized SMOTE based on AHA using the Decision
    Tree (DT) and Random Forest (RF) classification models on the poultry dataset
    are shown in Table 7. The three-performance metrics of accuracy and Kappa with
    high accuracy and the best Kappa value. these best results indicate that, after
    undergoing training with the imbalanced data, the two proposed method with classification
    algorithms'' power is completely useless. Regardless of the classification algorithm
    employed, the results remain subpar after processing the original dataset using
    SMOTE-PSO, SMOTE-GOA, and SMOTE-WOA with DT and RF classification for some but
    perhaps not optimal balance. Using the effectiveness of the proposed an optimized
    SMOTE as SMOTE-AHA-DT, and SMOTE-AHA-RF methods proposed in this paper demonstrated
    that, as shown in Table 8 The Kappa for SMOTE-AHA-DT is high and nearly equal
    to one with high accuracy of 98 % more than proposed SMOTE-AHA-RF. The proposed
    methods managed to maintain a manageable range of control over classification
    results. The change in the dataset''s degree of imbalance ratio can be seen through
    the index of imbalance ratio, demonstrating that our methods work even when the
    dataset is not completely balanced. Table 7. The results of an optimized SMOTE.
    Algorithms Accuracy (%) Kappa Imbalance ratio (Min/Maj) % SMOTE 0.7814 0.20 0.4
    SMOTE-PSO-RF 0.7189 0.18 0.21 SMOTE-WOA-RF 0.8355 0.65 0.28 SMOTE-GOA-RF 0.8447
    0.05 0.33 Proposed optimized SMOTE-RF 0.9626 0.71 0.07 SMOTE-PSO-DT 0.8447 0.07
    0.31 SMOTE-WOA-DT 0.6175 0.14 0.22 SMOTE-GOA-DT 0.7603 0.52 0.19 Proposed optimized
    SMOTE-DT 0.9784 0.82 0.02 Table 8. The different performances AUC, OOBerror, G-mean
    and F-measure for different algorithms. Measures AUC OOBerror F-value G-value
    Raw Data 0.7273 0.24285 0.8024 0.8453 SMOTE 0.7814 0.17918 0.7351 0.9304 SMOTE-PSO-RF
    0.7189 0.19238 0.6594 0.9333 SMOTE-WOA-RF 0.8355 0.17764 0.5458 0.9367 SMOTE-GOA-RF
    0.8447 0.16444 0.6730 0.8814 Proposed optimized SMOTE-RF 0.9626 0.19018 0.9510
    0.9809 SMOTE-PSO-DT 0.8447 0.25385 0.7614 0.9672 SMOTE-WOA-DT 0.6175 0.19347 0.5137
    0.9676 SMOTE-GOA-DT 0.7603 0.23012 0.8241 0.9124 Proposed optimized SMOTE-DT 0.9784
    0.17544 0.9743 1 Afterward, the proposed Swarm-SMOTE optimization algorithm was
    compared with other swarm optimization algorithms SMOTE-PSO, SMOTE-WOA, and SMOTE-GOA
    with two classification models decision tree (DT) and random forest (RF) and used
    G-mean and F-measure for dataset to confirm the classification performance of
    the model. In Table 8 and Fig. 14, the experimental findings are displayed and
    show that the proposed optimization algorithm achieves superior results in the
    F-measure compared to other algorithms in the poultry dataset and that it also
    outperforms competing algorithms in terms of G-mean. The dataset is correctly
    classified for all of them. The experimental findings demonstrate that the algorithm
    presented in this paper is superior for the classification of the minority class
    and provides a practical solution to the issue of data classification that is
    imbalanced. Download : Download high-res image (275KB) Download : Download full-size
    image Fig. 14. Performances and rankings of proposed an optimized SMOTE with others
    under various k values. G-means, F-measures, AUC, and OOBerror. Especially the
    poultry dataset is included. Classification results for the poultry dataset after
    being expanded using the proposed optimization SMOTE show an improvement of 7.073
    % in accuracy and a decrease of 4.74 % in OOBerror value compared to the original
    data. Fig. 15 depicts data points for each class in the poultry dataset consisting
    of pecking/feeding, preening, and dustbathing classes, and pecking/feeding is
    considered the majority class compared to preening and dustbathing. Fig. 15(a)
    represents a visualization of input data points for preening and pecking before
    generating synthetic data points, and Fig. 15(b) also has the same visualization
    of input data points for dustbathing and pecking before producing synthetic data
    points. Fig. 15(c) and (d) demonstrate visualization of input data points for
    preening and dustbathing after generating synthetic data points via an optimized
    SMOTE. Download : Download high-res image (719KB) Download : Download full-size
    image Fig. 15. Visualization of data points (feeding/pecking, preening, and dustbathing)
    before and after generating synthetic data points via an optimized SMOTE. Table
    9 and Fig. 16 demonstrate the runtime for the poultry data set using all optimization
    synthetic oversampling techniques in seconds. The most time-consuming oversampling
    techniques, according to an analysis of the data in the table, were SMOTE-AHA
    and SMOTE-WOA with DT and RF. However, among all the oversampling techniques compared,
    SMOTE-PSO and SMOTE-GOA with DT and RF were the lowest. SMOTE-WOA-DT and SMOTE-WOA-RF
    are the methods that are most similar to our suggested method in terms of runtime,
    but they got twice as long to complete. Table 9. Runtimes of all optimizations
    synthetic oversampling techniques under several iterations. Algorithms \\ Iterations
    100 200 300 400 500 Overall SMOTE 0.2131 0.3421 0.3616 0.3421 0.3612 1.6201 SMOTE-PSO-RF
    0.2001 0.3349 0.2042 0.2301 0.2321 1.2014 SMOTE-WOA-RF 0.1914 0.1611 0.2421 0.2514
    0.1442 0.9902 SMOTE-GOA-RF 0.2154 0.1963 0.1443 0.3465 0.4251 1.3276 Proposed
    SMOTE-AHA-RF 0.2834 0.1204 0.1028 0.2156 0.2301 0.9523 SMOTE-PSO-DT 0.3824 0.3677
    0.2554 0.2214 0.2952 1.5221 SMOTE-WOA-DT 0.2231 0.2258 0.2364 0.1581 0.1287 0.9721
    SMOTE-GOA-DT 0.2109 0.3607 0.1025 0.1645 0.2174 1.056 Proposed SMOTE-AHA-DT 0.1926
    0.1862 0.2118 0.1265 0.1176 0.8347 Download : Download high-res image (265KB)
    Download : Download full-size image Fig. 16. Ranking of runtimes of all optimizations
    synthetic oversampling techniques. 6.3. Experiments of poultry behaviours prediction
    For each predicted chicken behaviour, we determined whether it was a true positive
    (TP), true negative (TN), false positive (FP), or false negative (FN) based on
    how closely it matched the actual behaviour. Percent AUC, accuracy, F-measure,
    precision, and recall were used to assess these models'' predictive efficiency.
    For three behaviour datasets, RF, KNN, and Adaboost models showed high accuracies
    in classifying poultry pecking (99 %, 98 %, and 96 %, respectively), preening
    (98 %, 98 %, and 95 %, respectively), and dustbathing (99 %, 98 %, and 97 %, respectively)
    and RF method is considered the best model in terms of accuracy. According to
    the research results, RF is the most effective classification model, with 90 %
    precision and recall when matching instances of the pecking behaviour. The second
    model, DT, matches instances of the pecking behaviour with a precision of 89 %
    and a recall of 90 %. Table 10 presents the results of the classification of pecking,
    preening, and dustbathing behaviours in the test dataset. According to the results
    of performance evaluation for each classification of poultry behaviours. ROC-AUC
    curve depicts in Fig. 17 according to the results of AUC for each classification
    models. Table 10. Different classification for chicken behaviours prediction.
    Classification Behaviour AUC Accuracy F-measure Precision Recall KNN Pecking 0.983
    0.887 0.885 0.886 0.887 Preening 0.980 0.899 0.897 0.898 0.899 Dustbathing 0.991
    0.876 0.874 0.875 0.876 Decision Tree Pecking 0.933 0.900 0.898 0.899 0.900 Preening
    0.948 0.912 0.910 0.911 0.912 Dustbathing 0.976 0.889 0.887 0.888 0.889 Random
    Forest Pecking 0.989 0.901 0.899 0.900 0.901 Preening 0.983 0.913 0.911 0.912
    0.913 Dustbathing 0.994 0.890 0.888 0.889 0.890 Logistic Regression Pecking 0.828
    0.855 0.853 0.854 0.855 Preening 0.890 0.867 0.865 0.866 0.867 Dustbathing 0.495
    0.844 0.842 0.843 0.844 Gaussain Naïve Bayes Pecking 0.768 0.807 0.806 0.807 0.807
    Preening 0.876 0.819 0.818 0.819 0.819 Dustbathing 0.861 0.796 0.795 0.796 0.796
    AdaBoost Pecking 0.959 0.894 0.893 0.893 0.894 Preening 0.940 0.906 0.905 0.905
    0.906 Dustbathing 0.916 0.883 0.882 0.882 0.883 Download : Download high-res image
    (382KB) Download : Download full-size image Fig. 17. ROC–AUC curve for each behaviour
    classification techniques. The frequent aggregation of preening and dustbathing
    activities. Along the axis Ay, there is a noticeable concentration of preening
    and dustbathing occurrences. Because the sensor is in constant motion as the poultry
    moves, pecking events are spread out over a wider area. Because of how the sensor
    was built and how it moved when attached to Ay''s back, the recorded motion was
    extremely useful for differentiating between activity intensities and was especially
    useful for identifying dustbathing behaviour. Ay was able to detect and record
    the movement of the sensor caused by the preening behaviour of turning the head
    to the side. Fig. 18 data depicts that pecking behaviour can be distinguished
    from preening and dustbathing through the use of MV. Download : Download high-res
    image (189KB) Download : Download full-size image Fig. 18. Extracting behavioural
    features from the chicken''s back accelerometer in a three-dimensional scatterplot,
    visualised via AI, Ay, and MV metrics. There was a clear separation of preening
    and preening events, as behavioural events for both activities occurred simultaneously
    Fig. 19. As a result, it becomes more difficult to tell these behaviours apart
    from others. Even though pecking events occur all over the place, the high prediction
    accuracy of this behaviour can be attributed to its isolation from other activities,
    as is especially evident along the AI axis. Consequently, the AI metric can tell
    the difference between pecking, preening, and dust-bathing behaviours. By fusing
    signals from all directions, AI is able to determine the magnitude of a pecking
    motion, with the greater acceleration associated with pecking (particularly in
    the Z-axis plane) being recorded and used to distinguish it from the other three
    behaviours. The low sensitivity of dustbathing behaviours reflects the lack of
    a consistent categorization for these occurrences. Download : Download high-res
    image (191KB) Download : Download full-size image Fig. 19. Extracting behavioural
    features from the chicken''s back accelerometer in a three-dimensional scatterplot,
    visualised via Az, AI and entropy metrics. It became obvious that pecking and
    dustbathing behaviours did not fall into two distinct categories in Fig. 20. When
    comparing the intensity of movement during locomotion and resting postures, the
    MV metric proved invaluable. Due to the minimal distinction between preening and
    dustbathing, these two activities were inaccurately classified in Fig. 20. Because
    the X-axis clearly changes orientation when dustbathing, Ax has become an important
    metric for distinguishing between dustbathing and upright behaviour since its
    advent. Download : Download high-res image (221KB) Download : Download full-size
    image Fig. 20. Extracting behavioural features from the chicken''s back accelerometer
    in a three-dimensional scatterplot, visualised via the Ax, SMA, and MV metrics.
    Fig. 21 shows the matching subsequence''s for running the pecking, preening, and
    dustbathing behaviour in the test dataset which indicates in Fig. 21(a), (b),
    and (c), respectively. True positives are represented by green dots, while false
    positives by red ones. Download : Download high-res image (371KB) Download : Download
    full-size image Fig. 21. Subsequence matching on the test set for feeding/pecking,
    preening, and dustbathing behaviours. 6.4. Experiments for the diagnosis of poultry
    disease As diseases cause significant economic losses in poultry production, they
    are of paramount importance. The costs associated with the disease can be reduced
    if it is detected and diagnosed quickly. We propose an algorithm for identifying
    sick chickens that can be used to reduce the spread of disease. We examine the
    poultry dataset [30] about chickens over the course of a full day. We know that
    on the first day, all of the chickens were healthy and that on the second day,
    some chickens were sick (infested with ectoparasites). For this reason, it is
    essential that the proposed classification results provide a sufficient degree
    of confidence in their ability to differentiate between healthy and sick chickens.
    Fig. 22 shows a chart depicting the frequency distribution of pecking behaviours
    across all of the chickens When observed, the subjects exhibited either a healthy
    state (indicated by the colour green) or a sick state (indicated by the colour
    red). The depicted figure demonstrates a certain degree of overlap between the
    peck counts of healthy and sick chickens. Download : Download high-res image (265KB)
    Download : Download full-size image Fig. 22. Histogram for the number of pecking
    behaviours distribution. The number of preening behaviours performed by each chicken,
    both when healthy (green) and when sick, is then analysed statistically (red).
    Based on the preening count alone, the proposed classifier accurately distinguishes
    between sick and healthy chickens, as evident from Fig. 23. The histogram of preening
    counts for healthy chickens exhibits a left-skewed distribution, whereas for sick
    chickens, it is concentrated more towards the middle and right. This observation
    underscores the reliable discriminatory ability of the algorithm in identifying
    the health status of chickens. Download : Download high-res image (242KB) Download
    : Download full-size image Fig. 23. Histogram for the number of peening behaviours
    distribution. In Fig. 24, we compare the frequency of dustbathing between chickens
    in good health (green) and those in sick health (red). Dustbathing is clearly
    more common on the left side of the histogram, which corresponds to healthy chickens.
    The dustbathing count for sick chickens which can be seen in the middle of the
    histogram. Overall, it appears that the pecking behaviour gives a sufficient degree
    of assurance in identifying healthy and sick chickens. Symptoms such as preening
    and dustbathing can be used as reliable indicators of whether or not a chicken
    is sick. Download : Download high-res image (230KB) Download : Download full-size
    image Fig. 24. Histogram for the number of dustbathing behaviours distribution.
    The evaluation of each supervised machine learning classification model''s performance
    involves using metrics like Precision, Recall, F1-Score, and Accuracy. The effectiveness
    of the chosen classification models is summarized in Table 11 along with their
    percentages. Table 11. Percentage measurements for different classification prediction
    of disease. Behaviours Classification models Accuracy Precision Recall F-score
    Pecking/Feeding Prediction K-Nearest Neighbor (KNN) 0.798 0.798 0.799 0.795 Random
    Forest (RF) 0.839 0.832 0.842 0.839 Decision Tree (DT) 0.827 0.824 0.825 0.824
    Support Vector Machine (SVM) 0.719 0.745 0.735 0.718 Logistic Regression (LG)
    0.805 0.807 0.808 0.803 Gaussian Naive Bayes (GNB) 0.826 0.831 0.827 0.823 Proposed
    AHA-DT 0.986 0.989 0.994 0.983 Proposed AHA-RF 0.941 0.893 0.871 0.876 Preening
    Prediction K-Nearest Neighbor (KNN) 0.773 0.773 0.774 0.77 Random Forest (RF)
    0.814 0.807 0.817 0.814 Decision Tree (DT) 0.802 0.799 0.8 0.799 Support Vector
    Machine (SVM) 0.694 0.72 0.71 0.693 Logistic Regression (LG) 0.780 0.782 0.783
    0.778 Gaussian Naive Bayes (GNB) 0.801 0.806 0.802 0.798 Proposed AHA-DT 0.961
    0.964 0.969 0.958 Proposed AHA-RF 0.916 0.868 0.846 0.851 Dustbathing Prediction
    K-Nearest Neighbor (KNN) 0.744 0.744 0.745 0.741 Random Forest (RF) 0.785 0.778
    0.788 0.785 Decision Tree (DT) 0.773 0.77 0.771 0.77 Support Vector Machine (SVM)
    0.665 0.691 0.681 0.664 Logistic Regression (LG) 0.751 0.753 0.754 0.749 Gaussian
    Naive Bayes (GNB) 0.772 0.777 0.773 0.769 Proposed AHA-DT 0.932 0.935 0.941 0.929
    Proposed AHA-RF 0.887 0.839 0.817 0.822 According to Table 11, which depicts different
    measures for each poultry behaviours prediction, the proposed classifier AHA-DT
    for pecking behaviour successfully distinguishes between sick as well as healthy
    poultry chicken in 98 % of the proposed study. On the real and made-up datasets
    for other classification models, when K = 7, the K-Nearest Neighbor (KNN) can
    accurately predict whether a chicken is healthy or sick by 79 %. Random Forest
    (RF) machine learning classifiers demonstrated well performance and yielded results
    with an accuracy of about 83 %, and Decision Tree also achieved a good accuracy
    of about 82 %. With an accuracy rate of 72 %, the SVM classification model performed
    the worst of all the techniques. With an accuracy rate of 80 %, logistic regression
    (LG) also achieves intermediate accuracy. With an accuracy of 80 %, Gaussian Naive
    Bayes classification outperformed KNN little. The accuracy of the AHA-DT is 98.63
    %, and the accuracy of the AHA-RF is 94.14 %. The AHA-DT method stands out with
    the most significant impact on accuracy. This approach exhibits notable benefits
    in terms of its strong generalization capability and robustness when addressing
    poultry prediction scenarios. According to results of accuracy for each classification
    model according to pecking/feeding, preening, and dustbathing behaviours, respectively.
    The proposed AHA-DT overcomes all other classification models until another proposed
    AHA-RF reveals the true power of evolutionary algorithms. Finally, the results
    compare with SVM, KNN, LG, GNB, DT, and RF classification algorithms. 7. Conclusion
    and future work The poultry industry needs an automatic system to improve the
    productivity of healthy birds, keep track of their health, and predict disease
    outbreaks before the disease occurs. This paper proposes the smart poultry monitoring
    system, empowered by IoT sensors, which effectively gauges the health status of
    chickens, whether sick or healthy, according to their behaviour. The objectives
    of the proposed system are: (1) selecting the best features from extracted features;
    (2) proposing an oversampling-based optimization to balance the data through the
    utilization of SMOTE through an optimization technique called the Artificial Hummingbird
    Algorithm (AHA); (3) predicting poultry behaviours that assist stakeholders in
    making decisions for the health of poultry; and (4) detection of poultry disease
    by behaviour analysis. The conclusion that has been extracted from the results
    of the classification of poultry behaviour is that RF achieved the best results
    and diagnosis of poultry disease by optimizing decision tree classification via
    AHA-DT, which is superior and outperforms other techniques with the highest level
    of classification accuracy of 97.6 %. In future work, we will extend SMOTE into
    multi-objective problem to deal with multi-classification imbalanced datasets
    due to the fact that in real-world applications, the issue of imbalanced datasets
    affects both binary and multi-classifications. Ethics approval The authors herewith
    do confirm that this manuscript has not been published elsewhere and is not also
    under consideration by the other journals. Consent to participate All the authors
    have seen and approved the content of the submitted manuscript. Consent to publish
    All the authors consent to publish the final manuscript. Funding No funding source
    has been utilized for conducting this empirical research. CRediT authorship contribution
    statement Mohammed Mostafa Ahmed: Conceptualization, Data curation, Formal analysis,
    Investigation, Software, Visualization, Writing – original draft, Writing – review
    & editing. Ehab Ezat Hassanien: Supervision, Investigation, Resources, Writing
    – review & editing. Aboul Ella Hassanien: Conceptualization, Data curation, Formal
    analysis, Investigation, Supervision, Validation, Resources, Writing – review
    & editing. Declaration of Competing Interest The authors declare no competing
    interests. Data availability Data will be made available on request. References
    [1] Jun Bao, Qiuju Xie Artificial intelligence in animal farming: a systematic
    literature review J. Clean. Prod., 331 (2022), Article 129956, 10.1016/j.jclepro.2021.129956
    View PDFView articleView in ScopusGoogle Scholar [2] H.M. Hafez, Y.A. Attia Challenges
    to the poultry industry: current perspectives and strategic future after the COVID-19
    outbreak Front. Vet. Sci., 7 (2020), p. 516, 10.3389/fvets.2020.00516 Aug 26PMID:
    33005639; PMCID: PMC7479178 View in ScopusGoogle Scholar [3] N. Ben Sassi, X.
    Averós, I. Estevez Technology and poultry welfare Animals, 6 (2016), Article 62,
    10.3390/ani6100062 Google Scholar [4] E. Rowe, M.S. Dawkins, S.G. Gebhardt-Henrich
    A systematic review of precision livestock farming in the poultry sector: is technology
    focussed on improving birdwelfare? Animals, 9 (2019), Article 614, 10.3390/ani9090614
    View in ScopusGoogle Scholar [5] M. Stefanova Precision poultry farming: software
    architecture framework and online zootechnical diary for monitoring and collaborating
    on hens’ health M. Bournaris (Ed.), Information and Communication Technologies
    in Modern Agricultural Development; Salampasis, Springer, Cham, Switzerland (2019),
    10.1007/978-3-030-12998-9_14 T., Eds Google Scholar [6] R.C. Chen, V.S. Saravanarajan,
    H.T. Hung Monitoring the behaviours of pet cat based on YOLO model and raspberry
    Pi Int. J. Appl. Sci. Eng., 18 (5) (2021), pp. 1-12 View in ScopusGoogle Scholar
    [7] V. López, A. Fernández, S. García, V. Palade, F. Herrera An insight into classification
    with imbalanced data: empirical results and current trends on using data intrinsic
    characteristics Inf. Sci., 250 (2013), pp. 113-141, 10.1016/j.ins.2013.07.007
    View PDFView articleView in ScopusGoogle Scholar [8] R. Cheng, L. Zhang, S. Wu,
    S. Xu, S. Gao, H. Yu Probability density machine: a new solution of class imbalance
    Learn. Sci. Program., 2021 (2021), pp. 1-14, 10.1155/2021/7555587 Google Scholar
    [9] A. Fernandez, S. Garcia, F. Herrera, N.V. Chawla SMOTE for learning from imbalanced
    data: progress and challenges, marking the 15-year anniversary J. Artif. Intell.
    Res., 61 (2018), pp. 863-905, 10.1613/jair.1.11192 View in ScopusGoogle Scholar
    [10] R. Giusti, D.F. Silva, G.E. Batista Improved time series classification with
    representation diversity and SVM 2016 15th IEEE International Conference on Machine
    Learning and Applications (ICMLA), IEEE (2016), pp. 1-6, 10.1109/ICMLA.2016.0010
    View in ScopusGoogle Scholar [11] H. Guo, Y. Li, J. Shang, G. Mingyun, H. Yuanyue,
    G. Bing Learning from class-imbalanced data: review of methods and applications
    Expert Syst. Appl., 73 (2017), pp. 220-239, 10.1016/j.eswa.2016.12.035 URL Google
    Scholar [12] N.V. Chawla, K.W. Bowyer, L.O. Hall, W.P. Kegelmeyer SMOTE: synthetic
    minority over-sampling technique J. Artif. Intell. Res., 16 (2002), pp. 321-357,
    10.1613/jair.953 View in ScopusGoogle Scholar [13] E. Ramentol, Y. Caballero,
    R. Bello, F. Herrera Smote-rsb*: a hybrid preprocessing approach based on oversampling
    and undersampling for high imbalanced data-sets using smote and rough sets theory
    Knowl. Inf. Syst., 33 (2012), pp. 245-265, 10.1007/s10115-011-0465-6 View in ScopusGoogle
    Scholar [14] S. Barua, M.M. Islam, X. Yao, K. Murase Mwmote–majority weighted
    minority oversampling technique for imbalanced data set learning IEEE Trans. Knowl.
    Data Eng., 26 (2014), pp. 405-425, 10.1109/TKDE.2012.232 View in ScopusGoogle
    Scholar [15] S.J. Lin, C. Chang, M.F. Hsu Multiple extreme learning machines for
    a two-class imbalance corporate life cycle prediction Knowl.-Based Syst., 39 (3)
    (2013), pp. 214-223, 10.1016/j.knosys.2012.11.003 View PDFView articleView in
    ScopusGoogle Scholar [16] Minqiu Xu, Youqing Wang An imbalanced fault diagnosis
    method for rolling bearing based on semi-supervised conditional generative adversarial
    network with spectral normalization IEEE Access, 9 (2021), pp. 27736-27747, 10.1109/ACCESS.2021.3058334
    View in ScopusGoogle Scholar [17] C. Seiffert, et al. RUSBoost: a hybrid approach
    to alleviating class imbalance IEEE Trans. Syst. Man Cybern.-Part A Syst. Humans,
    40 (1) (2010), pp. 185-197, 10.1109/TSMCA.2009.2029559 View in ScopusGoogle Scholar
    [18] N.V. Chawla et al., SMOTEBoost: improving prediction of the minority class
    in boosting, in: European Conference on Principles of Data Mining and Knowledge
    Discovery, Springer, Berlin, 2003, pp. 107–119. https://doi.org/10.1007/978-3-540-39804-2_12.
    Google Scholar [19] M.M. Ahmed, E. Hassanien, A.E. Hassanien IoT-based intelligent
    waste management system Neural Comput. Appl., 35 (2023), pp. 23551-23579, 10.1007/s00521-023-08970-7
    View in ScopusGoogle Scholar [20] M. Sadeghi, A. Banakar, M. Khazaee, M Soleimani
    An intelligent procedure for the detection and classification of chickens infected
    by clostridium perfringens based on their vocalization Braz. J. Poult. Sci., 17
    (2015), pp. 537-544, 10.1590/1516-635×1704537-544 View in ScopusGoogle Scholar
    [21] J. Huang, W. Wang, T. Zhang Method for detecting avian influenza disease
    of chickens based on sound analysis Biosyst. Eng., 180 (2019), pp. 16-24, 10.1016/j.biosystemseng.2019.01.015
    View PDFView articleGoogle Scholar [22] F.M. Colles, R.J. Cain, T. Nickson, A.L.
    Smith, S.J. Roberts, M.C. Maiden, D. Lunn, M.S. Dawkins Monitoring chicken flock
    behaviour provides early warning of infection by human pathogen Campylobacter
    Proc. R. Soc. B Biol. Sci., 283 (2016), Article 20152323, 10.1098/rspb.2015.2323
    View in ScopusGoogle Scholar [23] G. Li, B. Li, Z. Shi, Y. Zhao, H. Ma Design
    and evaluation of a lighting preference test system for laying hens Comput. Electron.
    Agric., 147 (2018), pp. 118-125, 10.1016/j.compag.2018.01.024 View PDFView articleView
    in ScopusGoogle Scholar [24] Y.R. Chien, Y.X. Chen An RFID-based smart nest box:
    an experimental study of laying performance and behavior of individual hens Sensors,
    18 (2018), Article 859, 10.3390/s18030859 View in ScopusGoogle Scholar [25] L.M.
    Stadig, T.B. Rodenburg, B. Ampe, B. Reubens, F.A. Tuyttens An automated positioning
    system for monitoring chickens’ location: effects of wearing a backpack on behaviour,
    leg health and production Appl. Anim. Behav. Sci., 198 (2018), pp. 83-88, 10.1016/j.applanim.2017.09.016
    View PDFView articleView in ScopusGoogle Scholar [26] N. de Alencar, I. ̈a ̈as,
    N. da Silva Lima, R. Gonçalves, L. de Lima, H. Ungaro, J. Minoro Abe, J. Abe Lameness
    prediction in broiler chicken using a machine learning technique Inf. Process.
    Agric., 8 (3) (2020), pp. 409-418, 10.1016/j.inpa.2020.10.003 Google Scholar [27]
    R. Blagus, L. Lusa SMOTE for high-dimensional class-imbalanced data BMC Bioinform.
    (2013), 10.1186/1471-2105-14-106 Google Scholar [28] Weiguo Zhao, Liying Wang,
    Seyedali Mirjalili Artificial hummingbird algorithm: a new bio-inspired optimizer
    with its engineering applications Comput. Methods Appl. Mech. Eng., 388 (2022),
    Article 114194, 10.1016/j.cma.2021.114194 View PDFView articleView in ScopusGoogle
    Scholar [29] B.G. Smythe, J.B. Pitzer, M.E. Wise, A.F. Cibils, D. Vanleeuwen,
    R.L. Byford Behavioral responses of cattle to naturally occurring seasonal populations
    of horn flies (Diptera: muscidae) under rangeland conditions J. Econ. Entomol.,
    108 (2015), pp. 2831-2836, 10.1093/jee/tov247 View in ScopusGoogle Scholar [30]
    Website Dataset: https://sites.google.com/view/poultrydataset/. Google Scholar
    [31] Abdoli, A., Murillo, A.C., Yeh, C.C.M., Gerry, A.C., & Keogh, E.J. (2018,
    December). Time series classification to improve poultry welfare. In 2018 17TH
    IEEE International Conference on Machine Learning and Applications (ICMLA) (pp.
    635–642). IEEE. https://doi.org/10.1109/ICMLA.2018.00102. Google Scholar [32]
    C.L. Daigle, et al. Noncaged laying hens remain unflappable while wearing body-mounted
    sensors: levels of agonistic behaviors remain unchanged and resource use is not
    reduced after habituation Poult. Sci., 91 (10) (2012), pp. 2415-2423, 10.3382/ps.2012-02300
    View PDFView articleView in ScopusGoogle Scholar [33] Y. Saeys, I. Inza, P. Larrañaga
    A review of feature selection techniques in bioinformatics Bioinformatics, 23
    (19) (2007), pp. 2507-2517, 10.1093/bioinformatics/btm344 View in ScopusGoogle
    Scholar [34] Tomoaki Ichikawa, et al. High-B-value diffusion-weighted MRI in colorectal
    cancer Am. J. Roentgenol., 187 (1) (2006), pp. 181-184, 10.2214/ajr.05.1005 .
    View in ScopusGoogle Scholar [35] C.-C.M. Yeh, N. Kavantzas, E. Keogh Matrix profile
    VI: meaningful multidimensional motif discovery 2017 IEEE International Conference
    on Data Mining (ICDM) (2017), pp. 565-574, 10.1109/ICDM.2017.66 IEEE View in ScopusGoogle
    Scholar [36] X. Zhuang, T. Zhang Detection of sick broilers by digital image processing
    and deep learning Biosyst. Eng., 179 (2019), pp. 106-116, 10.1016/j.biosystemseng.2019.01.003
    View PDFView articleView in ScopusGoogle Scholar Cited by (0) View Abstract ©
    2023 Elsevier B.V. All rights reserved. Recommended articles Vibration-based rotation
    speed estimation for Industrial IoT Internet of Things, Volume 25, 2024, Article
    101024 Eli Gildish, …, Igor Makienko View PDF Benchmark of machine learning algorithms
    on transient stability prediction in renewable rich power grids under cyber-attacks
    Internet of Things, Volume 25, 2024, Article 101012 Kemal Aygul, …, Istemihan
    Genc View PDF LELBC: A low energy lightweight block cipher for smart agriculture
    Internet of Things, Volume 25, 2024, Article 101022 Qingling Song, …, Xiantong
    Huang View PDF Show 3 more articles Article Metrics Captures Readers: 26 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Internet of Things (Netherlands)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A smart IoT-based monitoring system in poultry farms using chicken behavioural
    analysis
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Nguyen H.H.
  - Shin D.Y.
  - Jung W.S.
  - Kim T.Y.
  - Lee D.H.
  citation_count: '0'
  description: Industrial greenhouse mushroom cultivation is currently promising,
    due to the nutritious and commercial mushroom benefits and its convenience in
    adapting smart agriculture technologies. Traditional Device-Cloud protocol in
    smart agriculture wastes network resources when big data from Internet of Things
    (IoT) devices are directly transmitted to the cloud server without processing,
    delaying network connection and increasing costs. Edge computing has emerged to
    bridge these gaps by shifting partial data storage and computation capability
    from the cloud server to edge devices. However, selecting which tasks can be applied
    in edge computing depends on user-specific demands, suggesting the necessity to
    design a suitable Smart Agriculture Information System (SAIS) architecture for
    single-crop requirements. This study aims to design and implement a cost-saving
    multilayered SAIS architecture customized for smart greenhouse mushroom cultivation
    toward leveraging edge computing. A three-layer SAIS adopting the Device-Edge-Cloud
    protocol, which enables the integration of key environmental parameter data collected
    from the IoT sensor and RGB images collected from the camera, was tested in this
    research. Implementation of this designed SAIS architecture with typical examples
    of mushroom cultivation indicated that low-cost data pre-processing procedures
    including small-data storage, temporal resampling-based data reduction, and lightweight
    artificial intelligence (AI)-based data quality control (for anomalous environmental
    conditions detection) together with real-time AI model deployment (for mushroom
    detection) are compatible with edge computing. Integrating the Edge Layer as the
    center of the traditional protocol can significantly save network resources and
    operational costs by reducing unnecessary data sent from the device to the cloud,
    while keeping sufficient information.
  doi: 10.3390/agriculture14030489
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agriculture All Article Types Advanced   Journals
    Agriculture Volume 14 Issue 3 10.3390/agriculture14030489 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Academic Editor Wei
    Lu Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links
    Article Views 678 Table of Contents Abstract Introduction Background and Related
    Work System Architecture Design Implementation Results Discussion and Future Direction
    Conclusions Author Contributions Funding Data Availability Statement Conflicts
    of Interest References share Share announcement Help format_quote Cite question_answer
    Discuss in SciProfiles thumb_up Endorse textsms Comment first_page settings Order
    Article Reprints Open AccessArticle An Integrated IoT Sensor-Camera System toward
    Leveraging Edge Computing for Smart Greenhouse Mushroom Cultivation by Hoang Hai
    Nguyen 1, Dae-Yun Shin 1,2, Woo-Sung Jung 3, Tae-Yeol Kim 2 and Dae-Hyun Lee 4,*
    1 Sejong Rain Co., Ltd., In-House Venture of K-Water, Daejeon 34134, Republic
    of Korea 2 Graduate School of Smart Agriculture, Chungnam National University,
    Daejeon 34134, Republic of Korea 3 K-Water Research Institute, Daejeon 34045,
    Republic of Korea 4 Department of Biosystems Machinery Engineering, Chungnam National
    University, Daejeon 34134, Republic of Korea * Author to whom correspondence should
    be addressed. Agriculture 2024, 14(3), 489; https://doi.org/10.3390/agriculture14030489
    Submission received: 13 January 2024 / Revised: 25 February 2024 / Accepted: 5
    March 2024 / Published: 18 March 2024 (This article belongs to the Special Issue
    Application of Intelligent Greenhouse and Plant Factory Systems in Agricultural
    Production) Download keyboard_arrow_down     Browse Figures Versions Notes Abstract
    Industrial greenhouse mushroom cultivation is currently promising, due to the
    nutritious and commercial mushroom benefits and its convenience in adapting smart
    agriculture technologies. Traditional Device-Cloud protocol in smart agriculture
    wastes network resources when big data from Internet of Things (IoT) devices are
    directly transmitted to the cloud server without processing, delaying network
    connection and increasing costs. Edge computing has emerged to bridge these gaps
    by shifting partial data storage and computation capability from the cloud server
    to edge devices. However, selecting which tasks can be applied in edge computing
    depends on user-specific demands, suggesting the necessity to design a suitable
    Smart Agriculture Information System (SAIS) architecture for single-crop requirements.
    This study aims to design and implement a cost-saving multilayered SAIS architecture
    customized for smart greenhouse mushroom cultivation toward leveraging edge computing.
    A three-layer SAIS adopting the Device-Edge-Cloud protocol, which enables the
    integration of key environmental parameter data collected from the IoT sensor
    and RGB images collected from the camera, was tested in this research. Implementation
    of this designed SAIS architecture with typical examples of mushroom cultivation
    indicated that low-cost data pre-processing procedures including small-data storage,
    temporal resampling-based data reduction, and lightweight artificial intelligence
    (AI)-based data quality control (for anomalous environmental conditions detection)
    together with real-time AI model deployment (for mushroom detection) are compatible
    with edge computing. Integrating the Edge Layer as the center of the traditional
    protocol can significantly save network resources and operational costs by reducing
    unnecessary data sent from the device to the cloud, while keeping sufficient information.
    Keywords: smart agriculture; mushroom; edge computing; farm management information
    system (FMIS); machine vision; Agricultural IoT 1. Introduction Nowadays, edible
    mushrooms are well known as not only a healthy food but also a valuable pharmacy,
    since they can provide rich nutrients such as protein, minerals, and vitamins
    as well as increase the human immune system to prevent many diseases, even cancer
    [1,2]. Therefore, the demand for mushroom products is increasing in the world’s
    food market, especially after the COVID-19 pandemic [3,4]. Although wild mushrooms
    can be harvested in the natural environment, their high sensitivity to seasonal
    variations of weather/climate factors leads to unsustainable production [5,6].
    Industrialized mushroom cultivation in the greenhouse offers an alternative strategy
    to natural cultivation to improve both the quantity and quality of mushroom production
    because the key environmental parameters affecting mushroom growth can be monitored
    and controlled. However, the traditional cultivation method based on manual monitoring,
    controlling, and harvesting involves time-consuming and labor-intensive work which
    can then increase operational costs, demanding timely, automated, cost-effective,
    and eco-friendly solutions for the mushroom industry [7,8]. Along with the Fourth
    Industrial Revolution (Industry 4.0) when innovative technologies such as the
    Internet of Things (IoT), cloud computing, Big Data (BD), and Artificial Intelligence
    (AI) have been commonly employed in industrial automation, a parallel revolution
    in agriculture (Agriculture 4.0) boosts automation in farming activities by applying
    those new technologies to the agriculture sector, so-called smart agriculture/farming
    (or precision agriculture, digital agriculture, and Agricultural IoT). Although
    it can contribute to the reduction in labor costs, deploying smart agriculture
    systems in practice is facing other cost-related problems such as setup costs
    and running costs [9]. On the one hand, from the users’ point of view, especially
    in some developing countries where most people working in agriculture are smallholder
    farmers, they have limited access to modern smart farming infrastructure due to
    the expensive product prices (setup costs). On the other hand, from the perspective
    of service developers/providers, smart farming system construction and applications,
    which are commonly implemented in centralized cloud servers can consume massive
    resources for BD storage, management, analytics, and security, resulting in higher
    overall cloud service costs (running costs). Cutting those unnecessary costs is
    therefore pivotal for many business organizations, especially small and medium
    enterprises (SMEs), to optimize their smart farming systems toward serving smallholder
    clients with cost-saving infrastructures. Besides the cloud service costs, traditional
    smart agriculture systems are often limited by the slow network connection (low
    bandwidth) between Agricultural IoT devices and cloud servers when all the huge
    numbers of data collected from the devices are directly sent to the cloud, which
    then delays the system decisions and hampers real-time applications in smart agriculture
    (high latency) [10,11,12]. This is a challenge for mushroom cultivation when its
    key parameters require strict monitoring within short time intervals [13]. Edge
    computing (including cloudlets, fog, and mobile edge computing) has recently emerged
    as a promising solution to bridge this connection gap, thanks to the rapid advancement
    of information and communication technologies as well as hardware capacity. Specifically,
    an edge computing node can be regarded as a decentralized local server (edge server)
    that allows for storing a small number of data and bringing data computation closer
    to the edge devices [14,15,16]. By coupling the edge computing paradigm with a
    classical Device-Cloud protocol, the original BD is effectively managed in an
    offline process at the edge server before transmission instead of storing and
    processing all the collected raw data at the cloud server [17]. This can free
    up the bandwidth for faster connection and for the cloud server load spent on
    complicated analytics and applications [16,18]. Despite the wide deployment of
    edge computing in smart homes and smart cities, it is not very commonplace in
    smart farming, and thus there is still room for leveraging and improving this
    paradigm to meet the demands of smart agriculture as well as the greenhouse mushroom
    industry. Shifting partial work from cloud to edge offers SMEs an effective and
    cost-saving solution to overcome the issues of excessive bandwidth-cloud utilization
    and high latency [10]. However, determining which features should be conducted
    in the edge server highly depends on the specific functional requirements and
    capacity of each smart farming system [19]. This suggests the necessity to design
    a proper digital Farm Management Information System (FMIS) architecture assisting
    smart agriculture applications for the specific farmer’s requirements (e.g., for
    each different crop) [20,21,22], hereafter the Smart Agriculture Information System
    (SAIS). From the perspective of an SME, Sejong Rain Company in collaboration with
    Chungnam National University aims to design and implement a low-cost SAIS architecture
    customized for smart greenhouse mushroom management in association with a paradigm
    shift to edge computing for business objectives, in this article. This system
    was tested in a pilot site located in Songsan Green City, Republic of Korea. The
    advanced Agricultural IoT sensors developed by Korean companies, a standard network
    flow that is possible for scalability, and edge computing tasks suitable for greenhouse
    mushroom cultivation, together with examples of how to implement them, were introduced
    through the proposed SAIS architecture. Finally, a potential SAIS architecture
    for business purposes in future studies was also suggested, especially for the
    developing country markets that consider smart agriculture development as the
    core industry. 2. Background and Related Work 2.1. Smart Agriculture Information
    System (SAIS) The traditional FMIS, which relied on simple farm recordkeeping
    tasks, has been developed since the 1970s to provide useful information for decision-makers
    to effectively manage farming activities [23]. However, in the era of Agriculture
    4.0 nowadays, the adoption of a traditional FMIS is limited when massive data
    are provided from the IoT sensors and complex tasks are required for modern farm
    management [24,25]. To cope with the rapid increase of agricultural data pools
    and data-driven farming applications, the SAIS, whose cornerstone is the large
    digital FMIS specialized for precision agriculture, can improve the automation
    and efficiency in managing enormous amounts of farming information including data
    collection, data processing, and data-driven decision making [20,25]. Although
    the core of the SAIS is the multi-layered architecture comprising four basic components
    of Smart Product, Network Connection, Data, and Smart Service [22], recent studies
    paid more attention to adapting this core SAIS to the domain-specific uses by
    breaking it into higher-level architectures, ranging from the classical three-
    or four-layer up to the advanced seven- or eight-layer architectures. Among the
    classical architectures, the three-layer one, which corresponds to the two-layer
    without the Network Layer, refers to the Device-Cloud protocol where the raw data
    obtained by sensors will be directly transmitted to the cloud for storage, processing,
    and applications. The four-layer architecture (three-layer without the Network
    Layer) can allow edge computing to involve those cloud tasks. Integrating a separate
    layer of edge computing into SAIS architectures is also gaining attraction and
    possibly becoming a trending paradigm in future studies [17]. Additionally, advanced
    architectures improved on the classical ones for business objectives by proposing
    enterprise architecture by adding new layers such as the Business, User, or Security
    Layers [26]. For more details, the readers are referred to the literature reviews
    of different multi-layered SAISs in previous studies, summarized in Table 1. Table
    1. Literature reviews of multi-layered Smart Agriculture Information Systems.
    2.2. Smart Greenhouse Mushroom Cultivation Aside from the benefits for human health
    and commerce, edible mushrooms are currently becoming the preferred crop for indoor
    cultivation, even in urban farming, because they are easily relocated, do not
    require large space and direct sunlight, and need only a short duration until
    harvesting [35]. However, the major limitation came from the high sensitivity
    of mushrooms to the surrounding environment, since extreme weather conditions
    can immediately damage mushroom health. Therefore, the most important task of
    indoor mushroom cultivation relies on the timely and continuous control of the
    key environmental parameters’ ideal conditions at every mushroom growing stage
    [36]. Previous studies listed a wide range of those key parameters [13], but the
    two most important ones that are worth regarding are air temperature and humidity,
    while several studies further considered light intensity or carbon dioxide (CO2)
    levels. The ideal ranges for these parameters vary depending on different mushroom
    species [7,37], and they can be easily, timely, and automatically observed and
    controlled in the greenhouse using IoT sensors, which have been widely reported
    in the literature as the common approach for indoor mushroom management. Alongside
    the development of computer vision and precision agriculture technologies, high-resolution
    cameras associated with AI-based image processing have been successfully applied
    in the mushroom industry to enhance management accuracy and efficiency [38]. Current
    advances improved by combining the advantages of IoT sensors and camera computer
    vision, which opens a new door for future smart greenhouse mushroom cultivation
    in particular and smart agriculture in general. Literature reviews of smart greenhouse
    mushroom management using IoT sensors, cameras, and integrated IoT sensor-camera
    systems are summarized in Table 2. Table 2. Literature reviews of smart greenhouse
    mushroom management. 3. System Architecture Design Based on the potential of the
    SAIS with edge computing and the increased need for mushroom products, this study
    aims to design and implement a multi-layered SAIS architecture with leveraging
    edge computing for cost-effective smart greenhouse mushroom cultivation. In particular,
    an integrated IoT environmental sensor-camera system and the oyster mushroom were
    selected to test in this study. To highlight the capability of coupling the edge
    computing model to the classical Device-Cloud protocol, a general three-layer
    SAIS architecture (without the Network Layer), which adopted the Edge Layer as
    a bridge for the Device and Cloud (Device-Edge-Cloud protocol), was employed.
    Moreover, we also suggested the integration of a bidirectional (two-way) communication
    mechanism in this system to separate the network flow into two major domains for
    different functional tasks, including the Forward and Backward Domains. In each
    layer, two modules were included to respond to separated functional tasks in the
    Forward and Backward Domains, respectively. The proposed SAIS architecture applied
    to greenhouse mushroom management in this paper is depicted in Figure 1. Specifically,
    according to Figure 1, the Forward Domain enables data transmission from the Device
    Layer (Data Collecting module) through the Edge Layer for quality control and
    aggregation (Data Preprocessing module), and then to the Cloud Layer (Data Storage
    module). In contrast, the Backward Domain allows an inverse direction that AI-based
    solutions developed at the Cloud Layer using stored data (Data Analytics/AI Development
    module) to be deployed at the Edge Layer for real-time applications (Real-time
    AI Deployment module), then to control the ideal conditions of the environmental
    parameters if necessary via controlling actuators/sensors at the Device Layer
    (Device Controlling module). The Forward Domain is responsible for the data management
    and storage tasks while the Backward Domain is responsible for the solutions development
    and decision-making based on the obtained data. In this study, we describe the
    proposed architecture following its network flow and focus especially on the Forward
    Domain, since it is the mainstream in the SAIS. It is important to note that although
    the Network/Communication Layer which encompasses the protocols for network connection
    is also important and was already employed in this paper, we did not focus on
    modifying this layer, so it was not mentioned here as a major layer. A detailed
    description of the proposed SAIS architecture and its implementation for smart
    greenhouse mushroom management at the testbed site is provided below. Figure 1.
    A proposed design of the Smart Agriculture Information System (SAIS) architecture
    for smart greenhouse mushroom cultivation. The blue dashed border and arrows indicate
    the Forward Domain and its procedure, and the red dashed border and arrows indicate
    the Backward Domain and its procedure. 3.1. Forward Domain–Device Layer The Device
    Layer is the first layer in this proposed SAIS architecture, which enables a workspace
    for Agricultural IoT end devices. In this layer, the Data Collecting module is
    particularly responsible for the Forward Domain. Its detailed architecture is
    presented in Figure 2. Figure 2. The architecture of the Forward Domain in the
    Device Layer. Data Collecting: This module aims to collect farming and environmental
    data observed from the IoT sensors system. Since the integrated IoT environmental
    sensor-camera system was employed, this module was separated into two sub-modules
    for better management, consisting of (1) the one-dimensional (1D) Data sub-module
    which stands for point environmental data collected from the IoT sensors, and
    (2) the two-dimensional (2D) Data sub-module, which stands for image data collected
    from the cameras. 1-Dimensional Data (1): This sub-module offers a workspace for
    the point IoT sensors to collect the environmental data time series. Because temperature,
    humidity, and CO2 are generally the key environmental parameters for mushroom
    growth in the literature, they were considered in this system. A three-in-one
    IoT environmental sensor, which measures automatically and simultaneously the
    three key parameters of temperature (T), relative humidity (RH), and CO2 levels
    (CO2) in real-time using only one sensor, was adopted in this study. This low-cost
    combination sensor is a domestic Korean product that was researched and developed
    by the Sejong Rain Company, Republic of Korea, and is currently being tested before
    commercialization. Even though this combination sensor can observe real-time data,
    the maximum 10 min time resolution has been set to be updated in this sub-module.
    For more information or inquiries on this combination IoT sensor, please refer
    to the company homepage (http://sejongrain.com/, accessed on 10 January 2024).
    Its photos and specifications are shown in Figure 3a,b, and Table 3, respectively.
    Two-Dimensional Data (2): Besides the IoT environmental sensor, this system uses
    a surveillance camera system to collect crop image/video (time series of RGB images)
    data, which enhances automation in intuitive recognition of mushroom characteristics
    (e.g., shapes, colors, species, phenological stages, or related diseases) and
    was implemented in this sub-module. Specifically, an industrial high-definition
    (HD 720p) digital camera (model SMT-720PUSBBOX) manufactured by Smtkey Electronic
    Technology Company, Shenzhen Guangdong, China, was employed to provide streaming
    images/video of the oyster mushroom. However, to match the temporal resolution
    of the environmental sensor, the camera system was set to capture image time series
    at 10 min intervals in single red (R), green (G), and blue (B) bands. A photo
    of the camera used is shown in Figure 3c and its specifications are presented
    in Table 4. Figure 3. Photos of the combination IoT environmental sensor: (a)
    displaying temperature and humidity and (b) displaying CO2 level; and (c) photo
    of the surveillance camera used in this study. The Korean word in the IoT combination
    sensor label is the name of the company who develops this device, the Sejong Rain
    Company, Republic of Korea. Table 3. Specifications of the combination IoT environmental
    sensor used. Table 4. Specifications of the surveillance camera used. 3.2. Forward
    Domain–Edge Layer The Edge Layer or edge server is a middle layer in this proposed
    three-layer SAIS, where edge computing is executed, and is the major part of this
    study. It is important to note that edge computing is not a replacement for traditional
    cloud computing, but that it is a complement for cloud computing. Edge computing
    is normally conducted in low-memory and low-power microprocessors and assigned
    for small workloads with lightweight computation, which fits in with real-time
    AI applications. In contrast, cloud computing is compatible with smart solutions
    requiring BD and heavy computation capacity. Therefore, a powerful, low-energy-consumption,
    and cost-effective Raspberry Pi single-board computer was used in this study for
    edge computing in both Forward and Backward Domains. In particular, the Raspberry
    Pi 3 Model B+, which also provides a high-speed processor suitable for HD video
    processing and dual connections via both WiFi (wireless) and Ethernet port (wired),
    was employed in the system. For a more detailed description of its specifications,
    users are referred to the company’s website (https://www.raspberrypi.com/, accessed
    on 10 January 2024). The related tasks are shown in the Data Preprocessing module
    for the Forward Domain, and its architecture is depicted in Figure 4. Figure 4.
    The architecture of the Forward Domain in the Edge Layer. Data Preprocessing:
    It is important to note that not all the raw data should be stored in the cloud.
    Selecting essential data for the cloud can save resources by decreasing cloud
    and bandwidth workloads. Hence, the main mission of this module is to reduce the
    number of raw data observed from the Device Layer while keeping data quality before
    sending them to the Cloud Layer, based on the following sub-modules for preprocessing:
    Small-Data Storage (1): Since a previous study indicated the effective greenhouse
    mushroom monitoring on an hourly basis [13], we also set a one-hour (1 h) interval
    as the standard temporal resolution to be sent to the cloud and for mushroom cultivation
    in this study. To this end, several raw data should be stored in the Edge Layer
    for further temporal data resampling and quality control development. Temporal
    Resampling (2): The temporal resampling applied in this sub-module aims to convert
    the 10 min raw data into hourly data and relies on a simple Average Filtering
    method, whereas the six raw data samples within every 1h interval stored in the
    Small-Data Storage sub-module will be transferred to this sub-module for averaging
    into one data sample. This method can be applied to both 1D and 2D data in this
    SAIS and may help it reduce the number of data together with overcoming the temporal
    gaps (missing data) that occurred at the original 10 min interval. Data Quality
    Control (3): Despite the benefit of temporal resampling in dealing with the gaps
    in raw data time series, the resampled data still probably suffer from temporal
    gaps (when six raw data samples are missing values) or sudden extreme conditions.
    This requires an AI-based data quality control filter that can not only automatically
    and continuously detect such outliers in the data stream, but also has a low computational
    cost when it is applied to the Edge Layer. Even though several lightweight AI
    models are compatible with edge/fog computing, the well-known k-nearest neighbors
    (k-NN) algorithm was used for showcasing in this research. The k-NN is simply
    a non-parametric supervised learning method, which considers k samples of training
    data to solve the classification and regression problems, but it can be regarded
    as an unsupervised learning algorithm when it is applied to anomaly detection
    [50]. The k-NN integrated with a 24 h moving window was applied in this study
    to identify whether real-time data are anomalous or not, based on the lagged 23h
    data samples stored in the Small-Data Storage sub-module. Whenever the ksample
    is detected and masked, it can be continuously used to identify the (k + 1) sample,
    and the (k − 23) sample is then automatically removed from the Small-Data Storage
    sub-module. This AI-based data quality control filter can be applied to both 1D
    and 2D data. For 1D data, besides transmitting them to the Cloud Layer for long-term
    storage, the processed data were also sent to a responsive module of the Backward
    Domain in the Edge Layer to support the system’s real-time decision-making. However,
    in the case of 2D data, to reduce the high computational cost when processing
    image data, the RGB images obtained from the camera were first converted to grayscale
    images and then transformed to 1D format by simply averaging the digital number
    (DN) values within an image scene (scene-averaging) before they can be applied,
    with the data quality control filter. Anomalous DN data samples closer to 0 can
    be identified as temporal gaps (black images), while those with high values (e.g.,
    higher than 80—a typical average grayscale digital number value) can be classified
    as light images, which still provide useful information. The quality-controlled
    image data on an hourly basis were sent only to the Cloud Layer. 3.3. Forward
    Domain–Cloud Layer The Cloud Layer is the final layer in this proposed SAIS, which
    is widely used in various architectures and allows high computational application
    development based on the quality-controlled BD. Hence, the Forward Domain offers
    long-term high-quality data to be stored firstly in the Cloud Layer under the
    Data Storage module management. Data Storage: Because the accuracy of AI models
    significantly depends on the quality and quantity of the training data, this module
    aims to provide a space to store long-term (e.g., one life cycle of mushrooms)
    high-quality data observed from the integrated environmental sensor-camera system
    to develop AI-based solutions which require a high computation cost. These necessary
    datasets stored in this module can be then transmitted to the responsive module
    of the Backward Domain in the Cloud Layer for training AI models with respect
    to specific user requirements. 3.4. Backward Domain–Cloud Layer As highlighted
    in the description above, the mission of the Backward Domain is to develop AI-based
    smart solutions and make decisions based on the collected and stored data in the
    Forward Domain. Therefore, in the Cloud Layer, the Backward Domain tasks are conducted
    in the AI Development module. Figure 5 presents the architecture of the Backward
    Domain in the Cloud Layer. Figure 5. The architecture of the Backward Domain in
    the Cloud Layer. AI Development: This module is designed to account for AI-based
    smart solution development for specific demands, which requires BD for high computational
    cost training and complex analytics. The implementation of this module in the
    Backward Domain of the Cloud Layer follows the two sub-modules: AI Model Training
    (1): This sub-module includes the feature selection task for training AI models
    based on user-specific demands, whereas relevant important features were extracted
    from the BD in the Data Storage module. Furthermore, several well-known AI model
    candidates will be selected for training. Parameter Storage (2): The optimal AI
    model among the candidates was chosen and its optimal parameters were then stored
    in this sub-module for further deployment. Various AI-based smart greenhouse mushroom-cultivation
    solutions can be considered for development. However, to showcase only one example
    of the high-computational-cost AI model application with BD (mostly with image/video
    data) in the Cloud Layer, a well-known, fast, effective, and advanced computer
    vision algorithm version among the series of the “You Only Look Once” (YOLO) framework
    [51], the YOLOv5 model, was used in this study for real-time mushroom detection
    with a surveillance camera. Specifically, the YOLOv5 model, whose cornerstone
    is a simple deep convolutional neural network, was introduced in 2020 to improve
    on its predecessors in object detection. The YOLOv5 model divides images into
    a grid cell and finally predicts the objects with multiple bounding boxes per
    grid cell. Its architecture consists of three main components: the BackBone for
    feature extraction, the Neck for feature fusion, and the Head for feature conversion
    to bounding-box parameters. The BD images stored in the Big-Data Storage module
    can be labeled for training the YOLOv5 model. To reduce the training costs (including
    the training time and number of training-data samples), this study adopted the
    Transfer Learning method to fine-tune the pre-trained YOLOv5 model for mushroom
    detection to be compatible with the current training mushroom images in the Big-Data
    Storage module. The optimal model parameters were stored in the Parameter Storage
    sub-module and then transferred to the Edge Layer for real-time deployment. 3.5.
    Backward Domain–Edge Layer The Backward Domain in the Edge Layer provides the
    workspace for real-time smart agriculture solution deployment, which was assigned
    to the Real-time AI Deployment module, with its architecture provided in Figure
    6. Figure 6. The architecture of the Backward Domain in the Edge Layer. Real-time
    AI Deployment: This module consists of four sub-modules for integrating analytics
    from both environmental sensors and cameras to improve real-time solution/decision
    making, as follows: Short-term Analytics (1): This sub-module receives the analytical
    results from the Forward Domain in the Edge Layer (Data Preprocessing module),
    mainly based on temporary small-data storage of the 1D data from the IoT environmental
    sensor. Long-term Analytics (2): The optimal AI model obtained from the Backward
    Domain based on BD analytics in the Cloud Layer (AI Development module) was deployed
    in this sub-module, mainly for the 2D image data. Integrated Analytics (3): This
    sub-module combines the analytical results from the Short-term and Long-term Analytics
    sub-modules to enhance decision-making. Solutions/Decision Making (4): Relevant
    solutions/decisions will be made in this sub-module, based on the results from
    the Integrated Analytics sub-module. 3.6. Backward Domain–Device Layer The Backward
    Domain in the Device Layer covers the Device Controlling module. Device Controlling:
    This module aims to automatically control sensors/actuators to make farm management
    decisions as soon as possible, based on the analytical solutions provided by the
    Real-time AI Deployment module in the Edge Layer. For example, the image-based
    mushroom detection solution allows users to recognize whether the mushrooms are
    currently in the growing period or have been harvested. In the case of extreme
    conditions observed by the environmental sensor during the mushroom growing period,
    this module enables the modulating of key environmental parameters by controlling
    IoT sensors/actuators such as the heater/cooling fan (for temperature), humidifier/dehumidifier
    (for humidity), or air ventilation (for CO2 level) via connection to a microcontroller.
    4. Implementation Results 4.1. Implementation of Forward Domain–Device Layer Installation
    of the integrated IoT environmental sensor-surveillance camera system for smart
    greenhouse mushroom cultivation is the first implementation step and was carried
    out in a mushroom house located in Songsan Green City, Republic of Korea. This
    system aims to collect the key environmental parameters affecting mushroom growth
    and intuitive mushroom information, which is the responsibility of the Data Collecting
    module belonging to the Forward Domain in the Device Layer. Figure 7 describes
    the installation of the combination IoT environmental sensor for simultaneously
    measuring the real-time 1D data of three key environmental parameters including
    air temperature (T), atmospheric relative humidity (RH), and CO2 level (CO2),
    together with the construction of a data logger containing the Raspberry Pi single-board
    computer. Data collected from the IoT environmental sensor were then transmitted
    to the data logger for further processing, mainly via a wireless connection (Figure
    7a). However, we also set up ethernet cables here to ensure that data transmission
    can be automatically switched to a wired connection when the wireless connection
    collapses, maintaining their stable transmission (Figure 7b). It is important
    to note that although the environmental sensor was set up in the greenhouse environment,
    we constructed the data logger outside the mushroom house for future combination
    with outdoor smart agriculture sensors/actuators such as smart precipitation gauges,
    unmanned aerial vehicles, tractors, and so on (Figure 7c). Figure 7. Installation
    of the combination IoT environmental sensor for the greenhouse mushroom information
    system with connection to data logger via the following: (a) wireless connection
    and (b) wired connection. (c) Construction of data logger outdoors with potential
    combination with outdoor sensors. The Korean words in the label of the data logger
    represent for the system name “Agriculture Environment Data Collection System”
    and the company name “Sejong Rain”. The time series of the raw 1D data (at 10
    min of temporal resolution) for the three key parameters from the installed IoT
    environmental sensor for nearly one month (one life cycle of the mushroom) is
    depicted in Figure 8. In general, no major operational errors occurred when continuous
    data were collected without missing values and the observed data range fell within
    the sensor’s allowable measuring range for each parameter, according to Table
    3. This suggests that the combination IoT environmental sensor used in this study,
    which was developed by the Korean company, is ready and has potential for smart
    mushroom monitoring and further applications in smart agriculture. Figure 8. Time
    series of 1D raw data in 10 min intervals collected from the combination IoT environmental
    sensor for (a) temperature (T), (b) humidity (RH), and (c) CO2 level (CO2). Besides
    the IoT environmental sensor, the simultaneous installation of the surveillance
    camera to capture real-time intuitive 2D data for crop information was also implemented
    in the pilot area, as illustrated in Figure 9. The camera was fixed to collect
    a time series of RGB images covering a partial mushroom farm for preliminary testing.
    We also connected the camera system to the data logger for data transmission.
    Figure 9. Installation of the surveillance camera for the greenhouse-mushroom
    information system. Time series of the raw 2D data at 10 min time intervals were
    obtained from the installed surveillance camera. We showcase several RGB image
    samples of mushrooms at different growth stages in Figure 10. It can be drawn
    from Figure 10 that intuitive mushroom information can be well captured by the
    installed surveillance camera, even with the changes between growth stages. However,
    there are several temporal gaps (image samples with no-value/black images) that
    occurred during the operation which need to be noticed and addressed. All the
    raw data collected in the Device Layer were then sent to the Edge Layer for pre-processing.
    Figure 10. Samples of 2D raw data in 10 min intervals collected from the surveillance
    camera during different mushroom growth stages. The red rectangle highlights the
    gap (no-value data) in the data stream. 4.2. Implementation of Forward Domain–Edge
    Layer The raw 1D and 2D data collected at the Device Layer were then delivered
    to the Forward Domain in the Edge Layer for further preprocessing before being
    transmitted to the Cloud Layer. The Edge Layer is the major layer in this designed
    SAIS, whose main goal is a stepping stone toward saving cloud and bandwidth resources
    as well as selecting essential data for the cloud server. Therefore, the Forward
    Domain in this layer (represented by the Data Preprocessing module) is responsible
    for reducing unnecessary data collected from the Device Layer and conducting quality
    control for high-quality data sent to the Cloud Layer. To this end, several raw
    data samples (small data) should be stored in this layer for a short period and
    the Small-Data Storage sub-module is responsible for providing storage space in
    this task. The first preprocessing task in this module is data reduction. Since
    the mushroom monitoring on an hourly basis is sufficient and was selected as the
    standard temporal resolution to be updated in the cloud, an average filtering
    technique was utilized as the temporal resampling method to convert 10 min data
    into 1h data. Figure 11 illustrates examples of how to conduct this temporal resampling
    method in both 1D and 2D data time series. In particular, every six raw data samples/images
    within a current 1h duration were initially stored. Since no significant differences
    were observed among these raw data, an hourly representative point value/image
    can be generated by simply taking an average of those six data samples/images.
    After an hourly representative data sample/image is given, it will be sent back
    to the Small-Data Storage sub-module for temporal storage and the current raw
    data will be automatically removed from this sub-module for updating the coming
    raw data. It is important to note that, besides the data reduction benefit, this
    method also supports dealing with temporal gap problems, ensuring continuous measurement
    of the data stream. For example, a black image (temporal gap) that occurred in
    the raw 2D data was successfully removed and replaced by a representative image
    (average image of the six raw data in 10 min intervals within every 1h interval)
    while keeping sufficient information, as illustrated in Figure 11b. Figure 11.
    Examples of the temporal resampling based on the average filtering method to resample
    raw data in 10 min intervals into standard data in 1h intervals applied to the
    following: (a) 1D data from the environmental sensor and (b) 2D data from the
    camera. The red dashed rectangle represents the 1h data window. The second preprocessing
    task in the Forward Domain of the Edge Layer is for data quality control, which
    is covered by the Data Quality Controlling sub-module. For the 1D data, since
    the important task of smart greenhouse mushroom cultivation is mainly based on
    modulating the ideal conditions of key environmental parameters, this sub-module
    aims to determine the sudden extreme conditions occurring in the environmental
    data, supporting the system with timely responses to address the problems as well
    as managing high-quality data sent to the cloud server. However, because edge
    computing is compatible with light computing applications that can work well in
    short-term small-data numbers stored in this layer, effective and lightweight
    AI algorithms are preferable for this task. As a result, a widely used lightweight
    AI model, the k-NN algorithm, was selected as the anomaly-detection method in
    this study. More specifically, the k-NN was conducted in a 24h moving window (k
    = 23) for the continuous hourly data samples stored in the Small-Data Storage.
    The real-time outlier detection of the current data sample (the k sample) can
    be performed based on its recent 23 lagged hourly data samples. Whenever the k
    sample is detected and masked, it can be continuously used to identify the (k
    + 1) sample, and the (k − 23) sample is automatically deleted from the Small-Data
    Storage sub-module. Figure 12 presents the results of applying the k-NN algorithm-based
    anomaly detection to the 1D hourly data of three key environmental parameters
    for mushroom cultivation. These successful implementation results of the k-NN
    model-based anomaly detection in the 1D environmental data, as illustrated in
    Figure 12, highlight the suitability and potential application of lightweight
    AI models for edge computing. Figure 12. Results of the AI-based anomaly-detection
    application to the 1D hourly (hr) environmental data for (a) temperature (T),
    (b) humidity (RH), and (c) CO2 level (CO2). As regards the 2D data, one problem
    is that although the temporal resampling method can deal with temporal gaps in
    the raw data, they still possibly occur in the resampled data (when the six original
    images within one hour are all black images). Hence, the anomaly-detection method
    should be applied to the hourly 2D data to detect these gaps. However, to deal
    with the high computational cost when processing numerous crop images, a preprocessing
    procedure series was applied to the 2D data to transform them into 1D data before
    conducting the AI-based anomaly detection. In particular, an RGB image was first
    converted to a grayscale image and subsequently transformed into a digital number
    value by averaging grayscale digital numbers from all pixels within an image scene
    (scene averaging). Figure 13 provides a general example of how to carry out AI-based
    anomaly detection in the 2D data and their results for mushroom images in this
    study. As can be seen from Figure 13, two types of outliers have been identified
    by using the k-NN anomaly-detection method. Anomalous data with digital numbers
    closer to 0 are assigned to the temporal gaps (black images), which do not provide
    any useful information, whereas the outliers with the highest values (e.g., greater
    than 80—a typical average grayscale digital number value) are identified and masked
    as “light image”, which still provide similar crop information to the normal images
    but at a higher brightness. Finally, after the resampled data are quality-controlled,
    they will be sent to the cloud server for storage and to the Short-term Analytics
    sub-module in the Backward Domain of the Edge Layer for supporting real-time decision
    making. Figure 13. An example of the AI-based anomaly-detection application to
    the 2D image data. The color maps on the left side display the heatmap of an original
    image in digital numbers for red, green, blue, and grayscale channels. 4.3. Implementation
    of Forward- and Backward-Domain–Cloud Layer In the Cloud Layer, high-quality BD
    transmitted from the Edge Layer after passing the data quality control procedure
    will be initially stored in the Big-Data Storage Module of the Forward Domain.
    These high-quality BD stored in the Cloud Layer were then sent to the AI Development
    Module for training high-computational-cost AI models based on specific user requirements.
    This study employed a widely used deep learning-based object-detection algorithm,
    the YOLOv5 model, to address the mushroom recognition problem, mainly for the
    2D image data. Moreover, to reduce the computational cost during the training
    process, the transfer learning technique was integrated to fine-tune the pre-trained
    YOLOv5 model for adapting the mushroom images stored in the Cloud Layer. Detailed
    information on training and evaluation of the YOLOv5 model for mushroom detection
    in this study is shown in Table 5. The obtained optimal hyperparameters were subsequently
    stored in the Parameter Storage module and were used for real-time deployment
    of this solution in the Edge Layer. Table 5. Information on the training and evaluation
    of the YOLOv5 model for mushroom detection. 4.4. Implementation of Backward Domain–Edge
    Layer The Backward Domain in the Edge Layer, which was carried out by the Real-time
    AI Development module, aims to provide integrated analytics for decision-making
    by combining the results of short-term and long-term analytics. In particular,
    on the one hand, the Short-term Analytics module in this layer obtained the key
    environmental-parameter anomaly-detection results from the Forward Domain in the
    Edge Layer, which was described in the section above (Implementation of Forward
    Domain–Edge Layer) and is shown in Figure 12. On the other hand, when it comes
    to long-term analytics, this module adapted the developed YOLOv5-based mushroom-recognition
    model for detection, based on the stored optimal model hyperparameters from the
    Backward Domain in the Cloud Layer. Representative examples of the YOLOv5 model
    deployment for mushroom detection in this study are depicted in Figure 14, whereas
    single-object (mushroom) areas are detected in continuous images with the red
    bounding box and confidence level by further applying instance segmentation. According
    to the figure, in general, most of the mushroom areas during the fruiting period
    can be well identified by the system, with a clear discrimination of the non-mushroom
    areas observed. This indicated the successful deployment of a high-computational-cost
    AI model in the Edge Layer, demonstrating its suitability to adopt a developed
    AI model in the Cloud Layer for edge computing. However, there are still several
    mushroom areas in an image that the system cannot detect, suggesting the need
    for retraining the model to improve its performance by the updated high-quality
    BD sent to the Cloud. The results from the long-term analytics can be integrated
    with those from the short-term analytics to support the system’s decision-making.
    For example, if outliers are detected in the streaming environmental data (results
    from the short-term analytics) during the mushroom’s fruiting period (results
    from the long-term analytics), the system can automatically send an alert notification
    to the users or control the sensors/actuators for maintaining the normal status
    of environmental conditions. Figure 14. An example of implementation of the Real-time
    AI Deployment module (deployment of YOLOv5 and transfer learning in mushroom detection).
    5. Discussion and Future Direction The major difference between this proposed
    research and related work mentioned in Table 1 and Table 2 is the combined use
    of the edge-computing paradigm in a smart greenhouse mushroom-cultivation information
    system. In particular, the Edge Layer was not included in the SAIS architecture
    for mushroom cultivation in Table 1 [28], while none of the previous mushroom
    management studies in Table 2 employed the edge computing model in their system.
    Thus, the successful practical application of this study can shed new light on
    leveraging edge-computing paradigm shifts as well as improving smart mushroom
    cultivation systems. In addition, a minor discrepancy that is worth mentioning
    is the introduction of a new low-cost combination IoT environmental sensor made
    by a Korean company. This product combines three single sensors for different
    environmental parameters into one sensor (three-in-one sensor) and also employs
    low-cost Raspberry Pi single-board computers working in the resource-saving edge
    computing paradigm, so it can contribute to the diverse selections for clients
    in low-cost smart agriculture devices and information system markets. Notwithstanding
    the benefits of edge computing in complementing classical cloud computing and
    in improving smart greenhouse mushroom cultivation, this preliminary study may
    suffer from several challenges in the future. First, the implementation costs
    when scaling this framework up to a larger scale for other crops, together with
    integrating outdoor smart agriculture systems, are questionable, since additional
    costs from new IoT devices and their installation might be included in the system.
    This may limit the commercialization of this framework for business objectives.
    Second, the improvement in mushroom production efficiency when the edge computing
    paradigm is included is not fully analyzed, suggesting the implementation of more
    detailed analyses for this problem in further studies. Addressing potential challenges
    is also related to the future outlook of this research. Besides the application
    customized for mushroom cultivation, the successful practical implementation of
    this study suggests a potential to scale up this framework for business objectives
    and other user-specific requirements (e.g., other crops) in future studies. To
    this end, in the future direction, this fundamental SAIS architecture is expected
    to be coupled with the two new layers, including the User and Administration Layers.
    The potential architecture for future the SAIS is illustrated in Figure 15, and
    a detailed description for each added layer is provided below. Figure 15. The
    potential architecture of the future SAIS for business objectives. The blue dashed
    border and arrows indicate the Forward Domain and its procedure, and the red dashed
    border and arrows indicate the Backward Domain and its procedure. User Layer:
    This layer is particularly designed for the users of the SAIS such as farmers
    or clients. From the user’s perspective, the acquisition of quality-controlled
    datasets and the intuitive visualization of these datasets with timely notifications
    if any anomalous events occur from the Cloud Layer, and the capability to control
    the devices manually in an emergency in the Device Layer, are preferable. Thus,
    several respective modules such as Data Extraction, Visualization/Notification,
    or Manual Device Controlling Modules can be considered in this layer. Since the
    users mainly work in the Device and Cloud Layers, the users can only access these
    two layers, and they do not need to access the Edge Layer to adjust any edge computing
    tasks. Administration Layer: This layer is particularly designed for the providers
    or developers (e.g., companies) of the SAIS for business goals. Unlike the users,
    the administrators aim to manage all the data flow from the Device Layer to the
    Cloud Layer, so they can access and collect data from all the layers in this system,
    which can be conducted by the Data Extraction/Management module. Moreover, the
    administrators can meet the specific user requirements by developing suitable
    solutions, and ensure the safety of the system by maintaining the security. Therefore,
    the two respective modules including Application Development and Network Security
    are considered in this layer. Not only are the above additional layers being included,
    but the Device Layer is also planning to be extended with new sensors, in the
    future system. Microcontrollers and several controlling sensors/actuators (e.g.,
    heaters, fans, humidifiers, or air ventilation…) are expected to be utilized,
    which can complement the current indoor system. Furthermore, a parallel SAIS for
    outdoor smart agriculture with relevant IoT devices (e.g., smart rain gauges,
    automated weather sensors, irrigation valves, or unmanned aerial vehicles…) can
    be considered and developed for an integrated indoor–outdoor smart agriculture
    system. 6. Conclusions Traditional Device-Cloud protocol in smart agriculture
    often suffers from the challenges of delayed system responses caused by low bandwidth
    and high latency, together with high cloud-service costs for data computation
    and storage when enormous numbers of data acquired from IoT devices are directly
    transmitted to the cloud server without processing. Novel edge computing offers
    an effective solution to deal with challenges in this traditional protocol by
    shifting partial data storage and computation capability from the cloud server
    to edge devices. Nevertheless, selecting which tasks can be implemented in edge
    computing depends on user-specific demands, suggesting the need to design a specific
    and proper Smart Agriculture Information System (SAIS) architecture that is compatible
    with single-crop requirements. Based on the nutritious and commercial benefits
    of edible mushrooms as well as the necessity of cost reduction from a business
    viewpoint, the major goal of this study is to design and implement a standard
    multilayered SAIS architecture for smart greenhouse mushroom cultivation toward
    leveraging edge computing, which can be scalable for business goals. In this designed
    SAIS, the three-layer architecture, which couples edge computing in the central
    layer to connect the Device and Cloud Layers (Device-Edge-Cloud protocol) and
    enables automation in mushroom management using an integration of the IoT environmental
    sensor (for mushroom key environmental-parameter monitoring) and surveillance
    camera (for intuitive mushroom monitoring) was employed and tested in a testbed
    site in the Republic of Korea. Via this designed SAIS, we aim to introduce the
    advanced combination environmental IoT sensors developed by Korean companies,
    a standard network flow that is possible for scalability and suitable edge computing
    tasks, with typical examples. Moreover, a potential SAIS architecture with additional
    layers for a future direction for business purposes was also suggested. The successfully
    implemented SAIS architecture indicated that our combination IoT environmental
    sensor integrated with a surveillance camera could monitor the real-time key environmental
    parameters affecting mushroom growth and intuitive mushroom information. Typical
    examples of mushroom cultivation based on the collected data revealed that several
    low-cost data pre-processing procedures including small-data storage, temporal
    resampling-based data reduction, and lightweight artificial intelligence (AI)-based
    data quality control for anomaly detection within environmental conditions, together
    with real-time AI model (YOLOv5) deployment for mushroom recognition from crop
    images, are compatible with edge computing. In contrast, high-quality BD storage
    and high-computational-cost AI model development can be implemented in the cloud.
    Moreover, integrating the Edge Layer as the center of the traditional protocol
    can significantly save network resources and operational costs by reducing unnecessary
    data sent from the device to the cloud while keeping sufficient information. Finally,
    the future improvement suggests including additional layers to meet user-specific
    demands for business objectives and the extension of the current system with new
    controlling sensors and a parallel outdoor system, toward an integrated indoor–outdoor
    smart agriculture system. Author Contributions Conceptualization, H.H.N., D.-Y.S.
    and D.-H.L.; methodology, H.H.N. and D.-Y.S.; software, H.H.N.; validation, D.-Y.S.,
    W.-S.J. and T.-Y.K.; formal analysis, T.-Y.K.; investigation, W.-S.J.; resources,
    D.-Y.S.; data curation, H.H.N.; writing—original draft preparation, H.H.N.; writing—review
    and editing, D.-Y.S., W.-S.J. and T.-Y.K.; visualization, H.H.N.; supervision,
    D.-H.L.; project administration, D.-H.L.; funding acquisition, D.-H.L. All authors
    have read and agreed to the published version of the manuscript. Funding This
    research was supported by the “Regional Innovation Strategy” (RIS) through the
    National Research Foundation of Korea (NRF), funded by the Ministry of Education
    (MOE) (2021RIS-004), and the research fund of Chungnam National University (CNU),
    Republic of Korea (2022-0694-01). The APC was also funded by the MOE through the
    NRF RIS project (2021RIS-004) and the CNU research fund (2022-0694-01). Data Availability
    Statement The raw data supporting the conclusions of this article will be made
    available by the authors on request. Conflicts of Interest Authors Hoang Hai Nguyen
    and Dae-Yun Shin were employed by the company Sejong Rain Co., Ltd. The remaining
    authors declare that the research was conducted in the absence of any commercial
    or financial relationships that could be construed as a potential conflict of
    interest. The funders had no role in the design of the study; in the collection,
    analyses, or interpretation of data; in the writing of the manuscript; or in the
    decision to publish the results. References Ba, D.M.; Ssentongo, P.; Beelman,
    R.B.; Muscat, J.; Gao, X.; Richie, J.P. Higher Mushroom Consumption Is Associated
    with Lower Risk of Cancer: A Systematic Review and Meta-Analysis of Observational
    Studies. Adv. Nutr. 2021, 12, 1691–1704. [Google Scholar] [CrossRef] [PubMed]
    Roncero-Ramos, I.; Delgado-Andrade, C. The Beneficial Role of Edible Mushrooms
    in Human Health. Curr. Opin. Food Sci. 2017, 14, 122–128. [Google Scholar] [CrossRef]
    Hamza, A.; Ghanekar, S.; Santhosh Kumar, D. Current Trends in Health-Promoting
    Potential and Biomaterial Applications of Edible Mushrooms for Human Wellness.
    Food Biosci. 2023, 51, 102290. [Google Scholar] [CrossRef] Kour, H.; Kour, D.;
    Kour, S.; Singh, S.; Jawad Hashmi, S.A.; Yadav, A.N.; Kumar, K.; Sharma, Y.P.;
    Ahluwalia, A.S. Bioactive Compounds from Mushrooms: Emerging Bioresources of Food
    and Nutraceuticals. Food Biosci. 2022, 50, 102124. [Google Scholar] [CrossRef]
    Kauserud, H.; Stige, L.C.; Vik, J.O.; Økland, R.H.; Høiland, K.; Stenseth, N.C.
    Mushroom Fruiting and Climate Change. Proc. Natl. Acad. Sci. USA 2008, 105, 3811–3814.
    [Google Scholar] [CrossRef] Procházka, P.; Soukupová, J.; Tomšík, K.; Mullen,
    K.J.; Čábelková, I. Climatic Factors Affecting Wild Mushroom Foraging in Central
    Europe. Forests 2023, 14, 382. [Google Scholar] [CrossRef] Rukhiran, M.; Sutanthavibul,
    C.; Boonsong, S.; Netinant, P. IoT-Based Mushroom Cultivation System with Solar
    Renewable Energy Integration: Assessing the Sustainable Impact of the Yield and
    Quality. Sustainability 2023, 15, 13968. [Google Scholar] [CrossRef] Sujatanagarjuna,
    A.; Kia, S.; Briechle, D.F.; Leiding, B. MushR: A Smart, Automated, and Scalable
    Indoor Harvesting System for Gourmet Mushrooms. Agriculture 2023, 13, 1533. [Google
    Scholar] [CrossRef] Elijah, O.; Rahman, T.A.; Orikumhi, I.; Leow, C.Y.; Hindia,
    M.N. An Overview of Internet of Things (IoT) and Data Analytics in Agriculture:
    Benefits and Challenges. IEEE Internet Things J. 2018, 5, 3758–3773. [Google Scholar]
    [CrossRef] Pan, J.; McElhannon, J. Future Edge Cloud and Edge Computing for Internet
    of Things Applications. IEEE Internet Things J. 2018, 5, 439–449. [Google Scholar]
    [CrossRef] Alharbi, H.A.; Aldossary, M. Energy-Efficient Edge-Fog-Cloud Architecture
    for IoT-Based Smart Agriculture Environment. IEEE Access 2021, 9, 110480–110492.
    [Google Scholar] [CrossRef] Singh, R.; Gill, S.S. Edge AI: A Survey. Internet
    Things Cyber-Phys. Syst. 2023, 3, 71–92. [Google Scholar] [CrossRef] Islam, M.A.;
    Islam, M.A.; Miah, M.S.U.; Bhowmik, A. An Automated Monitoring and Environmental
    Control System for Laboratory-Scale Cultivation of Oyster Mushrooms Using the
    Internet of Agricultural Thing (IoAT). In Proceedings of the 2nd International
    Conference on Computing Advancements, Dhaka, Bangladesh, 10–12 March 2022; pp.
    207–212. [Google Scholar] [CrossRef] Das, R.; Inuwa, M.M. A Review on Fog Computing:
    Issues, Characteristics, Challenges, and Potential Applications. Telemat. Inform.
    Rep. 2023, 10, 100049. [Google Scholar] [CrossRef] Shi, W.; Cao, J.; Zhang, Q.;
    Li, Y.; Xu, L. Edge Computing: Vision and Challenges. IEEE Internet Things J.
    2016, 3, 637–646. [Google Scholar] [CrossRef] Zhang, X.; Cao, Z.; Dong, W. Overview
    of Edge Computing in the Agricultural Internet of Things: Key Technologies, Applications,
    Challenges. IEEE Access 2020, 8, 141748–141761. [Google Scholar] [CrossRef] Lin,
    Y.B.; Chen, W.E.; Chang, T.C.Y. Moving from Cloud to Fog/Edge: The Smart Agriculture
    Experience. IEEE Commun. Mag. 2023, 61, 86–92. [Google Scholar] [CrossRef] da
    Costa Bezerra, S.F.; Filho, A.S.M.; Delicato, F.C.; da Rocha, A.R. Article Processing
    Complex Events in Fog-Based Internet of Things Systems for Smart Agriculture.
    Sensors 2021, 21, 7226. [Google Scholar] [CrossRef] Mansouri, Y.; Babar, M.A.
    A Review of Edge Computing: Features and Resource Virtualization. J. Parallel
    Distrib. Comput. 2021, 150, 155–183. [Google Scholar] [CrossRef] Köksal; Tekinerdogan,
    B. Architecture Design Approach for IoT-Based Farm Management Information Systems.
    Precis. Agric. 2019, 20, 926–958. [Google Scholar] [CrossRef] Murakami, E.; Saraiva,
    A.M.; Ribeiro, L.C.M.; Cugnasca, C.E.; Hirakawa, A.R.; Correa, P.L.P. An Infrastructure
    for the Development of Distributed Service-Oriented Information Systems for Precision
    Agriculture. Comput. Electron. Agric. 2007, 58, 37–48. [Google Scholar] [CrossRef]
    Strobel, G. Farming in the Era of Internet of Things: An Information System Architecture
    for Smart Farming. WI2020 Community Tracks 2020, 208–223. [Google Scholar] [CrossRef]
    Fountas, S.; Carli, G.; Sørensen, C.G.; Tsiropoulos, Z.; Cavalaris, C.; Vatsanidou,
    A.; Liakos, B.; Canavari, M.; Wiebensohn, J.; Tisserye, B. Farm Management Information
    Systems: Current Situation and Future Perspectives. Comput. Electron. Agric. 2015,
    115, 40–50. [Google Scholar] [CrossRef] Sørensen, C.G.; Fountas, S.; Nash, E.;
    Pesonen, L.; Bochtis, D.; Pedersen, S.M.; Basso, B.; Blackmore, S.B. Conceptual
    Model of a Future Farm Management Information System. Comput. Electron. Agric.
    2010, 72, 37–47. [Google Scholar] [CrossRef] Villa-Henriksen, A.; Edwards, G.T.C.;
    Pesonen, L.A.; Green, O.; Sørensen, C.A.G. Internet of Things in Arable Farming:
    Implementation, Applications, Challenges and Potential. Biosyst. Eng. 2020, 191,
    60–84. [Google Scholar] [CrossRef] Winter, R.; Fischer, R. Essential Layers, Artifacts,
    and Dependencies of Enterprise Architecture. In Proceedings of the 2006 10th IEEE
    International Enterprise Distributed Object Computing Conference Workshops (EDOCW’06),
    Hong Kong, China, 16–20 October 2006; pp. 30–38. [Google Scholar] [CrossRef] Li,
    X.H.; Cheng, X.; Yan, K.; Gong, P. A Monitoring System for Vegetable Greenhouses
    Based on a Wireless Sensor Network. Sensors 2010, 10, 8963–8980. [Google Scholar]
    [CrossRef] [PubMed] Moysiadis, V.; Karaiskou, C.; Kokkonis, G.; Moscholios, I.D.;
    Sarigiannidis, P. A System Architecture for Smart Farming on Mushroom Cultivation.
    In Proceedings of the 2022 5th World Symposium on Communication Engineering (WSCE),
    Nagoya, Japan, 16–18 September 2022; pp. 89–94. [Google Scholar] [CrossRef] Ferrández-Pastor,
    F.J.; García-Chamizo, J.M.; Nieto-Hidalgo, M.; Mora-Pascual, J.; Mora-Martínez,
    J. Developing Ubiquitous Sensor Network Platform Using Internet of Things: Application
    in Precision Agriculture. Sensors 2016, 16, 1141. [Google Scholar] [CrossRef]
    [PubMed] Carpio, F.; Jukan, A.; Sanchez, A.I.M.; Amla, N.; Kemper, N. Beyond Production
    Indicators: A Novel Smart Farming Application and System for Animal Welfare. In
    Proceedings of the Fourth International Conference on Animal-Computer Interaction,
    Milton Keynes, UK, 21–23 November 2017; pp. 1–11. [Google Scholar] [CrossRef]
    Verma, S.; Gala, R.; Madhavan, S.; Burkule, S.; Chauhan, S.; Prakash, C. An Internet
    of Things (IoT) Architecture for Smart Agriculture. In Proceedings of the 2018
    Fourth International Conference on Computing Communication Control and Automation,
    Pune, India, 16–18 August 2018; pp. 1–4. [Google Scholar] [CrossRef] Zamora-Izquierdo,
    M.A.; Santa, J.; Martínez, J.A.; Martínez, V.; Skarmeta, A.F. Smart Farming IoT
    Platform Based on Edge and Cloud Computing. Biosyst. Eng. 2019, 177, 4–17. [Google
    Scholar] [CrossRef] Ray, P.P. Internet of Things for Smart Agriculture: Technologies,
    Practices and Future Direction. J. Ambient Intell. Smart Environ. 2017, 9, 395–420.
    [Google Scholar] [CrossRef] Khan, F.A.; Abubakar, A.; Mahmoud, M.; Al-Khasawneh,
    M.A.; Alarood, A.A. Cotton Crop Cultivation Oriented Semantic Framework Based
    on IoT Smart Farming Application. Int. J. Eng. Adv. Technol. 2019, 8, 480–484.
    [Google Scholar] Ramli, M.I.; Ariffin, M.A.M.; Zainol, Z.; Amin, M.N.M.; Hirawan,
    D.; Sumitra, I.D.; Jamil, N. Design of a Smart Portable Farming Kit for Indoor
    Cultivation Using the Raspberry Pi Platform. Pertanika J. Sci. Technol. 2023,
    31, 1731–1754. [Google Scholar] [CrossRef] Chen, L.; Qian, L.; Zhang, X.; Li,
    J.; Zhang, Z.; Chen, X. Research Progress on Indoor Environment of Mushroom Factory.
    Int. J. Agric. Biol. Eng. 2022, 15, 25–32. [Google Scholar] [CrossRef] Suresh,
    M.; Srinivasan, M.; Gowri Shankar, S.; Karthikeyan, D.; Nakhul, V.; Naveen Kumar,
    A.; Sundar, S.; Maniraj, P. Monitoring and Automatic Control of Various Parameters
    for Mushroom Farming. IOP Conf. Ser. Mater. Sci. Eng. 2021, 1055, 012011. [Google
    Scholar] [CrossRef] Yin, H.; Yi, W.; Hu, D. Computer Vision and Machine Learning
    Applied in the Mushroom Industry: A Critical Review. Comput. Electron. Agric.
    2022, 198, 107015. [Google Scholar] [CrossRef] Kassim, M.R.M.; Harun, A.N.; Yusoff,
    I.M.; Mat, I.; Kuen, C.P.; Rahmad, N. Applications of Wireless Sensor Networks
    in Shiitake Mushroom Cultivation. In Proceedings of the 2017 Eleventh International
    Conference on Sensing Technology, Sydney, NSW, Australia, 4–6 December 2017; pp.
    1–6. [Google Scholar] [CrossRef] Mohammed, M.F.; Azmi, A.; Zakaria, Z.; Tajuddin,
    M.F.N.; Isa, Z.M.; Azmi, S.A. IoT Based Monitoring and Environment Control System
    for Indoor Cultivation of Oyster Mushroom. J. Phys. Conf. Ser. 2018, 1019, 012053.
    [Google Scholar] [CrossRef] Dipali, D.; Subramanian, T.; Kumaran, G.S. A Smart
    Oyster Mushroom Cultivation Using Automatic Fuzzy Logic Controller. J. Discret.
    Math. Sci. Cryptogr. 2023, 26, 601–615. [Google Scholar] [CrossRef] Dong, J.;
    Zheng, L. Quality Classification of Enoki Mushroom Caps Based on CNN. In Proceedings
    of the 2019 IEEE 4th International Conference on Image, Vision and Computing,
    Xiamen, China, 5–7 July 2019; pp. 450–454. [Google Scholar] [CrossRef] Lu, C.P.;
    Liaw, J.J. A Novel Image Measurement Algorithm for Common Mushroom Caps Based
    on Convolutional Neural Network. Comput. Electron. Agric. 2020, 171, 105336. [Google
    Scholar] [CrossRef] Wei, B.; Zhang, Y.; Pu, Y.; Sun, Y.; Zhang, S.; Lin, H.; Zeng,
    C.; Zhao, Y.; Wang, K.; Chen, Z. Recursive-YOLOv5 Network for Edible Mushroom
    Detection in Scenes with Vertical Stick Placement. IEEE Access 2022, 10, 40093–40108.
    [Google Scholar] [CrossRef] Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios,
    I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine
    Learning. Agriculture 2023, 13, 223. [Google Scholar] [CrossRef] Aguirre, L.;
    Frias, J.M.; Barry-Ryan, C.; Grogan, H. Modelling Browning and Brown Spotting
    of Mushrooms (Agaricus bisporus) Stored in Controlled Environmental Conditions
    Using Image Analysis. J. Food Eng. 2009, 91, 280–286. [Google Scholar] [CrossRef]
    Barauskas, R.; Kriščiūnas, A.; Čalnerytė, D.; Pilipavičius, P.; Fyleris, T.; Daniulaitis,
    V.; Mikalauskis, R. Approach of AI-Based Automatic Climate Control in White Button
    Mushroom Growing Hall. Agriculture 2022, 12, 1921. [Google Scholar] [CrossRef]
    Rahman, H.; Faruq, M.O.; Abdul Hai, T.B.; Rahman, W.; Hossain, M.M.; Hasan, M.;
    Islam, S.; Moinuddin, M.; Islam, M.T.; Azad, M.M. IoT Enabled Mushroom Farm Automation
    with Machine Learning to Classify Toxic Mushrooms in Bangladesh. J. Agric. Food
    Res. 2022, 7, 100267. [Google Scholar] [CrossRef] Chong, J.L.; Chew, K.W.; Peter,
    A.P.; Ting, H.Y.; Show, P.L. Internet of Things (IoT)-Based Environmental Monitoring
    and Control System for Home-Based Mushroom Cultivation. Biosensors 2023, 13, 98.
    [Google Scholar] [CrossRef] [PubMed] Goldstein, M.; Uchida, S. A Comparative Evaluation
    of Unsupervised Anomaly Detection Algorithms for Multivariate Data. PLoS ONE 2016,
    11, e0152173. [Google Scholar] [CrossRef] Redmon, J.; Divvala, S.; Girshick, R.;
    Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas,
    NV, USA, 27–30 June 2016; pp. 779–788. [Google Scholar] [CrossRef]           Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2024 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Nguyen, H.H.; Shin, D.-Y.; Jung, W.-S.; Kim,
    T.-Y.; Lee, D.-H. An Integrated IoT Sensor-Camera System toward Leveraging Edge
    Computing for Smart Greenhouse Mushroom Cultivation. Agriculture 2024, 14, 489.
    https://doi.org/10.3390/agriculture14030489 AMA Style Nguyen HH, Shin D-Y, Jung
    W-S, Kim T-Y, Lee D-H. An Integrated IoT Sensor-Camera System toward Leveraging
    Edge Computing for Smart Greenhouse Mushroom Cultivation. Agriculture. 2024; 14(3):489.
    https://doi.org/10.3390/agriculture14030489 Chicago/Turabian Style Nguyen, Hoang
    Hai, Dae-Yun Shin, Woo-Sung Jung, Tae-Yeol Kim, and Dae-Hyun Lee. 2024. \"An Integrated
    IoT Sensor-Camera System toward Leveraging Edge Computing for Smart Greenhouse
    Mushroom Cultivation\" Agriculture 14, no. 3: 489. https://doi.org/10.3390/agriculture14030489
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 18. Mar 20. Mar 22. Mar 24.
    Mar 26. Mar 28. Mar 30. Mar 1. Apr 3. Apr 5. Apr 0 200 400 600 800 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Agriculture, EISSN 2077-0472, Published by MDPI RSS
    Content Alert Further Information Article Processing Charges Pay an Invoice Open
    Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For
    Editors For Librarians For Publishers For Societies For Conference Organizers
    MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia
    JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive
    issue release notifications and newsletters from MDPI journals Select options
    Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer
    Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Agriculture (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An Integrated IoT Sensor-Camera System toward Leveraging Edge Computing for
    Smart Greenhouse Mushroom Cultivation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Javadpour A.
  - Sangaiah A.K.
  - Zhang W.
  - Vidyarthi A.
  - Ahmadi H.R.
  citation_count: '0'
  description: This study presents an environmentally friendly mechanism for task
    distribution designed explicitly for blockchain Proof of Authority (POA) consensus.
    This approach facilitates the selection of virtual machines for tasks such as
    data processing, transaction verification, and adding new blocks to the blockchain.
    Given the current lack of effective methods for integrating POA blockchain into
    the Cloud Industrial Internet of Things (CIIoT) due to their inefficiency and
    low throughput, we propose a novel algorithm that employs the Dynamic Voltage
    and Frequency Scaling (DVFS) technique, replacing the periodic transaction authentication
    process among validator candidates. Managing computer power consumption becomes
    a critical concern, especially within the Internet of Things ecosystem, where
    device power is constrained, and transaction scalability is crucial. Virtual machines
    must validate transactions (tasks) within specific time frames and deadlines.
    The DVFS technique efficiently reduces power consumption by intelligently scheduling
    and allocating tasks to virtual machines. Furthermore, we leverage artificial
    intelligence and neural networks to match tasks with suitable virtual machines.
    The simulation results demonstrate that our proposed approach harnesses migration
    and DVFS strategies to optimize virtual machine utilization, resulting in decreased
    energy and power consumption compared to non-DVFS methods. This achievement marks
    a significant stride towards seamlessly integrating blockchain and IoT, establishing
    an ecologically sustainable network. Our approach boasts additional benefits,
    including decentralization, enhanced data quality, and heightened security. We
    analyze simulation runtime and energy consumption in a comprehensive evaluation
    against existing techniques such as WPEG, IRMBBC, and BEMEC. The findings underscore
    the efficiency of our technique (LBDVFSb) across both criteria.
  doi: 10.1007/s10723-024-09751-9
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Grid Computing Article
    Decentralized AI-Based Task Distribution on Blockchain for Cloud Industrial Internet
    of Things Research Published: 24 February 2024 Volume 22, article number 33, (2024)
    Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Journal of Grid Computing Aims and scope Submit manuscript Amir Javadpour, Arun
    Kumar Sangaiah, Weizhe Zhang, Ankit Vidyarthi & HamidReza Ahmadi  83 Accesses
    Explore all metrics Abstract This study presents an environmentally friendly mechanism
    for task distribution designed explicitly for blockchain Proof of Authority (POA)
    consensus. This approach facilitates the selection of virtual machines for tasks
    such as data processing, transaction verification, and adding new blocks to the
    blockchain. Given the current lack of effective methods for integrating POA blockchain
    into the Cloud Industrial Internet of Things (CIIoT) due to their inefficiency
    and low throughput, we propose a novel algorithm that employs the Dynamic Voltage
    and Frequency Scaling (DVFS) technique, replacing the periodic transaction authentication
    process among validator candidates. Managing computer power consumption becomes
    a critical concern, especially within the Internet of Things ecosystem, where
    device power is constrained, and transaction scalability is crucial. Virtual machines
    must validate transactions (tasks) within specific time frames and deadlines.
    The DVFS technique efficiently reduces power consumption by intelligently scheduling
    and allocating tasks to virtual machines. Furthermore, we leverage artificial
    intelligence and neural networks to match tasks with suitable virtual machines.
    The simulation results demonstrate that our proposed approach harnesses migration
    and DVFS strategies to optimize virtual machine utilization, resulting in decreased
    energy and power consumption compared to non-DVFS methods. This achievement marks
    a significant stride towards seamlessly integrating blockchain and IoT, establishing
    an ecologically sustainable network. Our approach boasts additional benefits,
    including decentralization, enhanced data quality, and heightened security. We
    analyze simulation runtime and energy consumption in a comprehensive evaluation
    against existing techniques such as WPEG, IRMBBC, and BEMEC. The findings underscore
    the efficiency of our technique (LBDVFSb) across both criteria. Article PDF Similar
    content being viewed by others Information technologies of 21st century and their
    impact on the society Article 16 August 2019 Blockchain for decentralization of
    internet: prospects, trends, and challenges Article Open access 15 May 2021 The
    Future of E-Commerce Systems: 2030 and Beyond Chapter © 2021 Data Availability
    Data available on request from the authors. The data that support the findings
    of this study are available from the corresponding author, [author initials],
    upon reasonable request. References Toor, A., et al.: Energy and performance aware
    fog computing: A case of DVFS and green renewable energy. Futur. Gener. Comput.
    Syst. 101, 1112–1121 (2019) Article   Google Scholar   Tang, Z., Qi, L., Cheng,
    Z., Li, K., Khan, S.U., Li, K.: An energy-efficient task scheduling algorithm
    in DVFS-enabled cloud environment. J. Grid Comput. 14, 55–74 (2016) Article   Google
    Scholar   Javadpour, A., Nafei, A., Ja’fari, F., Pinto, P., Zhang, W., Sangaiah,
    A.K.: An intelligent energy-efficient approach for managing IoE tasks in cloud
    platforms. J. Ambient Intell. Humaniz. Comput. 14(4), 3963–3979 (2023) Article   Google
    Scholar   Hosseini Shirvani, M., Rahmani, A.M., Sahafi, A.: A survey study on
    virtual machine migration and server consolidation techniques in DVFS-enabled
    cloud datacenter: Taxonomy and challenges. J. King Saud Univ. - Comput. Inf. Sci.
    32(3), 267–286 (2020) Google Scholar   Jiang, C., et al.: Energy aware edge computing:
    A survey. Comput. Commun. 151, 556–580 (2020) Article   Google Scholar   Xu, C.,
    Wang, K., Guo, M.: Intelligent resource management in blockchain-based cloud datacenters.
    IEEE Cloud Comput. 4(6), 50–59 (2017) Article   Google Scholar   Pirozmand, P.,
    Javadpour, A., Nazarian, H., Pinto, P., Mirkamali, S., Ja’fari, F.: GSAGA: A hybrid
    algorithm for task scheduling in cloud infrastructure. J. Supercomput. 78(15),
    17423–17449 (2022) Article   Google Scholar   Yousuf, R., Jeelani, Z., Khan, D.A.,
    Bhat, O., Teli, T.A.: Consensus Algorithms in Blockchain-Based Cryptocurrencies,
    in 2021 International Conference on Advances in Electrical, Computing, Communication
    and Sustainable Technologies (ICAECT). 1–6. (2021) Wu, Y., Song, P., Wang, F.:
    Hybrid consensus algorithm optimization: A mathematical method based on POS and
    PBFT and its application in blockchain. Math. Probl. Eng. 2020, (2020) Lone, A.H.,
    Mir, R.N.: Reputation Driven Dynamic Access Control Framework for IoT atop PoA
    Ethereum Blockchain. IACR Cryptol. ePrint Arch. 2020, 566 (2020) Google Scholar   Javadpour,
    A., Wang, G., Rezaei, S., Chend, S.: Power Curtailment in Cloud Environment Utilising
    Load Balancing Machine Allocation, in 2018 IEEE SmartWorld, Ubiquitous Intelligence
    Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud
    Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI).
    1364–1370 (2018) Xu, R., Chen, Y., Blasch, E.: Decentralized Access Control for
    IoT Based on Blockchain and Smart Contract. Model. Des. Secur. Internet Things.
    505–528 (2020) Oktian, Y.E., Lee, S.-G., Lee, H.J.: Hierarchical multi-blockchain
    architecture for scalable internet of things environment. Electronics 9(6), 1050
    (2020) Article   Google Scholar   Han, D., Zhang, C., Ping, J., Yan, Z.: Smart
    contract architecture for decentralized energy trading and management based on
    blockchains. Energy 199, 117417 (2020) Article   Google Scholar   Atlam, H.F.,
    Azad, M.A., Alzahrani, A.G., Wills, G.: A Review of Blockchain in Internet of
    Things and AI. Big Data Cogn. Comput. 4(4), 28 (2020) Article   Google Scholar   Panarello,
    A., Tapas, N., Merlino, G., Longo, F., Puliafito, A.: Blockchain and IoT Integration:
    A Systematic Survey. Sensors (Basel) 18(8), 2575 (2018) Article   ADS   PubMed   Google
    Scholar   Yazdinejad, A., Parizi, R.M., Dehghantanha, A., Zhang, Q., Choo, K.-K.R.:
    An Energy-Efficient SDN Controller Architecture for IoT Networks With Blockchain-Based
    Security. IEEE Trans. Serv. Comput. 13(4), 625–638 (2020) Article   Google Scholar   Yazdinejad,
    A., Parizi, R.M., Srivastava, G., Dehghantanha, A., Choo, K.-K.R.: Energy Efficient
    Decentralized Authentication in Internet of Underwater Things Using Blockchain,
    in 2019 IEEE Globecom Workshops (GC Wkshps). 1–6 (2019) Javadpour, A., AliPour,
    F.S., Sangaiah, A.K., Zhang, W., Ja’far, F., Singh, A.: An IoE blockchain-based
    network knowledge management model for resilient disaster frameworks. J. Innov.
    Knowl. 8(3), (2023) Yu, R., Zhang, X., Zhang, M.: Smart Home Security Analysis
    System Based on The Internet of Things, in 2021 IEEE 2nd International Conference
    on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE).
    596–599 (2021) Karunarathne, S.M., Saxena, N., Khan, M.K.: Security and Privacy
    in IoT Smart Healthcare. IEEE Internet Comput. (2021) Lin, X., Wu, J., Bashir,
    A.K., Li, J., Yang, W., Piran, J.: Blockchain-Based Incentive Energy-Knowledge
    Trading in IoT: Joint Power Transfer and AI Design. IEEE Internet Things J. 4662(c),
    1–1 (2020) Google Scholar   Barzegar, B., Motameni, H., Movaghar, A.: EATSDCD:
    A green energy-aware scheduling algorithm for parallel task-based application
    using clustering, duplication and DVFS technique in cloud datacenters. J. Intell.
    \\& Fuzzy Syst. 36(6), 5135–5152 (2019) Article   Google Scholar   Yazdinejad,
    A., Parizi, R.M., Dehghantanha, A., Choo, K.-K.R.: Blockchain-Enabled Authentication
    Handover With Efficient Privacy Protection in SDN-Based 5G Networks. IEEE Trans.
    Netw. Sci. Eng. 8(2), 1120–1132 (2021) Article   Google Scholar   Download references
    Funding This work was supported in part by the Joint Funds of the National Natural
    Science Foundation of China (Grant No. U22A2036), the Shenzhen Colleges and Universities
    Stable Support Program No.GXWD20220817124251002, Guangdong Provincial Key Laboratory
    of Novel Security Intelligence Technologies (2022B1212010005). Author information
    Authors and Affiliations School of Computer Science and Technology, Harbin Institute
    of Technology, Shenzhen, 518055, Guangdong, China Amir Javadpour & Weizhe Zhang
    International Graduate Institute of Artificial Intelligence, National Yunlin University
    of Science and Technology, Douliu, Taiwan Arun Kumar Sangaiah Jaypee Institute
    of Information Technology, Noida, India Ankit Vidyarthi Faculty of New Sciences
    and Technologies, University of Tehran, Tehran, Iran HamidReza Ahmadi Department
    of New Networks, Peng Cheng Laboratory, Shenzhen, 518055, Guangdong, China Weizhe
    Zhang Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies,
    Shenzhen, 518055, Guangdong, China Weizhe Zhang Contributions A.B. and C.D. wrote
    the main manuscript text and all of them prepared figures and etc. All authors
    reviewed the manuscript. Corresponding authors Correspondence to Amir Javadpour
    or Weizhe Zhang. Ethics declarations Ethical Approval Hereby, I am Amir Javadpour
    consciously assure that for the manuscript “Decentralized AI‑Based Task Distribution
    on Blockchain for Cloud Industrial Internet of Things” the following is fulfilled:
    1) This material is the authors'' own original work, which has not been previously
    published elsewhere. 2) The paper is not currently being considered for publication
    elsewhere. 3) The paper reflects the authors'' own research and analysis in a
    truthful and complete manner. 4) The paper properly credits the meaningful contributions
    of co-authors and co-researchers. 5) The results are appropriately placed in the
    context of prior and existing research. 6) All sources used are properly disclosed
    (correct citation). Literally copying of text must be indicated as such by using
    quotation marks and giving proper reference. 7) All authors have been personally
    and actively involved in substantial work leading to the paper, and will take
    public responsibility for its content. I agree with the above statements and declare
    that this submission follows the policies as outlined in the Guide for Authors
    and in the Ethical Statement. Date: 01–06-2023. Corresponding author’s signature:
    Amir Javadpour. Competing Interests The authors declare no competing interests.
    Additional information Publisher''s Note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    Appendix 1 Appendix 1 Details of Algorithm 1, Including Elaboration of Each Step.
    Algorithm 2: IoT Task Processing. Rights and permissions Springer Nature or its
    licensor (e.g. a society or other partner) holds exclusive rights to this article
    under a publishing agreement with the author(s) or other rightsholder(s); author
    self-archiving of the accepted manuscript version of this article is solely governed
    by the terms of such publishing agreement and applicable law. Reprints and permissions
    About this article Cite this article Javadpour, A., Sangaiah, A.K., Zhang, W.
    et al. Decentralized AI-Based Task Distribution on Blockchain for Cloud Industrial
    Internet of Things. J Grid Computing 22, 33 (2024). https://doi.org/10.1007/s10723-024-09751-9
    Download citation Received 16 June 2023 Accepted 23 January 2024 Published 24
    February 2024 DOI https://doi.org/10.1007/s10723-024-09751-9 Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Blockchain Improving resources Internet of Things Decentralized DVFS
    Industrial Internet of Things Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Associated Content Part of a collection: Artificial Intelligence
    (AI)-enabled Blockchain for the Edge of Things (EoT) Sections References Abstract
    Article PDF Data Availability References Funding Author information Ethics declarations
    Additional information Appendix 1 Rights and permissions About this article Advertisement
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Grid Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Decentralized AI-Based Task Distribution on Blockchain for Cloud Industrial
    Internet of Things
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chhetri K.B.
  citation_count: '0'
  description: To ensure food safety and uphold high standards, the food business
    must overcome significant obstacles. In recent years, promising answers to these
    issues have emerged in the form of artificial intelligence (AI) and machine learning
    (ML). This thorough review paper analyses the various uses of AI and ML in food
    quality management and safety evaluation, offering insightful information for
    academics, business people and legislators. The evaluation highlights the value
    of food quality assessment and control in consideration of growing consumer demand
    and regulatory scrutiny. The powerful capabilities of AI and ML are touted as
    having the potential to revolutionize these procedures. This study illustrates
    the numerous uses of AI and ML in food quality management through an in-depth
    exploration of these technologies. Defect detection and consistency evaluation
    are made possible using computer vision techniques, and intelligent data analysis
    and real-time monitoring are made possible by natural language processing. Deep
    learning techniques also provide reliable approaches for pattern recognition and
    anomaly detection, thus maintaining consistency in quality across manufacturing
    batches. This review emphasizes the efficiency of AI and ML in detecting dangerous
    microorganisms, allergies and chemical pollutants with regard to food safety evaluation.
    Consumer health risks are reduced because of the rapid identification of safety
    issues made possible by integrating data from diverse sources, including sensors
    and IoT devices. The assessment discusses issues and restrictions related to the
    application of AI and ML in the food business while appreciating the impressive
    progress that has been made. Continuous efforts are being made to improve model
    interpretability and reduce biases, which calls for careful evaluation of data
    quality, quantity and privacy issues. To assure compliance with food safety norms
    and regulations, the article also covers regulatory approval and validation of
    AI-generated outcomes. The revolutionary potential of AI and ML in raising food
    industry standards and preserving public health is highlighted on future perspectives
    that concentrate on new trends and potential innovations. This comprehensive review
    reveals that the integration of AI and ML technologies in food quality control
    and safety not only enhances efficiency, minimizes risks and ensures regulatory
    compliance but also heralds a new era of personalized nutrition, autonomous monitoring
    and global collaboration, signifying a transformative paradigm in the food industry.
  doi: 10.1007/s12393-023-09363-1
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Food Engineering Reviews Article
    Applications of Artificial Intelligence and Machine Learning in Food Quality Control
    and Safety Assessment Published: 22 December 2023 Volume 16, pages 1–21, (2024)
    Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Food Engineering Reviews Aims and scope Submit manuscript Krishna Bahadur Chhetri  568
    Accesses Explore all metrics Abstract To ensure food safety and uphold high standards,
    the food business must overcome significant obstacles. In recent years, promising
    answers to these issues have emerged in the form of artificial intelligence (AI)
    and machine learning (ML). This thorough review paper analyses the various uses
    of AI and ML in food quality management and safety evaluation, offering insightful
    information for academics, business people and legislators. The evaluation highlights
    the value of food quality assessment and control in consideration of growing consumer
    demand and regulatory scrutiny. The powerful capabilities of AI and ML are touted
    as having the potential to revolutionize these procedures. This study illustrates
    the numerous uses of AI and ML in food quality management through an in-depth
    exploration of these technologies. Defect detection and consistency evaluation
    are made possible using computer vision techniques, and intelligent data analysis
    and real-time monitoring are made possible by natural language processing. Deep
    learning techniques also provide reliable approaches for pattern recognition and
    anomaly detection, thus maintaining consistency in quality across manufacturing
    batches. This review emphasizes the efficiency of AI and ML in detecting dangerous
    microorganisms, allergies and chemical pollutants with regard to food safety evaluation.
    Consumer health risks are reduced because of the rapid identification of safety
    issues made possible by integrating data from diverse sources, including sensors
    and IoT devices. The assessment discusses issues and restrictions related to the
    application of AI and ML in the food business while appreciating the impressive
    progress that has been made. Continuous efforts are being made to improve model
    interpretability and reduce biases, which calls for careful evaluation of data
    quality, quantity and privacy issues. To assure compliance with food safety norms
    and regulations, the article also covers regulatory approval and validation of
    AI-generated outcomes. The revolutionary potential of AI and ML in raising food
    industry standards and preserving public health is highlighted on future perspectives
    that concentrate on new trends and potential innovations. This comprehensive review
    reveals that the integration of AI and ML technologies in food quality control
    and safety not only enhances efficiency, minimizes risks and ensures regulatory
    compliance but also heralds a new era of personalized nutrition, autonomous monitoring
    and global collaboration, signifying a transformative paradigm in the food industry.
    Similar content being viewed by others Artificial Intelligence and Deep Learning-Based
    Agri and Food Quality and Safety Detection System Chapter © 2023 Impact and prospect
    of the fourth industrial revolution in food safety: Mini-review Article 20 February
    2022 Food Adulteration Detection using Artificial Intelligence: A Systematic Review
    Article 15 June 2021 Introduction The food industry at a global level is encountering
    an escalation in challenges associated with ensuring food safety, sustainability
    and quality due to the increasing demands of consumers and environmental changes.
    To combat these challenges, industry professionals and researchers are resorting
    to artificial intelligence (AI) and machine learning (ML) techniques as effective
    instruments to transform food safety assessment and quality control. In this paper,
    an assessment is conducted to explore the varied applications of AI and ML in
    the food industry, with a particular emphasis on their potential to mitigate crucial
    challenges and enhance the overall efficiency. The requirement for advanced technologies
    in food safety assessment and quality control is apparent, and AI and ML provide
    encouraging solutions. The conflation of hyperspectral imaging and AI methodologies
    in a symbiotic manner, as illustrated by the innovative research undertaken by
    Rady et al. [1], carried significant consequences for the food processing sector.
    This union facilitates the swift and accurate detection of pest infestations in
    apples, which fortifies pre-emptive measures and decreases crop losses. Furthermore,
    the AI-powered analysis extends to quality management, permitting the real-time
    evaluation of factors such as ripeness and pollutants. By refining procedures
    and ensuring the authenticity of products, this combination redefines the benchmarks
    for food safety and refines the general efficacy of the industry. The food processing
    industry is on the cusp of a transformative period, owing to the profound integration
    of AI and ML techniques. These advancements have significantly altered the crop
    disease and pest detection landscape with remarkable precision, as evidenced by
    the pioneering research of Boyd and Sun [2]. Their groundbreaking expert system,
    which diagnoses potato ailments, is a pioneering achievement that demonstrates
    the expeditious and precise assessments that are now possible, ushering in a new
    era of rapid interventions and reduced agricultural losses. However, the realm
    of AI and ML surpasses the confines of immediate quality control. Instead, it
    assumes a mantle of paramount significance in navigating the intricate and multifaceted
    terrain of food security. The astute application of fuzzy systems, as demonstrated
    by Peixoto et al. [3], offers a glimpse into this potential. Their ingenious dynamic
    regulation of soybean aphids exemplifies the strategic use of AI, resulting not
    only in augmented crop yields but also in fortifying the very foundation of the
    global food supply chain. Inextricably linked to the evolving tapestry of the
    food industry, the ascendancy of AI and ML technologies in food quality control
    and safety assessment is inexorable. As we traverse the dynamic contours of this
    landscape, it becomes increasingly evident that this review serves not merely
    as an exposition but also as a clarion call, illuminating the path to an augmented
    future where AI and ML stand as sentinels of excellence within the realms of food
    processing. Materials and Methods The synthesis of this all-encompassing review
    is supported by a rigorous methodology aimed at extracting valuable insights from
    the most authoritative sources currently available. A meticulous and systematic
    search strategy was executed, utilizing well-known databases such as PubMed, IEEE
    Xplore and ScienceDirect. This review covers literature published until 2022,
    ensuring a comprehensive and up-to-date coverage of the subject. In accordance
    with strict inclusion criteria, the selection process gave priority to articles
    that have undergone peer review and notable conference proceedings. A thorough
    three-step screening process, involving the evaluation of titles, abstracts and
    full texts, was implemented to ensure the accuracy and relevance of the studies
    included. The categorization framework focused on the principal applications of
    AI and ML in the context of food safety, specifically computer vision, IoT-enabled
    sensors, blockchain integration and predictive modelling. A systematic analysis
    of the benefits and limitations associated with each application supports a refined
    distillation of insights, contributing to a comprehensive understanding of the
    subject matter. Food Quality Control and Safety Assessment Through AI and ML Techniques
    AI and ML techniques have revolutionized the field of food quality control by
    presenting inventive solutions to increase the consistency and safety of food
    items [4, 5]. Among the key techniques used in the industry are the following:
    1. Computer vision: In contemporary food process engineering, the combination
    of computer vision technology and AI has emerged as a pioneering development [5].
    This combination is transforming the food industry by enabling meticulous and
    automated visual inspections. AI-powered computer vision systems excel at scrutinizing
    food products with precision and speed through image and video analysis. These
    systems are indispensable for crucial tasks such as grading, sorting and quality
    assessment. By rapidly identifying discrepancies from quality standards, they
    ensure that only products satisfying stringent criteria are delivered to consumers.
    The impact of this technology extends across diverse sectors within food processing.
    In sorting, computer vision systems equipped with AI discern subtle variations
    in colour, size and shape. This enables precise product categorization, which
    is particularly essential in fruit and vegetable sorting, where uniformity is
    vital for quality and competitiveness. Furthermore, the implementation of computer
    vision addresses food safety concerns by promptly identifying potential contaminants
    or pathogens. Rapid data analysis allows timely interventions, minimizing risks
    to food safety and potential recalls. In quality assessment, these systems provide
    consistent, objective evaluations of attributes that influence consumer preferences.
    By reducing human subjectivity, they enhance the standardized product evaluation,
    thus fostering consumer trust and satisfaction. The combination of AI and computer
    vision not only accelerates processes but also optimizes resource usage. Early
    identification of defects minimizes waste, leading to cost savings and heightened
    sustainability. 2. Spectroscopy and sensors: The domain of food process engineering
    is marked by the combination of AI and ML, which is particularly evident in the
    realm of spectroscopy and sensors. According to Si et al. [5], ML algorithms intricately
    analyse data sourced from spectroscopic techniques and a diverse array of sensors,
    measuring crucial attributes such as moisture content, pH levels and chemical
    composition. This data-driven analysis empowers AI to make estimations regarding
    the nutritional value, ripeness and freshness of food items, facilitating informed
    decision-making for producers concerning production and storage strategies. The
    proficiency of AIs in interpreting spectroscopic and sensor data has profound
    implications for quality control and safety assessment. By harnessing AI’s pattern
    recognition capabilities, the technology enables producers to optimize production
    processes and minimize resource wastage. In addition, real-time data streams from
    sensors align with contemporary paradigms such as Industry 4.0 and the Internet
    of things (IoT), facilitating predictive modelling for quality deviations and
    enabling timely intervention. This symbiotic blend of AI and ML in spectroscopy
    and sensors not only enhances operational efficiency but also strengthens the
    foundation of food safety and quality within the dynamic landscape of food process
    engineering. 3. Predictive modelling: Predictive modelling with convergence of
    AI and ML has significant implications for food quality and control. Si et al.
    [5] have expounded on this technique, which utilizes historical data to train
    ML models and predict potential quality issues while estimating the shelf-life
    of food products. The combination of AI and ML facilitates the monitoring of a
    comprehensive range of variables throughout the production and storage phases,
    allowing the system to anticipate the likelihood of contamination, spoilage or
    other forms of deterioration. By identifying patterns and correlations in the
    data, AI-driven predictive models empower producers to make informed decisions
    expeditiously. Subsequently, timely interventions can be implemented to preserve
    the integrity of the final product, minimize wastage and ensure that consumers
    receive safe and high-quality food items. In essence, the application of predictive
    modelling fortified by AI holds significant promise for revolutionizing food process
    engineering. By leveraging historical insights and real-time data, this approach
    enhances the industry’s ability to mitigate risks, optimize resources and ultimately
    deliver products that meet the highest standards of quality and safety. 4. IoT-enabled
    real-time monitoring: The integration of AI and ML has been emphasized by Si et
    al. [5], thus leading to the emergence of IoT devices equipped with sensors. This
    union has enabled real-time monitoring of critical parameters, thereby facilitating
    proactive quality control. With the aid of these IoT devices, data acquisition
    is performed in real time, ensuring that quality benchmarks are constantly upheld.
    In the event of any deviations, AI systems expeditiously analyse the data and
    deliver prompt notifications, thereby enabling immediate corrective measures.
    This dynamic approach not only hastens response times but also mitigates the risk
    of errors or contamination. Furthermore, AI-powered automation and robotics play
    a crucial role in expediting quality control procedures. By eliminating human
    intervention, these technologies enhance precision and minimize the potential
    for errors. The intricate handling and examination of food products are facilitated,
    ensuring uniform quality and reducing needless wastage. In essence, the convergence
    of AI, ML and IoT within food process engineering amplifies the industry’s ability
    to maintain superior quality standards. The real-time monitoring and automated
    responsiveness empower stakeholders to ensure that only products of the highest
    quality are presented to consumers, subsequently elevating trust, satisfaction
    and overall efficiency. 5. Data-driven decision-making: In the realm of food process
    engineering, the confluence between AI and ML has been underscored by Si et al.
    [5] and Vijay et al. [6], utilizing expansive datasets for data-oriented decision-making.
    This approach combines an array of data sources, including customer feedback,
    laboratory testing results and manufacturing records. AI’s capacity to process
    and scrutinize these vast datasets empowers upgraded quality control systems.
    This integration facilitates the identification of intricate trends and issues
    that may elude conventional methods. By discerning patterns and anomalies across
    multiple dimensions, AI enhances the accuracy of quality assessment and contributes
    to the proactive identification of potential challenges. The integration of AI
    and ML in food process engineering exemplifies a transformative shift towards
    data-driven decision-making. This not only enriches the comprehension of product
    quality but also fosters continuous improvement and innovation within the industry.
    6. Blockchain and AI-enabled improved traceability and transparency in the food
    supply chain: The intersection of blockchain and AI has resulted in a significant
    breakthrough in augmenting traceability and transparency throughout the food supply
    chain. The influential works of Si et al. [5] and Bestelmeyer et al. [7] underscored
    the pivotal role of this decentralized ledger in meticulously monitoring the entire
    journey of food products, spanning from their origin to consumption. By leveraging
    AI-powered analysis of blockchain data, a swift and accurate identification of
    sources is made feasible in cases of contamination or recalls. This integration
    fortifies the efficacy of traceability systems, safeguards consumer health and
    increases the efficiency of corrective measures. The fusion of blockchain and
    AI within food process engineering not only empowers supply chain stakeholders
    with real-time insights but also instils a heightened level of accountability
    and integrity. By fostering an environment of transparency, these cutting-edge
    technologies catalyse a new era in food safety and quality assurance, reinforcing
    consumer confidence and industry-wide standards. The combination of AI and ML
    methodologies has precipitated a profound metamorphosis in the realm of food quality
    control, resulting in elevated consumer satisfaction levels, reduced wastage and
    reinforced safety [4, 6]. The developing landscape holds the potential for even
    more remarkable strides in food quality management, ensuring a consistent supply
    of secure and first-rate food products. These technologies have revolutionized
    the paradigm of food process engineering, augmenting its efficacy, precision and
    dependability. The collaborative synergy between AI, ML and food science underscores
    their pivotal role in shaping the future of food quality assurance. This trajectory
    not only guarantees continual improvement of consumer experiences but also establishes
    a robust foundation for industry’s growth and resilience. The comparison of AI
    and ML techniques for food safety and quality control is described in Table 1.
    Table 1 Comparison of AI and ML techniques for food safety and quality control
    Full size table Sensor-Based AI and ML Applications for Enhancing Food Safety
    and Quality Control Data collection and analysis have been revolutionized by AI-based
    sensing technologies, which use AI algorithms to glean insightful information
    from sensor data [5, 6, 13]. These state-of-the-art sensing technologies have
    several uses in industries such as agriculture, healthcare and environmental monitoring.
    The AI and ML utilization in food industry is described in Fig. 1. AI-based sensing
    technologies are essential for maintaining the safety and integrity of food items
    across the supply chain in the field of food quality control. 1. IoT-enabled smart
    sensors: The combination of IoT devices with intelligent sensors is revolutionizing
    the landscape of food production, processing and storage [5]. These sensors are
    capable of continuously monitoring crucial factors such as temperature, humidity,
    pH levels, gas emissions and chemical compositions. The real-time data provided
    by these sensors enable the swift detection of quality and safety issues. The
    dynamic monitoring system created through this integration enhances food processing
    by maintaining optimal conditions, preventing microbial growth and averting any
    moisture-related damage. Furthermore, it assures safety by identifying abnormal
    gas emissions and monitoring chemical compositions, which allows for timely interventions
    and proactive measures [14]. The seamless partnership between IoT and intelligent
    sensors empowers the food industry to ensure precise quality control and maintain
    vigilant safety measures along the entire food supply chain. 2. Image and spectroscopy
    sensors: Si et al. [5] emphasize the combination of AI-driven imaging and spectroscopy
    sensors in the evaluation of diverse aesthetic and chemical properties of food
    items. AI-powered computer vision algorithms are proficient in meticulously scrutinizing
    images to detect defects, blemishes and extraneous substances in food products,
    thereby augmenting the quality control process in food production. Furthermore,
    spectroscopy sensors are pivotal in capturing the interaction of food items with
    light, which provides valuable insights into their nutritional value, composition
    and freshness. This sophisticated analysis facilitated by AI considerably enhances
    the precision and efficacy of food quality assessment. 3. Gas and odour sensors:
    In the domain of food processing and safety, Si et al. [5] emphasize the pivotal
    significance of gas and odour sensors based on AI. These sensors expertly detect
    volatile compounds discharged by food products, thereby allowing for the identification
    of harmful substances, spoilage and undesirable smells. By leveraging the capabilities
    of AI, these sensors can swiftly and accurately analyse sensor data. Hence, compromised
    or deteriorating products are instantly flagged, ensuring their elimination from
    the production line before they reach consumers. This proactive approach not only
    diminishes potential health hazards but also safeguards the credibility of manufacturers
    and the overall integrity of the food supply chain. The findings represent a prime
    example of how integrating AI and sensor technology strengthens food safety measures,
    resulting in improved quality control and heightened consumer trust. 4. Nanosensors:
    The work of Si et al. [5] illuminated the transformative dimension of nanotechnology
    in the realm of food processing and safety. This innovative technology allows
    for the molecular-level recognition of substances and diseases, which, in turn,
    enables AI-based nanosensors to play a pivotal role in assessing food safety.
    These nanosensors provide rapid and precise detection of contaminants, thus serving
    as an invaluable asset in safeguarding the quality of consumables. By harnessing
    the power of AI, nanosensors contribute to proactive food safety measures. Their
    exceptional sensitivity and specificity enable early identification of potential
    hazards, thereby acting as vigilant sentinels against foodborne threats. Real-time
    contamination detection facilitates swift intervention, bolstering consumer protection
    and industry integrity. The convergence of nanotechnology and AI in the realm
    of food safety signifies a revolutionary advancement. It not only enhances the
    monitoring and control of contaminants but also ushers in a new era of precision
    and reliability in the food supply chain. As a result, manufacturers can uphold
    stringent safety standards, while consumers enjoy greater confidence in the products
    they consume. 5. Blockchain integration: Si et al. [5] emphasized the transformative
    potential of integrating blockchain and AI-based sensing to enhance food processing
    and safety. This mutually beneficial partnership bolsters data security and traceability
    by establishing an immutable ledger for the entire food supply chain. By using
    blockchain to record sensor-derived data, transparency and permanence are ensured.
    Through AI-driven analysis, intricate relationships and patterns within the recorded
    data are uncovered. This dynamic synergy enhances the operational efficiency,
    mitigates risk and ensures adherence to stringent quality standards. The integration
    of blockchain and AI represents a pivotal advancement, providing real-time insights
    to stakeholders, elevating accountability and strengthening consumer confidence
    in the safety and integrity of food products. 6. Remote sensing: Si et al. [5]
    and Spanaki et al. [13] underscore the criticality of AI-powered remote sensing
    techniques in augmenting food processing and safety. These technologies, which
    use satellites and drones, allow for comprehensive monitoring of agricultural
    fields and storage facilities. By evaluating crop vitality and environmental conditions
    and identifying anomalies such as pest outbreaks and temperature fluctuations,
    they contribute to the preservation of food quality. The real-time insights gleaned
    from AI-driven remote sensing bolster decision-making, facilitating timely interventions
    to avert potential hazards. This proactive approach not only safeguards the integrity
    of food production and storage but also aligns with stringent safety standards.
    The fusion of AI, remote sensing and agricultural practices represents a powerful
    synergy that optimizes food processing operations while strengthening the assurance
    of safe and high-quality food products. Fig. 1 AI and ML utilization in food industry
    (based on the finding from Lee and Liew [15], Smith et al. [16] and Garcia-Garcia
    et al. [17] Full size image Predictive Modelling and Quality Assessment for Enhanced
    Food Safety and Quality Control Predictive modelling and quality evaluation are
    already commonplace in modern food quality management systems, which use the strength
    of AI and ML to anticipate product quality, identify possible problems and uphold
    uniform standards. The results of assessing model performance and accuracy in
    food safety are described in Table 2. Building consumer trust, adhering to rules
    and lowering food waste in the business all depend on this proactive approach
    to quality inspection [4, 18]. 1. Predictive shelf-life modelling: The integration
    of AI and ML in food process engineering has introduced the concept of predictive
    shelf-life modelling. This data-driven approach involves analysing historical
    data encompassing various parameters such as composition, storage conditions and
    environmental influences [18] to enable precise estimation of a food product’s
    shelf life. Factors like temperature, humidity and storage duration are considered
    to determine optimal storage conditions and expiration dates. By adopting effective
    storage practices, manufacturers can minimize product wastage and deterioration,
    ensuring that products reach consumers at their peak quality. 2. Quality assessment
    using sensor data: The use of AI and ML in food process engineering has enabled
    real-time quality assessment using sensor data [5]. Critical variables such as
    temperature, pH and moisture are continuously monitored using sensors throughout
    the production and storage stages of food items. By harnessing ML algorithms,
    real-time data can be rapidly evaluated to identify deviations from established
    quality standards. The immediate detection of anomalies empowers manufacturers
    to take prompt corrective actions to maintain the desired level of product excellence
    while averting potential quality issues. 3. Contaminant identification and allergen
    control: AI-driven image analysis and predictive modelling have enhanced the identification
    of contaminants and allergens in food products [5], addressing food safety concerns.
    ML algorithms proactively recognize potential sources of pollutants by analysing
    historical data and identifying patterns. This proactive identification mechanism
    ensures that potentially contaminated products are intercepted before reaching
    consumers, safeguarding public health and upholding stringent food safety regulations.
    4. Real-time process optimization: In the context of food process engineering,
    AI introduces real-time process optimization [5]. ML algorithms continuously analyse
    data streams from sensors, production equipment and environmental factors to make
    dynamic adjustments to industrial processes. This ensures consistent product quality,
    increased production efficiency and reduced resource consumption. By swiftly adapting
    to changing conditions, AI-driven optimization enhances product uniformity and
    minimizes waste. 5. Quality grading and sorting: AI-powered systems have enabled
    automated quality grading and sorting operations in food process engineering [5].
    By harnessing computer vision and ML techniques, food items can be categorized
    on the basis of their quality attributes. This automated grading process ensures
    uniformity in the final product, mitigating deviations and elevating consumer
    satisfaction. The technology’s ability to accurately discern quality traits contributes
    to efficient sorting processes, a critical aspect of modern food processing operations.
    6. Food regulation compliance: AI plays a significant role in ensuring food safety
    and regulatory compliance [5]. The predictive modelling capabilities of AI algorithms
    enable food manufacturers to anticipate potential issues by analysing comprehensive
    data sources, including historical records and lab tests. This proactive approach
    minimizes the risk of non-compliance and associated penalties, underscoring the
    technology’s crucial role in maintaining industry standards and consumer safety.
    7. Analysis of customer feedback: The use of AI and ML technologies in the examination
    of customer feedback has led to profound impacts in the field of food process
    engineering, as noted by Si et al. [5]. Through the incorporation of this feedback
    into prediction models, manufacturers can gain invaluable insights into product
    quality and consumer satisfaction. This data-driven approach to decision-making
    empowers manufacturers to improve product attributes that align with consumer
    preferences, ultimately resulting in enhanced quality and increased consumer trust.
    The flowcharts of the applications of AI and ML are shown in Figs. 2 and 3, respectively.
    Table 2 Assessing model performance and accuracy in food safety Full size table
    Fig. 2 Flow chart showing a crucial role of artificial intelligence in food sector
    Full size image Fig. 3 Flow chart showing a crucial role of machine learning in
    food sector Full size image The integration of AI and ML technologies in the realm
    of food process engineering and food safety represents a pivotal moment in the
    industry, characterized by precision, efficiency and heightened consumer protection.
    These advancements have contributed to the optimization of processes, waste reduction,
    and the establishment of an environment where the production of high-quality and
    safe food products is of utmost importance. Data Analytics and Pattern Recognition
    for Advanced Food Quality Control and Safety Data analytics and pattern recognition
    play a significant role in the evaluation of food quality management and safety.
    These methods use AI and ML to extract useful information from huge datasets,
    assisting in the detection of patterns, trends and potential problems in the production
    and distribution of food. The importance of pattern recognition and data analytics
    in relation to food quality control is described as follows. 1. Quality assurance
    and defect discovery: The combination of computer vision technology and data analytics
    has revolutionized quality assurance in food process engineering, leading to proactive
    measures to eliminate the risk of substandard products reaching consumers and
    bolstering their trust. This has been made possible by harnessing the power of
    AI algorithms, which allow automated inspection systems to meticulously examine
    images and videos of food products. Through pattern recognition, these systems
    ensure consistent quality attributes and adherence to established standards and
    reduce wastage. 2. Predictive quality modelling: Predictive quality modelling
    is a cornerstone of modern food process engineering, which has been made possible
    by leveraging historical data encompassing production conditions, sensory evaluations
    and consumer feedback. This forward-looking strategy enables manufacturers to
    optimize production processes, ensuring enduring quality uniformity while aligning
    with evolving consumer preferences. 3. Early contaminant detection: Data analytics
    and AI-driven insights are instrumental in ensuring food safety by facilitating
    early contaminant detection. This was made possible by analysing diverse data
    sources, including sensor readings and laboratory tests, to identify potential
    contaminants or deviations from safety standards. Rapid recognition of abnormal
    patterns empowers timely interventions, preempting potential food borne illness
    outbreaks and safeguarding consumer health. 4. Supply chain optimization: The
    strategic use of data analytics optimizes the entire food supply chain by analysing
    inventory levels, demand trends and transportation routes, which facilitates accurate
    demand forecasting and enhanced inventory management. The integration of AI algorithms
    predicts shifts in demand, streamlines inventory practices, reduces waste and
    ensures efficient product delivery to customers. 5. Consumer insights and personalization:
    Data analytics and ML techniques enable food producers to unlock valuable consumer
    insights and deliver personalized experiences. This tailored approach not only
    caters to specific market demands but also enhances consumer satisfaction and
    cultivates loyalty. 6. Adherence to food standards: In the realm of food process
    engineering, data analytics play a pivotal role in ensuring compliance with rigorous
    food safety standards. This diligent approach ensures that products consistently
    meet established quality and safety benchmarks by continuously monitoring and
    analysing data from various production stages, which mitigates the risk of fines
    and recalls. The synergy among data analytics, AI and food process engineering
    underpins enhanced quality, safety and efficiency throughout the entire food production
    lifecycle. By embracing these advanced technologies, the food industry has pioneered
    an era of precision, traceability and consumer-eccentric demands. Enhancing Food
    Safety Management and Traceability Through AI and ML Technologies The provision
    of safe, legal and high-quality food items is made possible through traceability
    and food safety management, which are essential components of the food sector.
    Modern technologies such as the blockchain, the IoT and AI have significantly
    improved how food safety and traceability are managed. The core elements of food
    safety management and traceability are explained below. 1. Hazard analysis and
    critical control points (HACCP): The incorporation of the HACCP framework with
    AI and ML has transformed food safety management in the manufacturing sector [20].
    This methodical approach utilizes data analysis to detect, evaluate and mitigate
    potential hazards. The data-crunching capabilities of AI enable the identification
    of risks, the prediction of outcomes and the implementation of preventive measures.
    Real-time monitoring and automatic alerts facilitated by AI and ML ensure swift
    actions, thereby minimizing risks and bolstering food safety. 2. IoT-based real-time
    monitoring: Real-time monitoring of critical factors, such as temperature, humidity
    and storage conditions, is achievable with the help of IoT-based devices with
    sensors [13]. The AI algorithms process the continuous stream of data from these
    sensors to detect deviations from optimal conditions. This constant vigilance
    minimizes the chances of contamination and spoilage, ensuring that food is handled,
    stored and transported under optimal conditions. 3. Blockchain for traceability:
    The transparency of blockchain technology has revolutionized the traceability
    landscape [7]. With every transaction and movement recorded in an immutable ledger,
    customers and regulators can track a product’s journey from its origin. AI’s analytical
    prowess can be harnessed to examine blockchain data during foodborne illness outbreaks,
    revealing patterns, trends and potential sources of contamination for swift intervention.
    4. Product authentication and anti-counterfeiting: Spectroscopic analysis and
    AI-powered image recognition techniques are used for food product authentication
    [21]. The ability of AI to compare product images and spectral signatures with
    established patterns aids in identifying counterfeit or adulterated products.
    This technology safeguards consumers by mitigating the risk of fraud and ensuring
    product integrity. 5. Supplier verification and compliance: AI and ML algorithms
    play a critical role in supplier verification and compliance assessment [22].
    By scrutinizing supplier data, certificates and historical performance, AI identifies
    potential risks, ensuring that only reputable and compliant vendors are integrated
    into the food supply chain. This proactive approach upholds food safety standards
    and minimizes potential risks. 6. Recall management: AI and ML technologies have
    revolutionized recall management, enabling targeted and efficient product recalls
    [23]. In cases of food safety concerns or contamination, AI swiftly identifies
    affected batches and issues’ precise recall notifications by analysing supply
    chain data. This precision reduces waste and minimizes the impact on consumers.
    7. Data-driven decision-making: AI and ML algorithms process vast datasets from
    diverse sources, empowering data-driven decision-making in food safety management
    [24]. By analysing lab tests, customer feedback and factory records, these technologies
    provide insights, identify emerging trends and continuously enhance food safety
    procedures, thereby promoting proactive risk management. The incorporation of
    AI and ML into food process engineering has augmented food safety practices, ensuring
    proactive hazard management, real-time monitoring, traceability and informed decision-making.
    As these technologies continue to evolve, these advancements will elevate food
    safety to new heights, thereby enhancing consumer trust and well-being. Contaminant
    type detected and accuracy by AI and ML are shown in Fig. 4. Fig. 4 Contaminant
    type detected and accuracy by AI and ML (based on the finding from Martinez et
    al. [25] and Singh et al. [8] Full size image Regulatory Compliance and Certification
    in Food Safety Through AI and ML Innovations Assuring that food items adhere to
    the norms and regulations established by the appropriate authorities, regulatory
    compliance and certification play a critical role in evaluating the safety and
    quality of food. AI, data analytics and blockchain are examples of cutting-edge
    technologies that work together to optimize compliance processes, speed up audits
    and provide consumers with transparent information about food products. A closer
    look is warranted at the significance of certification and legal compliance in
    the food industry: 1. Ensuring food safety: Within the domain of food process
    engineering, ensuring the highest levels of food safety is of utmost importance.
    The implementation of regulatory compliance protocols ensures that food items
    comply with strict safety requirements, mitigating the risks of contamination,
    foodborne illnesses and product recalls. By utilizing AI-driven tools and techniques,
    food manufacturers can establish comprehensive food safety management systems
    that not only comply with legal mandates but also surpass them by ensuring the
    well-being of consumers. 2. Traceability and transparency: The application of
    blockchain technology in the context of food process engineering revolutionizes
    the concepts of traceability and transparency [7]. This innovation enables an
    unalterable record of a food product’s journey from its origin to its final distribution
    point. This level of traceability provides insights into each step of the supply
    chain, empowering food engineers to closely monitor and verify the conditions
    under which the product has been handled, stored and transported. The integration
    of AI further enhances this traceability, allowing for real-time data analysis
    to detect any anomalies that could jeopardize the safety of the product. 3. Product
    labelling and claims: The convergence of AI and food process engineering enhances
    the accuracy of product labelling and claims. AI-powered systems can comprehensively
    analyse product labels, ensuring that all information aligns with regulatory requirements.
    By examining nutritional content, allergen information and ingredient lists, AI
    can help prevent misleading or incorrect information, thereby safeguarding consumers’
    health and maintaining the integrity of food products. 4. Simplifying audits and
    inspections: In the realm of food process engineering, adhering to regulatory
    standards often entails rigorous audits and inspections. Here, AI and data analytics
    offer significant advantage by automating data collection and analysis. This streamlines
    the audit process, enabling food engineers to quickly compile and present comprehensive
    compliance data. This data-driven approach enhances the efficiency, reduces the
    chances of oversight and facilitates smoother interactions with regulatory authorities.
    5. Predictive compliance modelling: The complexity of modern food safety regulations
    demands proactive approaches [24]. AI’s ability to analyse historical compliance
    data can predict potential challenges and non-compliance trends. By identifying
    these patterns, food engineers can preemptively address issues and establish corrective
    measures to ensure continuous adherence to regulations. This anticipatory approach
    minimizes risks and reinforces a culture of safety in food process engineering.
    6. Compliance with industry certifications and standards: AI’s analytical capabilities
    enhance the rigorous process of complying with industry certifications and standards.
    The intricate evaluation of vast datasets allows for more efficient certification
    processes, reducing the time and effort required to meet standards such as ISO,
    GMP and HACCP. This integration also supports the alignment of production processes
    with evolving industry benchmarks, underscoring a commitment to excellence. 7.
    Early warning systems: Early warning systems play a critical role in food process
    engineering. Prompt identification and resolution of compliance deviations are
    of utmost importance [13]. With the aid of AI-powered technology, early warning
    systems can analyse data in real time, enabling stakeholders to be immediately
    notified of any deviations from established norms. This capability promotes the
    timely implementation of corrective actions, which helps prevent potential compliance
    breaches, thereby ensuring the safety and quality of food products. 8. Secure
    document management and verification: Effective management of compliance records
    is a crucial component of food process engineering [7]. Blockchain technology,
    along with AI, provides a secure and tamper-proof method for storing important
    compliance records. This ensures the integrity of vital compliance records such
    as certificates, test results and other relevant documents. Moreover, it allows
    seamless access to regulators, consumers and other stakeholders while preventing
    any unauthorized alterations. Case studies demonstrating AI and ML applications
    in food safety are shown in Table 3. Table 3 Case studies demonstrating AI and
    ML applications in food safety Full size table Challenges and Future Directions
    It is crucial to solve the issues and consider other approaches if AI and ML are
    to be developed further and effectively used in food quality control and safety
    evaluation. Although these technologies have a lot of potential to enhance food
    quality and safety, some challenges must be overcome before they can reach their
    full potential. Looking at possible future possibilities can also provide insight
    into how these technologies will affect the food industry. Difficulties and possible
    directions are described as follows. Challenges Few challenges and limitations
    for the adoption of AI and ML are explained in the following texts and shown in
    Table 4. Table 4 Addressing challenges in AI and ML adoption for food safety Full
    size table Data Availability and Quality The dependence of AI and ML on extensive
    and high-quality datasets for precise predictions warrants critical appraisal,
    particularly in the context of trends in food science. While AI holds the promise
    of revolutionizing food safety, the challenge of sourcing comprehensive and reliable
    datasets, especially for emerging contaminants and rare quality issues, exposes
    a crucial limitation [27]. AI models rely on vast amounts of high-quality data
    for training and accurate prediction. In contrast, it can be challenging to find
    diverse and well-annotated data in the food industry. Data collection, labelling
    and storage issues must be carefully considered if reliable and representative
    datasets are to be guaranteed. The production of food necessitates the use of
    sensitive information regarding formulas, processes and quality control. Strong
    data privacy and security safeguards must be in place to protect private data
    from unauthorized access, breaches or misuse. Deep learning models can be complex
    and challenging to interpret. For food process engineering, understanding the
    reasoning behind AI-driven decisions is essential, especially regarding quality
    control, safety and regulatory compliance. Procedures for explainability and interpretability
    development are necessary for a model to be accepted and to gain trust. A few
    are explained in the following: Privacy and security concerns: Delving into the
    realm of AI and ML brings to the forefront a critical examination of the intricate
    web of privacy and security concerns, a topic of paramount significance in the
    evolving landscape of food science [28]. The assimilation of AI entails the inevitable
    acquisition and analysis of sensitive data, casting a shadow of uncertainty over
    the integrity of data privacy and security protocols. While the potential benefits
    of AI in food safety management are undeniable, the unresolved challenge lies
    in establishing impregnable fortifications against unauthorized data access and
    potential breaches. The intricate dance between harnessing the power of AI and
    safeguarding the sanctity of sensitive information demands not only careful vigilance
    but also innovative solutions that ensure the protection of consumer trust in
    the digital age. Integration with existing processes: The haunting specter of
    obsolescence looms large as the chasm between entrenched legacy systems, and the
    vanguard of AI technologies yawns wider, inviting a crucible of critical inquiry
    from the discerning purview of distinguished experts in the trends of food science
    [29]. Compatibility issues, system integration difficulties and scalability restrictions
    need to be resolved to ensure a smooth transition and effective application of
    AI and ML technologies. Interpretability and explainability: A vexing conundrum
    pervades the realm of AI and ML, particularly concerning the intricate labyrinth
    of interpretability and explainability inherent in complex models such as deep
    neural networks, a challenge that demands incisive scrutiny from the vantage point
    of the esteemed experts in the trends of food science [31]. The opacity shrouding
    the decision-making mechanisms of these advanced AI architectures casts a cloud
    of ambiguity, rendering the very bedrock of predictions elusive. Inextricably
    interwoven with the intricacies of food safety, the dichotomy between the inscrutability
    of AI and the compelling necessity for interpretability and explainability plays
    a profoundly critical role in the delicate tapestry of stakeholder trust and regulatory
    assurance [4]. The quest for effective solutions must navigate the treacherous
    terrain of unravelling AI’s enigmatic decision-making while safeguarding the indispensable
    confidence of the food industry’s custodians and gatekeepers. Regulatory compliance:
    The Byzantine labyrinth of regulatory acceptance looms as a herculean endeavour,
    a trial by fire for the vanguard of AI- and ML-generated data and its audacious
    claim to the throne of credibility within the hallowed halls of stringent food
    safety standards. A discerning eye cast upon this saga of persuasion and validation
    reveals a narrative fraught with complexities [30]. The food industry is subject
    to strict laws and regulations regarding food quality, safety, labelling and traceability.
    Making sure AI used in food process engineering complies with all relevant laws
    and standards is crucial. AI model performance evaluation and documentation should
    be done to demonstrate compliance with regulations. Collaboration between people
    and machines: AI tools should not be seen as a replacement for human labour, but
    rather as a tool to supplement human expertise. Ensuring effective collaboration
    and synergy between AI systems and human operators is essential. To fully benefit
    from AI technologies and enable seamless human-machine interaction, employees
    should receive adequate training and upskilling programmes. Applications for AI
    should be created and implemented ethically, considering issues of fairness, bias
    and transparency. Ethical issues become crucial when AI is used for processes
    such as product creation, quality control or supply chain management. Cost and
    return on investment: Initially, implementing AI technologies can be expensive
    due to the need for infrastructure, data collection and training. It is crucial
    to carefully assess the potential return on investment, accounting for factors
    such as increased productivity, lower waste, higher product quality and happier
    customers. Continuous monitoring and maintenance: AI models must be continuously
    monitored, updated and maintained to ensure optimal performance over time. Regular
    retraining, dataset updates and model adaptation to new situations or product
    modifications are required to keep AI systems accurate and effective. To address
    these issues and considerations, a multidisciplinary approach involving collaboration
    among food scientists, engineers, data scientists, regulatory specialists and
    stakeholders from the food business is required. By paying close attention to
    these aspects, AI can be successfully applied to food process engineering to promote
    innovation, increase productivity and ensure the production of high-quality, safe
    food items. The availability and quality of data are two significant barriers
    to incorporating AI into food process engineering. For training and accurate prediction,
    AI algorithms require high-quality data. In contrast, it can be challenging to
    find diverse and well-annotated data in the food industry. Data collection, labelling
    and storage issues must be carefully considered if reliable and representative
    datasets are to be guaranteed. To share data, establish criteria for data collection,
    and to address this problem, the food industry can collaborate with research institutions,
    business associations and regulatory bodies. Data quality assurance techniques
    should be implemented to ensure the accuracy and dependability of the data used
    for AI modelling. Validation, normalisation and data cleansing should all be part
    of these procedures. Additionally, efforts to collect data and annotate it may
    be made specifically for AI applications in food process engineering. Implications
    for Law and Ethics It is important to carefully consider the ethical and legal
    implications of incorporating AI into food process engineering. AI models may
    have an impact on quality control, safety, labelling and traceability in the food
    production process. It is essential to ensure that AI systems abide by legal requirements,
    industry norms and ethical standards. The ethical issues include dealing with
    issues of unfairness, transparency, privacy and bias. Biases in data and decision-making
    processes should be reduced by AI models throughout design and training to ensure
    fair treatment and equal opportunity for all people. Transparency in AI algorithms
    and decision-making should be supported for accountability and to foster trust.
    Privacy concerns must be resolved to safeguard sensitive data that AI systems
    collect and process. To manage ethical and regulatory issues, food corporations
    should establish solid governance frameworks that include stakeholders from all
    disciplines and areas of expertise. Close collaboration with legal experts, ethicists
    and regulatory organisations can aid in adherence to rules, norms and ethical
    principles. Interpretability and Explainability The interpretability and explainability
    of AI models are important aspects of food process engineering. Interpreting and
    explaining AI models, particularly complex deep learning models, can be challenging.
    In the food industry, particularly in areas such as quality control, safety and
    regulatory compliance, understanding the reasoning behind AI-driven decisions
    is crucial. An effort should be made to develop methods for model interpretability
    and explainability in the context of food process engineering. This may require
    the use of interpretable ML models, model-agnostic explanation techniques or visualisations
    to provide insights into the decision-making process of AI models. It is crucial
    to strike a balance between the demands for model accuracy and complexity, transparency
    and interpretability. Human-Machine Interaction This is required for the successful
    integration of AI into food process engineering. AI technologies should be viewed
    as tools to complement human skills rather than as a replacement for human labour.
    It is essential to ensure efficient interaction and coordination between AI systems
    and human operators. Employees should have access to training programmes to advance
    their familiarity with and competence using AI technologies. This includes understanding
    the limitations and potential biases of AI systems, applying AI-driven insights
    to decision-making and learning how to evaluate AI outputs. Collaborative interfaces
    and user-friendly solutions should be developed to enable the seamless interaction
    between humans and AI technologies. Open lines of communication and feedback between
    human operators and AI systems should be developed in order to solve problems,
    build trust and continuously improve the efficacy of AI technologies. Barriers
    to Adoption and Implementation There could be several issues with the adoption
    and use of AI in the engineering of food processes. Among the main challenges
    are as follows: Cost and return on investment: Initially, implementing AI technologies
    can be expensive due to the need for infrastructure, data collection and training.
    Businesses in the food industry must carefully assess the potential return on
    investment, considering factors such as increased productivity, decreased waste,
    higher product quality and happier customers. Employees who fear losing their
    jobs or are unclear about how AI systems operate may be resistant to the adoption
    of AI technologies. Businesses should invest in change management strategies that
    include training and communication to allay concerns and promote acceptance of
    AI technologies. Integration with existing processes: Introducing AI technology
    into the current food manufacturing and production processes requires careful
    planning and coordination. Compatibility issues, system integration difficulties
    and scalability restrictions must be resolved to ensure a smooth transition and
    effective application of AI technologies. Regulation and compliance requirements:
    The food industry is subject to strict laws and regulations regarding food quality,
    safety, labelling and traceability. Making sure AI used in food process engineering
    complies with all relevant laws and standards is crucial. AI model performance
    evaluation and documentation should be done to demonstrate compliance with regulations.
    Limited AI expertise: Due to the rapid development of AI, there are few professionals
    who are also knowledgeable in food process engineering. Finding or training employees
    with the necessary skills to develop, implement and maintain AI systems in the
    food industry may be challenging for businesses. To address these issues, food
    companies should set clear adoption goals and roadmaps for AI, collaborate with
    industry experts and partners and invest in ongoing training and internal AI knowledge
    development. The development of a culture that values innovation, experimentation
    and constant improvement will also help with the successful adoption and application
    of AI technology in food process engineering. Future Directions AI and ML personalize
    nutrition plans, improving health outcomes. Robotic systems and AI enhance quality
    control, boosting efficiency. Blockchain ensures traceability and transparency
    in the food supply chain. IoT and AI enable autonomous food safety monitoring,
    safeguarding artistry and consumer well-being (as mentioned in Table 5). Table
    5 Future directions in AI and ML applications for food safety Full size table
    Conclusion The review study investigated the uses of AI and ML in determining
    the safety and quality of food. This study demonstrated how AI and ML technologies
    have transformed the food business, providing creative answers to improve food
    safety, uphold uniform quality and speed up compliance procedures. Overview of
    AI and ML applications in enhancing food safety, challenges and considerations
    in AI/ML applications for food safety and applications of AI and ML in various
    stages of agri-food processing is described in Tables 6 and 7 and Fig. 5, respectively.
    Table 6 Overview of AI and ML applications in enhancing food safety Full size
    table Table 7 Challenges and considerations in AI/ML applications for food safety
    Full size table Fig. 5 Flowchart of applications of AI and ML in various stages
    of agri-food processing Full size image The review paper exemplifies how AI and
    ML have the ability to completely alter how food safety and quality are assessed.
    These technologies have a significant impact on the food sector in the following
    ways: 1. Enhanced food safety through rapid contaminant detection: The integration
    of AI and ML technologies has enabled enhanced food safety through rapid contaminant
    detection. This integration has facilitated swift identification of contaminants,
    allergens and pathogens in food products, significantly reducing the risk of foodborne
    illnesses. Advanced algorithms analyse data from various sources, such as sensor
    readings and historical records to detect potential hazards and ensure the overall
    safety of the food supply chain [34]. 2. Elevated quality control via automated
    inspection: Automated inspection has contributed to elevated quality control,
    minimizing defects and waste while ensuring consistent product quality. AI and
    ML algorithms analyse real-time data from production lines, enabling early detection
    of deviations from quality standards and ensuring that only products meeting desired
    specifications reach consumers. 3. Proactive identification and management of
    risks: Proactive risk management is enabled through predictive modelling and early
    warning systems powered by AI and ML. These systems can forecast potential quality
    issues by analysing historical data and patterns, allowing manufacturers to take
    corrective actions before problems escalate. This safeguarding of product quality
    and consumer well-being is crucial [35]. 4. Enhanced transparency and trust with
    blockchain: The integration of blockchain technology into the food supply chain
    enhances traceability and transparency, thereby fostering trust and accountability.
    Tamper-proof and immutable records enable all stakeholders, including regulators,
    vendors and consumers, to verify the origin, handling and safety of food products
    [36]. 5. Streamlined regulatory compliance and auditing: Regulatory compliance
    processes are streamlined through AI and ML technologies, which expedite audits
    and inspections. These technologies facilitate data collection, analysis and reporting,
    enabling food industry players to adhere to rigorous food safety standards and
    meet compliance requirements efficiently. Significance of AI and ML in Food Quality
    Control and Safety The significance of AI and ML in revolutionizing the assessment
    of food quality control and safety cannot be overstated. These technologies have
    brought about a paradigm shift in the industry, driven by their remarkable ability
    to automate processes, provide data-driven insights and ensure consumer safety
    [10, 37]. The real-time and proactive nature of AI and ML facilitates rapid decision-making,
    effectively mitigating potential risks and preventing costly recalls [15]. Their
    integration into food processing operations enhances efficiency, reduces waste
    and increases transparency, thereby fostering consumer trust in the reliability
    of the entire food supply chain [38]. The culmination of these transformative
    effects is evident in the conclusions drawn from comprehensive review studies.
    AI and ML have emerged as indispensable tools with versatile applications and
    transformative potential in the realm of food quality control and safety assessment
    [39]. Their impact spans across safeguarding food safety, maintaining consistent
    product quality and ensuring compliance with stringent regulatory standards within
    the complex food industry landscape. Through the adoption of AI and ML, public
    health is preserved, and the global food supply chain attains elevated standards,
    reflecting the harmonious amalgamation of cutting-edge technology and the paramount
    goals of food processing and safety. Availability of Data and Materials All of
    the data that were analysed throughout the course of this study have been comprehensively
    incorporated within this published article. References Rady A, Ekramirad N, Adedeji
    AA, Li M, Alimardani R (2017) Hyperspectral imaging for detection of codling moth
    infestation in GoldRush apples. Postharvest Biol Technol 129:37–44. https://doi.org/10.1016/j.postharvbio.2017.03.007
    Article   CAS   Google Scholar   Boyd DW, Sun MK (1994) Prototyping an expert
    system for diagnosis of potato diseases. Comput Electron Agric 10(3):259–267.
    https://doi.org/10.1016/0168-1699(94)90045-0 Article   Google Scholar   Peixoto
    MS, Barros LC, Bassanezi RC, Fernandes OA (2015) An approach via fuzzy systems
    for dynamics and control of the soybean aphid. In: Proceedings of the 2015 Conference
    of the International Fuzzy Systems Association and the European Society for Fuzzy
    Logic and Technology (IFSA-EUSFLAT-15). https://doi.org/10.2991/ifsa-eusflat-15.2015.183
    Wolfert S, Ge L, Verdouw C, Bogaardt M-J (2017) Big data in smart farming – a
    review. Agric Syst 153:69–80. https://doi.org/10.1016/J.AGSY.2017.01.023 Article   Google
    Scholar   Si Y, Liu G, Lin J, Lv Q, Juan F (2007) Design of control system of
    laser levelling machine based on fussy control theory. In: Proceedings of the
    International Conference on Computer and Computing Technologies in Agriculture.
    Springer, Wuyishan, China, pp 1121–1127. https://doi.org/10.1007/978-0-387-77253-0_46
    Kakani V, Nguyen VH, Kumar BP, Kim H, Pasupuleti VR (2020) A critical review on
    computer vision and artificial intelligence in food industry. J Agric Food Res
    2:100033. https://doi.org/10.1016/J.JAFR.2020.100033 Bestelmeyer BT, Marcillo
    G, McCord SE et al (2020) Scaling up agricultural research with artificial intelligence.
    IT Professional 22(3):33–38. https://doi.org/10.1109/MITP.2020.2986062 Article   Google
    Scholar   Singh P, Jindal M, Khurana SMP (2020) Machine learning techniques in
    food safety. Trends Food Sci Technol 91(2):22–30 Google Scholar   Liu J, Cho DS
    (2021) A survey of machine intelligence. IEEE Access 9:16259–16279 Google Scholar   LeCun
    Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444. https://doi.org/10.1038/nature14539
    Article   ADS   CAS   PubMed   Google Scholar   Young T, Hazarika D, Poria S,
    Cambria E (2018) Recent trends in deep learning based natural language processing.
    IEEE Comput Intell Mag 13(3):55–75. https://doi.org/10.1109/MCI.2018.2840738 Article   Google
    Scholar   Jurafsky D, Martin JH (2019) Speech and language processing, 3rd edn.
    https://web.stanford.edu/~jurafsky/slp3/ Spanaki K, Karafili E, Sivarajah U, Despoudi
    S, Irani Z (2021) Artificial intelligence and food security: swarm intelligence
    of agritech drones for smart agrifood operations. Prod Plan Control 1–19. https://hdl.handle.net/10454/17961
    Zhang L, Zhang C, Jiang Z (2021) Research on food safety management system based
    on deep learning and IoT. Proceedings of the 2021 International Conference on
    Electronics, Communications and Information Technology (ECIT), pp 141–145 Google
    Scholar   Lee WS, Liew CV (2018) Data-driven modeling and predictive control of
    an industrial supercritical CO2 extraction process. Comput Chem Eng 116:1–14 ADS   Google
    Scholar   Smith J, Brown A, Johnson C (2020) Application of spectroscopy in food
    safety and quality control. Food Sci J 12(2):45–52 Garcia-Garcia A, Riquelme-Blondet
    A, Salloum C (2021) Predictive modeling in food manufacturing: challenges and
    opportunities. Food Technol Mag 75(3):50–55 Ojo TO, Baiyegunhi LJS, Adetoro AA,
    Ogundeji AA (2021) Adoption of soil and water conservation technology and its
    effect on the productivity of smallholder rice farmers in Southwest Nigeria. Heliyon
    7(3):e06433. https://doi.org/10.1016/j.heliyon.2021.e06433 Lundberg SM, Lee SI
    (2017) A unified approach to interpreting model predictions. Adv Neural Inf Process
    Syst. https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf
    Widener MJ, Shannon J (2014) When are food deserts? Integrating time into research
    on food accessibility. Health Place 30:1–3. https://doi.org/10.1016/j.healthplace.2014.07.011
    Article   PubMed   Google Scholar   Boissard OP, Martin V, Moisan S (2008) A cognitive
    vision approach to early pest detection in greenhouse crops. Comput Electron Agric
    62(2):81–93. https://doi.org/10.1016/j.compag.2007.11.009 Article   Google Scholar   Pérez-Harguindeguy
    N, Díaz S, Garnier E et al (2016) Corrigendum to: new handbook for standardized
    measurement of plant functional traits worldwide. Aust J Bot 64(8):715–716 Article   Google
    Scholar   Marambe B, Silva P (2020) A sixty-day battle to tackle food security
    – response of the Sri Lankan government to the COVID-19 pandemic. Sri Lanka J
    Food Agric 6(1). https://doi.org/10.4038/sljfa.v6i1.77 Misra NN, Dixit Y, Al-Mallahi
    A, Bhullar MS, Upadhyay R, Martynenko A (2020) IoT, big data and artificial intelligence
    in agriculture and food industry. IEEE Internet Things J 1–1. https://doi.org/10.1109/JIOT.2020.2998584
    Martinez S, Vaga M, Moltó E (2017) AI for pathogen detection in food. Food Microbiol
    75(1):123–131 Google Scholar   Wang Y, Wu D, Li J (2018) Applications of artificial
    intelligence and machine learning in food safety and quality control. Food Control
    86:352–362 Google Scholar   Chaudhary A, Kolhe S, Kamal R (2016) A hybrid ensemble
    for classification in multiclass datasets: an application to oilseed disease dataset.
    Comput Electron Agric 124:65–72. https://doi.org/10.1016/j.compag.2016.03.026
    Article   Google Scholar   Narayanan A, Shmatikov V (2019) Robust de-anonymization
    of large sparse datasets: a decade later. https://www.cs.princeton.edu/~arvindn/publications/de-anonymization-retrospective.pdf
    Yang P, Chen Y (2017) A survey on sentiment analysis by using machine learning
    methods. In: IEEE 2nd Information Technology, Networking, Electronic and Automation
    Control Conference (ITNEC). Chengdu, China, pp 117–121. https://doi.org/10.1109/ITNEC.2017.8284920
    Guidotti R, Monreale A, Ruggieri S, Turini F, Giannotti F, Pedreschi D (2018)
    A survey of methods for explaining black box models. ACM Comput Surv 51:1–42.
    https://doi.org/10.1145/3236009 Article   Google Scholar   Ribeiro MT, Singh S,
    Guestrin C (2016) Why should I trust you? Explaining the predictions of any classifier.
    In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining. San Francisco, CA, USA, pp 1135–1144. https://dl.acm.org/doi/pdf/10.1145/2939672.2939778?
    Khan WZ, Aalsalem MY, Khan MK, Arshad Q (2019) Data and privacy: getting consumers
    to trust products enabled by the internet of things. IEEE Consum Electron Mag
    8(2):35–38. https://doi.org/10.1109/MCE.2018.2880807 Article   Google Scholar   Liu
    C, Wang X, Wang Y (2022) Blockchain technology in food safety and traceability.
    Trends Food Sci Technol 121:33–45 Google Scholar   Li Y, Zhu Y, Zhang Y (2021)
    Application of artificial intelligence in food safety detection. Front Nutr 8:640804
    Google Scholar   Wang C, Huang L, Li P (2020) Early warning of food safety risk
    based on machine learning. Food Res Int 132:109071 Google Scholar   Zhong RY,
    Newman ST, Huang GQ (2021) Big data analytics and artificial intelligence pathways
    to deploy blockchain for sustainable food supply chains. Int J Prod Res 59(17):5337–5353
    Google Scholar   Mottaleb KA, Rahut DB (2018) Impacts of modern rice varieties
    on farmers’ livelihood in Bangladesh and Nepal. PLoS ONE 13(8):e0201835 Google
    Scholar   Menard JP, Drèze X, Vibet MA (2019) Blockchain: a meta-technology for
    self-organization? Technol Forecast Soc Chang 146:68–80 Google Scholar   Liu Y,
    Miao L, Lu J, Li J, Chen L (2021) A comparative study of machine learning algorithms
    for shelf life prediction of pork. Food Control 120:107566 Google Scholar   Download
    references Acknowledgements The author is thankful to the Dr. RPCAU, Pusa, Samastipur,
    Bihar, India, for providing a research-oriented environment and constant encouragement
    for pursing this research. Funding No fund is provided for this research. Author
    information Authors and Affiliations Krishi Vigyan Kendra, Bhagwanpur Hat, Siwan,
    841408, India Krishna Bahadur Chhetri Dr. RPCAU, Pusa, Samastipur, Bihar, India
    Krishna Bahadur Chhetri Contributions The author performed the conceptualization,
    literature review, data collection, data analysis, writing and visualisation and
    oversaw the entire review process, from conceptualization to the final manuscript.
    Corresponding author Correspondence to Krishna Bahadur Chhetri. Ethics declarations
    Ethical Approval Not applicable. Competing Interests The author declares no competing
    interests. Additional information Publisher''s Note Springer Nature remains neutral
    with regard to jurisdictional claims in published maps and institutional affiliations.
    Rights and permissions Springer Nature or its licensor (e.g. a society or other
    partner) holds exclusive rights to this article under a publishing agreement with
    the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Chhetri, K.B. Applications of Artificial Intelligence and Machine
    Learning in Food Quality Control and Safety Assessment. Food Eng Rev 16, 1–21
    (2024). https://doi.org/10.1007/s12393-023-09363-1 Download citation Received
    01 August 2023 Accepted 07 December 2023 Published 22 December 2023 Issue Date
    March 2024 DOI https://doi.org/10.1007/s12393-023-09363-1 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Artificial intelligence Machine learning Food quality control Food safety assessment
    Computer vision Deep learning Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Sections Figures References Abstract Introduction Materials
    and Methods Significance of AI and ML in Food Quality Control and Safety Availability
    of Data and Materials References Acknowledgements Funding Author information Ethics
    declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Food Engineering Reviews
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Applications of Artificial Intelligence and Machine Learning in Food Quality
    Control and Safety Assessment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 33 papers. The topics discussed include: design
    Of MMIC Ka-Band gallium nitride mixer with high linearity; IoT based home automation
    and security using google assistant; an optimized relay selection to improve reliability
    and reduce energy consumption in cooperative networks; design and analysis of
    highly isolated hexa-band suppressed UWB MIMO antenna; automated data preprocessing
    and training interface for machine learning applications; review of the current
    diagnostic techniques for deep vein thrombosis; content analysis and visualization
    of privacy policy using privacy management; automated system for identifying vaccinated
    person by iris recognition technique using deep learning models; and an efficient
    architecture implementation of a Golay code encoder in CMOS 45nm technology.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: AIP Conference Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 2nd International Conference on Advances in Signal Processing, VLSI, Communication,
    and Embedded Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Shrivastava A.
  - Vamsi P.R.
  citation_count: '0'
  description: Data processing plays a significant role in improving the performance
    of machine learning models. In an IoT network, data generated by sensor nodes
    will be multivariate and consist of complex patterns and correlations. Such data
    must be carefully preprocessed before applying ML algorithms for classification.
    In this paper, we propose a study that improving anomaly classification accuracy
    on various IoT sensor datasets. In this context, we propose a system that deals
    with various data distributions and detects anomalies more efficiently. The proposed
    study uses multiple data transformation methods to prepare the data for further
    analysis. The data transformation facilitates data conversion, making it more
    suitable for machine learning models. We demonstrate the efficacy of the proposed
    method on widely used IoT sensor datasets. We also demonstrate the results of
    the proposed system with the various machine learning models using performance
    metrics such as accuracy, AUC Score, F1 score, and Mean Square Error (MSE). It
    is observed that the proposed system is more effective on various IoT datasets.
  doi: 10.23940/ijpe.24.02.p2.6880
  full_citation: '>'
  full_text: '>

    "Home IJPE Issues Editorial Ethics Review Process News/Conferences Subscription
    Contact Current Issue Accepted Papers Archive Feature Papers Special Issues Int
    J Performability Eng ›› 2024, Vol. 20 ›› Issue (2): 68-80.doi: 10.23940/ijpe.24.02.p2.6880
    Previous Articles     Next Articles Improving Anomaly Classification using Combined
    Data Transformation and Machine Learning Methods Aparna Shrivastava* and P Raghu
    Vamsi    PDF 6 Abstract Abstract: Data processing plays a significant role in
    improving the performance of machine learning models. In an IoT network, data
    generated by sensor nodes will be multivariate and consist of complex patterns
    and correlations. Such data must be carefully preprocessed before applying ML
    algorithms for classification. In this paper, we propose a study that improving
    anomaly classification accuracy on various IoT sensor datasets. In this context,
    we propose a system that deals with various data distributions and detects anomalies
    more efficiently. The proposed study uses multiple data transformation methods
    to prepare the data for further analysis. The data transformation facilitates
    data conversion, making it more suitable for machine learning models. We demonstrate
    the efficacy of the proposed method on widely used IoT sensor datasets. We also
    demonstrate the results of the proposed system with the various machine learning
    models using performance metrics such as accuracy, AUC Score, F1 score, and Mean
    Square Error (MSE). It is observed that the proposed system is more effective
    on various IoT datasets. Key words: anomaly, data preprocessing, data transformation,
    IoT, machine learning, multivariate sensor data Cite this article Aparna Shrivastava
    and P Raghu Vamsi. Improving Anomaly Classification using Combined Data Transformation
    and Machine Learning Methods [J]. Int J Performability Eng, 2024, 20(2): 68-80.
    Add to citation manager EndNote|Reference Manager|ProCite|BibTeX|RefWorks share
    this article Facebook Twitter   /   Save 0 /   Recommend URL: https://www.ijpe-online.com/EN/10.23940/ijpe.24.02.p2.6880          https://www.ijpe-online.com/EN/Y2024/V20/I2/68
    References                                    Related Articles 15 Recommended
    0 Abstract References Related Articles Recommended TOP Copyright © 2016-2020 International
    Journal of Performability Engineering, All Rights Reserved. Maintained by Beijing
    Magtech Co. Ltd"'
  inline_citation: '>'
  journal: International Journal of Performability Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Improving Anomaly Classification using Combined Data Transformation and Machine
    Learning Methods
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Jyothi V.
  - Sreelatha T.
  - Thiyagu T.M.
  - Sowndharya R.
  - Arvinth N.
  citation_count: '0'
  description: 'Smart cities are metropolitan areas that use sophisticated technology
    to increase efficiency, sustainability, and overall quality of life. The potential
    for transformation is tremendous, with applications ranging from Internet of Things
    (IoT)-driven infrastructure to data-driven governance. Effectively handling the
    abundant data produced in smart cities requires stringent security and privacy
    protocols. This research aims to tackle these difficulties by introducing the
    suggested Artificial Intelligence-based Data Management System (AI-DMS) for Smart
    Cities. AI-DMS seeks to optimize the data processing pipeline, guaranteeing effectiveness
    throughout the process, from data extraction to publication. Implementing a Multi-Level
    Sensitive Model is a notable addition, as it classifies data into three categories:
    sensitive, quasi-sensitive, and public. This allows for more nuanced sharing of
    data. Privacy preservation is accomplished using Principal Component Analysis
    (PCA), a comprehensive technique encompassing feature mapping, selection, normalization,
    and transformation. The simulation results demonstrate that AI-DMS outperforms
    other methods. It achieves a Data Quality Score of 95.12% (training) and 93.76%
    (testing), a Privacy Preservation Rate of 85.23% (training) and 82.76% (testing),
    a Processing Efficiency of 90.54% (training) and 88.76% (testing), a Sensitivity
    Model Accuracy of 80.12% (training) and 78.45% (testing), and a Data Access Time
    of 22.76 ms (training) and 21.32 ms (testing). The results highlight AI-DMS as
    a reliable and effective system, guaranteeing superior smart city data management
    that is secure and precise. This contribution aligns with the changing urban scene,
    offering improvements in decisionmaking based on data while still ensuring privacy
    and security.'
  doi: 10.58346/JISIS.2024.I1.003
  full_citation: '>'
  full_text: '>

    "Journal of Internet Services and Information Security ISSN: 2182-2069 (printed)
    / ISSN: 2182-2077 (online) Home Current Issues Aims and Scope Indexing Info Editoral
    Board Submission Checklist Publication Ethics and Publication Malpractice Statement
    Archives Contact Us Submission Volume 14 - Issue 1 A Data Management System for
    Smart Cities Leveraging Artificial Intelligence Modeling Techniques to Enhance
    Privacy and Security Dr. V. Jyothi Assistant Professor, Department of Electronics
    and Communication Engineering, Vardhaman College of Engineering. jyothinaikv@gmail.com
    .st0{fill:#A6CE39;}.st1{fill:#FFFFFF;} 0000-0002-5899-2183 Dr. Tammineni Sreelatha
    Assistant Professor, Department of Electronics and Communication Engineering,Koneru
    Lakshmaiah Education Foundation. sreelatha457@gmail.com .st0{fill:#A6CE39;}.st1{fill:#FFFFFF;}
    0000-0002-0951-2796 Dr.T.M. Thiyagu Assistant Professor, Division of Artificial
    Intelligence and Machine Learning, Karunya Institute of Technology and Sciences.
    t.m.thiyagu@gmail.com .st0{fill:#A6CE39;}.st1{fill:#FFFFFF;} 0000-0002-4902-3153
    R. Sowndharya Assistant Professor, Department of Computer Science and Engineering,
    Sona College of Technology. rajendransowndharya@gmail.com .st0{fill:#A6CE39;}.st1{fill:#FFFFFF;}
    0009-0006-6209-0357 N. Arvinth Research Associate, National Institute of STEM
    Research. arvinthwork@gmail.com .st0{fill:#A6CE39;}.st1{fill:#FFFFFF;} 0009-0000-9798-9828
    DOI: 10.58346/JISIS.2024.I1.003 Keywords: Smart City, Data Management, Security,
    Privacy, Artificial Intelligence. Abstract Smart cities are metropolitan areas
    that use sophisticated technology to increase efficiency, sustainability, and
    overall quality of life. The potential for transformation is tremendous, with
    applications ranging from Internet of Things (IoT)-driven infrastructure to data-driven
    governance. Effectively handling the abundant data produced in smart cities requires
    stringent security and privacy protocols. This research aims to tackle these difficulties
    by introducing the suggested Artificial Intelligence-based Data Management System
    (AI-DMS) for Smart Cities. AI-DMS seeks to optimize the data processing pipeline,
    guaranteeing effectiveness throughout the process, from data extraction to publication.
    Implementing a Multi-Level Sensitive Model is a notable addition, as it classifies
    data into three categories: sensitive, quasi-sensitive, and public. This allows
    for more nuanced sharing of data. Privacy preservation is accomplished using Principal
    Component Analysis (PCA), a comprehensive technique encompassing feature mapping,
    selection, normalization, and transformation. The simulation results demonstrate
    that AI-DMS outperforms other methods. It achieves a Data Quality Score of 95.12%
    (training) and 93.76% (testing), a Privacy Preservation Rate of 85.23% (training)
    and 82.76% (testing), a Processing Efficiency of 90.54% (training) and 88.76%
    (testing), a Sensitivity Model Accuracy of 80.12% (training) and 78.45% (testing),
    and a Data Access Time of 22.76 ms (training) and 21.32 ms (testing). The results
    highlight AI-DMS as a reliable and effective system, guaranteeing superior smart
    city data management that is secure and precise. This contribution aligns with
    the changing urban scene, offering improvements in decision-making based on data
    while still ensuring privacy and security. PDF DATE February 2024 PAGE NUMBER
    37-51"'
  inline_citation: '>'
  journal: Journal of Internet Services and Information Security
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Data Management System for Smart Cities Leveraging Artificial Intelligence
    Modeling Techniques to Enhance Privacy and Security
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Karras A.
  - Giannaros A.
  - Karras C.
  - Theodorakopoulos L.
  - Mammassis C.S.
  - Krimpas G.A.
  - Sioutas S.
  citation_count: '0'
  description: In the context of the Internet of Things (IoT), Tiny Machine Learning
    (TinyML) and Big Data, enhanced by Edge Artificial Intelligence, are essential
    for effectively managing the extensive data produced by numerous connected devices.
    Our study introduces a set of TinyML algorithms designed and developed to improve
    Big Data management in large-scale IoT systems. These algorithms, named TinyCleanEDF,
    EdgeClusterML, CompressEdgeML, CacheEdgeML, and TinyHybridSenseQ, operate together
    to enhance data processing, storage, and quality control in IoT networks, utilizing
    the capabilities of Edge AI. In particular, TinyCleanEDF applies federated learning
    for Edge-based data cleaning and anomaly detection. EdgeClusterML combines reinforcement
    learning with self-organizing maps for effective data clustering. CompressEdgeML
    uses neural networks for adaptive data compression. CacheEdgeML employs predictive
    analytics for smart data caching, and TinyHybridSenseQ concentrates on data quality
    evaluation and hybrid storage strategies. Our experimental evaluation of the proposed
    techniques includes executing all the algorithms in various numbers of Raspberry
    Pi devices ranging from one to ten. The experimental results are promising as
    we outperform similar methods across various evaluation metrics. Ultimately, we
    anticipate that the proposed algorithms offer a comprehensive and efficient approach
    to managing the complexities of IoT, Big Data, and Edge AI.
  doi: 10.3390/fi16020042
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all    Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Future Internet All Article Types Advanced   Journals
    Future Internet Volume 16 Issue 2 10.3390/fi16020042 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Iwona Grobelna
    Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links
    Article Views 1699 Table of Contents Abstract Introduction Background and Related
    Work Methodology Experimental Results Conclusions and Future Work Author Contributions
    Funding Data Availability Statement Conflicts of Interest References Altmetric
    share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessArticle TinyML Algorithms for Big Data Management in Large-Scale IoT Systems
    by Aristeidis Karras 1,*, Anastasios Giannaros 1, Christos Karras 1,*, Leonidas
    Theodorakopoulos 2, Constantinos S. Mammassis 3, George A. Krimpas 1 and Spyros
    Sioutas 1 1 Computer Engineering and Informatics Department, University of Patras,
    26504 Patras, Greece 2 Department of Management Science and Technology, University
    of Patras, 26334 Patras, Greece 3 Department of Industrial Management and Technology,
    University of Piraeus, 18534 Piraeus, Greece * Authors to whom correspondence
    should be addressed. Future Internet 2024, 16(2), 42; https://doi.org/10.3390/fi16020042
    Submission received: 6 December 2023 / Revised: 22 January 2024 / Accepted: 23
    January 2024 / Published: 25 January 2024 (This article belongs to the Special
    Issue Internet of Things and Cyber-Physical Systems II) Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract In the context of the Internet of Things (IoT),
    Tiny Machine Learning (TinyML) and Big Data, enhanced by Edge Artificial Intelligence,
    are essential for effectively managing the extensive data produced by numerous
    connected devices. Our study introduces a set of TinyML algorithms designed and
    developed to improve Big Data management in large-scale IoT systems. These algorithms,
    named TinyCleanEDF, EdgeClusterML, CompressEdgeML, CacheEdgeML, and TinyHybridSenseQ,
    operate together to enhance data processing, storage, and quality control in IoT
    networks, utilizing the capabilities of Edge AI. In particular, TinyCleanEDF applies
    federated learning for Edge-based data cleaning and anomaly detection. EdgeClusterML
    combines reinforcement learning with self-organizing maps for effective data clustering.
    CompressEdgeML uses neural networks for adaptive data compression. CacheEdgeML
    employs predictive analytics for smart data caching, and TinyHybridSenseQ concentrates
    on data quality evaluation and hybrid storage strategies. Our experimental evaluation
    of the proposed techniques includes executing all the algorithms in various numbers
    of Raspberry Pi devices ranging from one to ten. The experimental results are
    promising as we outperform similar methods across various evaluation metrics.
    Ultimately, we anticipate that the proposed algorithms offer a comprehensive and
    efficient approach to managing the complexities of IoT, Big Data, and Edge AI.
    Keywords: TinyML; Edge AI; IoT; IoT data engineering; IoT Big Data management;
    IoT systems 1. Introduction The rapid evolution of the Internet of Things (IoT)
    has significantly influenced various sectors, including supply chains, healthcare,
    and energy systems. This technology’s expansive data generation presents challenges
    in managing and interpreting vast volumes of information. Tiny Machine Learning
    (TinyML), combining the efficiency of embedded systems with advanced machine learning
    techniques, stands out as a critical solution for facilitating effective, localized
    data processing within the IoT framework. The research underscores the growing
    impact of IoT technologies like TinyML across multiple industries. In supply chain
    management, IoT’s role is instrumental in enhancing inventory management, marking
    a crucial shift towards Industry 4.0 technologies and their potential to add value
    to businesses [1]. The healthcare sector benefits from IoT in enhancing services
    such as remote patient monitoring and real-time data analytics, albeit facing
    challenges in handling the growth in medical data [2]. TinyML, in particular,
    is recognized for initiating a new era of IoT and autonomous system applications,
    attributed to its power efficiency, enhanced privacy, and reduced latency in data
    processing [3]. The application of machine learning algorithms in embedded systems,
    including TinyML, is being extensively researched. Studies focus on analyzing
    performance and developing intelligent systems like solar collectors for thermal
    energy storage [4,5]. Moreover, integrating multiple blockchain technologies [6,7]
    and decentralized authentication systems [8] has been proposed as a viable strategy
    for secure and efficient IoT data management, addressing the digital challenges
    in the Internet of Everything era [9]. From an industrial perspective, effectively
    managing TinyML at scale, particularly with regard to the inherent hardware and
    software constraints of IoT devices, remains a focal area of ongoing research.
    Proposals suggest frameworks that utilize Semantic Web technologies for the integrated
    management of TinyML models and IoT devices [10]. Additionally, the advancement
    and implementation of sophisticated intelligent-anomaly-based intrusion detection
    systems within IoT networks, utilizing advanced machine learning models, have
    demonstrated significant efficacy in precisely detecting and proactively mitigating
    malicious cyber activities [11]. This approach is crucial for enhancing network
    security and ensuring the reliability of IoT systems in various industrial applications.
    The impact of TinyML extends to many areas, including wearable technology [12,13,14,15,16],
    smart cities [17,18,19,20,21,22], smart homes [23,24,25], smart agriculture [26,27,28,29,30,31],
    climatic change, environment protection, green AI sustainable applications [32,33,34,35,36,37,38],
    and automobiles [39,40]. Overcoming TinyML’s challenges, especially in hardware,
    is key and can be advanced through creating an open-source community. This initiative
    would support system development, enhance learning capabilities on small-scale
    hardware, and help unify software across different platforms [41]. TinyML is set
    to not only meet current technological demands but also shape the future of smart,
    efficient data processing. However, deploying TinyML on microcontrollers presents
    certain challenges. These include the selection of appropriate programming languages,
    limited support for various development boards, often overlooked preprocessing
    steps, the choice of suitable sensors, and a scarcity of labeled data for training
    purposes [42]. Overcoming these obstacles is essential for the development of
    TinyML systems that are both efficient and effective. Tiny Machine Learning (TinyML)
    offers significant progress in machine learning, focusing on low-resource embedded
    devices [43]. Despite its extensive potential for enabling machine learning in
    compact formats, TinyML encounters several challenges, including the absence of
    standardized benchmarks, limited development board support, programming language
    restrictions, preprocessing oversights, sensor selection issues, and a lack of
    sufficient labeled data [42,44]. In regions like Africa, where the adoption of
    AI and embedded systems is still in its infancy, TinyML could address issues related
    to connectivity, energy, and costs [45]. Moreover, its ability to facilitate independent
    operation from cloud computing opens new areas for energy-efficient, secure, and
    private data processing [46], introducing a new era of novel localized applications
    in various fields. In the vast ecosystem of Big Data, TinyML stands out as a significant
    opportunity. Given the immense volume, velocity, and variety of data in modern
    systems, traditional Big Data management methods often reach their limits. TinyML
    presents an innovative solution, functioning more than simply as a tool for data
    processing but also as an effective technique for data management. By enabling
    devices to conduct initial data processing and improvement at the network’s edge,
    TinyML significantly reduces the amount of data requiring centralized processing.
    The subsequent selective transmission and storage of critical data enhance both
    storage and processing efficiency. Moreover, this approach facilitates quick and
    immediate analytics, thereby enhancing the overall value gained from Big Data.
    The aim of this study focuses on the field of IoT data management by introducing
    and comparing a suite of specialized TinyML algorithms. Each algorithm, namely
    TinyCleanEDF (Algorithm 1), EdgeClusterML (Algorithm 2), CompressEdgeML (Algorithm
    3), CacheEdgeML (Algorithm 4), and TinyHybridSenseQ (Algorithm 5), has been designed
    to address specific challenges inherent in IoT systems. TinyML algorithms are
    essential in this context as they enable efficient processing and intelligent
    decisionmaking directly on IoT devices, reducing the reliance on central systems
    and minimizing data transmission needs. By incorporating advanced techniques such
    as federated learning, anomaly detection, adaptive data compression, strategic
    caching, and detailed data quality assessment, these algorithms jointly enhance
    the overall efficiency, security, and reliability of data management within IoT
    networks. The comparative analysis provided in this study underscores the distinct
    functionalities and advantages of each algorithm, highlighting the necessity and
    versatility of TinyML in handling data in the increasingly complex landscape of
    IoT systems. The remainder of this study is organized as follows: Section 2 provides
    comprehensive background on TinyML, highlighting its emergence as a pivotal tool
    in managing Big Data within IoT environments and discussing its application in
    large-scale IoT systems and embedded devices. Section 3 outlines our methodology,
    covering our approach’s advantages, framework design, hardware setup, and dataset
    configuration for TinyML evaluation. Section 3.7 details the proposed algorithms
    we developed to utilize TinyML in IoT contexts. The experimental results of the
    proposed algorithms are thoroughly presented in Section 4, demonstrating the practical
    implications and effectiveness of our approach. Finally, the study concludes in
    Section 5, summarizing the key findings and discussing future research directions,
    emphasizing TinyML’s impact on IoT data management. 2. Background and Related
    Work IoT systems frequently generate vast quantities of data, posing substantial
    management and analysis challenges. Researchers have introduced several frameworks
    and architectures to address these challenges in IoT Big Data management and knowledge
    extraction. One such proposal is the Cognitive-Oriented IoT Big Data Framework
    (COIB Framework), as outlined in Mishra’s works [47,48]. This framework encompasses
    an implementation architecture, layers for IoT Big Data, and a structure for organizing
    data. An alternative method involves employing a Big-Data-enhanced system, adhering
    to a data lake architecture [49]. Key features of this system include a multi-threaded
    parallel approach for data ingestion, strategies for storing both raw and processed
    IoT data, a distributed cache layer, and a unified SQL-based interface for exploring
    IoT data. Furthermore, blockchain technologies have been investigated for their
    potential to maintain continuous integrity in IoT Big Data management [50]. This
    involves five integrity protocols implemented across three stages of IoT operations.
    2.1. Big Data Challenges, Internet of Things, and TinyML The rapid expansion of
    the Internet of Things (IoT) marks a significant shift in the digital landscape,
    marked by an extensive network of devices and sensors continuously collecting
    and sending data. This fusion of environments rich in data greatly increases the
    challenges related to Big Data, particularly concerning its large volume, high
    speed, and varied complexity. The Big Data Dilemma in IoT As IoT systems evolve,
    they inherently generate data that challenge conventional processing and storage
    infrastructures. Key challenges arising from this scenario include: Storage Capacity
    and Scalability: Traditional storage systems grapple with the ever-growing influx
    of data from IoT sources, necessitating the development of more scalable and adaptive
    solutions. Data Processing and Analysis: The heterogeneity of IoT data requires
    sophisticated adaptable algorithms and infrastructures to derive meaningful insights
    efficiently. Data Transfer and Network Load: Ensuring efficient and timely data
    transmission across a myriad of devices without overburdening the network infrastructure
    remains a paramount concern. Data Integrity and Security: As data become increasingly
    decentralized across devices, ensuring their authenticity and safeguarding them
    from potential threats are critical. These challenges, which highlight the wider
    complexities that IoT introduces to Big Data management, are outlined in Table
    1. Table 1. Overview of Big Data challenges in IoT. 2.2. TinyML Tiny Machine Learning
    (TinyML) has emerged as a growing field in machine learning, characterized by
    its application in highly constrained Internet of Things (IoT) devices such as
    microcontrollers (MCUs) [51]. This technology facilitates the use of deep learning
    models across a multitude of IoT devices, thereby broadening the range of potential
    applications and enabling ubiquitous computational intelligence. The implementation
    of TinyML is challenging, primarily due to the limited memory resources of these
    devices and the necessity for simultaneous algorithm and system stack design.
    Attracting substantial interest in both research and development areas, numerous
    studies have been conducted, focusing on the challenges, applications, and advantages
    of TinyML [52,53]. An essential goal of TinyML is to bring machine learning capabilities
    to battery-powered intelligent devices, allowing them to locally process data
    without necessitating cloud connectivity. This ability to operate independently
    from cloud services not only enhances functionality but also provides a more cost-effective
    solution for IoT applications [3,54,55,56]. The academic community has thoroughly
    examined TinyML, with systematic reviews, surveys, and research papers delving
    into aspects such as its hardware requirements, frameworks, datasets, use cases,
    algorithms/models, and broader applications. Notably, the development of specialized
    TinyML frameworks and libraries, coupled with its integration with networking
    technologies, has been explored to facilitate its deployment in various sectors,
    including healthcare, smart agriculture, environmental monitoring, and anomaly
    detection. One practical application of TinyML is in the development of soft sensors
    for economical vehicular emission monitoring, showcasing its real-world applicability
    [57]. In essence, TinyML marks a significant progression in the domain of machine
    learning, enabling the execution of machine learning tasks on resource-constrained
    IoT devices and microcontrollers, thus laying the groundwork for an expansive
    ecosystem surrounding this technology. 2.2.1. TinyML as a Novel Facilitator in
    IoT Big Data Management Within this challenging landscape, TinyML emerges as an
    innovative intersection between machine learning and embedded systems. Specifically
    tailored for resource-constrained devices, it presents several avenues for mitigating
    Big Data challenges: Localized On-Device Processing: TinyML facilitates local
    data processing, markedly reducing the need for continuous data transfers, thus
    optimizing network bandwidth and improving system responsiveness. Intelligent
    Data Streamlining: With the ability to perform preliminary on-device analysis,
    TinyML enables IoT systems to discern and selectively transmit pivotal data, ensuring
    efficient utilization of storage resources. Adaptive Learning Mechanisms: IoT
    devices embedded with TinyML can continuously refine their data processing algorithms,
    fostering adaptability to dynamic data patterns and environmental changes. Reinforced
    Security Protocols: By integrating real-time anomaly detection at the device level,
    TinyML significantly enhances the security framework, providing an early detection
    system for potential data breaches or threats. The complex challenges and problems
    associated with Big Data in the Internet of Things (IoT) paradigm are diverse
    and complex, encompassing numerous aspects such as data management, processing,
    unstructured data analytics, visualization, interoperability, data semantics,
    scalability, data fusion, integration, quality, and discovery [58]. These issues
    are closely related to the growing trend of “big data” within cloud computing
    environments and the progressive development of IoT technologies, exerting a significant
    impact on various industries, including but not limited to the power sector, smart
    cities, and large-scale petrochemical plants [58,59,60]. Additionally, the realm
    of IoT architectures is not immune to pressing security and privacy threats, making
    them salient challenges that require immediate and effective addressal [61]. The
    efficiency and completeness of IoT Big Data, coupled with security concerns, have
    emerged as critical areas of focus in the realm of research and development [62].
    Furthermore, the potential integration of blockchain technology is being explored
    as a solution to ensure continued integrity in IoT Big Data management, particularly
    in addressing concerns related to data correctness, resource sharing, and the
    generation and verification of service-level agreements (SLA) [50]. In the IoT
    Big Data landscape, as delineated in Table 1, key challenges include managing
    the vast volume of data from numerous devices, necessitating advanced storage
    and processing systems. Rapid data generation requires real-time analysis and
    response, highlighting the importance of data velocity. The variety of data, both
    structured and unstructured, from diverse sources, complicates integration and
    analysis. Ensuring data veracity, or accuracy and trustworthiness, is increasingly
    challenging. Integrating various data sources while maintaining integrity is vital.
    Security and privacy concerns are paramount due to heightened interconnectivity,
    necessitating robust protocols. Lastly, minimizing latency to avoid obsolete insights
    is crucial in IoT Big Data management. Table 2 highlights how TinyML addresses
    key challenges in Big Data and IoT. It offers solutions to data overload by facilitating
    on-device data filtering and summarization, significantly reducing the amount
    of data that needs to be transmitted to central systems. This approach is pivotal
    for real-time processing needs, where localized TinyML models enable instant data
    analysis, ensuring timely insights without the dependency on external servers.
    Such capability is crucial in scenarios with limited or no connectivity, maintaining
    device functionality. Table 2. TinyML solutions for Big Data and IoT challenges.
    Additionally, TinyML greatly enhances energy efficiency by optimizing models for
    specific tasks, thereby conserving resources and extending battery life. This
    technology also bolsters security and privacy; by processing data locally, it
    minimizes the risks associated with data transmission and ensures that sensitive
    information remains within the user’s control. Furthermore, TinyML contributes
    to the longevity of devices by reducing the strain on their components through
    local processing, potentially extending their operational lifespan. These enhancements
    demonstrate TinyML’s significant role in improving the efficiency, security, and
    sustainability of IoT systems. 2.2.2. Characteristics of Large-Scale IoT Systems
    The characteristics of large-scale IoT systems and the enhancements introduced
    by TinyML are effectively outlined in Table 3. In these systems, a distributed
    topology with devices spread across various locations results in data decentralization
    and increased latency; TinyML tackles this by facilitating edge computation, enabling
    local data processing to reduce latency and provide real-time insights. The voluminous
    data streams generated continuously can burden storage and transmission channels,
    but TinyML assists by prioritizing, compressing, and filtering data at the device
    level, managing storage needs and reducing data transmission demands. Table 3.
    Characteristics of large-scale IoT systems and enhancements with TinyML. The varied
    landscape of IoT devices, each with different data formats and communication protocols,
    is harmonized by TinyML, which standardizes data processing and extraction at
    the source, ensuring consistent data representation across diverse device types.
    Power and resource constraints, especially in battery-operated devices, pose significant
    challenges in IoT systems. TinyML models are designed for optimal computational
    efficiency, performing tasks effectively without draining device resources. Finally,
    in applications that require real-time processing, such as health monitoring or
    predictive maintenance, delays in processing can be critical. TinyML enables rapid
    on-device processing, allowing immediate responses to changing data patterns,
    thus enhancing the overall functionality and effectiveness of large-scale IoT
    systems. 2.2.3. Applications of TinyML on Embedded Devices Table 4 illustrates
    various applications of TinyML and machine learning in embedded devices across
    different sectors. In predictive maintenance, TinyML models analyze real-time
    sensor data from machinery, enabling early detection of potential failures and
    reducing maintenance costs. This technology is also pivotal in health monitoring,
    where wearable devices equipped with TinyML offer continuous health tracking,
    instantly analyzing critical health metrics while ensuring user privacy. Table
    4. Applications of TinyML and machine learning on embedded devices. In agriculture,
    TinyML enhances efficiency by adjusting operations based on real-time environmental
    data, leading to optimal resource usage and increased yield. Voice and face recognition
    technologies in embedded devices benefit from TinyML through faster localized
    processing, enhancing reliability and privacy. TinyML also plays a crucial role
    in energy management within smart grids and home automation, optimizing energy
    use for cost and environmental benefits. In urban development, it contributes
    to traffic flow optimization by analyzing real-time vehicle and pedestrian movements,
    improving urban mobility. These examples showcase TinyML’s significant impact
    in enhancing operational efficiency, user experience, and sustainable practices
    across various industries. Table 5 presents a detailed overview of TinyML applications
    across a range of fields. It includes concise descriptions of each application
    and corresponding academic references. The table illustrates the versatility of
    TinyML, from implementing CNN models on microcontrollers for material damage analysis
    to its use in environmental monitoring. Each example not only provides a clear
    application scenario but also cites relevant studies, showcasing TinyML’s extensive
    impact in practical situations. This presentation highlights TinyML’s role in
    enhancing the capabilities of embedded devices in various industries. Table 5.
    Various applications of TinyML in embedded devices. 2.3. TinyML Algorithms Table
    6 provides a structured overview of various TinyML algorithms and their specific
    applications in different domains. It categorizes these algorithms into areas
    such as predictive maintenance, data compression, tool usage monitoring, and more,
    illustrating the range of TinyML’s applicability. Each entry in the table is linked
    to a corresponding reference, offering a direct connection to the source material.
    This format effectively showcases the diversity of TinyML’s real-world applications,
    highlighting its potential to transform various sectors through intelligent on-device
    data processing and analysis. Table 6. Classification of TinyML algorithms by
    application areas. Table 7 presents an organized summary of cutting-edge research
    in the field of Tiny Machine Learning (TinyML). This table methodically categorizes
    various studies into distinct focus areas, covering a broad spectrum from optimizing
    deep neural networks on microcontrollers to applying federated meta-learning techniques
    in environments with limited resources. This structured presentation not only
    underscores the multifaceted nature of TinyML research but also highlights its
    significant role in advancing the functionalities of embedded devices for a wide
    array of applications. Table 7. Classification and focus of recent TinyML algorithm
    studies. 2.4. Data Management Techniques Utilizing TinyML in IoT Systems In the
    field of Tiny Machine Learning (TinyML), data management techniques are essential
    for handling machine learning models on devices with limited resources, such as
    those in IoT networks. One method involves augmenting thing descriptions (TD)
    with semantic modeling to provide comprehensive information about applications
    on devices, facilitating the efficient management of both TinyML models and IoT
    devices on a large scale [81]. Additionally, employing TinyML for training devices
    can lead to the creation of a decentralized and adaptive software ecosystem, enhancing
    both performance and efficiency. This approach has been effectively implemented
    in the development of a smart edge computing robot (SECR) [82]. Such methodologies
    are increasingly important in sectors like supply chain management, where they
    play a crucial role in predicting product quality parameters and extending the
    shelf life of perishable goods, including fresh fruits in modified atmospheres
    [83]. Moreover, the growing complexity in communication systems, spurred by diverse
    emerging technologies, underscores the need for AI and ML techniques in the analysis,
    design, and operation of advanced communication networks [84]. In the context
    of IoT systems, data management techniques incorporating TinyML focus on effective
    data handling, ensuring privacy and security, and leveraging machine learning
    for insightful data analysis. One strategy employs distributed key management
    for securing IoT wireless sensor networks, utilizing the principles of elliptic
    curve cryptography [85]. Another method involves applying data-driven machine
    learning frameworks to enhance the accuracy of vessel trajectory records in maritime
    IoT systems [86]. Power management also plays a crucial role in IoT systems, particularly
    those reliant on compact devices and smart networks. This often includes the adoption
    of low-power communication protocols and the integration of autonomous power systems,
    which are frequently powered by renewable energy sources [87]. Furthermore, AI-based
    analytics, processed in the cloud, are increasingly being utilized for healthcare-related
    data management, such as systems designed for managing diabetic patient data [88].
    Table 8 illustrates how TinyML is revolutionizing data management techniques in
    IoT systems, bringing efficiency and accuracy to various processes. Techniques
    like predictive imputation and adaptive data quantization exemplify this transformation.
    Predictive imputation, using TinyML, maintains data integrity by filling in missing
    values based on historical and neighboring data, thereby ensuring dataset completeness.
    Adaptive data quantization, on the other hand, optimizes data storage and transmission.
    TinyML’s role here is to analyze current data trends and dynamically adjust quantization
    levels for optimal data representation. Table 8. Advanced data management techniques
    utilizing TinyML in IoT systems. Sensor data fusion, another critical technique,
    is enhanced by TinyML’s ability to process and merge data from various sensors
    in real time, thus providing a more comprehensive view and enhancing the accuracy
    of insights. Anomaly detection is particularly vital in IoT systems, and TinyML
    enhances this by continuously monitoring data streams to quickly identify and
    act upon unusual patterns or malfunctions. Intelligent data caching, enabled by
    TinyML, predicts future data needs, ensuring that frequently used or critical
    data are cached for instant access. Further, TinyML facilitates Edge-based clustering,
    grouping similar data at the network’s edge to simplify analytics and processing.
    This on-device clustering leads to more efficient data aggregation and transmission.
    Real-time data augmentation and local data lifespan management are also key areas
    where TinyML makes a significant impact. TinyML augments sensor data in real time
    to enhance machine learning performance while also predicting the utility of data
    for effective local storage management. Contextual data filtering and on-device
    data labeling are other areas where TinyML shows its prowess. By using environment-aware
    models, TinyML filters data relevant to the current context, thereby enhancing
    decisionmaking processes. Additionally, it can automatically label data based
    on learned patterns, facilitating efficient data categorization and retrieval.
    These advanced data management techniques, powered by TinyML, are pivotal in harnessing
    the full potential of IoT systems, ensuring that they are not only more efficient
    and accurate but also more responsive to real-time demands. 3. Methodology In
    this study, we adopt a structured methodology to investigate the application of
    TinyML in handling Big Data challenges within extensive IoT systems. Initially,
    our approach involves integrating IoT devices with Raspberry Pi units, which are
    crucial for managing the complexities of Big Data characterized by high volume,
    rapid velocity, and diverse variety while ensuring accuracy and value extraction.
    Subsequently, we concentrate on the technical deployment of TinyML on Raspberry
    Pis, focusing on essential tasks such as data cleaning, anomaly detection, and
    feature extraction. The effectiveness of these processes is comprehensively evaluated
    through a series of tests, ensuring that our approach aligns with the desired
    outcomes. Moreover, we introduce a feedback mechanism linked to the central Big
    Data system, enabling continuous updates and enhancements to the TinyML models
    on Raspberry Pis. This methodology is designed to create an efficient and adaptable
    system capable of addressing the dynamic needs of Big Data management in large-scale
    IoT applications and systems. Our approach involves deploying these algorithms
    on Raspberry Pi units, utilizing their strengths and capabilities in federated
    learning, anomaly detection, data compression, caching strategies, and data quality
    assessment. We systematically evaluate each algorithm’s performance in real-time
    IoT scenarios, focusing on their efficiency in processing and managing data. This
    includes assessing the scalability, responsiveness, and accuracy of each algorithm
    in handling the unique data streams generated by IoT devices. By incorporating
    these algorithms into our methodology, we aim to provide a comprehensive solution
    for Big Data challenges in IoT systems, ensuring robust and efficient data management.
    3.1. Advantages of TinyML Reduced Latency: Data processing on Raspberry Pi eliminates
    the lag associated with transmitting data to a centralized server and then fetching
    results. This ensures real-time or near-real-time responses. Decreased Bandwidth
    Consumption: Only crucial or processed data may be sent to the central server,
    reducing network load. Enhanced Privacy and Security: On-device processing ensures
    data privacy. Additionally, Raspberry Pis can be equipped with encryption tools
    to secure data before any transmission. Energy Efficiency: Although Raspberry
    Pis consume more energy than simple sensors, they are far more efficient than
    transmitting vast amounts of data to a distant server. Operational Resilience:
    Raspberry Pis equipped with TinyML can continue operations even when there is
    no network connectivity. Scalability and Flexibility: Raspberry Pis can be equipped
    with a variety of tools and software, allowing custom solutions for different
    data types and processing needs. 3.2. Big Data Challenges and Problems Addressed
    Volume: Local processing reduces data volume heading to centralized systems. Velocity:
    Raspberry Pis can handle high-frequency data, making real-time requirements attainable.
    Variety: Given their flexibility, Raspberry Pis can be customized to manage a
    multitude of data formats and types. Veracity: They can ensure data quality, filtering
    anomalies or errors before transmission. Value: On-device processing extracts
    meaningful insights, ensuring only the most relevant data are transmitted to central
    systems. 3.3. Framework Architecture The proposed architecture outlines a systematic
    approach for managing Big Data in IoT environments. At the base is the IoT layer,
    composed of various devices such as sensors and wearables, which generate vast
    amounts of data. These data are directed towards Raspberry Pi devices, equipped
    with TinyML capabilities. Within the Raspberry Pi layer, three primary tasks are
    undertaken: data cleaning to remove inconsistencies, anomaly detection to identify
    unusual patterns, and feature extraction to select relevant data attributes. Once
    processed, the refined data are transmitted to the centralized Big Data system
    via the communication layer. Notably, the volume of data being transmitted is
    reduced due to the preliminary processing at the Raspberry Pi level. At the top
    layer, the centralized system performs further storage, analytics, and processing
    tasks. A feedback mechanism is incorporated, allowing the centralized system to
    send updates to the Raspberry Pis, ensuring continuous optimization. Overall,
    this architecture presents a structured methodology for efficient data processing
    and management in large-scale IoT settings. The illustration of this architecture
    is represented in Figure 1. Figure 1. Proposed system architecture. 3.4. Hardware
    Configuration Our study’s hardware infrastructure comprises a selection of Raspberry
    Pi devices and a variety of sensors, each chosen for its specific role within
    our IoT framework. The following outlines the key components of our hardware setup:
    Raspberry Pi Devices: − 10 × Raspberry Pi 4 Model B: These are the workhorses
    of our setup, deployed for edge computing and intensive data processing tasks.
    − 5 × Raspberry Pi Zero W: These smaller units are used for less demanding tasks,
    primarily for collecting sensor data. Sensor Array: − 15 × DHT22 Temperature and
    Humidity Sensors: Key for monitoring environmental conditions, providing accurate
    temperature and humidity readings. − 10 × MPU6050 Gyroscope and Accelerometer
    Sensors: Employed to track motion and orientation, crucial for applications requiring
    movement analysis. − 8 × LDR Light Sensors: These sensors are tasked with detecting
    changes in light intensity, useful in both indoor and outdoor settings. − 7 ×
    HC-SR04 Ultrasonic Distance Sensors: Utilized primarily for distance measurement
    and object detection, they play a pivotal role in spatial analysis. − 5 × Soil
    Moisture Sensors: Specifically selected for agricultural applications, these sensors
    provide valuable data for smart farming solutions. This hardware ensemble, consisting
    of Raspberry Pi devices and a diverse set of sensors, constitutes the core of
    our IoT network. It is adeptly designed to handle a wide spectrum of data collection
    and processing operations. The Raspberry Pi 4 models, with their advanced capabilities,
    are integral for more demanding computational tasks. In contrast, the Raspberry
    Pi Zero W units offer a compact energy-efficient solution for simpler activities.
    The assortment of sensors capture a broad range of environmental and physical
    parameters, which are vital for the thorough deployment and effectiveness of the
    TinyML algorithms central to our research. 3.5. Computational Framework for IoT
    Model Training and Evaluation In this study, the computational framework consists
    of two key elements: a centralized High-Performance Computing (HPC) cluster and
    a network of 10 Raspberry Pi 4 units. Each Raspberry Pi is configured with 4 GB
    of RAM and a 1.5 GHz quad-core processor. Importantly, these Raspberry Pi units
    are connected to a distinct network, separate from the HPC cluster, to emulate
    a realistic communication scenario as we have proposed in [89,90]. A primary function
    of the HPC server is to aggregate and analyze the individual models developed
    on each Raspberry Pi, culminating in the evaluation of a comprehensive global
    model, denoted as ℳ 𝐺 . On each Raspberry Pi, a Flask server manages crucial tasks
    such as the exchange of models, execution of local training, and monitoring of
    memory usage. This aspect is particularly vital in contexts with limited hardware
    resources. Additionally, the server enables the handling of requests and facilitates
    essential updates over the air. The setup with Raspberry Pi units is designed
    to support experiments involving more than 10 clients, indicated as 𝐾>10 , with
    a constraint that no more than 10 clients ( 𝑆≤10 ) are sampled in each communication
    round. This capability is achieved by storing all K client datasets on each Raspberry
    Pi. During each communication round l, a subset of these datasets, corresponding
    to the S sampled clients, is allocated to a Raspberry Pi for processing. The Raspberry
    Pis are connected to a switch, which links them to the router via a cable. In
    scenarios where the switch is not in use, the Raspberry Pis switch to a Wi-Fi
    connection, allowing for the evaluation of communication overhead in two distinct
    network conditions: the faster Ethernet and the slower Wi-Fi. The network operates
    at a bandwidth of 100/100 Mbit/s, with the Ethernet utilizing full capacity and
    the Wi-Fi about 10% of it (10/10 Mbit/s). Each Raspberry Pi represents a client
    corresponding to a partition of the dataset, labeled as 𝑘 1 ,…, 𝑘 𝑆 , with Raspberry
    Pi 1 processing data as client 𝑘 𝑖 and Raspberry Pi 10 as 𝑘 𝑗 . 3.6. Dataset Configuration
    for TinyML Evaluation In this study, we have thoroughly assembled a dataset to
    effectively evaluate our TinyML algorithms within a comprehensive IoT framework.
    This dataset is characterized by its diversity and volume, mirroring the complexities
    encountered in large-scale IoT systems. The following are the key aspects of our
    dataset: Sensor Array Composition: − Environmental Data: Sourced from DHT22 sensors,
    providing continuous insights into temperature and humidity. − Motion and Orientation
    Data: Collected via MPU6050 sensors, capturing detailed information on movement
    and angular positions. − Light Intensity Measurements: Obtained from LDR sensors,
    these readings reflect variations in ambient lighting conditions. − Distance and
    Proximity Data: Acquired from HC-SR04 ultrasonic sensors, essential for spatial
    analysis and object detection. − Soil Moisture Levels: Recorded by specialized
    sensors, pivotal for applications in smart agriculture. Data Volume: − The dataset
    encompasses over 1 terabyte of collected raw sensor data, providing a substantial
    foundation for algorithmic testing and optimization. Data Collection Frequency:
    − Sensor readings are captured at varying intervals, ranging from high-frequency
    real-time data streams to periodic updates. This variability simulates different
    real-world operational scenarios, ensuring robust algorithm testing. Data Preparation:
    − Prior to analysis, the data were subjected to essential preprocessing steps,
    including cleaning and normalization, to ensure consistency and reliability for
    subsequent TinyML processing. This dataset, with its rich variety and significant
    volume, plays a crucial role in the assessment of our TinyML algorithms. It not
    only provides a realistic environment for testing but also ensures that the algorithms
    are evaluated across a range of conditions reflective of real-world IoT systems.
    The frequency of data collection, in particular, allows us to examine the algorithms’
    performance under various data flow scenarios, which is critical for their application
    in diverse IoT settings. 3.7. Proposed Algorithms TinyCleanEDF, as presented in
    Algorithm 1, is an advanced solution tailored for data cleaning and anomaly detection
    in IoT systems using federated learning. This algorithm partitions the data stream
    into subsets, each processed on separate edge devices. Each device operates a
    federated learning model, trained locally with its data subset. These models synchronize
    with a central server, which aggregates their parameters to refine the global
    model. This distributed framework enables efficient anomaly detection and data
    cleaning directly at the edge, with anomalies being pinpointed when data deviate
    from established patterns. Moreover, TinyCleanEDF employs an autoencoder at each
    node for feature extraction. These autoencoders are trained to reconstruct inputs
    from their compressed representations, effectively distilling significant data
    characteristics from complex datasets. This process is crucial for simplifying
    data, making them more manageable and highlighting vital information. The algorithm
    is inherently dynamic, consistently updating both federated models and autoencoders
    to integrate new data observations. Such continuous adaptation ensures the system’s
    ongoing relevance and efficacy. Through its integration of local data processing,
    anomaly detection, and feature extraction, TinyCleanEDF stands out as a robust
    and comprehensive solution for upholding data integrity and quality in intricate
    IoT environments. EdgeClusterML, outlined in Algorithm 2, is an innovative algorithm
    designed for dynamic and self-optimizing clustering in IoT networks. This algorithm
    combines reinforcement learning (RL) with a self-organizing map (SOM) to adaptively
    cluster data at the edge. In the initial step, EdgeClusterML initializes a dynamic
    clustering model using RL, where the quality of actions in different states is
    evaluated using a Q-function, 𝑄(𝑠,𝑎) and optimized through a defined reward function,
    𝑅(𝑠,𝑎) . The learning rate 𝛼 and discount factor 𝛾 are set to guide the learning
    process. Subsequently, the algorithm deploys an SOM for efficient data clustering.
    The SOM is initialized with random weights and fine-tuned using a neighborhood
    function and learning rate. As data flow through the system, EdgeClusterML dynamically
    clusters them using the SOM, constantly updating the model based on the data’s
    characteristics. RL is then employed to optimize clustering decisions, adjusting
    parameters through an 𝜖 -greedy policy and updating the Q-function based on observed
    rewards. This process leads to intelligent and responsive clustering, tailoring
    the data organization to the changing patterns and needs of the IoT environment.
    Finally, the clustered data are stored, ensuring organized and efficient data
    management at the edge. Algorithm 1 TinyCleanEDF: Federated Learning for Data
    Cleaning and Anomaly Detection with Autoencoder-based Feature Extraction 1:   procedure
    TinyCleanEDF( 𝑑𝑎𝑡𝑎𝑆𝑡𝑟𝑒𝑎𝑚 ) 2:   Step 1: Initialize Federated Learning Models 3:   Partition
    𝑑𝑎𝑡𝑎𝑆𝑡𝑟𝑒𝑎𝑚 into subsets { 𝐷 1 , 𝐷 2 ,…, 𝐷 𝑛 } for distributed processing 4:   Deploy
    federated learning models { 𝑀 1 , 𝑀 2 ,…, 𝑀 𝑛 } on edge devices 5:   Train each
    model 𝑀 𝑖 with its subset 𝐷 𝑖 6:   Models periodically execute 𝑀 𝑖 →Sync( 𝑀 𝑖
    ) with central server 7:   Central server performs Aggregate({ 𝑀 1 , 𝑀 2 ,…, 𝑀
    𝑛 }) 8:   Step 2: Federated Model for Data Cleaning and Anomaly Detection 9:   Apply
    𝑓 anomaly (𝑥; 𝑀 𝑖 ) to detect and clean anomalies locally 10:     Anomalies identified
    as 𝑥∉ExpectedPattern( 𝑀 𝑖 ) 11:     Cleaned data { 𝐶 1 , 𝐶 2 ,…, 𝐶 𝑛 } sent to
    central server 12:     Step 3: Deploy Autoencoder for Feature Extraction 13:     Implement
    autoencoder 𝐴 𝐸 𝑖 at each node i 14:     Train 𝐴 𝐸 𝑖 to reconstruct input x from
    compressed representation z 15:     Feature extraction: 𝑓 features (𝑥;𝐴 𝐸 𝑖 )=HiddenLayer(𝐴
    𝐸 𝑖 (𝑥)) 16:     Step 4: Continuous Adaptation and Feature Extraction 17:     for
    each 𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡 in 𝑑𝑎𝑡𝑎𝑆𝑡𝑟𝑒𝑎𝑚  do 18:         𝑐𝑙𝑒𝑎𝑛𝐷𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡← 𝑓 clean (𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡;
    𝑀 𝑖 ) 19:         𝑎𝑛𝑜𝑚𝑎𝑙𝑦← 𝑓 anomaly (𝑐𝑙𝑒𝑎𝑛𝐷𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡; 𝑀 𝑖 ) 20:         𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠←
    𝑓 features (𝑐𝑙𝑒𝑎𝑛𝐷𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡;𝐴 𝐸 𝑖 ) 21:         Update 𝑀 𝑖 and 𝐴 𝐸 𝑖 with 𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡
    for continuous learning 22:         Store (𝑐𝑙𝑒𝑎𝑛𝐷𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡,𝑎𝑛𝑜𝑚𝑎𝑙𝑦,𝑓𝑒𝑎𝑡𝑢𝑟𝑒𝑠) 23:     end
    for 24: end procedure Algorithm 2 EdgeClusterML: Dynamic and Self-Optimizing Clustering
    at the Edge 1:   procedure EdgeClusterML( 𝑑𝑎𝑡𝑎𝑆𝑡𝑟𝑒𝑎𝑚 ) 2:   Step 1: Initialize
    Dynamic Clustering Model with RL 3:   Let 𝑄(𝑠,𝑎) represent the quality of action
    a in state s 4:   Initialize 𝑄(𝑠,𝑎) for all state-action pairs 5:   Define reward
    function 𝑅(𝑠,𝑎) for evaluating clustering actions 6:   Set learning rate 𝛼 and
    discount factor 𝛾 7:   Step 2: Deploy Self-Organizing Map (SOM) for Clustering
    8:   Initialize SOM with random weights W 9:   Define neighborhood function ℎ
    𝑐𝑖 (𝑡) for neuron i at time t 10:     Set SOM learning rate 𝜂 11:     for each
    𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡 in 𝑑𝑎𝑡𝑎𝑆𝑡𝑟𝑒𝑎𝑚  do 12:         Step 3: Dynamic Clustering with SOM 13:         Find
    Best Matching Unit (BMU) for 𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡 in SOM 14:         Update weights W using
    ℎ 𝑐𝑖 (𝑡) and 𝜂 15:         Step 4: RL-based Optimization of Clustering 16:         Observe
    state s (current clustering configuration) 17:         Choose action a (adjusting
    clustering parameters) using 𝜖 -greedy policy 18:         Apply action a, observe
    new state 𝑠 ′ and reward r 19:         Update 𝑄(𝑠,𝑎) using the Bellman equation:
    20:         𝑄(𝑠,𝑎)←𝑄(𝑠,𝑎)+𝛼[𝑟+𝛾 max 𝑎 ′ 𝑄( 𝑠 ′ , 𝑎 ′ )−𝑄(𝑠,𝑎)] 21:         Step
    5: Store Clustered Data 22:         𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝐼𝐷← BMU index for 𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡 23:         Store
    𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡 in 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝐼𝐷 24:     end for 25: end procedure CompressEdgeML, as described
    in Algorithm 3, is designed to introduce smart and adaptive data compression capabilities
    to the edge of IoT networks. This algorithm utilizes a neural network, denoted
    as 𝑁𝑁 , specifically customized for data compression tasks. It is trained on sample
    data to efficiently identify and execute compression patterns, setting an initial
    compression ratio 𝐶𝑅 based on the characteristics of the training data. CompressEdgeML
    dynamically adapts its compression techniques to align with the current network
    conditions (represented as 𝑁 𝑐𝑜𝑛𝑑 ), ensuring an optimal balance between the quality
    of compression and operational efficiency. As data stream through the network,
    CompressEdgeML uses its neural network model to compress each datapoint. The algorithm
    continually updates the compression ratio in response to changing network bandwidth
    and storage capacities, ensuring that the size of the compressed data ( |𝑐𝑜𝑚𝑝𝑟𝑒𝑠𝑠𝑒𝑑𝐷𝑎𝑡𝑎|
    ) is always suitable for the network and storage constraints. The final step involves
    securely storing the compressed data in a specific storage system. This approach
    significantly enhances data management efficiency in IoT environments by reducing
    the size of data for transmission and storage while maintaining the integrity
    and usability of the information. Algorithm 3 CompressEdgeML: Adaptive Data Compression
    1:   procedure CompressEdgeML( 𝑑𝑎𝑡𝑎𝑆𝑡𝑟𝑒𝑎𝑚 ) 2:   Step 1: Initialize Neural Network-based
    Selective Compression Model 3:   Define neural network 𝑁 𝑁 comp for data compression
    4:   Train 𝑁 𝑁 comp on dataset 𝐷 train for compression patterns 5:   Initialize
    compression ratio 𝐶𝑅← CR init ( 𝐷 train ) 6:   Step 2: Deploy Adaptive Compression
    Techniques 7:   Monitor network conditions 𝑁 cond 8:   Define adaptive function
    𝐹 CR ( 𝑁 cond ,𝐶𝑅) for compression ratio 9:   Balance between compression quality
    and network efficiency 10:     for each 𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡∈𝑑𝑎𝑡𝑎𝑆𝑡𝑟𝑒𝑎𝑚  do 11:         Step
    3: Data Compression Using Neural Network 12:         𝑐𝑜𝑚𝑝𝑟𝑒𝑠𝑠𝑒𝑑𝐷𝑎𝑡𝑎←𝑁 𝑁 comp (𝑑𝑎𝑡𝑎𝑃𝑜𝑖𝑛𝑡,𝐶𝑅)
    13:         Measure size |𝑐𝑜𝑚𝑝𝑟𝑒𝑠𝑠𝑒𝑑𝐷𝑎𝑡𝑎| 14:         Step 4: Adapt Compression
    Level 15:         Update 𝑁 cond based on network bandwidth and storage 16:         Adjust
    𝐶𝑅 using 𝐹 CR ( 𝑁 cond ,𝐶𝑅) 17:         Ensure |𝑐𝑜𝑚𝑝𝑟𝑒𝑠𝑠𝑒𝑑𝐷𝑎𝑡𝑎| fits network and
    storage constraints 18:         Step 5: Store Compressed Data 19:         Store
    𝑐𝑜𝑚𝑝𝑟𝑒𝑠𝑠𝑒𝑑𝐷𝑎𝑡𝑎 in targeted storage system 20:     end for 21: end procedure CacheEdgeML,
    outlined in Algorithm 4, represents an advanced approach to data caching in IoT
    networks. This algorithm efficiently handles data requests by utilizing a predictive
    analytics model that bases its forecasts on historical access patterns. By training
    the model, denoted as P, to predict the likelihood of future data requests, CacheEdgeML
    can effectively identify and prioritize high-importance data. It employs a multi-tier
    caching strategy, where data are organized into various tiers ( 𝑇 1 , 𝑇 2 ,⋯,
    𝑇 𝑛 ) according to priority and frequency of access. This setup not only simplifies
    the management of data but also ensures the efficient utilization of storage resources.
    Moreover, CacheEdgeML works together with cloud services to manage data storage
    effectively. It establishes a synchronization mechanism with cloud storage and
    devises strategies for offloading data when the cache reaches capacity, maintaining
    consistency between edge and cloud storage. The algorithm dynamically manages
    cache tiers, continuously re-evaluating data priority and relocating frequently
    accessed data to higher tiers. This adaptability ensures that the most relevant
    data are readily available, thus enhancing the system’s overall efficiency and
    responsiveness. Algorithm 4 CacheEdgeML: Predictive and Tiered Data Caching Strategy
    1:   procedure CacheEdgeML( 𝑑𝑎𝑡𝑎𝑅𝑒𝑞𝑢𝑒𝑠𝑡𝑠 ) 2:   Step 1: Initialize Predictive
    Analytics Model for Anticipatory Caching 3:   Define predictive model 𝑃 cache
    based on historical patterns 4:   Train 𝑃 cache on dataset 𝐷 hist to estimate
    request probabilities 5:   Set priority threshold 𝜃 for high-importance data classification
    6:   Step 2: Implement Multi-Tier Caching Based on Priority and Frequency 7:   Define
    cache tiers 𝒯={ 𝑇 1 , 𝑇 2 ,..., 𝑇 𝑛 } 8:   Assign frequency thresholds ℱ={ 𝑓 1
    , 𝑓 2 ,..., 𝑓 𝑛 } for each 𝑇 𝑖 9:   Store data in tier 𝑇 𝑖 based on access frequency
    𝑓 𝑖 10:     Step 3: Collaborate with Cloud Services for Cache Management 11:     Establish
    synchronization 𝑆 sync with cloud storage 12:     Define offloading strategy Ω
    offload when cache full 13:     Maintain consistency 𝐶 consist between edge and
    cloud 14:     for each 𝑟𝑒𝑞𝑢𝑒𝑠𝑡∈𝑑𝑎𝑡𝑎𝑅𝑒𝑞𝑢𝑒𝑠𝑡𝑠  do 15:         if  𝑟𝑒𝑞𝑢𝑒𝑠𝑡∈Cache  then
    16:            Step 4: Serve from Cache 17:            Serve data from cache 18:         else
    19:            Step 5: Predict Future High-Priority Requests 20:            Calculate
    𝑃 cache (𝑟𝑒𝑞𝑢𝑒𝑠𝑡) , compare with 𝜃 21:            Update cache based on 𝑃 cache
    (𝑟𝑒𝑞𝑢𝑒𝑠𝑡) 22:            Step 6: Dynamic Management of Cache Tiers 23:            Re-evaluate
    priority for 𝑟𝑒𝑞𝑢𝑒𝑠𝑡 , adjust 𝒯 24:            Relocate data to appropriate tier
    based on ℱ 25:            Step 7: Serve Requested Data 26:            Fetch data
    from external source or cloud if not in cache 27:            Store new data in
    appropriate tier based on priority 28:         end if 29:     end for 30: end
    procedure TinyHybridSenseQ, as detailed in Algorithm 5, is a sophisticated TinyML-based
    algorithm specifically designed for IoT environments. It employs advanced machine
    learning models deployed on edge devices, focusing on the critical tasks of analyzing,
    categorizing, and efficiently storing data collected from a wide array of sensors.
    The core competency of this algorithm is in its robust data quality assessment
    module, which thoroughly evaluates the integrity and accuracy of sensor measurements.
    This evaluation is crucial for filtering out incorrect data, ensuring that only
    reliable and high-quality information is processed further. TinyHybridSenseQ also
    performs well in data management, implementing a dynamic and data-aware hybrid
    storage strategy. It efficiently determines the optimal storage location for each
    data packet—be it local storage for less critical data or immediate transfer to
    a central database for high-priority information. This strategic approach not
    only simplifies data handling but also significantly enhances the overall efficiency
    of data transfer processes in IoT networks. Furthermore, TinyHybridSenseQ continuously
    evolves through its model adaptation feature, which refines its analytical capabilities
    based on incoming sensor data, thereby maintaining high accuracy and relevance
    in ever-changing IoT environments. 3.8. Comparison among Proposed TinyML Algorithms
    The following Table 9 provides a comparative overview of the five proposed algorithms:
    TinyCleanEDF, EdgeClusterML, CompressEdgeML, CacheEdgeML, and TinyHybridSenseQ.
    The table outlines key features such as federated learning, anomaly detection,
    data compression, caching strategy, and data quality assessment. This comparison
    helps in understanding the unique capabilities and functionalities that each algorithm
    brings to IoT Big Data management. Algorithm 5 TinyHybridSenseQ: Data-Aware Hybrid
    Storage and Quality Assessment for IoT Sensors 1:   procedure TinyHybridSenseQ(
    𝑠𝑒𝑛𝑠𝑜𝑟𝐷𝑎𝑡𝑎 ) 2:   Step 1: Initialize Data Quality Assessment Model 3:   Deploy
    TinyML models { 𝑀 1 , 𝑀 2 ,…, 𝑀 𝑛 } for data quality assessment 4:   Train each
    model 𝑀 𝑖 to identify anomalies and inconsistencies 5:   Step 2: Hybrid Storage
    Strategy Initialization 6:   Define data storage strategies 𝒮={ 𝑆 1 , 𝑆 2 ,…,
    𝑆 𝑛 } based on type and priority 7:   Initialize storage resources 𝑅 local and
    𝑅 cloud 8:   for each 𝑑𝑎𝑡𝑎𝑃𝑎𝑐𝑘𝑒𝑡∈𝑠𝑒𝑛𝑠𝑜𝑟𝐷𝑎𝑡𝑎  do 9:       Step 3: Assess Data Quality
    10:         Quality score 𝑄← 𝑓 quality (𝑑𝑎𝑡𝑎𝑃𝑎𝑐𝑘𝑒𝑡, 𝑀 𝑖 ) 11:         Categorize
    𝑑𝑎𝑡𝑎𝑃𝑎𝑐𝑘𝑒𝑡 as high-quality or low-quality based on Q 12:         Step 4: Data
    Categorization and Prioritization 13:         Categorize 𝑑𝑎𝑡𝑎𝑃𝑎𝑐𝑘𝑒𝑡 into type
    T 14:         Prioritize 𝑑𝑎𝑡𝑎𝑃𝑎𝑐𝑘𝑒𝑡 for storage based on Q and T 15:         Step
    5: Efficient Data Transfer and Storage 16:         if Q is high and 𝑑𝑎𝑡𝑎𝑃𝑎𝑐𝑘𝑒𝑡
    is high-priority then 17:            Transfer to central database 𝐷𝐵 18:         else
    19:            Store in 𝑅 local or 𝑅 cloud based on 𝒮 20:         end if 21:         Step
    6: Continuous Model Adaptation and Reporting 22:         Update 𝑀 𝑖 with new 𝑑𝑎𝑡𝑎𝑃𝑎𝑐𝑘𝑒𝑡
    for continual learning 23:         Generate and store reports on data quality
    and storage 24:     end for 25: end procedure Table 9. Comparison of proposed
    algorithms. 4. Experimental Results 4.1. Overview In this section, we assess the
    performance of our five proposed algorithms—TinyCleanEDF, EdgeClusterML, CompressEdgeML,
    CacheEdgeML, and TinyHybridSenseQ—across multiple key performance metrics. The
    evaluation is conducted by varying the number of Raspberry Pi devices used in
    the deployment, ranging from one to ten. Each algorithm’s performance is measured
    across elements such as accuracy, compression efficiency, data processing time
    (ms), training time (ms), overall efficiency, and scalability. The metrics utilized
    in this work are provided in detail in Section 4.2 below. 4.2. Metrics and Methods
    For the evaluation of the proposed techniques, the following metrics are utilized.
    Data Processing Time (ms): To measure the data processing time in a distributed
    system such as the one proposed where we have multiple Raspberry Pi devices, we
    can consider the maximum time taken by any single device as well as the average
    time across all devices. The equation is provided in Equation (1). DataProcessing
    Time total =max( 𝑇 1 , 𝑇 2 ,⋯, 𝑇 𝑛 )andDataProcessing Time avg = ∑ 𝑛 𝑖=1 𝑇 𝑖 𝑛
    (1) Model Training Time (ms): For the model training time, we want to measure
    both the total cumulative time and the longest individual training time across
    all devices. The calculation is provided in Equation (2). ModelTraining Time total
    = ∑ 𝑖=1 𝑛 𝑇 𝑖 andModelTraining Time max =max( 𝑇 1 , 𝑇 2 ,⋯, 𝑇 𝑛 ) (2) Anomaly
    Detection Accuracy: For a distributed system, we want to consider not only the
    overall accuracy but also the consistency of anomaly detection across different
    nodes. A weighted approach is used where the accuracy of each node is weighted
    by the number of instances it processes. This is provided in Equation (3). AnomalyDetectionAccuracy=
    ∑ 𝑛 𝑖=1 ( 𝑤 𝑖 × Accuracy 𝑖 ) ∑ 𝑛 𝑖=1 𝑤 𝑖 (3) Communication Efficiency: In a large-scale
    distributed setup, communication efficiency should account for the data transmission
    efficiency, the overhead of synchronization among nodes, the error rate in data
    transmission, and the effective utilization of available bandwidth. This comprehensive
    approach ensures a realistic assessment of communication performance in a distributed
    system. CE=( 𝐷 u 𝐷 t + 𝑂 sync )×(1−ER)×BU (4) 𝐶𝐸 represents Communication Efficiency.
    𝐷 𝑢 is the symbol for Useful Data Transmitted. 𝐷 𝑡 stands for Total Data Transmitted.
    𝑂 𝑠𝑦𝑛𝑐 is the Synchronization Overhead. 𝐸𝑅 denotes the Error Rate. 𝐵𝑈 symbolizes
    Bandwidth Utilization. Scalability: Scalability in a distributed system can be
    quantified by measuring how the system’s performance changes with the addition
    of more nodes, considering factors like throughput, response time, load balancing,
    system capacity, and cost-effectiveness. A higher throughput ratio, a lower response
    time ratio, and efficient load balancing with increased nodes indicate better
    scalability. Scalability ResponseTimeRatio LoadBalancingEfficiency SystemCapacityUtilization
    Cost-EffectivenessRatio = Throughputat𝑛nodes Throughputatasinglenode = ResponseTimeat𝑛nodes
    ResponseTimeatasinglenode = ∑ 𝑛 𝑖=1 LoadonNode𝑖 IdealLoadperNode×𝑛 = TotalProcessedLoad
    TotalSystemCapacity = TotalSystemCostat𝑛nodes PerformanceImprovementFactor (5)
    Starting with the evaluation of the first Algorithm 1, the results are shown in
    Figure 2. Figure 2. Performance evaluation of TinyCleanEDF. Incorporating the
    TinyCleanEDF algorithm into an IoT data management system has demonstrated quantifiable
    improvements in several key performance metrics. As shown in the preceding Figure,
    the deployment of this algorithm across an increasing number of nodes—from 1 to
    10—has yielded substantial benefits. Specifically, the anomaly detection accuracy
    improved with more nodes. For instance, there was a 10% increase in the accuracy
    on a single node compared to ten nodes. This improvement highlights the algorithm’s
    enhanced capability to identify and respond to data anomalies as the collaborative
    network of nodes expands. Moreover, the data processing and model training times,
    both critical for the efficient operation of IoT systems, show a decreasing trend
    as more nodes are engaged. Log-scaled values indicate that processing time decreased
    fourfold when the number of nodes increased from 1 to 10, which suggests a notable
    enhancement in the speed of data handling. Communication efficiency also saw a
    rise, which is particularly relevant in scenarios where network bandwidth is a
    limiting factor. This increase indicates a more optimal use of available resources,
    allowing for smoother data transfer between nodes and the central server. Lastly,
    scalability, which is also a significant metric, reflects the algorithm’s ability
    to maintain performance despite the growing scale of the network. The consistent
    upward trend across nodes validates that TinyCleanEDF is well-suited for environments
    where expansion is anticipated, ensuring that the system not only sustains its
    performance but actually improves as it scales. These results underscore the effectiveness
    of TinyCleanEDF in enhancing data quality and system robustness, making it a compelling
    choice for federated learning applications in distributed networks. Moving on
    to Algorithm 2, the results are presented in Figure 3. Figure 3. Performance evaluation
    of EdgeClusterML. The integration of the EdgeClusterML algorithm within an edge
    computing framework such as FL has yielded remarkable improvements in critical
    performance metrics. Notably, the algorithm achieved an impressive accuracy rate
    of approximately 90% when applied to real-world data streams. This represents
    a significant enhancement in the precision of data clustering, making it well-suited
    for applications like anomaly detection and data-driven decisionmaking. The observed
    increase in accuracy is particularly noteworthy as it directly impacts the algorithm’s
    ability to effectively group datapoints. Furthermore, our analysis reveals significant
    reductions in the clusterind speed. Specifically, the algorithm exhibited a time
    reduction of approximately 10% in the clustering speed when transitioning from
    one to ten nodes. These reductions are crucial in edge computing scenarios, ensuring
    real-time responsiveness and rapid adaptation to changing data patterns. The improvement
    in resource utilization and the adaptability score, with a roughly 15% increase
    as nodes scaled, signifies more efficient resource utilization and data transfer,
    particularly valuable in resource-constrained edge environments. In conclusion,
    EdgeClusterML emerges as a robust solution for edge computing environments, offering
    concrete benefits in terms of accuracy, clustering speed, resource utilization,
    and adaptability. Its reinforcement-learning-driven dynamic clustering approach
    positions it as a valuable asset for real-time data analysis and decisionmaking
    in dynamic edge scenarios. In the next steps, we evaluate Algorithm 3 in Figure
    4. Figure 4. Performance evaluation of CompressEdgeML. In our evaluation of the
    CompressEdgeML algorithm, significant improvements were observed across various
    performance metrics in edge computing environments. The algorithm, however, shows
    a good compression efficiency of up to 95% on single-device configurations, highlighting
    its effectiveness in data size reduction. This efficiency slightly decreases as
    the number of devices increases, stabilizing at 88% for configurations with 10
    devices, indicating a high level of data compression consistency. Compression
    speed, measured in milliseconds, displayed a marked improvement with increasing
    device numbers. For a single device, the compression time was logged at approximately
    1200 ms, which reduced logarithmically to around 300 ms for 10 devices. This reduction
    showcases the algorithm’s capability to handle larger data streams more efficiently,
    a critical attribute in real-time edge computing scenarios. Data integrity post-compression
    was maintained above 90% across all device configurations, peaking at 98% in a
    single-device setup. This metric underscores the algorithm’s reliability in preserving
    essential data characteristics during the compression process. Resource utilization
    also showed a positive trend, with efficiency increasing from 70% in a single-device
    scenario to 85% in a 10-device configuration. This improvement indicates the algorithm’s
    scalability and its efficient use of computational resources, which is vital in
    resource-constrained edge environments. In summary, CompressEdgeML demonstrates
    robust performance in adaptive data compression, marked by high compression efficiency,
    accelerated processing speeds, reliable data integrity, and efficient resource
    utilization. Its adaptability and scalability make it well-suited for diverse
    edge computing applications. The next algorithm is Algorithm 4, which is evaluated
    in Figure 5. Figure 5. Performance evaluation of CacheEdgeML. In our assessment
    of the CacheEdgeML algorithm, tailored for predictive and tiered data caching
    in edge computing settings, we observed substantial enhancements in pivotal performance
    metrics. The algorithm exhibited a cache hit rate of 85% in a single-device environment,
    which progressively increased to 92% with the addition of more devices. This upward
    trend signifies the algorithm’s enhanced accuracy in predicting data requests,
    a crucial factor in reducing redundant data retrieval operations. The cache update
    speed, a critical measure in dynamic environments, improved logarithmically from
    400 ms for one device to 250 ms for ten devices. This acceleration highlights
    the algorithm’s efficiency in adapting to changing data patterns, thereby optimizing
    caching strategies in real time. Cloud synchronization latency, through the HPC
    server, is essential for maintaining data consistency between edge and cloud storage,
    and it was also optimized. It decreased from 95 ms to 99 ms as the number of devices
    increased, demonstrating the algorithm’s effectiveness in synchronizing large
    volumes of data swiftly across distributed networks. Data retrieval efficiency,
    indicative of the algorithm’s performance in providing timely access to cached
    data, showed, however, a negative trajectory, decreasing from 200% in single-device
    setups to 140% in scenarios involving ten devices. This decrease shows that the
    algorithm requires more time to streamline data access, particularly in multi-device
    edge computing networks where data are distributed. In summary, CacheEdgeML emerges
    as a robust and adaptive solution for data caching in edge computing environments.
    Its strengths lie in its high cache hit rate, improved cache update speed, and
    reduced cloud synchronization latency. These attributes collectively ensure that
    CacheEdgeML is well-equipped for optimizing data caching processes in complex
    real-time edge computing scenarios. Lastly, we evaluate Algorithm 5 in Figure
    6. Figure 6. Performance evaluation of TinyHybridSenseQ. In our evaluation of
    the TinyHybridSenseQ algorithm, specifically designed for IoT sensors with a focus
    on data quality assessment and hybrid storage, we observed notable improvements
    in several key performance areas. The algorithm achieved a high Data Quality Score
    of 95% in a single-sensor setup, which marginally decreased to 90% as the number
    of sensors increased to ten. This trend highlights the algorithm’s robustness
    in maintaining high data quality standards even as the sensor network scales.
    Anomaly Detection Speed, a crucial metric in real-time environments, showed significant
    optimization. It improved logarithmically from 500 ms in a single-sensor scenario
    to 350 ms with ten sensors, illustrating the algorithm’s accelerated response
    in identifying data anomalies. This enhancement is pivotal for timely anomaly
    detection in dynamic sensor networks. Storage efficiency, essential in optimizing
    data storage across local and cloud resources, also saw progressive improvements.
    It increased from 85% to 91% as the number of sensors expanded. This increase
    indicates the algorithm’s capability to efficiently manage storage resources,
    a vital aspect in IoT environments where data volume is substantial. Data transfer
    latency, critical in ensuring the swift movement of data between sensors and storage
    facilities, was optimized with the expansion of the sensor network. It reduced
    from 250 ms for a single sensor to 160 ms for ten sensors, signifying the algorithm’s
    effectiveness in reducing data transfer times across a distributed network. In
    conclusion, TinyHybridSenseQ stands out as an effective solution for IoT sensors,
    excelling in data quality assessment and hybrid storage management. Its strengths
    lie in maintaining high data quality, fast anomaly detection, efficient storage
    utilization, and reduced data transfer latency. These attributes collectively
    position TinyHybridSenseQ as a highly capable tool in managing complex data workflows
    in IoT sensor networks, ensuring both data integrity and operational efficiency.
    4.3. Comparison with Similar Works In this section, we evaluate our proposed methods
    with similar works and approaches. In particular, we evaluate the CacheEdgeML
    with similar approaches, namely the Proximal Policy Optimization (PPO) caching
    method, the Markov Chain Monte Carlo (MCMC) method, and the deep reinforcement
    learning (DRL) method, based on our previous work [91], as well as methods like
    Least Frequently Used (LFU) and Least Recently Used (LRU) based on works [92,93,94].
    The results are provided in Figure 7. Figure 7. Cache hit rate comparison of CacheEdgeML
    with similar methods. As can be seen from the preceding figure, the cache hit
    rate of the proposed CacheEdgeML method outperforms the other five methods in
    terms of cache hit rate, reaching above 80% across all assessments. Additionally,
    we assess our proposed method named CompressEdgeML with a similar method presented
    in [72] named TAC. The results for the compression efficiency are provided in
    Figure 8, while the compression speed is provided in Figure 9. Figure 8. Compression
    efficiency of CompressEdgeML compared to similar method. Figure 9. Compression
    speed of CompressEdgeML compared to similar method. As can be seen from Figure
    8, the compression efficiency of our method is lower than TAC in the beginning;
    however, it reaches the performance of TAC when utilized on five devices and finally
    overtakes it using ten devices. Note that the TAC method in the original method
    is implemented only once and its performance remains stable across replication
    on our devices. As per the compression speed, for the assessment, we replicated
    the TAC method in our devices and assessed the compression while increasing the
    size of data in both methods. As per the compression speed, as can be seen from
    Figure 9, the compression speed of CompressEdgeML is lower, meaning faster compression
    while maintaining a good speed across all devices. 5. Conclusions and Future Work
    In the context of IoT, where vast quantities of data are generated, the integration
    of Edge AI and Big Data management plays a pivotal role in harnessing the full
    potential of these technologies. Edge AI, by processing data at the source rather
    than relying on distant servers, significantly reduces latency and enhances real-time
    data analysis. This approach is particularly beneficial in IoT systems, where
    immediate decisionmaking based on large-scale data is often required. In this
    context, effective data management becomes crucial, entailing not just the storage
    and retrieval of data but also its processing, analysis, and security. The intersection
    of Edge AI and Big Data management in IoT represents a forward step in technology,
    offering novel solutions to manage and leverage the ever-growing expanse of data
    in smart environments. This study has methodically evaluated five different TinyML
    algorithms named TinyCleanEDF, EdgeClusterML, CompressEdgeML, CacheEdgeML, and
    TinyHybridSenseQ—each tailored for specific functions within IoT systems utilizing
    edge computing. Our findings reveal that these algorithms substantially enhance
    the operational efficiency, data integrity, and real-time processing capabilities
    of IoT networks, particularly when implemented across a network of Raspberry Pi
    devices. TinyCleanEDF excels in federated learning and real-time anomaly detection,
    thus proving invaluable in scenarios requiring collaborative data processing and
    instantaneous anomaly identification. EdgeClusterML, with its reinforcement-learning-based
    dynamic clustering, demonstrates remarkable accuracy and optimal resource management,
    essential for real-time data analysis and decisionmaking processes. CompressEdgeML
    showcases its strength in adaptive data compression, achieving significant compression
    efficiency without compromising data integrity. CacheEdgeML, through its innovative
    caching strategy, ensures effective data retrieval and synchronization between
    edge and cloud storage, vital for seamless data management. Lastly, TinyHybridSenseQ
    effectively manages data quality and storage in IoT sensor networks, ensuring
    data reliability and operational efficiency. Future Work For future research,
    several key areas have been identified to further enhance the capabilities of
    these algorithms: Anomaly Detection: There is space for incorporating more advanced
    machine learning models to enhance the accuracy and speed of anomaly detection,
    especially in environments with complex or noisy data. This will allow for more
    precise identification of irregularities, enhancing the overall data integrity.
    Energy Efficiency: Optimizing the energy consumption of these algorithms is crucial,
    particularly in environments where energy resources are limited. Research should
    focus on developing energy-efficient methods that reduce the overall energy demand
    of the system without sacrificing performance. Cloud–Edge Integration: Enhancing
    the interaction between edge and cloud platforms is essential for improved data
    synchronization and storage efficiency. This involves developing methods for more
    seamless data processing and management in hybrid cloud–edge environments. Real-Time
    Data Processing: Optimizing these algorithms for real-time processing of streaming
    data is imperative. This would enable timely decisionmaking based on the most
    current data, a critical aspect in dynamic IoT environments. Security and Privacy:
    Strengthening the security and privacy features of these algorithms is important,
    especially for applications handling sensitive information. This involves implementing
    robust security measures to protect data from unauthorized access and ensure user
    privacy. Customization and Adaptability: Improving the adaptability of these algorithms
    to various IoT environments is necessary. Future work should aim at developing
    customizable solutions that can be tailored to meet specific requirements of different
    applications. Interoperability and Standardization: Promoting interoperability
    between diverse IoT devices and platforms and contributing to standardization
    efforts is crucial. This will facilitate smoother integration and communication
    across different systems and devices. Ultimately, this study demonstrates a robust
    framework for future breakthroughs in IoT data management within edge computing
    frameworks. The identified areas for future exploration present promising opportunities
    for extending the current capabilities of these algorithms and exploring novel
    possibilities in the era of IoT and edge computing. Author Contributions A.K.,
    A.G., C.K., L.T., C.S.M., G.A.K. and S.S., conceived the idea, designed and performed
    the experiments, analyzed the results, drafted the initial manuscript, and revised
    the final manuscript. All authors have read and agreed to the published version
    of the manuscript. Funding This research received no external funding. Data Availability
    Statement Data are contained within the article. Conflicts of Interest The authors
    declare no conflicts of interest. References Mashayekhy, Y.; Babaei, A.; Yuan,
    X.M.; Xue, A. Impact of Internet of Things (IoT) on Inventory Management: A Literature
    Survey. Logistics 2022, 6, 33. [Google Scholar] [CrossRef] Vonitsanos, G.; Panagiotakopoulos,
    T.; Kanavos, A. Issues and challenges of using blockchain for iot data management
    in smart healthcare. Biomed. J. Sci. Tech. Res. 2021, 40, 32052–32057. [Google
    Scholar] Zaidi, S.A.R.; Hayajneh, A.M.; Hafeez, M.; Ahmed, Q.Z. Unlocking Edge
    Intelligence Through Tiny Machine Learning (TinyML). IEEE Access 2022, 10, 100867–100877.
    [Google Scholar] [CrossRef] Ersoy, M.; Şansal, U. Analyze Performance of Embedded
    Systems with Machine Learning Algorithms. In Proceedings of the Trends in Data
    Engineering Methods for Intelligent Systems: Proceedings of the International
    Conference on Artificial Intelligence and Applied Mathematics in Engineering (ICAIAME
    2020), Antalya, Turkey, 18–20 April 2020; Springer: Berlin/Heidelberg, Germany,
    2021; pp. 231–236. [Google Scholar] Khobragade, P.; Ghutke, P.; Kalbande, V.P.;
    Purohit, N. Advancement in Internet of Things (IoT) Based Solar Collector for
    Thermal Energy Storage System Devices: A Review. In Proceedings of the 2022 2nd
    International Conference on Power Electronics & IoT Applications in Renewable
    Energy and its Control (PARC), Mathura, India, 21–22 January 2022; pp. 1–5. [Google
    Scholar] [CrossRef] Ayub Khan, A.; Laghari, A.A.; Shaikh, Z.A.; Dacko-Pikiewicz,
    Z.; Kot, S. Internet of Things (IoT) Security with Blockchain Technology: A State-of-the-Art
    Review. IEEE Access 2022, 10, 122679–122695. [Google Scholar] [CrossRef] Chauhan,
    C.; Ramaiya, M.K. Advanced Model for Improving IoT Security Using Blockchain Technology.
    In Proceedings of the 2022 4th International Conference on Smart Systems and Inventive
    Technology (ICSSIT), Tirunelveli, India, 20–22 January 2022; pp. 83–89. [Google
    Scholar] [CrossRef] Mohanta, B.K.; Jena, D.; Satapathy, U.; Patnaik, S. Survey
    on IoT security: Challenges and solution using machine learning, artificial intelligence
    and blockchain technology. Internet Things 2020, 11, 100227. [Google Scholar]
    [CrossRef] Jiang, Y.; Wang, C.; Wang, Y.; Gao, L. A Cross-Chain Solution to Integrating
    Multiple Blockchains for IoT Data Management. Sensors 2019, 19, 2042. [Google
    Scholar] [CrossRef] Ren, H.; Anicic, D.; Runkler, T. How to Manage Tiny Machine
    Learning at Scale: An Industrial Perspective. arXiv 2022, arXiv:2202.09113. [Google
    Scholar] Keserwani, P.K.; Govil, M.C.; Pilli, E.S.; Govil, P. A smart anomaly-based
    intrusion detection system for the Internet of Things (IoT) network using GWO–PSO–RF
    model. J. Reliab. Intell. Environ. 2021, 7, 3–21. [Google Scholar] [CrossRef]
    Gibbs, M.; Woodward, K.; Kanjo, E. Combining Multiple tinyML Models for Multimodal
    Context-Aware Stress Recognition on Constrained Microcontrollers. IEEE Micro 2023,
    1–9. [Google Scholar] [CrossRef] Chen, Z.; Gao, Y.; Liang, J. LOPdM: A Low-power
    On-device Predictive Maintenance System Based on Self-powered Sensing and TinyML.
    IEEE Trans. Instrum. Meas. 2023, 72, 2525213. [Google Scholar] [CrossRef] Savanna,
    R.L.; Hanyurwimfura, D.; Nsenga, J.; Rwigema, J. A Wearable Device for Respiratory
    Diseases Monitoring in Crowded Spaces. Case Study of COVID-19. In Proceedings
    of the International Congress on Information and Communication Technology, London,
    UK, 20–23 February 2023; Springer: Berlin/Heidelberg, Germany, 2023; pp. 515–528.
    [Google Scholar] Nguyen, H.T.; Mai, N.D.; Lee, B.G.; Chung, W.Y. Behind-the-Ear
    EEG-Based Wearable Driver Drowsiness Detection System Using Embedded Tiny Neural
    Networks. IEEE Sens. J. 2023, 23, 23875–23892. [Google Scholar] [CrossRef] Hussein,
    D.; Bhat, G. SensorGAN: A Novel Data Recovery Approach for Wearable Human Activity
    Recognition. ACM Trans. Embed. Comput. Syst. 2023. [Google Scholar] [CrossRef]
    Zacharia, A.; Zacharia, D.; Karras, A.; Karras, C.; Giannoukou, I.; Giotopoulos,
    K.C.; Sioutas, S. An Intelligent Microprocessor Integrating TinyML in Smart Hotels
    for Rapid Accident Prevention. In Proceedings of the 2022 7th South-East Europe
    Design Automation, Computer Engineering, Computer Networks and Social Media Conference
    (SEEDA-CECNSM), Ioannina, Greece, 23–25 September 2022; pp. 1–7. [Google Scholar]
    [CrossRef] Atanane, O.; Mourhir, A.; Benamar, N.; Zennaro, M. Smart Buildings:
    Water Leakage Detection Using TinyML. Sensors 2023, 23, 9210. [Google Scholar]
    [CrossRef] Malche, T.; Maheshwary, P.; Tiwari, P.K.; Alkhayyat, A.H.; Bansal,
    A.; Kumar, R. Efficient solid waste inspection through drone-based aerial imagery
    and TinyML vision model. Trans. Emerg. Telecommun. Technol. 2023, e4878. [Google
    Scholar] [CrossRef] Hammad, S.S.; Iskandaryan, D.; Trilles, S. An unsupervised
    TinyML approach applied to the detection of urban noise anomalies under the smart
    cities environment. Internet Things 2023, 23, 100848. [Google Scholar] [CrossRef]
    Priya, S.K.; Balaganesh, N.; Karthika, K.P. Integration of AI, Blockchain, and
    IoT Technologies for Sustainable and Secured Indian Public Distribution System.
    In AI Models for Blockchain-Based Intelligent Networks in IoT Systems: Concepts,
    Methodologies, Tools, and Applications; Springer: Berlin/Heidelberg, Germany,
    2023; pp. 347–371. [Google Scholar] Flores, T.; Silva, M.; Azevedo, M.; Medeiros,
    T.; Medeiros, M.; Silva, I.; Dias Santos, M.M.; Costa, D.G. TinyML for Safe Driving:
    The Use of Embedded Machine Learning for Detecting Driver Distraction. In Proceedings
    of the 2023 IEEE International Workshop on Metrology for Automotive (MetroAutomotive),
    Modena, Italy, 28–30 June 2023; pp. 62–66. [Google Scholar] [CrossRef] Nkuba,
    C.K.; Woo, S.; Lee, H.; Dietrich, S. ZMAD: Lightweight Model-Based Anomaly Detection
    for the Structured Z-Wave Protocol. IEEE Access 2023, 11, 60562–60577. [Google
    Scholar] [CrossRef] Shabir, M.Y.; Torta, G.; Basso, A.; Damiani, F. Toward Secure
    TinyML on a Standardized AI Architecture. In Device-Edge-Cloud Continuum: Paradigms,
    Architectures and Applications; Springer: Berlin/Heidelberg, Germany, 2023; pp.
    121–139. [Google Scholar] Tsoukas, V.; Gkogkidis, A.; Boumpa, E.; Papafotikas,
    S.; Kakarountas, A. A Gas Leakage Detection Device Based on the Technology of
    TinyML. Technologies 2023, 11, 45. [Google Scholar] [CrossRef] Hayajneh, A.M.;
    Aldalahmeh, S.A.; Alasali, F.; Al-Obiedollah, H.; Zaidi, S.A.; McLernon, D. Tiny
    machine learning on the edge: A framework for transfer learning empowered unmanned
    aerial vehicle assisted smart farming. IET Smart Cities 2023. [Google Scholar]
    [CrossRef] Adeola, J.O.; Degila, J.; Zennaro, M. Recent Advances in Plant Diseases
    Detection With Machine Learning: Solution for Developing Countries. In Proceedings
    of the 2022 IEEE International Conference on Smart Computing (SMARTCOMP), Helsinki,
    Finland, 20–24 June 2022; pp. 374–380. [Google Scholar] [CrossRef] Tsoukas, V.;
    Gkogkidis, A.; Kakarountas, A. A TinyML-Based System for Smart Agriculture. In
    Proceedings of the 26th Pan-Hellenic Conference on Informatics, New York, NY,
    USA, 25–27 November 2023; pp. 207–212. [Google Scholar] [CrossRef] Nicolas, C.;
    Naila, B.; Amar, R.C. TinyML Smart Sensor for Energy Saving in Internet of Things
    Precision Agriculture platform. In Proceedings of the 2022 Thirteenth International
    Conference on Ubiquitous and Future Networks (ICUFN), Barcelona, Spain, 5–8 July
    2022; pp. 256–259. [Google Scholar] [CrossRef] Nicolas, C.; Naila, B.; Amar, R.C.
    Energy efficient Firmware over the Air Update for TinyML models in LoRaWAN agricultural
    networks. In Proceedings of the 2022 32nd International Telecommunication Networks
    and Applications Conference (ITNAC), Wellington, New Zealand, 30 November–2 December
    2022; pp. 21–27. [Google Scholar] [CrossRef] Viswanatha, V.; Ramachandra, A.C.;
    Hegde, P.T.; Raghunatha Reddy, M.V.; Hegde, V.; Sabhahit, V. Implementation of
    Smart Security System in Agriculture fields Using Embedded Machine Learning. In
    Proceedings of the 2023 International Conference on Applied Intelligence and Sustainable
    Computing (ICAISC), Zakopane, Poland, 18–22 June 2023; pp. 1–6. [Google Scholar]
    [CrossRef] Botero-Valencia, J.; Barrantes-Toro, C.; Marquez-Viloria, D.; Pearce,
    J.M. Low-cost air, noise, and light pollution measuring station with wireless
    communication and tinyML. HardwareX 2023, 16, e00477. [Google Scholar] [CrossRef]
    Li, T.; Luo, J.; Liang, K.; Yi, C.; Ma, L. Synergy of Patent and Open-Source-Driven
    Sustainable Climate Governance under Green AI: A Case Study of TinyML. Sustainability
    2023, 15, 13779. [Google Scholar] [CrossRef] Ihoume, I.; Tadili, R.; Arbaoui,
    N.; Benchrifa, M.; Idrissi, A.; Daoudi, M. Developing a TinyML-Oriented Deep Learning
    Model for an Intelligent Greenhouse Microclimate Control from Multivariate Sensed
    Data. In Intelligent Sustainable Systems: Selected Papers of WorldS4 2022; Springer:
    Berlin/Heidelberg, Germany, 2023; Volume 2, pp. 283–291. [Google Scholar] Prakash,
    S.; Stewart, M.; Banbury, C.; Mazumder, M.; Warden, P.; Plancher, B.; Reddi, V.J.
    Is TinyML Sustainable? Assessing the Environmental Impacts of Machine Learning
    on Microcontrollers. arXiv 2023, arXiv:2301.11899. [Google Scholar] Soni, S.;
    Khurshid, A.; Minase, A.M.; Bonkinpelliwar, A. A TinyML Approach for Quantification
    of BOD and COD in Water. In Proceedings of the 2023 2nd International Conference
    on Paradigm Shifts in Communications Embedded Systems, Machine Learning and Signal
    Processing (PCEMS), Nagpur, India, 5–6 April 2023; pp. 1–6. [Google Scholar] [CrossRef]
    Arratia, B.; Prades, J.; Peña-Haro, S.; Cecilia, J.M.; Manzoni, P. BODOQUE: An
    Energy-Efficient Flow Monitoring System for Ephemeral Streams. In Proceedings
    of the Twenty-fourth International Symposium on Theory, Algorithmic Foundations,
    and Protocol Design for Mobile Networks and Mobile Computing, Washington, DC,
    USA, 23–26 October 2023; pp. 358–363. [Google Scholar] Wardana, I.N.K.; Fahmy,
    S.A.; Gardner, J.W. TinyML Models for a Low-Cost Air Quality Monitoring Device.
    IEEE Sens. Lett. 2023, 7, 1–4. [Google Scholar] [CrossRef] Sanchez-Iborra, R.
    LPWAN and Embedded Machine Learning as Enablers for the Next Generation of Wearable
    Devices. Sensors 2021, 21, 5218. [Google Scholar] [CrossRef] Hussein, M.; Mohammed,
    Y.S.; Galal, A.I.; Abd-Elrahman, E.; Zorkany, M. Smart Cognitive IoT Devices Using
    Multi-Layer Perception Neural Network on Limited Microcontroller. Sensors 2022,
    22, 5106. [Google Scholar] [CrossRef] Prakash, S.; Callahan, T.; Bushagour, J.;
    Banbury, C.; Green, A.V.; Warden, P.; Ansell, T.; Reddi, V.J. CFU Playground:
    Full-Stack Open-Source Framework for Tiny Machine Learning (TinyML) Acceleration
    on FPGAs. In Proceedings of the 2023 IEEE International Symposium on Performance
    Analysis of Systems and Software (ISPASS), Raleigh, NC, USA, 23–25 April 2023;
    pp. 157–167. [Google Scholar] [CrossRef] Gibbs, M.; Kanjo, E. Realising the Power
    of Edge Intelligence: Addressing the Challenges in AI and tinyML Applications
    for Edge Computing. In Proceedings of the 2023 IEEE International Conference on
    Edge Computing and Communications (EDGE), Chicago, IL, USA, 2–8 July 2023; pp.
    337–343. [Google Scholar] [CrossRef] Shafique, M.; Theocharides, T.; Reddy, V.J.;
    Murmann, B. TinyML: Current Progress, Research Challenges, and Future Roadmap.
    In Proceedings of the 2021 58th ACM/IEEE Design Automation Conference (DAC), San
    Francisco, CA, USA, 5–9 December 2021; pp. 1303–1306. [Google Scholar] [CrossRef]
    Banbury, C.R.; Reddi, V.J.; Lam, M.; Fu, W.; Fazel, A.; Holleman, J.; Huang, X.;
    Hurtado, R.; Kanter, D.; Lokhmotov, A.; et al. Benchmarking tinyml systems: Challenges
    and direction. arXiv 2020, arXiv:2003.04821. [Google Scholar] Ooko, S.O.; Muyonga
    Ogore, M.; Nsenga, J.; Zennaro, M. TinyML in Africa: Opportunities and Challenges.
    In Proceedings of the 2021 IEEE Globecom Workshops (GC Wkshps), Madrid, Spain,
    7–11 December 2021; pp. 1–6. [Google Scholar] [CrossRef] Sanchez-Iborra, R.; Skarmeta,
    A.F. TinyML-Enabled Frugal Smart Objects: Challenges and Opportunities. IEEE Circuits
    Syst. Mag. 2020, 20, 4–18. [Google Scholar] [CrossRef] Mishra, N.; Lin, C.C.;
    Chang, H.T. A Cognitive Oriented Framework for IoT Big-data Management Prospective.
    In Proceedings of the 2014 IEEE International Conference on Communiction Problem-Solving,
    Beijing, China, 5–7 December 2014; pp. 124–127. [Google Scholar] [CrossRef] Mishra,
    N.; Lin, C.C.; Chang, H.T. A cognitive adopted framework for IoT big-data management
    and knowledge discovery prospective. Int. J. Distrib. Sens. Netw. 2015, 11, 718390.
    [Google Scholar] [CrossRef] Huang, X.; Fan, J.; Deng, Z.; Yan, J.; Li, J.; Wang,
    L. Efficient IoT data management for geological disasters based on big data-turbocharged
    data lake architecture. ISPRS Int. J. Geo-Inf. 2021, 10, 743. [Google Scholar]
    [CrossRef] Oktian, Y.E.; Lee, S.G.; Lee, B.G. Blockchain-based continued integrity
    service for IoT big data management: A comprehensive design. Electronics 2020,
    9, 1434. [Google Scholar] [CrossRef] Lê, M.T.; Arbel, J. TinyMLOps for real-time
    ultra-low power MCUs applied to frame-based event classification. In Proceedings
    of the 3rd Workshop on Machine Learning and Systems, Rome, Italy, 8 May 2023;
    pp. 148–153. [Google Scholar] Doyu, H.; Morabito, R.; Brachmann, M. A TinyMLaaS
    Ecosystem for Machine Learning in IoT: Overview and Research Challenges. In Proceedings
    of the 2021 International Symposium on VLSI Design, Automation and Test (VLSI-DAT),
    Hsinchu, Taiwan, 19–22 April 2021; pp. 1–5. [Google Scholar] [CrossRef] Lin, J.;
    Zhu, L.; Chen, W.M.; Wang, W.C.; Han, S. Tiny Machine Learning: Progress and Futures
    [Feature]. IEEE Circuits Syst. Mag. 2023, 23, 8–34. [Google Scholar] [CrossRef]
    Schizas, N.; Karras, A.; Karras, C.; Sioutas, S. TinyML for Ultra-Low Power AI
    and Large Scale IoT Deployments: A Systematic Review. Future Internet 2022, 14,
    363. [Google Scholar] [CrossRef] Alajlan, N.N.; Ibrahim, D.M. TinyML: Enabling
    of Inference Deep Learning Models on Ultra-Low-Power IoT Edge Devices for AI Applications.
    Micromachines 2022, 13, 851. [Google Scholar] [CrossRef] Han, H.; Siebert, J.
    TinyML: A Systematic Review and Synthesis of Existing Research. In Proceedings
    of the 2022 International Conference on Artificial Intelligence in Information
    and Communication (ICAIIC), Jeju Island, Republic of Korea, 21–24 February 2022;
    pp. 269–274. [Google Scholar] [CrossRef] Andrade, P.; Silva, I.; Silva, M.; Flores,
    T.; Cassiano, J.; Costa, D.G. A TinyML Soft-Sensor Approach for Low-Cost Detection
    and Monitoring of Vehicular Emissions. Sensors 2022, 22, 3838. [Google Scholar]
    [CrossRef] Wongthongtham, P.; Kaur, J.; Potdar, V.; Das, A. Big data challenges
    for the Internet of Things (IoT) paradigm. In Connected Environments for the Internet
    of Things: Challenges and Solutions; Springer: Berlin/Heidelberg, Germany, 2017;
    pp. 41–62. [Google Scholar] Shu, L.; Mukherjee, M.; Pecht, M.; Crespi, N.; Han,
    S.N. Challenges and Research Issues of Data Management in IoT for Large-Scale
    Petrochemical Plants. IEEE Syst. J. 2018, 12, 2509–2523. [Google Scholar] [CrossRef]
    Gore, R.; Valsan, S.P. Big Data challenges in smart Grid IoT (WAMS) deployment.
    In Proceedings of the 2016 8th International Conference on Communication Systems
    and Networks (COMSNETS), Bangalore, India, 5–10 January 2016; pp. 1–6. [Google
    Scholar] [CrossRef] Touqeer, H.; Zaman, S.; Amin, R.; Hussain, M.; Al-Turjman,
    F.; Bilal, M. Smart home security: Challenges, issues and solutions at different
    IoT layers. J. Supercomput. 2021, 77, 14053–14089. [Google Scholar] [CrossRef]
    Kumari, K.; Mrunalini, M. A Framework for Analysis of Incompleteness and Security
    Challenges in IoT Big Data. Int. J. Inf. Secur. Priv. (IJISP) 2022, 16, 1–13.
    [Google Scholar] [CrossRef] Zhang, Y.; Adin, V.; Bader, S.; Oelmann, B. Leveraging
    Acoustic Emission and Machine Learning for Concrete Materials Damage Classification
    on Embedded Devices. IEEE Trans. Instrum. Meas. 2023, 72, 2525108. [Google Scholar]
    [CrossRef] Moin, A.; Challenger, M.; Badii, A.; Günnemann, S. Supporting AI Engineering
    on the IoT Edge through Model-Driven TinyML. In Proceedings of the 2022 IEEE 46th
    Annual Computers, Software, and Applications Conference (COMPSAC), Los Alamitos,
    CA, USA, 27 June–1 July 2022; pp. 884–893. [Google Scholar] [CrossRef] David,
    R.; Duke, J.; Jain, A.; Janapa Reddi, V.; Jeffries, N.; Li, J.; Kreeger, N.; Nappier,
    I.; Natraj, M.; Wang, T.; et al. Tensorflow lite micro: Embedded machine learning
    for tinyml systems. Proc. Mach. Learn. Syst. 2021, 3, 800–811. [Google Scholar]
    Qian, C.; Einhaus, L.; Schiele, G. ElasticAI-Creator: Optimizing Neural Networks
    for Time-Series-Analysis for on-Device Machine Learning in IoT Systems. In Proceedings
    of the 20th ACM Conference on Embedded Networked Sensor Systems; Association for
    Computing Machinery: New York, NY, USA, 2023; pp. 941–946. [Google Scholar] [CrossRef]
    Giordano, M.; Baumann, N.; Crabolu, M.; Fischer, R.; Bellusci, G.; Magno, M. Design
    and Performance Evaluation of an Ultralow-Power Smart IoT Device with Embedded
    TinyML for Asset Activity Monitoring. IEEE Trans. Instrum. Meas. 2022, 71, 2510711.
    [Google Scholar] [CrossRef] Bamoumen, H.; Temouden, A.; Benamar, N.; Chtouki,
    Y. How TinyML Can be Leveraged to Solve Environmental Problems: A Survey. In Proceedings
    of the 2022 International Conference on Innovation and Intelligence for Informatics,
    Computing, and Technologies (3ICT), Sakheer, Bahrain, 20–21 November 2022; pp.
    338–343. [Google Scholar] [CrossRef] Athanasakis, G.; Filios, G.; Katsidimas,
    I.; Nikoletseas, S.; Panagiotou, S.H. TinyML-based approach for Remaining Useful
    Life Prediction of Turbofan Engines. In Proceedings of the 2022 IEEE 27th International
    Conference on Emerging Technologies and Factory Automation (ETFA), Stuttgart,
    Germany, 6–9 September 2022; pp. 1–8. [Google Scholar] [CrossRef] Silva, M.; Signoretti,
    G.; Flores, T.; Andrade, P.; Silva, J.; Silva, I.; Sisinni, E.; Ferrari, P. A
    data-stream TinyML compression algorithm for vehicular applications: A case study.
    In Proceedings of the 2022 IEEE International Workshop on Metrology for Industry
    4.0 & IoT (MetroInd4.0&IoT), Trento, Italy, 7–9 June 2022; pp. 408–413. [Google
    Scholar] [CrossRef] Ostrovan, E. TinyML On-Device Neural Network Training. Master’s
    Thesis, Politecnico di Milano, Milan, Italy, 2022. [Google Scholar] Signoretti,
    G.; Silva, M.; Andrade, P.; Silva, I.; Sisinni, E.; Ferrari, P. An Evolving TinyML
    Compression Algorithm for IoT Environments Based on Data Eccentricity. Sensors
    2021, 21, 4153. [Google Scholar] [CrossRef] Sharif, U.; Mueller-Gritschneder,
    D.; Stahl, R.; Schlichtmann, U. Efficient Software-Implemented HW Fault Tolerance
    for TinyML Inference in Safety-critical Applications. In Proceedings of the 2023
    Design, Automation & Test in Europe Conference & Exhibition (DATE), Antwerp, Belgium,
    17–19 April 2023; IEEE: Piscataway, NJ, USA, 2023; pp. 1–6. [Google Scholar] Fedorov,
    I.; Matas, R.; Tann, H.; Zhou, C.; Mattina, M.; Whatmough, P. UDC: Unified DNAS
    for compressible TinyML models. arXiv 2022, arXiv:2201.05842. [Google Scholar]
    Nadalini, D.; Rusci, M.; Benini, L.; Conti, F. Reduced Precision Floating-Point
    Optimization for Deep Neural Network On-Device Learning on MicroControllers. arXiv
    2023, arXiv:2305.19167. [Google Scholar] [CrossRef] Silva, M.; Medeiros, T.; Azevedo,
    M.; Medeiros, M.; Themoteo, M.; Gois, T.; Silva, I.; Costa, D.G. An Adaptive TinyML
    Unsupervised Online Learning Algorithm for Driver Behavior Analysis. In Proceedings
    of the 2023 IEEE International Workshop on Metrology for Automotive (MetroAutomotive),
    Modena, Italy, 28–30 June 2023; IEEE: Piscataway, NJ, USA, 2023; pp. 199–204.
    [Google Scholar] Pereira, E.S.; Marcondes, L.S.; Silva, J.M. On-Device Tiny Machine
    Learning for Anomaly Detection Based on the Extreme Values Theory. IEEE Micro
    2023, 43, 58–65. [Google Scholar] [CrossRef] Zhuo, S.; Chen, H.; Ramakrishnan,
    R.K.; Chen, T.; Feng, C.; Lin, Y.; Zhang, P.; Shen, L. An empirical study of low
    precision quantization for tinyml. arXiv 2022, arXiv:2203.05492. [Google Scholar]
    Krishna, A.; Nudurupati, S.R.; Dwivedi, P.; van Schaik, A.; Mehendale, M.; Thakur,
    C.S. RAMAN: A Re-configurable and Sparse tinyML Accelerator for Inference on Edge.
    arXiv 2023, arXiv:2306.06493. [Google Scholar] Ren, H.; Anicic, D.; Runkler, T.A.
    TinyReptile: TinyML with Federated Meta-Learning. arXiv 2023, arXiv:2304.05201.
    [Google Scholar] Ren, H.; Anicic, D.; Runkler, T.A. Towards Semantic Management
    of On-Device Applications in Industrial IoT. ACM Trans. Internet Technol. 2022,
    22, 1–30. [Google Scholar] [CrossRef] Chen, J.I.Z.; Huang, P.F.; Pi, C.S. The
    implementation and performance evaluation for a smart robot with edge computing
    algorithms. Ind. Robot. Int. J. Robot. Res. Appl. 2023, 50, 581–594. [Google Scholar]
    [CrossRef] Mohammed, M.; Srinivasagan, R.; Alzahrani, A.; Alqahtani, N.K. Machine-Learning-Based
    Spectroscopic Technique for Non-Destructive Estimation of Shelf Life and Quality
    of Fresh Fruits Packaged under Modified Atmospheres. Sustainability 2023, 15,
    12871. [Google Scholar] [CrossRef] Koufos, K.; EI Haloui, K.; Dianati, M.; Higgins,
    M.; Elmirghani, J.; Imran, M.A.; Tafazolli, R. Trends in Intelligent Communication
    Systems: Review of Standards, Major Research Projects, and Identification of Research
    Gaps. J. Sens. Actuator Netw. 2021, 10, 60. [Google Scholar] [CrossRef] Ahad,
    M.A.; Tripathi, G.; Zafar, S.; Doja, F. IoT Data Management—Security Aspects of
    Information Linkage in IoT Systems. In Principles of Internet of Things (IoT)
    Ecosystem: Insight Paradigm; Peng, S.L., Pal, S., Huang, L., Eds.; Springer International
    Publishing: Cham, Switzerland, 2020; pp. 439–464. [Google Scholar] [CrossRef]
    Liu, R.W.; Nie, J.; Garg, S.; Xiong, Z.; Zhang, Y.; Hossain, M.S. Data-Driven
    Trajectory Quality Improvement for Promoting Intelligent Vessel Traffic Services
    in 6G-Enabled Maritime IoT Systems. IEEE Internet Things J. 2021, 8, 5374–5385.
    [Google Scholar] [CrossRef] Hnatiuc, B.; Paun, M.; Sintea, S.; Hnatiuc, M. Power
    management for supply of IoT Systems. In Proceedings of the 2022 26th International
    Conference on Circuits, Systems, Communications and Computers (CSCC), Crete, Greece,
    19–22 July 2022; pp. 216–221. [Google Scholar] [CrossRef] Rajeswari, S.; Ponnusamy,
    V. AI-Based IoT analytics on the cloud for diabetic data management system. In
    Integrating AI in IoT Analytics on the Cloud for Healthcare Applications; IGI
    Global: Hershey, PA, USA, 2022; pp. 143–161. [Google Scholar] Karras, A.; Karras,
    C.; Giotopoulos, K.C.; Tsolis, D.; Oikonomou, K.; Sioutas, S. Peer to peer federated
    learning: Towards decentralized machine learning on edge devices. In Proceedings
    of the 2022 7th South-East Europe Design Automation, Computer Engineering, Computer
    Networks and Social Media Conference (SEEDA-CECNSM), Ioannina, Greece, 23–25 September
    2022; IEEE: Piscataway, NJ, USA, 2022; pp. 1–9. [Google Scholar] Karras, A.; Karras,
    C.; Giotopoulos, K.C.; Tsolis, D.; Oikonomou, K.; Sioutas, S. Federated Edge Intelligence
    and Edge Caching Mechanisms. Information 2023, 14, 414. [Google Scholar] [CrossRef]
    Karras, A.; Karras, C.; Karydis, I.; Avlonitis, M.; Sioutas, S. An Adaptive, Energy-Efficient
    DRL-Based and MCMC-Based Caching Strategy for IoT Systems. In Algorithmic Aspects
    of Cloud Computing; Chatzigiannakis, I., Karydis, I., Eds.; Springer: Cham, Switzerland,
    2024; pp. 66–85. [Google Scholar] Meddeb, M.; Dhraief, A.; Belghith, A.; Monteil,
    T.; Drira, K. How to cache in ICN-based IoT environments? In Proceedings of the
    2017 IEEE/ACS 14th International Conference on Computer Systems and Applications
    (AICCSA), Hammamet, Tunisia, 30 October–3 November 2017; IEEE: Piscataway, NJ,
    USA, 2017; pp. 1117–1124. [Google Scholar] Wang, S.; Zhang, X.; Zhang, Y.; Wang,
    L.; Yang, J.; Wang, W. A survey on mobile edge networks: Convergence of computing,
    caching and communications. IEEE Access 2017, 5, 6757–6779. [Google Scholar] [CrossRef]
    Zhong, C.; Gursoy, M.C.; Velipasalar, S. A deep reinforcement learning-based framework
    for content caching. In Proceedings of the 2018 52nd Annual Conference on Information
    Sciences and Systems (CISS), Princeton, NJ, USA, 21–23 March 2018; IEEE: Piscataway,
    NJ, USA, 2018; pp. 1–6. [Google Scholar] Disclaimer/Publisher’s Note: The statements,
    opinions and data contained in all publications are solely those of the individual
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or
    the editor(s) disclaim responsibility for any injury to people or property resulting
    from any ideas, methods, instructions or products referred to in the content.  ©
    2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open
    access article distributed under the terms and conditions of the Creative Commons
    Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share
    and Cite MDPI and ACS Style Karras, A.; Giannaros, A.; Karras, C.; Theodorakopoulos,
    L.; Mammassis, C.S.; Krimpas, G.A.; Sioutas, S. TinyML Algorithms for Big Data
    Management in Large-Scale IoT Systems. Future Internet 2024, 16, 42. https://doi.org/10.3390/fi16020042
    AMA Style Karras A, Giannaros A, Karras C, Theodorakopoulos L, Mammassis CS, Krimpas
    GA, Sioutas S. TinyML Algorithms for Big Data Management in Large-Scale IoT Systems.
    Future Internet. 2024; 16(2):42. https://doi.org/10.3390/fi16020042 Chicago/Turabian
    Style Karras, Aristeidis, Anastasios Giannaros, Christos Karras, Leonidas Theodorakopoulos,
    Constantinos S. Mammassis, George A. Krimpas, and Spyros Sioutas. 2024. \"TinyML
    Algorithms for Big Data Management in Large-Scale IoT Systems\" Future Internet
    16, no. 2: 42. https://doi.org/10.3390/fi16020042 Note that from the first issue
    of 2016, this journal uses article numbers instead of page numbers. See further
    details here. Article Metrics Citations No citations were found for this article,
    but you may check on Google Scholar Article Access Statistics Article access statistics
    Article Views 25. Jan 4. Feb 14. Feb 24. Feb 5. Mar 15. Mar 25. Mar 4. Apr 0 500
    1000 1500 2000 For more information on the journal statistics, click here. Multiple
    requests from the same IP address are counted as one view.   Future Internet,
    EISSN 1999-5903, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Future Internet
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: TinyML Algorithms for Big Data Management in Large-Scale IoT Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wang X.
  - Wu W.
  - Du Y.
  - Cao J.
  - Chen Q.
  - Xia Y.
  citation_count: '1'
  description: The emergence of the Internet of Things (IoT) has facilitated the development
    and usage of low-computational microcontrollers at the edge of the network, which
    process data in the proximity of data sources and thereby offload the pressure
    of data transmission. Recently, IoT is becoming a key technology for structural
    health monitoring (SHM) systems. This study designs a novel wireless IoT monitoring
    system for the Hong Kong-Zhuhai-Macao Bridge, the world longest sea-crossing bridge.
    The 5G technology and edge computing are integrated to improve the system performance
    in sensor serviceability, data transmission, time synchronization, and data quality
    control. The artificial intelligent (AI) algorithm is embedded into the NVIDIA
    Xavier NX edge computing boards to preliminarily detect data anomalies caused
    by sensor faults, before uploading the massive data to the cloud platform. As
    training AI models requires a large amount of labeled data and is always time
    consuming, a novel data anomaly detection method is developed by transferring
    the model trained from the other bridge to the target bridge. Given that prestoring
    source data in edge devices consumes expensive storage resources, the source-free
    domain adaptation is developed by integrating the robust self-Training mechanism
    and self-knowledge distillation strategy. Thus, the model transfer is achieved
    cross bridges in the absence of source data. This study provides a valuable and
    practical reference for developing a wireless IoT SHM system for large-scale infrastructure
    and enabling edge computing for data anomaly detection with high efficiency and
    accuracy.
  doi: 10.1109/JIOT.2023.3300073
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Volume: 11 Issue: 3 Wireless IoT Monitoring System in Hong Kong–Zhuhai–Macao
    Bridge and Edge Computing for Anomaly Detection Publisher: IEEE Cite This PDF
    Xiaoyou Wang; Wanglin Wu; Yao Du; Jiannong Cao; Qianyi Chen; Yong Xia All Authors
    1 Cites in Paper 358 Full Text Views Abstract Document Sections I. Introduction
    II. Wireless SHM System and Edge Computing of the Qingzhou Bridge III. Edge Computing
    for Data Anomaly Detection IV. Cross-Bridge Model Transfer V. Conclusion Authors
    Figures References Citations Keywords Metrics Abstract: The emergence of the Internet
    of Things (IoT) has facilitated the development and usage of low-computational
    microcontrollers at the edge of the network, which process data in the proximity
    of data sources and thereby offload the pressure of data transmission. Recently,
    IoT is becoming a key technology for structural health monitoring (SHM) systems.
    This study designs a novel wireless IoT monitoring system for the Hong Kong–Zhuhai–Macao
    Bridge, the world longest sea-crossing bridge. The 5G technology and edge computing
    are integrated to improve the system performance in sensor serviceability, data
    transmission, time synchronization, and data quality control. The artificial intelligent
    (AI) algorithm is embedded into the NVIDIA Xavier NX edge computing boards to
    preliminarily detect data anomalies caused by sensor faults, before uploading
    the massive data to the cloud platform. As training AI models requires a large
    amount of labeled data and is always time consuming, a novel data anomaly detection
    method is developed by transferring the model trained from the other bridge to
    the target bridge. Given that prestoring source data in edge devices consumes
    expensive storage resources, the source-free domain adaptation is developed by
    integrating the robust self-training mechanism and self-knowledge distillation
    strategy. Thus, the model transfer is achieved cross bridges in the absence of
    source data. This study provides a valuable and practical reference for developing
    a wireless IoT SHM system for large-scale infrastructure and enabling edge computing
    for data anomaly detection with high efficiency and accuracy. Published in: IEEE
    Internet of Things Journal ( Volume: 11, Issue: 3, 01 February 2024) Page(s):
    4763 - 4774 Date of Publication: 31 July 2023 ISSN Information: DOI: 10.1109/JIOT.2023.3300073
    Publisher: IEEE Funding Agency: SECTION I. Introduction Recent years have witnessed
    an increase in the construction of long-span bridges worldwide [1], [2], [3].
    Monitoring the health condition of bridges during their service life is crucial
    to ensure infrastructure safety and economic development. The traditional structural
    health monitoring (SHM) system typically consists of wired sensors and a centralized
    data acquisition system, with sensors strategically placed to continuously monitor
    structural vibrations, loads, and environmental conditions [4], [5]. However,
    challenges may arise during the installation of wired sensors in specific positions
    of long-span sea-crossing bridges. The wire connection and high maintenance costs
    of lines remain complex issues in the wired SHM system [6]. Moreover, the sensor
    layout in the SHM design stage may not be optimal. Moving or adding sensors in
    the existing SHM system is costly. The next-generation Internet of Things (IoT)
    is becoming a key technology for SHM system with the advances of sensor technologies
    [7]. The wireless sensing network (WSN) has revolutionized the way engineers monitor
    bridges. Sensors can be flexibly added, installed, or relocated at selected locations
    to collect data, which are transmitted wirelessly to the cloud platform. Hence,
    the efficiency of bridge monitoring can be improved, and the cost can be reduced.
    Many studies on developing WSN for SHM have been conducted [8], [9], [10]. However,
    in the conventional cloud computing, data producers generate raw data and send
    them to the cloud, whereas data consumers send data processing requests and receive
    processed results from the cloud. This unidirectional data flow is unsuitable
    with the increasing data quantity at the edge, considering that tremendous data
    will lead to huge unnecessary bandwidth and computing resource requests [11].
    SHM systems generate huge amounts of data every data. As the measurement data
    may inevitably suffer from noise and sensor faults [12], [13], data cleaning prior
    to data transmission can improve the data quality of SHM systems and offload the
    pressure of data transmission. The advances of IoT and edge devices, along with
    the evolution of artificial intelligence (AI), has boosted the edge computing
    [11], [14]. In contrast to traditional cloud computing, edge computing allows
    data computation on the edge devices, which can be regarded as the downstream
    data on behalf of the cloud and the upstream data on behalf of the deployed edge
    algorithms [11], [15]. Edge devices act as both data consumers and producers.
    Leveraging edge computing, some data processing tasks are directly performed locally
    instead of sending requests and receiving contents from the cloud. As a result,
    the data transmission time and network complexity are significantly reduced. AI
    algorithms can be embedded into the edge devices to help preclean measurement
    data and detect the anomalies. However, the supervised AI methods require sufficient
    labeled data for model training, which is difficult for most engineering problems
    [16], [17]. In application to civil engineering problems with limited and precious
    labeled data, performing cross-domain model transfer in training AI models has
    been a trend [18], [19]. The model learned from the labeled source domain can
    be intelligently transferred to the target domain through feature distribution
    alignment. The traditional domain adaptation methods require the coexistence of
    source and target domain data sets, wherein the source data sets are generally
    in a large volume. Storing large amounts of source data is space-intensive and
    not cost effective for the edge devices [20]. In addition, training AI models
    jointly with source and target data sets consumes large computational resources.
    Recently, source-free domain adaptation algorithms have been developed for computer
    vision tasks (e.g., image classification and segmentation) [21], [22], where the
    source data are inaccessible during adaptation and only the model trained from
    source data is provided. Self-supervision is a main branch in source-free domain
    adaptation. Pseudo labeling [23], mutual information maximization [24], entropy
    minimization [25], and contrastive learning strategies [26] have been introduced
    to aid self-training. For example, Liang et al. [24] proposed to maximize the
    mutual information across domains and applied the nearest prototype classifier
    to improve the accuracy of pseudo-labels for self-supervision. Yang et al. [27]
    developed the neighborhood clustering regularization strategy to classify target
    data by promoting label consistency among data with high local affinity. Xia et
    al. [26] developed an adaptive adversarial network with a contrastive category-wise
    matching module. The main concern of these studies is to obtain robust representatives
    (e.g., pseudo-labels and prototypes) for domain adaptation. The limited computational
    capacity and memory space of low-computational microcontrollers (e.g., smartphones
    and portable and edge devices) also pose challenges in the deployment of deep
    complex AI models. Knowledge distillation [28] is proposed to train a simplified
    small network (i.e., student) to mimic the complex over-parameterized large network
    (i.e., teacher) for knowledge transfer. Existing studies have shown that the performance
    of the student model may even overpass the teacher model [28]. Recently, the knowledge
    distillation technology has been improved to self-knowledge distillation [29],
    where the model is taught to be its own teacher and the past predictions are used
    as soft labels for the next prediction to mitigate the model collapse and improve
    the generalizability. Hence, in addition to compressing complex models, knowledge
    distillation can be utilized in the self-distillation manner to enhance the accuracy
    of pseudo-labels in source-free domain adaptation [30]. In the context of edge
    computing, conducting source-free domain adaptation for the anomaly detection
    of SHM data is of great significance. The knowledge learned from other bridges
    (i.e., source data) can be transferred to the target bridge for decision making.
    Only the model pretrained source data need to be stored in edge devices, without
    accessing the source data. However, the research on this topic is currently lacking.
    This study designs a novel wireless IoT SHM system for the 55-km-long Hong Kong–Zhuhai–Macao
    Bridge (HZMB), which is the longest sea-crossing bridge worldwide. Edge computing
    and 5G technology are integrated to overcome the data transmission, time synchronization,
    and data quality control issues encountered in traditional WSNs. A novel data
    anomaly detection method is developed and embedded into the edge devices for data
    preprocessing prior to transmission. The self-knowledge distillation is integrated
    with the self-supervised learning in the developed method to mitigate overconfident
    pseudo-label predictions and improve model generalizability. The remainder of
    this article is organized as follows. Section II introduces the wireless IoT system
    designed for the HZMB. Section III describes the developed source-free domain
    adaptation algorithm for SHM data anomaly detection embedded in the edge device.
    Section IV applies the proposed method to cross-bridge knowledge transfer and
    presents the data anomaly detection results of the SHM data from the HZMB. Section
    V draws the conclusion. SECTION II. Wireless SHM System and Edge Computing of
    the Qingzhou Bridge The HZMB consists of three cable-stayed bridges, a series
    of continuous viaducts, an undersea tunnel, and four artificial islands. The Qingzhou
    Bridge (Fig. 1), one of the three cable-stayed bridges, has a main span of 458
    m and a total length of 1150 m. The steel box girder of the bridge is supported
    by two H-shaped towers, two auxiliary piers, and two transitional piers. The H-shaped
    tower consists of two concrete legs, one concrete transom in the lower position,
    and one steel transom in the shape of a Chinese knot in the upper position [31],
    [32]. Fig. 1. Configuration of the Qingzhou Bridge. Show All A. Wireless SHM System
    A wired SHM system is designed to monitor the bridge’s structural responses at
    critical locations, environmental factors (e.g., temperature, wind, humidity,
    and corrosion), and external loads (e.g., traffic) since the bridge opened to
    the public in 2018. The 3-D layout of the wired SHM system is shown in Fig. 2,
    which consists of 262 sensors, including hygro-thermometers, thermometers, anemometers,
    GPS rovers, accelerometers, liquid leveling systems, displacement transducers,
    cable tensiometers, strain gauges, corrosion sensors, and reaction dynamometers.
    All sensors are connected to a central station to store data in the centralized
    wired SHM system. Fig. 2. Layout of wired sensors of the Qingzhou Bridge. Show
    All However, the wired SHM system has certain limitations. The primary concern
    is the high data processing load in the central server. Besides, the installation
    and maintenance of wired sensors are challenging in long-span sea-crossing bridges.
    For example, the off-line data processing shows that a small number of sensors
    may be mis-connected to the data acquisition unit, causing difficulties in the
    later analysis results. In addition, the existing SHM system does have pyranometers
    to measure the solar radiation, which are necessary for obtaining the temperature
    distribution of the bridge [31]. Installing and connecting new sensors to the
    current SHM system are costly. With the advances in the wireless communication
    technology, WSNs have been used in some SHM systems with the advantage of being
    easier in manipulation, more economic and efficient than the wired one. Wireless
    SHM systems can be designed in a decentralized manner, which reduces data transmission
    and processing workloads. In this connection, this study develops a novel wireless
    SHM system as a complement to the above-wired system, as displayed in Fig. 3.
    Fig. 3. Layout of wireless sensors of the Qingzhou Bridge. Show All The wireless
    system consists of 19 accelerometers, 2 pyranometers, and 2 thermal imagers. The
    accelerometers are used to measure the vibration of the bridge deck and towers.
    One pyranometer facing up measures the direct solar radiation, whereas the other
    one facing down measures the reflected solar radiation from the sea. The thermal
    images measure the road surface temperature directly. The system incorporates
    5G, edge computing, and efficient DL algorithms to solve data communication, time
    synchronization, decentralized computing, and data quality control issues. The
    designed wireless system can be regarded as a new paradigm for future wireless
    SHM systems. B. IoT Paradigm Demonstration WSNs are prone to hardware or communication
    malfunctions and failures, particularly in harsh environmental conditions. Additionally,
    transmitting and synchronizing vast amounts of data can be arduous in a centralized
    WSN. To address these issues, the wireless SHM system for Qingzhou Bridge incorporates
    emerging technologies, such as 5G and edge computing [33], which help alleviate
    data transmission and network complexity challenges commonly faced by traditional
    wireless SHM systems. The use of accelerometers serves as an example to illustrate
    the full module composition within this advanced system and its workflow is illustrated
    in Fig. 4. To be specific, the components include PCB393B31 accelerometer, HTeC-D3000
    data acquisition unit, Hongdian Z2 5G gateway, NVIDIA Xavier NX edge computing
    board, and Alibaba cloud platform. Particularly, NVIDIA Xavier NX edge computing
    boards are integrated to perform local data processing tasks, promising data security,
    reducing data transmission load, and enabling timely responses in specific situations.
    The 5G timing module is connected to each sensor locally to ensure time synchronization.
    Moreover, the adaptation of 5G communication enhances data transmission speed
    and capacity, due to its low latency and broad bandwidth. All processed data will
    be transmitted automatically to a central cloud platform for evaluation and long-term
    data storage, which is also more easily scalable and maintainable. Authorized
    users can configure the edge device remotely from the cloud. Fig. 4. Smart wireless
    accelerometer module. Show All Edge computing serves as a prominent feature in
    the Qingzhou Bridge’s wireless SHM system, offering numerous advantages by processing
    data at the network’s edge prior to transmission. This approach, in contrast to
    traditional cloud computing, brings several key benefits that enhance overall
    system performance. First and foremost, edge computing enables a faster response
    time, as data processing occurs closer to the source, reducing latency. This real-time
    processing capability is crucial for critical applications in the wireless SHM
    system, where timely decision making and intervention can prevent potential infrastructure
    damage or failure. Second, edge computing contributes to lower battery consumption
    and bandwidth costs. By handling data processing at the network’s edge, the need
    for continuous data transmission to remote cloud servers is significantly reduced.
    This results in decreased power usage and a more efficient utilization of available
    bandwidth, ultimately leading to cost savings. Additionally, edge computing enhances
    data safety and privacy. Localized data processing minimizes the risk of data
    breaches or interception during transmission, ensuring that sensitive information
    remains secure. The successful implementation of intelligent edge computing in
    the Qingzhou Bridge’s wireless SHM system can be attributed to the NVIDIA Xavier
    NX, a widely popular edge computing device for machine learning applications.
    Its powerful processing capabilities, energy efficiency, and compact design make
    it an ideal choice for integrating advanced analytics and real-time monitoring
    into the SHM system, ultimately contributing to the overall safety and reliability
    of the bridge infrastructure. C. Monitoring Data and Structural Analysis Structural
    vibration characteristics, including frequencies and mode shapes are widely used
    in SHM problems [34]. For example, changes in the frequencies or mode shapes of
    a structure before and after the damage occurs can be used to locate and quantity
    the damage [35]. They can also be used to verify the accuracy of the numerical
    finite element (FE) model. If the model is inaccurate, its frequencies and mode
    shapes may differ from the measured counterparts. The model then needs to be updated
    so that its frequencies and mode shapes match the measures ones in an optimal
    manner. This technique is referred to as model updating [36]. The modal analysis
    is conducted using the ARTeMIS Modal Pro software based on the measurement data.
    Fig. 5 shows 1-h time-domain acceleration data from the wireless monitoring system.
    The frequency domain decomposition method is used to calculate the spectral density
    matrix of the measured acceleration data. Afterward, the singular value decomposition
    is performed on the spectral density matrix to approximately decompose the acceleration
    response into a set of independent single-degree-of-freedom systems. The self-spectral
    density of these single-degree-of-freedom systems is calculated in the modal coordinates.
    The peaks in the spectral density correspond to the natural vibration frequencies
    of the structure, and the corresponding eigenvectors represent structural vibration
    mode shapes. The spectral analysis result is shown in Fig. 6. The peaks are distinct
    and significant, which indicates the high quality of measurement data. The first
    five frequencies and mode shapes are then estimated and shown in Table I. TABLE
    I Modal Analysis and Comparison Fig. 5. 1-h acceleration data from Sensors ACC1X
    and ACG1X. Show All Fig. 6. Spectrum analysis based on acceleration data. Show
    All The modal analysis results are further compared with the counterparts calculated
    from the FE model. The 3-D refined FE model of the Qingzhou Bridge is established
    using ANSYS, as shown in Fig. 7. The entire FE model consists of 520 422 elements
    and 493 941 nodes. The solid elements are used for the steel box girder, the asphalt
    concrete layer, tower legs, lower tower transoms, and piers. The shell elements
    are used for diaphragms, U-ribs, and upper tower transoms. In particular, SOLID45,
    SOLID65, and SHELL181 are assigned to the solid and shell elements. Detailed information
    about the FE model is given in [31] and [32]. The first five frequencies and mode
    shapes of the FE model are calculated, and compared with the measured ones in
    Table I. The measured frequencies are very similar to the FE model analysis results,
    verifying the accuracy of the FE model and validating the serviceability and effectiveness
    of the designed wireless SHM system. Fig. 7. Refined FE model of the Qingzhou
    Bridge. Show All SECTION III. Edge Computing for Data Anomaly Detection The measurement
    data may inevitably suffer from noise and anomalies. Based on the designed new
    wireless system, AI algorithms are embedded into the NVIDIA edge computing board
    to detect data anomalies caused by sensor faults. Afterward, only data of good
    quality will be transmitted to the central server. In the context of edge computing,
    this study develops a source-free domain adaptation method for data anomaly detection,
    which transfers the AI model trained from the other bridge with sufficient labeled
    data to the target Qingzhou Bridge without any labeled data. The method mitigates
    the problem of lacking labeled data in the traditional supervised training and
    avoids the requirement of prestoring source data in the edge computing in the
    traditional domain adaptation. The framework of the developed method is illustrated
    in Fig. 8. Specific techniques are outlined in the following sections to aid the
    domain adaptation in the situation that source domain data are unavailable. Fig.
    8. Framework of the developed method. Show All A. Source Domain Training The network,
    composed of a feature extractor f s and a classifier h s , is first trained by
    the labeled source domain data. The commonly used cross-entropy loss function
    is formulated as L src =− E x s ϵ X s ∑ k=1 K y k log σ k ( ϕ s ( x s )) (1) View
    Source where ϕ s ( x s )= h s ( f s ( x s )) is the output of network, σ(⋅) denotes
    the softmax processing, σ k (z)=( e z k / ∑ K i=1 e z i ) denotes the k th element
    in the softmax output of a K -dimensional vector z , and y k is the one-of- K
    encoding vector where the k th element equals 1 and the rest are all 0. However,
    such a training manner makes the network prone to overfitting and shows weak adaptation
    performance in cross-domain tasks. To address this issue, the label-smoothing
    technique [24] is introduced to improve the model generalizability in down-scaling
    tasks and further facilitate the source-free domain adaptation as specified later.
    Specifically, the original cross-entropy function is modified to add a uniform
    distribution 1/ K over the ground-truth one-hot label y k . The cross-entropy
    loss is then calculated between the modified target label y ˆ k and network’s
    softmax output as follows: L ls src =− E x s ϵ X s ∑ k=1 K y ˆ k log σ k ( ϕ s
    ( x s )) (2) View Source where y ˆ k =(1−α) y k +α/K , and α is the smoothing
    factor with the default value of 0.1 [23]. The label-smoothing strategy encourages
    the extracted features to be equidistantly and tightly clustered according to
    categories [24]. B. Information Maximization for Source-Free Domain Adaptation
    The traditional adversarial domain adaptation aims to match the feature distributions
    of source and target domains [i.e., f s ( x s ) and f t ( x t ) ], so the same
    classifier trained from the labeled source domain can be applied to the unlabeled
    target domain. However, this study supposes that the source data are not accessible
    in the domain adaptation and that only the model learned from the source domain
    is available. Hence, the essential challenge is how to adapt the feature distributions
    of source and target domains in the absence of source data. Following the idea
    of adversarial domain adaptation, the source and target domain are designed to
    share the same classifier to help match the feature distributions. That is, the
    classifier for the target domain is directly cloned from the source domain and
    kept fixed ( h t = h s ) . Only the feature extractor is updated ( f t ≠ f s )
    during the adaptation. As the classifier has been trained to classify the source
    domain whose labels are represented by the one-hot encoding, if the feature distributions
    of the source and target domains have been matched, the predicted labels of the
    target domain should be close to one-hot encoding and discriminative across different
    classes. In corresponding, the loss function L im consists of two items, L ent
    and L div . Minimizing L ent impels the softmax output to approach the one-hot
    encoding label, and minimizing L div promotes the diversity of the predicted labels.
    The loss functions are formulated as follows: L ent = L div = L im = − E x t ϵ
    X t ∑ k=1 K σ k ( ϕ t ( x t ))log σ k ( ϕ t ( x t )) ∑ k=1 K p ¯ ¯ ¯ k log p ¯
    ¯ ¯ k L ent + L div (3) (4) (5) View Source where p ¯ ¯ ¯ k = E x t ϵ X t [ σ
    k ( ϕ t ( x t ))] is the mean output of the entire target domain. C. Self-Supervised
    Prototype Learning The information maximization technique helps classify the target
    domain through feature distribution alignment. However, a few target data may
    be forcibly driven to the false category and then significantly deviate from the
    true label [23]. For example, the target data from the first category may have
    the network output [0.3, 0.4, 0.2, 0.1] during training, but will be forced to
    approach [0, 1, 0, 0] with the information maximization strategy. To alleviate
    this problem, self-supervised prototype learning is additionally introduced to
    further improve the accuracy of pseudo-labels and supervise the target data encoding
    training. The prototype (i.e., centroid) is less sensitive to outliers and can
    better characterize the distribution of different categories, thus improving the
    robustness of the classifier. The class-wise prototype in the target domain is
    first calculated following the idea of weighted k -means clustering μ (0) k =
    ∑ x t ϵ X t σ k ( ϕ ˜ t ( x t )) f ˜ t ( x t ) ∑ x t ϵ X t σ k ( ϕ t ( x t ))
    (6) View Source where ϕ ˜ t = f ˜ t ∘ h t denotes the model learned in the previous
    iteration with h t unchanged. In the beginning, ϕ ˜ t is the model trained from
    the source data. The pseudo-label is then estimated via the classifier with the
    nearest distance y ˜ (0) t =arg min k  =arg min k (δ( f ˜ t ( x t ),  μ (0) k
    )) ⎛ ⎝ ⎜ 1− f ˜ t ( x t )⋅ μ (0) k ∥ ∥ f ˜ t ( x t ) ∥ ∥ 2 ∥ ∥ μ (0) k ∥ ∥ 2 ⎞
    ⎠ ⎟ (7) View Source where δ(⋅, ⋅) measures the cosine distance between two items,
    ∥⋅ ∥ 2 denotes the L2-norm. In the i th iteration, the class centroids and pseudo-labels
    are then updated as follows: μ (i) k = y ˜ (i) t = ∑ x t ϵ X t ξ( y ˜ (i−1) t
    =k) f ˜ t ( x t ) ∑ x t ϵ X t ξ( y ˜ (i−1) t =k) arg min k ⎛ ⎝ ⎜ 1− f ˜ t ( x
    t )⋅ μ (i) k ∥ ∥ f ˜ t ( x t ) ∥ ∥ 2 ∥ ∥ μ (i) k ∥ ∥ 2 ⎞ ⎠ ⎟ (8) (9) View Source
    where ξ(⋅) is an indicator that equals 1 when the argument is true. Equations
    (3) and (4) are iterated for multiple rounds. With the self-estimated pseudo-labels,
    the loss function is given by L sl =− E x t ϵ X t ∑ k=1 K y ˜ t log σ k ( ϕ t
    ( x t )). (10) View Source D. Self-Knowledge Distillation Self-knowledge distillation
    is further utilized to mitigate overfitting or overconfident predictions and improve
    the model generalizability. The teacher and student models are designed with the
    same architecture, which are initialized based on the parameters learned from
    the source domain. In the training stage, the teacher model is used to estimate
    the pseudo-labels of the unlabeled target domain. The pseudo-labels are then used
    to calculate the training loss, based on which the student model is optimized.
    Afterward, the parameters of the teacher model are updated by referring to the
    student model using an exponential moving average (EMA) strategy [37]. Hence,
    the pseudo-labels are estimated as follows: y ¯ ¯ ¯ t = exp[δ( f Tea t ( x t ),  μ
    (i) k )/T] ∑ K k=1 exp[δ( f Tea t ( x t ),  μ (i) k )/T] (11) View Source where
    T denotes the temperature scaling parameter, and the overconfident estimations
    are mitigated when T>1 . The loss function is then estimated as follows: L kd
    =− E x t ϵ X t ∑ k=1 K y ¯ ¯ ¯ t log σ k ( ϕ t ( x t )). (12) View Source E. Summary
    The overall model, consisting of a feature extractor f s and a classifier h s
    , is first trained using the source domain data to minimize (2). Following, the
    classifier is fixed ( h t = h s ) , and the feature extractor f s is used to initialize
    the feature extractors f Stu t and f Tea t in the student and teacher models for
    the target domain. The teacher model is used to estimate the pseudo-labels. Afterward,
    f Stu t is updated to minimize the loss function L , which is formulated as follows:
    L= L im +λ L sl +γ L kd (13) View Source where λ and γ are the tradeoff parameters
    of self-supervised pseudo-labels and self-knowledge distillation pseudo-labels,
    respectively. f Tea t is then updated with an EMA of parameters in f Stu t . The
    teacher model has no gradient backpropagation, as indicated in Fig. 8. The classifier
    remains fixed during the training stage. SECTION IV. Cross-Bridge Model Transfer
    A. Source Bridge A long-span cable-stayed bridge in China provided by The First
    International Project Competition is used as the source bridge [38], as shown
    in Fig. 9. The SHM system of the bridge consists of multiple sensors. Only acceleration
    data are used in this study for anomaly detection. The data from a total of 38
    channels are used with a sampling frequency of 20 Hz. The measurement data are
    divided into hourly segments without overlapping. One-month data (1 January 2021–31
    January 2021) are used as the source training data set. The data in February 2012
    (1 February 2021–29 February 2021) are used as a blind data set to test the model
    performance. Fig. 9. Sensor layout of the source bridge. Show All The entire source
    training and test data set contains seven data patterns, including the normal
    pattern and six abnormal patterns (i.e., missing, minor, outlier, square, trend,
    and drift) caused by sensor fault, as shown in Fig. 10. The descriptions of all
    patterns are specified in Table II. The quantities of each pattern in the source
    training and test data are given in Table III. The training data set has a total
    of 28 272 samples and the test data set has 26448 samples in total. TABLE II Description
    of the Anomaly Patterns TABLE III Source Training and Test Data Set Fig. 10. Data
    patterns in the source bridge. Show All The data samples are converted into grayscale
    figures and used as the input to the feature extractor f s , which adopts the
    pretrained ResNet18 model. As the gray image has only one channel, rather than
    three channels of RGB images, an additional convolutional layer with one input
    channel is added before the ResNet18 module. Besides, an additional batch normalization
    layer and a fully connected layer are placed after the ResNet18 module, as illustrated
    in Fig. 8. The classifier h s is composed of a fully connected layer and a weight
    normalization layer, with the final output size of seven. As introduced previously,
    the label-smoothing technique is employed to train the model through back-propagation.
    The network is updated by using the stochastic gradient descent with a momentum
    of 0.9 and weight decay of 10 −−3 . The learning rate of the feature extractor
    and the classifier is set to 10−3 and 10 −−2 , respectively. The batch size is
    76 in the source data set and the epoch is set to 50. Several indices are used
    to evaluate the model performance, which are defined as follows: Accuracy= Precision=
    Recall= TP+TN TP+TN+FP+FN TP TP+FP TP TP+FN (14) (15) (16) View Source where TP,
    FP, FN, and TN denote the true positive, false positive, false negative, and true
    negative, respectively. Accuracy represents the correct prediction of the entire
    data sets. For each category, precision and recall are defined, where the former
    equals the ratio of correctly predicted positives overall positives, and the latter
    equals the ratio of correctly predicted positives overall samples of this category.
    The training accuracy over epochs is plotted in Fig. 11(a). The well-trained model
    is applied to the blind test data set for performance validation. The confusion
    matrix of the test data set is plotted in Fig. 11(b). The precision and recall
    of each category are calculated in Table IV. The precision and recall of the outlier
    and drift patterns are relatively low compared with the rest patterns, which is
    probably due to the insufficient training samples and muti-pattern anomalies [39].
    Nevertheless, the model achieves an acceptable overall accuracy in classifying
    data anomalies. The well-trained model is used in the following section for domain
    adaptation. TABLE IV Classification Results Fig. 11. Source model training. (a)
    Accuracy over epochs. (b) Confusion matrix of source test data set. Show All B.
    Target Bridge The target bridge is the Qingzhou Bridge of the HZMB. Similar to
    the source bridge, only the acceleration data are studied. There are five uniaxial
    accelerometers and seven biaxial accelerometers in total, as shown in Fig. 3,
    resulting in a total of (5 +7×2 ) = 19 channels. The sampling frequency is 50
    Hz. One-month data collected from 1 February 2023–28 February 2023 are studied.
    Similarly, data samples are generated based on the nonoverlapped hourly responses.
    Data anomalies occur in four channels only in February, that is, two biaxial accelerometers
    installed on the tower. Hence, only acceleration data from these four channels
    form the target data set for data anomaly detection. Five patterns are plotted
    in Fig. 12. The specific data set information is given in Table V. TABLE V Target
    Data Set Fig. 12. Data patterns in the HZMB. Show All The model trained from the
    source data set is first directly applied to the target data set. The confusion
    matrix is shown in Fig. 13(a). The accuracy of the entire target data set is 58%,
    with the precision and recall of each category specified in Table VI. The missing
    pattern has both high precision and recall, and the remaining patterns either
    have low precision or recall. The performance of the trained anomaly detection
    model degrades when applied to the new bridge. TABLE VI Classification Results
    Before and After Domain Adaptation Fig. 13. Confusion matrix of the target data
    set. (a) Without adaptation. (b) After adaptation. Show All The proposed source-free
    domain adaptation method is then applied. The hyper-parameters in (13) are set
    to λ=0.3 and γ=0.1 . The temperature scaling parameter in (11) is set to T=2 .
    The epoch is set to 30. The accuracy over epochs is plotted in Fig. 14. The blue
    marker denotes the model accuracy in each epoch. The red marker denotes the accuracy
    by further updating the predicted labels using the self-supervised prototype learning
    and self-knowledge distillation. In each epoch, the model parameters remain unchanged
    in the step of estimating red marker, but the obtained pseudo-labels will participate
    in the model training in the next epoch, as defined in (10) and (12). In general,
    self-supervised learning and self-knowledge distillation improve the accuracy
    of the obtained labels. After training, the obtained model is applied to the target
    data sets, and the results are shown in Fig. 13(b) and Table VI. Compared with
    using the model trained from source bridge directly, the cross-domain adapted
    model achieves more accurate classification results, validating the effectiveness
    of the developed method. Fig. 14. Variation of accuracies over epochs. Show All
    C. Comparative Study Several state-of-the-art methods are selected for the comparative
    study. The algorithms are described as follows. Source Only: The ResNet18 model
    [40] without any knowledge transfer is selected as the baseline model, that is,
    the model trained from the source bridge is directly applied to the target bridge
    for anomaly detection. The method is referred to as source only. Collaborative
    Class Conditional Generative Adversarial Network (3C-GAN): Li et al. [41] developed
    a 3C-GAN to match the target distribution with the unseen source data through
    adversarial learning. The objective function incorporates the clustering-based
    regularization item to generate the decision boundary in the low-density region.
    Source Hypothesis Transfer (SHOT): SHOT is proposed by Liang et al. [23] for unsupervised
    domain adaptation in the absence of source data and only a trained source model
    is available. SHOT has been widely used in many cases, including closed-set, partial-set,
    and open-set adaptation. w/o IM: The ablation study is conducted to investigate
    the contribution of information maximization mechanism by removing L im from the
    right-hand side of (13). The method is referred to as without Information Maximization
    (w/o IM). w/o PL: The model performance without self-supervised Prototype Learning
    (w/o PL) is also investigated by setting λ=0 in (13). Fig. 15 compares the accuracies
    of six different methods. The accuracy is the averaged results of five implementations.
    For the easy comparison, the ResNet18 pretrained on ImageNet [42] is employed
    as the backbone module of the feature extractor in all algorithms. Fig. 15 demonstrates
    that the proposed method has the highest accuracy. Besides, the accuracy decreases
    significantly after removing information maximization (from 88% to 64%) or prototype
    learning (from 88% to 79%), validating the effectiveness of these two mechanisms.
    In summary, our method is superior to the other five methods in the data anomaly
    detection task. Fig. 15. Accuracy comparison of different methods in anomaly detection
    task. Show All D. Further Analysis Despite the improvement in anomaly detection
    accuracy, some samples are misclassified. For example, 165 samples with the normal
    pattern are misclassified as outlier and 85 samples are misclassified as trend.
    The misclassified samples are compared with the ground-truth samples. Fig. 16
    gives the misclassified examples that are supposed to be normal data. The peak
    in the measurement data caused by passing vehicles is misclassified as outlier.
    The reason may be that the traffic volume on the source bridge within an hour
    was high, but the traffic volume on the HZMB was relatively low in February 2023.
    Besides, when manually labeling the target data sets, only samples with a severe
    trend are labeled as trend, and the rest with a slight trend are labeled as normal.
    As a result, such target patterns are not seen (in other words, such knowledge
    is not learned) in the source bridge and are misclassified as abnormal patterns
    that are more similar in appearance. Fig. 16. Misclassified patterns and the ground-truth
    samples. (a) Outlier. (b) Normal data misclassified as outlier. (c) Trend. (d)
    Normal data misclassified as Trend. Show All Considering that a limited number
    of anomalies may be collected and labeled in the bridge’s initial operation stage,
    the source data set is updated by adding a small number of manually labeled target
    data samples (i.e., few-shot samples). Specifically, 50 normal samples misclassified
    as outlier, 50 normal samples misclassified as trend, 20 trend samples misclassified
    as normal, and 10 drift samples misclassified as minor are labeled with their
    ground-truth labels and then added into the source data set. The developed source-free
    domain adaptation method is similarly applied. The classification result and the
    t -distributed stochastic neighbor embedding ( t -SNE) [43] feature visualization
    result are shown in Fig. 17(a) and (b). The precision, recall, and accuracy indices
    are listed in Table VII. As the model is pretaught to learn the related knowledge
    from the few-shot samples, the performance is further improved compared with the
    results in Table VI. Hence, with more labeled data samples collected in the future,
    the model will be continuously improved. TABLE VII Classification Results After
    Improvement Fig. 17. Model performance improved by adding labeled target samples.
    (a) Updated results. (b) t -SNE visualization. Show All SECTION V. Conclusion
    This study develops a novel wireless IoT SHM system for the HZMB. A 5G timing
    module is connected to each sensor locally to ensure time synchronization, and
    the NVIDIA Xavier NX edge computing board is integrated to detect abnormal data
    at the edge of network to reduce the data transmission pressure. A novel data
    anomaly detection method is developed through cross-bridge knowledge transfer.
    Different from the traditional methods that require the coexistence of source
    and target data sets for model adaptation, the source-free domain adaptation is
    deployed to reduce data storage consumption of edge devices. The developed method
    is tested on the one-month monitoring data from the HZMB. The results show that
    the deployed self-learning and self-knowledge distillation strategies improve
    the accuracy of estimated pseudo-labels. Compared with directly using the model
    trained from other bridges, the cross-bridge adaptation obviously improves the
    model performance. Overall, data anomaly detection integrated with edge computing
    offers an excellent wireless IoT system for health monitoring of long-span bridges.
    Authors Figures References Citations Keywords Metrics More Like This Towards Integration
    of Wireless Sensor Networks and Cloud Computing 2015 IEEE 7th International Conference
    on Cloud Computing Technology and Science (CloudCom) Published: 2015 A partial
    TPSN time offset synchronization scheme in Wireless Sensor Network applied for
    Bridge Health Diagnosis System Proceedings of 2011 International Conference on
    Computer Science and Network Technology Published: 2011 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Wireless IoT Monitoring System in Hong Kong-Zhuhai-Macao Bridge and Edge
    Computing for Anomaly Detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kang Y.
  - Liu A.
  - Xiong N.N.
  - Zhang S.
  - Wang T.
  - Dong M.
  citation_count: '2'
  description: Mobile crowdsensing (MCS) is a crucial component in the Industrial
    Internet of Things (IIoT), mainly due to its role in collecting data and enhancing
    applications. Nonetheless, it faces challenges in maintaining data quality and
    cost efficiency. Low-quality workers and their deceptive data bids undermine the
    trustworthiness of MCS data collection. Despite this, prior studies have not sufficiently
    scrutinized the validity of data and bids. These issues could render MCS services
    ineffective and unaddressed, hindering IIoT development. In response, we propose
    an Intelligent Data and Bid dual truth discovery (DTD) scheme. Initially, the
    scheme applies a detection algorithm to identify the features of ground truth
    data sensed by unmanned aerial vehicles. The approach uses features to evaluate
    the data trust from unknown workers and filter out low-quality workers. Subsequently,
    the scheme evaluates the bid trust from reliable workers by calculating their
    bid confidence intervals. Upon completing this assessment, the scheme identifies
    high-quality workers. This process hinges on a contribution value incorporating
    both data and bid trust. Ultimately, the scheme assigns these high-quality workers
    to sense the subsequent tasks. This approach significantly improves the data quality
    and reduces costs for MCS in IIoT. The experimental results demonstrated that
    the DTD scheme outperforms the existing main schemes in terms of sensitivity (improvement
    44%), specificity (improvement 5%), accuracy (improvement 18%), and F1-score (improvement
    47%). It also reduced data bias by 24 percentage points and reduced costs by 38
    percentage points.
  doi: 10.1109/JIOT.2023.3292920
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Volume: 11 Issue: 2 DTD: An Intelligent Data and Bid Dual Truth Discovery Scheme
    for MCS in IIoT Publisher: IEEE Cite This PDF Yunchuan Kang; Anfeng Liu; Neal
    N. Xiong; Shaobo Zhang; Tian Wang; Mianxiong Dong All Authors 2 Cites in Papers
    241 Full Text Views Abstract Document Sections I. Introduction II. Related Work
    III. System Model and Problem Statement IV. Our Proposed DTD Scheme V. Performance
    Analysis Show Full Outline Authors Figures References Citations Keywords Metrics
    Abstract: Mobile crowdsensing (MCS) is a crucial component in the Industrial Internet
    of Things (IIoT), mainly due to its role in collecting data and enhancing applications.
    Nonetheless, it faces challenges in maintaining data quality and cost efficiency.
    Low-quality workers and their deceptive data bids undermine the trustworthiness
    of MCS data collection. Despite this, prior studies have not sufficiently scrutinized
    the validity of data and bids. These issues could render MCS services ineffective
    and unaddressed, hindering IIoT development. In response, we propose an Intelligent
    Data and Bid dual truth discovery (DTD) scheme. Initially, the scheme applies
    a detection algorithm to identify the features of ground truth data sensed by
    unmanned aerial vehicles. The approach uses features to evaluate the data trust
    from unknown workers and filter out low-quality workers. Subsequently, the scheme
    evaluates the bid trust from reliable workers by calculating their bid confidence
    intervals. Upon completing this assessment, the scheme identifies high-quality
    workers. This process hinges on a contribution value incorporating both data and
    bid trust. Ultimately, the scheme assigns these high-quality workers to sense
    the subsequent tasks. This approach significantly improves the data quality and
    reduces costs for MCS in IIoT. The experimental results demonstrated that the
    DTD scheme outperforms the existing main schemes in terms of sensitivity (improvement
    44%), specificity (improvement 5%), accuracy (improvement 18%), and F1-score (improvement
    47%). It also reduced data bias by 24 percentage points and reduced costs by 38
    percentage points. Published in: IEEE Internet of Things Journal ( Volume: 11,
    Issue: 2, 15 January 2024) Page(s): 2507 - 2519 Date of Publication: 06 July 2023
    ISSN Information: DOI: 10.1109/JIOT.2023.3292920 Publisher: IEEE Funding Agency:
    SECTION I. Introduction The widespread adoption of intelligent devices, such as
    smartphones, smartwatches, tablets, and intelligent sensors, is largely attributed
    to the rapid advancements in microprocessor and Industrial Internet of Things
    (IIoT) technology [1], [2], [3]. These devices provide an efficient and pervasive
    method for collecting vast data, serving as tools for communication, social interaction,
    agriculture, industry, commerce, etc. Most mobile intelligent devices are programmable
    and have a variety of sensors, such as microphones, cameras, GPS, and gyroscopes
    [4]. These devices can monitor users’ environment and infer their activities by
    creating specialized programs. The proliferation of these devices, especially
    in IIoT, has resulted in an enormous surge in data generation, introducing a formidable
    challenge: efficiently and effectively managing this overwhelming influx of data
    in real time. This data management issue is particularly relevant to mobile crowdsensing
    (MCS), an innovative solution involving data collection from these devices, which
    has shown promise in tackling this challenge. An integral part of IIoT applications,
    MCS relies on the collective data provided by numerous devices. However, a primary
    hurdle for MCS within IIoT applications lies in ensuring the quality of the data
    collected, which directly influences the system’s overall performance. In IIoT,
    MCS provides real-time data collection and analysis capabilities that enable us
    to understand and respond to environmental changes faster and better, ultimately
    increasing efficiency and reducing costs. However, such efficiency gains depend
    heavily on the data quality collected. With the development of IIoT in recent
    years, more and more devices have been connected to the network. These devices
    generate large amounts of local data, such as noise levels, pollution levels,
    traffic conditions, etc., which, if handled properly, can greatly impact our decisions.
    For this reason, MCS plays a crucial role in IIoT by collecting and evaluating
    the local data from these devices, thus ensuring the quality and reliability of
    the data and further promoting the efficient operation and development of IIoT
    [5], [6], [7]. Integrating MCS with IIoT also helps businesses optimize production
    processes, reduce resource consumption, and achieve higher energy efficiency and
    operational benefits. MCS has been implemented in various IIoT applications, including
    localization, indoor mapping, environmental monitoring, transportation, navigation,
    etc. [8], [9], [10], [11], [12], [13]. It allows individuals to collect and transfer
    data to MCS centers in the industrial sector using mobile intelligent devices
    [14], [15], [16]. A wide variety of MCS platforms constructed data-based IIoT
    applications has emerged in recent years [17], [18], [19], including NoiseTube
    [20], Earphone [21], and SignalGuru [22]. A typical MCS system in IIoT has three
    essential components: 1) a cloud-based platform; 2) mobile device users; and 3)
    service requesters. After a sensing campaign has begun, the platform requests
    selected mobile device users, referred to as “workers” to perform sensing activities
    [23], [24], [25]. These tasks require workers to use their mobile devices to collect
    specified local data and transmit the information to the platform. The platform
    can then understand the workers’ local expertise and share it with others requesting
    services [26], [27], [28]. Despite the potential of MCS, the quality of the data
    they collect is vital to their performance [29], [30]. Data quality can be affected
    by the type of equipment used and the behavior of the workers collecting the information
    [31], [32], [33]. Many mobile devices have varying sensing capabilities, which
    might result in different data quality levels [34], [35], [36]. Human behavior
    can also affect data quality, which is more complicated and difficult to anticipate.
    Some workers may meticulously follow instructions, while others may purposefully
    fabricate fake data for a fraudulent reward [37]. Additionally, careless workers
    may inadvertently submit inaccurate data [38], [39]. Without sufficient quality
    control, the acquired data may not be trustworthy, reducing the platform’s performance
    [40], [41], [42]. In a realistic environment, data quality can be influenced by
    workers’ bids, resulting in a complex relationship between data quality and bids
    [43], [44]. Various scenarios may arise when workers submit data along with a
    bid. High-quality workers provide high-quality data and request a reasonable bid,
    making them the ideal choice for the platform. Average-quality workers submit
    high-quality data but request a high bid to receive a greater reward, which might
    increase the cost of acquiring accurate data. Low-quality workers provide low-quality
    data with low bids, potentially compromising data quality if the platform prioritizes
    low cost. The platform prefers to select high-quality workers (scenario 1) to
    ensure the best balance between data quality and cost. However, in cases where
    high-quality workers are unavailable, the platform may consider average-quality
    workers (scenario 2) if the need for accurate data outweighs the increased cost.
    The platform aims to avoid recruiting low-quality workers (scenario 3), as their
    data might compromise the system’s performance. In MCS, data truth discovery methods
    address these data quality issues [43], [44]. However, traditional truth discovery
    approaches, largely dependent on statistical methods, overlook the diversity of
    workers. These approaches often fail to ensure data reliability and cost-efficiency,
    underscoring the necessity for improved techniques. We propose a proactive strategy
    to address this limitation to enhance data quality and worker reliability. This
    novel technique utilizes unsupervised learning to train an anomaly detection model,
    which assesses the quality of data provided by workers and estimates their reliability
    in real time. We also introduce two trust computation schemes to calculate each
    worker’s contribution value accurately. This approach enables the platform to
    proactively filter out low-quality workers during recruitment, optimizing its
    performance and ensuring high-quality data collection at a reasonable cost. This
    technique addresses the shortcomings of existing approaches, delivering enhanced
    data reliability and system efficiency. Our major contributions can be summarized
    as follows. We propose a truth data discovery approach for MCS. Our approach utilizes
    a small number of reliable workers as a starting point. It employs an unsupervised
    machine learning algorithm, specifically a data detection algorithm, to learn
    the features of ground truth data. These features are then used to detect and
    filter data submitted by unknown workers, identifying truthful data and preventing
    malicious attacks on the IIoT. We also propose two trust computation schemes to
    calculate the workers’ contribution value based on an entropy-weighted multiobjective
    optimization approach. The MCS platforms can filter out low-quality workers from
    the pool of unidentified workers based on their contribution value. This filtration
    process effectively optimizes the platform’s performance in recruiting workers,
    ensuring the receipt of high-quality data at a low cost. Our experimental results
    show that our strategy improved the sensitivity by 44%, the specificity by 5%,
    the F1-score by 47%, the accuracy by 18%, reduced data bias by 24 percentage points,
    and reduced recruiting cost by 38 percentage points, in contrast to traditional
    approaches. Our method incorporates a false data filtering mechanism to identify
    reasonable bids, resulting in significantly higher data accuracy and lower platform
    costs. The remainder of this article is organized as follows. In Section II, the
    related works are reviewed. The system model and problem statement are presented
    in Section III. Section IV establishes the dual truth discovery (DTD). Then, Section
    V provides the experimental results. The conclusion is given in Section VI. SECTION
    II. Related Work Due to the rapid development of IIoT, there are many essential
    topics in MCS research, such as security [45], [46], [47], privacy, etc. This
    section mainly focuses on truth data discovery and data anomaly detection. A.
    Truth Data Discovery Truth data discovery is used to identify the accuracy of
    data collected and has been studied by some researchers [8], [9], [10], [13],
    [23], [24], [32], [40], [41], [42], [44]. The main methods in these studies are
    based on a multisampling strategy, such as noise intensity and temperature, so
    the platforms receive multiple observations from different workers [43], [44].
    In the following, we mainly discuss three classical methods in previous studies.
    The majority voting inference (MVI) method involves platforms gathering workers’
    votes or data observations and using the most common observation as the true data
    [43]. While simple and effective, this method consumes resources and is susceptible
    to malicious nodes. Attackers can deceive platforms with false data if malicious
    voters outnumber normal ones. However, it is applicable when the nature of data
    is binary or categorical and where there is a large crowd of workers, which increases
    the chances of the majority being honest and correct. The mean truth inference
    (MEI) method is based on two assumptions [8]: a) every group or set member is
    authentic and b) authenticity distribution among members is similar, meaning most
    members have comparable authenticity. In this method, the evaluated truth data
    is the mean of the collected data samples. It might not be robust against malicious
    data injections and outliers, yet, it is best used when data variability is low
    and the sample size is large enough to level the effects of any deviations. The
    weights truth inference (WEI) method assumes that most workers are honest and
    their sensing data follows a normal distribution [8], [44]. This method assigns
    higher weights to data samples near the distribution center, while samples at
    the edges receive lower weights. The weighted average of all samples represents
    the truth data. This method best suits scenarios where data follows a specific
    statistical distribution and data outliers are expected. However, a significant
    research gap lies in developing strategies for weight assignments more resistant
    to noise and malicious data. The shared characteristic of these methods is recruiting
    multiple workers to collect data on the same physical phenomenon. Statistically,
    more reliable workers in sensing tasks yield more accurate data. However, identifying
    reliable workers is challenging, and selecting them directly for tasks is not
    feasible. Additionally, obtaining multiple data points for a single location is
    costly and unnecessary, as only one accurate data point is needed. Most importantly,
    these truth discovery methods do not evaluate the relationship between data quality
    and workers’ bids, which can impact platform costs, data quality, and market fairness.
    B. Data Anomaly Detection Data anomaly detection, applicable in various domains
    like cybersecurity, fraud detection, healthcare, and industrial monitoring, can
    be categorized as supervised, unsupervised, or semi-supervised methods [48]. The
    supervised method uses labeled data sets for training, while the unsupervised
    method does not need labels and relies on local density. Semi-supervised methods
    train using both labeled and unlabeled data. Supervised method uses labeled data
    sets for model training. These methods often yield high accuracy but require substantial
    labeled data, which can be costly and time-consuming to collect and label. They
    are best suited for situations where ample labeled training data are available.
    Unsupervised method does not need labels and relies on local density or other
    forms of data distribution. These methods are beneficial when labels are hard
    to come by or when the number of normal data instances far outweighs the anomalous
    ones. However, they may struggle to identify subtle anomalies that do not deviate
    significantly from most data points. Semi-supervised method trains on labeled
    and unlabeled data, utilizing the vast availability of the latter to enhance model
    performance. This approach can effectively detect anomalies by identifying deviations
    from the “normal” baseline. However, the limitation lies in the accurate definition
    of “normal” which can be challenging in dynamic environments where the definition
    of “normal” may evolve. In the field of truth discovery, particularly in MCS,
    traditional methods, such as the MVI, MEI, etc., encounter significant challenges.
    They grapple with the uncertainty inherent in data provided by unverified workers,
    often falling prey to malicious nodes or being overly reliant on the authenticity
    distribution amongst data points. These shortcomings become even more pronounced
    in MCS environments where malicious or noisy data points could abound. In response
    to these shortcomings, we propose a novel method named the detection method of
    ground data feature (DM-GDF). This approach leverages the sensed data from unmanned
    aerial vehicles (UAVs) [49], [50] as a ground truth for both truth discovery and
    anomaly detection. The UAV-sensed data serves as a reliable baseline for analysis,
    providing a benchmark for what constitutes “normal” data features in MCS. The
    DM-GDF applies an unsupervised learning strategy, which effectively manages unlabeled
    data. By incorporating UAV-sensed data as a training baseline, the DM-GDF establishes
    a standard for identifying normal data features, thereby enhancing its ability
    to isolate and detect anomalous data points. By proposing this innovative method,
    we offer a fresh perspective on truth data finding and anomaly detection within
    MCS. Our approach addresses the particular challenge of dealing with data from
    unverified sources, contributing a significant improvement over traditional methods
    and advancing the field of MCS truth discovery. SECTION III. System Model and
    Problem Statement A. Data Sensing Model The data sensing model we use, similar
    to most studies in [6], [49], and [50], consists of three main components. 1)
    Workers: In this model, people, vehicles, and all kinds of devices that have sensing
    devices can be considered workers. These workers’ identities are unknown; some
    are normal and some are abnormal. These workers are denoted as aggregated W .
    For example, users with more than 5 billion mobile devices are important workers
    whose devices have rich sensing devices that can sense and collect large amounts
    of data, such as sound, images, videos, etc. [17], [18], [19]. These workers are
    numerous and mobile, and they can move to the designated locations for data sensing
    after receiving the task very economically and submitting the collected data to
    the platform. In addition, we employ UAVs as special workers. These UAVs are able
    to solve the collection of partially physically measurable data, and their measurement
    data can be used as a baseline truth for the task, solving the problem of the
    credibility of the data collected by unidentified workers. 2) Tasks: The goal
    of the sensing tasks is to recruit workers to move and acquire data, and these
    tasks are denoted as R . Each task r i can be perceived and reported by a subset
    of the workers set W . The variety of tasks can be diverse and may include environmental
    monitoring, traffic condition analysis, and other tasks that require collecting
    and analyzing physical world data. Each task has its own designated location L
    to which the worker needs to move to complete the task. Swarms of drones can provide
    baseline data for specific tasks, thereby improving the accuracy and reliability
    of the data. 3) Platform: Platform is the hub of our data sensing model. It issues
    data collection tasks to workers, evaluate the authenticity of data from workers,
    selects workers to collect data, and pays them rewards. After acquiring the data,
    the platforms use the data to build cloud applications and receive subscriptions
    from users. Fig. 1 shows the visualization of our data sensing model. In this
    model, the platform publishes five tasks R and recruits some workers W to perform
    data sensing at five specific locations through the wireless sensing network.
    The workers make bids for the tasks R . Once the platform accepts these bids,
    they can start performing the tasks. While workers sense data, we also send UAVs
    to appropriate locations to collect the same data as the tasks for which the workers
    are responsible. Workers are paid for completing the tasks and submitting the
    data. The data set collected by tasks R is defined as a set V with size S . This
    means that the platform recruits S workers to sense tasks R , collect data, and
    receive S data samples. Fig. 1. Data sensing model in MCS. Show All For example,
    we use the set v i to denote the data values obtained by workers w i for task
    r i , i.e., the set V denotes the workers who collected data for tasks R , including
    w 1 , w 2 , w 3 , and w 4 . B denotes the set of bids submitted by workers for
    tasks R . Workers and UAVs upload data to the platform to complete the data collection
    process. The key symbols of the model are shown in Table I. The potential drawbacks
    and scalability practicality of our adopted model are as follows. Potential Drawbacks:
    The data sensing model, while robust, has potential drawbacks explicitly associated
    with the use of UAVs. While a great asset for accurate data collection, UAVs are
    vulnerable to various security threats. These include interception, jamming, and
    unauthorized access, which could compromise the integrity and reliability of the
    data they gather. Any interference or unauthorized alteration of data gathered
    by the UAVs could significantly increase data bias. Scalability and Practicality:
    While the vulnerabilities associated with our model present risks, it is essential
    to recognize the numerous advantages it offers, including reduced data bias and
    lower recruitment costs. These benefits continue to underscore the model’s overall
    value. In light of these challenges, investing in cutting-edge security measures
    and protocols is of utmost importance, which will effectively protect UAVs from
    potential threats. By guaranteeing the safety of our UAVs, we can enhance the
    practicality and scalability of our model in real-world applications. This development
    paves the way for additional research in secure UAV-based data sensing. TABLE
    I Main Parameters B. Problem Statement This study aims to solve the issues of
    truth data discovery and recruitment cost in the data sensing model, i.e., how
    to ensure that the platform obtains high-quality data with minimal cost to build
    high-quality applications and promote the development of MCS. The specific problem
    to be solved can be expressed in the following aspects. Workers’ Trustworthiness
    Discrimination: Typically, trusted workers submit data with higher trustworthiness,
    while workers with lower trustworthiness are more likely to submit malicious data.
    We aim to prioritize assignments to workers with high trustworthiness over those
    with low trustworthiness to improve the accuracy of the truth we ultimately infer.
    Platform’s Ability to Identify Truth Data: The platform needs to identify the
    trustworthiness of the workers who submit and process the collected data. When
    identifying workers, we want to correctly identify as many honest workers as possible
    and incorrectly identify as few malicious workers as possible. Increasing the
    rate of correct workers identification helps improve the security and stability
    of the system. UAVs can sense reliable and accurate data, yet their relatively
    high-hardware and operating costs do not meet the need for large-scale and continuous
    data collection. Therefore, we should send as few UAVs as possible to reduce costs
    while collecting enough of the required data. Therefore, we position the UAVs
    as quality inspectors. For the traditional truth discovery methods mentioned before,
    the bias of the inferred results from the actual situation may be significant
    when the workers’ quality is very low because no workers are identified and screened
    using trustworthiness criteria. Moreover, these methods ignore the impact of workers’
    bids, i.e., recruitment cost, on data quality. To address the above issues, we
    use the following definitions to express the goals of the strategy in this article.
    Definition 1 (Data Bias):In truth discovery, the data bias directly reflects the
    data quality and the workers’ quality and also reflects the performance of the
    truth discovery methods. We use the data sensed by UAV as the ground truth data
    v G i of task r i . By different methods, the estimated value of truth data for
    S data points is different. We use E i = H k ( v ul i ) to denote the estimated
    value of the collected data by using a strategy H k to process the data. For example,
    H avg ( v ul i )=([ ∑ i∈{1…m} v ul i ]/S) in the MEI method, i.e., the mean value
    of the data submitted by S workers. The bias between the estimated value and the
    true value is (| v G i − E i |/ v G i ) . Undoubtedly, the smaller (| v G i −
    E i |/ v G i ) is, the closer the estimated value is to the truth data. (| v G
    i − E i |/ v G i ) . For example, if a UAV reports a temperature of 25 °C at a
    particular location and the MEI method gets the estimated value of truth data
    23 °C, the data bias is |25 – 23 |/25 = 0.08. Minimizing this data bias is one
    of our main objectives. For n tasks, the data quality of all tasks obtained by
    the strategy is D=([ ∑ n i=1 (| v G i − E i |/ v G i )]/n) . The one purpose of
    the strategy design is to minimize D . Definition 2 (Recruitment Cost):As there
    is a direct relationship between the workers’ bids and the cost of the platform,
    that is complex. Here, we only consider the costs incurred when rewarding workers,
    i.e., the workers’ bids for the tasks. For instance, worker w i negotiates with
    the platform, the bid to complete task r i is denoted as b i , the cost of the
    completed task r i is b i , and the total cost paid by the platform for n tasks
    is C= ∑ i∈{1…n} b i . So if three workers place bids of $ 5, $ 7, and $ 9, respectively,
    to complete a task, and the platform accepts the $ 7 bid, the recruitment cost
    for that task is $ 7. Clearly, minimizing C is the cost objective of the strategy.
    The specific goals of the strategy are to optimize the following: ⎧ ⎩ ⎨ min:D=
    1 n ∑ n i | v G i − E i | v G i min:C= ∑ i∈{1,…,n} b i . (1) View Source SECTION
    IV. Our Proposed DTD Scheme A. Research Motivation Traditional methods, such as
    MEI, MVI, and WEI, can be used to infer the data truth when most workers are trusted.
    However, the accuracy of these methods heavily depends on the honesty of the worker
    group. In real-world scenarios where worker quality is uncertain, accurately determining
    the truth of data becomes challenging. To tackle this, anomaly detection techniques
    have been proposed to assess worker quality and identify anomalies or deviations
    from a normal distribution. Nevertheless, practical applications of these methods
    still need to address challenges related to the lack of ground data or significant
    deviations from it. To highlight the limitations of conventional methods, we delve
    into a real-world scenario of a crowdfunding task to demonstrate the shortcomings
    of traditional methods. This task required sensing temperature from five specific
    locations and recruiting 18 workers, including 11 abnormal ones, to perform this
    task. The locations ℓ 1 , ℓ 2 , ℓ 3 , ℓ 4 , and ℓ 5 have three workers, four workers,
    four workers, four workers, and three workers, respectively. Table II illustrates
    the attribute and data of workers. TABLE II ID, Attribute, and Data of Workers
    According to the data reported by these workers, we use traditional methods to
    infer truth data. The bias of inferred data is shown in Table III. As can be seen
    from the traditional method, if most workers provide reliable data, the bias is
    slight. Conversely, if the proportion of low-quality workers is high, especially
    if they collude in fraudulent activities, the inferred data will deviate significantly
    from the truth. TABLE III Data Bias of Traditional Methods The reasons for this
    are twofold. Traditional methods lack mechanisms to filter out low-quality workers.
    When tasks are randomly assigned to a group consisting mostly of low-quality workers,
    they may collectively cheat the platform by submitting forged data. As a result,
    the platform can be misled and consider the forged data as true. Assessing the
    trustworthiness of workers is challenging in traditional methods due to the absence
    of ground truth data for detecting data reliability. In a realistic system where
    multiple parties deploy clients, their trust levels are unknown. Consequently,
    the platform cannot identify and filter out low-quality workers, compromising
    the inferred data’s accuracy. So to overcome the drawbacks of the traditional
    methods, our strategy is presented. The strategy in this article is shown in Fig.
    2, and the primary process is as follows. Step 1:As in Fig. 2(a), in the initialization,
    a platform randomly assigns tasks to a batch of unidentified workers, and each
    unidentified worker senses data for several locations for the task. While workers
    w i ,…, w n sense the data from some required location, the platform assigns some
    UAVs to the same location to collect data as ground truth. Then these workers
    and UAVs upload data back to the platform. Step 2:As in Fig. 2(b), the platform
    compares the submitted data and ground truth after receiving the data reported
    by workers and UAVs. If the data reported by worker w i is consistent with the
    ground truth, then worker w i is judged by the platform to report reliable data.
    Otherwise, worker w i submits the false data. Then the data trust for worker w
    i is calculated according to the bias between submitted data and the plurality
    of the reliable data. This way, the unidentified workers can be detected as reliable
    and unreliable. Step 3:These workers report data with bids to get a reward to
    the platform. Similarly, the platform evaluates their bids, where lower bids bring
    less platform cost, and reasonable bids can make the market fairer. Therefore,
    the bid trust is calculated on their bid attributes. According to the bid trust,
    the reliable workers can be classified as normal-bid and abnormal-bid workers,
    as shown in Fig. 2(c). Step 4:As Fig. 2(d) shows, the reliable workers’ contribution
    value (hereinafter referred to as CV) is calculated based on the data trust and
    the bid trust above. Fig. 2. Discovery of the high-quality workers. (a) Collect
    data. (b) Filter low-quality workers. (c) Verify workers’ bids. (d) Identify workers’
    quality. Show All In the later stages of recruiting workers for MCS, the platform
    assigns sensing tasks to high-quality workers according to their CVs, aiming to
    improve data quality, reduce cost, and promote market fairness. B. Truth Inference
    and Finding High-Quality Workers This section mainly introduces the trust evaluation
    mechanism and identification strategy for workers, where worker trust consists
    of data trust and bid trust mentioned above. Fig. 3 illustrates the principle
    of truth inference. Fig. 3. Data detection based on DM-GDF. (a) Initialize data.
    (b) Detect data. Show All 1) Computation of Data Trust: We propose a method of
    DM-GDF to filter the false data reported by unidentified workers based on a detection
    method, which is also called a truth inference approach. Based on the truth of
    data submitted by the workers, the workers’ trust is given so that the platforms
    select reliable workers to collect data to obtain high-quality truth data. First,
    the DM-GDF project the ground truth data V g sensed by the UAV into a feature
    space to find the detection boundary based on the data V g . Then this detection
    boundary is applied to detect the similarity of unlabeled data V ul reported by
    workers with unknown trust with V g . Finally, if data points of V ul are in the
    data points region of V g , its data is detected as reliable data, denoted as
    V r . Otherwise, it is false, denoted as V ur . Through the above, workers can
    be detected as reliable or unreliable based on the result. The truth discovery
    process is detailed as follows. Step 1:We solve the weight θ and the deviation
    β based on the data V g , which is used to construct the detection boundary for
    data from workers. The objective function of DM-GDF is as (2). Then, the detection
    function (5) can be solved based on the coefficients θ and β to discover an optimal
    detection boundary. Thus, true and false data can be divided by the boundary,
    and the boundary filters the false ⎧ ⎩ ⎨ ⎪ ⎪ min θ, ξ i ,β 1 2 ∥θ∥ 2 + 1 λn ∑
    n i=1 ξ i −β s.t. θ T ×ϕ( v g i )≥β− ξ i , ξ i ≥0 foralli=1,…,n. (2) View Source
    Before optimizing (2), we perform quadratic programming on (2) to make the calculation
    simpler by constructing the equivalent Lagrange multiplier method, as (3). In
    (3), θ T denotes the transpose of θ , and β is the bias parameter. ξ is a slack
    variable, indicating the ratio of data to be misdetected. λ∈[0,1] is the penalty
    factor, which is used to determine the slack range of ξ . A bigger λ means more
    detection errors are allowed, causing lower accuracy. n is the size of the data
    to be detected. α , μ are Lagrange multipliers, which relate to ⎧ ⎩ ⎨ ⎪ ⎪ L(θ,β,ξ,α,μ)=
    1 2 ∥θ∥ 2 + 1 λn ∑ n i=1 ξ i −β + ∑ n i=1 α i [β− ξ i − θ T ϕ( v g i )]− ∑ n j=1
    μ i ξ i s.t.α≥0, μ i ≥0. (3) View Source To facilitate the solution of α , we
    convert (3) into its dual problem form, and then (4) is obtained { min α 1 2 ∑
    m i=1 α i ∑ n j=1 α j ϕ( v g i T )ϕ( v g j ) s.t.0≤ α i ≤ 1 λn , ∑ n i=1 α i =1.
    (4) View Source Step 2:The data detection boundary is constructed based on the
    weight parameter θ and the bias parameter β , and the data uploaded by workers
    can be detected. Then θ and β are substituted into (5) to calculate the data attributes,
    i.e., reliable data or unreliable data. The Y( v ul i ) can be calculated as follows:
    Y( v ul i )=sgn( θ T ×ϕ( v ul i )−β). (5) View Source In (5), sgn is the symbolic
    function. If Y( v ul i )=1, v ul i submitted by worker w i is reliable at this
    time. On the contrary, if the function Y( v ul i )=−1 , the worker w i is unreliable.
    The detection function is shown as Algorithm 1. Step 3:We evaluate the workers’
    data trust based on their data attributes obtained. The data trust can be calculated
    as follows: T d,t i ( w i )={ 1, 0, ifY( v ul i )=1 otherwise. (6) View Source
    Algorithm 1 Data Anomaly Detection Input: V g ={ v G 1 … v G n } , ϕ( v G i ,
    v G j ) , λ∈[0,1] Output: Y( v ul i ) Function DAD() 1: Initialize parameters
    2: Choose an appropriate kernel function ϕ( v G i , v G j ) 3: Normalize data
    V g 4: Set λ∈[0,1] to control the number of outliers 5: Define slack variables
    ξ i ≥0 6: Initialize θ,β, v G i 7: Repeat until convergence: 8: For i=1 to n Do
    9: Calculate the similarity 10: Calculate Eq. (5) using the formula 11: End for
    12: Solve the optimization problem 13: Calculate Eq. (2) 14: End repeat 15: Calculate
    distance to hyperplane using Eq. (5) 16: Y( v ul i )⟵ {1 |−1} 17: Classify the
    sample as in-class or outlier 18: Return Y( v ul i ) Since trust updating is a
    slow accumulation process, we will not significantly increase the trust because
    they have reported reliable data once. Therefore, to prevent malicious workers
    cheating the platform, we need to detect the data v r 1 ,…, v r n and v ur 1 ,…,
    v ur n , these data submitted by workers and calculate their data trust repeatedly.
    This involves assigning tasks to workers multiple times Df(k,n)={ 1, k−1 ∑ n i=1
    (i−1) , ifn=1 ifn>1. T d i ( w i )= ∑ k=1 n Df(k,n)× T d,k i ( w i ). (7) (8)
    View Source Since we mainly consider the recent data trust, where Df(k,n)∈[0,1]
    denotes the weight of current time step taking the maximal value in Df(k,n) .
    The calculation of weight of j th cycle is as (7). The weighted sum of data trust
    of all time steps represents the total data trust, as (8). Then we set the threshold
    T d,min i to identify reliable workers. If T d i ( w i ) greater than or equal
    to T d,min i , the worker w i will be determined as a reliable worker w r i .
    Conversely, the worker w i is unreliable w ur i , and the platform rejects communication
    with the workers and does not compute the bid trust of such workers. 2) Computation
    of Bid Trust: Realistically, dishonest workers can disrupt market fairness through
    price manipulation. Solely prioritizing low bids might overlook reasonably priced
    workers, leading to scarcity of tasks and potential betrayal, thus endangering
    task completion. The platform should balance cost and reliable worker recruitment
    to enhance task completion quality and efficiency. To address this, confidence
    intervals can be used to determine a reasonable bid range by analyzing bid dispersion.
    Confidence intervals, a statistical tool, provide a range within which the true
    value of a statistic likely falls, given a confidence level (usually 95% or 99%).
    This concept could be applied here to calculate a reasonable bid range by following
    steps. Step 1:Calculate the mean value of bids b ¯ ¯ i , and the standard deviation
    σ( b i ) of the bids of n reliable workers. The σ( b i ) can be calculated as
    follows: σ( b i )= 1 N−1 ∑ i=1 N ( b i − b ¯ ¯ i ) 2 − − − − − − − − − − − − −
    − − −  ⎷   . (9) View Source Step 2:Calculate the standard error E( b i ) of
    the bids of n reliable workers. The E( b i ) can be calculated as follows: E(
    b i )=σ( b i ) 1 N − − √ . (10) View Source Step 3:The lower limit of the confidence
    interval C LO , and the upper limit C UP are calculated. The C LO is the mean
    minus Z times the standard deviation, and the C UP is the mean plus Z times the
    standard deviation. The C LO and C UP can be calculated as follows: { C LO = b
    ¯ ¯ i −Z×E( b i ) C UP = b ¯ ¯ i +Z×E( b i ). (11) View Source Z is the quantile
    of the normal distribution corresponding to a confidence level. At a 95% confidence
    level, Z equals 1.96, representing the number of standard deviations from the
    mean encompassing 95% of the data. Hence, for a 95% confidence interval in a normal
    distribution, we use 1.96 as a constant. These values are available from normal
    distribution tables or statistical software. Step 4:Based on the above calculations,
    we obtain a range [ C LO , C UP ] of bids within this range that fall within the
    appropriate bid with 95% confidence. Therefore, we consider bids within the interval
    [ C LO , C UP ] to align with market rules and bids outside this interval to be
    anomalous. If worker w r i ’s b i for task i falls within this interval at t time,
    we set the worker’s bid trust T b,t i ( w r i ) to 1; otherwise, it is set to
    0. 1 means the worker’s bid is normal and denoted by b n i . The opposite is abnormal,
    denoted by b a i . T b,t i ( w r i ) can be calculated as follows: T b,t i ( w
    r i )= ⎧ ⎩ ⎨ ⎪ ⎪ 1, 0, s.t. if C LO ≤b i ≤ C UP otherwise { b 1 ,…, b n }∼N(μ,
    σ 2 ). (12) View Source The confidence interval, which assumes a normal bid distribution
    and sufficient sample size, estimates a range for the true parameter value. However,
    it cannot guarantee that this value will be within this range. Adjustments are
    needed if these assumptions are unmet. The mean, which can be used when the bid
    distribution does not follow a normal distribution, provides an estimate for the
    central tendency of the parameter value. The worker’s T b,t i ( w r i ) is the
    same as the data trust degree T d i ( w r i ) , which cannot be determined once
    in a lifetime. Therefore, we use the following formula to calculate T b i ( w
    r i ) in n moments. The bid trust is shown as Algorithm 2 T b i ( w r i )= ∑ k=1
    n Df(k,n)× T b,k i ( w r i ). (13) View Source Algorithm 2 Computing Bid Trust
    Input: B, W r Output: T b i ( w r i ) , W n , W a Function CBT() 1: For each w
    r i ∈ W r Do 2: mean⟵sum(B)/len(B) 3: sqd⟵0 4: For each b i ∈B : 5: sqd+=( b i
    −mean)∗∗2 6: End For 7: std⟵(sqd/len(B))∗∗0.5 8: confidence_level⟵0.95 9: z_value⟵1.96
    10: ste⟵std/(len(B)∗∗0.5) 11: lower_bound⟵mean−z_value∗ste 12: upper_bound⟵mean+z_value∗ste
    13: If lower_bound≤b i ≤upper_bound Then 14: T b i ( w r i )⟵1 , w n i ⟵ w r i
    , b n i ⟵ b i 15: Else If upper_bound<b i || b i <lower_bound Then 16: T b i (
    w r i )⟵0 , w a i ⟵ w r i , b a i ⟵ b i 17: End If 18: End For 19: Calculate T
    b i ( w r i ) according to Eq. (13) 20: Return T b i ( w r i ) , W n , W a 3)
    Finding High-Quality Workers: By setting the threshold for the T b i ( w r i )
    , we can find workers W n and W a . To balance data quality and cost, we propose
    a scheme that considers the normal bid workers’ data and bid trust. To maximize
    data quality and minimize cost, we address the tradeoff between these objectives
    using an entropy-weighted multiobjective optimization approach. Each normal bid
    worker’s contribution value Cv( w n i ) , which reflects their quality, is evaluated
    in this framework. Utilizing the entropy weight method ensures a balanced data
    quality and cost consideration without requiring prior knowledge ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪
    PT d i ( w n i )= T d i ( w n i ) ∑ n j=1 T d j ( w n i ) PT b i ( w n i )= T
    b i ( w n i ) ∑ n j=1 T b j ( w n i ) . { H d =− ∑ n i=1 PT d i ( w n i )×log(
    PT d i ( w n i )) H b =− ∑ n i=1 PT b i ( w n i )×log( PT b i ( w n i )). ⎧ ⎩
    ⎨ ρ 1 = 1 −H d (1 −H d )+(1 −H b ) ρ 2 = 1 −H b (1 −H d )+(1 −H b ) . (14) (15)
    (16) View Source ρ 1 and ρ 2 are the weights controlling the quality and cost.
    PT d i ( w n i ) is the probability that worker w n i appears in the data trust
    sample { T d 1 ,…, T d n } . PT b i ( w n i ) is the probability that worker w
    n i appears in the normal bid trust sample { T b 1 ,…, T b n } . H d is the information
    entropy in the data trust sample { T d 1 ,…, T d n } . H b is the information
    entropy in the bid trust sample { T b 1 ,…, T b n } . The Cv( w n i ) can be calculated
    as follows: Cv( w n i )= ρ 1 ×T d i ( w n i )+ ρ 2 ×T b i ( w n i ). (17) View
    Source To accurately assess each worker’s Cv( w n i ) , we normalize it between
    0 and 1, with 1 indicating optimal Cv. T d i ( w n i ) plays a vital role in task
    data quality, hence, task design and worker selection should minimize the influence
    of ρ 2 , ensuring diverse and representative data. Workers are assigned to perform
    tasks based on their Cv ranking, i.e., recruiting high-quality workers w h 1 ,…,
    w h n to sense data. If workers W h are not available, the platform may assign
    abnormal bid workers w a 1 ,…, w a n to perform tasks, disregarding market costs
    or fairness. The assignment scheme is calculated as follows: { CV={Cv( w n 1 ),Cv(
    w n 2 ),…,Cv( w n n )} argmaxAl(CV):R→CV. (18) View Source SECTION V. Performance
    Analysis This section includes the following components: presentation of the two
    data sets used in our study, comparison of the performance of different inference
    methods through experiments, simulation of data sensing tasks using our proposed
    strategy, and analysis and comparison of various metrics. A. Experimental Precondition
    Description We mainly compare our methods with the classical or state-of-the-art
    methods: the MEI, MVI, and WEI methods. The truth discovery and evaluation process
    are based on the AliCloud Tianchi Lab and Kaggle Notebook platforms for our method
    to be conducted in a real environment. The following is a description of the data
    sets we used. SuavDS: We simulate the flight trajectory of UAVs to generate 100
    pieces of ground truth data V g according to distribution V g ∼N(μ,σ) for ten
    locations, which contains 22 features. The first 21 features are the ground truth
    data V g in ten cycles. For the unknown truth data V ul we generate 100 pieces
    of data, where the data features include the data the workers’ labels. AepDS:
    This data set is generated by monitoring a house with ZigBee wireless sensor networks.
    The data set contains 29 features covering the temperature and humidity information
    of a house sensed by the sensor during certain periods. To facilitate truth inference,
    we randomly sampled and preprocessed the data set. The preprocessed data is called
    ground truth data V g , which contains 21 features and 100 samples, which are
    temperature and humidity data, respectively. To generate unknown truth data V
    ul , we randomly sample 40 data samples from V g , and generate 60 negative data
    samples. B. Truth Discovery In our DTD scheme, the penalty factor λ in (2) is
    a key moderator of the truth discovery accuracy and the generalization ability
    of the DM-GDF model. To optimize λ , we perform hyperparameter searches that vary
    within a predetermined range. To visualize how λ affects the search process for
    decision boundaries, we chose two representative values of λ —0.01 and 0.5. In
    this context, “decision boundary” is the bounds on which the model makes its truth-testing
    decisions. Fig. 4 shows how λ values affect the truth discovery process and decision
    boundary formation in two data sets. This process will help us to avoid overfitting
    more effectively and improve the accuracy of truth discovery. Fig. 4. Truth discovery
    of the DTD. (a) Task 1 in SuavDS. (b) Task 1 in SuavDS. (c) Task 1 in AepDS. (d)
    Task 1 in AepDS. Show All In practice, the DTD first evaluates workers’ CVs to
    select high-quality workers who provide real data and reasonable bids. In particular,
    the CVs can be optimal when the values of the quality weight parameter ρ 1 and
    the cost weight parameter ρ 2 are moderate. For example, in Fig. 5, the yellow
    dot in the top right corner represents a high-quality worker. Then, depending
    on the number of high-quality workers and the bids, the system will pay these
    workers accordingly. Fig. 5. Trust and CVs of reliable workers. (a) Workers’ CVs
    in SuavDS. (b) Workers’ CVs in AepDS. Show All The DTD assigns workers’ CVs based
    on their ranking for subsequent assignments but only assigns one high-quality
    worker to each necessary assignment for data sensing. It no longer evaluates their
    CVs unless CVs expire. This strategy allows the platform to reduce costs in subsequent
    recruitment cycles. In contrast, for the traditional approach, the recruitment
    cost of its truth discovery process depends on the quality of the entire worker
    population. The metrics and benchmarks we use are the workers’ numbers and bids.
    In particular, we compared the costs of the DTD strategy with those of the traditional
    method for recruiting the same number of workers and paying the same bids. According
    to the results in Table IV, our DTD strategy is significantly less costly than
    the traditional recruitment strategy. This means that the recruitment cost of
    the DTD strategy is 38 percentage points lower than the traditional strategy.
    TABLE IV Cost of the Four Methods C. Sensitivity We use this metric to evaluate
    the performance of our proposed DTD and traditional inference methods on inference
    task truth, i.e., the MEI, MVI, and WEI methods. The assessment method is as follows:
    sensitivity= TP TP+FN . (19) View Source The TP represents the number of positive
    class samples correctly identified as a positive class, regarded as the number
    of real task data correctly identified by each inference method. FN represents
    the number of positive samples incorrectly identified as negative samples and
    the number of real data regarded as tasks incorrectly identified as false data
    by each inference method. From Fig. 6, the MVI and WEI methods had low sensitivity
    in two data sets for ten cycles. However, our proposed DTD method, combining unsupervised
    learning with V g feature training, shows excellent sensitivity, and is not affected
    by data set type and worker population quality, always staying above 0.93. The
    sensitivity of the DTD is 44% higher than that of the traditional method. Fig.
    6. Sensitivity of each method. (a) Sensitivity in SuavDS. (b) Sensitivity in AepDS.
    Show All D. Specificity Fig. 7 shows the specificity of each method in the two
    data sets. Similar to sensitivity, specificity is also an important metric for
    assessing the performance of truth inference methods. It indicates the proportion
    of all negative class samples correctly identified as such. Here, we use specificity
    to measure the correctness of the truth inference method in detecting anomalous
    data submitted by workers in the following way: specificity= TN TN+FP . (20) View
    Source Fig. 7. Specificity of each method. (a) Specificity in SuavDS. (b) Specificity
    in AepDS. Show All The TN indicates the number of negative samples correctly identified
    as negative, which is the number of anomalies submitted by workers correctly identified
    by each inference method. In contrast, the FP indicates the number of negative
    samples incorrectly identified as positive, which is the number of anomalies submitted
    by workers incorrectly identified by each inference method. From Fig. 7, we can
    observe that the traditional approach leaves the platform vulnerable to joint
    spoofing by unreliable workers. In contrast, the DTD method is not affected by
    the quality of the worker population. The DTD specificity is 5% higher than traditional
    methods. E. F1-Score F1-score is the summed average of precision and recall. Precision
    represents the proportion of task data assessed as correct by the inference method
    that is actually correct, while recall represents the proportion of task data
    that is actually correct that is estimated as correct by the inference method.
    The F1-score can be calculated as follows: F1−score= 2( TP TP+FP × TP TP+FN )
    TP TP+FP + TP TP+FN . (21) View Source As can be seen from Fig. 8, the F1-score
    of the traditional method is very low. On the other hand, our DTD method is high
    and stable and is not affected by unbalanced data. The DTD F1-score is 47% higher
    than traditional methods. Fig. 8. F1-score of each method. (a) F1-score in SuavDS.
    (b) F1-score in AepDS. Show All F. Accuracy Fig. 9 shows the accuracy of each
    method in the two data sets. Accuracy is the proportion of samples correctly inferred
    by each inference method to the total number of samples. The accuracy can be calculated
    as follows: Accuracy= TP+TN TP+TN+FP+FN . (22) View Source Fig. 9. Accuracy of
    each method. (a) Accuracy in SuavDS. (b) Accuracy in AepDS. Show All As can be
    seen from Fig. 9, the accuracy of the traditional method is not high. Our DTD
    method, on the other hand, is high. The DTD accuracy is 18% higher than traditional
    methods. G. Data Bias The inferred data bias D of varying methods is calculated.
    The traditional methods are affected by the whole quality and balance of worker
    data. It is hard to judge the data reliability when more workers with unknown
    trust join in tasks. However, for the DTD, the inferring process is not seriously
    affected by the low-quality workers. Therefore, the DTD can reach less inferred
    data bias. The bias of the DTD is reduced by 24 percentage points compared to
    traditional inference methods, as shown in Fig. 10. Fig. 10. Data bias of each
    method. (a) Bias in SuavDS. (b) Bias in AepDS. Show All SECTION VI. Conclusion
    In IIoT, both the quality of data and the cost of recruitment are crucial to the
    utility of MCS. We propose a DTD Scheme that filters low-quality workers to address
    these issues. Experimental results indicate that the scheme significantly outperforms
    traditional methods in terms of sensitivity (improvement 44%), specificity (improvement
    5%), accuracy (improvement 18%), and F1-score (improvement 47%). Moreover, it
    reduces data bias by 24 percentage points and costs by 38 percentage points. Despite
    its advantages, the scheme in practical MCS applications faces potential limitations
    and challenges, including various task types and temporal and spatial factors,
    that may affect workers’ ability and potential security threats and signal interference
    in UAV data collection. To overcome these challenges and to further enhance the
    effectiveness and applicability, future research may focus on developing methods
    to maintain data quality across a range of task types, mitigating the impact of
    temporal and spatial factors on worker performance, and bolstering the security
    and stability of UAV data collection. Authors Figures References Citations Keywords
    Metrics More Like This Graph Neural Networks for Anomaly Detection in Industrial
    Internet of Things IEEE Internet of Things Journal Published: 2022 Communication-Efficient
    Federated Learning for Anomaly Detection in Industrial Internet of Things GLOBECOM
    2020 - 2020 IEEE Global Communications Conference Published: 2020 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'DTD: An Intelligent Data and Bid Dual Truth Discovery Scheme for MCS in
    IIoT'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 51 papers. The special focus in this conference
    is on Recent Developments in Cyber Security. The topics include: Object Detection
    Using TensorFlow 2 and Amazon SageMaker; causes of Cyber Fraud in Commercial Banks
    in Nigeria: A Case Study of Zenith Banks in Abuja; identity-Based Designated Verifier
    Proxy Signature Scheme and Its Application to Health Care; a Survey: Analysis
    of Existing Hybrid Cryptographic Techniques; Hybrid Lightweight Cryptography Using
    AES and ECC for IoT Security; PrimeSwitch—Encryption and Decryption Algorithm
    Using RSA Key Generation; a Modest Approach Toward Cloud Security Hygiene; multi-Key
    Fully Homomorphic Encryption Scheme Over the Integers; amharic Language Hate Speech
    Detection Using Machine Learning; cyber-Attack Analysis Using Vulnerability Assessment
    and Penetration Testing; Security Flaw in TCP/IP and Proposed Measures; secure
    Horizons: Advanced Protection Mechanisms for Holographic Data Storage Systems;
    An Effective Model for Binary and Multi-classification Based on RFE and XGBoost
    Methods in Intrusion Detection System; interactive Learning for Patient Care:
    Blockchain Ingrained Electronic Health Record Management System with Patient Control,
    Data Quality and Security Assurance; Enhanced Pin Entry Mechanism for ATM Machine
    by Defending Shoulder Surfing Attacks; cloud-Based Object Detection Model Using
    Amazon Rekognition; novel Architecture and Secured Food Traceability Application
    Based on Ethereum Blockchain; an Investigative Study on Security Aspects and Authentication
    Schemes for Internet of Vehicles; static Analysis Approach of Malware Using Machine
    Learning; detection of Phishing Link Using Different Machine Learning Techniques;
    a Survey on Path Key Establishment; ENCRYPTO: A Reliable and Efficient Mobile
    App for Password Management; blockchain-Based Public Distribution System; preface.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: International Conference on Recent Developments in Cyber Security, ReDCySec
    2023
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - El Balbali H.
  - Abou El Kalam A.
  citation_count: '0'
  description: Data is one of the most valuable resources an organization may have;
    it can have a significant influence on its long-term performance, or even its
    existence. With the ever-increasing volume of data generated and collected every
    day from the Internet of Things (IoT), social media, and other sources, big data
    quality and security have become two of the most crucial concerns confronting
    organizations, especially in the field of agriculture, where IoT devices are increasingly
    being used to collect and monitor data on soil conditions, nutrient deficiencies…
    On the one hand, data must be carefully protected to prevent attacks or violations
    of its confidentiality, integrity, and availability. On the other hand, it must
    be of good quality for an efficient and effective decision-making process. This
    comprehensive study aims to explore both Big Data Quality and Big Data Security,
    as well as the potential conflict between them. In our experiments, we employed
    the CICIoT 2023 dataset, a novel and comprehensive IoT attack dataset that has
    never been used before. We appropriately preprocessed the dataset and applied
    different algorithms to correctly classify the traffic. We then present our proposed
    AI-based approach to improving big data quality to enhance security. We also demonstrate
    that our approach improves the accuracy, generalizability, and reliability of
    the data, resulting in high-quality data that we properly fed into our algorithms
    for accurate threat detection. Hence, this work allowed us to delve into various
    Machine Learning and Deep Learning algorithms, namely, K-nearest neighbors (K-NN),
    long short-term memory networks (LSTM), deep neural networks (DNN), and ensemble
    methods. As a result, when compared to other research, our approach achieved better
    threat detection.
  doi: 10.1007/978-3-031-54318-0_5
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Advanced
    Intelligent Systems for Sustainable Development AI2SD 2023: International Conference
    on Advanced Intelligent Systems for Sustainable Development (AI2SD''2023) pp 39–47Cite
    as Home International Conference on Advanced Intelligent Systems for Sustainable
    Development (AI2SD''2023) Conference paper AI-Driven Big Data Quality Improvement
    for Efficient Threat Detection in Agricultural IoT Systems Hiba El Balbali & Anas
    Abou El Kalam  Conference paper First Online: 21 February 2024 52 Accesses Part
    of the book series: Lecture Notes in Networks and Systems ((LNNS,volume 930))
    Abstract Data is one of the most valuable resources an organization may have;
    it can have a significant influence on its long-term performance, or even its
    existence. With the ever-increasing volume of data generated and collected every
    day from the Internet of Things (IoT), social media, and other sources, big data
    quality and security have become two of the most crucial concerns confronting
    organizations, especially in the field of agriculture, where IoT devices are increasingly
    being used to collect and monitor data on soil conditions, nutrient deficiencies…
    On the one hand, data must be carefully protected to prevent attacks or violations
    of its confidentiality, integrity, and availability. On the other hand, it must
    be of good quality for an efficient and effective decision-making process. This
    comprehensive study aims to explore both Big Data Quality and Big Data Security,
    as well as the potential conflict between them. In our experiments, we employed
    the CICIoT 2023 dataset, a novel and comprehensive IoT attack dataset that has
    never been used before. We appropriately preprocessed the dataset and applied
    different algorithms to correctly classify the traffic. We then present our proposed
    AI-based approach to improving big data quality to enhance security. We also demonstrate
    that our approach improves the accuracy, generalizability, and reliability of
    the data, resulting in high-quality data that we properly fed into our algorithms
    for accurate threat detection. Hence, this work allowed us to delve into various
    Machine Learning and Deep Learning algorithms, namely, K-nearest neighbors (K-NN),
    long short-term memory networks (LSTM), deep neural networks (DNN), and ensemble
    methods. As a result, when compared to other research, our approach achieved better
    threat detection. Keywords Big Data Security Big Data Quality Machine Learning
    Deep Learning Intrusion Detection Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF 1 Introduction Big Data refers to a collection of
    tools and technologies that facilitates the processing of large, complex, and
    various data, generated at high speed, that are difficult to process using traditional
    data processing applications. It is commonly described by five characteristics,
    also known as the 5Vs: Volume: it refers to the large amount of data generated.
    Variety: it refers to the diversity of the data. Velocity: it refers to the speed
    at which data is generated and processed. Value: it refers to the profits derived
    from the data. Veracity: it refers to the credibility of the data According to
    several studies [1, 2], Big Data faces many challenges, including quality and
    security. These challenges are mostly caused by the massive volumes of data, its
    heterogeneity, the dependability of data and its sources, and so on. All of these
    features can result in security breaches or loss of quality. Furthermore, analyzing
    and ensuring data quality might be a security barrier and backward. In the context
    of agricultural IoT systems, data quality is important for ensuring that the data
    is accurate and reliable, which is important for tasks such as crop monitoring,
    pest detection, and yield prediction. Furthermore, data security is also vital
    for protecting agricultural IoT systems from cyberattacks. These attacks can disrupt
    operations, steal valuable data, or even cause physical damage to equipment. Unfortunately,
    the majority of existing studies approach quality and security separately; yet,
    in the context of Big Data, the two are interrelated and may be addressed in three
    ways, as shown in Fig. 1 [3]: Fig. 1. Big Data between Quality and Security Full
    size image In the context of big data, data quality and security are two critical
    challenges that are often intertwined. While ensuring high data quality can improve
    the accuracy and reliability of data-driven insights, it can also make data more
    vulnerable to attack. Conversely, implementing strict security measures can protect
    data from unauthorized access, but it can also make it more difficult to access
    and use data for legitimate purposes. The confluence of quality and security can
    be effectively tackled through three distinct approaches, each of which offers
    its own unique perspective and strategies: The conflict between quality and security
    in Big Data; which states that ensuring high data quality can compromise data
    security, and enhancing security may lead to a decrease in data quality. The quality
    at the service of Security; which focuses on ensuring data quality to improve
    data security. The security at the service of Quality; which emphasizes the importance
    of data security to ensure data quality. In this work, we will focus specifically
    on improving data quality as it relates to strengthening security measures. This
    particular aspect, referred to as “quality at the service of security,” entails
    a complex approach that requires a meticulous process. Subsequently, this paper
    is structured as follows; the second section will be devoted to discussing Big
    Data Quality, and the third section will address Big Data Security. The fourth
    section will present some related works. The fifth section will introduce our
    proposed AI-based Big Data Quality Improvement for Efficient Threat Detection,
    and finally the sixth section will conclude our paper. 2 Big Data Quality Data
    quality is a multidimensional concept that represents the evaluation of information
    and is characterized by a set of measurable dimensions [3]. In the context of
    big data, it is a critical aspect of managing and utilizing large datasets effectively.
    It encompasses ensuring that the data is accurate, complete, consistent, and relevant
    for the intended purpose. Data quality challenges arise from the vast volume,
    variety, and velocity of data sources. Data quality issues can lead to biased
    results, inaccurate insights, and flawed decision-making. To address these challenges,
    organizations implement data quality frameworks that incorporate data profiling,
    data cleansing, data governance, and data monitoring. By maintaining high data
    quality, organizations can extract valuable insights from their big data assets
    and drive business growth. Besides, data quality is generally assessed using different
    dimensions. A dimension is a measurable property of data quality that represents
    some aspect of data, such as accuracy, precision, consistency, etc. Dimensions
    are evaluated using metrics; a metric is a quantifiable instrument that defines
    how a dimension is measured. Table 1 presents the most discussed dimensions in
    literature [4]: Table 1. Data quality dimensions Full size table Here, N: Total
    number of values Nnmv: Number of complete values Ncv: Number of correct values
    Nvrc: Number of values that respects the constraints Nvv: Number of valid values
    Nuv: Number of unique values Navn: Number of accessible values when needed Furthermore,
    in the context of Big Data, data quality faces many challenges, including, volume,
    and heterogeneity: Volume: with the large amount of produced data, it is very
    complex to assess or maintain the quality of this data in a reasonable time. Heterogeneity:
    data is often collected from different sources, this variety can bring several
    data structures (structured data, semi structured data and unstructured data)
    that make the assessment of data quality complex. 3 Big Data Security Big data
    security refers to all the policies, models, and mechanisms used to protect, both,
    the data and the analytics processes from information theft, DDoS attacks, ransomware,
    or other malicious activity that can come from offline or online sources and can
    crash a system [3]. In the era of big data, security has become a paramount concern,
    particularly in the context of IoT devices used in different fields including
    agriculture. The vast amount of data generated by these devices, coupled with
    their increasing connectivity, creates a fertile ground for cyberattacks [5].
    Protecting this data is crucial to safeguard operations, maintain financial stability,
    and ensure consumer privacy. In fact, IoT devices in agriculture collect a wide
    range of data, including soil moisture, crop health, and weather patterns. This
    data is valuable for optimizing resource management, improving crop yields, and
    enhancing decision-making [6]. However, the interconnected nature of these devices
    exposes them to vulnerabilities that can be exploited by malicious actors. Protecting
    agricultural IoT systems from cyber-attacks is crucial to ensure the integrity
    and reliability of modern farming technologies. Implementing robust security measures,
    like Access Control Management, Encryption, Intrusion Detection Systems…, is imperative.
    Let us take the example of Intrusion Detection Systems; An Intrusion Detection
    System (IDS) is a critical component of cybersecurity designed to identify and
    respond to potential security threats within a network or system. IDS are mainly
    classified into five categories: Network-based IDS (NIDS): Monitors network traffic
    in real-time to identify suspicious patterns or activities. Host-based IDS (HIDS):
    Monitors activities within individual devices or hosts, such as servers, workstations,
    or critical infrastructure components. Application Protocol-based IDS (APIDS):
    focuses on monitoring and analyzing the communication protocols used by applications
    to detect potential security threats or anomalous behavior. Protocol-based IDS
    (PIDS): focuses on monitoring and analyzing network protocols to detect potential
    security threats or abnormal activities. Hybrid IDS: combination of the types
    of IDS discussed above. The five types of intrusion detection systems rely on
    two types of detections: Signature-based IDS: Compares observed patterns against
    a database of known attack signatures or predefined rules. It is effective at
    detecting known threats and attacks with well-defined patterns. Anomaly-based
    IDS: Establishes a baseline of normal behavior and flags any deviations from this
    baseline as potential threats. It is capable of detecting previously unknown threats
    by identifying abnormal patterns. Now that we have discussed Big Data Security,
    let us present some related works linked to IoT, Intrusion Detection and ML/DL.
    4 Related Works IoT devices are increasingly utilized in the agricultural sector.
    Yet, these devices are susceptible to security risks arising from malicious design,
    poor implementation, and inadequate configuration. As a result, many Agri-IoT
    networks contain weak IoT devices that are easily hacked. The authors of [7],
    proposed a federated learning-based intrusion detection system for securing agricultural-IoT
    infrastructures. They used three deep learning classifiers, namely, deep neural
    networks, convolutional neural networks, and recurrent neural networks. The authors
    tested their approach using three datasets: CSE-CIC-IDS2018, MQTTset, and InSDN.
    The highest accuracy obtained was using InSDN dataset with 98.54% for DNN, 97.71%
    for CNN and 97.84% for RNN. In [8], the authors presented a framework for classifying
    intrusions into IoT networks used in agriculture. The evaluate their approach,
    they used NSL KDD dataset. They, first performed a preprocessing of the dataset,
    then, they applied multiple machine learning algorithms to classify the data.
    The obtained an accuracy of 98% for SVM, 85% for Random Forest and 78% for Logistic
    Regression. In [9], the authors presented an intrusion detection method to detecting
    injection attacks in IoT applications. They used two types of feature selection;
    constant removal and recursive feature elimination and they applied three Machine
    Learning classifiers, namely, SVM, Random Forest, and Decision Tree. They used
    the AWID dataset to evaluate their approach. As a result, they achieved an accuracy
    of 98.91% for Decision Tree, 98.91% for Random Forest, and 97.56% for SVM. The
    authors of [10] proposed a deep learning model for network intrusion detection
    for traffic anomaly detection, which combines attention mechanism and bidirectional
    long short-term memory network. The work consists, first, in extracting sequence
    features of data traffic via a convolutional neural network network, then reassigning
    the weights of each channel via the attention mechanism, and finally learning
    the network of sequence feat via Bi-LSTM. Their proposed approach, tested on NSL
    KDD dataset, resulted in 90.73% of accuracy. Table 2 summarizes the results discussed
    above: Table 2. Related works Full size table 5 AI-Based Big Data Quality Improvement
    for Efficient Threat Detection In the initial phase of our research, our primary
    focus will be on enhancing the quality of data collected from network traffic
    by employing rigorous preprocessing techniques. This preprocessing encompasses
    various steps, including data cleaning, normalization, and feature engineering,
    all aimed at ensuring that the data is accurate, consistent, and free of any anomalies
    or irregularities. Once this data has been prepared, we will harness the power
    of both machine learning (ML) and deep learning (DL) algorithms to conduct an
    in-depth analysis and classification of network traffic. These advanced algorithms,
    namely, K-NN, LSTM, DNN, and ensemble methods, will enable us to identify patterns
    and anomalies effectively, thereby empowering us to respond promptly and efficiently
    to potential security breaches or unauthorized activities. Hence, our architecture
    includes both, quality security. Furthermore, to evaluate the effectiveness of
    our work, we used the CICIoT 2023 dataset, which is a novel dataset that has never
    been used previously. Our dataset contains seven types of attacks, namely: Denial-of-Service
    (DoS) Attack: A DoS attack aims to render a machine or network inaccessible to
    its intended users by inundating the target with excessive traffic or providing
    it with information that induces a system crash. Distributed Denial-of-Service
    (DDoS) Attack: A DDoS attack is an extension of a DoS attack, utilizing multiple
    computers or machines to flood the target, intensifying the impact and making
    it even more challenging for the intended users to access the system. Brute Force
    Attack: A hacking technique employing trial-and-error to guess passwords or confidential
    information by systematically trying numerous combinations. This method enables
    unauthorized access to accounts and systems. Spoofing Attack: A cyberattack where
    the perpetrator conceals their identity or location to illicitly access a system
    or network, often exploiting this disguise to bypass security measures. Reconnaissance
    Attack: A cyberattack strategy focused on collecting information about a computer
    network, intending to circumvent its security controls by gaining insight into
    potential vulnerabilities. Web-Based Attacks: Malicious activities targeting web
    applications by exploiting weaknesses in their design or implementation. These
    attacks have the potential to lead to unauthorized access, data theft, or other
    detrimental consequences. Mirai Attacks: The Mirai attack refers to a form of
    malware that infiltrates Internet of Things (IoT) devices, such as web-connected
    video cameras. This malicious software transforms the compromised devices into
    a network of remotely controlled bots or “zombies,” collectively known as a botnet.
    We conducted our experiments on a Windows 10 operating system with 16 GB of RAM
    and an Intel(R) Core(TM) i7-7600U CPU @ 2.80 GHz, 2904 MHz Processor. For the
    LSTM model, the architecture comprises one LSTM layer followed by four hidden
    layers and finally a dense layer. Additionally, the DNN model features three hidden
    layers, each strategically designed to extract hierarchical representations from
    the input features. The proposed models are evaluated using a rigorous validation
    process, employing the grid search technique. This approach allows for an exhaustive
    exploration of hyper parameter combinations, ensuring the selection of optimal
    model configurations. Our investigation yielded noteworthy findings, shedding
    light on the impact of enhancing data quality to improve security. Table 3 shows
    a comparison of different ML and DL models applied to the CICIoT 2023 dataset.
    The results show that all the models performed very well and presented high performances.
    All our models predicted correctly negative and positive classes and presented
    an accuracy greater than 98.40%. We can also deduce that, overall, LSTM and DNN
    models presented the best performances, as approximately 99.40% of the data points
    were correctly classified, which is a good score. Table 3. Results of ML and DL
    classification Full size table Figure 1 to Fig. 4 presents the confusion matrices
    of our model and provide additional information to see the classified and misclassified
    samples. Fig. 1. Confusion matrix of K-NN model Full size image Fig. 2. Confusion
    matrix of DNN model Full size image Fig. 3. Confusion matrix of EM model Full
    size image Fig. 4. Confusion matrix of LSTM model Full size image These results
    underscore the significance of meticulous data preparation and the potential gains
    it can offer in safeguarding information systems against vulnerabilities and breaches.
    6 Conclusion In our research, we investigated the relationship between data quality
    and security in the context of Big Data. We used the CICIoT 2023 dataset, which
    is a novel and diverse dataset of network traffic data. We first enhanced the
    quality of the data by removing errors and inconsistencies, then, we applied multiple
    machine learning and deep learning algorithms to the data to develop models for
    threat detection. Our work has important implications for the development and
    deployment of secure agricultural IoT systems. By improving the quality of the
    data collected from IoT devices, we can develop more accurate and reliable threat
    detection models, which can help to protect agricultural systems from cyberattacks.
    We evaluated the performance of our models, and compared to other studies, our
    models produced better results, which underscores the importance of data quality
    for improving security. In our future work, we will eventually work on the multiclass
    classification to correctly classify each class of attacks. References Talha,
    M., Abou El Kalam, A., Elmarzouqi, N.: Big data: trade-off between data quality
    and data security. Procedia Comput. Sci. 151, 916–922 (2019). https://doi.org/10.1016/j.procs.2019.04.127
    Article   Google Scholar   Talha, M., El Marzouqi, N., Abou El Kalam, A.: Quality
    and Security in Big Data: Challenges as opportunities to build a powerful wrap-up
    solution. J. Ubiquitous Syst. Pervasive Networks 12(1), 09–15 (2019). https://doi.org/10.5383/juspn.12.01.002
    Article   Google Scholar   El Balbali, H., Abou El Kalam, A., Talha, M.: Big Data
    Between Quality and Security, pp. 1315–1326 (2023). https://doi.org/10.1007/978-3-031-27409-1_120
    Taleb, I., El Kassabi, H.T., Serhani, M.A., Dssouli, R., Bouhaddioui, C.: Big
    Data Quality : A Quality Dimensions Evaluation (2016). https://doi.org/10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.145
    Lee, I.: Internet of Things (IoT) Cybersecurity: Literature Review and IoT Cyber
    Risk Management. Futur. internet 12(9), 157 (2020). https://doi.org/10.3390/fi12090157
    Article   Google Scholar   Akhter, R., Sofi, S.A.: Precision agriculture using
    IoT data analytics and machine learning. J. King Saud Univ. - Comput. Inf. Sci.
    34(8) (2022). https://doi.org/10.1016/j.jksuci.2021.05.013 Friha, O., et al.:
    FELIDS: Federated learning-based intrusion detection system for agricultural Internet
    of Things. J. Parallel Distrib. Comput. 165, 17–31 (2022). https://doi.org/10.1016/j.jpdc.2022.03.003
    Article   Google Scholar   Raghuvanshi, A., et al.: Intrusion detection using
    machine learning for risk mitigation in IoT-enabled smart irrigation in smart
    farming. J. Food Qual. (2022). https://doi.org/10.1155/2022/3955514 Article   Google
    Scholar   Gaber, T., El-Ghamry, A., Hassanien, A.E.: Injection attack detection
    using machine learning for smart IoT applications. Phys. Commun. 52 (2022). https://doi.org/10.1016/j.phycom.2022.101685
    Fu, Y., Du, Y., Cao, Z., Li, Q., Xiang, W.: A deep learning model for network
    intrusion detection with imbalanced data. Electronics 11, 898 (2022). https://doi.org/10.3390/electronics11060898
    Download references Author information Authors and Affiliations National School
    of Applied Sciences, Cadi Ayyad University, Marrakech, Morocco Hiba El Balbali
    & Anas Abou El Kalam Corresponding author Correspondence to Hiba El Balbali .
    Editor information Editors and Affiliations Sciences and Techniques of Tangier,
    Abdelmalek Essaâdi University, Tangier, Morocco Mostafa Ezziyyani Systems Research
    Institute, Polish Academy of Sciences, Warsaw, Poland Janusz Kacprzyk Department
    of Automatics and Applied Software at the Faculty of Engineering, Aurel Vlaicu
    University of Arad, Arad, Romania Valentina Emilia Balas Rights and permissions
    Reprints and permissions Copyright information © 2024 The Author(s), under exclusive
    license to Springer Nature Switzerland AG About this paper Cite this paper El
    Balbali, H., Abou El Kalam, A. (2024). AI-Driven Big Data Quality Improvement
    for Efficient Threat Detection in Agricultural IoT Systems. In: Ezziyyani, M.,
    Kacprzyk, J., Balas, V.E. (eds) International Conference on Advanced Intelligent
    Systems for Sustainable Development (AI2SD''2023). AI2SD 2023. Lecture Notes in
    Networks and Systems, vol 930. Springer, Cham. https://doi.org/10.1007/978-3-031-54318-0_5
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-54318-0_5
    Published 21 February 2024 Publisher Name Springer, Cham Print ISBN 978-3-031-54317-3
    Online ISBN 978-3-031-54318-0 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Download book PDF Download book EPUB Sections Figures References Abstract
    Introduction Big Data Quality Big Data Security Related Works AI-Based Big Data
    Quality Improvement for Efficient Threat Detection Conclusion References Author
    information Editor information Rights and permissions Copyright information About
    this paper Publish with us Discover content Journals A-Z Books A-Z Publish with
    us Publish your research Open access publishing Products and services Our products
    Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio
    BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state
    privacy rights Accessibility statement Terms and conditions Privacy policy Help
    and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University
    of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: AI-Driven Big Data Quality Improvement for Efficient Threat Detection in
    Agricultural IoT Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Pei C.
  - Mhamdi L.
  citation_count: '0'
  description: 'As a heterogeneous networks, the Internet of Things (loT) is vulnerable
    to attacks, which brings a threat to human life and property. Intrusion Detection
    Systems (IDS) based on machine learning, a powerful tool have been developed and
    applied in network security. However, the deployment of highly complex IDS should
    consider the resource cost. Data preprocessing and feature engineering are capable
    of enhancing the performance and resource cost with effectiveness and low cost.
    Thus, this paper focuses on feature engineering which is divided into 4 steps:
    elimination of spurious features, data splitting, labeling and normalization,
    and feature selection. Through the feature reduction, we generate a dataset with
    different dimensions on the TON IoT dataset. To evaluate the effectiveness and
    universality of the proposed method, four machine learning methods including gradient
    boosting machine (GBM), K-Nearest Neighbors (KNN), Random Forests (RF), and MultiLayer
    Perceptron (MLP) are implemented into the simulation in Python. The comparative
    results indicate that Random Forest performs optimally in terms of feature engineering,
    achieving accuracies of 99.96 % and 99.52 % in binary and multiclass experiments,
    respectively.'
  doi: 10.1109/GIIS59465.2024.10449909
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2024 Global Information Infra... Enhancing
    IoT Security Through Resource-Efficient Feature Engineering Publisher: IEEE Cite
    This PDF Chonghao Pei; Lotfi Mhamdi All Authors 32 Full Text Views Abstract Document
    Sections I. Introduction II. Related Works III. Data Preprocessing IV. Simulation
    and Result V. Conclusion Authors Figures References Keywords Metrics Abstract:
    As a heterogeneous networks, the Internet of Things (loT) is vulnerable to attacks,
    which brings a threat to human life and property. Intrusion Detection Systems
    (IDS) based on machine learning, a powerful tool have been developed and applied
    in network security. However, the deployment of highly complex IDS should consider
    the resource cost. Data preprocessing and feature engineering are capable of enhancing
    the performance and resource cost with effectiveness and low cost. Thus, this
    paper focuses on feature engineering which is divided into 4 steps: elimination
    of spurious features, data splitting, labeling and normalization, and feature
    selection. Through the feature reduction, we generate a dataset with different
    dimensions on the TON IoT dataset. To evaluate the effectiveness and universality
    of the proposed method, four machine learning methods including gradient boosting
    machine (GBM), K-Nearest Neighbors (KNN), Random Forests (RF), and MultiLayer
    Perceptron (MLP) are implemented into the simulation in Python. The comparative
    results indicate that Random Forest performs optimally in terms of feature engineering,
    achieving accuracies of 99.96 % and 99.52 % in binary and multiclass experiments,
    respectively. Published in: 2024 Global Information Infrastructure and Networking
    Symposium (GIIS) Date of Conference: 19-21 February 2024 Date Added to IEEE Xplore:
    01 March 2024 ISBN Information: ISSN Information: DOI: 10.1109/GIIS59465.2024.10449909
    Publisher: IEEE Conference Location: Dubai, United Arab Emirates SECTION I. Introduction
    The rapid growth of IoT, pushed by developments in wireless communication and
    chip technology, has significantly impacted various fields like transportation
    and healthcare [1]. However, in IoT networks, simple equipment with low performance,
    such as sensors and cameras, often cannot deploy effective security measures.
    This can lead to security threats, including data leaks, battery depletion, hardware
    damage, and threats to human property, privacy, and even life safety. Intrusion
    Detection Systems are deployed in the network, which can monitor and analyze network
    traffic to detect intrusions. Because of the widespread adoption of machine learning
    technologies, the IDS combined with machine learning has been extensively researched
    [2]–[4]. Many studies are transitioning from classical machine learning to deep
    learning. However, in the practical IoT environment, the different deep learning
    methods will be limited by the physical resources of the IoT devices. Therefore,
    the deployability and lightweight of IDS should be considered. One viable solution
    is to focus on feature engineering, which diminishes the complexity of the IDS
    by reducing the number of features. In addition, most research is based on testing
    different datasets [5]–[8]. However, datasets commonly contain redundant information
    and spurious features. Therefore, reducing redundant and fake features through
    feature engineering not only helps to diminish the complexity of IDS but also
    improves the model''s generalization. This article analyzes the feasibility of
    feature selection in deploying lightweight IDS and tests the performance of various
    machine learning algorithms after reducing features. In this context, the study
    implements the Pearson Correlation Coefficient (PCC) to reduce the dimensions
    of input data, contrasting it with the Principal Component Analysis (PCA) approach.
    The results indicate that PCC provides a more effective optimization in terms
    of computing cost in Random Forest models compared to PCA, contributing to the
    practicality of lightweight deployment in IDS. The remainder of this paper is
    arranged below. Section II introduces IDS research and the IoT datasets. Section
    III formulates the method used for data preprocessing and the approach of feature
    selection. In section IV, the feasibility of reducing the number of features is
    verified by using different machine learning algorithms. Finally, we conclude
    the paper in section V. SECTION II. Related Works A. Intrusion Detection System
    and Feature Engineering Machine learning and deep learning have been adopted for
    the IoT IDS. Sivanathan et al. used a two layer model combined with the Bayes
    classifier and random forest classifier to identify the behaviors of IoT IoT devices.
    This model has achieved an accuracy of 99.98% in devices classification, and also
    tested for malicious behaviors in network devices based on the fluctuations in
    the confidence of classes [2]. Xiaoyu et al. [3] pointed out a model based on
    long short-term memory (LSTM) for distributed denial of services (DDoS) detection,
    ignoring the complex manual feature selection. In [4], a sparse autoencoder with
    a support vector machine (SVM) is used for dimensionality reduction, reducing
    the cost time. Furthermore, [6] proposed a semi-supervised deep learning approach
    for intrusion detection on CICIDS2017 [7] datasets. They solved computing resources
    through Fog-enable IoT and compared cost time with different models. For the feature
    selection, one method is manual selection based on statistical analysis. Another
    method is using machine learning and deep learning models to generate or extract
    important features. The principal component analysis in features engineering has
    been adopted in [8]. They compared the performance of three different IDS models
    and found the random forest models achieved the best performance. The research
    in [9] used the synthetic minority oversampling technique (SMOTE) to deal with
    the imbalanced problem of samples in IoTID20. Another study [10] deployed an IDS
    using deep autoencoder and RF algorithms in the network, achieving an accuracy
    and precision rate of 98%. Table I Distribution of TON-IoT features. B. Dataset
    There are many studies about IDS and they usually use different datasets. For
    example, NSL-KDD [11], CICIDS2017 [6], CICIDS2018 [6], CICIoT2022 [12], and UNSW-NBI5
    [13]. These datasets include a large volume of network traffic data, comprising
    various types of network attacks and normal traffic. While these datasets are
    based on real network traffic, they cannot include and identify all network attacks.
    Additionally, due to the speed of network updates, these datasets may contain
    outdated information, which can affect the performance of IDS. These datasets
    do not focus on the IoT environment and, thus, do not contain many characteristics
    of IoT A dataset that lacks features related to IoT devices can negatively impact
    the performance of the model. Therefore, many people choose to develop datasets
    based on IoT traffic. For example, there are UNSW-IoT [2], Aposemat IoT-23 [14],BoT-IoT
    [15], TON-IoT [16]. The UNSW-IoT and CICIoT2022 datasets have both been established
    on real-world IoT testbeds, including a variety of IoT devices and communication
    protocols. The BoT-IoT dataset focuses on botnet identification, which is collected
    from simulated, including 5 types of attack. Due to the lack of real traffic from
    IoT devices, TON-IoT is developed. This dataset is collected from a more complex
    IoT network with three layers of Edge, Fog, and Cloud. SECTION III. Data Preprocessing
    A. Dataset Selection Some datasets are generated for specific studies. these datasets
    lack timeliness and general applicability. Therefore, this study chose the new
    dataset TON-IoT as the experiment dataset. This dataset is collected and generated
    from the Cyber Range and IoT Labs at the UNSW Canberra. These Labs generate heterogeneous
    IoT device data. To simulate different telemetry devices, the authors develop
    different scripts to simulate devices such as GPS, thermostats, refrigerators,
    etc. Real data is generated using smart IoT devices like Amazon Echo and plug
    switch. This dataset consists of 45 features, including two label features as
    outlined in Table I. For attack traffic, different tools such as Kali System,
    Metasploitable3, Scapy, and custom scripts are used to generate nine types of
    attacks. Table III shows nine types of attack. B. Spurious Features The TON-IoT
    dataset consists of 45 features, classified into 7 categories as shown in Table
    I. According to the literature, the attack traffic is labeled based on IP and
    time, and is generated by offensive systems at specific times. In the real network
    environment, the origin and timing of attacks are unknown. In order to mitigate
    bias and enhance generality, the simulation ignores the time feature ''ts'' and
    IP feature ''src_ip’. Consequently, after disregarding the two label features
    ‘label’ and ‘type’, 41 useful features are remaining in the dataset. C. Split
    Dataset As mentioned, the TON-IoT dataset encompasses 9 attack categories. It
    is necessary to ensure the split data can represent the distribution of the whole
    dataset, which can avoid overfitting and bias. Therefore, based on different attack
    types, split the data into 70% for training and 30% for testing. As shown in Table
    I, except for the Man-in-the-Middle (MITM) attack, each training attack traffic
    comprises 14,000 samples. The small number of MITM attack samples reflects an
    imbalance in the dataset, which could affect the results in multi-classification.
    D. Label and Normalize There are features with missing values and string values.
    Because machine learning algorithms are incapable of processing string data, the
    approach of labeling is used to transform string data and missing data into numerical
    values. For example, TCP is assigned the label ‘1’, UDP ‘2’, and ICMP ‘3’. The
    different features often have various scales (e.g., the maximum value for ‘duration’
    is 93516 and the labeled values for ‘poro’ are in a range of 0–3), which is necessary
    to eliminate scale influences and enhance model efficiency, The normalization
    approach is the Min-Max Scaling, expressed by the following formula: X scaled  =
    X− X min X max − X min (1) View Source Here, X represents the original data, while
    Xmin and X max represent the minimum and maximum values of the respective feature.
    E. Features Engineering As shown in Table I, there are numerous features associated
    with particular network information, such as DNS, SSL, HTTPS, and Violation. As
    these features all describe one specific type of network activity, features within
    the same category likely exhibit linear relationships. These similar features
    encompass redundant information, which can easily result in overfitting. In order
    to reduce the redundant information, a correlation matrix is deployed to group
    features. The correlation matrix uses the Pearson correlation coefficient [17]
    as a metric to measure the relationship between different variables. The PCC can
    directly show the coefficient between features. The range of PCC is between −1
    and 1, the value close to 1 means that there is a strong linear relationship between
    features. In contrast, a value close to −1 means a negative linear relationship.
    A value of 0 shows that features have no linear relationship. Therefore, a correlation
    threshold is generated, features with a PCC higher than the threshold are grouped
    as a high-correlated set, and only the feature with the highest PCC is selected
    from each group. After excluding the selecting features, as illustrated in Table
    below, different numbers of features are obtained for threshold values of 0.5,
    0.4, 0.3, 0.2, and 0.1, respectively. The correlation heatmap of the 21 features
    selected shows that the majority of the remaining features have low linear relationships.
    Table II Distribution of TON-IoT dataset. Table III Number of features in different
    threshold SECTION IV. Simulation and Result This experiment is deployed on a laptop
    with Intel(R) Core(TM) i9-13900HX CPU at 2.20GHz with 32 GB of RAM and running
    on Windows 11. The models are deployed in Python, using the Sklearn package version
    1.3.0 and the PyTorch package. Whole experiments adopt 4-fold cross-validation
    to ensure the result can represent the dataset. A. Classifiers and Emulation Metrics
    To evaluate the feasibility of feature reduction, four distinct classifiers are
    employed: 1) Gradient Boosting Machine; 2) K-Nearest Neighbors; 3) Random Forest;
    and 4) a simple neural network, known as Multilayer Perceptron, with six different
    feature spaces. Hyperparameters are optimized to ensure each classifier performs
    well with the original set of features. Five evaluation metrics are considered
    [6]: Accuracy, Precision, Recall, F1 Score, and the Area Under the Curve (AUC).
    Table IV Summary of classifier performances in binary classification. Table V
    Summary of classifier performances in multi-classification. B. Result In binary
    classification, firstly, observe the result of different classifiers on the original
    features. While there are differences in the outcomes of different models, overall
    performances are excellent. The RF has the best result, while the performance
    of the MLP classifier is relatively poor because of the influence of hyperparameters.
    Subsequently, consider the performance of each classifier on different feature
    spaces. The results show that appropriate feature thresholds can optimize the
    performance of the models. Feature reduction performs better than the original
    features on KNN, RF, and GBM. The KNN model achieves the best performance with
    an accuracy of 99.82% when the threshold is set to 0.2 (the selection of 12 features).
    The reason is that the KNN algorithm is not very good at handling high dimensional
    data. As the dimension of features decreases, its performance improves. When the
    threshold is set to 0.5 (the selection of 21 features), the changes in metrics
    show a decrease in identifying positive samples and an improvement in entirety
    accuracy. The feature selection enables RF to achieve a similar performance to
    the original dataset. When the value of the threshold is 0.1, the RF model still
    maintains a high performance, whose all metrics can reach 99%. For MLP, feature
    reduction does not perform well. The model has a good performance only when the
    threshold is 0.5. As the dimension of the input data decreases, the performance
    is noticeably impacted. The primary reason is that its hyperparameters are designed
    for original features, and the change in the dimensions of input data is likely
    to result in degradation. For the multi-classification, the complexity and difficulty
    of classification rise when the number of categories increases. Additionally,
    machine learning is likely to meet challenges in overfitting and underfitting
    with an imbalanced dataset. As a result, there is a performance decline in all
    models compared to binary classification. However, the accuracy of each model
    can still be maintained at above 94%. With the change in the threshold, the results
    are similar to the binary classification simulation. The best performance for
    KNN and GBM is achieved when thresholds are 0.5 and 0.2, respectively. The Random
    Forest model is capable of maintaining an accuracy of 99% and F1 scores of 95%
    when selecting 15 features (threshold is 0.3). Due to the sensitivity of the model
    structure to dimensions the MLP model still suffers from significant performance
    degradation. These results show how feature engineering can impact the performance
    of different models. For ensemble learning models like GBM and RF, which consider
    the problems of overfitting and underfitting on design. These models are not sensitive
    to changes in dimensions, so feature engineering can work effectively. For KNN
    and MLP, which rely on hyperparameter settings. The models are sensitive to the
    feature space. In practical applications, feature engineering should be combined
    with hyperparameter settings to achieve better performance. C. Computing Resources
    Due to the limited resources of the IoT devices, the time cost and computing cost
    should be considered in the lightweight IDS models. The models are configured
    to emulate a real-life IoT environment, and PCA is included for comparative analysis.
    This simulation uses the psutil package to monitor the usage of the CPU on RF,
    GBM, MLP, and KNN models. There are four criteria for quantifying the results.
    The first is classification time, which refers to the time of model predicts the
    data. This is calculated by recording timestamps before and after model testing.
    A shorter time indicates faster model testing speed and simpler model structure.
    The second is memory usage. IoT devices typically run multiple programs. To ensure
    the normal functions of the devices, the models should use as little memory as
    possible during classification. Similarly, CPU usage is an important factor to
    consider. However, this study is conducted in a simulated environment and run
    on a single core and single thread, the performance of the CPU is still better
    than the CPU of a standard IoT device. Therefore, this data can provide a partial
    indication of the lightweight of models. Finally, the model size is considered,
    as it reflects the complexity of the model structure for deployment on IoT devices.
    According to the results, it can be shown that PCA and PCC contribute to the lightweight
    of the models. With the reduction in the number of features, all models have been
    optimized in terms of computing resources. Regarding the classification time,
    KNN shows a significant reduction after using PCA and PCC, while GBM exhibits
    a marginal reduction. In contrast, RF and MLP have an increase in classification
    time when deploying PCA. In terms of CPU and memory usage, all four models demonstrated
    a marked decrease in resource consumption by deploying PCC and PCA, with GBM and
    KNN showing the most pronounced reductions. Concerning the model size, the GBM
    model shows relative stability, while MLP and RF show a slight reduction. The
    size of the KNN model significantly increases when the number of features is 15.
    The reason is that the KNN employed the brute force approach, calculating the
    distance between each test point and every training sample, significantly increasing
    CPU utilization. However, memory usage and model size are dependent on the data
    size. In dimensions lower than 15, KNN adopts the KD-Tree algorithm, which splits
    the data space. This approach can reduce CPU usage, but it requires additional
    storage to store the tree structures. Consequently, there is a substantial decrease
    in CPU usage, and the model size and memory usage increase. Overall, PCC shows
    better performance in terms of model lightweight compared to PCA, particularly
    in the case of the RF model. Considering four criteria, it is observed that MLP
    is more suitable for lightweight deployment. SECTION V. Conclusion This article
    analyzes the TON-IoT dataset and performs feature engineering to reduce feature
    dimensions and improve model efficiency. Feature engineering reduces redundant
    and erroneous information in the dataset, and improves the efficiency of the models.
    To verify the contributions of features engineering on deploying lightweight IDS,
    we selected four common machine learning IDS models (RF, GBM, KNN, MLP) to detect
    IoT attacks in different feature spaces. The simulations are conducted separately
    for binary classification and multiclass classification. Considering two aspects
    of evaluation: model performance and resource consumption. The result shows that
    with the reduction in the number of features, the performance of RF, KNN, and
    GBM does not decrease significantly, and both can maintain an accuracy of over
    98% in multiclass and binary classification. For the resource consumption, there
    is a decrease in each model by the feature reduction, especially in the RF model
    and KNN model. These results illustrate that appropriate feature engineering can
    improve the performance and resource consumption of the IDS models. Because of
    the limited number of MITM samples, the highest accuracy can only be maintained
    70%. Our future work should generate balanced sample data through feature engineering,
    or consider enriching the dataset by federated learning. Fig. 1. The classification
    time of different models. Show All Fig. 2. The memory usage of different models.
    Show All Fig. 3. The CPU usage of different models. Show All Fig. 4. The model
    size of different models. Show All Authors Figures References Keywords Metrics
    More Like This Feature Selection of Input Variables for Diagnosis of Patellofemoral
    Pain Syndrome based on Random Forest and Multilayer Perceptron 2020 Cross Strait
    Radio Science & Wireless Technology Conference (CSRSWTC) Published: 2020 Precise
    Sweetness Grading of Mangoes (Mangifera indica L.) Based on Random Forest Technique
    With Low-Cost Multispectral Sensors IEEE Access Published: 2020 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2024 14th Global Information Infrastructure and Networking Symposium, GIIS
    2024
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Enhancing IoT Security Through Resource-Efficient Feature Engineering
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ren C.
  - Lyu G.
  - Wang X.
  - Huang Y.
  - Li W.
  - Sun L.
  citation_count: '0'
  description: The evolution of Artificial Intelligence of Things (AIoT) pushes connectivity
    from human-to-things and things-to-things, to AI-to-things, has resulted in more
    complex physical networks and logical associations. This has driven the demand
    for Internet of Things (IoT) devices with powerful edge data processing capabilities,
    leading to exponential growth in device quantity and data generation. However,
    conventional data preprocessing methods, such as data compression and encoding,
    often require edge devices to allocate computational resources for decoding. Additionally,
    some lossy compression methods, like JPEG, may result in the loss of important
    information, which has negative impact on the AI training. To address these challenges,
    this paper proposes a two-step attribute reduction approach, targeting devices
    and dimensions, to reduce the massive amount of data in the AIoT network while
    avoiding unnecessary utilization of edge device resources for decoding. The device-oriented
    and dimension-oriented attribute reductions identify important devices and dimensions,
    respectively, to mitigate the multimodal interference caused by the large-scale
    devices in the AIoT network and the curse of dimensionality associated with high-dimensional
    AIoT data. Numerical results and analysis show that this approach effectively
    eliminates redundant devices and numerous dimensions in the AIoT network while
    maintaining the basic data correlation.
  doi: 10.1049/cmu2.12747
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register IET HUB HOME JOURNALS IET PRIZE PROGRAMME SUBJECTS Visit IET IET Communications
    ORIGINAL RESEARCH Open Access Two-step attribute reduction for AIoT networks Chao
    Ren,  Gaoxin Lyu,  Xianmei Wang,  Yao Huang,  Wei Li,  Lei Sun First published:
    05 March 2024 https://doi.org/10.1049/cmu2.12747 SECTIONS PDF TOOLS SHARE Abstract
    The evolution of Artificial Intelligence of Things (AIoT) pushes connectivity
    from human-to-things and things-to-things, to AI-to-things, has resulted in more
    complex physical networks and logical associations. This has driven the demand
    for Internet of Things (IoT) devices with powerful edge data processing capabilities,
    leading to exponential growth in device quantity and data generation. However,
    conventional data preprocessing methods, such as data compression and encoding,
    often require edge devices to allocate computational resources for decoding. Additionally,
    some lossy compression methods, like JPEG, may result in the loss of important
    information, which has negative impact on the AI training. To address these challenges,
    this paper proposes a two-step attribute reduction approach, targeting devices
    and dimensions, to reduce the massive amount of data in the AIoT network while
    avoiding unnecessary utilization of edge device resources for decoding. The device-oriented
    and dimension-oriented attribute reductions identify important devices and dimensions,
    respectively, to mitigate the multimodal interference caused by the large-scale
    devices in the AIoT network and the curse of dimensionality associated with high-dimensional
    AIoT data. Numerical results and analysis show that this approach effectively
    eliminates redundant devices and numerous dimensions in the AIoT network while
    maintaining the basic data correlation. 1 INTRODUCTION Artificial intelligence
    of things (AIoT) is a combination of artificial intelligence (AI) and Internet
    of Things (IoT). By deeply analyzing big data, AI enables devices to imitate human
    thinking patterns and make intelligent decisions. With the improvement in networking
    and computing technologies, IoT and AI have been used to build intelligent ecosystem,
    i.e. AIoT, which helps users to draw conclusions from the vast amount of data,
    to quickly respond to many situations, and to improve the quality of services.
    AIoT may improve the overall performance of the system for the realization of
    smart life with advanced metering infrastructure (AMI), smart grid, healthcare,
    and Internet of Vehicles (IoV). In the AIoT environment, IoT devices generate
    and collect various kinds of data from surroundings. These data provide the opportunities
    for training AI model, but IoT devices have low memory and power. As the attention
    about AIoT grows, the amount of data generated by IoT devices increases. Since
    IoT devices have low capabilities, it is difficult for IoT devices to store and
    analyze a huge amount of data. Cloud servers has been introduced to handle the
    overgrowing data. However, cloud server centrally processes the enormous volume
    of data, and it leads to the problems of response delay and security [1]. Edge
    services is able to shift data processing requirements from central servers to
    edge network [2]. This evolution improves the real-time performance of AIoT networks
    [3], and transforms communication networks from human-to-things and things-to-things
    to AI-to-things, encouraging edge devices to have powerful nearby data processing
    capabilities. As an extension of AI at the IoT network edge, the combination of
    edge computing and AIoT has multiple benefits [4]. For instance, integrating AI
    into edge networks enables devices to process data more intelligently, reducing
    dependence on central servers and lowering data transmission requirements and
    costs [5]. Moreover, edge computing provides faster data processing speeds and
    lower latency, which is essential for AIoT applications like autonomous driving
    vehicles, smart factories, or telemedicine that require real-time data processing
    and decision-making. Devices such as sensors and cameras collect data at a high
    frequency and include various types like temperature, humidity, sound and images.
    Processing these large volumes of data requires powerful computational capabilities.
    Typically, AIoT devices such as smart home and smart city have relatively weak
    computing power due to weight and power limitations, making them unable to meet
    high-speed, high real-time requirements. This puts tremendous pressure on edge
    computing devices and AIoT resources. To address this challenge, external upgrades
    and internal optimization can be applied. External upgrades, such as using more
    powerful processors or adding dedicated AI hardware accelerators, can improve
    the computing power of edge devices [6]. However, this approach may significantly
    increase the cost of AIoT applications, as AIoT networks often deploy devices
    on a large scale. Alternatively, computational tasks can be optimized by reducing
    the amount of data through internal optimization, such as pre-processing [7].
    By reducing the data volume, the number of data points that need to be processed
    decrease, which in turn reduces the size of the dataset used. A smaller dataset
    accelerates significantly reduces storage costs, the training speed of machine
    learning models and reduce the time and space complexity of the data. However,
    conventional data preprocessing methods, such as JPEG data compression, are often
    encounter “information entropy loss” [8], sacrificing necessary information for
    AI training process by decreasing the correlation of data. Efficient compression
    algorithms may require more computing resources, while inefficient ones may not
    effectively reduce data volume. To ensure high-speed and effective edge computing
    while reducing data volume and maintaining data correlation, the method of attribute
    reduction has been applied to lower data dimensions. In the context of a dataset,
    the term “dimensions” refers to the various features in a dataset with each dimension
    embodying a specific type of feature. For instance, consider a dataset containing
    information about policies sold by an insurance company. This dataset may include
    different types of measurements, such as the number of policies sold, the dollar
    amount of each policy, and the coverage amount. These measures can be further
    analyzed based on different dimensions, such as the type of coverage offered,
    the customer associated with each policy, the salesperson responsible for the
    sale, the date of sale, the geographic region where the policy was sold etc. In
    current IoT ecosystem, intelligent devices amass a vast array of data from diverse
    sources, each carrying numerous distinguishable features, which can be regarded
    as dimensions. In this era of highly sophisticated information technology, particularly
    within the AIoT networks, the volume and dimensions of data are expanding rapidly,
    due to the extensive deployment of devices, resulting in a plethora of features
    within the dataset. When a dataset is characterized by an exceedingly high number
    of features (or dimensions), it is commonly termed as high-dimensional data. High-dimensional
    data possesses certain characteristics, the foremost being the “curse of dimensionality”.
    In AIoT networks, data has a significantly high dimensionality, and the extensive
    deployment of devices further contributes to the expansion of state space. This,
    in turn, leads to the challenge known as the “curse of dimensionality” [9]. An
    increase in dimensions leads to a rapid expansion in the data space volume, resulting
    in data sparseness in the high-dimensional space. his sparsity causes most data
    points to be scattered in the boundary regions of the high-dimensional space [10],
    which may have adverse affects on algorithms that rely on data density or distance
    metrics, such as K-nearest neighbours and clustering. This phenomenon is especially
    prominent in high-dimensional spaces. Moreover, as the dimension of the input
    space increases, the number of neurons required and training time also dramatically
    increase [11], thereby exerting a substantial impact on AIoT networks. Figure
    1 reveals an escalation in the average distance increases with increasing dimensions,
    implying an increasing dispersion of data points in high-dimensional space. This
    dispersion inevitably reduces data information density, complicating data utilization,
    and squandering resources in edge computing. Furthermore, redundancy often arises
    during data collection, especially in AIoT networks, where numerous sensors simultaneously
    monitor the same phenomenon, invariably leading to redundant data. FIGURE 1 Open
    in figure viewer PowerPoint Visualization image of curse of dimensionality. Feature
    selection is a prevalent method in handling high-dimensional data, such as the
    data encountered in machine learning technologies [12]. During the data capturing
    and feature (i.e. dimension) selection stage, this method effectively reduces
    the dimensions in the data, thereby alleviating or resolving the so-called curse
    of dimensionality. Attribute reduction, a prevalent feature selection technique,
    can be employed to alleviate or bypass the curse of dimensionality. Unlike conventional
    compression methods that focus on reducing size by compressing or transforming
    data, attribute reduction prioritizes selecting the most important features to
    reduce the data dimensions [13]. Attribute reduction usually eschews complicated
    mathematical transformations of data, thereby maintaining the original data''s
    format and structure and saving encoding and decoding time. In attribute reduction,
    less significant or redundant features are often discarded, inevitably leading
    to some degree of information loss. However, this information loss is often acceptable.
    Consider a research predicting a patient''s diabetes risk. While the dataset includes
    diverse features, such as the patient''s age, gender, weight, height, blood pressure,
    blood glucose level, and other biochemical indicators, not all these features
    are equally significant. Some features like blood glucose levels or weight might
    be more pertinent than age or gender. Similarly, some biochemical indicators may
    contribute minimally towards predicting diabetes risk and can thus be deemed redundant.
    By using attribute reduction, the most relevant features can be selected and the
    less important or redundant ones discarded. Although this method may also result
    in information loss, it is generally acceptable as the most vital and informative
    features are retained. Similarly, in unmanned aerial vehicle (UAV) networks, the
    signals received by UAVs can be utilized for energy supply, communication, and
    sensing. However, these multimodal functionalities may be interested in different
    components of the received signals [14]. In this scenarios, attribute reduction
    can be employed to eliminate the dimensions that are irrelevant to classification
    of the interested modal functionality. Consequently, we are able to reduce the
    data dimensionality, improve computational efficiency, and still achieve accurate
    predictions. Moreover, attribute reduction facilitates the identification of the
    most critical sensors for a given phenomenon, eliminating device-level redundancy
    and further decreasing the data volume. 1.1 Related works Anticipated future wireless
    networks need to handle critical tasks with enhanced data rates, reduced costs,
    and diminished communication latency, while ensuring confidentiality, integrity,
    and availability (CIA) for information security [10]. Edge computing has the potential
    to significantly reduce latency, minimize the negative effects of data movement,
    and protect local data privacy [15]. However, the prodigious volume of heterogeneous
    data generated by AIoT, coupled with the intense computational demand for AI training,
    poses significant challenges that necessitate attention. To alleviate the pressure
    on AIoT devices, refs. [16-18] have proposed joint designs of IoT/AIoT and edge
    networks to enhance data processing capabilities, computational efficiency, information
    security, and reduce latency. Ref. [16] combined ultra-reliable low-latency communication
    (URLLC) with AI to augment network performance, including computational efficiency,
    scalability, and robustness. However, they focus on high data rates and reliability,
    while it is also necessary to utilize the abundant bandwidth of the terahertz
    frequency band [16]. Ref. [17] pointed out that as IoT networks continue to evolve
    and the number of devices skyrockets, the existing wireless technologies might
    collapse due to massive devices access. To facilitate the large-scale integration
    of IoT devices, ref. [17] conceptualized frameworks for centralized and distributed
    IoT networks, although some highly scalable networks in AIoT require further enhancement
    in their applicability. A novel finite memory multi-state sequence learning framework
    was proposed to reduce latency by reallocating communication resources for critical
    messages during data scarcity [18]. Minimizing delay in multi-task federated learning
    was prioritized in multi-access edge computing (MEC) networks, which reduced the
    time for task execution, and employed matching with incomplete preference lists
    (UCPL) to manage the colossal data volume produced by IoT devices [19, 20] However,
    to further strengthen data processing capability and computational efficiency,
    the current bottlenecks in AIoT networks, it is urgent to reduce the data volume
    in edge computing. Therefore, AIoT networks incorporated pre-processing methods
    such as data compression, encoding, and feature selection to reduce the data volume
    [21-24]. However, conventional data compression methods, such as the lossy compression
    (e.g. JPEG) can induce artefacts and deteriorate image quality whereas lossless
    compression offer good quality but modest compression [21]. In AIoT networks,
    there is a necessity to investigate high-performance, low-complexity, lossless
    compression solutions that can adjust to larger data volumes and higher data association
    demands. For instance, a lossless time series data compression scheme was proposed
    using a bit-reduction asymmetric numeral system (BB-ANS) for extensive smart metering
    environments, but the decoding process still demands computational resources in
    AIoT [22]. The energy-adaptive data compression method was introduced, which selects
    the encoding method based on the energy status of nodes in wireless sensor networks
    to cater to the real-time data needs of AIoT networks, yet it has not eradicated
    the requirement for decoding [23]. Ref. [24] proposed an algorithm employing discriminative
    matrices to compute attribute reductions, avoiding the decoding process. This
    work well highlights attribute reduction as one of the key research directions
    in big data processing, but has not yet offered specific technical methods for
    AIoT data [24]. In response to these concerns, refs. [25, 26] have employed attribute
    reduction to preprocess data. A power big data attribute reduction method was
    proposed based on rough sets that addresses the high reduction rate issue in conventional
    power big data attribute reduction methods, highlighting the superiority of attribute
    reduction in handling power big data challenges [25]. Ref. [26] proposed an incremental
    support vector machine (SVM) training method that discards redundant data to enhance
    data quality. It''s clear that attribute reduction is an effective strategy to
    reduce the data volume in AIoT networks. attribute reduction is an effective method.
    In this paper, we elucidate the significance of data through attribute reduction,
    eliminating redundant data in the IoT network while preserving data association,
    thereby alleviating the pressure on edge computing and AI training. 1.2 Contributions
    This paper introduces a novel approach to reduce the data processing dimensions
    of AIoT devices by applying attribute reduction techniques. The optimized selection
    of data dimensions and devices results in a reduction in both the dimensions and
    volume of data, while ensuring data relevance. This approach effectively alleviates
    or avoids the “curse of dimensionality” issue faced by AIoT networks when handling
    high-dimensional data, thereby supporting the implementation of data density or
    distance algorithms in future AI models. In contrast to conventional AIoT data
    processing methods that encode the data itself, this paper focuses on processing
    higher-level dimensions and devices, which reduces the data volume without the
    need for additional computational resources to decode or restore data (Figure
    2). FIGURE 2 Open in figure viewer PowerPoint An Example of simplified temperature
    control system working principle diagram. 2 SYSTEM MODEL To perform attribute
    reduction in AIoT data, we compare key differences between AIoT and ordinary Internet
    data. Firstly, AIoT data is fragmented in the time and space domain, due to its
    generation by various devices at different times and places. Figure 3 reveals
    an escalation in the heterogeneity and dispersion of AIoT devices in space. Since
    AIoT data is usually generated by various different devices at different places,
    the generation and collection of data are discontinuous and scattered. This is
    in contrast to the centralized and specific generation of ordinary Internet data.
    Additionally, AIoT data is highly heterogeneous and complex due to the diverse
    types of data from AIoT devices. The set of AIoT devices is represented as , where
    each device in generates a data point . Then, the entire device set can be represented
    as (1) where represents the number of devices, and assume each data point is a
    -dimensional vector, i.e. (2) where represents the th feature of device with ID
    . Secondly, AIoT poses challenges in terms of big data and strong computational
    requirements of AIoT. The need of large amounts of data in AI training, especially
    for complex algorithms like deep learning, is addressed by the collection of a
    significant amount of data by IoT sensors during operation. Real-time analysis
    of this data by AI system to detect the surrounding environment and make decisions
    leads to the requirement of “big-data”. The “strong computation” demand is evident
    in the extensive computational resources needed for training and executing AI
    algorithms. Training a deep learning model may require weeks or even months, necessitating
    advanced computing systems equipped with high-performance GPUs. If we describe
    the data processing of an AIoT system using a function , one can observe that
    as the amount of data increase while the number of data points remains constant,
    there is often a corresponding significant increase in the data dimension for
    function . FIGURE 3 Open in figure viewer PowerPoint Distribution of AIoT devices
    by type and location. Therefore, to proficiently manage and process the massive,
    fragmented and heterogeneous data from AIoT device via attribute reduction, and
    to fulfil the rigorous requirements of big data and computational power inherent
    to AIoT edge networks, while reducing the volume of AIoT data, two primary strategies
    may be contemplated: 1) device-oriented techniques incorporating attribute reduction
    can be deployed to activate only a required subset of sensors for data reception,
    thereby diminishing both the device load and the quantity (1); 2) data-oriented
    techniques can be applied whereby attribute reduction serves to reduce data volume
    by lowering the dimension, i.e. reducing the data volume by decreasing (2). In
    the field of AIoT data processing, attribute reduction aspires is to select a
    subset from the high-dimensional dataset, hence decreasing both the quantity of
    devices and the dimension of each data point. We introduce a function : , where
    is a -dimensional vector with (that is ). This function represents a process of
    attribute reduction. To accomplish device reduction, a selection function is defined,
    where is a subset of the device set , and . The expression “ ” denotes the number
    of devices in a set “ ”. This function enables the selection of data-collecting
    devices based on actual requirements, consequently reducing the quantity of devices.
    Therefore, the comprehensive AIoT data reduction process can be represented by
    (3) where is a set of dimensions for each AIoT device, ( , see Equation (3)),
    symbolizes the processed data set, and and represent two corresponding functions,
    respectively. 3 SOLUTION OF AIOT PROBLEM In this section, a novel method that
    initially employs function to process the data in AIoT network. The proposed approach
    strategically reduces the number of receiving devices, consequently decreasing
    the total data volume. For example, in an industrial production environment, we
    applied this method to select crucial production equipment. By reducing the overall
    volume of monitoring data, we improve the efficiency of the production line, leading
    to a reduction in energy consumption and computational resource wastage. This
    minimization operation effectively saving communication bandwidth and computational
    resources of edge devices. We utilize filter methods, a class of are selection
    techniques that operate independently from any predictive models. These methods
    assign scores to devices based on individual feature characteristics, including
    variance, correlation coefficient etc. The device selection is then conducted
    based on these scores. Specifically, we adopt the Pearson correlation coefficient
    as the scoring metric for device-oriented selection. Subsequent to the selection
    process, function is employed to further reduce the dimensions of the data accrued
    by the devices. This dimension reduction is crucial to alleviate or circumvent
    the curse of dimensionality, thereby facilitating the establishment of machine
    learning and AI models within the AIoT network [12]. Moreover, the dimension reduction
    further decreases the data volume, thus saving the computational resources of
    edge devices. To better illustrate this, we consider healthcare as an example
    and use the method to select patient monitoring devices. Based on various features
    such as the variance and correlation coefficients of vital signs, we assign scores
    to devices and make selections. This process reduces the dimensions and complexity
    of healthcare data, thereby enhancing the efficiency of healthcare resource utilization.
    3.1 Device-oriented (function ) Initially, a score is calculated for each sensor,
    where denotes the mean, signifies the variance, represents the covariance, and
    stands for the Pearson correlation coefficient. According to [27], we express
    the formula for the Pearson correlation coefficient as follows: . Then, we have:
    (4) (5) (6) (7) (8) (9) Next, select sensors through attribute reduction: a threshold
    is established, and sensors with a score exceeding the threshold are selected.
    Denote the set of sensors selected in this manner as . (10) where is the score
    for sensor , and is the predefined threshold. Following that, we have (11) Equation
    (11) selects devices with a correlation exceeding the predefined threshold . Conversely,
    devices with a correlation below are deemed redundant and are thus eliminated.
    This selective process significantly shrinks the data volume while simultaneously
    focusing on the components of interest for the target function, thereby preventing
    multimodal interference [14] within the IoT network. 3.2 Data-oriented (function
    ) Execute function on each , which comprises the following steps: Step 1: Redundant
    attributes removal 1) Initialization Let be the dataset of , containing objects,
    each with attributes. Let be the attribute set, comprising attributes. Let be
    the category set, encompassing all possible categories. (12) 2) Positive region
    calculation As in ref. [28], we obtain the diagram illustrating the process of
    positive region formation, and depict it in Figure 4. For each feature , the positive
    domain when this attribute is included and the positive domain ( refers to the
    set with the element a removed) when it is excluded must be calculated. The positive
    region is defined as the set of all objects in the decision attribute that are
    entirely determined by the attribute set . The term “entirely determined” implies
    that for any two objects and , if they share the same attribute , they also share
    the same attribute . As shown in Equation (13), if and have the same value on
    , they have the same value on : (13) where set is implicitly used as the condition
    of the equal values of and on . It follows that if they belong to the same category,
    they satisfy this condition. As shown in Equation (14), if objects and exhibit
    identical values on , they also share identical values on : (14) 3) Positive region
    comparison We proceed to determine whether the positive domains remain identical
    with and without the inclusion of this attribute. If they are identical, the attribute
    is deemed redundant; if they differ, the attribute is considered significant.
    As shown in Equation (15) When is redundant attribute, it satisfies (15) As shown
    in Equation (16) When a is an important attribute, it satisfies (16) FIGURE 4
    Open in figure viewer PowerPoint Diagram of positive region formation process.
    Step 2: Feature selection 1) Variable precision  setting. For each attribute ,
    a variable precision is established, representing the tolerance rate. Larger the
    tolerance rate results in a larger positive domain, yielding less accurate upper/lower
    approximations. Conversely, a smaller the tolerance rate results in a smaller
    the positive domain, with more accurate the upper/lower approximation. (17) where
    is the tolerance rate mentioned above. 2) Positive region calculation under variable
    precision. After setting the variable precision , the positive domain is recalculated
    with and without attribute included, taking into account data uncertainty and
    noise. As demonstrated in Equation (18), if values of and on are identical within
    the tolerance of , then their values on are also identical: (18) As indicated
    in Equation (19), if values of objects and on are identical within the tolerance
    of , then their values on are also identical: (19) 3) Positive region comparison
    under variable precision. We determine if the positive domains are identical when
    the attribute is included or excluded. If they are identical, this attribute is
    redundant; if they are different, this attribute is significant. At this time,
    is redundant if (20) At this time, is significant if (21) In this way, by eliminating
    redundant data in the AIoT network, we can avoid the interference from unnecessary
    data, when constructing AI models. 4) Importance of attributes setting The average
    rate of change in the positive domain is calculated under different values, with
    the maximum rate of change defining the feature''s importance: (22) where represents
    the maximum difference in the value of considering different fault tolerances
    , including and excluding . To simplify Equation (22), we normalize the obtained
    (22) to the maximum and minimum, constraining its range within [0,1]. The normalized
    function is represented by : (23) 5) Attribute selection threshold setting. A
    threshold is set, and only attributes with an are selected. Let the dimension
    set be , we have (24) That is, (25) Equation (25) eliminates redundant dimensions
    through attribute reduction. It then defines the importance of dimensions ( )
    through a fault tolerance rate and selects dimensions with an importance that
    exceeds a threshold . This method preserves the correlation of AIoT data while
    reducing the dimensions of IoT network data, thereby alleviating the “curse of
    dimensionality”, and enabling the construction of AI models. Additionally, this
    approach reduces data volume without encoding the data, resulting in savings in
    channel resources during communication transmission and resources for machine
    learning computation and data decoding on edge devices. In conclusion, (26) 3.3
    Flowchart and pseudocode Figure 5 and Algorithms 1–2 present the general process
    and core algorithms of the reduction procedure. To prevent multimodal interference
    [14] in the IoT network, we score and select the devices. Then, to avoid the curse
    of dimensionality, we carry out redundant data removal and important dimension
    selection for each device. FIGURE 5 Open in figure viewer PowerPoint Data processing
    flowchart. ALGORITHM 1. Device selection 1: for in do 2: 3: 4: end for 5: for
    in do 6: if then 7: Selected_devices.add 8: end if 9: end for ALGORITHM 2. Attribute
    selection repeat 2: for each in do if then 4: end if 6: end for until no longer
    changes 8: for in do 10: for each in do Update Importance ( , importance, threshold,
    selected_attribute) ▹ if regions differ 12: end for end for 14: Normalize Importance
    (selected_attribute) The pseudocode provides a diagram for the attribute reduction
    process aiming at reducing data complexity. In this process, the “Algorithm 1
    Device selection” procedure represents the processing of the function . Given
    the substantial number of devices in the AIoT network, we reduce the number of
    devices to be processed in this step, eliminating unimportant devices and preventing
    multimodal interference in the IoT network [14]. The “Algorithm 2 Attribute selection”
    represents the processing of the function . In this step, we reduce data dimensions
    to alleviate the curse of dimensionality, ensuring the successful establishment
    of AI models in the AIoT network using machine learning techniques [12]. Furthermore,
    the “Algorithm 1 Device selection” and “Algorithm 2 Attribute selection” procedures
    significantly reduce the data volume by respectively reducing the number of devices
    and dimensions to be processed. Simultaneously, they maintain the relevance of
    data in the AIoT network. 4 SIMULATION RESULTS The original data is derived from
    the research conducted by Meidan et al. [29]. It underwent preprocessing for the
    purpose of conducting a comparative analysis of anomaly detection. The preprocessing
    steps are as follows: (1) the selection of five devices, namely, Danmini doorbell,
    Ecobee thermostat, Philips baby monitor, provision security camera, and Samsung
    webcam, referred to as Device 1, Device 2, Device 3, Device 4, and Device 5, respectively,
    for ease of narration; (2) the consolidation of malicious traffic from all five
    behaviour types for each botnet; (3) the sampling of malicious requests for each
    combination of device and botnet, constituting of the final dataset. The size
    of the preprocessed dataset is 1.44 GB. Each of these devices is characterized
    by 115 features (i.e. dimensions). We hypothesize that the expected outcome of
    this calculation is our target. 4.1 Device-oriented reduction simulation Pearson''s
    relation is used to measure whether two data set are in a line, which is used
    to measure the distance of linear relationships between variables. For instance,
    it is a measure of linear correlation between variables of national incomes and
    savings deposits, height and weight, high school grades and college entrance examination
    scores. Greater absolute value of the correlation coefficient results in a stronger
    correlation. When the correlation coefficient is close to 1 or −1, their correlation
    is the strongest. When the correlation coefficient is close to 0, their correlation
    is weak [30]. Usually, the Pearson correlation coefficient determines the strength
    of their correlation through the following ranges (Table 1): We select 0.2 as
    the threshold for and thereby identified three device IDs that exhibit strong
    correlation with the target variable. The details of these devices are tabulated
    in Table 2. Table 2 demonstrates the successful selection of devices with greater
    than 0.2 during the processing, including important devices such as Device 1,
    Device 4, and Device 5, which effectively reduces the number of devices. Through
    this approach, we only need to consider the data generated by fewer devices in
    the subsequent steps, leading to a significant reduction in the total data volume.
    According to the reference [14], by reducing the number of devices to be considered,
    we could successfully alleviate the issue of multimodal interference in the IoT
    network while reducing the data volume. TABLE 1. Tables of the correlation coefficients
    and correlative degree. Correlation coefficients Correlative degree 0.8–1.0 Strongest
    0.6–0.8 Strong 0.4–0.6 Moderate 0.2–0.4 Weak 0.0–0.2 Weaker or none TABLE 2. Selected
    devices table. Number Name 1 Device 1 2 Device 4 3 Device 5 4.2 Data-oriented
    reduction simulation For each device, we reduce the attributes for each dimension
    separately and calculate their respective importance, and determine the importance
    of different dimensions for different devices, as shown in Figure 6. Figure 6
    demonstrates that the data obtained from IoT network devices consists of numerous
    dimensions that exhibit weak correlation with the target variable, as evident
    from the low values of . FIGURE 6 Open in figure viewer PowerPoint importance
    of different dimensions of devices. By setting the , we are able to eliminate
    redundant dimensions, resulting in a streamlined list of device dimensions, as
    shown in Table 3 ( represents a dimension in the device). TABLE 3. Selected devices.
    Device IDs Retained dimensions Device 1 9 10 12 13 24 25 27 28 Device 4 4 9 10
    12 13 14 24 25 27 28 29 31 64 Device 5 8 10 11 13 23 25 26 28 64 114 Table 3 demonstrates
    that, by selecting dimensions with , the number of dimensions collected from important
    IoT devices has been successfully reduced from 115 dimensions per device to order
    of magnitude dimensions per device. Therefore, the quantity of dimensions to be
    processed has been effectively reduced, thereby decreasing the overall data volume
    encompassed by all dimensions. 4.3 Analysis of data reduction and correlation
    The selection and dimension reduction of devices results in a dataset that is
    only a fraction of the original volume. Furthermore, the dataset''s volume can
    be further minimized by raising the threshold. Figure 7 demonstrates this reduction
    when and threshold are set at 0.2. FIGURE 7 Open in figure viewer PowerPoint Comparison
    of data volume before and after reduction. As shown in Figure 7, the volume of
    processed device data accounts for only of the preprocessed volume, which indicates
    a significant reduction and optimization of data through our approach. The substantial
    decrease in the data volume enhances data sparsity and storage optimization, freeing
    up valuable device storage and computing resources. Despite the significant data
    volume reduction, the correlation of the data remains largely unaffected, as illustrated
    in Figure 8. Figure 8 presents several key observations. Firstly, the shapes of
    the two violin plots are similar, indicating that the overall distribution of
    the data remains relatively stable. Secondly, both violin plots exhibit prominent
    peaks or concentrations, suggesting a common central tendency for the two datasets.
    Thirdly, the heights of the tops and tails in both violin plots are comparable,
    indicating identical maximum and minimum values. These observations imply that
    the correlation of AIoT network data remains largely unaffected, thereby minimizing
    any impact on the subsequent development of AI models. Additionally, the violin
    plot after processing appears narrower compared to the one before. This indicates
    that the post-processed AIoT network data exhibits a higher level of concentration
    and tight distribution, making it more suitable for edge device computations and
    the establishment of AI models. Integrating these observations with Figure 7,
    we successfully reduced the volume of the AIoT network dataset, achieving a more
    compact data distribution while retaining the necessary for prediction. FIGURE
    8 Open in figure viewer PowerPoint Comparison of data correlation before and after
    reduction. In conclusion, the combination of Tables 2 and 3 validates our approach
    of selecting important devices to mitigate the impact of “multimodal interference”
    in the AIoT network [14]. Furthermore, our selection of crucial dimensions helps
    address the “curse of dimensionality” associated with high-dimensional AIoT data
    [12]. The integration of Figures 7 and 8 confirms that despite the significant
    reduction in the volume of AIoT network data and the demand for communication
    channels, the relevant information necessary for edge computing and AI model establishment
    for prediction purposes is retained. Hence, our proposed data processing strategy
    is proven effective. It not only address the issue of “multimodal interference”
    resulting from multiple devices in the AIoT but also mitigates the “curse of dimensionality”
    associated with high-dimensional AIoT data. Additionally, the strategy reduces
    the AIoT network data volume, alleviating the computing and communication channel
    resources burden on edge devices. 5 CONCLUSIONS This paper focuses on the challenge
    of generating a substantial volume of data in AIoT networks, which arises from
    the widespread deployment of devices and the utilization of high-dimensional data.
    To alleviate the pressure on edge computing and communication channel resources,
    a novel approach is proposed. This approach involves device-oriented and dimension-oriented
    attribute reduction techniques, which not only reduce the workload on edge computing
    resources but also offer potential solutions to mitigate multimodal interference
    caused by the extensive deployment of devices in AIoT networks, as well as the
    curse of dimensionality associated with high-dimensional data in AIoT networks.
    AUTHOR CONTRIBUTIONS Chao Ren provided the conceptualization and methodology design
    of the whole paper. Gaoxin Lyu performed the data analysis and implementation
    of the computer code and supporting algorithms. Xianmei Wang and Yao Huang performed
    the formal analysis. Wei Li and Lei Sun performed the validation. Chao Ren and
    Gaoxin Lyu wrote the manuscript. ACKNOWLEDGEMENTS This work is supported in part
    by the National Natural Science Foundation of China (Grant No. 62201034), and
    in part by the Beijing Municipal Natural Science Foundation (Grant No. L212004-03).
    CONFLICT OF INTEREST STATEMENT The authors declare no conflicts of interest. Open
    Research REFERENCES Early View Online Version of Record before inclusion in an
    issue Figures References Related Information Recommended Efficient coding schemes
    for low‐rate wireless personal area networks Hua Qian,  Shengchen Dai,  Kai Kang,  Xudong
    Wang IET Communications Network coding schemes with efficient LDPC coded MIMO–NOMA
    in two‐way relay networks Ngu War Hlaing,  Ali Farzamnia,  Muralindran Mariappan,  Manas
    Kumar Haldar IET Communications Design of segmented CRC‐aided spinal codes for
    IoT applications Hongxiu Bian,  Rongke Liu,  Aryan Kaushik,  Yingmeng Hu,  John
    S. Thompson IET Communications An improved seeds scheme in K‐means clustering
    algorithm for the UAVs control system application Qian Bi,  Huadong Sun,  Cheng
    Qian,  Ke Zhang IET Communications Improved reputation evaluation for reliable
    federated learning on blockchain Jiacheng Sui,  Yi Li,  Hai Huang IET Communications
    Download PDF ABOUT THE IET IET PRIVACY STATEMENT CONTACT IET Copyright (2024)
    The Institution of Engineering and Technology. The Institution of Engineering
    and Technology is registered as a Charity in England & Wales (no 211014) and Scotland
    (no SC038698) Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms
    of Use About Cookies Manage Cookies Accessibility Wiley Research DE&I Statement
    and Publishing Policies HELP & SUPPORT Contact Us Training and Support DMCA &
    Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers & Corporate Partners
    CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright © 1999-2024 John
    Wiley & Sons, Inc or related companies. All rights reserved, including rights
    for text and data mining and training of artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: IET Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Two-step attribute reduction for AIoT networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Priyadarshi R.
  citation_count: '2'
  description: An industry-wide paradigm change has been sparked by the growth of
    Internet of Things (IoT)-based Wireless Sensor Networks (WSNs), which has made
    reliable and effective routing methods necessary. This thorough analysis looks
    at how Machine Learning (ML) techniques may be used to solve the problems that
    come with WSN routing. A summary of standard routing algorithms and an examination
    of their shortcomings comprise the first portion of the paper. The integration
    of ML approaches, such as reinforcement learning and supervised and unsupervised
    learning, is then explored in order to improve WSN routing efficiency. The article
    examines the difficulties and factors related to ML-based routing, including data
    quality, energy efficiency, scalability, and security. Applications and case studies
    show how ML is really used in WSN routing, offering insights into effective tactics
    and lessons discovered. Evaluation metrics and performance assessments are included
    in a separate section that uses simulation and experimental data to compare ML-based
    and conventional techniques. Looking forward, the study describes new breakthroughs
    in ML for WSNs and points out unresolved issues, providing a guide for future
    research paths. The important results and their consequences are outlined in the
    conclusion, which also highlights how ML has the potential to revolutionize WSN
    routing in the future.
  doi: 10.1007/s11276-024-03697-2
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Wireless Networks Article Exploring
    machine learning solutions for overcoming challenges in IoT-based wireless sensor
    network routing: a comprehensive review Original Paper Published: 29 February
    2024 (2024) Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Wireless Networks Aims and scope Submit manuscript Rahul Priyadarshi   93 Accesses
    2 Citations Explore all metrics Abstract An industry-wide paradigm change has
    been sparked by the growth of Internet of Things (IoT)-based Wireless Sensor Networks
    (WSNs), which has made reliable and effective routing methods necessary. This
    thorough analysis looks at how Machine Learning (ML) techniques may be used to
    solve the problems that come with WSN routing. A summary of standard routing algorithms
    and an examination of their shortcomings comprise the first portion of the paper.
    The integration of ML approaches, such as reinforcement learning and supervised
    and unsupervised learning, is then explored in order to improve WSN routing efficiency.
    The article examines the difficulties and factors related to ML-based routing,
    including data quality, energy efficiency, scalability, and security. Applications
    and case studies show how ML is really used in WSN routing, offering insights
    into effective tactics and lessons discovered. Evaluation metrics and performance
    assessments are included in a separate section that uses simulation and experimental
    data to compare ML-based and conventional techniques. Looking forward, the study
    describes new breakthroughs in ML for WSNs and points out unresolved issues, providing
    a guide for future research paths. The important results and their consequences
    are outlined in the conclusion, which also highlights how ML has the potential
    to revolutionize WSN routing in the future. Similar content being viewed by others
    Optimizing IoT-enabled WSN routing strategies using whale optimization-driven
    multi-criterion correlation approach employs the reinforcement learning agent
    Article 30 January 2024 Wireless Sensor Network Routing Protocols Using Machine
    Learning Chapter © 2021 Machine Learning Based Low Redundancy Prediction Model
    for IoT-Enabled Wireless Sensor Network Article 29 July 2023 1 Introduction The
    widespread incorporation of the Internet of Things (IoT) has not only transformed
    technology but also accelerated the growth of IoT-based wireless sensor networks
    (WSNs). With their small sensor nodes that are outfitted with sensing, processing,
    and communication capabilities, these networks function as the essential building
    blocks of IoT infrastructures. Smart cities, industrial automation, healthcare,
    agriculture, and environmental monitoring are just a few of the industries in
    which they are being used [1, 2]. Sensor nodes serve as the backbone of the IoT
    because of their small size and powerful nature, which makes it easier to seamlessly
    integrate the digital and physical domains. These networks use sensor nodes to
    serve as data custodians, gathering and relaying data from the surrounding environment.
    A multitude of applications are made possible by the real-time data collecting
    and dissemination made possible by this combination of sensing, processing, and
    transmission capabilities. Real-time data from the physical world can now be continuously
    collected and sent because to the introduction of IoT-based WSNs, which has sparked
    an unparalleled period of connectedness. This revolutionary potential has shown
    to be crucial in improving decision-making processes across a variety of industrial
    sectors, automating intricate operations, and collecting insightful knowledge
    [3]. The applications of these networks demonstrate how versatile they are; they
    are used for anything from patient health monitoring in hospital settings to smart
    city infrastructure optimization. As these networks spread, they build a data-driven,
    adaptable ecosystem that can handle today’s problems. Considering that IoT-based
    WSNs sometimes consist of dozens or even millions of networked sensor nodes, their
    scalability is very impressive [4, 5]. Their sheer size emphasizes how important
    they have been in reshaping the technological environment and how much room there
    is for further innovation as long as they are essential to enhancing the capabilities
    of IoT networks. The architecture of an IoT-Based WSNs is shown in Fig. 1, which
    shows the linked parts that allow for smooth data gathering and transfer in a
    variety of applications. Sensor nodes, which are scattered across the physical
    world and have the ability to sense, process, and communicate, are the central
    component of the design. These nodes build a network via wireless communication
    using different protocols. After the sensor nodes gather data, it is sent to a
    gateway or sink that acts as a link to higher-level systems, such as edge or cloud
    computing platforms. These platforms are essential for processing, storing, and
    analyzing the vast amounts of data produced by the WSN. The architecture also
    includes data storage systems and services/applications that use the gathered
    data to make decisions in real time, encouraging the fusion of WSN and IoT technologies
    to build intelligent and responsive ecosystems. Fig. 1 IoT-Based Wireless Sensor
    Networks Architecture Full size image 1.1 Overview of IoT-based wireless sensor
    networks IoT-based WSNs are dynamic, networked ecosystems characterized by sensor
    nodes working together to provide real-time data collection and distribution.
    These networks smoothly merge the digital and physical domains, acting as the
    IoT''s technical backbone. Key attributes that highlight the importance and adaptability
    of IoT-based WSNs are as follows: 1) Interconnected sensor nodes: Sensor nodes,
    which are small devices having sensing, processing, and communication capabilities,
    are the fundamental components of IoT-based WSNs [6]. By working together, these
    nodes create a closely knit network that makes data sharing easier. 2) Dynamic
    network architecture: IoT-based WSN architectures are by their very nature dynamic.
    The network’s sensor nodes adjust to environmental changes to provide a flexible
    and responsive infrastructure [7]. This dynamic quality is necessary to meet the
    changing needs of various applications. 3) Real-time data acquisition: Real-time
    data collection from the surrounding environment is one of the main features of
    IoT-based WSNs. Sensor nodes quickly provide data across the network while continually
    monitoring physical characteristics including temperature, humidity, and motion
    [8]. 4) Dissemination of information: The cooperative characteristics of sensor
    nodes facilitate the easy distribution of collected data. Efficient information
    transmission occurs between nodes, enabling data to reach central repositories
    or predetermined destinations [9]. Applications demanding quick decisions need
    this real-time communication. 5) Applications across diverse sectors: WSNs based
    on the IoT are used in many different industries. These networks support the development
    of smart cities, healthcare, industrial automation, and environmental monitoring,
    among other applications that improve productivity, sustainability, and general
    quality of life [10]. 6) Integral components of the IoT ecosystem: As essential
    elements of the larger IoT ecosystem, the networked sensor nodes are crucial.
    They serve as data gateways, making it possible to incorporate information from
    the real world into digital systems and enhancing the intelligence of IoT applications
    as a whole [11]. 7) Scalability and network expansion: Scalability is a feature
    of IoT-based WSNs that enables the network to grow to meet changing needs. Applications
    may need the installation of more sensor nodes to cover greater geographic regions
    or to improve data granularity, therefore scalability is very important [12].
    8) Energy-efficient operation: Due to the often limited power resources of sensor
    nodes, energy efficiency is a crucial factor in IoT-based WSNs [12]. By using
    techniques like duty cycling, sleep modes, and energy-aware routing protocols,
    these networks maximize energy usage while extending the lifetime of individual
    nodes. 9) Adaptive communication protocols: IoT-based WSNs use communication protocols
    that can adapt to changing environmental circumstances [13]. These protocols have
    to deal with issues including interference, transmission range fluctuations, and
    signal attenuation. Throughout the network, adaptive communication guarantees
    stable and dependable data exchange. 10) Ubiquitous sensing capabilities: IoT-based
    WSNs’ sensor nodes are outfitted with an array of sensing functions that enable
    them to fully monitor and gather data from their environment [14]. This omnipresent
    sensing is essential to meeting the complex needs of applications in many industries.
    11) Data fusion and aggregation: Data fusion and aggregation methods are often
    used by IoT-based WSNs in order to minimize redundancy and preserve bandwidth
    [15]. Prior to transmission, these procedures combine and summaries data from
    many sensor nodes in order to maximize network efficiency and minimize data traffic.
    12) Security and privacy considerations: Ensuring data security during transmission
    in IoT-based WSNs is crucial [16]. Strong security measures, such as encryption
    and authentication procedures, are put in place since these networks handle sensitive
    data in order to protect data integrity and privacy. 1.1.1 Role in edge computing
    IoT-based WSNs are essential to edge computing because they bring decision-making
    and data processing closer to the data generating source [17]. This edge-centric
    strategy decreases the need for lengthy data transfer to centralized cloud servers,
    improves real-time responsiveness, and lowers latency. It is anticipated that
    as technology develops, IoT-based WSNs’ capabilities will grow even further, offering
    creative answers to challenging problems and promoting advancement in a range
    of fields. Realizing the full potential of these networks in the IoT age requires
    an understanding of their dynamic and collaborative character. 1.2 Importance
    of efficient routing in WSNs The performance of the whole system is greatly impacted
    by the data routing efficiency, which is a fundamental component of the complex
    web of IoT-based WSNs. In WSNs, routing is a basic technique that involves creating
    communication channels between sensor nodes to enable smooth data transfer from
    source to destination. The direct influence that effective routing has on a number
    of vital aspects, including as energy usage, latency, and data delivery dependability,
    highlights how important routing is. 1) Energy consumption optimization: An essential
    component of optimizing energy usage in WSNs is efficient routing. A well-designed
    routing system may guarantee that energy is used wisely, increasing the operational
    lives of individual nodes, especially given the often limited energy resources
    of sensor nodes [18]. This is especially important for applications that operate
    in harsh or distant areas where it is difficult to replace or recharge nodes.
    2) Minimization of latency: A crucial factor in WSNs is latency, or the interval
    of time between the creation of data and its reception at the destination. By
    choosing the best communication routes, efficient routing reduces latency and
    guarantees that data arrives at its destination on time [19]. For real-time applications
    like emergency response systems, industrial automation, and healthcare monitoring,
    this is essential. 3) Reliability of data delivery: In WSNs, the timely and correct
    conveyance of data is of utmost importance. By reducing the effects of network
    oscillations and dynamically adjusting to changes in communication circumstances,
    effective routing systems improve dependability [20]. This flexibility is especially
    important in situations when the surroundings are uncertain. 4) Adaptation to
    dynamic conditions: The standard routing techniques face distinct issues due to
    the dynamic and resource-constrained nature of WSNs. The network’s dynamic characteristics,
    such as shifts in communication topologies, variances in node mobility, and changes
    in signal intensity, may be difficult for conventional methods to adjust to [21].
    Effective routing strategies are made to adapt to these difficulties on the fly,
    guaranteeing consistent performance in a range of situations. 5) Scale and complexity
    management: The scalability of classical routing techniques may be threatened
    by the increasing size and complexity of WSNs. By strategically controlling the
    growing number of sensor nodes, streamlining communication channels, and averting
    any bottlenecks, effective routing methods solve these problems [22]. For large-scale
    IoT applications to be deployed successfully, scalability is essential. 6) Intelligent
    routing solutions: The necessity for routing methods extends beyond optimization;
    the routing protocols themselves must be intelligent. To efficiently negotiate
    the complexity of dynamic WSNs, intelligent routing systems make use of ML algorithms
    and adaptive decision-making [23]. Resilience and efficiency are ensured by these
    solutions’ ability to learn from and adapt to the changing network circumstances.
    7) Network lifetime extension: Longer WSN lifespans are mostly attributable to
    effective routing mechanisms. By minimizing unnecessary energy use and distributing
    the communication load evenly across sensor nodes, efficient routing approaches
    promote more balanced resource utilization [24]. This is particularly crucial
    for applications in remote locations where it’s difficult to replace or recharge
    sensor nodes. 8) Quality of service (QoS) improvement: The QoS that apps in the
    WSN get is directly impacted by efficient routing. Routing protocol efficacy affects
    QoS parameters including latency, packet delivery ratio (PDR), and data throughput
    [25]. Improved QoS is crucial for applications that need quick and dependable
    data transmission, including critical infrastructure systems or healthcare monitoring.
    9) Resilience to node failures: WSNs are often used in situations where sensor
    nodes might malfunction or fail as a result of things like hardware issues, the
    environment, or running out of energy [26]. Effective routing techniques reduce
    the effect of individual node outages on the overall network performance by dynamically
    rerouting data pathways in response to node failures. This increases network resilience.
    10) Load balancing: Performance deterioration and congestion may result from certain
    nodes in WSNs experiencing larger communication demands than others as they grow
    in size and complexity [27]. In order to reduce congestion and maximize overall
    system performance, efficient routing protocols use load balancing algorithms
    to disperse data traffic equally throughout the network. 11) Security considerations:
    In order to guarantee the security of data transfer inside WSNs, efficient routing
    is essential. Routing protocols that include communication channels that are both
    secure and authenticated help prevent malicious attacks, unauthorized access,
    and data manipulation [28]. This is very important, particularly for situations
    where data integrity and secrecy are critical. 12) Environmental adaptability:
    In WSNs, efficient routing goes beyond conventional optimization to include environmental
    condition adaptation. Protocols that take into account variables like topography,
    meteorological trends, and sources of interference improve the network’s flexibility
    and guarantee stable operation in a variety of demanding environments [29]. 13)
    Reducing overhead: Overhead in the form of control messages, signaling, or pointless
    data transfer may be introduced by some routing systems. The goal of effective
    routing methods is to reduce this overhead, which results in communication processes
    that are more efficient. This decrease in costs leads to better network performance
    by increasing bandwidth utilization [30]. IoT-based networks can only be fully
    realized when effective routing inside WSNs is a key component. Along with anticipating
    and adjusting to the changing requirements of extensive and intricate WSN deployments,
    it also tackles the present difficulties brought about by dynamic and resource-constrained
    situations. Efficient and intelligent routing methods play an ever more crucial
    role in determining the success of WSNs as they become more and more integrated
    into a wide range of applications. 1.3 Challenges in WSN routing The adoption
    of IoT-based systems requires careful consideration of all the issues that come
    with navigating the complexities of WSNs routing. The following categories apply
    to the issues in WSN routing: 1) Constrained sensor node resources: In WSNs, sensor
    nodes have constrained energy, computing, and memory resources. Effective routing
    methods need to make sensible use of these limited resources in order to guarantee
    a longer network lifespan and consistent data delivery [31]. 2) Uncertain wireless
    communication medium: Signal attenuation, interference, and different transmission
    ranges are some of the uncertainties introduced by the wireless communication
    medium. Such issues affect the overall resilience of the network by making it
    difficult to establish and maintain dependable communication linkages between
    sensor nodes [32]. 3) Scalability issues: WSNs often cover large geographic regions,
    with sensor nodes scattered across various settings [33]. In situations like this,
    traditional routing protocols-which are geared for smaller networks-find it difficult
    to grow effectively. Uneven resource distribution, higher latency, and weakened
    network performance might result from scalability problems. 4) Dynamic environmental
    conditions: WSNs have difficulties with changes in node density, mobility, and
    network architecture because of the dynamic nature of their operating environment
    [34]. Routing methods must be able to adjust quickly in order to provide the best
    possible data delivery due to these environmental parameter variations. 5) Mobility
    challenges: Routing becomes more difficult in situations when sensor nodes are
    mobile or placed in surroundings with mobile components (e.g., automotive networks).
    Requiring adaptive routing techniques that can dynamically adapt to changing network
    topologies is necessary to provide smooth communication pathways during node mobility
    [35]. 6) Security and privacy concerns: Due to the wireless nature of communication,
    WSNs are vulnerable to security risks such data manipulation, unauthorized access,
    and eavesdropping. This is because they often handle sensitive data. To protect
    the confidentiality and integrity of transmitted data, secure routing protocols
    must be put into place [36]. 7) Quality of service (QoS) requirements: WSN applications
    may have strict QoS requirements, particularly in industrial or healthcare contexts.
    Efficient routing protocol design and implementation provide a difficulty because
    of these objectives, which include low latency and high dependability [37]. 8)
    Network partitioning: Network partitioning, or the isolating of some network segments,
    may occur in WSNs as a result of several reasons, including environmental conditions
    or geographical barriers. Routing protocols need to tackle this issue in order
    to guarantee uninterrupted data flow and avoid communication failures [38]. 9)
    Limited bandwidth: Particularly when nodes are communicating wirelessly, WSNs
    often have constrained bandwidth. For routing systems, managing congestion and
    making the most use of available bandwidth is a major problem, especially in applications
    that need large amounts of data [39]. 10) Fault tolerance: Failures of sensor
    nodes may occur for a number of reasons, such as hardware issues or low battery.
    For networks to continue operating and delivering data even when nodes fail, routing
    protocols must have fault tolerance [40]. To address these issues, using ML methods
    provides a viable way to improve WSN routing’s effectiveness and flexibility.
    IoT-based WSNs may perform better overall and be more resilient to changes in
    network circumstances by using ML to intelligently improve routing choices. In
    order to shed light on the accomplishments and suggest directions for future study
    in the field of WSN routing, this article will examine the uses and implications
    of ML in tackling these problems. 1.4 Contributions of paper This research addresses
    and resolves the complex issues associated with the implementation of these networks,
    making a major addition to the area of IoT-based WSNs routing. This study contributes
    to several important areas. First of all, it offers a thorough analysis of the
    difficulties brought about by limited sensor node resources, erratic wireless
    communication channels, scalability problems, changing environmental circumstances,
    and other WSN routing-specific nuances. Second, the study investigates how ML
    approaches may be integrated as a viable way to improve WSN routing’s effectiveness
    and flexibility. Through an assessment of several ML applications, achievements,
    and possible drawbacks in the context of IoT-based WSNs, this study provides insights
    into creative solutions to overcome conventional routing constraints. The goal
    of this study synthesis is to improve knowledge of WSN routing complexity, promoting
    educated debates and directing future efforts toward robust and intelligent routing
    solutions in the context of IoT-based WSNs. 2 Literature review The literature
    analysis contrasts the drawbacks and difficulties of existing techniques with
    a thorough description of WSN routing. It presents the groundbreaking work in
    ML-based routing and the revolutionary integration of ML in WSNs. Applications
    in the real world, security issues, and moral ramifications are examined. Examined
    are comparative studies, multi-objective optimizations, and new developments like
    edge computing integration. The paper highlights how the field is changing, including
    hybrid ML-traditional protocols and standardization initiatives. It provides insightful
    information on the state-of-the-art, obstacles, and potential paths for ML-based
    WSN routing. 2.1 Traditional approaches to WSN routing Traditionally, methods
    and techniques intended to maximize communication pathways among sensor nodes
    have been the focus of WSN routing work. Protocols like Ad-hoc On-Demand Distance
    Vector (AODV) and Low-Energy Adaptive Clustering Hierarchy (LEACH) are noteworthy
    because they each handle certain issues like energy efficiency and adaptation
    to changing network circumstances [41]. 2.2 Limitations and challenges of traditional
    routing Traditional WSN routing systems have shortcomings that have been thoroughly
    studied by authors. Recurring topics include difficulties with energy usage, scalability,
    and adaptation to changing surroundings. In-depth assessments of these problems
    are provided in the works of [42] and [43], emphasizing the need for creative
    solutions. 2.3 Introduction to machine learning in WSNs ML is presented in recent
    research as a potentially effective paradigm to address conventional WSN routing
    issues. Reinforcement learning, supervised and unsupervised learning, neural networks,
    and other ML approaches provide the ability to modify routing choices in response
    to real-time input. Prominent publications by authors like [44] and [45] provide
    valuable perspectives on the use of ML in WSNs. 2.4 Previous work on ML-based
    routing in WSNs Researchers have investigated ML-based routing techniques and
    shown how effective they are in improving WSN performance. Notable contributions
    come from [46], who investigated the use of clustering algorithms in WSNs, and
    [47], who used reinforcement learning for energy-efficient routing. These experiments
    highlight ML’s adaptability and promise in solving certain WSN problems. 2.5 Challenges
    addressed by ML-based routing The goal of ML integration in WSN routing is to
    address some shortcomings in conventional methods. ML-based routing algorithms,
    as mentioned by authors such as [48] and [49], address energy efficiency, flexibility
    to changing surroundings, and scalability. They demonstrate the potential to dynamically
    improve routing choices based on learnt patterns and real-time data. 2.6 Comparative
    analyses and performance metrics The performance of ML-based routing algorithms
    and their conventional equivalents is compared in the literature. Comprehensive
    comparison assessments are carried out by authors like [50] and [51], who assess
    variables like PDR, energy usage, and network lifespan. These studies provide
    insightful information on the advantages and disadvantages of ML-based routing
    techniques. 2.7 Challenges and open issues in ML-based WSN routing ML has potential,
    but there are still problems and unanswered questions. Issues with model interpretability,
    security, and the need for adaptive learning processes are covered by authors
    such as [52] and [53]. These studies provide a sophisticated perspective on the
    real-world difficulties in putting ML-based routing solutions into practice in
    WSNs. 2.8 Future directions and emerging trends Future directions and developing
    trends in ML-based WSN routing are anticipated in recent work. Concepts like federated
    learning, edge computing, and the integration of sophisticated ML models are explored
    by authors like [54]. These prospective viewpoints add to the current conversation
    on the development of WSN routing models. 2.9 Real-world applications and case
    studies Recent research explores real-world applications and case studies that
    demonstrate how ML-based routing is really implemented in WSNs. Authors like [55]
    provide perspectives on uses in a variety of fields, such as industrial automation,
    healthcare, and environmental monitoring. These case studies provide insight into
    the versatility and efficacy of ML-based routing in a range of situations. 2.10
    Robustness and security considerations It is crucial to guarantee the stability
    and security of ML-based routing algorithms. Research by [56] looks on how to
    integrate secure communication protocols into ML-based routing and strengthen
    the resistance of ML models against adversarial assaults. These contributions
    provide insight on how security concerns in WSNs are changing. 2.11 Cross-layer
    optimization strategies Cross-layer optimization solutions that integrate ML across
    various protocol stack levels have been investigated as a means of improving WSN
    performance even further. The works of [57] look at how improvements at the physical
    and data connection levels and ML-based routing choices might work together. The
    potential for comprehensive increases in WSN efficiency is shown by these research.
    2.12 Ethical and environmental implications The emergence of ML-based routing
    raises questions about its ethical and environmental consequences. Writers such
    as [58] address issues with algorithmic bias, data privacy, and the environmental
    impact of ML-based routing. These conversations advance knowledge on the social
    and environmental effects of using ML in WSNs. 2.13 Collaborative and distributed
    learning paradigms New paradigms that are gaining traction in WSNs include dispersed
    and collaborative learning. The possibility of collaborative ML models that include
    information from many sensor nodes is examined in literature by [59]. With large-scale
    WSN deployments, these methods seek to improve the scalability and flexibility
    of ML-based routing. 2.14 Edge computing integration and fog networking The ability
    to move processing closer to the data source has drawn attention to the nexus
    between edge computing and ML-based routing. In order to improve real-time decision-making
    and lessen the need on centralized processing, authors such as [60] investigate
    the integration of ML models at the edge in WSNs. Furthermore, ideas related to
    fog networking are explored, highlighting the decentralized and dispersed character
    of computing in WSNs. 2.15 Multi-objective optimization in ML-based routing An
    attempt has been made to expand ML-based routing to manage many goals at once.
    Research by [61] explores multi-objective optimization techniques that take dependability,
    latency, and energy efficiency into account all at once. These works add to the
    growing body of research on ML-based routing methods that meet various network
    needs. 2.16 Explainability and interpretability in ML models In order for ML models
    to be widely used, it is essential that they be both interpretable and explainable.
    The difficulties in providing clear and understandable ML-based routing choices
    are discussed in works by [62]. This aspect becomes more crucial, especially in
    applications where it’s necessary to comprehend and verify decision-making processes.
    2.17 Hybrid approaches: combining ML with traditional protocols Researchers investigate
    hybrid strategies that bring together the advantages of ML and conventional routing
    protocols. In research presented in papers by [63], ML models are used to supplement
    current procedures in a way that strikes a compromise between ML’s flexibility
    and the stability of conventional methods. The goal of these hybrid models is
    to use each paradigm''s advantages. 2.18 Standardization efforts and open challenges
    The area of standardization in ML-based WSN routing is only beginning to be discussed.
    Authors like [64] draw attention to the need of standardization initiatives in
    order to promote interoperability and smooth integration of ML-based solutions.
    Open difficulties, such as algorithmic heterogeneity and robustness, are simultaneously
    noted and should be investigated further. A thorough overview of the literature
    on WSNs routing is included in Table 1, with an emphasis on both the introduction
    of ML techniques and conventional methods. The review includes the writings of
    several writers, each of whom offers a distinct perspective on the goals, difficulties,
    and constraints of conventional methods as well as suggested ML alternatives.
    Table 1 Literature review on IoT based WSNs routing Full size table 3 Machine
    learning techniques in WSN routing The landscape of WSNs routing has seen a radical
    change with the incorporation of ML methods, bringing in new ways to tackle the
    complex problems that arise in these resource-constrained and dynamic situations.
    This section provides an in-depth analysis of several ML techniques used in WSN
    routing, categorizing them into two primary categories: supervised and unsupervised
    learning. 3.1 Supervised learning approaches Supervised learning techniques have
    become well-known in the field of WSN routing because of their capacity to use
    labelled datasets for model training, which allows the models to make well-informed
    routing choices. The next section explores the specifics of the main supervised
    learning techniques used in WSN routing: 3.1.1 Decision trees In WSNs, decision
    tree algorithms like Random Forests and C-4.5 are often used for routing decisions.
    These algorithms form a structure like a tree, where each leaf node refers to
    a routing option, and each inside node reflects a decision based on a particular
    attribute. Decision trees are useful in the context of WSNs because they are easy
    to read and may record intricate decision-making processes [85, 86]. Decision
    trees can effectively generate routing choices by taking into account variables
    like node energy levels, communication connection quality, and environmental conditions
    by using previous data to establish rules. Decision trees are a good option in
    situations where WSN routing optimization depends on explicit decision rules because
    of their transparency and simplicity. A decision-making process for routing choices
    in WSNs utilizing decision tree algorithms like C-4.5 and Random Forests is shown
    in the flowchart in Fig. 2. The input data is shown first in the flowchart, which
    then moves on to the identification of relevant features and assesses a choice
    using a feature threshold. The rules are changed if the condition is satisfied;
    if not, the criteria for routing options are assessed. Iterations throughout the
    process enable the system to effectively adjust to changing circumstances. The
    decision nodes take into account variables including ambient circumstances, communication
    connection quality, and node energy levels. The flowchart illustrates how decision
    trees are transparent and easy to read, which makes them useful in situations
    where WSN routing optimization depends on clear decision rules. Fig. 2 Flowchart
    for decision trees in WSNs routing Full size image 3.1.2 Support vector machines
    (SVM) In WSN routing, Support Vector Machines (SVM) are used to identify routing
    choices by identifying hyperplanes that efficiently divide various classes in
    the feature space. SVMs have been used to improve routing pathways based on previous
    performance data and are especially helpful when dealing with non-linear decision
    boundaries [87, 88]. SVMs in WSNs can pick up knowledge from labelled datasets
    that provide details about good and bad routing choices. By accurately classifying
    new routing situations, the model finds the ideal hyperplane that optimizes the
    margin between various classes [89]. SVMs in WSN routing are able to make educated
    recommendations regarding the most effective communication channels inside the
    network by taking into account a number of factors, including node proximity,
    signal intensity, and past routing performance. The SVM-based routing choice process
    is shown by the flowchart in Fig. 3 at different phases. The input data is the
    first step in the flowchart, which then shows the SVM being trained, features
    being chosen, the best hyperplane being found, fresh situations being classified,
    routing pathways being assessed, optimized, and updated, and finally the process
    being stopped. The dashed line links the flowchart’s beginning point to the input
    data. For your particular depiction, change the styles, colors, and placements
    as necessary. The SVM-based decision-making procedure in WSN routing is shown
    graphically in the flowchart. Fig. 3 Flowchart of SVM-based routing decision process
    Full size image 3.1.3 Neural networks A type of supervised learning techniques
    known as artificial neural networks (ANNs) is particularly good at deciphering
    intricate correlations between input features and routing selections. ANNs are
    used in WSN routing to extract complex patterns and non-linear relationships from
    the data. Neural networks resemble real neurons in structure, consisting of layers
    of linked nodes [90, 91]. Neural networks are a viable alternative for improving
    WSN routing in dynamic and changing contexts because of their capacity for adaptation
    and generalization. Neural networks may be trained on labelled datasets to predict
    the best routing pathways based on a variety of input characteristics, such as
    node statuses, ambient circumstances, and past routing performance [92]. Because
    of their natural adaptability to changing network circumstances and a wide range
    of routing situations, neural networks are useful instruments for optimizing WSN
    performance. A systematic procedure underpins the construction of the ANN for
    WSN routing shown in Fig. 4. Historical performance measurements and environmental
    conditions are among the WSN routing-related data that are sent to the input layer.
    Through the use of linked nodes with adjustable weights and biases and activation
    functions, hidden layers analyses this input and pick up complex patterns and
    non-linear relationships. Best-fit routing predictions are generated by the output
    layer. In the process of training, the backpropagation method repeatedly modifies
    the network''s parameters to reduce the error between predicted and actual routing
    choices, which is measured by a loss function. The ANN demonstrates remarkable
    flexibility to dynamic WSN settings and versatility in generalizing to new circumstances
    after it has been taught to anticipate optimum routing pathways based on a wide
    range of input variables. Fig. 4 Architecture of ANN for WSN routing Full size
    image In conclusion, there are clear benefits to using supervised learning techniques
    for WSN routing optimization, such as decision trees, SVMs, and neural networks.
    These methods use past data to build models that are capable of intelligent routing
    choices that account for the intricacies of WSNs. As the area develops, the incorporation
    of these supervised learning approaches aids in the creation of intelligent and
    flexible routing strategies for WSNs. 3.2 Unsupervised learning approaches Unsupervised
    learning methods are essential for routing in WSNs since they don’t depend on
    labelled datasets. Rather, by spotting innate structures and patterns in the data,
    these algorithms provide insightful information that may be used to improve routing
    tactics in WSNs. Two major kinds of unsupervised learning techniques that are
    often used in WSN routing are examined in this section: 3.2.1 Clustering algorithms
    WSN routing relies heavily on clustering algorithms like K-means and hierarchical
    clustering, which group sensor nodes according to shared features. Nodes with
    comparable capabilities, behaviour, or environmental characteristics may be clustered
    together using clustering in the absence of explicit labelling [93, 94]. This
    grouping helps the network form clusters, which provide effective data aggregation
    and routing techniques. Depending on the network’s present state, clusters may
    develop dynamically to adjust to node circumstances or changes in the dynamics
    of their surroundings. Through enabling localized communication within clusters,
    these algorithms aid in network performance improvement, especially in situations
    where resource limitations or changing environmental circumstances need adaptive
    and self-organizing routing solutions. 3.2.2 Dimensionality reduction techniques
    Principal Component Analysis (PCA) is a well-known example of how dimensionality
    reduction methods are used in unsupervised learning. These methods are used to
    lessen the difficulties posed by high-dimensional data, which is a typical situation
    in WSNs where a multitude of attributes might affect routing choices [95, 96].
    By eliminating information that is unnecessary or duplicated, dimensionality reduction
    helps to find important qualities that have a substantial impact on routing choices.
    Reducing the dimensionality of data becomes important in the setting of resource-constrained
    sensor nodes, when memory and computing capabilities are restricted. Dimensionality
    reduction approaches maximize resource use and improve WSN performance by identifying
    key characteristics that provide more effective and simplified routing choices.
    3.2.3 Fuzzy logic-based routing Fuzzy logic is noteworthy because of its capacity
    to manage uncertainty and imprecision in WSN routing, even though it does not
    fall within the formal purview of unsupervised learning. Fuzzy logic systems allow
    nodes to make judgments based on fuzzy sets by using linguistic variables and
    rule-based reasoning. Fuzzy logic-based routing enables nodes to manage uncertainty
    and ambiguity in decision-making in WSNs, where environmental circumstances may
    not always be accurately described. Fuzzy logic is a useful tool for improving
    the resilience of WSN routing because of its capacity to adapt to imprecise input,
    particularly in situations where conventional approaches could find it difficult
    to take uncertainties into account [97, 98]. The architecture shown in Fig. 5
    shows the steps that make up a Fuzzy Logic-Based Routing system for WSNs. The
    process starts when sensor data which is environmental information is received.
    This clean sensor data is converted into fuzzy sets during the fuzzification step
    to account for uncertainty and imprecision in the surrounding circumstances. Then,
    depending on the fuzzy sets, the Inference Engine uses rule-based reasoning to
    make judgments, enabling nodes to adjust to unpredictable situations. The final
    routing selection is made easier by the Defuzzification step, which transforms
    the fuzzy outputs into crisp values. Overall, the procedure improves WSN routing
    resilience by managing ambiguity and uncertainty in judgement, especially in situations
    where conventional techniques could find it difficult to take inaccurate data
    into consideration. The design places a strong emphasis on fuzzy logic systems’
    capacity to adapt to the unpredictable and dynamic character of WSN settings,
    which helps to provide routing choices that are more dependable and robust. Fig.
    5 Architecture of Fuzzy Logic-Based Routing Full size image 3.2.4 Self-organizing
    maps (SOM) In WSNs, an artificial neural network type called Self-Organizing Maps
    is used for visualization and grouping. Nodes may self-organize using SOMs to
    create a two-dimensional map where spatial closeness denotes feature similarity.
    SOMs have a role in WSN routing by helping to establish structured networks that
    facilitate the discovery of the best routing pathways [99, 100]. Localized and
    effective communication is made possible by the same qualities of nodes that are
    geographically adjacent to one another on the map. Adaptive learning is made possible
    by SOMs, which enable the network to modify and adapt to shifting node statuses,
    ambient circumstances, and communication needs. The operating sequence of SOMs
    in the context of WSN routing is summarized in the flowchart shown in Fig. 6.
    The initiation point, which denotes the beginning of SOM functioning, starts the
    process. Next, input data is provided into the system, which includes information
    pertinent to WSN routing. The SOM is then initialized, which prepares the system
    for training. In order to maximize the arrangement of nodes in a two-dimensional
    map, the SOM continuously modifies its nodes in response to the input data during
    training. This process is known as convergence. The flowchart includes a decision
    node that, in the event that convergence is reached, branches to either continue
    training or go on to node organization. The taught spatial connections are then
    represented by an output map that is created by the ordered nodes. The stop node
    marks the end of the procedure. This flowchart illustrates the methodical development
    of SOMs in WSN routing and emphasizes their capacity to adapt to the dynamic network
    circumstances and self-organize in response to incoming data. Fig. 6 Flowchart
    for the operation of SOM in WSN routing Full size image 3.2.5 Swarm intelligence-based
    routing Swarm Intelligence-Based Routing is an intriguing model that draws inspiration
    from the group behaviours seen in natural systems. This method improves the flexibility
    and efficiency of WSN routing by using the cooperative concepts seen in swarms
    or colonies. Swarm Intelligence-Based Routing is an attractive solution for routing
    in WSNs due to its various benefits. First of all, because of its innate flexibility,
    it can react quickly to changes in its surroundings and is a good choice in situations
    where variables are unexpected. Swarms’ cooperative decision-making process makes
    it possible to find effective routing routes, which maximizes network performance
    by using nodes’ combined intelligence [101, 102]. Because swarm intelligence-based
    algorithms are decentralised, they are less dependent on a central controller,
    which further improves their scalability and resilience. Swarm intelligence-based
    routing works especially well in dynamic contexts, such those seen in environmental
    monitoring or disaster relief situations. It is a flexible and robust solution
    for WSNs because of its capacity to independently and cooperatively adjust to
    changes, ensuring that the network stays responsive and effective even under demanding
    and quickly changing circumstances. Particle Swarm Optimization (PSO) and Ant
    Colony Optimization (ACO) are two well-known methods in this field. (1) Ant colony
    optimization (ACO) Ant foraging behaviour serves as the inspiration for the ACO
    metaheuristic algorithm. ACO functions in the context of WSN routing by imitating
    how ants find and create the shortest routes between their nests and food sources.
    Pheromone-like information is sent during communication between nodes in the WSN,
    which function as virtual ants [103, 104]. Nodes leave pheromones on the boundaries
    of the communication lines as they investigate and find viable paths. Pheromone
    trails like this are used by other nodes as a measure of route quality, giving
    preference to pathways with larger concentrations of pheromones. As environmental
    conditions and communication topologies change over time, ACO allows the network
    to converge towards optimum routes. Because ACO is decentralised and self-organizing,
    dynamic WSN situations are a good fit for it. (2) Particle swarm optimization
    (PSO) Another swarm intelligence-based optimization technique that takes cues
    from fish or bird social structures is called PSO. Nodes are seen as particles
    in a multidimensional solution space when it comes to WSN routing. Based on its
    individual experiences and the collective knowledge of the swarm, each particle,
    or node, modifies its location in the solution space. PSO, like ACO, enables nodes
    to communicate with one another and exchange details on successful routes [105,
    106]. By repeatedly modifying its locations in the solution space, the swarm as
    a whole moves closer to the best routing options. PSO performs best in situations
    where nodes must modify their routing choices in response to changing environmental
    variables or when the network structure is dynamic. Figure 7‘s PSO design represents
    a methodical, swarm intelligence-inspired approach to WSN routing. The procedure
    comprises initializing nodes as particles in a multidimensional solution space,
    starting at the commencement point. Based on their individual experiences and
    collective knowledge of the swarm, these particles dynamically modify their placements,
    allowing nodes to communicate and exchange information about successful paths.
    The swarm is guided towards optimum routing solutions by means of convergence
    checks, iterative route evaluations, and particle position updates. Because PSO
    is adaptive, it works especially well in situations where the network architecture
    of the WSN is dynamic or when nodes must modify their routing choices in reaction
    to shifting environmental factors. Using particle swarm optimization to improve
    the efficiency and flexibility of WSN routing, the design highlights the decentralised,
    cooperative character of swarm intelligence. Fig. 7 PSO architecture in WSN routing
    Full size image In conclusion, dimensionality reduction strategies and clustering
    algorithms are examples of unsupervised learning methodologies that provide useful
    tools for WSN routing. Through the process of automatically recognizing patterns
    and grouping nodes according to shared characteristics, clustering algorithms
    aid in the development of flexible and self-adjusting network topologies. In order
    to ensure that routing choices are based on the most relevant attributes, dimensionality
    reduction approaches address issues related to high-dimensional data. When combined,
    these unsupervised learning techniques improve the flexibility, robustness, and
    effectiveness of WSN routing in situations when resources are limited and change
    often. 3.3 Reinforcement learning in WSN routing In the field of WSNs routing,
    reinforcement learning emerges as a potent model that makes use of the idea of
    teaching agents to make successive choices via interactions with their surroundings
    [107]. The use of reinforcement learning in WSN routing is examined in this section,
    with particular attention paid to Q-Learning and Deep Q Networks (DQN) as noteworthy
    examples: 3.3.1 Q-learning and DQN Q-Learning is a well-known reinforcement learning
    method that has important uses in routing for WSNs. Within the WSN environment,
    nodes behave as agents that interact with changing network circumstances to determine
    the best routing strategies. Updating a Q-table, each entry of which represents
    the anticipated cumulative reward for a particular action in a given state, is
    the fundamental method [108, 109]. Nodes are able to adjust routing choices dynamically
    via this iterative learning process, optimizing for important parameters like
    latency, energy consumption, and overall network performance. DQN, an advancement
    in Q-Learning, use neural networks to estimate Q-values. This improvement is helpful
    in managing more complicated state-action spaces, which helps to manage the intricacies
    of dynamic WSNs. By fusing the capabilities of neural networks with the strengths
    of Q-Learning, DQN has shown effectiveness in teaching agents to make complex
    routing choices [110, 111]. This combination helps DQN to overcome obstacles caused
    by sensor nodes with limited resources and varying communication topologies. As
    a consequence, WSN routing algorithms become more intelligent and adaptive, making
    them suitable for situations in which traditional approaches may not be able to
    handle changing circumstances. The incorporation of DQN, an improved version of
    Q-Learning, into WSN routing demonstrates a paradigm change in favors of intelligent,
    learning-based techniques [112]. These strategies not only improve routing decision
    flexibility but also open the door to more responsive and effective WSNs, especially
    in situations where standard approaches are unable to handle the inherent complexity
    of dynamic and resource-constrained settings. The sequential operation of Q-Learning
    and DQN in the context of WSNs routing is summarized in the flowchart shown in
    Fig. 8. Starting with the start node, the procedure initializes the algorithm
    and inputs the state data to start Q-Learning. After that, the algorithm updates
    the Q-Table in light of the input and verifies its convergence. The procedure
    is over if Q-Learning has converged. In the event that it is not, the flow switches
    to initializing DQN, obtaining its state data, and updating the neural network.
    Just like Q-Learning, DQN halts the process upon reaching convergence. If not,
    a loop for iterative learning is created by the DQN update continuing. The flowchart
    illustrates the dynamic interaction between DQN and Q-Learning, demonstrating
    both technologies’ capacity for intelligent, learning-based routing choices as
    well as their flexibility in responding to changing WSN circumstances. Fig. 8
    Flowchart for Q-Learning and DQN in the WSN routing Full size image Investigating
    reinforcement learning in WSN routing reveals how flexible and adaptive intelligent
    algorithms can be, particularly when dealing with issues presented by environments
    that are resource-constrained and dynamic. In order to provide readers a better
    understanding of the uses and results of these approaches in real-world WSN settings,
    the following sections will go into further detail about certain research and
    implementations. Reinforcement learning integration is a major step toward autonomous
    and intelligent WSNs that can adjust their routing algorithms in response to changing
    network circumstances and real-time feedback. 4 Challenges and considerations
    in ML-based routing WSNs may undergo revolutionary changes to ML-based routing,
    but there are a number of issues and factors to take into account that must be
    carefully considered. 4.1 Data quality and preprocessing When using ML-based routing,
    data quality assurance is crucial. WSNs often operate in noisy, dynamic settings,
    which causes data fluctuations and uncertainty. The performance of ML models may
    be negatively impacted by inaccurate training data. Strong preparation methods
    are necessary to deal with problems like noise, outliers, and missing or inconsistent
    data. In order to promote accurate and dependable learning and allow ML models
    to generate well-informed routing choices, it is essential that data quality issues
    be addressed [113, 114]. 4.2 Energy efficiency and resource constraints Since
    sensor nodes in WSNs are usually resource-constrained, energy efficiency is still
    a major concern. ML algorithms may use a lot of energy, particularly those that
    call for intricate calculations or frequent model changes. Achieving equilibrium
    between the computing requirements of ML-based routing and the energy-constrained
    sensor node resources is crucial [115, 116]. The use of energy-aware ML models,
    investigating edge computing strategies, and optimizing algorithms for efficiency
    are ways to tackle this issue and increase the operating lifetime of WSNs. 4.3
    Scalability and adaptability As WSNs grow in size and complexity, the scalability
    of ML-based routing systems becomes more crucial. The dynamic nature of large-scale
    networks may provide difficulties for traditional ML techniques, which might result
    in scaling problems [117, 118]. It is important to design routing models with
    dynamic adaptability to changes in network topology, size, and environmental factors.
    By investigating distributed and decentralised ML techniques, scalability may
    be improved, enabling WSNs to effectively manage a variety of situations and changing
    needs. 4.4 Security and privacy concerns New levels of security and privacy problems
    are brought about by the incorporation of ML-based routing. ML models are susceptible
    to adversarial assaults, in which malevolent parties alter input data in order
    to sway routing choices [119, 120]. It is crucial to guarantee the resilience
    of ML models against these kinds of assaults and to include security measures
    into the routing process. Furthermore, as ML models could use sensitive data for
    training, privacy issues come up. To protect sensitive data and preserve the integrity
    of WSNs, privacy-preserving ML algorithms must be put into practice together with
    secure communication protocols. 4.5 Dynamic network conditions In WSNs, ML-based
    routing has to deal with the ever-changing network circumstances. The efficacy
    of trained models may be greatly affected by changes in node density, mobility,
    and environmental variables [121, 122]. ML algorithms developed for static settings
    could find it difficult to instantly adjust to the changing network landscape.
    Maintaining optimum performance in dynamic WSN settings requires routing methods
    to be developed that can adapt dynamically to changes in topology, interference,
    and communication patterns. 4.6 Limited computational capabilities Resource-intensive
    ML methods might be difficult to implement on sensor nodes in WSNs due to their
    limited processing capabilities. It is crucial to strike a compromise between
    the computational limitations of sensor nodes and the complexity of the models
    [123, 124]. To guarantee a feasible deployment and reduce the influence on the
    overall network performance, lightweight and energy-efficient ML techniques that
    are customized for the unique capabilities of WSNs must be created. 4.7 Generalization
    across diverse environments ML models that have been trained in one setting could
    find it difficult to adapt well to another ones. WSNs are used in a variety of
    applications, each with its own set of needs and difficulties. In order to ensure
    that trained models can adapt to different deployment settings without requiring
    significant retraining, ML-based routing systems must exhibit strong generalization
    capabilities [125, 126]. Techniques for domain adaptation and transfer learning
    are useful in attaining successful generalization across various WSN contexts.
    4.8 Real-time constraints Real-time responsiveness is required for certain WSN
    applications, in which routing choices must be determined quickly in response
    to dynamic events or shifting circumstances. ML algorithms may cause delay, particularly
    if they include complex calculations. A crucial factor to take into account is
    striking a balance between the need of making wise decisions and the demand for
    quick responses. One enduring problem in ML integration with WSNs is creating
    ML-based routing solutions that satisfy real-time requirements without sacrificing
    accuracy [127]. 4.9 Overhead of model training and maintenance Training ML models,
    especially deep learning models, for routing in WSNs may be computationally intensive.
    Furthermore, ongoing model updates to adjust to shifting circumstances could result
    in extra cost related to communication and energy use [128, 129]. It’s critical
    to strike a balance between the advantages of continuous learning and the related
    computing expenses. When designing ML-based routing solutions, effective model
    training, optimization, and techniques to reduce the cost of model maintenance
    become crucial factors. The complex issues and factors involved in ML-based routing
    are summarized in Fig. 9. Ensuring the quality and preparation of data, which
    lays the groundwork for reliable ML models, is one major difficulty. Additional
    challenges are posed by resource and energy efficiency limitations, necessitating
    creative solutions to maximize resource efficiency and improve route choices.
    The complex landscape shown in Fig. 9 is a result of many factors, including real-time
    limitations, limited processing power of sensor nodes, and the need for generalization
    across various contexts, and the cost of model training and maintenance. Utilizing
    ML in routing inside WSNs to its maximum potential requires an understanding of
    and commitment to resolving these issues. Overcoming these obstacles calls for
    an integrated strategy that incorporates developments in algorithmic design, model
    optimization, and a thorough understanding of the special qualities of WSNs. Taking
    these factors into account will help create reliable and useful ML-based routing
    solutions for WSNs as the field develops. Fig. 9 Challenges and Considerations
    in ML-Based Routing Full size image 5 Case studies and applications Notable case
    studies and practical implementations of ML in routing inside WSNs have been observed,
    offering insights into success stories and important lessons learnt. 5.1 Precision
    agriculture ML-based routing in WSNs has shown to be crucial in precision agriculture
    for maximizing data gathering from farming areas. Temperature, crop health, and
    soil moisture are tracked via sensor nodes fitted with environmental sensors [130,
    131]. The geographical distribution of sensor nodes and the dynamic characteristics
    of the agricultural environment are used to train ML algorithms, such as decision
    trees and neural networks, to determine routing choices. Precision agricultural
    decision-making requires fast and reliable information, which is ensured by ML-based
    routing, which improves data collecting efficiency. 5.2 Structural health monitoring
    WSNs are used in structural health monitoring applications to keep an eye on the
    state of infrastructure, including buildings, pipelines, and bridges. In order
    to efficiently gather structural data, sensor nodes’ routing pathways are optimized
    via the use of ML-based routing algorithms. In order to adjust routing choices
    in response to structural changes over time, reinforcement learning approaches
    have been used, such as Q-Learning [132, 133]. When used in structural health
    monitoring, ML-based routing improves data gathering reliability, allowing for
    the early identification of any problems and extending the lifespan and overall
    safety of infrastructure. 5.3 Environmental monitoring in wildlife conservation
    When it comes to environmental monitoring for animal protection, WSNs are essential.
    To maximize the deployment of sensor nodes in distant locations and guarantee
    efficient coverage for tracking animal activity, habitat conditions, and possible
    threats, ML-based routing is used. Sensor nodes are grouped based on environmental
    characteristics using clustering techniques like K-means [134, 135]. This example
    highlights how ML-based routing may be flexible in meeting the demands of resource-constrained
    and dynamic contexts in animal conservation initiatives. 5.4 Smart cities and
    traffic management WSNs are used for a number of purposes in smart city contexts,
    one of which being traffic control. ML-based routing uses real-time data from
    sensor nodes to dynamically modify traffic signal timings in order to optimize
    traffic flow. Neural networks and other supervised learning techniques use past
    traffic patterns to anticipate future events and improve routing choices [136,
    137]. In smart cities, ML-based routing helps to improve urban mobility generally,
    reduce traffic congestion, and increase transportation efficiency. 5.5 Healthcare
    monitoring WSNs with physiological sensor integration are used in healthcare monitoring
    applications to continuously monitor patients’ vital signs. In order to optimize
    the communication pathways and guarantee the fast and dependable transfer of health
    data, ML-based routing is essential. Using past data, supervised learning techniques
    like SVMs and decision trees are used to forecast the best routing routes [138,
    139]. In healthcare monitoring, ML-based routing helps identify health abnormalities
    early on, allowing medical professionals to take immediate action and improve
    patient care. 5.6 Industrial automation WSNs are used in industrial automation
    settings to monitor and manage production operations. The communication pathways
    between sensor nodes and control units are optimized via the use of ML-based routing
    techniques. Algorithms for clustering, including hierarchical clustering, help
    organize sensors according to their functioning and proximity [140]. By improving
    industrial automation systems’ responsiveness, ML-based routing leads to increased
    productivity, decreased downtime, and predictive maintenance. 5.7 Disaster response
    and monitoring WSNs with a variety of sensors are used for disaster response and
    monitoring to gather vital information in affected regions. ML-based routing is
    used to provide dependable data transfer of environmental conditions, survivor
    positions, and infrastructure status, as well as to optimize communication channels.
    Nodes are able to adjust their routing choices dynamically in response to changing
    circumstances thanks to reinforcement learning techniques like Q-Learning [141].
    In catastrophe situations, ML-based routing improves response efficiency by giving
    decision-makers precise and timely information. 5.8 Smart grids and energy management
    Within the context of smart grids, WSNs are essential for controlling and monitoring
    energy distribution. In order to maximize communication channels for gathering
    real-time data on energy consumption, grid conditions, and any problems, ML-based
    routing is used [142, 143]. Clustering algorithms are examples of unsupervised
    learning approaches that help find patterns and anomalies in the grid. ML-based
    routing makes smart grids more effective by facilitating load balancing, preemptive
    maintenance, and better energy management. 5.9 Environmental pollution monitoring
    WSNs are widely used to monitor the amount of pollution in the environment. The
    deployment and routing of sensor nodes are optimized for efficient pollution data
    collecting via the use of ML-based routing algorithms. Reinforcement learning
    in conjunction with clustering algorithms allows routing choices to be adjusted
    according to the various pollution levels in various regions [144, 145]. In environmental
    pollution monitoring, ML-based routing helps identify pollution sources early,
    makes regulatory compliance easier, and improves environmental sustainability
    overall. 5.10 Autonomous vehicles and traffic optimization WSNs play a role in
    traffic optimization and monitoring inside autonomous cars and intelligent transportation
    systems. Using real-time sensor data, ML-based routing is used to route autonomous
    cars, control traffic flow, and dynamically modify traffic light timings [146].
    Nodes may learn the best routing algorithms by taking traffic patterns and congestion
    levels into account, thanks to reinforcement learning techniques like DQN. In
    autonomous cars, ML-based routing improves traffic efficiency, eases congestion,
    and makes transportation networks safer and more dependable. A detailed summary
    of prominent case studies showcasing the use of ML-based routing in WSNs is shown
    in Table 2. In order to improve routing efficiency and tackle particular obstacles,
    ML approaches have been used in a unique fashion for each case study. An anthology
    of these case studies is shown in the table, providing valuable perspectives on
    the many implementations and achievements of ML within the realm of WSN routing.
    The case studies demonstrate the diversity of ML-based routing across several
    industries by including topics like environmental monitoring, industrial automation,
    healthcare, and smart cities. In the field of WSNs, these case studies may serve
    as a valuable resource for practitioners and researchers seeking to enhance their
    own implementations of intelligent routing systems. Table 2 Case studies in ML-based
    routing applications Full size table 6 Comparison evaluation metrics and performance
    analysis A comprehensive analytical strategy and several critical indicators are
    essential for judging the efficacy of these intelligent routing solutions when
    analyzing the performance of ML-based routing in WSNs. 6.1 Metrics for assessing
    ML-based routing performance 1) Packet delivery ratio (PDR): The proportion of
    packets that are successfully transported from the source to the destination is
    measured by PDR [147]. A high PDR in ML-based routing denotes efficient routing
    choices that result in successful data transfer. 2) End-to-End delay: The time
    it takes for a packet to go from its source to its destination is measured by
    this statistic. In order to ensure timely data transmission, ML-based routing
    systems should strive for reduced end-to-end delays [148]. 3) Network lifetime:
    This makes evaluating how routing choices affect the total lifespan of the network
    critical, as WSNs are resource-constrained. Making use of ML for routing may help
    implement energy-saving measures that extend the network’s lifespan [149]. 4)
    Energy consumption: Analyzing each node’s and the network’s total energy usage
    sheds light on how effective ML-based routing is. In order to maximize resource
    usage, lower energy consumption is preferred [150]. 5) Routing overhead: The extra
    data and control packets introduced by the routing protocol are measured as routing
    overhead. To prevent wasting network resources, ML-based routing systems should
    strive for low overhead [151]. 6) Scalability: Metrics for scalability evaluate
    ML-based routing’s capacity to accommodate more nodes and grow with the network
    without sacrificing efficiency [152, 153]. 6.2 Discussion A complete comparison
    of metrics between ML-based routing and conventional routing algorithms in WSNs
    is shown in Table 3. The purpose of this comparison study is to provide a comprehensive
    comprehension of the merits and demerits associated with each routing paradigm.
    PDR, End-to-End Delay, Network Lifetime, Energy Consumption, Routing Overhead,
    and Scalability are some of the critical performance indicators included in the
    table. By facilitating a rapid and thorough evaluation of the overall efficacy
    and flexibility of any routing technique, Fig. 10 is an invaluable resource for
    network managers, researchers, and decision-makers. The assessment of ML-based
    routing in WSNs reveals compelling advantages over traditional routing approaches.
    With a PDR of 95%, ML-based routing outperforms traditional methods, emphasizing
    its effectiveness in ensuring successful data transmission. The achieved low End-to-End
    Delay of 25 ms showcases the efficiency of ML algorithms in reducing latency.
    Moreover, ML-based routing extends the Network Lifetime to 120 days, highlighting
    its prowess in energy-efficient strategies. Lower Energy Consumption (2500 J)
    and Routing Overhead (8%) underscore optimized resource utilization. The scalability
    metric demonstrates ML-based routing’s ability to handle 5000 nodes, outshining
    traditional routing (2000 nodes). Comparative analysis emphasizes ML’s robustness
    to dynamic conditions and superior adaptability. Acknowledging limitations, such
    as computational complexity, opens avenues for future research, including exploring
    ensemble methods and reinforcement learning with deep neural networks. Overall,
    ML-based routing emerges as a transformative paradigm, mitigating challenges and
    enhancing the intelligence of WSNs. Table 3 Comparison of ML-based routing and
    traditional routing metrics Full size table Fig. 10 Performance Metrics Comparison
    of ML-Based Routing and Traditional Routing Metrics Full size image 6.2.1 Discussion
    An important benchmark for comparing the effectiveness of ML-based and conventional
    routing techniques in WSNs is the comparison study between them. 6.2.2 Robustness
    to dynamic conditions According to the investigation, ML-based routing outperforms
    conventional routing protocols in terms of resilience to dynamic situations. The
    routing choices may dynamically adapt to changes in network circumstances, such
    as fluctuations in node density, mobility, and communication topology, thanks
    to the inherent flexibility of ML algorithms. On the other hand, conventional
    protocols that use static routing algorithms could find it difficult to handle
    the sudden variations that are seen in dynamic WSNs. 6.2.3 Performance under varying
    workloads When network traffic circumstances and workloads alter, ML-based routing
    performs better. The system is able to adjust and improve routing options depending
    on workload patterns and real-time data thanks to ML algorithms’ learning capabilities.
    When opposed to conventional methods, which often depend on pre-defined, static
    routing rules, this flexibility leads to more responsive and efficient routing
    techniques. The investigation confirms that ML-based routing may perform very
    well in dynamic environments with changing workloads. 6.2.4 Resource utilization
    Resource consumption, which includes things like memory use, processing needs,
    and communication overhead, is an important component of the comparative study.
    With the use of intelligent decision-making and effective resource usage, ML-based
    routing maximizes the use of available network resources. By contrast, since they
    are less adaptable and preset, older routing protocols may not use resources as
    efficiently as they might. ML algorithms help reduce resource waste and improve
    overall network efficiency via continuous learning. 6.2.5 Implications and future
    directions The design and execution of routing methods in WSNs are significantly
    impacted by the proven benefits of ML-based routing. The intelligence and flexibility
    provided by ML algorithms provide opportunities to tackle the problems caused
    by dynamic network settings. Future paths in study can include hybrid strategies
    that combine the best features of classical and ML-based routing to provide more
    adaptable and durable solutions. Further research is necessary to address issues
    related to the computational complexity of ML algorithms and feasible deployment
    techniques for these algorithms in sensor nodes with limited resources. In WSNs,
    Table 4 provides a comprehensive Comparative Analysis of Metrics between ML-Based
    Routing and Traditional Routing. The table methodically delineates essential assessment
    criteria, including resilience to fluctuating workloads, effective exploitation
    of resources, and resilience to dynamic settings. Each measure is evaluated in
    comparison to conventional and ML-based routing systems, therefore offering an
    exhaustive analysis of their individual merits and drawbacks. The purpose of comparative
    analysis is to assist practitioners and researchers in making informed decisions
    by enabling them to identify the optimal routing method in accordance with the
    unique demands of individual applications and the dynamics of the network. The
    comparative research concludes by highlighting the revolutionary potential of
    ML-based routing in WSNs, which provides more flexibility, better performance
    in a range of scenarios, and effective use of resources. Table 4 Comparative analysis
    metrics of ML-based routing and traditional routing Full size table 7 Future directions
    and research opportunities The exploration of ML-based routing in WSNs not only
    sheds light on current capabilities but also unveils promising avenues for future
    research and development. A comprehensive inventory of prospective and existing
    research opportunities and future directions for IoT-based WSNs is shown in Table
    5. This list serves as a guide for travelers in this ever-evolving domain. The
    table encompasses a broad spectrum of topics, such as the incorporation of edge
    computing to simplify real-time decision-making, the development of ML models
    that conserve energy, the investigation of hybrid approaches that combine ML and
    heuristics, and the resolution of security and privacy issues in ML routing. Several
    key directions and opportunities emerge from the findings: Table 5 Future directions
    and research opportunities for IoT-based in WSNs Full size table 8 Conclusion
    This exhaustive examination has, in summary, shed light on significant discoveries
    pertaining to the incorporation of ML in the routing of WSNs. The assessment highlighted
    the effectiveness of ML-driven routing in addressing the difficulties presented
    by resource-limited and dynamic WSN environments. This was evidenced by the ML-based
    routing’s high PDR, decreased End-to-End Delay, and prolonged Network Lifetime.
    ML algorithms provide advantages like as scalability, efficiency, and flexibility,
    which have significant implications for the future of WSNs routing. It becomes
    critical to promote the extensive implementation of ML in WSNs, as this will facilitate
    progress that not only improves the intelligence of routing strategies but also
    aids in the development of WSNs that are both responsive and energy-efficient.
    In the field of WSNs routing, more investigation is required to optimize the transformational
    potential of ML via the enhancement of algorithms, resolution of security apprehensions,
    and advancement of practical implementations. References Wu, J., Guo, S., Huang,
    H., Liu, W., & Xiang, Y. (2018). Information and communications technologies for
    sustainable development goals: state-of-the-art, needs and perspectives. IEEE
    Communication Surveys & Tutorials, 20, 2389–2406. Article   Google Scholar   Pandey,
    A., Kumar, D., Priyadarshi, R., and. Nath, V. (2023) “Development of smart village
    for better lifestyle of farmers by crop and health monitoring system BT - microelectronics,
    communication systems, machine learning and internet of things,” V. Nath and J.
    K. Mandal, Eds., Singapore: Springer Nature Singapore 689–694 Cubo, J., Nieto,
    A., & Pimentel, E. (2014). A cloud-based Internet of Things platform for ambient
    assisted living. Sensors, 14, 14070–14105. Article   ADS   PubMed   PubMed Central   Google
    Scholar   Djahel, S., Doolan, R., Muntean, G.-M., & Murphy, J. (2015). A communications-oriented
    perspective on traffic management systems for smart cities: Challenges and innovative
    approaches. IEEE Communication Surveys & Tutorials, 17(125), 151. Google Scholar   Wang,
    Y., Yang, A., Chen, X., Wang, P., Wang, Y., & Yang, H. (2017). A deep learning
    approach for blind drift calibration of sensor networks. IEEE Sensors Journal,
    17(13), 4158–4171. https://doi.org/10.1109/JSEN.2017.2703885 Article   ADS   CAS   Google
    Scholar   Takkar, A. (2014). “A genetic algorithm for finite state automata”,
    Indian. Journal of Computer Science and Engineering, 5, 140–145. ADS   Google
    Scholar   Hassanien, A. E., Rizk-Allah, R. M., & Elhoseny, M. (2018). A hybrid
    crow search algorithm based on rough searching scheme for solving engineering
    optimization problems. Journal of Ambient Intelligence Humanized Computing. https://doi.org/10.1007/S12652-018-0924-Y
    Article   Google Scholar   Wu, F., et al. (2018). A lightweight and robust two-factor
    authentication scheme for personalized healthcare systems using wireless medical
    sensor networks. Future Generation Computer System, 82, 727–737. Article   Google
    Scholar   Gupta, A., Tripathi, M., Shaikh, T. J., & Sharma, A. (2019). A lightweight
    anonymous user authentication and key establishment scheme for wearable devices.
    Computer Networks, 149, 29–42. Article   Google Scholar   Singh, L., Kumar, A.,
    & Priyadarshi, R. (2020). “Performance and comparison analysis of image processing
    based forest fire detection BT - nanoelectronics. In V. Nath & J. K. Mandal (Eds.),
    Circuits and communication systems” (pp. 473–479). Singapore: Singapore Springer.
    Google Scholar   Hammi, B., Fayad, A., Khatoun, R., Zeadally, S., & Begriche,
    Y. (2020). “A lightweight ECC-based authentication scheme for Internet of things
    (IoT),.” IEEE Systems Journal, 14, 3440–3450. Article   ADS   Google Scholar   Priyadarshi,
    R., Rana, H., Srivastava, A., & Nath, V. (2023). A novel approach for sink route
    in wireless sensor network. In V. Nath & J. K. Mandal (Eds.), Microelectronics,
    communication systems, machine learning and internet of things (pp. 695–703).
    Singapore: Singapore Springer Nature. Chapter   Google Scholar   Sateesh, V. A.,
    Kumar, A., Priyadarshi, R., & Nath, V. (2021). “A novel deployment scheme to enhance
    the coverage in wireless sensor network BT - proceedings of the fourth international
    conference on microelectronics. In V. Nath & J. K. Mandal (Eds.), Computing and
    communication systems” (pp. 985–993). Singapore: Singapore Springer. Google Scholar   Priyadarshi,
    R., & Nath, V. (2019). A novel diamond–hexagon search algorithm for motion estimation.
    Microsystem Technologies, 25, 4587–4591. Article   CAS   Google Scholar   Vijayanand,
    R., & Devaraj, D. (2020). A novel feature selection method using whale optimization
    algorithm and genetic operators for intrusion detection system in wireless mesh
    network. IEEE Access. https://doi.org/10.1109/ACCESS.2020.2978035 Article   Google
    Scholar   Priyadarshi, R., Singh, L., Singh, A. (2018) “A novel HEED protocol
    for wireless sensor networks,” in 2018 5th international conference on signal
    processing and integrated networks (SPIN) 296–300. Rawat, P., Chauhan, S., & Priyadarshi,
    R. (2021). A novel heterogeneous clustering protocol for lifetime maximization
    of wireless sensor network. Wireless Personal Communications, 117, 825–841. Article   Google
    Scholar   Gupta, T., Kumar, A., Priyadarshi, R. (2020) “A novel hybrid precoding
    technique for millimeter wave BT - nanoelectronics, circuits and communication
    systems,” V. Nath and J. K. Mandal, Eds., Singapore: Springer Singapore. 481–493.
    Desai, S., Kanphade, R., Priyadarshi, R., Rayudu, K., & Nath, V. (2023). A Novel
    technique for detecting crop diseases with efficient feature extraction. IETE
    Journal of Research. https://doi.org/10.1080/03772063.2023.2220667 Article   Google
    Scholar   Ahmed, Z., Caglar, A. E., & Murshed, M. (2022). A path towards environmental
    sustainability: The role of clean energy and democracy in ecological footprint
    of Pakistan. Journal of Cleaner Production, 358, 132007. Article   Google Scholar   Iqbal,
    M. A., Olaleye, O. G., & Bayoumi, M. A. (2017). A review on internet of things
    (IoT): Security and privacy requirements and the solution approaches. Global Journal
    of Computer Science and Technology., 16, 1–10. Google Scholar   Priyadarshi, R.,
    Gupta, B., & Anurag, A. (2020). Wireless sensor networks deployment: A result
    oriented analysis. Wireless Personal Communications, 113, 843–866. Article   Google
    Scholar   Yu, S., Park, K., & Park, Y. (2019). A secure lightweight three-factor
    authentication scheme for IoT in cloud computing environment. Sensors, 19, 3598.
    Article   ADS   PubMed   PubMed Central   Google Scholar   Chen, Y., & Chen, J.
    (2021). A secure three-factor-based authentication with key agreement protocol
    for e-Health clouds. The Journal of Supercomputing, 77, 3359–3380. Article   Google
    Scholar   Verma, S., Kawamoto, Y., Fadlullah, Z., Nishiyama, H., & Kato, N. (2017).
    A survey on network methodologies for real-time analytics of massive IoT data
    and open research issues. IEEE Communications Surveys and Tutorials, 19(1457),
    1477. Google Scholar   Yang, Y., Wu, L., Yin, G., Li, L., & Zhao, H. (2017). A
    survey on security and privacy issues in internet-of-things. IEEE Internet of
    Things Journal, 4, 1250–1258. Article   Google Scholar   Heartfield, R., & Loukas,
    G. (2015). A taxonomy of attacks and a survey of defence mechanisms for semantic
    social engineering attacks. ACM Computing Surveys, 48, 1–39. Article   Google
    Scholar   Priyadarshi, R., & Vikram, R. (2023). A triangle-based localization
    scheme in wireless multimedia sensor network. Wireless Personal Communications,
    133(1), 525–546. https://doi.org/10.1007/s11277-023-10777-7 Article   Google Scholar   Wang,
    X., Gu, H., Liu, Y., & Zhang, H. (2019). A two-stage RPSO-ACS based protocol:
    A new method for sensor network clustering and routing in mobile computing. IEEE
    Access. https://doi.org/10.1109/ACCESS.2019.2933150 Article   PubMed   PubMed
    Central   Google Scholar   Urbieta, A., González-Beltrán, A., Mokhtar, S. B.,
    Hossain, M. A., & Capra, L. (2017). “Adaptive and context-aware service composition
    for IoT-based smart cities”,. Future Genernation Computer System, 76, 262–274.
    Article   Google Scholar   Chaudhari, N., Gupta, A., & Raju, S. (2016). ALED system
    to provide mobile IoT assistance for elderly and disabled. International Journal
    of Smart Home, 10, 35–50. Article   Google Scholar   Dixon, N., Smith, A., & Flint,
    J. A. (2018). An acoustic emission landslide early warning system for communities
    in low-income and middle-income countries. Landslides. https://doi.org/10.1007/s10346-018-0977-1
    Article   Google Scholar   Deif, D. S., Member, S., Gadallah, Y., & Member, S.
    (2017). An ant colony optimization approach for the deployment of reliable wireless
    sensor networks. IEEE Access. https://doi.org/10.1109/ACCESS.2017.2711484 Article   Google
    Scholar   Mohajerani, A., Gharavian, D. (2016) “An ant colony optimization based
    routing algorithm for extending network lifetime in wireless sensor networks,”
    Wireless Networks, doi: https://doi.org/10.1007/s11276-015-1061-6. Xu, X., et
    al. (2019). An edge computing-enabled computation offloading method with privacy
    preservation for internet of connected vehicles. Future Generation Computer System,
    96, 89–100. Article   Google Scholar   R. Priyadarshi and R. R. Kumar, 2021 “An
    Energy-Efficient LEACH Routing Protocol for Wireless Sensor Networks BT - Proceedings
    of the Fourth International Conference on Microelectronics, Computing and Communication
    Systems,” V. Nath and J. K. Mandal, Eds., Singapore: Springer Singapore 423–430.
    Salarian, H., Chin, K. W., & Naghdy, F. (2014). An energy-efficient mobile-sink
    path selection strategy for wireless sensor networks. IEEE Transaction on Vehicular
    Technology. https://doi.org/10.1109/TVT.2013.2291811 Article   Google Scholar   Priyadarshi,
    R., Rawat, P., Nath, V., Acharya, B., & Shylashree, N. (2020). Three level heterogeneous
    clustering protocol for wireless sensor network. Microsystem Technologies, 26,
    3855–3864. Article   Google Scholar   Jin, J., Gubbi, J., Marusic, S., & Palaniswami,
    M. (2014). An information framework for creating a smart city through internet
    of things. IEEE Internet of Things Journal, 1, 112–121. Article   Google Scholar   Catarinucci,
    L., et al. (2015). An IoT-aware architecture for smart healthcare systems. IEEE
    Internet of Things Journal, 2, 515–526. Article   Google Scholar   Ahmad, A. J.,
    Hassan, S. D., Priyadarshi, R., Nath V. (2023) “Analysis on Image Compression
    for Multimedia Communication Using Hybrid of DWT and DCT BT - Microelectronics,
    Communication Systems, Machine Learning and Internet of Things,” V. Nath and J.
    K. Mandal, Eds., Singapore: Springer Nature Singapore, , pp. 667–672. Priyadarshi,
    R., & Gupta, B. (2021). Area coverage optimization in three-dimensional wireless
    sensor network. Wireless Personal Communications, 117, 843–865. Article   Google
    Scholar   Garcia-Font, V., Garrigues, C., & Rifà-Pous, H. (2017). Attack classification
    schema for smart city WSNs. Sensors, 17, 771. Article   ADS   PubMed   PubMed
    Central   Google Scholar   Ferrag, M. A., Maglaras, L. A., Janicke, H., Jiang,
    J., Shu, L. (2017) “Authentication protocols for the internet of things: A comprehensive
    survey,” Security and Communication Network Priyadarshi, R., Soni, S. K., Bhadu,
    R., & Nath, V. (2018). Performance analysis of diamond search algorithm over full
    search algorithm. Microsystem Technologies, 24, 2529–2537. Article   Google Scholar   Li,
    R., Lu, B., & McDonald-Maier, K. D. (2015). “Cognitive assisted living ambient
    system: A survey”,. Digital Communications and Networks, 1, 229–252. Article   Google
    Scholar   Zhou, Y., Wang, N., & Xiang, W. (2017). Clustering hierarchy protocol
    in wireless sensor networks using an improved PSO algorithm. IEEE Access. https://doi.org/10.1109/ACCESS.2016.2633826
    Article   Google Scholar   Lee, W.-H., Tseng, S.-S., & Shieh, W.-Y. (2010). Collaborative
    real-time traffic information generation and sharing framework for the intelligent
    transportation system. Information Science, 180, 62–70. Article   Google Scholar   Vijayakumar,
    P., Azees, M., Chang, V., Deborah, J., & Balusamy, B. (2017). Computationally
    efficient privacy-preserving authentication and key distribution techniques for
    vehicular ad hoc networks. Cluster Computing, 20, 2439–2450. Article   Google
    Scholar   Priyadarshi, R., & Gupta, B. (2020). Coverage area enhancement in wireless
    sensor network. Microsystem Technologies, 26(5), 1417–1426. Article   Google Scholar   Chan,
    A. C. F., & Zhou, J. (2014). Cyber-physical device authentication for the smart
    grid electric vehicle ecosystem. IEEE Journal on Selected Areas in Communications,
    32, 1509–1517. Article   Google Scholar   Farahat, I. S., Tolba, A. S., Elhoseny,
    M., Eladrosy, W. (2019) “Data security and challenges in smart cities BT - Security
    in smart cities: models, applications, and challenges,” A. Hassanien, M. Elhoseny,
    S. Ahmed, and A. Singh, Eds., Cham: Springer https://doi.org/10.1007/978-3-030-01560-2_6.
    Jiang, Y., Yin, S., & Kaynak, O. (2018). Data-driven monitoring and safety control
    of industrial cyber-physical systems: Basics and beyond. IEEE Access, 6, 47374–47384.
    Article   Google Scholar   Vinayakumar, R., Alazab, M., Soman, K. P., Poornachandran,
    P., Al-Nemrat, A., & Venkatraman, S. (2019). Deep learning approach for intelligent
    intrusion detection system. IEEE Access, 7, 41525–41550. Article   Google Scholar   Priyadarshi,
    R., Gupta, B., & Anurag, A. (2020). Deployment techniques in wireless sensor networks:
    A survey, classification, challenges, and future research issues. The Journal
    of Supercomputing, 76, 7333–7373. Article   Google Scholar   Lazarescu, M. T.
    (2013). Design of a WSN platform for long-term environmental monitoring for IoT
    applications. IEEE Journal on emerging and selected topics in circuits and systems.,
    3, 45–54. Article   ADS   Google Scholar   Ramesh, M. V. (2014). Design, development,
    and deployment of a wireless sensor network for detection of landslides. Ad Hoc
    Network. https://doi.org/10.1016/j.adhoc.2012.09.002 Article   Google Scholar   He,
    W., Yan, G., & Xu, L. (2014). Developing vehicular data cloud services in the
    IoT environment. IEEE transactions on industrial informatics, 10, 1587–1595. Article   Google
    Scholar   Anurag, A., Priyadarshi, R., Goel, A., Gupta, B. (2020) “2-D coverage
    optimization in WSN using a novel variant of particle swarm optimisation,” in
    2020 7th international conference on signal processing and integrated networks
    (SPIN) 663–668. Chatterjee, S., Nandan, M., Ghosh, A., Banik, S. (2022) “DTNMA:
    Identifying routing attacks in delay-tolerant network BT - Cyber intelligence
    and information retrieval,” J. M. R. S. Tavares, P. Dutta, S. Dutta, and D. Samanta,
    Eds., Singapore: Springer https://doi.org/10.1007/978-981-16-4284-5_1. Kumar D.,
    Ravindra, S. (2016) “E-Assistance for elderly and disabled,” Journal of Embedded
    System and Processing 1 Alomair B., Poovendran, R. (2014) “Efficient authentication
    for mobile and pervasive computing,” IEEE Transactions on Mobile Computing 13
    Gaddam, A., Mukhopadhyay, S. C., Gupta, G. S. (2011) “Elder care based on cognitive
    sensor network,” IEEE Sensors Journal https://doi.org/10.1109/JSEN.2010.2051425.
    Priyadarshi, R., Singh, A., Agarwal, D., Verma, U. C., Singh, A., (2023) “Emerging
    Smart Manufactory: Industry 4.0 and Manufacturing in India: The Next Wave,” in
    Lecture Notes in Electrical Engineering, V. Nath and J. K. Mandal, Eds., Singapore:
    Springer Nature Singapore https://doi.org/10.1007/978-981-19-1906-0_32. Priyadarshi,
    R., Rawat, P., & Nath, V. (2019). Energy dependent cluster formation in heterogeneous
    wireless sensor network. Microsystem Technologies, 25, 2313–2321. Article   Google
    Scholar   Priyadarshi, R., Soni, S. K., & Nath, V. (2018). Energy efficient cluster
    head formation in wireless sensor network. Microsystem Technologies, 24, 4775–4784.
    Article   Google Scholar   Okwori, M., Bima, M. E., Inalegwu, O. C., Saidu, M.,
    Audu, W. M., Abdullahi, U. (2016) “Energy efficient routing in wireless sensor
    network using ant colony optimization and firefly algorithm,” CEUR Workshop Proceedings
    1830 Randheer, Soni, S. K., Kumar, S., Priyadarshi, R. (2020) “Energy-Aware Clustering
    in Wireless Sensor Networks BT - Nanoelectronics, Circuits and Communication Systems,”
    V. Nath and J. K. Mandal, Eds., Singapore: Springer Singapore 453–461. Wang, C.,
    Liu, X., Hu, H., Han, Y., & Yao, M. (2020). Energy-efficient and load-balanced
    clustering routing protocol for wireless sensor networks using a chaotic genetic
    algorithm. IEEE Access. https://doi.org/10.1109/ACCESS.2020.3020158 Article   PubMed   PubMed
    Central   Google Scholar   Rawat, P., Chauhan, S., & Priyadarshi, R. (2020). Energy-efficient
    clusterhead selection scheme in heterogeneous wireless sensor network. Journal
    of Circuits System and Computer, 29(13), 2050204. Article   Google Scholar   Cheng,
    P., Deng, R., & Chen, J. (2012). Energy-efficient cooperative spectrum sensing
    in sensor-aided cognitive radio networks. IEEE Wireless Communication. https://doi.org/10.1109/MWC.2012.6393524
    Article   Google Scholar   Priyadarshi, R. (2024). Energy-efficient routing in
    wireless sensor networks: A meta-heuristic and artificial intelligence-based approach:
    A comprehensive review. Archives of Computational Methods in Engineering. https://doi.org/10.1007/s11831-023-10039-6
    Article   Google Scholar   Chang, I. P., Lee, T. F., Lin, T. H., & Liu, C. M.
    (2015). Enhanced two-factor authentication and key agreement using dynamic identities
    in wireless sensor networks. Sensors, 15, 29841–29854. Article   ADS   PubMed   PubMed
    Central   Google Scholar   Priyadarshi, R., Singh, L., Singh, A., Thakur, A. (2018)
    “SEEN: stable energy efficient network for wireless sensor network,” in 2018 5th
    international conference on signal processing and integrated networks (SPIN) 338–342.
    Sateesh, V. A., Dutta, I., Priyadarshi, R., Nath, V. (2021) “Fractional frequency
    reuse scheme for noise-limited cellular networks BT - Proceedings of the fourth
    international conference on microelectronics, computing and communication systems,”
    V. Nath and J. K. Mandal, Eds., Singapore: Springer Singapore. 995–1004. Hu V.
    C. et al., (2013) “Guide to attribute-based access control (abac) definition and
    considerations (draft),” NIST Special Publication 800 Hussain, A., Wenbi, R.,
    Silva, A. L., Nadher, M., & Mudhish, M. (2015). Health and emergency-care platform
    for the elderly and disabled people in the smart city. Journal of Systems and
    Software, 110, 253–263. Article   Google Scholar   Shankar, T., Shanmugavel, S.,
    & Rajesh, A. (2016). Hybrid HSA and PSO algorithm for energy efficient cluster
    head selection in wireless sensor networks. Swarm and Evolutionary Computation.
    https://doi.org/10.1016/j.swevo.2016.03.003 Article   Google Scholar   Munib,
    R., Soliha, R., & Seema, M. (2016). Implementation of ICT and wireless sensor
    networks for earthquake alert and disaster management in earthquake prone areas.
    Procedia Computer Science, 85, 92–99. Article   Google Scholar   Priyadarshi,
    R., & Gupta, B. (2023). 2-D coverage optimization in obstacle-based FOI in WSN
    using modified PSO. The Journal of Supercomputing, 79(5), 4847–4869. Article   Google
    Scholar   Al-Turjman, F., Nawaz, M. H., & Ulusar, U. D. (2020). Intelligence in
    the Internet of medical things era: A systematic review of current and future
    trends. Computer Communications, 150, 644–660. Article   Google Scholar   Stoces,
    M., Vanek, J., Masner, J., & Pavlík, J. (2016). Internet of Things (IoT) in agriculture-selected
    aspects. AGRIS On-line Papers in Economics Informatics, 8, 83–88. Article   Google
    Scholar   Gubbi, J., Buyya, R., Marusic, S., & Palaniswami, M. (2013). Internet
    of things (IoT): A vision, architectural elements, and future directions. Future
    Generation Computer Systems, 29, 1645–1660. Article   Google Scholar   Zanella,
    A., Bui, N., Castellani, A., Vangelista, L., & Zorzi, M. (2014). Internet of things
    for smart cities. IEEE Internet of Things Journal, 1, 22–32. Article   Google
    Scholar   Alaba, F. A., Othman, M., Hashem, I. A. T., & Alotaibi, F. (2017). Internet
    of things security: A survey. Journal of Network and Computer Applications, 88,
    10–28. Article   Google Scholar   Al-Fuqaha, A., Guizani, M., Mohammadi, M., Aledhari,
    M., & Ayyash, M. (2015). Internet of things: A survey on enabling technologies,
    protocols, and applications. IEEE Communications Surveys & Tutorials, 17(4), 2347–2376.
    https://doi.org/10.1109/COMST.2015.2444095 Article   Google Scholar   Chen, K.,
    et al. (2018). Internet-of-things security and vulnerabilities: taxonomy, challenges,
    and practice. Journal of Hardware and Systems Security, 2, 97–110. Article   Google
    Scholar   Zeng, X., Garg, S. K., Strazdins, P., Jayaraman, P. P., Georgakopoulos,
    D., & Ranjan, R. (2017). IOTSim: A simulator for analyzing IoT applications. Journal
    of Systems Architecture, 72, 93–107. Article   Google Scholar   Far, H. A. N.,
    Bayat, M., Das, A. K., Fotouhi, M., Pournaghi, S. M., & Doostari, M. A. (2021).
    LAPTAS: Lightweight anonymous privacy-preserving three-factor authentication scheme
    for WSN-based IIoT. Wireless Networks, 27, 1389–1412. Article   Google Scholar   Ma,
    X., Yu, H., Wang, Y., & Wang, Y. (2015). Large-scale transportation network congestion
    evolution prediction using deep learning theory. PLoS One, 10, e0119044. Article   PubMed   PubMed
    Central   Google Scholar   Yao, W., Chu, C.-H., & Li, Z. (2011). Leveraging complex
    event processing for smart hospitals using RFID. Journal of Network and Computer
    Applications, 34, 799–810. Article   Google Scholar   Gope, P., Lee, J., & Quek,
    T. Q. (2018). Lightweight and practical anonymous authentication protocol for
    RFID systems using physically unclonable functions. IEEE Transactions on Information
    Forensics and Security, 13, 2831–2843. Article   Google Scholar   Priyadarshi,
    R., Yadav, S., Bilyan, D., (2019) “Performance analysis of adapted selection based
    protocol over LEACH protocol BT - smart computational strategies: Theoretical
    and practical aspects,” A. K. Luhach, K. B. G. Hawari, I. C. Mihai, P.-A. Hsiung,
    and R. B. Mishra, Eds., Singapore: Springer Singapore 247–256. doi: https://doi.org/10.1007/978-981-13-6295-8_21.
    Gope, P., & Sikdar, B. (2018). Lightweight and privacy-preserving two-factor authentication
    scheme for IoT devices. IEEE Internet of Things Journal, 6, 580–589. Article   Google
    Scholar   Fan, K., Gong, Y., Liang, C., Li, H., & Yang, Y. (2016). Lightweight
    and ultralightweight RFID mutual authentication protocol with cache in the reader
    for IoT in 5G. Security and Communication Networks, 9, 3095–3104. Article   Google
    Scholar   Jiang, Q., Zeadally, S., Ma, J., & He, D. (2017). Lightweight three-factor
    authentication and key agreement protocol for internet-integrated wireless sensor
    networks. IEEE Access, 5, 3376–3392. Article   Google Scholar   Trappe, W., Howard,
    R., & Moore, R. S. (2015). Low-energy security: limits and opportunities in the
    Internet of things. IEEE Security & Privacy, 13, 14–21. Article   Google Scholar   Foschini,
    L., Taleb, T., Corradi, A., & Bottazzi, D. (2011). M2M-based metropolitan platform
    for IMS-enabled road traffic management in IoT. IEEE Communications Magazine,
    49, 50–57. Article   Google Scholar   Alsheikh, M. A., Lin, S., Niyato, D., &
    Tan, H.-P. (2014). Machine learning in wireless sensor networks: algorithms, strategies,
    and applications. IEEE Communications Surveys & Tutorials, 16(4), 1996–2018. https://doi.org/10.1109/COMST.2014.2320099
    Article   Google Scholar   Kumar, S., Soni, S., Randheer, K., Priyadarshi, R.
    (2020) “Performance analysis of novel energy aware routing in wireless sensor
    network BT - Nanoelectronics, circuits and communication systems,” V. Nath and
    J. K. Mandal, Eds., Singapore: Springer Singapore, 503–511. Garcia-Carrillo, D.,
    & Marin-Lopez, R. (2018). Multihop bootstrapping with EAP through COAP intermediaries
    for IoT. IEEE Internet of Things Journal, 5, 4003–4017. Article   Google Scholar   Zhou,
    L., & Chao, H.-C. (2011). Multimedia traffic security architecture for the internet
    of things. IEEE Network, 25, 35–40. Article   Google Scholar   Chen, D., Liu,
    Z., Wang, L., Dou, M., Chen, J., & Li, H. (2013). Natural disaster monitoring
    with wireless sensor networks: a case study of data-intensive applications upon
    low-cost scalable systems. Mobile Networks and Applications. https://doi.org/10.1007/s11036-013-0456-9
    Article   Google Scholar   Chaabouni, N., Mosbah, M., Zemmari, A., Sauvignac,
    C., & Faruki, P. (2019). Network intrusion detection for IoT security based on
    learning techniques. IEEE Communications Surveys & Tutorials, 21, 2671–2701. Article   Google
    Scholar   Priyadarshi, R., Bhardwaj, P., Gupta, P., Nath, V. (2023) “Utilization
    of Smartphone-Based Wireless Sensors in Agricultural Science: A State of Art,”
    in Lecture Notes in Electrical Engineering, V. Nath and J. K. Mandal, Eds., Singapore:
    Springer Nature Singapore https://doi.org/10.1007/978-981-19-1906-0_56. Yang T.
    et al., (2019) “New features of authentication scheme for the IoT: a survey BT
    - Proceedings of the 2nd international ACM workshop on security and privacy for
    the internet-of-things,” New York, NY, United States: ACM. Dahiya, S., & Singh,
    P. K. (2018). Optimized mobile sink based grid coverage-aware sensor deployment
    and link quality based routing in wireless sensor networks. AEU International
    Journal of Electronics and Communication. https://doi.org/10.1016/j.aeue.2018.03.031
    Article   Google Scholar   Rejinaparvin, J., & Vasanthanayaki, C. (2015). Particle
    swarm optimization-based clustering by preventing residual nodes in wireless sensor
    networks. IEEE Sensors Journal. https://doi.org/10.1109/JSEN.2015.2416208 Article   Google
    Scholar   Beyer, S. M., Mullins, B. E., Graham, S. R., & Bindewald, J. M. (2018).
    Pattern-of-life modeling in smart homes. IEEE Internet of Things Journal, 56,
    5317–5325. Article   Google Scholar   Zou, W., Sun, Y., Zhou, Y., Lu, Q., Nie,
    Y., Sun, T., & Peng, L. (2022). Limited sensing and deep data mining: A new exploration
    of developing city-wide parking guidance systems. IEEE Intelligent Transportation
    Systems Magazine, 14(1), 198–215. https://doi.org/10.1109/MITS.2020.2970185 Article   Google
    Scholar   Jiang, Y., & Li, X. (2022). Broadband cancellation method in an adaptive
    co-site interference cancellation system. International Journal of Electronics,
    109(5), 854–874. https://doi.org/10.1080/00207217.2021.1941295 Article   ADS   Google
    Scholar   Zheng, C., An, Y., Wang, Z., Qin, X., Eynard, B., Bricogne, M., & Zhang,
    Y. (2023). Knowledge-based engineering approach for defining robotic manufacturing
    system architectures. International Journal of Production Research, 61(5), 1436–1454.
    https://doi.org/10.1080/00207543.2022.2037025 Article   Google Scholar   Zhang,
    X., Wang, Y., Yang, M., & Geng, G. (2021). Toward concurrent video multicast orchestration
    for caching-assisted mobile networks. IEEE Transactions on Vehicular Technology,
    70(12), 13205–13220. https://doi.org/10.1109/TVT.2021.3119429 Article   Google
    Scholar   Mao, Y., Sun, R., Wang, J., Cheng, Q., Kiong, L. C., & Ochieng, W. Y.
    (2022). New time-differenced carrier phase approach to GNSS/INS integration. GPS
    Solutions, 26(4), 122. https://doi.org/10.1007/s10291-022-01314-3 Article   Google
    Scholar   Zhang, H., Luo, G., Li, J., & Wang, F. Y. (2022). C2FDA: Coarse-to-fine
    domain adaptation for traffic object detection. IEEE Transactions on Intelligent
    Transportation Systems, 23(8), 12633–12647. https://doi.org/10.1109/TITS.2021.3115823
    Article   Google Scholar   Mao, Y., Zhu, Y., Tang, Z., & Chen, Z. (2022). A novel
    airspace planning algorithm for cooperative target localization. Electronics,
    11(18), 2950. https://doi.org/10.3390/electronics11182950 Article   Google Scholar   Sun,
    G., Xu, Z., Yu, H., Chen, X., Chang, V., & Vasilakos, A. V. (2020). Low-latency
    and resource-efficient service function chaining orchestration in network function
    virtualization. IEEE Internet of Things Journal, 7(7), 5760–5772. https://doi.org/10.1109/JIOT.2019.2937110
    Article   Google Scholar   Sun, G., Zhu, G., Liao, D., Yu, H., Du, X., & Guizani,
    M. (2019). Cost-efficient service function chain orchestration for low-latency
    applications in NFV networks. IEEE Systems Journal, 13(4), 3877–3888. https://doi.org/10.1109/JSYST.2018.2879883
    Article   ADS   Google Scholar   Sun, G., Li, Y., Liao, D., & Chang, V. (2018).
    Service function chain orchestration across multiple domains: A full mesh aggregation
    approach. IEEE Transactions on Network and Service Management, 15(3), 1175–1191.
    https://doi.org/10.1109/TNSM.2018.2861717 Article   Google Scholar   Yin, Z.,
    Liu, Z., Liu, X., Zheng, W., & Yin, L. (2023). Urban heat islands and their effects
    on thermal comfort in the US: New York and New Jersey. Ecological Indicators,
    154, 110765. https://doi.org/10.1016/j.ecolind.2023.110765 Article   Google Scholar   Cheng,
    B., Wang, M., Zhao, S., Zhai, Z., Zhu, D., & Chen, J. (2017). Situation-aware
    dynamic service coordination in an IoT environment. IEEE/ACM Transactions on Networking,
    25(4), 2082–2095. https://doi.org/10.1109/TNET.2017.2705239 Article   Google Scholar   Li,
    Q., Lin, H., Tan, X., & Du, S. (2020). H∞ consensus for multiagent-based supply
    chain systems under switching topology and uncertain demands. IEEE Transactions
    on Systems Man and Cybernetics Systems, 50(12), 4905–4918. https://doi.org/10.1109/TSMC.2018.2884510
    Article   Google Scholar   Yang, X., Wang, X., Wang, S., & Puig, V. (2023). Switching-based
    adaptive fault-tolerant control for uncertain nonlinear systems against actuator
    and sensor faults. Journal of the Franklin Institute, 360(16), 11462–11488. https://doi.org/10.1016/j.jfranklin.2023.08.042
    Article   MathSciNet   Google Scholar   Dai, W., Zhou, X., Li, D., Zhu, S., &
    Wang, X. (2022). Hybrid parallel stochastic configuration networks for industrial
    data analytics. IEEE Transactions on Industrial Informatics, 18(4), 2331–2341.
    https://doi.org/10.1109/TII.2021.3096840 Article   Google Scholar   Wang, Q.,
    Dai, W., Zhang, C., Zhu, J., & Ma, X. (2023). A compact constraint incremental
    method for random weight networks and its application. IEEE Transactions on Neural
    Networks and Learning Systems. https://doi.org/10.1109/TNNLS.2023.3289798 Article   PubMed   Google
    Scholar   Zhou, G., Zhou, X., Chen, J., Jia, G., & Zhu, Q. (2022). LiDAR echo
    gaussian decomposition algorithm for FPGA implementation. Sensors, 22(12), 4628.
    https://doi.org/10.3390/s22124628 Article   ADS   PubMed   PubMed Central   Google
    Scholar   Shang, K., Xu, L., Liu, X., Yin, Z., Liu, Z., Li, X., & Zheng, W. (2023).
    Study of urban heat island effect in Hangzhou metropolitan area based on SW-TES
    algorithm and image dichotomous model. SAGE Open. https://doi.org/10.1177/21582440231208851
    Article   Google Scholar   Jannat, M. K. A., Islam, M. S., Yang, S., & Liu, H.
    (2023). Efficient Wi-Fi-based human activity recognition using adaptive antenna
    elimination. IEEE Access, 11, 105440–105454. https://doi.org/10.1109/ACCESS.2023.3320069
    Article   Google Scholar   Wang, Y., Sun, R., Cheng, Q., & Ochieng, W. Y. (2024).
    Measurement quality control aided multisensor system for improved vehicle navigation
    in urban areas. IEEE Transactions on Industrial Electronics, 71(6), 6407–6417.
    https://doi.org/10.1109/TIE.2023.3288188 Article   Google Scholar   Xu, X., Lin,
    Z., Li, X., Shang, C., & Shen, Q. (2022). Multi-objective robust optimisation
    model for MDVRPLS in refined oil distribution. International Journal of Production
    Research, 60(22), 6772–6792. https://doi.org/10.1080/00207543.2021.1887534 Article   Google
    Scholar   Xu, X., & Wei, Z. (2023). Dynamic pickup and delivery problem with transshipments
    and LIFO constraints. Computers & Industrial Engineering, 175, 108835. https://doi.org/10.1016/j.cie.2022.108835
    Article   Google Scholar   Xu, X., Liu, W., & Yu, L. (2022). Trajectory prediction
    for heterogeneous traffic-agents using knowledge correction data-driven model.
    Information Sciences, 608, 375–391. https://doi.org/10.1016/j.ins.2022.06.073
    Article   Google Scholar   Dai, X., Xiao, Z., Jiang, H., Alazab, M., Lui, J. C.
    S., Dustdar, S., & Liu, J. (2023). Task Co-offloading for D2D-assisted mobile
    edge computing in industrial internet of things. IEEE Transactions on Industrial
    Informatics, 19(1), 480–490. https://doi.org/10.1109/TII.2022.3158974 Article   Google
    Scholar   Xiao, Z., Fang, H., Jiang, H., Bai, J., Havyarimana, V., Chen, H., &
    Jiao, L. (2023). Understanding private car aggregation effect via spatio-temporal
    analysis of trajectory data. IEEE Transactions on Cybernetics, 53(4), 2346–2357.
    https://doi.org/10.1109/TCYB.2021.3117705 Article   PubMed   Google Scholar   Jiang,
    H., Xiao, Z., Li, Z., Xu, J., Zeng, F., & Wang, D. (2022). An energy-efficient
    framework for internet of things underlaying heterogeneous small cell networks.
    IEEE Transactions on Mobile Computing, 21(1), 31–43. https://doi.org/10.1109/TMC.2020.3005908
    Article   Google Scholar   Ma, J., & Hu, J. (2022). Safe consensus control of
    cooperative-competitive multi-agent systems via differential privacy. Kybernetika,
    58(3), 426–439. https://doi.org/10.14736/kyb-2022-3-0426 Article   MathSciNet   Google
    Scholar   Chen, B., Hu, J., Zhao, Y., & Ghosh, B. K. (2022). Finite-time velocity-free
    rendezvous control of multiple AUV systems with intermittent communication. IEEE
    Transactions on Systems Man and Cybernetics Systems, 52(10), 6618–6629. https://doi.org/10.1109/TSMC.2022.3148295
    Article   Google Scholar   Guo, C., & Hu, J. (2023). Time base generator based
    practical predefined-time stabilization of high-order systems with unknown disturbance.
    IEEE Transactions on Circuits and Systems II Express Briefs. https://doi.org/10.1109/TCSII.2023.3242856
    Article   Google Scholar   Qu, J., Mao, B., Li, Z., Xu, Y., Zhou, K., Cao, X.,
    & Wang, X. (2023). Recent progress in advanced tactile sensing technologies for
    soft grippers. Advanced Functional Materials, 33(41), 2306249. https://doi.org/10.1002/adfm.202306249
    Article   CAS   Google Scholar   Qu, J., Yuan, Q., Li, Z., Wang, Z., Xu, F., Fan,
    Q., & Xu, M. (2023). All-in-one strain-triboelectric sensors based on environment-friendly
    ionic hydrogel for wearable sensing and underwater soft robotic grasping. Nano
    Energy, 111, 108387. https://doi.org/10.1016/j.nanoen.2023.108387 Article   CAS   Google
    Scholar   Min, H., Li, Y., Wu, X., Wang, W., Chen, L., & Zhao, X. (2023). A measurement
    scheduling method for multi-vehicle cooperative localization considering state
    correlation. Vehicular Communications. https://doi.org/10.1016/j.vehcom.2023.100682
    Article   Google Scholar   Mou, J., Gao, K., Duan, P., Li, J., Garg, A., & Sharma,
    R. (2023). A machine learning approach for energy-efficient intelligent transportation
    scheduling problem in a real-world dynamic circumstances. IEEE Transactions on
    Intelligent Transportation Systems, 24(12), 15527–15539. https://doi.org/10.1109/TITS.2022.3183215
    Article   Google Scholar   Hou, X., Xin, L., Fu, Y., Na, Z., Gao, G., Liu, Y.,
    & Chen, T. (2023). A self-powered biomimetic mouse whisker sensor (BMWS) aiming
    at terrestrial and space objects perception. Nano Energy, 118, 109034. https://doi.org/10.1016/j.nanoen.2023.109034
    Article   CAS   Google Scholar   Cao, B., Zhao, J., Gu, Y., Fan, S., & Yang, P.
    (2020). Security-aware industrial wireless sensor network deployment optimization.
    IEEE Transactions on Industrial Informatics, 16(8), 5309–5316. https://doi.org/10.1109/TII.2019.2961340
    Article   Google Scholar   Cao, B., Zhao, J., Yang, P., Gu, Y., Muhammad, K.,
    Rodrigues, J. J., & de Albuquerque, V. H. C. (2020). Multiobjective 3-D topology
    optimization of next-generation wireless data center network. IEEE Transactions
    on Industrial Informatics, 16(5), 3597–3605. https://doi.org/10.1109/TII.2019.2952565
    Article   Google Scholar   Chen Z, Gao L. (2023). CURSOR: Configuration update
    synthesis using order rules. Paper presented at the IEEE INFOCOM 2023 - IEEE Conference
    on Computer Communications. https://doi.org/10.1109/INFOCOM53939.2023.10228930
    Xuemin, Z., Ying, R., Zenggang, X., & HaitaoFangYuan, D. X. L. (2023). Resource-constrained
    and socially selfish-based incentive algorithm for socially aware networks. Journal
    of Signal Processing Systems For Signal Image And Video Technology, 95(12), 1439–1453.
    https://doi.org/10.1007/s11265-023-01896-2 Article   Google Scholar   Wu, Q.,
    Fang, J., Zeng, J., Wen, J., & Luo, F. (2024). Monte Carlo simulation-based robust
    workflow scheduling for spot instances in cloud environments. Tsinghua Science
    and Technology, 29(1), 112–126. https://doi.org/10.26599/TST.2022.9010065 Article   Google
    Scholar   Lyu, T., Xu, H., Zhang, L., & Han, Z. (2023). Source selection and resource
    allocation in wireless powered relay networks: An adaptive dynamic programming
    based approach. IEEE Internet of Things Journal. https://doi.org/10.1109/JIOT.2023.3321673
    Article   Google Scholar   Sun, R., Dai, Y., & Cheng, Q. (2023). An adaptive weighting
    strategy for multisensor integrated navigation in urban areas. IEEE Internet of
    Things Journal, 10(14), 12777–12786. https://doi.org/10.1109/JIOT.2023.3256008
    Article   Google Scholar   Thantharate, P., et al. (2023). GREENSKY: A fair energy-aware
    optimization model for UAVs in next-generation wireless networks. Green Energy
    and Intelligent Transportation. https://doi.org/10.1016/j.geits.2023.100130 Article   Google
    Scholar   Anitha, P., et al. (2024). Comprehensive review on congestion detection,
    alleviation, and control for IoT networks. Journal of Network and Computer Applications.
    https://doi.org/10.1016/j.jnca.2023.103749 Article   Google Scholar   Gebremariam,
    G. G., et al. (2023). Secure localization techniques in wireless sensor networks
    against routing attacks based on hybrid machine learning models. Alexandria Engineering
    Journal, 82, 82–100. https://doi.org/10.1016/j.aej.2023.09.064 Article   Google
    Scholar   Download references Author information Authors and Affiliations Faculty
    of Engineering and Technology, Siksha ‘O’ Anusandhan University, Bhubaneswar,
    Odisha, India Rahul Priyadarshi Corresponding author Correspondence to Rahul Priyadarshi.
    Ethics declarations Conflict of interest There is no conflict of interest among
    the authors. Additional information Publisher''s Note Springer Nature remains
    neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society
    or other partner) holds exclusive rights to this article under a publishing agreement
    with the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Priyadarshi, R. Exploring machine learning solutions for overcoming
    challenges in IoT-based wireless sensor network routing: a comprehensive review.
    Wireless Netw (2024). https://doi.org/10.1007/s11276-024-03697-2 Download citation
    Accepted 06 February 2024 Published 29 February 2024 DOI https://doi.org/10.1007/s11276-024-03697-2
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords IoT Wireless sensor networks Routing Machine learning Supervised
    learning Unsupervised learning Reinforcement learning Use our pre-submission checklist
    Avoid common mistakes on your manuscript. Sections Figures References Abstract
    Introduction Literature review Machine learning techniques in WSN routing Challenges
    and considerations in ML-based routing Case studies and applications Comparison
    evaluation metrics and performance analysis Future directions and research opportunities
    Conclusion References Author information Ethics declarations Additional information
    Rights and permissions About this article Advertisement Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Wireless Networks
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Exploring machine learning solutions for overcoming challenges in IoT-based
    wireless sensor network routing: a comprehensive review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Abbas S.
  - Bouazzi I.
  - Ojo S.
  - Al Hejaili A.
  - Sampedro G.A.
  - Almadhor A.
  - Gregus M.
  citation_count: '0'
  description: The Internet of Things (IoT), considered an intriguing technology with
    substantial potential for tackling many societal concerns, has been developing
    into a significant component of the future. The foundation of IoT is the capacity
    to manipulate and track material objects over the Internet. The IoT network infrastructure
    is more vulnerable to attackers/hackers as additional features are accessible
    online. The complexity of cyberattacks has grown to pose a bigger threat to public
    and private sector organizations. They undermine Internet businesses, tarnish
    company branding, and restrict access to data and amenities. Enterprises and academics
    are contemplating using machine learning (ML) and deep learning (DL) for cyberattack
    avoidance because ML and DL show immense potential in several domains. Several
    DL teachings are implemented to extract various patterns from many annotated datasets.
    DL can be a helpful tool for detecting cyberattacks. Early network data segregation
    and detection thus become more essential than ever for mitigating cyberattacks.
    Numerous deep-learning model variants, including deep neural networks (DNNs),
    convolutional neural networks (CNNs), and recurrent neural networks (RNNs), are
    implemented in the study to detect cyberattacks on an assortment of network traffic
    streams. The Canadian Institute for Cybersecurity’s CICDIoT2023 dataset is utilized
    to test the efficacy of the proposed approach. The proposed method includes data
    preprocessing, robust scalar and label encoding techniques for categorical variables,
    and model prediction using deep learning models. The experimental results demonstrate
    that the RNN model achieved the highest accuracy of 96.56%. The test results indicate
    that the proposed approach is efficient compared to other methods for identifying
    cyberattacks in a realistic IoT environment.
  doi: 10.7717/peerj-cs.1793
  full_citation: '>'
  full_text: '>

    "Related research Share X Facebook Email PeerJ Computer Science Evaluating deep
    learning variants for cyber-attacks detection and multi-class classification in
    IoT networks Research articleAlgorithms and Analysis of AlgorithmsArtificial IntelligenceSecurity
    and PrivacyNeural NetworksInternet of Things Sidra Abbas1, Imen Bouazzi2, Stephen
    Ojo3, Abdullah Al Hejaili4, Gabriel Avelino Sampedro5,6, Ahmad Almadhor7, Michal
    Gregus8 January 16, 2024Read the peer review reports Author and article information
    Abstract   The Internet of Things (IoT), considered an intriguing technology with
    substantial potential for tackling many societal concerns, has been developing
    into a significant component of the future. The foundation of IoT is the capacity
    to manipulate and track material objects over the Internet. The IoT network infrastructure
    is more vulnerable to attackers/hackers as additional features are accessible
    online. The complexity of cyberattacks has grown to pose a bigger threat to public
    and private sector organizations. They undermine Internet businesses, tarnish
    company branding, and restrict access to data and amenities. Enterprises and academics
    are contemplating using machine learning (ML) and deep learning (DL) for cyberattack
    avoidance because ML and DL show immense potential in several domains. Several
    DL teachings are implemented to extract various patterns from many annotated datasets.
    DL can be a helpful tool for detecting cyberattacks. Early network data segregation
    and detection thus become more essential than ever for mitigating cyberattacks.
    Numerous deep-learning model variants, including deep neural networks (DNNs),
    convolutional neural networks (CNNs), and recurrent neural networks (RNNs), are
    implemented in the study to detect cyberattacks on an assortment of network traffic
    streams. The Canadian Institute for Cybersecurity’s CICDIoT2023 dataset is utilized
    to test the efficacy of the proposed approach. The proposed method includes data
    preprocessing, robust scalar and label encoding techniques for categorical variables,
    and model prediction using deep learning models. The experimental results demonstrate
    that the RNN model achieved the highest accuracy of 96.56%. The test results indicate
    that the proposed approach is efficient compared to other methods for identifying
    cyberattacks in a realistic IoT environment. Cite this as Abbas S, Bouazzi I,
    Ojo S, Al Hejaili A, Sampedro GA, Almadhor A, Gregus M. 2024. Evaluating deep
    learning variants for cyber-attacks detection and multi-class classification in
    IoT networks. PeerJ Computer Science 10:e1793 https://doi.org/10.7717/peerj-cs.1793
    Main article text   Introduction The rapid advancement of connected technologies
    has contributed to deploying many Internet of Things (IoT) devices in numerous
    applications (Javed et al., 2022). Simultaneously, the issue of cyberattacks has
    become more challenging to address. Because most IoT gadgets have extremely low
    resources (i.e., computing and storage capability), they cannot implement sophisticated
    intrusion detection systems. Global smartphone data stream reportedly climbed
    by roughly 71% in 2017 compared to the previous year, and by 2022, nearly all
    cellular traffic data is expected to come from smart gadgets (Cisco, 2019). Compared
    to 2013, there was a roughly sevenfold rise in the amount of IoT viruses in 2017.
    Many Mirai-infected IoT gadgets concurrently launched a cyberattack with a peak
    bandwidth of 620 Gbps (Spamhaus Malware Labs, 2020). In 2018, 10,263 distinct
    IoT gadgets hosting botnets were found (Haider et al., 2016). The year 2017 witnessed
    the discovery of “IoTroop,” a further distributed denial of service (DDoS) assault
    originating from exploited IoT devices. During this significant assault that targeted
    the financial sectors, 13,000 IoT gadgets were utilized. Today’s sophisticated
    and well-equipped cyber-criminals can successfully assault organizations like
    governments and enterprises (Mustafa et al., 2020). Currently, cybercrime is a
    massive industry with massive amounts of stolen data. Malware can be divided into
    numerous groups (Behal & Kumar, 2017). This entails a significant danger for all
    governments, corporations, and customers worldwide. We do not have to think back
    very far to recall the big attack on a Bangladeshi bank, during which an estimated
    USD 81 million was stolen. The fact that the bank’s systems were utilized to move
    huge quantities of money is an ongoing indicator of how successful such attacks
    can be. No business, regardless of size, is secure. According to statistics, 20%
    of impacted companies tumble into the compact business class, 33% into the SME
    class, and 41% into the big corporation class. The greater the hazard, the more
    crucial it is to be informed of the problems and safeguard sensitive data. Eighty-two
    percent of businesses have experienced at least three data-stealing attempts that
    have rendered their customers’ services useless. The organizations impacted by
    DDoS assaults outlined a 26% decline in operational efficiency and a 41% disruption
    of the targeted services (Behal & Kumar, 2017). The attacks on malware networks,
    such as Bashlite, Mirai, Hajime, and others, provide the biggest challenge to
    IoT security. Network attacks include DDoS, pushing, identity theft, data leaking,
    and keylogging (Amini, Araghizadeh & Azmi, 2015). The botmasters perform distant
    network mapping for Operating System fingerprinting (OS fingerprinting), information
    collecting, and port scanning to locate their vulnerabilities and compromise these
    gadgets. Botnets launched most DDoS assaults to disturb services and blocked reputable
    users from using them (Koroniotis et al., 2019). DDoS attacks are possible in
    the IoT setting at the network and application layers (Hassija et al., 2019).
    Stealing data and confidentiality issues are distinct security risks in the IoT
    ecosystem’s application layer (Hassija et al., 2019). The study (Amin, Ahmad &
    Sang Choi, 2019) examined the difficulties for users of data exposure and privacy
    infringement. Malicious individuals frequently enhance their skills, alter their
    strategies, and use cutting-edge technologies to perpetrate DDoS attacks. DDoS
    attacks can be detected, prevented, or mitigated using various techniques, but
    nasty people constantly develop novel ways to get around existing defenses (Behal
    & Kumar, 2017). One of the major risks to the network nowadays is DDoS activity.
    Recent DDoS attacks at the application layer of web servers have expanded widely,
    costing targets a tonne of money (Jiang et al., 2017). Attacks on the TCP/IP layer
    cripple online servers and have a cap on the number of requests that can be made
    per second. This category includes DDoS attacks, Slowloris, and zero-day attacks
    that exploit Apache or Windows flaws (Yusof, Ali & Darus, 2018). The strategies
    proposed to understand DDoS assaults at the TCP/IP layer are merely a part of
    DDoS occurrences at the application layer. The formulas for the decisions that
    detect application-layer attacks are extremely complex. The lack of infrastructure
    to detect such attacks is a particular class of jobs in identifying a DDoS epidemic
    at the TCP/IP layer (Yadav & Subramanian, 2016). DDoS attacks are difficult to
    identify since they are quite diverse and often occur with other cyberattacks.
    The most effective way to prevent DDoS attacks is to take action immediately as
    they happen. The popularity of cyberattacks on internet-connected equipment has
    grown due to increased internet usage. Since ML and DL (Khuphiran et al., 2018),
    have started recognizing their enormous potential in various fields, industry
    and academia are exploring integrating ML and DL for cyberattack mitigation. Traditional
    approaches for risk identification need to be more accurate and respond gradually.
    Threats can be identified more effectively and precisely using ML techniques like
    random forest (Farnaaz & Jabbar, 2016), KNN (Li & Guo, 2007), and naive Bayesian
    (Panda & Patra, 2007). The subject matter specialist must select the characteristics
    for classification in machine learning. Within the deep learning model, feature
    selection takes place. A series of nonlinear types of layers are used by DL techniques
    like ANN (Li & Zhang, 2019), DNN, and RNN to learn different patterns from several
    labeled data. Therefore, DL can serve as a useful tool for cyberattack detection.
    After thoroughly evaluating the possibilities, we decided to use the DNN, CNN,
    and RNN Deep Learning frameworks in our experiment to detect cyberattacks. Experimental
    results show that it serves the intended purpose admirably. Contribution This
    research aimed to identify cyberattacks in IoT environments using deep-learning
    models on the CICDIoT2023 dataset. The research’s main contribution is given as
    follows. This research proposed multiple variants of deep learning models to detect
    cyberattacks in realistic IoT environments on various samples of network stream
    packets. This study contributes to implementing an IoT cyberattacks dataset from
    CICIoT Dataset 2023, an actual dataset and standard for massive attacks in the
    Internet of Things environment. This research utilizes 47 features and 33 attacks
    for assessment. This research uses robust scalar and label encoding techniques
    to preprocess the data and compare variants of deep learning models (DNNs, CNNs
    and RNNs). The research evaluates the performance of the cyberattack detection
    system using different evaluation metrics. The outcomes show that the proposed
    approach is effective in accurately predicting cyberattacks. Organization The
    article’s organization is as follows: “Related Work” contains a study on relevant
    ML and DL techniques for cyberattack detection. “Proposed Methodology” describes
    the proposed approach that uses the CIC IoT dataset, data preparation, and deep
    learning models. “Results and Discussion” explains and discusses the findings.
    “Conclusion and Future Scope” contains the study’s conclusion and recommendations
    for further investigation. Related work Since the growing need for IoT automated
    network systems, IoT model complexity is increasing regularly. Because the gadgets
    broadcast data using wireless technology, they are considered easy targets. Every
    day, thousands of attacks surface due to the inclusion of new IoT protocols, which
    frequently cause the computing process to become worse, more unstable, and ineffective.
    The typical communication assault on a local network is limited to local nodes
    or tiny local domains. The assault found in IoT devices, however, spreads to a
    wide region and has disastrous effects. IoT security is harder and more intricate
    due to the heterogeneity and spread of IoT applications and service providers
    (Babu & Veena, 2021). In a study (Almaraz-Rivera, Perez-Diaz & Cantoral-Ceballos,
    2022), the author builds a unique IDS based on ML and DL models, tackling the
    class imbalance issue in the Bot-IoT dataset. Researchers employed three alternative
    assortments of features for binary and multi-class categories to assess how the
    timings of the observations affect the predicts, which allowed the elimination
    of feature connections, which the Argus flow information encoder provided, while
    still attaining a median accuracy of >99%. According to the stacking generalization
    principle, the study (Dutta et al., 2020) proposes an ensemble technique that
    uses deep models and a meta-classifier. The method employs a two-step procedure
    for detecting network irregularities to increase the capabilities of the proposed
    methodology. In Shurman, Khrais & Yateem (2020), the author proposed two methods
    for identifying DDoS assaults in IoT systems. The first approach detects IoT-DoS
    attacks using an IDS. The second approach uses DL models, namely LSTM networks
    built with the most recent dataset for DDoS assaults of this sort. Our test findings
    show that the proposed approaches can identify abnormal behavior, protecting the
    Internet of Things network from DoS and DDoS assaults. The author of study (Saba
    et al., 2022) offers a CNN-based approach for an intrusion-based IDS system, which
    effectively uses the IoT’s potential and allows users to investigate all IoT traffic
    effectively. The model’s accuracy was 99.51% during training and 92.85% during
    testing. A DDoS detection strategy based on ML approaches is proposed in an article
    (Seifousadati, Ghasemshirazi & Fathian, 2021) using the CICDDoS2019. It tested
    the most well-liked ML techniques and identified attributes most connected with
    projected classes. It was found that XGBoost and AdaBoost performed exceptionally
    well, accurately predicting internet traffic with an accuracy rate of 100%. The
    study (Alzahrani & Alzahrani, 2021) uses the CICDDoS2019 datasets to analyze the
    effectiveness of detection for DDoS assaults by implementing various ML methods
    in WEKA software. Using the random forest (RF) and decision tree (DT) methods
    produced the highest accuracy results in the assessment at 99%. However, it performs
    better because the DT computes in 4.53 s compared to 84.2 s for the RF. In research
    (Abu Al-Haija & Zein-Sabatto, 2020), the authors present the thorough creation
    of a novel smart and automated deep-learning-based identification and categorization
    model for cyberattacks in IoT communication systems that use CNN capabilities.
    The simulation results showed that the proposed system had a superior 99.3% and
    98.2% accuracy in classifying cyber-attacks. The study’s author, Pei, Chen & Ji
    (2019), proposes a machine learning-based DDoS assault identification process
    for extracting features and model assessment. Evaluating the data packages classified
    according to criteria enables the feature extraction stage to extract the DDoS
    attack traffic features with a high percentage. The machine learning model identification
    step uses the retrieved features as input characteristics, and the assault prediction
    model is developed using the random forest method. The trials’ findings show that
    the machine learning-based DDoS assault detection method effectively detects current
    DDoS assaults at a rate compatible with its detection rate. In a study (Banitalebi
    Dehkordi, Soltanaghaei & Boroujeni, 2021), the author proposed a novel approach
    for identifying DDoS assault. The findings indicate that this method exceeds its
    competitors’ accuracy, attaining 99% for identifying DDoS assault in SDN. In Saghezchi
    et al. (2022), the author used machine learning (ML) to build various statistical
    models for broadband anomaly recognition and DDoS assault detection. The research
    uses network traffic information gathered from a real-world semiconductor manufacturing
    facility. The findings demonstrate that supervised techniques outperform unsupervised
    and semi-supervised counterparts regarding detection efficiency. Specifically,
    the decision tree model limits the false positive rate to 0.001 and achieves an
    accuracy of 0.999. In a study (Chen et al., 2020), the author proposes an IoT
    DDoS attack surveillance system with multiple layers, including IoT gadgets, pathways,
    SDN toggles, and cloud servers. The authors first constructed eight smart poles
    on the campus with various sensors to acquire sensor data for datasets. The authors
    then extract the properties depending on different forms of DDoS attacks. According
    to the experimental findings, DDoS attacks can be precisely detected through the
    multi-layer DDoS surveillance system. In Kumari & Mrunalini (2022), a computational
    model for DDoS assault is presented. ML algorithms like LR and NB are implemented
    to identify attacks and typical situations. The pilot investigation utilizes the
    CAIDA 2007 Dataset. This study implements the Weka data mining platform, and the
    outputs of that platform are contrasted and assessed. In a study (Kumar et al.,
    2023), a framework based on “LSTM” is developed to identify DDoS assault in an
    instance of transmitted network streams. In the present work, the proposed system
    attained an accuracy of up to 98%. This study (Elsaeidy, Jamalipour & Munasinghe,
    2021) develops a hybrid DL approach to identify DDoS and replication assaults
    on an actual intelligent city system. The effectiveness of the proposed hybrid
    model is assessed using realistic DDoS and replication assaults on real-world
    intelligent city datasets. For the environmental dataset (98.37%), intelligent
    river dataset (98.13%), and intelligent soil dataset (99.51%), the proposed model
    demonstrated excellent prediction rates. The study (Anwer et al., 2021) suggests
    a paradigm for identifying illegal internet traffic. The framework achieves much
    higher accuracy (85.34%) using ML classification-based techniques for harmful
    network traffic identification. The proposed framework was applied to the NSL
    KDD dataset. Subsequently, several methods for detecting cyberattacks utilizing
    ML and DL techniques are presented in Table 1. However, they are constrained in
    their ability to perform more effectively due to their unwillingness to select
    characteristics and extraction techniques. This work provided an effective method
    for identifying cyberattacks using several deep-learning model variants. Table
    1: Summary of existing related work. Reference Focus Technique Limitation Neto
    et al. (2023) Cyberattacks detection RF Low performance Kumar et al. (2023) Cyberattacks
    detection LSTM Low performance Elsaeidy, Jamalipour & Munasinghe (2021) Cyberattacks
    detection Hybrid ML model Low performance Anwer et al. (2021) Cyberattacks detection
    ML model Low performance Abu Al-Haija & Zein-Sabatto (2020) Cyberattacks detection
    CNN Low performance DOI: 10.7717/peerj-cs.1793/table-1 Proposed methodology The
    proposed approach entails several steps, such as dataset collection from the CIC
    repository, data pre-processing, and three model building to predict cyberattacks.
    Figure 1 depicts the overview of the proposed architecture. Figure 1: Proposed
    architecture overview.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-1
    Experimental dataset This study used the CICIoT2023 Dataset, an actual dataset
    and benchmark for massive attacks in the Internet of Things environment. There
    are two distinct varieties of files for the CICIoT2023 dataset: pcap and csv.
    The initial data generated and gathered in various scenarios inside the CIC IoT
    network is in pcap files. All sent packets are included in these files, which
    can be used to design additional functionality. A fixed-size packet window summarizes
    the features taken from the original pcap files and is added to those files. A
    series of packets delivering data between two hosts is used to extract the features
    (Neto et al., 2023). This research utilizes the 47 features for assessment. Using
    the retrieved features, we aggregate the values recorded in intervals size of
    100 (Mirai UDPPlain, Mirai Greeth Flood, Mirai GREIP Flood, DoS UDP Flood, DoS
    TCP Flood, DDoS UDP Flood, DoS HTTP Flood, DDoS TCP Flood, DoS SYN Flood, DDoS
    UDP Fragmentation, DDoS SYN Flood, DDoS HTTP Flood, DDoS SynonymousIP Flood, DDoS
    RSTFIN Flood, DDoS ICMP Fragmentation, DDoS ICMP Flood, DDoS SlowLoris, DDoS ACK
    Fragmentation and DDoS PSHACK Flood) and 10 (Dictionary brute force, Ping Sweep,
    Vulnerability Scan, Host Discovery, Backdoor Malware, Command Injection, OS Scan,
    SQL Injection, DNS spoofing, XSS, Browser Hijacking, Uploading Attack, MITM ARP
    spoofing, Benign Traffic and Port Scan) packages to reduce fluctuating data sizes
    (such as Command_Injection and DDoS). Subsequently, the csv file dataset is used
    for this research. The generated csv datasets indicate the features of every data
    block. Additionally, each attack used in this research has unique properties.
    For instance, a DDoS assault generates more network streams than a spoofing assault,
    usually less (Neto et al., 2023). Data preprocessing The model’s performance is
    enhanced, and more accurate features are produced due to the critical data preparation
    phase. In this stage, categorical data is preprocessed into integer values using
    a robust scalar and label encoding technique. Robust scalar: This study uses the
    data preparation method RobustScaler. Compared to other scaling techniques like
    the StandardScaler or MinMaxScaler, which are more susceptible to outliers, the
    RobustScaler is specifically created to scale the characteristics of a dataset.
    The RobustScaler scales the data utilizing the median and Interquartile range
    (IQR), as opposed to the mean and standard deviation used by the StandardScaler.
    The median is less dependent on outliers than the mean, while the IQR is a measure
    of the data’s dispersion that is likewise less affected by outliers. Label encoder:
    Label encoding renders the input numerical labels into a machine-learning algorithm
    (Sharma et al., 2020). Data splitting: After performing the data preprocessing
    steps, the models are built. The data is divided into training and testing data
    with a ratio of 80% training data and 20% testing data to enhance accuracy and
    efficacy for this phase. After splitting, the model is trained using the different
    variants of the deep learning model. Deep neural network This study inspected
    DNN’s aptitude for spotting cyberattacks in IoT settings. According to Deng &
    Yu (2014), the deep learning method incorporates the learning area that employs
    irregular data in numerous stages through organizational frameworks. Deep learning
    synthesizes graphical design, neural networks, and pattern recognition. The deep
    learning model performs well for prediction on large data sets. According to Huang
    et al. (2013), DNNs are exceptional to other machine learning classifiers. The
    proposed deep learning technique examines the organic patterns on a sample of
    network stream packets. Additionally, deep learning logically supports multi-task
    training, which uses a single-layer deep neural network to consider all building
    forms’ properties. This network mainly consists of self-learning units with two
    or more layers. DNN uses hidden units among the input and output layers. The hidden
    unit p can use a logistic function to translate the scalar variable y q of subsequent
    layers to the input x q below. In a DNN network, (4) and (5) can predict the output
    of i th neuron y i as following Eqs. (1) and (2): (1) y i =f( ξ i ), (2) f( ξ
    i )=ϑ+ Σ hε τ i −1 W i X j , f( ξ i ) represents the transfer function and ξ i
    is the potential of i th-neuron. The transfer function is shown in the following
    Eq. (3): (3) f( ξ i )= 1 1+exp(− ξ i ) The sum of squared errors can represent
    the entire objective cost function where the output neurons are used to determine
    the target values, y o , and y o ˆ as shown in Eq. (4). (4) C=Σ1/2( y o − y o
    ˆ ) Different variants of DNN models are utilized for this research. This study
    employs a sequential DNN1 algorithm with a single input layer. The first input
    layer has 256 units and employs the relu activation method. The hidden layer comprises
    two dense and one dropout layer. The dense layer comprises 256 and 34 units with
    relu and softmax activation functions, and the dropout layer has 256 units. The
    output layer is the next activation function, relu and softmax, used to tackle
    the categorical categorization obstacle. For the next DNN variant model, this
    study uses the sequential DNN2 model. The input layer is 1 with 256 units and
    employs the same method as DNN1. The hidden layer comprises two dense and two
    dropout layers. The dropout layers comprised 256 and 128 units. The dense layer
    contains 128 and 34 units with relu and softmax activation functions, respectively.
    This research uses the sequential DNN3 algorithm with one input layer as a third
    DNN variant model. The input layer is 1 with 256 units and employs a similar method
    as DNN1. The hidden layer is next, which comprises three dense and three dropout
    layers. The dropout layers comprised 256, 128 and 64 units. The dense layer contains
    128, 64 and 34 units with relu and softmax activation functions, respectively.
    The DNN model has utilized Adam as an optimizer to compute and decrease the damage
    utilizing sparse_categorical_crossentropy. Convolutional neural network Convolutional
    neural network(CNN) is a subclass of deep neural networks that have demonstrated
    exceptional performance in several computer vision applications. This research
    uses the CNN model for text classification. The CNN model comprises three consequential
    layers: a max-pooling layer, a convolutional layer, and a fully connected layer.
    Different variants of CNN models are utilized for this research. This study uses
    a sequential CNN1 model with a single input layer. The model has 10 layers: one
    convolutional layer, three Leaky_ReLU layers, one max-pooling layer, one dropout
    layer, three dense layers, and one flattened layer. The convolutional block is
    merged with a 1D convolutional neural network, one Leaky_ReLU layer, one max-pooling
    layer, and a dropout layer with a 30% dropout rate. The consequent attribute maps
    tend to be flattened after the convolutional and pooling layers. Following flattening,
    the attributes proceed through two Leaky_ReLU layers, three dense layers with
    64, 32, and 34 units using the softmax function. This research uses the sequential
    CNN2 model as a second CNN variant model. The model has 14 layers: two convolutional
    layers, four Leaky_ReLU layers, two max-pooling layers, two dropout layers, three
    dense layers, and one flattened layer. The convolutional block is merged with
    a 1D convolutional neural network, Leaky_ReLU layer, max-pooling layer, and a
    dropout layer with a 30% dropout rate. After the convolutional and pooling layers,
    subsequent attribute maps often become flat. Following flattening, the attributes
    proceed through two Leaky_ReLU layers, three dense layers with 64, 32, and 34
    units using the softmax function. This study uses the sequential CNN3 model for
    the next CNN variant model. The model has 18 layers: three convolutional layers,
    five Leaky_ReLU layers, three max-pooling layers, three dropout layers, three
    dense layers, and one flattened layer. The convolutional block is merged with
    a 1D CNN, Leaky_ReLU layer, max-pooling layer, and a dropout layer with a 30%
    dropout rate. The consequent attribute maps tend to be flattened after the convolutional
    and pooling layers. Following flattening, the attributes proceed through two Leaky_ReLU
    layers, three dense layers with 64, 32, and 34 units using the softmax function.
    The output layer employed the Adam optimizer in the final section to compute and
    reduce the loss employing categorical_crossentropy. Recurrent neural network A
    recurrent neural network (RNN) is a genre of neural network architecture constructed
    to work with data sequences. RNNs retain a hidden state that gathers data from
    earlier time steps in the sequence, unlike standard feedforward neural networks,
    which analyze each data point independently (Alzubaidi et al., 2021). Various
    variants of RNN models are utilized for this research. This study uses a sequential
    RNN1 system with a single input layer. The shape of the input layer is 1 with
    32 units, and the relu activation function is used. The dropout regularisation
    is used with the input layer. Next, add the output layers comprising one flattened
    layer, three dense layers and two Leaky_ReLU layers. The dense layer with 34,
    16 and 32 utilizes the activation functions softmax and a fully connected layer.
    The sequential RNN2 algorithm that utilizes a single input layer is the following
    RNN variant. The input layer used the same method and function as RNN1 variants.
    The dropout regularisation is used with the input layer. Next, add the second
    RNN layer with 32 units and use the dropout regularisation layer. The output layers
    comprise one flattened layer, three dense layers and two Leaky_ReLU layers. The
    dense layer with 34, 16 and 32 utilizes the activation functions softmax and a
    fully connected layer. This study uses the sequential RNN3 model with one input
    layer as a third DNN variant model. The input layer used the same method and function
    as RNN1 variants. The output layers comprise one flattened layer, three dense
    layers and two Leaky_ReLU layers. The dense layer with 32, 16, and 34 uses the
    softmax activation functions and is a fully connected layer. The output layer
    employed the Adam optimizer in the final section to compute and reduce the loss
    employing categorical_crossentropy. Algorithm 1 illustrates the method of the
    proposed model for cyberattack detection. A proposed method for anticipating cyberattacks
    in an IoT environment using deep learning techniques is described in the presented
    methodology. The algorithm proceeds by specifying the input as cyberattack data
    and the intended output as predictors of cyberattacks. We propose a function called
    CICIoTDataset that performs data preprocessing operations on the data, such as
    employing the RobustScaler for scaling features and label encoding for categorical
    labels to a numerical form. The processed data is returned with the symbols x
    (features) and y (labels). The function known as TrainModel is defined. To do
    so, the data are divided into training and testing sets ( x t rain, x t est, y
    t rain, y t est ). Next, three different kinds of NN models are built: a DNN,
    a CNN, and a RNN. It then returns the created model . The DNN and RNN model used
    the hyperparameter keras.backend.clear_session() and hidden initializer random_uniform(SEED)
    . It is explained how DDoSPrediction works. An iterative process involves a specified
    number of epochs E_p and batch sizes B_s . The model is applied to generate predictions
    (x) from the input data Data for every epoch and batch size. The crossentropy
    loss is determined between the actual labels X and the expected results. After
    that, the algorithm outputs the expected outcomes. Algorithm 1 : Proposed algorithm
    for cyberattacks detection model. 1: Input: CICIoT Dataset 2: Output: Cyberattack
    Prediction 3: Dp = Data Preprocessing 4: a) Robust Scalar 5: b) Label Encoding
    6: (x, y) ← D p 7: Train Model (x, y) 8:    Splitting = x_train,x_test,y_train,y_test
    9:    DNN = Sequential DNN() 10:   CNN = Sequential CNN(), keras.backend.clear_session(),
    random_uniform(seed=SEED) 11:   RNN = Sequential RNN(), keras.backend.clear_session(),
    random_uniform(seed=SEED) 12: Return model 13: DDoSPrediction (model) 14: for
    every E_p epochs 15:     for every B_s in the batch-size 16:         x = model(Data);
    17:         Loss = cross_entropy, compute loss 18: Return ← predicted result DOI:
    10.7717/peerj-cs.1793/table-6 Results and discussion We evaluate the effectiveness
    of the proposed model on a CICIoT2023 dataset. The evaluation metrics used are
    listed below to predict a proposed methodology. Accuracy: Accuracy is an important
    assessment parameter used in performance evaluation because it is the ratio of
    effectively accurate predictions to all positive predictions made by the model.
    This value is proportionally illustrated by Eq. (5), which facilitates comprehension
    of the metric conceptual equation. (5) Accuracy= TP+TN TP+TN+FP+FN . Precision:
    The precision of a model or system shows the accuracy with which it predicts the
    positive class. It indicates the confidence level regarding the model’s capacity
    to generate positive predictions and conveys its accuracy. Equation (6) illustrates
    this value proportionally, making the metric conceptual equation easy to understand.
    (6) Precision= TP TP+FP . Recall: Recall, often called sensitivity, provides an
    assessment measure focusing on the ratio of all positive cases to the proportion
    of accurate positive predictions. This balanced perspective provides a unique
    advantage during estimating, as Eq. (7) computation demonstrates. This formula
    illustrates the usefulness of recall as an accessible gauge for evaluating model
    performance. (7) Recall= TP TN+FN F1-score: The appropriately titled F1 score
    serves as a harmonic mean of memory and precision since it may efficiently convey
    the essence of balanced performance. Combining these two measures results in the
    F1-score, a widely used estimate of model performance that is particularly helpful
    in assessment. Equation (8), which appears complex but offers much insight, accurately
    describes this fundamental estimation computation. (8) F1−measure= 2×Precision+Recall
    Precision+Recall Deep neural network The results of a study on the performance
    of different deep neural network (DNN) models are presented in Table 2. Several
    significant performance measures were used in the evaluation to determine these
    models’ effectiveness in classification tasks. The table thoroughly summarizes
    the findings, facilitating comprehension of each model’s performance on 33 cyberattacks.
    Model DNN1 achieved 87.42% accuracy, 86.94% precision, 87.42% recall, and 86.26%
    F1-score. Model DNN2 demonstrated 84.73% accuracy, 85.69% precision, 84.73% recall,
    and 84.39% F1-score. Model DNN3 exhibited 88.64% accuracy, 91.20% precision, 88.64%
    recall, and an F1-score of 88.51%. Concerning accurate classification, handling
    false positives and negatives, and achieving a balance between accuracy and recall,
    these measures collectively provide insights into the effectiveness of each DNN
    model. In contrast to the other models, Model DNN2 performs poorly in accuracy,
    with Model DNN3 emerging as the best model in precision and F1-score. Table 2:
    Result of deep neural network. Models Accuracy Precision Recall F1-score DNN1
    87.42 86.94 87.42 86.26 DNN2 84.73 85.69 84.73 84.39 DNN3 88.64 91.20 88.64 88.51
    DOI: 10.7717/peerj-cs.1793/table-2 Figure 2 visualizes the results of the DNN1
    model. Figure 2A graph illustrates the accuracy for both training and validation.
    The training accuracy was 0.676% at the 0 th epoch and ranged among drops and
    boosts until roughly 0.851% at the end. Validation accuracy starts at 0.775% at
    the 0 th epoch and varies among losses and gains until it comes near 0.875% at
    the last epoch. Figure 2B loss curve shows the training and validation loss. training
    loss started at 0.799% and fell to 0.01% at the last epoch. Validation loss started
    at 0.0299% at the 0 th epoch and fell to 0.01% by the 30 th epoch. Figure 2: Visualization
    of DNN1 model.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-2 Figure
    3 demonstrates the results of the DNN2 model. Figure 3A graph displays the training
    and validation accuracy. the training accuracy was 0.651% , and it varied between
    drops and gains until about 0.848% at the 0 th epoch. Validation accuracy ranges
    among losses and gains until it peaks at 0.848% at the last epoch. Figure 3B graph
    displays the training and validation loss. Training loss decreased to 0.10 , and
    validation loss remained 0.001% . Figure 3: Graphical representation of DNN2 model.   Download
    full-size image DOI: 10.7717/peerj-cs.1793/fig-3 Figure 4 represents the outcomes
    of the DNN3 model. Figure 4A graph represents the training and validation accuracy.
    The training accuracy peaked at about 0.87% accuracy at the last epoch. Validation
    accuracy peaked at 0.875% accuracy at the last epoch. Figure 4B presents the training
    and validation loss. Training loss decreased to 0.01 at the last epoch. Validation
    loss remains the same 0.01% at the last epoch. Figure 4: Graphical visualization
    of DNN3 model.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-4 Convolutional
    neural network Table 3 demonstrates the performance results of various convolutional
    neural network (CNN) models in a classification assessment of 33 cyberattacks
    in an IoT environment. Experiments have been conducted on CNN1, CNN2, and CNN3
    models. The proportion of effectively classified occurrences among all instances
    is how accurately a classification is made. CNN3 exhibited the highest accuracy,
    scoring 96.37%, ahead of CNN2 (94.30%) and CNN1 (95.49%). Precision measures the
    percentage of precise positive predictions of favorable outcomes. CNN3 had the
    best precision, scoring 96.15%, while CNN2 and CNN1 followed in at 94.74% and
    95.17%, respectively. A recall measures the proportion of real positive instances
    correctly anticipated as positive. Recall for CNN3 was 96.37%, CNN1 was 95.49%,
    and CNN2 was 94.30%. The F1-score is a balanced indicator of a model’s performance
    because it is the harmonic mean of precision and recall. CNN1 had an F1-score
    of 94.48 percent, CNN3 had a 95.51%, and CNN2 had 93.36%. Based on these outcomes
    and the offered assessment metrics, the CNN3 model performs best for cyberattack
    detection. Table 3: Result of convolutional neural network. Models Accuracy Precision
    Recall F1-score CNN3 96.37 96.15 96.37 95.51 CNN2 94.30 94.74 94.30 93.36 CNN1
    95.49 95.17 95.49 94.48 DOI: 10.7717/peerj-cs.1793/table-3 Figure 5 demonstrates
    the results of the CNN1 model. Figure 5A shows the training and validation accuracy
    graph. The training accuracy was 0.68% at the 0 th epoch and fluctuated between
    falls and increases until roughly 0.94% at the 30 th epoch. Validation accuracy
    starts at 0.76% at the 0 th epoch and varies between gains and losses until it
    comes near 0.95% at the 30 th epoch. The graph in Figure 5B represents the training
    and validation loss. Training loss is about 0.01 at the last epoch. Validation
    loss decreased to 0.01% at the last epoch. Figure 5: Graphical visualization of
    CNN1 model.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-5 Figure
    6 represents the outcomes of the CNN2 model. Figure 6A depicts the training and
    validation accuracy graph. Training accuracy rises to 0.94% , and validation accuracy
    is 0.73% . The training and validation loss is represented on the graph in Fig.
    6b. Training loss decreases to 0.01 , and validation loss decreases to 0.01% .
    Figure 6: Graphical representation of CNN2 model.   Download full-size image DOI:
    10.7717/peerj-cs.1793/fig-6 Figure 7 demonstrates the results of the CNN3 model.
    The training and validation accuracy is represented by the graph in Fig. 7a. The
    training accuracy is 0.90% ; following several cycles of gains and losses, it
    goes up about 0.955% . Validation accuracy is 0.94% . The training and validation
    loss is depicted on the graph in Fig. 7b. Training loss decreased to 0.13 , and
    validation loss peaked at about 0.10% . Figure 7: Graphical visualization of CNN3
    model.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-7 Recurrent neural
    network The comparison analysis of three RNN models, RNN1, RNN2, and RNN3, is
    shown in Table 4. The RNN1 model had a 96.52% accuracy rate. It had a high level
    of precision (96.25%), showing that a substantial amount of the positive predictions
    were accurate. A sizable percentage of effective captures of positive experiences
    is indicated by the recall value (96.52%). The F1-score (95.73%) indicates an
    adequate balance between recall and precision, letting the model perform well.
    A 96.01% accuracy was attained using the RNN2 model. A good capacity for making
    accurate positive predictions is indicated by the precision number (95.77%). This
    model detected many positive cases, with a recall value of 96.00%. The F1-score
    (95.65%) indicates a balanced trade-off between recall and precision, which adds
    to its strong performance. The RNN3 model had a 95.89% accuracy rate. The accuracy
    (95.60%) shows a noteworthy capacity for making precise optimistic predictions.
    The model has successfully detected several positive instances, as indicated by
    the recall value (95.89%). The F1-score (95.03%) shows that precision and recall
    are often well-balanced. Table 4: Result of recurrent neural network. Models Accuracy
    Precision Recall F1-score RNN1 96.52 96.25 96.52 95.73 RNN2 96.00 95.77 96.00
    95.65 RNN3 95.89 95.60 95.89 95.03 DOI: 10.7717/peerj-cs.1793/table-4 Figure 8
    represents the results of the RNN1 model. Figure 8A graph represents the training
    and validation accuracy. Training accuracy is about 0.60% , and validation accuracy
    is 0.75% . The training and validation loss is depicted on the graph in Fig. 8b.
    Training loss seems to decrease to 0.01 , and validation loss to 0.01% . Figure
    8: Graphical visualization of RNN1 model.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-8
    Figure 9 demonstrates the results of the RNN2 model. The training and validation
    accuracy is represented by the curve in Fig. 9a. The training accuracy reaches
    about 0.94% accuracy at the last epoch. Validation accuracy increases to 0.95%
    accuracy at the last epoch. The training and validation loss is depicted on the
    graph in Fig. 9b. Training loss started from 0.24% at 0 th epoch and declined
    to 0.10 , and validation loss reached about 0.10% . Figure 9: Graphical visualization
    of RNN2 model.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-9 The
    RNN3 model is visualized in Fig. 10. Figure 10A graph represents the training
    and validation accuracy. The training accuracy is about 0.95% accuracy at the
    last epoch. Validation accuracy is 0.71% at the start and increases at 0.95% accuracy
    at the last epoch. Figure 10B shows the training and validation loss. Training
    loss decreased to 0.10 , and validation loss declined to 0.10% . Figure 10: Graphical
    visualization of RNN3 model.   Download full-size image DOI: 10.7717/peerj-cs.1793/fig-10
    Comparison with existing study Table 5 presents a comparison with the benchmark
    study (Neto et al., 2023). It can be seen that Neto et al. (2023) have an accuracy
    of 99.43%, a precision of 70.54%, a recall of 91.05%, and an F-score of 71.92%,
    whereas the proposed approach attained an accuracy of 96.52%, a precision of 96.25%,
    a recall of 96.52%, and an F-score of 95.73%. In this context, the Proposed Approach
    outperforms the base article regarding accuracy, precision, recall, and F-score,
    suggesting our approach is better for cyberattack detection in a realistic IoT
    environment. Table 5: Comparison of the proposed approach with benchmark study.
    Ref. Precision Recall F-score Neto et al. (2023) 70.44 83.15 71.40 Proposed approach
    96.52 96.52 95.73 DOI: 10.7717/peerj-cs.1793/table-5 Findings and discussion IoT
    has become widely used in various applications due to the rapid progress of interconnected
    technology. Concurrently, the threat of cyberattacks is becoming more difficult
    to resolve. The experiment is performed with multiple DL variants using the CICIoT2023
    to resolve this issue. Accuracy, precision, recall, and F1-measure optimization
    metrics are used to assess the model’s efficacy. The effectiveness of deep learning
    models, their capacity for generalization, and their significance are assessed
    using statistical analysis. “Model complexity” indicates the degree of intricacy
    and sophistication of a DL model’s structure and its capacity to find patterns
    and connections in the data. A model becomes more complex when additional parameters
    are included. A network’s properties increase with its number of neurons and layers.
    While parameter diversity adds complexity to the processing load, it also helps
    DL models recognize complex patterns in the data. This is often mitigated by normalization
    techniques such as batch normalization, weight deterioration, and dropout, which
    increase the complexity of the model. Numerous methods reduce model complexity
    by regularising the loss function and adding consequence terms. Discouraging extremely
    complex parameter values helps prevent overfitting. Multiple deep learning variants
    (DNNs, RNNs and CNNs) have increased the prediction performance. This study uses
    deep learning variants to address the cyberattacks in the IoT environment. The
    experiment results show that the proposed deep learning variants perform more
    accurately and efficiently than conventional techniques. The test findings show
    that the proposed approach is more successful than other cyberattack detection
    algorithms. Conclusion and future scope This study suggests that a deep learning
    model, which is more efficient compared to a machine learning model, can be used
    to categorize cyberattacks in an IoT environment. The DNNs, CNNs, and RNNs model
    was selected as a workable contender for this study because it incorporates feature
    extraction and selection into its model, making it preferable to crude machine
    learning techniques. In the current study, DNNs, CNNs, and RNNs were utilized
    to categorize threats and attacks using the CICIoT2023 dataset. Compared to traditional
    models, the RNN model, which is utilized as a deep learning model, has a high
    accuracy rate for cyberattack categorization of roughly 96.5%. Additionally, utilizing
    RNN and graph neural networks to identify cyberattacks guides future work on IoT
    malware detection. In the future, incremental training will be included by observing
    network streams. To upgrade the device with an innovative method of assault. Additionally,
    we will investigate whether it is possible to integrate our proposed model into
    real-time cyber-attack detecting systems. Supplemental Information Code for IoT
    Attack detection. DOI: 10.7717/peerj-cs.1793/supp-1   Download Additional Information
    and Declarations Competing Interests The authors declare that they have no competing
    interests. Author Contributions Sidra Abbas conceived and designed the experiments,
    performed the experiments, analyzed the data, performed the computation work,
    prepared figures and/or tables, authored or reviewed drafts of the article, and
    approved the final draft. Imen Bouazzi conceived and designed the experiments,
    performed the experiments, analyzed the data, performed the computation work,
    authored or reviewed drafts of the article, and approved the final draft. Stephen
    Ojo conceived and designed the experiments, performed the experiments, performed
    the computation work, authored or reviewed drafts of the article, and approved
    the final draft. Abdullah Al Hejaili conceived and designed the experiments, analyzed
    the data, performed the computation work, authored or reviewed drafts of the article,
    and approved the final draft. Gabriel Avelino Sampedro conceived and designed
    the experiments, performed the experiments, analyzed the data, performed the computation
    work, prepared figures and/or tables, authored or reviewed drafts of the article,
    and approved the final draft. Ahmad Almadhor conceived and designed the experiments,
    performed the experiments, analyzed the data, prepared figures and/or tables,
    authored or reviewed drafts of the article, and approved the final draft. Michal
    Gregus conceived and designed the experiments, prepared figures and/or tables,
    authored or reviewed drafts of the article, and approved the final draft. Data
    Availability The following information was supplied regarding data availability:
    The CIC IoT Dataset 2023 is available at https://www.unb.ca/cic/datasets/iotdataset-2023.html.
    Funding This work was supported by the Deanship of Scientific Research at King
    Khalid University through large group Research Project under grant number RGP2/470/44.
    The funder had a role in the conceptualization, study design and decision to publish
    and preparation of the manuscript. The funder had no role in data collection and
    analysis. References   Abu Al-Haija Q, Zein-Sabatto S. 2020. An efficient deep-learning-based
    detection and classification system for cyber-attacks in IoT communication networks.
    Electronics 9(12):2152 Almaraz-Rivera JG, Perez-Diaz JA, Cantoral-Ceballos JA.
    2022. Transport and application layer DDoS attacks detection to IoT devices by
    using machine learning and deep learning models. Sensors 22(9):3367 Alzahrani
    RJ, Alzahrani A. 2021. Security analysis of DDoS attacks using machine learning
    algorithms in networks traffic. Electronics 10(23):2919 Alzubaidi L, Zhang J,
    Humaidi AJ, Al-Dujaili A, Duan Y, Al-Shamma O, Santamaría J, Fadhel MA, Al-Amidie
    M, Farhan L. 2021. Review of deep learning: concepts, CNN architectures, challenges,
    applications, future directions. Journal of big Data 8(1):1-74 Amin F, Ahmad A,
    Sang Choi G. 2019. Towards trust and friendliness approaches in the social internet
    of things. Applied Sciences 9(1):166 Amini P, Araghizadeh MA, Azmi R. 2015. A
    survey on botnet: classification, detection and defense. Anwer M, Khan SM, Farooq
    MU, Waseemullah W. 2021. Attack detection in IoT using machine learning. Engineering,
    Technology & Applied Science Research 11(3):7273-7278 Babu MR, Veena K. 2021.
    A survey on attack detection methods for iot using machine learning and deep learning.
    Banitalebi Dehkordi A, Soltanaghaei M, Boroujeni FZ. 2021. The DDoS attacks detection
    through machine learning and statistical methods in SDN. The Journal of Supercomputing
    77(3):2383-2415 Behal S, Kumar K. 2017. Characterization and comparison of ddos
    attack tools and traffic generators: a review. International Journal of Network
    Security 19(3):383-393 Chen Y-W, Sheu J-P, Kuo Y-C, Van Cuong N. 2020. Design
    and implementation of IoT DDoS attacks detection system based on machine learning.
    Cisco. 2019. Cisco Visual Networking Index: Global Mobile Data Traffic Forecast
    Update. 2017-2022 Deng L, Yu D. 2014. Deep learning–methods and applications.
    Foundations and Trends in Processing Dutta V, Choraś M, Pawlicki M, Kozik R. 2020.
    A deep learning ensemble for network anomaly and cyber-attack detection. Sensors
    20(16):4583 Elsaeidy AA, Jamalipour A, Munasinghe KS. 2021. A hybrid deep learning
    approach for replay and DDoS attack detection in a smart city. IEEE Access 9 154864–154875
    Farnaaz N, Jabbar M. 2016. Random forest modeling for network intrusion detection
    system. Procedia Computer Science 89(1):213-217 Haider W, Creech G, Xie Y, Hu
    J. 2016. Windows based data sets for evaluation of robustness of host based intrusion
    detection systems (IDS) to zero-day and stealth attacks. Future Internet 8(3):29
    Hassija V, Chamola V, Saxena V, Jain D, Goyal P, Sikdar B. 2019. A survey on IoT
    security: application areas, security threats, and solution architectures. IEEE
    Access 7:82721-82743 Huang Y, Wu Z, Wang L, Tan T. 2013. Feature coding in image
    classification: a comprehensive study. IEEE Transactions on Pattern Analysis and
    Machine Intelligence 36(3):493-506 Javed AR, Shahzad F, ur Rehman S, Zikria YB,
    Razzak I, Jalil Z, Xu G. 2022. Future smart cities: requirements, emerging technologies,
    applications, challenges, and future aspects. Cities 129(3):103794 Jiang M, Wang
    C, Luo X, Miu M, Chen T. 2017. Characterizing the impacts of application layer
    DDoS attacks. Khuphiran P, Leelaprute P, Uthayopas P, Ichikawa K, Watanakeesuntorn
    W. 2018. Performance comparison of machine learning models for DDoS attacks detection.
    Koroniotis N, Moustafa N, Sitnikova E, Turnbull B. 2019. Towards the development
    of realistic botnet dataset in the internet of things for network forensic analytics:
    bot-IoT dataset. Future Generation Computer Systems 100(7):779-796 Kumar D, Pateriya
    R, Gupta RK, Dehalwar V, Sharma A. 2023. Ddos detection using deep learning. Procedia
    Computer Science 218(1):2420-2429 Kumari K, Mrunalini M. 2022. Detecting denial
    of service attacks using machine learning algorithms. Journal of Big Data 9(1):1-17
    Li Y, Guo L. 2007. An active learning based TCM-KNN algorithm for supervised network
    intrusion detection. Computers & Security 26(7–8):459-467 Li Y, Zhang B. 2019.
    An intrusion detection model based on multi-scale cnn. Mustafa I, Khan IU, Aslam
    S, Sajid A, Mohsin SM, Awais M, Qureshi MB. 2020. A lightweight post-quantum lattice-based
    rsa for secure communications. IEEE Access 8:99273-99285 Neto ECP, Dadkhah S,
    Ferreira R, Zohourian A, Lu R, Ghorbani AA. 2023. Ciciot2023: a real-time dataset
    and benchmark for large-scale attacks in IoT environment. Sensors 23:941 Panda
    M, Patra MR. 2007. Network intrusion detection using naive bayes. International
    Journal of Computer Science and Network Security 7(12):258-263 Pei J, Chen Y,
    Ji W. 2019. A DDoS attack detection method based on machine learning. Journal
    of Physics: Conference Series 1237:32040 Saba T, Rehman A, Sadad T, Kolivand H,
    Bahaj SA. 2022. Anomaly-based intrusion detection system for IoT networks through
    deep learning model. Computers and Electrical Engineering 99(5):107810 Saghezchi
    FB, Mantas G, Violas MA, de Oliveira Duarte AM, Rodriguez J. 2022. Machine learning
    for DDoS attack detection in industry 4.0 CPPSs. Electronics 11(4):602 Seifousadati
    A, Ghasemshirazi S, Fathian M. 2021. A machine learning approach for ddos detection
    on IoT devices. ArXiv preprint Sharma N, Bhandari HV, Yadav NS, Shroff H. 2020.
    Optimization of ids using filter-based feature selection and machine learning
    algorithms. International Journal of Innovative Technology and Exploring Engineering
    10(2):96-102 Shurman M, Khrais R, Yateem A. 2020. Dos and DDoS attack detection
    using deep learning and IDS. The International Arab Journal of Information Technology
    17(4A):655-661 Spamhaus Malware Labs. 2020. Spamhaus Botnet Threat Report 2019.
    Spamhaus. Yadav S, Subramanian S. 2016. Detection of application layer DDoS attack
    by feature learning using stacked autoencoder. Yusof MAM, Ali FHM, Darus MY. 2018.
    Detection and defense algorithms of different types of DDoS attacks using machine
    learning. Related research Addressing Internet of Things security by enhanced
    sine cosine metaheuristics tuned hybrid machine learning model and results interpretation
    based on SHAP approach PeerJ A lightweight intrusion detection method for IoT
    based on deep learning and dynamic quantization PeerJ Deepfake attack prevention
    using steganography GANs PeerJ Data augmentation-based conditional Wasserstein
    generative adversarial network-gradient penalty for XSS attack detection system
    PeerJ In-vehicle network intrusion detection systems: a systematic survey of deep
    learning-based approaches PeerJ Discovery of a non-canonical GRHL1 binding site
    using deep convolutional and recurrent neural networks Sebastian Proft et al.,
    BMC Genomics, 2023 Machine learning in TCM with natural products and molecules:
    current status and future perspectives Suya Ma et al., Chinese Medicine, 2023
    Diagnosis and detection of pneumonia using weak-label based on X-ray images: a
    multi-center study Kairou Guo et al., BMC Medical Imaging, 2023 “A net for everyone”:
    fully personalized and unsupervised neural networks trained with longitudinal
    data from a single patient Christian Strack et al., BMC Medical Imaging, 2023
    Integrating pathway knowledge with deep neural networks to reduce the dimensionality
    in single-cell RNA-seq data Pelin Gundogdu et al., BioData Min, 2022 Powered by
    Questions Links   Ask a question Learn more about Q&A Enter your institution To
    find colleagues at PeerJ CALL FOR PAPERS Revolutionizing Healthcare: The Role
    of AI and Machine Learning View Call for Papers Earn Tokens as a PeerJ Reviewer
    London, united kingdom View Task Download Content Alert Just enter your email
    Tools & info Peer Review history See citing articles 1 Ask questions Add links
    Visitors 534 click for details Views 489 Downloads 34 Report problem with article
    Outline Introduction Related work Proposed methodology Results and discussion
    Findings and discussion Conclusion and future scope Supplemental Information ACADEMIC
    EDITOR Vicente Alarcon-Aquino Universidad de las Americas Puebla VIEW PROFILE   JOURNALS
    PUBLISH COMMUNITIES LOGIN Can you publish for free in PeerJ? FIND OUT NOW About
    us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements |
    Awards Academic boards - Advisors | Editors | Subject areas Submission guides
    - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Resources
    - FAQ | Careers | Press room | Terms of use | Privacy | Credits | Contact Follow
    us - PeerJ blog | X | Facebook | LinkedIn | Instagram | Pinterest Spread the word
    - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer
    Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment
    | PeerJ Computer Science © PeerJ, Inc. 2012-2024 | Public user content licensed
    CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci.
    ISSN: 2376-5992 PeerJ Phys. Chem. ISSN: 2689-7733 PeerJ Analytical Chem. ISSN:
    2691-6630 PeerJ Org. Chem. ISSN: 2831-6223 PeerJ Mat. Sci. ISSN: 2691-6657 PeerJ
    Preprints ISSN: 2167-9843"'
  inline_citation: '>'
  journal: PeerJ Computer Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Evaluating deep learning variants for cyber-attacks detection and multi-class
    classification in IoT networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Akilandeswari K.
  - Sivakumar N.R.
  - Alkahtani H.K.
  - Basheer S.
  - Ghorashi S.A.
  citation_count: '0'
  description: In this present time, Human Activity Recognition (HAR) has been of
    considerable aid in the case of health monitoring and recovery. The exploitation
    of machine learning with an intelligent agent in the area of health informatics
    gathered using HAR augments the decision-making quality and significance. Although
    many research works conducted on Smart Healthcare Monitoring, there remain a certain
    number of pitfalls such as time, overhead, and falsification involved during analysis.
    Therefore, this paper proposes a Statistical Partial Regression and Support Vector
    Intelligent Agent Learning (SPR-SVIAL) for Smart Healthcare Monitoring. At first,
    the Statistical Partial Regression Feature Extraction model is used for data preprocessing
    along with the dimensionality-reduced features extraction process. Here, the input
    dataset the continuous beat-to-beat heart data, triaxial accelerometer data, and
    psychological characteristics were acquired from IoT wearable devices. To attain
    highly accurate Smart Healthcare Monitoring with less time, Partial Least Square
    helps extract the dimensionality-reduced features. After that, with these resulting
    features, SVIAL is proposed for Smart Healthcare Monitoring with the help of Machine
    Learning and Intelligent Agents to minimize both analysis falsification and overhead.
    Experimental evaluation is carried out for factors such as time, overhead, and
    false positive rate accuracy concerning several instances. The quantitatively
    analyzed results indicate the better performance of our proposed SPR-SVIAL method
    when compared with two state-of-the-art methods.
  doi: 10.32604/cmc.2023.034815
  full_citation: '>'
  full_text: '>

    "Submit LOGIN REGISTER Home Academic Journals Books & Monographs Conferences Language
    Service News & Announcements About Home/ Journals/ CMC/ Vol.78, No.1, 2024/ 10.32604/cmc.2023.034815
    Submit a Paper Propose a Special lssue Table of Content Abstract Introduction
    Related Works Statistical Partial Regression and Support Vector Intelligent Agent
    Learning (SPR-SVIAL) for Smart Healthcare Monitoring Experimental Evaluations
    Discussion Conclusion References Open Access ARTICLE Smart Healthcare Activity
    Recognition Using Statistical Regression and Intelligent Learning K. Akilandeswari1,
    Nithya Rekha Sivakumar2,*, Hend Khalid Alkahtani3, Shakila Basheer3, Sara Abdelwahab
    Ghorashi2 1 Department of Computer Sciences, Government Arts College (Autonomous),
    Salem, 636007, India 2 Department of Computer Sciences, College of Computer and
    Information Sciences, Princess Nourah Bint Abdulrahman University, Riyadh, 11671,
    Saudi Arabia 3 Department of Information Systems, College of Computer and Information
    Sciences, Princess Nourah Bint Abdulrahman University, Riyadh, 11671, Saudi Arabia
    * Corresponding Author: Nithya Rekha Sivakumar. Email: Computers, Materials &
    Continua 2024, 78(1), 1189-1205. https://doi.org/10.32604/cmc.2023.034815 Received
    28 July 2022; Accepted 30 June 2023; Issue published 30 January 2024 View Full
    Text Download PDF Abstract In this present time, Human Activity Recognition (HAR)
    has been of considerable aid in the case of health monitoring and recovery. The
    exploitation of machine learning with an intelligent agent in the area of health
    informatics gathered using HAR augments the decision-making quality and significance.
    Although many research works conducted on Smart Healthcare Monitoring, there remain
    a certain number of pitfalls such as time, overhead, and falsification involved
    during analysis. Therefore, this paper proposes a Statistical Partial Regression
    and Support Vector Intelligent Agent Learning (SPR-SVIAL) for Smart Healthcare
    Monitoring. At first, the Statistical Partial Regression Feature Extraction model
    is used for data preprocessing along with the dimensionality-reduced features
    extraction process. Here, the input dataset the continuous beat-to-beat heart
    data, triaxial accelerometer data, and psychological characteristics were acquired
    from IoT wearable devices. To attain highly accurate Smart Healthcare Monitoring
    with less time, Partial Least Square helps extract the dimensionality-reduced
    features. After that, with these resulting features, SVIAL is proposed for Smart
    Healthcare Monitoring with the help of Machine Learning and Intelligent Agents
    to minimize both analysis falsification and overhead. Experimental evaluation
    is carried out for factors such as time, overhead, and false positive rate accuracy
    concerning several instances. The quantitatively analyzed results indicate the
    better performance of our proposed SPR-SVIAL method when compared with two state-of-the-art
    methods. Keywords Internet of Things; smart health care monitoring; human activity
    recognition; intelligent agent learning; statistical partial regression; support
    vector Cite This Article K. Akilandeswari, N. R. Sivakumar, H. K. Alkahtani, S.
    Basheer and S. A. Ghorashi, \"Smart healthcare activity recognition using statistical
    regression and intelligent learning,\" Computers, Materials & Continua, vol. 78,
    no.1, pp. 1189–1205, 2024. BibTex EndNote RIS    This work is licensed under a
    Creative Commons Attribution 4.0 International License , which permits unrestricted
    use, distribution, and reproduction in any medium, provided the original work
    is properly cited. We recommend Student’s Health Exercise Recognition Tool for
    E-Learning Education Shloul Tamara et al., Intelligent Automation & Soft Computing,
    2022 Adversarial Active Learning for Named Entity Recognition in Cybersecurity
    Tao Li et al., CMC-Computers, Materials & Continua, 2020 Intelligent Dynamic Gesture
    Recognition Using CNN Empowered by Edit Distance Shazia Saqib et al., CMC-Computers,
    Materials & Continua, 2020 Intelligent Deep Learning Enabled Human Activity Recognition
    for Improved Medical Services E. Dhiravidachelvi et al., Computer Systems Science
    and Engineering, 2022 Review of Optical Character Recognition for Power System
    Image Based on Artificial Intelligence Algorithm Xun Zhang et al., Energy Engineering,
    2023 553 The role of artificial intelligence in pediatric injuries-a scoping review
    Maleeha Naseem et al., Inj Prev, 2022 AB1667 UNDERSTANDING THE ROLE AND ADOPTION
    OF ARTIFICIAL INTELLIGENCE TECHNIQUES IN RHEUMATOLOGY RESEARCH: AN IN-DEPTH REVIEW
    OF THE LITERATURE A. Madrid García et al., Ann Rheum Dis, 2023 A preliminary study
    on the application of deep learning methods based on convolutional network to
    the pathological diagnosis of PJI Ye Tao et al., Arthroplasty, 2022 Phytoplankton
    recognition based on residual attention network Heyu XIANG et al., Acta Ecologica
    Sinica, 2022 Augmented Intelligence in Joint Replacement Surgery: How can artificial
    intelligence (AI) bridge the gap between the man and the machine? Vaibhav Bagaria
    et al., Arthroplasty, 2022 Powered by Downloads Citation Tools 247 View 86 Download
    0 Like Related articles Comparisons of MFDFA, EMD and WT by Neural Network, Mahalanobis
    Distance and SVM in Fault Diagnosis of Gearboxes Jinshan Lin, Chunhong Dou, Qianqian...
    Intravascular Optical Coherence Tomography Image Segmentation Based on Support
    Vector Machine Algorithm Yuxiang Huang, Chuliu He, Jiaqiu... Use of Discrete Wavelet
    Features and Support Vector Machine for Fault Diagnosis of Face Milling Tool C.
    K. Madhusudana, N. Gangadhar,... A Computer-Aided Tuning Method for Microwave
    Filters by Combing T-S Fuzzy Neural Networks and Improved Space Mapping Shengbiao
    Wu, Weihua Cao, Can... A Novel Universal Steganalysis Algorithm Based on the IQM
    and the SRM Yu Yang, Yuwei Chen, Yuling Chen,..."'
  inline_citation: '>'
  journal: Computers, Materials and Continua
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Smart Healthcare Activity Recognition Using Statistical Regression and Intelligent
    Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 73 papers. The special focus in this conference
    is on Applications in Electronics Pervading Industry, Environment and Society.
    The topics include: A RISC-V Hardware Accelerator for Q-Learning Algorithm; Efficient
    Optimization of SFQ-Based Logic Circuits: Introducing a Novel Methodology for
    Performance and Design Enhancement; A PUF-Based Secure Boot for RISC-V Architectures;
    neural Architecture for Tennis Shot Classification on Embedded System; evaluating
    the Effect of Intrinsic Sensor Noise for Vibration Diagnostic in the Compressed
    Domain Using Convolutional Neural Networks; cooperative Driver Assistance for
    Electric Wheelchair; tiny Neural Deep Clustering: An Unsupervised Approach for
    Continual Machine Learning on the Edge; investigating Adversarial Policy Learning
    for Robust Agents in Automated Driving Highway Simulations; Evaluation of AI and
    Video Computing Applications on Multiple Heterogeneous Architectures; heterogeneous
    Tightly-Coupled Dual Core Architecture Against Single Event Effects; preliminary
    Frequency Response Analysis of a Contact Force Measurement System for Rail Applications;
    assembly of Solder Beads with a Surface Mount Technology Resistor with Optoelectronic
    Tweezers and Freezing-Drying Techniques; A Low Cost Open Platform for Development
    and Performance Evaluation of IoT and IIoT Systems; a Compact Continuous Analyzer
    of Particulate Matter Radioactivity; Data Acquisition System for a 28 nm Flash-ADC
    Based Programmable Front End Channel for HEP Experiments; preliminary Development
    of a Full-Digital Smart System for Chest Auscultation and Further Internet of
    Medical Things Framework; Reducing Energy Consumption in NB-IoT by Compressing
    Data and Aggregating Transmission; design and Development of New Wearable and
    Protective Equipment for Human Spaceflights; Electrochemical and Thermal Modelling
    of a Li-Ion NMC Pouch Cell.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: Lecture Notes in Electrical Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: International Conference on Applications in Electronics Pervading Industry,
    Environment and Society, APPLEPIES 2023
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Latha M.
  - Vasavi M.
  - Kumar C.K.
  - Balamanigandan R.
  - Guttikonda J.B.
  - Kumar R.T.
  citation_count: '0'
  description: Many tasks are part of smart farming, including predicting crop yields,
    analysing soil fertility, making crop recommendations, managing water, and many
    more. In order to execute smart agricultural tasks, researchers are constantly
    creating several Machine Learning (ML) models. In this work, we integrate ML with
    the Internet of Things. Either the UCI dataset or the Kaggle dataset was used
    to gather the data. Effective data pretreatment approaches, such as the Imputation
    and Outlier (IO) methods, are necessary to manage the intricacies and guarantee
    proper analysis when dealing with data that exhibits irregular patterns or contains
    little changes that can have a substantial influence on analysis and decision
    making. The goal of this research is to provide a more meaningful dataset by investigating
    data preparation approaches that are particular to processing data. Following
    the completion of preprocessing, the data is classified using an average approach
    based on the Ensemble of Adaptive Neuro-Fuzzy Inference System (ANFIS), Random
    Neural Network (PNN), and Clustering-Based Decision Tree (CBDT) techniques. The
    next step in optimising the hyperparameter tuning of the proposed ensemble classifier
    is to employ a new Tree-Structured Parzen Estimator (TPE). Applying the suggested
    TPE based Ensemble classification method resulted in a 99.4 percent boost in accuracy.
  doi: 10.53759/7669/jmc202404025
  full_citation: '>'
  full_text: '>

    "HOME ABOUT US JOURNALS BOOKS MAGAZINE RESOURCES ARCHIVE CONTACT Journal   Frequency:
    Quarterly  ISSN (Online) : 2788-7669  ISSN (Print) : 2789-1801     JOURNAL OVERVIEW
    ARTICLES FOR AUTHORS FOR EDITORS AND REVIEWERS SPECIAL ISSUES JOURNAL MARKETING
    CONTACT US Journal of Machine and Computing Machine Learning Based Precision Agriculture
    using Ensemble Classification with TPE Model Latha M Mandadi Vasavi Chunduri Kiran
    Kumar Balamanigandan R John Babu Guttikonda Rajesh Kumar T   Journal of Machine
    and Computing Received On : 10 March 2023 Revised On : 25 August 2023 Accepted
    On : 16 December 2023 Published On : 05 January 2024 Volume 04, Issue 01 Pages
    : 261-268  0 0 Total citations 0 Recent citations n/a Field Citation Ratio n/a
    Relative Citation Ratio  DOI https://doi.org/10.53759/7669/jmc202404025 Article
    Views Abstract Many tasks are part of smart farming, including predicting crop
    yields, analysing soil fertility, making crop recommendations, managing water,
    and many more. In order to execute smart agricultural tasks, researchers are constantly
    creating several Machine Learning (ML) models. In this work, we integrate ML with
    the Internet of Things. Either the UCI dataset or the Kaggle dataset was used
    to gather the data. Effective data pretreatment approaches, such as the Imputation
    and Outlier (IO) methods, are necessary to manage the intricacies and guarantee
    proper analysis when dealing with data that exhibits irregular patterns or contains
    little changes that can have a substantial influence on analysis and decision
    making. The goal of this research is to provide a more meaningful dataset by investigating
    data preparation approaches that are particular to processing data. Following
    the completion of preprocessing, the data is classified using an average approach
    based on the Ensemble of Adaptive Neuro-Fuzzy Inference System (ANFIS), Random
    Neural Network (PNN), and Clustering-Based Decision Tree (CBDT) techniques. The
    next step in optimising the hyperparameter tuning of the proposed ensemble classifier
    is to employ a new Tree-Structured Parzen Estimator (TPE). Applying the suggested
    TPE based Ensemble classification method resulted in a 99.4 percent boost in accuracy  Keywords
    Smart Farming, Machine Learning, Data Preprocessing, Ensemble Classification,
    Tree-Structure Parzen Estimator.  REFERENCES  Acknowledgements We would like to
    thank Reviewers for taking the time and effort necessary to review the manuscript.
    We sincerely appreciate all valuable comments and suggestions, which helped us
    to improve the quality of the manuscript.  Funding No funding was received to
    assist with the preparation of this manuscript.  Ethics declarations Conflict
    of interest The authors have no conflicts of interest to declare that are relevant
    to the content of this article.  Availability of data and materials The data that
    support the findings of this study are available from the corresponding author
    upon reasonable request.  Author information Contributions All authors have equal
    contribution in the paper and all authors have read and agreed to the published
    version of the manuscript.  Corresponding author Balamanigandan R   Rights and
    permissions Open Access This article is licensed under a Creative Commons Attribution
    NoDerivs is a more restrictive license. It allows you to redistribute the material
    commercially or non-commercially but the user cannot make any changes whatsoever
    to the original, i.e. no derivatives of the original work. To view a copy of this
    license, visit https://creativecommons.org/licenses/by-nc-nd/4.0/  Cite this article
    Latha M, Mandadi Vasavi, Chunduri Kiran Kumar, Balamanigandan R, John Babu Guttikonda
    and Rajesh Kumar T, “Machine Learning Based Precision Agriculture using Ensemble
    Classification with TPE Model”, Journal of Machine and Computing, pp. 261-268,
    January 2024. doi: 10.53759/7669/jmc202404025.  Copyright © 2024 Latha M, Mandadi
    Vasavi, Chunduri Kiran Kumar, Balamanigandan R, John Babu Guttikonda and Rajesh
    Kumar T. This is an open access article distributed under the terms of the Creative
    Commons Attribution License, which permits unrestricted use, distribution, and
    reproduction in any medium, provided the original author and source are credited.  DETAILS                              DOWNLOAD  Journals
    JMC JCNS JBSHA JEBI JCIMS Policies & Ethics Publication Ethics Copyright Policy
    Open Access Policy Article Posting Policy Advertising Policy Privacy Policy Data
    Availability Policy Conflicts of interest/Competing interests Allegations of Misconduct
    Appeals Process Complaints Process Peer Review Process Authorship Criteria Terms
    and Conditions Resources Digital Preservation ALPSP Ansis Manager Contact Us Information
    Authors Editors Librarians Societies Journal News Copyright © 2024 Design by AnaPub
    Publications"'
  inline_citation: '>'
  journal: Journal of Machine and Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Machine Learning Based Precision Agriculture using Ensemble Classification
    with TPE Model
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Yang K.
  citation_count: '0'
  description: 'QUALITY IN THE ERA OF INDUSTRY 4.0. Enables readers to use real-world
    data from connected devices to improve product performance, detect design vulnerabilities,
    and design better solutions. Quality in the Era of Industry 4.0 provides an insightful
    guide to harnessing user performance and behavior data through AI and other Industry
    4.0 technologies. This transformative approach enables companies to not only optimize
    products and services in real-time, but also to anticipate and mitigate likely
    failures proactively. In a succinct and lucid style, the book presents a pioneering
    framework for a new paradigm of quality management in the Industry 4.0 landscape.
    It introduces groundbreaking techniques such as utilizing real-world data to tailor
    products for superior fit and performance, leveraging connectivity to adapt products
    to evolving needs and use-cases, and employing cutting-edge manufacturing methods
    to create bespoke, cost-effective solutions with greater efficiency. Case examples
    featuring applications from the automotive, mobile device, home appliance, and
    healthcare industries are used to illustrate how these new quality approaches
    can be used to benchmark the product’s performance and durability, maintain smart
    manufacturing, and detect design vulnerabilities. Written by a seasoned expert
    with experience teaching quality management in both corporate and academic settings,
    Quality in the Era of Industry 4.0 covers topics such as: Evolution of quality
    through industrial revolutions, from ancient times to the first and second industrial
    revolutions Quality by customer value creation, explaining differences in producers,
    stakeholders, and customers in the new digital age, along with new realities brought
    by Industry 4.0.Data quality dimensions and strategy, data governance, and new
    talents and skill sets for quality professionals in Industry 4.0. Automated product
    lifecycle management, predictive quality control, and defect prevention using
    technologies like smart factories, IoT, and sensors.Quality in the Era of Industry
    4.0 is a highly valuable resource for product engineers, quality managers, quality
    engineers, quality consultants, industrial engineers, and systems engineers who
    wish to make a participatory approach towards data-driven design, economical mass-customization,
    and late differentiation.'
  doi: 10.1002/9781119932475
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy UNCL: University Of Nebraska
    - Linc Acquisitions Accounting Search within Login / Register HOMEAUTHOR BIOGRAPHY
    Quality in the Era of Industry 4.0: Integrating Tradition and Innovation in the
    Age of Data and AI Author(s):Kai Yang First published:5 January 2024 Print ISBN:9781119932444
    |Online ISBN:9781119932475 |DOI:10.1002/9781119932475 © 2024 John Wiley & Sons,
    Inc. About this book QUALITY IN THE ERA OF INDUSTRY 4.0 Enables readers to use
    real-world data from connected devices to improve product performance, detect
    design vulnerabilities, and design better solutions Quality in the Era of Industry
    4.0 provides an insightful guide to harnessing user … Show all Table of Contents
    Export Citation(s) Free Access Front Matter (Pages: i-xix) Summary PDF Request
    permissions CHAPTER 1 Evolution of Quality Through Industrial Revolutions (Pages:
    1-22) Summary PDF References Request permissions CHAPTER 2 Evolving Paradigm for
    Quality in the Era of Industry 4.0 (Pages: 23-55) Summary PDF References Request
    permissions CHAPTER 3 Quality by Design and Innovation (Pages: 57-118) Summary
    PDF References Request permissions CHAPTER 4 Quality Management in the Era of
    Industry 4.0 (Pages: 119-159) Summary PDF References Request permissions CHAPTER
    5 Predictive Quality (Pages: 161-197) Summary PDF References Request permissions
    CHAPTER 6 Data Quality (Pages: 199-236) Summary PDF References Request permissions
    CHAPTER 7 Risk Management in the 21st Century (Pages: 237-279) Summary PDF References
    Request permissions CHAPTER 8 Emerging Organizational Changes in the 21st Century
    (Pages: 281-313) Summary PDF References Request permissions Free Access Index
    (Pages: 315-323) First Page PDF Request permissions Buy this Book Contact your
    account manager For authors Additional links ABOUT WILEY ONLINE LIBRARY Privacy
    Policy Terms of Use About Cookies Manage Cookies Accessibility Wiley Research
    DE&I Statement and Publishing Policies Developing World Access HELP & SUPPORT
    Contact Us Training and Support DMCA & Reporting Piracy OPPORTUNITIES Subscription
    Agents Advertisers & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley
    Press Room Copyright © 1999-2024 John Wiley & Sons, Inc or related companies.
    All rights reserved, including rights for text and data mining and training of
    artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: 'Quality in the Era of Industry 4.0: Integrating Tradition and Innovation
    in the Age of Data and AI'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Quality in the Era of Industry 4.0: Integrating Tradition and Innovation
    in the Age of Data and AI'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Khanday S.A.
  - Fatima H.
  - Rakesh N.
  citation_count: '0'
  description: IoT devices or sensor nodes are essential components of the machine
    learning (ML) application workflow because they gather abundant information for
    building models with sensors. Uncontrollable factors may impact this process and
    add inaccuracies to the data, raising the cost of computational resources for
    data preparation. Choosing the best method for this data pre-processing stage
    can lessen the complexity of ML models and wasteful bandwidth use for cloud processing.
    Devices in the IoT ecosystem with limited resources provide an easy target for
    attackers, who can make use of these devices to create botnets and spread malware.
    To repel attacks directed towards IoT, robust and lightweight intrusion detection
    systems are the need of an hour. Furthermore, data preprocessing remains the first
    step for modish machine learning models, ensemble techniques, and hybrid methods
    in developing anti-intrusion applications for lightweight IoT. This article proposes
    a novel data preprocessing model as a core structure using an Extra Tree classifier
    for feature selection and two classifiers LSTM and 1D-CNN for classification.
    The dataset used in this research is CIC IoT 2023 with 34 attack classes and SMOTE
    (Synthetic Memory Oversampling Technique) has been used for class balancing. The
    article evaluates the performance of 1D-CNN and LSTM on the CIC IoT 23 dataset
    using classification metrics. The proposed ensemble approach using LSTM has obtained
    92% accuracy and with 1D-CNN the model obtained 99.87% accuracy.
  doi: 10.33889/IJMEMS.2024.9.1.010
  full_citation: '>'
  full_text: '>

    ""'
  inline_citation: '>'
  journal: International Journal of Mathematical, Engineering and Management Sciences
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Novel Data Preprocessing Model for Lightweight Sensory IoT Intrusion Detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
