- analysis: '>'
  authors:
  - Arnau Muñoz L.
  - Berná Martínez J.V.
  - Maciá Pérez F.
  - Lorenzo Fonseca I.
  citation_count: '0'
  description: The inclusion of IoT in digital platforms is very common nowadays due
    to the ease of deployment, low power consumption and low cost. It is also common
    to use heterogeneous IoT devices of ad-hoc or commercial development, using private
    or third-party network infrastructures. This scenario makes it difficult to detect
    invalid packets from malfunctioning devices, from sensors to application servers.
    These invalid packets generate low quality or erroneous data, which negatively
    influence the services that use them. For this reason, we need to create procedures
    and mechanisms to ensure the quality of the data obtained from IoT infrastructures,
    regardless of the type of infrastructure and the control we have over them, so
    that the systems that use this data can be reliable. In this work we propose the
    development of an Anomaly Detection System for IoT infrastructures based on Machine
    Learning using unsupervised learning. We validate the proposal by implementing
    it on the IoT infrastructure of the University of Alicante, which has a multiple
    sensing system and uses third-party services, over a campus of one million square
    meters. The contribution of this work has been the generation of an anomaly detection
    system capable of revealing incidents in IoT infrastructures, without knowing
    details about the infrastructures or devices, through the analysis of data in
    real time. This proposal allows to discard from the IoT data flow all those packets
    that are suspected to be anomalous to ensure a high quality of information to
    the tools that consume IoT data.
  doi: 10.1016/j.iot.2024.101095
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Background and related work 3. Proposed
    solution 4. Implementation and analysis of results 5. Conclusions Declaration
    of competing interest Acknowledgement Data availability References Show full outline
    Figures (12) Show 6 more figures Tables (2) Table 1 Table 2 Internet of Things
    Volume 25, April 2024, 101095 Anomaly detection system for data quality assurance
    in IoT infrastructures based on machine learning Author links open overlay panel
    Lucia Arnau Muñoz, José Vicente Berná Martínez, Francisco Maciá Pérez, Iren Lorenzo
    Fonseca Show more Share Cite https://doi.org/10.1016/j.iot.2024.101095 Get rights
    and content Under a Creative Commons license open access Abstract The inclusion
    of IoT in digital platforms is very common nowadays due to the ease of deployment,
    low power consumption and low cost. It is also common to use heterogeneous IoT
    devices of ad-hoc or commercial development, using private or third-party network
    infrastructures. This scenario makes it difficult to detect invalid packets from
    malfunctioning devices, from sensors to application servers. These invalid packets
    generate low quality or erroneous data, which negatively influence the services
    that use them. For this reason, we need to create procedures and mechanisms to
    ensure the quality of the data obtained from IoT infrastructures, regardless of
    the type of infrastructure and the control we have over them, so that the systems
    that use this data can be reliable. In this work we propose the development of
    an Anomaly Detection System for IoT infrastructures based on Machine Learning
    using unsupervised learning. We validate the proposal by implementing it on the
    IoT infrastructure of the University of Alicante, which has a multiple sensing
    system and uses third-party services, over a campus of one million square meters.
    The contribution of this work has been the generation of an anomaly detection
    system capable of revealing incidents in IoT infrastructures, without knowing
    details about the infrastructures or devices, through the analysis of data in
    real time. This proposal allows to discard from the IoT data flow all those packets
    that are suspected to be anomalous to ensure a high quality of information to
    the tools that consume IoT data. Previous article in issue Next article in issue
    Keywords Internet of thingsAnomaly detectionMachine learningIsolation forest 1.
    Introduction IoT infrastructures are now widely deployed in our society. They
    were first driven mainly by the needs of large companies to control certain processes,
    speed up and improve their efficiency, or reduce the occurrence of errors [1].
    Subsequently, due to their use in Smart City environments, for any type of use,
    such as air quality monitoring, traffic monitoring, waste management or citizen
    safety [2]. Nowadays, they have become common sensor systems in Digital Twin platforms
    [3], where a realistic representation of the controlled world requires knowledge
    of the real-time state of the system. It is precisely in these latter environments,
    where data quality is crucial to achieve the objectives. These systems also generate
    new challenges and problems, some of them concerning the proper functioning of
    the infrastructures themselves. These types of problems related to monitoring
    anomalies in the performance of infrastructures have been widely addressed from
    the perspective of traditional TCP/IP ethernet networks, using for example network
    intrusion detection systems (IDS), which is just a subtype of anomaly detection
    systems (ADS) [4] but focused on the use of infrastructures. Such systems are
    capable of monitoring events occurring in the infrastructure, detecting anomalies
    of various nature, and generating an automated and controlled processing of these
    events to ensure the proper functioning of the system [5]. The strength of these
    systems lies in the use of highly standardised and accepted protocols, such as
    TCP/IP, and services whose behaviour is tightly controlled, such as HTTP, FTP,
    STMP, etc. However, in IoT networks we find a large number of technologies, encodings,
    packaging, applications and in general much more heterogeneous systems where we
    cannot analyse headers as is done in traditional IDSs over Ethernet networks [6].
    In addition, in IoT environments it is common to use infrastructures based on
    Open Source initiatives, such as TTN [7], and therefore the data-emitting devices
    cross intermediate networks that are beyond the control of the administrator of
    the IoT sensors that emit data, so there is even metadata over which there is
    no control or access. It is also very common to find that, in our control system
    platform, Smart City platform or digital twin, the information that is being integrated
    comes from different subsystems, manufactured by different companies, with different
    technologies and different natures. In order for these tools to work as expected,
    it is necessary to ensure that the data with which we are going to feed them are
    valid and have an adequate degree of quality, discarding anything that may come
    from an element of the infrastructure that exhibits a malfunction. Likewise, it
    is necessary to do so even when we do not have details about the infrastructure
    that originates the data. That is why, to ensure the quality of the data generated
    by the IoT infrastructure, it is necessary to conceive mechanisms and procedures
    to be applied on these IoT data flows, of which the types of incidents that can
    be detected are unknown. That is, to create an Anomaly Detection System (ADS)
    that is capable of working on information generated by the IoT, detecting everything
    that is out of the expected normality. For an ADS to work correctly and efficiently
    when detecting possible threats or anomalous elements, it is necessary to follow
    a series of processes [8]: data collection to obtain the parameters of the packet
    to be analysed; generation of rules and algorithms for the definition of anomalies;
    execution of filters and analysis using the rules and algorithms on the collected
    packets; and detection and treatment of the detected anomalies. Only the systematisation
    of the processes to be carried out can ensure a good result, and, in addition,
    these processes must be adapted to the scope of the problem to be addressed, since
    the search for anomalies or outliers requires techniques and strategies coupled
    to the environment [9]. If we look at some of the contributions that review the
    different techniques for the creation of anomaly detection systems [10], we can
    see that the systematic treatment of the data is one of the first tasks to be
    completed before moving on to the selection and training of the Machine Learning
    (ML) algorithms. Once the data adequacy process has been completed, it is essential
    to select an algorithm in accordance with the objective to be achieved, so that
    the resulting trained model can correctly ensure the operation of the infrastructure.
    This work proposes the development of an ADS on IoT infrastructures based on Machine
    Learning, so that the system can be sufficiently generic to cover a wide range
    of platforms and at the same time be able to detect anomalies in the infrastructures.
    The objective is not to diagnose the infrastructures, to know the exact incident
    that is occurring, but to be able, on real-time traffic, to classify the data
    packets we receive as valid or invalid, regardless of their content or origin.
    In this way, a filter can be generated prior to the injection of this data on
    the applications that use it and prevent incorrect information from contaminating
    the services. The paper proposes the design and development of an ADS system based
    on Machine Learning and also to validate its operation, it is instantiated on
    the Smart City platform of the University of Alicante, in which several IoT systems
    coexist, thus generating the previous data analysis that ensures the quality of
    the data before being used by the platform. The motivation to propose, develop
    and implement this model in our platform comes from the need to control a large
    infrastructure, in which different interconnected sensorization areas coexist,
    in which being aware of each anomaly that may occur is a complex and laborious
    work for the people responsible for its management. The rest of the work is organised
    into the following sections: Section 2 contains a preliminary study of the techniques
    and problems related to the project objectives; Section 3 shows the design of
    the ADS proposal and the processes and algorithms involved; Section 4 carries
    out the instantiation of the system on a real platform, the Smart University platform
    of the University of Alicante; Section 5 finally draws the main conclusions of
    the work and sets out the lines of future work. 2. Background and related work
    With the increasing evolution of cities and the lifestyle of the citizens living
    in them, IoT environments are evolving faster and faster, mainly due to the number
    of devices in these environments, using different communication protocols, great
    diversity of data to be sent, and varied formats and packaging for transmission.
    This is why the level of complexity for their control also increases along with
    the evolution of these systems, and with it, the need to develop solutions that
    can guarantee their security, efficiency, and reliability in these heterogeneous
    systems [11]. This complexity has derived in a need to expand research to find
    feasible solutions to the control of these systems, and over the last few years,
    the use of Machine Learning based applications has been chosen, so that they can
    be applied to IoT environments, since they are already implemented in many different
    areas of our daily life, such as medicine, in research or applications such as
    cancer detection [12]. Moreover, within the new difficulties that these environments
    already present when controlling them, even with the use of artificial intelligence,
    we must consider the aforementioned heterogeneity of the sensorisation devices,
    both in terms of the values they emit and the rest of the parameters they present,
    including their semantics and syntax [13], since they are still systems with a
    dynamic nature. This is why developing an automated model for this type of detection
    is a challenge to say the least, since the data for training cannot always be
    labelled, as required in some Machine Learning algorithms, as it can be very difficult
    to categorise them for this purpose. In addition, the data often contains noise
    and other types of values that can interfere with the reliability of the data,
    thus resulting in false anomalies [14]. Even today, even with the information
    available to address this problem, there is still no definitive solution due to
    the diversity of these environments, coupled with the lack of standardisation
    in the IoT domain. Although today''s IoT environments are still very much in flux,
    AI-based techniques have managed to provide a new approach from which to work
    and continue to gain knowledge. In AI, there are different valid techniques for
    anomaly detection, which already take into account this diversity of the system,
    both in the data sent and in the device that sends it [15], with approaches focused
    on Machine Learning to adjust to these changes in the data, or other techniques
    used in the transformation of the data itself, such as its normalisation before
    the application of the algorithm [16]. Some of the most outstanding algorithms,
    mainly due to their great adaptive capacity, are Neural Networks [17], Support
    Vector Machines (SVM) [18], or Random Forest [19], working satisfactorily in systems
    where the data used have a heterogeneous and uncategorized nature [20]. Some examples
    of their use are SPAM detection [21], mainly developed using supervised learning
    techniques, such as collaborative approaches, or content-based models [22] or
    the detection of malicious URLS [23], using techniques such as Decision Trees
    or Random Forest [24], mainly due to the large amount of features that must be
    extracted for this type of detections. Within the field of anomaly detection systems
    development, algorithms based on the creation of random trees and decision trees
    show high efficiency and accuracy in detecting such anomalies, testing these algorithms
    with different types of datasets [25]. In the context of IoT, the Isolation Forest
    [26] algorithm has been one of the most prominent ones currently, due to its effectiveness
    in such varied, heterogeneous, and dynamic environments, as well as presenting
    good results when working with a large amount of data volumes, such as data broadcasts
    from IoT devices. Some of the reasons for its effectiveness is that it does not
    require a lot of training data when developing the model, and its decision making
    returns a concrete yes or no result, identifying the anomaly or not [27]. An example
    of the use of this technique can be found in the detection of fraudulent banking
    transactions [28]. However, these proposals tend to focus detection on already
    identified anomalies and have not been applied to open and uncertain scenarios.
    Table 1 show a comparative summary of the analysed techniques. Table 1. Comparative
    summary of the main techniques used for anomaly detection. Algorithm Type Description
    Typical IoT applications Advantages Disadvantages Random Forest Supervised Set
    of decision trees. Classification and regression Good performance and handling
    characteristics. Requires adjustment of hyperparameters. Neural Networks Supervised
    Model inspired by the human brain. Pattern recognition Ability to learn complex
    relationships. Requires large data sets and computation SVM Supervised Finds a
    hyperplane that maximizes margin. Anomaly detection Efficient in high-dimensional
    spaces. Requires adjustment of hyperparameters. Isolation Forest Not Supervised
    Based on construction of random trees. Anomaly detection Efficient and scalable.
    Sensitive to noise. 3. Proposed solution For the development of our proposal,
    we will use a sequence of independent processes that form the framework, as shown
    in Fig. 1. The aim of this framework is to provide a generic procedure that can
    be extrapolated to any IoT infrastructure. Download : Download high-res image
    (216KB) Download : Download full-size image Fig. 1. Framework for general anomaly
    detection in IoT. The first process is responsible for collecting the dataset
    from the IoT infrastructure and performing the pre-analysis of the data. In this
    process, data capture from IoT data channels is performed and stored in persistence
    systems, and meticulous observation of the data is performed to know the types
    of data captured. The following process performs a processing of the data in such
    a way that invalid or incomplete data is cleaned, feature engineering is carried
    out to determine which of all the data received are necessary for the study, the
    construction of processable data vectors is performed and finally a data representation
    is made in order to be able to observe the distribution of the dataset. This obtained
    dataset is divided into three subsets with a ratio of 70–20–10. The first subset
    will be used for training, the second will be used for testing the training, and
    the third for validating the testing and therefore the trained model. Through
    these processes of testing and validation, or cross-validation, we can ensure
    that the trained model meets its objectives. Finally, the last process will be
    the evaluation of the model through its implementation in a real scenario. 3.1.
    Data collection and pre-analysis To develop our proposal, we used a dataset collected
    directly from the IoT infrastructures of the University of Alicante. The use of
    open-source datasets such as those provided by Kaggle [29] could allow us to follow
    strategies and proposals from other authors and compare results more easily, but
    it did not provide us with a realistic context, nor would it be useful to apply
    it to our infrastructures. It was therefore decided to generate our own dataset.
    For this purpose, a packet collector was developed using Message Queuing Telemetry
    Transport (MQTT) to connect to the IoT management system based on TheThingStack
    [30]. A dataset of approximately 320,000 packets has been collected, the equivalent
    of one day of sensing on the platform. Each packet consists of 117 features. This
    dataset is made up of sensor packages from different sources: • Climatology: sensorisation
    data on environment such as outdoor temperature, relative humidity, light level,
    UV rays, rain. • Quality&Comfort: sensorisation related to indoor spaces of buildings
    such as temperature, humidity, air quality, CO2 concentration, suspended particles,
    presence of harmful gases, etc. • Consumption: information related to infrastructure
    consumption, water, electricity, and gas consumption. • Production: sensorisation
    of the energy generation of photovoltaic plants. • Recharging electric vehicles:
    data on the use and consumption of electric vehicle charging stations. • WIFI:
    data on the real-time use of the campus wireless network infrastructures. • Luminaire:
    data concerning lighting systems including outdoor streetlights, signage, monument
    courtesy lights, etc. • Others: other sensorisations, where data received that
    do not belong to any previous collection will be dumped. In addition, the amount
    of data generated by each sensor group is different. Fig 2 shows the distribution
    of data per set. Download : Download high-res image (132KB) Download : Download
    full-size image Fig. 2. Percentage of data of the total collected by type of sensorisation.
    In the pre-analysis, all fields in each packet were inspected and from the initial
    117, only 47 fields were selected. All fields that are unique tokens, additional
    timestamps or unique identifiers are eliminated. These are fields that do not
    correlate or do not provide useful information in principle. 3.2. Processing The
    use of Machine Learning techniques requires the ability to explore the dataset,
    i.e., it must be able to undergo classification techniques. This involves processing
    the dataset to perform several operations. First, null or invalid values must
    be dealt with, cleaning the dataset of these records to avoid contamination. Unexpected
    values are also analysed in order to treat them and transform them into expected
    values. The aim is to have a dataset that can be processed by the algorithms.
    On the set of 47 selected features, a study of the entropy of the data has been
    carried out through correlation, to decide which ones are determinant in the Machine
    Learning algorithms. Fig. 3 shows the result of the study. As can be seen, there
    are many variables that, although important from a technical point of view, such
    as the timestamp of the data emission, do not contribute value within the correlation,
    because they are fields that will always grow. Download : Download high-res image
    (773KB) Download : Download full-size image Fig. 3. Results of the correlation
    study. Through the correlation matrix it was decided to finally select 18 characteristics,
    which are shown in Table 2. Table 2. List of attributes to be used for algorithm
    training. Attributes Description received_at The date and time the data packet
    was received at the TTN server. uplink_message.session_key_id The uplink message
    session key identifier. This key is used in data encryption. uplink_message.frm_payload
    The raw data field of the uplink message. uplink_message.decoded_payload.bytes
    The decoded data contained in the uplink message, expressed in bytes. gateway_ids.gateway_id
    The unique identifier of the gateway that received the message. time The date
    and time of the message timestamp A timestamp indicating when the message was
    received, usually in Unix timestamp format. rssi Received Signal Strength Indicator
    - Received Signal Strength Indicator, which measures the strength of the signal
    received from the device. channel_rssi The signal strength on the specific channel
    on which the message was received. snr Signal-to-Noise Ratio - The signal-to-noise
    ratio, which indicates the quality of the received signal. location.latitude The
    geographic latitude of the location of the gateway that received the message.
    location.longitude The geographic longitude of the location of the gateway that
    received the message. location.altitude The geographic altitude of the location
    of the gateway that received the message. uplink_token A token associated with
    the uplink message. channel_index The index of the channel used to transmit the
    message. uplink_message.settings.data_ rate.lora.spreading_factor The spreading
    factor used in the LoRa modulation of the message. uplink_message.received_at
    The date and time the uplink message was received. uplink_message.consumed_airtime
    The airtime consumed by the uplink message in the network. Regarding the decision
    factors for feature selection, we have focused on two determining elements, the
    numerical factor obtained in the correlation study, from the perspective of \"Pearson''s
    Correlation\" [31], and also on the know-how of the technical staff, since the
    experience gained from daily work with IoT traffic helps us to know which attributes
    can contribute to indicate infrastructure failures. The elements that were finally
    selected have correlation indexes from 0.1 to 0.99, indicating different levels
    of correlation between the different values. If we look at the central part of
    Fig. 3, we realize that most of the values present fall within this range of correlation
    values between each other. After this first indicator, we move on to the experience
    factor of the technical staff. During data transmission, there are essential values
    to control such as the noise signal, the received signal strength indicator, the
    byte packet sent, or the gateway that received the packet, amongst others. For
    this reason, these attributes will be taken into account in the analysis regardless
    of their correlation factor. The discarded values are fundamentally unique values,
    credentials to be exact, which are repeated during data sending, providing information
    such as the cluster in which the device is located, or the fixed port through
    which it works, so it did not provide us with useful information for the detection
    of anomalies in our system. Finally, another important operation is the transformation
    of categorical values to nominal values. In the dataset there are many columns
    of data indicating identifiers of network elements, services, or application.
    One Hot Encoding labelling is used for this purpose, converting the text data
    into feature vectors that can be further processed. 3.3. Theoretical considerations
    After the analysis of viable AI techniques in our context, it was determined that
    the Isolation Forest algorithm can generate the most correct results. This algorithm
    belongs to the most widely used unsupervised algorithms for anomaly detection
    and provides great flexibility in training as it does not need to label the data
    as valid or invalid from the beginning. This type of algorithm works on the detection
    of erroneous and unlabelled values within the datasets, also called outliers or
    anomalies. When training and implementing this type of algorithms, it has been
    demonstrated in different studies its efficient performance when dealing with
    large volumes of data, in addition to presenting a linear time complexity with
    a very low memory cost [32]. In our work, we will define outliers as: \"an observation
    that, being atypical and/or erroneous, deviates decidedly from the general behaviour
    of the experimental data with respect to the criteria that should be analysed
    about it\" [33]. The methodology employed by the Isolation Forest algorithm is
    based on the detection of outliers by using decision trees to isolate outliers
    from the rest of the data. To do this, a feature is selected and a random split
    between the minimum and maximum value is performed, repeating this process until
    all possible data splits are performed, or a specified limit on the number of
    splits is reached. The number of divisions needed to isolate a data item will
    be smaller for an outlier, while for normal values the number of divisions will
    be larger, since the algorithm attributes to each division an \"anomaly score\",
    calculated as the average of the number of subdivisions needed to isolate the
    outlier. The \"anomaly score\" is a value calculated using the following formula
    (1): (1) The parameters of which are: • h(x): is the average depth of constructed
    trees. • c(n): is the average height to find a node in one of the trees. • n:
    size of the dataset. • s: if the value obtained is close to 1, it is generally
    an anomaly, while if the value of s is less than 0.5, it is a correct value. The
    term E(h(x)) in formula (1) is calculated as: (2) where t is a tree, c(|lt(x)|)
    is a normalization factor needed when t is not fully grown (which estimates the
    average tree depth that can be constructed from lt(x)) and ht(x) = |Pt(x)| with
    Pt(x) being the path of x, the set of nodes visited by x from the root to the
    leaf containing x. From formula (1) it can be inferred that the score of an object
    x is proportional to the inverse of the average length of its path in the forest:
    if x ends in very deep leaves of the trees, its score will be quite low (close
    to 0), if on the contrary its path ends very soon the score will be high (close
    to 1) [34]. The reason for using this algorithm over other existing algorithms
    is mainly because it is an easily scalable algorithm for use on large datasets.
    In addition, it works well when features that may initially be irrelevant are
    included, as multi-modal datasets. This is the case for IoT infrastructures, where
    the cohesion or internal correlation between the data being sent is unknown, and
    we simply acquire a dataset with its corresponding parameters and want to detect
    outliers. In terms of implementation for model building, we must consider the
    basic elements with which we can train and subsequently improve the accuracy of
    the result: • \"contamination\", the amount of overall data that we expect to
    be considered outliers, indicates the estimated proportion of outliers that the
    dataset possesses. Based on this value, the limit by which the values are classified
    as anomalous or normal is set. • “n_estimators\", this value represents the number
    of isolation trees to be used to construct the Isolation Forest itself. Using
    higher estimator values can improve the accuracy for detection, but should always
    be appropriate to the dataset used, as it also results in increased training time.
    Varying the value of this parameter will serve to adjust the final performance
    of the model. • “max_samples\", number of observations used to train each tree;
    serves to control the maximum number of samples to be used to train each tree
    generated. You can use the value of auto which implies that all the samples in
    the dataset will be used, however, if you have a dataset that is too large, this
    value will have to be readjusted. • “max_features\", which indicates the maximum
    number of features to be used when splitting each node of the tree. By setting
    the value to 1, we specify that all available features will be used for each split.
    This parameter can be used as such in this case since many columns have been filtered
    during the data fitting process, however, if the dataset has many different columns,
    it can be adjusted in a way that improves performance and avoids overfitting.
    4. Implementation and analysis of results 4.1. Experimental setup The experimentation
    was carried out on an HP Pro-SFF 400 G9, with Windows 11 Pro 64-bit operating
    system, Intel® Core I7–12,700 CPU up to 4.9 GHz and 12 cores, 32GB RAM, and NVIDIA
    Quadro T400 graphics card. For the development of the algorithm, use was made
    of different libraries. NumPy [35] was used to carry out the relevant numerical
    arrays to process data, and Pandas [36] was used for the analysis and manipulation
    of structured data, providing the DataFrame format required for the algorithm.
    As for the implementation of the algorithm, the scikit-learn library [37] was
    used for its development, specifically the modules: sklearn.ensemble.IsolationForest,
    as the final algorithm for anomaly detection; sklearn.model_selection.train_test_split,
    to split the dataset for the training, testing and validation phases; sklearn.impute.SimpleImputer,
    mainly used for handling null values in the dataset, so that missing values are
    filled in a certain way; and finally, sklearn.preprocessing.RobustScaler, whose
    function is to scale features in a dataset, being very useful in datasets that
    may contain outliers. For the implementation of our model in particular, the following
    parameters were used in the configuration of the Isolation Forest algorithm. •
    contamination=float(0.1667). In the case of the training dataset, it is estimated
    that 16.67 % of the data contains an erroneous value that must be detected, so
    the training is adjusted to this contamination rate. • n_estimators=100 • max_samples=''auto
    • max_features=1.0 The choice of parameters depends on several issues and may
    condition the effectiveness of the model. The parameter \"contamination\" depends
    on the known number of erroneous packets, this value is known since the erroneous
    packets are introduced in a random but controlled way. The value of \"max_samples\"
    is set to \"auto\" to take all the data in the set, and the value of \"max_features\"
    is set to \"1″ to use all the properties of the dataset, since we have previously
    selected the significant properties. To set the value of \"n_estimators\", an
    empirical study was carried out for which: a reduced dataset was generated from
    the original one; and the Isolation Forest algorithm was tested by varying only
    the value of n_estimators until reaching the minimum value above which detection
    was no longer improved. Once the training is finished, we can visualise the results
    obtained for each field of the dataset. In order to have a graph for each field,
    we consider as a common element the \"time\" field (the timestamp of the packet),
    so the following graphs correspond to the time field, and another parameter of
    the dataset, such as snr, uplink_message.senssion_key_id,channel_index,uplink_message.setittings.data_rate.lora.spreading_factor,uplink_message.consumed_airtime
    or uplink_message.decode_payload.bytes. The following Fig. 4, Fig. 5, Fig. 6,
    Fig. 7, Fig. 8, Fig. 9 is the result of the training with some variables where
    the outliers can be seen. Download : Download high-res image (233KB) Download
    : Download full-size image Fig. 4. Result of the detection on the snr column in
    relation to time. Download : Download high-res image (244KB) Download : Download
    full-size image Fig. 5. Detection result on the uplink_message_session_key_id
    column in relation to time. Download : Download high-res image (208KB) Download
    : Download full-size image Fig. 6. Detection result on the channel_index column
    in relation to time. Download : Download high-res image (180KB) Download : Download
    full-size image Fig. 7. Result of the detection on the uplink_message_settings_data_rate.lora.spreading_factor
    column in relation to time. Download : Download high-res image (185KB) Download
    : Download full-size image Fig. 8. Result of the detection on the uplink_message.consumed_airtime
    column in relation to time. Download : Download high-res image (188KB) Download
    : Download full-size image Fig. 9. Result of the detection on the uplink_message.decoded_payload.bytes
    column in relation to time. 4.2. Analysis of results and evaluation The marking
    of outliers requires a subsequent analysis and evaluation, to validate whether
    or not errors are really being detected. To this end, a sampling has been carried
    out on the set of outliers, selecting several dozens to analyse them in detail
    and find out the causes of their cataloguing as outliers, and therefore anomalous
    value within the packet flow. The detailed analysis has identified that most of
    the anomalous packets are due to three types of malfunctions. 4.2.1. Drop in signal
    strength One of the first anomalies detected by the system was a sudden change
    in the signal strength of the sensors. This issue in a device that is stable in
    its power was unusual, so the system flagged an anomaly at that instant, and after
    analysis of this detection, it was possible to locate that there had been a change
    in the position of the antenna that caused a shielding of the signal. Fig. 10
    shows an instant in which this sudden change in signal power detected by the algorithm
    can be seen. Download : Download high-res image (197KB) Download : Download full-size
    image Fig. 10. Sudden change of signal power. 4.2.2. Detection of unusual gateways
    Another anomaly detected by the algorithm is the appearance of gateways that do
    not belong to the organisation''s infrastructure. This can occur because the Lora
    technology used has a large range and by using The Things Networks as the base
    application, there are more open-source antennas in the project nearby Fig. 11.
    Being something out of the ordinary, two possibilities are established, the probe
    has changed location and with it the gateways around it, or a new gateway has
    been activated within its range, without being one of the ones they have configured
    from the Smart University group. After analysing the packet following the detection
    of the anomaly, using the TTN Mapper tool [38] it was concluded that one of the
    devices had not only been moved from a room, but also taken out of the university
    area, without anyone notifying of this fact. Download : Download high-res image
    (466KB) Download : Download full-size image Fig. 11. Location of the gateway detected
    from TTN Mapper. 4.2.3. Receiving messages without a data packet Finally, one
    of the most important anomalies detected is when a device stops sending data within
    the packet. This is the case when a sensor is not working properly, but the board
    to which it is connected to the sensor, together with its LoRa antenna, are in
    good condition, so that the packets arrive at the gateway, but there is no coded
    message inside. Fig. 12 shows an example of one of the devices that exhibited
    this behaviour. Download : Download high-res image (399KB) Download : Download
    full-size image Fig. 12. Sample of the data sending flow until its sudden cut-off.
    As can be seen in the image, from approximately 13:38, the sensor stopped emitting
    quantitative values, whether they were C02, GPS, or other sensors. In the IoT
    platform of the University of Alicante, the data could not be visualised, while
    the package had been processed. This implies that data is being received by the
    platform, but does not actually exist, which can lead to errors and contradictory
    actions in the IoT platform. 5. Conclusions Several interesting results have been
    generated through this project. Firstly, a model for anomaly detection based on
    Machine Learning on IoT infrastructure has been proposed. One of the most outstanding
    characteristics of IoT infrastructures is precisely not having control over them,
    using services and even open intermediate infrastructures (as in our case TTN).
    This means that we are faced with the ignorance of information, which in traditional
    network infrastructure is very useful for detecting anomalies, such as IPs, MACs
    or protocols. Our model aims to perform broad-spectrum detections, i.e., to detect
    packets that are not correct, remove them from the data ingestion flow in the
    applications that use the IoT, and then analyse them and extract the anomaly.
    An analysis of the characteristics of the IoT data packets has also been carried
    out, choosing the parameters to be considered for anomaly analysis after the correlation
    study, together with our own experience gained from working with the incoming
    packets on the TTN platform. This has allowed us to focus and narrow down the
    dimension in which these unexpected values can be detected, a factor that has
    helped to improve the quality of the anomalies detected and to optimise the training
    time. Based on the model developed, an implementation has been carried out on
    the campus of the University of Alicante, based on IoT infrastructure, The Things
    Network, which uses the open-source software The Thing Stack as a base, which
    is widely used by the IoT community. This instance of the algorithm has allowed
    to implement an anomaly detector on the real IoT platform of the University of
    Alicante, so that it has been possible to detect possible failures in the infrastructure
    and unexpected situations in terms of the behaviour of the sensors, has allowed
    to take measures in this regard, being an important support point for the overall
    monitoring of the project. In other words, the proposal is valid, it is useful,
    and it is up and running. And it can be used by the TTN community. The proposed
    work has some limitations. The most important one is the resources needed to train
    the algorithm, since it is necessary to perform the learning process with large
    volumes of data, including a large variety of packets, originating from the correct
    operation of the infrastructure to quickly detect anomalies. This affects us especially
    when there are permanent changes in the infrastructures since it is necessary
    to retrain the algorithm with this new, but unknown, configuration. If retraining
    is not performed, the new infrastructure elements would generate packets that
    would be flagged as anomalies by the ADS. Another limitation of the proposal is
    that the anomaly is not classified, which requires further analysis by the administrator
    after detection. As for possible improvements and future work, we want to implement
    different Machine Learning modules for anomaly detection and identification. First,
    we will look for the comparison of different algorithms that allow us to detect
    more situations of anomalies, to avoid that incorrect packets can \"sneak in\".
    We are even considering the possibility of having several algorithms processing
    in parallel and generating an aggregation of their results. This first step should
    serve to solve the problem of the resources needed for training. And, secondly,
    once we can discriminate packets and put them in a state of observation, other
    specialised algorithms can be used to generate a cataloguing that identifies the
    specific anomaly, such as: transmission failures, device subtraction, device malfunction
    or the implantation of unexpected devices (a very common problem in IoT, which
    appear to be sensors of unknown origin). Then, chaining anomaly detection systems
    with anomaly identification can help to have scalable, heterogeneous systems where
    there is uncertainty due to the lack of knowledge of uncontrolled intermediate
    IoT infrastructures. Being able to obtain a library of trained modules for different
    infrastructures would facilitate the construction of more efficient systems as
    it would only incorporate the necessary modules, and could even automate the system
    to search, amongst a possible collection of analysis and identification modules,
    those that best fit the infrastructure or circumstances. Declaration of competing
    interest The authors declare the following financial interests/personal relationships
    which may be considered as potential competing interests: Jose Vicente Berna Martinez
    reports financial support was provided by University of Alicante. Acknowledgement
    This project has been funded by the UAIND22–01B project \"Adaptive control of
    urban supply systems\" of the University of Alicante. Data availability The data
    that has been used is confidential. References [1] G. Eason, B. Noble, I.N. Sneddon
    On certain integrals of Lipschitz-Hankel type involving products of Bessel functions
    Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Sci., 247 (935) (1955), pp. 529-551
    Google Scholar [2] T.H. Nasution, M.A. Muchtar, A. Simon Designing an IoT-based
    air quality monitoring system Proceedings of the IOP Conference Series: Materials
    Science and Engineering, 648, IOP Publishing (2019), Article 012037 CrossRefView
    in ScopusGoogle Scholar [3] R. Minerva, G.M. Lee, N. Crespi Digital twin in the
    IoT context: a survey on technical features, scenarios, and architectural models
    Proc. IEEE, 108 (10) (2020), pp. 1785-1824 CrossRefView in ScopusGoogle Scholar
    [4] J.J. Davis, A.J. Clark Data preprocessing for anomaly based network intrusion
    detection: a review Comput. Secur., 30 (6–7) (2011), pp. 353-375 View PDFView
    articleView in ScopusGoogle Scholar [5] T.J. Veasey, S.J. Dodson Anomaly detection
    in application performance monitoring data Int. J. Mach. Learn. Comput., 4 (2)
    (2014), p. 120 CrossRefGoogle Scholar [6] K. Xu, Y. Qu, K. Yang A tutorial on
    the internet of things: from a heterogeneous network integration perspective IEEE
    Netw., 30 (2) (2016), pp. 102-108 View in ScopusGoogle Scholar [7] Blenn, N.,
    & Kuipers, F. (2017). LoRaWAN in the wild: measurements from the things network.
    arXiv preprint arXiv:1706.03086. Google Scholar [8] Z. Ahmad, A. Shahid Khan,
    C. Wai Shiang, J. Abdullah, F. Ahmad Network intrusion detection system: a systematic
    study of machine learning and deep learning approaches Trans. Emerg. Telecommun.
    Technol., 32 (1) (2021), p. e4150 View in ScopusGoogle Scholar [9] Fernández Oliva,
    A., Maciá Pérez, F., Berna-Martinez, J.V., & Abreu Ortega, M. A Meth-od non-deterministic
    and computationally viable for detecting outliers in large datasets. (2020). Google
    Scholar [10] T. Gu, A. Abhishek, H. Fu, H. Zhang, D. Basu, P. Mohapatra Towards
    learning-automation IoT attack detection through reinforcement learning Proceedings
    of the 2020 IEEE 21st International Symposium on\" A World of Wireless, Mobile
    and Multimedia Networks\"(WoWMoM), IEEE (2020), pp. 88-97 CrossRefView in ScopusGoogle
    Scholar [11] C. Perera, A. Zaslavsky, P. Christen, D. Georgakopoulos Context aware
    computing for the internet of things: a survey IEEE Commun. Surv. Tutor., 16 (1)
    (2014), pp. 414-454 View in ScopusGoogle Scholar [12] M.M. Hasan, M.M. Islam,
    I.I. Zarif, M. Hashem Attack and anomaly detection in IoT sensors in IoT sites
    using machine learning approaches Internet Things, 7 (2019), Article 100059, 10.1016/j.iot.2019.100059
    View PDFView articleView in ScopusGoogle Scholar [13] A. Ukil, S. Bandyoapdhyay,
    C. Puri, A. Pal IoT healthcare analytics: the importance of anomaly detection
    Proceedings of the 2016 IEEE 30th International Conference on Advanced Information
    Networking and Applications (AINA), IEEE (2016), pp. 994-997 View in ScopusGoogle
    Scholar [14] A. Chatterjee, B.S. Ahmed IoT anomaly detection methods and applications:
    a survey Internet Things, 19 (2022), Article 100568 View PDFView articleView in
    ScopusGoogle Scholar [15] H.F. Nweke, Y.W. Teh, M.A. Al-Garadi, U.R. Alo Deep
    learning algorithms for human activity recognition using mobile and wearable sensor
    networks: state of the art and research challenges Expert. Syst. Appl., 105 (2018),
    pp. 233-261 View PDFView articleView in ScopusGoogle Scholar [16] L. Erhan, M.
    Ndubuaku, M. Di Mauro, W. Song, M. Chen, G. Fortino, A. Liotta Smart anomaly detection
    in sensor systems: a multi-perspective review Inf. Fusion, 67 (2021), pp. 64-79
    View PDFView articleView in ScopusGoogle Scholar [17] J.E. Albuquerque Filho,
    L.C. Brandão, B.J. Fernandes, A.M Maciel A Review of Neural Networks For Anomaly
    Detection IEEE Access (2022) Google Scholar [18] M. Hosseinzadeh, A.M. Rahmani,
    B. Vo, M. Bidaki, M. Masdari, M. Zangakani Improving security using SVM-based
    anomaly detection: issues and challenges Soft Comput., 25 (2021), pp. 3195-3223
    CrossRefView in ScopusGoogle Scholar [19] R. Primartha, B.A. Tama Anomaly detection
    using random forest: a performance revisited Proceedings of the 2017 International
    Conference on Data and Software Engineering (ICoDSE), IEEE (2017), pp. 1-6 View
    in ScopusGoogle Scholar [20] V. Chandola, A. Banerjee, V. Kumar Anomaly detection:
    a survey ACM Comput. Surv. (CSUR), 41 (3) (2009), pp. 1-58 CrossRefGoogle Scholar
    [21] I. Cid, L.R. Janeiro, J.R. Méndez, D. Glez-Peña, F. Fdez-Riverola The impact
    of noise in spam filtering: a case study Advances in Data Mining. Medical Applications,
    E-Commerce, Marketing, and Theoretical Aspects: 8th Industrial Conference, ICDM
    2008 Leipzig, Germany, July 16-18, 2008, Proceedings, 8, Springer Berlin Heidelberg
    (2008), pp. 228-241 CrossRefView in ScopusGoogle Scholar [22] O.Z. Maimon, L.
    Rokach Data Mining With Decision Trees: Theory and Applications, 81, World scientific
    (2014) Google Scholar [23] V. Vundavalli, F. Barsha, M. Masum, H. Shahriar, H.
    Haddad Malicious URL detection using supervised machine learning techniques Proceedings
    of the 13th International Conference on Security of Information and Networks (2020),
    pp. 1-6 CrossRefGoogle Scholar [24] B. Janet, R.J.A. Kumar Malicious URL detection:
    a comparative study Proceedings of the 2021 International Conference on Artificial
    Intelligence and Smart Systems (ICAIS), IEEE (2021), pp. 1147-1151 Google Scholar
    [25] M. Douiba, S. Benkirane, A. Guezzaz, M. Azrour An improved anomaly detection
    model for IoT security using decision tree and gradient boosting J. Supercomput.,
    79 (3) (2023), pp. 3392-3411 CrossRefView in ScopusGoogle Scholar [26] J. Lesouple,
    C. Baudoin, M. Spigai, J.Y. Tourneret Generalized isolation forest for anomaly
    detection Pattern Recognit. Lett., 149 (2021), pp. 109-119 View PDFView articleGoogle
    Scholar [27] F.T. Liu, K.M. Ting, Z.H. Zhou Isolation forest Proceedings of the
    2008 8th IEEE International Conference on Data Mining, IEEE (2008), pp. 413-422
    View in ScopusGoogle Scholar [28] S. Dhankhad, E. Mohammed, B. Far Supervised
    machine learning algorithms for credit card fraudulent transaction detection:
    a comparative study Proceedings of the 2018 IEEE International Conference on Information
    Reuse and Integration (IRI), IEEE (2018), pp. 122-125 CrossRefView in ScopusGoogle
    Scholar [29] DS2OS traffic traces, Kaggle. https://www.kaggle.com/francoisxa/ds2ostraffictraces.
    Accessed September 2023. Google Scholar [30] The Things Industries. Platform For
    LoraWAN Networks Server. www.thethingsindustries.com. Accessed September 2023.
    Google Scholar [31] I. Jebli, F.Z. Belouadha, M.I. Kabbaj, A. Tilioua Prediction
    of solar energy guided by Pearson correlation using machine learning Energy, 224
    (2021), Article 120109 View PDFView articleView in ScopusGoogle Scholar [32] M.
    Mohy-eddine, A. Guezzaz, S. Benkirane, M. Azrour An effective intrusion detection
    approach based on ensemble learning for IIoT edge computing J. Comput. Virol.
    Hacking Techn. (2022), pp. 1-13 Google Scholar [33] D.M. Hawkins Identification
    of Outliers, 11, Chapman and Hall, London (1980) Google Scholar [34] A. Mensi,
    M. Bicego A novel anomaly score for isolation forests Image Analysis and Processing–ICIAP
    2019: 20th International Conference, Trento, Italy, September 9–13, 2019, Proceedings,
    Part I, 20, Springer International Publishing (2019), pp. 152-163 CrossRefView
    in ScopusGoogle Scholar [35] NumPy. Package For Scientific Computing With Python.
    https://numpy.org/. Accessed September 2023. Google Scholar [36] Pandas. Open
    Source Data Analysis and Manipulation Tool. https://pandas.pydata.org/. Accessed
    September 2023. Google Scholar [37] F. Pedregosa, G. Varoquaux, A. Gramfort, V.
    Michel, B. Thirion, O. Grisel, É. Duchesnay Scikit-learn: machine learning in
    Python J. Mach. Learn. Res., 12 (2011), pp. 2825-2830 Google Scholar [38] TTN
    Mapper. Tool to Map Coverage of The Things Networks. https://ttnmapper.org/. Accessed
    September 2023. Google Scholar Cited by (0) © 2024 The Authors. Published by Elsevier
    B.V. Recommended articles Detecting cyberthreats in Metaverse learning platforms
    using an explainable DNN Internet of Things, Volume 25, 2024, Article 101046 Ebuka
    Chinaechetam Nkoro, …, Dong-Seong Kim View PDF Are perfect transcripts necessary
    when we analyze classroom dialogue using AIoT? Internet of Things, Volume 25,
    2024, Article 101105 Deliang Wang, Gaowei Chen View PDF Advancing 6G-IoT networks:
    Willow catkin packet transmission scheduling with AI and bayesian game-theoretic
    approach-based resource allocation. Internet of Things, Volume 25, 2024, Article
    101119 Ali. M. A. Ibrahim, …, Wail M. Idress View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 825821490 Captures Readers: 8 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Internet of Things (Netherlands)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Anomaly detection system for data quality assurance in IoT infrastructures
    based on machine learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Das A.
  - Singh N.
  - Chakraborty S.
  citation_count: '0'
  description: Next-generation smart city applications, attributed to the power of
    the Internet of Things (IoT) and Cyber–Physical Systems (CPS), significantly rely
    on sensing data quality. With an exponential increase in intelligent applications
    for urban development and enterprises offering sensing-as-a-service these days,
    it is imperative that a shared sensing infrastructure could thwart the better
    utilization of resources. However, a shared sensing infrastructure that leverages
    low-cost sensing devices for a cost-effective solution remains unexplored territory.
    A significant research effort is still needed to make edge-based data shaping
    solutions more reliable, feature-rich, and cost-effective while addressing the
    associated challenges in sharing the sensing infrastructure among multiple collocated
    services with diverse Quality of Service (QoS) requirements. Towards this, we
    propose UniPreCIS, a novel edge-based data preprocessing solution that accounts
    for the inherent characteristics of low-cost ambient sensors and their exhibited
    measurement dynamics concerning application-specific QoS. UniPreCIS aims to identify
    and select quality data sources by performing sensor ranking and selection that
    dynamically adapts to the change in sensor attributes. Finally, multimodal data
    preprocessing is performed in a unified manner to meet heterogeneous application
    QoS and, at the same time, reduce the resource consumption footprint for the resource-constrained
    network edge. We study the effectiveness of UniPreCIS on a real-world testbed
    deployed on our campus. As observed, the processing time and memory utilization
    of the stakeholder services have been reduced in the proposed approach while achieving
    up to 90% accuracy, which is arguably significant compared to state-of-the-art
    sensing techniques.
  doi: 10.1016/j.future.2023.11.029
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Background 3. System design
    and assumptions 4. Proposed methodology 5. Experimental setup 6. Results and discussions
    7. Literature survey 8. Conclusion CRediT authorship contribution statement Declaration
    of competing interest Acknowledgment Appendix. Data availability References Vitae
    Show full outline Figures (20) Show 14 more figures Tables (6) Table 1 Table 2
    Table 3 Table 4 Table Table Future Generation Computer Systems Volume 153, April
    2024, Pages 543-557 UniPreCIS: A data preprocessing solution for collocated services
    on shared IoT Author links open overlay panel Anirban Das a, Navlika Singh b,
    Suchetana Chakraborty b Show more Share Cite https://doi.org/10.1016/j.future.2023.11.029
    Get rights and content Highlights • A novel data preprocessing solution for shared
    IoT infrastructure. • A novel multi-dimensional sensor selection approach as per
    the service requirements.. • Low-cost modeling of sensor characteristics that
    are dynamic in nature. • Unified preprocessing of sensor data for resource consumption
    optimization. Abstract Next-generation smart city applications, attributed to
    the power of the Internet of Things (IoT) and Cyber–Physical Systems (CPS), significantly
    rely on sensing data quality. With an exponential increase in intelligent applications
    for urban development and enterprises offering sensing-as-a-service these days,
    it is imperative that a shared sensing infrastructure could thwart the better
    utilization of resources. However, a shared sensing infrastructure that leverages
    low-cost sensing devices for a cost-effective solution remains unexplored territory.
    A significant research effort is still needed to make edge-based data shaping
    solutions more reliable, feature-rich, and cost-effective while addressing the
    associated challenges in sharing the sensing infrastructure among multiple collocated
    services with diverse Quality of Service (QoS) requirements. Towards this, we
    propose UniPreCIS , a novel edge-based data preprocessing solution that accounts
    for the inherent characteristics of low-cost ambient sensors and their exhibited
    measurement dynamics concerning application-specific QoS. UniPreCIS aims to identify
    and select quality data sources by performing sensor ranking and selection that
    dynamically adapts to the change in sensor attributes. Finally, multimodal data
    preprocessing is performed in a unified manner to meet heterogeneous application
    QoS and, at the same time, reduce the resource consumption footprint for the resource-constrained
    network edge. We study the effectiveness of UniPreCIS on a real-world testbed
    deployed on our campus. As observed, the processing time and memory utilization
    of the stakeholder services have been reduced in the proposed approach while achieving
    up to 90% accuracy, which is arguably significant compared to state-of-the-art
    sensing techniques. Previous article in issue Next article in issue Keywords Shared
    IoTDynamic sensor ranking and selectionSensor attributesCollocated servicesEdge
    preprocessing 1. Introduction Rapid advent in the Internet of Things (IoT) and
    Cyber-Physical Systems (CPS), along with the emerging demand for ubiquitous intelligent
    services across all smart city verticals, have strongly motivated the rise of
    ambient sensing. For being less intrusive (unlike cameras and wearables), ambient
    sensing [1] has risen as a more practical solution in numerous use cases. Towards
    effective utilization of the resources and delivery of improved Quality of Services,
    next-generation IoT is moving towards shared infrastructure for sensing [2]. Multiple
    applications can therefore share the deployed sensory sources [3] for individual
    computation. In parallel to this, the adoption of low-cost sensors [4], [5], [6]
    is becoming more pervasive across numerous domains. These two factors, in turn,
    significantly encourage the decoupling of IoT service providers from hardware
    deployment authority [7] as a cost-effective, easy-to-manage, and quick-deployable
    solution for the service providers. The ideal implication of this sensing-as-a-service
    provisioning can be characterized mainly by two prominent features: heterogeneity
    and redundancy. Variety of intelligent services [8] running at the edge depends
    on the quality of data generated from a massive amount of sensory sources of different
    types and deployed in abundance. This primarily addresses two aspects: fault tolerance
    and coverage. While the redundancy can offer resilience to error and heterogeneity
    has the power to uncover the feature-rich insights of the sensing context. It
    is worth noting that a redundant instance of a sensor not just facilitates fault
    tolerance but in fact is a necessity for shared infrastructure, as a single instance
    of a sensor may not be ideal for all stakeholder services in terms of its functional
    characteristics. However, redundancy adds to the overhead of data processing [9].
    For the sensors, the accuracy and reliability of measurements, which characterize
    the data quality, not only depend on the fabrication quality of the sensors or
    various environmental factors like dust, humidity, pressure, etc. but also the
    location of the deployment that feeds a running microservice [10], [11]. The level
    of noise associated with the sensory data could show a significant temporal variation
    due to the inherent characteristics of MEMS (Micro-Electro-Mechanical Systems)
    devices. Each service has certain Quality of Service (QoS) constraints, such as
    timely inference, reliability, measurement accuracy, etc. For instance, a smart
    Fire Alarm requires critical timeliness, whereas a smart HVAC (Heating, Ventilation,
    and Air-Conditioning) could be more aligned with users’ preferences and comfort.
    It is evident that these QoS constraints vary, and their adequacy is overwhelmingly
    dependent on the underlying hardware infrastructure. However, with hardware deployment
    being separated, finding a set of perfectly matched hardware or ideally located
    sensors on the part of a service deployment authority is only sometimes feasible.
    Traditionally, the redundancy, noise, and error embedded in the sensory data are
    preprocessed [12], [13] for effective utilization. However, considering the multiplicity,
    heterogeneity, and resource scarcity towards the edge, the overall cost of streaming
    a considerable amount of multimodal data during continuous monitoring, [14] could
    be significant. Recent times have observed significant attention for data management
    towards the edge [15]. Addressing the challenging task of data management towards
    the edge, enterprise-level solutions such as Apache Kafka, RabbitMQ, Apache Spark,
    Confluent, Apache Storm, etc., support processing and managing heterogeneous IoT
    data in an edge-cloud architecture. These frameworks support various preprocessing
    functionalities like aggregation, compression, and fusion and are state-of-the-art
    for real-life use cases on dedicated IoT infrastructure. However, to the best
    of our knowledge, no existing framework for edge-based data preprocessing offers
    the shaping of data in terms of quality and quantity towards the source for multiple
    services on shared sensory infrastructure. In this direction, we identify the
    following set of challenges: • Challenge 1: When redundant sources are available,
    how can the amount of data be reduced, and poorly performing (less reliable) sources
    be suppressed towards the extreme edge of the network? • Challenge 2: When redundant
    sources are available, how can we choose the most appropriate set of sources for
    a specific service out of all the services relying on that particular modality?
    • Challenge 3: While selecting a sensory source, only measurement accuracy may
    not be an adequate characteristic as a deciding factor; in such a case, how to
    address multiple characteristics such as response time, resolution, etc. of the
    sensory sources while selecting a set of sources? • Challenge 4: While performing
    multi-factor based selection, it is evident that not all the characteristics of
    a sensory source are consistent over time; in such a case, how to address this
    dynamicity? • Challenge 5: When multiple services rely on shared sensory sources,
    how can we take advantage of this while performing data processing to achieve
    certain benefits in terms of time and available resources? • Challenge 6: How
    can the needs of modalities (fed by the sensory sources) of individual depending
    services be formulated while preserving the necessary information, at the same
    time reducing the overall computation cost (in terms of resource utilization)?
    To address these questions, we introduce UniPreCIS (Unified Preprocessing for
    Collocated IoT Services), a novel edge-based data preprocessing solution for shared
    IoT scenarios. The proposed preprocessing solution can effectively reduce the
    amount of data generated towards the edge of the network thereby reducing the
    transmission cost of redundant data. The key contribution of this work can be
    summarized as follows: • Towards data quality regulation, a novel multi-dimensional
    sensor ranking and selection approach based on dynamic sensor characteristics
    and application-specific QoS feedback has been introduced. The sensor ranking
    and selection addresses challenges 1–3, while modeling of dynamic characteristics
    addresses challenge 4. • An edge-based sensory data preprocessing framework has
    been proposed to fuse multimodal data and feed a set of collocated microservices
    corresponding to various smart applications sharing an IoT infrastructure. This
    aims to address challenges 5 and 6. • Experimental results from testbed implementation
    demonstrate the effectiveness. The framework can reduce the overall processing
    time by up to 23% in our evaluation case. At the same time, the proposed sensor
    selection maintained the classification accuracy of a use case beyond 90% without
    compromising the computation cost. The rest of the paper is organized as follows.
    In Section 2, we provide the background information to motivate the problem. We
    then introduce the system design and modeling in Section 3. Next, in Section 4
    we introduce UniPreCIS . In Section 5 we introduce the experiment design to study
    the performance of UniPreCIS . After that, in Section 6 we discuss the performance
    of UniPreCIS as per the experimental outcome. We state the relevant literature
    and conclusion in Section 7 and Section 8 respectively. Finally, we conclude with
    the appropriate proofs provided in Appendix. 2. Background In a shared IoT infrastructure,
    strong temporal correlation among different streams of generated data is the key
    requirement to leverage the power of multimodality and unfold the feature-rich
    deep insights of the sensing context. This section first briefly introduces relevant
    properties of the sensory data. Next it provides a summary of different sensor
    characteristics and the external factors influencing them. The list of various
    symbols used throughout the text and their corresponding meanings are listed in
    Table 1 Table 1. List of symbols. Symbol Meaning Set of all sensors th instance
    of sensor Measurement reported by sensor Set of all sensory measurements Total
    number of sensors th instance of sensor type Utopia vector Sensor attribute: Accuracy
    Sensor attribute: Reliability Sensor attribute: Resolution Sensor attribute: Response
    time Sensor attribute: Range Estimated minimum number of sensors that are optimally
    functioning p, c Scaling constants a, b Sensor attributes Simplistic representation
    of, Poisson parameter Total number of dimensions (sensor attributes) Euclidean
    component of the distance of a sensor from the Utopia sensor in m-dimensional
    space Cosine component of the distance of a sensor from the Utopia sensor in m-dimensional
    space Temporal correlation Spatial correlation 2.1. Data and its characteristics
    The sensors for continuous monitoring services usually generate data in the form
    of streams. We therefore first formalize the notification used to specify a data
    stream. After that, we introduce the key issues observed while fusing multiple
    data streams. 2.1.1. Formalization of data streams The data generated from any
    particular sensor device, can be expressed in the form of a set of tuples ( ,
    ) where is the th measured value with as the timestamp. That said the generated
    stream of data, for the specific sensor , can be expressed as, (1) Where is the
    last instance of measurements over time. 2.1.2. Fusion of multiple data streams
    A Linear Kalman Filter (KF) can be used to fuse the data streams from an available
    set of sensors [16] to a single stream. Apart from fusing the streams, the KF
    can effectively filter out any noises generated by the sensors [17]. However,
    incorrect reading can have a significant effect on the KF prediction. Let us consider
    a scenario, as shown in Fig. 1, where three streams from different humidity sensors
    have been plotted. In this case, sensor 3 was generating incorrect readings. The
    effect of these incorrect readings is visible as the output from the KF drifts
    far from the correct measurements of the rest of the two sensors. Download : Download
    high-res image (231KB) Download : Download full-size image Fig. 1. Variation of
    humidity with time. 2.2. Sensor and its characteristics The low-cost sensors significantly
    boost affordability which is a major reason for the rapidly growing research landscape
    of these sensors, and their wide use in various applications [18], [19], [20].
    However, there are certain characteristics where all are not equal. For instance,
    two low-cost temperature sensors (say DHT11 and DS18B20) can have different accuracy
    ( at 0 to 50 °C and at −10 to 85 °C respectively) and different measurement range
    (0 to 50 °C and −55 to 125 °C respectively) [21] due to the difference in make
    and model. Even for identical sensors, there can be missing values given a time
    interval. For instance, in Fig. 2, the raw data [22] from two identical temperature
    sensors can be seen to have an unequal quantity of data points generated given
    a time interval of 10:00 to 13:00. The sensor characteristics such as accuracy,
    response time, range of measurements, residual energy, resolution, and so on play
    a vital role in satisfying the QoS concerns of running services. For instance,
    a service may prefer a highly sensitive sensor to detect a minute change. In contrast,
    a different one may choose a sensor with a lower sensitivity to avoid rapid fluctuations.
    A particular sensor’s characteristics may be affected by various external factors
    such as interference, dust, human intervention, pressure, deployment, etc. For
    instance, a Passive Infra-Red (PIR) CO2 sensor may respond inadequately if polled
    before its warm-up period. Apart from that, many types of possible faults [23]
    may affect the sensor’s characteristics. Specific scenarios may employ multiple
    instances of homogeneous sensors for improved resolution, enrichment of data,
    or even mere extended coverage. Computation on data from all the sensory sources
    every time is computationally expensive and can also be affected by incorrect
    sensor measurements such as the one described in Fig. 1. Download : Download high-res
    image (262KB) Download : Download full-size image Fig. 2. Generated data points
    from two identical temperature sensors. 3. System design and assumptions A deployed
    set of sensors can be mapped dynamically to the stakeholder services while addressing
    the QoS awareness of these services. The system model for our solution can be
    represented by Fig. 3. A set of strategically deployed sensory sources measures
    the environment. The measurements are leveraged by the depending services while
    performing their respective computation. Towards this, we consider the following
    assumptions. • N no. of heterogeneous services run on the edge. The services require
    data from heterogeneous sensors. Multi-modal sensory data attributes are used
    to formulate the feature set meant to serve the stakeholder services, which is
    pretty generic. • A set of heterogeneous sensors periodically generate data to
    feed the available services. The sampling period of these sensors is independent.
    Time is the key entity stating the temporal correlation among the individual data
    points. • The characteristics of the available sensors, like accuracy, response
    time, etc., can be defined by numerical metrics, of which some are known apriori.
    We now build the problem in accordance with the system stated above. Download
    : Download high-res image (461KB) Download : Download full-size image Fig. 3.
    An overview of the system model. 3.1. Error model To study the sensor characteristics,
    we assume that the sensor properties change dynamically. Although a sensor can
    be characterized by a number of its attributes, such as resolution, accuracy,
    etc., in this work, we assume the dynamic modeling of accuracy and reliability.
    To address that, we assume that a faulty behavior demonstrated by a sensor instance
    is transient [24], which means the measurement error is temporary. Therefore,
    as per the assumption, an error in measurement is only limited to that instance
    of time and does not necessarily affect the subsequent measurements by the particular
    sensor. Such stochastic behavior of error occurrence is often modeled by Poisson
    distribution [25]. At this point, we can conclude that if we rely on a single
    sensory source, a random occurrence of error could result in system inconsistency.
    This motivates the need for dynamic sensor selection. Download : Download high-res
    image (221KB) Download : Download full-size image Fig. 4. Spurious readings generated
    by a Temperature sensor. 3.2. Need of dynamic sensor selection To understand the
    need for dynamic modeling of sensor attributes, we refer to the time series data
    collected from 3 instances of collocated temperature sensors of the same make
    and model from our testbed as shown in Fig. 4. Considering a single attribute,
    measurement accuracy, for instance, it is observable from the figure that initially
    all the sensors (sensor 1 sensor 2, and sensor 3) generate similar measurements.
    Considering the accuracy to be identical across all three sensors, a static sensor
    selection approach can therefore select any one of the three sensors. However,
    at time instances marked by and , sensor 1 generates incorrect measurements. At
    this point, a sensor selection approach that does not adapt [26] to the dynamicity
    of sensor accuracy will not be able to ignore the incorrect measurements performed
    by sensor 1 and may end up selecting it as the potential source of measurement.
    The effect is not negligible in case the frequency of occurrences of such spurious
    measurements is significant. This problem of the dynamic behavior of sensor characteristics
    becomes even more significant when we incorporate multiple sensor attributes such
    as reliability, range, etc. Therefore, there is a need for a sensor selection
    approach, that can dynamically adapt to changing sensor attributes over time.
    We now state the proposed solution to the problem specifically within the domain
    of shared IoT. Download : Download high-res image (315KB) Download : Download
    full-size image Fig. 5. Functional components of UniPreCIS . 4. Proposed methodology
    In this section, we introduce UniPreCIS , which eventually decouples the underlying
    sensor deployment from the relying services. The various functional components
    of UniPreCIS are shown in Fig. 5. UniPreCIS first isolates a set of optimal sensory
    sources to address the QoS awareness of the services. It then performs the preprocessing
    of the sensory measurements to generate service-specific features in a unified
    manner. We detail these two key components below. 4.1. Adaptive sensor ranking
    and selection Concerning the dynamic nature of the sensor characteristic as mentioned
    in Section 2.2, let the th sensor, of type , can be represented as, (2) where
    is the vector representation at time instance, . In this case, we limit the no.
    of sensor properties to five for simplicity and refer to them as for accuracy,
    for reliability, for resolution, for response time, and for a range of measurements.
    We assume that and are dynamic properties that vary over time. Whereas the rest
    adhere to the sensor documentation. For each service, we specify a set of sensor-specific
    attributes signifying the requirement by the service. The attributes can therefore
    be assigned a numerical magnitude adhering to this requirement. These are the
    ideal values corresponding to each attribute of the sensor which is expected by
    the specific service. We call this ideal vector, a utopia vector represented by,
    We propose modeling two sensor attributes in an effective and lightweight manner.
    4.1.1. Modeling of accuracy Since it is difficult to assess the correctness of
    the values reported by a sensor on the field [27], we exploit the existence of
    redundancy in the system. Towards that we combine a low-cost majority voting-based
    model [28] with dynamic inference [29] for assigning weights to a set of collocated
    homogeneous sensors. We discuss the idea below. Let a set of collocated sensors,
    sense the same environment. Let k, ( ) be the no. of sensors generating near-accurate
    value. Let be the measurements performed by the sensors within a given time interval.
    Therefore, given this set, , the task is to find the sensors with near-optimal
    measurements. Assumptions: • (threshold condition) is the minimum no. of sensors
    that are generating optimal values. • It is a single-dimensional linear system
    as the sensors are homogeneous. • The number of sensors is limited to a few hundred.
    The sensors generating near-optimal values will report measurements close to each
    other. Therefore, there exists a fuzzy correlation between each pair of measurements.
    We first normalize [30] the values as, Now, we define the fuzzy degree of membership
    function as, where, ( , , ) and ( , ) are two constants for scaling. Now we form
    a matrix, , with the degree of membership between each pair of measurements as,
    Let the matrix be of the form: Of course, can be optimally represented by an upper
    or lower triangular matrix due to the repetition of the members. Now, for each
    , we compute,1 as, (3) After that, we sort all the s in descending order. Finally,
    we isolate first elements from the sorted list. Let the sorted list be renamed
    (each term) as where is the first element of the sorted list, is the second element
    and so on. Since the values are computed on s, each value of has a corresponding
    on which it was computed (as in Eq. (3)). To find the optimal set of s, we now
    take the corresponding s of the sorted list of s and formulate, (4) Here, and
    the elements are sorted in descending order. Once the optimal set of sensors is
    formulated, we calculate the true state of the sensors by invoking a linear KF
    on . Let be the measurement calculated by KF( ). In that case, the value for accuracy
    of the sensor, , is given by, The ideal value, in this case, is one and represents
    the highest accuracy. The overall idea for the accuracy assignment can be presented
    by Algorithm 1. Download : Download high-res image (244KB) Download : Download
    full-size image 4.1.2. QoS oriented modeling of reliability A highly reliable
    sensor may be preferred over a highly accurate sensor in a mission-critical scenario.
    Therefore, if the system only depends on accuracy, it may sometimes collapse while
    handling an event. Although reliability can be modeled in various ways, here we
    ascertain reliability at the data level (Refer Section 7.3.2). Thus, the assignment
    of weight, corresponding to the reliability, to a sensor is performed based on
    the consistency in generating optimal values by the sensor over a certain period.
    To achieve this, we take advantage of multiple sensors. The measurements included
    in the set in the expression (4) can be referred to as near optimal values. That
    said, we have the following assumptions: • For all, , is a value which is close
    to correct. Similarly, for all , is a reported value that is likely to be an incorrect
    measurement. • The incorrect value measured is irrespective of hardware issues
    and noise. • Cumulative QoS demands to specify the expected data granularity within
    the time interval, and A sensor may not report a QoS demanded granularity at a
    specific interval of time, due to inherent configuration, thus skipping a value
    at a required instance. Let be the no. of expected values within the time interval
    . Let be the no. of correct values generated within . From this, we define the
    Poisson parameter, within Here, signifies the number of correct occurrences (measurements)
    and overall measurements done by the sensor. probability of during this time interval
    to be perfectly functioning, can be expressed using a Poisson distribution [31]
    defined as, where, Again, considering computed on various time intervals, , of
    variable lengths, the cumulative probability that the sensor, is correct can be
    computed within (where ) as, Here, is the interval of measurements. Since both
    and , either of these can be used as the assigned reliability metric for the sensor
    . That said, we have, The assignment of weight corresponding to reliability can
    thus be expressed in terms of Algorithm 2 Download : Download high-res image (163KB)
    Download : Download full-size image 4.1.3. Multi-dimensional sensor ranking The
    QoS specification of stakeholder services must match a specific set of attributes
    for a given sensor. However, practically, we cannot expect to find the ideal sensor
    in the field. Therefore, we instead need to find the closest match to the requirements
    of a stakeholder service. We discuss the finding of the most suitable sensor below.
    For all other static attributes , numerical values as magnitude can appropriately
    be assigned between [0, 1]. For simplicity, given the documented specification,
    , we state as, Therefore, we now have some values assigned to both static and
    dynamic attributes of a given sensor. Once the value of , and are assigned for
    each , the next is to perform a multi-dimensional ranking of the sensors. We consider
    an ideal sensor as the utopia vector of reference. As stated in Eq. (2), let be
    the utopia vector of reference and is constituted as, (5) and will have their
    maximum possible values as unity. However, the QoS specifications of the stakeholder
    services specify the characteristics to formulate the vector . When is common
    for both and , we can rewrite them as, Here, and are attributes of and , respectively.
    The selection of the most prominent sensor expressed as the compromised vector,
    based on all associated attribute specifications, can be performed by measuring
    the Euclidean distance between the positions of each sensor from the utopia sensor
    as, (6) In the multi-dimensional vector space, a directional component can be
    incorporated as, (7) Here, is the number of dimensions, each corresponding to
    a sensor attribute. Again, if the QoS specifications of the stakeholder services
    demand priority of one characteristic over another, a weighted version of and
    can be expressed as, (8) (9) where is the weight assigned, signifying the priority
    level of the particular attribute. Therefore, combining, Eqs. (6), (7) or Eqs.
    (8), (9), the direction aware distance, between each vector and the utopia vector
    can be expressed as [32], (10) where and are a pair of scaling coefficients, which
    can be expressed as the inverse of the average of and , respectively, for equal
    importance to both the components. Finally, the ranking of the sensors can be
    performed by sorting the sensors as per the value of their distances as measured
    by . The ranking and selection can be expressed as Algorithm 3. The selected streams
    from this can be further aggregated in the aggregation module. Download : Download
    high-res image (238KB) Download : Download full-size image 4.2. Unified data preprocessing
    Unified preprocessing is performed to reduce redundant preprocessing steps in
    a unified manner. Apart from performing redundant preprocessing steps as a single
    instance, unified preprocessing also has one critical significance: piping of
    low-level inference. As shown in Fig. 6, certain low-level inferences such as
    human presence detection, location information etc. could serve as contextual
    information for a set of context-aware services [33], [34] and are required for
    their primary inference. That being said, unified preprocessing covers two key
    aspects: data aggregation and feature extraction. Download : Download high-res
    image (87KB) Download : Download full-size image Fig. 6. Piping of low-level inference
    for feature formulation. 4.2.1. Aggregation To serve a service that requires a
    single stream from a group of sensors, individual streams from the selected sensor
    set are combined into a single stream. The module performs two steps, as presented
    below. Interpolation. It must be noted that a data point might not be available
    in a sensor-generated stream that a service needs at any specific instance. This
    is obvious due to the independent data generation period of sensors. On a stream,
    the interpolation can be performed as [35], (11) Here, and are two known data
    points in a time series (data stream), and and are their indices, respectively.
    is an interpolated data point at index . The linear interpolation method stated
    in Eq. (11) is effective and can be implemented efficiently on the stream [36].
    Data fusion. In this case, the term fusion refers to a combination of multiple
    data streams into a single stream. The resultant is a single continuous stream
    aggregating the numerous input sources. For sensor measurements where the expected
    co-variance is usually low (for instance, room temperature, humidity, etc.), a
    linear KF stated in Section 2.1.2 has been leveraged to combine the data streams
    into a single one effectively. A KF is chosen over a Moving Average because, with
    a moving average (MA), there is a challenge in selecting the window. A smaller
    window is highly affected by the noise in noisy measurements, whereas a larger
    one is much slower in reflecting the actual changes. Similarly, existing study
    [37] also shows that selection of the parameter, , the degree of weighting decrease
    ( ) for Exponential Moving Average (EMA), is a challenging task. A large value
    of causes a stronger influence of noise in the output and a smaller results in
    a slower convergence. 4.2.2. Feature extraction The final step is to formulate
    feature vectors for different services that perform machine inference on sensory
    measurements [38], [39]. Recall from Eq. (1) that a sensor data stream is composed
    of data points identifiable by timestamps. Feature formulation is achieved through
    three steps. Association of data and events. The data received from heterogeneous
    sensory sources could be sensed independently. However, the data needs to be correlated
    to associate these measurements with a specific event and, therefore, to be mapped
    in a particular service. To achieve this, we require two pieces of information
    that can be exploited: time and sensor collocation. The timestamps assigned to
    the data points signify temporal correlation, whereas the sensor collocation within
    a locality can signify spatial correlation. With that information, we can define
    two arbitrary functions: and . The outcome of these two functions is binary. It
    signifies if there exists a temporal and spatial correlation within the set of
    data points and need not be individually defined for each stakeholder service.
    Therefore, the necessary set of data points for a specific event and a specific
    service can be isolated with simple logic as, (12) The associated data points
    signify multimodal assessment corresponding to a specific event. Dynamic window
    formulation. The next task is formulating a window on the data streams for feature
    extraction. Let be the timestamp at which a window is to be formed on correlated
    data streams for the application . Let be the size of the window required. In
    that case, the window can be represented as where and are the timestamps at the
    two endpoints of the window. (13) where, and are the no. of intervals of size
    on either side of along the time axis. Feature vector formulation. A multi-dimensional
    feature vector is composed of information from heterogeneous sensors. Each stakeholder
    service requires specific constituents for its feature vector. For instance, an
    HVAC and an ambiance lighting system may require an occupancy estimate for feature
    formulation. In such a case, the occupancy estimate derived from a set of sensory
    measurements is computed once. It can be utilized by all the available applications
    towards optimal resource utilization. Let be a set of functions that generate
    individual sensory data features. Therefore, is a finite set of arbitrary functions
    for feature extraction. Download : Download high-res image (341KB) Download :
    Download full-size image Fig. 7. Experimental setup. Download : Download high-res
    image (151KB) Download : Download full-size image Fig. 8. Measurements from seven
    temperature sensors. 5. Experimental setup We evaluate the performance of UniPreCIS
    primarily from two aspects: (I) the performance evaluation of the sensor selection
    approach. (II) the resource utilization of the shared infrastructure. We now detail
    the experiment designs for the same. 5.1. Testbed setup To analyze the performance
    of UniPreCIS , we deploy a room-scale testbed in our institute campus as shown
    in Fig. 7. The testbed employs three nodes, each having a temperature sensor,
    a humidity sensor, a CO2 sensor, and two instances of motion sensors. Considering
    the room environment to be a single environment under study, we find these sensors
    to be collocated. Therefore, redundant data is generated due to the presence of
    multiple instances of sensors. However, a service for occupancy estimation [40]
    requires one value from each type of sensor for its computation. Adhering to the
    acquisition limitations, temperature, and humidity sensors were set to generate
    data at an interval of 3 s, the CO2 sensors an interval of 30 s, and the motion
    sensors have 1 s each. The sensors generate data in the form of streams. Each
    stream is fed to a PC (8-core i7 3.6 GHz, RAM: 8 GB) where UniPreCIS performs
    the necessary computation. We consider a service for detecting the room occupancy
    [40] based on ambient sensing, where multiple sensors are fused to estimate the
    occupancy level. Each feature representation contains one instance of temperature
    and humidity measurements and a set of motion sensor measurements. We consider
    another service for real-time indoor air-quality monitoring [41] based on temperature,
    humidity, and CO2 to study a multi-service scenario. 5.2. Dataset The availability
    of sensor data that have measurement incorrectness is pretty scarce. We have motivated
    earlier that the sensors are not faulty and the measurements error reported by
    sensors are considered to be transient. Therefore, to take random error into account,
    we use the benchmark dataset prepared in [22], which contains random measurement
    incorrectness exhibited by temperature sensors. From this dataset, we address
    a scenario shown in Fig. 8, where seven temperature sensors ( ) are in play. As
    per the documentation of the dataset, the sensors, and generated incorrect measurements
    in various instances and can be easily identified by sudden spikes in their measurement
    curves in the time domain. 5.3. Baseline For evaluating the sensor selection approach,
    we first study its impact on the performance of inference on a service. However,
    we need to study what happens when any existing approaches for selecting the sensory
    sources are considered. For this, we have three primary cases: (1) When there
    is a one-to-one mapping of a sensor to a service. In this case, we map each of
    the existing instances of a sensor at a time and assess the performance of the
    service. (2) The second case considers that all the sensor-reported measurements
    are fused into a single value, and the outcome is leveraged by the service. (3)
    In this case, we compare UniPreCIS with an existing static sensor selection approach.
    Since, in the given context, the selected sensor by all the static sensor selection
    approaches will remain the same over time, the behavior of the service will remain
    identical. Therefore we choose one of the state-of-the-art approaches titled TOPSIS
    [42] for finding the best sensor. TOPSIS represents each sensor in its vector
    form, where each dimension represents a sensor attribute. TOPSIS then finds the
    compromised point (the most suitable sensor) from a given utopia point (ideal
    sensor). TOPSIS functions in the following manner: step (i) Given a set of sensors,
    { } and a set of values for the attributes of the sensors, first, it creates an
    analysis matrix, as, Download : Download high-res image (41KB) Download : Download
    full-size image Here, is the value assigned for the attribute of sensor . It then
    normalizes each to get in Q using the equation, step (ii) For each attribute,
    it next hypothesizes two sensors and as ideal (Zenith) and anti-ideal (Nadir)
    points, respectively. The ideal point can be formed from the QoS requirements
    of the stakeholder services, whereas the anti-ideal point is derived to define
    the worst possible case. Each dimension of these two points is nothing but a sensor
    attribute. step (iii) In the next step, Euclidean distances, and of each sensor
    are calculated from the two ideal points as step (iv) Next, the relative closeness
    ( ) of each sensor, to the ideal points is calculated as, step (v) At the final
    step, All the sensors, , are sorted in ascending order according to the value.
    Therefore, the closest of the sensors to the ideal sensor is finally selected
    as the optimal one. However, it is worth noting that this selection happens on
    the attribute specifications and does not adapt to the dynamicity of these attributes
    5.4. Experiment design The experiments to evaluate UniPreCIS are designed in the
    following terms. 5.4.1. Service performance on adaptive sensing Since we do not
    have ground truth information about the optimal sensor given selection criteria,
    we evaluate the impact of the proposed adaptive sensor selection approach by studying
    its effects on a stakeholder service. For that, a knowledge file trained with
    2000 instances of features is employed in service for occupancy detection say
    . The data from the most optimal sensor is chosen among the redundant instances
    to perform this training. We use different models to develop viz. a K-Nearest
    Neighbor (KNN), a Logical Regression (LR), a Decision Tree (DT), and a Support
    Vector Machine (SVM) to establish the correctness of UniPreCIS . is invoked periodically.
    First, we study the accuracy demonstrated by considering each of the redundant
    sensor instances without any selection. Next, we study in three scenarios: (i)
    when the sensor selected by UniPreCIS is used to select a sensor from a given
    set, and (ii) the sensory measurements from the available instances were aggregated
    during the feature formulation without sensor selection and (iii) when TOPSIS
    selects a sensor. For this prediction by the service, we collect the ground truth
    for human presence and absence within the room throughout the evaluation and therefore
    measure the accuracy in prediction. 5.4.2. Resource consumption footprint Since
    the primary objective of incorporating the proposed sensor selection approach
    is to reduce the resource consumption footprint, we show how UniPreCIS can affect
    the computation time of . We also compare the amount of data being processed with
    the optimal sensor selection for . 5.4.3. Optimal sensor selection We discuss
    dynamic modeling of two sensor attributes viz. accuracy and reliability. For this
    study, first, we evaluate how the sensor selection is achieved when sensor accuracy
    has a higher weightage. Then we prioritize the sensor reliability to select the
    most optimal sensor. To study this we use the benchmark dataset [22] discussed
    in Section 5.2 Download : Download high-res image (723KB) Download : Download
    full-size image Fig. 9. The response time of the service while predicting the
    occupancy in R1 every 5 min. Download : Download high-res image (242KB) Download
    : Download full-size image Fig. 10. Comparison of the number of data points processed.
    5.4.4. Resource utilization of unified preprocessing To evaluate the proposed
    preprocessing component, we study the computation duration and memory utilization
    footprint of the running services with and without the unified preprocessing.
    Here, the occupancy detection service is invoked at an interval of 5 s, and the
    air quality measurement service is invoked every second. Therefore we have a scenario
    of parallel execution of services that use features with different data granularity
    and depend on a standard set of sensors in a collocated fashion. When the framework
    is not used, each service performs its own preprocessing and respective inferences
    independently. When UniPreCIS is in use, it serves the preprocessing steps in
    a unified manner and provides the depending services with their feature vectors.
    Download : Download high-res image (827KB) Download : Download full-size image
    Fig. 11. Sensor selection applied to prioritize accuracy. Download : Download
    high-res image (755KB) Download : Download full-size image Fig. 12. Sensor selection
    applied to prioritize reliability. Download : Download high-res image (173KB)
    Download : Download full-size image Fig. 13. Impact on overall execution time.
    Download : Download high-res image (366KB) Download : Download full-size image
    Fig. 14. Comparison of Memory consumption profile by the services when the minimum
    granularity of data is kept as a 1-second interval. 6. Results and discussions
    The achieved results are grouped according to the experiment specifications mentioned
    in Section 5 Table 2. Accuracy demonstrated on an average by the classifiers for
    each of the three collocated sensors. Classifier Sensor 1 Sensor 2 Sensor 3 KNN
    44.6 22.26 94.31 LR 24.54 21.87 85.69 SVM 56.55 20.45 94.31 DT 47.51 26.99 94.31
    6.1. The accuracy in service’s performance When we keep a sensory source constant,
    the obtained accuracy of for individual sensory sources is shown in Table 2. Here,
    significantly high accuracy is demonstrated in the case when the system operates
    using only sensor 3. This is due to the fact that sensor 3 is the most optimal
    sensor in terms of the measurement it performs. However, it is worth noting that
    relying on a single sensor results in a single point of failure, which is the
    sole motivation for sensor redundancy. Whereas the other two sensors, when individually
    relied on, did not demonstrate a notable performance accuracy. When the system
    is studied with all the sensors combined, the results are observable from Table
    3. The result achieved through fusing all the sources did not demonstrate satisfactory
    values since the effect of non-optimal sensors is also induced when all the sensory
    measurements are processed. Similarly, TOPSIS did not show encouraging results
    as the sensors were selected statically. However, with UniPreCIS , the prediction
    accuracy by involving selection could reach up to 90%. This is because UniPreCIS
    dynamically selects the optimal sensor and, therefore any measurements due to
    temporary error gets suppressed. 6.2. Computation delay of the service Fig. 9
    shows the processing time of the service while being invoked at a regular interval
    (5 min in this case). It can be seen from the figure that since the optimal source
    is selected instead of aggregating all the available sources, there is a significant
    reduction in computation time, and therefore there is a reduction in the response
    time of the service. 6.3. Achieved data reduction Fig. 10 presents the number
    of data points processed over time. It clearly shows that sensor selection induced
    a reduced amount of data processing, thereby preserving valuable computation resources.
    6.4. Correctness of selection Next, we invoke sensor selection on the data from
    [22] as shown in Fig. 8 . With accuracy as a priority, we observe that the proposed
    approach does not select the sensors or whenever they generate spurious values,
    as shown in Fig. 11. Whereas, as we prioritize reliability with (similar results
    were obtained at both and ), the proposed approach avoids and due to the least
    reliability factors and is shown in Fig. 12. Since TOPSIS [42] does not consider
    the dynamic change in sensor characteristics, with the requirement specification
    being constant, TOPSIS will not change the sensor selected over time. Hence, we
    avoid showing any outcome from TOPSIS. 6.5. Resource consumption with unified
    preprocessing Moving to the performance of the proposed preprocessing component,
    for the two available services, we observe the overhead reduction in terms of
    execution time to up to 25% for three different durations of generated data in
    Fig. 13. The corresponding 3 cases of memory consumption profile are shown in
    Fig. 14. The memory profile clearly shows a 10% reduction in maximum memory footprint
    by UniPreCIS . Table 3. Accuracy demonstrated on an average by the classifiers
    with and without sensor selection. Classifier Without sensor selection (fused)
    TOPSIS UniPreCIS KNN 53.6 39.96 86.44 DT 36.59 39.73 84.21 LR 73.7 38.83 84.2
    SVM 56.08 42.99 94.31 7. Literature survey We aim to perform a unified data preprocessing
    and sensor ranking and selection in order to shape the data in terms of quality
    and quantity for shared IoT. We present the relevant state-of-the-art works in
    the following subsections. 7.1. Shared IoT and service collocation The idea of
    shared IoT promotes the sharing of resources among stakeholder services. When
    multiple services are deployed on shared resources, the effective cost of deployment
    and maintenance can be significantly reduced. However, certain factors, such as
    QoS concerns about services, are crucial [43] in such infrastructures. Apart from
    that, low latency, high reliability, and huge density of low-cost sensory sources
    are also some key characteristics of shared IoT infrastructures [7]. Since multiple
    services share the underlying infrastructure, service collocation [44] can also
    benefit from the deployment setup. Early studies [45] also show that establishing
    collocation among services can significantly reduce the energy consumption footprint,
    especially for communication. However, in the case of collocated services, finding
    the correct sensory source from a given set of sensors satisfying the QoS objectives
    of the services is crucial [46]. 7.2. Data preprocessing for multi-sensor data
    fusion Multi-sensor data fusion [47] is a pretty prominent research topic in sensory
    big data analytics and has observed a plethora of use cases over the last decades.
    The idea is to combine data from multiple sensors for a more specific inference
    [48]. The preprocessing of the data involves various activities, including reducing
    instances [49]. Towards this preprocessing, very few works [50], [51] deal with
    the streaming data specific to the IoT environment. Interestingly, it is not easy
    to find a generalized approach. In a multi-service scenario, there exists a need
    to bridge the gap between data accumulation and feature formulation for IoT applications
    [52]. The earlier works studied a set of preprocessing steps for big data [53],
    highlighting the significant efforts involved. The authors in [50] propose a generalized
    architecture for preprocessing involving stream data fusion. This is a substantial
    motivation towards our proposed preprocessing mechanism that is directed towards
    a shared IoT infrastructure and introduces mapping of resources to multiple stakeholder
    services. However, their work does not exploit a collocated multi-service scenario
    and mainly focuses on incremental learning applications. Moreover, their framework
    does not incorporate measures for QoS-aware sensory sources. 7.3. Sensor ranking
    and selection for optimization Selecting a set of the most optimal sensors among
    an available group has been of interest for quite some time. In the literature,
    many studies [54], [55] can be seen for sensor ranking and selection. The primary
    goal is to assign some rank to the available sensors based on certain factors.
    The ranking can be based on the remaining energy, relevance towards the task at
    hand, location, etc. It can be observed that most of the works of ranking are
    domain specific [56], [57]. Additionally, the majority [13], [26], [58] consider
    the sensor characteristics in a static fashion. It is a significant setback in
    making a system adaptive. For instance, in [58], the authors consider sensor characteristics
    such as accuracy, range of measurement, precision, latency, etc., for selecting
    an optimal sensor. The modeling is performed by considering the measurements as
    discrete events, which fails to encompass the longitudinal behavior of the sensors.
    In this context, to make the selection adaptive, we model two sensor characteristics:
    Accuracy and Reliability. The relevant literature studies are stated below: 7.3.1.
    Modeling of accuracy The sensor systems are susceptible to inaccurate measurements
    due to various underlying factors. Even the smartphone sensors, one of the most
    pervasive kinds of IoT devices, vary in measurement accuracy [59]. Existing approaches
    for accuracy estimation can be categorized into three main classes: voting-based,
    inference-based, and learning-based. The voting-based approaches primarily consider
    measurements from multiple homogeneous sensors and make a collective decision.
    Thus, it is a perfect fit when there is collocation. It has been used across various
    domains [28] addressing a variety of problems [60]. The voting-based approaches
    enjoy a significant advantage over all the others as the requirement of any priors
    is minimal or, in most cases, nil. The inference-based approaches [29] estimate
    the true state of a sensor measurement based on a set of reported measures and/or
    priors. Static inference is when the estimation is based on current measurements
    and the priors, whereas dynamic inference considers a set of previous measurement
    states. The learning-based approaches [61] need a model to be trained with data.
    A static variant requires the training to be performed once, and a dynamic variant
    keeps updating the knowledge with new observations. The most recent studies prefer
    machine learning for estimating the accuracy of sensors. However, it has its limitations
    due to the requirement of domain knowledge. Also, the problem is usually formulated
    to address the accuracy of the model [62] rather than the accuracy of the sensor
    device itself. An earlier study [29] on performance evaluation of these approaches
    clearly shows that the learning-based approaches are better among all but at a
    significantly high computation cost. The findings argue that the dynamic inference-based
    approaches (such as KF) are comparable with learning-based approaches with a substantially
    lower computation cost. That said, we incorporate the high performance of a dynamic
    inference-based approach at a low cost with the advantage of a voting-based approach
    and derive a novel estimation strategy. 7.3.2. Modeling of reliability Sensors
    can report incorrect readings because of the dynamically changing operating conditions
    such as temperature, humidity, etc., or due to bias and calibration drifts caused
    by age-related degradation. Even after performing sensing and quantization, the
    data can still be affected by various hardware errors such as crosstalk and radiation
    effects [63]. These errors can differ widely in terms of severity, frequency of
    occurrence, and other statistical properties. In literature, the reliability of
    a sensor has been addressed at various levels, as shown in Fig. 15 Download :
    Download high-res image (112KB) Download : Download full-size image Fig. 15. Problem
    of reliability addressed at different levels. We consider the problem of reliability
    in a data-oriented manner. This can be addressed in terms of the extent of inconsistency
    in reported data by a sensor concerning the phenomenon of interest’s actual behavior
    [64]. The state-of-the-art approaches for modeling this are mostly machine learning-based
    approaches [65], [66]. Although reported performances are notable, they suffer
    from a common disadvantage, application dependency. Therefore, different models
    need to be trained for a heterogeneous environment to circumvent available objectives.
    The machine-based training and inference are usually computationally expensive
    and require suitable datasets for effective performance. In addition, in almost
    all cases, the available datasets were prepared based on simulated parameters
    [22]. The other significant approaches are based on evidence theory [67], [68].
    However, addressing reliability following this approach is still at a theoretical
    level. The analysis of the study is usually performed based on several assumptions
    for parameters such as masses, whose practical relevance is questionable. However,
    with the availability of multi-instances of sensors and existing collocations,
    low-computation mathematical models can be a good fit for the reliability problem.
    We now highlight how UniPreCIS places itself in comparison to the relevant works
    belonging to the individual categories mentioned in the surveyed literature. Table
    4 captures the comparison of the works which are closely relevant to UniPreCIS  Table
    4. A comparison of relevant works withUniPreCIS . References Infrastructure Service
    consideration Sensor selection criteria In-network data preprocessing End-to-end
    preprocessing solution Preprocessing technique(s) QoS awareness Modeling of reliability
    Modeling of accuracy Theodorou, et al. [7] Shared IoT IoT services NA No No NA
    No NA NA Ballotta, et al. IoT IoT services Computation cost, delay Yes No Data
    fusion No No No Perera, et al. [58] IoT IoT services Sensor attributes (manufacturer
    provided) No No NA No NA NA Younas, et al. [44] IoT IoT services Many objectives
    NA No NA Yes NA NA Costa, et al. [54] IoT IoT services Normal and abnormal data
    No No NA No NA NA Kenda, et al. [50] IoT IoT services NA Yes Yes Aggregation,
    resampling, feature extraction No NA NA UniPreCIS  Shared IoT Collocated IoT services
    Sensor attributes (adaptive) Yes Yes Sensor selection, aggregation, feature extraction
    Yes Low-computation, window-based Dynamic inference and voting-based 8. Conclusion
    As IoT infrastructure is becoming part of our daily lives, more smart applications
    are emerging regularly. A single infrastructure can have numerous applications
    running as services on a shared setup. Our primary motivation addressed the fact
    that in such a scenario, the services can be decoupled from the hardware infrastructure
    to enhance cost-effective deployment. Leveraging such existing infrastructure
    however, introduces new challenges, such as redundant data instances and incorrect
    measurements, QoS requirements of stakeholder services, etc. That is where we
    have introduced UniPreCIS , which promotes optimal sensor selection addressing
    the QoS requirements of the stakeholder services. The introduced dynamic selection
    mechanism can significantly reduce the computation on redundancy. At the same
    time, this also adapts to the random yet momentary change in sensor attributes.
    Apart from isolating optimal sensors, UniPreCIS also performs unified preprocessing
    of the sensory data for collocated services, thereby minimizing redundant computation
    by individual parts. A testbed study and dataset-based validation showed that
    UniPreCIS can adapt to the changing dynamics of the setup while selecting the
    most optimal set of sensors. At the same time, it preserves the performance of
    the depending applications by maintaining the inference accuracy. The unified
    preprocessing also showed a reduction in the computation time by up to 25%. In
    this work, the dynamicity of two sensor characteristics viz. accuracy and reliability
    has been modeled. As a future direction, we would like to explore different sensor
    types and their characteristics demonstrating certain levels of dynamicity under
    different circumstances. Currently, UniPreCIS has been evaluated on two smart
    services; therefore, there is a need to explore more complex smart services deployed
    on standard platforms. Also, integrating UniPreCIS to low-power IoT devices to
    study the real-world impact could be interesting and we would like to explore
    this in future work. A scalability study on an extended testbed setup could also
    be an additional direction to explore. The source code for the implementation
    is available to download from our GitHub repository.2 CRediT authorship contribution
    statement Anirban Das: Conceptualization, Investigation, Methodology, Software,
    Writing – original draft, Writing – review & editing. Navlika Singh: Investigation,
    Software, Validation, Writing – review & editing. Suchetana Chakraborty: Conceptualization,
    Methodology, Supervision, Validation, Writing – review & editing. Declaration
    of competing interest The authors declare the following financial interests/personal
    relationships which may be considered as potential competing interests: Suchetana
    Chakraborty reports financial support was provided by SERB, DST, Govt. of India.
    Acknowledgment This work has been partially supported by SERB, DST, Government
    of India through project no. ECR/2017/000813. Appendix. Proof that is optimal:
    Let X be the set of all optimal values given a set of values, , Case (1) When
    all the values of are equal. ( ) Then, (14) Thus, in this case, all are equal,
    and all values are optimal. Case (2) When all the values of can be arranged in
    ascending order and are of equal difference. The values are in AP For a given
    value of , will give two APs for different values of (one for and another for
    ) For a given value of , will be in two GPs, and the sum of these two GPs is a
    constant. The values of is the same for any specific value of . Case (3) In all
    other cases, Let be no. of elements in the set of optimal values, . Given, , will
    be less than or equal to , and considering the linear distance between each pair
    of measurements. (15) Example: Let the values measured be, , , , , On cognitive
    analysis, we can say that the near-optimal measurements lie among, and as is too
    far a value from the rest. After normalization, using, we get, Now we formulate
    using membership function, , as, Download : Download high-res image (68KB) Download
    : Download full-size image Therefore, we have the values of s 1.841 3.632 3.728
    3.756 2.663 In sorted order, 3.756 3.728 3.632 2.663 1.841 Here, if then the selection
    becomes matching our cognitive assumption. Data availability Data will be made
    available on request. References [1] Ranieri C.M., MacLeod S., Dragone M., Vargas
    P.A., Romero R.A.F. Activity recognition for ambient assisted living with videos,
    inertial units and ambient sensors Sensors, 21 (3) (2021), p. 768 CrossRefGoogle
    Scholar [2] Byabazaire J., O’Hare G., Delaney D. Using trust as a measure to derive
    data quality in data shared IoT deployments 2020 29th International Conference
    on Computer Communications and Networks (ICCCN), IEEE (2020), pp. 1-9 CrossRefGoogle
    Scholar [3] Laput G., Zhang Y., Harrison C. Synthetic sensors: Towards general-purpose
    sensing Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems
    (2017), pp. 3986-3999 CrossRefView in ScopusGoogle Scholar [4] Palmisani J., Di
    Gilio A., Viana M., de Gennaro G., Ferro A. Indoor air quality evaluation in oncology
    units at two European hospitals: Low-cost sensors for TVOCs, PM2. 5 and CO2 real-time
    monitoring Build. Environ., 205 (2021), Article 108237 View PDFView articleView
    in ScopusGoogle Scholar [5] deSouza P., Kinney P.L. On the distribution of low-cost
    PM2. 5 sensors in the US: demographic and air quality associations J. Expo. Sci.
    Environ. Epidemiol., 31 (3) (2021), pp. 514-524 CrossRefView in ScopusGoogle Scholar
    [6] Placidi P., Papini N., Delle Vergini C.V., Mezzanotte P., Scorzoni A. Capacitive
    low-cost system for soil water content measurement in the IoT precision agriculture
    2022 IEEE International Instrumentation and Measurement Technology Conference
    (I2MTC), IEEE (2022), pp. 1-6 CrossRefGoogle Scholar [7] Theodorou V., Xezonaki
    M.-E. Network slicing for multi-tenant edge processing over shared IoT infrastructure
    2020 6th IEEE Conference on Network Softwarization (NetSoft), IEEE (2020), pp.
    8-14 CrossRefView in ScopusGoogle Scholar [8] Preuveneers D., Ilie-Zudor E. Big
    Data for context-aware applications and intelligent environments (2019) Google
    Scholar [9] Li D., Wang Y., Wang J., Wang C., Duan Y. Recent advances in sensor
    fault diagnosis: A review Sensors Actuators A, 309 (2020), Article 111990, 10.1016/j.sna.2020.111990
    URL https://www.sciencedirect.com/science/article/pii/S0924424719308635 View PDFView
    articleView in ScopusGoogle Scholar [10] Pallewatta S., Kostakos V., Buyya R.
    QoS-aware placement of microservices-based IoT applications in Fog computing environments
    Future Gener. Comput. Syst., 131 (2022), pp. 121-136, 10.1016/j.future.2022.01.012
    URL https://www.sciencedirect.com/science/article/pii/S0167739X22000206 View PDFView
    articleView in ScopusGoogle Scholar [11] Dragoni N., Giallorenzo S., Lafuente
    A.L., Mazzara M., Montesi F., Mustafin R., Safina L. Microservices: yesterday,
    today, and tomorrow Present and Ulterior Software Engineering, Springer (2017),
    pp. 195-216 CrossRefView in ScopusGoogle Scholar [12] Grueneberg K., Ko B., Wood
    D., Wang X., Steuer D., Lim Y. IoT data management system for rapid development
    of machine learning models 2019 IEEE International Conference on Cognitive Computing
    (ICCC) (2019), pp. 59-63, 10.1109/ICCC.2019.00021 View in ScopusGoogle Scholar
    [13] Ballotta L., Schenato L., Carlone L. Computation-communication trade-offs
    and sensor selection in real-time estimation for processing networks IEEE Trans.
    Netw. Sci. Eng., 7 (4) (2020), pp. 2952-2965 CrossRefView in ScopusGoogle Scholar
    [14] Xu J., Palanisamy B., Wang Q., Ludwig H., Gopisetty S. Amnis: Optimized stream
    processing for edge computing J. Parallel Distrib. Comput., 160 (2022), pp. 49-64
    View PDFView articleCrossRefGoogle Scholar [15] Savaglio C., Gerace P., Di Fatta
    G., Fortino G. Data mining at the IoT edge 2019 28th International Conference
    on Computer Communication and Networks (ICCCN), IEEE (2019), pp. 1-6 CrossRefGoogle
    Scholar [16] Das A., Gupta R., Chakraborty S. Motivating in-network fusion for
    smart infrastructure monitoring 2019 11th International Conference on Communication
    Systems Networks (COMSNETS) (2019), pp. 513-516, 10.1109/COMSNETS.2019.8711204
    View in ScopusGoogle Scholar [17] Das A., Gupta R., Chakraborty S. Significance
    of adaptive sensing for smart building monitoring: A practical study 2019 11th
    International Conference on Communication Systems & Networks (COMSNETS) (2019),
    pp. 684-689, 10.1109/COMSNETS.2019.8711469 View in ScopusGoogle Scholar [18] Shen
    H., Hou W., Zhu Y., Zheng S., Ainiwaer S., Shen G., Chen Y., Cheng H., Hu J.,
    Wan Y., et al. Temporal and spatial variation of PM2. 5 in indoor air monitored
    by low-cost sensors Sci. Total Environ., 770 (2021), Article 145304 View PDFView
    articleView in ScopusGoogle Scholar [19] Giordano M.R., Malings C., Pandis S.N.,
    Presto A.A., McNeill V., Westervelt D.M., Beekmann M., Subramanian R. From low-cost
    sensors to high-quality data: A summary of challenges and best practices for effectively
    calibrating low-cost particulate matter mass sensors J. Aerosol Sci., 158 (2021),
    Article 105833 View PDFView articleView in ScopusGoogle Scholar [20] Concas F.,
    Mineraud J., Lagerspetz E., Varjonen S., Liu X., Puolamäki K., Nurmi P., Tarkoma
    S. Low-cost outdoor air quality monitoring and sensor calibration: A survey and
    critical analysis ACM Trans. Sensor Netw., 17 (2) (2021), pp. 1-44 CrossRefGoogle
    Scholar [21] Petrich C., Sæther I.V., Dang N.P., Kleven Ø., O’Sadnick M. A note
    on remote temperature measurements with DS18B20 digital sensors Proceedings of
    the 25th International Symposium on Ice (2020) Google Scholar [22] de Bruijn B.,
    Nguyen T.A., Bucur D., Tei K. Benchmark datasets for fault detection and classification
    in sensor data SENSORNETS (2016), pp. 185-195 CrossRefView in ScopusGoogle Scholar
    [23] Jan S.U., Lee Y.-D., Shin J., Koo I. Sensor fault classification based on
    support vector machine and statistical time-domain features IEEE Access, 5 (2017),
    pp. 8682-8690 View in ScopusGoogle Scholar [24] Saeed U., Jan S.U., Lee Y.-D.,
    Koo I. Fault diagnosis based on extremely randomized trees in wireless sensor
    networks Reliab. Eng. Syst. Saf., 205 (2021), Article 107284 View PDFView articleView
    in ScopusGoogle Scholar [25] Zhao J., Xu Y., Luo F., Dong Z., Peng Y. Power system
    fault diagnosis based on history driven differential evolution and stochastic
    time domain simulation Inform. Sci., 275 (2014), pp. 13-29 View PDFView articleView
    in ScopusGoogle Scholar [26] Babu K.R., Prathap M.V., Samuel P. Context aware
    reliable sensor selection in IoT Int. J. Intell. Syst. Technol. Appl., 18 (1–2)
    (2019), pp. 34-51 CrossRefGoogle Scholar [27] Wang D., Huang C. Confidence-aware
    truth estimation in social sensing applications 2015 12th Annual IEEE International
    Conference on Sensing, Communication, and Networking (SECON), IEEE (2015), pp.
    336-344 View in ScopusGoogle Scholar [28] Tsarev R., Durmuş M., Üstoglu I., Morozov
    V., Pupkov A. Fuzzy voting algorithms for N-version software Journal of Physics:
    Conference Series, Vol. 1333, IOP Publishing (2019), Article 032087 CrossRefView
    in ScopusGoogle Scholar [29] Wen H., Xiao Z., Markham A., Trigoni N. Accuracy
    estimation for sensor systems IEEE Trans. Mob. Comput., 14 (7) (2014), pp. 1330-1343
    CrossRefGoogle Scholar [30] Ping W. A voting strategy for N-version program based
    on fuzzy clustering 2007 8th International Conference on Electronic Measurement
    and Instruments, IEEE (2007), pp. 2-666 Google Scholar [31] Kuo W., Zuo M.J. Optimal
    Reliability Modeling: Principles and Applications John Wiley & Sons (2003) Google
    Scholar [32] Gu X., Angelov P.P., Kangin D., Principe J.C. A new type of distance
    metric and its use for clustering Evol. Syst., 8 (3) (2017), pp. 167-177 CrossRefView
    in ScopusGoogle Scholar [33] Choudhury B., Choudhury S., Dutta A. A proactive
    context-aware service replication scheme for adhoc IoT scenarios IEEE Trans. Netw.
    Serv. Manag., 16 (4) (2019), pp. 1797-1811 CrossRefView in ScopusGoogle Scholar
    [34] da Silva M.P., Gonçalves A.L., Dantas M.A.R. A conceptual model for quality
    of experience management to provide context-aware eHealth services Future Gener.
    Comput. Syst., 101 (2019), pp. 1041-1061, 10.1016/j.future.2019.07.033 URL https://www.sciencedirect.com/science/article/pii/S0167739X18314079
    View PDFView articleView in ScopusGoogle Scholar [35] Gnauck A. Interpolation
    and approximation of water quality time series and process identification Anal.
    Bioanal. Chem., 380 (3) (2004), pp. 484-492 View in ScopusGoogle Scholar [36]
    Lepot M., Aubin J.-B., Clemens F.H. Interpolation in time series: An introductive
    overview of existing methods, their performance criteria and uncertainty assessment
    Water, 9 (10) (2017), p. 796 CrossRefView in ScopusGoogle Scholar [37] Fikri M.,
    Herdjunanto S., Cahyadi A. On the performance similarity between exponential moving
    average and discrete linear Kalman filter 2019 Asia Pacific Conference on Research
    in Industrial and Systems Engineering (APCoRISE) (2019), pp. 1-5, 10.1109/APCoRISE46197.2019.9318810
    Google Scholar [38] Qiu S., Zhao H., Jiang N., Wang Z., Liu L., An Y., Zhao H.,
    Miao X., Liu R., Fortino G. Multi-sensor information fusion based on machine learning
    for real applications in human activity recognition: State-of-the-art and research
    challenges Inf. Fusion, 80 (2022), pp. 241-265, 10.1016/j.inffus.2021.11.006 URL
    https://www.sciencedirect.com/science/article/pii/S1566253521002311 View PDFView
    articleView in ScopusGoogle Scholar [39] Malik H., Fatema N., Alzubi J.A. AI and
    Machine Learning Paradigms for Health Monitoring System Springer (2021) Google
    Scholar [40] Candanedo L.M., Feldheim V. Accurate occupancy detection of an office
    room from light, temperature, humidity and CO2 measurements using statistical
    learning models Energy Build., 112 (2016), pp. 28-39 View PDFView articleView
    in ScopusGoogle Scholar [41] Benammar M., Abdaoui A., Ahmad S.H., Touati F., Kadri
    A. A modular IoT platform for real-time indoor air quality monitoring Sensors,
    18 (2) (2018), p. 581 CrossRefView in ScopusGoogle Scholar [42] Nunes L.H., Estrella
    J.C., Perera C., Reiff-Marganiec S., Delbem A.C. The elimination-selection based
    algorithm for efficient resource discovery in Internet of Things environments
    2018 15th IEEE Annual Consumer Communications & Networking Conference (CCNC),
    IEEE (2018), pp. 1-7 View in ScopusGoogle Scholar [43] Bolettieri S., Bruno R.
    QoS-aware data management mechanisms for optimal resource utilisation in crowd-assisted
    shared sensor networks 2020 IEEE International Conference on Smart Computing (SMARTCOMP),
    IEEE (2020), pp. 81-88 CrossRefView in ScopusGoogle Scholar [44] Younas I., Naeem
    A. Optimization of sensor selection problem in IoT systems using opposition-based
    learning in many-objective evolutionary algorithms Comput. Electr. Eng., 97 (2022),
    Article 107625 View PDFView articleView in ScopusGoogle Scholar [45] Huang Z.,
    Lin K.-J., Yu S.-Y., Hsu J.Y.-j. Co-locating services in IoT systems to minimize
    the communication energy cost J. Innov. Digit. Ecosyst., 1 (1–2) (2014), pp. 47-57
    View PDFView articleCrossRefGoogle Scholar [46] Shukla J., Maiti P., Sahoo B.
    Low latency and energy efficient sensor selection for IoT services 2018 Technologies
    for Smart-City Energy Security and Power (ICSESP), IEEE (2018), pp. 1-5 View in
    ScopusGoogle Scholar [47] Xiao F. Multi-sensor data fusion based on the belief
    divergence measure of evidences and the belief entropy Inf. Fusion, 46 (2019),
    pp. 23-32, 10.1016/j.inffus.2018.04.003 URL https://www.sciencedirect.com/science/article/pii/S1566253517305584
    View PDFView articleView in ScopusGoogle Scholar [48] Liggins II M., Hall D.,
    Llinas J. Handbook of Multisensor Data Fusion: Theory and Practice CRC Press (2017)
    Google Scholar [49] Ramírez-Gallego S., Krawczyk B., García S., Woźniak M., Herrera
    F. A survey on data preprocessing for data stream mining: Current status and future
    directions Neurocomputing, 239 (2017), pp. 39-57 View PDFView articleView in ScopusGoogle
    Scholar [50] Kenda K., Kažič B., Novak E., Mladenić D. Streaming data fusion for
    the Internet of Things Sensors, 19 (8) (2019), p. 1955 CrossRefView in ScopusGoogle
    Scholar [51] Yin Y., Xu B., Cai H., Yu H. A novel temporal and spatial panorama
    stream processing engine on IoT applications J. Ind. Inf. Integr., 18 (2020),
    Article 100143 View PDFView articleView in ScopusGoogle Scholar [52] Peros S.,
    Joosen W., Hughes D. Ermis: a middleware for bridging data collection and data
    processing in IoT streaming applications 2021 17th International Conference on
    Distributed Computing in Sensor Systems (DCOSS), IEEE (2021), pp. 259-266 CrossRefView
    in ScopusGoogle Scholar [53] García S., Ramírez-Gallego S., Luengo J., Benítez
    J.M., Herrera F. Big data preprocessing: methods and prospects Big Data Anal.,
    1 (1) (2016), pp. 1-22 View in ScopusGoogle Scholar [54] Costa F.S., Nassar S.M.,
    Dantas M.A. GoAT: A sensor ranking approach for IoT environments CLOSER (2021),
    pp. 169-177 CrossRefView in ScopusGoogle Scholar [55] Saito Y., Nonomura T., Yamada
    K., Nakai K., Nagata T., Asai K., Sasaki Y., Tsubakino D. Determinant-based fast
    greedy sensor selection algorithm IEEE Access, 9 (2021), pp. 68535-68551 CrossRefView
    in ScopusGoogle Scholar [56] Bharti S., Pattanaik K.K., Bellavista P. Value of
    information based sensor ranking for efficient sensor service allocation in service
    oriented wireless sensor networks IEEE Trans. Emerg. Top. Comput. (2019) Google
    Scholar [57] Bharti M., Kumar R., Saxena S., Jindal H. Optimal resource selection
    framework for Internet-of-Things Comput. Electr. Eng., 86 (2020), Article 106693
    View PDFView articleView in ScopusGoogle Scholar [58] Perera C., Zaslavsky A.,
    Christen P., Compton M., Georgakopoulos D. Context-aware sensor search, selection
    and ranking model for internet of things middleware 2013 IEEE 14th International
    Conference on Mobile Data Management, Vol. 1, IEEE (2013), pp. 314-322 CrossRefView
    in ScopusGoogle Scholar [59] Kuhlmann T., Garaizar P., Reips U.-D. Smartphone
    sensor accuracy varies from device to device in mobile research: The case of spatial
    orientation Behav. Res. Methods, 53 (2021), pp. 22-33 CrossRefView in ScopusGoogle
    Scholar [60] Tsarev R.Y., Durmuş M.S., Üstoglu I., Morozov V. Classification of
    voting algorithms for N-version software Journal of Physics: Conference Series,
    Vol. 1015, IOP Publishing (2018), Article 042060 CrossRefView in ScopusGoogle
    Scholar [61] Yan J., Pu W., Zhou S., Liu H., Bao Z. Collaborative detection and
    power allocation framework for target tracking in multiple radar system Inf. Fusion,
    55 (2020), pp. 173-183 View PDFView articleView in ScopusGoogle Scholar [62] Özcan
    M., Aliew F., Görgün H. Accurate and precise distance estimation for noisy IR
    sensor readings contaminated by outliers Measurement, 156 (2020), Article 107633
    View PDFView articleView in ScopusGoogle Scholar [63] Mukhopadhyay S., Schurgers
    C., Panigrahi D., Dey S. Model-based techniques for data reliability in wireless
    sensor networks IEEE Trans. Mob. Comput., 8 (4) (2008), pp. 528-543 Google Scholar
    [64] Ni K., Ramanathan N., Chehade M.N.H., Balzano L., Nair S., Zahedi S., Kohler
    E., Pottie G., Hansen M., Srivastava M. Sensor network data fault types ACM Trans.
    Sensor Netw., 5 (3) (2009), pp. 1-29 CrossRefView in ScopusGoogle Scholar [65]
    Noshad Z., Javaid N., Saba T., Wadud Z., Saleem M.Q., Alzahrani M.E., Sheta O.E.
    Fault detection in wireless sensor networks through the random forest classifier
    Sensors, 19 (7) (2019), p. 1568 CrossRefView in ScopusGoogle Scholar [66] Zidi
    S., Moulahi T., Alaya B. Fault detection in wireless sensor networks through SVM
    classifier IEEE Sens. J., 18 (1) (2017), pp. 340-347 Google Scholar [67] Ghosh
    N., Paul R., Maity S., Maity K., Saha S. Fault Matters: Sensor data fusion for
    detection of faults using Dempster–Shafer theory of evidence in IoT-based applications
    Expert Syst. Appl., 162 (2020), Article 113887 View PDFView articleView in ScopusGoogle
    Scholar [68] Jiwei Q., Jianguo Z., Yupeng M. Reliability analysis based on the
    principle of maximum entropy and Dempster–Shafer evidence theory J. Mech. Sci.
    Technol., 32 (2) (2018), pp. 605-613 CrossRefView in ScopusGoogle Scholar Cited
    by (0) Anirban Das is currently serving as an Assistant Professor in the Department
    of Computer Science and Engineering, NIIT University, Rajasthan, India. He has
    completed his Ph.D. from the Department of Computer Science and Engineering, IIIT
    Guwahati, India. His research studies data collection approaches for heterogeneous
    applications in a shared IoT infrastructure. His study primarily focuses on context-aware
    approaches for cost minimization in a multi-application scenario. He has experience
    working on a live project at the Indian Statistical Institute, Kolkata, India,
    and serving as a lecturer for 3+ years at Jimma University, Ethiopia. His other
    research interests are in-network data fusion, machine inference on low-power
    devices, sentiment analysis on textual data, and detection of community clusters
    in social networks. Anirban holds a Master’s degree in Information and Technology
    from Assam University, Silchar, India. Navlika Singh is currently pursuing a Bachelor
    of Technology degree in Artificial Intelligence and Data Science at the Indian
    Institute of Technology, Jodhpur, Rajasthan, India. She has been in sync with
    events, projects, and hackathons concerning her domain of interest and has been
    rewarded for her excellence. Her current research interests span classical machine
    learning and computer vision. Dr. Suchetana Chakraborty is working as an Assistant
    Professor at the Department of Computer Science and Engineering at Indian Institute
    of Technology Jodhpur. She completed her MTech and Ph.D. from IIT Guwahati in
    2011 and 2014, respectively. Prior to joining IIT Jodhpur she was working as an
    Assistant Professor at IIIT Guwahati since 2016. After completing her Ph.D. she
    spent a year as a faculty member in BITS Pilani Hyderabad Campus followed by a
    research visit to the Department of BioMedical Informatics, The Ohio State University,
    USA. So far she has published 2 book-chapters, 8 journal papers, and 27 peer-reviewed
    conference papers at various places of International repute. She is a member of
    IEEE COMSOC, IEEE Women In Engineering (WIE), ACM, and ACM-W. Dr. Chakraborty
    has received various awards and accolades including TCS Research Fellowship, Microsoft
    Research India Travel Grant, National Internet Exchange of India (NIXI) fellowship,
    Postdoc Mobility Fellowship to name a few. She has also been awarded the Early
    Career Research Award to lead a project on ’smart building monitoring’ in the
    year 2017 by the SERB Board, DST, Govt. of India. She has been associated with
    various conferences like IEEE LCN, IEEE ANTS, COMSNETS, and so on as a TPC member
    and organizing co-chairs. She is also currently serving as the Area Editor of
    the Elsevier journal ’Ad Hoc Networks’. The primary research interests of Dr.
    Chakraborty are in the areas of IoT and Edge Computing, Ubiquitous & Mobile Computing
    and its applications on Smart Living, Distributed Systems, Wireless sensor, and
    ad hoc networks. 1 Detailed in Appendix. 2 https://github.com/Navlika-Singh/UniPreCIS.
    View Abstract © 2023 Elsevier B.V. All rights reserved. Recommended articles Autonomous
    selection of the fault classification models for diagnosing microservice applications
    Future Generation Computer Systems, Volume 153, 2024, pp. 326-339 Yujia Song,
    …, Zhiming Zhao View PDF Computation offloading in blockchain-enabled MCS systems:
    A scalable deep reinforcement learning approach Future Generation Computer Systems,
    Volume 153, 2024, pp. 301-311 Zheyi Chen, …, Wang Miao View PDF Continuous agile
    cyber–physical systems architectures based on digital twins Future Generation
    Computer Systems, Volume 153, 2024, pp. 350-359 Alexander Vodyaho, …, Alexey Subbotin
    View PDF Show 3 more articles Article Metrics Captures Readers: 3 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Future Generation Computer Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'UniPreCIS: A data preprocessing solution for collocated services on shared
    IoT'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Nguyen H.H.
  - Shin D.Y.
  - Jung W.S.
  - Kim T.Y.
  - Lee D.H.
  citation_count: '0'
  description: Industrial greenhouse mushroom cultivation is currently promising,
    due to the nutritious and commercial mushroom benefits and its convenience in
    adapting smart agriculture technologies. Traditional Device-Cloud protocol in
    smart agriculture wastes network resources when big data from Internet of Things
    (IoT) devices are directly transmitted to the cloud server without processing,
    delaying network connection and increasing costs. Edge computing has emerged to
    bridge these gaps by shifting partial data storage and computation capability
    from the cloud server to edge devices. However, selecting which tasks can be applied
    in edge computing depends on user-specific demands, suggesting the necessity to
    design a suitable Smart Agriculture Information System (SAIS) architecture for
    single-crop requirements. This study aims to design and implement a cost-saving
    multilayered SAIS architecture customized for smart greenhouse mushroom cultivation
    toward leveraging edge computing. A three-layer SAIS adopting the Device-Edge-Cloud
    protocol, which enables the integration of key environmental parameter data collected
    from the IoT sensor and RGB images collected from the camera, was tested in this
    research. Implementation of this designed SAIS architecture with typical examples
    of mushroom cultivation indicated that low-cost data pre-processing procedures
    including small-data storage, temporal resampling-based data reduction, and lightweight
    artificial intelligence (AI)-based data quality control (for anomalous environmental
    conditions detection) together with real-time AI model deployment (for mushroom
    detection) are compatible with edge computing. Integrating the Edge Layer as the
    center of the traditional protocol can significantly save network resources and
    operational costs by reducing unnecessary data sent from the device to the cloud,
    while keeping sufficient information.
  doi: 10.3390/agriculture14030489
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agriculture All Article Types Advanced   Journals
    Agriculture Volume 14 Issue 3 10.3390/agriculture14030489 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Academic Editor Wei
    Lu Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links
    Article Views 678 Table of Contents Abstract Introduction Background and Related
    Work System Architecture Design Implementation Results Discussion and Future Direction
    Conclusions Author Contributions Funding Data Availability Statement Conflicts
    of Interest References share Share announcement Help format_quote Cite question_answer
    Discuss in SciProfiles thumb_up Endorse textsms Comment first_page settings Order
    Article Reprints Open AccessArticle An Integrated IoT Sensor-Camera System toward
    Leveraging Edge Computing for Smart Greenhouse Mushroom Cultivation by Hoang Hai
    Nguyen 1, Dae-Yun Shin 1,2, Woo-Sung Jung 3, Tae-Yeol Kim 2 and Dae-Hyun Lee 4,*
    1 Sejong Rain Co., Ltd., In-House Venture of K-Water, Daejeon 34134, Republic
    of Korea 2 Graduate School of Smart Agriculture, Chungnam National University,
    Daejeon 34134, Republic of Korea 3 K-Water Research Institute, Daejeon 34045,
    Republic of Korea 4 Department of Biosystems Machinery Engineering, Chungnam National
    University, Daejeon 34134, Republic of Korea * Author to whom correspondence should
    be addressed. Agriculture 2024, 14(3), 489; https://doi.org/10.3390/agriculture14030489
    Submission received: 13 January 2024 / Revised: 25 February 2024 / Accepted: 5
    March 2024 / Published: 18 March 2024 (This article belongs to the Special Issue
    Application of Intelligent Greenhouse and Plant Factory Systems in Agricultural
    Production) Download keyboard_arrow_down     Browse Figures Versions Notes Abstract
    Industrial greenhouse mushroom cultivation is currently promising, due to the
    nutritious and commercial mushroom benefits and its convenience in adapting smart
    agriculture technologies. Traditional Device-Cloud protocol in smart agriculture
    wastes network resources when big data from Internet of Things (IoT) devices are
    directly transmitted to the cloud server without processing, delaying network
    connection and increasing costs. Edge computing has emerged to bridge these gaps
    by shifting partial data storage and computation capability from the cloud server
    to edge devices. However, selecting which tasks can be applied in edge computing
    depends on user-specific demands, suggesting the necessity to design a suitable
    Smart Agriculture Information System (SAIS) architecture for single-crop requirements.
    This study aims to design and implement a cost-saving multilayered SAIS architecture
    customized for smart greenhouse mushroom cultivation toward leveraging edge computing.
    A three-layer SAIS adopting the Device-Edge-Cloud protocol, which enables the
    integration of key environmental parameter data collected from the IoT sensor
    and RGB images collected from the camera, was tested in this research. Implementation
    of this designed SAIS architecture with typical examples of mushroom cultivation
    indicated that low-cost data pre-processing procedures including small-data storage,
    temporal resampling-based data reduction, and lightweight artificial intelligence
    (AI)-based data quality control (for anomalous environmental conditions detection)
    together with real-time AI model deployment (for mushroom detection) are compatible
    with edge computing. Integrating the Edge Layer as the center of the traditional
    protocol can significantly save network resources and operational costs by reducing
    unnecessary data sent from the device to the cloud, while keeping sufficient information.
    Keywords: smart agriculture; mushroom; edge computing; farm management information
    system (FMIS); machine vision; Agricultural IoT 1. Introduction Nowadays, edible
    mushrooms are well known as not only a healthy food but also a valuable pharmacy,
    since they can provide rich nutrients such as protein, minerals, and vitamins
    as well as increase the human immune system to prevent many diseases, even cancer
    [1,2]. Therefore, the demand for mushroom products is increasing in the world’s
    food market, especially after the COVID-19 pandemic [3,4]. Although wild mushrooms
    can be harvested in the natural environment, their high sensitivity to seasonal
    variations of weather/climate factors leads to unsustainable production [5,6].
    Industrialized mushroom cultivation in the greenhouse offers an alternative strategy
    to natural cultivation to improve both the quantity and quality of mushroom production
    because the key environmental parameters affecting mushroom growth can be monitored
    and controlled. However, the traditional cultivation method based on manual monitoring,
    controlling, and harvesting involves time-consuming and labor-intensive work which
    can then increase operational costs, demanding timely, automated, cost-effective,
    and eco-friendly solutions for the mushroom industry [7,8]. Along with the Fourth
    Industrial Revolution (Industry 4.0) when innovative technologies such as the
    Internet of Things (IoT), cloud computing, Big Data (BD), and Artificial Intelligence
    (AI) have been commonly employed in industrial automation, a parallel revolution
    in agriculture (Agriculture 4.0) boosts automation in farming activities by applying
    those new technologies to the agriculture sector, so-called smart agriculture/farming
    (or precision agriculture, digital agriculture, and Agricultural IoT). Although
    it can contribute to the reduction in labor costs, deploying smart agriculture
    systems in practice is facing other cost-related problems such as setup costs
    and running costs [9]. On the one hand, from the users’ point of view, especially
    in some developing countries where most people working in agriculture are smallholder
    farmers, they have limited access to modern smart farming infrastructure due to
    the expensive product prices (setup costs). On the other hand, from the perspective
    of service developers/providers, smart farming system construction and applications,
    which are commonly implemented in centralized cloud servers can consume massive
    resources for BD storage, management, analytics, and security, resulting in higher
    overall cloud service costs (running costs). Cutting those unnecessary costs is
    therefore pivotal for many business organizations, especially small and medium
    enterprises (SMEs), to optimize their smart farming systems toward serving smallholder
    clients with cost-saving infrastructures. Besides the cloud service costs, traditional
    smart agriculture systems are often limited by the slow network connection (low
    bandwidth) between Agricultural IoT devices and cloud servers when all the huge
    numbers of data collected from the devices are directly sent to the cloud, which
    then delays the system decisions and hampers real-time applications in smart agriculture
    (high latency) [10,11,12]. This is a challenge for mushroom cultivation when its
    key parameters require strict monitoring within short time intervals [13]. Edge
    computing (including cloudlets, fog, and mobile edge computing) has recently emerged
    as a promising solution to bridge this connection gap, thanks to the rapid advancement
    of information and communication technologies as well as hardware capacity. Specifically,
    an edge computing node can be regarded as a decentralized local server (edge server)
    that allows for storing a small number of data and bringing data computation closer
    to the edge devices [14,15,16]. By coupling the edge computing paradigm with a
    classical Device-Cloud protocol, the original BD is effectively managed in an
    offline process at the edge server before transmission instead of storing and
    processing all the collected raw data at the cloud server [17]. This can free
    up the bandwidth for faster connection and for the cloud server load spent on
    complicated analytics and applications [16,18]. Despite the wide deployment of
    edge computing in smart homes and smart cities, it is not very commonplace in
    smart farming, and thus there is still room for leveraging and improving this
    paradigm to meet the demands of smart agriculture as well as the greenhouse mushroom
    industry. Shifting partial work from cloud to edge offers SMEs an effective and
    cost-saving solution to overcome the issues of excessive bandwidth-cloud utilization
    and high latency [10]. However, determining which features should be conducted
    in the edge server highly depends on the specific functional requirements and
    capacity of each smart farming system [19]. This suggests the necessity to design
    a proper digital Farm Management Information System (FMIS) architecture assisting
    smart agriculture applications for the specific farmer’s requirements (e.g., for
    each different crop) [20,21,22], hereafter the Smart Agriculture Information System
    (SAIS). From the perspective of an SME, Sejong Rain Company in collaboration with
    Chungnam National University aims to design and implement a low-cost SAIS architecture
    customized for smart greenhouse mushroom management in association with a paradigm
    shift to edge computing for business objectives, in this article. This system
    was tested in a pilot site located in Songsan Green City, Republic of Korea. The
    advanced Agricultural IoT sensors developed by Korean companies, a standard network
    flow that is possible for scalability, and edge computing tasks suitable for greenhouse
    mushroom cultivation, together with examples of how to implement them, were introduced
    through the proposed SAIS architecture. Finally, a potential SAIS architecture
    for business purposes in future studies was also suggested, especially for the
    developing country markets that consider smart agriculture development as the
    core industry. 2. Background and Related Work 2.1. Smart Agriculture Information
    System (SAIS) The traditional FMIS, which relied on simple farm recordkeeping
    tasks, has been developed since the 1970s to provide useful information for decision-makers
    to effectively manage farming activities [23]. However, in the era of Agriculture
    4.0 nowadays, the adoption of a traditional FMIS is limited when massive data
    are provided from the IoT sensors and complex tasks are required for modern farm
    management [24,25]. To cope with the rapid increase of agricultural data pools
    and data-driven farming applications, the SAIS, whose cornerstone is the large
    digital FMIS specialized for precision agriculture, can improve the automation
    and efficiency in managing enormous amounts of farming information including data
    collection, data processing, and data-driven decision making [20,25]. Although
    the core of the SAIS is the multi-layered architecture comprising four basic components
    of Smart Product, Network Connection, Data, and Smart Service [22], recent studies
    paid more attention to adapting this core SAIS to the domain-specific uses by
    breaking it into higher-level architectures, ranging from the classical three-
    or four-layer up to the advanced seven- or eight-layer architectures. Among the
    classical architectures, the three-layer one, which corresponds to the two-layer
    without the Network Layer, refers to the Device-Cloud protocol where the raw data
    obtained by sensors will be directly transmitted to the cloud for storage, processing,
    and applications. The four-layer architecture (three-layer without the Network
    Layer) can allow edge computing to involve those cloud tasks. Integrating a separate
    layer of edge computing into SAIS architectures is also gaining attraction and
    possibly becoming a trending paradigm in future studies [17]. Additionally, advanced
    architectures improved on the classical ones for business objectives by proposing
    enterprise architecture by adding new layers such as the Business, User, or Security
    Layers [26]. For more details, the readers are referred to the literature reviews
    of different multi-layered SAISs in previous studies, summarized in Table 1. Table
    1. Literature reviews of multi-layered Smart Agriculture Information Systems.
    2.2. Smart Greenhouse Mushroom Cultivation Aside from the benefits for human health
    and commerce, edible mushrooms are currently becoming the preferred crop for indoor
    cultivation, even in urban farming, because they are easily relocated, do not
    require large space and direct sunlight, and need only a short duration until
    harvesting [35]. However, the major limitation came from the high sensitivity
    of mushrooms to the surrounding environment, since extreme weather conditions
    can immediately damage mushroom health. Therefore, the most important task of
    indoor mushroom cultivation relies on the timely and continuous control of the
    key environmental parameters’ ideal conditions at every mushroom growing stage
    [36]. Previous studies listed a wide range of those key parameters [13], but the
    two most important ones that are worth regarding are air temperature and humidity,
    while several studies further considered light intensity or carbon dioxide (CO2)
    levels. The ideal ranges for these parameters vary depending on different mushroom
    species [7,37], and they can be easily, timely, and automatically observed and
    controlled in the greenhouse using IoT sensors, which have been widely reported
    in the literature as the common approach for indoor mushroom management. Alongside
    the development of computer vision and precision agriculture technologies, high-resolution
    cameras associated with AI-based image processing have been successfully applied
    in the mushroom industry to enhance management accuracy and efficiency [38]. Current
    advances improved by combining the advantages of IoT sensors and camera computer
    vision, which opens a new door for future smart greenhouse mushroom cultivation
    in particular and smart agriculture in general. Literature reviews of smart greenhouse
    mushroom management using IoT sensors, cameras, and integrated IoT sensor-camera
    systems are summarized in Table 2. Table 2. Literature reviews of smart greenhouse
    mushroom management. 3. System Architecture Design Based on the potential of the
    SAIS with edge computing and the increased need for mushroom products, this study
    aims to design and implement a multi-layered SAIS architecture with leveraging
    edge computing for cost-effective smart greenhouse mushroom cultivation. In particular,
    an integrated IoT environmental sensor-camera system and the oyster mushroom were
    selected to test in this study. To highlight the capability of coupling the edge
    computing model to the classical Device-Cloud protocol, a general three-layer
    SAIS architecture (without the Network Layer), which adopted the Edge Layer as
    a bridge for the Device and Cloud (Device-Edge-Cloud protocol), was employed.
    Moreover, we also suggested the integration of a bidirectional (two-way) communication
    mechanism in this system to separate the network flow into two major domains for
    different functional tasks, including the Forward and Backward Domains. In each
    layer, two modules were included to respond to separated functional tasks in the
    Forward and Backward Domains, respectively. The proposed SAIS architecture applied
    to greenhouse mushroom management in this paper is depicted in Figure 1. Specifically,
    according to Figure 1, the Forward Domain enables data transmission from the Device
    Layer (Data Collecting module) through the Edge Layer for quality control and
    aggregation (Data Preprocessing module), and then to the Cloud Layer (Data Storage
    module). In contrast, the Backward Domain allows an inverse direction that AI-based
    solutions developed at the Cloud Layer using stored data (Data Analytics/AI Development
    module) to be deployed at the Edge Layer for real-time applications (Real-time
    AI Deployment module), then to control the ideal conditions of the environmental
    parameters if necessary via controlling actuators/sensors at the Device Layer
    (Device Controlling module). The Forward Domain is responsible for the data management
    and storage tasks while the Backward Domain is responsible for the solutions development
    and decision-making based on the obtained data. In this study, we describe the
    proposed architecture following its network flow and focus especially on the Forward
    Domain, since it is the mainstream in the SAIS. It is important to note that although
    the Network/Communication Layer which encompasses the protocols for network connection
    is also important and was already employed in this paper, we did not focus on
    modifying this layer, so it was not mentioned here as a major layer. A detailed
    description of the proposed SAIS architecture and its implementation for smart
    greenhouse mushroom management at the testbed site is provided below. Figure 1.
    A proposed design of the Smart Agriculture Information System (SAIS) architecture
    for smart greenhouse mushroom cultivation. The blue dashed border and arrows indicate
    the Forward Domain and its procedure, and the red dashed border and arrows indicate
    the Backward Domain and its procedure. 3.1. Forward Domain–Device Layer The Device
    Layer is the first layer in this proposed SAIS architecture, which enables a workspace
    for Agricultural IoT end devices. In this layer, the Data Collecting module is
    particularly responsible for the Forward Domain. Its detailed architecture is
    presented in Figure 2. Figure 2. The architecture of the Forward Domain in the
    Device Layer. Data Collecting: This module aims to collect farming and environmental
    data observed from the IoT sensors system. Since the integrated IoT environmental
    sensor-camera system was employed, this module was separated into two sub-modules
    for better management, consisting of (1) the one-dimensional (1D) Data sub-module
    which stands for point environmental data collected from the IoT sensors, and
    (2) the two-dimensional (2D) Data sub-module, which stands for image data collected
    from the cameras. 1-Dimensional Data (1): This sub-module offers a workspace for
    the point IoT sensors to collect the environmental data time series. Because temperature,
    humidity, and CO2 are generally the key environmental parameters for mushroom
    growth in the literature, they were considered in this system. A three-in-one
    IoT environmental sensor, which measures automatically and simultaneously the
    three key parameters of temperature (T), relative humidity (RH), and CO2 levels
    (CO2) in real-time using only one sensor, was adopted in this study. This low-cost
    combination sensor is a domestic Korean product that was researched and developed
    by the Sejong Rain Company, Republic of Korea, and is currently being tested before
    commercialization. Even though this combination sensor can observe real-time data,
    the maximum 10 min time resolution has been set to be updated in this sub-module.
    For more information or inquiries on this combination IoT sensor, please refer
    to the company homepage (http://sejongrain.com/, accessed on 10 January 2024).
    Its photos and specifications are shown in Figure 3a,b, and Table 3, respectively.
    Two-Dimensional Data (2): Besides the IoT environmental sensor, this system uses
    a surveillance camera system to collect crop image/video (time series of RGB images)
    data, which enhances automation in intuitive recognition of mushroom characteristics
    (e.g., shapes, colors, species, phenological stages, or related diseases) and
    was implemented in this sub-module. Specifically, an industrial high-definition
    (HD 720p) digital camera (model SMT-720PUSBBOX) manufactured by Smtkey Electronic
    Technology Company, Shenzhen Guangdong, China, was employed to provide streaming
    images/video of the oyster mushroom. However, to match the temporal resolution
    of the environmental sensor, the camera system was set to capture image time series
    at 10 min intervals in single red (R), green (G), and blue (B) bands. A photo
    of the camera used is shown in Figure 3c and its specifications are presented
    in Table 4. Figure 3. Photos of the combination IoT environmental sensor: (a)
    displaying temperature and humidity and (b) displaying CO2 level; and (c) photo
    of the surveillance camera used in this study. The Korean word in the IoT combination
    sensor label is the name of the company who develops this device, the Sejong Rain
    Company, Republic of Korea. Table 3. Specifications of the combination IoT environmental
    sensor used. Table 4. Specifications of the surveillance camera used. 3.2. Forward
    Domain–Edge Layer The Edge Layer or edge server is a middle layer in this proposed
    three-layer SAIS, where edge computing is executed, and is the major part of this
    study. It is important to note that edge computing is not a replacement for traditional
    cloud computing, but that it is a complement for cloud computing. Edge computing
    is normally conducted in low-memory and low-power microprocessors and assigned
    for small workloads with lightweight computation, which fits in with real-time
    AI applications. In contrast, cloud computing is compatible with smart solutions
    requiring BD and heavy computation capacity. Therefore, a powerful, low-energy-consumption,
    and cost-effective Raspberry Pi single-board computer was used in this study for
    edge computing in both Forward and Backward Domains. In particular, the Raspberry
    Pi 3 Model B+, which also provides a high-speed processor suitable for HD video
    processing and dual connections via both WiFi (wireless) and Ethernet port (wired),
    was employed in the system. For a more detailed description of its specifications,
    users are referred to the company’s website (https://www.raspberrypi.com/, accessed
    on 10 January 2024). The related tasks are shown in the Data Preprocessing module
    for the Forward Domain, and its architecture is depicted in Figure 4. Figure 4.
    The architecture of the Forward Domain in the Edge Layer. Data Preprocessing:
    It is important to note that not all the raw data should be stored in the cloud.
    Selecting essential data for the cloud can save resources by decreasing cloud
    and bandwidth workloads. Hence, the main mission of this module is to reduce the
    number of raw data observed from the Device Layer while keeping data quality before
    sending them to the Cloud Layer, based on the following sub-modules for preprocessing:
    Small-Data Storage (1): Since a previous study indicated the effective greenhouse
    mushroom monitoring on an hourly basis [13], we also set a one-hour (1 h) interval
    as the standard temporal resolution to be sent to the cloud and for mushroom cultivation
    in this study. To this end, several raw data should be stored in the Edge Layer
    for further temporal data resampling and quality control development. Temporal
    Resampling (2): The temporal resampling applied in this sub-module aims to convert
    the 10 min raw data into hourly data and relies on a simple Average Filtering
    method, whereas the six raw data samples within every 1h interval stored in the
    Small-Data Storage sub-module will be transferred to this sub-module for averaging
    into one data sample. This method can be applied to both 1D and 2D data in this
    SAIS and may help it reduce the number of data together with overcoming the temporal
    gaps (missing data) that occurred at the original 10 min interval. Data Quality
    Control (3): Despite the benefit of temporal resampling in dealing with the gaps
    in raw data time series, the resampled data still probably suffer from temporal
    gaps (when six raw data samples are missing values) or sudden extreme conditions.
    This requires an AI-based data quality control filter that can not only automatically
    and continuously detect such outliers in the data stream, but also has a low computational
    cost when it is applied to the Edge Layer. Even though several lightweight AI
    models are compatible with edge/fog computing, the well-known k-nearest neighbors
    (k-NN) algorithm was used for showcasing in this research. The k-NN is simply
    a non-parametric supervised learning method, which considers k samples of training
    data to solve the classification and regression problems, but it can be regarded
    as an unsupervised learning algorithm when it is applied to anomaly detection
    [50]. The k-NN integrated with a 24 h moving window was applied in this study
    to identify whether real-time data are anomalous or not, based on the lagged 23h
    data samples stored in the Small-Data Storage sub-module. Whenever the ksample
    is detected and masked, it can be continuously used to identify the (k + 1) sample,
    and the (k − 23) sample is then automatically removed from the Small-Data Storage
    sub-module. This AI-based data quality control filter can be applied to both 1D
    and 2D data. For 1D data, besides transmitting them to the Cloud Layer for long-term
    storage, the processed data were also sent to a responsive module of the Backward
    Domain in the Edge Layer to support the system’s real-time decision-making. However,
    in the case of 2D data, to reduce the high computational cost when processing
    image data, the RGB images obtained from the camera were first converted to grayscale
    images and then transformed to 1D format by simply averaging the digital number
    (DN) values within an image scene (scene-averaging) before they can be applied,
    with the data quality control filter. Anomalous DN data samples closer to 0 can
    be identified as temporal gaps (black images), while those with high values (e.g.,
    higher than 80—a typical average grayscale digital number value) can be classified
    as light images, which still provide useful information. The quality-controlled
    image data on an hourly basis were sent only to the Cloud Layer. 3.3. Forward
    Domain–Cloud Layer The Cloud Layer is the final layer in this proposed SAIS, which
    is widely used in various architectures and allows high computational application
    development based on the quality-controlled BD. Hence, the Forward Domain offers
    long-term high-quality data to be stored firstly in the Cloud Layer under the
    Data Storage module management. Data Storage: Because the accuracy of AI models
    significantly depends on the quality and quantity of the training data, this module
    aims to provide a space to store long-term (e.g., one life cycle of mushrooms)
    high-quality data observed from the integrated environmental sensor-camera system
    to develop AI-based solutions which require a high computation cost. These necessary
    datasets stored in this module can be then transmitted to the responsive module
    of the Backward Domain in the Cloud Layer for training AI models with respect
    to specific user requirements. 3.4. Backward Domain–Cloud Layer As highlighted
    in the description above, the mission of the Backward Domain is to develop AI-based
    smart solutions and make decisions based on the collected and stored data in the
    Forward Domain. Therefore, in the Cloud Layer, the Backward Domain tasks are conducted
    in the AI Development module. Figure 5 presents the architecture of the Backward
    Domain in the Cloud Layer. Figure 5. The architecture of the Backward Domain in
    the Cloud Layer. AI Development: This module is designed to account for AI-based
    smart solution development for specific demands, which requires BD for high computational
    cost training and complex analytics. The implementation of this module in the
    Backward Domain of the Cloud Layer follows the two sub-modules: AI Model Training
    (1): This sub-module includes the feature selection task for training AI models
    based on user-specific demands, whereas relevant important features were extracted
    from the BD in the Data Storage module. Furthermore, several well-known AI model
    candidates will be selected for training. Parameter Storage (2): The optimal AI
    model among the candidates was chosen and its optimal parameters were then stored
    in this sub-module for further deployment. Various AI-based smart greenhouse mushroom-cultivation
    solutions can be considered for development. However, to showcase only one example
    of the high-computational-cost AI model application with BD (mostly with image/video
    data) in the Cloud Layer, a well-known, fast, effective, and advanced computer
    vision algorithm version among the series of the “You Only Look Once” (YOLO) framework
    [51], the YOLOv5 model, was used in this study for real-time mushroom detection
    with a surveillance camera. Specifically, the YOLOv5 model, whose cornerstone
    is a simple deep convolutional neural network, was introduced in 2020 to improve
    on its predecessors in object detection. The YOLOv5 model divides images into
    a grid cell and finally predicts the objects with multiple bounding boxes per
    grid cell. Its architecture consists of three main components: the BackBone for
    feature extraction, the Neck for feature fusion, and the Head for feature conversion
    to bounding-box parameters. The BD images stored in the Big-Data Storage module
    can be labeled for training the YOLOv5 model. To reduce the training costs (including
    the training time and number of training-data samples), this study adopted the
    Transfer Learning method to fine-tune the pre-trained YOLOv5 model for mushroom
    detection to be compatible with the current training mushroom images in the Big-Data
    Storage module. The optimal model parameters were stored in the Parameter Storage
    sub-module and then transferred to the Edge Layer for real-time deployment. 3.5.
    Backward Domain–Edge Layer The Backward Domain in the Edge Layer provides the
    workspace for real-time smart agriculture solution deployment, which was assigned
    to the Real-time AI Deployment module, with its architecture provided in Figure
    6. Figure 6. The architecture of the Backward Domain in the Edge Layer. Real-time
    AI Deployment: This module consists of four sub-modules for integrating analytics
    from both environmental sensors and cameras to improve real-time solution/decision
    making, as follows: Short-term Analytics (1): This sub-module receives the analytical
    results from the Forward Domain in the Edge Layer (Data Preprocessing module),
    mainly based on temporary small-data storage of the 1D data from the IoT environmental
    sensor. Long-term Analytics (2): The optimal AI model obtained from the Backward
    Domain based on BD analytics in the Cloud Layer (AI Development module) was deployed
    in this sub-module, mainly for the 2D image data. Integrated Analytics (3): This
    sub-module combines the analytical results from the Short-term and Long-term Analytics
    sub-modules to enhance decision-making. Solutions/Decision Making (4): Relevant
    solutions/decisions will be made in this sub-module, based on the results from
    the Integrated Analytics sub-module. 3.6. Backward Domain–Device Layer The Backward
    Domain in the Device Layer covers the Device Controlling module. Device Controlling:
    This module aims to automatically control sensors/actuators to make farm management
    decisions as soon as possible, based on the analytical solutions provided by the
    Real-time AI Deployment module in the Edge Layer. For example, the image-based
    mushroom detection solution allows users to recognize whether the mushrooms are
    currently in the growing period or have been harvested. In the case of extreme
    conditions observed by the environmental sensor during the mushroom growing period,
    this module enables the modulating of key environmental parameters by controlling
    IoT sensors/actuators such as the heater/cooling fan (for temperature), humidifier/dehumidifier
    (for humidity), or air ventilation (for CO2 level) via connection to a microcontroller.
    4. Implementation Results 4.1. Implementation of Forward Domain–Device Layer Installation
    of the integrated IoT environmental sensor-surveillance camera system for smart
    greenhouse mushroom cultivation is the first implementation step and was carried
    out in a mushroom house located in Songsan Green City, Republic of Korea. This
    system aims to collect the key environmental parameters affecting mushroom growth
    and intuitive mushroom information, which is the responsibility of the Data Collecting
    module belonging to the Forward Domain in the Device Layer. Figure 7 describes
    the installation of the combination IoT environmental sensor for simultaneously
    measuring the real-time 1D data of three key environmental parameters including
    air temperature (T), atmospheric relative humidity (RH), and CO2 level (CO2),
    together with the construction of a data logger containing the Raspberry Pi single-board
    computer. Data collected from the IoT environmental sensor were then transmitted
    to the data logger for further processing, mainly via a wireless connection (Figure
    7a). However, we also set up ethernet cables here to ensure that data transmission
    can be automatically switched to a wired connection when the wireless connection
    collapses, maintaining their stable transmission (Figure 7b). It is important
    to note that although the environmental sensor was set up in the greenhouse environment,
    we constructed the data logger outside the mushroom house for future combination
    with outdoor smart agriculture sensors/actuators such as smart precipitation gauges,
    unmanned aerial vehicles, tractors, and so on (Figure 7c). Figure 7. Installation
    of the combination IoT environmental sensor for the greenhouse mushroom information
    system with connection to data logger via the following: (a) wireless connection
    and (b) wired connection. (c) Construction of data logger outdoors with potential
    combination with outdoor sensors. The Korean words in the label of the data logger
    represent for the system name “Agriculture Environment Data Collection System”
    and the company name “Sejong Rain”. The time series of the raw 1D data (at 10
    min of temporal resolution) for the three key parameters from the installed IoT
    environmental sensor for nearly one month (one life cycle of the mushroom) is
    depicted in Figure 8. In general, no major operational errors occurred when continuous
    data were collected without missing values and the observed data range fell within
    the sensor’s allowable measuring range for each parameter, according to Table
    3. This suggests that the combination IoT environmental sensor used in this study,
    which was developed by the Korean company, is ready and has potential for smart
    mushroom monitoring and further applications in smart agriculture. Figure 8. Time
    series of 1D raw data in 10 min intervals collected from the combination IoT environmental
    sensor for (a) temperature (T), (b) humidity (RH), and (c) CO2 level (CO2). Besides
    the IoT environmental sensor, the simultaneous installation of the surveillance
    camera to capture real-time intuitive 2D data for crop information was also implemented
    in the pilot area, as illustrated in Figure 9. The camera was fixed to collect
    a time series of RGB images covering a partial mushroom farm for preliminary testing.
    We also connected the camera system to the data logger for data transmission.
    Figure 9. Installation of the surveillance camera for the greenhouse-mushroom
    information system. Time series of the raw 2D data at 10 min time intervals were
    obtained from the installed surveillance camera. We showcase several RGB image
    samples of mushrooms at different growth stages in Figure 10. It can be drawn
    from Figure 10 that intuitive mushroom information can be well captured by the
    installed surveillance camera, even with the changes between growth stages. However,
    there are several temporal gaps (image samples with no-value/black images) that
    occurred during the operation which need to be noticed and addressed. All the
    raw data collected in the Device Layer were then sent to the Edge Layer for pre-processing.
    Figure 10. Samples of 2D raw data in 10 min intervals collected from the surveillance
    camera during different mushroom growth stages. The red rectangle highlights the
    gap (no-value data) in the data stream. 4.2. Implementation of Forward Domain–Edge
    Layer The raw 1D and 2D data collected at the Device Layer were then delivered
    to the Forward Domain in the Edge Layer for further preprocessing before being
    transmitted to the Cloud Layer. The Edge Layer is the major layer in this designed
    SAIS, whose main goal is a stepping stone toward saving cloud and bandwidth resources
    as well as selecting essential data for the cloud server. Therefore, the Forward
    Domain in this layer (represented by the Data Preprocessing module) is responsible
    for reducing unnecessary data collected from the Device Layer and conducting quality
    control for high-quality data sent to the Cloud Layer. To this end, several raw
    data samples (small data) should be stored in this layer for a short period and
    the Small-Data Storage sub-module is responsible for providing storage space in
    this task. The first preprocessing task in this module is data reduction. Since
    the mushroom monitoring on an hourly basis is sufficient and was selected as the
    standard temporal resolution to be updated in the cloud, an average filtering
    technique was utilized as the temporal resampling method to convert 10 min data
    into 1h data. Figure 11 illustrates examples of how to conduct this temporal resampling
    method in both 1D and 2D data time series. In particular, every six raw data samples/images
    within a current 1h duration were initially stored. Since no significant differences
    were observed among these raw data, an hourly representative point value/image
    can be generated by simply taking an average of those six data samples/images.
    After an hourly representative data sample/image is given, it will be sent back
    to the Small-Data Storage sub-module for temporal storage and the current raw
    data will be automatically removed from this sub-module for updating the coming
    raw data. It is important to note that, besides the data reduction benefit, this
    method also supports dealing with temporal gap problems, ensuring continuous measurement
    of the data stream. For example, a black image (temporal gap) that occurred in
    the raw 2D data was successfully removed and replaced by a representative image
    (average image of the six raw data in 10 min intervals within every 1h interval)
    while keeping sufficient information, as illustrated in Figure 11b. Figure 11.
    Examples of the temporal resampling based on the average filtering method to resample
    raw data in 10 min intervals into standard data in 1h intervals applied to the
    following: (a) 1D data from the environmental sensor and (b) 2D data from the
    camera. The red dashed rectangle represents the 1h data window. The second preprocessing
    task in the Forward Domain of the Edge Layer is for data quality control, which
    is covered by the Data Quality Controlling sub-module. For the 1D data, since
    the important task of smart greenhouse mushroom cultivation is mainly based on
    modulating the ideal conditions of key environmental parameters, this sub-module
    aims to determine the sudden extreme conditions occurring in the environmental
    data, supporting the system with timely responses to address the problems as well
    as managing high-quality data sent to the cloud server. However, because edge
    computing is compatible with light computing applications that can work well in
    short-term small-data numbers stored in this layer, effective and lightweight
    AI algorithms are preferable for this task. As a result, a widely used lightweight
    AI model, the k-NN algorithm, was selected as the anomaly-detection method in
    this study. More specifically, the k-NN was conducted in a 24h moving window (k
    = 23) for the continuous hourly data samples stored in the Small-Data Storage.
    The real-time outlier detection of the current data sample (the k sample) can
    be performed based on its recent 23 lagged hourly data samples. Whenever the k
    sample is detected and masked, it can be continuously used to identify the (k
    + 1) sample, and the (k − 23) sample is automatically deleted from the Small-Data
    Storage sub-module. Figure 12 presents the results of applying the k-NN algorithm-based
    anomaly detection to the 1D hourly data of three key environmental parameters
    for mushroom cultivation. These successful implementation results of the k-NN
    model-based anomaly detection in the 1D environmental data, as illustrated in
    Figure 12, highlight the suitability and potential application of lightweight
    AI models for edge computing. Figure 12. Results of the AI-based anomaly-detection
    application to the 1D hourly (hr) environmental data for (a) temperature (T),
    (b) humidity (RH), and (c) CO2 level (CO2). As regards the 2D data, one problem
    is that although the temporal resampling method can deal with temporal gaps in
    the raw data, they still possibly occur in the resampled data (when the six original
    images within one hour are all black images). Hence, the anomaly-detection method
    should be applied to the hourly 2D data to detect these gaps. However, to deal
    with the high computational cost when processing numerous crop images, a preprocessing
    procedure series was applied to the 2D data to transform them into 1D data before
    conducting the AI-based anomaly detection. In particular, an RGB image was first
    converted to a grayscale image and subsequently transformed into a digital number
    value by averaging grayscale digital numbers from all pixels within an image scene
    (scene averaging). Figure 13 provides a general example of how to carry out AI-based
    anomaly detection in the 2D data and their results for mushroom images in this
    study. As can be seen from Figure 13, two types of outliers have been identified
    by using the k-NN anomaly-detection method. Anomalous data with digital numbers
    closer to 0 are assigned to the temporal gaps (black images), which do not provide
    any useful information, whereas the outliers with the highest values (e.g., greater
    than 80—a typical average grayscale digital number value) are identified and masked
    as “light image”, which still provide similar crop information to the normal images
    but at a higher brightness. Finally, after the resampled data are quality-controlled,
    they will be sent to the cloud server for storage and to the Short-term Analytics
    sub-module in the Backward Domain of the Edge Layer for supporting real-time decision
    making. Figure 13. An example of the AI-based anomaly-detection application to
    the 2D image data. The color maps on the left side display the heatmap of an original
    image in digital numbers for red, green, blue, and grayscale channels. 4.3. Implementation
    of Forward- and Backward-Domain–Cloud Layer In the Cloud Layer, high-quality BD
    transmitted from the Edge Layer after passing the data quality control procedure
    will be initially stored in the Big-Data Storage Module of the Forward Domain.
    These high-quality BD stored in the Cloud Layer were then sent to the AI Development
    Module for training high-computational-cost AI models based on specific user requirements.
    This study employed a widely used deep learning-based object-detection algorithm,
    the YOLOv5 model, to address the mushroom recognition problem, mainly for the
    2D image data. Moreover, to reduce the computational cost during the training
    process, the transfer learning technique was integrated to fine-tune the pre-trained
    YOLOv5 model for adapting the mushroom images stored in the Cloud Layer. Detailed
    information on training and evaluation of the YOLOv5 model for mushroom detection
    in this study is shown in Table 5. The obtained optimal hyperparameters were subsequently
    stored in the Parameter Storage module and were used for real-time deployment
    of this solution in the Edge Layer. Table 5. Information on the training and evaluation
    of the YOLOv5 model for mushroom detection. 4.4. Implementation of Backward Domain–Edge
    Layer The Backward Domain in the Edge Layer, which was carried out by the Real-time
    AI Development module, aims to provide integrated analytics for decision-making
    by combining the results of short-term and long-term analytics. In particular,
    on the one hand, the Short-term Analytics module in this layer obtained the key
    environmental-parameter anomaly-detection results from the Forward Domain in the
    Edge Layer, which was described in the section above (Implementation of Forward
    Domain–Edge Layer) and is shown in Figure 12. On the other hand, when it comes
    to long-term analytics, this module adapted the developed YOLOv5-based mushroom-recognition
    model for detection, based on the stored optimal model hyperparameters from the
    Backward Domain in the Cloud Layer. Representative examples of the YOLOv5 model
    deployment for mushroom detection in this study are depicted in Figure 14, whereas
    single-object (mushroom) areas are detected in continuous images with the red
    bounding box and confidence level by further applying instance segmentation. According
    to the figure, in general, most of the mushroom areas during the fruiting period
    can be well identified by the system, with a clear discrimination of the non-mushroom
    areas observed. This indicated the successful deployment of a high-computational-cost
    AI model in the Edge Layer, demonstrating its suitability to adopt a developed
    AI model in the Cloud Layer for edge computing. However, there are still several
    mushroom areas in an image that the system cannot detect, suggesting the need
    for retraining the model to improve its performance by the updated high-quality
    BD sent to the Cloud. The results from the long-term analytics can be integrated
    with those from the short-term analytics to support the system’s decision-making.
    For example, if outliers are detected in the streaming environmental data (results
    from the short-term analytics) during the mushroom’s fruiting period (results
    from the long-term analytics), the system can automatically send an alert notification
    to the users or control the sensors/actuators for maintaining the normal status
    of environmental conditions. Figure 14. An example of implementation of the Real-time
    AI Deployment module (deployment of YOLOv5 and transfer learning in mushroom detection).
    5. Discussion and Future Direction The major difference between this proposed
    research and related work mentioned in Table 1 and Table 2 is the combined use
    of the edge-computing paradigm in a smart greenhouse mushroom-cultivation information
    system. In particular, the Edge Layer was not included in the SAIS architecture
    for mushroom cultivation in Table 1 [28], while none of the previous mushroom
    management studies in Table 2 employed the edge computing model in their system.
    Thus, the successful practical application of this study can shed new light on
    leveraging edge-computing paradigm shifts as well as improving smart mushroom
    cultivation systems. In addition, a minor discrepancy that is worth mentioning
    is the introduction of a new low-cost combination IoT environmental sensor made
    by a Korean company. This product combines three single sensors for different
    environmental parameters into one sensor (three-in-one sensor) and also employs
    low-cost Raspberry Pi single-board computers working in the resource-saving edge
    computing paradigm, so it can contribute to the diverse selections for clients
    in low-cost smart agriculture devices and information system markets. Notwithstanding
    the benefits of edge computing in complementing classical cloud computing and
    in improving smart greenhouse mushroom cultivation, this preliminary study may
    suffer from several challenges in the future. First, the implementation costs
    when scaling this framework up to a larger scale for other crops, together with
    integrating outdoor smart agriculture systems, are questionable, since additional
    costs from new IoT devices and their installation might be included in the system.
    This may limit the commercialization of this framework for business objectives.
    Second, the improvement in mushroom production efficiency when the edge computing
    paradigm is included is not fully analyzed, suggesting the implementation of more
    detailed analyses for this problem in further studies. Addressing potential challenges
    is also related to the future outlook of this research. Besides the application
    customized for mushroom cultivation, the successful practical implementation of
    this study suggests a potential to scale up this framework for business objectives
    and other user-specific requirements (e.g., other crops) in future studies. To
    this end, in the future direction, this fundamental SAIS architecture is expected
    to be coupled with the two new layers, including the User and Administration Layers.
    The potential architecture for future the SAIS is illustrated in Figure 15, and
    a detailed description for each added layer is provided below. Figure 15. The
    potential architecture of the future SAIS for business objectives. The blue dashed
    border and arrows indicate the Forward Domain and its procedure, and the red dashed
    border and arrows indicate the Backward Domain and its procedure. User Layer:
    This layer is particularly designed for the users of the SAIS such as farmers
    or clients. From the user’s perspective, the acquisition of quality-controlled
    datasets and the intuitive visualization of these datasets with timely notifications
    if any anomalous events occur from the Cloud Layer, and the capability to control
    the devices manually in an emergency in the Device Layer, are preferable. Thus,
    several respective modules such as Data Extraction, Visualization/Notification,
    or Manual Device Controlling Modules can be considered in this layer. Since the
    users mainly work in the Device and Cloud Layers, the users can only access these
    two layers, and they do not need to access the Edge Layer to adjust any edge computing
    tasks. Administration Layer: This layer is particularly designed for the providers
    or developers (e.g., companies) of the SAIS for business goals. Unlike the users,
    the administrators aim to manage all the data flow from the Device Layer to the
    Cloud Layer, so they can access and collect data from all the layers in this system,
    which can be conducted by the Data Extraction/Management module. Moreover, the
    administrators can meet the specific user requirements by developing suitable
    solutions, and ensure the safety of the system by maintaining the security. Therefore,
    the two respective modules including Application Development and Network Security
    are considered in this layer. Not only are the above additional layers being included,
    but the Device Layer is also planning to be extended with new sensors, in the
    future system. Microcontrollers and several controlling sensors/actuators (e.g.,
    heaters, fans, humidifiers, or air ventilation…) are expected to be utilized,
    which can complement the current indoor system. Furthermore, a parallel SAIS for
    outdoor smart agriculture with relevant IoT devices (e.g., smart rain gauges,
    automated weather sensors, irrigation valves, or unmanned aerial vehicles…) can
    be considered and developed for an integrated indoor–outdoor smart agriculture
    system. 6. Conclusions Traditional Device-Cloud protocol in smart agriculture
    often suffers from the challenges of delayed system responses caused by low bandwidth
    and high latency, together with high cloud-service costs for data computation
    and storage when enormous numbers of data acquired from IoT devices are directly
    transmitted to the cloud server without processing. Novel edge computing offers
    an effective solution to deal with challenges in this traditional protocol by
    shifting partial data storage and computation capability from the cloud server
    to edge devices. Nevertheless, selecting which tasks can be implemented in edge
    computing depends on user-specific demands, suggesting the need to design a specific
    and proper Smart Agriculture Information System (SAIS) architecture that is compatible
    with single-crop requirements. Based on the nutritious and commercial benefits
    of edible mushrooms as well as the necessity of cost reduction from a business
    viewpoint, the major goal of this study is to design and implement a standard
    multilayered SAIS architecture for smart greenhouse mushroom cultivation toward
    leveraging edge computing, which can be scalable for business goals. In this designed
    SAIS, the three-layer architecture, which couples edge computing in the central
    layer to connect the Device and Cloud Layers (Device-Edge-Cloud protocol) and
    enables automation in mushroom management using an integration of the IoT environmental
    sensor (for mushroom key environmental-parameter monitoring) and surveillance
    camera (for intuitive mushroom monitoring) was employed and tested in a testbed
    site in the Republic of Korea. Via this designed SAIS, we aim to introduce the
    advanced combination environmental IoT sensors developed by Korean companies,
    a standard network flow that is possible for scalability and suitable edge computing
    tasks, with typical examples. Moreover, a potential SAIS architecture with additional
    layers for a future direction for business purposes was also suggested. The successfully
    implemented SAIS architecture indicated that our combination IoT environmental
    sensor integrated with a surveillance camera could monitor the real-time key environmental
    parameters affecting mushroom growth and intuitive mushroom information. Typical
    examples of mushroom cultivation based on the collected data revealed that several
    low-cost data pre-processing procedures including small-data storage, temporal
    resampling-based data reduction, and lightweight artificial intelligence (AI)-based
    data quality control for anomaly detection within environmental conditions, together
    with real-time AI model (YOLOv5) deployment for mushroom recognition from crop
    images, are compatible with edge computing. In contrast, high-quality BD storage
    and high-computational-cost AI model development can be implemented in the cloud.
    Moreover, integrating the Edge Layer as the center of the traditional protocol
    can significantly save network resources and operational costs by reducing unnecessary
    data sent from the device to the cloud while keeping sufficient information. Finally,
    the future improvement suggests including additional layers to meet user-specific
    demands for business objectives and the extension of the current system with new
    controlling sensors and a parallel outdoor system, toward an integrated indoor–outdoor
    smart agriculture system. Author Contributions Conceptualization, H.H.N., D.-Y.S.
    and D.-H.L.; methodology, H.H.N. and D.-Y.S.; software, H.H.N.; validation, D.-Y.S.,
    W.-S.J. and T.-Y.K.; formal analysis, T.-Y.K.; investigation, W.-S.J.; resources,
    D.-Y.S.; data curation, H.H.N.; writing—original draft preparation, H.H.N.; writing—review
    and editing, D.-Y.S., W.-S.J. and T.-Y.K.; visualization, H.H.N.; supervision,
    D.-H.L.; project administration, D.-H.L.; funding acquisition, D.-H.L. All authors
    have read and agreed to the published version of the manuscript. Funding This
    research was supported by the “Regional Innovation Strategy” (RIS) through the
    National Research Foundation of Korea (NRF), funded by the Ministry of Education
    (MOE) (2021RIS-004), and the research fund of Chungnam National University (CNU),
    Republic of Korea (2022-0694-01). The APC was also funded by the MOE through the
    NRF RIS project (2021RIS-004) and the CNU research fund (2022-0694-01). Data Availability
    Statement The raw data supporting the conclusions of this article will be made
    available by the authors on request. Conflicts of Interest Authors Hoang Hai Nguyen
    and Dae-Yun Shin were employed by the company Sejong Rain Co., Ltd. The remaining
    authors declare that the research was conducted in the absence of any commercial
    or financial relationships that could be construed as a potential conflict of
    interest. The funders had no role in the design of the study; in the collection,
    analyses, or interpretation of data; in the writing of the manuscript; or in the
    decision to publish the results. References Ba, D.M.; Ssentongo, P.; Beelman,
    R.B.; Muscat, J.; Gao, X.; Richie, J.P. Higher Mushroom Consumption Is Associated
    with Lower Risk of Cancer: A Systematic Review and Meta-Analysis of Observational
    Studies. Adv. Nutr. 2021, 12, 1691–1704. [Google Scholar] [CrossRef] [PubMed]
    Roncero-Ramos, I.; Delgado-Andrade, C. The Beneficial Role of Edible Mushrooms
    in Human Health. Curr. Opin. Food Sci. 2017, 14, 122–128. [Google Scholar] [CrossRef]
    Hamza, A.; Ghanekar, S.; Santhosh Kumar, D. Current Trends in Health-Promoting
    Potential and Biomaterial Applications of Edible Mushrooms for Human Wellness.
    Food Biosci. 2023, 51, 102290. [Google Scholar] [CrossRef] Kour, H.; Kour, D.;
    Kour, S.; Singh, S.; Jawad Hashmi, S.A.; Yadav, A.N.; Kumar, K.; Sharma, Y.P.;
    Ahluwalia, A.S. Bioactive Compounds from Mushrooms: Emerging Bioresources of Food
    and Nutraceuticals. Food Biosci. 2022, 50, 102124. [Google Scholar] [CrossRef]
    Kauserud, H.; Stige, L.C.; Vik, J.O.; Økland, R.H.; Høiland, K.; Stenseth, N.C.
    Mushroom Fruiting and Climate Change. Proc. Natl. Acad. Sci. USA 2008, 105, 3811–3814.
    [Google Scholar] [CrossRef] Procházka, P.; Soukupová, J.; Tomšík, K.; Mullen,
    K.J.; Čábelková, I. Climatic Factors Affecting Wild Mushroom Foraging in Central
    Europe. Forests 2023, 14, 382. [Google Scholar] [CrossRef] Rukhiran, M.; Sutanthavibul,
    C.; Boonsong, S.; Netinant, P. IoT-Based Mushroom Cultivation System with Solar
    Renewable Energy Integration: Assessing the Sustainable Impact of the Yield and
    Quality. Sustainability 2023, 15, 13968. [Google Scholar] [CrossRef] Sujatanagarjuna,
    A.; Kia, S.; Briechle, D.F.; Leiding, B. MushR: A Smart, Automated, and Scalable
    Indoor Harvesting System for Gourmet Mushrooms. Agriculture 2023, 13, 1533. [Google
    Scholar] [CrossRef] Elijah, O.; Rahman, T.A.; Orikumhi, I.; Leow, C.Y.; Hindia,
    M.N. An Overview of Internet of Things (IoT) and Data Analytics in Agriculture:
    Benefits and Challenges. IEEE Internet Things J. 2018, 5, 3758–3773. [Google Scholar]
    [CrossRef] Pan, J.; McElhannon, J. Future Edge Cloud and Edge Computing for Internet
    of Things Applications. IEEE Internet Things J. 2018, 5, 439–449. [Google Scholar]
    [CrossRef] Alharbi, H.A.; Aldossary, M. Energy-Efficient Edge-Fog-Cloud Architecture
    for IoT-Based Smart Agriculture Environment. IEEE Access 2021, 9, 110480–110492.
    [Google Scholar] [CrossRef] Singh, R.; Gill, S.S. Edge AI: A Survey. Internet
    Things Cyber-Phys. Syst. 2023, 3, 71–92. [Google Scholar] [CrossRef] Islam, M.A.;
    Islam, M.A.; Miah, M.S.U.; Bhowmik, A. An Automated Monitoring and Environmental
    Control System for Laboratory-Scale Cultivation of Oyster Mushrooms Using the
    Internet of Agricultural Thing (IoAT). In Proceedings of the 2nd International
    Conference on Computing Advancements, Dhaka, Bangladesh, 10–12 March 2022; pp.
    207–212. [Google Scholar] [CrossRef] Das, R.; Inuwa, M.M. A Review on Fog Computing:
    Issues, Characteristics, Challenges, and Potential Applications. Telemat. Inform.
    Rep. 2023, 10, 100049. [Google Scholar] [CrossRef] Shi, W.; Cao, J.; Zhang, Q.;
    Li, Y.; Xu, L. Edge Computing: Vision and Challenges. IEEE Internet Things J.
    2016, 3, 637–646. [Google Scholar] [CrossRef] Zhang, X.; Cao, Z.; Dong, W. Overview
    of Edge Computing in the Agricultural Internet of Things: Key Technologies, Applications,
    Challenges. IEEE Access 2020, 8, 141748–141761. [Google Scholar] [CrossRef] Lin,
    Y.B.; Chen, W.E.; Chang, T.C.Y. Moving from Cloud to Fog/Edge: The Smart Agriculture
    Experience. IEEE Commun. Mag. 2023, 61, 86–92. [Google Scholar] [CrossRef] da
    Costa Bezerra, S.F.; Filho, A.S.M.; Delicato, F.C.; da Rocha, A.R. Article Processing
    Complex Events in Fog-Based Internet of Things Systems for Smart Agriculture.
    Sensors 2021, 21, 7226. [Google Scholar] [CrossRef] Mansouri, Y.; Babar, M.A.
    A Review of Edge Computing: Features and Resource Virtualization. J. Parallel
    Distrib. Comput. 2021, 150, 155–183. [Google Scholar] [CrossRef] Köksal; Tekinerdogan,
    B. Architecture Design Approach for IoT-Based Farm Management Information Systems.
    Precis. Agric. 2019, 20, 926–958. [Google Scholar] [CrossRef] Murakami, E.; Saraiva,
    A.M.; Ribeiro, L.C.M.; Cugnasca, C.E.; Hirakawa, A.R.; Correa, P.L.P. An Infrastructure
    for the Development of Distributed Service-Oriented Information Systems for Precision
    Agriculture. Comput. Electron. Agric. 2007, 58, 37–48. [Google Scholar] [CrossRef]
    Strobel, G. Farming in the Era of Internet of Things: An Information System Architecture
    for Smart Farming. WI2020 Community Tracks 2020, 208–223. [Google Scholar] [CrossRef]
    Fountas, S.; Carli, G.; Sørensen, C.G.; Tsiropoulos, Z.; Cavalaris, C.; Vatsanidou,
    A.; Liakos, B.; Canavari, M.; Wiebensohn, J.; Tisserye, B. Farm Management Information
    Systems: Current Situation and Future Perspectives. Comput. Electron. Agric. 2015,
    115, 40–50. [Google Scholar] [CrossRef] Sørensen, C.G.; Fountas, S.; Nash, E.;
    Pesonen, L.; Bochtis, D.; Pedersen, S.M.; Basso, B.; Blackmore, S.B. Conceptual
    Model of a Future Farm Management Information System. Comput. Electron. Agric.
    2010, 72, 37–47. [Google Scholar] [CrossRef] Villa-Henriksen, A.; Edwards, G.T.C.;
    Pesonen, L.A.; Green, O.; Sørensen, C.A.G. Internet of Things in Arable Farming:
    Implementation, Applications, Challenges and Potential. Biosyst. Eng. 2020, 191,
    60–84. [Google Scholar] [CrossRef] Winter, R.; Fischer, R. Essential Layers, Artifacts,
    and Dependencies of Enterprise Architecture. In Proceedings of the 2006 10th IEEE
    International Enterprise Distributed Object Computing Conference Workshops (EDOCW’06),
    Hong Kong, China, 16–20 October 2006; pp. 30–38. [Google Scholar] [CrossRef] Li,
    X.H.; Cheng, X.; Yan, K.; Gong, P. A Monitoring System for Vegetable Greenhouses
    Based on a Wireless Sensor Network. Sensors 2010, 10, 8963–8980. [Google Scholar]
    [CrossRef] [PubMed] Moysiadis, V.; Karaiskou, C.; Kokkonis, G.; Moscholios, I.D.;
    Sarigiannidis, P. A System Architecture for Smart Farming on Mushroom Cultivation.
    In Proceedings of the 2022 5th World Symposium on Communication Engineering (WSCE),
    Nagoya, Japan, 16–18 September 2022; pp. 89–94. [Google Scholar] [CrossRef] Ferrández-Pastor,
    F.J.; García-Chamizo, J.M.; Nieto-Hidalgo, M.; Mora-Pascual, J.; Mora-Martínez,
    J. Developing Ubiquitous Sensor Network Platform Using Internet of Things: Application
    in Precision Agriculture. Sensors 2016, 16, 1141. [Google Scholar] [CrossRef]
    [PubMed] Carpio, F.; Jukan, A.; Sanchez, A.I.M.; Amla, N.; Kemper, N. Beyond Production
    Indicators: A Novel Smart Farming Application and System for Animal Welfare. In
    Proceedings of the Fourth International Conference on Animal-Computer Interaction,
    Milton Keynes, UK, 21–23 November 2017; pp. 1–11. [Google Scholar] [CrossRef]
    Verma, S.; Gala, R.; Madhavan, S.; Burkule, S.; Chauhan, S.; Prakash, C. An Internet
    of Things (IoT) Architecture for Smart Agriculture. In Proceedings of the 2018
    Fourth International Conference on Computing Communication Control and Automation,
    Pune, India, 16–18 August 2018; pp. 1–4. [Google Scholar] [CrossRef] Zamora-Izquierdo,
    M.A.; Santa, J.; Martínez, J.A.; Martínez, V.; Skarmeta, A.F. Smart Farming IoT
    Platform Based on Edge and Cloud Computing. Biosyst. Eng. 2019, 177, 4–17. [Google
    Scholar] [CrossRef] Ray, P.P. Internet of Things for Smart Agriculture: Technologies,
    Practices and Future Direction. J. Ambient Intell. Smart Environ. 2017, 9, 395–420.
    [Google Scholar] [CrossRef] Khan, F.A.; Abubakar, A.; Mahmoud, M.; Al-Khasawneh,
    M.A.; Alarood, A.A. Cotton Crop Cultivation Oriented Semantic Framework Based
    on IoT Smart Farming Application. Int. J. Eng. Adv. Technol. 2019, 8, 480–484.
    [Google Scholar] Ramli, M.I.; Ariffin, M.A.M.; Zainol, Z.; Amin, M.N.M.; Hirawan,
    D.; Sumitra, I.D.; Jamil, N. Design of a Smart Portable Farming Kit for Indoor
    Cultivation Using the Raspberry Pi Platform. Pertanika J. Sci. Technol. 2023,
    31, 1731–1754. [Google Scholar] [CrossRef] Chen, L.; Qian, L.; Zhang, X.; Li,
    J.; Zhang, Z.; Chen, X. Research Progress on Indoor Environment of Mushroom Factory.
    Int. J. Agric. Biol. Eng. 2022, 15, 25–32. [Google Scholar] [CrossRef] Suresh,
    M.; Srinivasan, M.; Gowri Shankar, S.; Karthikeyan, D.; Nakhul, V.; Naveen Kumar,
    A.; Sundar, S.; Maniraj, P. Monitoring and Automatic Control of Various Parameters
    for Mushroom Farming. IOP Conf. Ser. Mater. Sci. Eng. 2021, 1055, 012011. [Google
    Scholar] [CrossRef] Yin, H.; Yi, W.; Hu, D. Computer Vision and Machine Learning
    Applied in the Mushroom Industry: A Critical Review. Comput. Electron. Agric.
    2022, 198, 107015. [Google Scholar] [CrossRef] Kassim, M.R.M.; Harun, A.N.; Yusoff,
    I.M.; Mat, I.; Kuen, C.P.; Rahmad, N. Applications of Wireless Sensor Networks
    in Shiitake Mushroom Cultivation. In Proceedings of the 2017 Eleventh International
    Conference on Sensing Technology, Sydney, NSW, Australia, 4–6 December 2017; pp.
    1–6. [Google Scholar] [CrossRef] Mohammed, M.F.; Azmi, A.; Zakaria, Z.; Tajuddin,
    M.F.N.; Isa, Z.M.; Azmi, S.A. IoT Based Monitoring and Environment Control System
    for Indoor Cultivation of Oyster Mushroom. J. Phys. Conf. Ser. 2018, 1019, 012053.
    [Google Scholar] [CrossRef] Dipali, D.; Subramanian, T.; Kumaran, G.S. A Smart
    Oyster Mushroom Cultivation Using Automatic Fuzzy Logic Controller. J. Discret.
    Math. Sci. Cryptogr. 2023, 26, 601–615. [Google Scholar] [CrossRef] Dong, J.;
    Zheng, L. Quality Classification of Enoki Mushroom Caps Based on CNN. In Proceedings
    of the 2019 IEEE 4th International Conference on Image, Vision and Computing,
    Xiamen, China, 5–7 July 2019; pp. 450–454. [Google Scholar] [CrossRef] Lu, C.P.;
    Liaw, J.J. A Novel Image Measurement Algorithm for Common Mushroom Caps Based
    on Convolutional Neural Network. Comput. Electron. Agric. 2020, 171, 105336. [Google
    Scholar] [CrossRef] Wei, B.; Zhang, Y.; Pu, Y.; Sun, Y.; Zhang, S.; Lin, H.; Zeng,
    C.; Zhao, Y.; Wang, K.; Chen, Z. Recursive-YOLOv5 Network for Edible Mushroom
    Detection in Scenes with Vertical Stick Placement. IEEE Access 2022, 10, 40093–40108.
    [Google Scholar] [CrossRef] Moysiadis, V.; Kokkonis, G.; Bibi, S.; Moscholios,
    I.; Maropoulos, N.; Sarigiannidis, P. Monitoring Mushroom Growth with Machine
    Learning. Agriculture 2023, 13, 223. [Google Scholar] [CrossRef] Aguirre, L.;
    Frias, J.M.; Barry-Ryan, C.; Grogan, H. Modelling Browning and Brown Spotting
    of Mushrooms (Agaricus bisporus) Stored in Controlled Environmental Conditions
    Using Image Analysis. J. Food Eng. 2009, 91, 280–286. [Google Scholar] [CrossRef]
    Barauskas, R.; Kriščiūnas, A.; Čalnerytė, D.; Pilipavičius, P.; Fyleris, T.; Daniulaitis,
    V.; Mikalauskis, R. Approach of AI-Based Automatic Climate Control in White Button
    Mushroom Growing Hall. Agriculture 2022, 12, 1921. [Google Scholar] [CrossRef]
    Rahman, H.; Faruq, M.O.; Abdul Hai, T.B.; Rahman, W.; Hossain, M.M.; Hasan, M.;
    Islam, S.; Moinuddin, M.; Islam, M.T.; Azad, M.M. IoT Enabled Mushroom Farm Automation
    with Machine Learning to Classify Toxic Mushrooms in Bangladesh. J. Agric. Food
    Res. 2022, 7, 100267. [Google Scholar] [CrossRef] Chong, J.L.; Chew, K.W.; Peter,
    A.P.; Ting, H.Y.; Show, P.L. Internet of Things (IoT)-Based Environmental Monitoring
    and Control System for Home-Based Mushroom Cultivation. Biosensors 2023, 13, 98.
    [Google Scholar] [CrossRef] [PubMed] Goldstein, M.; Uchida, S. A Comparative Evaluation
    of Unsupervised Anomaly Detection Algorithms for Multivariate Data. PLoS ONE 2016,
    11, e0152173. [Google Scholar] [CrossRef] Redmon, J.; Divvala, S.; Girshick, R.;
    Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas,
    NV, USA, 27–30 June 2016; pp. 779–788. [Google Scholar] [CrossRef]           Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2024 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Nguyen, H.H.; Shin, D.-Y.; Jung, W.-S.; Kim,
    T.-Y.; Lee, D.-H. An Integrated IoT Sensor-Camera System toward Leveraging Edge
    Computing for Smart Greenhouse Mushroom Cultivation. Agriculture 2024, 14, 489.
    https://doi.org/10.3390/agriculture14030489 AMA Style Nguyen HH, Shin D-Y, Jung
    W-S, Kim T-Y, Lee D-H. An Integrated IoT Sensor-Camera System toward Leveraging
    Edge Computing for Smart Greenhouse Mushroom Cultivation. Agriculture. 2024; 14(3):489.
    https://doi.org/10.3390/agriculture14030489 Chicago/Turabian Style Nguyen, Hoang
    Hai, Dae-Yun Shin, Woo-Sung Jung, Tae-Yeol Kim, and Dae-Hyun Lee. 2024. \"An Integrated
    IoT Sensor-Camera System toward Leveraging Edge Computing for Smart Greenhouse
    Mushroom Cultivation\" Agriculture 14, no. 3: 489. https://doi.org/10.3390/agriculture14030489
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 18. Mar 20. Mar 22. Mar 24.
    Mar 26. Mar 28. Mar 30. Mar 1. Apr 3. Apr 5. Apr 0 200 400 600 800 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Agriculture, EISSN 2077-0472, Published by MDPI RSS
    Content Alert Further Information Article Processing Charges Pay an Invoice Open
    Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For
    Editors For Librarians For Publishers For Societies For Conference Organizers
    MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia
    JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive
    issue release notifications and newsletters from MDPI journals Select options
    Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer
    Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Agriculture (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An Integrated IoT Sensor-Camera System toward Leveraging Edge Computing for
    Smart Greenhouse Mushroom Cultivation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Javadpour A.
  - Sangaiah A.K.
  - Zhang W.
  - Vidyarthi A.
  - Ahmadi H.R.
  citation_count: '0'
  description: This study presents an environmentally friendly mechanism for task
    distribution designed explicitly for blockchain Proof of Authority (POA) consensus.
    This approach facilitates the selection of virtual machines for tasks such as
    data processing, transaction verification, and adding new blocks to the blockchain.
    Given the current lack of effective methods for integrating POA blockchain into
    the Cloud Industrial Internet of Things (CIIoT) due to their inefficiency and
    low throughput, we propose a novel algorithm that employs the Dynamic Voltage
    and Frequency Scaling (DVFS) technique, replacing the periodic transaction authentication
    process among validator candidates. Managing computer power consumption becomes
    a critical concern, especially within the Internet of Things ecosystem, where
    device power is constrained, and transaction scalability is crucial. Virtual machines
    must validate transactions (tasks) within specific time frames and deadlines.
    The DVFS technique efficiently reduces power consumption by intelligently scheduling
    and allocating tasks to virtual machines. Furthermore, we leverage artificial
    intelligence and neural networks to match tasks with suitable virtual machines.
    The simulation results demonstrate that our proposed approach harnesses migration
    and DVFS strategies to optimize virtual machine utilization, resulting in decreased
    energy and power consumption compared to non-DVFS methods. This achievement marks
    a significant stride towards seamlessly integrating blockchain and IoT, establishing
    an ecologically sustainable network. Our approach boasts additional benefits,
    including decentralization, enhanced data quality, and heightened security. We
    analyze simulation runtime and energy consumption in a comprehensive evaluation
    against existing techniques such as WPEG, IRMBBC, and BEMEC. The findings underscore
    the efficiency of our technique (LBDVFSb) across both criteria.
  doi: 10.1007/s10723-024-09751-9
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Grid Computing Article
    Decentralized AI-Based Task Distribution on Blockchain for Cloud Industrial Internet
    of Things Research Published: 24 February 2024 Volume 22, article number 33, (2024)
    Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Journal of Grid Computing Aims and scope Submit manuscript Amir Javadpour, Arun
    Kumar Sangaiah, Weizhe Zhang, Ankit Vidyarthi & HamidReza Ahmadi  83 Accesses
    Explore all metrics Abstract This study presents an environmentally friendly mechanism
    for task distribution designed explicitly for blockchain Proof of Authority (POA)
    consensus. This approach facilitates the selection of virtual machines for tasks
    such as data processing, transaction verification, and adding new blocks to the
    blockchain. Given the current lack of effective methods for integrating POA blockchain
    into the Cloud Industrial Internet of Things (CIIoT) due to their inefficiency
    and low throughput, we propose a novel algorithm that employs the Dynamic Voltage
    and Frequency Scaling (DVFS) technique, replacing the periodic transaction authentication
    process among validator candidates. Managing computer power consumption becomes
    a critical concern, especially within the Internet of Things ecosystem, where
    device power is constrained, and transaction scalability is crucial. Virtual machines
    must validate transactions (tasks) within specific time frames and deadlines.
    The DVFS technique efficiently reduces power consumption by intelligently scheduling
    and allocating tasks to virtual machines. Furthermore, we leverage artificial
    intelligence and neural networks to match tasks with suitable virtual machines.
    The simulation results demonstrate that our proposed approach harnesses migration
    and DVFS strategies to optimize virtual machine utilization, resulting in decreased
    energy and power consumption compared to non-DVFS methods. This achievement marks
    a significant stride towards seamlessly integrating blockchain and IoT, establishing
    an ecologically sustainable network. Our approach boasts additional benefits,
    including decentralization, enhanced data quality, and heightened security. We
    analyze simulation runtime and energy consumption in a comprehensive evaluation
    against existing techniques such as WPEG, IRMBBC, and BEMEC. The findings underscore
    the efficiency of our technique (LBDVFSb) across both criteria. Article PDF Similar
    content being viewed by others Information technologies of 21st century and their
    impact on the society Article 16 August 2019 Blockchain for decentralization of
    internet: prospects, trends, and challenges Article Open access 15 May 2021 The
    Future of E-Commerce Systems: 2030 and Beyond Chapter © 2021 Data Availability
    Data available on request from the authors. The data that support the findings
    of this study are available from the corresponding author, [author initials],
    upon reasonable request. References Toor, A., et al.: Energy and performance aware
    fog computing: A case of DVFS and green renewable energy. Futur. Gener. Comput.
    Syst. 101, 1112–1121 (2019) Article   Google Scholar   Tang, Z., Qi, L., Cheng,
    Z., Li, K., Khan, S.U., Li, K.: An energy-efficient task scheduling algorithm
    in DVFS-enabled cloud environment. J. Grid Comput. 14, 55–74 (2016) Article   Google
    Scholar   Javadpour, A., Nafei, A., Ja’fari, F., Pinto, P., Zhang, W., Sangaiah,
    A.K.: An intelligent energy-efficient approach for managing IoE tasks in cloud
    platforms. J. Ambient Intell. Humaniz. Comput. 14(4), 3963–3979 (2023) Article   Google
    Scholar   Hosseini Shirvani, M., Rahmani, A.M., Sahafi, A.: A survey study on
    virtual machine migration and server consolidation techniques in DVFS-enabled
    cloud datacenter: Taxonomy and challenges. J. King Saud Univ. - Comput. Inf. Sci.
    32(3), 267–286 (2020) Google Scholar   Jiang, C., et al.: Energy aware edge computing:
    A survey. Comput. Commun. 151, 556–580 (2020) Article   Google Scholar   Xu, C.,
    Wang, K., Guo, M.: Intelligent resource management in blockchain-based cloud datacenters.
    IEEE Cloud Comput. 4(6), 50–59 (2017) Article   Google Scholar   Pirozmand, P.,
    Javadpour, A., Nazarian, H., Pinto, P., Mirkamali, S., Ja’fari, F.: GSAGA: A hybrid
    algorithm for task scheduling in cloud infrastructure. J. Supercomput. 78(15),
    17423–17449 (2022) Article   Google Scholar   Yousuf, R., Jeelani, Z., Khan, D.A.,
    Bhat, O., Teli, T.A.: Consensus Algorithms in Blockchain-Based Cryptocurrencies,
    in 2021 International Conference on Advances in Electrical, Computing, Communication
    and Sustainable Technologies (ICAECT). 1–6. (2021) Wu, Y., Song, P., Wang, F.:
    Hybrid consensus algorithm optimization: A mathematical method based on POS and
    PBFT and its application in blockchain. Math. Probl. Eng. 2020, (2020) Lone, A.H.,
    Mir, R.N.: Reputation Driven Dynamic Access Control Framework for IoT atop PoA
    Ethereum Blockchain. IACR Cryptol. ePrint Arch. 2020, 566 (2020) Google Scholar   Javadpour,
    A., Wang, G., Rezaei, S., Chend, S.: Power Curtailment in Cloud Environment Utilising
    Load Balancing Machine Allocation, in 2018 IEEE SmartWorld, Ubiquitous Intelligence
    Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud
    Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI).
    1364–1370 (2018) Xu, R., Chen, Y., Blasch, E.: Decentralized Access Control for
    IoT Based on Blockchain and Smart Contract. Model. Des. Secur. Internet Things.
    505–528 (2020) Oktian, Y.E., Lee, S.-G., Lee, H.J.: Hierarchical multi-blockchain
    architecture for scalable internet of things environment. Electronics 9(6), 1050
    (2020) Article   Google Scholar   Han, D., Zhang, C., Ping, J., Yan, Z.: Smart
    contract architecture for decentralized energy trading and management based on
    blockchains. Energy 199, 117417 (2020) Article   Google Scholar   Atlam, H.F.,
    Azad, M.A., Alzahrani, A.G., Wills, G.: A Review of Blockchain in Internet of
    Things and AI. Big Data Cogn. Comput. 4(4), 28 (2020) Article   Google Scholar   Panarello,
    A., Tapas, N., Merlino, G., Longo, F., Puliafito, A.: Blockchain and IoT Integration:
    A Systematic Survey. Sensors (Basel) 18(8), 2575 (2018) Article   ADS   PubMed   Google
    Scholar   Yazdinejad, A., Parizi, R.M., Dehghantanha, A., Zhang, Q., Choo, K.-K.R.:
    An Energy-Efficient SDN Controller Architecture for IoT Networks With Blockchain-Based
    Security. IEEE Trans. Serv. Comput. 13(4), 625–638 (2020) Article   Google Scholar   Yazdinejad,
    A., Parizi, R.M., Srivastava, G., Dehghantanha, A., Choo, K.-K.R.: Energy Efficient
    Decentralized Authentication in Internet of Underwater Things Using Blockchain,
    in 2019 IEEE Globecom Workshops (GC Wkshps). 1–6 (2019) Javadpour, A., AliPour,
    F.S., Sangaiah, A.K., Zhang, W., Ja’far, F., Singh, A.: An IoE blockchain-based
    network knowledge management model for resilient disaster frameworks. J. Innov.
    Knowl. 8(3), (2023) Yu, R., Zhang, X., Zhang, M.: Smart Home Security Analysis
    System Based on The Internet of Things, in 2021 IEEE 2nd International Conference
    on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE).
    596–599 (2021) Karunarathne, S.M., Saxena, N., Khan, M.K.: Security and Privacy
    in IoT Smart Healthcare. IEEE Internet Comput. (2021) Lin, X., Wu, J., Bashir,
    A.K., Li, J., Yang, W., Piran, J.: Blockchain-Based Incentive Energy-Knowledge
    Trading in IoT: Joint Power Transfer and AI Design. IEEE Internet Things J. 4662(c),
    1–1 (2020) Google Scholar   Barzegar, B., Motameni, H., Movaghar, A.: EATSDCD:
    A green energy-aware scheduling algorithm for parallel task-based application
    using clustering, duplication and DVFS technique in cloud datacenters. J. Intell.
    \\& Fuzzy Syst. 36(6), 5135–5152 (2019) Article   Google Scholar   Yazdinejad,
    A., Parizi, R.M., Dehghantanha, A., Choo, K.-K.R.: Blockchain-Enabled Authentication
    Handover With Efficient Privacy Protection in SDN-Based 5G Networks. IEEE Trans.
    Netw. Sci. Eng. 8(2), 1120–1132 (2021) Article   Google Scholar   Download references
    Funding This work was supported in part by the Joint Funds of the National Natural
    Science Foundation of China (Grant No. U22A2036), the Shenzhen Colleges and Universities
    Stable Support Program No.GXWD20220817124251002, Guangdong Provincial Key Laboratory
    of Novel Security Intelligence Technologies (2022B1212010005). Author information
    Authors and Affiliations School of Computer Science and Technology, Harbin Institute
    of Technology, Shenzhen, 518055, Guangdong, China Amir Javadpour & Weizhe Zhang
    International Graduate Institute of Artificial Intelligence, National Yunlin University
    of Science and Technology, Douliu, Taiwan Arun Kumar Sangaiah Jaypee Institute
    of Information Technology, Noida, India Ankit Vidyarthi Faculty of New Sciences
    and Technologies, University of Tehran, Tehran, Iran HamidReza Ahmadi Department
    of New Networks, Peng Cheng Laboratory, Shenzhen, 518055, Guangdong, China Weizhe
    Zhang Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies,
    Shenzhen, 518055, Guangdong, China Weizhe Zhang Contributions A.B. and C.D. wrote
    the main manuscript text and all of them prepared figures and etc. All authors
    reviewed the manuscript. Corresponding authors Correspondence to Amir Javadpour
    or Weizhe Zhang. Ethics declarations Ethical Approval Hereby, I am Amir Javadpour
    consciously assure that for the manuscript “Decentralized AI‑Based Task Distribution
    on Blockchain for Cloud Industrial Internet of Things” the following is fulfilled:
    1) This material is the authors'' own original work, which has not been previously
    published elsewhere. 2) The paper is not currently being considered for publication
    elsewhere. 3) The paper reflects the authors'' own research and analysis in a
    truthful and complete manner. 4) The paper properly credits the meaningful contributions
    of co-authors and co-researchers. 5) The results are appropriately placed in the
    context of prior and existing research. 6) All sources used are properly disclosed
    (correct citation). Literally copying of text must be indicated as such by using
    quotation marks and giving proper reference. 7) All authors have been personally
    and actively involved in substantial work leading to the paper, and will take
    public responsibility for its content. I agree with the above statements and declare
    that this submission follows the policies as outlined in the Guide for Authors
    and in the Ethical Statement. Date: 01–06-2023. Corresponding author’s signature:
    Amir Javadpour. Competing Interests The authors declare no competing interests.
    Additional information Publisher''s Note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    Appendix 1 Appendix 1 Details of Algorithm 1, Including Elaboration of Each Step.
    Algorithm 2: IoT Task Processing. Rights and permissions Springer Nature or its
    licensor (e.g. a society or other partner) holds exclusive rights to this article
    under a publishing agreement with the author(s) or other rightsholder(s); author
    self-archiving of the accepted manuscript version of this article is solely governed
    by the terms of such publishing agreement and applicable law. Reprints and permissions
    About this article Cite this article Javadpour, A., Sangaiah, A.K., Zhang, W.
    et al. Decentralized AI-Based Task Distribution on Blockchain for Cloud Industrial
    Internet of Things. J Grid Computing 22, 33 (2024). https://doi.org/10.1007/s10723-024-09751-9
    Download citation Received 16 June 2023 Accepted 23 January 2024 Published 24
    February 2024 DOI https://doi.org/10.1007/s10723-024-09751-9 Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Blockchain Improving resources Internet of Things Decentralized DVFS
    Industrial Internet of Things Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Associated Content Part of a collection: Artificial Intelligence
    (AI)-enabled Blockchain for the Edge of Things (EoT) Sections References Abstract
    Article PDF Data Availability References Funding Author information Ethics declarations
    Additional information Appendix 1 Rights and permissions About this article Advertisement
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Grid Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Decentralized AI-Based Task Distribution on Blockchain for Cloud Industrial
    Internet of Things
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Dahdouh Y.
  - Anouar A.B.
  - Ahmed M.B.
  citation_count: '0'
  description: 'Melanoma is a kind of skin cancer that originates in melanocytes responsible
    for producing melanin, it can be a severe and potentially deadly form of cancer
    because it can metastasize to other regions of the body if not detected and treated
    early. To facilitate this process, Recently, various computer-assisted low-cost,
    reliable, and accurate diagnostic systems have been proposed based on artificial
    intelligence (AI) algorithms, particularly deep learning techniques. This work
    proposed an innovative and intelligent system that combines the internet of things
    (IoT) with a Raspberry Pi connected to a camera and a deep learning model based
    on the deep convolutional neural network (CNN) algorithm for real-time detection
    and classification of melanoma cancer lesions. The key stages of our model before
    serializing to the Raspberry Pi: Firstly, the preprocessing part contains data
    cleaning, data transformation (normalization), and data augmentation to reduce
    overfitting when training. Then, the deep CNN algorithm is used to extract the
    features part. Finally, the classification part with applied Sigmoid Activation
    Function. The experimental results indicate the efficiency of our proposed classification
    system as we achieved an accuracy rate of 92%, a precision of 91%, a sensitivity
    of 91%, and an area under the curve-receiver operating characteristics (AUC-ROC)
    of 0.9133.'
  doi: 10.11591/ijai.v13.i1.pp1104-1111
  full_citation: '>'
  full_text: '>

    "USER Username Password Remember me CITATION ANALYSIS Dimensions Google Scholar
    Scholar Metrics Scimagojr Scopus Web of Science Scilit QUICK LINKS Editorial Boards
    Reviewers Author Guidelines Online Submission Peer Review Process Publication
    Fee Abstracting and Indexing Publication Ethics Visitor Statistics Contact Us
    Registration for IJ-AI''s Professional Reviewers JOURNAL CONTENT Search Search
    Scope      All Authors Title Abstract Index terms Full Text      Browse By Issue
    By Author By Title INFORMATION For Readers For Authors For Librarians HOME ABOUT
    LOGIN REGISTER SEARCH CURRENT ARCHIVES ANNOUNCEMENTS Home > Vol 13, No 1 > Dahdouh
    Embedded artificial intelligence system using deep learning and raspberrypi for
    the detection and classification of melanoma Yousra Dahdouh, Abdelhakim Boudhir
    Anouar, Mohamed Ben Ahmed  Abstract  Melanoma is a kind of skin cancer that originates
    in melanocytes responsible for producing melanin, it can be a severe and potentially
    deadly form of cancer because it can metastasize to other regions of the body
    if not detected and treated early. To facilitate this process, Recently, various
    computer-assisted low-cost, reliable, and accurate diagnostic systems have been
    proposed based on artificial intelligence (AI) algorithms, particularly deep learning
    techniques. This work proposed an innovative and intelligent system that combines
    the internet of things (IoT) with a Raspberry Pi connected to a camera and a deep
    learning model based on the deep convolutional neural network (CNN) algorithm
    for real-time detection and classification of melanoma cancer lesions. The key
    stages of our model before serializing to the Raspberry Pi: Firstly, the preprocessing
    part contains data cleaning, data transformation (normalization), and data augmentation
    to reduce overfitting when training. Then, the deep CNN algorithm is used to extract
    the features part. Finally, the classification part with applied Sigmoid Activation
    Function. The experimental results indicate the efficiency of our proposed classification
    system as we achieved an accuracy rate of 92%, a precision of 91%, a sensitivity
    of 91%, and an area under the curve- receiver operating characteristics (AUC-ROC)
    of 0.9133.  Keywords  Convolutional neural network; Deep learning; Dermoscopy
    images; Internet of things; Melanoma; Raspberry Pi; Skin cancer  Full Text: PDF   DOI:
    http://doi.org/10.11591/ijai.v13.i1.pp1104-1111 Refbacks There are currently no
    refbacks.    This work is licensed under a Creative Commons Attribution-ShareAlike
    4.0 International License. IAES International Journal of Artificial Intelligence
    (IJ-AI) ISSN/e-ISSN 2089-4872/2252-8938  This journal is published by the Institute
    of Advanced Engineering and Science (IAES) in collaboration with Intelektual Pustaka
    Media Utama (IPMU). View IJAI Stats"'
  inline_citation: '>'
  journal: IAES International Journal of Artificial Intelligence
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Embedded artificial intelligence system using deep learning and raspberrypi
    for the detection and classification of melanoma
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chhetri K.B.
  citation_count: '0'
  description: To ensure food safety and uphold high standards, the food business
    must overcome significant obstacles. In recent years, promising answers to these
    issues have emerged in the form of artificial intelligence (AI) and machine learning
    (ML). This thorough review paper analyses the various uses of AI and ML in food
    quality management and safety evaluation, offering insightful information for
    academics, business people and legislators. The evaluation highlights the value
    of food quality assessment and control in consideration of growing consumer demand
    and regulatory scrutiny. The powerful capabilities of AI and ML are touted as
    having the potential to revolutionize these procedures. This study illustrates
    the numerous uses of AI and ML in food quality management through an in-depth
    exploration of these technologies. Defect detection and consistency evaluation
    are made possible using computer vision techniques, and intelligent data analysis
    and real-time monitoring are made possible by natural language processing. Deep
    learning techniques also provide reliable approaches for pattern recognition and
    anomaly detection, thus maintaining consistency in quality across manufacturing
    batches. This review emphasizes the efficiency of AI and ML in detecting dangerous
    microorganisms, allergies and chemical pollutants with regard to food safety evaluation.
    Consumer health risks are reduced because of the rapid identification of safety
    issues made possible by integrating data from diverse sources, including sensors
    and IoT devices. The assessment discusses issues and restrictions related to the
    application of AI and ML in the food business while appreciating the impressive
    progress that has been made. Continuous efforts are being made to improve model
    interpretability and reduce biases, which calls for careful evaluation of data
    quality, quantity and privacy issues. To assure compliance with food safety norms
    and regulations, the article also covers regulatory approval and validation of
    AI-generated outcomes. The revolutionary potential of AI and ML in raising food
    industry standards and preserving public health is highlighted on future perspectives
    that concentrate on new trends and potential innovations. This comprehensive review
    reveals that the integration of AI and ML technologies in food quality control
    and safety not only enhances efficiency, minimizes risks and ensures regulatory
    compliance but also heralds a new era of personalized nutrition, autonomous monitoring
    and global collaboration, signifying a transformative paradigm in the food industry.
  doi: 10.1007/s12393-023-09363-1
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Food Engineering Reviews Article
    Applications of Artificial Intelligence and Machine Learning in Food Quality Control
    and Safety Assessment Published: 22 December 2023 Volume 16, pages 1–21, (2024)
    Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Food Engineering Reviews Aims and scope Submit manuscript Krishna Bahadur Chhetri  564
    Accesses Explore all metrics Abstract To ensure food safety and uphold high standards,
    the food business must overcome significant obstacles. In recent years, promising
    answers to these issues have emerged in the form of artificial intelligence (AI)
    and machine learning (ML). This thorough review paper analyses the various uses
    of AI and ML in food quality management and safety evaluation, offering insightful
    information for academics, business people and legislators. The evaluation highlights
    the value of food quality assessment and control in consideration of growing consumer
    demand and regulatory scrutiny. The powerful capabilities of AI and ML are touted
    as having the potential to revolutionize these procedures. This study illustrates
    the numerous uses of AI and ML in food quality management through an in-depth
    exploration of these technologies. Defect detection and consistency evaluation
    are made possible using computer vision techniques, and intelligent data analysis
    and real-time monitoring are made possible by natural language processing. Deep
    learning techniques also provide reliable approaches for pattern recognition and
    anomaly detection, thus maintaining consistency in quality across manufacturing
    batches. This review emphasizes the efficiency of AI and ML in detecting dangerous
    microorganisms, allergies and chemical pollutants with regard to food safety evaluation.
    Consumer health risks are reduced because of the rapid identification of safety
    issues made possible by integrating data from diverse sources, including sensors
    and IoT devices. The assessment discusses issues and restrictions related to the
    application of AI and ML in the food business while appreciating the impressive
    progress that has been made. Continuous efforts are being made to improve model
    interpretability and reduce biases, which calls for careful evaluation of data
    quality, quantity and privacy issues. To assure compliance with food safety norms
    and regulations, the article also covers regulatory approval and validation of
    AI-generated outcomes. The revolutionary potential of AI and ML in raising food
    industry standards and preserving public health is highlighted on future perspectives
    that concentrate on new trends and potential innovations. This comprehensive review
    reveals that the integration of AI and ML technologies in food quality control
    and safety not only enhances efficiency, minimizes risks and ensures regulatory
    compliance but also heralds a new era of personalized nutrition, autonomous monitoring
    and global collaboration, signifying a transformative paradigm in the food industry.
    Similar content being viewed by others Artificial Intelligence and Deep Learning-Based
    Agri and Food Quality and Safety Detection System Chapter © 2023 Impact and prospect
    of the fourth industrial revolution in food safety: Mini-review Article 20 February
    2022 Food Adulteration Detection using Artificial Intelligence: A Systematic Review
    Article 15 June 2021 Introduction The food industry at a global level is encountering
    an escalation in challenges associated with ensuring food safety, sustainability
    and quality due to the increasing demands of consumers and environmental changes.
    To combat these challenges, industry professionals and researchers are resorting
    to artificial intelligence (AI) and machine learning (ML) techniques as effective
    instruments to transform food safety assessment and quality control. In this paper,
    an assessment is conducted to explore the varied applications of AI and ML in
    the food industry, with a particular emphasis on their potential to mitigate crucial
    challenges and enhance the overall efficiency. The requirement for advanced technologies
    in food safety assessment and quality control is apparent, and AI and ML provide
    encouraging solutions. The conflation of hyperspectral imaging and AI methodologies
    in a symbiotic manner, as illustrated by the innovative research undertaken by
    Rady et al. [1], carried significant consequences for the food processing sector.
    This union facilitates the swift and accurate detection of pest infestations in
    apples, which fortifies pre-emptive measures and decreases crop losses. Furthermore,
    the AI-powered analysis extends to quality management, permitting the real-time
    evaluation of factors such as ripeness and pollutants. By refining procedures
    and ensuring the authenticity of products, this combination redefines the benchmarks
    for food safety and refines the general efficacy of the industry. The food processing
    industry is on the cusp of a transformative period, owing to the profound integration
    of AI and ML techniques. These advancements have significantly altered the crop
    disease and pest detection landscape with remarkable precision, as evidenced by
    the pioneering research of Boyd and Sun [2]. Their groundbreaking expert system,
    which diagnoses potato ailments, is a pioneering achievement that demonstrates
    the expeditious and precise assessments that are now possible, ushering in a new
    era of rapid interventions and reduced agricultural losses. However, the realm
    of AI and ML surpasses the confines of immediate quality control. Instead, it
    assumes a mantle of paramount significance in navigating the intricate and multifaceted
    terrain of food security. The astute application of fuzzy systems, as demonstrated
    by Peixoto et al. [3], offers a glimpse into this potential. Their ingenious dynamic
    regulation of soybean aphids exemplifies the strategic use of AI, resulting not
    only in augmented crop yields but also in fortifying the very foundation of the
    global food supply chain. Inextricably linked to the evolving tapestry of the
    food industry, the ascendancy of AI and ML technologies in food quality control
    and safety assessment is inexorable. As we traverse the dynamic contours of this
    landscape, it becomes increasingly evident that this review serves not merely
    as an exposition but also as a clarion call, illuminating the path to an augmented
    future where AI and ML stand as sentinels of excellence within the realms of food
    processing. Materials and Methods The synthesis of this all-encompassing review
    is supported by a rigorous methodology aimed at extracting valuable insights from
    the most authoritative sources currently available. A meticulous and systematic
    search strategy was executed, utilizing well-known databases such as PubMed, IEEE
    Xplore and ScienceDirect. This review covers literature published until 2022,
    ensuring a comprehensive and up-to-date coverage of the subject. In accordance
    with strict inclusion criteria, the selection process gave priority to articles
    that have undergone peer review and notable conference proceedings. A thorough
    three-step screening process, involving the evaluation of titles, abstracts and
    full texts, was implemented to ensure the accuracy and relevance of the studies
    included. The categorization framework focused on the principal applications of
    AI and ML in the context of food safety, specifically computer vision, IoT-enabled
    sensors, blockchain integration and predictive modelling. A systematic analysis
    of the benefits and limitations associated with each application supports a refined
    distillation of insights, contributing to a comprehensive understanding of the
    subject matter. Food Quality Control and Safety Assessment Through AI and ML Techniques
    AI and ML techniques have revolutionized the field of food quality control by
    presenting inventive solutions to increase the consistency and safety of food
    items [4, 5]. Among the key techniques used in the industry are the following:
    1. Computer vision: In contemporary food process engineering, the combination
    of computer vision technology and AI has emerged as a pioneering development [5].
    This combination is transforming the food industry by enabling meticulous and
    automated visual inspections. AI-powered computer vision systems excel at scrutinizing
    food products with precision and speed through image and video analysis. These
    systems are indispensable for crucial tasks such as grading, sorting and quality
    assessment. By rapidly identifying discrepancies from quality standards, they
    ensure that only products satisfying stringent criteria are delivered to consumers.
    The impact of this technology extends across diverse sectors within food processing.
    In sorting, computer vision systems equipped with AI discern subtle variations
    in colour, size and shape. This enables precise product categorization, which
    is particularly essential in fruit and vegetable sorting, where uniformity is
    vital for quality and competitiveness. Furthermore, the implementation of computer
    vision addresses food safety concerns by promptly identifying potential contaminants
    or pathogens. Rapid data analysis allows timely interventions, minimizing risks
    to food safety and potential recalls. In quality assessment, these systems provide
    consistent, objective evaluations of attributes that influence consumer preferences.
    By reducing human subjectivity, they enhance the standardized product evaluation,
    thus fostering consumer trust and satisfaction. The combination of AI and computer
    vision not only accelerates processes but also optimizes resource usage. Early
    identification of defects minimizes waste, leading to cost savings and heightened
    sustainability. 2. Spectroscopy and sensors: The domain of food process engineering
    is marked by the combination of AI and ML, which is particularly evident in the
    realm of spectroscopy and sensors. According to Si et al. [5], ML algorithms intricately
    analyse data sourced from spectroscopic techniques and a diverse array of sensors,
    measuring crucial attributes such as moisture content, pH levels and chemical
    composition. This data-driven analysis empowers AI to make estimations regarding
    the nutritional value, ripeness and freshness of food items, facilitating informed
    decision-making for producers concerning production and storage strategies. The
    proficiency of AIs in interpreting spectroscopic and sensor data has profound
    implications for quality control and safety assessment. By harnessing AI’s pattern
    recognition capabilities, the technology enables producers to optimize production
    processes and minimize resource wastage. In addition, real-time data streams from
    sensors align with contemporary paradigms such as Industry 4.0 and the Internet
    of things (IoT), facilitating predictive modelling for quality deviations and
    enabling timely intervention. This symbiotic blend of AI and ML in spectroscopy
    and sensors not only enhances operational efficiency but also strengthens the
    foundation of food safety and quality within the dynamic landscape of food process
    engineering. 3. Predictive modelling: Predictive modelling with convergence of
    AI and ML has significant implications for food quality and control. Si et al.
    [5] have expounded on this technique, which utilizes historical data to train
    ML models and predict potential quality issues while estimating the shelf-life
    of food products. The combination of AI and ML facilitates the monitoring of a
    comprehensive range of variables throughout the production and storage phases,
    allowing the system to anticipate the likelihood of contamination, spoilage or
    other forms of deterioration. By identifying patterns and correlations in the
    data, AI-driven predictive models empower producers to make informed decisions
    expeditiously. Subsequently, timely interventions can be implemented to preserve
    the integrity of the final product, minimize wastage and ensure that consumers
    receive safe and high-quality food items. In essence, the application of predictive
    modelling fortified by AI holds significant promise for revolutionizing food process
    engineering. By leveraging historical insights and real-time data, this approach
    enhances the industry’s ability to mitigate risks, optimize resources and ultimately
    deliver products that meet the highest standards of quality and safety. 4. IoT-enabled
    real-time monitoring: The integration of AI and ML has been emphasized by Si et
    al. [5], thus leading to the emergence of IoT devices equipped with sensors. This
    union has enabled real-time monitoring of critical parameters, thereby facilitating
    proactive quality control. With the aid of these IoT devices, data acquisition
    is performed in real time, ensuring that quality benchmarks are constantly upheld.
    In the event of any deviations, AI systems expeditiously analyse the data and
    deliver prompt notifications, thereby enabling immediate corrective measures.
    This dynamic approach not only hastens response times but also mitigates the risk
    of errors or contamination. Furthermore, AI-powered automation and robotics play
    a crucial role in expediting quality control procedures. By eliminating human
    intervention, these technologies enhance precision and minimize the potential
    for errors. The intricate handling and examination of food products are facilitated,
    ensuring uniform quality and reducing needless wastage. In essence, the convergence
    of AI, ML and IoT within food process engineering amplifies the industry’s ability
    to maintain superior quality standards. The real-time monitoring and automated
    responsiveness empower stakeholders to ensure that only products of the highest
    quality are presented to consumers, subsequently elevating trust, satisfaction
    and overall efficiency. 5. Data-driven decision-making: In the realm of food process
    engineering, the confluence between AI and ML has been underscored by Si et al.
    [5] and Vijay et al. [6], utilizing expansive datasets for data-oriented decision-making.
    This approach combines an array of data sources, including customer feedback,
    laboratory testing results and manufacturing records. AI’s capacity to process
    and scrutinize these vast datasets empowers upgraded quality control systems.
    This integration facilitates the identification of intricate trends and issues
    that may elude conventional methods. By discerning patterns and anomalies across
    multiple dimensions, AI enhances the accuracy of quality assessment and contributes
    to the proactive identification of potential challenges. The integration of AI
    and ML in food process engineering exemplifies a transformative shift towards
    data-driven decision-making. This not only enriches the comprehension of product
    quality but also fosters continuous improvement and innovation within the industry.
    6. Blockchain and AI-enabled improved traceability and transparency in the food
    supply chain: The intersection of blockchain and AI has resulted in a significant
    breakthrough in augmenting traceability and transparency throughout the food supply
    chain. The influential works of Si et al. [5] and Bestelmeyer et al. [7] underscored
    the pivotal role of this decentralized ledger in meticulously monitoring the entire
    journey of food products, spanning from their origin to consumption. By leveraging
    AI-powered analysis of blockchain data, a swift and accurate identification of
    sources is made feasible in cases of contamination or recalls. This integration
    fortifies the efficacy of traceability systems, safeguards consumer health and
    increases the efficiency of corrective measures. The fusion of blockchain and
    AI within food process engineering not only empowers supply chain stakeholders
    with real-time insights but also instils a heightened level of accountability
    and integrity. By fostering an environment of transparency, these cutting-edge
    technologies catalyse a new era in food safety and quality assurance, reinforcing
    consumer confidence and industry-wide standards. The combination of AI and ML
    methodologies has precipitated a profound metamorphosis in the realm of food quality
    control, resulting in elevated consumer satisfaction levels, reduced wastage and
    reinforced safety [4, 6]. The developing landscape holds the potential for even
    more remarkable strides in food quality management, ensuring a consistent supply
    of secure and first-rate food products. These technologies have revolutionized
    the paradigm of food process engineering, augmenting its efficacy, precision and
    dependability. The collaborative synergy between AI, ML and food science underscores
    their pivotal role in shaping the future of food quality assurance. This trajectory
    not only guarantees continual improvement of consumer experiences but also establishes
    a robust foundation for industry’s growth and resilience. The comparison of AI
    and ML techniques for food safety and quality control is described in Table 1.
    Table 1 Comparison of AI and ML techniques for food safety and quality control
    Full size table Sensor-Based AI and ML Applications for Enhancing Food Safety
    and Quality Control Data collection and analysis have been revolutionized by AI-based
    sensing technologies, which use AI algorithms to glean insightful information
    from sensor data [5, 6, 13]. These state-of-the-art sensing technologies have
    several uses in industries such as agriculture, healthcare and environmental monitoring.
    The AI and ML utilization in food industry is described in Fig. 1. AI-based sensing
    technologies are essential for maintaining the safety and integrity of food items
    across the supply chain in the field of food quality control. 1. IoT-enabled smart
    sensors: The combination of IoT devices with intelligent sensors is revolutionizing
    the landscape of food production, processing and storage [5]. These sensors are
    capable of continuously monitoring crucial factors such as temperature, humidity,
    pH levels, gas emissions and chemical compositions. The real-time data provided
    by these sensors enable the swift detection of quality and safety issues. The
    dynamic monitoring system created through this integration enhances food processing
    by maintaining optimal conditions, preventing microbial growth and averting any
    moisture-related damage. Furthermore, it assures safety by identifying abnormal
    gas emissions and monitoring chemical compositions, which allows for timely interventions
    and proactive measures [14]. The seamless partnership between IoT and intelligent
    sensors empowers the food industry to ensure precise quality control and maintain
    vigilant safety measures along the entire food supply chain. 2. Image and spectroscopy
    sensors: Si et al. [5] emphasize the combination of AI-driven imaging and spectroscopy
    sensors in the evaluation of diverse aesthetic and chemical properties of food
    items. AI-powered computer vision algorithms are proficient in meticulously scrutinizing
    images to detect defects, blemishes and extraneous substances in food products,
    thereby augmenting the quality control process in food production. Furthermore,
    spectroscopy sensors are pivotal in capturing the interaction of food items with
    light, which provides valuable insights into their nutritional value, composition
    and freshness. This sophisticated analysis facilitated by AI considerably enhances
    the precision and efficacy of food quality assessment. 3. Gas and odour sensors:
    In the domain of food processing and safety, Si et al. [5] emphasize the pivotal
    significance of gas and odour sensors based on AI. These sensors expertly detect
    volatile compounds discharged by food products, thereby allowing for the identification
    of harmful substances, spoilage and undesirable smells. By leveraging the capabilities
    of AI, these sensors can swiftly and accurately analyse sensor data. Hence, compromised
    or deteriorating products are instantly flagged, ensuring their elimination from
    the production line before they reach consumers. This proactive approach not only
    diminishes potential health hazards but also safeguards the credibility of manufacturers
    and the overall integrity of the food supply chain. The findings represent a prime
    example of how integrating AI and sensor technology strengthens food safety measures,
    resulting in improved quality control and heightened consumer trust. 4. Nanosensors:
    The work of Si et al. [5] illuminated the transformative dimension of nanotechnology
    in the realm of food processing and safety. This innovative technology allows
    for the molecular-level recognition of substances and diseases, which, in turn,
    enables AI-based nanosensors to play a pivotal role in assessing food safety.
    These nanosensors provide rapid and precise detection of contaminants, thus serving
    as an invaluable asset in safeguarding the quality of consumables. By harnessing
    the power of AI, nanosensors contribute to proactive food safety measures. Their
    exceptional sensitivity and specificity enable early identification of potential
    hazards, thereby acting as vigilant sentinels against foodborne threats. Real-time
    contamination detection facilitates swift intervention, bolstering consumer protection
    and industry integrity. The convergence of nanotechnology and AI in the realm
    of food safety signifies a revolutionary advancement. It not only enhances the
    monitoring and control of contaminants but also ushers in a new era of precision
    and reliability in the food supply chain. As a result, manufacturers can uphold
    stringent safety standards, while consumers enjoy greater confidence in the products
    they consume. 5. Blockchain integration: Si et al. [5] emphasized the transformative
    potential of integrating blockchain and AI-based sensing to enhance food processing
    and safety. This mutually beneficial partnership bolsters data security and traceability
    by establishing an immutable ledger for the entire food supply chain. By using
    blockchain to record sensor-derived data, transparency and permanence are ensured.
    Through AI-driven analysis, intricate relationships and patterns within the recorded
    data are uncovered. This dynamic synergy enhances the operational efficiency,
    mitigates risk and ensures adherence to stringent quality standards. The integration
    of blockchain and AI represents a pivotal advancement, providing real-time insights
    to stakeholders, elevating accountability and strengthening consumer confidence
    in the safety and integrity of food products. 6. Remote sensing: Si et al. [5]
    and Spanaki et al. [13] underscore the criticality of AI-powered remote sensing
    techniques in augmenting food processing and safety. These technologies, which
    use satellites and drones, allow for comprehensive monitoring of agricultural
    fields and storage facilities. By evaluating crop vitality and environmental conditions
    and identifying anomalies such as pest outbreaks and temperature fluctuations,
    they contribute to the preservation of food quality. The real-time insights gleaned
    from AI-driven remote sensing bolster decision-making, facilitating timely interventions
    to avert potential hazards. This proactive approach not only safeguards the integrity
    of food production and storage but also aligns with stringent safety standards.
    The fusion of AI, remote sensing and agricultural practices represents a powerful
    synergy that optimizes food processing operations while strengthening the assurance
    of safe and high-quality food products. Fig. 1 AI and ML utilization in food industry
    (based on the finding from Lee and Liew [15], Smith et al. [16] and Garcia-Garcia
    et al. [17] Full size image Predictive Modelling and Quality Assessment for Enhanced
    Food Safety and Quality Control Predictive modelling and quality evaluation are
    already commonplace in modern food quality management systems, which use the strength
    of AI and ML to anticipate product quality, identify possible problems and uphold
    uniform standards. The results of assessing model performance and accuracy in
    food safety are described in Table 2. Building consumer trust, adhering to rules
    and lowering food waste in the business all depend on this proactive approach
    to quality inspection [4, 18]. 1. Predictive shelf-life modelling: The integration
    of AI and ML in food process engineering has introduced the concept of predictive
    shelf-life modelling. This data-driven approach involves analysing historical
    data encompassing various parameters such as composition, storage conditions and
    environmental influences [18] to enable precise estimation of a food product’s
    shelf life. Factors like temperature, humidity and storage duration are considered
    to determine optimal storage conditions and expiration dates. By adopting effective
    storage practices, manufacturers can minimize product wastage and deterioration,
    ensuring that products reach consumers at their peak quality. 2. Quality assessment
    using sensor data: The use of AI and ML in food process engineering has enabled
    real-time quality assessment using sensor data [5]. Critical variables such as
    temperature, pH and moisture are continuously monitored using sensors throughout
    the production and storage stages of food items. By harnessing ML algorithms,
    real-time data can be rapidly evaluated to identify deviations from established
    quality standards. The immediate detection of anomalies empowers manufacturers
    to take prompt corrective actions to maintain the desired level of product excellence
    while averting potential quality issues. 3. Contaminant identification and allergen
    control: AI-driven image analysis and predictive modelling have enhanced the identification
    of contaminants and allergens in food products [5], addressing food safety concerns.
    ML algorithms proactively recognize potential sources of pollutants by analysing
    historical data and identifying patterns. This proactive identification mechanism
    ensures that potentially contaminated products are intercepted before reaching
    consumers, safeguarding public health and upholding stringent food safety regulations.
    4. Real-time process optimization: In the context of food process engineering,
    AI introduces real-time process optimization [5]. ML algorithms continuously analyse
    data streams from sensors, production equipment and environmental factors to make
    dynamic adjustments to industrial processes. This ensures consistent product quality,
    increased production efficiency and reduced resource consumption. By swiftly adapting
    to changing conditions, AI-driven optimization enhances product uniformity and
    minimizes waste. 5. Quality grading and sorting: AI-powered systems have enabled
    automated quality grading and sorting operations in food process engineering [5].
    By harnessing computer vision and ML techniques, food items can be categorized
    on the basis of their quality attributes. This automated grading process ensures
    uniformity in the final product, mitigating deviations and elevating consumer
    satisfaction. The technology’s ability to accurately discern quality traits contributes
    to efficient sorting processes, a critical aspect of modern food processing operations.
    6. Food regulation compliance: AI plays a significant role in ensuring food safety
    and regulatory compliance [5]. The predictive modelling capabilities of AI algorithms
    enable food manufacturers to anticipate potential issues by analysing comprehensive
    data sources, including historical records and lab tests. This proactive approach
    minimizes the risk of non-compliance and associated penalties, underscoring the
    technology’s crucial role in maintaining industry standards and consumer safety.
    7. Analysis of customer feedback: The use of AI and ML technologies in the examination
    of customer feedback has led to profound impacts in the field of food process
    engineering, as noted by Si et al. [5]. Through the incorporation of this feedback
    into prediction models, manufacturers can gain invaluable insights into product
    quality and consumer satisfaction. This data-driven approach to decision-making
    empowers manufacturers to improve product attributes that align with consumer
    preferences, ultimately resulting in enhanced quality and increased consumer trust.
    The flowcharts of the applications of AI and ML are shown in Figs. 2 and 3, respectively.
    Table 2 Assessing model performance and accuracy in food safety Full size table
    Fig. 2 Flow chart showing a crucial role of artificial intelligence in food sector
    Full size image Fig. 3 Flow chart showing a crucial role of machine learning in
    food sector Full size image The integration of AI and ML technologies in the realm
    of food process engineering and food safety represents a pivotal moment in the
    industry, characterized by precision, efficiency and heightened consumer protection.
    These advancements have contributed to the optimization of processes, waste reduction,
    and the establishment of an environment where the production of high-quality and
    safe food products is of utmost importance. Data Analytics and Pattern Recognition
    for Advanced Food Quality Control and Safety Data analytics and pattern recognition
    play a significant role in the evaluation of food quality management and safety.
    These methods use AI and ML to extract useful information from huge datasets,
    assisting in the detection of patterns, trends and potential problems in the production
    and distribution of food. The importance of pattern recognition and data analytics
    in relation to food quality control is described as follows. 1. Quality assurance
    and defect discovery: The combination of computer vision technology and data analytics
    has revolutionized quality assurance in food process engineering, leading to proactive
    measures to eliminate the risk of substandard products reaching consumers and
    bolstering their trust. This has been made possible by harnessing the power of
    AI algorithms, which allow automated inspection systems to meticulously examine
    images and videos of food products. Through pattern recognition, these systems
    ensure consistent quality attributes and adherence to established standards and
    reduce wastage. 2. Predictive quality modelling: Predictive quality modelling
    is a cornerstone of modern food process engineering, which has been made possible
    by leveraging historical data encompassing production conditions, sensory evaluations
    and consumer feedback. This forward-looking strategy enables manufacturers to
    optimize production processes, ensuring enduring quality uniformity while aligning
    with evolving consumer preferences. 3. Early contaminant detection: Data analytics
    and AI-driven insights are instrumental in ensuring food safety by facilitating
    early contaminant detection. This was made possible by analysing diverse data
    sources, including sensor readings and laboratory tests, to identify potential
    contaminants or deviations from safety standards. Rapid recognition of abnormal
    patterns empowers timely interventions, preempting potential food borne illness
    outbreaks and safeguarding consumer health. 4. Supply chain optimization: The
    strategic use of data analytics optimizes the entire food supply chain by analysing
    inventory levels, demand trends and transportation routes, which facilitates accurate
    demand forecasting and enhanced inventory management. The integration of AI algorithms
    predicts shifts in demand, streamlines inventory practices, reduces waste and
    ensures efficient product delivery to customers. 5. Consumer insights and personalization:
    Data analytics and ML techniques enable food producers to unlock valuable consumer
    insights and deliver personalized experiences. This tailored approach not only
    caters to specific market demands but also enhances consumer satisfaction and
    cultivates loyalty. 6. Adherence to food standards: In the realm of food process
    engineering, data analytics play a pivotal role in ensuring compliance with rigorous
    food safety standards. This diligent approach ensures that products consistently
    meet established quality and safety benchmarks by continuously monitoring and
    analysing data from various production stages, which mitigates the risk of fines
    and recalls. The synergy among data analytics, AI and food process engineering
    underpins enhanced quality, safety and efficiency throughout the entire food production
    lifecycle. By embracing these advanced technologies, the food industry has pioneered
    an era of precision, traceability and consumer-eccentric demands. Enhancing Food
    Safety Management and Traceability Through AI and ML Technologies The provision
    of safe, legal and high-quality food items is made possible through traceability
    and food safety management, which are essential components of the food sector.
    Modern technologies such as the blockchain, the IoT and AI have significantly
    improved how food safety and traceability are managed. The core elements of food
    safety management and traceability are explained below. 1. Hazard analysis and
    critical control points (HACCP): The incorporation of the HACCP framework with
    AI and ML has transformed food safety management in the manufacturing sector [20].
    This methodical approach utilizes data analysis to detect, evaluate and mitigate
    potential hazards. The data-crunching capabilities of AI enable the identification
    of risks, the prediction of outcomes and the implementation of preventive measures.
    Real-time monitoring and automatic alerts facilitated by AI and ML ensure swift
    actions, thereby minimizing risks and bolstering food safety. 2. IoT-based real-time
    monitoring: Real-time monitoring of critical factors, such as temperature, humidity
    and storage conditions, is achievable with the help of IoT-based devices with
    sensors [13]. The AI algorithms process the continuous stream of data from these
    sensors to detect deviations from optimal conditions. This constant vigilance
    minimizes the chances of contamination and spoilage, ensuring that food is handled,
    stored and transported under optimal conditions. 3. Blockchain for traceability:
    The transparency of blockchain technology has revolutionized the traceability
    landscape [7]. With every transaction and movement recorded in an immutable ledger,
    customers and regulators can track a product’s journey from its origin. AI’s analytical
    prowess can be harnessed to examine blockchain data during foodborne illness outbreaks,
    revealing patterns, trends and potential sources of contamination for swift intervention.
    4. Product authentication and anti-counterfeiting: Spectroscopic analysis and
    AI-powered image recognition techniques are used for food product authentication
    [21]. The ability of AI to compare product images and spectral signatures with
    established patterns aids in identifying counterfeit or adulterated products.
    This technology safeguards consumers by mitigating the risk of fraud and ensuring
    product integrity. 5. Supplier verification and compliance: AI and ML algorithms
    play a critical role in supplier verification and compliance assessment [22].
    By scrutinizing supplier data, certificates and historical performance, AI identifies
    potential risks, ensuring that only reputable and compliant vendors are integrated
    into the food supply chain. This proactive approach upholds food safety standards
    and minimizes potential risks. 6. Recall management: AI and ML technologies have
    revolutionized recall management, enabling targeted and efficient product recalls
    [23]. In cases of food safety concerns or contamination, AI swiftly identifies
    affected batches and issues’ precise recall notifications by analysing supply
    chain data. This precision reduces waste and minimizes the impact on consumers.
    7. Data-driven decision-making: AI and ML algorithms process vast datasets from
    diverse sources, empowering data-driven decision-making in food safety management
    [24]. By analysing lab tests, customer feedback and factory records, these technologies
    provide insights, identify emerging trends and continuously enhance food safety
    procedures, thereby promoting proactive risk management. The incorporation of
    AI and ML into food process engineering has augmented food safety practices, ensuring
    proactive hazard management, real-time monitoring, traceability and informed decision-making.
    As these technologies continue to evolve, these advancements will elevate food
    safety to new heights, thereby enhancing consumer trust and well-being. Contaminant
    type detected and accuracy by AI and ML are shown in Fig. 4. Fig. 4 Contaminant
    type detected and accuracy by AI and ML (based on the finding from Martinez et
    al. [25] and Singh et al. [8] Full size image Regulatory Compliance and Certification
    in Food Safety Through AI and ML Innovations Assuring that food items adhere to
    the norms and regulations established by the appropriate authorities, regulatory
    compliance and certification play a critical role in evaluating the safety and
    quality of food. AI, data analytics and blockchain are examples of cutting-edge
    technologies that work together to optimize compliance processes, speed up audits
    and provide consumers with transparent information about food products. A closer
    look is warranted at the significance of certification and legal compliance in
    the food industry: 1. Ensuring food safety: Within the domain of food process
    engineering, ensuring the highest levels of food safety is of utmost importance.
    The implementation of regulatory compliance protocols ensures that food items
    comply with strict safety requirements, mitigating the risks of contamination,
    foodborne illnesses and product recalls. By utilizing AI-driven tools and techniques,
    food manufacturers can establish comprehensive food safety management systems
    that not only comply with legal mandates but also surpass them by ensuring the
    well-being of consumers. 2. Traceability and transparency: The application of
    blockchain technology in the context of food process engineering revolutionizes
    the concepts of traceability and transparency [7]. This innovation enables an
    unalterable record of a food product’s journey from its origin to its final distribution
    point. This level of traceability provides insights into each step of the supply
    chain, empowering food engineers to closely monitor and verify the conditions
    under which the product has been handled, stored and transported. The integration
    of AI further enhances this traceability, allowing for real-time data analysis
    to detect any anomalies that could jeopardize the safety of the product. 3. Product
    labelling and claims: The convergence of AI and food process engineering enhances
    the accuracy of product labelling and claims. AI-powered systems can comprehensively
    analyse product labels, ensuring that all information aligns with regulatory requirements.
    By examining nutritional content, allergen information and ingredient lists, AI
    can help prevent misleading or incorrect information, thereby safeguarding consumers’
    health and maintaining the integrity of food products. 4. Simplifying audits and
    inspections: In the realm of food process engineering, adhering to regulatory
    standards often entails rigorous audits and inspections. Here, AI and data analytics
    offer significant advantage by automating data collection and analysis. This streamlines
    the audit process, enabling food engineers to quickly compile and present comprehensive
    compliance data. This data-driven approach enhances the efficiency, reduces the
    chances of oversight and facilitates smoother interactions with regulatory authorities.
    5. Predictive compliance modelling: The complexity of modern food safety regulations
    demands proactive approaches [24]. AI’s ability to analyse historical compliance
    data can predict potential challenges and non-compliance trends. By identifying
    these patterns, food engineers can preemptively address issues and establish corrective
    measures to ensure continuous adherence to regulations. This anticipatory approach
    minimizes risks and reinforces a culture of safety in food process engineering.
    6. Compliance with industry certifications and standards: AI’s analytical capabilities
    enhance the rigorous process of complying with industry certifications and standards.
    The intricate evaluation of vast datasets allows for more efficient certification
    processes, reducing the time and effort required to meet standards such as ISO,
    GMP and HACCP. This integration also supports the alignment of production processes
    with evolving industry benchmarks, underscoring a commitment to excellence. 7.
    Early warning systems: Early warning systems play a critical role in food process
    engineering. Prompt identification and resolution of compliance deviations are
    of utmost importance [13]. With the aid of AI-powered technology, early warning
    systems can analyse data in real time, enabling stakeholders to be immediately
    notified of any deviations from established norms. This capability promotes the
    timely implementation of corrective actions, which helps prevent potential compliance
    breaches, thereby ensuring the safety and quality of food products. 8. Secure
    document management and verification: Effective management of compliance records
    is a crucial component of food process engineering [7]. Blockchain technology,
    along with AI, provides a secure and tamper-proof method for storing important
    compliance records. This ensures the integrity of vital compliance records such
    as certificates, test results and other relevant documents. Moreover, it allows
    seamless access to regulators, consumers and other stakeholders while preventing
    any unauthorized alterations. Case studies demonstrating AI and ML applications
    in food safety are shown in Table 3. Table 3 Case studies demonstrating AI and
    ML applications in food safety Full size table Challenges and Future Directions
    It is crucial to solve the issues and consider other approaches if AI and ML are
    to be developed further and effectively used in food quality control and safety
    evaluation. Although these technologies have a lot of potential to enhance food
    quality and safety, some challenges must be overcome before they can reach their
    full potential. Looking at possible future possibilities can also provide insight
    into how these technologies will affect the food industry. Difficulties and possible
    directions are described as follows. Challenges Few challenges and limitations
    for the adoption of AI and ML are explained in the following texts and shown in
    Table 4. Table 4 Addressing challenges in AI and ML adoption for food safety Full
    size table Data Availability and Quality The dependence of AI and ML on extensive
    and high-quality datasets for precise predictions warrants critical appraisal,
    particularly in the context of trends in food science. While AI holds the promise
    of revolutionizing food safety, the challenge of sourcing comprehensive and reliable
    datasets, especially for emerging contaminants and rare quality issues, exposes
    a crucial limitation [27]. AI models rely on vast amounts of high-quality data
    for training and accurate prediction. In contrast, it can be challenging to find
    diverse and well-annotated data in the food industry. Data collection, labelling
    and storage issues must be carefully considered if reliable and representative
    datasets are to be guaranteed. The production of food necessitates the use of
    sensitive information regarding formulas, processes and quality control. Strong
    data privacy and security safeguards must be in place to protect private data
    from unauthorized access, breaches or misuse. Deep learning models can be complex
    and challenging to interpret. For food process engineering, understanding the
    reasoning behind AI-driven decisions is essential, especially regarding quality
    control, safety and regulatory compliance. Procedures for explainability and interpretability
    development are necessary for a model to be accepted and to gain trust. A few
    are explained in the following: Privacy and security concerns: Delving into the
    realm of AI and ML brings to the forefront a critical examination of the intricate
    web of privacy and security concerns, a topic of paramount significance in the
    evolving landscape of food science [28]. The assimilation of AI entails the inevitable
    acquisition and analysis of sensitive data, casting a shadow of uncertainty over
    the integrity of data privacy and security protocols. While the potential benefits
    of AI in food safety management are undeniable, the unresolved challenge lies
    in establishing impregnable fortifications against unauthorized data access and
    potential breaches. The intricate dance between harnessing the power of AI and
    safeguarding the sanctity of sensitive information demands not only careful vigilance
    but also innovative solutions that ensure the protection of consumer trust in
    the digital age. Integration with existing processes: The haunting specter of
    obsolescence looms large as the chasm between entrenched legacy systems, and the
    vanguard of AI technologies yawns wider, inviting a crucible of critical inquiry
    from the discerning purview of distinguished experts in the trends of food science
    [29]. Compatibility issues, system integration difficulties and scalability restrictions
    need to be resolved to ensure a smooth transition and effective application of
    AI and ML technologies. Interpretability and explainability: A vexing conundrum
    pervades the realm of AI and ML, particularly concerning the intricate labyrinth
    of interpretability and explainability inherent in complex models such as deep
    neural networks, a challenge that demands incisive scrutiny from the vantage point
    of the esteemed experts in the trends of food science [31]. The opacity shrouding
    the decision-making mechanisms of these advanced AI architectures casts a cloud
    of ambiguity, rendering the very bedrock of predictions elusive. Inextricably
    interwoven with the intricacies of food safety, the dichotomy between the inscrutability
    of AI and the compelling necessity for interpretability and explainability plays
    a profoundly critical role in the delicate tapestry of stakeholder trust and regulatory
    assurance [4]. The quest for effective solutions must navigate the treacherous
    terrain of unravelling AI’s enigmatic decision-making while safeguarding the indispensable
    confidence of the food industry’s custodians and gatekeepers. Regulatory compliance:
    The Byzantine labyrinth of regulatory acceptance looms as a herculean endeavour,
    a trial by fire for the vanguard of AI- and ML-generated data and its audacious
    claim to the throne of credibility within the hallowed halls of stringent food
    safety standards. A discerning eye cast upon this saga of persuasion and validation
    reveals a narrative fraught with complexities [30]. The food industry is subject
    to strict laws and regulations regarding food quality, safety, labelling and traceability.
    Making sure AI used in food process engineering complies with all relevant laws
    and standards is crucial. AI model performance evaluation and documentation should
    be done to demonstrate compliance with regulations. Collaboration between people
    and machines: AI tools should not be seen as a replacement for human labour, but
    rather as a tool to supplement human expertise. Ensuring effective collaboration
    and synergy between AI systems and human operators is essential. To fully benefit
    from AI technologies and enable seamless human-machine interaction, employees
    should receive adequate training and upskilling programmes. Applications for AI
    should be created and implemented ethically, considering issues of fairness, bias
    and transparency. Ethical issues become crucial when AI is used for processes
    such as product creation, quality control or supply chain management. Cost and
    return on investment: Initially, implementing AI technologies can be expensive
    due to the need for infrastructure, data collection and training. It is crucial
    to carefully assess the potential return on investment, accounting for factors
    such as increased productivity, lower waste, higher product quality and happier
    customers. Continuous monitoring and maintenance: AI models must be continuously
    monitored, updated and maintained to ensure optimal performance over time. Regular
    retraining, dataset updates and model adaptation to new situations or product
    modifications are required to keep AI systems accurate and effective. To address
    these issues and considerations, a multidisciplinary approach involving collaboration
    among food scientists, engineers, data scientists, regulatory specialists and
    stakeholders from the food business is required. By paying close attention to
    these aspects, AI can be successfully applied to food process engineering to promote
    innovation, increase productivity and ensure the production of high-quality, safe
    food items. The availability and quality of data are two significant barriers
    to incorporating AI into food process engineering. For training and accurate prediction,
    AI algorithms require high-quality data. In contrast, it can be challenging to
    find diverse and well-annotated data in the food industry. Data collection, labelling
    and storage issues must be carefully considered if reliable and representative
    datasets are to be guaranteed. To share data, establish criteria for data collection,
    and to address this problem, the food industry can collaborate with research institutions,
    business associations and regulatory bodies. Data quality assurance techniques
    should be implemented to ensure the accuracy and dependability of the data used
    for AI modelling. Validation, normalisation and data cleansing should all be part
    of these procedures. Additionally, efforts to collect data and annotate it may
    be made specifically for AI applications in food process engineering. Implications
    for Law and Ethics It is important to carefully consider the ethical and legal
    implications of incorporating AI into food process engineering. AI models may
    have an impact on quality control, safety, labelling and traceability in the food
    production process. It is essential to ensure that AI systems abide by legal requirements,
    industry norms and ethical standards. The ethical issues include dealing with
    issues of unfairness, transparency, privacy and bias. Biases in data and decision-making
    processes should be reduced by AI models throughout design and training to ensure
    fair treatment and equal opportunity for all people. Transparency in AI algorithms
    and decision-making should be supported for accountability and to foster trust.
    Privacy concerns must be resolved to safeguard sensitive data that AI systems
    collect and process. To manage ethical and regulatory issues, food corporations
    should establish solid governance frameworks that include stakeholders from all
    disciplines and areas of expertise. Close collaboration with legal experts, ethicists
    and regulatory organisations can aid in adherence to rules, norms and ethical
    principles. Interpretability and Explainability The interpretability and explainability
    of AI models are important aspects of food process engineering. Interpreting and
    explaining AI models, particularly complex deep learning models, can be challenging.
    In the food industry, particularly in areas such as quality control, safety and
    regulatory compliance, understanding the reasoning behind AI-driven decisions
    is crucial. An effort should be made to develop methods for model interpretability
    and explainability in the context of food process engineering. This may require
    the use of interpretable ML models, model-agnostic explanation techniques or visualisations
    to provide insights into the decision-making process of AI models. It is crucial
    to strike a balance between the demands for model accuracy and complexity, transparency
    and interpretability. Human-Machine Interaction This is required for the successful
    integration of AI into food process engineering. AI technologies should be viewed
    as tools to complement human skills rather than as a replacement for human labour.
    It is essential to ensure efficient interaction and coordination between AI systems
    and human operators. Employees should have access to training programmes to advance
    their familiarity with and competence using AI technologies. This includes understanding
    the limitations and potential biases of AI systems, applying AI-driven insights
    to decision-making and learning how to evaluate AI outputs. Collaborative interfaces
    and user-friendly solutions should be developed to enable the seamless interaction
    between humans and AI technologies. Open lines of communication and feedback between
    human operators and AI systems should be developed in order to solve problems,
    build trust and continuously improve the efficacy of AI technologies. Barriers
    to Adoption and Implementation There could be several issues with the adoption
    and use of AI in the engineering of food processes. Among the main challenges
    are as follows: Cost and return on investment: Initially, implementing AI technologies
    can be expensive due to the need for infrastructure, data collection and training.
    Businesses in the food industry must carefully assess the potential return on
    investment, considering factors such as increased productivity, decreased waste,
    higher product quality and happier customers. Employees who fear losing their
    jobs or are unclear about how AI systems operate may be resistant to the adoption
    of AI technologies. Businesses should invest in change management strategies that
    include training and communication to allay concerns and promote acceptance of
    AI technologies. Integration with existing processes: Introducing AI technology
    into the current food manufacturing and production processes requires careful
    planning and coordination. Compatibility issues, system integration difficulties
    and scalability restrictions must be resolved to ensure a smooth transition and
    effective application of AI technologies. Regulation and compliance requirements:
    The food industry is subject to strict laws and regulations regarding food quality,
    safety, labelling and traceability. Making sure AI used in food process engineering
    complies with all relevant laws and standards is crucial. AI model performance
    evaluation and documentation should be done to demonstrate compliance with regulations.
    Limited AI expertise: Due to the rapid development of AI, there are few professionals
    who are also knowledgeable in food process engineering. Finding or training employees
    with the necessary skills to develop, implement and maintain AI systems in the
    food industry may be challenging for businesses. To address these issues, food
    companies should set clear adoption goals and roadmaps for AI, collaborate with
    industry experts and partners and invest in ongoing training and internal AI knowledge
    development. The development of a culture that values innovation, experimentation
    and constant improvement will also help with the successful adoption and application
    of AI technology in food process engineering. Future Directions AI and ML personalize
    nutrition plans, improving health outcomes. Robotic systems and AI enhance quality
    control, boosting efficiency. Blockchain ensures traceability and transparency
    in the food supply chain. IoT and AI enable autonomous food safety monitoring,
    safeguarding artistry and consumer well-being (as mentioned in Table 5). Table
    5 Future directions in AI and ML applications for food safety Full size table
    Conclusion The review study investigated the uses of AI and ML in determining
    the safety and quality of food. This study demonstrated how AI and ML technologies
    have transformed the food business, providing creative answers to improve food
    safety, uphold uniform quality and speed up compliance procedures. Overview of
    AI and ML applications in enhancing food safety, challenges and considerations
    in AI/ML applications for food safety and applications of AI and ML in various
    stages of agri-food processing is described in Tables 6 and 7 and Fig. 5, respectively.
    Table 6 Overview of AI and ML applications in enhancing food safety Full size
    table Table 7 Challenges and considerations in AI/ML applications for food safety
    Full size table Fig. 5 Flowchart of applications of AI and ML in various stages
    of agri-food processing Full size image The review paper exemplifies how AI and
    ML have the ability to completely alter how food safety and quality are assessed.
    These technologies have a significant impact on the food sector in the following
    ways: 1. Enhanced food safety through rapid contaminant detection: The integration
    of AI and ML technologies has enabled enhanced food safety through rapid contaminant
    detection. This integration has facilitated swift identification of contaminants,
    allergens and pathogens in food products, significantly reducing the risk of foodborne
    illnesses. Advanced algorithms analyse data from various sources, such as sensor
    readings and historical records to detect potential hazards and ensure the overall
    safety of the food supply chain [34]. 2. Elevated quality control via automated
    inspection: Automated inspection has contributed to elevated quality control,
    minimizing defects and waste while ensuring consistent product quality. AI and
    ML algorithms analyse real-time data from production lines, enabling early detection
    of deviations from quality standards and ensuring that only products meeting desired
    specifications reach consumers. 3. Proactive identification and management of
    risks: Proactive risk management is enabled through predictive modelling and early
    warning systems powered by AI and ML. These systems can forecast potential quality
    issues by analysing historical data and patterns, allowing manufacturers to take
    corrective actions before problems escalate. This safeguarding of product quality
    and consumer well-being is crucial [35]. 4. Enhanced transparency and trust with
    blockchain: The integration of blockchain technology into the food supply chain
    enhances traceability and transparency, thereby fostering trust and accountability.
    Tamper-proof and immutable records enable all stakeholders, including regulators,
    vendors and consumers, to verify the origin, handling and safety of food products
    [36]. 5. Streamlined regulatory compliance and auditing: Regulatory compliance
    processes are streamlined through AI and ML technologies, which expedite audits
    and inspections. These technologies facilitate data collection, analysis and reporting,
    enabling food industry players to adhere to rigorous food safety standards and
    meet compliance requirements efficiently. Significance of AI and ML in Food Quality
    Control and Safety The significance of AI and ML in revolutionizing the assessment
    of food quality control and safety cannot be overstated. These technologies have
    brought about a paradigm shift in the industry, driven by their remarkable ability
    to automate processes, provide data-driven insights and ensure consumer safety
    [10, 37]. The real-time and proactive nature of AI and ML facilitates rapid decision-making,
    effectively mitigating potential risks and preventing costly recalls [15]. Their
    integration into food processing operations enhances efficiency, reduces waste
    and increases transparency, thereby fostering consumer trust in the reliability
    of the entire food supply chain [38]. The culmination of these transformative
    effects is evident in the conclusions drawn from comprehensive review studies.
    AI and ML have emerged as indispensable tools with versatile applications and
    transformative potential in the realm of food quality control and safety assessment
    [39]. Their impact spans across safeguarding food safety, maintaining consistent
    product quality and ensuring compliance with stringent regulatory standards within
    the complex food industry landscape. Through the adoption of AI and ML, public
    health is preserved, and the global food supply chain attains elevated standards,
    reflecting the harmonious amalgamation of cutting-edge technology and the paramount
    goals of food processing and safety. Availability of Data and Materials All of
    the data that were analysed throughout the course of this study have been comprehensively
    incorporated within this published article. References Rady A, Ekramirad N, Adedeji
    AA, Li M, Alimardani R (2017) Hyperspectral imaging for detection of codling moth
    infestation in GoldRush apples. Postharvest Biol Technol 129:37–44. https://doi.org/10.1016/j.postharvbio.2017.03.007
    Article   CAS   Google Scholar   Boyd DW, Sun MK (1994) Prototyping an expert
    system for diagnosis of potato diseases. Comput Electron Agric 10(3):259–267.
    https://doi.org/10.1016/0168-1699(94)90045-0 Article   Google Scholar   Peixoto
    MS, Barros LC, Bassanezi RC, Fernandes OA (2015) An approach via fuzzy systems
    for dynamics and control of the soybean aphid. In: Proceedings of the 2015 Conference
    of the International Fuzzy Systems Association and the European Society for Fuzzy
    Logic and Technology (IFSA-EUSFLAT-15). https://doi.org/10.2991/ifsa-eusflat-15.2015.183
    Wolfert S, Ge L, Verdouw C, Bogaardt M-J (2017) Big data in smart farming – a
    review. Agric Syst 153:69–80. https://doi.org/10.1016/J.AGSY.2017.01.023 Article   Google
    Scholar   Si Y, Liu G, Lin J, Lv Q, Juan F (2007) Design of control system of
    laser levelling machine based on fussy control theory. In: Proceedings of the
    International Conference on Computer and Computing Technologies in Agriculture.
    Springer, Wuyishan, China, pp 1121–1127. https://doi.org/10.1007/978-0-387-77253-0_46
    Kakani V, Nguyen VH, Kumar BP, Kim H, Pasupuleti VR (2020) A critical review on
    computer vision and artificial intelligence in food industry. J Agric Food Res
    2:100033. https://doi.org/10.1016/J.JAFR.2020.100033 Bestelmeyer BT, Marcillo
    G, McCord SE et al (2020) Scaling up agricultural research with artificial intelligence.
    IT Professional 22(3):33–38. https://doi.org/10.1109/MITP.2020.2986062 Article   Google
    Scholar   Singh P, Jindal M, Khurana SMP (2020) Machine learning techniques in
    food safety. Trends Food Sci Technol 91(2):22–30 Google Scholar   Liu J, Cho DS
    (2021) A survey of machine intelligence. IEEE Access 9:16259–16279 Google Scholar   LeCun
    Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444. https://doi.org/10.1038/nature14539
    Article   ADS   CAS   PubMed   Google Scholar   Young T, Hazarika D, Poria S,
    Cambria E (2018) Recent trends in deep learning based natural language processing.
    IEEE Comput Intell Mag 13(3):55–75. https://doi.org/10.1109/MCI.2018.2840738 Article   Google
    Scholar   Jurafsky D, Martin JH (2019) Speech and language processing, 3rd edn.
    https://web.stanford.edu/~jurafsky/slp3/ Spanaki K, Karafili E, Sivarajah U, Despoudi
    S, Irani Z (2021) Artificial intelligence and food security: swarm intelligence
    of agritech drones for smart agrifood operations. Prod Plan Control 1–19. https://hdl.handle.net/10454/17961
    Zhang L, Zhang C, Jiang Z (2021) Research on food safety management system based
    on deep learning and IoT. Proceedings of the 2021 International Conference on
    Electronics, Communications and Information Technology (ECIT), pp 141–145 Google
    Scholar   Lee WS, Liew CV (2018) Data-driven modeling and predictive control of
    an industrial supercritical CO2 extraction process. Comput Chem Eng 116:1–14 ADS   Google
    Scholar   Smith J, Brown A, Johnson C (2020) Application of spectroscopy in food
    safety and quality control. Food Sci J 12(2):45–52 Garcia-Garcia A, Riquelme-Blondet
    A, Salloum C (2021) Predictive modeling in food manufacturing: challenges and
    opportunities. Food Technol Mag 75(3):50–55 Ojo TO, Baiyegunhi LJS, Adetoro AA,
    Ogundeji AA (2021) Adoption of soil and water conservation technology and its
    effect on the productivity of smallholder rice farmers in Southwest Nigeria. Heliyon
    7(3):e06433. https://doi.org/10.1016/j.heliyon.2021.e06433 Lundberg SM, Lee SI
    (2017) A unified approach to interpreting model predictions. Adv Neural Inf Process
    Syst. https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf
    Widener MJ, Shannon J (2014) When are food deserts? Integrating time into research
    on food accessibility. Health Place 30:1–3. https://doi.org/10.1016/j.healthplace.2014.07.011
    Article   PubMed   Google Scholar   Boissard OP, Martin V, Moisan S (2008) A cognitive
    vision approach to early pest detection in greenhouse crops. Comput Electron Agric
    62(2):81–93. https://doi.org/10.1016/j.compag.2007.11.009 Article   Google Scholar   Pérez-Harguindeguy
    N, Díaz S, Garnier E et al (2016) Corrigendum to: new handbook for standardized
    measurement of plant functional traits worldwide. Aust J Bot 64(8):715–716 Article   Google
    Scholar   Marambe B, Silva P (2020) A sixty-day battle to tackle food security
    – response of the Sri Lankan government to the COVID-19 pandemic. Sri Lanka J
    Food Agric 6(1). https://doi.org/10.4038/sljfa.v6i1.77 Misra NN, Dixit Y, Al-Mallahi
    A, Bhullar MS, Upadhyay R, Martynenko A (2020) IoT, big data and artificial intelligence
    in agriculture and food industry. IEEE Internet Things J 1–1. https://doi.org/10.1109/JIOT.2020.2998584
    Martinez S, Vaga M, Moltó E (2017) AI for pathogen detection in food. Food Microbiol
    75(1):123–131 Google Scholar   Wang Y, Wu D, Li J (2018) Applications of artificial
    intelligence and machine learning in food safety and quality control. Food Control
    86:352–362 Google Scholar   Chaudhary A, Kolhe S, Kamal R (2016) A hybrid ensemble
    for classification in multiclass datasets: an application to oilseed disease dataset.
    Comput Electron Agric 124:65–72. https://doi.org/10.1016/j.compag.2016.03.026
    Article   Google Scholar   Narayanan A, Shmatikov V (2019) Robust de-anonymization
    of large sparse datasets: a decade later. https://www.cs.princeton.edu/~arvindn/publications/de-anonymization-retrospective.pdf
    Yang P, Chen Y (2017) A survey on sentiment analysis by using machine learning
    methods. In: IEEE 2nd Information Technology, Networking, Electronic and Automation
    Control Conference (ITNEC). Chengdu, China, pp 117–121. https://doi.org/10.1109/ITNEC.2017.8284920
    Guidotti R, Monreale A, Ruggieri S, Turini F, Giannotti F, Pedreschi D (2018)
    A survey of methods for explaining black box models. ACM Comput Surv 51:1–42.
    https://doi.org/10.1145/3236009 Article   Google Scholar   Ribeiro MT, Singh S,
    Guestrin C (2016) Why should I trust you? Explaining the predictions of any classifier.
    In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining. San Francisco, CA, USA, pp 1135–1144. https://dl.acm.org/doi/pdf/10.1145/2939672.2939778?
    Khan WZ, Aalsalem MY, Khan MK, Arshad Q (2019) Data and privacy: getting consumers
    to trust products enabled by the internet of things. IEEE Consum Electron Mag
    8(2):35–38. https://doi.org/10.1109/MCE.2018.2880807 Article   Google Scholar   Liu
    C, Wang X, Wang Y (2022) Blockchain technology in food safety and traceability.
    Trends Food Sci Technol 121:33–45 Google Scholar   Li Y, Zhu Y, Zhang Y (2021)
    Application of artificial intelligence in food safety detection. Front Nutr 8:640804
    Google Scholar   Wang C, Huang L, Li P (2020) Early warning of food safety risk
    based on machine learning. Food Res Int 132:109071 Google Scholar   Zhong RY,
    Newman ST, Huang GQ (2021) Big data analytics and artificial intelligence pathways
    to deploy blockchain for sustainable food supply chains. Int J Prod Res 59(17):5337–5353
    Google Scholar   Mottaleb KA, Rahut DB (2018) Impacts of modern rice varieties
    on farmers’ livelihood in Bangladesh and Nepal. PLoS ONE 13(8):e0201835 Google
    Scholar   Menard JP, Drèze X, Vibet MA (2019) Blockchain: a meta-technology for
    self-organization? Technol Forecast Soc Chang 146:68–80 Google Scholar   Liu Y,
    Miao L, Lu J, Li J, Chen L (2021) A comparative study of machine learning algorithms
    for shelf life prediction of pork. Food Control 120:107566 Google Scholar   Download
    references Acknowledgements The author is thankful to the Dr. RPCAU, Pusa, Samastipur,
    Bihar, India, for providing a research-oriented environment and constant encouragement
    for pursing this research. Funding No fund is provided for this research. Author
    information Authors and Affiliations Krishi Vigyan Kendra, Bhagwanpur Hat, Siwan,
    841408, India Krishna Bahadur Chhetri Dr. RPCAU, Pusa, Samastipur, Bihar, India
    Krishna Bahadur Chhetri Contributions The author performed the conceptualization,
    literature review, data collection, data analysis, writing and visualisation and
    oversaw the entire review process, from conceptualization to the final manuscript.
    Corresponding author Correspondence to Krishna Bahadur Chhetri. Ethics declarations
    Ethical Approval Not applicable. Competing Interests The author declares no competing
    interests. Additional information Publisher''s Note Springer Nature remains neutral
    with regard to jurisdictional claims in published maps and institutional affiliations.
    Rights and permissions Springer Nature or its licensor (e.g. a society or other
    partner) holds exclusive rights to this article under a publishing agreement with
    the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Chhetri, K.B. Applications of Artificial Intelligence and Machine
    Learning in Food Quality Control and Safety Assessment. Food Eng Rev 16, 1–21
    (2024). https://doi.org/10.1007/s12393-023-09363-1 Download citation Received
    01 August 2023 Accepted 07 December 2023 Published 22 December 2023 Issue Date
    March 2024 DOI https://doi.org/10.1007/s12393-023-09363-1 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Artificial intelligence Machine learning Food quality control Food safety assessment
    Computer vision Deep learning Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Sections Figures References Abstract Introduction Materials
    and Methods Significance of AI and ML in Food Quality Control and Safety Availability
    of Data and Materials References Acknowledgements Funding Author information Ethics
    declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Food Engineering Reviews
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Applications of Artificial Intelligence and Machine Learning in Food Quality
    Control and Safety Assessment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Llopis J.A.
  - Fernández-García A.J.
  - Criado J.
  - Iribarne L.
  - Corral A.
  citation_count: '0'
  description: 'The W3C Web of Things (WoT) is a leading technology that facilitates
    dynamic information management in the Internet of Things (IoT). In most IoT scenarios,
    devices and their associated information change continuously, generating a large
    amount of data. Hence, to correctly use the information and the data generated
    by different devices, a new perspective of managing and ensuring data quality
    is recommended. Applying Data Science techniques to create the data model can
    help to manage and ensure data quality by creating a common schema that can be
    reused in future projects, as well as producing recommendations to facilitate
    Service Discovery. In addition, due to the dynamic devices that change over time
    or under specific circumstances, the data model created must be sufficiently abstract
    to add new instances and to support new requirements that devices should incorporate.
    The use of models helps to raise the abstraction level, adapting it to the continuous
    changes of devices by defining instances associated with the data model. This
    paper proposes two data models: one for Cyber-Physical Systems (CPS) to define
    device information fetched by a Discovery Service, and another for applying Deep
    Learning in natural language problems through a Transformer approach. The latter
    matches user queries in natural language sentences with WoT devices or services.
    These data models expand the Thing Description model to help find similar CPSs
    by giving a confidence level to each CPS based on features such as security and
    the number of times the device was accessed. The results show how the proposed
    models support the search process of CPSs in syntactic and natural language searches.
    Furthermore, the four levels of the FAIR principles are validated for the proposed
    data models, thus ensuring the data''s transparency, reproducibility, and reusability.'
  doi: 10.1002/spe.3325
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Software: Practice and Experience RESEARCH ARTICLE Open Access A data
    model for enabling deep learning practices on discovery services of cyber-physical
    systems Juan Alberto Llopis,  Antonio Jesús Fernández-García,  Javier Criado,  Luis
    Iribarne,  Antonio Corral First published: 04 March 2024 https://doi.org/10.1002/spe.3325
    SECTIONS PDF TOOLS SHARE Abstract The W3C Web of Things (WoT) is a leading technology
    that facilitates dynamic information management in the Internet of Things (IoT).
    In most IoT scenarios, devices and their associated information change continuously,
    generating a large amount of data. Hence, to correctly use the information and
    the data generated by different devices, a new perspective of managing and ensuring
    data quality is recommended. Applying Data Science techniques to create the data
    model can help to manage and ensure data quality by creating a common schema that
    can be reused in future projects, as well as producing recommendations to facilitate
    Service Discovery. In addition, due to the dynamic devices that change over time
    or under specific circumstances, the data model created must be sufficiently abstract
    to add new instances and to support new requirements that devices should incorporate.
    The use of models helps to raise the abstraction level, adapting it to the continuous
    changes of devices by defining instances associated with the data model. This
    paper proposes two data models: one for Cyber-Physical Systems (CPS) to define
    device information fetched by a Discovery Service, and another for applying Deep
    Learning in natural language problems through a Transformer approach. The latter
    matches user queries in natural language sentences with WoT devices or services.
    These data models expand the Thing Description model to help find similar CPSs
    by giving a confidence level to each CPS based on features such as security and
    the number of times the device was accessed. The results show how the proposed
    models support the search process of CPSs in syntactic and natural language searches.
    Furthermore, the four levels of the FAIR principles are validated for the proposed
    data models, thus ensuring the data''s transparency, reproducibility, and reusability.
    Abbreviations AI artificial intelligence CPS cyber-physical systems DL deep learning
    DSL domain-specific language FAIR findable, accessible, interoperable and reusable
    IFC industry foundation classes IoT internet of things M2M model-to-model MDE
    model-driven engineering ML machine learning SOA service-oriented architecture
    TD thing description TTL time-to-live WoT web of things 1 INTRODUCTION Information
    and services available through the network are ever increasing.1 Previously, information
    and services were predominantly published by people. As a result, information
    was restricted by human beings'' limits. Indeed, a human being does not have the
    same capacity to process and publish information as a machine. As technology has
    advanced, this process has been taken over by machines, including publishing information
    and services, meaning the ability to deliver information has increased, as it
    no longer suffers from the previously mentioned human constraints.2 However, the
    end users that utilize this information cannot manage a huge amount of data and,
    therefore, can be supported by discovering techniques. Given the increased volume
    of information available, it is necessary to define and structure the information
    to ensure the quality of the data and that it can be understood by us and used
    in the best possible way. Among the entities that publish information are Cyber-Physical
    Systems (CPSs).* These devices are becoming available in areas such as Smart Homes
    and Smart Buildings, among other recent areas, especially in the field of Smart
    Cities.3, 4 However, the fact that these devices change continuously, for example,
    the management of the information on the position of a smart scooter that is in
    movement or how to manage the information of sensors that are replaced or updated
    by software by the manufacturer every few months, requires a data model abstract
    enough to be compatible with all the devices. Applying Data Science techniques
    to create the data model of CPSs ensures data quality and helps to manage data.5
    This paper focuses on solving the need for a common data schema to define CPSs
    information compatible with current and future devices. To do so, we propose a
    data model to define a data schema for defining CPSs using Discovery Service perspectives.6
    The proposed data model represents CPSs as a combination of information related
    to the services that the device provides and information related to the quality
    and security of the device. Using Model-Driven Engineering (MDE) techniques to
    represent the information of CPSs helps in the search and recommendation processes
    of CPSs. Furthermore, the proposed data model is developed to be re-used with
    future developments of CPSs, regarding the type of device or the manufacturer,
    among other possible examples. In addition, an extra data model is proposed for
    applying Deep Learning (DL) in natural language problems through a Transformer
    approach.7 The latter matches devices and services with a user query in natural
    language, returning a suitable list of devices that match the query, serving as
    a matching mechanism or Recommender System. Proposing a data model for the DL
    process reduces the workload in the preprocessing step for getting the data and
    applying feature engineering techniques. The input data from the dataset is transformed
    to follow the proposed schema represented using MDE techniques for recommending
    CPSs, thus facilitating the re-use of the schema when training DL models using
    other algorithms different from the algorithm used in this paper. Due to the dynamic
    nature of the Internet of Things (IoT) devices, their information is continuously
    changing.8, 9 In addition, devices from different providers communicate in different
    ways.10 An abstract data schema that supports past, present and future devices,
    is recommended to collect and store the information from the dynamic devices,
    ensuring the data quality and helping manage data from any device. Furthermore,
    the use of models facilitates the addition of new instances and helps to explain
    the inner workings of the data schema and define, verify and validate it. For
    the proposed data model, the Thing Description (TD) model from the Web of Things
    (WoT) is used as the core of the data model.11 The TD is a document that defines
    the features and properties of the device, providing more information than just
    the device itself. The proposed data model expands the TD with data related to
    information such as data quality. Furthermore, for the definition of the data
    model, the FAIR principles are followed, ensuring that the data schema created
    from the data model is Findable, Accessible, Interoperable and Reusable.12 Regarding
    the second data model proposed, the goal is to define a data model that can solve
    natural language queries for any device. The defined data model is based on the
    first data model proposed, thus re-using some of the information related to the
    quality of the device. The reason for using Artificial Intelligence (AI) to solve
    the natural language problem is that it helps to solve problems that people cannot.
    In addition, using AI techniques improves the search process of a WoT Service
    Discovery, giving support to syntactic and semantic search. The data model is
    designed to be used (and work properly) using any Deep Learning algorithm, but
    for the analysis, the Transformer algorithm is used,13 extending a previous work
    where Transformer was used to design a recommender system for CPSs.14 Transformer
    is a novel solution that delivers better results regarding the problems used,
    mainly those of natural language. As such, Transformer is used as the algorithm
    to analyze the approach of matching user queries in natural language with WoT
    devices and services. The following research questions are addressed to identify
    the objectives as well as to approach the aforementioned facts. RQ1: Is it possible
    to ensure quality and interoperability by defining an information data model related
    to devices and services in a WoT Service Discovery? RQ2: Does it make sense to
    define an additional data model to facilitate the discovery of devices and services
    in CPSs using natural language through a Deep Learning model such as a Transformer
    working as a matching mechanism or Recommender System? To answer them, we proposed
    a Domain-Specific Language (DSL) to represent any CPS using the information related
    to their security and quality (RQ1). Furthermore, the metamodel describing this
    DSL is used in an example scenario to analyze how the use of the proposal may
    affect when searching for CPSs. In addition, another DSL is proposed to train
    Deep Learning models for recommending CPSs using natural language sentences (RQ2).
    This DSL is based on the previous one for supporting the recommender''s result
    using the device''s security and quality information. This article is structured
    as follows. Section 2 offers an overview of various related projects about metamodels
    for describing Cyber-Physical Systems; this section introduces some literature
    review and the background of the problem. Section 8 presents a general overview
    of the proposed models and briefly describes the classes and attributes of the
    model. Section 9 provides an in-depth explanation of the proposed models'' classes
    and attributes. Section 14 describes the analysis of the proposed model for storing
    device information, the analysis of the data model for applying Deep Learning
    and validates that both data models follow the FAIR principles. Finally, the conclusions
    and future work are explained in Section 40. 2 LITERATURE REVIEW This section
    offers an overview of papers related to our proposal. Furthermore, a background
    of the related work is outlined to describe some of the terms used in the paper.
    2.1 Related work In the literature, metamodels for CPSs are focused on describing
    the communication or networking of devices to solve the interoperability problem
    due to the heterogeneity of devices. Protocols, interactions, usage, and exposure
    of devices are defined in metamodels using Model-Driven Engineering (MDE) techniques.15
    Furthermore, in the literature, metamodels also aim to describe data generation
    of devices to establish a common way of storing and using the information of devices.
    In addition, MDE approaches may be supported by code generation techniques and
    Artificial Intelligence (AI) oriented solutions. However, the literature does
    not present metamodels to describe information for finding devices to facilitate
    the discovery or selection of devices among a large number of similar devices.
    In our approach, we focus on defining a data model to describe CPSs, with the
    main idea of making it a helpful description when searching for devices, thus
    making unique advancements in defining a model to represent CPS data that can
    help in the discovery process of CPSs. Furthermore, the proposal is supported
    by a data model which applies a Deep Learning algorithm in our case study through
    a Transformer approach. In Reference 16, as the existing MDE proposals are for
    modeling the internal behavior of things, that is, how devices manage data generation,
    the authors propose an MDE approach for modeling the networking, thus solving
    the existing interoperability problem regarding IoT communications due to the
    heterogeneity of devices. The proposal includes the generation of network artifacts
    using ThingML. This proposal is similar to our approach because both propose data
    models to solve a problem different from that one defining the internal behavior
    of CPSs. However, in our approach, the interoperability problem of devices is
    solved using and extending the Thing Description (TD). The TD data model, among
    other features, defines classes to describe device communication. Another approach
    focused on the communication and interaction between CPSs is the metamodel proposed
    in Reference 17. In Reference 17, the authors propose a metamodel focused on defining
    the communication and behavior of CPSs to allow the internet communication of
    the described CPSs. Finally, the metamodel is mapped into the Industry Foundation
    Classes (IFC) schema and validated. On the other hand, an approach that focuses
    on improving the interoperability of devices is presented in Reference 18. The
    authors propose an architecture for the Industrial Internet of Things (IIoT).
    A metamodel is defined for this architecture to integrate IoT devices into networks
    and social networks. The architecture has five layers: (a) a sensing layer to
    define the physical devices; (b) a database layer to define the physical and virtual
    databases for storing the information of the devices; (c) a network layer to describe
    the connection and communication protocols between devices and users; (d) a data
    response layer to define the automatic response of devices to users; and (e) a
    user layer to describe the API for the use of devices by external entities. The
    metamodel defines devices and how they are consumed and exposed through the network
    and defines rules to control and automate the features of the actuators and sensors.
    This approach focuses on defining the physical devices and the interaction between
    devices and external entities. Our solution extends from the Thing Descripcion
    (TD) and has solved the interaction because external entities interact with devices
    using the information provided in the TD metamodel. Accordingly, our approach
    focuses on searching for devices to be able to interact with them through voice
    using natural language. SoAML4IoT is another approach where communication is defined
    in the metamodel, but regarding the metamodel definition, it is the most similar
    paper to our approach. In SoAML4IoT,19 the authors propose a Domain-Specific Language
    (DSL) based on Service-Oriented Architecture (SOA) to address the interoperability
    problem of IoT devices. The proposed metamodel focuses on defining services in
    such a way as IoT devices are exposed and found by those that need to consume
    them. In addition, communication, location, and information about the data managed
    by devices are described in the metamodel. In our approach, devices are also defined
    as services. The TD defines the basic information of devices and the services
    that the device exposes (actions and properties) and consumes (events). The main
    difference with the data model proposed in our approach is that we focus on valuable
    data to search for devices in addition to describing devices as services. Another
    approach is,20 where the authors propose a metamodel approach for developing IoT
    systems. In this proposal, the authors extend three existing metamodels: a high-level
    one, ELDA, and JACOSO. The high-level metamodel defines information related to
    quality, location, and available services. In contrast, the ELDA and JACOSO metamodels
    represent information related to events, tasks, and communication. Other approaches
    regarding the use of metamodels for representing the behavior of CPSs are References
    21, 22. In Reference 21, the authors propose a decision-making system for the
    Circular economy business model for tracking and monitoring IoT products. To develop
    the decision-making system, the authors propose an ontology model where the product
    is classified according to its UseCycle and LifeCycle, creating rules to decide
    when the product can be reused or must be changed. In Reference 22, the authors
    propose an extension of the Business Process Model and Notation (BPMN) represented
    by a metamodel. The extension aims to support IoT systems by representing them
    in the BPMN model. The extension includes information related to physical and
    virtual entities, information related to the computer resource, and information
    related to the quality of the devices, like security and data quality. Finally,
    the authors propose a fog/cloud federation architecture using the BPMN extension
    and validate the proposal in two scenarios, a smart autistic child scenario and
    a coronavirus disease scenario. Similar to our approach, the authors model the
    quality of service of the CPSs, defining the security and the data quality among
    other features. However, the model focuses on increasing the interoperability
    between CPSs, modeling other features such as hardware information. In our approach,
    the model is defined to facilitate searching for devices by differentiating similar
    devices with metrics that can be helpful in the search process. As shown, current
    research focuses on defining the quality of the data managed by the CPSs and on
    solving the interoperability problem between CPSs. The aim is to extract and use
    the data managed by devices from different manufacturers. In our proposal, we
    focus on searching for devices instead of searching for information on devices.
    We aim to return a device that the user will use to get information such as the
    temperature and not a value from the device (i.e., its temperature). Therefore,
    the novelty of our approach, compared to the related work, is the use of models
    to define an abstract data schema to give extra information in the search process
    of CPSs based on information such as the security and search quality of the device.
    Related to finding CPSs, current research tries to manage the high volume of data
    produced by the CPSs. For instance, in Reference 23, the authors propose a model
    to improve the performance of collecting, analyzing, and using the data produced
    by IoT devices. Our proposal advances the research of the data space domain by
    the mean of facilitating searching for similar devices and processing large amounts
    of devices. The information of CPSs is extended with contextual information (the
    location) and quality information that can help in the recommendation of CPSs.
    In addition, as our approach works in the domain of the device''s information
    instead of the values that the device produce, the amount of data managed by our
    system is smaller, helping in the performance of the search process. Other approaches
    advance research in the field of data space in the meaning of the communication
    of data from different providers or places and how the data is collected, stored,
    processed, and consumed. In Reference 24, the authors propose an architecture
    that, following the FAIR principles, process and analyze data from different IoT
    sources to create a system capable of searching the information about the energy
    information of Italy. Another paper that focuses on data space is,25 where the
    authors propose an architecture, following the FAIR principles, for integrating
    the data from different providers by applying techniques such as machine learning
    and a federation approach. With this approach, data providers can access the data
    from other providers and create a collaborative and shared environment. Our approach,
    compared to these two research about data space, advances the data space domain
    by proposing a common data model that can be used to compare similar data and
    data from different sources. The CPSs get a score based on information that can
    help the search process. Furthermore, that information can be extended to be able
    to compare the quality of the data from different providers when delegating queries
    to other discovery services. Regarding Artificial Intelligence (AI) techniques
    in modeling CPSs, in Reference 26, the authors propose a Model-Driven solution
    for CPSs with a Machine Learning (ML) approach. The proposed data model for CPSs
    is focused on describing the communication protocols and information managed by
    devices. In addition, ML algorithms are defined to create models for different
    things. Finally, the proposal uses a code generator in ThingML for the data model.
    This code generator creates an API capable of instantiating machine learning models
    for prediction, classification, and clustering, among others. However, the Transformer
    algorithm is not supported directly by the code generator. The main difference
    with our approach is that in Reference 26, the authors focus on generating machine
    learning models through an MDE approach. The main proposal of our approach involves
    the definition of a data model for storing CPSs information in the repository
    of a Discovery Service, extended with a data model for supporting the recommendation
    of CPSs using Deep Learning. Furthermore. our approach, instead of generating
    code, focuses on defining a data model which solves natural language problems
    through a Transformer approach. On top of that, our approach describes the actions
    and events of the CPSs through the extension of the TD. Lastly, for papers using
    the Thing Description to propose an MDE approach, in Reference 27, the authors
    propose an approach for generating Web of Things (WoT) compatible devices using
    MDE techniques. The authors define a metamodel based on the TD from October 2018.
    WoT device codes are also generated using the proposed metamodel, Xtext, and Xtend.
    To finish, the work is extended with concrete and abstract syntax to perform a
    Model-to-Model (M2M) transformation for automatically generating WoT Servients.28
    Other papers that propose a metamodel for the Web of Things are,29 which explores
    a metamodel focused on representing devices as virtual entities that offer services
    and Reference 30, that proposes a metamodel for context information. The comparison
    between our proposal and the related work is shown in Table 1 and exposes that
    five aspects have been considered to determine that our approach includes these
    characteristics. In contrast, the rest of the existing works do not cover or address
    them partially: Interoperability of data model about IoT communications; Code
    generation from an MDE approach; FAIR principles applied in data records; Search
    information for the data model or improve the search process; and Artificial Intelligence
    techniques. TABLE 1. Summary of related work ( included, not included). Research
    work Interoperable Code generation FAIR principles Search information Artificial
    intelligence Berrouyne et al. (2022) 16 Fitz et al. (2019) 17 Molano et al. (2017)
    18 Costa et al. (2019) 19 Fortino et al. (2015) 20 Mboli et al. (2022) 21 Kallel
    et al. (2021) 22 Andrade et al. (2023) 23 Rucco et al. (2022) 24 Farahani et al.
    (2023) 25 Moin et al. (2022) 26 Iglesias-Urkia et al. (2020) 28 Ruppen et al.
    (2013) 29 Terdjimi et al. (2016) 30 (Our proposal) As shown, our approach combines
    the interoperability of TD with the search information proposed in the data model
    and the AI data model for applying Deep Learning. The proposal of Reference 28
    extends the TD but focuses on automatically generating WoT Servients; unlike us,
    the data model is not their main proposal. Regarding the Deep Learning (DL) approach,
    in Reference 26, the authors combine AI and data models. However, the AI approach
    in Reference 26 is used to generate machine learning models automatically. In
    contrast, our proposal focuses on the data model used for the Transformer algorithm
    to support DL models for recommending CPSs. Furthermore, the Transformer approach
    is used to solve natural language problems for CPSs, where a user sends a query,
    and the Transformer has to match the user query with devices or services, focusing
    on the search information in the data model. 2.2 Background The aim of the data
    model is to define, in an abstract way, the information that describes a device,
    facilitating communication between devices from different providers and the discovery
    of devices. For the definition of the data model, the Thing Description (TD) of
    the Web of Things (WoT) is used as a basis for the proposed data model. In addition,
    the FAIR Guiding Principles are followed to ensure the data model is Findable,
    Accessible, Interoperable and Reusable. Finally, Artificial Intelligence techniques
    are used to create a Deep Learning model that matches natural language queries
    with Cyber-Physical Systems using the defined data model. 2.2.1 Thing description
    The TD document is a JSON-LD template that describes the basic information about
    the device.31 The TD was created with the idea of facilitating communication between
    devices using WoT technology. The Web of Things is an initiative that was created
    in 2010 to adapt application layer technology to IoT.32 With WoT, devices are
    described through the web in a common way, improving the interoperability and
    heterogeneity of devices. The TD is modeled, structured and based on the TD Information
    Model,33 a model that defines the semantics of the vocabularies of the TD by representing
    them as classes and associations between classes. The TD Information Model is
    divided into four blocks: TD core vocabulary: Contains the basic information about
    the device and describes how it interacts with a thing by defining its properties,
    actions and events. Data schema vocabulary: Defines the type of data used by the
    TD core vocabulary. WoT security vocabulary: Contains information about the security
    mechanisms used by the device. Hypermedia controls vocabulary: Defines the hypermedia
    controls used by the TD core vocabulary for interacting with the thing. 2.2.2
    FAIR principles The FAIR principles (Findable, Accessible, Interoperable and Reusable)
    define the data model to ensure the data''s transparency, reproducibility and
    reusability. FAIR principles are guidelines used to make the data findable and
    reusable by machines and individuals.12 To apply the FAIR principles in a data
    schema, the data model used by the data schema must comply with the four levels
    of the FAIR principles. For each level, a set of requirements are established
    through a questionnaire.34, 35 The four levels of the FAIR principles are as follows:
    Findable: Findable is the first level of the FAIR principles. Before using the
    available data, the data must be discovered automatically. To make the data findable,
    it must use machine-readable metadata. Accessible: Accessible is the second level
    of the FAIR principles. After the data is discovered, it must be accessible, through
    different protocols, for all the entities that want to access the data. Furthermore,
    the data must have authentication and authorization to ensure data security. Interoperable:
    Interoperable is the third level of the FAIR principles. Once the data is accessible,
    it must use a common vocabulary understood by other systems. Reusable: The last
    level of the FAIR principles is reusability. The data and metadata must use licenses
    and schemas that ensure data replication in different settings. 2.2.3 AI techniques
    The selection of an Artificial Intelligence (AI) model is necessary to define
    the data model for the Deep Learning approach. The AI model used is Transformer,
    a novel sequence transduction model based on multi-head self-attention.13 The
    reason for using Transformer and not any other AI model is because Transformer
    is a novel approach that delivers better results in Deep Learning approaches,
    especially when solving natural language problems, as it considers all the possible
    relations between words of a sequence, paying attention to them in parallel. In
    translation tasks, it performs better than convolution and recurrent-based solutions.36
    As the input of the Transformer algorithm is an embedding, the training process
    requires an embedding layer before applying the Transformer algorithm. The embedding
    layer compact the vectors that represent the user''s sentences, transforming the
    high-dimensional vectors into a low-dimensional space. Therefore, the sentences
    have to be transformed into vectors before being used in the training process.
    The technique used to transform categorical data into numerical data to make the
    data usable by machine learning algorithms, in this case, transforming words into
    vectors, is called encoding. The AI model used in this paper for analyzing the
    proposed data model uses hash encoding to transform the words from each sentence
    into a numerical value based on a defined size of the vocabulary. Finally, after
    the sentences are transformed into vectors and before applying the embedding layer,
    the vectors have to be normalized by using padding techniques, transforming all
    the vectors into vectors of the same size. Further information about the main
    features needed to apply this kind of algorithm is explained in a previous work,
    from which the AI model used for analyzing the proposed data model in this paper
    is extracted.14 3 DATA MODELS Cyber-Physical Systems using Web of Things (WoT)
    technology have a data model for defining the data schema of devices. This data
    model called the Thing Description Information Model defines the basic information
    of the device, the available interactions and the security configuration. This
    data model is helpful for a Discovery Service as it allows it to store devices
    in a repository using a standard data schema. The information stored in the repository
    is returned to entities searching for a specific device set. However, regarding
    information related to the search process, the location and the availability of
    devices are missing. When searching for devices using the Thing Description as
    the data schema, a query is sent to the Discovery Service using the information
    in the Thing Description, and all the devices that match the query are returned.
    For instance, if we have a set of CPSs deployed in a Smart Home and want to discover
    the movement sensors, the discovery service will return the Thing Description
    of all the movement sensors deployed in the house. However, in a real scenario,
    it is common for a device to be broken or run out of battery. Furthermore, the
    user may want a device deployed in a specific location, for instance, the camera
    in the child''s room, and not all the available cameras. In addition, some devices
    with the same functionality may be better than others, for instance, two coffee
    machines where one is older and everyone uses the newest one. When we say that
    one device is better than another, we mean that for most external users, the first
    device is useful or better suits their request. Other information that can be
    used to compare two devices can be security and performance. For these problems,
    when searching for devices, the current data model for Cyber-Physical Systems
    requires more information related to recommending and searching among a set of
    devices. As information such as location, availability and information related
    to the search process is important for searching CPSs, we propose a data model
    based on the Thing Description of the WoT for storing information fetched by a
    Discovery Service. With the proposed data model, we ensure that past, current
    and future CPSs can communicate with each other and be discovered by external
    entities. Furthermore, the search process for these devices is improved by including
    information in the data model related to the quality of the devices. Figure 1
    represents the metamodel used for the proposed data model, which uses the Thing
    class and the Security class provided by the Thing Description Information Model.
    The Thing class is extended with the location of the device, its availability
    and the associated confidence level. Apart from that, the Security class is used
    with the information related to devising quality to calculate the confidence level
    associated with the device. FIGURE 1 Open in figure viewer PowerPoint Metamodel
    for describing both DSLs (transformer and CPS). The proposed data model extends
    the Thing Description model by making it easy for a Discovery Service to find
    devices. We use the location for finding devices regarding their position and
    Time-To-Live (TTL) to check the availability of the device, complying with one
    of the sections of the Accessibility of the FAIR principles: the subsection A2,
    discussed in Section 17 (i.e., “Which metadata longevity plan do you use?”). If
    a device is unreachable, the proposed data model returns the metadata associated
    with the device. The device is classified as reachable, unreachable or unknown
    in the metadata returned. In-depth details of this classification can be found
    in Section 9. Finally, we use a confidence level to return a list that best suits
    the user requirements, using quality information to compare similar devices. Using
    a confidence level, the Discovery Service can return a list of recommended devices,
    where similar devices that match the user query are sorted based on their security
    and quality. The proposed data model allows CPSs to be searched using queries
    sent by the user. The Discovery Service searches for the CPSs, looking at the
    information stored in the repository following the data model. However, the system
    cannot understand natural language sentences. To be able to use natural language,
    a data model based on our approach is presented to apply a Deep Learning technique,
    for example, a Transformer that takes natural language sentences as input to make
    queries. In Figure 1, the metamodel used for the proposed data model to apply
    Deep Learning is also represented, where a subset of the CPS data model is used
    to be able to train the Transformer. As some datasets do not represent natural
    language sentences, we propose a data model that ensures that the Deep Learning
    model has enough information to be trained to recommend CPS, containing information
    to build natural language sentences. It is represented as another data model because
    Deep Learning services require the information more reduced than the one used
    for representing CPSs. For instance, the quality and security of the device are
    represented in a single attribute, the Confidence rating. Regarding the rest of
    the classes and attributes from the data model for applying Deep Learning, the
    attribute title is used from the Location class to be able to recommend CPSs according
    to the place they are deployed; attribute ttlStatus from the TimeToLive class
    is used to differentiate between available and unavailable CPSs. For instance,
    a sentence for Deep Learning services using ttlStatus would be “I want to see
    all the available cameras”. The id attribute from the Thing class is used to define
    the device that is involved in the operation, the type of operation is defined
    by the InteractionAffordance (property, action or event), and the operation is
    defined by the DataSchema. Finally, as the Transformer approach matches natural
    language sentences with CPSs, it cannot rank the result list using device quality
    and confidential information. The rating attribute from the Confidence class is
    therefore used by the Deep Learning model as a value to support the decision of
    the Deep Learning model. For instance, if the user wants a washing machine and
    the Deep Learning model returns two washing machines, one disconnected and another
    connected, the confidence rating will be able to recommend the connected washing
    machine over the disconnected one. In-depth details can be found in Section 9.
    Both proposed models follow the FAIR Principles to ensure the Findability, Accessibility,
    Interoperability and Reusability of CPSs. In-depth details are described in Section
    14. 4 DEFINING THE DATA MODELS This section describes both data models in detail.
    Each of the proposed classes of the data model is defined, and the utility of
    using each of the classes is explained. Furthermore, in the analysis section,
    an example of each class is presented, explaining the usage of the proposed classes
    in real situations. 4.1 Location IoT devices are dynamic devices, that is, the
    location and information of the device may change over time. Furthermore, the
    same devices may be stored in a repository, with the difference that one device
    is deployed in a building in London and the other is deployed on New York''s streets.
    Thus, the location must distinguish devices with the same functionality from the
    same provider but deployed in different places. The proposed Location class uses
    a title or name to identify the device''s location. In addition, geolocation information
    can be used to locate the device''s position accurately. The name of the device''s
    location is called the title to follow the nomenclature of the Thing Description.
    Some devices do not have geolocation information, meaning geolocation is not required
    when using location information. For instance, we have an ESP32 with a temperature
    and humidity sensor deployed in a building. We need to find the temperature sensor
    in the meeting room on the second floor. Despite not having its geolocation, using
    the class proposed, the device can be found by its title attribute, where the
    title could be “Meeting room on the second floor”. Therefore, this class supports
    devices with and without geolocation information to find devices deployed in different
    locations. For representing the device''s geolocation, we decided to use Geodetic
    coordinates (latitude, longitude and height) over Cartesian coordinates as they
    are more commonly used for finding devices in the world.37 4.2 TimeToLive For
    IoT devices, it is common not to have a continuous connection.38 As explained
    in the previous subsection, IoT devices are dynamic, meaning they can move between
    different locations. Hence the connection can be lost. Furthermore, some devices,
    called sleeping devices, deactivate features to reduce energy consumption and
    limit incoming connections. In our data model, we propose using a class to represent
    the connection status of the devices. The TimeToLive class proposed uses an attribute,
    lastTimeChecked, to monitor the last time the device connection was checked. This
    attribute allows the Discovery Service to establish a timer to check the availability
    status of the device. Another attribute implemented in the TimeToLive class is
    lastTimeAvailable to represent the last time the device was available. This attribute
    helps differentiate between an unused device and a device that temporarily loses
    its connection for maintenance due to a reduction in energy consumption or any
    other temporary reason. However, to measure the quality of devices to improve
    the search process, other attributes related to the lastTimeAvailable attribute
    are proposed in the SearchQuality class. TimeToLive is not included in the Quality
    class because the current availability status of a device is not related to quality.
    For instance, a smart bike or a smart scooter traveling in a city may temporarily
    lose connection. That temporary loss of connection can modify the response of
    the query sent to the Discovery Service, but the quality of the device will remain
    at the same value. However, a situation where a device loses its connection for
    an extended period affects the quality of the device through a set of attributes
    related to the search quality. For instance, losing the connection for a long
    period will reduce the number of times the device is accessed, lowering the device''s
    position in the returned list when recommended to the user. In-depth details about
    the SearchQuality class are described in the following subsection. Finally, the
    ttlStatus defines the current connection status of the device. The different types
    of connection statuses are: Reachable: Defines a status where external entities
    can connect to the device and receive its information. Unreachable: Represents
    a status where external entities can''t start a connection with the device. Unknown:
    Represents a status where external entities can connect to the device, but external
    entities can''t communicate with it. For instance, a device that is reachable
    but doesn''t return its information. For the implementation of TimeToLive, the
    Discovery Service checks the status of every CPS by sending a request. The request
    sent by the Discovery Service can be configured to change the temporizer used.
    Furthermore, a maximum number of tries can be configured to avoid indefinitely
    sending requests to a lost device. 4.3 Confidence When searching, we, as humans,
    use a set of information to rank the list of products that the system returns.
    The information used to rank the product list differs depending on what we are
    searching for. However, there is a set of information used for every search: the
    product''s confidence. This confidence is used to differentiate two products with
    the same characteristics, even if we are not searching for them. For instance,
    we are searching for a camera that brings back two cameras and two movement sensors,
    where one of the cameras allows five concurrent connections and the other one
    only has one concurrent connection. One of the movement sensors is deployed without
    security systems, while the other has a username and password. When we check the
    information, we rank the cameras and movement sensors. The camera with more than
    one concurrent connection will be our first option. Between the movement sensors,
    the one with security systems will be the first choice between the movement sensors
    and the third option among all the returned devices. Therefore, we rank every
    result returned using the search query, that is, the information we are looking
    for, and the confidence we have in each product. In the Confidence class, we formalize
    the confidence when we search for products but in a specific search for CPSs.
    With this formalization, machines can differentiate between devices with the same
    characteristics using the confidence that we, as humans, use when searching for
    products. This formalization is required for large-scale IoT environments because
    a machine is needed to manage large amounts of data, and we cannot review each
    of the available devices to rank them in our search. In the Confidence class,
    rating is used to provide a confidence value to the device. In addition, Confidence
    is composed of Quality and Security. Security refers to the Security class of
    the Thing Description, a class that defines the possible security configurations
    that a device may have. The rating used by the Confidence class is calculated
    using Equation (1). The confidence rating calculated in Equation (1) is for the
    individual equation of a device, . In the equation, refers to the quality rating
    of the device , and refers to the security configuration used by the device. Table
    2 shows the rating of each of the security configurations, where no configuration
    has the lowest rating, 0, and PSK, OAuth2, Bearer, and API have the highest rating,
    100. TABLE 2. Ratings for each security configuration. S Security Rating No Security
    0 Basic 25 Digest 50 API 100 Bearer 100 PSK 100 OAuth2 100 The rating value is
    calculated with a maximum value of 100, with of the weight for the quality of
    the device and of the weight for the security of the device. For instance, a device
    with a quality rating of 100 and no security systems will have a confidence rating
    of 25. (1) Another equation used is shown in (2). This equation is used for calculating
    the global confidence of the Discovery Service, . Global confidence may be used
    in the future with the security configuration of the Discovery Service to search
    for Discovery Services in a Federation of Discovery Services. (2) Like the Confidence
    class, the Quality class has a rating for giving a quality value to the device.
    Quality is composed of DeviceQuality and SearchQuality. The first one, DeviceQuality,
    defines attributes related to the quality of the device. In contrast, concurrentConnections
    represents the connections that can be opened simultaneously by the device and
    responseTime defines how long it takes for the device to answer the requests.
    These attributes help to define the quality of the device and to be able to differentiate
    between devices with the same functionalities. For instance, a weather station
    with a responseTime of 10 ms has higher quality than a weather station with a
    responseTime of 200 ms. For the implementation of responseTime and concurrentConnections,
    the Discovery Service connects to the devices to complete these attributes. responseTime
    and concurrentConnections are obtained when Discovery Services checks for the
    device availability to complete the ttlStatus attribute. For the calculation of
    concurrentConnections, the Discovery Service opens concurrent connections to the
    device until the connection fails or reaches a number of 25 concurrent connections.
    However, the calculation of concurrentConnections entails two problems: (a) an
    external entity may use the device while the Discovery Service is trying to test
    the capabilities of the device, and (b) the operation of opening concurrent connections
    may drain too much energy from the device. To solve the last issue, an independent
    method from the ttlStatus can be implemented to check for concurrentConnections
    less often and to solve the first issue, the highest number obtained when getting
    the concurrentConnections can be stored instead of replacing the value every time
    concurrentConnections are obtained. SearchQuality class defines attributes related
    to the quality of searching for a specific device. The attributes included in
    this class are: (a) lastTimeAccesed for storing the last time the device information
    was accessed using the Discovery Service, (b) indivAccess to store the number
    of times the device information was returned individually by the Discovery Service,
    that is, no more devices were returned in the search process; (c) groupAccess
    for storing the number of times the device information was returned by the Discovery
    Service in a query that contained other devices, and (d) accessedTimes to store
    the number of times the device information was accessed using the Discovery Service.
    To determine the quality of the device, we look at the device''s capabilities
    and the usefulness or popularity of the device, that is, the number of times the
    device is requested. As the last step, this information is represented as a number
    that defines overall quality, combined with the security information of the device
    to define its confidence rate. When external entities perform queries, the attributes
    of SearchQuality are obtained. The attribute lastTimeAccessed is a date that changes
    to the current date each time the device is returned as a result of a query. The
    attribute accessedTimes is calculated as the attribute lastTimeAccessed with the
    difference that instead of saving a date, an integer is increased by one when
    the device is returned as a result of a query. The attribute indivAccess only
    increases when the device is the single result in a query, and the attribute groupAccess
    increases when the device is returned with other devices. Finally, the rating
    attribute of the Quality class is calculated following the Equation (3), where
    is the search quality of the device and is the device quality . The values of
    0.6 and 0.4 are the weights of the device quality and the search quality, respectively.
    As quality is used for searching for devices, search quality has more weight in
    the search process than device quality. (3) To obtain the qualities that fall
    in line with the overall quality of the device, Equations (4) and (14) are used.
    Equation (4) calculates a value for rating the SearchQuality class to be used
    in (3). (4) In (4), the weights are distributed, placing more importance on individual
    access and the last time the device was accessed. The number of times the device
    is accessed and the number of times it is returned in a list with other devices
    equals a lower value due to it already being represented in the attribute indivAccess.
    The lastTimeAccessed attribute is represented as a range of values, where the
    highest value is for devices that were accessed in the last 24 h and the lowest
    value is for devices that were accessed more than 10 days ago. Table 3 shows the
    rating values for each range of days. TABLE 3. Ratings for the last day the device
    was accessed. lta Range Rating 10 0 (5,10] 25 (3,5] 50 [1,3] 75 1 100 To represent
    the attribute accessedTimes, (5) is used as the quartile of the total sum of the
    accessedTimes of every device, divided by the total number of devices ( ) (7).
    In mathematical terms, as we are dividing the data into five ranges, the quartile
    that we define is not a mathematical quartile, so it will be represented by (8),
    where is the union of every possible range (9). In Equation (9), represents the
    calculated range, where n has a maximum value of 4; and represents the maximum
    value that our range of values has. For instance, if we are calculating for and
    we have an accessMean value of 80 ( ) (6), will return values between 20 and 40,
    . (5) (6) (7) (8) (9) With Equation (7) we represent the mean number of times
    each device is accessed. Therefore, as shown in Table 4, using the proposed ,
    devices are ranked into five categories, where devices in are devices with the
    number of times accessed higher or equal to the mean number of times each device
    is accessed. For these ranges, we give more weight to devices that are accessed
    more times than expected ( ) and less weight to devices that are accessed fewer
    times than expected ( ). TABLE 4. Ratings for each subset of . Range Rating [0%,25%)
    0 [25%,50%) 25 [50%,75%) 50 [75%,100%) 75 100% 100 The attribute indivAccess is
    represented using , where is calculated as the of of the accessedTimes (10). Therefore,
    in this case, the variable is the of the accessedTimes instead of the mean value
    of the accessedTimes (11). The groupAccess, represented using , is calculated
    as the of of the accessedTimes (12). As with the attribute , the value of is modified
    to be the of the accessedTimes instead of the mean value of the accessedTimes
    (13). With these equations, we are defining the maximum value of indivAccess as
    of the total number of times the device is accessed, and groupAccess as of the
    same, thus giving more weight to individual accesses than to group accesses. (10)
    (11) (12) (13) Each subset of used for calculating the different ratings has a
    value assigned to the highest with a maximum value of 100 ( ) and the lowest with
    a minimum value of 0 ( ). These values are represented in Table 4. Equation (14)
    calculates the value for rating DeviceQuality to be used in (3). As the response
    time of a device is more important than allowing concurrent connections, the attribute
    responseTime is given more weight than concurrentConnections. The attribute responseTime
    is represented as , where is the response time of the device . Equation (15) represents
    the calculation of . In Equation (15) a rating is calculated by measuring the
    response time of the device in milliseconds, considering 5000 ms as the highest
    time we allow to give the device a rating above 0. For instance, a device with
    a response time of 78 ms will return a rating of 98, while a device with a response
    time of 7000 ms will return a rating of 0. (14) (15) Finally, attribute concurrentConnections
    is defined as , where represents the range of concurrent connections the device
    supports. Table 5 describes each defined range, where depending on the number
    of concurrent connections the device supports, gets a certain rating value. TABLE
    5. Ratings for concurrent connections. Con Range Rating 1 0 (1,2] 10 (2,5] 25
    (5,10] 50 (10,15] 70 (15,20] 90 100 4.4 Transformer As explained in the previous
    section, the data model proposed for the Transformer approach aims to support
    natural language queries in the search process. The proposed data model is used
    to improve a Deep Learning model from a previous work, where Transformer was used
    to recommend CPSs using natural language queries.14 For this to happen, classes
    Location, TimeToLive, DataSchema and InteractionAffordance are used to build natural
    language sentences for datasets that don''t contain sentences in the form of natural
    language. Furthermore, Confidence is used to support the Transformer. When creating
    a Deep Learning model, it is essential to preprocess the information correctly
    for producing the training dataset. Using a data model to preprocess the information
    helps automate and simplify the preprocessing step. Furthermore, if the data model
    is prepared for training Deep Learning models to solve a specific problem, it
    can also help create Deep Learning models with good results. In this proposal,
    the data model is prepared for training Deep Learning models for recommending
    Web of Things devices or services, matching them with user queries in natural
    language sentences. The Deep Learning model used for analyzing the proposal of
    this paper was trained using a set of observations from a Smart Home scenario.14
    As not all the devices from the dataset have a natural language sentence associated,
    the attributes from the proposed data model are merged to build a sentence that
    can help in the training process (16). The sentence represents the place where
    the device is deployed (title from Location) if the device has to be reachable
    (ttlStatus from TimeToLive), the operation that we want to perform (DataSchema)
    and the kind of operation that we want (InteractionAffordance). The class TimeToLive
    is only included in the sentence when the CPS is reachable, as this class is only
    useful for finding available devices. When using TimeToLive as a filter, Confidence
    class is used to support the decision of the Deep Learning model, which includes
    attributes that represent the class TimeToLive. An example of a sentence is defined
    in (17). (16) (17) After the sentences are generated, they are transformed into
    vectors using hash encoding and normalized using padding techniques to transform
    all the vectors into vectors of the same size. The created vectors are used for
    training the Deep Learning model by applying an embedding layer and the Transformer
    algorithm, using a Softmax as an activation function to represent each possible
    CPS that can match the user''s sentence. Finally, the Confidence class is used
    as a support system for the Deep Learning model, which is able to recommend a
    set of devices from a natural language sentence. However, it is not able to distinguish
    devices using quality and security configurations. The Confidence class improves
    the Deep Learning model result by providing knowledge about the quality and security
    of each device. Consequently, with the use of both data models and Deep Learning
    through a Transformer approach, the system is able to match natural language sentences
    with CPSs and recommend them using the confidential information of each CPS. 5
    ANALYSIS OF THE RESULTS After defining both data models and describing each of
    the elements that comply with the proposed data models, both data models are analyzed
    using two analysis scenarios. The first one focuses on describing how a Discovery
    Service adapts the existing devices using the Thing Description to our proposal
    and how that information is stored in a repository. The second one then describes
    how a Transformer approach for matching user queries with Web of Things devices
    (or devices) uses the proposed data model to return a list of recommended devices.
    5.1 Data model for discovery services Figure 2 shows the analysis scenario for
    the data model proposed to store CPSs information fetched by a Discovery Service.
    In the analysis scenario, the Discovery Service is deployed in a Smart Home. FIGURE
    2 Open in figure viewer PowerPoint Analysis scenario in a smart home. CPSs with
    10 devices deployed and a discovery service to register the deployed CPSs. Kitchen
    (light, movement sensor and washing machine). Bathroom (light and movement sensor).
    Hall (movement sensor). Parents'' bedroom (light and movement sensor). Children''s
    bedroom (light and movement sensor). Office (discovery service). The CPS devices
    in the Smart Home are (a) a washing machine in the kitchen, (b) one light in the
    parents'' bedroom, in the children''s bedroom, in the kitchen and in the bathroom;
    (c) movement sensors in the same rooms as the lights deployed, and (d) a lock
    on the entrance door. For the analysis scenario, the CPSs deployed require a Thing
    Description. Using the Thing Description, the Discovery Service will extend it
    with the proposed data model. For devices that don''t have a Thing Description,
    two solutions are used: (a) using middleware to adapt the device to the Web of
    Things, thus generating a TD;39 or (b) automatically creating the TD of devices
    that uses MQTT communication protocol.40 After each device has a Thing Description
    attached, the TDs are stored in the repository. Listing 1 shows an example of
    a Thing Description for one of the deployed lights. Finally, the stored TD is
    extended by the Discovery Service using the proposed data model. To analyze the
    data model, each deployed CPS device has a different configuration, hence having
    a different confidence level. Table 6 shows the configuration of each deployed
    CPS device in the Smart Home and the confidence and quality rating given by the
    following Discovery Service Equations (1) and (3). TABLE 6. Configuration of the
    CPSs deployed in the analysis scenario (D: Device, T: Title, LA: lastTimeAvailable,
    TS: ttlStatus, RT: Response time, CC: Concurrent connections, S: Security, QR:
    Quality rating, CR: Confidence rating, AT: Accessed Times, IA: Indiv. Access,
    G: Group Access, LD: lastTime Accessed). D T LA TS RT CC S QR CR AT IA G LD Light
    Bed. Child. Bedroom Children 2022-5-5 11:40 Reachable 70 1 No 82 21 52 40 12 2022-5-5
    11:50 Light Bed. Par. Bedroom Parents 2022-5-5 11:40 Reachable 100 1 API 44 86
    10 0 10 2022-5-14 8:20 Light Kitchen Kitchen 2022-4-20 7:30 Unreachable 80 1 API
    61 90 20 15 5 2022-4-22 16:42 Light Bathroom Bathroom 2022-5-5 11:40 Reachable
    170 1 No 78 20 122 100 12 2022-5-5 12:03 Move. Bed. Child. Bedroom Children 2022-5-5
    11:40 Reachable 124 4 API 83 96 344 302 42 2022-5-5 8:12 Move. Bed. Par. Bedroom
    Parents 2022-5-5 11:40 Reachable 61 2 No 79 20 123 81 42 2022-5-5 8:00 Move. Kitchen
    Kitchen 2022-3-20 16:12 Unreachable 94 2 API 83 96 367 325 42 2022-5-5 7:45 Move.
    Bathroom Bathroom 2022-5-5 11:40 Reachable 2031 10 Digest 69 55 681 639 42 2022-5-5
    8:11 Move. Hall Hall 2022-5-5 11:40 Unknown 1012 19 Basic 75 38 1587 1545 42 2022-5-5
    8:14 Washing M. Kitchen 2022-5-5 11:40 Reachable 160 12 API 77 94 10 7 3 2022-5-5
    12:06 Door lock Entr. Entrance 2022-5-5 11:40 Reachable 32 1 API 72 93 14 8 6
    2022-5-5 8:14 After each TD has been extended by the Discovery Service, it is
    able to use the proposed data model for performing the user queries. For instance,
    a query searching for lights would return in the first position, despite being
    unreachable, the light deployed in the parents'' bedroom due to having a better
    confidence level. In order to return CPSs that are reachable, users can perform
    queries with conditions that exclude unreachable levels. In the next subsection,
    the analysis scenario for the Transformer approach is presented, in which a recommender
    system uses the proposed data model to recommend a list of CPS devices. In this
    list, values such as the CPSs connection status are used. Accordingly, the list
    of deployed lights may have a different order than that returned by the Discovery
    Service. Listing 1. TD of the light located in a bedroom. 5.2 Data model for transformer
    For the Transformer approach, the same scenario as the previous subsection is
    used. The Discovery Service searches for CPS devices through user queries, and
    together with Deep Learning, it is able to recommend devices through a Transformer
    approach. Figure 3 shows the interaction between the Discovery Service and the
    service that creates and uses the proposed Deep Learning model using the Transformer
    algorithm. The creation of the Deep Learning model needs a CPSs data history dataset.
    The used data set must employ the data model proposed. Our analysis scenario has
    a repository where the Discovery Service stores all the accesses to the CPSs using
    the proposed data model. This dataset is used by the Deep Learning service to
    create the model using the Transformer algorithm. After the Deep Learning service
    gets the dataset, it is analyzed using an external service. In our analysis scenario,
    the external service is deployed in the Discovery Service. The analysis of the
    dataset checks that the dataset sent to the Deep Learning service uses the data
    model proposed. In our example, the Discovery Service sends the dataset, ensuring
    it uses the data model. However, the dataset may be sent by another entity. FIGURE
    3 Open in figure viewer PowerPoint Data model analysis for the deep learning approach.
    Table 7 shows an example of a dataset sent to the Deep Learning service, where
    the classes that comprise the data model for the Transformer approach are represented.
    The Device column defines the id of the device, and the Service column represents
    the service executed in the operation. Furthermore, the Sentence column is empty
    for some devices, thus forcing the Deep Learning service to fill those empty columns.
    For instance, the second row of Table 7, following Equation (16), will be completed
    as I need available action washing in kitchen. With this solution, we ensure that
    datasets not containing natural language sentences can be used by the Deep Learning
    model. TABLE 7. Analysis scenario example dataset (DV: Device; LC: Location; TS:
    ttlStatus; CF: Confidence; IA: Interaction Affordance; SV: Service; ST: Sentence).
    DV LC TS CF IA SV ST lightcontrol1 BedroomChildren Reachable 82 Action /lightcontrol1/
    lighton Turn on the light in the children bedroom washingmachine1 Kitchen Reachable
    94 Action /washingmachine3/ washing — lightcontrol3 BedroomParents Reachable 86
    Action lightcontrol3/ lighton — lightControl1 BedroomChildren Reachable 82 Action
    /lightcontrol1/ lighton — movement5 Kitchen Unreachable 96 Property /movement5/
    movement Is anyone in the kitchen? movement3 Bathroom Reachable 55 Property /movement3/
    lastChange — After the dataset is analyzed, the Deep Learning model using the
    Transformer algorithm is created. To be able to use the created Deep Learning
    model, an external entity has to send a query to the Deep Learning service, which,
    using the created model, returns a list of recommended devices sorted by the confidence
    level of the model. For instance, in the analysis scenario, the sentence Did the
    movement sensor read any movement?, returns all the available movement sensors,
    where the first option in the returned list is the device with the highest confidence
    level, that is, the device that best suits the user''s request. With both analysis
    scenarios, the novelty of the solution has been demonstrated. CPSs from different
    providers are supported by using the proposed data models. Furthermore, the CPSs
    search process is improved and even supports natural language queries by a recommender
    system using Transformer. 5.3 FAIR principles For the definition of the data model,
    the FAIR principles are followed to make the data Findable, Accessible, Interoperable
    and Reusable. This section describes the justification of complying with all the
    levels of the FAIR principles.34, 35 As the proposal tries to follow the four
    levels of the FAIR principles, we cannot ensure complete compliance. However,
    validating our data model using the FAIR principles helps us find the limits of
    our proposal and propose future work that would help us improve the data model,
    thus making the proposal more findable, accessible, interoperable and reusable.
    In our data model, metadata and dataset are represented similarly. Therefore,
    questions regarding metadata and dataset are answered as the same question, for
    example, the question F1. In the following, we comply with most of the questions
    from the four levels, even with questions not related to the data model. For the
    questions we don''t comply with, all related to the Discovery Service, we intend
    to improve the Discovery Service to comply with all the questions from the four
    levels of the FAIR principles. 5.3.1 Findable F1. What globally unique, persistent,
    resolvable identifiers do you use for metadata records or datasets? The attribute
    id from the class Thing of the Thing Description is used to identify the metadata.
    The identifier is represented in the form of a URI following the RFC standard
    RFC3986.41 F2. Which metadata schemas do you use for findability? The data model
    proposed aims to improve the findability of CPSs by including information regarding
    the location of the deployed device, its connection status and information about
    the quality of the device. F3. What is the technology that links the persistent
    identifiers of your data to the metadata description? To store the metadata, JSON-LD
    is used, whereas namespaces are used for integrating or linking the identifiers
    with the description. F4. In which search engines are your metadata records or
    datasets indexed? Although this question is out of the scope of our proposal,
    as we propose a data model and not the Discovery Service, we are working on a
    solution that complies with this question. Currently, the metadata is not indexed
    in a search engine but in a Discovery Service. However, in future work, we intend
    to improve the Discovery Service to support a Federation of Discovery Services,
    thus linking our Discovery Service with other Discovery Services or vice-versa.
    5.3.2 Accessible A1.1. Which standardized communication protocol do you use for
    metadata records or datasets? To discover the CPSs stored in the repository, a
    RESTful API is used with HTTP communication protocol. A1.2. Which authentication
    & authorization techniques do you use for metadata records or datasets? Although
    this question is out of the scope of this work. In this paper, we propose a data
    model, not the Discovery Service; we are working on a solution that complies with
    this question. Currently, the Discovery Service used does not support authentication
    or authorization. However, in future work, we intend to improve the Discovery
    Service to support authentication and authorization to limit access to some of
    the stored information. A2. Which metadata longevity plan do you use? For the
    metadata records, the date of creation and the last update is stored using the
    Thing class of the Thing Description. Furthermore, with our proposal, the status
    of the CPSs is stored, allowing access to the CPSs even without a connection.
    This is not currently being carried out regarding the deletion of the metadata.
    However, using the quality information described in the metamodel, data not used
    is less recommended in the search process, thus reducing the relevance of disconnected
    CPSs. 5.3.3 Interoperable I1. Which Knowledge Representation Languages (allowing
    Machine Interoperation) Do You Use For Metadata Records Or Datasets? To represent
    the information stored in the repository, JSON-LD is used. I2. Which structured
    vocabularies do you use to annotate your metadata records or datasets? The vocabularies
    of the CPSs ontologies are used in the metadata. Furthermore, we intend to propose
    a common ontology for future work that supports all the available CPSs ontologies.
    I3. Which models and schema(s) do you use for your metadata records or datasets?
    We use the proposed model in this paper and the Thing Description schema for the
    metadata records and datasets. 5.3.4 Reusable R1.1. Which usage license do you
    use for your metadata records or datasets? As the stored data is open data: anyone
    can use it, and a copy version of the data can be modified; we use the MIT license
    for the proposed data model. R1.2. Which metadata schemas do you use for describing
    the provenance of your metadata records or datasets? For the description of the
    provenance or maintainer of the metadata, the attribute support of the TD is used.
    5.4 Threats to validity To validate our proposal, we answer the four main validity
    threads discussed in the literature:42 Conclusion, Internal, Construct, and External
    validity. This ensures detection of the objectives are fulfilled and the study''s
    limitations. 5.4.1 Conclusion validity. Did the introduced treatment/change have
    a statistically significant effect on the outcome we measure? Yes, the results
    obtained using the proposed DSL differ from those obtained by not using the DSL.
    The order of the returned list of CPSs is modified to give more importance to
    devices with higher quality and security, represented by the confidence level
    (1). However, there is a lack of comparison between the current system without
    the DSLs and the proposed system. This comparison may be made using metrics from
    the literature to evaluate the difference between both approaches. 5.4.2 Internal
    validity. Did the introduced treatment/change cause an effect on the outcome?
    Can other factors also have had an effect? The outcome is altered as we use the
    proposed models to support recommendation systems using Deep Learning. In addition
    to the result of the Deep Learning model, the confidence level is used to sort
    the returned CPSs. However, using the calculated confidence level over the confidence
    given by the Deep Learning model may negatively alter the result. For that reason,
    as the proposed DSL is used to support and improve Deep Learning models by creating
    a common schema of the data, the proposed approach must be studied and compared
    using: The confidence level as the main score to sort the returned list of CPSs.
    Using the confidence level and the value given by the Deep Learning model to calculate
    a new variable used to sort the returned list of CPSs. Using the value given by
    the Deep Learning model as the main value with the confidence level only to support
    the Deep Learning response. Other factors that may have affected are the hyperparameters
    used to train the Deep Learning model. With the use of different hyperparameters,
    the result of the Deep Learning model may change, altering the outcome. This research
    is focused on the metamodel used by the Deep Learning model, not the use of the
    Deep Learning itself. However, the study of the hyperparameters may help replicate
    the experimentation. 5.4.3 Construct validity. Does the treatment correspond to
    the actual cause we are interested in? Does the outcome correspond to the effect
    we are interested in? The aim of the research is: Propose a DSL representing the
    CPS information related to the search and recommendation process. Propose a DSL
    that can be used with different brands of CPSs to be re-used with future CPSs.
    Propose another DSL for Deep Learning recommenders to reduce the workload in the
    preprocessing step regarding the algorithm used and to improve the recommendation
    result using information related to the device''s quality and security. Follow
    the FAIR principles to ensure that the data schema is Findable, Accessible, Interoperable,
    and Reusable, thus fulfilling objective (b). Objectives (a) and (c) are fulfilled
    in Section 8, explained in Section 9, and studied in Section 14; objective (b)
    is fulfilled in Section 14; and objective (d) is studied in Section 14. For objective
    (d), the questions related to the DSLs have been answered affirmatively. However,
    other questions related to other aspects of the proposal were not answered affirmatively.
    Regarding the DSL, the outcome corresponds to what was expected, where similar
    CPSs are returned in a sorted list that depends on the quality of the device.
    In addition, the model is analyzed using an example scenario. However, there is
    a lack of a scenario where the Deep Learning model is analyzed using metrics to
    compare its performance between using the DSL and not using it. Furthermore, it
    would be helpful to expand the study to multiple algorithms and not only the Transformer
    algorithm to validate objective (c). 5.4.4 External validity. Is the cause-and-effect
    relationship we have shown valid in other situations? Can we generalize our results?
    Do the results apply in other contexts? With the use of MDE, we proposed two DSLs
    to generalize the representation of CPSs, regarding the brand or type of CPS.
    Thus, these DSLs are valid in other scenarios and with other CPSs different from
    those used in the analysis of the result. Furthermore, the second DSL is also
    proposed for re-using with different Deep Learning algorithms for creating recommender
    systems. 6 CONCLUSIONS AND FURTHER CONSIDERATIONS This paper proposes a data model
    based on the Thing Description for storing information fetched by a Discovery
    Service. The data model uses the Thing Description to solve the interoperability
    problem in communication between devices and expands it with information related
    to the search process and the quality of devices. In addition, a data model based
    on the previous one is proposed for matching Cyber-Physical Systems, and user
    queries in natural language through a Transformer approach that serves as a matching
    mechanism (or Recommender System). This data model supports the primary data model
    by searching for CPS devices using the Transformer model outcomes. Furthermore,
    the result of the model is adjusted to the device confidence level, improving
    the usefulness and effectiveness of the recommendations. To analyze the first
    data model, a Smart Home scenario was established. A Discovery Service fetched
    the deployed CPSs and completed the information following the proposed data model.
    After the information was fulfilled following the data model, the confidence rating
    of the CPSs was compared to analyze the utility of the data model when representing
    and searching for CPSs. After the first data model was analyzed, the scenario
    was used to analyze the second data model. For the second data model analysis,
    the Deep Learning service used the dataset from the smart home scenario to create
    a Deep Learning model. The used dataset was completed using the proposed data
    model for the Deep Learning approach. Finally, the Deep Learning model was created,
    and the resulting list of recommended CPSs was adjusted using the confidence rating
    of each CPSs. Thus, the posed research questions can be answered positively. Regarding
    the first one, a data model is defined, built, and analyzed, ensuring the interoperability
    and quality of the WoT Discovery Service. Regarding the latter, a data model to
    support Deep Learning-trained models has also been defined, allowing the deployment
    of a Deep Learning model that processes natural language to facilitate the matching
    between queries and devices and services. Regarding the FAIR principles, in the
    analysis section, the four levels were validated by answering the questionnaire
    built for the FAIR implementation. All the questions related to the data model
    have been answered affirmatively. Furthermore, most questions not related to the
    data model have been answered affirmatively. For the questions that were not answered
    affirmatively, we intend to comply with these questions in future implementations
    with the improvement of the Discovery Service used in the analysis scenario. In
    future work, the Deep Learning model using the Transformer approach could be improved
    by proposing a fully functional and described Deep Learning model. In addition,
    as the use of the proposed data model for applying Deep Learning in natural language
    problems has been demonstrated to support the search process, the data model using
    the Transformer algorithm could be compared with the use of the proposed data
    model with other algorithms. For the validation, both data models could be deployed
    in a large-scale IoT scenario to validate the proposal in scenarios with a large
    amount of CPSs. Furthermore, we intend to improve the proposed data model with
    an automatic generation of Thing Descriptions using the defined data model. The
    generated Thing Descriptions would be able to simulate real devices to validate
    real scenarios before deploying the CPSs. Finally, we are researching a federation
    approach of discovery services, where the queries can be delegated to discovery
    services located in different places. However, we need a way of comparing the
    discovery services. For this reason, we intend to use the proposed data model
    to represent the CPSs that populate the discovery services, adapting the CPSs
    that use the Thing Description to the new model. In addition, we intend to extend
    the data model to represent the quality or confidence of each discovery service,
    thus being able to compare the discovery services when delegating the queries.
    AUTHOR CONTRIBUTIONS Juan Alberto Llopis: Conceptualization, Methodology, Writing-original
    draft, Writing-review & editing. Antonio Jesús Fernández-García: Conceptualization,
    Writing-review & editing. Javier Criado: Conceptualization, Methodology, Supervision,
    Writing-review & editing. Luis Iribarne: Conceptualization, Writing-review & editing,
    Funding acquisition. Antonio Corral: Conceptualization, Writing-review & editing.
    ACKNOWLEDGEMENTS Grants PID2021-124124OB-I00 and FPU19/00727 funded by the Spanish
    Government MCIN/AEI/10.13039/501100011033 and the “ERDF A way of making Europe”.
    Grant PY20_00809 (UrbanITA Project) funded by the Andalusian Government. Open
    Research REFERENCES Early View Online Version of Record before inclusion in an
    issue Figures References Related Information Recommended Generic input template
    for cloud simulators: A case study of CloudSim Manar Jammal,  Hassan Hawilo,  Ali
    Kanso,  Abdallah Shami Software: Practice and Experience Synergizing edge computing
    and blockchain for cyber-physical systems Payal Thakur,  Vivek Kumar Sehgal Concurrency
    and Computation: Practice and Experience Can Cyber‐Physical Systems Reliably Collaborate
    within a Blockchain? Ben van Lier Metaphilosophy Kulla‐RIV: A composing model
    with integrity verification for efficient and reliable data processing services
    Hugo G. Reyes-Anastacio,  Jose L. Gonzalez-Compeán,  Victor J. Sosa-Sosa,  Ricardo
    Marcelín-Jiménez,  Miguel Morales-Sandoval Software: Practice and Experience A
    review of the application of virtual and augmented reality in physical and occupational
    therapy Agrawal Luckykumar Dwarkadas,  Viswanath Talasila,  Rama Krishna Challa,  Srinivasa
    K G Software: Practice and Experience Download PDF Additional links ABOUT WILEY
    ONLINE LIBRARY Privacy Policy Terms of Use About Cookies Manage Cookies Accessibility
    Wiley Research DE&I Statement and Publishing Policies Developing World Access
    HELP & SUPPORT Contact Us Training and Support DMCA & Reporting Piracy OPPORTUNITIES
    Subscription Agents Advertisers & Corporate Partners CONNECT WITH WILEY The Wiley
    Network Wiley Press Room Copyright © 1999-2024 John Wiley & Sons, Inc or related
    companies. All rights reserved, including rights for text and data mining and
    training of artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: Software - Practice and Experience
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A data model for enabling deep learning practices on discovery services of
    cyber-physical systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Fernandez E.I.
  - Martinez I.C.
  - Breis J.T.F.
  - Valera A.J.J.
  citation_count: '0'
  description: 'Particulate matter monitoring and climate change mitigation actions
    have been promoted due to the Paris Agreement because of their impact on health
    and high mortality rate. High-resolution networks based on hyperlocal IoT sensors
    can play a fundamental role in improving data quality. In this context, hyperlocal
    refers to air quality IoT systems that allow collecting data in real-time and
    in the cheapest way in comparison with local reference stations. Despite these
    methods are powerful and widely used by the scientific community, the signal is
    highly affected by relative humidity. In this paper, we present a system for measuring
    nanoparticles based on drying the air sampled and avoiding the hygroscopic growing
    of the particles. To the best of our knowledge, this is the first dryer system
    approach developed for IoT hyperlocal sensors. In addition, the relevance of our
    solution is supported by the following points: i) we propose a new dryer system
    that has been patented; ii) our solution can be integrated into an IoT infrastructure
    that allows it to interact with other services and iii) our solution has been
    validated in a real scenario in the city of Madrid. We have observed that the
    integration of a dryer system improves the performance of the OPC-N3 sensor and
    that we can measure the PM10 and PM2.5 fractions with high precision, R2 = 0.83.
    In addition, our solution can measure small particles such as PM1 with a good
    correlation against the reference air quality stations. Thus, our work contributes
    by improving high-spatial-resolution nanoparticle monitoring in correlation to
    official measurements to mitigate climate change.'
  doi: 10.1109/JSEN.2024.3364537
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Sensors Journal >Volume:
    24 Issue: 7 Design and Evaluation of a Dryer System for IoT Hyperlocal Particulate
    Matter Monitoring Device Publisher: IEEE Cite This PDF Eduardo Illueca Fernández;
    Iris Cuevas Martínez; Jesualdo Tomás Fernández Breis; Antonio Jesús Jara Valera
    All Authors 54 Full Text Views Open Access Under a Creative Commons License Abstract
    Document Sections I. Introduction II. State of the Art III. Materials and Methods
    IV. Results V. Discussion Show Full Outline Authors Figures References Keywords
    Metrics Footnotes Abstract: Particulate matter (PM) monitoring and climate change
    mitigation actions have been promoted due to the Paris agreement because of their
    impact on health and high mortality rate. High-resolution networks based on hyperlocal
    Internet of Things (IoT) sensors can play a fundamental role in improving data
    quality. In this context, hyperlocal refers to air quality IoT systems that allow
    collecting data in real time and in the cheapest way in comparison with local
    reference stations. Despite these methods are powerful and widely used by the
    scientific community, the signal is highly affected by relative humidity (RH).
    In this article, we present a system for measuring nanoparticles based on drying
    the air sampled and avoiding the hygroscopic growing of the particles. To the
    best of our knowledge, this is the first dryer system approach developed for IoT
    hyperlocal sensors. In addition, the relevance of our solution is supported by
    the following points: 1) we propose a new dryer system that has been patented;
    2) our solution can be integrated into an IoT infrastructure that allows it to
    interact with other services; and 3) our solution has been validated in a real
    scenario in the city of Madrid. We have observed that the integration of a dryer
    system improves the performance of the OPC-N3 sensor and that we can measure the
    PM10 and PM2.5 fractions with high precision, R 2 =0.83 . In addition, our solution
    can measure small particles, such as PM1, with a good correlation against the
    reference air quality stations. Thus, our work contributes by improving high-spatial-resolution
    nanoparticle monitoring in correlation to official measurements to mitigate climate
    change. Published in: IEEE Sensors Journal ( Volume: 24, Issue: 7, 01 April 2024)
    Page(s): 11152 - 11165 Date of Publication: 15 February 2024 ISSN Information:
    DOI: 10.1109/JSEN.2024.3364537 Publisher: IEEE Funding Agency: SECTION I. Introduction
    One of the key aspects in current digital transformation, and into the challenges
    for the next years, is particulate matter (PM) monitoring as one of the main pollutants
    related to traffic emissions in urban areas [1]. In 2015, PM became one of the
    top five mortality risk factors in the world, with an estimation of 4.2 million
    people premature deaths [2]. In Europe, there are about 300000 deaths per year
    due to PM2.5 and 245000 due to inorganic particles [3]. In addition, particles
    with a high content of heavy metals, such as vanadium and nickel, correlate with
    cardiovascular mortality and morbidity [4]. The maximum levels of exposure to
    these particles are regulated in Directive 2008/50/EC of the European parliament
    and of the council on air quality and cleaner air for Europe, in addition to other
    pollutants, such as CO, SO2, and NO x . The categorization of PM mostly depends
    on the aerodynamic diameter size of particles. It is divided into three subclasses
    PM10 ( d≤10 μm ), PM2.5 ( d≤2.5 μm ), and PM1 ( d≤1 μm ), where the smaller particles
    generate a higher risk to health [5]. However, the PM monitoring methods proposed
    in Directive 2008/50/EC are too expensive to be implemented in a network with
    enough nodes to provide a high spatial resolution. Hyperlocal air quality monitoring
    refers to high spatiotemporal measurements taken by combining Internet of Things
    (IoT) devices that allow the collection of real-time data and are easy to deploy
    in several locations, allowing public authorities to make policy changes [6].
    Therefore, cost-effective high-resolution sensor networks monitoring pollution
    at fine spatial and temporal resolutions [7] are essential to regulating and performing
    smart management and making decisions in smart cities [8]. The importance of hyperlocal
    air quality sensors is that an extensive network of several compliant reference
    stations and a much larger number of hyperlocal sensors can deliver reliable,
    high-temporal-resolution data at neighborhood scales [9]. In this context, the
    use of IoT architectures and smart cities strategies allows for integrating the
    sensors into a bigger ecosystem and interacting with other components and services
    that improve its performance [10]. In this sense, the use of and research on so-called
    hyperlocal IoT sensors has increased in recent years. The need of the IoT on nanoparticles
    is highlighted by the fact that real-time data can ease the process of decision-making,
    and the IoT should address this [11]. The IoT devices are based on optical particle
    counters (OPCs) that base the measurement on the assumption that the number of
    particles is proportional to the light scattering. In this method, a light source
    illuminates the particles, and then, the scattered light from the particles is
    measured by a photometer. For particles with diameters greater than 0.3 μm , the
    scattered light is roughly proportional to their number concentration [12]. It
    has been shown that such sensors can have an acceptable linear response in the
    laboratory. However, there are deviations under real conditions due to ambient
    factors, such as temperature and, especially, relative humidity (RH) [13], [14].
    Regarding this last factor, the correction for humidity growth reduces the bias
    of the particulate monitors. [15]. Related to this last issue, water vapor can
    condense on aerosol particles, making them grow hygroscopically under high RH
    conditions and changing the light scattering coefficient. So, the light scattering
    method quantifies a larger diameter and changes, and this error is propagated
    to calculate mass concentration [16]. Thus, accuracy and error influenced by RH
    are nonlinear with time and depend on the chemical composition of the particles
    and season, factors that affect the growth of the parcel under humidity conditions
    [17]. To correct this effect, reference instruments usually have dryer systems
    that remove water from particles before measurement. In contrast, hyperlocal air
    quality sensors do not include such drying processes, resulting in particle sizes
    being overestimated at high RH, resulting in PM values being then enhanced relative
    to reference measurements, supposing a limitation of state-of-the-art approaches.
    Therefore, we hypothesize that adding a dryer system to hyperlocal air quality
    sensors will improve the quality of the measurements. In this work, we describe
    the development and validation of a humidity correction dryer system based on
    silica gel and infrared radiation. The dryer system has been developed and integrated
    into three IoT hyperlocal PM devices—based on the Alphasense OPC-N3 sensor technology—and
    validated by collocation tests in three reference stations in the city of Madrid.
    The system is a circuit composed of two columns that dries the sample before it
    arrives at the sensor. The main contribution of the work is the validation of
    a dryer integrated into a hyperlocal air quality device, since, to the best of
    our knowledge, this is the first dryer system implemented in a hyperlocal nanoparticle
    sensor and the first time this dryer system is tested in a real-time environment.
    The rest of this article is structured as follows. First, Section II presents
    all the work done on the use of hyperlocal air quality sensor and humidity correction
    approaches. Then, Section III describes the physical design of our solution as
    well as the resources used. From these tests, Section IV is obtained, which are
    analyzed in Section V. Finally, the main milestones and future challenges are
    presented in Section VI. SECTION II. State of the Art According to the European
    legislation, the reference method for PM determination is manual gravimetry, defined
    in the norm UNE-EN 12341:2015. However, the same directive states that methods
    showing a good correlation with manual gravimetry can be used for monitoring.
    This last group includes methods, such as automatic gravimetry, betta attenuation,
    and OPCs, which are quite important as they can be miniaturized in hyperlocal
    air quality sensors. For this reason, other methodologies have been explored to
    be miniaturized, such as separation methods. Table I states the technology behind
    the devices discussed in this article. TABLE I PM Devices and Technologies OPCs
    are based on light scattering, and each particle’s diameter is computed. Then,
    the particle’s mass is calculated using the density of the standard used for factory
    calibration and assuming that the particle is spheric. The mass of all particles
    belonging to one bin is summed to obtain the mass concentration, and all the bins
    belonging to one fraction are summed to compute the total mass concentration.
    A. Hyperlocal Air Quality Sensors Numerous studies have been conducted to evaluate
    hyperlocal air quality devices against medium—and high-cost—reference techniques.
    In laboratory conditions, the hyperlocal air quality sensors perform well, normally
    R 2 >0.85 [30]. However, this is not a realistic result, because these devices
    tend to be less precise under real conditions—named field evaluation in contrast
    to laboratory evaluation studied the performance of a low-cost sensor in California,
    finding a moderate agreement with beta attenuation monitoring (BAM) ( R 2 =0.35
    –0.81) [31]. On the other hand, evaluation against the tapered element oscillating
    microbalance (TEOM) monitor has a lower performance ( R 2 ≤0.30 ) with a low concentration
    range. However, a higher correlation with the BAM monitor ( R 2 >0.80 ) was observed
    in places with high concentrations in the same city [32], showing a great difference
    between concentration ranges and reference techniques used for validation. Consequently,
    it is necessary to understand which factors affect IoT hyperlocal sensors’ performance.
    The first study to make a significant impact on this field was conducted the study
    by Wang et al. [24] where three low-cost sensors Shiney PPD42NS, Samyoung DSM501A,
    and Sharp GP2Y1010AU0F were evaluated with the SidePark reference technique. As
    a result, very high linearity was obtained with all sensors ( R 2 values greater
    than 0.89) in laboratory conditions. This study was the first to highlight the
    critical dependence on humidity, particle size, and particle chemical composition.
    Another problem typically exhibited by these sensors is scale bias. For example,
    in this study, Speck sensors for PM2.5 were used and found to overestimate the
    concentration by 200% in indoor conditions and 500% in outdoor conditions, compared
    to the GRIMM Reference Dust Monitor [21]. On the other hand, the Plantower PMS1003
    sensor had a bias of +46% when measuring PM10 [28]. Another important feature
    is intrasensor variability, the variation between the measures taken from different
    devices for the same manufacturer. In [22], 12 low-cost sensors were evaluated
    in their measurement of the PM2.5 fraction, testing three sensors of the same
    brand under the same conditions. For the OPC-N2, a wide range of R 2 values were
    obtained—between 0.38 and 0.67. This variability was also related to the treatment
    done to report data. For example, there were differences between daily means and
    hourly means. For the PMS3003 sensor, the evaluation was conducted for hourly
    and daily averages, obtaining R 2 values ranging from 0.40 to 0.90. However, the
    slopes were close to one when the averages were hourly, and they were much larger
    in the case of daily averages [29]. Finally, a deeper analysis is done based on
    the performance of the Alphasense OPC-N3, the sensor that will be integrated into
    our solution. The South Coast AQMD1 conducted a field evaluation. This report
    showed the best performance for the PM1 fraction and a poor performance for the
    PM10 fraction. In this sense, the results for hourly means were R 2 =0.78 –0.82
    for PM1, R 2 =0.61 –0.69 for PM2.5, and R 2 =0.48 –0.53 for PM10. However, those
    results got improved for small fractions when daily mean are applied, R 2 =0.88
    –0.90 for PM1 and R 2 =0.69 –0.76 for PM2.5, except for PM10, R 2 =0.22 –0.26.
    Regarding other technologies, capacitance-based PM sensors are becoming a hot
    topic in air quality monitoring, because they can be easily integrated into small
    devices and can be combined with particle discrimination methods [33], [34]. These
    separation techniques are based on the movement of phoretic particles in a fluid.
    The movement of the particles is based on a gradient that could be electrical
    (electrophoresis), chemical (diffusiophoresis), or thermal (thermophoresis) [35].
    The motion of particles across gradients, whether thermal, electric, or concentration,
    is a well-established phenomenon, which already has applications in several fields,
    such as the study of proteins in molecular biology [36]. More concretely, thermophoresis
    has been studied to design real-time PM sensors, and a sensor has been modeled
    using the finite-elements method, with promising results [37], [38]. Thermophoresis
    is a transport force that occurs due to a temperature gradient, moving particles
    less than 2.5 μm in heater regions than bigger particles, allowing separation
    [39]. B. Humidity Correction Systems The effect of humidity on particle size and
    its application to particle measurements through light scattering has been widely
    studied. The physical explanation is that water vapor condenses on the surface
    of aerosol particles, making them grow hygroscopically—increasing their diameter—under
    high RH conditions [40]. To correct for this effect, reference instruments are
    usually equipped with dryer systems that remove water from particles by applying
    heat before measurement.. For instance, Palas Fidas 200 S is an EN 16450 certified
    static light scattering PM measurement instrument that incorporates a dryer system,
    the intelligent aerosol dryer system (IADS), used to remove water from particles
    before measurements [41]. On the other hand, the TEOM microbalance sample passes
    through a pair of Nafion dryers, with a specific geometry to control particle
    loss, remove water vapor, and reduce humidity levels. From the exit of the sample
    equilibration system (SES), the sample flow is routed through the TEOM mass sensor,
    and the bypass flow is routed to the control unit in the usual fashion. Because
    of the pressure drop created at the mass flow controllers, the zone between the
    mass flow controllers and the vacuum pump contains air with reduced water vapor
    pressure. This dried, low-pressure merged system flow then becomes the purge flow
    for the sample and bypasses Nafion dryers [42]. Other approaches consist of implementing
    technologies that are not temperature-dependent. For instance, a real-time PM2.5
    sensor module with the high mass resolution was developed with the module consisting
    of a two-stage aerosol impactor, a thin-film piezoelectric on silicon (TPoS) MEMS
    oscillator, and a micropump, achieving a collection efficiency of 51% for 2.54
    μm and 50% for 1.03 μm [43]. In contrast, hyperlocal sensors cannot implement
    these systems due to their low size and the complexity of the previous systems,
    so there is a need for coupling these devices with dryer systems. There are some
    approaches in this sense in the state of the art, such as the design of a sample
    inlet containing Perma Pure dryers [44], allowing the control of sample RH and
    temperature. Then, the system also contains a nephelometer and particle-sizing
    instrument. In Perma Pure dryers, the sample aerosol entered the inlet through
    a protective cover, which eliminated rain or insects from the sample train [45].
    Another work has developed dryer systems for reducing the error by humidity on
    measuring PM in the ambient air with optical particle measuring instruments. Two
    types of dryer systems were designed: dryer systems using heating and dilution
    methods, showing a high correlation with reference systems [46]. Parallel to the
    physical dryer systems, there are many state-of-the-art approaches to the moisture
    problem from an algorithmic point of view. For example, a mathematical method
    was developed to assess the PM levels based on the resolution of the inverse problem
    in aerosol tomography [47]. Beyond this, other approaches seek to remove the signal
    due to the humidity. One of these solutions has proposed an RH correction factor
    that could be applied to PM data in high RH conditions. This factor was calculated
    from historical PM data measured with an Alphasense OPC-N2 compared against a
    TEOM reference instrument based on gravimetry, which is not affected by humidity
    measurements [20]. However, this strategy is not coherent with the physical phenomenon.
    On dehydration, particles would reduce in size, not in number, thus affecting
    the derived PM in ways that now would depend on the detailed particle size spectrum.
    To overcome this, exhaustive work has been done to calculate the correction factor
    from the measured particle size distribution rather than mass values. The results
    not only showed significant improvement in sensor performance but also retained
    fundamental information related to particle composition. Thus, a particle size
    distribution-based correction algorithm was developed to correct the influence
    of RH on sensor measurements. The application of the correction algorithm, which
    assumed a physically reasonable correction factor, with the overestimation of
    PM measurements reduced from a factor of 5 before correction to 1.05 after correction
    [40]. The main drawback of this study is that this correction factor is highly
    dependent on the chemical composition, and it is difficult to extrapolate these
    results to other locations or to explain the seasonal intrinsic variation associated
    with PM chemical composition. C. Hyperlocal Air Quality Networks Across Europe
    Beyond a research level, the implementation of hyperlocal air quality networks
    has been done by public authorities in cities under the guidelines of the European
    Union, aiming to be climate neutral by 2050, and it needs to reduce at least 55%
    of emissions by 2030. In Spain, cities, such as Valencia (https://geoportal.valencia.es/apps/GeoportalHome/es/inicio/),
    Santander (https://www.smartsantander.eu/), or Molina de Segura (https://ciudadinteligente.molinadesegura.es/),
    have developed their own smart city platforms. At the European level, the French
    AIR Parif platform is of special relevance, which integrates air quality data,
    emissions, and forecasting over an extensive network of sensors (https://www.airparif.
    asso.fr/). Helsinki has a promising real-time air quality monitoring platform
    (https://hri.fi/data/en/dataset/reaaliaikainen- ilmanlaatu-hsyn-ilmanlaadun-mittausasemilla),
    and Amsterdam provides hyperlocal heat maps with hyperlocal data at street level
    by combining observations with models (https://maps.amsterdam.nl/klimaatadaptatie/?LANG=nl).
    Finally, implementing policies through these local initiatives allows for reducing
    emissions. For instance, research suggests that a combination of direct regulation
    of diesel vehicles and a targeted rebate to encourage substitution between diesel
    vehicles and BEVs can reduce PM emissions by 13% [48]. SECTION III. Materials
    and Methods Three units have been developed and installed in three reference stations
    in Madrid. This section describes the different methodologies and resources used
    for this work. First, a description of the hardware system proposed is done, followed
    by a summary of the IoT-cloud architecture in which these sensors are integrated.
    This architecture allows us to perform a validation based on colocation tests
    described in Section III-C. Finally, information about the software and computational
    resources used is provided. A. Hardware of the Dryer System The designed hyperlocal
    PM sensors are based on light scattering, where particles move through an airflow
    and are illuminated by a laser beam. The light is then scattered in all directions
    according to the particle properties—size, shape, absorption, and refractive index—and
    the beam’s wavelength. This does not affect the measurement process, because it
    remains constant [47]. Our solution integrates the Alphasense OPC-N3 sensor, selected
    for its measurement capacity and price competitiveness. However, it is important
    to highlight that OPC-N3 is more expensive than the rest of the sensors presented
    in the state of the art, because its hardware is more complex and achieves higher
    accuracy [23], reaching a range of 0.35 a 40 μm —based on a refraction index of
    1.5 + 0i —and a maximum mass concentration of 2000 μm/ m 3 . It samples particles
    with a typical flow rate of 280 mL/min and a velocity of 1000 particles per second.
    This sensor is known as an OPC, because it calculates the diameter of the particle
    and classifies it into one bin, a size range in which all particles with a diameter
    within this size range are contained. Then, the particle’s mass is calculated
    as PM1, PM2.5, and PM10 according to the following formula: m= ∑ i=0 n 4 3 π r
    2 ρ. (1) View Source It is easy to note that particle mass scales with particle
    size according to a cubic relationship, so small errors in particle classification
    lead to large errors in calculated mass. For this reason, a patented system [49]
    has been implemented to dry the air sample and avoid hygroscopic error, as shown
    in Fig. 1. The dryer system is based on a Teflon column filled by silica gel and
    an infrared radiation lamp. When the lamp is on, the silica gel absorbs the moisture
    from the passing air, drying it, and when it is off, desorption takes place, and
    the moisture returns to the air. Therefore, the airflow is as follows: 1) it enters
    through the first column and dries out as the lamp is switched on; 2) it leaves
    the column and passes through the optical sensor; and 3) it leaves the sensor
    and passes through the exit column, taking up the moisture and regenerating the
    silica gel. In order to optimize this process, two optical sensors are placed,
    so when air enters one column, it is measured by one sensor, and regeneration
    of the second column is allowed. In the next cycle, the air enters the regenerated
    column, and the second sensor measures the particles. Fig. 1. Scheme of the proposed
    dryer system, based on a circuit with two silica columns and two OPC-N3. Show
    All As can be deduced from Fig. 1, the air sample is dried before passing the
    sensor. Thus, this affects the signal, because the hygroscopic growth is removed
    before the sensor takes the measurement. In this context, the computed size by
    the sensor is closer to the real size, implying a lower value of r in (1). Consequently,
    computing mass concentration in a cubic way generates a more accurate value. This
    approach presents the following advantages in comparison with the techniques exposed
    in state of the art: 1) it can be implemented inside an IoT device, which is not
    possible for the size selectors in TEOM microbalances [42]; 2) this approach does
    not need to generate heat to correct humidity, as occurs in dryer systems implemented
    in reference devices or thermophoretic approaches [37], which implies an energy
    consumption not feasible for IoT devices [41]; 3) it is more exhaustive than the
    approach proposed by Perma Pure dryers; and 4) it acts directly over the measurements,
    which has not done the algorithmic approaches [47]. Fig. 2 details how the circuit
    is organized inside the box. Both dryer columns are connected to one OPC-N3, and
    each one is connected to a valve connected to the pump. This component maintains
    the flux across the circuit and allows particles to pass through the sensors.
    At the bottom of Fig. 2, a detail of what is inside the column is filled with
    the silica gel. It is important to note that the infrared lamp is placed at the
    end of the tube, which is in charge of heating the gel and dry particles when
    it is on. Fig. 2. On the left top, the circuit is presented with the two OPC-N3
    sensors, the dryer columns, the pump, and the valve that changes the cycles. On
    the right top, a delta of the OPC-N3 and the connection tube to the valve are
    shown. On the bottom, a detail of what is inside the dryer column is shown. Show
    All The prototype schematized in Fig. 1 is based on two dryer columns that are
    in charge of drying the sample and removing the hygroscopic growth of the particles.
    For this purpose, we use two Teflon tubes of 14 and 80 mm in radius, coupled with
    two paper filters at the input and output to remove particles—a surface of 26
    239 mm2 of Teflon (Fig. 3). The radius of the filter is 36 mm, and the radius
    of the overture that allows the air to pass is 14 mm—meaning an area of 6912 mm2
    of filter paper. The reason for using Teflon as the main material is that other
    options, such as silicone, could induce error, as the particles were prone to
    adhere to the walls. However, it is true that for some connections, silicone tubing
    must be used to achieve a fixed connection between the component (filter, solenoid
    valve, and so on) and the Teflon tubing. In other words, the aim is to achieve
    zero contact between the silicone and the airflow to obtain a lower error. The
    tubes are filled with silica gel that is in charge of absorbing water from particles.
    This process is accelerated by an infrared lamp controlled by a temperature sensor
    inside the column. Fig. 3. Scheme of dryer column, composed of Teflon column of
    80 mm and two paper filters with a radio of 36 mm, and 14 mm the radio of the
    cavity. Show All Despite the key components described previously, the new IoT
    system comprises several components. Table II lists all the elements needed to
    replicate the prototype with the required quantity for a single unit. It is important
    to note that here, we refer to the dryer column as the whole system described
    in the previous paragraph, with the Teflon cylinder, the silica gel, and the filters.
    TABLE II Components Per Device According to the Alphasense web page (https://www.alphasense.com/faqs/),
    OPC-N3 units are calibrated for sizing using controlled aerosols of monodisperse
    polystyrene latex microspheres of specific sizes. Aerosol number concentration
    is compared to an OPC gold standard, previously calibrated against a certified
    TSI 3330 OPS instrument. After this factory calibration, the whole unit, after
    integrating OPC-N3 with the dryer, is calibrated in the field with the GRIMM 11-D
    OPC reference system, which has a high correlation with manual gravimetry [20].
    B. Architecture The data collection process is realized through an IoT-cloud architecture
    that is organized in different layers, as shown in Fig. 4. The sensing layer comprises
    the IoT systems and is the one that directly performs the measurements, using
    hyperlocal particle sensors as “things” according to the IoT paradigm. Then, the
    contextual intelligence layer integrates the collected data into an Orion context
    broker through an IoT agent. It is important to note that this layer integrates
    real-time contextual data. Next, these data are stored by the persistence layer
    in a CrateDB database. Finally, the analytics layer is the JupyterLab interface
    to perform the statistics through pandas and sklearn. In this layer, the reference
    data are injected from the European Air Quality Portal via the respective application
    programming interfaces (APIs) and merged into a single csv per location. In addition,
    the csv provided by the Madrid City Council is added to this layer, as the mobile
    station (see Section III-C) does not report to the European Air Quality Portal.
    The generated csv files are used to validate our solution, as explained in Section
    III-C. Fig. 4. IoT-cloud architecture for data collection, composed from bottom
    to top by a sensing layer, a contextual layer, a persistence layer, and a data
    analytics layer. Show All C. System Validation To validate our measurements, this
    work proposes a data mining approach based on statistics and inference to determine
    the quality of the data, according to the standardized workflow in data science,
    by placing our devices next to the reference stations of local air quality network
    (colocation test). The proposed workflow is summarized in Fig. 5, starting on
    the csv file generated in the data collection. It is composed of the following
    steps: 1) the prevalidation and intercomparison of the different PM sensors using
    the student’s t-test, since we can assume the normality of the data because of
    the central limit theorem and the correlation matrix; 2) validation against an
    air quality station using the Pearson test and the linear regression model; 3)
    study of the noise with the Z -score algorithm for outliers detection; and 4)
    the iterative redesign of the solution and/or colocation tests to improve the
    performance of the system. Fig. 5. Workflow for data processing and validation.
    Show All The evaluation corresponding to the second step of the workflow has been
    performed in three different locations in Madrid, and it was conducted at three
    granularity levels: on hourly averaged real data—to validate against the reference
    devices, on daily averaged real data—to assess the capacity for long-term monitoring,
    and on maximum daily—to study the effect of high concentrations. At these three
    levels, we have applied metrics to compare the different devices to the reference
    station concentrations quantitatively. These metrics included Pearson r (2), which
    is a measure of the strength and direction of a linear relationship; mean absolute
    error (MAE) (3); and coefficient of determination R 2 (4). In these equations,
    y ^ and y are the predicted and real vectors, m y ^ is the mean of the vector
    y ^ , and m y is the mean of the vector y r MAE R 2 = ∑ n i=1 ( y i − m y )( y
    ^ i − m y ^ ) ∑ n i=1 ( y i − m y ) 2 − − − − − − − − − − − − − √ ∑ n i=1 ( y
    ^ i − m y ^ ) 2 − − − − − − − − − − − − − √ = 1 n ∑ i=1 n | y i − y ^ i | =1−
    ∑ n i=1 ( y i − y ^ i ) 2 ∑ n i=1 ( y i − m y ) 2 . (2) (3) (4) View Source The
    reference devices used to validate our solution were located on the official air
    quality stations. These devices are based on technologies that shows a high correlation
    with the manual normalized gravimetry defined in the norm UNE-EN 12341:2015. The
    official air quality stations report measurements every hour, as the mean of all
    values measured in an hour. Later, they are validated and stored in the European
    Air Quality Portal from where they can be accessed through an API. There are two
    kinds of reference devices. The first one is based on automatic microbalance (TEOM),
    and the physical principle beyond this technology is gravimetry. This device is
    installed in the Cuatro Caminos and Escuelas Aguirre air quality stations. On
    the other hand, optical methods show a high relationship with manual gravimetry.
    In this work, we will use data from two devices: the FIDA Palas, installed into
    Unidad Móvil and the GRIMM 11-D OPC [31], available in our laboratory. Fig. 6
    shows the devices installed in the reference stations. Fig. 6. Colocation tests
    in reference air quality stations. Show All D. Data Analysis Data analysis has
    been done in a Python 3 environment, due to the wide availability of data science
    tools. The libraries used in this work and its references are summarized in Table
    III. TABLE III List of Used Libraries These libraries have been used to implement
    the workflow proposed in Fig. 2. The first step is the extraction of the csv file,
    which can be found in Zenodo (Zenodo Respository). Once the csv file is obtained,
    the intravariability test is performed with the ttest_ind() function from scipy
    by separating data from different columns. Then, the linear models are performed
    with the oms() function from statmodel and the Z -score and outliers by implementing
    (5). The reference data were obtained from the European Air Quality Portal API
    (https://fme.discomap.eea.europa.eu), by setting as parameters CountryCode = ES,
    Pollutant = 10 (PM10) or 6001 (PM2.5), Year_from = 2021, Year_to = 2022, Source
    = E1a, and EoICode = ES0118A (Escuelas Aguirre) or ES1525A (Cuatro Caminos). The
    meteorology has been extracted from the Madrid Open Data Portal (https://datos.madrid.es).
    The data from the mobile unit have been provided by the Madrid city council, as
    it is not available and it is necessary to ask for them Z= x−μ σ . (5) View Source
    The system is hosted in an OVHcloud (OVH) server with an Intel1 Xeon CPU E3-1245
    V2 @ 3.40 GHz—eight cores operating system with 31 GB of RAM and 1 TB of storage.
    The operating system is Ubuntu 18.04×86 _64. SECTION IV. Results The following
    results were obtained from three collocation tests. A total of two devices were
    deployed in the Escuelas Aguirre reference station and Cuatro Caminos reference
    station, and one device was deployed in the Unit Movil station. Just to clarify,
    the three deployed devices’ locations were the same during all the tests. Thus,
    three locations were tested, and the devices did not move during the testing period.
    All experiments were performed simultaneously, with an hourly sampling rate from
    July 10, 2021 02:00:00 to August 31, 2021 23:00:00. We obtained data from 1205
    observations. Regarding weather conditions, collocations tests were taken in summer
    conditions in Madrid, with humidity values ranging from 11% to 85% and temperature
    values ranging from 17 °C to 42 °C. The RH median is equal to 32%, and the temperature
    median is equal to 26.6 ∘ C . A. Data Analytics and Data Mining Fig. 7 shows the
    correlations between the different variables that our device can measure against
    our reference OPC, GRIMM 11-D. This is important, because a high correlation could
    help to implement better calibration models. It is possible to observe a high
    correlation against the largest bins—particles with a diameter between 0.35 and
    10 μm —and the PM1, PM2.5, and PM10 fractions. Also, the correlation with humidity
    and temperature is also significant. Fig. 7. Correlations between reference variables
    and device features. Show All As explained in materials and methods, the solution
    is based on a two-component system with two OPC-N3 sensors fed by two different
    columns, so it is important to test if there is high variability between sensors
    and columns. Using an independent t -test to compare the measured means between
    columns, we obtain significant p -values for PM1, PM2.5, and PM1 ( p=2.75× 10
    −14 , p = 2.75× 10 −39 , and p=2.75× 10 −34 ). Fig. 8 shows the probability distribution
    for both columns and the comparison of the mean and standard deviation values.
    It is observed that the distribution represented in orange is wider, counting
    particles for a bigger size. On the other hand, the blue distribution is narrower,
    meaning lower variability in the measurements. Fig. 8. Comparison of probability
    distributions between columns per fraction (PM1, PM2.5, and PM10). The text in
    each histogram shows the results of the independent t -test. Show All B. Humidity
    Effect and Drying Regarding the humidity monitoring, the results are summarized
    in Fig. 9. It can be noted that the humidity values were low in general. Still,
    higher values were indeed obtained at night. The same occurred with the temperature,
    with high values during all the monitoring periods—even greater than 40 °C. The
    comparison between the ambient humidity and the OPC-N3 internal humidity provided
    interesting insights, allowing us to quantify the effect of the dryer system.
    This analysis was done in Escuelas Aguirre and Cuatro Caminos, since the mobile
    unit did not provide humidity data. Fig. 9 shows that internal humidity (orange)
    was lower than ambient humidity (blue), especially at night—for Escuelas Aguirre,
    the mean and maximum humidity reduction were 43% and 56%, respectively. In contrast,
    for Cuatro Caminos, they were 46% and 60%, respectively. However, the correlation
    between the internal and external humidities is quite high. This pattern occurs
    in both stations, with Pearson coefficient 0.82 for Escuelas Aguirre and 0.98
    for Cuatro Caminos. Fig. 9. Hourly mean validation of our device against air quality
    reference station. The metrics used are Pearson coefficient and MAE. Show All
    C. Hourly Mean Validation The hourly mean validation was performed against the
    hourly values of the reference air quality data (Fig. 10). For this, hourly means
    were computed for the data of each device. This treatment is unnecessary for the
    reference measurements, because air quality stations report hourly means by default.
    The metrics used were the Pearson coefficient and the MAE. The best results were
    obtained for the mobile station (mobile unit) with r=0.84 , r=0.88 , and r=0.68
    for PM10, PM2.5, and PM1, respectively. In the other two stations, the results
    were quite similar for PM10 (0.86 for Cuatro Caminos and 0.79 for Escuelas Aguirre)
    but worse for PM2.5 (0.60 and 0.59, respectively). Fig. 10. Hourly mean validation
    of our device against air quality reference station. The metrics used are the
    Pearson coefficient and MAE. The 1264 hourly samples were taken. Show All Regarding
    MAE, the patterns differed for PM10 fraction, where the higher value was for the
    Unidad Móvil ( 18.12 μg/ m 3 ). In contrast, the PM2.5 fraction showed a higher
    error for Cuatro Caminos and Escuelas Aguirre (5.89 and 5.30 μg/ m 3 ) in comparison
    with Unidad Móvil ( 4.65 μg/ m 3 ). For PM1, the error observed was larger than
    PM2.5 ( 4.11 μg/ m 3 ). The next step in the validation of the measures is to
    use linear regression to test how close the measured values and the real ones
    were. Thus, the relationship between the two variables was a linear function of
    slope equal to 1 and intercept equal to 0. The slope represents the bias error,
    and the intercept represents the stochastic error. The graphic outputs of the
    model are presented in Fig. 11, from where a high linearity could be deduced,
    and error bars have been calculated from the tests at the three locations and
    coincide with the ranges of R 2 and MAE for all the tests. Fig. 11. Linear regression
    model for each fraction at each air quality station. Show All Studying this hypothesis
    requires to analyze the statistics related to the regression model: the intercept,
    the slope, the F -test, and the associated P -value. In addition, the most important
    metric to evaluate the model is the coefficient of determination. All these results
    are shown in Table IV. The results were quite similar for all the fractions. For
    PM10, R 2 was equal to 0.74, 0.62, and 0.83 for Cuatro Caminos, Escuelas Aguirre,
    and Unidad Móvil, respectively. In contrast, for PM2.5, R 2 was equal to 0.356,
    0.344, and 0.828 for Cuatro Caminos, Escuelas Aguirre, and Unidad Móvil. Finally,
    for PM1, the result was R 2 =0.50 . TABLE IV List of Used Libraries D. Daily Mean
    Validation The daily mean validation was performed against the hourly values of
    the reference air quality data (Fig. 12). For this, daily means were computed
    for the data of each device and each air quality station. The best results were
    obtained for Unidad Móvil with r = 0.97, r = 0.93, and r = 0.78 for PM10, PM2.5,
    and PM1, respectively. In the other two stations, the results were quite similar
    for PM10 (0.96 for Cuatro Caminos and 0.95 for Escuelas Aguirre), but worse for
    PM2.5 (0.80 in both devices). It is important to note that the correlations improved
    when applying daily means. Fig. 12. Daily mean validation of our device against
    air quality reference station. The metrics used are Pearson coefficient and MAE.
    The 53 daily samples were taken. Show All About MAE, the patterns differed for
    the PM10 fraction, where the highest value was for the Unidad Móvil ( 16.95 μg/
    m 3 ). However, the PM2.5 fraction presented a lower error for Cuatro Caminos
    and Escuelas Aguirre (3.79 and 3.94 μg/ m 3 ) in comparison with Unidad Móvil
    ( 4.12 μg/ m 3 ). For PM1, the error observed was 4.11 μg/ m 3 . An improvement
    in comparison with hourly means was also observed, but it was less notorious than
    for the Pearson coefficient. E. Daily Maximum Validation The daily maximum value
    validation was performed against the hourly values of the reference air quality
    data (Fig. 13). For this, maximum values were computed for each device’s data
    and air quality station. The best results were obtained for the mobile station
    (Unidad Móvil) with r=0.94 , r=0.94 , and r=0.58 for PM10, PM2.5, and PM1, respectively.
    The results in the other two stations were r=0.91 for Cuatro Caminos and r=0.85
    for Escuelas Aguirre for PM10. On the other hand, the results for PM2.5 were r=0.60
    and r=0.61 for Cuatro Caminos and Escuelas Aguirre, respectively. Fig. 13. Daily
    maximum validation of our device against air quality reference station. The metrics
    used are Pearson coefficient and MAE. The 53 daily samples were taken. Show All
    Regarding MAE, the values were quite higher than daily and hourly means. For PM10
    fraction, the highest value was for Escuelas Aguirre ( 42.65 μg/ m 3 ). In addition,
    the PM2.5 fraction showed a higher error for Cuatro Caminos and Escuelas Aguirre
    (11.18 and 11.60 μg/ m 3 ) in comparison with Unidad Móvil ( 6.29 μg/ m 3 ). For
    PM1, the error observed was larger in comparison with PM2.5 ( 6.36 μg/ m 3 ).
    F. Outliers Detection The outliers analysis was done for the PM10 fraction and
    in the Cuatro Caminos device, because it was the noisiest device, and the PM10
    fraction had a high variability and a large error due to the size of the particles
    belonging to this fraction. The first step was to compute the Z -score according
    to the definition exposed in statistical methods. The results are presented in
    Fig. 14. Therefore, we show the temporal evolution of the Z -score across time.
    Positive values mean the measurements were higher than expected, and negative
    values mean the values were lower than expected. In general, values over 3 and
    under −3 are usually considered outliers. Here, measurements with a high Z -score
    in the first week and some peaks in the second week of August are possible. The
    highest value was a peak close to Z=14 . Fig. 14. Temporal distribution of Z -score
    for PM10 in Cuatro Caminos air quality station. Show All Once the outliers were
    identified, we applied a filter and an imputation method (mean imputation) to
    substitute the abnormal values. This step is critical, because a high Z -score
    could be an outlier or an event with extreme values—in the context of particulate,
    a dust Saharan intrusion may be an example. To analyze if these high Z -score
    values were outliers, we computed the Pearson coefficient after applying a filter
    of z=10 , z=3 , and z=1 (Fig. 15). The initial correlation ( r=0.86 ) became lower
    when the filter was stricter ( r=0.78 , r=0.53 , and r=0.45 for z=10 , z=3 , and
    z=1 ). In contrast, the filters could improve the initial error ( 13.9 μg/ m 3
    ) when using a narrow window for the Z -score (13.5, 12.5, and 10.4 μg/ m 3 ).
    Fig. 15. Comparison between signals after applying a Z -score filter ( z=10 ,
    z=3 , and z=1 ). Show All SECTION V. Discussion Emerging hyperlocal IoT nanoparticle
    sensors are devices based on light scattering that allow to implementation of
    high-resolution networks and assess the effectiveness of climate change mitigation
    policies. This technology is currently limited by external factors, such as humidity.
    For this reason, in this article, we have proposed a new hyperlocal IoT system
    that helps in sample drying and removes the effect of humidity in the measurement.
    The first important point that can be extracted from the results presented is
    that a high variability exists between the two sensors integrated into the device.
    The p -values for the t -test showed a significant difference between the means
    in all fractions, and Fig. 8 also showed a significant difference between the
    probability distributions. In this sense, this variability could affect the results
    and should be considered. This may be due to load loss in different points of
    the circuit that cause asymmetries or differences in the sensor coming from the
    factory, especially related to the wear of the laser of the optical sensor. A
    different hypothesis could be that the variability of the sensors is high [22].
    The proposed solution considerably improved the performance of the Alphasense
    OPC-N3 sensor in field conditions, showing high linearity with the reference equipment,
    as seen in Fig. 10. In general terms, our solution outperformed OPC-N3 according
    to the evaluation carried out by the South Coast AQMD. For PM10, we obtained R
    2 ranging from 0.62 to 0.83 for hourly measurements, contrasting with R 2 =0.48
    –0.53 for the single sensor. A similar pattern occurred for the PM2.5 fraction,
    achieving values ranging from 0.34 to 0.82 compared with R 2 =0.61 –0.69. In this
    case, we achieved a greater score in the best scenario but a lower score in the
    worst one. However, the PM2.5 results were more coherent with the 0.38–0.67 proposed
    for OPC-N2 [29]. Finally, R 2 =0.50 for PM1, which was lower than the obtained
    in the validation done by the South Coast AQMD (0.78–0.82). It is important to
    notice that we could only validate PM1 against one station, so there is a lack
    of statistical evidence to compare with the proposed ranges. On the other hand,
    we achieved similar performances to those obtained in the state of the art for
    PM2.5 using machine learning approaches, with R 2 ranging from 0.78 to 0.83, compared
    with 0.78 and 0.88 [50]. Regarding the dryer’s effectiveness, the analysis presented
    in Fig. 9 shows a significant reduction in the humidity levels after passing the
    columns. All internal humidity values collected were below the ammonium sulfate
    efflorescence point (35% RH) [41], which can be considered representative of urban
    environments [51]. That means that the effect of the hygroscopic growth is not
    significant on the measurement. However, to confirm this hypothesis, some limitations
    of our approach should be addressed. The first one is that the correlation between
    internal and ambient humidity is quite high, meaning that the dryer is reducing
    the humidity but not removing it. Thus, the humidity changes still affect the
    system, but with a lower impact. The second one is related to locations, as we
    have only tested our system in the city of Madrid. The weather conditions could
    be quite different in coastal cities, such as Valencia or Santander, more continental
    locations, such as Berlin, or Nordic places, such as Helsinki. It is true that
    the experiments’ humidity and temperature ranges are wide—covering from 11% to
    85% and from 17 °C to 42 °C. Still, it is necessary to test the system in different
    cities to confirm its effectiveness, because the humidity and temperature distributions
    are centered in dry and hot values. Another important topic is how the temperature
    and the chemical composition affect the system and the measurements. The results
    show a low correlation between the temperature and the mass concentration signal,
    suggesting that there is no significant dependency between them. This is in line
    with (1), as temperature is not used for the computation of mass concentration.
    It could indeed affect density, but this effect is not significant in the study
    ranges. In addition, we demonstrate that our system is robust to extreme high
    temperature conditions, as it happens in Madrid during July. However, future tests
    should evaluate the system for extreme cold conditions. Finally, the effect of
    the chemical composition cannot be deduced from this study, as it is necessary
    to test the system in colocation tests in stations with different aerosol compositions.
    Urban environments indeed present similar compositions, with a κ=0.61 [52]. Thus,
    to test this effect, comparing urban with rural locations is necessary. Despite
    the linearity of sensor response not being assumed, it is the proper way to check
    the quality of the air quality system according to new standards, such as CEN/TS
    17660 [53]. In an ideal scenario, the relation between the response of the sensor
    and the reference should be linear, with a slope = 1 and an intercept = 0. The
    slope stands for the bias error, whereas the intercept represents the nonsystematic
    error. Our results show that the slope is close to one, reducing the systematic
    error. There is indeed a considerable nonsystematic error. The same occurs with
    the MAE, which has high values in the three fractions. These facts show a high
    noise and unbiased error that should be addressed in the following iterations.
    The hypothesis is that we are dealing with two sensors in the devices, and the
    uncertainties related to each one are added. New designs could try to use only
    one device and check if the noise problems are reduced. In addition, more robustness
    checks are needed in more cities and different reference stations. Regarding daily
    means, the devices were colocated to the reference station for 52 days. The results
    improved for all the fractions and all the locations in terms of correlations
    and, in consequence, in terms of R 2 , as occurred when we compare against the
    reference technique. In contrast, as concluded in previous studies, the MAE does
    not get large when we apply daily mean measurements [40]. Compared with the evaluation
    of the South Coast AQMD, the solution improved the performance of the OPC-N3.
    For PM10, we obtained R 2 = 0.90–0.94 in contrast to the R 2 =0.22 –0.26 for the
    OPC-N3, and for PM2.5, we achieved R 2 =0.80 –0.86. Finally, for PM1, we reached
    R 2 =0.68 , lower than the values obtained by AQMD. Finally, the results obtained
    by the daily maximum showed a good correlation with the reference, but the values
    of MAE were so high, especially for PM10, with values higher than 40 μg/ m 3 .
    This is because the signal is especially noisy in the higher values, so the computation
    is inaccurate. For this reason, it is necessary to analyze the effect of this
    noise and the presence of outliers. In this sense, we observed that, for the Cuatro
    Caminos station, there were some measurements with a Z -score over than Z=10 —close
    to Z=14 —that supposes a high deviation of the means. When we imputed these values
    with the mean, we obtained a reduction of the error, but also a reduction of the
    correlation. That could mean that these extreme peaks are associated with real
    events but, due to the intrinsic noise to the signal, are also amplified and obtain
    an extreme value and reduce the accuracy, especially for the daily maximum values.
    This increment in the noise linked to maximum values is related to optical methods,
    since the error in the measurement increases cubically when we compute mass concentration
    from number concentration, thus being higher for large values. An important discussion
    point is the effect of the reference technology on the PM2.5 fraction. This study
    used FIDAS Palas as a reference instrument in the Unidad Móvil, based on light
    scattering, and TEOM in Cuatro Caminos and Escuelas Aguirre, based on oscillating
    microbalance. In this sense, we observed better results against optical methods
    than against TEOM, and this aspect should be studied. Other factors could cause
    the difference observed, but the similar results for PM10 suggest that this is
    not a problem for the stations. However, the proposed system presents some limitations,
    which are detailed as follows: 1) the use of two sensors in the circuit could
    add error due to the intrasensor variability; 2) the error is still important
    when measuring higher concentrations; and 3) the proposed system does not take
    into account the effect of particle composition and size [13]. To better understand
    these issues, it is necessary to implement more exhaustive networks. Thus, the
    main challenge is to validate the system in more reference points across Europe,
    implementing a dense monitoring network covering all the factors affecting performance.
    The next steps will be to implement and perform this validation, also trying different
    approaches to optimize the intrasensor variability. In addition, some algorithmic
    approaches should be implemented to consider size and chemical composition. Thus,
    coupling optical particle methods with spectrometric techniques to assess chemical
    composition in real time can help in assessing particle composition impact on
    measurements [54]. The technology proposed in this article highlights several
    challenges and future research steps in applying hyperlocal devices. The first
    one is the evaluation of low emission zones (LEZs), because these devices allow
    monitoring the evolution of nanoparticles and their correlation with the policies
    taken by public administrations. In this sense, it is possible to compute key
    performance indicators (KPIs) in a function of the concentration of nanoparticles.
    Those indicators could be related to several measures implemented in some cities
    in state of the art. For instance, several studies highlight the importance of
    implementing electric buses as urban transport and correctly managing schedules
    and routes [55]. In this sense, our solution could help to optimize the parameters
    seeking to reduce nanoparticle emissions. On the other hand, different approaches
    look for the security of different users, such as cyclists [56]. In this sense,
    our solution could improve the recommendation of healthy routes and bike stations.
    Thus, the quality of the hyperlocal devices allows us to go beyond and use this
    information as input for more complex models. Recent studies have shown that hyperlocal
    air quality sensors could identify the sources of the particles by coupling the
    sensor to a mobile vehicle [57]. Our solution meets the requirements to be linked
    to a mobile device well. Therefore, it could improve the accuracy in the state
    of the art. Other approaches deal with exhaustively quantifying the impact of
    different scenarios in terms of environmental benefits (i.e., reduction of CO2
    emissions, noise pollution, and traffic congestion) and quality of service for
    the users [58]. In this context, monitoring nanoparticles could be a key strategy,
    because it provides a wide variety of real-time data that could be used to feed
    these models. So, the research and improvement on hyperlocal air quality sensors
    will play an essential role in the solutions related to LEZs and impact estimations.
    SECTION VI. Conclusion In this work, we have described and evaluated a nanoparticle
    measurement system based on two silicon columns, demonstrating that the drying
    of the air samples substantially improves the measurements made with respect to
    the OPC-N3 hyperlocal sensor. Moreover, the results obtained with respect to the
    reference measurements are quite good, with the values of R 2 =0.83 for PM10 and
    PM2.5 in the best scenario for hourly averages and Pearson’s coefficients above
    0.80 for daily averages and maxima. In addition, we have shown that these devices
    can be integrated into an IoT architecture and interact with a real-time calibration
    service to improve data quality. The results obtained open new research possibilities,
    which can be classified along two lines. On the one hand, improving the quality
    of the data in three different aspects: 1) design of the dryer system, seeking
    to optimize certain aspects, such as airflow pressure losses, dryer efficiency,
    or sensor wear; 2) eliminating intrinsic noise from the signal, by means of methods
    to reduce electronic noise or the application of moving averages or the elimination
    of outliers; and 3) using real-time calibration models. On the other hand, the
    generation of high-quality hyperlocal data allows for progress in the following
    lines that have been limited so far in state of the art: 1) the generation of
    high-resolution air quality networks that allow estimating the impact of measures,
    such as LEZ; 2) the improvement and calibration of chemical transport models used
    to model the spatiotemporal distribution of pollutants; and 3) the calculation
    and estimation of the origins of pollution. Authors Figures References Keywords
    Metrics Footnotes More Like This A performance evaluation of container technologies
    on Internet of Things devices 2016 IEEE Conference on Computer Communications
    Workshops (INFOCOM WKSHPS) Published: 2016 An Experimental Evaluation of Electrocardiogram
    Monitoring System Using Internet of Things and Intelligent Sensors Support 2023
    Annual International Conference on Emerging Research Areas: International Conference
    on Intelligent Systems (AICERA/ICIS) Published: 2023 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Sensors Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Design and Evaluation of a Dryer System for IoT Hyperlocal Particulate Matter
    Monitoring Device
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Yang K.
  citation_count: '0'
  description: 'QUALITY IN THE ERA OF INDUSTRY 4.0. Enables readers to use real-world
    data from connected devices to improve product performance, detect design vulnerabilities,
    and design better solutions. Quality in the Era of Industry 4.0 provides an insightful
    guide to harnessing user performance and behavior data through AI and other Industry
    4.0 technologies. This transformative approach enables companies to not only optimize
    products and services in real-time, but also to anticipate and mitigate likely
    failures proactively. In a succinct and lucid style, the book presents a pioneering
    framework for a new paradigm of quality management in the Industry 4.0 landscape.
    It introduces groundbreaking techniques such as utilizing real-world data to tailor
    products for superior fit and performance, leveraging connectivity to adapt products
    to evolving needs and use-cases, and employing cutting-edge manufacturing methods
    to create bespoke, cost-effective solutions with greater efficiency. Case examples
    featuring applications from the automotive, mobile device, home appliance, and
    healthcare industries are used to illustrate how these new quality approaches
    can be used to benchmark the product’s performance and durability, maintain smart
    manufacturing, and detect design vulnerabilities. Written by a seasoned expert
    with experience teaching quality management in both corporate and academic settings,
    Quality in the Era of Industry 4.0 covers topics such as: Evolution of quality
    through industrial revolutions, from ancient times to the first and second industrial
    revolutions Quality by customer value creation, explaining differences in producers,
    stakeholders, and customers in the new digital age, along with new realities brought
    by Industry 4.0.Data quality dimensions and strategy, data governance, and new
    talents and skill sets for quality professionals in Industry 4.0. Automated product
    lifecycle management, predictive quality control, and defect prevention using
    technologies like smart factories, IoT, and sensors.Quality in the Era of Industry
    4.0 is a highly valuable resource for product engineers, quality managers, quality
    engineers, quality consultants, industrial engineers, and systems engineers who
    wish to make a participatory approach towards data-driven design, economical mass-customization,
    and late differentiation.'
  doi: 10.1002/9781119932475
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register HOMEAUTHOR BIOGRAPHY Quality in the Era of Industry 4.0: Integrating
    Tradition and Innovation in the Age of Data and AI Author(s):Kai Yang First published:5
    January 2024 Print ISBN:9781119932444 |Online ISBN:9781119932475 |DOI:10.1002/9781119932475
    © 2024 John Wiley & Sons, Inc. About this book QUALITY IN THE ERA OF INDUSTRY
    4.0 Enables readers to use real-world data from connected devices to improve product
    performance, detect design vulnerabilities, and design better solutions Quality
    in the Era of Industry 4.0 provides an insightful guide to harnessing user … Show
    all Table of Contents Export Citation(s) Free Access Front Matter (Pages: i-xix)
    Summary PDF Request permissions CHAPTER 1 Evolution of Quality Through Industrial
    Revolutions (Pages: 1-22) Summary PDF References Request permissions CHAPTER 2
    Evolving Paradigm for Quality in the Era of Industry 4.0 (Pages: 23-55) Summary
    PDF References Request permissions CHAPTER 3 Quality by Design and Innovation
    (Pages: 57-118) Summary PDF References Request permissions CHAPTER 4 Quality Management
    in the Era of Industry 4.0 (Pages: 119-159) Summary PDF References Request permissions
    CHAPTER 5 Predictive Quality (Pages: 161-197) Summary PDF References Request permissions
    CHAPTER 6 Data Quality (Pages: 199-236) Summary PDF References Request permissions
    CHAPTER 7 Risk Management in the 21st Century (Pages: 237-279) Summary PDF References
    Request permissions CHAPTER 8 Emerging Organizational Changes in the 21st Century
    (Pages: 281-313) Summary PDF References Request permissions Free Access Index
    (Pages: 315-323) First Page PDF Request permissions Buy this Book Contact your
    account manager For authors Additional links ABOUT WILEY ONLINE LIBRARY Privacy
    Policy Terms of Use About Cookies Manage Cookies Accessibility Wiley Research
    DE&I Statement and Publishing Policies Developing World Access HELP & SUPPORT
    Contact Us Training and Support DMCA & Reporting Piracy OPPORTUNITIES Subscription
    Agents Advertisers & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley
    Press Room Copyright © 1999-2024 John Wiley & Sons, Inc or related companies.
    All rights reserved, including rights for text and data mining and training of
    artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: 'Quality in the Era of Industry 4.0: Integrating Tradition and Innovation
    in the Age of Data and AI'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Quality in the Era of Industry 4.0: Integrating Tradition and Innovation
    in the Age of Data and AI'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liu M.
  - Cheng X.
  - Shi F.
  - Liu X.
  - Dai H.
  - Chen S.
  citation_count: '0'
  description: 'Sea State Estimation (SSE) is essential for Internet of Things (IoT)-enabled
    autonomous ships, which rely on favorable sea conditions for safe and efficient
    navigation. Traditional methods, such as wave buoys and radars, are costly, less
    accurate, and lack real-time capability. Model-driven methods, based on physical
    models of ship dynamics, are impractical due to wave randomness. Data-driven methods
    are limited by the data imbalance problem, as some sea states are more frequent
    and observable than others. To overcome these challenges, we propose a novel data-driven
    approach for SSE based on ship motion data. Our approach consists of three main
    components: a data preprocessing module, a parallel convolution feature extractor,
    and a theoretical-ensured distance-based classifier. The data preprocessing module
    aims to enhance the data quality and reduce sensor noise. The parallel convolution
    feature extractor uses a kernel-varying convolutional structure to capture distinctive
    features. The distance-based classifier learns representative prototypes for each
    sea state and assigns a sample to the nearest prototype based on a distance metric.
    The efficiency of our model is validated through experiments on two SSE datasets
    and the UEA archive, encompassing thirty multivariate time series classification
    tasks. The results reveal the generalizability and robustness of our approach.'
  doi: 10.1109/TSUSC.2024.3353183
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Sustaina...
    >Early Access A Prototype-Empowered Kernel-Varying Convolutional Model for Imbalanced
    Sea State Estimation in IoT-Enabled Autonomous Ship Publisher: IEEE Cite This
    PDF Mengna Liu; Xu Cheng; Fan Shi; Xiufeng Liu; Hongning Dai; Shengyong Chen All
    Authors 45 Full Text Views Abstract Authors Keywords Metrics Abstract: Sea State
    Estimation (SSE) is essential for Internet of Things (IoT)-enabled autonomous
    ships, which rely on favorable sea conditions for safe and efficient navigation.
    Traditional methods, such as wave buoys and radars, are costly, less accurate,
    and lack real-time capability. Model-driven methods, based on physical models
    of ship dynamics, are impractical due to wave randomness. Data-driven methods
    are limited by the data imbalance problem, as some sea states are more frequent
    and observable than others. To overcome these challenges, we propose a novel data-driven
    approach for SSE based on ship motion data. Our approach consists of three main
    components: a data preprocessing module, a parallel convolution feature extractor,
    and a theoretical-ensured distance-based classifier. The data preprocessing module
    aims to enhance the data quality and reduce sensor noise. The parallel convolution
    feature extractor uses a kernel-varying convolutional structure to capture distinctive
    features. The distance-based classifier learns representative prototypes for each
    sea state and assigns a sample to the nearest prototype based on a distance metric.
    The efficiency of our model is validated through experiments on two SSE datasets
    and the UEA archive, encompassing thirty multivariate time series classification
    tasks. The results reveal the generalizability and robustness of our approach.
    Published in: IEEE Transactions on Sustainable Computing ( Early Access ) Page(s):
    1 - 11 Date of Publication: 12 January 2024 ISSN Information: DOI: 10.1109/TSUSC.2024.3353183
    Publisher: IEEE Authors Keywords Metrics More Like This A Novel Class-Imbalanced
    Ship Motion Data-Based Cross-Scale Model for Sea State Estimation IEEE Transactions
    on Intelligent Transportation Systems Published: 2023 A Novel Densely Connected
    Convolutional Neural Network for Sea-State Estimation Using Ship Motion Data IEEE
    Transactions on Instrumentation and Measurement Published: 2020 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Sustainable Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Prototype-Empowered Kernel-Varying Convolutional Model for Imbalanced Sea
    State Estimation in IoT-Enabled Autonomous Ship
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Agrawal K.
  - Nargund N.
  citation_count: '0'
  description: 'Industry 4.0 is reshaping manufacturing by seamlessly integrating
    data acquisition, analysis, and modeling, creating intelligent and interconnected
    production ecosystems. Driven by cyber-physical systems, the Internet of Things
    (IoT), and advanced analytics, it enables real-time monitoring, predictive maintenance,
    adaptable production, and enhanced customization. By amalgamating data from sensors,
    machines, and human inputs, Industry 4.0 provides holistic insights, resulting
    in heightened efficiency, and optimized resource allocation. Deep Learning (DL),
    a crucial facet of artificial intelligence, plays a pivotal role in this transformation.
    This article delves into DL fundamentals, Autoencoders, Convolutional Neural Networks
    (CNNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs)
    and, Deep Reinforcement Learning discussing their functions and applications.
    It also elaborates on key DL components: neurons, layers, activation functions,
    weights, bias, loss functions, and optimizers, contributing to network efficacy.
    The piece underscores Industry 4.0’s principles: interoperability, virtualization,
    decentralization, real-time capabilities, service orientation, and modularity.
    It highlights DL’s diverse applications within Industry 4.0 domains, including
    predictive maintenance, quality control, resource optimization, logistics, process
    enhancement, energy efficiency, and personalized production. Despite transformative
    potential, implementing DL in manufacturing poses challenges: data quality and
    quantity, model interpretability, computation demands, and scalability. The article
    anticipates trends, emphasizing explainable AI, federated learning, edge computing,
    and collaborative robotics. In conclusion, DL’s integration with Industry 4.0
    heralds a monumental manufacturing paradigm shift, fostering adaptive, efficient,
    and data-driven production ecosystems. Despite challenges, a future envisions
    Industry 4.0 empowered by DL’s capabilities, ushering in a new era of production
    excellence, transparency, and collaboration.'
  doi: 10.1007/978-3-031-50583-6_15
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Distributed Computing and
    Intelligent Technology Conference paper Deep Learning in Industry 4.0: Transforming
    Manufacturing Through Data-Driven Innovation Conference paper First Online: 04
    January 2024 pp 222–236 Cite this conference paper Access provided by University
    of Nebraska-Lincoln Download book PDF Download book EPUB Distributed Computing
    and Intelligent Technology (ICDCIT 2024) Kushagra Agrawal & Nisharg Nargund   Part
    of the book series: Lecture Notes in Computer Science ((LNCS,volume 14501)) Included
    in the following conference series: International Conference on Distributed Computing
    and Intelligent Technology 181 Accesses Abstract Industry 4.0 is reshaping manufacturing
    by seamlessly integrating data acquisition, analysis, and modeling, creating intelligent
    and interconnected production ecosystems. Driven by cyber-physical systems, the
    Internet of Things (IoT), and advanced analytics, it enables real-time monitoring,
    predictive maintenance, adaptable production, and enhanced customization. By amalgamating
    data from sensors, machines, and human inputs, Industry 4.0 provides holistic
    insights, resulting in heightened efficiency, and optimized resource allocation.
    Deep Learning (DL), a crucial facet of artificial intelligence, plays a pivotal
    role in this transformation. This article delves into DL fundamentals, Autoencoders,
    Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative
    Adversarial Networks (GANs) and, Deep Reinforcement Learning discussing their
    functions and applications. It also elaborates on key DL components: neurons,
    layers, activation functions, weights, bias, loss functions, and optimizers, contributing
    to network efficacy. The piece underscores Industry 4.0’s principles: interoperability,
    virtualization, decentralization, real-time capabilities, service orientation,
    and modularity. It highlights DL’s diverse applications within Industry 4.0 domains,
    including predictive maintenance, quality control, resource optimization, logistics,
    process enhancement, energy efficiency, and personalized production. Despite transformative
    potential, implementing DL in manufacturing poses challenges: data quality and
    quantity, model interpretability, computation demands, and scalability. The article
    anticipates trends, emphasizing explainable AI, federated learning, edge computing,
    and collaborative robotics. In conclusion, DL’s integration with Industry 4.0
    heralds a monumental manufacturing paradigm shift, fostering adaptive, efficient,
    and data-driven production ecosystems. Despite challenges, a future envisions
    Industry 4.0 empowered by DL’s capabilities, ushering in a new era of production
    excellence, transparency, and collaboration. Keywords Industry 4.0 Deep Learning
    Manufacturing Transformation Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others A comprehensive
    literature review of the applications of AI techniques through the lifecycle of
    industrial equipment Article Open access 07 December 2023 Recent Advances of Artificial
    Intelligence in Manufacturing Industrial Sectors: A Review Article Open access
    03 November 2021 Digital Transformation in Smart Manufacturing with Industrial
    Robot Through Predictive Data Analysis Chapter © 2021 1 Introduction Industry
    4.0, recognized as a significant advancement in the manufacturing sector, revolves
    around the utilization of data and models within industrial contexts through data
    acquisition, analysis, and application [8]. Unlike its predecessors, Industry
    4.0 places a distinct emphasis on the amalgamation of cyber-physical systems,
    the Internet of Things (IoT), and sophisticated data analytics. This integration
    results in the establishment of intelligent and interconnected manufacturing ecosystems.
    This integration also enables real time monitoring, predictive maintenance, flexible
    production, and enhanced customization. By harnessing data from various sources
    within the manufacturing process, including sensors, machines, and human inputs,
    Industry 4.0 enables a holistic view of operations, leading to improved efficiency,
    reduce downtime, and optimized resource allocation. The significance of Industry
    4.0 lies in its potential to revolutionize manufacturing by ushering in a new
    era of agility, adaptability, and efficiency. It addresses the limitations of
    traditional manufacturing, where manual data collection, isolated processes, and
    reactive approaches were predominant. With Industry 4.0, manufacturers can transition
    towards proactive and data-driven strategies, allowing them to respond swiftly
    to changing market demands, reduce waste, and enhance overall productivity. Despite
    just a few decades passing since the onset of the first industrial revolution,
    we are on the cusp of the fourth revolution. Central to industry 4.0 is the fusion
    of digitalization and integration within manufacturing and logistics, facilitate
    by the internet and “smart” objects. Given the increasing complexity of modern
    industrial challenges, intelligent systems are imperative, an deep machine learning
    within Artificial Intelligence (AI) has emerged as a key player. While the field
    of Deep Learning is expansive and continually evolving, this discussion centers
    on prominent techniques. The methods covered include Convolutional Neural Networks,
    Autoencoders, Recurrent Neural Networks, Deep Reinforcement Learning, and Generative
    Adversarial Networks. These methods collectively represent a powerful toolkit
    for driving the industrial landscape towards the possibilities of the fourth industrial
    revolution. 2 Fundamentals of Deep Learning 2.1 Overview of Deep Learning Neural
    Networks Deep learning is really a vast field, presenting some of the most promising
    methods. There are many techniques but will be focusing on some of the prominent
    ones [5]. 1. Convolutional Neural Networks (CNNs): Convolutional Neural Networks
    (CNNs) have demonstrated significant prowess in tasks pertaining to images [10].
    Their capabilities shine particularly in domains such as image classification,
    object detection, semantic segmentation, and human pose estimation. The incorporation
    of techniques such as Rectified Linear Units (ReLU) nonlinearity, dropout, and
    data augmentation has led to notable enhancements in the performance of CNN models.
    2. Autoencoders (AEs): Auto encoders are designed for data representation learning.
    They consist of an encoder that abstracts data features and a decoder that reproduces
    input. Auto encoders find applications in dimensionality reduction, anomaly detection,
    data denoising and information retrieval. 3. Recurrent Neural Networks (RNNs)
    and Long Short-Term Memory (LSTM): RNNs, equipped with memory of past states,
    are suitable for sequential data tasks. LSTMs, a type of RNN, address long-term
    dependency problems and are essential for tasks involving sequences like language
    modelling and speech recognition. 4. Deep Reinforcement Learning (RL): Deep Reinforcement
    Learning agents interacting with environments to maximize rewards. Applications
    include robotics, optimization, control, and monitoring tasks in Industry 4.0.
    5. Generative Adversarial Networks (GANs): GANs consist of a generator and a discriminator.
    They create data that’s indistinguishable from real data. GANs find applications
    in image-to-image translation, text-to-image synthesis, video generation, 3D object
    generation, music composition, and medical imaging. Fig. 1. Different Layers of
    Neural Networks Full size image These deep learning neural networks collectively
    offers a diverse range of capabilities, driving innovation and progress in Industry
    4.0 and various other fields. 2.2 The Foundations of Deep Learning Going deep
    into the deep learning field, coming up are some of the key components of a neural
    networks like neurons, layers, activation functions. A schematic view of deep
    learning model is given in Fig. 1. Fig. 2. Popularly used Activation Functions
    [9, 13] Full size image 1. Neurons: The basic unit of a neural network is the
    neuron. Neurons are interconnected and work together to process information. Each
    neuron has a number of inputs, each of which is multiplied by the weight. The
    weighted inputs are later summed together and passes through an activation function
    to produce an output. 2. Layers:Neurons are arranged into distinct layers. Commencing
    with the input layer as the first tier, which ingests raw data, and concluding
    with the output layer as the ultimate stratum, generating the conclusive output.
    Intermediate to these, the hidden layers engage in the data processing role. 3.
    Activation functions: Activation functions are pivotal for instilling non-linearity
    within neural networks. They render the output of a neuron a non-linear function
    of its inputs. This non-linearity holds significance as it empowers neural networks
    to grasp intricate associations existing between input and output data. Figure
    2 discussed some of the widely used activation functions. 4. Weights: The weights
    in a neural network delineate the connections linking neurons. These weights dictate
    the degree of influence that each input wields over a neuron’s output. During
    the training phase, these weights undergo adjustments aimed at minimizing the
    discrepancy between projected and actual outputs. 5. Bias: Introducing a bias
    parameter, added to the aggregated, weighted input sum prior to traversing the
    activation function, significantly impacts the neuron’s output. This bias parameter
    contributes to regulating the neuron’s behavior. 6. Loss function:The role of
    the loss function is to gauge the disparity between the anticipated and factual
    output. In guiding the neural network’s training process, the loss function plays
    a pivotal role. 7. Optimizer: The optimizer is an algorithm that updates the weights
    and biases of the neural network to minimize the loss function. The performance
    of a neural network can be significantly influenced by critical hyperparameters
    including the quantity of layers, the number of neurons housed within each layer,
    the selection of activation functions, and the learning rate associated with the
    optimizer. Typically, these hyperparameters are determined via an iterative process
    involving experimentation and refinement. It’s worth noting that the optimal values
    for these hyperparameters may vary based on the specific problem and dataset.
    As a result, a thorough exploration of different configurations is often necessary
    to attain the best performance for a given task. 3 Industry 4.0: The Fourth Industrial
    Revolution Industry 4.0 is the name given to the current trend of automation and
    data exchange in manufacturing technologies [2]. It is characterized by the use
    of cyber-physical systems (CPS), the Internet of Things (IoT), cloud computing,
    and Artificial Intelligence (AI) [14]. These technologies are converging to create
    a more connected, intelligent, and efficient manufacturing environment. The core
    principles of Industry 4.0 are shown in Fig. 3 Fig. 3. Six Principles of Industry
    4.0 Full size image Interoperability: The ability of different systems and devices
    to communicate and exchange data. Virtualization: The creation of a virtual representation
    of the physical world. Decentralization: The distribution of control and decision-making
    to the edge of the network. Real-time capability: The ability to collect, analyze,
    and act on data in real time. Service orientation: The provision of services as
    a way to interact with and manage systems. Modularity: The ability to easily add,
    remove, or replace components. Automation, IoT and Data-driven decision making
    are the three key technologies that are driving the fourth industrial revolution,
    also known as Industry 4.0. These three technologies are converging to create
    a more connected, intelligent, and efficient manufacturing environment. By automating
    tasks, collecting data, and using data to make decisions, manufacturers can improve
    their productivity, quality, and profitability. Automation is the use of machines
    and software to perform tasks that would otherwise be done by humans. In manufacturing,
    automation can be used to automate tasks such as welding, painting, and assembly.
    This can help to improve efficiency and productivity, as well as reduce the risk
    of human error. Internet of Things (IoT) is a network of physical objects that
    are embedded with sensors, software, and network connectivity to enable them to
    collect and exchange data. In manufacturing, the IoT can be used to collect data
    from machines, sensors, and other devices. This data can be used to monitor the
    performance of equipment, identify potential problems, and improve efficiency.
    Also, In Healthcare Industry IoT has been used in the remote patient monitoring
    systems, fitness tracker devices, etc. [3]. Data-driven decision-making is the
    use of data to make decisions. In manufacturing, data-driven decision-making can
    be used to optimize production processes, improve quality, and reduce costs. For
    example, data can be used to identify the most efficient way to produce a product,
    or to predict when a machine is likely to fail. 4 Deep Learning Techniques and
    Architectures In continuation to Sect. 2, various techniques and architectures
    associated like CNNs, RNNs, GANs are listed in Fig. 4 and are discussed as follows:
    [5]. Also, Table 1 gives a comparison between CNNs, RNNs and GANs. Fig. 4. Different
    Deep Learning Techniques Full size image Convolutional Neural Networks (CNNs)
    represent a category of deep learning algorithms that find widespread application
    in tasks like image classification, object detection, and segmentation. These
    networks draw inspiration from the functioning of the human visual cortex, effectively
    learning to identify relevant image features for a given task. Structurally, CNNs
    consist of a sequence of specialized layers, each assigned a distinct role in
    the process [1]. The initial layer, known as the convolutional layer, employs
    a convolution operation on the input image. This operation extracts pertinent
    attributes from the image, such as edges and textures. The outcome of this convolutional
    layer is then channeled into a pooling layer, which downsamples the data to mitigate
    overfitting and reduce data dimensions. Subsequently, the output from the pooling
    layer is directed through a sequence of fully connected layers. These layers are
    responsible for learning and classifying the features as extracted by the convolutional
    counterparts. Ultimately, the CNN culminates with an output layer that generates
    the anticipated class label for the input image. Demonstrating impressive effectiveness
    across various image processing tasks, CNNs excel in object classification, object
    detection, and image segmentation. Their scope extends beyond images as well,
    encompassing domains like natural language processing and speech recognition.
    Recurrent Neural Networks (RNNs) stand as a class of deep learning algorithms
    with prominent usage in the domain of natural language processing, encompassing
    tasks like speech recognition and machine translation [15]. RNNs exhibit the capability
    to comprehend and model sequential data, such as text and speech. Constituted
    by an array of interconnected nodes, RNNs embrace the ability to store values.
    These nodes are configured in a loop, enabling the output of each node to be fed
    into the succeeding one. This cyclic arrangement empowers the RNN to discern and
    model the interdependencies existing among distinct elements of the sequence.
    Ultimately, the RNN yields a prognosis of the subsequent element within the sequence.
    The RNN undergoes training through the minimization of the error between predicted
    and actual outputs. Proven to be remarkably potent in an array of natural language
    processing undertakings, RNNs have been harnessed for speech recognition, language
    translation, and text generation. Furthermore, their utility extends to various
    other domains like robotics and financial forecasting. Generative Adversarial
    Networks (GANs) represent a category of deep learning algorithms primarily employed
    for the purpose of image generation. This architecture comprises two neural networks,
    namely the generator and the discriminator [5]. The generator is tasked with crafting
    new images, while the discriminator’s role is to differentiate between authentic
    and counterfeit images [18]. The generator is trained to fabricate images that
    exhibit the highest degree of realism achievable. Conversely, the discriminator
    undergoes training to proficiently discriminate between genuine and fabricated
    images. A distinctive aspect of GANs is their adversarial training approach, wherein
    the two networks enter into a competitive relationship. The generator strives
    to deceive the discriminator, while the latter endeavors to accurately distinguish
    authentic and synthetic images. GANs have notably demonstrated their efficacy
    in generating lifelike images. Their applications encompass generating depictions
    of objects, crafting authentic facial representations, and producing images of
    nonexistent individuals. Beyond image generation, GANs are finding application
    in diverse domains including text and music generation. In a more generalized
    way, GANs are super-intelligent artistic robots that may produce outputs of different
    types. Table 2 discusses different types of GANs on the basis of function they
    perform. Table 1. Comparison of CNNs, GANs, and RNNs Full size table Table 2.
    Application of Different Types of GANs Full size table These are just three of
    the many popular deep learning architectures. Other popular architectures include
    deep belief networks (DBNs), stacked autoencoders, and capsule networks. Deep
    learning is a rapidly evolving field, and new architectures are being developed
    all the time. Now, Deep Learning is applied in various industries but lets see
    how it is applied in the Industry 4.0 challenges: 1. Predictive maintenance: Deep
    Learning can be used to predict when the machines are likely to fail. This can
    mainly help in preventing unplanned downtime and improve the efficiency of the
    manufacturing process. 2. Quality Control: Deep Learning can be used to identify
    defects in products. This can help to improve the quality of products and reduce
    the number of products that need to be scrapped. 3. Resource optimization: Deep
    Learning can be used to optimize the use of resources, such as energy and material.
    This can help to reduce costs and improve the environmental impact of the manufacturing
    process. 4. Logistics: Deep Learning can be used to optimize the logistics of
    the manufacturing process, such as transportation and warehousing. This can help
    to reduce costs and improve the efficiency of the supply chain. 5 Data Acquisition
    and Preprocessing The realm of deep learning applications rests upon a foundational
    dependency on data, rendering an ample supply of data imperative for both effective
    training and operational success. Within this context, the quality and quantity
    of data assume crucial roles that significantly influence the performance of deep
    learning models. The significance of data in deep learning applications is underscored
    by several key reasons. Primarily, data forms the bedrock upon which model training
    is built. This foundational aspect allows the model to discern intricate patterns
    and facilitate predictions by engaging with extensive sets of labeled data. Moreover,
    data occupies a pivotal role in evaluating model performance. Through the use
    of dedicated test data, the model’s capacity to generalize to novel and unseen
    data is measured. Furthermore, data contributes to model refinement. This is achieved
    either through the introduction of additional data or the calibration of model
    parameters, a process that culminates in an enhanced overall model performance.
    The pivotal role of data quality is evidenced by the fact that subpar data quality
    hampers proper model learning, consequently leading to inaccurate predictions.
    Moreover, the accuracy of data labeling is equally crucial. Mislabeling data can
    steer the model towards learning incorrect patterns, ultimately undermining its
    predictive capability. The quantity of data is equally pivotal, as deep learning
    models thrive on extensive datasets for effective training. Insufficient data,
    on the other hand, stymies proper learning and compromises predictive accuracy.
    It is also important to acknowledge various characteristics of data that hold
    significance. Considerations such as data quality, quantity, and preprocessing
    techniques hold paramount importance within deep learning applications, shaping
    the course of model development and predictive outcomes. Data quality: The quality
    of the data is essential for the performance of deep learning models. The data
    should be clean, accurate, and representative of the problem that the model is
    trying to solve. Data quantity: The quantity of the data is also important. Deep
    learning models need a lot of data to train properly. If the data is not enough,
    the model will not be able to learn properly and will not be able to make accurate
    predictions. Data preprocessing: Data preprocessing is the process of cleaning
    and preparing the data for training the model. This includes tasks such as removing
    noise, correcting errors, and transforming the data into a format that the model
    can understand. In deep learning applications, ensuring data quality, quantity,
    and preprocessing involves employing specific techniques. Data cleaning targets
    noise and outliers, which can hinder model training, by removing them from the
    dataset. Data normalization transforms data to have a mean of 0 and a standard
    deviation of 1 [6], enhancing model performance and comparability. Data augmentation
    generates new data from existing sources through actions like rotation or cropping,
    bolstering model robustness against data variations [16]. Additionally, feature
    selection aims to extract essential data attributes, often using statistical tests
    or machine learning algorithms, reducing noise and ultimately refining model performance.
    These techniques collectively fortify the data-driven foundation of deep learning
    models, enabling more accurate and effective outcomes. 6 Process Optimization
    and Energy Efficiency Deep Learning optimizes manufacturing processes through
    its ability to extract insights, recognize patterns, and make predictions from
    complex data [8]. Deep learning has revolutionized manufacturing processes by
    harnessing its data analysis capabilities to enhance efficiency, quality, and
    productivity across various aspects of the production chain. Through the application
    of advanced neural networks, manufacturing industries have gained the ability
    to optimize operations and make informed decisions based on insights extracted
    from intricate data streams. In the realm of quality control and defect detection,
    Convolutional Neural Networks (CNNs) have emerged as powerful tools. These networks
    are adept at scrutinizing visual data, such as images of products or materials,
    to identify imperfections, anomalies, or deviations from the desired standard.
    By employing CNNs along assembly lines, manufacturers can detect issues in real-time
    and promptly initiate corrective measures, ensuring that only products meeting
    stringent quality criteria proceed further in the production process. Predictive
    maintenance, another pivotal application, leverages Recurrent Neural Networks
    (RNNs) to foresee equipment failures. By analyzing data from sensors embedded
    in machinery, RNNs can predict potential breakdowns before they occur. This proactive
    approach enables maintenance teams to schedule timely interventions, minimizing
    downtime and preventing costly production disruptions. Process optimization is
    yet another domain transformed by deep learning. The technology’s aptitude for
    deciphering intricate patterns within extensive datasets enables manufacturers
    to fine-tune parameters influencing production. Deep learning algorithms can analyze
    factors like temperature, pressure, and material composition, leading to refined
    processes, reduced waste, and enhanced operational efficiency. Supply chain management
    is further optimized by deep learning’s predictive capabilities. Recurrent Neural
    Networks excel in forecasting demand by analyzing historical trends, market shifts,
    and other pertinent data points. Manufacturers can align production schedules
    and resource allocation more accurately, thereby reducing excess inventory and
    streamlining operations. Moreover, deep learning contributes to energy efficiency
    efforts. By scrutinizing energy consumption patterns, models can propose strategies
    for optimizing energy usage. This might involve scheduling energy-intensive tasks
    during periods of lower demand or recommending adjustments to equipment settings
    to minimize energy consumption. In a context of increasing demand for customized
    products, deep learning aids in efficiently fulfilling individual preferences.
    By analyzing customer data and production constraints, models can suggest configurations
    that align with consumer desires while maintaining production efficiency. Deeper
    insights into root causes of quality issues or process failures are unlocked through
    deep learning. By scrutinizing data across production stages, models can reveal
    correlations and patterns that contribute to problems, facilitating continuous
    improvement initiatives. 7 Challenges and Limitations The implementation of deep
    learning in manufacturing processes is accompanied by notable challenges and limitations.
    Acquiring sufficient and accurate data, a prerequisite for effective model training,
    remains a hurdle, particularly for rare events or intricate processes. The opacity
    of deep learning models, due to their complex architectures, raises concerns about
    interpretability, critical for regulatory compliance and troubleshooting. High
    computational demands for training hinder accessibility, particularly for smaller
    manufacturers, while ensuring model generalization and scalability across diverse
    environments proves intricate. Data security and privacy concerns arise when sharing
    proprietary manufacturing data for model development. Additionally, the trade-off
    between real-time processing demands and model processing time must be balanced,
    and vigilance is needed to prevent biased decisions originating from biased training
    data. Ultimately, managing these challenges while achieving a positive return
    on investment remains pivotal for successful integration of deep learning in manufacturing.
    8 Future Trends In the forthcoming landscape of manufacturing, the application
    of deep learning is projected to usher in a series of transformative trends that
    promise to redefine industry practices. A notable trend on the horizon is the
    advancement of explainable AI, a response to the increasing complexity of deep
    learning models. This trajectory emphasizes the development of techniques that
    provide transparent insights into the decision-making processes of these models.
    By unraveling the rationale behind predictions, explainable AI aims to cultivate
    trust among stakeholders, enabling them to comprehend and endorse the reasoning
    driving these AI-driven outcomes. This will be crucial in sectors where accountability,
    regulatory compliance, and user confidence are paramount. Concerns surrounding
    data privacy and security have fueled the rise of federated learning. This innovative
    approach allows models to be collaboratively trained across multiple devices or
    locations while keeping the underlying data decentralized. In the manufacturing
    context, where proprietary data is sensitive, federated learning stands as a solution
    to drive collective learning while safeguarding data privacy. This trend is poised
    to reshape how manufacturers harness the power of deep learning while respecting
    data confidentiality. The synergy between deep learning and edge computing is
    set to play a pivotal role in manufacturing’s future. With the growing capabilities
    of edge devices, the deployment of deep learning models directly at the data source
    is becoming increasingly viable. This paradigm shift enables real-time analysis
    and decision-making at the edge, circumventing the need to transmit massive data
    volumes to central servers. The outcome is reduced latency, enhanced operational
    efficiency, and the potential to react swiftly to critical events, underscoring
    the transformative potential of edge-driven deep learning applications in manufacturing.
    Integration with the Internet of Things (IoT) is another key trend that will reshape
    manufacturing. Deep learning models integrated with IoT devices will bring about
    highly accurate predictive maintenance and optimization capabilities. Sensors
    embedded within manufacturing equipment will continuously feed data to these models,
    enabling early identification of potential issues and averting costly downtimes.
    This seamless connection between devices and AI models is poised to elevate manufacturing
    efficiency to new heights, offering an intelligent and proactive approach to maintenance
    and resource utilization. Furthermore, deep learning’s impact on collaborative
    robotics (cobots) is set to expand. Future developments in machine learning algorithms
    will facilitate more profound insights into human behavior, enabling cobots to
    interact more safely and efficiently with human counterparts on the factory floor.
    Enhanced human-machine collaboration will foster an environment where automation
    and human expertise harmonize, optimizing productivity and safety in manufacturing
    processes. An overarching theme that is expected to define the future of deep
    learning in manufacturing is cross-disciplinary collaboration. AI experts, domain
    specialists, and manufacturers will increasingly join forces to tailor deep learning
    solutions to the unique challenges of the industry. This multidisciplinary approach
    promises to fuel innovation, driving the industry towards smarter, adaptive manufacturing
    systems that address challenges and seize opportunities across diverse sectors.
    As these future trends unfold, the manufacturing landscape is poised to witness
    a profound transformation driven by the capabilities of deep learning. From transparency
    and privacy considerations to real-time edge analysis, IoT integration, and collaborative
    robotics, deep learning is on the brink of revolutionizing manufacturing practices
    in ways that were once only imagined. Through careful navigation of these trends,
    industries stand to gain a competitive edge and unlock new avenues of growth and
    efficiency in the Industry 4.0 era. In the dynamic landscape of Industry 4.0,
    the amalgamation of data-driven technologies has paved the way for transformative
    shifts in manufacturing. Deep Learning, as a cornerstone of this revolution, has
    illuminated avenues that were once only conceivable in the realm of science fiction.
    The fusion of cyber-physical systems, IoT, and advanced analytics has ignited
    a paradigm shift from traditional manufacturing methodologies towards a future
    brimming with agility, adaptability, and efficiency. As we delve into the depths
    of deep learning techniques and architectures, it becomes evident that these methods
    hold the power to unravel complexities, discern patterns, and predict outcomes
    in ways that were previously unattainable. Convolutional Neural Networks have
    empowered the identification of defects and quality deviations, opening the door
    to real-time intervention and elevated product excellence. Recurrent Neural Networks,
    with their sequential analysis prowess, have unlocked the realm of predictive
    maintenance, minimizing downtimes and optimizing resource allocation. Generative
    Adversarial Networks have transcended the boundaries of imagination, enabling
    the generation of synthetic data that parallels reality. These profound advancements,
    bolstered by the principles of data quality, quantity, and preprocessing, are
    propelling the industry towards a new era of production excellence. However, these
    leaps are not without their challenges. Data remains both the fuel and the bottleneck
    of deep learning applications, necessitating a delicate balance between quality,
    quantity, and privacy concerns. The complexity of deep learning models raises
    interpretability issues, requiring innovative approaches to ensure transparent
    decision-making. Moreover, the computational demands and scalability concerns
    must be addressed to democratize the benefits of this technology across the manufacturing
    spectrum. As we forge ahead, the synthesis of innovative solutions and collaborative
    endeavors is essential to surmount these obstacles and reap the rewards of deep
    learning integration. The future holds promises that extend beyond the horizon.
    Explainable AI is poised to infuse transparency into the decision-making processes,
    fostering trust and accountability. Federated learning is set to revolutionize
    data privacy by enabling collective learning while preserving sensitive information.
    Edge computing, in tandem with IoT integration, is propelling us towards real-time
    insights at the source, rendering processes swift and responsive. Collaborative
    robotics is poised for safer, more efficient interactions, where human expertise
    and automation harmonize seamlessly. Cross-disciplinary collaboration, the cornerstone
    of innovation, will orchestrate the rise of smarter, adaptive manufacturing systems,
    creating a future where technology is harnessed to its fullest potential. As we
    stand at the confluence of human ingenuity and technological prowess, the journey
    towards a deeply integrated Industry 4.0 is underway. Through perseverance, collaboration,
    and a dedication to overcoming challenges, the fusion of deep learning and manufacturing
    promises a future that is not only intelligent and efficient, but also profoundly
    transformative. The fourth industrial revolution is not a distant concept; it
    is unfolding before us, driven by the power of deep learning and the boundless
    possibilities it bestows upon the manufacturing landscape. References Albawi,
    S., Mohammed, T.A., Al-Zawi, S.: Understanding of a convolutional neural network.
    In: 2017 International Conference on Engineering and Technology (ICET), pp. 1–6
    (2017). https://doi.org/10.1109/ICEngTechnol.2017.8308186 Bali, V., Bhatnagar,
    V., Aggarwal, D., Bali, S., Diván, M.J.: Cyber-Physical, IoT, and Autonomous Systems
    in Industry 4.0. CRC Press, Boca Raton (2021) Google Scholar   Bhat, O., Gokhale,
    P., Bhat, S.: Introduction to IoT. Int. Adv. Res. J. Sci. Eng. Technol. 5(1),
    41–44 (2007). https://doi.org/10.17148/IARJSET.2018.517 Han, Z., et al.: Stackgan:
    text to photo-realistic image synthesis with stacked generative adversarial networks.
    In: 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5908–5916.
    IEEE (2017) Google Scholar   Hernavs, J., Ficko, M., Berus, L., Rudolf, R., Klančnik,
    S.: Deep learning in industry 4.0 - brief. J. Prod. Eng. 21(2), 1–5 (2018). https://doi.org/10.24867/JPE-2018-02-001
    Jamal, P., Ali, M., Faraj, R.H.: Data normalization and standardization: a technical
    report. In: Machine Learning Technical Reports, pp. 1–6. Machine Learning Lab,
    Koya University (2014). https://doi.org/10.13140/RG.2.2.28948.04489 Li-Chia, Y.,
    Szu-Yu, C., Yi-Hsuan, Y.: Midinet: a convolutional generative adversarial network
    for symbolic-domain music generation. In: Europe: ISMIR (2017) Google Scholar   May,
    M.C., Neidhöfer, J., Körner, T., Schäfer, L., Lanza, G.: Applying natural language
    processing in manufacturing. In: Procedia CIRP, pp. 184–189. Elsevier B.V. (2022).
    https://doi.org/10.1016/j.procir.2022.10.071 Menon, A., Mehrotra, K., Mohan, C.K.,
    Ranka, S.: Characterization of a class of sigmoid functions with applications
    to neural networks. Neural Netw. 9(5), 819–835 (1996). https://doi.org/10.1016/0893-6080(95)00107-7.
    https://www.sciencedirect.com/science/article/pii/0893608095001077 O’Shea, K.,
    Nash, R.: An introduction to convolutional neural networks (2015). https://doi.org/10.48550/arXiv.1511.08458
    Schlegl, T., Seeböck, P., Waldstein, S.M., Schmidt-Erfurth, U., Langs, G.: Unsupervised
    anomaly detection with generative adversarial networks to guide marker discovery.
    In: Niethammer, M., et al. (eds.) IPMI 2017. LNCS, vol. 10265, pp. 146–157. Springer,
    Cham (2017). https://doi.org/10.1007/978-3-319-59050-9_12 Chapter   Google Scholar   Shah,
    P., Sekhar, R., Kulkarni, A., Siarry, P.: Metaheuristic Algorithms in Industry
    4.0. Advances in Metaheuristics. CRC Press (2021). https://books.google.co.in/books?id=7jc8EAAAQBAJ
    Sharma, S., Sharma, S., Athaiya, A.: Activation functions in neural networks.
    Int. J. Eng. Appl. Sci. Technol. 4, 310–316 (2020). https://www.ijeast.com Sony,
    M., Naik, S.: Key ingredients for evaluating industry 4.0 readiness for organizations:
    a literature review. Benchmark. Int. J. 27(7), 2213–2232 (2020). https://doi.org/10.1108/BIJ-09-2018-0284
    Tarwani, K., Edem, S.: Survey on recurrent neural network in natural language
    processing. Int. J. Eng. Trends Technol. 48(6), 301–304 (2017). https://doi.org/10.14445/22315381/IJETT-V48P253
    Van Dyk, D.A., Meng, X.L.: The art of data augmentation. J. Comput. Graph. Stat.
    10(1), 1–50 (2001). https://doi.org/10.1198/10618600152418584 Article   MathSciNet   Google
    Scholar   Vondrick, C., Pirsiavash, H., Torralba, A.: Generating videos with scene
    dynamics. In: Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R. (eds.)
    Advances in Neural Information Processing Systems, vol. 29. Curran Associates,
    Inc. (2016) Google Scholar   Wang, K., Gou, C., Duan, Y., Lin, Y., Zheng, X.,
    Wang, F.Y.: Generative adversarial networks: introduction and outlook. IEEE/CAA
    J. Automatica Sinica 4(4), 588–598 (2017). https://doi.org/10.1109/JAS.2017.7510583
    Article   MathSciNet   Google Scholar   Wu, J., Zhang, C., Xue, T., Freeman, B.,
    Tenenbaum, J.: Learning a probabilistic latent space of object shapes via 3D generative-adversarial
    modeling. In: Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R. (eds.)
    Advances in Neural Information Processing Systems, vol. 29. Curran Associates,
    Inc. (2016) Google Scholar   Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired
    image-to-image translation using cycle-consistent adversarial networks. In: 2017
    IEEE International Conference on Computer Vision (ICCV), pp. 2242–2251. IEEE (2017)
    Google Scholar   Download references Author information Authors and Affiliations
    School of Computer Engineering, KIIT Deemed to be University, Bhubaneswar, Odisha,
    India Kushagra Agrawal & Nisharg Nargund Corresponding author Correspondence to
    Kushagra Agrawal . Editor information Editors and Affiliations University of Picardie
    Jules Verne, Amiens, France Stéphane Devismes Indian Institute of Technology Guwahati,
    Guwahati, India Partha Sarathi Mandal Indian Institute of Technology Guwahati,
    Guwahati, India V. Vijaya Saradhi Florida Agricultural and Mechanical University,
    Tallahassee, FL, USA Bhanu Prasad Indian Statistical Institute, Kolkata, India
    Anisur Rahaman Molla Kent State University, Kent, OH, USA Gokarna Sharma Rights
    and permissions Reprints and permissions Copyright information © 2024 The Author(s),
    under exclusive license to Springer Nature Switzerland AG About this paper Cite
    this paper Agrawal, K., Nargund, N. (2024). Deep Learning in Industry 4.0: Transforming
    Manufacturing Through Data-Driven Innovation. In: Devismes, S., Mandal, P.S.,
    Saradhi, V.V., Prasad, B., Molla, A.R., Sharma, G. (eds) Distributed Computing
    and Intelligent Technology. ICDCIT 2024. Lecture Notes in Computer Science, vol
    14501. Springer, Cham. https://doi.org/10.1007/978-3-031-50583-6_15 Download citation
    .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-50583-6_15 Published 04 January
    2024 Publisher Name Springer, Cham Print ISBN 978-3-031-50582-9 Online ISBN 978-3-031-50583-6
    eBook Packages Computer Science Computer Science (R0) Share this paper Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Publish
    with us Policies and ethics Sections Figures References Abstract Introduction
    Fundamentals of Deep Learning Industry 4.0: The Fourth Industrial Revolution Deep
    Learning Techniques and Architectures Data Acquisition and Preprocessing Process
    Optimization and Energy Efficiency Challenges and Limitations Future Trends References
    Author information Editor information Rights and permissions Copyright information
    About this paper Publish with us Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Computer Science (including subseries Lecture Notes in
    Artificial Intelligence and Lecture Notes in Bioinformatics)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Deep Learning in Industry 4.0: Transforming Manufacturing Through Data-Driven
    Innovation'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Qiao J.
  - Peng L.
  - Zhou A.
  - Pan S.
  - Yang P.
  - Ou Z.
  - Yao J.
  citation_count: '0'
  description: The digital twin of electric topology grid refers to the digital twin
    of topology grid, mapping the real world topology grid into a virtual digital
    space, and then fully reflecting the dynamic changes of entities and their relationships
    in the whole life cycle time scale through full coverage of high-density dynamic
    data, so as to achieve intelligent analysis, dynamic decision-making and mutual
    inductance cooperation based on the full model data of digital twin. The digital
    topology grid can affect the twin interaction of real reality and digital reality
    by constructing the closed loop data flow of the electric topology grid. The digitization
    electric network constructed by it relies on the efficient real-time data flow
    to realize the situation awareness, virtual deduction, and operation management
    regulation of the electric grid. Digital twin grid is an inevitable stage for
    the transformation of fresh forms of electric energy, for example, the smart electric
    grid, energy Internet of Things, active topology grid, etc., in recent years,
    and is also an effective test method for new digital infrastructure tasks of electric
    power from 2020. From the perspective of topology grid digital twin object rules,
    this paper uses a semantic understanding method to extract topology grid digital
    object rules, to meet the needs of topology grid digital technology, and promote
    the business appliance of topology grid digital technology.
  doi: 10.1007/978-981-99-7011-7_16
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Multidimensional Signals,
    Augmented Reality and Information Technologies Conference paper Research and Implementation
    of Rules Extraction Technology for Digital Twin Objects in Distribution Network
    Based on Semantic Understanding Conference paper First Online: 02 January 2024
    pp 205–215 Cite this conference paper Access provided by University of Nebraska-Lincoln
    Download book PDF Download book EPUB Multidimensional Signals, Augmented Reality
    and Information Technologies (WCI3DT 2023) Junfeng Qiao, Lin Peng, Aihua Zhou,
    Sen Pan, Pei Yang, Zhujian Ou & Jianguang Yao  Part of the book series: Smart
    Innovation, Systems and Technologies ((SIST,volume 374)) Included in the following
    conference series: The World Conference on Intelligent and 3D Technologies 74
    Accesses Abstract The digital twin of electric topology grid refers to the digital
    twin of topology grid, mapping the real world topology grid into a virtual digital
    space, and then fully reflecting the dynamic changes of entities and their relationships
    in the whole life cycle time scale through full coverage of high-density dynamic
    data, so as to achieve intelligent analysis, dynamic decision-making and mutual
    inductance cooperation based on the full model data of digital twin. The digital
    topology grid can affect the twin interaction of real reality and digital reality
    by constructing the closed loop data flow of the electric topology grid. The digitization
    electric network constructed by it relies on the efficient real-time data flow
    to realize the situation awareness, virtual deduction, and operation management
    regulation of the electric grid. Digital twin grid is an inevitable stage for
    the transformation of fresh forms of electric energy, for example, the smart electric
    grid, energy Internet of Things, active topology grid, etc., in recent years,
    and is also an effective test method for new digital infrastructure tasks of electric
    power from 2020. From the perspective of topology grid digital twin object rules,
    this paper uses a semantic understanding method to extract topology grid digital
    object rules, to meet the needs of topology grid digital technology, and promote
    the business appliance of topology grid digital technology. Keywords Electric
    data analysis Electric distribution data Characteristics of electric data Electric
    data quality Distributed new energy Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others Enterprise-Level
    Model Construction of Distribution Network Topology Based on Graph Database Chapter
    © 2022 A novel application architecture of digital twin in smart grid Article
    10 June 2021 Research and Implementation of Hybrid Storage Method for Multi-source
    Heterogeneous Data of Electric Distribution Network Chapter © 2023 1 Introduction
    Digital Twin is an advanced simulation and analysis technology that integrates
    digital information technology, interactive technology, mathematical logic analysis
    technology, and real-time simulation analysis, which helps solve the electric
    bottleneck problems tucked by the current power system development [1]. The key
    to digital twin lies in the deep coupling, joint simulation, and panoramic presentation
    of information flow and energy flow, to improve the transformation of electric
    grid from control based on simple signals to intelligent control based on information
    physics. However, the fusion and interaction of information flow and energy flow
    will inevitably lead to the transmission of the consequences of network attacks
    suffered by the information system to the electric grid, which is likely to cause
    the chain failure reaction of the information physical system and cause the expansion
    and severity of the failure [2]. With the launch of the ten major tasks of the
    new digital infrastructure of electric power in 2020, digital twins will have
    broad application prospects in the electric grid, and will definitively promote
    the emergence of digital twin dispatching system, digital twin generation system,
    digital twin substation, digital twin topology grid, and other new forms [3].
    Among them, the topology grid is the best practice scenario that can really be
    implemented by the digital twins in terms of coverage, user experience closeness,
    transformation investment, and effectiveness. However, due to the large scale
    of the topology grid, complex network structure, a lot of terminals, and various
    communication modes, its information system and communication network are facing
    great security challenges [4]. Based on digital twin technology and semantic understanding
    method, this paper designs a digital twin-based ubiquitous electric grid network
    of equipment model architecture, and studies and describes the digital twin ubiquitous
    power internet of things model composition (including physical object, virtual
    object, ubiquitous electric power internet of things twin data, service system),
    operation mechanism, etc., to achieve the optimization of fast calculation, equipment
    chain management technology of electric grid internet of things system [5]. 2
    Related Work Before the concept of digital twins was put forward, there were some
    similar definitions, such as digital model, digital shadow, etc. In order to avoid
    confusion among readers, this paper gives clear differences between digital models,
    digital shadows, and digital twins [6]. Digital models are digital representatives
    of real reality, while whether the data collection is real-time enough is uncertain,
    so they belong to static models. There is an inevitable correlation between the
    virtual model and real model [7]. The two affect each other, and the virtual digital
    model can provide guidance for the correct status of the actual power equipment
    and facilities. However, in real data description entities, it is unknown whether
    the data description is correct. Digital twins are not only the description of
    physical entities, but also two-way data transmission between them [8]. From one
    perspective, the situation of real reality is transferred to virtual twins to
    achieve actual-time rectification of virtual twins. On the other hand, the imitation
    and optimization results completed by Digital Twin in the virtual model can also
    reflect real reality to correct real results. So, different data sets interaction
    with real reality is the most important factor of virtual twins and an important
    measure to ensure the synchronization of virtual twins and real reality in the
    state. In recent years, virtual twin has been proposed in the top five important
    technology which respected by company of Gartner for many years (2019, 2020, and
    2021). Meanwhile, American virtual market research institutions emphasize that
    the scale of the virtual twin market will reach 65 billion dollars by 2030 [9].
    Up to now, there have been more than 100 academic articles on digital twins, most
    of which were published after 2016, and the fields involved have gradually expanded
    from the initial aerospace field to manufacturing, navigation, automobile, petroleum,
    and other fields [10]. Virtual technology is a useful technology to realize the
    fast decomposition and analysis of the real reality and the virtual model; a method
    that combines big data governance, artificial intelligence technology, and digital
    construction technology. With virtual digital technology, electric power equipment
    is abstracted into a virtual three-dimensional model, and then through the analysis
    of the three-dimensional model, the problems in the power operation and business
    process are found, which will react to the improvement and improvement of the
    power business. 3 Key Techniques of Rule Extraction for Digital Twin Objects in
    Topology Grid In this article, the basic architecture of the virtual model in
    an electric topology grid is divided into four modules: topology grid physical
    object, information object, and service system and twin data. Physical objects
    are entities, processes, and systems that exist objectively in complex electric
    power network topology. The information object is a multi-dimensional digital
    simulation of physical objects. It mainly simulates, optimizes, and evaluates
    the various parts of the complex electric power network topology, and conducts
    fast electric power operation index prediction, regulation, and evaluation of
    its operation process. The service system is the general name of all kinds of
    data-driven business functions for different power customers and relevant government
    departments. It is mainly responsible for providing support and services for the
    intelligent control of complex electric power network topology driven by twin
    data. The data contained by virtual objects, service systems, and physical objects,
    as well as their multi-dimensional fusion data sets, constitute the twin data
    system of complex electric power network topology, which is the driving force
    for electric grid operation and interaction. 3.1 Digital Twin Physical Object
    and Information Object of Topology Grid Physical objects can acquire real-time
    and accurate data and evaluate the operation method of the electric grid. Compared
    with the current power system, the complex electric power network topology which
    was built by the digital twin method, in addition to the existing functions, also
    has the ability to collect and integrate multi-dimensional real-time data for
    its physical objects, as well as the ability to integrate all elements of the
    complex electric power network topology in the “customer grid computer environment”.
    All parts of the complex electric power network topology can not only design their
    own action mechanism according to operation data, state sensing data, and interference
    data, but also carry out collaborative control and optimization of their respective
    functions under the goal of global optimization. For semantic understanding, it
    is important to first establish a plan semantic understanding model, establish
    a plan entity analysis model based on regular expressions, and establish a plan
    entity standardization model based on a text similarity algorithm. By integrating
    the above models, the semantic understanding, semantic analysis, semantic standardization,
    and other functions of the power grid fault handling plan are realized, including
    notification, startup and shutdown, unit output adjustment, power flow control,
    and control voltage. The entity library of fault handling plans such as control
    frequency strongly supports the application of fault handling plans in regulating
    business scenarios. The information object in the article mainly refers to the
    collection of digital models. We build these models from three aspects: role,
    behavior, and indicators. The complex electric power network topology based on
    digital twins collects and processes the real-time data and operation status of
    real entities during operation, and conducts real-time and dynamic regulation
    and improvement of its operation process through digital simulation under the
    condition that the physical prototype is highly consistent. At the same time,
    the digital complex electric power network topology model and related information
    can interact with real reality in real time to achieve the integration of digital
    complex electric power network topology and physical objects. As is shown in Fig.
    16.1, the digital twin information object consists of the grid layer and platform
    layer of complex electric power network topology. The network layer uses edge
    computing to reprocess the terminal sensing data, distribution terminal data,
    regional user load data, power Internet of Things data, etc., and upload them
    to the remote layer through 5G and other intelligent sensing technologies of wireless
    networks and advanced technologies of wired networks. Fig. 16.1 Data extraction
    of digital twin model in topology grid Full size image 3.2 Electric Topology Grid
    Digital Twin Service System and Twin Data The topology grid digital twin service
    system refers to the collection of all functions that drive the complex electric
    power network topology system to fully operate by using the data obtained from
    information objects and the model built, such as equipment operation situation
    awareness, load density analysis and power flow of the topology grid system, active
    perception of low-voltage faults in the operation and distribution of the complex
    electric power network topology and early prediction of electric grid risks. The
    complex electric power network topology twin data mainly consists of four parts:
    data perceived by physical objects, data simulated by information objects, data
    processed by service systems, and data generated by the integration of the three.
    The twin data of physical objects are composed of complex equipment files related
    to power, main network of the power system, and equipment data of the distribution
    system. The twin data of the information object includes the data received in
    real time during the operation of the complex electric power network topology
    and the data perceived and constructed during the operation. The data related
    to the service system includes the data from top management to down operation
    control. The continuous data fusion of complex electric power network topology
    twin data and the iteration and increase of its data are the driving forces to
    complete the operation of physical objects, service systems, information objects,
    and the mutual fusion of the two. The natural language semantic understanding
    algorithm is generally applied in the field of power grid fault identification
    due to its excellent feature extraction capability. The semantic understanding
    of an equipment fault handling plan is essentially a text classification task.
    The text data is treated as a dimensional time series and processed according
    to one-dimensional images to achieve text semantic understanding modeling. 4 Implementation
    of Digital Twin Technology for Application Requirements of Topology Grid Operation
    Scenarios The virtual model of the topology grid mainly collects the basic archive
    data and status operation data of the physical measurement and control equipment,
    protection measurement and control equipment, temperature monitoring equipment,
    operating environment monitoring equipment, and other equipment in the topology
    grid, and then perceives the basic information data of physical objects through
    high-precision micro intelligent sensing technology, distributed advanced sensing
    technology, and land navigation technology. Then in the network layer, edge computing
    is carried out for distribution terminals, load management terminals, and physical
    gateways through terminal awareness. Wired network transmission and wireless network
    transmission based on stereo sensing technology are realized through peer Ethernet
    and power carriers. The digital twin technology of the topology grid simulates
    the power grid through real-time control, multi-network coordination, and full-service
    monitoring in the digital twin processing system after collecting the power grid
    information and other relevant real-time state information. Finally, on the basis
    of multi-dimensional data processing and fusion, the knowledge base and experience
    base based on simulation data are constructed. 4.1 Power Consumption Prediction
    of Topology Grid Based on Digital Twin Technology The power consumption of users
    is related to economy, humidity, temperature, season, and human factors, and the
    effect on electricity customers is different in different environments. In the
    article, the most important influencing factor is to describe the differences
    between users’ consumption and distinctive effects, and the contraction of each
    affective factor is statistically sorted to remove the factors with low coupling
    and is very important for reducing power loss in transmission lines. In the statistical
    calculation of the relevance coefficient, parameter a is the electric power consumption
    situation, parameter b represents the electricity consumption index. The similarity
    between the two data sets depends on the curve fitting degree of the two data
    sets. Parameter C is an ordered even set, including a variable and b variable,
    and the parameter C|E represents the proportion of C in parameter E. $$\\mathrm{FID}\\left(\\mathrm{C}\\right)=\\underset{\\mathit{ab}<B(|C|)}{\\mathrm{max}}\\frac{I*(C,a,b)}{{\\mathrm{log}}_{2}\\mathrm{min}\\{a,b\\}}$$
    (16.1) As is shown in formula (16.1), I*\\(\\left(C,a,b\\right)=\\underset{E}{\\mathrm{max}}(C|E)\\)
    is the FID under area E. F is a monotone increasing function and satisfies \\(\\mathrm{F}\\left(\\mathrm{i}\\right)=\\mathrm{o}(\\mathrm{i})\\).
    For data set \\({C}_{k}\\) (k = 1, 2, …, n), FID is used to evaluate the correlation
    degree between various influencing factors and the electric operation index. The
    solution process of this data set is divided into several steps, ranking each
    effective factor, and influencing factors of data value. Due to different input
    parameters, extract several indicators with typical power operation characteristics
    into this data set. The data set composed of these indicators can fully display
    and describe the consumption of electricity customers. For the data set \\(\\mathrm{C}=\\left\\{{a}_{i},{b}_{i}\\right\\}\\)
    containing l data set and j role electricity characteristics, where |C|=l, the
    integration model of the X training tree uses k sub-functions to get the following
    calculation result: $$\\mathrm{b}=\\mathrm{\\varphi }\\left({a}_{i}\\right)=\\sum_{k=1}^{n}{f}_{k}\\left({a}_{i}\\right),
    {f}_{k}\\in \\mathrm{M}$$ (16.2) In formula (16.2), \\(\\mathrm{M}=\\left\\{f\\left(a\\right)=wq\\left(a\\right)\\right\\}\\).
    Where M represents the node position at the beginning of traversal, q represents
    the data pattern of this model, and parameter Y represents the order of the corresponding
    terminal of the structure. Each Mk represents the connectivity path between any
    two nodes. Use \\({w}_{i}\\) to calculate the value range of each terminal node.
    Given a sample, use the calculation configuration of the data structure to be
    divided into multiple calculation steps. At the same time, the sum of the scores
    of the related terminal nodes is added as the last calculating output. $$\\left\\{\\begin{array}{c}N\\left(\\varphi
    \\right)=\\sum_{i=1}^{k}n\\left({\\widehat{b}}_{i},{b}_{i}\\right)+\\sum_{k}\\Omega
    {M}_{k}\\\\ \\Omega \\left(M\\right)=\\gamma Y+\\frac{1}{2}\\lambda {|\\left|a\\right||}^{2}\\end{array}\\right.$$
    (16.3) As is shown in the formula (16.3), N represents the key factor for calculating
    line loss, which is the difference between the predicted value of electricity
    consumption and the target value, and Ω is a restricted interval to determine
    the time interval and performance of the calculation process. 4.2 Implementation
    of Digital Twin Technology for Power Topology Grid The following describes the
    operation process of the twin electric topology grid from three aspects: key equipment,
    overall regional power consumption, and middle-voltage electric supply scope of
    influence. The operation process of the analysis is shown in Fig. 16.2, then the
    specific process is shown below. Fig. 16.2 Digital twin realization of topology
    grid for power application Full size image As is shown in Fig. 16.2, high-performance
    data acquisition equipment is used to collect real-time massive data during electric
    operation and maintenance, equipment failure, and use in the electric grid topology.
    Based on the gathered real-time data of power grid equipment and the historical
    data of the service cloud platform of the electric device, the real-time data
    of the electric grid device is collated, the quality comparison algorithm is used
    to compare with the historical data of the service platform, and the specification
    data of the service cloud platform of power grid equipment is used to judge whether
    the power grid equipment in operation meets the quality requirements. If it does
    not meet the quality requirements, it will convey to the probable electric device
    with virtual data and find the three-dimensional maintenance instructions in the
    complex electric grid graph topology experience database. The three-dimensional
    maintenance instructions are associated with the relevant maintenance operation
    instructions through the actual equipment maintenance services, and a digital
    twin model based on high fidelity is built. This model forms the three-dimensional
    maintenance instructions by simulating the maintenance process of electric multi-type
    devices and improves the accuracy of maintenance. The data of the power grid device
    after maintenance shall be collected and iterated interactively with the matching
    twin model. When all requirements of power supply quality control are completed,
    it shall be iterated interactively with the simulation data of shafting equipment
    consumption, value depreciation and failure in daily operation, etc. The virtual
    model can detect these data, as well as find out the cause of the accident through
    multi-dimensional cause analysis, analyze, and find out potential power grid operation
    faults. Compare and interact with the current expected state defined by the digital
    twin model of ubiquitous power Internet of Things, evaluate the current operating
    state of ubiquitous power Internet of Things, and achieve equipment whole-process
    management, equipment fault analysis, equipment network access status detection
    technologies of ubiquitous electric grid frame topology. According to the acquired
    user profile information, process the real-time data and historical data of power
    consumption, make statistics on all relevant influencing factors, train the corresponding
    power consumption prediction model for power users 1, 2,…, and n using the twin
    support vector machine algorithm, and fuse all the prediction results to obtain
    the overall power consumption prediction model for each region. When there is
    a load problem in the whole area, it interacts with the matching power twin model,
    finds a three-dimensional maintenance guide in the ubiquitous electric grid frame
    topology experience database, and simulates the electric power flow calculation
    process to improve the prediction and analysis of the matching degree of power
    customer’s power load and power supply. If there is no topology error and insufficient
    power supply in the whole area, the simulation data of shafting fatigue wear,
    torsional vibration, thermal stress, and other simulation data generated by the
    digital twin model will be iterated interactively to complete the assessment of
    power grid recover-ability and health status of the electric grid. Compare and
    interact with the current expected state defined by the digital twin model of
    complex electric grid frame topology Internet of Things, evaluate the current
    operating state of complex electric Internet of Things, and achieve iterative
    optimization of fault diagnosis, prediction, and health management technologies
    of ubiquitous power Internet of Things system. Based on real-time measurement
    data in public transformer areas and special transformer areas, weather, environment,
    and other data of other relative data, as well as the historical data of low-voltage
    power supply service cloud platform, access the fault recording data by connecting
    the digital twin knowledge base and experience base. When problems occur in the
    low-voltage power supply area, it interacts with the matching low-voltage power
    supply system twin model, finds three-dimensional maintenance instructions in
    the ubiquitous power Internet of Things experience database, and simulates the
    fault maintenance process on the digital twin model. When there is no problem
    in the low-voltage power supply area, it interacts with the real data generated
    by the digital twin model and the fusion data obtained through multi-dimensional
    data processing and fusion to realize active awareness of low-voltage faults in
    the operation and distribution deployment process. Compare and interact with the
    current expected state defined by the digital twin model of the topology grid,
    evaluate the current operation state of the complex electric Internet of Things,
    and realize the equipment networking posture, sudden probability of equipment
    operation failure, and equipment operation situation awareness of the complex
    electric Internet of Things system. 5 Application Results A digital twin model
    is built for generating units in power plants to analyze and predict their working
    life and operating conditions. The intelligent management system of the generator
    set based on a digital twin monitors the bearing vibration and shaft speed of
    the entity in real time by current power sensor, angular speed sensor, etc. Based
    on the specification data, stored historical data, and real-time collected data,
    the system describes the components of the generator set from three dimensions
    of physics, information, and behavior on the cloud platform, and integrates the
    high-fidelity digital twin model of the generator set. The entity of the generator
    set and its digital twin information model operate together. The twin data generated
    include the data sensed during operation, as well as the simulation data of shafting
    fatigue wear, torsional vibration, thermal stress, etc., generated by the digital
    function. The virtual technology of the generator set includes condition monitoring
    service, equipment operation service, optimized operation service, and maintenance
    guidance service. The condition monitoring model is mainly used by the operation
    and maintenance personnel to check the internal operation status of the generator
    set through the digital twin information model built, compare and integrate the
    expected operation status of the twin digital model with the actual operation
    status of the generator set, and control the overall working status of the generator
    set. The semantic understanding-based digital twin rule extraction method proposed
    in this article can make the construction of digital twins in power equipment
    more efficient and easy, reduce manual intervention, and reduce the errors of
    digital twin models, providing technical support for future automated digital
    twin model construction. The fault diagnosis service comprehensively analyzes
    the digital twin model of the generator set and the relevant data of the generator
    set entity to accurately warn the generator set of overspeed, broken shaft, and
    other accidents. The optimization operation service mainly uses the dynamic interaction
    between the digital twin model and the physical entity of the generator set to
    complete the iterative optimization of the bearing opening and operation parameters.
    The maintenance guidance service mainly simulates the maintenance process of the
    generator set based on the digital twin model by querying the maintenance operation
    experience database, forming a three-dimensional operation experience database,
    which provides an effective way for the reliable operation and maintenance of
    the generator set. References Zhang, K., Lu, G.Y., Wu, L., et al.: Validation
    and analysis of image information for sentence semantic understanding and representation.
    J. Comput. Sci. 44(3), 15 (2021) Google Scholar   Huo, T.T.: Design and application
    of virtual simulation technology for power system relay protection. J. Heilongjiang
    Inst. Technol. Compreh. Edn. (007), 022 (2022) Google Scholar   Sun, P.Y.: Research
    on Network Attack Reconstruction and Simulation Technology of Power Information
    Physical System. Zhejiang University (2020) Google Scholar   Cao, Y., Xu, A.D.,
    Tao, W.W., et al.: Performance Analysis and Simulation of Power Internet of Things
    LPWAN. China Southern Power Grid Technol. (008), 014 (2020) Google Scholar   Zhao,
    H., Miao, K., Li, Z., et al.: Research on edge sensing adaptive data processing
    technology of power Internet of things. Single Chip Microcomput. Embedded Syst.
    Appl. 21(011), 34–37, 41 (2021) Google Scholar   Cheng, Y. M., Jiang, M., Ma,
    Y. L.: Design of real-time monitoring of power customer flow data based on metadata.
    Inf. Techno. 44(4), 5 (2020) Google Scholar   Chen, J.X., Liang, L., Fu, J.F.,
    et al.: Research on smart grid data dispatching and fast distribution method based
    on big data. Electric Measur. Inst. 57(6), 6 (2020) Google Scholar   Zhao, G.S.,
    Liu, D.W., Chen, S.Y., et al.: Power system transient stability boundary analysis
    from the perspective of data. Power Gener. Technol. 41(2), 167–174 (2020) Google
    Scholar   Zhao, Q., Wang, X.Y., Qiao, J.: Key technologies for data-driven modeling
    and simulation of energy Internet. Power Inf. Commun. Technol. 18(1), 7 (2020)
    Google Scholar   Zeng, J.R., Li, P., Gao, L., et al.: Detection of false data
    injection attacks in smart grid based on TNPE. Sci. Technol. Work Safety China
    (2021) Google Scholar   Download references Acknowledgements This work was supported
    by State Grid Corporation of China’s Science and Technology Project (5108-202218280A-2-396-XG)
    which is ’Research on rapid construction and linkage analysis technology of distribution
    network facilities digital twins integrated with electrical topology’. Author
    information Authors and Affiliations State Grid Laboratory of Power Cyber-Security
    Protection & Monitoring Technology, State Grid Smart Grid Research Institute Co.,
    LTD., Nanjing, 210003, Jiangsu, China Junfeng Qiao, Lin Peng, Aihua Zhou, Sen
    Pan & Pei Yang Nantong Power Supply Company, State Grid Jiangsu Electric Power
    Supply Co., LTD, Nanjing, 210014, Jiangsu, China Zhujian Ou & Jianguang Yao Corresponding
    author Correspondence to Junfeng Qiao . Editor information Editors and Affiliations
    Technical University of Sofia, Sofia, Bulgaria Roumen Kountchev Interscience Institute
    of Management and Technology, Bhubaneswar, Odisha, India Srikanta Patnaik Shanghai
    Institute of Technology, Shanghai, China Wenfeng Wang TK Engineering, Sofia, Bulgaria
    Roumiana Kountcheva Rights and permissions Reprints and permissions Copyright
    information © 2024 The Author(s), under exclusive license to Springer Nature Singapore
    Pte Ltd. About this paper Cite this paper Qiao, J. et al. (2024). Research and
    Implementation of Rules Extraction Technology for Digital Twin Objects in Distribution
    Network Based on Semantic Understanding. In: Kountchev, R., Patnaik, S., Wang,
    W., Kountcheva, R. (eds) Multidimensional Signals, Augmented Reality and Information
    Technologies. WCI3DT 2023. Smart Innovation, Systems and Technologies, vol 374.
    Springer, Singapore. https://doi.org/10.1007/978-981-99-7011-7_16 Download citation
    .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-99-7011-7_16 Published 02 January
    2024 Publisher Name Springer, Singapore Print ISBN 978-981-99-7010-0 Online ISBN
    978-981-99-7011-7 eBook Packages Intelligent Technologies and Robotics Intelligent
    Technologies and Robotics (R0) Share this paper Anyone you share the following
    link with will be able to read this content: Get shareable link Provided by the
    Springer Nature SharedIt content-sharing initiative Publish with us Policies and
    ethics Sections Figures References Abstract Introduction Related Work Key Techniques
    of Rule Extraction for Digital Twin Objects in Topology Grid Implementation of
    Digital Twin Technology for Application Requirements of Topology Grid Operation
    Scenarios Application Results References Acknowledgements Author information Editor
    information Rights and permissions Copyright information About this paper Publish
    with us Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Smart Innovation, Systems and Technologies
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Research and Implementation of Rules Extraction Technology for Digital Twin
    Objects in Distribution Network Based on Semantic Understanding
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mughal F.R.
  - He J.
  - Zhu N.
  - Hussain S.
  - Zardari Z.A.
  - Mallah G.A.
  - Piran M.J.
  - Dharejo F.A.
  citation_count: '0'
  description: The heterogeneous cluster networks (HCN) have recently benefited from
    federated learning (FL). On distributed data, FL is used to train privacy-preserving
    models. In heterogeneous networks (HetNet) and the Internet of Things (IoT), FL
    implementation is challenged by resource optimization, robustness, and security
    issues. There is a significant risk of data security being compromised by disregarding
    the nodes’ clustering behavior and quickly varying asynchronous streaming data.
    Moreover, in HCN-based wireless sensor networks (WSNs), FL enhances asynchronous
    node performance. Using naturally clustered HCN, distributed nodes train a local
    and global model collectively. In this paper, we propose a Intra-Clustered FL
    (ICFL) model. By optimizing computation and communication, ICFL selects heterogeneous
    FL nodes in each cluster. Despite heterogeneous data, it is highly robust. There
    are currently no FL frameworks that can handle varying data quality across devices
    and non-identical distributions. With ICFL, sensitive asynchronous data is not
    exposed to possible misuse while adapting to changing environments. In addition
    to being time-efficient, our strategy requires low-power computing nodes. According
    to our extensive simulation results, ICFL performs better than FedCH in terms
    of computational performance and provides flexible conditions under which ICFL
    is more efficient in terms of communication. In extensive testing, ICFL decreased
    training rounds by 62% and increased accuracy by 6.5%. It can execute evaluations
    7.46 times faster than existing models, and its average accuracy has increased
    by 4.39%. A resource-aware FL system can be successfully implemented in real-time
    applications according to our research.
  doi: 10.1016/j.comcom.2023.10.026
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related work 3. Proposed work 4.
    Experimental results and discussions 5. Conclusion Declaration of competing interest
    Data availability References Vitae Show full outline Figures (9) Show 3 more figures
    Tables (1) Table 1 Computer Communications Volume 213, 1 January 2024, Pages 236-245
    Resource management in multi-heterogeneous cluster networks using intelligent
    intra-clustered federated learning Author links open overlay panel Fahad Razaque
    Mughal a, Jingsha He a, Nafei Zhu a, Saqib Hussain a, Zulfiqar Ali Zardari b,
    Ghulam Ali Mallah c, Md. Jalil Piran d, Fayaz Ali Dharejo e Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.comcom.2023.10.026 Get rights and content
    Abstract The heterogeneous cluster networks (HCN) have recently benefited from
    federated learning (FL). On distributed data, FL is used to train privacy-preserving
    models. In heterogeneous networks (HetNet) and the Internet of Things (IoT), FL
    implementation is challenged by resource optimization, robustness, and security
    issues. There is a significant risk of data security being compromised by disregarding
    the nodes’ clustering behavior and quickly varying asynchronous streaming data.
    Moreover, in HCN-based wireless sensor networks (WSNs), FL enhances asynchronous
    node performance. Using naturally clustered HCN, distributed nodes train a local
    and global model collectively. In this paper, we propose a Intra-Clustered FL
    (ICFL) model. By optimizing computation and communication, ICFL selects heterogeneous
    FL nodes in each cluster. Despite heterogeneous data, it is highly robust. There
    are currently no FL frameworks that can handle varying data quality across devices
    and non-identical distributions. With ICFL, sensitive asynchronous data is not
    exposed to possible misuse while adapting to changing environments. In addition
    to being time-efficient, our strategy requires low-power computing nodes. According
    to our extensive simulation results, ICFL performs better than FedCH in terms
    of computational performance and provides flexible conditions under which ICFL
    is more efficient in terms of communication. In extensive testing, ICFL decreased
    training rounds by 62% and increased accuracy by 6.5%. It can execute evaluations
    7.46 times faster than existing models, and its average accuracy has increased
    by 4.39%. A resource-aware FL system can be successfully implemented in real-time
    applications according to our research. Previous article in issue Next article
    in issue Keywords Internet of ThingsFederated learningResource managementData
    transmissionHeterogeneous cluster networksCooperative-learning 1. Introduction
    A smart city can provide a number of vital real-time applications, such as intelligent
    vehicles, smart industries, health and education, and intelligent monitoring systems
    [1], [2]. To implement these intelligent solutions properly, many wireless sensor
    nodes are required. By 2030, there will probably be 50 billion connected nodes
    to the Internet of Things (IoT), as reported by the authorities. Massive amounts
    of data will be generated by these nodes. By 2025, nodes’ data will total 79.4
    zettabytes (ZB), according to recent research [3]. This substantial expansion
    of the IoT data capacity opens up a lot of possibilities for artificial intelligence
    (AI) [4]. It is for this purpose that centralized machine learning (ML) algorithms
    enable intelligent IoT applications. As a result of data transportation between
    the end device and the central server, centralized ML suffers from fundamental
    privacy and security issues. It is possible to support these intelligent IoT applications
    by using centralized ML algorithms [5]. Centralized ML algorithms have inherent
    privacy and security leakage problems, even though end-device data must be sent
    to a third-party server for training. Moreover, centralized ML may be feasible
    if data quantities are enormous (e.g., astronomy information) and distributed
    across many locations [6]. Identical and independent distributions are considered
    to be the most significant indicator of data heterogeneity. Comparatively, application
    heterogeneity refers to computing capabilities, communication resources, and end-device
    storage capacities. There are no state-specific, more practical difficulties such
    as heterogeneous data and applications. Decentralized ML allows for asynchronous
    computation of models over widely separated nodes [7]. Furthermore, distributed
    and centralized ML techniques that rely on actual models or resources are not
    entirely parallel and secure. Federated learning (FL) has been proposed as a solution
    to ensure privacy in ML while dealing with the heterogeneity of systems and data
    [8]. While traditional ML methods require data to be moved from end devices to
    a central edge or cloud server, FL does not require this. Local learning parameters
    are computed at end devices before being transmitted to the edge or cloud server
    for global model aggregation in FL. In the end, a global model parameter is sent
    to the end devices [9]. IoT will present several challenges when it comes to deploying
    FL. It takes substantial computing resources and standby electricity to compute
    a local learning model for FL. Moreover, FL-based IoT applications with an incredible
    number of sensors appear to be easier to deploy. Because FL permits end-device
    ML, it faces optimization challenges related to security, stability, and resources
    (computing and communication). To reap the benefits of FL in IoT, these issues
    must be addressed. Communication load balances prevent IoT devices from communicating
    with essential base stations (BS). End devices can participate in FL through cooperative
    FL [10]. In lieu of the BS, cooperative FL allows resource-controlled end devices
    to communicate their local learning models to neighboring devices. Provides for
    a local combination of the receiving processes and the local models received.
    A locally clustered model is sent to a centralized edge server or cloud-to-global
    combination server. In different contexts of the ML model, collaborative FL can
    also be cloud-based or edge-based. By exploiting knowledge model limitations,
    malicious proxy servers can collect end-device data, resulting in privacy leaks.
    It can be used as a mechanism for addressing asymmetrical privacy-aware FL [11].
    To improve the accuracy of a local learning model, a substantial amount of data
    must be collected. Therefore, it is difficult to train a local model that is more
    accurate for computing devices with limited resources. For a local model train,
    the data from resource IoT devices can be sent to a neighboring trustworthy. In
    a typical day, self-driving vehicles create 4000 GB of information [12]. Thus,
    dispatching everyday self-drive cars will require outstanding resource communication.
    It is possible to send the entire data set to the combination server for local
    model learning with smaller resources. Furthermore, autonomous vehicles with sufficient
    energy have a higher computational power. Therefore, FL for self-driving cars
    can be easily implemented regarding resource usage. As smart gadgets, like smartphones,
    tablets, and wearable devices, have grown in number and type, technological communication
    networks and networking technologies have evolved. These networks (or wireless
    body area networks) include cellular networks, IoT, and personal computers. Device-to-device
    (D2D) communications, holographic images, haptic communications, and vehicle networks
    [13]. It is expected that future applications and services will be able to take
    advantage of such networks by increasing data rate, coverage, and connection,
    as well as reducing latency, reducing energy consumption, and improving dependability.
    Thus, achieving this goal in large-scale, adaptable, and dynamic wireless networks
    is complex because it requires careful allocation of resources and administration
    [14]. Intra-clustered federated learning (ICFL) is primarily concerned with securing
    node management. Using a cluster of corporate network nodes with limited computing
    resources, this study shows low-power nodes sending data to corporate network
    clusters for processing. Data transmission, however, optimizes computing and communication
    resources. Therefore, this article proposes using cluster-based data transmission
    and node resources to ensure that data is delivered individually to each cluster
    station. In data transfers between nodes, node selection is done securely without
    revealing any information. At the HCN, Clusters, and CFS layers, we level different
    categorization parameters to clarify their relation to FL-enabled HCNs and IoTs.
    Our main contributions are summarized as follows. • In HCN, low computing power
    nodes are unsuitable for FL due to their low computational power and efficiency.
    Currently, no framework enables low computing power nodes to transfer data to
    clusters. FL implementations on HCN architectures face challenges in submitting
    data from such resource-constrained nodes. It is necessary to find a way to allow
    nodes with limited computing power to participate in FL by sending their data
    to clusters within the hierarchical topology of the HCN. • Data transmission across
    heterogeneous nodes with varying capabilities is managed using FL in HCNs. In
    order to enable FL, an efficient resource-determined heterogeneous cluster network
    is required. The paper proposes the development of a new ICFL. Therefore, resources
    must be allocated following the environment learning task. Assigning additional
    properties reduces convergence time and node energy in FL. • A wide-ranging training
    model is developed here by combining resource allocation for ICFL in the HCN setting.
    A novel communication resource-efficient federated optimization scheme for ICFL.
    Reliable communication in the middle of the clusters and cluster federated servers
    is reliable. The nodes in a cluster calculate their local learning and global
    models iteratively, exchanging them with other clusters to produce. Built on HCN
    in this experiment differs from previously proposed solutions DFL [15], FL-DRL
    [16], and FedCH [17]. We propose state-of-the-art ICFL to train models for heterogeneous
    cluster networks and IoT. Wireless networks, network resources, and computational
    resources are required for these nodes. Different nodes, including cluster nodes
    and cluster federated servers, are involved in FL across wireless networks to
    communicate effectively. According to theoretical analysis, ICFL can be more time-efficient
    under comfortable circumstances. By enabling effective intra-clustered FL for
    the scene of HCN across an IoT, computational and communication resources can
    be optimized. Finally, we assess the proposed ICFL built on HCN by comparing the
    most widely used techniques, DFL [15], FL-DRL [16], and FedCH [17]. FedCH approach
    generates local model updates on a low-performance development platform, rather
    than real devices, for testing purposes. Both DFL and FL-DRL use each cluster’s
    components. The remainder of the article is structured as follows. Section 2 covers
    Related Work, while Section 3 discusses the Proposed Work. Section 4 presents
    Simulation Results and Discussion, and finally, Section 5 provides the Conclusion.
    2. Related work Three significant areas of current heterogeneous networks with
    FL solutions can be categorized: (1) clustering-based learning, (2) adaptive optimization,
    and (3) data augmentation. The success of ML has been enhanced by a number of
    key factors. This is primarily due to the massive amount of data gathered. Secondly,
    there is the issue of computational power. With advances in technology, we are
    moving away from standard smart devices in favor of scalability and integrated
    microcircuits. It is possible to train models more quickly and deploy them directly
    on devices with lower computational costs using these devices. AI-ready smartphones,
    for instance, have an AI chip pre-installed to make them considerably intelligent,
    helping humans in day-to-day activities more effectively. Two significant barriers
    prevent many domains from utilizing ML’s advantages: ML and deep learning, which
    give ML models much-needed intelligence [18]. The success rate of self-taught
    deep learning algorithms is commendable. Creating a third-party system and increasing
    client trust are the main reasons for choosing this method. A FL framework assisted
    by deep reinforcement learning (DRL) is presented in [16]. For Industrial Internet-of-Things
    (IIoT) devices, they developed a FL algorithm with DRL assistance. A distributed
    approach is used to train DRL agents separately based on the data characteristics
    of IIoT devices. To ensure successful FL for IIoT applications, this DRL-aided
    FL scheme is applied to each IIoT device. Data generated by IIoT devices can result
    in redundant training data, drain wireless channels, and compromise privacy. A
    centralized authority oversees the training and learning processes according to
    the original server and diverse client network architecture [19]. Despite FL’s
    decentralized data strategy, a centralized server is still required to gather
    training sets from clients active in FL environments, create a global model, and
    make it available to them all. Research from [20] suggests three algorithms to
    achieve a customized local model with a minimum number of communication rounds.
    The authors recommend using the algorithms separately or in combination, depending
    on the use cases of user access clustering, data extrapolation, and model extrapolation.
    By using the user clustering technique, clusters are produced by combining data
    from clients with similar distributions. In order to speed up the convergence
    of the global model, an interim model is created for each client. A hypothesis-based
    clustering technique is used to detect clusters. In [21], the authors proposed
    a method of federated multi-task learning that finds FL client clusters based
    on the similarity measures of local models. Based on the estimated cosine similarity,
    the original set of customers is recursively divided into clusters. Plants in
    innovative industrial parks have IIoT devices that are physically adjacent, that
    allow them to automatically cluster into groups and to be coupled by extremely
    accurate networks, such as regional 5G networks. Industrial FL is limited by the
    underutilization of centralized access resources at the edges, and this vital
    quality is frequently overlooked [22]. Using local datasets, the server updates
    the model using shared data and the local models of the clients [23]. FL accuracy
    is increased by these methods because they share clients’ local datasets, but
    there is a concern that sensitive data may be leaked. Furthermore, openly accessible
    datasets are not always available, particularly in industries with sensitive data.
    Furthermore, it is forbidden to disclose or alter private raw data in any way
    [24]. An IoT-based distributed FL (DFL) system for smart industries was presented
    in [15]. Three critical components were identified for establishing DFL for C-IoT:
    learning algorithms, incentive mechanisms, and resource optimization (computation
    and communication). C-IoT DFL was enabled by resource minimization in [15]. Under
    the DFL framework, the authors formulated a binary linear optimization problem
    to reduce the overall cost of federated learning. This is the first study to consider
    resource optimization for DFL in smart industries utilizing C-IoT. Particular
    industries (such as manufacturing, logistics, and transportation) frequently face
    greater data risks due to their extensive possession of important information
    [25]. Due to this, these industries must strengthen security to secure data urgently
    and crucially. Accordingly, they described how FL can be used to reduce communication
    overheads in wireless technology applications [26]. In addition to actual production
    implementations, many research proposals with practical application use cases
    explore the use of FL to develop privacy-preserving ML systems. The authors in
    [27], presented a clustering strategy based on ALQR that improves cooperative
    communication outcomes regarding node asymmetry, network lifetime, and energy
    usage of network nodes. To manage asymmetric links and distribute energy consumption,
    the ALQR protocol rotates cluster heads between clusters, allocates appropriate
    resources, and divides groups. A solution to connection asymmetry is to choose
    an accessible, resourceful device. Communicates actively, passively, and cooperatively.
    Fedvision is a protected FL object detection method that Webank has presented
    in [28] for application use cases in the security field, including avoiding data
    leaks in automotive cyber–physical methods, predicting transportation flow, and
    detecting objects using secure FL [29]. In [30], Federated Stochastic Expectation
    Maximization (FedSEM) was considered as a model that is trained on several global
    models to solve problems. Using a Distance-based Federated loss function (DF-Loss),
    multi-center FL identifies the best global model among various global models.
    A random initialization and multiple restarts are used by IFCA to identify clients
    that belong to a particular cluster and achieve optimal values. According to IFCA’s
    experimental data, the proposed approach performs well in both convex and non-convex
    neural networks. [31] provided the Iterative Federated Cluster Algorithm (IFCA)
    framework, which reduces FL clients’ loss functions and tags them with clusters
    during training. Two variations are proposed based on the source for averaging
    IFCA: model and gradient. In [17], FedCH has been proposed as an effective FL
    accelerator for heterogeneous edge computing. In contrast to prior efforts that
    follow the established system structure and train models in synchronous or asynchronous
    ways, FedCH builds a unique cluster architecture and performs aggregation in a
    hierarchical structure for training. According to the diverse training capacities
    of FedCH’s clients, it divides them into several groups. According to various
    train capabilities (e.g., networking and computing), FedCH groups every client
    into K clusters. According to [32], a technique that performed well when FL was
    performed within each cluster, and FL models were created for each cluster. As
    opposed to our goal of training one FL model that can be used by all FL customers,
    this goal is different. As proposed in [33], FedRL combines reinforcement ML with
    FL to achieve customized A.I. By using FL, FedRL implements transfer learning
    through a secure model when direct transfer learning is not possible. In [34],
    a real-world example is transferring a global model from a server to a personalized
    user, which can then be applied to a specific user’s innovative (IoT) devices.
    The devices were equipped with cameras that capture product photos and use ML
    models to identify product categories [35]. In this way, the recognized items
    can be accurately selected for further assembly and shipment. Before the items
    can be picked, the product identification process must be completed. Through better
    use of FL clients (active devices), these studies aim to speed up convergence.
    For instance, recent work [36] aims to increase the number of substantial FL clients
    with more local data so that the global model can converge with fewer iterations.
    In addition to training FL models, stakeholders in a healthcare ecosystem may
    have additional needs. As an example, a pharmaceutical corporation may want to
    integrate data from different hospitals to aid drug development. Even though it
    can be difficult to fairly compensate hospitals without a way to evaluate the
    calibre of each hospital’s local data [37], pharmaceutical companies may have
    to offer incentive payouts for participating hospitals as compensation. The FL
    subfield evaluates the contributions of FL participants [38]. By assessing the
    final FL model’s performance, each FL participant is evaluated without disclosing
    their private local data. The GTG-Shapley [39] algorithm and the contribution-aware
    FL model aggregation algorithm. Using the AI Engine GTG-Shapley system architecture,
    the FL server efficiently computes the contributions of participants. Fed-SV proposed
    in [40] approaches the “federated Shapley value” using group testing-based estimations.
    The differences are that SVs are estimated independently each round and aggregated
    at the end, and SVs are evaluated using reconstructed sub-models that take into
    account the utility values of subsets for estimating Shapley differences. 3. Proposed
    work Our goal in this paper is to develop a ICFL framework that allows models
    to be trained in heterogeneous cluster networks (HCN) and IoT environments. Through
    wireless networks, FL involves many nodes, such as end devices and cluster federated
    servers. To successfully interact with one another, these nodes need wireless
    channels, core network resources, and computational resources. For FL global model
    computation, we can use cluster federated server collaboration. By taking into
    account all three layers: HCN, Clusters, and CFS, they are placed in the correct
    order. Our next step is to go over the parameters of FL via HCN and IoT. According
    to Fig. 1, the key parameters include the global model, resources, local learning
    models, clusters, clustered FL, clustered federated server, and heterogeneous
    cluster network architecture. To clarify the association between FL-enabled HCN
    and IoT, Fig. 1 addresses key categorization factors at three layers, HCN, clusters,
    and CFS. In addition to local models and design, HCN also trains local models
    for ICFL according to local policies. In addition to local models, we trained
    global models on a cluster federated server. To deal with heterogeneous data,
    a novel FL protocol is required, and one possible solution is to cluster statistically
    homogeneous IoT devices. Each cluster has a cluster head responsible for aggregating
    local learning models. As shown in Fig. 1, this method combines reinforcement
    learning with FL. When direct collaborative learning is not possible, FL is used
    to implement cooperative learning through a secure model in heterogeneous cluster
    networks (HCN). Download : Download high-res image (361KB) Download : Download
    full-size image Fig. 1. Intra-clustered FL diagram. Devices for cluster heads
    must be selected based on optimal eligibility criteria. A cluster head selection
    criterion can, for example, boost the cluster’s overall performance. Socially
    aware clustering is another option. Cluster heads can infer sensitive information
    from IoT nodes or agents through their local learning model, so forming cluster
    heads with a high social trust is preferable, as shown in Fig. 2. It is possible
    to calculate a global model within each cluster, just as it is possible in cluster
    FL. A cluster federated server receives global models from cluster heads and produces
    ICFL, which is then given to the cluster heads. Cluster heads update global models
    for nodes or agents in each cluster. Through communication-assisted FL, nodes
    and agents form clusters based on attractive characteristics (such as community,
    environment, or interaction). In order to create a global model, the nodes or
    agents in a cluster calculate their local learning model and continuously exchange
    it with the others. Global model processing is advantageous in that it allows
    cluster members to reuse previously used ranges. Nodes or agents receive the global
    model changes from cluster heads, which are then distributed to them. Based on
    HCN, Algorithm 1 illustrates detailed steps of federated intra-clustered learning
    for IoT nodes or agents. Download : Download high-res image (408KB) Download :
    Download full-size image Download : Download high-res image (93KB) Download :
    Download full-size image Fig. 2. Mechanism of reinforcement learning phase at
    each cluster. In addition, quality control and monitoring of local systems and
    wireless resources are essential in FLs with IOT and HCN networks. In this paper,
    we propose a cooperative learning and communication architecture for wireless
    FL. A detailed analysis of FL for HCN networks is presented in the study. By combining
    wireless resource distribution, power control, and user selection, the FL role
    was optimized. Scalability of FL can be achieved through resource optimization,
    low computational energy nodes, compression techniques, and communication of knowledge
    model parameters. By optimizing the resource technique for secure resource transmission,
    more nodes can collaborate in the FL experience, resulting in more excellent performance.
    Groups of nodes are usually involved in the FL method. This will allow more nodes
    to participate in asynchronous FL techniques that require local model calculation
    time for all networked nodes by selecting low computational energy (cluster-cycles/s)
    nodes. We present ICFL learning for learning multi-tasks with data distributed
    across different nodes (i.e., data heterogeneity system). To allow FL for various
    tasks, a resource-controlled heterogeneous wireless network must arrange processes
    efficiently. In addition, resources must be distributed according to the type
    of learning task. It is therefore necessary to develop new collaborative mechanisms
    for allocating ICFL resources. Global cluster federated servers communicate more
    frequently with clusters than intra clustering servers in distributed FL. Therefore,
    we can use strong link-learning strategies (such as block codes) to secure communications
    between the local cluster server and the global cluster federated server. 3.1.
    ICFL on clusters resource management activities HCN and IoT Enabling efficient
    intra-clustered FL for the scene of IoT across an HCN network enhances both communication
    and computational optimization of resources, as shown in Fig. 3. The computation
    resource can be local computing resource clusters and global computation resources
    clustered on federated servers. The performance of a cluster (i.e., local accuracy
    and calculation time) is primarily controlled by available energy, cluster extent,
    and resource computing cluster rounds/s to evaluate the local learning model.
    The protocol definition that the clusters will initially provide with the input
    in the number of nodes is depicted in Fig. 6. With standard accuracy, the cluster
    size and computing node resource cluster rounds/s directly impact the calculation
    and local learning model. The following energy usage for a node with local iterations,
    operating frequency f, and cluster size C are given as follows. (1) where C and
    stand the node reliant on continuous parameter and the number of node cycles taken
    respectively, the computational time for the local learning model is: (2) As can
    be seen in (1), (2), there is a balance between reducing energy consumption and
    calculating the local model. For a given local accuracy and node operational frequency,
    the number of clusters controls the computational phase of the local model. For
    a precise local model and stable cluster nodes, the computation time can be reduced
    by lowering the operation frequency of the node. Because of this, we must make
    a trade-off between cluster operation frequency and energy consumption when calculating
    a local model. There is a wide variation in computational resources, cluster number,
    operating frequency, and energy availability between ICFL nodes. Due to the wide
    range of clustering attributes, local learning models have varying accuracy for
    an identified model. Download : Download high-res image (276KB) Download : Download
    full-size image Fig. 3. Resources of intra-clustered FL, Flow chart of sequence
    diagram. Using the concept of comparative local accuracy , we can account for
    clusters’ different local model performances due to heterogeneity. A lower local
    accuracy value indicates a more accurate local model. After each cluster computes
    its local model, the learning parameters are forwarded to the cluster federated
    server. It is possible to run local model clusters on a central cluster server
    if the ICFL technique is used. In standard FL, the global model follows the cluster
    server. Global model updates are notified to clusters. All nodes’ local learning
    model updates are shared between cluster-enabled nodes in a ICFL model. A distributed
    network securely communicates the local learning parameters of the model among
    the clusters. Upon agreement, all clusters send the block containing the nodes’
    local learning models to their respective clusters, where the global model, the
    heterogeneous wireless technique, and the variation of nodes are sent. The time
    transmission it takes to send model updates between nodes and clusters is determined.
    The node rate , interval and model of scale combination server, calculate: (3)
    The energy used during transmission from clusters used during communication transmits
    energy to the combination server is: (4) The previous descriptions of computational
    and communication resource utilization can be used to calculate communication
    and computational overheads. For local model calculations, the total number of
    local iterations might represent the computational cost. The total global iterations
    among the clusters and the combination server can be used to measure communication
    overhead. A global cluster federated server approach has disturbing relationships
    between accuracy, computation, and communication. As processing costs decrease,
    the delivery ratio frequently increases. The global iterations for comparative
    global accuracy and local accuracy are: (5) (6) It can be seen in (6) that to
    reduce the comparative local accuracy for a specific global accuracy, further
    local iterations are necessary (i.e., to maximize local model accuracy). here
    is a constant determined by the local cluster subproblem and nodes, Fig. 5 shows
    the changes in local, global accuracy, and global, local iterations. To achieve
    lower overall local accuracy results simultaneously, additional local and global
    iterations are required. In light of the preceding considerations, there must
    be a practical approach to determining the operational frequency range for local
    nodes. The nodes’ operating frequencies reduce energy consumption, local model
    calculation latency, and local model accuracy for a given cluster of points. Local
    models are transmitted to the CF server at various latencies based on the data
    transmission, and the data transmission varies significantly from one node to
    another. Smart cities and industries can benefit from energy and latency improvements
    with multi-tasking ICFLs. We will implement a multi-task ICFL algorithm that optimizes
    latency and energy simultaneously. Therefore, combined latency and energy optimization
    is required for ICFL over HCN networks. In addition, the ICFL approach recommended
    in this study performed better than any other method. The training data produced
    outstanding results, and the intra clustered learning approach updated the model
    effectively. As described in this article, the low computing power of the node
    constrains the standard FL technique. The model is adversely affected by its accuracy.
    In addition to F1-Score, Precision, and Recall, three other variables are examined
    in the study. On a per-class basis, these measures can be used to evaluate classifier
    output: accuracy, F1 score, and recall. (7) (8) (9) (10) 4. Experimental results
    and discussions 4.1. Experimental environment In this section, To test ICFL based
    on cluster resources management, multiple heterogeneous cluster networks and one
    clustered federated server are set up in Fig. 4. Approximately 45% of the data
    in each cluster comes from randomly extracted nodes, ensuring that each cluster
    stores essentially separate data. In this experiment, we compared the proposed
    algorithm to DFL, FL-DRL, and FedCH to verify its accuracy on HCN. In the clusters
    data traffic test, the proposed scheme is compared to the HCN network with intra-clustered
    FL in terms of network traffic load. The cluster query per second (CQPS) assesses
    traffic resources using the top list. Data is delivered directly to the clustered
    federated server and cluster queries are offered as balance traffic tests. A greater
    number of randomly chosen service nodes was added in order to respond to requests
    more quickly and accurately. This situation uses the trust model to determine
    the impact on the network. The cluster data traffic test results are shown in
    Table 1. Experimental results indicate that the technique optimizes data flow
    without affecting network performance. Download : Download high-res image (316KB)
    Download : Download full-size image Fig. 4. Deployment of heterogeneous cluster
    networks using ICFL based on clusters resources management. In this part, we performed
    cluster validation the cluster’s (local learning model) accuracy of Intra-Clustered
    FL and CFS’s global learning model accuracy in Fig. 5 shows the experimental outcomes.
    Table 1. Clusters data traffic test results. CQPS [QP/s] Cluster query traffic
    [MB/s] Cluster traffic resources and uplink or downlink [%] Size [MB/] 20 20.001
    99.80 20.005 40 40.004 99.70 40.016 60 60.006 99.50 60.023 80 80.008 99.35 80.029
    100 100.010 99.10 100.036 To calculate the reliability and accuracy of the proposed
    ICFL built on HCN, we compare it to DFL, FL-DRL, and FedCH. In the experiment,
    FedCH is updated locally on a limited node (small progress board). Despite the
    similarities between DFL and FL-DRL, a piece cluster is used only for local training
    in FL-DRL. Download : Download high-res image (231KB) Download : Download full-size
    image Fig. 5. Global learning model accuracy and Local learning model accuracy
    with various access methods: we can see as rounds enhance as accuracy enhance.
    4.2. Experiment of ICFL based on clusters resources management individuality To
    validate the optimization of both computing and communication resources presented
    in this paper, this section performs the following tests. With 1000 training rounds,
    we compare the ICFL proposed in this manuscript with the standard FL optimization
    techniques DFL, FL-DRL, and FedCH. Communication resources are already occupied
    by cellular users in the DFL training mode, allowing for a more efficient use
    of communication resources. In order to expand the number of sub-global model
    rounds, communication and local computing resources are required. With the assistance
    of DRL, FL training can be performed in wireless network contexts. The development
    of IOT can be facilitated if data sets are used to reduce model deviation and
    private data is used to train the models. Data generated by IOT is being trained
    and collected in large quantities. In FedCH training mode, the trained model is
    sent to the edge network for combined training after being trained in tiny act
    devices. As a final step, the edge server drives are triggered by device learning
    outcomes. In this manuscript, the ICFL sends training data to the CFS global learning
    model, and the CFS global learning model sends training results to the cluster
    local learning model and tests the experimental results on the same test clusters
    as in the previous manuscript. According to Fig. 6, clusters 1, 2, 3, and 4 clearly
    illustrate the accuracy of all the techniques learned and evaluated using nodes
    from clusters 1, 2, 3, and 4. The accuracy of models assessed on nodes varies
    significantly due to the data distribution between nodes. The accuracy of the
    technique presented container scope after 1000 training rounds is the best for
    cluster 1, even though accuracy testing on cluster 1 is the worst for cluster
    3. FL, on the other hand, is done on cluster 3 and has the highest accuracy rate.
    The results are shown in Fig. 6. According to the simulation results, the developed
    ICFL technique performs best on Cluster 3. A comparison of the proposed ICFL method
    to the standard FL method can be found in Fig. 6. Accordingly, the presented ICFL
    technique is more accurate than other techniques. ICFL in HCN can be undertaken
    by nodes with low computational power. Download : Download high-res image (284KB)
    Download : Download full-size image Fig. 6. Cluster’s accuracy with various access
    methods. For the scenario presented in this paper, Fig. 7, Fig. 8 illustrate the
    results of many techniques. In this paper, the accuracy and F1-score of various
    techniques are primarily discussed. Based on the above two figures, our model
    achieves more than 80% accuracy for Cluster1, Cluster2, Cluster3, and Cluster4
    of HCN, two times higher than the other three methods. There is a substantial
    difference between Cluster 1 and other techniques in terms of accuracy, although
    Cluster 1 is not exceptionally high. F1 score in Fig. 8 shows chains. The technique
    achieved better feature knowledge accuracy in this paper, as per the test results.
    However, the technique for Cluster 1 training outcomes of the transmission has
    no effect. Download : Download high-res image (324KB) Download : Download full-size
    image Fig. 7. Cluster’s precision, recall and F1-Score with various access methods.
    In the context of HCN and IoT, ICFL is proposed. FL is a new machine-learning
    technique that aims to ensure nodes are secure while data is transferred between
    them. In an IoT setting with limited computing resources, this study successfully
    delivered data from low-power nodes to CSF clusters. In conclusion, this study
    shows that using ICFL for secure communication and node identification prevents
    information from being leaked. In order to facilitate learning, node data is only
    delivered to specified connections identified by clusters. Download : Download
    high-res image (164KB) Download : Download full-size image Fig. 8. Clusters accuracy
    of each method with various cluster and Clusters F1-Score of each method with
    various cluster. The FL illustrates the concept of decentralized learning. Clusters
    can secure nodes by training models based on local information. By collecting
    local model changes from the clusters, the global model was updated iteratively.
    The clusters update the global model and send it back to the nodes. An iterative
    process continues until a federated cluster server reaches the efficiency of a
    global model. 5. Conclusion In this paper, ICFL was proposed to enable multiple
    heterogeneous cluster networks using FL, while enhancing their lifetime reliability
    and energy efficiency. Cluster query per second (CQPS) was used to measure performance
    metrics such as average accuracy, local accuracy, Precision, Recall, F1-Score,
    and global accuracy. To select a ICFL model that efficiently distributes resources,
    satisfying the preferences of the nodes, was the main objective. Through dynamic
    simulations and network traffic tests, the approach demonstrates superior performance
    compared to previous techniques. ICFL distributes cluster nodes rapidly and outperforms
    existing methods (DFL, FL-DRL, and FedCH), increasing network lifespan while reducing
    computational time and energy consumption. Furthermore, the proposed technique
    is best suited to real-time applications. According to extensive network testing,
    ICFL reduced training rounds by 62% on average and increases accuracy by 6.5%.
    ICFL performed evaluations 7.46 times faster than other models, increasing the
    model’s accuracy by 4.39 percent. According to our research, the first resource-aware
    FL system has been implemented successfully in real-time applications. It is our
    intention to examine if it is feasible to examine resource quality at both receivers
    and senders in the future. Recent studies have only examined resources used at
    the receiver and sender. Since real-time communication occurs across many cluster
    nodes, the resource quality must be implemented at both source and destination.
    Declaration of competing interest The authors declare that they have no known
    competing financial interests or personal relationships that could have appeared
    to influence the work reported in this paper. Data availability Data will be made
    available on request. References [1] Singh S., Sharma P.K., Yoon B., Shojafar
    M., Cho G.H., Ra I.-H. Convergence of blockchain and artificial intelligence in
    IoT network for the sustainable smart city Sustainable Cities Soc., 63 (2020),
    Article 102364 View PDFView articleView in ScopusGoogle Scholar [2] Al-Turjman
    F., Zahmatkesh H., Shahroze R. An overview of security and privacy in smart cities’
    IoT communications Trans. Emerg. Telecommun. Technol., 33 (3) (2022), Article
    e3677 View in ScopusGoogle Scholar [3] Nguyen D.C., Ding M., Pathirana P.N., Seneviratne
    A., Li J., Niyato D., Dobre O., Poor H.V. 6G Internet of Things: A comprehensive
    survey IEEE Internet Things J. (2021) Google Scholar [4] Wu Y., Dai H.-N., Wang
    H. Convergence of blockchain and edge computing for secure and scalable IIoT critical
    infrastructures in industry 4.0 IEEE Internet Things J., 8 (4) (2020), pp. 2300-2317
    Google Scholar [5] Mughal F.R., He J., Zhu N., Almutiq M., Dharejo F.A., Jain
    D.K., Hussain S., Zardari Z.A. An intelligent Hybrid-Q Learning clustering approach
    and resource management within heterogeneous cluster networks based on reinforcement
    learning Trans. Emerg. Telecommun. Technol. (2023), Article e4852 Google Scholar
    [6] Adi E., Anwar A., Baig Z., Zeadally S. Machine learning and data analytics
    for the IoT Neural Comput. Appl., 32 (20) (2020), pp. 16205-16233 CrossRefView
    in ScopusGoogle Scholar [7] Liu Y., Zhu Y., James J. Resource-constrained federated
    learning with heterogeneous data: Formulation and analysis IEEE Trans. Netw. Sci.
    Eng. (2021) Google Scholar [8] Nguyen D.C., Ding M., Pham Q.-V., Pathirana P.N.,
    Le L.B., Seneviratne A., Li J., Niyato D., Poor H.V. Federated learning meets
    blockchain in edge computing: Opportunities and challenges IEEE Internet Things
    J., 8 (16) (2021), pp. 12806-12825 CrossRefView in ScopusGoogle Scholar [9] Liu
    L., Zhang J., Song S., Letaief K.B. Client-edge-cloud hierarchical federated learning
    ICC 2020-2020 IEEE International Conference on Communications (ICC), IEEE (2020),
    pp. 1-6 Google Scholar [10] Lu Y., Huang X., Zhang K., Maharjan S., Zhang Y. Blockchain
    empowered asynchronous federated learning for secure data sharing in internet
    of vehicles IEEE Trans. Veh. Technol., 69 (4) (2020), pp. 4298-4311 CrossRefView
    in ScopusGoogle Scholar [11] Zhou Z., Chen X., Li E., Zeng L., Luo K., Zhang J.
    Edge intelligence: Paving the last mile of artificial intelligence with edge computing
    Proc. IEEE, 107 (8) (2019), pp. 1738-1762 CrossRefView in ScopusGoogle Scholar
    [12] Mashwama P., Fashoto S.G., Mbunge E., Gwebu S. Development of a mobile inter-vehicular
    communication system based on gossip algorithm Int. J. Interact. Mob. Technol.,
    14 (11) (2020) Google Scholar [13] Janjua M.B., Duranay A.E., Arslan H. Role of
    wireless communication in healthcare system to cater disaster situations under
    6G vision Front. Commun. Netw., 1 (2020), Article 610879 View in ScopusGoogle
    Scholar [14] Wang J., Jiang C., Zhang H., Ren Y., Chen K.-C., Hanzo L. Thirty
    years of machine learning: The road to Pareto-optimal wireless networks IEEE Commun.
    Surv. Tutor., 22 (3) (2020), pp. 1472-1514 CrossRefView in ScopusGoogle Scholar
    [15] Khan L.U., Alsenwi M., Yaqoob I., Imran M., Han Z., Hong C.S. Resource optimized
    federated learning-enabled cognitive internet of things for smart industries IEEE
    Access, 8 (2020), pp. 168854-168864 CrossRefView in ScopusGoogle Scholar [16]
    Zhang P., Wang C., Jiang C., Han Z. Deep reinforcement learning assisted federated
    learning algorithm for data management of IIoT IEEE Trans. Ind. Inform., 17 (12)
    (2021), pp. 8475-8484 CrossRefView in ScopusGoogle Scholar [17] Wang Z., Xu H.,
    Liu J., Xu Y., Huang H., Zhao Y. Accelerating federated learning with cluster
    construction and hierarchical aggregation IEEE Trans. Mob. Comput. (2022) Google
    Scholar [18] Mothukuri V., Parizi R.M., Pouriyeh S., Huang Y., Dehghantanha A.,
    Srivastava G. A survey on security and privacy of federated learning Future Gener.
    Comput. Syst., 115 (2021), pp. 619-640 View PDFView articleView in ScopusGoogle
    Scholar [19] Kairouz P., McMahan H.B., Avent B., Bellet A., Bennis M., Bhagoji
    A.N., Bonawitz K., Charles Z., Cormode G., Cummings R., et al. Advances and open
    problems in federated learning Found. Trends® Mach. Learn., 14 (1–2) (2021), pp.
    1-210 CrossRefView in ScopusGoogle Scholar [20] Mansour Y., Mohri M., Ro J., Suresh
    A.T. Three approaches for personalization with applications to federated learning
    (2020) arXiv preprint arXiv:2002.10619 Google Scholar [21] Sattler F., Müller
    K.-R., Samek W. Clustered federated learning: Model-agnostic distributed multitask
    optimization under privacy constraints IEEE Trans. Neural Netw. Learn. Syst.,
    32 (8) (2020), pp. 3710-3722 Google Scholar [22] Zeng S., Li Z., Yu H., He Y.,
    Xu Z., Niyato D., Yu H. Heterogeneous federated learning via grouped sequential-to-parallel
    training International Conference on Database Systems for Advanced Applications,
    Springer (2022), pp. 455-471 CrossRefView in ScopusGoogle Scholar [23] Yoshida
    N., Nishio T., Morikura M., Yamamoto K., Yonetani R. Hybrid-FL for wireless networks:
    Cooperative learning mechanism using non-IID data ICC 2020-2020 IEEE International
    Conference on Communications (ICC), IEEE (2020), pp. 1-7 CrossRefGoogle Scholar
    [24] Zhao Z., Feng C., Hong W., Jiang J., Jia C., Quek T.Q., Peng M. Federated
    learning with non-iid data in wireless networks IEEE Trans. Wireless Commun.,
    21 (3) (2021), pp. 1927-1942 Google Scholar [25] Wen H., Wu Y., Yang C., Duan
    H., Yu S. A unified federated learning framework for wireless communications:
    Towards privacy, efficiency, and security IEEE INFOCOM 2020-IEEE Conference on
    Computer Communications Workshops (INFOCOM WKSHPS), IEEE (2020), pp. 653-658 CrossRefView
    in ScopusGoogle Scholar [26] Chen M., Poor H.V., Saad W., Cui S. Wireless communications
    for collaborative federated learning IEEE Commun. Mag., 58 (12) (2020), pp. 48-54
    CrossRefGoogle Scholar [27] Mughal F.R., He J., Zhu N., Mallah G.A., Qiao Z.,
    Haider A., Hussain S., Hussain M.I., Zardari Z.A. A new asymmetric link quality
    routing protocol (ALQR) for heterogeneous WSNs Microprocess. Microsyst. (2022),
    Article 104617 View PDFView articleView in ScopusGoogle Scholar [28] Y. Liu, A.
    Huang, Y. Luo, H. Huang, Y. Liu, Y. Chen, L. Feng, T. Chen, H. Yu, Q. Yang, Fedvision:
    An online visual object detection platform powered by federated learning, in:
    Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, 2020,
    pp. 13172–13179. Google Scholar [29] Liu Y., James J., Kang J., Niyato D., Zhang
    S. Privacy-preserving traffic flow prediction: A federated learning approach IEEE
    Internet Things J., 7 (8) (2020), pp. 7751-7763 CrossRefView in ScopusGoogle Scholar
    [30] Xie M., Long G., Shen T., Zhou T., Wang X., Jiang J., Zhang C. Multi-center
    federated learning (2020) arXiv preprint arXiv:2005.01026 Google Scholar [31]
    Ghosh A., Chung J., Yin D., Ramchandran K. An efficient framework for clustered
    federated learning Adv. Neural Inf. Process. Syst., 33 (2020), pp. 19586-19597
    Google Scholar [32] Fallah A., Mokhtari A., Ozdaglar A. Personalized federated
    learning: A meta-learning approach (2020) arXiv preprint arXiv:2002.07948 Google
    Scholar [33] Lim H.-K., Kim J.-B., Kim C.-M., Hwang G.-Y., Choi H.-b., Han Y.-H.
    Federated reinforcement learning for controlling multiple rotary inverted pendulums
    in edge computing environments 2020 International Conference on Artificial Intelligence
    in Information and Communication (ICAIIC), IEEE (2020), pp. 463-464 CrossRefView
    in ScopusGoogle Scholar [34] Chen Y., Qin X., Wang J., Yu C., Gao W. Fedhealth:
    A federated transfer learning framework for wearable healthcare IEEE Intell. Syst.,
    35 (4) (2020), pp. 83-93 Google Scholar [35] Arachchige P.C.M., Bertok P., Khalil
    I., Liu D., Camtepe S., Atiquzzaman M. A trustworthy privacy preserving framework
    for machine learning in industrial IoT systems IEEE Trans. Ind. Inform., 16 (9)
    (2020), pp. 6092-6102 CrossRefView in ScopusGoogle Scholar [36] Cho Y.J., Wang
    J., Joshi G. Client selection in federated learning: Convergence analysis and
    power-of-choice selection strategies (2020) arXiv preprint arXiv:2010.01243 Google
    Scholar [37] Lyu L., Xu X., Wang Q., Yu H. Collaborative fairness in federated
    learning Federated Learning, Springer (2020), pp. 189-204 CrossRefView in ScopusGoogle
    Scholar [38] Ghorbani A., Zou J. Data shapley: Equitable valuation of data for
    machine learning International Conference on Machine Learning, PMLR (2019), pp.
    2242-2251 Google Scholar [39] Liu Z., Chen Y., Yu H., Liu Y., Cui L. GTG-Shapley:
    Efficient and accurate participant contribution evaluation in federated learning
    ACM Trans. Intell. Syst. Technol., 13 (4) (2022), pp. 1-21 Google Scholar [40]
    Wang T., Rausch J., Zhang C., Jia R., Song D. A principled approach to data valuation
    for federated learning Federated Learning, Springer (2020), pp. 153-167 View in
    ScopusGoogle Scholar Cited by (0) Fahad Razaque Mughal currently he is doing Ph.D.
    in Faculty of Information Technology, Beijing University of Technology, China.
    received his B.S from Sindh University Jamshoro and M.S degree from Indus University
    Karachi in Sindh, Pakistan 2011 and 2017 respectively. He has published research
    papers as a 1st and co-author in national and international journals. His research
    interest area is Homogeneous and Heterogeneous Networks, Mobile ad hoc networks,
    Wireless Communications, Information Security, Network Security in Deep learning
    and Machine learning. Jingsha He received the bachelor’s degree in computer science
    from Xi’an Jiaotong University, China, and the master’s and Ph.D. degrees in computer
    engineering from the University of Maryland, College Park, MD, USA. He worked
    for several multinational companies in USA, including IBM Corp., MCI Communications
    Corp and Fujitsu Laboratories. He is currently a Professor with the Faculty of
    Information Technology, Beijing University of Technology (BJUT), Beijing. He has
    published more than ten articles. He holds 12 U.S. patents. Since August 2003,
    he has been published over 300 papers in scholarly journals and international
    conferences. He also holds over 84 patents and 57 software copyrights in China
    and authored nine books. He was a principal investigator of more than 40 research
    and development projects. His research interests include information security,
    wireless networks, and digital forensics. Nafei Zhu received her B.S. and M.S.
    degrees from Central South University, China in 2003 and 2006, respectively, and
    her Ph.D. degree in computer science and technology from Beijing University of
    Technology in Beijing, China in 2012. From 2015 to 2017, she was a postdoc and
    an assistant researcher in the Trusted Computing and Information Assurance Laboratory,
    State Key Laboratory of Computer Science, Institute of software Chinese Academy
    of Sciences in China. She is now on the Faculty of Information Technology at Beijing
    University of Technology. Dr Zhu has published over 20 research papers in scholarly
    journals and international conferences (16 of which have been indexed by SCI/EI/ISTP).
    Her re-search interests include information security and privacy, wireless communications
    and network measurement. Saqib Hussain currently a Ph.D student in software Engineering
    Department of Beijing University of Technology majoring in software Engineering.
    His MS degree is also in software Engineering from Beijing Institute of Technology.
    He has been a part of many R&D projects. His research interests include Deep Learning,
    Image Processing, and IOT. Zulfiqar Ali Zardari Ph.D. in software engineering
    from Beijing university of Technology. Received his B.E. and M.E degree from Mehran
    University of Engineering and Technology Jamshoro in Sindh, Pakistan. Currently
    he is doing Ph.D. in Faculty of Information Technology, Beijing University of
    Technology, China. He has published six research papers as a first and co-author
    in national and international journals. His research interest area is Mobile ad
    hoc networks, Wireless communications, Information Security, sensor network security,
    Computer networks and Network Security. Ghulam Ali Mallah received the master’s
    degree from Quaid-iAzam University Islamabad and the Ph.D. degree in computer
    science on HEC sponsorship and the Post doctorate degree in educational technology
    from Glasgow University, Scotland, U.K., on sponsorship of the British Council.
    He is currently the Head of computer science with Shah Abdul Latif University,
    Khairpur, Pakistan. His research interests include software agents, artificial
    intelligence, and digital image processing, and so on. He has visited more than
    a dozen countries to present his research work. He has more than 40 HEC recognized
    national and international research articles at his credit. He has supervised
    two dozen M.S. /Ph.D. students. He is actively involved in various national and
    international professional bodies for quality assurance in HEIs, curriculum designing,
    accreditations, tertiary education support program, development of strategic and
    business plans for HEIs, and international linkages. He is a member of various
    national and international academic forums/bodies, including the IEEE and ACM.
    He is the Winner of 04 HEC-Funded Research and Development Projects and the ICT
    Excellence Award in the category of IT Education. He is the Organizer of the four
    consecutive International Conferences on Computer and Emerging Technologies and
    one International Conference on Research in Education and Technologies, in 2017.
    Md. Jalil Piran holds a distinguished academic background and currently serves
    as an Associate Professor at the Department of Computer Science and Engineering
    in Sejong University, Seoul, South Korea. He earned his Ph.D. degree in Electronics
    and Information Engineering from Kyung Hee University, South Korea, in 2016, and
    subsequently worked as a Post-Doctoral Fellow at the Networking Laboratory of
    the same institution. Prof. Piran has made significant contributions to the field
    of Intelligent Information and Communication Technology (IICT) through his extensive
    research publications in esteemed international journals and conferences. His
    areas of expertise encompass Machine Learning, Data Science, Wireless Communications
    and Networking, 5G/6G, Internet of Things (IoT), and Cyber Security. In addition
    to his research endeavors, Prof. Jalil Piran actively engages with scholarly journals
    as an Editor, including the “IEEE Transactions on Intelligent Transportation Systems,”
    “Elsevier Journal of Engineering Applications of Artificial Intelligence,” “Elsevier
    Journal of Physical Communication,” and “Elsevier Journal of Computer Communication.”
    He also serves as the Secretary of the IEEE Consumer Technology Society on Machine
    Learning, Deep Learning, and AI. Furthermore, he assumes the role of Track Chair
    for Machine Learning, Deep Learning, and AI in theCE (MDA) Track for the upcoming
    2024 IEEE International Conference on Consumer Electronics (ICCE). In 2022, he
    chaired the “5G and Beyond Communications” Session at the prestigious IEEE International
    Conference on Communications (ICC). His expertise as a reviewer extends to leading
    journals, and he actively participates in various conferences. Prof. Piran is
    esteemed as a Senior Member of IEEE and represents South Korea as an Active Delegate
    to the Moving Picture Experts Group (MPEG). His outstanding research contributions
    have been recognized internationally, as evidenced by the prestigious “Scientist
    Medal of the Year 2017” awarded by IAAM in Stockholm, Sweden. Moreover, he received
    accolades from the Iranian Ministry of Science, Technology, and Research as an
    “Outstanding Emerging Researcher” in 2017. His exceptional Ph.D. dissertation
    was honored as the “Dissertation of the Year 2016” by the Iranian Academic Centre
    for Education, Culture, and Research in the Engineering Group. Fayaz Ali Dharejo
    (Senior IEEE Member). He is currently Postdoc fellow at Khalifa University since
    September 2022. He received his Ph.D. Degree in Computer Applied Engineering from
    the Institute of Computer Network Information Center Chinese Academy of Sciences,
    University of Chinese Academy of Sciences Beijing, China in 2022. He received
    B.E. degree in Electronic Engineering from QUEST, Pakistan in 2016 and received
    M.S. degrees from University of Electronic Science and Technology of China, Chengdu,
    China in 2018. He has published more than 30 articles within in reputed ISI impact
    factor journals including; IEEE TFS, ACM TIST, International Journal of Intelligent
    Systems, IEEE TCBB, IEEEE GRSL and etc. He is a member of professional bodies
    such as PEC, ACM, IEEE, IEEE Societies, such as IEEE Geoscience and Remote Sensing
    Society, IEEE Computer Society and IEEE Signal Processing Society. His research
    interests are image enhancement, lightweight models, and computer vision. View
    Abstract © 2023 Elsevier B.V. All rights reserved. Recommended articles Enabling
    simulation services for digital twins of 5G/B5G mobile networks Computer Communications,
    Volume 213, 2024, pp. 33-48 Giovanni Nardini, Giovanni Stea View PDF OSF-EIMTC:
    An open-source framework for standardized encrypted internet traffic classification
    Computer Communications, Volume 213, 2024, pp. 271-284 Ofek Bader, …, Chen Hajaj
    View PDF Multi-mobile vehicles task offloading for vehicle-edge-cloud collaboration:
    A dependency-aware and deep reinforcement learning approach Computer Communications,
    Volume 213, 2024, pp. 359-371 Shanchen Pang, …, Yawu Zhao View PDF Show 3 more
    articles Article Metrics Captures Readers: 1 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Computer Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Resource management in multi-heterogeneous cluster networks using intelligent
    intra-clustered federated learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Raja Gopal S.
  - Prabhakar V.S.V.
  citation_count: '3'
  description: 'Internet of Things (IoT) acts as an important role in the area of
    farming to enhance quality and productivity. In this paper, an intelligent edge
    based module with LoRa and IoT is proposed for smart farming. Smart farming consists
    of five layered architecture including edge computing to improve quality of data
    (QoD) and latency performance. For QoD, a Double selecting algorithm is used,
    and performance is measured using the following parameters: communications latency,
    data collecting time, energy consumption, and data quality. Quality of data is
    100% for the implemented double selection approach and energy consumption, communication
    latency, data collection time parameters are also minimum compared to other approaches.
    A test-bed for smart farming and auto-irrigation is implemented using LoRa and
    cloud. The proposed test-bed is evaluated in real-time, with temperature, humidity,
    and soil moisture being relayed to the cloud on a regular basis using LoRa, and
    the results assessed. With improved QoD and latency performance, the suggested
    intelligent edge based smart farming test-bed with LoRa and IoT delivers good
    acceptable results for smart farming and auto-irrigation.'
  doi: 10.1007/s13198-021-01576-z
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home International Journal of System Assurance
    Engineering and Management Article Intelligent edge based smart farming with LoRa
    and IoT Original article Published: 31 January 2022 Volume 15, pages 21–27, (2024)
    Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    International Journal of System Assurance Engineering and Management Aims and
    scope Submit manuscript S. Raja Gopal & V. S. V. Prabhakar  548 Accesses 4 Citations
    Explore all metrics Abstract Internet of Things (IoT) acts as an important role
    in the area of farming to enhance quality and productivity. In this paper, an
    intelligent edge based module with LoRa and IoT is proposed for smart farming.
    Smart farming consists of five layered architecture including edge computing to
    improve quality of data (QoD) and latency performance. For QoD, a Double selecting
    algorithm is used, and performance is measured using the following parameters:
    communications latency, data collecting time, energy consumption, and data quality.
    Quality of data is 100% for the implemented double selection approach and energy
    consumption, communication latency, data collection time parameters are also minimum
    compared to other approaches. A test-bed for smart farming and auto-irrigation
    is implemented using LoRa and cloud. The proposed test-bed is evaluated in real-time,
    with temperature, humidity, and soil moisture being relayed to the cloud on a
    regular basis using LoRa, and the results assessed. With improved QoD and latency
    performance, the suggested intelligent edge based smart farming test-bed with
    LoRa and IoT delivers good acceptable results for smart farming and auto-irrigation.
    Similar content being viewed by others Agriculture Management Based on LoRa Edge
    Computing System Chapter © 2020 Smart Farming System Based on Fog Computing and
    LoRa Technology Chapter © 2020 IoT-Based Agricultural Automation Using LoRaWAN
    Chapter © 2022 1 Introduction Human resources required for farming may increase
    operational costs and the impact on productivity and quality may not improve.
    Integration of IoT with farming deploys remote management and monitoring systems
    and leads to the concept of smart farming. IoT for smart farming relies on wireless-sensor-networks
    (WSN). Further, the inclusion of intelligence and cloud computing along with IoT
    offer online analytics, real time monitoring and remote management. To support
    long distance communication with low power, LoRa technology can also be used in
    smart farming. Due to limitations and local regulations, LoRa does not support
    high data rates. In addition, the integration of Edge and Fog computing with LoRa
    into IoT systems can help to achieve a high level of energy efficiency for sensor
    nodes. When some abnormalities occur, Fog-assisted IoT systems can provide support
    for latency-critical applications. Lot of research is being done for smart farming
    to improve productivity and efficiency. Many researchers have proposed various
    algorithms and approaches to collect data from sensor nodes in wireless sensor
    networks meeting the current requirements (Xiong 2011; Pilloni 2013; Gao 2013;
    Rodríguez 2015; Tomovic 2016; Hassani 2017). Significance of internet of things,
    big data, cloud computing and fog computing, their opportunities and challenges
    are well described by various authors in (Wolfert 2017; Morota 2018; Schepers
    2018; Fleming 2018; Weersink 2018; Elijah 2018). Green house, smart farming effects
    are presented in (Trinh, 2018; Pathak, 2019; Dalezios 2019; Li 2019). In (Gia
    2019), author demonstrated edge AI based smart farming with IoT. The author, Xiaomin
    Li et. al. illustrated edge-computing enabled smart agriculture where double selection
    algorithm is used for multiple task data collection from sensor nodes in wireless
    sensor networks.This paper implements five layered architecture for smart farming
    using edge layer, LoRa and IoT. Double selection algorithm (Li, 2020) is used
    for the improvement of quality of data (QoD) and latency performance in wireless
    sensor networks. Because of its potential uses in a variety of fields, the Internet
    of Things has gained a great deal of interest. Some deployment environments, on
    the other hand, may be hostile, affecting the quality of data (QoD) and its accuracy.
    An IoT system should be able to clean its own sensed data by eliminating instances
    that are erroneous or incoherent in order to achieve a high level of reliability.
    Managing a vast volume of data produced by sensors is one of the most important
    difficulties for IoT. Latency, security, privacy, and excessive bandwidth use
    will all be issues if this large volume of data is sent directly to the cloud.
    Edge computing solves this problem (EC). In EC, data is processed at the network''s
    edge, close to the embedded devices. (Muthukumaran et al. 2021) discussed a thorough
    examination of smart systems based on IoT and EC. The study highlights the evolution
    of emerging technologies, the framework for EC-IoT-based smart farming, and the
    prerequisites for implementing the EC-IoT-based system. The framework for an EC-IoT-based
    smart agricultural system is investigated, and key prerequisites for implementing
    the system are specified. Further, test-bed is developed and smart farming parameters
    and auto-irrigation application are investigated and analyzed. The paper is organized
    as: Sect. 2 describes the five stage architecture for smart farming. Section 3
    illustrates the implementation of double selection algorithm in WSN for data collection
    and simulation results in comparison with traditional approaches are presented
    in Sect. 4. Section 5 presents test-bed model for smart farming and the results
    in cloud environment. Section 6 gives the conclusion of paper. 2 Edge based smart
    farming architecture For effective implementation of farming with improved productivity
    and quality, five layered smart farming architecture is proposed using edge layer,
    IoT and LoRa as represented in Fig. 1. Layer-1 is sensor layer that consists of
    several sensor node and actuator node clusters. This layer is connected with edge
    computing layer to send sensed data or to receive control commands from LoRa gateway.
    Fig. 1 Edge based smart farming architecture with IoT and LoRa Full size image
    Layer-2 is edge computing layer, an intermediate layer that reduces data transmitted
    over LoRa links. Egde gateways in edge computing layer receive and transmit data.
    Better security can be provided by including encryption algorithms like AES-128,
    256 or cryptography algorithms like ECDSA. Edge gateway can also use lossless
    or lossy data compression techniques to reduce traffic over LoRa gateway. Layer-3
    is a group of repeaters for the purpose of receiving data from edge layer and
    forwarding the data to next layer i.e. cloud layer. This layer consists of LoRa
    gateways. Layer-4 is cloud layer that offers big data analysis, data storage and
    complex algorithms based data processing. Application specific cloud services
    can be integrated in this layer. Layer-5 is end user layer which usually consists
    of web browser and mobile applications to have access to real time data and based
    on data to monitor or control the farm remotely. 3 Double selecting algorithm
    In this section, a suitable data collection technique is used in edge computing
    layer. This approach depends on data-type, nodes-position and other parameters.
    It also ensures high quality of data (QoD) and real time execution. This data
    acquisition method: Edge-Computing-Driven-for-Sensing-Control (ECDSC) using double
    selecting for QoD algorithm is implemented in simulation environment and the results:
    QoD, communication latency and energy consumption are investigated and compared
    with traditional approaches Periodically-Sensing-with-All-Nodes (PSAN) and Effective-Node-Sensing
    (ENS). Three performance parameters are used to investigate the performance of
    proposed architecture. (1) Quality of data (QoD): the ratio of suitable (valid)
    data to total sum of collected data. (2) Communication latency and data-collection-time
    (DCT): It is the time point where multi-sensor node sensed the data to the time
    when data server received all the data. (3) Energy-consumption (EC): It is the
    energy consumed for sensing, valid data transmission, and waiting. Quality of
    data (QoD): QoD index can be calculated mathematically as given in Eq. (1). $$
    \\theta_{j} = \\frac{{D_{v} \\left( {ta_{j} } \\right)}}{{D_{g} \\left( {ta_{j}
    } \\right)}} $$ (1) where, \\(\\theta_{j}\\) is QoD index for the task \\(ta_{j}\\).
    \\(D_{v} \\left( {ta_{j} } \\right)\\) = valid data of task \\(ta_{j}\\). \\(D_{g}
    \\left( {ta_{j} } \\right)\\) = sum of collected data of task \\(ta_{j}\\). The
    total QoD index for multi-task WSN with “m” number of tasks is presented as Eq.
    (2) $$ QoD\\left( {TA} \\right) = \\mathop \\sum \\limits_{j = 1}^{m} \\theta_{j}
    $$ (2) Maximum QoD can be calculated using Eq. (3) $$ \\begin{gathered} \\max
    QoD\\left( {TA} \\right) \\hfill \\\\ T_{g} \\left( {ta_{j} } \\right) \\le T_{{limit}}
    \\left( {ta_{j} } \\right) \\hfill \\\\ S_{i} \\cap S_{j} = \\phi \\left( {1 \\le
    i \\ne j \\le m} \\right)\\sum\\limits_{{i \\in m}} {S_{i} } \\le \\Delta \\hfill
    \\\\ \\end{gathered} $$ (3) Data communication time is the summation of node sensing
    time, queue waiting time and data transmission time. DCT of multiple tasks tg
    is presented in Eq. (4). $$ t_{g} = t_{s} + t_{c} + t_{w} $$ (4) where, ts is
    node sensing time, tc is data transmission time, and. tw is queue waiting time.
    For any sensor node vl, DCT can be calculated as formulated in Eq. (5) $$ T_{g}
    \\left( {ta_{j} ,v_{l} } \\right) \\leftarrow \\sum h_{i\\eta } t_{s} (v_{l} ,s_{\\eta
    } ,ta_{j} ) + k\\left( {v_{l} } \\right)\\left( {\\frac{{D\\left( {ta_{j} ,v_{l}
    } \\right)}}{{V_{c} }}} \\right) + t_{w} \\left( {ta_{j} ,v_{l} } \\right) $$
    (5) where, Vc is WSN communication speed, \\(k\\left({v}_{l}\\right)\\) is network
    route hop number. Energy consumption can be calculated by using Eq. (6) $$ E_{c}
    = \\sigma t_{s} + \\zeta t_{c} + \\tau t_{w} $$ (6) where, Ec is energy consumption,
    σ is power consumption of sensing time, ζ is power consumption of communication
    time, τ is power consumption of waiting time. Algorithm-1 gives double selecting
    algorithm implemented for enhancement of QoD and latency parameters. Algorithm-1.
    Double selecting for QoD algorithm. Flowchart implemented [18] in simulation environment
    is illustrated in Fig. 2. Simulation runs until the number of simulation count
    is reached and collects data. Once count is reached, results are obtained. Table
    1 tabulates the simulation parameters used to set-up simulation environment and
    to obtain results. Fig. 2 Flow chart implemented in simulation environment Full
    size image Table 1 Parameters in simulation environment Full size table 4 Results
    and discussions QoD with respect to various valid sensor numbers and included
    node numbers is illustrated in FigS. 3 and 4 respectively. These results show
    the comparative results of ECDSC (double selection algorithm) with traditional
    techniques PSAN and ESN. It can be observed from Fig. 3 that ECDSC achieved 100%
    QoD for different sensor numbers where as ESN obtained 20, 40, 60, 80 and 100%
    QoD. However, ESN performs better than PSAN approach. As observed from Fig. 4,
    with increase in included node number, QoD of PSAN approach increases while QoD
    of ESN and ECDSC is at stable value of 40 and 100% respectively. Therefore, ECDSC
    collects more valid data compared to other approaches. Fig. 3 Quality of data
    for different sensor numbers Full size image Fig. 4 Quality of data for different
    cover node numbers Full size image To evaluate the latency of double selection
    algorithm, communication latency and data collection time (DCT) are used. Figure
    5 illustrates communication latency results at various data rates. It is observed
    that communication latency decreases with increase in data rate as higher data
    rates support less transmission time. Out of the three approaches ECDSC, ESN and
    PSAN, ECDSC outperforms and ESN performs better than PSAN. At low data rates,
    communication latency is very high in PSAN approach. Figure 6 shows the outcomes
    of three approaches in terms of data collecting time for different numbers of
    covering nodes. When compared to ESN and PSAN, ECDSC has a much lower DCT. Communication
    latency and DCT are expected to be as low as feasible to improve performance.
    As data is acquired from all, the PSAN approach depicts the highest DCT and communication
    latency. In ESN technique, data is collected only from selected sensor nodes and
    hence it exhibits better performance than PSAN in DCT and communication latency.
    ECDSC achieved minimum DCT and communication latency. Fig. 5 Communication latency
    at various data rates Full size image Fig. 6 DCT for different covering node numbers
    Full size image Figure 7 shows the energy consumption data for three techniques
    for varying valid sensor counts. The ECDSC approach uses a little amount of energy,
    which increases as the number of good sensors grows. The adopted double selection
    strategy produces the best results of the three possibilities. As PSAN and ESN
    do not consider suitable sensors in their respective techniques, ECDSC outperforms
    PSAN and ESN approaches in terms of energy consumption. The results (rounded values)
    obtained for three approaches: ECDSC, ESN and PSAN for the parameters QoD, communication
    latency, DCT and energy consumption are tabulated in Tables 2, 3, 4, 5 and 6 respectively.
    Fig. 7 Energy consumption for different sensor numbers Full size image Table 2
    QoD different sensor numbers Full size table Table 3 QoD different cover node
    numbers Full size table Table 4 Communication latency (sec) at various data rates
    (Mbps) Full size table Table 5 DCT (sec) for various cover node numbers Full size
    table Table 6 For various valid sensor numbers, Energy consumption (J) Full size
    table 5 Smart farming test-bed The DHT11 sensor, soil moisture sensor, controller
    unit, and LoRa gateway are used to create a prototype test-bed for smart farming
    applications. It is used in a small test farm to get the parameters temperature,
    humidity, and soil moisture by connecting the test-bed to a cloud platform. Figure
    8a and b present soil moisture before and after watering respectively. Temperature
    and humidity results obtained through DHT11 sensor are demonstrated in Figs. 9
    and 10 respectively. When soil moisture decreases below 50%, automatic watering
    starts and when moisture level is 80%, watering stops. Figure 11 illustrates soil
    moisture results for automatic watering. Fig. 8 a Before watering, checking the
    soil using a soil moisture sensor. b After watering, soil with soil moisture sensor
    Full size image Fig. 9 Cloud results of farm temperature Full size image Fig.
    10 Cloud results of farm Humidity Full size image Fig. 11 Cloud results of farm
    Soil moisture Full size image 6 Conclusion In this paper, edge based IoT test-bed
    is implemented for smart farming application with five layered architecture. To
    enhance the parameters: quality of data, communication latency, energy consumption
    and data collection time double selecting algorithm is implemented. Quality of
    data is 100% for the implemented double selection approach and energy consumption,
    communication latency, data collection time parameters are also minimum compared
    to ESN and PSAN approaches. The LoRa gateway and cloud platform are used to create
    a test-bed for smart farming and auto-irrigation. In a cloud environment, farm
    characteristics like as temperature, humidity, and soil moisture are measured
    and results are obtained. The proposed test-bed with five layered architecture
    shows good results in agreement for smart farming applications. Edge computing
    layer reduces latency and energy consumption and achieved 100% QoD. References
    Dalezios NR, Dercas N, Spyropoulos NV, Psomiadis E (2019) Remotely sensed methodologies
    for crop water availability and requirements in precision farming of vulnerable
    agriculture. Water Resour Manage 33(4):1499–1519 Article   Google Scholar   Elijah
    O, Rahman TA, Orikumhi I, Leow CY, Hindia MN (2018) An overview of Internet of
    Things (IoT) and data analytics in agriculture: Benefits and challenges. IEEE
    Internet Things J 5(5):3758–3773 Article   Google Scholar   Fleming A, Jakku E,
    Lim-Camacho L, Taylor B, Thorburn P (2018) Is big data for big farming or for
    everyone? Perceptions in the Australian grains industry. Agron Sustain Dev 38(3):1–10
    Article   Google Scholar   Gao H, Fang X, Li J, Li Y (2013) Data collection in
    multi-application sharing wireless sensor networks. IEEE Trans Parallel Distrib
    Syst 26(2):403–412 Article   Google Scholar   Gia TN, Qingqing L, Queralta JP,
    Zou Z, Tenhunen H, Westerlund T (2019) Edge AI in smart farming IoT: CNNs at the
    edge and fog computing with LoRa. In 2019 IEEE AFRICON (pp. 1–6). IEEE Hassani
    A, Plata-Chaves J, Bahari MH, Moonen M, Bertrand A (2017) Multi-task wireless
    sensor network for joint distributed node-specific signal enhancement, LCMV beamforming
    and DOA estimation. IEEE J Sel Topics Signal Process 11(3):518–533 Article   Google
    Scholar   Li X, Ma Z, Chu X, Liu Y (2019) A cloud-assisted region monitoring strategy
    of mobile robot in smart greenhouse. Mob Inf Syst 2019:1–10 Google Scholar   Li
    X, Zhu L, Chu X, Fu H (2020) Edge computing-enabled wireless sensor networks for
    multiple data collection tasks in smart agriculture. J Sens 2020:1–9 Google Scholar   Morota
    G, Ventura RV, Silva FF, Koyama M, Fernando SC (2018) Big data analytics and precision
    animal agriculture symposium: Machine learning and data mining advance predictive
    big data analysis in precision animal agriculture. J Anim Sci 96(4):1540–1550
    Article   Google Scholar   Muthukumaran V, Kumar VV, Joseph RB, Munirathanam M,
    Jeyakumar B (2021) Improving network security based on trust-aware routing protocols
    using long short-term memory-queuing segment-routing algorithms. Int J Inf Technol
    Project Manag 12(4):47–60. https://doi.org/10.4018/ijitpm.2021100105 Article   Google
    Scholar   Pathak HS, Brown P, Best T (2019) A systematic literature review of
    the factors affecting the precision agriculture adoption process. Precision Agric
    20(6):1292–1316 Article   Google Scholar   Pilloni V, Navaratnam P, Vural S, Atzori
    L, Tafazolli R (2013) TAN: a distributed algorithm for dynamic task assignment
    in WSNs. IEEE Sens J 14(4):1266–1279 Article   Google Scholar   Rodríguez S, De
    Paz JF, Villarrubia G, Zato C, Bajo J, Corchado JM (2015) Multi-agent information
    fusion system to manage data from a WSN in a residential home. Inf Fusion 23:43–57
    Article   Google Scholar   Schepers J (2018) Precision agriculture for sustainability.
    Precision Agric 20(1):1–3 Article   Google Scholar   Tomovic S, Radusinovic I
    (2016) Allocation algorithm for handling multiple applications in software-defined
    WSN. In 2016 24th Telecommunications Forum (TELFOR) (pp. 1–4). IEEE Trinh DC,
    Truvant TC, Bui TD (2018) Design of automatic irrigation system for greenhouse
    based on LoRa technology. In 2018 International conference on advanced technologies
    for communications (ATC) (pp. 72–77). IEEE Weersink A, Fraser E, Pannell D, Duncan
    E, Rotz S (2018) Opportunities and challenges for big data in agricultural and
    environmental analysis. Annu Rev Res Econ 10:19–37 Google Scholar   Wolfert S,
    Ge L, Verdouw C, Bogaardt MJ (2017) Big data in smart farming–a review. Agric
    Syst 153:69–80 Article   Google Scholar   Xiong S, Li J, Li M, Wang J, Liu Y (2011)
    Multiple task scheduling for low-duty-cycled wireless sensor networks. In: 2011
    Proceedings IEEE INFOCOM (pp. 1323–1331). IEEE Download references Funding No
    funding. Author information Authors and Affiliations Department of ECE, Koneru
    Lakshmaiah Education Foundation, Guntur, India S. Raja Gopal & V. S. V. Prabhakar
    Corresponding author Correspondence to S. Raja Gopal. Ethics declarations Conflicts
    of interest The writers claim that they have no conflicts of interest. Human participants
    and/or animals No animals or humans were harmed, according to the authors. Additional
    information Publisher''s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Reprints and permissions About this article Cite this article Raja Gopal, S.,
    Prabhakar, V.S.V. Intelligent edge based smart farming with LoRa and IoT. Int
    J Syst Assur Eng Manag 15, 21–27 (2024). https://doi.org/10.1007/s13198-021-01576-z
    Download citation Received 01 September 2021 Revised 21 October 2021 Accepted
    30 November 2021 Published 31 January 2022 Issue Date January 2024 DOI https://doi.org/10.1007/s13198-021-01576-z
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Data-collection-time Edge computing Internet of Things Latency
    Quality of data Smart farming Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Sections Figures References Abstract Introduction Edge based
    smart farming architecture Double selecting algorithm Results and discussions
    Smart farming test-bed Conclusion References Funding Author information Ethics
    declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: International Journal of System Assurance Engineering and Management
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Intelligent edge based smart farming with LoRa and IoT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Lee G.H.
  - Park H.
  - Jang J.W.
  - Han J.
  - Choi J.K.
  citation_count: '3'
  description: With the recent explosive growth of the Internet of Things (IoT), edge
    computing is emerging as a modern computing paradigm that coexists with the cloud
    to process massive amounts of data particularly distributed at the edge network.
    Meanwhile, edge computing directly permits the use of artificial intelligent (AI)
    models at the edge. Currently in numerous IoT applications, a tremendous amount
    of data is being measured and transmitted from IoT sensors to monitor surrounding
    information in real-time. Considering that continuous transmission of sensed data
    substantially requires high-energy consumption of IoT sensors, this article proposes
    a proximal policy optimization (PPO)-based autonomous transmission period control
    (PPO-ATPC) system in IoT edge computing that can automatically and adaptively
    control the transmission period of each IoT sensor. Here, the design of state,
    action, and reward is shaped in an unprecedented way and furthermore, a PPO, which
    is one of the most effective model-free deep reinforcement learning (DRL) algorithms,
    is fully leveraged to achieve an optimal or nearly optimal policy that can significantly
    shorten the data volume while maintaining high-data quality. The merits of the
    proposed PPO-ATPC are extensively validated through quantitative comparison with
    other alternative approaches using three different sensor data collected from
    real-time environmental monitoring, and in-depth insights into the effectiveness
    of PPO-ATPC are further provided from diverse perspectives. The performance results
    show that total data volume for each data set could be reduced by 73.849%, 89.931%,
    and 81.310%, with the average root mean square error of 0.322, 13.896, and 0.048.
  doi: 10.1109/JIOT.2023.3293511
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Volume: 10 Issue: 24 PPO-Based Autonomous Transmission Period Control System
    in IoT Edge Computing Publisher: IEEE Cite This PDF Gyeong Ho Lee; Hyunseo Park;
    Jae Won Jang; Jaeseob Han; Jun Kyun Choi All Authors 3 Cites in Papers 502 Full
    Text Views Abstract Document Sections I. Introduction II. Related Work III. Proposed
    Methodology IV. Performance Evaluation V. Conclusion and Future Work Authors Figures
    References Citations Keywords Metrics Abstract: With the recent explosive growth
    of the Internet of Things (IoT), edge computing is emerging as a modern computing
    paradigm that coexists with the cloud to process massive amounts of data particularly
    distributed at the edge network. Meanwhile, edge computing directly permits the
    use of artificial intelligent (AI) models at the edge. Currently in numerous IoT
    applications, a tremendous amount of data is being measured and transmitted from
    IoT sensors to monitor surrounding information in real-time. Considering that
    continuous transmission of sensed data substantially requires high-energy consumption
    of IoT sensors, this article proposes a proximal policy optimization (PPO)-based
    autonomous transmission period control (PPO-ATPC) system in IoT edge computing
    that can automatically and adaptively control the transmission period of each
    IoT sensor. Here, the design of state, action, and reward is shaped in an unprecedented
    way and furthermore, a PPO, which is one of the most effective model-free deep
    reinforcement learning (DRL) algorithms, is fully leveraged to achieve an optimal
    or nearly optimal policy that can significantly shorten the data volume while
    maintaining high-data quality. The merits of the proposed PPO-ATPC are extensively
    validated through quantitative comparison with other alternative approaches using
    three different sensor data collected from real-time environmental monitoring,
    and in-depth insights into the effectiveness of PPO-ATPC are further provided
    from diverse perspectives. The performance results show that total data volume
    for each data set could be reduced by 73.849%, 89.931%, and 81.310%, with the
    average root mean square error of 0.322, 13.896, and 0.048. Published in: IEEE
    Internet of Things Journal ( Volume: 10, Issue: 24, 15 December 2023) Page(s):
    21705 - 21720 Date of Publication: 07 July 2023 ISSN Information: DOI: 10.1109/JIOT.2023.3293511
    Publisher: IEEE Funding Agency: SECTION I. Introduction Recent advances in wireless
    communication and sensing technologies along with edge computing are driving more
    sophisticated and pragmatic Internet of Things (IoT) applications in a full range
    of areas, including smart homes, smart agriculture, smart cities, and smart healthcare
    [1], [2], [3], [4], [5], [6], [7]. In particular, the primary use case for these
    IoT applications is to thoroughly represent the real-word phenomena with data
    sensed by IoT sensors. Inevitably, with the advent of the era of big data, issues
    related to the unprecedented outburst of data and the lifespan of IoT sensors
    are of the utmost concern to governments, industrial enterprises, and even individual
    users [8], [9], [10], [11], [12], [13], [14]. As fully discussed in [13], continuous
    transmission of sensed data requires high-energy consumption of IoT sensors, which
    shortens the lifespan of IoT sensors. In addition, high expenditure is exceptionally
    required for storing a tremendous amount of data in the cloud. As reported by
    the statistics, the energy consumption of data centers accounts for nearly 1.5%
    of global energy consumption, of which 40% is spent solely by storage systems
    [14], [15]. Nowadays, such realities call for cutting-edge methods to address
    both challenges related to the data volume and the lifespan of IoT sensors. These
    matters can be resolved by controlling the transmission period of IoT sensors
    without deteriorating the quality of data, as presented in [13]. In general, the
    optimal transmission period policy can vary depending on the target use case as
    well as changes in the environment in which each individual IoT sensor is deployed.
    Thus, determining the optimal policy is a remarkably complex task. As individual
    IoT sensors can be deployed in different locations, it is in need to have a self-trained
    model for each IoT sensor that can determine the appropriate transmission period
    freely for a particular situation without human intervention. Deep reinforcement
    learning (DRL), which combines deep learning and reinforcement learning, has recently
    received the most attention with great success in optimal decision-making problems
    [16], [17], [18], [19]. Particularly, DRL algorithms empower the agents to successfully
    learn on their own to achieve optimal strategies by interacting with the environment.
    Here, there are two main types of DRL algorithms: 1) off-policy DRL and 2) on-policy
    DRL. Deep Q -network (DQN), one of the representative off-policy models, approximates
    a state-value function in Q -learning framework with a neural network [16]. Next,
    both advantage actor-critic (A2C) and proximal policy optimization (PPO) are popular
    on-policy DRL algorithms that employ the advantage function [17], [18]. Especially,
    PPO is a state-of-the-art algorithm that is less sensitive to the hyperparameters
    and can avoid exceptionally large policy updates by the clipped surrogate objective
    function [18], [19]. In this article, we now propose a newly developed PPO-based
    autonomous transmission period control (PPO-ATPC) system that can adaptively and
    autonomously control the transmission period of IoT sensor. In particular, this
    PPO-ATPC can quickly react to the abnormal phenomena such as rapid changes in
    data. To fully understand the dynamics of the data, we mainly employ the concept
    of bollinger bands inspired by the work of Kulau et al. [20]. Herein, an OpenAI
    gym compliant environment is built as a toolkit for efficiently training an DRL
    agent and numerical results assuredly demonstrate the superiority of our proposed
    PPO-APTC over other methodologies. The rest of this article is organized as follows:
    In Section II, related work and contributions are presented. In Section III, the
    methodology is proposed. In Section IV, the performance evaluation is investigated.
    Finally, conclusion and future work are given in Section V. SECTION II. Related
    Work Energy-efficient operation of resource-constrained IoT sensors has recently
    emerged as one of the most important topics in various IoT applications. And,
    a large body of literature has often raised data transmission as a major cause
    of high-energy consumption of IoT sensors [21], [22], [23]. That is to say, the
    continuous transmission of sensed data has a significant impact on the data volume,
    energy consumption, and finally the lifespan of IoT sensors. Thereby, various
    studies on transmission period control of IoT sensors are continuously being conducted
    so as to resolve these issues. One common approach is to enable IoT sensors to
    immediately transmit data only when the sensed measurement exceeds a certain predetermined
    threshold [24]. Although this approach is relatively simple, it could no longer
    yield meaningful effects as the data variability becomes larger. In particular,
    statistical analysis-based mechanisms in [20], [25], [26], [27], and [28] are
    at first promising solutions that reduce the energy consumption and the number
    of data transmissions. First, Kulau et al. [20] utilized bollinger bands, which
    is a common descriptive analysis tool for time series, to perceive the trend of
    measured data to acquire a proper transmission period value. Next, Li and Wang
    [25] fully developed an automatic ARIMA modeling-based data aggregation scheme
    to diminish the redundancy of raw data in wireless sensor networks. In accordance
    with this scheme, the sensed value will only be transmitted when the difference
    between the sensed value and the predicted value is above the predefined error
    threshold. But, Fathy et al. [26] utilized the least-mean-square (LMS) filters
    as opposed to the ARIMA model to forecast the value for comparison with the sensed
    value. Finally, Salim et al. [27] applied the ANOVA model with Fisher test, whereas
    Malik et al. [28] used the principal component analysis (PCA) to analyze the variance
    of the data in order to reduce the number of transmissions. As these statistical
    analysis-based mechanisms are difficult to find the complex dynamics commonly
    observed in time series data, numerous researchers in [13], [29], [30], and [31]
    have actively begun to adopt artificial intelligent techniques, such as deep learning
    and reinforcement learning. Especially, reinforcement learning has indeed been
    exploited to autonomously control IoT sensors without human guidance, as referred
    to [32]. First, Han et al. [13] exploited a combination of convolutional neural
    network (CNN) and spatio-temporal correlations to enable adaptive control of data
    transmission period for each IoT sensor. They then implemented the proposed algorithm
    on a testbed under the IEEE 802.11-based WLANs communication protocol and evaluated
    its performance in terms of power consumption. Next, Dias et al. [29] implemented
    a Q -learning algorithm in a simple environment to learn the policy that selects
    the transmission period of a sensor based on changes in the environment. They
    concisely defined a set of state space as a combination of measurement quality,
    action, and validation of working hours. Addition to this, the reward function
    was designed to penalize only if the quality exceeds the predefined threshold.
    Likewise, Cobb and Markham [30] utilized various reinforcement learning techniques,
    including tabular Q -learning, Deep Q -learning with NN, and Deep Q -learning
    with long-short term memory (LSTM) on accelerometer data collected from a lion
    to learn an optimal transmission period policy. They designed a reward function
    based on the compromise between the data accuracy (reconstruction error) and the
    energy expenditure (transmission period). The authors showed that the energy consumption
    was reduced by roughly 73% with a reconstruction accuracy of 51%. Murad et al.
    [31] introduced a Fisher information-based reward function and utilized a Gaussian
    process (GP) model to produce a Bayesian probabilistic model of the monitored
    phenomenon over a period of time. Although this technique can achieve a more generalized
    model, the computation complexity for training this model may overwhelm the advantage
    of transmission period control. Finally, as the efficiency of RL algorithms also
    highly depends on their reward functions, the performance in terms of energy consumption
    was investigated with four different reward functions in [10]. In recent decades,
    there have been various techniques proposed for autonomous transmission period
    control, but many of them have not yet incorporated the latest DRL algorithms.
    Furthermore, most of these methods still rely on data-specific thresholds, which
    are inadequate in ensuring high-data quality while conserving energy consumption
    of IoT devices. Such existing limitations have thereby motivated our study to
    focus on developing an adaptive transmission period control using PPO, which takes
    into account both data quality and the energy consumption. A. Contributions of
    This Article In this article, we present a novel PPO-ATPC system in IoT edge computing.
    Main contributions can be summarized as follows. PPO, one of the policy-based
    DRL algorithms, is leveraged to acquire the optimal policy that controls the transmission
    period of IoT sensor autonomously and adaptively. A2C and DQN are further used
    to evaluate the convergence performance. An effective and simple approach to derive
    each state from transmitted data is fully presented using the concept of bollinger
    bands for better decision-making in controlling the transmission period. A novel
    reward function that considers both data quality and energy consumption is proposed
    with the aim of prolonging battery life while ensuring high-data quality. Extensive
    performance evaluation verifies the superiority of PPO-ATPC in achieving high-data
    quality with greatly reduced data volume through the use of sensor data collected
    from real-time environmental monitoring. SECTION III. Proposed Methodology As
    illustrated in Fig. 1, several wireless IoT sensor nodes are responsible for collecting
    and transmitting ambient information about all relevant entities in a designated
    environment to an adjacent edge server via communication protocol. Upon receiving
    immediate sensed values from the sensor nodes, the edge server swiftly forwards
    the data to Amazon Web services (AWS), a widely utilized public cloud service,
    where incoming data can be comprehensively monitored by clients and service providers
    for quality and volume, through a user-friendly Web-based interface. One notable
    capability of the edge server in the presented system model is its ability to
    independently assign a new transmission period to each sensor node on a fixed
    cycle ( p max ) using the proposed PPO-ATPC system, which is meticulously trained
    through DRL. Herein, the control policy for each sensor node is trained independently
    on the cloud server, and then each resulting trained policy is deployed on the
    edge server for inference. The policy deployed on the edge server initially derives
    the state from the data transmitted over a fixed cycle. However, due to the transmission
    period control, the amount of data that may arrive at the edge server can vary,
    and as a result, any missing values present in the transmitted data should be
    precisely reconstructed by linear interpolation before making a decision for the
    next transmission period. As depicted in Fig. 2, once the next transmission period
    is chosen, the IoT sensor node receives a newly updated transmission period value
    with an acknowledgement (ACK) and transmits data uniformly with the newly assigned
    transmission period over the next cycle. Overall, we thoroughly consider an environment
    in which each sensor node that generates its own univariate data stream is broadly
    distributed, and each of these sensor nodes steadily transmits the sensed values
    without any collision in both downlink and uplink directions, as discussed in
    [13], [33], [34], and [35]. A detailed aspect of the control policy corresponding
    to each sensor node within the PPO-ATPC system is addressed in next few sections,
    but we initially begin by defining some of fundamental definitions and notations
    as follows: Fig. 1. Autonomous transmission period control system. Show All Fig.
    2. Operation between IoT sensor node and edge server. Show All Definition 1:An
    univariate time series Y is a sequence of real-valued numbers; Y={ y i | x i =
    x 1 +(i−1) p o ,i≤L,i∈N} , where x i indicates the timestamp of y i while p o
    and L denote the predefined transmission period and the total length of Y . Definition
    2:A subsequence Y i,l of Y is a contiguous subset of values with length l , ranging
    from position i−l+1 to i ; Y i,l ={ y i−l+1 , y i−l+2 ,…, y i } , where l≤i≤L
    . Here, a given Y denotes the historical time series uniformly transmitted from
    a single sensor node to the edge server with a transmission period p o . Thus,
    one can think of this given Y as the underlying data set used for DRL. In this
    article, we concretely introduce how to achieve an optimal or nearly optimal policy
    that can automatically and adaptively control the transmission period by virtue
    of DRL. Most importantly, having an environment in which the agent can play is
    indispensable as the agent must continuously interact with the environment by
    taking actions, and observing the immediate reward and the next state during the
    learning process. Thus, to adeptly train the agent, we built an OpenAI gym compliant
    environment, which is specifically described in next sections. Prior to discussing
    action, state, reward, and DRL, we first present the concept of bollinger bands,
    which is adopted in this article to understand the dynamics of the data. A. Bollinger
    Bands Bollinger bands are known as a prevalent technical analysis tool introduced
    by John Bollinger in the 1980s, typically to foresee the direction of stock prices
    [36]. Addition to this, the bands are also strongly capable of comparing the volatility
    and relative price levels over a period of time [37]. Due to their dynamic nature
    and splendid ability to perceive the trend of time series, the bollinger bands
    are exploited in this article. As highlighted in Fig. 3, the bollinger bands comprise
    three different trendlines: 1) the upper; 2) middle; and 3) lower band. To begin
    with, given Y , the middle band at the position i can be computed as a simple
    moving average (SMA) using a fixed number of last n data points MB Y i = 1 n ⋅
    ∑ j=0 n−1 y i−j (1) View Source where n≤i≤L . The upper and lower bands are positioned
    above and below the middle band. These two bands, UB Y i and LB Y i , can be,
    respectively, obtained by adding and subtracting m times the standard deviation
    σ i to the middle band MB Y i UB Y i = LB Y i = MB Y i +m⋅ σ i MB Y i −m⋅ σ i
    (2) (3) View Source where the factor m is utilized to regulate the width between
    UB Y i and LB Y i while the standard deviation σ i is given by σ i = 1 n ⋅ ∑ j=0
    n−1 ( y i−j − MB Y i ) 2 − − − − − − − − − − − − − − − − − −  ⎷   . (4) View
    Source Fig. 3. Step t movement ( p max ,n=30, p min =1, a t =10) . Show All Considering
    that the increase in dynamics of the data itself leads to wider bollinger bands,
    the width between the upper and lower bands can be utilized as a feasible indicator
    for estimating the data dynamics [20]. At the position i , the width between UB
    Y i and LB Y i can be expressed as follows: WB Y i = UB Y i − LB Y i =2m⋅ σ i
    . (5) View Source A set W Y having all the widths of bollinger bands can now be
    obtained as it is necessary to acquire true widths of bollinger bands for later
    use in a reward function. Definition 3: W Y of Y is a set containing all the widths
    of bollinger bands, initially starting from the position n to L ; W Y ={ WB Y
    i |n≤i≤L} . Finally, given l day representing the number of data points that can
    be accumulated in a day, a set W Y d can be acquired, which is also an essential
    prerequisite for our reward function. Definition 4: W Y d of W Y is a contiguous
    subset of width values; W Y d ={ WB Y i ∈ W Y |(d−1)⋅ l day ≤i−n<d⋅ l day } ,
    where d∈N,D=⌊(L−n+1/ l day )⌋ , and 1≤d≤D . B. Action Representation As written
    in [38], there are three main categories according to the action space, which
    are discrete action space, continuous action space, and discrete-continuous hybrid
    action space. In our study, there are a number of discrete steps with index t∈{1,2,…,T}
    , and an action a t ∈A is executed at each step t to control the transmission
    period of the sensor node for next p max , as depicted in Fig. 3. Herein, the
    action space exists between [ p min , p max ], where p min and p max represent
    the minimum and maximum transmission periods, respectively. Thus, if p max :(in
    minutes)= 30, then a new transmission period is selected and assigned to the sensor
    node every 30 min. At each step t , the number of data points l new t accumulated
    over the past p max thoroughly depends on the previously selected action a t−1
    , defined as follows: l new t = p max a t−1 (6) View Source where l new t ∈N .
    As noticed, we have a discrete action space instead of a continuous action space
    as l new t must be a positive integer in our problem. The description of this
    action space can be further summarized as follows. Possible actions now exist
    as positive integers between [ p min , p max ], where p min = p o in this article.
    The maximum transmission period p max shall be a multiple of the minimum transmission
    period p min . Feasible actions in A shall be the factors of p max as well as
    the multiples of p min . Once a set A is defined, actions can be freely reselected
    within the set according to the monitoring application needs afterward. In our
    case, we add one more constraint to remove neighboring actions that are considered
    negligible in the set. The difference between actions in A sorted in ascending
    order shall be at least twice as large as p min . Herein, assuming the current
    step t is at the position i within the univariate time series Y , a finite set
    of measurements M t i transmitted over the past p max due to the previously chosen
    action a t−1 can be obtained as follows. Definition 5: M t i of Y is a finite
    set of measurements collected uniformly from the previous step t−1 to the current
    step t with the recently assigned transmission period a t−1 ; M t i ={ y i− g
    k | g k =(k a t−1 / p min ),0≤ g k ≤ l min ,k∈W, l min =( p max / p min )} In
    a case where a t−1 > p min , the missing values exist due to the different transmission
    period from the initial transmission period. Therefore, we replace these missing
    values by applying the linear interpolation technique [39]. Definition 6: I t
    i of M t i is a set of imputed values by applying the method of linear interpolation;
    I t i ={ y z = y h +(z−h)⋅( y q − y h /q−h)| y h , y q ∈ M t i ,h<z<q,q−h=( a
    t−1 / p min ),z∈N} . As a result, the combination of two previous sets, which
    are M t i and I t i , in ascending order can be represented as follows: O t i
    = M t i ∪ I t i . (7) View Source Overall, we denote Y ^ t as the entire data
    fully restored from the initial step to the current step t by linear interpolation
    after being uniformly transmitted with newly assigned transmission period every
    p max by the agent. It can be written by Y ^ t = ⋃ j=0 t−1 O t−j i−j⋅ l min .
    (8) View Source This corresponding Y ^ t could change in response to the actions
    chosen by the agent in the environment. Herein, a subsequence of Y ^ t can be
    expressed as Y ^ t i,l , as elucidated in Definition 2. Now, the agent at each
    step t receives a representation of the environment’s state from the given Y ^
    t i,l and selects an action a t that would maximize the reward. In next section,
    we fully discuss how to represent a state s t with the given Y ^ t i,l . C. State
    Representation In reinforcement learning, state is a representation of the current
    situation [40]. That is, in the absence of any of these states, the agent would
    become incompetent for learning the finest policy. Hence, given the state, the
    agent is then able to learn and decide which action to take from each state with
    an eye to maximizing the reward. As explicitly illustrated in Fig. 4, different
    shapes of bollinger bands exist as the dynamics of the data itself arise. This
    implies that the shape of bollinger bands has significant influence on the agent’s
    choice of action in its current state. In other words, as the variability of transmitted
    data increases, the agent would choose a shorter transmission period as an profitable
    action to preserve high-data quality. Herein, we attentively address how to effectively
    and simply represent the state for each step t , especially reflecting the characteristics
    of bollinger bands. As presented in Fig. 4(a), the state defined in this article
    is mainly composed of three decisive sets. Fig. 4. Overview of state representation
    and different shapes of bollinger bands (when l min =30 ). (a) Illustration of
    state representation (at random step t ). (b) At step t+ 1. (c) At step t+ 2.
    (d) At step t+ 3. (e) At step t+ 4. Show All 1) Bollinger Bands Width ( s B t
    ) : Predominantly, the width of bollinger bands is treated as an indicator frequently
    used to measure the dynamics of the data [20]. That is, the wider the width, the
    higher the standard deviation, meaning larger variation within the measurements.
    As a result, the choice of action a t is highly dependent on the dynamics of the
    data transmitted up to the step t . To reflect such information, three main widths
    are considered in this set: start, middle, and end. For now, let us suppose that
    the current step t is located at the position i within the fully reconstructed
    time series Y ^ t . Definition 7: s B t is a set that contains three significant
    width measurements of the bollinger bands, which can be used to evaluate the dynamics
    of Y ^ t i,l ; s B t ={ WB Y ^ t i−l+1 , WB Y ^ t mid , WB Y ^ t i } , where the
    value of WB Y ^ t mid can be computed as WB Y ^ t mid = ⎧ ⎩ ⎨ ⎪ ⎪ 1 2 ⋅ ∑ 1 j=0
    WB Y ^ t i−(l−2j)/2 , WB Y ^ t i−(l−1)/2 , iflis even iflis odd. (9) View Source
    For the agent, these three widths are sufficient to recognize the overall volatility
    of Y ^ t i,l , where l= l min +1 is considered in this article. For instance,
    if these widths are indeed small and nearly equal to each other, it implies that
    there is little or no variation within the transmitted data. As another example,
    if WB Y ^ t i−l+1 < WB Y ^ t mid < WB Y ^ t i , then it means that the closer
    to the position i , the larger the variation. Thus, to offer such details, this
    set is added in our state design. 2) Trend Direction Information ( s S t ) : To
    reflect the trend direction of the transmitted data, the slope, sometimes referred
    to as gradient in mathematics, is considered. In general, it is exploited to describe
    both the steepness and the direction of a line [41]. Thereby, a line with a positive
    slope indicates that it is trending upward, and a line with a negative slope implies
    that it is trending downward. For instance, assuming there are two data points,
    y i and y j , the slope then can be obtained as S( y i , y j )= y j − y i j−i
    . (10) View Source First and foremost, for the purpose of acquiring two major
    slopes in our problem, three data points that lie on the middle band are utilized.
    Definition 8: s S t is a set that contains two principal slopes, which can be
    used to evaluate the trend direction of Y ^ t i,l ; s S t ={S( MB Y ^ t i−l+1
    , MB Y ^ t mid ),S( MB Y ^ t mid , MB Y ^ t i )} , where MB Y ^ t mid exists at
    the position i−(l−1)/2 and its value can be computed as MB Y ^ t mid = ⎧ ⎩ ⎨ ⎪
    ⎪ 1 2 ⋅ ∑ 1 j=0 MB Y ^ t i−(l−2j)/2 , MB Y ^ t i−(l−1)/2 , iflis even iflis odd.
    (11) View Source 3) Relative Position of Data ( s P t ) : In stock trading, buy/sell
    signals are frequently determined by whether the current price breaks through
    the lower band or the upper band [42]. This explains that the information on whether
    the current value is inside or outside the bollinger bands is considered important
    at the decision-making level. Therefore, to adopt this concept to our problem,
    we introduce a position function P( y i ) that reflects the relative position
    of the data point within the bollinger bands. If the data point is outside the
    bollinger bands, the output variable is negative, whereas if the data point is
    inside the bollinger bands, the output variable is positive. Definition 9:A position
    function P( y i ) returns a value that represents the relative position of the
    input data point y i with regard to bollinger bands ( UB Y ^ t i , LB Y ^ t i
    ). It can be shown as P( y i )={ UB Y ^ t i − y i , y i − LB Y ^ t i , if y i
    > MB Y ^ t i if y i ≤ MB Y ^ t i . (12) View Source Definition 10: s P t is a
    set that comprises an output of the position function P( y i ) whose input is
    always the last element within Y ^ t i,l ; s P t ={P( y i )} . In summary, this
    set s P t informs the agent how close the most recently transmitted data point
    in Y ^ t i,l is to the bollinger bands. This helps the agent to accumulate a sense
    of how the dynamics of the forthcoming data will change. If the latest data point
    is way too far from the bollinger bands, then the shorter transmission period
    would be more suitable to avoid losing data quality. As a conclusion, the overall
    state at the current step t is the union of three decisive sets mentioned previously,
    as follows: s t = s B t ∪ s S t ∪ s P t . (13) View Source D. Reward Representation
    In reinforcement learning, the agent learns the best actions based on the reward
    or punishment. Recently, Silver et al. [43] revealed that in a complex environment,
    incentivizing the agent with proper reward is sufficient to motivate the agent
    to learn the intelligent skills itself. This implies that the level of agent’s
    expertise could vary depending on how the reward function is shaped. Herein, the
    goal of our agent is not only to maximize the total rewards but also to intelligently
    control the transmission period every p max while adequately balancing between
    the data quality Q t and the energy consumption E t . In particular, we consider
    v -delayed reward feedback instead of immediate reward. That is, the reward is
    only obtained every v -step(s) in our environment shown as follows: r t ={ (α⋅
    Q t +(1−α)⋅ E t )⋅ 1 t>1 , 0, if(t−1)modv=0 otherwise (14) View Source where α
    is a weighting factor and 1 t>1 is an indicator function with the condition t>1
    as no reward is given to the agent when initializing the step. 1) Data Quality
    Reward Function ( Q t ) : In this article, this reward function examines how similar
    the widths of bollinger bands in Y ^ t is to the true widths of bollinger bands
    in Y . Hence, the closer to the true widths, the higher the reward is given to
    the agent. Conversely, the greater the difference from the actual widths, the
    smaller the reward is given to the agent. At each step t , a normalized error
    function ERR t i should be initially computed as follows: ERR t i = 1 l min ⋅
    ∑ j=0 l min −1 | WB Y i−j − WB Y ^ t i−j | WB max − WB min ⋅100 (15) View Source
    where WB max and WB min can be, respectively, obtained as WB max = 1 D ⋅ ∑ j=1
    D max( W Y j ), WB min = 1 D ⋅ ∑ j=1 D min( W Y j ). (16) (17) View Source Now,
    the data quality reward function Q t can be structured as follows: Q t = ⎧ ⎩ ⎨
    ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ R max , R max 2 , 0, − R max , if0≤ ERR t i ¯ ¯ ¯ ¯ ¯ ¯ ¯
    ¯ ¯ ¯ ¯ ¯ ≤10 if10< ERR t i ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ≤20 if20< ERR t i ¯ ¯ ¯ ¯
    ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ≤30 otherwise (18) View Source where R max is the predefined maximum
    reward and ERR t i ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ can be computed as ERR t i ¯ ¯ ¯ ¯
    ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ = 1 v ⋅ ∑ j=0 v−1 ERR t−j i−j⋅ l min . (19) View Source In summary,
    for every v -step(s), the agent gets full reward R max only if ERR t i ¯ ¯ ¯ ¯
    ¯ ¯ ¯ ¯ ¯ ¯ ¯ ¯ is less than 10% error. On the other hand, if it exceeds 30% error,
    the agent is penalized as the data quality is assumed to be compromised in our
    problem so that the agent receives − R max as a result. 2) Energy Consumption
    Reward Function ( E t ) : This reward function particularly focuses on reducing
    the energy consumption. As discussed in [13] and [26], one way of saving such
    energy consumption is to effectively minimize the volume of data transmitted.
    Hence, we propose this reward function that gives the agent a higher reward if
    the agent chooses a longer transmission period. Contrary to this, the shorter
    the transmission period, the smaller the reward is given to the agent. In short,
    the energy consumption could be significantly saved when the longer transmission
    period is chosen instead of the shorter one as the data volume is substantially
    affected by the transmission period. Here, the energy consumption reward function
    E t can be constructed as E t = R max ⋅ ∑ v j=1 a t−j p max ⋅v . (20) View Source
    Overall, the novel reward function is proposed in this article that considers
    both aspects of data quality Q t and the energy consumption E t . Now, the optimal
    policy π ∗ θ ( a t | s t ) that selects the best transmission period based on
    the state extracted from the transmitted data can be expressed mathematically
    as π ∗ θ ( a t | s t )=arg max π θ L(θ) (21) View Source where π ∗ θ is the optimal
    policy learned via PPO, one of the most effective mode-free DRL algorithms. L(θ)
    is the surrogate objective function utilized to optimize the policy parameter
    θ . In the next section, the detailed procedure of PPO is explained, and a schematic
    overview of the proposed PPO-ATPC is illustrated in Fig. 5. Fig. 5. Schematic
    overview of the proposed PPO-ATPC. Show All E. DRL Algorithm: Proximal Policy
    Optimization PPO is known as the state-of-the-art on-policy algorithm, which is
    proposed by OpenAI [18] and is under actor-critic structure. Given the certain
    state, actor network is employed to output a particular action while critic network
    provides the value-function, which assesses the performance of actor network [44].
    In addition, PPO can be directly applied to the environments with either continuous
    or discrete action spaces and has some of the merits of trust region policy optimization
    (TRPO) in terms of both stability and reliability. Now, compared to TRPO, PPO
    is more general and much easier to implement because it is a first-order optimization
    algorithm. As explicitly written in [18], it optimizes a clipped surrogate objective
    function using mini-batch stochastic gradient ascent, which is given by L(θ)=
    E ^ [min( u t (θ) A ^ θ old ( s t , a t ), clip( u t (θ),1−ϵ,1+ϵ) A ^ θ old (
    s t , a t ))] (22) View Source where A ^ θ old ( s t , a t ) is an advantage function
    estimated by the generalized advantage estimation [45], ϵ is a clipping rate,
    and finally u t (θ) is a probability ratio between old and new policies, which
    can be expressed as u t (θ)=[ π θ ( a t | s t ) π θ old ( a t | s t ) ]. (23)
    View Source In particular, the clip function prevents exceptionally large policy
    updates and thus alleviates the problem of catastrophic loss of performance. That
    is, it aims to eliminate incentives for the new policy to break away from the
    old policy. Hence, if A ^ θ old ( s t , a t )>0 , then u t (θ) is clipped at 1+ϵ
    . Contrary to this, if A ^ θ old ( s t , a t )<0 , then u t (θ) is clipped at
    1−ϵ . Herein, A ^ θ old ( s t , a t ) can be computed as follows: A ^ θ old (
    s t , a t )= δ t +(γλ) δ t+1 +…+(γλ ) U−t δ U (24) View Source where U denotes
    the size of mini-batch and γ is a discount factor while λ is a GAE parameter utilized
    for lowering the variance, which enables training more stable. Next, δ t is the
    temporal difference (TD) error, which can be obtained as δ t = r t +γ V ϕ ( s
    t+1 )− V ϕ ( s t ) (25) View Source where V ϕ ( s t ) indicates the value-function
    approximated with a deep neural network (DNN). Here, the critic network can be
    updated by minimizing the following loss function: L(ϕ)= E ^ [( V ϕ ( s t )− V
    ^ t ) 2 ] (26) View Source where V ^ t can be computed as V ^ t = ∑ j=t U γ j−t
    r j . (27) View Source At the end, the critic parameter ϕ is updated by mini-batch
    stochastic gradient descent while the actor parameter θ is updated by mini-batch
    stochastic gradient ascent ϕ θ ←ϕ− ξ c ∇ ϕ L(ϕ) ←θ+ ξ a ∇ θ L(θ) (28) (29) View
    Source where ξ c is the learning rate of critic network and ξ a is the learning
    rate of actor network. Herein, the pseudocode of our proposed PPO-ATPC is explicitly
    shown in Algorithm 1. Algorithm 1 PPO-ATPC 1: Input Y={ y i | x i = x 1 +(i−1)
    p o ,i≤L,i∈N} 2: Set parameters p max , p min , α , n , m , R max , v , Z , T
    , U , H , γ , λ , ϵ , ξ a , ξ c 3: Randomly initialize the critic network V ϕ
    with ϕ 4: Randomly initialize the actor network π θ with θ 5: Initialize the old
    actor parameter θ old ←θ 6: Initialize the buffers B←∅ , B a ←∅ , B e ←∅ 7: Obtain
    a set of all the widths of bollinger bands 8: W Y ={ WB Y i |n≤i≤L} 8: Compute
    WB max and WB min according to (16) and (17) 9: for episode=1:Z do 10: Reset t←0
    , and a t ← p min 11: r t , s t+1 = INFO (t,i, a t ) 12: for t=1:T do 13: Observe
    the current state s t 14: Select an action a t using the old actor π θ old 15:
    Obtain the current reward r t and the next state s t+1 16: r t , s t+1 = INFO
    (t,i, a t ) 16: Collect the trajectory B←B∪( s t , a t , r t , s t+1 ) 17: Update
    the current position i←i+ l min 18: if |B|=U then 19: for epoch=1:H do 20: Sample
    the data from the replay buffer B 21: Train actor network: 22: Compute L(θ) by
    (22) 23: Update the actor parameter θ by (29) 24: Train critic network: 25: Compute
    L(ϕ) by (26) 26: Update the critic parameter ϕ by (28) 27: end for 28: Update
    θ old ←θ 29: Clear the replay buffer B←∅ 30: end if 31: end for 32: end for 33:
    function Info( t,i, a t ) ▹ Includes v -delayed reward 34: Store the action a
    t into B a 35: Update t←t+1 and i←i+ l min 36: Acquire M t i and I t i by Definition
    5 and 6 37: Acquire O t i and Y ^ t by (7) and (8) 38: Compute ERR t i by (15)
    and store it into B e 39: if (t−1)modv==0 and t>1 then 40: Bring the collected
    B e and B a 41: Compute Q t and E t by (18) and (20) 42: Obtain the reward r t
    ←α⋅ Q t +(1−α)⋅ E t 43: else 44: Obtain the reward r t ←0 45: end if 46: Compute
    s B t , s S t and s P t by Definition 7, 8 and 10 47: Obtain the current state
    s t by (13) 48: return r t and s t 49: end function In summary, given the state,
    the agent executes the action based on the outcome of actor network at every t
    step and subsequently obtains both the reward and the next state. In the meantime,
    the trajectory ( s t , a t , r t , s t+1 ) is stored into the replay buffer B
    . Taking U samples from B as one mini-batch, the agent later updates its network
    parameters individually for H times using the Adam optimizer method [46]. Furthermore,
    two fully connected hidden layers with 64 neurons are, respectively, leveraged
    for actor and critic networks, and ReLU activation function is used in each hidden
    layer. Finally, the output layer of actor network passes through a softmax layer.
    SECTION IV. Performance Evaluation In this section, the performance of PPO-ATPC
    is comprehensively investigated by comparing over other alternative benchmark
    methodologies. A series of simulation experiments are implemented in Python 3.6.13
    with PyTorch 1.10.2 running on a server equipped with an Intel Core i7-9700K CPU
    @ 3.60 GHz, 65.0-GB RAM, and NVIDIA GeForce GTX 1660 SUPER with CUDA 11.2. A.
    Data Description and Environment Setup We first configured an IoT-testbed for
    environmental monitoring in an office located in Seoul, South Korea. Then, three
    variables were accumulated in AWS DynamoDB: humidity, CO2, and temperature [6].
    Herein, our proposed PPO-ATPC started training from the initial position i on
    May 12, 2020 (00:00:00 A.M.) and continued until the last position on May 25,
    2020 (00:00:00 A.M.). Next, the data from May 25, 2020 (00:00:00 A.M.) to June
    01, 2020 (00:00:00 A.M.) was fully utilized to evaluate our individually trained
    policy. Importantly, since these data sets were originally accumulated every ten
    seconds, we resampled them to one minute using pandas [47] during the data preprocessing
    stage to allow p min to be set to 1. As a whole, we achieved one optimally trained
    policy per data set after training. The detailed parameters used for PPO-ATPC
    are adequately listed in Table I. As p max is set to 30, the feasible actions
    in A are 1, 3, 5, 10, 15, and 30, satisfying the conditions mentioned in Section
    III-B. During the training phase, the agent attempted those actions several times
    to eventually acquire an optimal or nearly optimal policy. TABLE I Parameter Setting
    for PPO-ATPC B. Performance Indicators In this article, the following three metrics
    are fully utilized to assess the performance of PPO-ATPC with other relevant methodologies.
    The first two metrics are predominantly used to measure the accuracy of continuous
    variables, while the third metric is used to quantity data volume savings (DVS)
    for evaluating energy consumption. Root Mean Square Error (RMSE): RMSE ( y ^ ,y)
    is denoted as the average squared difference between the reconstructed data (
    y ^ ) and the true data (y) . A better performing method will have a lower value
    of RMSE RMSE( y ^ ,y)= 1 w ⋅ ∑ i=1 w ( y ^ i − y i ) 2 − − − − − − − − − − − −
    − − √ . (30) View Source Mean Absolute Error (MAE): MAE ( y ^ ,y) is defined as
    the mean of the magnitude of difference between the reconstructed data ( y ^ )
    and the true data (y) MAE( y ^ ,y)= 1 w ⋅ ∑ i=1 w | y ^ i − y i |. (31) View Source
    DVS: DVS can be expressed as a percentage of the reduction in data volume DVS=(1−
    V new V o )⋅100 (32) View Source where V new is the size of data accrued due to
    the control of transmission period and V o is the size of original data. C. Convergence
    Performance of PPO-ATPC To investigate the convergence of our proposed PPO-ATPC,
    we first recorded the mean and variance of the cumulative reward every ten episodes
    up to 3500 episodes for each data set. As the weighting factor α has a significant
    impact on policies that will be made, we trained the model with different values
    of α as follows: 0.20, 0.35, 0.50, 0.65, 0.80, and 1.00. During training, a callback
    function was used to automatically save the best performing models for evaluation
    with test data sets. The results shown in Fig. 6 demonstrate that the rewards
    start with low values but converge rapidly within hundreds of episodes, regardless
    of the value of α . Additionally, different DRL algorithms, such as DQN and A2C,
    were further implemented in our environment to highlight the effectiveness of
    PPO. DQN: It is known as one of the famous DRL algorithms proposed by researchers
    at DeepMind. This algorithm approximates a state-value function in Q -learning
    framework with a neural network [16]. A2C: It is also fully developed by researchers
    at DeepMind. A2C is a policy gradient algorithm and part of the on-policy family
    [17]. Fig. 6. Convergence performance of PPO-ATPC. (a) Humidity data set. (b)
    CO2 data set. (c) Temperature data set. Show All In this study, an automatic hyperparameter
    optimization software framework called Optuna [48] was exploited with a tree-structured
    Parzen estimator (TPE) sampler to find the best parameters for both DQN and A2C
    models. Fig. 7(b) shows that PPO-ATPC not only converges faster than DQN-ATPC
    and A2C-ATPC but also achieves higher cumulative rewards. It can be observed from
    Fig. 7(a) that A2C-ATPC with the temperature data set failed to converge at the
    end of the episodes, possibly due to an update in the wrong direction during training.
    Meanwhile, DQN-ATPC achieved the lowest rewards and also failed to converge. This
    experiment confirms that one of the main drawbacks of DQN is the long training
    time required to train our task. Such similar drawbacks have also been reported
    in several papers [49], [50], [51]. In conclusion, PPO takes much less time to
    train and provides better and more stable performance when compared to DQN and
    A2C in our target environment. Fig. 7. Impact of different DRL algorithms on convergence
    performance (when α=0.65 ). (a) Humidity data set. (b) CO2 data set. (c) Temperature
    data set. Show All D. Effect of Weighting Factor on PPO-ATPC To explore the effect
    of weighting factor α on our proposed PPO-ATPC, we trained models with different
    values of α and evaluated the resulting policies using test data sets. The weighting
    factor α determines the balance between the data quality and energy consumption
    reward functions, which are used to guide the agent’s decisions. If data quality
    is considered more significant, α should be higher for more emphasis on the data
    quality function Q t , and vice versa for energy consumption. As a result, different
    policies are formed depending on the value of α . In this study, our primary objective
    is to determine the value of α that strikes the right balance between data quality
    and energy consumption. Here, we evaluated our policies with the test data sets
    in terms of average RMSE, MAE, and DVS, as indicated in Table II. Furthermore,
    changes in average RMSE and DVS as α varies are explicitly depicted in Fig. 8.
    Our experiments revealed that the average RMSE, MAE, and DVS decrease as α increases.
    In particular, when α was set to 0.20 for each data set, both the average RMSE
    and MAE were the highest among all other values of α , resulting in the selection
    of the maximum transmission period, p max , to save energy consumption. Conversely,
    setting α to 1.00 for each data set resulted in complete disregard for energy
    consumption while still maintaining focus on data quality. This led to the selection
    of the minimum transmission period, p min , achieving zero average RMSE, MAE,
    and DVS. TABLE II Performance of PPO-ATPC in Terms of Average RMSE, MAE, and DVS
    ( V o =10080) Fig. 8. Change of average RMSE and DVS depending on the weighting
    factor α . (a) Humidity data set. (b) CO2 data set. (c) Temperature data set.
    Show All Fig. 9 illustrates the relationship between α and the average transmission
    period. The translucent bands with vertical lines represent the standard deviation
    for each value of α , while the yellow solid lines with marks denote the average
    values. As α gets higher, the average value of transmission period starts to decline,
    indicating that more emphasis is being placed on data quality. Exceptionally,
    the vertical line [Fig. 9(a) to (c)] is greatest when α is set to 0.65, which
    suggests that this value allows for a more adaptive transmission period that balances
    energy consumption and data quality effectively. Overall, when α was set to 0.65,
    we reduced the total data volume for each data set by 73.849%, 89.931%, and 81.310%,
    respectively, with an average RMSE of 0.322, 13.896, and 0.048. This represents
    a significant improvement over the model trained with α of 0.20, as we were able
    to reduce the average RMSE by 55.2%, 26.0%, and 38.5%, respectively. Fig. 9. Relationship
    between α and the average transmission period. (a) Humidity data set. (b) CO2
    data set. (c) Temperature data set. Show All Fig. 10 shows the visual results
    of our proposed PPO-ATPC with different values of α using the humidity data set
    to provide a more detailed comparison. As depicted in Fig. 10(a), selecting α
    of 0.20 resulted in the maximum transmission period being chosen in most cases,
    which led to deteriorating data quality from approximately 10:00 A.M. to 16:00
    P.M. However, when α was set to 0.65, a more adaptive control of transmission
    period occurred efficiently on a fixed cycle as shown in Fig. 10(b). Specifically,
    shorter transmission periods were chosen in intervals with larger variations,
    while longer periods were chosen in intervals with little or no fluctuation. For
    instance, at 8:00 A.M., the longer transmission period was chosen for the next
    30 min as the variance of the data transmitted before 8:00 A.M. was quite small.
    Then, at 8:30 A.M., a shorter transmission period was chosen instead to accumulate
    values without compromising data quality. Finally, as clearly illustrated in Fig.
    10(c), the minimum transmission period p min was assigned all the time when α
    was set to 1. The results discussed here verify that PPO-ATPC works properly in
    an upright manner with α of 0.65, achieving a balanced tradeoff between energy
    consumption and data quality. Fig. 10. (Humidity data set) Visual comparison of
    PPO-ATPC with different values of α on May 25, 2020. (a) In a case where α=0.20
    . (b) In a case where α=0.65 . (c) In a case where α=1.00 . Show All In summary,
    our results support that the choice of α plays a vital role in balancing data
    quality and energy consumption in PPO-ATPC, and the optimal value of α depends
    on the specific application and user preferences. Our findings can provide guidance
    for selecting appropriate values of α in practice. E. Performance Comparison Table
    III presents a quantitative comparison of the performance of PPO-ATPC with eight
    other methodologies, all implemented using Python. The methodologies are as follows.
    Uniform (15): Data transmission at a fixed period of 15 min, which is one of the
    possible actions in A . Uniform (30): Data transmission at a maximum period of
    30 min, which is equivalent to p max . Random: Random control of transmission
    period in the designated environment. UDASA: A light-weight algorithm that controls
    the transmission period based on changes of MAD [52]. PBATPC: A prediction-based
    adaptive data transmission period control algorithm with a CNN model [13]. DEBB:
    A dynamic sample rate adaptation algorithm based on the estimation of single dynamics
    [20]. DQN-ATPC: A policy learned via DQN with α of 0.65. A2C-ATPC: A policy learned
    via A2C with α of 0.65. TABLE III Performance Evaluation of PPO-ATPC Over Other
    Methodologies Using Test Data Sets (α=0.65) Fig. 11 depicts the change of average
    RMSE and DVS across different methodologies. First, as seen in Fig. 11(a), our
    proposed PPO-ATPC outperformed all other methodologies on the humidity data set,
    achieving a total data volume reduction of up to 73.849% while maintaining the
    highest accuracy. In contrast, A2C-ATPC and DQN-ATPC, which are the second and
    third superior methods in terms of accuracy, reduced the data volume less than
    PPO-ATPC but exhibited much higher average RMSEs, suggesting that these DRL algorithms
    (DQN and A2C) could not find the optimal policy during training. Notably, the
    spacing difference between the bar and the red dotted line with marks in PPO-ATPC
    is the largest, showing its strong ability to balance energy consumption and data
    quality. Fig. 11. Change of average RMSE and DVS depending on the methodologies.
    (a) Humidity data set. (b) CO2 data set. (c) Temperature data set. Show All On
    the CO2 data set, DQN-ATPC achieved the best performance in terms of average RMSE
    and MAE, as shown in Fig. 11(b). However, when compared to PPO-ATPC, the difference
    in average RMSE was only 0.467, while the difference in DVS was almost 31%. This
    result reveals that PPO-ATPC was more efficient than DQN-ATPC. Likewise, the largest
    spacing difference between the bar and the red dotted line with marks was found
    in PPO-ATPC, demonstrating its superiority to others, such as UDASA, PBATPC, DEBB,
    and A2C-ATPC, which reduced data volume at the expense of data quality. On the
    temperature data set, A2C-ATPC led to the best performance in terms of average
    RMSE and MAE, as depicted in Fig. 11(c). Nevertheless, upon comparison with the
    proposed PPO-ATPC, the difference in average RMSE was only 0.002, almost negligible,
    while the difference in DVS was 20.854%. This finding suggests that A2C-ATPC was
    not trained as effectively as PPO-ATPC, as evidenced by the failure of A2C-ATPC
    to converge at the end of the episode, as seen in Fig. 7(a). The experimental
    results presented in Fig. 12 demonstrate the effectiveness of PPO-ATPC in adaptively
    controlling the transmission period of sensor nodes in three different test data
    sets. Fig. 12(a) shows that PPO-ATPC successfully adapts the transmission period
    to high-data fluctuations, as seen on 25th, 26th, 29th, and 31st, where relatively
    large amounts of data were transmitted compared to 27th, 28th, and 30th. This
    capability ensures that high-data quality is maintained. Next, as evident from
    Fig. 12(b), high DVS were achieved on 30th and 31st compared to the other days,
    as a result of the absence of significant fluctuations, leading to only a small
    fraction of data being transmitted. Consequently, this led to a substantial reduction
    in energy consumption of the IoT sensor during those days. Moreover, as seen in
    Fig. 12(c), the highest DVS were achieved on 30th among all other days due to
    fewer sudden, abrupt changes in the data. In summary, these findings provide strong
    evidence supporting the successful implementation of the adaptive control of the
    transmission period by PPO-ATPC, thereby underscoring its efficacy as a promising
    method for optimizing IoT sensor node operation. Fig. 12. Overall performance
    of PPO-ATPC on three different test data sets. (a) On the humidity test data set;
    only 26.151% of the data was transmitted. (b) On the CO2 test data set; only 10.069%
    of the data was transmitted. (c) On the temperature test data set; only 18.690%
    of the data was transmitted. Show All Overall, diverse experiments have demonstrated
    an inherent tradeoff between energy consumption and data quality, and our study
    confirms that PPO-ATPC is a superior solution for achieving autonomous and adaptive
    control of the transmission period while balancing both energy consumption and
    data quality. As illustrated in Fig. 12, the results show that PPO-ATPC accurately
    captures the underlying data pattern for each data set while transmitting only
    a small fraction of the data, i.e., 26.151%, 10.069%, and 18.690%, respectively.
    This signifies that PPO-ATPC is well-trained in terms of both accuracy and energy
    consumption. Notably, the humidity data set exhibited more abrupt changes compared
    to the CO2 and temperature data sets, resulting in the lowest overall data volume
    reduction on the humidity data set. In conclusion, PPO-ATPC achieved significant
    data reduction while maintaining accuracy and low-energy consumption, validating
    the objective of autonomous and adaptive control of the transmission period of
    each sensor node. The proposed PPO-ATPC has the potential to achieve significant
    energy savings without sacrificing accuracy, making it a promising approach for
    IoT applications. SECTION V. Conclusion and Future Work Wireless sensor networks
    have gained popularity in recent years, owing to their ability to monitor different
    environments, such as smart homes, smart cities, and industrial sites. However,
    the energy constraints of these sensors pose a significant challenge, as they
    need to conserve energy to prolong their battery life. One way to achieve this
    is by reducing the amount of data transmitted from the sensors, while ensuring
    that the data quality is maintained. In this article, we proposed a novel technique
    called PPO-ATPC, which enables the automatic and adaptive control of the transmission
    period of a sensor node, achieving an optimal balance between energy consumption
    and data quality. The proposed PPO-ATPC has integrated the principle of DRL, and
    employed an unprecedented design for the state, action, and reward. First, as
    the dimension of the action or state space significantly influences the computational
    workload in real-world applications [53], this article initially discretized the
    continuous action space and effectively minimized the state space to 1x6 by reflecting
    the characteristics of bollinger bands. Additionally, this article presented an
    innovative reward function that considers both data quality and energy consumption,
    which is significant for incentivizing the agent in the field of DRL. We also
    evaluated the performance of PPO-ATPC on three different data sets accumulated
    in AWS DynamoDB with eight other methods in terms of average RMSE, MAE, and DVS.
    Experimental results have confirmed that PPO-ATPC most effectively controls the
    transmission period through an appropriate balance between energy consumption
    and data quality, reducing the total data volume for each data set by 73.849%,
    89.931%, and 81.310%, respectively, with average RMSE of 0.322, 13.896, and 0.048.
    In summary, the methodology developed in this article helps extend the lifespan
    of the sensor while significantly reducing the number of data transmitted. Finally,
    this article concludes with some interesting future directions as follows. Real-World
    Deployment: It would be interesting to implement the PPO-ATPC in deployment scenarios
    to evaluate its feasibility and practicality in real-world settings. Application
    in Other Domains: The research can be extended to evaluate PPO-ATPC in various
    domains beyond environmental monitoring, including remote patient monitoring,
    industrial automation monitoring, and pedestrian or vehicular traffic monitoring.
    Integration With Continual Learning: The proposed PPO-ATPC could be integrated
    with continual learning techniques to enable the agent to continuously learn from
    the environment and improve its performance over time. Investigation of Different
    Reward Functions: The proposed reward function in this article considers both
    data quality and energy consumption. Future research could explore different reward
    functions to incentivize the agent to achieve other objectives, such as latency
    or security. Multiple Sensor Nodes: The research could be extended to develop
    a single policy that can autonomously control the transmission period of multiple
    sensor nodes simultaneously. This could help to improve the efficiency and effectiveness
    of the overall sensor network. ACKNOWLEDGMENT The authors would like to thank
    the anonymous reviewers for their insightful comments and suggestions. Authors
    Figures References Citations Keywords Metrics More Like This Edge QoE: Computation
    Offloading With Deep Reinforcement Learning for Internet of Things IEEE Internet
    of Things Journal Published: 2020 Deep Reinforcement Learning for Time-Energy
    Tradeoff Online Offloading in MEC-Enabled Industrial Internet of Things IEEE Transactions
    on Network Science and Engineering Published: 2023 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: PPO-Based Autonomous Transmission Period Control System in IoT Edge Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhou Z.
  - Zhang X.Y.
  - Yang S.J.
  - Li H.J.
  - Kuang X.H.
  - Ye H.L.
  - Xu C.Q.
  citation_count: '0'
  description: In consideration of the practical demands derived from“human-machine-thing”super-fusion
    and the vision of ubiquitous intelligence interconnection during the era of the
    internet of everything, federated compute first network are regarded as a promising
    solution, which jointly leverages the data aggregation advantages of distributed
    intelligent technologies, such as federated learning, and the collaborative computing
    advantages of the “information high-speed rail （i. e., low-entropy compute first
    network）” in the same time. The federated compute first networks efficiently utilize
    the abundant data and computing resources deployed ubiquitously and fragmentally
    in the network to maximize the satisfaction of various high-performance and intelligent
    computing tasks. Simultaneously, to establish comprehensive security guarantees
    for users engaged in ubiquitous collaborative computing processes throughout the
    entire life cycle, as well as foster mutual trust between distributed clients
    and aggregation servers within the federated compute first network, introducing
    privacy-preserving computing technologies like differential privacy has become
    one of the most essential requirements. Therefore, under the premise that users’security
    and privacy are not subject to newly emerging threat patterns such as model reverse
    attacks and gradient leakage attacks, how to effectively motivate a large number
    of personalized participants to participate in actively and honestly sharing local
    data and computing power is one of the key steps to realize the real-world deployment
    of multiple different federated computing tasks. However, current incentive mechanisms
    in federated compute first networks primarily focus on training performance-oriented
    factors such as data quality evaluation and fairness research, with more attention
    to be paid to user privacy requirements. These methods cannot effectively regulate
    the privacy noise injection process during collaborative training and information
    sharing. At the same time, edge computing nodes usually exaggerate their local
    privacy budget demands due to their goals of maximizing personal privacy protection
    levels. The irreconcilable conflict between the unsuspecting aggregator and self-interested
    participants may result in severe redundant accuracy loss. To address this problem,
    we propose an adaptive incentive approach of privacy computing in this work for
    federated compute first networks based on an improved Stackelberg leader-follower
    game model. The proposed method employs a two-stage dynamic game to offer differential
    pricing incentives based on the scale of privacy injection during distributed
    computing. Using a backward induction approach, participating users first engage
    in game equilibrium to obtain the optimal local privacy budget of the noise injection
    strategy, followed by the optimal privacy payment strategy determined by the federated
    parameter server. Theoretical analysis shows that the proposed solution achieves
    the optimal Nash equilibrium. Furthermore, the paper discusses the constraints
    on participating users and derives an upper bound for their privacy cost requirements.
    Experimental results on two public image classification datasets such as EMNIST
    and CIFAR, which serve as standard benchmarks for distributed learning tasks over
    a long period, demonstrate that compared to existing privacy incentive mechanisms
    based on contract theory or three-party games, the proposed method significantly
    improves the average utility of all parties involved in distributed intelligent
    collaborative computing tasks while enhancing computational performance and considerably
    reducing redundancy loss while ensuring user privacy requirements are guaranteed.
  doi: 10.11897/SP.J.1016.2023.02705
  full_citation: '>'
  full_text: '>

    ""'
  inline_citation: '>'
  journal: Jisuanji Xuebao/Chinese Journal of Computers
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Adaptive Incentive Mechanism for Privacy Computing in Federated Compute First
    Networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Qiu L.
  - Wang L.
  citation_count: '0'
  description: In this paper, a method based on deep learning to detect abnormal traffic
    of IoTs in edge computing environment is proposed. Firstly, the data are preprocessed
    by data cleaning, normalization, oversampling and undersampling, and data set
    segmentation to obtain a data set with balanced data distribution. Secondly, a
    method of calculating feature information based on data increment is adopted,
    which can accurately extract feature information from the dynamic data flow. Finally,
    the convolution neural network (CNN) is used to extract the local features of
    the data, and the bi-directional gated loop unit (BiGRU) is used to extract the
    long sequence correlation of the data. The two networks work together to extract
    data features. The self-focus mechanism is introduced to deal with redundant data.
    Experiments show that the accuracy, recall and F1 value of the proposed method
    are 97.36%, 98.38% and 97.16%, respectively, in the normal class, which are higher
    than the comparison algorithm.
  doi: 10.1142/S0218126623502833
  full_citation: '>'
  full_text: '>

    "brought to you by UNIVERSITY OF NEBRASKA-LINCOLN Search My Cart Sign in    Institutional
    Access Skip main navigation Subject Journals Books Major Reference Works Resources
    For Partners Open Access About Us Help Cookies Notification We use cookies on
    this site to enhance your user experience. By continuing to browse the site, you
    consent to the use of our cookies. Learn More ×   Journal of Circuits, Systems
    and ComputersVol. 32, No. 16, 2350283 (2023) Research Paper No Access Abnormal
    Traffic Detection Method of Internet of Things Based on Deep Learning in Edge
    Computing Environment Lingcong Qiu  and Lei Wang https://doi.org/10.1142/S0218126623502833Cited
    by:0 (Source: Crossref) Previous PDF/EPUB Tools Share Cite Recommend To Library
    Abstract In this paper, a method based on deep learning to detect abnormal traffic
    of IoTs in edge computing environment is proposed. Firstly, the data are preprocessed
    by data cleaning, normalization, oversampling and undersampling, and data set
    segmentation to obtain a data set with balanced data distribution. Secondly, a
    method of calculating feature information based on data increment is adopted,
    which can accurately extract feature information from the dynamic data flow. Finally,
    the convolution neural network (CNN) is used to extract the local features of
    the data, and the bi-directional gated loop unit (BiGRU) is used to extract the
    long sequence correlation of the data. The two networks work together to extract
    data features. The self-focus mechanism is introduced to deal with redundant data.
    Experiments show that the accuracy, recall and F 1 value of the proposed method
    are 97.36%, 98.38% and 97.16%, respectively, in the normal class, which are higher
    than the comparison algorithm. This paper was recommended by Regional Editor Takuro
    Sato. Keywords: Deep learningabnormal flow detectionBiGRUCNNIoTs We recommend
    Internet of Things Security Early Warning Model Based on Deep Learning in Edge
    Computing Environment Jiayong Zhong et al., Journal of Circuits, Systems and Computers,
    2024 Track signal Intrusion Detection method based on Deep Learning in cloud-edge
    collaborative computing environment Yaojun Zhong et al., Journal of Circuits,
    Systems and Computers, 2023 Harmony search Hawks optimization-based Deep reinforcement
    learning for intrusion detection in IoT using nonnegative matrix factorization
    P. G. Om Prakash et al., International Journal of Wavelets, Multiresolution and
    Information Processing, 2021 Cloud-Edge Computing-Based ICICOS Framework for Industrial
    Automation and Artificial Intelligence: A Survey Weibin Su et al., Journal of
    Circuits, Systems and Computers, 2023 A Review of Anomaly Intrusion Detection
    Systems in IoT using Deep Learning Techniques Muaadh A. Alsoufi et al., Advances
    in Data Science and Adaptive Analysis, 2021 COCAM: a cooperative video edge caching
    and multicasting approach based on multi-agent deep reinforcement learning in
    multi-clouds environment Ruohan Shi et al., Journal of Cloud Computing, 2023 Intelligent
    mobile edge computing for IoT big data Gwanggil Jeon et al., Complex & Intelligent
    Systems, 2022 Minimize average tasks processing time in satellite mobile edge
    computing systems via a deep reinforcement learning method Shanchen Pang et al.,
    Journal of Cloud Computing, 2023 Making accurate object detection at the edge:
    review and new approach Zhenhua Huang et al., Artificial Intelligence Review,
    2021 Real-Time Detection Network SI-SSD for Weak Targets in Complex Traffic Scenarios
    Yalin Miao et al., Neural Processing Letters, 2022 Powered by Figures References
    Related Details Vol. 32, No. 16 Metrics Downloaded 12 times History Received 24
    December 2022 Accepted 31 March 2023 Published: 6 July 2023 Keywords Deep learning
    abnormal flow detection BiGRU CNN IoTs PDF download Resources For Authors For
    Booksellers For Librarians Copyright & Permissions Translation Rights How to Order
    Contact Us Sitemap    About Us & Help About Us News Author Services Help Links
    World Scientific Europe World Scientific China 世界科技 WS Education (K-12) Global
    Publishing 八方文化 Asia-Pacific Biotech News World Century Privacy policy © 2024
    World Scientific Publishing Co Pte Ltd Powered by Atypon® Literatum"'
  inline_citation: '>'
  journal: Journal of Circuits, Systems and Computers
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Abnormal Traffic Detection Method of Internet of Things Based on Deep Learning
    in Edge Computing Environment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sirisha N.
  - Gopikrishna M.
  - Ramadevi P.
  - Bokka R.
  - Ganesh K.V.B.
  - Chakravarthi M.K.
  citation_count: '0'
  description: 'Increasing numbers of devices that output large amounts of geographically
    referenced data are being deployed as the Internet of Things (IoT) continues to
    expand. Partly as a result of the IoT''s dynamic, decentralized, and heterogeneous
    architecture. These are all examples of the Internet of items (IoT), despite the
    fact that we might be thinking that one of these items is different from the others.
    The physical and digital worlds are connected by the Internet of Things (IoT).
    Nowadays, one of the key goals of the Internet is its own development. This paper
    provides an in-depth analysis of IoT-based data quality and data preparation strategies
    developed with multinational corporations in mind. The goal is to make IoT data
    more trustworthy and practical so that MNCs may use it to their advantage in making
    educated business decisions. The proposed structure consists of three distinct
    actions: gathering data, evaluating data quality, and cleaning up raw data. Data
    preprocessing research is essential since it decides and significantly affects
    the accuracy of predictions made in later stages. Thus, the recommendation for
    a special and useful combination in the framework of different data preprocessing
    task types, which includes the following four technical elements and is briefly
    justified, is made. The Internet of Things (IoT) is a design pattern in which
    commonplace items can be equipped with classification, sensing, networking, and
    processing capabilities that will enable them to communicate with one another
    over the Internet to fulfill a specific function. The Internet of Things will
    eventually change physical objects into virtual objects with intelligence. In
    addition to a detailed analysis of the IoT layer, this article gives an overview
    of the existing Internet of Things (IoT), technical specifics, and applications
    in this recently growing field. However, this publication will provide future
    scholars who desire to conduct study in this area of Internet of Things with a
    better knowledge.'
  doi: 10.1016/j.hitech.2023.100477
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Background 3. Case study 4. Discussion
    5. Conclusion CRediT authorship contribution statement Data availability References
    Show full outline Figures (7) Show 1 more figure The Journal of High Technology
    Management Research Volume 34, Issue 2, November 2023, 100477 IoT-based data quality
    and data preprocessing of multinational corporations Author links open overlay
    panel N. Sirisha a, M. Gopikrishna b, P. Ramadevi c, Raveendranadh Bokka b, K.V.B.
    Ganesh d, M. Kalyan Chakravarthi e Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.hitech.2023.100477
    Get rights and content Abstract Increasing numbers of devices that output large
    amounts of geographically referenced data are being deployed as the Internet of
    Things (IoT) continues to expand. Partly as a result of the IoT''s dynamic, decentralized,
    and heterogeneous architecture. These are all examples of the Internet of items
    (IoT), despite the fact that we might be thinking that one of these items is different
    from the others. The physical and digital worlds are connected by the Internet
    of Things (IoT). Nowadays, one of the key goals of the Internet is its own development.
    This paper provides an in-depth analysis of IoT-based data quality and data preparation
    strategies developed with multinational corporations in mind. The goal is to make
    IoT data more trustworthy and practical so that MNCs may use it to their advantage
    in making educated business decisions. The proposed structure consists of three
    distinct actions: gathering data, evaluating data quality, and cleaning up raw
    data. Data preprocessing research is essential since it decides and significantly
    affects the accuracy of predictions made in later stages. Thus, the recommendation
    for a special and useful combination in the framework of different data preprocessing
    task types, which includes the following four technical elements and is briefly
    justified, is made. The Internet of Things (IoT) is a design pattern in which
    commonplace items can be equipped with classification, sensing, networking, and
    processing capabilities that will enable them to communicate with one another
    over the Internet to fulfill a specific function. The Internet of Things will
    eventually change physical objects into virtual objects with intelligence. In
    addition to a detailed analysis of the IoT layer, this article gives an overview
    of the existing Internet of Things (IoT), technical specifics, and applications
    in this recently growing field. However, this publication will provide future
    scholars who desire to conduct study in this area of Internet of Things with a
    better knowledge. Previous article in issue Next article in issue Keywords Data
    analysisMultinational organizationInternet of thingsData qualityData preprocessing
    1. Introduction The Internet of Things (IoT) is an emerging paradigm that enables
    previously inconceivable levels of interoperability between previously disparate
    electronic devices and sensors, all of which work together to improve our daily
    lives. The Internet of Things (IoT) takes advantage of high-tech devices and the
    web to provide novel solutions to issues encountered by corporations, governments,
    and public/private organizations worldwide (Yu, Liu, Dillon, Rahayu, & Mostafa,
    2022). The Internet of Things (IoT) is becoming increasingly vital and omnipresent
    in all facets of modern life. As a result, IoT represents a significant technological
    leap because to the integration of numerous previously separate smart systems,
    frameworks, intelligent devices, and sensors (Fig. 1). Indeed, data preparation
    is an essential technique used in tandem with Internet of Things data in multinational
    corporations. Raw data from IoT devices and sensors is transformed and cleaned
    in a number of ways in the context of IoT-based data preparation for MNCs before
    it can be used for analysis and decision-making. Multinational corporations (MNCs)
    can improve the quality of their IoT data for analysis, decision making, and the
    discovery of actionable insights by employing data pretreatment techniques in
    an IoT setting. In the end, this aids MNCs in realizing their IoT data''s full
    potential in enhancing operational efficiency, enhancing the customer experience,
    and bolstering strategic decision-making. (See Fig. 2, Fig. 3, Fig. 4, Fig. 5,
    Fig. 6, Fig. 7.) Download : Download high-res image (130KB) Download : Download
    full-size image Fig. 1. Process of data Analysis and preprocessing of Multinational
    Corporation based on IoT. Download : Download high-res image (91KB) Download :
    Download full-size image Fig. 2. Growth of the IoT utilization in Multinational
    Organization. Download : Download high-res image (260KB) Download : Download full-size
    image Fig. 3. Different Techniques of Preprocessing Techniques. Download : Download
    high-res image (293KB) Download : Download full-size image Fig. 4. IoT based data
    Quality and preprocessing of Multinational Organizations. Download : Download
    high-res image (147KB) Download : Download full-size image Fig. 5. Data analysis
    lifecycle of Multinational Organization using IoT. Download : Download high-res
    image (125KB) Download : Download full-size image Fig. 6. Data Information and
    management lifecycle of Multinational Organization. Download : Download high-res
    image (124KB) Download : Download full-size image Fig. 7. Plot for the stages
    of organizational growth. In addition, it takes advantage of quantum and nanotechnology
    in previously unimaginable ways in terms of storage, sensing, and computing speed
    (Wang et al., 2019). Extensive research studies have been undertaken and are available
    as scholarly articles, news reports, and printed materials to highlight the potential
    usefulness and applicability of IoT developments (Cai et al., 2022). It could
    serve as a foundation upon which to build novel, imaginative business strategies
    while keeping in mind the importance of security, assurance, and interoperability.
    For global organizations to make sure the data they collect is reliable, full,
    and accessible for analysis, IoT-based data quality and data pretreatment are
    essential. Multinational organizations are increasingly using IoT (Internet of
    Things) to enhance data quality and data preparation (Lv, Lou, Li, Singh, & Song,
    2021). IoT is utilized for these things in the following ways: Data Collection:
    Sensors, intelligent machines, and linked gadgets are just a few of the sources
    of data that IoT devices can be utilized to gather. To give a more accurate and
    complete picture of the business'' operations, this data can be automatically
    collected and processed in real-time. Predictive Analytics: Machine learning algorithms
    can be used to examine data collected by IoT devices and forecast future trends
    or spot potential issues before they arise. As a result, decision-making is enhanced
    by precise and timely information. Process Automation: Processes that would normally
    be laborious and prone to error can be automated with IoT. As an illustration,
    sensors can be used to track the performance of equipment in a plant and automatically
    change settings to improve output and lower downtime. Supply Chain Management:
    Using IoT, it is possible to monitor the flow of items across the supply chain,
    from raw materials to completed goods. By doing this, logistics are enhanced,
    waste is decreased, and products are delivered promptly and in good shape. Quality
    Control: Using sensors and other connected devices, IoT can be used to discover
    flaws or anomalies in production processes and monitor product quality in real-time.
    This lowers the possibility of product recalls or other quality-related problems
    while assisting in ensuring that products meet the highest standards. In 2020,
    there were 12.5 billion IoT-connected devices worldwide, and by 2025, that figure
    is expected to rise to 30.9 billion, according to a report by IoT Analytics (Li
    et al., 2020; Mohammadi, Al-Fuqaha, Sorour, & Guizani, 2018; Oh, Park, Lee, Choi,
    & Noh, 2020). Additionally, according to the same survey, IoT adoption is now
    being led by the manufacturing, logistics, and transportation sectors, with other
    sectors like healthcare, energy, and retail following closely behind (Babar et
    al., 2023). All things considered, it is obvious that IoT technology is growing
    more and more significant for companies of all kinds and in a wide range of industries,
    including international enterprises. 2. Background The Internet of Things (IoT)
    idea has undergone extensive development, including data collecting, transmission,
    storage, management, and analytics (Rawat et al., 2021). IoT monitoring data and
    associated analytics are key components of smart manufacturing, according to the
    background information on IoT-based smart maintenance. Nowadays, the IoT is linking
    smart devices on an unprecedented scale thanks to the quick adoption of contemporary
    sensor technologies (Gao et al., 2021). The data produced by large-scale smart
    devices serve as the foundation for new services like making wise decisions. Fewer
    works focus on data quality, which is actually a crucial issue in industrial practice,
    whereas the majority of existing publications look for an effective machine learning
    approach for an IoT application-specific goal such as predictive maintenance applications.
    2.1. IoT based data quality The term “IoT-based data quality” (Internet of Things)
    relates to the precision, thoroughness, consistency, timeliness, and relevance
    of the data produced by IoT devices (Zhang, Wang, & Xie, 2018). IoT devices produce
    a ton of data, and for businesses and organizations to benefit from and use this
    data, it must be of a high caliber. IoT data must be accurate and dependable in
    order to be used in important decision-making processes, hence data quality is
    crucial (Ding et al., 2022). IoT data must be of good quality, so numerous elements
    must be taken into account, including: Data accuracy: IoT device data must be
    accurate and reflect the actual state of the environment or system being monitored.
    Data completeness: IoT data needs to be comprehensive, gathering all pertinent
    data needed for analysis and decision-making. Data consistency: IoT device data
    must remain constant throughout time, with no changes or discrepancies. Data timeliness:
    The most recent status of the system or environment being monitored must be reflected
    in IoT data, which must be timely. Data relevancy: IoT device data must be pertinent
    to organizational or company objectives and aims. 2.2. IoT based data preprocessing
    The process of preparing raw Internet of Things (IoT) data generated by IoT devices
    for analysis and modeling includes cleaning, converting, and preprocessing (Demirbaga
    & Aujla, 2022). The fundamental objective of data preparation is to increase the
    data''s correctness and quality so that it is more suited for analysis. IoT devices
    produce a lot of data, but it may be loud, lacking, or inconsistent. As a result,
    preparing data is crucial to ensuring that it is reliable and clean for analysis.
    The primary actions in IoT-based data preparation are as follows: Data cleaning:
    Finding and fixing flaws and inconsistencies in the raw data, such as missing
    numbers, wrong data types, and outliers, is required. Data cleansing (Nguyen,
    Leyva-Mayorga, Lewis, & Popovski, 2021) is the process of removing the inaccurate
    and missing information from the data. There are numerous methods for addressing
    these noisy and missing values. Data integration: To develop a single dataset
    that can be analyzed, this entails merging data from several sources. Data transformation:
    This entails transforming the data into an analytically-ready format by scaling,
    normalization, and encoding. Data reduction: This entails trimming the dataset''s
    size by eliminating extraneous and pointless information, such as duplicates and
    pointless features. Data discretization: In order to make continuous data easier
    to examine and model, this includes turning it into discrete data. Data sampling:
    In order to analyze a representative subset of the data at a lower computing cost,
    this is done. Methods for transforming and preparing raw data for analysis fall
    under the umbrella term “data preprocessing.” With these methods, raw IoT data
    may be cleaned, transformed, and optimized so that it can be used in further analysis,
    modeling, and decision-making at MNCs. Using these methods, multinational corporations
    may clean up their IoT data, eliminate irrelevant information, deal with missing
    values, and gain useful insights. Data bases and data warehouses have seen an
    upsurge in the collection and storage of data over the past few decades. Therefore,
    performing data analysis and mining on enormous volumes of data can take a very
    long time. The same analytical conclusions can be obtained using relatively tiny
    data sets by using data reduction techniques (Elayan, Aloqaily, & Guizani, 2022).
    Data cube aggregation, Attribute subset choices, Dimensionality reduction, Discretization
    and idea hierarchy construction, and Numerosity reductions are examples of conventional
    data reduction techniques (Liu, Zhang, Wang, Dev, & Khowaja, 2023). Data cube
    aggregation was utilized to create a simple form of data. It was used to apply
    to the data and create data cubes. Using the attribute subset selection method,
    redundant, weak, or irrelevant traits are removed. Numerous statistical and computational
    techniques, including filter techniques, wrapper techniques, and embedding techniques,
    can be used to carry out this process. The reduction method used to lower the
    amount of the dataset is dimensionality reduction. By determining the set of the
    major variable, this approach was utilized to lower the number of random variables
    that needed to be taken into account. By removing irrelevant or dated features,
    dimension reduction minimizes the amount of data by applying methods like PCA,
    backward feature elimination, forward feature building, and discriminant approaches.
    Since mathematical models, tiny representations of data, such as parameters, or
    non-parametric methods like clustering, sampling, and histogram are used in place
    of real data, real data is substituted with real data (Song, Qi, & Liu, 2018).
    Hierarchy of Concepts and Discretization The raw data values for the attributes
    are modified using operation techniques by a range or by more conceptual requirements.
    This type of numerical reduction is excellent for automatically creating concept
    sequences. Top-down discretization and bottom-up discretization are the two main
    approaches used in discretization techniques. Binding, histogram analysis, and
    clustering are some of the techniques included in concept hierarchies for numerical
    data. 3. Case study Infosys integrates people, objects, robots, and businesses
    to improve intelligence and quality. The environment is made more convenient,
    secure, and sustainable; production processes are handled; remote operations are
    made possible; failures are forecast; human health is monitored; products are
    designed; and system intelligence is increased. New services and features for
    smart connected products, as well as additional revenue streams for OEMs, are
    made possible by our enhanced service management and warranty management capabilities.
    Thanks to IoT, consumers may bypass traditional middlemen in order to get high-quality
    goods and services at fair prices and with individualized experiences. It is crucial
    for companies to take use of “IoT-as-a-service” if they want to offer superior
    experiences to their customers and streamline their operations. This is because
    of the widespread adoption of virtual connections and other forms of remote working.
    Multiple concepts and methods included into IoT systems can be used to evaluate
    data quality in this context. Organizations can implement strong data quality
    checks by making use of the distinctive features and capabilities of IoT systems,
    such as real-time data collecting, sensor validation, and data fusion. To maximize
    the value of IoT projects, it is essential to validate, verify, and maintain the
    quality of the data collected through the Internet of Things. Infosys uses its
    considerable engineering, domain, and IT knowledge to provide comprehensive services
    throughout the entire Internet of Things (IoT) business environment, including
    consulting, system integration, technology services, platforms, and support. Through
    data-driven business models and innovative service offerings, we promise solutions
    that are stable, secure, and scalable, and which may integrate easily with enterprise
    systems. As ecosystem integrators, we bring together disparate technologies such
    as sensors, networks, AI, analytics, the cloud, and mobile apps to create tailor-made
    answers for our customers. IoT services are enhanced by our deep understanding
    of cutting-edge technologies like blockchain, 5G, mixed reality, and AI. Our mission
    is to help companies thrive in today''s competitive market by teaching them to
    welcome change and adapt to customers'' ever-evolving needs. As an ecosystem integrator,
    we leverage the resources of companies and organizations as diverse as universities,
    trade associations, sensor and device manufacturers, PTC, Amazon Web Services,
    and Microsoft. Our approach fosters an innovation culture to help customers along
    the path to digitalization by providing them with ready-to-deploy industry solutions,
    thought leadership, frameworks and methodologies, R&D opportunities, and co-creation
    opportunities with the IoT Living Labs. Infosys provides human-centered solutions
    that promote the adoption of IoT to create a smarter, safer, and better world
    in the areas of industrial IoT, product IoT, and smart spaces. The global IoT
    professional services market is broken down by type, deployment, organization,
    application, and region. North Americans, Asians, Europeans, Africans, and South
    Americans have all contributed to this study. The MMR market study provides a
    360-degree view of the rapid innovation taking place across all market segments.
    Facts, figures, graphs, and presentations are used to analyze crucial data during
    the time period of 2017–2020. The study analyzes the factors driving, restraining,
    opening, and preventing the growth of the global IoT professional services market.
    This MMR study offers advice for investors based on an examination of the global
    IoT professional services market and the companies operating within it. The year
    2022 serves as the starting point for the market prediction that extends through
    the year 2029. The size of the market in 2022 has been estimated using data and
    outputs from major and key companies around the globe. The market outlook beyond
    2029 is calculated using data analysis of the prior five years'' trends. The impacts
    of the regional lockdown, in particular, will make 2020 a year of analysis and
    exceptions. 4. Discussion Data is being generated at an amazing rate in this new
    digital environment, which creates new paradigms. Because we have powerful computers
    and a lot of data, we can use this information to make data-driven decisions.
    The key advantages of data-driven judgments are that they are formed by looking
    at historical trends that have produced favorable outcomes. In a nutshell, data
    analytics may be defined as the process of manipulating data to uncover hidden
    patterns and beneficial trends that can be used to provide insightful business
    forecasts. When it comes to everyday safety, financial, operational, maintenance,
    and engineering choices, asset information has a direct impact on efficiency and
    performance. The reliability and accessibility of high-quality asset information
    affects each of these choices. Many process firms have made significant investments
    in asset information systems and data collection, according to the Institute of
    Asset Management, to assist manage their assets and boost overall effectiveness
    and performance. Many firms lament the slow delivery of advantages, the difficulty
    of quantifying them, and the continued claim by their asset managers that they
    lack access to the information they require despite very large expenditure. Managing
    assets and information across many forms, programs, and repositories to guarantee
    availability, thoroughness, accuracy, compliance, and consistency over the course
    of an asset''s life is the main problem for businesses. Poor asset information
    increases the likelihood of events, value leakage, and asset failure. ReVisionz
    is aware of the particular difficulties management experiences, the frustrations
    your technical staff feels, and the advantages that a strong asset information
    system may provide. An industry life cycle is a representation of the ebb and
    flow of companies'' activity, growth, and eventual demise within an industry.
    The typical industrial life cycle consists of five phases: initiation, expansion,
    consolidation, peak, and decline. These phases might span anywhere from a few
    months to several years. Customers aren''t buying the new product in large quantities
    just yet because they don''t understand all of its potential uses. There is still
    room for development in the realm of distribution. Additionally, there aren''t
    enough supplementary offerings to customers, which cuts down the new product''s
    potential earnings. As complementary items enter the market, consumers can get
    the full benefits of both the original product and its enhancements. When supply
    is inadequate to meet demand, prices fall, which in turn increases demand. Businesses
    start making money once their income from products exceeds their expenses during
    the expansion phase. Whenever multiple companies merge into one, it''s called
    a shakeout. Some businesses fail because they are unable to adapt to changing
    market conditions or because they continue to incur losses. Some companies merge
    with their competitors or get acquired by those who gained a larger portion of
    the market throughout the expansion. Sales, cash flow, and profit growth all slow
    down during the shakeout phase as industries mature. When an industry reaches
    maturity, the vast majority of its companies have already established themselves
    and the market is essentially saturated. These companies coordinate to try to
    control the degree of competition in their field, protecting themselves and maintaining
    a profit margin by blocking new entrants. They devise strategies to outmaneuver
    rivals and reduce hostilities. At this juncture, a company''s sales, profitability,
    and cash flows are at their highest levels ever since client demand is high and
    stable. Products that have been around for a while gain popularity and become
    more widely available at lower prices because of this phenomenon. The decline
    phase is the last phase of an industry''s life cycle. How intensely competitive
    a market is depends on a number of factors, including the rate of decline, the
    height of exit barriers, and the total amount of fixed costs. Some companies may
    elect to prioritize revenue from their most successful offerings during this recession.
    In order to strengthen their position, some larger companies may attempt to buy
    more manageable competitors. Those who have suffered severe losses and see no
    hope for the future should consider selling their assets. 5. Conclusion New innovations
    in the IoT are of interest to researchers and developers everywhere. Researchers
    and developers of IoT systems are working together to scale up the technology
    and increase the positive impacts it can have on people''s lives. However, progress
    is possible only if we consider the various issues and shortcomings of present
    technical approaches. In this overview, we looked at a wide range of issues and
    challenges that IoT developers face while trying to improve the current model.
    The proposed IoT-based data quality and data preprocessing framework address the
    specific challenges faced by MNCs, considering the scale and complexity of their
    operations across multiple geographic regions. By improving the reliability and
    usability of IoT data, MNCs can unlock the full potential of their data-driven
    initiatives, leading to improved operational efficiency, enhanced customer experiences,
    and strategic decision-making. In addition, the quality of the data generated
    by IoT-based devices is crucial if businesses are going to use it to make informed
    decisions and uncover useful insights. Data pretreatment via IoT is crucial to
    guarantee the data is of high quality and fit for analysis. This paves the way
    for businesses to gain valuable intelligence and make informed choices. Companies
    operating on a global scale that are focusing on improving data quality and preprocessing
    may benefit greatly from IoT. Using the potential of connected devices and real-time
    data analytics, businesses can gain an advantage by making faster, more accurate
    decisions based on real-time insights. CRediT authorship contribution statement
    N. Sirisha: Conceptualization, Methodology, Software. M. Gopikrishna: Data curation,
    Writing – original draft. P. Ramadevi: Visualization, Investigation. Raveendranadh
    Bokka: Supervision. K.V.B. Ganesh: Software, Validation. M. Kalyan Chakravarthi:
    Writing – review & editing. Data availability No data was used for the research
    described in the article. References Babar et al., 2023 M. Babar, M.A. Jan, X.
    He, M.U. Tariq, S. Mastorakis, R. Alturki An optimized iot-enabled big data analytics
    architecture for edge–cloud computing IEEE Internet of Things Journal, 10 (5)
    (March1, 2023), pp. 3995-4005, 10.1109/JIOT.2022.3157552 View in ScopusGoogle
    Scholar Cai et al., 2022 K. Cai, H. Chen, W. Ai, X. Miao, Q. Lin, Q. Feng Feedback
    convolutional network for intelligent data fusion based on near-infrared collaborative
    IoT technology IEEE Transactions on Industrial Informatics, 18 (2) (Feb. 2022),
    pp. 1200-1209, 10.1109/TII.2021.3076513 View in ScopusGoogle Scholar Demirbaga
    and Aujla, 2022 U. Demirbaga, G.S. Aujla MapChain: a Blockchain-based verifiable
    healthcare service management in IoT-based big data ecosystem IEEE Transactions
    on Network and Service Management, 19 (4) (Dec. 2022), pp. 3896-3907, 10.1109/TNSM.2022.3204851
    View in ScopusGoogle Scholar Ding et al., 2022 X. Ding, H. Wang, G. Li, H. Li,
    Y. Li, Y. Liu IoT data cleaning techniques: A survey Intelligent and Converged
    Networks, 3 (4) (December 2022), pp. 325-339, 10.23919/ICN.2022.0026 View in ScopusGoogle
    Scholar Elayan et al., 2022 H. Elayan, M. Aloqaily, M. Guizani Sustainability
    of healthcare data analysis IoT-based systems using deep federated learning IEEE
    Internet of Things Journal, 9 (10) (May15, 2022), pp. 7338-7346, 10.1109/JIOT.2021.3103635
    View in ScopusGoogle Scholar Gao et al., 2021 Y. Gao, G. Zhang, C. Zhang, J. Wang,
    L.T. Yang, Y. Zhao Federated tensor decomposition-based feature extraction approach
    for industrial IoT IEEE Transactions on Industrial Informatics, 17 (12) (Dec.
    2021), pp. 8541-8549, 10.1109/TII.2021.3074152 View in ScopusGoogle Scholar Li
    et al., 2020 F. Li, et al. Online distributed IoT security monitoring with multidimensional
    streaming big data IEEE Internet of Things Journal, 7 (5) (May 2020), pp. 4387-4394,
    10.1109/JIOT.2019.2962788 View in ScopusGoogle Scholar Liu et al., 2023 D. Liu,
    Y. Zhang, W. Wang, K. Dev, S.A. Khowaja Flexible data integrity checking with
    original data recovery in IoT-enabled maritime transportation systems IEEE Transactions
    on Intelligent Transportation Systems, 24 (2) (Feb. 2023), pp. 2618-2629, 10.1109/TITS.2021.3125070
    View in ScopusGoogle Scholar Lv et al., 2021 Z. Lv, R. Lou, J. Li, A.K. Singh,
    H. Song Big data analytics for 6G-enabled massive internet of things IEEE Internet
    of Things Journal, 8 (7) (April1, 2021), pp. 5350-5359, 10.1109/JIOT.2021.3056128
    View in ScopusGoogle Scholar Mohammadi et al., 2018 M. Mohammadi, A. Al-Fuqaha,
    S. Sorour, M. Guizani Deep learning for IoT big data and streaming analytics:
    A survey IEEE Communication Surveys and Tutorials, 20 (4) (2018), pp. 2923-2960
    Fourthquarter https://doi.org/10.1109/COMST.2018.2844341 CrossRefView in ScopusGoogle
    Scholar Nguyen et al., 2021 L.D. Nguyen, I. Leyva-Mayorga, A.N. Lewis, P. Popovski
    Modeling and analysis of data trading on Blockchain-based market in IoT networks
    IEEE Internet of Things Journal, 8 (8) (April15, 2021), pp. 6487-6497, 10.1109/JIOT.2021.3051923
    View in ScopusGoogle Scholar Oh et al., 2020 H. Oh, S. Park, G.M. Lee, J.K. Choi,
    S. Noh Competitive data trading model with privacy valuation for multiple stakeholders
    in IoT data markets IEEE Internet of Things Journal, 7 (4) (April 2020), pp. 3623-3639,
    10.1109/JIOT.2020.2973662 View in ScopusGoogle Scholar Rawat et al., 2021 D.B.
    Rawat, R. Doku, M. Garuba Cybersecurity in big data era: From securing big data
    to data-driven security IEEE Transactions on Services Computing, 14 (6) (1 Nov.-Dec.
    2021), pp. 2055-2072, 10.1109/TSC.2019.2907247 View in ScopusGoogle Scholar Song
    et al., 2018 C. Song, Y. Qi, M. Liu One-request scheme for M2P data transmissions
    in software-defined IoT networks IEEE Access, 6 (2018), pp. 13090-13100, 10.1109/ACCESS.2018.2806621
    View in ScopusGoogle Scholar Wang et al., 2019 S. Wang, J. Yuan, X. Li, Z. Qian,
    F. Arena, I. You Active data replica recovery for quality-assurance big data analysis
    in IC-IoT IEEE Access, 7 (2019), pp. 106997-107005, 10.1109/ACCESS.2019.2932259
    View in ScopusGoogle Scholar Yu et al., 2022 W. Yu, Y. Liu, T. Dillon, W. Rahayu,
    F. Mostafa An integrated framework for health state monitoring in a smart factory
    employing IoT and big data techniques IEEE Internet of Things Journal, 9 (3) (Feb.1,
    2022), pp. 2443-2454, 10.1109/JIOT.2021.3096637 View in ScopusGoogle Scholar Zhang
    et al., 2018 Z. Zhang, Y. Wang, L. Xie A novel data integrity attack detection
    algorithm based on improved Grey relational analysis IEEE Access, 6 (2018), pp.
    73423-73433, 10.1109/ACCESS.2018.2884504 View in ScopusGoogle Scholar Cited by
    (0) View Abstract © 2023 Elsevier Inc. All rights reserved. Part of special issue
    Innovation and Evolving Relationship of Entrepreneurship, Small Businesses in
    the Era of Uncertainty Edited by Laxmi Lydia, Carlos Enrique Montenegro Marin,
    Bouziane BRIK View special issue Recommended articles Editorial note: Heterogeneity
    in management and governance in family firms The Journal of High Technology Management
    Research, Volume 34, Issue 2, 2023, Article 100484 Lucía Garcés-Galdeano View
    PDF Salient research topics on boards of directors of family firms in the Latin
    American context – An institutional perspective The Journal of High Technology
    Management Research, Volume 34, Issue 2, 2023, Article 100483 Miguel Méndez, Pedro
    Vázquez View PDF A stochastic process of software fault detection and correction
    for business operations The Journal of High Technology Management Research, Volume
    34, Issue 2, 2023, Article 100463 D. Srinivasa Kumar, …, S. Bhargavi Latha View
    PDF Show 3 more articles Article Metrics Captures Readers: 20 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Journal of High Technology Management Research
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: IoT-based data quality and data preprocessing of multinational corporations
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Martinez-Caro J.M.
  - Tasic I.
  - Cano M.D.
  citation_count: '0'
  description: Communication architectures based on the Internet of Things (IoT) are
    increasingly frequent. Commonly, these solutions are used to carry out control
    and monitoring activities. It is easy to find cases for manufacturing, prediction
    maintenance, Smart Cities, etc., where sensors are deployed to capture data that
    is sent to the cloud through edge devices or gateways. Then that data is processed
    to provide useful information and perform additional actions if required. As crucial
    as deploying these monitoring solutions is to verify their operation. In this
    article, we propose a novel warning method to monitor the performance of IoT-based
    systems. The proposal is based on a holistic quality model called Quality of X
    (QoX). QoX refers to the use of a variety of metrics to measure system performance
    at different quality dimensions. These quality dimensions are data (Quality of
    Data, QoD), information (Quality of Information, QoI), users' experience (Quality
    of user Experience, QoE), and cost (Quality Cost, QC). In addition to showing
    the IoT system performance in terms of QoX in real-time, our proposal includes
    (i) a forecasting model for independent estimation of QoX applying Deep Learning
    (DL), specifically using a Long Short-Term Memory (LSTM) and time series, and
    (ii) the warning system. In light of our results, our proposal shows a better
    capacity to forecast quality drops in the IoT-based monitoring system than other
    solutions from the related literature.
  doi: 10.1049/wss2.12066
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register IET HUB HOME JOURNALS IET PRIZE PROGRAMME SUBJECTS Visit IET IET Wireless
    Sensor Systems ORIGINAL RESEARCH Open Access A novel system to control and forecast
    QoX performance in IoT-based monitoring platforms Jose-Manuel Martinez-Caro,  Igor
    Tasic,  Maria-Dolores Cano First published: 14 September 2023 https://doi.org/10.1049/wss2.12066
    SECTIONS PDF TOOLS SHARE Abstract Communication architectures based on the Internet
    of Things (IoT) are increasingly frequent. Commonly, these solutions are used
    to carry out control and monitoring activities. It is easy to find cases for manufacturing,
    prediction maintenance, Smart Cities, etc., where sensors are deployed to capture
    data that is sent to the cloud through edge devices or gateways. Then that data
    is processed to provide useful information and perform additional actions if required.
    As crucial as deploying these monitoring solutions is to verify their operation.
    In this article, we propose a novel warning method to monitor the performance
    of IoT-based systems. The proposal is based on a holistic quality model called
    Quality of X (QoX). QoX refers to the use of a variety of metrics to measure system
    performance at different quality dimensions. These quality dimensions are data
    (Quality of Data, QoD), information (Quality of Information, QoI), users'' experience
    (Quality of user Experience, QoE), and cost (Quality Cost, QC). In addition to
    showing the IoT system performance in terms of QoX in real-time, our proposal
    includes (i) a forecasting model for independent estimation of QoX applying Deep
    Learning (DL), specifically using a Long Short-Term Memory (LSTM) and time series,
    and (ii) the warning system. In light of our results, our proposal shows a better
    capacity to forecast quality drops in the IoT-based monitoring system than other
    solutions from the related literature. 1 INTRODUCTION Deploying new technologies,
    such as the Internet of Things (IoT), adds value to numerous services and applications.
    IoT well represents the information value loop described in ref. [1]. The sensors
    generate data. This data is communicated to an end device that collects data from
    several sources. The collected data is then analysed in search for patterns or
    relationships that convert data into valuable information. Finally, some actions
    are carried out, such as initiating or changing a physical event. The benefits
    of incorporating this added value into everyday applications and services [2-5]
    are reflected in an increase in the number of devices connected to the network,
    expected to be 25 billion in 2030 [6]. As crucial as deploying these IoT-based
    monitoring solutions is to verify their operation, and at this point, we make
    two observations. On the one hand, Quality of Service (QoS) metrics, that is,
    delay, jitter, packet losses, and bandwidth, have been widely used as objective
    performance evaluation metrics in communications networks [7]. These metrics are
    still in use, but other emerging metrics supplement them. Quality of user Experience
    (QoE) was first adopted as a natural evolution of QoS [7-9], incorporating new
    measurements to get a complete overall vision of systems'' performance. Such is
    also the case of the quality model proposed by the authors in ref. [10], where
    we introduced the concept of Quality of X (QoX). QoX considers data, information,
    network behaviour, and user experience when evaluating or monitoring the quality
    of a particular system giving rise to four quality dimensions. Quality of Data
    (QoD) measures the quality of the raw data collected by system devices (e.g.,
    sensors). Quality of Information (QoI) measures the quality of the information
    obtained after processing the data. Quality of user Experience (QoE) enriches
    QoS by adding metrics relative to users and networks. Finally, Quality Cost (QC)
    shows the impact of the improvements in the different measured parameters according
    to available resources. For instance, sending more data could improve the QoI,
    but at the cost of using more bandwidth and consuming more energy. Therefore,
    QC can have a significant role in optimising resources [11]. A summary of the
    QoX model and its quality dimensions is shown in Table 1 and Equations (1-4)–(1-4).
    TABLE 1. A summary of the Quality of X (QoX) holistic model: A case for IoT-based
    monitoring. Quality dimension Metrics Description Quality of data (QoD) Precision
    (P) Exactness of the collected measurements in every sensor Truthfulness (Tr)
    Indicates the reliability degree of data resource Completeness (C) Integrity of
    the sensors system Quality of information (QoI) Quantity (Q) Information displayed
    and provided by LoRa devices to network server [0 < Q < 1] Precision (P) Fraction
    of data retrieved that are relevant regard to all information obtained from sensors/networks/services
    [0 < P < 1] Recall (R) Fraction of relevant geographic data retrieved [0 < R <
    1] Accuracy (A) Accuracy degree of information to the decision-maker [0 < A<1]
    Detail (D) Complete degree of information to the processing server [0 < D < 1]
    Timeliness (T) The information is timely for decision making [0 < T < 1] Validity
    (V) Provided information from minimum number of IoT devices [0 < T < 1] Quality
    of user experience (QoE) Jitter (J) Fluctuation of average delay between packets
    traversing the network Delay (D) Average end-to-end delay between end devices
    and the server Packet delivery ratio (PDR) Percentage of packets successfully
    traversing the network to the server Throughput (Th) Average rate of bps received
    on the server during a time interval Gateway availability (GW_av) Percentage of
    time that a gateway has the ethernet interface active and available to forward
    traffic over the traditional network to the server in a time interval period Quality
    cost (QC) Energy consumption (EC) Energy required to perform an action/set of
    actions by all the devices under evaluation Interface use (IU) It is binary and
    will take the value IU = 1 if the devices comply with the time constraint of the
    duty cycle in LoRa and IU = 0 if not. (1) (2) (3) (4) On the other hand, the IoT
    paradigm demands new metrics for performance evaluation since the devices and
    systems implemented are generally different and more complex than other traditional
    or more studied technologies [12]. Numerous works in the related literature present
    IoT monitoring platforms [13-15]. However, no matter the field, the Key Performance
    Indicators (KPI) chosen to evaluate the performance are usually different. Some
    platforms focus on energy consumption or battery life, others on the accuracy
    of the collected data or availability, and so on. It is expected that depending
    on the application field, some KPIs will have more impact than others. Still,
    the lack of a holistic model that accommodates the diversity and influence of
    a wide range of KPIs or metrics in the performance evaluation hinders other tasks,
    such as security or standardisation. This paper proposes a system to control and
    forecast the performance of IoT-based platforms from a broad perspective regarding
    KPIs. Notably, the proposal can detect drops in the QoX of an air-pollution measurement
    platform and could be adapted to other monitoring systems deployed in the framework
    of IoT. Our proposal measures and shows QoX parameters and forecasts possible
    changes, generating accurate and timely warnings. Deep Learning (DL) techniques
    have been used, specifically, a Long Short-Term Memory with time series, to analyse
    independently different QoX dimensions and to predict their behaviour and performance,
    where the estimation is made based on the values of the last recorded measurements.
    The performance of our proposed system is compared with another forecasting solution
    from the research literature [16], showing notable results. In summary, the contributions
    of this paper are the following: (i) A novel warning method to monitor the performance
    of IoT-based systems is presented. The performance is assessed from a holistic
    perspective, including several quality dimensions, from data to cost, with the
    advantage of being configurable to adapt to the application field under consideration.
    (ii) The operation of the proposed system is shown using collected data from a
    real-world scenario. (iii) A forecasting method has been included to detect and
    predict malfunctioning. (iv) The proposed complete system has been compared with
    a state-of-the-art solution, showing better results. The rest of the paper is
    organised as follows. Section 2 presents a comprehensive review of related works.
    Section 3 describes the proposal and the research methodology, including the data
    acquisition, regression model, and warning system. The results are shown and explained
    in Section 9. Finally, a summary of the most important findings is included in
    the conclusion. 2 RELATED WORKS Defining a model for performance evaluation in
    versatile IoT-based monitoring solutions is a complex task. Numerous factors will
    impact, such as the number of devices, distances between the IoT devices/nodes
    and the communication gateway (e.g., from LoRa nodes to the LoRa gateway), assigned
    hardware resources, type of devices, number of software layers implemented, communication
    protocols etc. Any service or application constructed over IoT technology has
    the challenge of measuring its operation quality. For other telecommunication
    services like video streaming, measuring is straightforward, just by assessing
    network delay, packet losses, and other computer network metrics. However, there
    are many influencing factors for IoT-based services, so new proposals are being
    studied in the related literature. In ref. [8], the authors introduced a very
    interesting methodology to measure quality metrics in an IoT service environment,
    but their focus was on the users'' experience and satisfaction. A complete analysis
    of QoE metrics in a wireless environment and a straightforward QoE estimation
    were presented in ref. [17]. Similarly, in ref. [18], the authors used cognitive
    capabilities to optimise radio communications resources in IoT, that is, focussing
    on the wireless network part of the IoT-based systems. The correct selection of
    the most relevant KPIs was emphasised to assess a proper QoE estimation in WiFi
    scenarios employing supervised and unsupervised Machine Learning algorithms. The
    works presented in refs. [19, 20] show how to optimise the accuracy of 5G networks
    minimising costs through Deep Learning techniques. In ref. [21], the authors applied
    a DL model based on a Convolutional Neural Network (CNN) and a Recurrent Neural
    Network (RNN) to predict current and future values of QoE in a time series, showing
    the advantages of incorporating the use of DL in these scenarios. Following the
    same trend, a real-time model based on LSTM was introduced in ref. [22] to detect
    and predict anomalies in wireless networks. These anomalies were the origin of
    performance collapse in any device or network. As another example, Casas et al.
    [16] used a mobile network environment where two phases were distinguished: (i)
    generating a new dataset and (ii) estimating the value of quality metrics using
    ML techniques based on decision trees. The authors used Mean Opinion Scale (MOS)
    scale to evaluate system performance. Other authors [23] proposed a model to estimate
    QoE employing multi-line regression and using as an example an IoT scenario. Finally,
    the works done in refs. [24, 25] addressed the issue of cognitive abilities on
    IoT, following a similar approach to QoX. Although the authors in ref. [24] referred
    to the use of Artificial Intelligence and Machine Learning, they did not specify
    the application of any particular technique. From the mentioned works, we can
    observe that it is critical to have a means of measuring the quality of a system
    operation. This is particularly important, for example, in IoT-based warning systems.
    When the goal is to reduce the material and economic impact of disasters and prevent
    loss of life, a warning system is beneficial to notify the user of a drop in quality.
    In other words, it will tell the user that the system''s performance is decreasing;
    therefore, corrective actions or response plans could be executed. Nevertheless,
    as mentioned previously, the lack of a holistic model able to accommodate the
    diversity and influence of a wide range of KPIs or metrics in the performance
    evaluation of IoT-based monitoring hinders other tasks, such as security or standardisation.
    3 METHODOLOGY AND PROPOSAL This section presents the research methodology followed
    and the description of the proposed warning system. 3.1 Data acquisition In our
    proposal, the air quality of a geographic area is monitored using IoT devices
    with a LoRa communication module, a Low Power Wide Area Network (LPWAN), and processing
    servers in the cloud. As a first step, this system is recreated using computer
    simulations and real data from an open air-quality dataset available in ref. [26].
    Using these real data, we simulate the operation of 53 LoRa devices located across
    a coverage area depicted in Figure 1. Each LoRa device represents an air-quality
    measurement station working continuously for 360000 s (approx. 100 h). The air-quality
    data used in the simulation belongs to a real dataset that can be found in ref.
    [26] and includes the following parameters: NO, NO2, NOx, PM10, PM2.5, wind speed,
    temperature, wind speed, and humidity. FIGURE 1 Open in figure viewer PowerPoint
    The emulated IoT-based air-quality control system. The system includes simulated
    devices and networks, but it uses real data. The system is composed of LoRa nodes
    (LoRa end-node), a LoRa gateway (LoRaGW), and a processing server (Network Server).
    Each LoRa device sends the collected air-quality data to the LoRa gateway station
    in data packets following an exponential distribution and conforming to the limitations
    of the LoRa technology (duty cycle). Extensive simulations are carried out in
    two different scenarios. The first is an ideal scenario where the complete system
    behaves ideally without losses or communication delays. The second one is a scenario
    with failures (lossy), incorporating random losses (<10%) and network delays (<100
    ms). Then, for each scenario, three different geographical environments are considered:
    urban, suburban, and rural, with an effect on the wireless communication channel.
    As a result, six test scenarios are emulated in total. For each emulated scenario,
    we collected the outputs in terms of quality performance QoX throughout the entire
    simulation, that is, the quality of raw data (QoD), the quality of the information
    obtained from that collected data (QoI), the quality of the communication architecture
    (QoE), and the quality cost (QC). Table 1 describes each quality dimension QoX
    and the related metrics. Equation (1-4)–(1-4) show how each quality dimension
    is computed. Figure 2 shows an example of the obtained quality measurements during
    the simulation. FIGURE 2 Open in figure viewer PowerPoint Example of Quality of
    X (QoX) measurements for the IoT-based air-quality system. Each line graph represents
    a different quality dimension (QoD, QoI, QoE, and QC) measured during the emulated
    operation. The closer to 1, the better the quality. Values different from 1 mean
    that there have been problems with any parameters related to a quality dimension
    as shown Table 1. For instance, it is observable that QoE is showing an average
    value because, in this simulation, it is impacted by delay and packet delivery
    ratio. More information about this quality model and its four quality dimensions
    can be found in [10]. 3.2 Preprocessing All outputs generated in those simulations
    were stored independently for each scenario and environment. This new dataset
    contained information about quality dimensions and KPI, namely QoD, QoI, QoE,
    QC, and parameters such as delay, jitter, etc. In order to minimise the effect
    of the random component and maximise the performance of the proposed forecasting
    model and monitoring system introduced in this paper, the dataset used by our
    model consists of data obtained from 10 independent simulations. It is also important
    to note that the dataset contains a wide variety of high and low-QoX values resulting
    from the emulated operation using real data, so it represents well a real scenario.
    Later, all this information is dumped on a data processing software that allows
    processing a large volume of data using Python. Next, data pre-processing eliminates
    failed records and groups the data. The last step in this phase is using sliding
    windows of size t, where t should be much smaller than the total number of data
    collected T (t << T). In this case, the sliding window size is equal to 5 (t =
    5), that is, a history of five data is required to estimate the value of the sixth
    one. It is important to choose a proper t because it allows (or not) the estimated
    value to be less (or more) conservative and adapt (or not) to sudden changes in
    system performance metrics. As an example (Figure 3), we need to know the values
    of A, B, C, D, and E in order to estimate F, and so on, until the last data is
    received. The smaller the window size, the more volatile the resulting prediction
    value is and the better it can adapt to sudden changes in our network. Once the
    data is organised in a time series, the data set is then divided into two parts,
    namely, train-set and test-set (75/25), without being shuffled. FIGURE 3 Open
    in figure viewer PowerPoint Time series with a sliding window of size t = 5. Real
    data from the first QoX measurements (A to E) is used to foresee the next QoX
    value (F), then real data from the next five QoX measurements (B to F) is used
    to foresee the next QoX value (G), and so on. This is done individually for each
    QoX dimension (QoD, QoI, QoE, and QC). 3.3 Proposing a Deep Learning model for
    forecasting The next step is to define an independent regression model for each
    quality dimension QoX. One of the most popular methods for forecasting is the
    use of Neural Networks (NN). Neural networks are a network of interconnected nodes
    in which each node is responsible for simple calculations and their combination
    allows obtaining the desired results. Artificial NN (ANN), Convolutional NN (CNN),
    and Recurrent NN (RNN) are three of the most important types. RNN remember the
    sequence of the data and use data patterns to provide predictions, so they are
    used in natural language processing models and speech recognition. The main difference
    among RNN and other NN is the feedback loops incorporated into RNN, which facilitate
    data sharing among the different nodes. RNN are considered short-term memory systems,
    that is, the processed sequence must be relatively short for previous activations
    to have a relevant effect on the current prediction. However, LSTM are a particular
    category of RNN able to remember a relevant piece of data in the sequence and
    preserving it for several instants of time. Therefore, it can have long-term memory.
    We propose a model based on a bi-directional LSTM with long-term memory blocks,
    back and forward propagation, a sliding window that adjusts the model from the
    training data, an optimiser based on Stochastic Gradient Descent (SGD), and a
    loss function based on Mean Squared Error (MSE). To manage the state and obtain
    the prediction of the next value, these memory blocks use gates, both input and
    output. Finally, forget gate is used to update the model memory. It is important
    to note that the model we propose is based on sequence sorting using two (bidirectional)
    LSTMs as input sequence as shown in Figure 4. The first one adds a copy of the
    sliding window data as is and the other one an inverted copy, which allows a faster
    and deeper learning. The training process updates the weights w of the model coefficients
    iteration after iteration, using SDG, optimising the weights to minimise the loss
    function. The proposed model has an input layer, an output layer, and a hidden
    layer, with the connections between the different layers being bidirectional as
    mentioned. The pseudocode of the proposed forecasting model is shown in Figure
    5. FIGURE 4 Open in figure viewer PowerPoint Graphical representation of the Bidirectional
    Long Short-Term Memory (LSTM) model (x≡input layer, h≡output sequence, y≡output
    layer, σ≡activation function). FIGURE 5 Open in figure viewer PowerPoint Pseudocode
    of the Bi-LSTM forecasting model to predict the values of the QoX dimensions (QoD,
    QoI, QoE, and QC). After the training phase, the model performance will be analysed
    using the test set. The R2 score is calculated as shown in Equations (5) and (6)
    to assess the performance of the forecasting model. R2 with values in the range
    [0,1] is also known as the determination coefficient. It determines the quality
    of the model to replicate the results and the proportion of variation in the results
    that the model can explain. (5) (6) 3.4 Proposing a warning system for performance
    control A simple yet efficient warning system is also introduced, completely configurable,
    that works as follows. In case the forecast predicts a d% drop in the quality
    performance in comparison with a previous observation time (called Teval and initially
    set to 5 min), and if this decrease is sustained over several n consecutive Teval,
    then a warning will be communicated to the user of the IoT-monitoring platform,
    together with the specific QoX dimension, metrics, and devices involved in that
    quality dropping. In this paper, for simplicity, it is assumed that d is equal
    to a 50% drop and n equals 5. Note that these values are entirely customisable
    and can be adapted to the specific IoT system under monitoring. Similarly, a notification
    warning will be generated if the estimated quality level remains identical between
    one Teval and the next. The proposed warning system can be seen as a classification
    problem that takes the value 1 when action is needed and 0 otherwise, allowing
    us to compare the results with other models from the related literature. A similar
    and interesting classification model was found in [16] for quality assessment,
    but that uses Mean Opinion Score (MOS) as the quality metric instead of using
    a more comprehensive quality model as QoX [10, 27]. The MOS scale ranges from
    1-5, where 1 is the minimum value for quality (lowest quality), and 5 is the maximum
    (highest quality). Therefore, in contrast to our QoX model, the model presented
    in [16] does not allow for estimating a quality value continuously but a discrete
    integer value based on the KPIs collected by the simulation. So, to be able to
    compare that proposal with the one suggested in this paper, we focus on the notification
    system that they proposed, and that generated an alarm if the estimated quality
    value was less than 0.4 on a scale [0–1] (or 2 in the MOS scale). 3.5 Metrics
    to evaluate the warning system The robustness of the warning system will be evaluated
    using the confusion matrix and its four well-known components: True Positive (TP),
    True Negative (TN), False Positive (FP), and False Negative (FN). TP is the number
    of positives correctly classified as positive by the model. TN is the number of
    negatives correctly classified as negative by the model. FN is the number of positives
    incorrectly classified as negative, and FP is the number of negatives incorrectly
    classified as positive. Based on the Confusion Matrix, more advanced metrics can
    be calculated. Precision Equation (7) measures the quality of estimated positives
    concerning actual positives. Recall Equation (8) measures the number of cases
    classified as true positives over everything positive, and F1 Score Equation (9)
    is a good metric that seeks to keep estimates away from FP and FN trying to balance
    Precision and Recall. These advanced metrics will be employed to compare the results
    obtained by the warning system with our real QoX performance data and with [16].
    (7) (8) (9) 4 RESULTS AND DISCUSSION This section discusses the results of the
    proposed Deep Learning model to forecast the values of each QoX dimension (QoD,
    QoI, QoE, and QC). Then, the proposed warning system for performance control is
    compared with the contribution from ref. [16]. As mentioned before, we have carried
    out simulations in two different scenarios, one without losses or communication
    delay and another one with random losses (<10%) and network delays (<100 ms).
    Then, for each scenario, three different geographical environments are considered:
    urban, suburban, and rural, with an effect on the wireless communication channel.
    4.1 Results of the Deep Learning model for forecasting The loss function is obtained
    at the end of the training phase. We can quantify the difference between predicted
    and actual values by observing the loss function. In other words, it shows how
    good our model is in predicting the expected outcome. A high value for the loss
    means our model performed poorly. A low value for the loss means our model performed
    very well. We can see in Figure 6 that the loss functions are quite similar. Figure
    6a represents the loss function in the ideal scenario described above for QoD,
    QoI, QoE, and QC. Similarly, Figure 6b represents the loss function in the lossy
    scenario for QoD, QoI, QoE, and QC. It can be deduced from Figure 6 that the model
    converges reasonable quick and well. FIGURE 6 Open in figure viewer PowerPoint
    Loss function to evaluate the performance of the deep learning model for forecasting.
    (a) Loss functions in an ideal urban scenario for QoD, QoI, QoE, and QC. (b) Loss
    functions in a lossy urban scenario for QoD, QoI, QoE, and QC. In both cases,
    the model converges quickly to low values. Once training is finished, we see the
    results of the test phase. Again, using time series, our model estimates the QoX
    value and compares it to the real one, but this time, using the test set. Throughout
    the test set, there are significant performance drops of variable duration over
    the recorded data, and our proposed model allows us to estimate performance drops
    correctly, including short-duration cases. This is due to the sliding window size
    used in the time series (t = 5). Because it is essential to consider multiple
    metrics when evaluating the performance of a deep learning regression model, we
    also use the R2 score to measure and compare the performance of the deep learning
    model for each QoX dimension in each emulated scenario of the IoT-based air-quality
    monitoring system. The closer to 1 R2 is the better the result. As shown in Figure
    7, the best forecasting performance is achieved for the QoI dimension in both
    the ideal and lossy environments, with an approximate R2 value of 0.7. Figures
    8 and 9 include two examples with the forecast for QoI and QoE and the real values
    respectively. A reasonable forecast is achieved for the other quality dimensions,
    QoD and QC. Therefore, on average, the performance obtained by the proposed forecasting
    model is good. FIGURE 7 Open in figure viewer PowerPoint Results in terms of R2
    of our deep learning QoX forecasting model under different emulation conditions.
    The graph on the left corresponds to the ideal scenario. The graph on the right
    corresponds to the scenario with delays and packet losses. Then, for each scenario,
    the four quality dimensions (QoD, QoI, QoE, and QC) are displayed in three different
    environments (urban, suburban, and rural). FIGURE 8 Open in figure viewer PowerPoint
    Example of the results obtained when forecasting QoI (above) versus the real values
    (below), with R2 = 0.75. It can be seen that the drops in quality (QoI dimension)
    are well predicted by the model, both the large and the small drops. (Teval =
    5 min). FIGURE 9 Open in figure viewer PowerPoint Example of the results obtained
    when forecasting QoE (above) versus the real values (below), with R2 = 0.566.
    It can be seen that the drops in quality (QoE dimension) are well predicted by
    the model, both the large and the small drops. (Teval = 5 min). 4.2 Results of
    the warning system for performance control In our warning system, there will be
    a notification when two conditions are met. First, if there is a sharp drop in
    the performance estimation of QoX dimensions, and second, if this drop is maintained
    for a specific time interval. Particularly, a warning will be communicated to
    the user of the IoT-monitoring platform, together with the specific QoX dimension,
    metrics, and devices involved in that quality dropping in case the forecast predicts
    a 50% drop in the quality performance in comparison with the previous observation
    time (Teval), and if this decrease is sustained over five consecutive Teval. As
    mentioned, these values are entirely customisable and can be adapted to the specific
    IoT system under monitoring. Figures 10 and 11 illustrate the operation of the
    warning system. In these graphs, the y-axis is a boolean value, 0 or 1. 1 means
    that an action is required due to a drop in the quality dimension under control,
    whereas 0 means that no action is required because the quality level is kept.
    FIGURE 10 Open in figure viewer PowerPoint Example of the results obtained by
    the warning system for the quality dimension QoI. The lower graph represents the
    real QoI performance; 1 means that during five consecutive Teval the value of
    QoI has decreased 50%. The upper graph shows the output of the warning system
    based on predictions, with Recall = 0.95, Precision = 0.8, and F1 = 0.87. (Teval
    = 5 min). FIGURE 11 Open in figure viewer PowerPoint Example of the results obtained
    by the warning system for the quality dimension QoE. The lower graph represents
    the real QoE performance; 1 means that during five consecutive Teval the value
    of QoE has decreased 50%. The upper graph shows the output of the warning system
    based on predictions, with Recall = 0.98, Precision = 1, and F = 0.99. (Teval
    = 5 min). As we have seen in the related literature, some articles address issues
    similar to this case study, but to the authors'' knowledge, none apply a holistic
    and multi-dimensional model. In order to make a comparison with other works, we
    will focus on the case of QoE monitoring, evaluation, and estimation. In particular,
    our warning system will be compared with the one introduced in ref. [16]. In that
    paper, the authors stated that obtaining data from all levels of the protocol
    stack was necessary to have an accurate estimation. Regarding the estimation,
    they compared different ML models to predict the QoE value and obtained the best
    performance with the C4.5 classification model based on decision trees. This model
    estimates the MOS value based only on the following parameters: gateway availability,
    jitter, delay, throughput, quantity, packet delivery rate, and recall, which were
    also part of our QoX model (see Table 1). A priori, the two forecasting models
    would not be comparable since the problem is addressed by employing two different
    solutions, one with a classification algorithm [16] and the other with a regression
    algorithm (our proposal). Therefore, to compare our proposal with ref. [16], we
    replicate the model presented in ref. [16] to obtain the forecast QoE values.
    Then, we apply the same warning system but either using the forecast QoE data
    obtained with the model from ref. [16] or our forecast QoE data obtained using
    our forecasting model. The results of the notification system with either input
    are compared to the real values using the confusion matrix and the Recall, Precision,
    and F1 metrics. This comparison is only performed for the QoE dimension since
    it is the only one used by the authors in ref. [16]. Each subplot in Figure 12
    represents a scenario (ideal or lossy) and an environment (urban, suburban, and
    rural). The graphs in the upper row show the associated performance for each environment
    under an ideal scenario, while the lower graphs show it for a lossy scenario.
    The urban, suburban, and rural environments are represented from left to right.
    In all scenarios and environments, our warning system performs better in terms
    of Precision, Recall, and F1-score. FIGURE 12 Open in figure viewer PowerPoint
    Comparing the performance of our warning system with the performance of ref. [16].
    Results are shown in terms of Precision, Recall, and F1- Score for the two scenarios
    (ideal and lossy) and the three environments (urban, suburban, and rural). The
    comparison has been made using the QoE quality dimension. Both proposals'' results
    are similar in some cases, such as in rural and urban environments. The differences
    are more significant in a suburban environment, especially in a lossy scenario
    that is closer to reality. For instance, Figure 13 compares model estimation for
    QoE in the lossy suburban environment, and Figure 14 illustrates warning system
    operation comparing the method proposed in ref. [16], our proposal, and the real
    values. Considering all the evaluated scenarios, the average improvement obtained
    with our proposal is 11.4% in Precision, 7.3% in Recall, and 14% in F1 Score.
    FIGURE 13 Open in figure viewer PowerPoint Comparison of forecasting model with
    respect to training values for QoE in lossy suburban environment. At the top,
    it is represented the MOS obtained using the model from ref. [16], in the middle,
    our forecasting model, and below the real QoE measurements. FIGURE 14 Open in
    figure viewer PowerPoint Comparison of the warning system to detect quality drops
    for QoE in a lossy suburban environment. At the top, it is represented the model
    from ref. [16] (Recall = 0.73, Precision = 0.69, F1 0.71; TP = 549, TN = 2601,
    FP = 202; False Negative (FN) = 247), in the middle, the output of our warning
    system (Recall = 0.97, Precision = 0.97, F1 = 0.97; TP = 726, TN = 2828, FP =
    25; FN = 20), and below the QoE warnings based on real values. 5 CONCLUSION The
    development and deployment of IoT technology are advancing rapidly. New applications,
    devices, and systems are being introduced because of their higher efficiency.
    While introducing improvements, it is essential to be able to measure the level
    of quality of these services and applications. An unexpected drop in the performance
    of these solutions, which are often used to monitor risk situations or environments,
    could have dramatic consequences. We have seen that several models have been proposed
    to measure the quality of IoT-based systems. However, just a few introduce a holistic
    approach that can encompass all the features inherent to this type of IoT monitoring
    services, and none present practical results. For the first time, to the authors''
    knowledge, this paper addresses the quality of an IoT system as the conjunction
    of several quality dimensions, called QoD, QoI, QoE, and QC, and in general, QoX.
    An IoT-based air quality monitoring service has been emulated using real data,
    including a deep learning model using LSTM to forecast the behaviour of the four
    quality components and a warning system to communicate sustained quality drops.
    The system has been compared with another method from the related literature,
    showing better performance and a broader spectrum of use. We plan to improve the
    models in future work and conduct new experimental tests in real environments.
    AUTHOR CONTRIBUTIONS Jose-Manuel Martinez-Caro: Conceptualisation; data curation;
    formal analysis; investigation; methodology; software; validation; visualisation;
    writing – original draft. Igor Tasic: Conceptualisation; validation; visualisation;
    writing – review & editing. Maria-Dolores Cano: Conceptualisation; data curation;
    formal analysis; funding acquisition; investigation; methodology; project administration;
    resources; software; supervision; validation; visualisation; writing – original
    draft; writing – review & editing. ACKNOWLEDGEMENTS This work was supported by
    Grant PID2020-116329GB-C22 funded by MCIN/AEI/10.13039/501100011033. CONFLICT
    OF INTEREST STATEMENT There are no conflict of interests. Open Research REFERENCES
    Volume13, Issue5 October 2023 Pages 178-189 Figures References Related Information
    Recommended Priority, network and energy‐aware placement of IoT‐based application
    services in fog‐cloud environments Hiwa Omer Hassan,  Sadoon Azizi,  Mohammad
    Shojafar IET Communications Congestion control in constrained Internet of Things
    networks Lotfi Mhamdi,  Hussam Abdul Khalek IET Wireless Sensor Systems Distance‐based
    congestion control mechanism for CoAP in IoT Sharu Bansal,  Dilip Kumar IET Communications
    Free device location independent WiFi‐based localisation using received signal
    strength indicator and channel state information Fahd Abuhoureyah,  Wong Yan Chiew,  Ahmad
    Sadhiqin Bin Mohd Isira,  Mohammed Al-Andoli IET Wireless Sensor Systems Closed‐form
    solution for scaling a wireless acoustic sensor network Kashyap Patel,  Anton
    Kovalyov,  Issa Panahi IET Wireless Sensor Systems Download PDF ABOUT THE IET
    IET PRIVACY STATEMENT CONTACT IET Copyright (2024) The Institution of Engineering
    and Technology. The Institution of Engineering and Technology is registered as
    a Charity in England & Wales (no 211014) and Scotland (no SC038698) Additional
    links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms of Use About Cookies Manage
    Cookies Accessibility Wiley Research DE&I Statement and Publishing Policies HELP
    & SUPPORT Contact Us Training and Support DMCA & Reporting Piracy OPPORTUNITIES
    Subscription Agents Advertisers & Corporate Partners CONNECT WITH WILEY The Wiley
    Network Wiley Press Room Copyright © 1999-2024 John Wiley & Sons, Inc or related
    companies. All rights reserved, including rights for text and data mining and
    training of artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: IET Wireless Sensor Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A novel system to control and forecast QoX performance in IoT-based monitoring
    platforms
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Khan S.
  - Ullah S.
  - Khan H.U.
  - Rehman I.U.
  citation_count: '2'
  description: The deadly coronavirus disease (COVID-19) has highlighted the importance
    of remote health monitoring (RHM). The digital-twins (DTs) paradigm enables RHM
    by creating a virtual replica that receives data from the physical asset, representing
    its real-world behavior. However, DTs use passive Internet of Things (IoT) sensors,
    which limit their potential to a specific location or entity. This problem can
    be addressed by using the Internet of Robotic Things (IoRT), which combines robotics
    and IoT, allowing the robotic things (RTs) to navigate in a particular environment
    and connect to IoT devices in the vicinity. Implementing DTs in IoRT, creates
    a virtual replica [virtual twin (VT)] that receives real-time data from the physical
    RT [physical twin (PT)] to mirror its status. However, DTs require a user interface
    for real-time interaction and visualization. Virtual reality (VR) can be used
    as an interface due to its natural ability to visualize and interact with DTs.
    This research proposes a real-time system for RHM of COVID-19 patients using the
    DTs-based IoRT and VR-based user interface. It also presents and evaluates robot
    navigation performance, which is vital for remote monitoring. The VT operates
    the PT in the real environment (RE), which collects data from the patient-mounted
    sensors and transmits it to the control service to visualize in VR for medical
    examination. The system prevents direct interaction of medical staff with contaminated
    patients, protecting them from infection and stress. The experimental results
    verify the monitoring data quality (accuracy, completeness, and timeliness) and
    high accuracy of PT's navigation.
  doi: 10.1109/JIOT.2023.3267171
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Volume: 10 Issue: 18 Digital-Twins-Based Internet of Robotic Things for Remote
    Health Monitoring of COVID-19 Patients Publisher: IEEE Cite This PDF Sangeen Khan;
    Sehat Ullah; Habib Ullah Khan; Inam Ur Rehman All Authors 3 Cites in Papers 1617
    Full Text Views Open Access Under a Creative Commons License Abstract Document
    Sections I. Introduction II. Related Work III. Proposed System IV. Experimental
    Setup V. Performance Evaluation Show Full Outline Authors Figures References Citations
    Keywords Metrics Abstract: The deadly coronavirus disease (COVID-19) has highlighted
    the importance of remote health monitoring (RHM). The digital-twins (DTs) paradigm
    enables RHM by creating a virtual replica that receives data from the physical
    asset, representing its real-world behavior. However, DTs use passive Internet
    of Things (IoT) sensors, which limit their potential to a specific location or
    entity. This problem can be addressed by using the Internet of Robotic Things
    (IoRT), which combines robotics and IoT, allowing the robotic things (RTs) to
    navigate in a particular environment and connect to IoT devices in the vicinity.
    Implementing DTs in IoRT, creates a virtual replica [virtual twin (VT)] that receives
    real-time data from the physical RT [physical twin (PT)] to mirror its status.
    However, DTs require a user interface for real-time interaction and visualization.
    Virtual reality (VR) can be used as an interface due to its natural ability to
    visualize and interact with DTs. This research proposes a real-time system for
    RHM of COVID-19 patients using the DTs-based IoRT and VR-based user interface.
    It also presents and evaluates robot navigation performance, which is vital for
    remote monitoring. The VT operates the PT in the real environment (RE), which
    collects data from the patient-mounted sensors and transmits it to the control
    service to visualize in VR for medical examination. The system prevents direct
    interaction of medical staff with contaminated patients, protecting them from
    infection and stress. The experimental results verify the monitoring data quality
    (accuracy, completeness, and timeliness) and high accuracy of PT’s navigation.
    Published in: IEEE Internet of Things Journal ( Volume: 10, Issue: 18, 15 September
    2023) Page(s): 16087 - 16098 Date of Publication: 14 April 2023 ISSN Information:
    DOI: 10.1109/JIOT.2023.3267171 Publisher: IEEE Funding Agency: CCBY - IEEE is
    not the copyright holder of this material. Please follow the instructions via
    https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and
    stipulations in the API documentation. SECTION I. Introduction The digital-twins
    (DTs) paradigm creates a virtual-physical system that allows bidirectional data
    exchange between a physical entity (device, equipment, object, or human) and its
    virtual counterpart using a communication network [1], [2], [3], [4]. More specifically,
    a DTs system consists of three major elements: the physical artifact, its virtual
    replication, and the bidirectional communication link between them [5]. DTs can
    link the virtual and physical spaces in real time, allowing more accurate and
    realistic measuring of unpredictable events [6]. A high-level mapping connects
    physical and virtual entities to convert the behavior of real objects into virtual
    objects [7], [8]. The real-time data exchange between the DTs allows continuous
    monitoring of the physical artifact [9]. Many researchers suggest that DT is a
    virtual model updated continuously to depict the behavior of a physical entity.
    However, DT is not just a virtual replica, it is a paradigm that incorporates
    multiple high-tech fields and executes various advanced technologies [10]. DTs
    have been implemented in different domains, including industrial production, building
    smart cities, aerospace, immersive shopping, and healthcare [11]. Over the preceding
    decade, DTs technology has transformed the healthcare industry, resulting in more
    intelligent, personalized, and realistic healthcare solutions [12]. Among the
    key solutions are simulation of hospitals’ physical spaces, modeling of organizational
    processes, and virtualization of clinical processes or individuals’ genetic/physiological/lifestyle
    characteristics [13]. The recent coronavirus disease (COVID-19) pandemic has increased
    the importance of DTs. DTs systems predict and manage contagious disease outbreaks
    more efficiently and improve hospital management and healthcare services [14].
    Unlike earlier pandemics like middle east respiratory syndrome (MERS) and severe
    acute respiratory syndrome (SARS), COVID-19 has a high and asymptomatic transmission
    capability. Consequently, its treatment becomes quite challenging for the medical
    staff [15]. Healthcare workers are highly exposed to this infection because they
    interact daily with COVID-19 patients to measure vital body signs, such as body
    temperature, blood oxygen level, heart rate, and respiratory rate, to monitor
    patient health status [16], [17], [18]. This problem has increased the importance
    of remote health monitoring (RHM), which collects and transmits patient health
    information to a remote server for medical examination [19], [20]. DTs infrastructure
    is a better choice for RHM during the COVID-19 outbreak [21]. DTs can enable medical
    professionals to monitor and treat patients remotely by providing real-time health-related
    information [22]. They exploit various technologies, including artificial intelligence
    (AI), blockchain, cloud computing, and the Internet of Things (IoT) [23], [24],
    [25], [26]. IoT is considered a building block for DTs as it employs different
    sensors to gather information about patients’ health and transmit it to the virtual
    counterpart for analysis [27], [28]. The real-time data analysis allows RHM, the
    treatment of infected patients, and the prediction of epidemic evolution [29],
    [30]. However, most IoT systems are based on devices with passive sensors that
    monitor and organize various systems and their functionalities. Although the frameworks
    are effective, more transformational and advanced aspects of IoT solutions must
    be explored for the ubiquitous connectivity and communication among intelligent
    devices to further enhance these initiatives [31]. Therefore, the DTs lay their
    foundation on using the Internet of Robotic Things (IoRT) [32] for RHM of infected
    patients. IoRT is a new field that combines IoT and robotics [33]. Unlike IoT
    devices, robotic things (RTs) are dynamic and can operate freely in real-world
    scenarios [34]. Robotic technologies can be applied in regions with a high risk
    of infection, such as hospitals, public places like parks, shopping malls, and
    markets during this epidemic situation and in the future [35]. They can identify
    IoT devices in their surroundings and obtain real-time information to facilitate
    the medical workers, avoiding unnecessary intervention or supporting healthcare
    activities in crucial situations [36]. Robotic devices can also help supply foodstuff
    [37], deliver medicines, and remove dirty laundry and waste within the hospital
    [38]. When implemented in the IoRT, the DTs paradigm creates a virtual model of
    the physical robot that obtains real-time information from the physical robotic
    device during its lifespan and acts like a real robot. The physical robot is known
    as a physical twin (PT), whereas the virtual robot is referred to as a virtual
    twin (VT) [32]. The PT communicates with the VT and with other RTs and IoT devices
    in the real environment (RE). Although a DTs system provides the best possible
    performance for a specific application, it is unlikely to be fully autonomous.
    Human intervention is often required to operate the system or redesign a specific
    process. Accordingly, an intuitive interface is desirable to enable real-time
    interaction between humans and DTs. Virtual reality (VR) has the natural capabilities
    of visualizing and interacting with the DTs [39]. VR interface allows the users
    to observe the VT that uses the received information to mirror the real-world
    status of the PT [40]. The three-dimensional (3-D) virtual representation of the
    PT and its surroundings provides the operator with a live virtual view of the
    RE and physical entity [41]. This research proposes an RHM system for COVID-19
    patients using DTs-based IoRT and VR. The system obtains real-time health-related
    data from patient-mounted biomedical sensors using the PT and transmits it to
    the control center for medical examination. The proposed system replaces the traditional
    medical care systems, where passive IoT devices or medical personnel who physically
    interact with isolated patients perform patient care and monitoring. This article
    also provides a detailed analysis of the PT’s navigation. The VR-based interface
    visualizes the PT and its surrounding environment, providing the operator with
    better perception and real-time user interaction. The interface also displays
    health monitoring data and PT’s navigation information, such as distance traveled,
    obstacles detection, and obstacles’ distance from the PT. The proposed system
    uses the DTs architecture presented in [42] that describes the functionalities
    of each component of a DTs system. The model comprises five entities. The Physical
    entities comprise the real assets in the physical environment and their interactions.
    The Virtual entities comprise the virtual models that receive real-time data from
    physical entities. The DTs data includes data obtained from the real world through
    sensors. The Services are the applications of the DTs, such as simulation, real-time
    monitoring, navigation, diagnosis, and prognosis. The Connections are the links
    among the components of the system. A. Motivation Due to the rapid spread of the
    COVID-19 pandemic, the existing healthcare systems are overburdened. The situation
    resulted in various complications, including inadequate patient health status
    monitoring, human errors in health parameters, and unavailability of medical staff
    in an emergency. Most healthcare professionals endure long working shifts and
    deal with the common hazard of getting infected, which results in mental and physical
    stress. As medical professionals are the backbone of every nation, this research
    aims to protect medical professionals by exploring and using advanced technologies
    in medical systems. Monitoring contagious patients remotely without having a direct
    physical connection can help the caretakers to avoid infection and mental stress.
    B. Contribution This research presents a real-time framework for remotely monitoring
    COVID-19 patients using DTs-based IoRT and VR. The main contributions of the study
    are as follows. In general, DTs systems use passive IoT sensors to update the
    status of the virtual counterpart, which restricts their applications to specific
    sites or entities. We implemented the DTs paradigm in the IoRT to uncover the
    dynamic aspect of the existing systems. Our physical entity (PT) allows bi-directional
    communication with the virtual model (VT) and communicates with other PTs and
    IoT devices while moving around in the RE. We developed an RHM system that can
    monitor contagious patients in real time without direct physical contact of the
    medical workers with the patients, protecting the health carers from infection.
    We employed a VR-based user interface that provides a better perception of the
    PT’s surroundings and real-time user interaction with the DTs. We presented and
    evaluated a robot navigation technique that uses DTs and a VR-based user interface,
    our method is not constrained by the limitations of camera-based navigation techniques,
    such as restricted field-of-view, low-quality video feedback, communication delay,
    two-dimensional (2-D) prospect, poor performance in low lighting, visual data
    transmission, and processing load [43], [44], [45], [46]. We developed a real-time
    mechanism to detect and visualize stationary and moving obstacles and their distances
    from the PT, allowing situational awareness of the RE. Our system functions off-line
    using the radio transceiver (NR24L01+) and Bluetooth modules, eliminating the
    limitations posed by Internet cloud-based communication, including increased energy
    consumption, high communication latency, and security issues [47], [48]. The remaining
    part of this article is organized as follows: Section II discusses the related
    work, Section III describes the proposed system, Section IV discusses the experimental
    setup, Section V describes performance evaluation, Section VI presents the discussion
    of the results, and Section VII describes the conclusion and future directions.
    SECTION II. Related Work Because of the increasing number of medical complications
    and emerging infectious outbreaks, the importance of digital technologies has
    grown significantly. The DTs paradigm employs various computing, communication,
    and visualization technologies, such as AI, cloud computing, blockchain, IoT,
    IoRT, and VR. Therefore, it is considered a significant breakthrough for improving
    industrial, engineering, and medical infrastructures and processes. Researchers
    have implemented DTs in different fields to improve living standards. De Benedictis
    et al. [13] provided a detailed review of the role of DTs in healthcare and proposed
    a generalized DTs architecture for identifying the essential functional components
    of a DTs system. They also presented CanTwin, the DT of canteen service, designed
    to monitor social distancing, queue status, table occupancy, and worker counting
    and tracking during the COVID-19 pandemic. The proposed architecture is based
    on six layers. The Physical layer consists of sensors that transmit the collected
    data to an aggregator. The Data layer comprises the data obtained from the aggregator.
    The Connectivity layer allows both the physical and virtual worlds to exchange
    information. The DTs layer includes the geometric model of the canteen enriched
    with points representing the number of workers, their respective distances, and
    the number of served workers. The Service layer consists of real-time monitoring,
    social distancing, table occupancy, and served workers. The Security layer provides
    authentication and authorization mechanism to facilitate device identity and access
    management. Bondoc et al. [49] proposed LIVE DT, a model-based solution to create
    DTs through sensor data for asset management. LIVE consists of four basic phases:
    1) Learn; 2) Identify; 3) Verify; and 4) Extend. It relies on the proper allocation
    and placement of various sensors on the physical entities to allow bi-directional
    communication with the simulated model. The designed DT is used for prognostic
    and health management of the Light Rail Transit system. The data collected from
    the physical structure is based on acceleration and vibration. The phases of the
    LIVE model are defined as follows. The Learn phase gathers basic knowledge and
    information about the physical object to create a virtual model. The Identify
    phase generates a low fidelity (LF) model, which is further optimized with the
    high fidelity (HF) model using the parameters to identify sensors locations. The
    Verify phase synchronizes the physical and virtual assets for real-time communication
    to analyze the physical assets’ health and allow fault detection based on the
    LF model. The Extend phase uses the HF model to suggest plans for eliminating
    the faults diagnosed during the previous stage. Dang et al. [50] introduced a
    DTs-based system using deep learning and cloud computing to monitor the structure’s
    health and proactive maintenance. The system uses cloud computing and a Web-based
    interface to enable data interaction among the users, virtual model, and physical
    structure. The framework is validated for a toy bridge in the lab and a real bridge
    structure. The system model has four components. Physical Structure: This component
    includes structural entities, auxiliary elements, external excitation, and measurement
    devices. Virtual Structure: It represents the virtual replica mirroring the physical
    structure’s status using sensor data. Cloud Computing: This element includes hybrid
    cloud computing (private fog and public cloud). First, the private fog performs
    data preprocessing, such as data cleaning and training of the virtual model. After
    that, the processed data and model are deployed to the public cloud. User Application:
    It consists of a Web-based interface to allow human–computer interaction and real-time
    monitoring and controlling. The application presents structural data (stress,
    vibration, and temperature) as graphical carts. Yu et al. [51] presented a hybrid
    monitoring system for the health of the cable-stayed bridge. The system employs
    an orthotropic steel deck fatigue-assessment method appropriate for ambient temperature
    actions, traffic loads, and welding residual stress. The DTs concept is used to
    understand the stress of fatigue-vulnerable deck areas during heavy traffic flow,
    and the submodel is implemented for a long-span bridge. The method allows data
    to flow from the monitoring unit to the numerical model. The deck is fixed in
    the middle lane of the bridge to measure the stress. Of the additional advances,
    AI, IoT, and IoRT have various advantages, including potential computing, reduced
    human errors, cost efficiency, unbiased decisions, zero errors, and remote access
    via a communication network, enabling RHM, and treatment of patients. Many researchers
    have employed these technologies for RHM, patient assistance, and medicine delivery
    to minimize the direct exposure of medical staff to contaminated patients. Wang
    et al. [52] presented an AI-based robotic system that can deliver parcels to customers
    autonomously. The robot can also deliver medicine to COVID-19 patients, reducing
    human contact with infected patients and freeing up valuable time for medical
    staff. The system employs a user authentication mechanism based on PIN code and
    biometrics verification (voice print and face recognition) to ensure safe delivery
    to the correct customers. When the customer places an order, the server receives
    the request and provides verification data to both the robot and the client. The
    robot loads the information and navigates to the target position. On reaching
    the destination, the customer is asked for cooperative authentication (PIN code).
    If the PIN code is provided, and one of the biometrics is verified, the parcel
    is handed over. If the PIN is incorrect, the robot switches to the noncooperative
    mode, i.e., facial identification. However, face recognition is one of the vital
    identification elements. Therefore, the system inherits the limitations of visual
    computing. Hamim et al. [53] proposed an IoT-based remote patient health monitoring
    system. The health information acquired by the biomedical sensors is transferred
    to cloud storage. An Android application is developed to access the cloud and
    show a graphical representation of the health parameters. However, the system
    uses passive IoT sensors and cloud services, thus inheriting their limitations.
    Akhund et al. [54] proposed an IoT-based robot for collecting patients’ health-related
    data and helping disabled or infected patients. The robot recognizes hand gestures
    measured with the Accelerometer/Gyroscope sensor (MPU-6050) rather than using
    the image processing techniques. The system utilizes a 433-kHz radio transceiver
    for data communication. However, the system functions merely when the robot is
    in a visual line of sight. There is no user interface for operating the robot
    beyond the visual range. Also, the system’s accuracy is measured only for gesture
    recognition; there is no actual implementation and evaluation of the health monitoring
    mechanism. Leila et al. [55] proposed an IoRT-based health monitoring system to
    combat the COVID-19 pandemic. The system uses a physical robotic device that moves
    in the hospital’s corridor, collects clinical parameters from the medical sensors
    attached to the patient, and transmits them to the Internet cloud through Wi-Fi.
    The health service obtains the information from the cloud using the Internet.
    The body sensors collect clinical data and send it to an aggregator device using
    the ZigBee module. The aggregator transmits the aggregate data to the room controller
    (RC) using another ZigBee module. The RC receives information from multiple aggregators.
    Communication between the RC and the RT is performed by using the Bluetooth technology.
    However, the system uses cloud services to communicate clinical information from
    RT to the medical service, thus inheriting the demerits of cloud computing. Furthermore,
    there is no clear description regarding the interface for controlling or navigating
    the RT in the corridor. Also, the clinical data passes through various gateways
    to reach the health service; as a result, the system suffers from latency issues.
    Bhardwaj et al. [56] developed a smart health monitoring system based on the IoT
    technology. The system can distantly monitor the health parameters (body temperature,
    heart rate, blood pressure, oxygen level) of COVID-19 patients. The collected
    information is displayed on a monitor and transmitted to a cloud server using
    Wi-Fi for monitoring by doctors or physicians. Medical service receives an alert
    message if the system detects abnormal values. However, the system is based on
    passive IoT sensors and cloud services, thus inheriting their limitations. Ruman
    et al. [57] proposed a remote patient monitoring system using the IoT technology.
    The health-related parameters (blood pressure, ECG, and body temperature) are
    collected via sensors and transmitted to the cloud via Wi-Fi. The stored data
    is retrieved by authorized personnel or doctors for monitoring using an Android
    application. However, the system depends on cloud service, consequently, it suffers
    from latency issues. El-Rashidy et al. [58] introduced a real-time system for
    remote monitoring of COVID-19 patients in hospitals and homes. The system uses
    wireless sensors, cloud and fog computation, and deep learning-based clinical
    decision support to create comprehensive disease detection and monitoring model.
    A mobile application is developed to retrieve the stored data from the cloud for
    patient monitoring. However, the system consumes high power during sensing and
    transmission. Introducing fog nodes to the network infrastructure may cause complexity.
    Maintenance of the distributed fog storage nodes is another complex problem. Various
    architectures have been suggested in the literature to create DTs for different
    applications. A thorough analysis leads to the common conclusion that DTs frameworks
    comprise physical and virtual elements connected through sensor data for a particular
    application. Our proposed method uses the same standard architectural components,
    i.e., physical entities, virtual entities, DTs data, services, and connectivity.
    However, it differs from the existing DTs systems that use passive IoT sensors
    and require a direct connection between the physical and virtual assets for updating
    the status. The proposed system employs IoRT to create a dynamic framework whose
    applications are not restricted to a specific entity or location. Furthermore,
    it does not require every physical asset to be directly synchronized with a virtual
    entity. The PT, which has a virtual replica, collects data from different real-world
    assets and transmits it to the virtual space to display or synchronize with a
    simulated model. It acts as an intermediary between the virtual space and other
    assets in the physical world. The PT can be easily upgraded to perform additional
    functions in real-world scenarios, such as diagnosing and maintaining faults in
    physical assets and their communication links. COVID-19 has put the lives of doctors
    and other medical personnel in extreme danger because they have daily physical
    interactions with the infected patients to collect vital body signs (heart rate,
    oxygen level, body temperature). This research proposes a real-time system for
    remotely monitoring infected patients without physical contact to protect healthcare
    workers from infection and stress. The proposed system employs VR and DTs-based
    IoRT for RHM of COVID-19 patients. The VR interface visualizes the PT and its
    surroundings and allows real-time interaction with the DTs, lacking in previous
    studies. Unlike the existing IoT and IoRT systems, the proposed method does not
    depend on Internet clouds or vision-based interfaces, thus preventing inheriting
    their limitations. SECTION III. Proposed System The proposed approach describes
    a real-time RHM system that uses DTs-based IoRT and VR. The framework uses the
    PT that moves around in the RE and collects health data from IoT sensors attached
    to the patient’s body. The data collected by the system is then transmitted to
    the health service for medical evaluation, thereby eliminating direct contact
    between medical professionals and COVID-19 patients. This not only protects medical
    professionals from infection but also relieves them of physical and mental stress.
    Additionally, this study provides a comprehensive evaluation of PT’s navigation.
    It includes a real-time obstacle detection and visualization mechanism that provides
    situational awareness during navigation. The VR interface developed in this study
    visualizes the PT and its surroundings, providing a better perception of the physical
    environment and real-time user interaction. The proposed system consists of five
    components, as shown in Fig. 1. Fig. 1. Architecture of the proposed system. Show
    All A. Physical Entities Simulating a physical entity requires knowledge of the
    physical asset and access to real-time data. Sensors, actuators, and controllers
    handle data collection, processing, and control. Sensors perceive events in the
    physical environment, actuators alter the physical world, and controllers analyze
    data and allow the data to flow between devices. The proposed system comprises
    two physical entities, i.e., PT and an RHM unit. The PT includes various sensors,
    including a speed sensor (LM393) for measuring the rotational speed of the wheel
    to locate the PT in the RE, an Accelerometer/Gyroscope (MPU-6050) to calculate
    the orientation of the PT in real space, ultrasonic distance sensor (HC-SR04)
    to determine the obstruction’s distance from the PT in the physical environment.
    It includes a microcontroller device (Arduino Uno) for analyzing the sensor data.
    It also comprises actuators (wheel motors) to navigate the PT to the target position.
    It can be easily upgraded for remote operations by introducing a mechanical hand,
    sanitizing spray pump, or lifting platform for medicine delivery. The main components
    of the RHM unit consist of biomedical sensors, i.e., a temperature probe (DS 18B20)
    for monitoring the patient’s body temperature, a pulse oximeter, and a heart rate
    sensor (MAX30100) to compute oxygen level in blood and heart rate of the patient.
    The RHM unit also comprises a microcontroller device allowing sensor connectivity
    and data analysis. The physical entities include communication devices (NRF24L01+,
    HC-05) that will be discussed in the Connectivity phase. B. DTs Data This component
    includes the parameters collected by physical entities in the RE and transmitted
    to the virtual space to reflect the changes in the physical asset. The data also
    comprises the responses generated by the virtual entities to alter the physical
    assets. Our system’s data represent the parameters (position, orientation, and
    detected obstacles) collected by the PT’s sensors from the physical space to provide
    situational awareness during navigation. It also includes the information (blood
    oxygen level, heart rate, and body temperature) obtained by the PT from various
    RHM unit sensors and the commands the end-user issued to navigate and control
    the PT in the RE. C. Virtual Entities The virtual replica is expected to reflect
    the physical object’s behavior accurately. Therefore, it continuously receives
    and integrates sensor data from the physical space for real-time mapping and control.
    The virtual entities comprise the virtual model of the tangible assets and the
    engine that makes data communication and synchronization possible. It also includes
    a user interface that allows visualization of the virtual model and real-time
    interaction with the DTs. Our virtual space consists of a simulated model (VT)
    of the PT that feeds on real-world information from different sensors, including
    a speed sensor, ultrasonic distance sensor, and Accelerometer/Gyroscope. Based
    on the obtained data, the VT depicts the PT’s status (position, orientation, and
    detected obstacles) for the navigation task. The virtual entities also comprise
    a VR-based user interface that simulates the PT and its real-world scenario, allowing
    intuitive interaction with the DTs. The interface is used to visualize the detected
    obstacles in the path of PT, along with their distance parameters, based on the
    collected data from the ultrasonic distance sensors. Another purpose of the interface
    is to exhibit the patient health data (blood oxygen level, heart rate, and body
    temperature) collected by the PT from the biomedical sensors of the RHM Unit.
    The doctors and physicians can use the information to examine the health status
    of the infected patients. D. Connectivity This part enables the exchange of information
    between physical and virtual entities in real time. According to [59], communication
    between the DTs is based on the following two criteria. 1) Intratwins Connectivity:
    It is the link between the physical objects that use sensors to collect real-time
    data regarding the physical environment and the virtual model that employs the
    gathered data to mirror the status of the actual asset. 2) Intertwins Connectivity:
    It enables communication between different tangible entities in the real world.
    To allow real-time data transmission actual communication network is required.
    In the proposed system, we used the NRF24L01+ communication modules for intratwins
    connection, avoiding Internet-based communication to eliminate the disadvantages
    of online methods. The modules enable the PT and VT to exchange information about
    PT’s navigation and real-time patient monitoring. We used the Bluetooth modules
    (HC-05) for intertwins communication to transmit patient monitoring data to PT,
    which then sends it to the virtual space using the intratwins connection. E. Services
    Services are the real applications of the DTs systems. Tao et al. [60] identified
    18 services of DTs. The major applications include simulation, remote monitoring,
    navigation, training, and tele-operation. We used DTs-based IoRT and VR to create
    a real-time system that monitors the health of COVID-19 patients. The method avoids
    direct interaction between healthcare professionals and infected patients, protecting
    them from infection. We presented and analyzed the navigation of PT by operating
    its VT for collecting patient health data remotely. Our proposed system can be
    easily upgraded for tele-operation service by mounting a mechanical arm, or disinfecting
    unit. F. Implementation The proposed system uses DTs-based IoRT and VR for RHM
    of the infected patients. Robot navigation is the first step of the health monitoring
    process because it allows the PT to access the monitoring unit and collect patient
    data for medical inspection. Therefore, we developed a robot navigation system
    that used VT to navigate the PT in the RE and provided a detailed analysis to
    validate the design. 1) PT’s Navigation Task: The navigation system utilizes a
    user interface based on the VT and VR technology to navigate the PT in physical
    space, rather than relying on traditional camera-based navigation techniques.
    This approach enables real-time user interaction and provides a more accurate
    perception of the surrounding environment. Additionally, the system incorporates
    a real-time obstacle detection and visualization mechanism to enhance the user’s
    situational awareness. All data processing and virtual rendering are carried out
    by a laptop computer. Unlike other navigation systems, the proposed system does
    not rely on additional devices or sensors to locate the PT in the physical environment.
    Instead, the VT accurately reflects the PT’s real-world status, as illustrated
    in Fig. 2. To simulate the physical robot and its surroundings, we utilized the
    Unity-3-D game engine. Mapping between the DTs enabled the PT’s navigation. The
    proposed framework is independent of the Internet data transmission and control
    service. It employs the NRF24L01+ communication modules to allow data exchange
    between the DTs. We mounted three ultrasonic sensors (HC-SR04) on the robot’s
    left, right, and front sides to detect static or moving obstacles. When the PT
    detects a stationary obstruction with less than 1 m from it, the system displays
    a solid cube in the VR interface at the same distance from the VT. The method
    visualizes a moving obstacle if the distance is less than 2 m. As the moving obstruction
    approaches the physical robot in the RE, the system updates its position in virtual
    space. The VR interface displays the obstacle distance and labels (front, left,
    and right) to provide additional information about the robot’s navigation. The
    Accelerometer/Gyroscope sensor (MPU-6050) determines the direction of physical
    RT. The virtual robot visually represents any change in the PT’s direction, while
    the speed sensor calculates the distance traveled by the real robot. To provide
    the operator with additional information, the VE displays this data. The arrow
    keys on the laptop keyboard are used to navigate the PT rather than any special
    and expensive interaction devices. The up and down keys move the robots forward
    and backward, respectively. Right and left keys control left and right turns,
    respectively. The abstract view of the PT’s navigation system is shown in Fig.
    3, and the data flow between the components is shown in Fig. 4. Fig. 2. (a) VT
    in the VR. (b) PT in the RE. Show All Fig. 3. Graphical abstract of the navigation
    system. Show All Fig. 4. Data flow between the components of navigation system.
    Show All 2) RHM Task: The primary purpose of the research is to present a real-time
    system that monitors the health of isolated patients with COVID-19. Doctors and
    other medical personnel are at high risk of infection because they directly contact
    the contaminated patients. The proposed method uses the PT as a nurse robot that
    navigates freely in the corridor for data collection. When the PT arrives at the
    patient isolation room (Destination), it connects to the medical sensors (DS-18B20
    and MAX30100) attached to the patient’s body using the Bluetooth module (HC-05).
    It collects health-related data, which is sent to the control center for visualization
    in VR. The medical staff can easily control the PT, examine clinical information,
    and take necessary action. The abstract view of the RHM system is shown in Fig.
    5. The data flow between the components of the RHM system is illustrated in Fig.
    6. Fig. 5. Graphical view of the RHM system. Show All Fig. 6. Data flow between
    the components of the proposed RHM system. Show All SECTION IV. Experimental Setup
    We installed the experimental setup in research lab 1 (control room) of the Department
    of CS & IT, University of Malakand. The PT was placed in the corridor outside
    the control room. To the right of the corridor are various offices, and stairs
    and grills to the left, as shown in Fig. 7. A laptop PC was used to implement
    the virtual setup. The PT and its surroundings were rendered in VR using the Unity
    3-D game engine. We performed several trials that perfectly synchronized DTs for
    the designed settings. The medical sensors were installed. The objective was to
    use the VT to navigate the PT to various points, collect real-time health data
    from medical sensors, and transmit it to the control center for visualization
    in VR. We created two setups to assess the quality of the monitoring data. Fig.
    7. Scenario of the experimental environment. Show All A. Setup 1 The medical sensors
    were installed in office 4 (Destination 1) at 19 m from the start. B. Setup 2
    The sensors were mounted in Lecture room 1 (Destination 2) at 22 m from the start.
    SECTION V. Performance Evaluation A. Experimental Protocol and Task 1) PT’s Navigation:
    To evaluate the proposed system’s performance, we conducted a user study involving
    ten volunteers. The study collected both quantitative data and qualitative feedback
    on the system’s accuracy and usability. All users were male, and their ages ranged
    between 25 and 35. They had already experienced computer gaming using keyboard,
    mouse, and touch screens, but no one was familiar with DTs and VR. We instructed
    all the participants about the functionality and use of the proposed system. After
    that, they performed the tasks. Each of the ten participants completed six tasks,
    resulting in a total of 60 tasks. To evaluate the system’s obstacle-detection
    performance, we placed plant pots as stationary obstructions at various positions
    in the corridor. Additionally, we used a cardboard box as a moving obstacle, which
    was dragged toward the PT at roughly the same speed as the PT. The experimentation
    comprised the following tasks. Task 1:To follow path 1 from start to Destination
    1 (see Fig. 7) with no obstacles in the way. Task 2:To follow path 2 from start
    to Destination 2 (see Fig. 7) with no obstacles in the way. Task 3:To follow path
    1 from start to Destination 1 with one stationary obstacle placed at point 4 (P4).
    Task 4:To follow path 2 from start to Destination 2 with one static obstacle placed
    at point 6 (P6). Task 5:To follow path 1 from the start to Destination 1 with
    3 stationary obstacles (as seen in Fig. 8) placed in a zigzag. The distance between
    each obstacle was set to 1 m. Task 6:To follow path 1 from the start to Destination
    1 with moving obstacles at two different locations: when the PT reaches point
    2 (P2), the obstacle is moved from point 3 (P3), and when the PT approaches point
    4 (P4); the obstacle is moved from point 5 (P5). Fig. 8. Real-time detection and
    visualization of obstacles during navigation. Show All 2) RHM: We divided the
    experimentation into two tasks to evaluate RHM performance. Each participant performed
    the tasks that resulted in a total number of 20 trials. Task 1:Navigating the
    PT to Destination 1 by following Path 1 (with no obstacles) and acquiring data
    from the medical sensors for transmitting to the control station. Task 2:Navigating
    the PT to Destination 2 by following Path 2 (with no obstacles) and acquiring
    data from the medical sensors for transmitting to the control station. B. Results
    Analysis We evaluated the performance of the proposed system using objective and
    subjective metrics. The time factor has a significant impact on the robot’s navigation
    performance. Also, errors may occur over the distance traveled by the PT because
    of various factors, such as wheel drift, improper turns, and low power supply
    to motors. Therefore, the task completion time and errors in the traveled distance
    were used to calculate the accuracy of PT’s navigation. The quality of monitoring
    data was assessed using the three most commonly used data-quality (DQ) dimensions
    (accuracy, completeness, and timeliness) described in [61]. 1) Objective Analysis
    of Navigation Task: The PT’s navigation accuracy is measured using the difference
    between the total distance and the traveled distance (i.e., error) and the time
    for each task. Table I shows the percent accuracies and mean time for each task.
    TABLE I Accuracy and Time for PT’s Navigation We used analysis of variance (ANOVA)
    [62], which determines the statistical difference among groups. ANOVA test calculates
    the F ratio (or simply F ), which refers to the ratio of how much variance exists
    among groups compared to variability within groups. If the null hypothesis is
    true, there is no difference between groups, and the ratio is close to the value
    of 1. The larger value of F shows a more significant difference among the groups.
    The groups are treated as equal if the alpha level ( p -value) is greater than
    0.05, showing that the difference between the averages of all groups is not statistically
    significant. However, the difference is significant if the p -value is less than
    0.05. The degrees of freedom (df), i.e., number of participants minus 1, are included
    in the test’s outcome, written after F in Parentheses. The ANOVA test results
    in Table II show a significant variation F (5, 54) = 22.36, p= 0.000, and F (5,
    54) = 36.50, and p= 0.000 among the Means of time and errors, respectively. TABLE
    II ANOVA Test Results The Mean and standard deviation (SD) of time and errors
    are listed in Table III and shown in Figs. 9 and 10, respectively. TABLE III Mean
    and SD of Tasks Completion Time (Sec) and Errors in the Traveled Distance (cm)
    Fig. 9. Mean and SD of task completion time. Show All Fig. 10. Mean and SD of
    errors. Show All 2) Objective Analysis of Monitoring Data Quality: The DQ dimensions
    ensure that the acquired data is consistent with the actual sensors’ readings
    (i.e., accuracy), no expected data is lost (i.e., completeness), and the acquired
    data is up-to-date (i.e., timeliness). The experimentation was performed by obtaining
    the monitoring data for a time slot of 10 s. To assess the system’s overall performance
    in RHM, the DQ parameters for Tasks 1 and 2 were calculated and averaged. The
    accuracy and completeness of the health monitoring data are calculated using the
    equations described in [63] Accuracy=1−( r e r ) (1) View Source where r e is
    the number of erroneous data records and r is the total number of acquired data
    records. The data units’ completeness is measured using the following equation:
    Completeness=1−( r c r ) (2) View Source where r c is the number of not-null records
    and r is the total number of received records. The monitoring data’s timeliness
    is computed by using the equation described in [64] Timeliness=1−( r o r ) (3)
    View Source where r o is the number of data records obtained within a defined
    time slot and r is the total number of records in the same time slot. The comparative
    analysis of DQ dimensions for the existing and proposed systems is given in Table
    IV. TABLE IV Comparative Analysis of the Existing and Proposed RHM Systems: The
    DQ Dimensions are Evaluated Using Temperature Sensor Values 3) Subjective Analysis:
    The system’s usability was measured using the system usability scale (SUS) [65].
    The SUS includes ten items; each has a 5-point response ranging from strongly
    disagree to strongly agree. The odd-numbered items (1, 3, 5, 7, and 9) have values
    from strongly disagree = 0 to strongly agree = 4, whereas even-numbered items
    (2, 4, 6, 8, and 10) have values from strongly agree = 0 to strongly disagree
    = 4. The score contribution for items with odd numbers is the scale position minus
    1. The score contribution for items with even numbers is 5 minus the scale position.
    The SUS provides scores having a range of 0 to 100. To measure the SUS score,
    the values from each item are summed up and then multiplied by 2.5 to obtain the
    overall usability score. The results show a high usability score for the proposed
    system, as shown in Table V. TABLE V Results of SUS. The Average SUS Score Is
    38.3, and the Total Score Is 38.3×2.5= 95.75 SECTION VI. Results Discussion Generally,
    DT is regarded as a virtual replica representing a physical artifact. However,
    this idea is unclear and general, narrowing down the concept of DTs. In a broader
    sense, the DTs paradigm combines different state-of-the-art technologies, including
    IoT, IoRT, VR, AI, and cloud computing, to create a virtual model that mirrors
    the real-world status of a physical artifact through real-time data exchange.
    DTs have their specific applications in different fields of life. However, depending
    on the application, their architecture remains the same with a slight component
    difference. The primary standard components of a DTs system are physical entities,
    virtual entities, DTs data, connectivity, and services. De Benedictis et al. [13]
    suggested an additional security layer to improve the safety and performance of
    the DTs frameworks. Our system employs the five-layer architecture proposed in
    [42] because it operates offline without relying on insecure public Internet service,
    preventing the need for an online security layer. The existing systems enable
    the physical asset to update the virtual models by sending sensor data straight
    to the latter. There is a one-to-one connection between virtual and physical entities.
    Our proposed DTs-based IoRT allows the PT to receive data from other real-world
    devices and transmit it to the virtual space for visualization or synchronization
    with another virtual model. The PT bridges the virtual world and the other physical
    entities in the real world. We displayed RHM data (body temperature, blood oxygen
    level, and heart rate) in VR rather than embedding it with a virtual model because
    it contains numerical parameters that medical staff can easily understand. Although
    many systems have been developed for remote patient monitoring, the majority of
    them lack experimental details. Hamim et al. [53] explored the functionality of
    sensors by altering environmental or behavioral factors to observe how each sensor
    responds to these changes. However, none of the DQ dimensions is used to validate
    the quality of health monitoring data. In [54], gesture recognition accuracy is
    calculated, but the DQ dimensions are not verified. The system proposed in [55]
    is not tested for DQ dimensions. In [56], the sensors’ accuracy is verified with
    commercially available medical devices, but no information about the completeness
    and timeliness of health data is provided. The health monitoring system proposed
    in [57] has analyzed the accuracy of sensors; however, completeness and timeliness
    are not measured. Similarly, the system presented in [58] is analyzed for the
    accuracy of monitoring data, but no details regarding the other two dimensions
    of DQ are given. Unlike the existing systems, the proposed system is subjected
    to detailed experimentation to evaluate the robot’s navigation performance. The
    ANOVA results rejected the null hypothesis, indicating significant differences
    among the Means of experimentation groups. The comparative analysis verified that
    the proposed system is equally or more accurate than the existing systems for
    RHM. Furthermore, it outperforms all existing approaches because it is validated
    for data completeness and timeliness. SECTION VII. Conclusion and Future Directions
    This article presented a real-time system for remotely monitoring the health of
    isolated patients and navigating the robotic device in the RE using a VR-based
    user interface and DTs-based IoRT. A predefined virtual space is used to visualize
    the PT and its surrounding environment and to display health-related parameters.
    Implementing the DTs paradigm in IoRT allows real-time control of the physical
    RT. The PT works as a nurse robot, an intermediary between the patient and the
    health service collecting health information without physically contacting the
    infected patient. The obstacle detection and visualization mechanism responds
    in real time if an obstruction is repositioned or a new obstacle is introduced
    in the RE. The experimental results validate the proposed system’s high performance
    and usability. Future work will be focused on autonomously navigating the DTs
    to collect and transmit health information. Also, to implement AI for predicting
    future abnormalities based on the collected data. Additionally, more precise measures
    for the synchronization of DTs will be made to achieve the real advantage of the
    system over longer distances. ACKNOWLEDGMENT The authors would like to thank the
    users for participating in the study and providing valuable feedback. Authors
    Figures References Citations Keywords Metrics More Like This Wireless Sensor Network
    Based Navigation of Micro Flying Robots in the Industrial Internet of Things IEEE
    Transactions on Industrial Informatics Published: 2018 A Smart and Reliable Health
    Monitoring System for Covid-19 using Internet of Things 2023 International Conference
    on Innovative Computing, Intelligent Communication and Smart Electrical Systems
    (ICSES) Published: 2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Digital-Twins-Based Internet of Robotic Things for Remote Health Monitoring
    of COVID-19 Patients
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ma X.
  - Huang Y.
  - Li Q.
  - Huang B.
  - Liu X.
  - Liu J.
  citation_count: '0'
  description: Mobile crowdsensing (MCS), which relies on a large number of ordinary
    participants to complete sensing tasks, has become a new data collection paradigm
    in the Internet of Things (IoT). In the MCS system, how to select reliable participants
    to participate in the sensing task and provide appropriate rewards for participants
    is one of the main problems faced by MCS. This article proposes a reliable participant
    selection strategy driven by dynamic incentives. The whole participant selection
    process is divided into multiple stages. First, the platform calculates participants
    reliability by considering the task completion rate and task pass rate. Then,
    when the platform releases tasks at each stage, it considers the task deadline
    and the task completion progress to quantify the task requirements, and then determines
    the maximum price of the tasks at that stage. Finally, the platform selects participants
    who meet the requirements for different tasks at each stage and conducts bargaining
    games with participants to determine appropriate rewards. Considering the situation
    that participants quit midway and the task cannot be completed, the platform adopts
    a task delegation mechanism to ensure the effective completion of the task. The
    results show that the proposed strategy can guarantee the task completion rate
    and data quality, and optimize the reward of participants.
  doi: 10.1109/JIOT.2023.3266022
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things Journal
    >Volume: 10 Issue: 18 Dynamic Incentive for Reliable MCS Participant Selection
    Publisher: IEEE Cite This PDF Xinqiang Ma; Yi Huang; Qiang Li; Biao Huang; Xiaoye
    Liu; Jia Liu All Authors 1 Cites in Paper 306 Full Text Views Abstract Document
    Sections I. Introduction II. Related Work III. System Model IV. Participants Reliability
    Assessment V. Participant Selection Strategy Show Full Outline Authors Figures
    References Citations Keywords Metrics Abstract: Mobile crowdsensing (MCS), which
    relies on a large number of ordinary participants to complete sensing tasks, has
    become a new data collection paradigm in the Internet of Things (IoT). In the
    MCS system, how to select reliable participants to participate in the sensing
    task and provide appropriate rewards for participants is one of the main problems
    faced by MCS. This article proposes a reliable participant selection strategy
    driven by dynamic incentives. The whole participant selection process is divided
    into multiple stages. First, the platform calculates participants reliability
    by considering the task completion rate and task pass rate. Then, when the platform
    releases tasks at each stage, it considers the task deadline and the task completion
    progress to quantify the task requirements, and then determines the maximum price
    of the tasks at that stage. Finally, the platform selects participants who meet
    the requirements for different tasks at each stage and conducts bargaining games
    with participants to determine appropriate rewards. Considering the situation
    that participants quit midway and the task cannot be completed, the platform adopts
    a task delegation mechanism to ensure the effective completion of the task. The
    results show that the proposed strategy can guarantee the task completion rate
    and data quality, and optimize the reward of participants. Published in: IEEE
    Internet of Things Journal ( Volume: 10, Issue: 18, 15 September 2023) Page(s):
    15912 - 15922 Date of Publication: 10 April 2023 ISSN Information: DOI: 10.1109/JIOT.2023.3266022
    Publisher: IEEE Funding Agency: SECTION I. Introduction In recent years, with
    the rapid development and accelerated integration of the Internet of Things (IoT),
    big data, and artificial intelligence technologies, many smart mobile devices
    were embedded with rich sensors and wireless communication modules [1]. With abundant
    embedded sensors and efficient computing power, these smart mobile devices can
    provide essential data collection services within a city scale. Unlike the form
    of data collected by wireless sensor networks, this method of collecting data
    by relying on ordinary participants to carry intelligent sensing devices is called
    mobile crowdsensing (MCS) [2], [3]. Due to its low cost and high flexibility,
    MCS has become a new paradigm for data collection, which is widely used in environmental
    monitoring, epidemic prevention, urban management, etc. [4], [5], [6]. MCS requires
    recruiting a large number of ordinary participants to collect sensing data, and
    an important research topic is how to identify those who can provide high-quality
    sensing data and provide appropriate rewards for these participants [7], [8],
    [9]. On the one hand, different participants’ professional abilities and attitudes
    toward tasks are different, and there are great differences in the state of participants
    at different times, resulting in the uneven quality of data collected by participants
    [10], [11]. On the other hand, participating in sensing tasks incurs certain costs
    for participants, including time, effort, and resource consumption. The platform
    needs to provide incentives for the selected participants to compensate for their
    cost of participation [12], [13]. Suitable incentives can motivate participants
    to actively participate in sensing tasks, improve the quality of perception data,
    and optimize platform costs, thereby improving the efficiency of the participant
    selection process. In the process of selecting participants for MCS, two important
    issues that need to be addressed. First, most research works [14], [15], [16]
    ignore the evaluation of participant reliability and assume that all participants
    provide the same data quality. They only optimize the overall system performance
    from the platform’s perspective, satisfying objectives, such as task coverage
    and cost optimization. These studies assume that the number of participants is
    sufficient and that participant information is known in advance. Second, appropriate
    rewards must be provided to participants in the participant selection process
    [17]. On the one hand, there are significant differences in task completion for
    different tasks and stages, and task prices should be determined based on market
    supply and demand. On the other hand, there are conflicting interests between
    participants and the platform, and the platform should assign different rewards
    to each participant. However, existing works [18], [19], [20] often do not consider
    the need for differentiated task requirements and pricing, which results in incentive
    mechanisms that cannot provide appropriate rewards for participants. The following
    issues need to be focused on in the participant selection process of MCS. First,
    the sensing platform often has both quantity and quality requirements for tasks,
    which means that the platform needs to select a sufficient number of reliable
    participants to complete the tasks. By considering that the number of available
    participants at different times is dynamically changing, and participants can
    join the task at any time during the entire duration, this article proposes a
    reliable participant selection strategy driven by dynamic incentives. To select
    reliable participants, the platform quantifies participant reliability based on
    task completion rate and task pass rate. The participant selection process is
    divided into multiple stages, considering that the completion of tasks is different
    at different stages. The platform evaluates the maximum price of the task based
    on task requirements and then selects participants that satisfy the quality and
    quantity requirements of the task. Considering the conflict of interest between
    participants and the platform, a bargaining game is conducted between the platform
    and participants to calculate the optimal reward for participants. For the case
    of selected participants withdrawing during the task, the platform proposes a
    task delegation mechanism to promote effective task completion. The main contributions
    of this article are summarized as follows. We propose a participant reliability
    evaluation method with task true value prediction. This method first evaluates
    the true value of the participant’s historical task records through a true value
    prediction algorithm, calculates the participant’s task completion rate and task
    pass rate based on the predicted value, and then evaluates the participant’s reliability.
    A multistage participant selection strategy is designed. In response to the different
    time of participants participating in the task, we divide the sensing process
    into multiple stages. Evaluating the completion of the task at each stage, and
    selecting participants whose reliability meets the requirements according to the
    quantity and quality requirements of the task, and allocates appropriate rewards
    to them. Considering the situation where the participants withdraw from the task
    in the middle, the platform proposes the task delegation mechanism to ensure that
    the task is effectively completed. A dynamic incentive mechanism with a bargaining
    game is proposed. When releasing tasks at each stage, the platform calculates
    the task requirements by considering the task deadline and task completion to
    formulate different maximum prices for the task. Then, the bargaining game between
    the platform and the participants determines the optimal reward of each participant.
    The remainder of this article is organized as follows. In Section II, we review
    and summarize the related work. In Section III, we show the system model. In Section
    IV, the reliability evaluation method of participants is proposed. Section V introduces
    the participant selection strategy proposed in this article and includes dynamic
    incentive mechanism, participant selection algorithm, and task delegation mechanism.
    The rationality of the proposed algorithm is verified in Section VI. Finally,
    we conclude this article in Section VII. SECTION II. Related Work Regarding the
    choice of participants in MCS, domestic and foreign scholars and research institutions
    have done a lot of research. The related work mainly falls into the following
    two categories: 1) incentive mechanism and 2) participant selection strategy.
    Regarding the incentive mechanism in MCS, Li and Cai [21] in the case that the
    participant chose a random task, taking into account his cost budget and the need
    for sensing data, an online real incentive mechanism is proposed. When the task
    arrives, the platform can make participant selection decisions in order and accurately,
    aiming to keep the social cost of the entire system to a minimum. Xiong et al.
    [22] proposed a task-oriented participant selection incentive mechanism, which
    releases corresponding sensing tasks according to participants’ needs for sensing
    data, and constructs task vectors for participants to participate in tasks in
    multiple dimensions, aiming to meet participants’ needs for tasks to the greatest
    extent. At the same time, participants construct user vectors to formalize their
    personalized preferences for participating task responses. By maximizing the similarity
    between task requirements and user preferences, the task reward of edge nodes
    is minimized, aiming to ensure the fairness of the selected sensing user. Luo
    et al. [23] proposed an MCS user incentive system for fine-grained quality control,
    which aims to meet the requirements of the fine-grained ability of all participants
    to sense tasks by solving the problem of social cost maximization. At the same
    time, the right winner selection is achieved by adopting a greedy algorithm and
    giving the right rewards based on the participant’s bid and fine-grained ability.
    Lin et al. [24] proposed an incentive mechanism assisted by multiple rounds of
    cooperation. Through monetary incentives, multiple rounds of cooperation are employed,
    task information diffusion and task assignment operations are alternate, taking
    into account the inevitable cold start phase of MCS that most people overlook.
    Combined with an effective task information diffusion algorithm, the number of
    users participating in the task is maximized by submitting bids. Yan et al. [25]
    proposed an incentive mechanism suitable for MCS privacy and reliable data sources
    with data collection scenarios. By combining signatures with hash functions, the
    provenance reliability of sensing data is verified without revealing participant
    privacy. And the reward is divided into two parts, fixed reward and floating reward,
    aiming to improve the flexibility of reward distribution. Regarding participant
    selection strategies in MCS, Wu et al. [26] proposed the optimal matching method
    between participating tasks and participants, aiming to solve the needs of fine-grained
    personalized task matching. At the same time, by considering the participants’
    preferences and the evaluation scores of the participant’s participation in the
    recommended tasks, a personalized participant task recommendation system is designed
    by recommending tasks to the participants according to the recommended scores.
    Wang et al. [27] proposed an edge-assisted vehicle recruitment incentive scheme,
    which uses the cooperation incentive between the incentive mechanism, edge server,
    and intelligent vehicle to obtain the optimal cooperative decision by bargaining
    game strategy, and at the same time, a heuristic algorithm is proposed to solve
    the problem of participant recruitment. Li et al. [28] proposed a method for the
    dynamic selection of participants under the framework of a flexible incentive
    mechanism. Game theory methods are first used to estimate the impact of incentives
    on participant behavior, and then predict mobility patterns. Based on the predicted
    participant movement pattern, a distributed approximation algorithm is designed
    to solve the participant selection problem. Liu et al. [29] proposed a motivational
    sensing edge-assisted vehicle recruitment scheme, designing an incentive mechanism
    to stimulate cooperation between edge servers and intelligent vehicles, applies
    Nash bargaining theory to obtain optimal cooperative decision making, and proposed
    heuristic algorithms to solve the problem of participant recruitment. Jiaying
    et al. [30] considered multiple attributes, such as user, task, and surrounding
    environment, and uses a fully connected deep neural network to establish a regression
    model based on user’s wishes, and quantitatively evaluates the user’s willingness
    to perform sensing tasks. Then, according to the sensing scenarios of different
    needs, a participant selection strategy considering user willingness and user
    utility is designed. The above-mentioned related works have solved the problems
    of participants’ incentives or participants, but most of the work does not consider
    the differentiated needs of the task, differentiated pricing, and the inconsistent
    quality of the participants. These problems will lead to the collection of MCS
    system collection poor data quality and high sensing costs. This article proposes
    a reliable participant selection strategy driven by dynamic incentives. By considering
    the participants’ reliability to optimize the quality of the sensing, the selection
    of the participants in different stages will be set differently. Then set different
    maximum prices for the task at different stages, and determine the best rewards
    for participants through bargaining games. Therefore, the strategy proposed can
    improve the quality of sensing data and allocate appropriate rewards for participants.
    SECTION III. System Model As shown in Fig. 1, the system model proposes in this
    article mainly consists of a data requester, MCS platforms, and mobile participants.
    Data requesters submit sensing tasks to the MCS platform according to their needs.
    The MCS platform conducts participants’ selection and task allocation at multiple
    sensing stages according to the different positions of the task and provides the
    participants with appropriate rewards. Participants in the area submitted requests
    to the task in the region after receiving the task message, and the selected participants
    collected the data upload platform and received rewards. Fig. 1. System model.
    Show All Considering the MCS platform, there are n tasks W={ w 1 ,…, w n } . The
    total budget of the platform is B . The entire sensing area is divided into R
    subareas. The entire sensing time is divided into K stages. The time of each stage
    is fixed. For each task w j , the position of the task is l j,r , indicating that
    task w j is in the r th subarea. The effective time of the task is the sum of
    the time of multiple stages, and the cutoff time τ j indicates that the effective
    time of the task w j is τ j stages. The requirement for the number of tasks is
    φ j , indicating that the task needs φ j participants to complete, and each participant
    provides data for the task. To ensure the quality of the data, each task w j has
    a quality requirement γ j , indicating that the reliability of the selected participants
    cannot be lower than the quality requirements of the task. Each stage starts with
    the platform’s maximum price p k w j and the number of current tasks. There is
    a group of participants U={ u 1 ,…, u m } in the entire area, and the reliability
    of each participant u i is q i . Participants move randomly throughout the sensing
    cycle and sensing area and decide whether to participate in the sensing task during
    any sensing stage. The platform obtains optional participants for each task to
    complete the participants’ selection process at each stage. When the participants
    are in the area corresponding to the corresponding period, they can submit a task
    request to the platform. The goal of the participant selection process is to select
    participants who meet the quality requirements for each task within the task deadline,
    provide them with optimal rewards, meet the number of tasks, and minimize the
    number of stages required for each task. When all tasks meet the number of requirements
    or exceed the task deadline, the participant selection process will stop. Considering
    the different tasks of different stages and the conflict of interests of participants
    and platforms, the maximum price for the task is set for the task under the budget
    restrictions and provided the best rewards for participants. SECTION IV. Participants
    Reliability Assessment The MCS network relies on a large number of ordinary users
    as providers of sensing data for the platform. Different participants may submit
    data of different quality for the same task due to differences in device performance,
    sensing ability, and behavioral habits, leading to variations in their reliability
    in completing sensing tasks. The platform prefers to receive data from more reliable
    participants to improve the quality of data collection. Therefore, this Section
    evaluates the reliability of the participants. Considering that the true value
    of a sensing task is often unknown, a task true value prediction algorithm is
    first proposed. Then, the reliability of participants is evaluated based on their
    task completion rate and task pass rate. The task completion rate indicates whether
    a participant can effectively complete a task and submit sensing data within the
    specified time frame. The task pass rate indicates whether the sensing data submitted
    by participants meet the requirements of the task. A. Task Truth Forecast To effectively
    evaluate whether the submitted sensing data in a participant’s history of completed
    tasks is qualified, it needs to be compared with the true value of the sensing
    data. However, the true value of sensing data is often unknown. Therefore, before
    evaluating the reliability of participants, a task true value prediction algorithm
    is proposed to predict the true value of the sensing data based on which the reliability
    of participants can be assessed by comparing their submitted data with the predicted
    true value [31]. For the historical sensing task w j of the participant, the sensing
    data set received by the platform is D w j ={ d 1,j ,…, d k,j } . As the true
    value is unknown, the platform obtains the true value of task d ∗ j after real
    value prediction. To effectively evaluate the true value of the task and reduce
    computation, anomaly detection needs to be performed before executing the true
    value prediction. The purpose of anomaly detection is to find data with large
    deviations, divide the original sensing data into normal data and anomalous data,
    and remove anomalous data, using only normal data for true value prediction. Assuming
    that in real life, most participants will provide reliable data. This Section
    adopts a classical distance-based outlier detection method [32]. For a given data
    set D w j , a distance threshold value δ is used to determine how many adjacent
    data each data d i,j has, if most of the data points are far from d i,j , then
    d i,j is considered an outlier. In addition, a proportion threshold ε is set,
    and data point d i,j is considered an anomalous data point if it satisfies the
    following equation: |{ d k,j |dist( d i,j , d k,j )≤δ}| | D w j | ≤ε (1) View
    Source where dist( d i,j , d k,j )=| d i,j − d k,j | is the distance between two
    values. From the sensing data set D w j , we can obtain a nonanomalous data set
    D ′ w j ⊆ D w j . Next, the true value of the task is predicted based on the nonanomalous
    data set using the following optimization formula, which minimizes the sum of
    loss functions between all the data and the true value d ∗ j [31]: d ∗ j = argmin
    d ∗ j ∑ d i,j ∈ D ′ w j ( d ∗ j − d i,j ) 2 × ω i,j ). (2) View Source Among them,
    ( d ∗ j − d i,j ) 2 represents the loss function between the two values, and ω
    i,j represents the weight of each data. The weight ω i,j represents the importance
    of the data, when the loss between the two values is smaller, the greater the
    weight of the data should be, and the weight ω i,j is updated in real time by
    the following equation: λ= ω i,j = ∑ d i,j ∈ D ′ w j ( d ∗ j − d i,j ) 2 1− (
    d ∗ j − d i,j ) 2 λ ∑ d i,j ∈ D ′ w j (1− ( d ∗ j − d i,j ) 2 λ ) . (3) (4) View
    Source Among them, ω i,j ∈(0,1) and ∑ d i,j ∈ D ′ w j ω i,j =1 . Weight ω i,j
    needs to be continuously renewed in real time, initialized the weight of the ownership
    to 1/| D ′ w j | , and then calculate the truth d ∗ j through (2), combined with
    (4) to update the weight. When the weight of each data changes less than the threshold
    value in the adjacent two iterations, the iteration process stops and the true
    value of the task d ∗ j is obtained according to the final weight. B. Participates
    Reliability For participants u i historic sensing task records, the collection
    of task collection is N A u i , complete the task collection of the Task and submit
    the data as N C u i before the deadline specified by the task, define the participant’s
    task completion rate T C u i T C u i = |N C u i | |N A u i | . (5) View Source
    For historical completion of tasks, the sensing data submitted by the participant
    u i are d i,j , and the real value of the sensing data obtained by the evaluation
    is d ∗ j . The error between d i,j and d ∗ j is evaluated whether the sensing
    data submitted by the participants are qualified. Different tasks are different
    for the quality standards of sensing data. The error threshold of the platform
    definition task w j is θ j . When the error between the data submitted by the
    participant and the true value of the sensing data is less than the threshold,
    the data submitted by the participant is considered to be qualified; otherwise,
    the data is unqualified. Therefore, the data qualification index f( d i,j ) is
    defined as f( d i,j )={ 1, 0, ∣ ∣ d i,j − d ∗ j ∣ ∣ ≤ θ j  otherwise.  (6) View
    Source For the sensing task collection N C u i in the historical task record of
    the participants, the platform evaluates the qualifications of the data in each
    task, and then calculates the task pass rate of the participants for the task
    T Q u i T Q u i = ∑ |N C u i | j=1 f( d i,j ) |N C u i | . (7) View Source To
    sum up, participants’ task completion rate and task pass rate will affect the
    reliability of participants. The higher the task completion rate and qualified
    rate of completing tasks, the higher the reliability of participants. Therefore,
    participants’ reliability is defined as q i =T C u i ×T Q u i . (8) View Source
    SECTION V. Participant Selection Strategy The platform conducts the participant
    selection process in different areas according to the different locations of sensing
    tasks. Considering that the budget and deadline of the task are limited, and the
    sensing time of different participants is different, the platform divides multiple
    stages for sensing tasks and selects the appropriate participants at each stage.
    First, a dynamic incentive mechanism is proposed, taking into account the dynamic
    changes of sensing needs of different stages and different tasks, and setting
    the maximum price for the task when the participants at each stage are selected.
    Then the platform selects the appropriate participants at different stages according
    to the reliability of the participants, and bargains with the participants to
    provide the participants with appropriate rewards. Participants participate in
    the sensing tasks according to the negotiated reward and submit data before the
    task deadline. For those participants who quit the sensing activity halfway through
    and cannot complete the task, the platform proposes a task delegation mechanism
    to promote the effective completion of the task. A. Dynamic Incentive Mechanism
    The entire participant selection process consists of multiple stages, for each
    stage of participant selection, the platform sets a maximum price for each task,
    and then a bargaining game between the platform and the selected participant determines
    the final reward that the participant will receive. 1) Task Maximum Price: When
    the participants at each stage are selected, the platform first releases a maximum
    price for each task, taking into account the different needs of different tasks
    at different stages, different prices should be set for each task in each stage
    to attract participants to actively participate in the sensing tasks [7], [17].
    This Section sets a maximum price for a task, considering the impact of the task
    deadline and completion progress on the task requirements. Each sensing task has
    a deadline within which the collection of sensing data must be completed. When
    the sensing phase is closer to the task deadline, it indicates that the remaining
    sensing time of the task is less, and the task requirement is greater. Moreover,
    the closer the current time is to the task deadline, the faster the task requirement
    grows [17]. Define the k th stage, the impact of the deadline τ j of task w j
    on the task requirements is f 1 (k, τ j ) , and the specific functional form is
    given in Section VI. Each task has some participants required, and the task completion
    progress is different at different stages, where the task completion progress
    represents the ratio υ j / φ j of the currently selected number of participants
    υ j to the number of participants φ j required by the task. The larger the completion
    progress, the smaller the task requirements. Also, the greater the completion
    progress, the faster the task requirements will decrease [17]. Define the k th
    stage, the impact function of task w j ’s completion progress υ j / φ j on task
    requirements is f 2 ( υ j , φ j ) , and the specific function form is given in
    Section VI. Both the deadline and the completion progress of the task have an
    impact on the task requirements, and the closer the current stage is to the deadline
    and the smaller the completion progress, the greater the requirements of the task.
    Therefore, at each stage k gets the requirement T D k w j for task w j is T D
    k w j = f 1 (k, τ j )+ f 2 ( υ j , φ j ) 2 . (9) View Source The maximum price
    of tasks in each stage varies depending on the requirement, and when the task
    requirement is greater, the maximum price of the task is higher. If p 0 is the
    base price of the task, then the maximum price p k w j of the k stage task is
    p k w j = p 0 ×(1+T D k w j ). (10) View Source Considering that the total budget
    B of the system is limited, the sum of the maximum prices of tasks for all phases
    cannot exceed the task budget, and it can be obtained ∑ j=1 n φ j ×2 p 0 ≤B. (11)
    View Source Therefore, you can set the task base price p 0 to p 0 = B 2 ∑ n j=1
    φ j . (12) View Source 2) Optimal Reward for Participants: The platform sets a
    maximum price p k w j for the task during the participant selection process at
    each stage, which is the maximum reward that the platform can pay to participants,
    but is not the real reward paid. Both the platform and the participants are rational
    and selfish, the participants want to get as many rewards as possible to compensate
    for the sensing consumption, and the platform wants to provide as few rewards
    as possible to save costs, so there is a contradiction in providing the right
    rewards for the participants. To solve this contradiction, this article introduces
    a bargaining model [33] to provide participants with optimal rewards, so that
    both participants and the platform are as satisfied as possible. The bargaining
    process between participants and platforms can be regarded as a complete information
    dynamic game process, and a complete information rotation bidding bargaining model
    is established to find an equilibrium solution. After selecting the appropriate
    participants for the task at each stage, the platform engages in one-on-one bargaining
    games with the participants. This article adopts an indefinite rotational bargaining
    model under complete information, in which participant u i has a psychological
    reserve price p 0 u i , which represents the minimum reward of the task that the
    participant can accept, and the maximum price p k w j of the task released by
    the platform represents the highest reward that the platform can provide. The
    bargaining process can be seen as the division between the platform and the participants
    in the proportion of the maximum price of the task and the difference between
    the participant’s psychological floor price. The difference between the two is
    expressed as P= p k w j − p 0 u i . (13) View Source The platform and participants
    divide the price P according to the proportion { x p , x u } , and x p + x u =1
    , the purpose of bargaining is to obtain a result acceptable to both participants
    and the platform. The process of an indefinite bargaining game is in which one
    party places a bid and the other chooses to accept or reject for a round. In the
    first round, the platform bids { x 1 p , x 1 u } first, and participants can choose
    to accept or reject. If the participant accepts, the bargaining ends; if the participant
    refuses, the participant makes a second bid { x 2 p , x 2 u } , at which point
    the platform can choose to accept or reject. If the platform accepts, the bargaining
    ends; if the platform refuses, the next round of gambling is played until one
    party accepts it. Suppose that the discount factors of both the platform and the
    participant are η 1 and η 2 , respectively, and η 1 , η 2 ∈[0,1] . The discount
    factor is the patience of both parties, reflecting the cost of both parties in
    the negotiation, in which the discount factor of the platform is related to the
    task requirements at the current stage, order η 1 =1−T D k w j ; the participant’s
    discount factor is related to the participant’s reliability, order η 2 = q i .
    Thus, the utility of both sides in Round k is expressed as U k p = U k u = ( η
    1 ) k−1 × x k p ( η 2 ) k−1 × x k u . (14) (15) View Source The indefinite rotation
    bargaining game can be solved by subgame refining Nash equilibrium. Theorem 1:There
    is a unique subgame refining Nash equilibrium result in an indefinite round of
    bargaining games x ∗ p = x ∗ u = 1− η 2 1− η 1 η 2 1− x ∗ p . (16) (17) View Source
    Proof:Apply the logic of finite-stage reverse induction to find subgame refinement
    equilibrium. Since the game does not have a final stage, it is impossible to directly
    apply the reverse induction method to solve the problem. Considering that any
    round of subgames bid from the platform is equivalent to the entire game from
    the first round, the logic of finite-stage reverse induction can be applied to
    find the subgame refined equilibrium. Assuming that the platform bids on the k≥3
    round, the maximum percentage that the platform can get is x ∗ p . For the platform,
    the x ∗ p of the round k is equivalent to the η 1 x ∗ p of the k−1 round, and
    the platform will accept it when the participant knows the proportion x k−1 p
    ≥ η 1 x ∗ p obtained by the k−1 round platform, so the participant bids { η 1
    x ∗ p ,1− η 1 x ∗ p } . For participants, the 1− η 1 x ∗ p of the round k−1 is
    equivalent to the η 2 (1− η 1 x ∗ p ) of the k−2 round, and the platform will
    only accept it when the proportion x k−2 u ≥ η 2 (1− η 1 x ∗ p ) received by the
    participants in the k−2 round, so the platform bids {1− η 2 (1− η 1 x ∗ p ), η
    2 (1− η 1 x ∗ p )} . The game starting from round k−2 is exactly the same as the
    game starting in round k , so the maximum proportion obtained by the platform
    in round k−2 must be the same as the maximum proportion obtained in round k ,
    yielding x ∗ p =1− η 2 (1− η 1 x ∗ p ). (18) View Source The solution yields x
    ∗ p = 1− η 2 1− η 1 η 2 . (19) View Source Assuming that in the k≥3 round platform
    bid, the minimum proportion that the platform can get is x ′ p , and the same
    can be found x ′ p = 1− η 2 1− η 1 η 2 . (20) View Source Therefore, the maximum
    proportion obtained by the platform is the same as the minimum proportion, and
    the equalization result is unique. According to the equilibrium solution of the
    bargaining game, the optimal reward given by the platform to participants is p
    ∗ u i = p 0 u i +P× x ∗ u . (21) View Source B. Participant Selection Algorithm
    The entire MCS participant selection process is divided into stages, starting
    with each stage k , the platform first evaluates the needs T D k w j of each task
    w j and then sets the maximum price for the task p k w j . At this stage, participants
    in the current participant set U k w j request task w j , and then the platform
    evaluates the reliability q i of each participant u i and selects participants
    who meet the requirements according to the quality requirements φ j and quantity
    requirements γ j of task w j . Then the platform and the participant bargain game
    to determine the optimal reward p ∗ u i for participant u i , waiting for the
    next stage of the participant selection process. When the task deadline is exceeded
    or all task requirements are met, the participant selection process stops. The
    proposed participant selection algorithm is shown in Algorithm 1. Algorithm 1
    Incentive-Driven Credible-Participant Selection Algorithm (IC-PS) Input: Participant
    Set U , Task Set W , Quality Requirement γ j , Quantity Requirement φ j , Deadline
    τ j , Sensing Phase K Output: Task w j subset U S w j of selected participants,
    the reward p ∗ u i for each participant; 1: Initialize Task w j subset U S w j
    of selected participants, the reward p ∗ u i for each participant 2: for k∈1,…,K
    do 3: for Each task w j in task set W do 4: According to formula (9), calculate
    the requirements T D k w j of the current stage task 5: According to formula (10),
    calculate the maximum price p k w j of the current stage task 6: Gets the current
    set U k w j of regional participants 7: for u i ∈ U k w j do 8: Based on formula
    (8), calculate the reliability q i of the every participant u i 9: if |U S w j
    |< φ j and q i ≥ γ j then 10: U S w j =U S w j ∪ u i 11: Based on formula (21),
    calculate the optimal rewards p ∗ u i of the every participant u i 12: end if
    13: end for 14: end for 15: end for 16: k=k+1 17: if |U S w j |= φ j then 18:
    W=W∖ w j 19: end if 20: Return to line 2 21: When all tasks reach the deadline
    τ j or the quantity requirement φ j for all tasks is met, the participant chooses
    to end C. Task Delegation Mechanism After selecting the appropriate participants
    for the task at each stage, the participants need to arrive at the designated
    location to complete the sensing task before the task deadline, but there are
    inevitable circumstances that cause the participant to temporarily withdraw from
    the sensing activity and be unable to complete the sensing task. This will result
    in the sensing task not being able to collect enough data, degrading the performance
    of the sensing system, and making it difficult for the platform to continue to
    recruit suitable participants in the remaining time, which will also increase
    the sensing cost [34]. Therefore, based on the participant selection algorithm
    in the previous section, this Section proposes a task delegation mechanism to
    ensure the effective completion of the task. For cases where participant u i assigns
    task w j but cannot complete it on time, participant u i can entrust other users
    in the MCS network to assist in completing the sensing task. Two types of users
    are considered here: the first type of user is the user who has some kind of social
    relationship with participant u i , and such users know or have some kind of connection
    with participant u i ; the second type of user is the user who does not have a
    social relationship with participant u i , and there is no interaction between
    such users and the participant. When entrusting tasks to other users, participant
    u i gives preference to participants he knows, who are more likely to complete
    the task than unfamiliar participants. For multiple participants who are willing
    to participate, participants can only select one participant to delegate tasks,
    consider their own interests and needs, and select the participant with the lowest
    quotation to recommend to the platform, and the platform will confirm whether
    the task delegation can be carried out. For the sensing task w j , participant
    u i releases a task delegation message on the platform, including the specific
    information of the task (sensing location, remaining time, quality requirements,
    etc.) and the maximum reward that participant u i can provide, which is the optimal
    reward p ∗ u i previously negotiated by participant u i . After participant u
    i releases the task delegation information, the user u k who intends to participate
    in the task submits his own quotation p u k , which is the remuneration that the
    user wants to get for participating in the task, p u k ≤ p ∗ u i . Participant
    u i first determines whether there is a first category of users in the user set,
    and if so, prefers to select the user submission platform with the lowest quotation
    from the first category of users; otherwise, select the user submission platform
    with the lowest offer from the second category of users. The platform confirms
    whether the delegated user u k can complete the sensing task, and the delegated
    participant must meet the following constraints: q k ≥ t k,j ≤ γ j Δt( w j ).
    (22) (23) View Source Equation (22) indicates that the reliability q k of the
    entrusted user cannot be lower than the quality requirements γ j of the task,
    and the quality of the task is guaranteed to a certain extent; (23) indicates
    that the desired sensing time t k,j declared by user u k is less than the remaining
    valid time Δt( w j ) of task w j , and this constraint ensures that the task can
    be completed on time. If the user u k does not meet the above constraints, the
    task cannot be delegated to user u k , and participant u i continues to select
    the next user. The delegated user can no longer delegate tasks to others, and
    when the delegated user completes the sensing task, rewards are distributed to
    the user according to the quotation p u k of user u k . SECTION VI. Numeric Results
    The following comparative algorithm is designed to analyze the performance of
    the participant selection strategy IC-PS, which is driven by dynamic incentives,
    to better demonstrate its performance as described in this article. Random-Participant
    Selection (R-PS): This algorithm randomly selects participants for each task in
    the sensing region and rewards participants according to their lowest price. Quality
    Priority-Participant Selection (QP-PS): This algorithm prioritizes participants
    with high reliability for each task in the sensing region, and rewards participants
    according to the maximum price of the task. The Section first analyzes the relevant
    indicators of the IC-PS strategy, including the number of stages required to sense
    the task, the task allocation rate, and the maximum price of the task. Then, the
    data quality pass rate, task completion rate, and participant reward are compared
    with the relevant comparison algorithms to verify the performance of the proposed
    strategy. For the specific function forms of f 1 (k, τ j ) and f 2 ( υ j , φ j
    ) in Section V-A, this article sets it up f 1 (k, τ j )= f 2 ( υ j , φ j )= 2
    π ×arcsin( k τ j ) cos( π 2 × υ j φ j ). (24) (25) View Source The specific simulation
    parameters are shown in Table I. TABLE I Simulation Parameters A. Analysis of
    Indicators Related to Participant Selection For each task, the participant selection
    method we proposed needs to select appropriate participants at each stage of the
    task, and the number of stages required by different tasks is different. For a
    single task, Fig. 2 shows the impact of the number of tasks required by the task
    on the number of stages required by the task with the number of selected participants
    of 2 and 4 at each stage. As can be seen from the figure, in the case of fixing
    the number of participants selected for a single stage, as the requirement for
    the number of tasks increases, the number of stages required by the task increases
    accordingly. This suggests that tasks with a high requirement for quantity tend
    to take more time to select enough participants. And, with the same number of
    requirements, the more participants selected for each stage, the fewer phases
    the task lasts. Therefore, for each task, it is necessary to select as many participants
    as possible at each stage to reduce the time for participant selection. Different
    tasks have different due dates, resulting in different progress in task completion.
    Considering that the individual task quantity requirement is 30, Fig. 3 shows
    the impact of task due dates on task allocation rates when the number of participants
    selected at each stage is 2 and 3, respectively. The task allocation rate is defined
    as the ratio of the number of participants assigned to the requirement for the
    number of tasks within the due date of the task. As can be seen from the figure,
    with a fixed number of participants selected for each stage, the longer the due
    date of the task, the greater the distribution rate of the task. This is because
    the longer the due date of the task, the more time the platform has to select
    enough participants for the task, the number of selected participants increases,
    and therefore the task allocation rate increases. And the more participants selected
    for each stage, the higher the task allocation success rate. In the multistage
    participant selection process, the task requirements of different stages are different,
    and the platform corresponds to different prices when releasing tasks. Fig. 4
    shows the impact of task requirements on the maximum price of tasks set by the
    platform when the total number of tasks is 10, the number of tasks required for
    each task is 20, and the total platform budget is 600 and 800, respectively. As
    can be seen from the figure, the greater the requirement for tasks, the higher
    the maximum price of tasks released at that stage. The greater the requirement,
    the lower the task completion progress at this time, and the more participants
    need to be selected, so the platform needs to set a higher task price to attract
    participants’ participation. And, the more budget the platform, the higher the
    price it can offer for tasks. Fig. 2. Impact of task quantity requirements on
    the number of stages required by task. Show All Fig. 3. Impact of task due dates
    on task allocation rate. Show All Fig. 4. Impact of task requirements on the maximum
    price of tasks. Show All B. Analysis of Indicators Related to Task Fig. 5 shows
    the distribution of quality requirement pass rates for different tasks under different
    quality requirement intervals. The quality requirement pass rate is the proportion
    of participants selected by the task who meet the quality requirements of the
    task. As can be seen from the figure, the quality requirement pass rate of the
    IC-PS algorithm has always been 1, which is because the IC-PS algorithm considers
    the reliability of the participants and the quality requirements of the task when
    selecting participants, and when the reliability of the participants is higher
    than the threshold, they will be selected as participants, so the selected participants
    meet the quality requirements of the task. Compared with R-PS random selection
    of participants, QP-PS also considers the reliability of participants and selects
    participants with the best quality, so the quality requirements pass rate is significantly
    improved. Fig. 5. Distribution of quality requirement pass rate under different
    tasks. Show All Fig. 6 shows the distribution of task completion rates for different
    tasks under different quality requirement intervals. Task completion rate is defined
    as the ratio of the number of completed task participants to the number of assigned
    task participants in a single task. It can be seen from the figure that the task
    completion rate of the IC-PS algorithm is the highest, because this article adds
    a task delegation mechanism to the participant selection strategy, and for the
    situation that participants cannot complete the task, other participants can be
    entrusted to assist in completing the task, so the task completion rate is significantly
    improved compared with other algorithms. Fig. 6. Distribution of task completion
    rates under different tasks. Show All C. Analysis of Indicators Related to Reward
    Fig. 7 shows the impact of the number of tasks on the average reward of participants
    when the total platform budget is 400 and the number of tasks requirement is 10.
    As can be seen from the figure, with a certain budget, as the number of tasks
    increases, the average reward of participants decreases. Among them, the participants
    of the QP-PS algorithm have the highest average reward, and the R-PS algorithm
    has the lowest average reward, which is due to the different reward mechanisms
    of different algorithms, the QP-PS algorithm rewards the participants according
    to the maximum price of the task, and the R-PS algorithm rewards the participants
    according to the lowest quotation required by the participants. The IC-PS algorithm
    proposed in this article takes into account the different interests and needs
    of both parties in the bargaining process between participants and platforms.
    Fig. 8 further illustrates the impact of the total task budget on the total reward
    of participants when the total number of tasks is 10 and the requirement for the
    number of tasks is 10. As you can see from the graph, as the total budget of the
    platform increases, the total reward for participants increases. The increase
    in the budget leads to an increase in the base price of the task, so the maximum
    price of the task released by the platform is increased, and the participant reward
    obtained by the three algorithms will increase. Compared with the other two algorithms,
    the IC-PS algorithm proposed in this article gets moderate participant rewards,
    and this distribution method balances the interests of both participants and platforms,
    and the rewards received by participants will not be too low, and the cost paid
    by the platform will not be too high. Fig. 7. Impact of the number of tasks on
    the average reward of participants. Show All Fig. 8. Impact of total task budget
    on participants’ total reward. Show All SECTION VII. Conclusion This article proposes
    a reliable participant selection strategy driven by dynamic incentives. To select
    high-quality participants, the platform evaluates the reliability of participants
    by considering the task completion rate and the task pass rate. Considering the
    difference in the time for participants to participate in the sensing task, the
    platform divides the participant selection process into multiple stages, and at
    each stage evaluates the needs of the task through the deadline of the sensing
    task and the progress of completing it, and sets different maximum prices for
    the task. Then, considering the different interests and needs of the platform
    and the participants, the optimal distribution of rewards among the participants
    is determined through bargaining games. Finally, a task delegation mechanism is
    proposed to ensure the effective completion of tasks. A large number of experiments
    show that the proposed participant selection strategy can select a sufficient
    number of participants to ensure the reliability of completing the task and optimize
    the reward of participants. Authors Figures References Citations Keywords Metrics
    More Like This A Game-Theoretic Approach for Cost-Effective Multicast Routing
    in the Internet of Things IEEE Internet of Things Journal Published: 2022 Trust
    Evaluation for Light Weight Security in Sensor Enabled Internet of Things: Game
    Theory Oriented Approach IEEE Internet of Things Journal Published: 2019 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Dynamic Incentive for Reliable MCS Participant Selection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - 'Aliyah '
  - Lukita C.
  - Pangilinan G.A.
  - Chakim M.H.R.
  - Saputra D.B.
  citation_count: '0'
  description: This research focuses on the high urgency of utilizing Artificial Intelligence
    (AI) and the Internet of Things (IoT) to enhance Smart Tourism Destinations (STDs).
    The integration of AI and IoT technologies offers unprecedented opportunities
    to revolutionize various aspects of tourism, from personalized recommendations
    to real-time data collection. The research aims to provide a comprehensive analysis
    of the current state, challenges, and future direction of STDs in the context
    of AI and IoT integration. It explores various AI techniques and IoT-enabled data
    collection mechanisms that can enrich the traveler experience and improve destination
    management. However, challenges such as privacy and data security issues need
    to be addressed. The research also provides foresight into future technologies
    like Augmented Reality (AR) and Virtual Reality (VR) that can further enhance
    STDs. The ultimate goal is to contribute to the development of smarter, visitor-oriented
    tourism destinations. The research highlights the significance of AI in shaping
    STDs and emphasizes the importance of addressing ethical considerations, data
    quality, interpretability, and human-AI collaboration to ensure responsible and
    effective use of AI in the tourism industry.
  doi: 10.34306/att.v5i2sp.332
  full_citation: '>'
  full_text: '>

    "HOME OVERVIEW ANNOUNCEMENTS ISSUE E-LETTERS EDITOR/REVIEWER VERIFICATION ABC
    Search Register Login HOME / ARCHIVES / VOL. 5 NO. 2SP (2023): SPECIAL ISSUE:
    SUPPORT TECHNOPRENEURSHIP IN THE MEDICAL / Articles Examining the Impact of Artificial
    Intelligence and Internet of Things on Smart Tourism Destinations: A Comprehensive
    Study Aliyah University of Cendekia Abditama Chandra Lukita University of Catur
    Insan Cendekia Cirebon Greian April Pangilinan Centro Escolar University Mochamad
    Heru Riza Chakim University of Raharja Dimas Bagus Saputra University of Raharja
    DOI: https://doi.org/10.34306/att.v5i2sp.332 Keywords: Artificial Intelligence,
    IoT, Smart Tourism Destinations, Traveler Experience, Destination Management ABSTRACT
    This research focuses on the high urgency of utilizing Artificial Intelligence
    (AI) and the Internet of Things (IoT) to enhance Smart Tourism Destinations (STDs).
    The integration of AI and IoT technologies offers unprecedented opportunities
    to revolutionize various aspects of tourism, from personalized recommendations
    to real-time data collection. The research aims to provide a comprehensive analysis
    of the current state, challenges, and future direction of STDs in the context
    of AI and IoT integration. It explores various AI techniques and IoT-enabled data
    collection mechanisms that can enrich the traveler experience and improve destination
    management. However, challenges such as privacy and data security issues need
    to be addressed. The research also provides foresight into future technologies
    like Augmented Reality (AR) and Virtual Reality (VR) that can further enhance
    STDs. The ultimate goal is to contribute to the development of smarter, visitor-oriented
    tourism destinations. The research highlights the significance of AI in shaping
    STDs and emphasizes the importance of addressing ethical considerations, data
    quality, interpretability, and human-AI collaboration to ensure responsible and
    effective use of AI in the tourism industry. REFERENCES J. M. Tien, “Internet
    of things, real-time decision making, and artificial intelligence,” Ann. Data
    Sci., vol. 4, pp. 149–178, 2017. A. Lemmetyinen and F. M. Go, “The key capabilities
    required for managing tourism business networks,” Tour. Manag., vol. 30, no. 1,
    pp. 31–40, 2009. G. Bedi, G. K. Venayagamoorthy, R. Singh, R. R. Brooks, and K.-C.
    Wang, “Review of Internet of Things (IoT) in electric power and energy systems,”
    IEEE Internet Things J., vol. 5, no. 2, pp. 847–870, 2018. Y. Gamil, M. A. Abdullah,
    I. Abd Rahman, and M. M. Asad, “Internet of things in construction industry revolution
    4.0: Recent trends and challenges in the Malaysian context,” J. Eng. Des. Technol.,
    vol. 18, no. 5, pp. 1091–1102, 2020. G. Giuggioli and M. M. Pellegrini, “Artificial
    intelligence as an enabler for entrepreneurs: a systematic literature review and
    an agenda for future research,” Int. J. Entrep. Behav. Res., vol. 29, no. 4, pp.
    816–837, 2023. W. Wang et al., “Realizing the potential of the internet of things
    for smart tourism with 5G and AI,” IEEE Netw., vol. 34, no. 6, pp. 295–301, 2020.
    Y. El Archi, B. Benbba, Z. Nizamatdinova, Y. Issakov, G. I. Vargáné, and L. D.
    Dávid, “Systematic Literature Review Analysing Smart Tourism Destinations in Context
    of Sustainable Development: Current Applications and Future Directions,” Sustainability,
    vol. 15, no. 6, p. 5086, 2023. P. Lee, W. C. Hunter, and N. Chung, “Smart tourism
    city: Developments and transformations,” Sustainability, vol. 12, no. 10, p. 3958,
    2020. F. González-Reverté, “Building sustainable smart destinations: An approach
    based on the development of Spanish smart tourism plans,” Sustainability, vol.
    11, no. 23, p. 6874, 2019. D. Buhalis and A. Amaranggana, “Smart tourism destinations,”
    in Information and communication technologies in tourism 2014, Springer, 2013,
    pp. 553–564. C. Tsai, “Memorable tourist experiences and place attachment when
    consuming local food,” Int. J. Tour. Res., vol. 18, no. 6, pp. 536–548, 2016.
    P. Buonincontri and R. Micera, “The experience co-creation in smart tourism destinations:
    a multiple case analysis of European destinations,” Inf. Technol. Tour., vol.
    16, pp. 285–315, 2016. M. Naramski, “The Application of ICT and Smart Technologies
    in Polish Museums—Towards Smart Tourism,” Sustainability, vol. 12, no. 21, p.
    9287, 2020. D. Z. Jovicic, “From the traditional understanding of tourism destination
    to the smart tourism destination,” Curr. Issues Tour., vol. 22, no. 3, pp. 276–282,
    2019. F. Femenia-Serra, B. Neuhofer, and J. A. Ivars-Baidal, “Towards a conceptualisation
    of smart tourists and their role within the smart destination scenario,” Serv.
    Ind. J., vol. 39, no. 2, pp. 109–133, 2019. M. Woschank, E. Rauch, and H. Zsifkovits,
    “A review of further directions for artificial intelligence, machine learning,
    and deep learning in smart logistics,” Sustainability, vol. 12, no. 9, p. 3760,
    2020. H. Benbya, S. Pachidi, and S. Jarvenpaa, “Special issue editorial: Artificial
    intelligence in organizations: Implications for information systems research,”
    J. Assoc. Inf. Syst., vol. 22, no. 2, p. 10, 2021. E. Cambria and B. White, “Jumping
    NLP curves: A review of natural language processing research,” IEEE Comput. Intell.
    Mag., vol. 9, no. 2, pp. 48–57, 2014. C. Zhang and Y. Lu, “Study on artificial
    intelligence: The state of the art and future prospects,” J. Ind. Inf. Integr.,
    vol. 23, p. 100224, 2021. K. Werder, B. Ramesh, and R. Zhang, “Establishing data
    provenance for responsible artificial intelligence systems,” ACM Trans. Manag.
    Inf. Syst., vol. 13, no. 2, pp. 1–23, 2022. Y. Mao et al., “How data scientistswork
    together with domain experts in scientific collaborations: To find the right answer
    or to ask the right question?,” Proc. ACM Human-Computer Interact., vol. 3, no.
    GROUP, pp. 1–23, 2019. M. M. Maas, “Artificial intelligence governance under change:
    Foundations, facets, frameworks,” SSRN Electron. J., 2021. A. Basili, W. Liguori,
    and F. Palumbo, “NFC smart tourist card: Combining mobile and contactless technologies
    towards a smart tourist experience,” in 2014 IEEE 23rd International WETICE Conference,
    2014, pp. 249–254. D. Edwards and T. Griffin, “Understanding tourists’ spatial
    behaviour: GPS tracking as an aid to sustainable destination management,” J. Sustain.
    Tour., vol. 21, no. 4, pp. 580–595, 2013. S. Mishra and A. K. Tyagi, “The role
    of machine learning techniques in internet of things-based cloud applications,”
    Artif. Intell. internet things Syst., pp. 105–135, 2022. S. Ibnou-Laaroussi, H.
    Rjoub, and W.-K. Wong, “Sustainability of green tourism among international tourists
    and its influence on the achievement of green environment: Evidence from North
    Cyprus,” Sustainability, vol. 12, no. 14, p. 5698, 2020. T. Alam, R. Gupta, S.
    Qamar, and A. Ullah, “Recent Applications of Artificial Intelligence for Sustainable
    Development in Smart Cities,” in Recent Innovations in Artificial Intelligence
    and Smart Applications, Springer, 2022, pp. 135–154. S. Nandi, J. Sarkis, A. A.
    Hervani, and M. M. Helms, “Redesigning supply chains using blockchain-enabled
    circular economy and COVID-19 experiences,” Sustain. Prod. Consum., vol. 27, pp.
    10–22, 2021. M. S. Rahman, S. Bag, M. A. Hossain, F. A. M. A. Fattah, M. O. Gani,
    and N. P. Rana, “The new wave of AI-powered luxury brands online shopping experience:
    The role of digital multisensory cues and customers’ engagement,” J. Retail. Consum.
    Serv., vol. 72, p. 103273, 2023. J. Srouji and T. Mechler, “How privacy-enhancing
    technologies are transforming privacy by design and default: Perspectives for
    today and tomorrow,” J. Data Prot. Priv., vol. 3, no. 3, pp. 268–280, 2020. N.
    Martinez-Martin et al., “Ethical issues in using ambient intelligence in health-care
    settings,” lancet Digit. Heal., vol. 3, no. 2, pp. e115–e123, 2021. A. Hosny,
    C. Parmar, J. Quackenbush, L. H. Schwartz, and H. J. W. L. Aerts, “Artificial
    intelligence in radiology,” Nat. Rev. Cancer, vol. 18, no. 8, pp. 500–510, 2018.
    D. Gunning and D. Aha, “DARPA’s explainable artificial intelligence (XAI) program,”
    AI Mag., vol. 40, no. 2, pp. 44–58, 2019. Q. V. Liao, D. Gruen, and S. Miller,
    “Questioning the AI: informing design practices for explainable AI user experiences,”
    in Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,
    2020, pp. 1–15. V. Dignum, “Responsibility and artificial intelligence,” oxford
    Handb. ethics AI, vol. 4698, p. 215, 2020. L. Lalicic and C. Weismayer, “Consumers’
    reasons and perceived value co-creation of using artificial intelligence-enabled
    travel service agents,” J. Bus. Res., vol. 129, pp. 891–901, 2021. R. K. Behera,
    P. K. Bala, and N. P. Rana, “Creation of sustainable growth with explainable artificial
    intelligence: An empirical insight from consumer packaged goods retailers,” J.
    Clean. Prod., vol. 399, p. 136605, 2023. N. M. Alzahrani and F. A. Alfouzan, “Augmented
    reality (AR) and cyber-security for smart cities—A systematic literature review,”
    Sensors, vol. 22, no. 7, p. 2792, 2022. U. Gretzel, “Smart tourism development.,”
    in Tourism in development: Reflective essays, CABI Wallingford UK, 2021, pp. 159–168.
    PDF PUBLISHED 2023-08-10 HOW TO CITE Aliyah, Lukita, C., Pangilinan, G. A., Chakim,
    M. H. R., & Saputra, D. B. (2023). Examining the Impact of Artificial Intelligence
    and Internet of Things on Smart Tourism Destinations: A Comprehensive Study. Aptisi
    Transactions on Technopreneurship (ATT), 5(2sp), 135–145. https://doi.org/10.34306/att.v5i2sp.332
    More Citation Formats ISSUE Vol. 5 No. 2sp (2023): Special Issue: Support Technopreneurship
    in the Medical SECTION Articles LICENSE Copyright (c) 2023 Aliyah, Chandra Lukita,
    Greian April Pangilinan, Mochamad Heru Riza Chakim, Dimas Bagus Saputra This work
    is licensed under a Creative Commons Attribution 4.0 International License. This
    journal permits and encourages authors to post items submitted to the journal
    on personal websites while providing bibliographic details that credit its publication
    in this journal. Authors are permitted to post their work online in institutional/disciplinary
    repositories or on their own websites. Pre-print versions posted online should
    include a citation and link to the final published version in Journal of Librarianship
    and Scholarly Communication as soon as the issue is available; post-print versions
    (including the final publisher''s PDF) should include a citation and link to the
    journal''s website. Most read articles by the same author(s) Sandy Kosasi, Chandra
    Lukita, Mochamad Heru Riza Chakim, Adam Faturahman, Dhiyah Ayu Rini Kusumawardhani,
    The Influence of Digital Artificial Intelligence Technology on Quality of Life
    with a Global Perspective , Aptisi Transactions on Technopreneurship (ATT): Vol.
    5 No. 3 (2023): November Nazim Hussain, Greian April Pangilinan, Robotics and
    Automation with Artificial Intelligence: Improving Efficiency and Quality , Aptisi
    Transactions on Technopreneurship (ATT): Vol. 5 No. 2 (2023): July Chandra Lukita,
    Marviola Hardini, Sudadi Pranata, Dwi Julianingsih, Nuke Puji Lestari Santoso,
    Transformation of Entrepreneurship and Digital Technology Students in the Era
    of Revolution 4.0 , Aptisi Transactions on Technopreneurship (ATT): Vol. 5 No.
    3 (2023): November INDEXED BY ATT has been Accepted for inclusion in Scopus. SINTA
    PUBLISHED BY SIDEMENU Focus & Scope Indexing Publication Ethics Peer Review Process
    Author Guidelines Author Fee Screening For Plagiarism Publication Frequency Visitor
    Statistics Open Access Statement Copyright Notice Copyright And License Statement
    Article Withdrawal EMPOWERING THE FUTURE WITH YOUR RESEARCH TEMPLATE   VISITOR
    JOURNAL METRICS Number of Volumes: 5 Number of Issues: 14 Number of Reviewers:
    18 Number of Contributors: 156 Contributing Countries: 18 No. of Scopus Citations:
    196 No. of Google Citations: 1924 TOOLS MOST READ LAST WEEK Analysis of Key Factors
    for Improved Customer Experience, Engagement, and Loyalty in the E-Commerce Industry
    in Indonesia 70 Examining the Impact of Artificial Intelligence and Internet of
    Things on Smart Tourism Destinations: A Comprehensive Study 43 Cryptocurrency
    Blockchain Technology in the Digital Revolution Era 22 Influence of Post Covid
    Change In Consumer Behaviour of Millennials On Advertising Techniques and Practices
    22 Analysis of The Effect of Servicescape and Service Quality on Customer Satisfaction
    at Post Shop Coffee Tofee in Bogor City 20 APTISI Transactions on Technopreneurship
    ISSN: 2656-888 (online) 2655-8807 (print) Supported by APTISI - Graphic Era Hill
    University - India Published by Pandawan Sejahtera Indonesia  Website: https://att.aptisi.or.id/index.php/att
    Email: att@aptisi.or.id, ankurbist@gehu.ac.in, marviola@raharja.info © 2023 This
    work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) | View ATT
    Stats"'
  inline_citation: '>'
  journal: APTISI Transactions on Technopreneurship
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Examining the Impact of Artificial Intelligence and Internet of Things on
    Smart Tourism Destinations: A Comprehensive Study'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Subha S.
  - Sathiaseelan J.G.R.
  citation_count: '0'
  description: In IoT, it is a collection of connected devices or sensors that allows
    data to be collected and shared over the internet. Sensor data quality is critical
    in IoT applications since they become unusable if the information is substandard.
    When the IoT sensors' data seem to be of bad quality, the analysis findings should
    be altered, resulting in smart service misdirection. Anomalies or aberrant behavior
    the data that these devices acquire may be affected via attacking problems or
    system failures. Cleaning One of the most vital and necessary processes is dealing
    with noisy sensor data for accurate data interpretation. This paper proposes an
    effective anomaly detection approach to classify noisy data using robust noise
    detection and removal technique called RoNDaR. There are two phases in the proposed
    approach. In the initial stage, the archived data (labeled) is initially clustered
    using the modified weight- based Self Organizing Map clustering algorithm called
    MWSOM. In the next phase, the noise score is computed, and based on the nearest
    neighbors the noise data is detected. Three classification algorithms (K-nearest
    Neighbor) KNN, RF (Random Forest) and SVM are used to evaluate the performance
    of the data on noise detection. The real-time data is used to assess the performance
    of the proposed method. The experimental results show that the proposed approach
    increases the classification accuracy after removing the noise data.
  doi: 10.1007/s42979-023-01890-2
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home SN Computer Science Article Effective
    Anomaly Detection Approach to Classify Noisy Data Using Robust Noise Detection
    and Removal Technique in IoT Healthcare Data Original Research Published: 08 July
    2023 Volume 4, article number 522, (2023) Cite this article SN Computer Science
    Aims and scope Submit manuscript S. Subha & J. G. R. Sathiaseelan  84 Accesses
    1 Citation Explore all metrics Abstract In IoT, it is a collection of connected
    devices or sensors that allows data to be collected and shared over the internet.
    Sensor data quality is critical in IoT applications since they become unusable
    if the information is substandard. When the IoT sensors'' data seem to be of bad
    quality, the analysis findings should be altered, resulting in smart service misdirection.
    Anomalies or aberrant behavior the data that these devices acquire may be affected
    via attacking problems or system failures. Cleaning One of the most vital and
    necessary processes is dealing with noisy sensor data for accurate data interpretation.
    This paper proposes an effective anomaly detection approach to classify noisy
    data using robust noise detection and removal technique called RoNDaR. There are
    two phases in the proposed approach. In the initial stage, the archived data (labeled)
    is initially clustered using the modified weight- based Self Organizing Map clustering
    algorithm called MWSOM. In the next phase, the noise score is computed, and based
    on the nearest neighbors the noise data is detected. Three classification algorithms
    (K-nearest Neighbor) KNN, RF (Random Forest) and SVM are used to evaluate the
    performance of the data on noise detection. The real-time data is used to assess
    the performance of the proposed method. The experimental results show that the
    proposed approach increases the classification accuracy after removing the noise
    data. This is a preview of subscription content, log in via an institution to
    check access.  Similar content being viewed by others Ensemble Bagging Approach
    for IoT Sensor Based Anomaly Detection Chapter © 2021 DRN: Detection and Removal
    of Noisy Instances with Self Organizing Map Chapter © 2022 Identifying Device
    Types for Anomaly Detection in IoT Chapter © 2021 Data availability The dataset
    Heart failure prediction is collected from Kaggle. References Yahyaoui T, Abdellatif
    SY, Attia R (2021) READ-IoT: Reliable Event and Anomaly Detection Framework for
    the Internet of Things. In: IEEE Access 9: 24168–24186 V. Chandola, A. Banerjee,
    and V. Kumar, \"Anomaly detection: A survey\", ACM Comput. Surv., vol. 41, no.
    3, 2009. Goldstein M, Uchida S (2016) A comparative evaluation of unsupervised
    anomaly detection algorithms for multivariate data. PloS ONE 11(4) Ahmed M, Mahmood
    AN, Hu J. A survey of network anomaly detection techniques. J Netw Comput Appl.
    2016;60:19–31. Article   Google Scholar   Mahmoudzadeh I, Azimi AM Rahmani, Liljeberg
    P (2021) Lightweight photoplethysmography quality assessment for real-time IoT-based
    health monitoring using unsupervised anomaly detection, Proc Comput Sci 184: 140–147
    Rettig L, Khayati M, Cudré-Mauroux P, Piórkowski M. Online anomaly detection over
    big data streams. Springer: In Applied data science; 2019. p. 289–312. Google
    Scholar   Borges H, Akbarinia R, Masseglia F. Anomaly detection in time series.
    Springer: In Transactions on Large-Scale Data-and Knowledge-Centered Systems;
    2021. p. 46–62. Google Scholar   Aggarwal CC (2017) Outlier Analysis, Springer
    International Publishing Thudumu S, Branch P, Jin J, Singh J (2020) A comprehensive
    survey of anomaly detection techniques for high dimensional big data. J Big Data7(1).
    Abdel-Kader RF, El-Sayad NE, Rizk RY. Efficient noise reduction system in industrial
    IoT data streams. Proc Int Conf Adv Intelligent Syst Inform Springer. 2021;100:219–32.
    Google Scholar   Liu Y, Dillon T, Yu W, Rahayu W, Mostafa F. Noise removal in
    the presence of significant anomalies for industrial IoT sensor data in manufacturing.
    IEEE Internet Things J. 2020;7(8):7084–96. Article   Google Scholar   Sabzevari
    M, Martínez-Muñoz G, Suárez A. A two-stage ensemble method for the detection of
    class-label noise. Neurocomputing. 2018;275:2374–83. Article   Google Scholar   Nematzadeh
    Z, Ibrahim R, Selamat A, Nazerian V. The synergistic combination of fuzzy C-means
    and ensemble filtering for class noise detection. Eng Comput. 2020;377:2337–55.
    Article   Google Scholar   Nematzadeh Z, Ibrahim R, Selamat A (2020) A hybrid
    model for class noise detection using k-means and classification filtering algorithms,
    SN Appl Sci 2 Sun H et al. Fast Anomaly Detection in Multiple Multi-Dimensional
    Data Streams. In: 2019 IEEE International Conference on Big Data (Big Data), pp.
    1218–1223, 2019 Li J, Izakian H, Pedrycz W, Jamal I (2021) Clustering-based anomaly
    detection in multivariate time series data Appl Soft Comput 100 Carrera D, Rossi
    B, Fragneto P, Boracchi G. Online anomaly detection for long-term ECG monitoring
    using wearable devices. Pattern Recogn. 2019;88:482–92. Article   Google Scholar   Munir
    M, Siddiqui SA, Dengel A, Ahmed S. DeepAnT: a deep learning approach for unsupervised
    anomaly detection in time series. IEEE Access. 2019;7:1991–2005. Article   Google
    Scholar   Ding N, Ma H, Gao H, Ma Y, Tan G (2019) Real-time anomaly detection
    based on long short-Term memory and Gaussian Mixture Model. Comput. Electr. Eng
    79 Turkoz M, Kim S, Son Y, K. Jeong M, Elsayed EA (2020) Generalized support vector
    data description for anomaly detection. Pattern Recognit 100 Zaheer MZ, Lee JH,
    Astrid M, Mahmood A, Lee SI Cleaning label noise with clusters for minimally supervised
    anomaly detection. arXiv preprint arXiv:2104.14770, 2021 Wei Z, Wang F Detecting
    Anomaly Data for IoT Sensor Networks, Scientific Programming, 2022 Aghabozorgi
    S, Shirkhorshidi AS, Wah TY. Time-series clustering–a decade review. Inf Syst.
    2015;53:16–38. Article   Google Scholar   Kohonen T. The self-organizing map.
    Proc IEEE. 1990;78:1464–80. Article   Google Scholar   Download references Author
    information Authors and Affiliations Department of Computer Science, Bishop Heber
    College, Affiliated to Bharathidasan University, Trichy, 17, Tamilnadu, India
    S. Subha & J. G. R. Sathiaseelan Corresponding author Correspondence to S. Subha.
    Ethics declarations Conflict of interest We have no conflict of interest to declare.
    On behalf of all co-authors, the corresponding author shall bear full responsibility
    for the submission. Additional information Publisher''s Note Springer Nature remains
    neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. This article is part of the topical collection “Industrial IoT and
    Cyber-Physical Systems” guest edited by Arun K Somani, Seeram Ramakrishnan, Anil
    Chaudhary and Mehul Mahrishi. Rights and permissions Springer Nature or its licensor
    (e.g. a society or other partner) holds exclusive rights to this article under
    a publishing agreement with the author(s) or other rightsholder(s); author self-archiving
    of the accepted manuscript version of this article is solely governed by the terms
    of such publishing agreement and applicable law. Reprints and permissions About
    this article Cite this article Subha, S., Sathiaseelan, J.G.R. Effective Anomaly
    Detection Approach to Classify Noisy Data Using Robust Noise Detection and Removal
    Technique in IoT Healthcare Data. SN COMPUT. SCI. 4, 522 (2023). https://doi.org/10.1007/s42979-023-01890-2
    Download citation Received 19 February 2023 Accepted 03 May 2023 Published 08
    July 2023 DOI https://doi.org/10.1007/s42979-023-01890-2 Keywords Anomaly detection
    IoT Noisy data Machine learning Associated Content Part of a collection: Industrial
    IoT and Cyber-Physical Systems Access this article Log in via an institution Buy
    article PDF USD 39.95 Price excludes VAT (USA) Tax calculation will be finalised
    during checkout. Instant access to the full article PDF. Rent this article via
    DeepDyve Institutional subscriptions Sections Figures References Abstract Data
    availability References Author information Ethics declarations Additional information
    Rights and permissions About this article Advertisement Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: SN Computer Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Effective Anomaly Detection Approach to Classify Noisy Data Using Robust
    Noise Detection and Removal Technique in IoT Healthcare Data
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Khajehali N.
  - Yan J.
  - Chow Y.W.
  - Fahmideh M.
  citation_count: '1'
  description: The integration of the Internet of Things (IoT) with machine learning
    (ML) is revolutionizing how services and applications impact our daily lives.
    In traditional ML methods, data are collected and processed centrally. However,
    modern IoT networks face challenges in implementing this approach due to their
    vast amount of data and privacy concerns. To overcome these issues, federated
    learning (FL) has emerged as a solution. FL allows ML methods to achieve collaborative
    training by transferring model parameters instead of client data. One of the significant
    challenges of federated learning is that IoT devices as clients usually have different
    computation and communication capacities in a dynamic environment. At the same
    time, their network availability is unstable, and their data quality varies. To
    achieve high-quality federated learning and handle these challenges, designing
    the proper client selection process and methods are essential, which involves
    selecting suitable clients from the candidates. This study presents a comprehensive
    systematic literature review (SLR) that focuses on the challenges of client selection
    (CS) in the context of federated learning (FL). The objective of this SLR is to
    facilitate future research and development of CS methods in FL. Additionally,
    a detailed and in-depth overview of the CS process is provided, encompassing its
    abstract implementation and essential characteristics. This comprehensive presentation
    enables the application of CS in diverse domains. Furthermore, various CS methods
    are thoroughly categorized and explained based on their key characteristics and
    their ability to address specific challenges. This categorization offers valuable
    insights into the current state of the literature while also providing a roadmap
    for prospective investigations in this area of research.
  doi: 10.3390/s23167235
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sensors All Article Types Advanced   Journals
    Sensors Volume 23 Issue 16 10.3390/s23167235 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Academic Editors Dawid Połap
    Robertas Damasevicius Hafiz Tayyab Rauf Subscribe SciFeed Related Info Links More
    by Authors Links Article Views 2052 Citations 1 Table of Contents Abstract Introduction
    Background Research Methodology Discussing CS’s Impact on FL Challenges and Its
    Challenges Pros and Cons of Different CS Methods Limitations and Research Possibilities
    Conclusions Funding Institutional Review Board Statement Informed Consent Statement
    Data Availability Statement Conflicts of Interest Abbreviations Appendix A References
    share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessSystematic Review A Comprehensive Overview of IoT-Based Federated Learning:
    Focusing on Client Selection Methods by Naghmeh Khajehali 1,*, Jun Yan 1, Yang-Wai
    Chow 1 and Mahdi Fahmideh 2 1 School of Computing and Information Technology,
    University of Wollongong, Wollongong, NSW 2522, Australia 2 School of Business,
    University of Southern Queensland (USQ), Brisbane, QLD 4350, Australia * Author
    to whom correspondence should be addressed. Sensors 2023, 23(16), 7235; https://doi.org/10.3390/s23167235
    Submission received: 15 June 2023 / Revised: 12 August 2023 / Accepted: 14 August
    2023 / Published: 17 August 2023 (This article belongs to the Topic Machine Learning
    in Internet of Things) Download keyboard_arrow_down     Browse Figures Versions
    Notes Abstract The integration of the Internet of Things (IoT) with machine learning
    (ML) is revolutionizing how services and applications impact our daily lives.
    In traditional ML methods, data are collected and processed centrally. However,
    modern IoT networks face challenges in implementing this approach due to their
    vast amount of data and privacy concerns. To overcome these issues, federated
    learning (FL) has emerged as a solution. FL allows ML methods to achieve collaborative
    training by transferring model parameters instead of client data. One of the significant
    challenges of federated learning is that IoT devices as clients usually have different
    computation and communication capacities in a dynamic environment. At the same
    time, their network availability is unstable, and their data quality varies. To
    achieve high-quality federated learning and handle these challenges, designing
    the proper client selection process and methods are essential, which involves
    selecting suitable clients from the candidates. This study presents a comprehensive
    systematic literature review (SLR) that focuses on the challenges of client selection
    (CS) in the context of federated learning (FL). The objective of this SLR is to
    facilitate future research and development of CS methods in FL. Additionally,
    a detailed and in-depth overview of the CS process is provided, encompassing its
    abstract implementation and essential characteristics. This comprehensive presentation
    enables the application of CS in diverse domains. Furthermore, various CS methods
    are thoroughly categorized and explained based on their key characteristics and
    their ability to address specific challenges. This categorization offers valuable
    insights into the current state of the literature while also providing a roadmap
    for prospective investigations in this area of research. Keywords: machine learning;
    federated learning; client selection; participant selection; node selection; device
    selection 1. Introduction IoT refers to a network of interconnected devices, sensors,
    and objects that collect and exchange data. These devices can be anything from
    smartphones and wearables to smart home appliances, industrial sensors, or autonomous
    vehicles. The convergence of the IoT and ML presents a compelling alliance with
    the capability to revolutionize IoT applications across diverse sectors. IoT devices
    continuously collect extensive data from various sources such as sensors and cameras.
    ML algorithms can effectively utilize this data to derive valuable insights, enable
    real-time decision-making, and enhance process optimization. In ML used in conjunction
    with the IoT, there is a concern about the amount of data involved in the training
    process, especially when the data are sensitive [1,2,3]. One of the most promising
    solutions to the isolated data island [1] problem is FL, where many clients ranging
    from edge devices to IoT devices collaboratively train a model under the orchestration
    of a central server. In FL, local data do not need to leave the clients. This
    means that ML training can be performed without transferring client data from
    their original location to the servers [4]. Using FL, clients can create centralized,
    robust, and precise local models that are sent to the server [5]. In this way,
    it reduces privacy concerns while allowing users to keep their information private
    [6,7]. In addition, it speeds up and improves local model training [8]. For instance,
    Dayan et al. [9] performed a representative study of an extensive, real-world
    healthcare FL examination across several sites and datasets. In their study, FL
    provided fast cooperation and improved results with no data transferred between
    the participating clients. They concluded that rapid and collaborative development
    of AI methods in healthcare can be accomplished with FL [9]. FL is also useful
    in the following areas: RS: RS are information filtering systems designed to anticipate
    user preferences and offer personalized recommendations. Employing FL in RS yields
    numerous benefits and enhancements, including the delivery of efficient and privacy-preserving
    personalized recommendations to users across diverse platforms and devices [10].
    IoT applications: ➢ IoV: FL also has the potential to bring about a revolutionary
    transformation in the automotive industry and the development of intelligent transportation
    systems [11]. ➢ MEC: The incorporation of FL into MEC is anticipated to have a
    crucial impact in achieving efficient and privacy-preserving intelligent applications
    at the network’s edge [12]. ➢ IIoT: The integration of IIoT with FL has the potential
    of revolutionizing industries and streamlining industrial operations. IIoT involves
    a network of interconnected devices, sensors, and equipment in industrial environments,
    enabling data collection and exchange. On the other hand, FL is a privacy-preserving
    machine learning approach that facilitates model training across distributed devices
    without the need to share sensitive raw data [13]. ➢ IoHT: The integration of
    IoHT with FL is a solution to enhance healthcare practices, enabling advancements
    in remote patient monitoring, disease prediction, and treatment optimization [14].
    Despite the advantages of FL, there are some serious challenges such as expensive
    and inefficient communication [15,16], statistical heterogeneity, poor data quality
    [17], privacy concerns [18,19,20], and client heterogeneity [21]. To solve these
    challenges, numerous investigations and studies have been performed. For instance,
    the authors in [22] focused on the communication efficiency and client heterogeneity
    problem of FL and proposed a new solution. However, the proposed solution suffers
    from a growing number of clients. The issue was solved by increasing the computation
    capability of clients [23]; however, this solution increased costs. In relation
    to the problem of poor data quality, an intelligent medical system was studied
    [24]. In such systems, different types of diseases have different data structures
    and non-IID data, so training heterogeneous datasets is a major issue [17,25].
    To address this challenge, a solution of enabling local model training and only
    communicating model updates is proposed [26,27]. Researchers also have proposed
    various training methods, such as clustering of training data [28], multi-stage
    training and fine-tuning of models [29], and edge computing [30]. However, these
    approaches are still immature, and dealing with data quality while preserving
    model performance remains an open problem [12,31]. While FL does not require raw
    data to leave client devices, it is still possible for the information to leak
    from local model gradient updates [28,32]. In addition, the existence of malicious
    clients in the training process can reduce system reliability and poison model
    performance. This can happen by disrupting the training process or providing false
    updates to the central server [6]. Hence, there is a need to develop and employ
    more comprehensive and robust solutions for enabling FL to better handle its challenges.
    In recent years, client selection (CS) methods have been introduced as one of
    the essential solutions to alleviate the above challenges [33,34,35]. Overall,
    the server evaluates a client’s performance based on information from the local
    models it receives [36]. Due to bandwidth limitations [37] and the availability
    of many clients [34], a selected subset of them can take part in the process at
    each training round [37]. It should be noted that clients usually have significant
    differences in terms of resource constraints, heterogeneous hardware conditions,
    and data resources in many procedures [38]. The CS process actively selects participating
    clients based on predefined criteria for each training round. Researchers have
    demonstrated that different criteria can be adopted to achieve objectives like
    fast convergence, low communication costs, optimal final model, and maximizing
    the overall performance of a model [1,34,35]. For example, the authors in [1]
    show that compared with the vanilla CS algorithm for FL, given the same number
    of rounds, their proposed CS solution (RBCS-F) decreased the training time. However,
    it is not clear why and how CS can enhance global model accuracy, and how to ensure
    a secure, reliable, and fast CS method that can cope with non-IID, unbalanced
    data, and bandwidth restrictions. More specifically, unlike data centers with
    unlimited resources and adequate bandwidth, clients in FL have resource constraints
    and are heterogeneous hardware systems, which can lead to training latency and
    significantly decreases the FL performance [35,39]. For instance, sensor faults
    and environmental restrictions can cause a cluster of mislabeled and non-IID on
    mobile clients, resulting in reduced local learning qualities [17,25]. Every client
    has differences in its dataset distribution, performance, model parameters, available
    computing resources, energy consumption, etc. As a result, they have different
    impacts on the round of training, the convergence speed of the FL process, and
    the global model’s accuracy. Furthermore, bandwidth limitations [5,40] pose a
    risk in the training of all clients and uploading all parameters to the server
    [41]. From a model owner’s standpoint, it is important to know whether CS has
    a significant influence on reducing the training time of a model. It is important
    to know whether CS can enhance the convergence rate, achieve more stable training,
    and improve final accuracy. In relation to these considerations, CS can be an
    effective solution for FL optimization [34,41]. Under large-scale FL scenarios,
    finding a suitable CS technique requires a massive search area with a non-polynomial
    time complexity that cannot be performed in real-time. As a result, to achieve
    a high standard of FL, the CS process and its categories are essential to selecting
    the best clients from the pool of candidates [28]. Therefore, it is necessary
    to thoroughly review, analyze and categorize this research domain. So far, different
    aspects of FL development have been examined and reviewed in the literature. However,
    to date, there is limited work on systematically reviewing the CS process and
    existing CS methods, along with their potential challenges, characteristics, and
    shortcomings. This paper provides an in-depth overview and detailed analysis of
    CS categories based on existing research. The aim of this is to assist industry
    practitioners and researchers in exploring the challenges and potential gaps related
    to CS methods and their development. The main contributions of this systematic
    literature review (SLR) are as follows: ➢ A thorough SLR is presented that examines
    the challenges of FL in adopting CS methods that can be used to aid future research
    and development of CS in FL. ➢ A detailed overview of the CS process, including
    its abstract implementation and characteristics, is presented that can be used
    in various domains. ➢ Different CS methods are categorized and explained based
    on their main characteristics and the challenges they solve. This provides insight
    into current literature and provides a plan for future investigations on this
    topic. This article is organized as follows. Section 2 presents a comprehensive
    background including the definition, challenges, and importance of CS in FL. Then,
    Section 3 discusses the research methodology. In Section 4, the challenges associated
    with FL, an overall structure-based review of CS as a potential solution to these
    challenges, and the prominent factors impacting a model’s performance are discussed.
    This is followed by Section 5, which presents different methods for enhancing
    the performance of FL based on CS. Additionally, major side effects and categories
    of CS methods are explained and analyzed. Finally, Section 6 future trends and
    directions, and Section 7 concludes the paper’s outlines. In general, this work
    presents a comprehensive study of the overall vision, structure, configurations,
    and significant structures associated with CS for FL. 2. Background 2.1. Client
    Selection The increasing number and type of network services and the proliferation
    of mobile edge have prompted the deployment of IoT [2] devices with advanced sensors,
    computing, and communication capabilities for crowd-sensing tasks [42]. The advent
    of AI has led to significant developments in numerous modern applications, such
    as air quality, weather monitoring, and video surveillance [1,2]. Nowadays, ML
    algorithms and intelligent applications have made it possible to analyze various
    types of data, including text, numeric, photographs, videos, and locations, from
    different IoT devices [20,43,44]. However, ML typically employs centralized data,
    which raises several problems [45]. Data privacy [46] is a major problem since
    data cannot be transferred from the devices. There are also challenges related
    to massive scale [47] and optimization [48]. In addition, non-uniform data distribution
    refers to a significant discrepancy between the size and distribution of data
    (texts, images, and videos) stored on devices, which makes data transfer challenging
    [3]. This is compounded by the limited bandwidth between devices and the server.
    To overcome these ML problems, FL was proposed by Google [8]. FL means that multiple
    entities are able to create common ML without data sharing. This addresses critical
    issues such as data privacy, access rights, and access to non-uniform data distribution
    data on a massive scale. FL can be classified into two types based on the participation
    of clients and the training scale: ➢ Cross-device FL with millions of clients
    such as smartphones, wearables, and edge device nodes, where each client typically
    stores local data [49]. ➢ Cross-silo FL in which the client is typically a company
    or organization, with a small number of participants and a huge amount of data,
    and each client is expected to participate in the entire training process [18].
    Using cross-device FL, the parties, entities, or clients can share trained and
    updated models more easily since the bandwidth obstacle in ML is removed [41].
    In FL, raw data from the clients do not need to be transferred to the central
    server to achieve an aggregated final model because all training is conducted
    locally on the clients [6,7]. To be specific, only the post-trained model or parameters
    are sent to the server once the training process has been completed by the local
    client nodes, which in turn protects the privacy of the data owners [33]. Then,
    the model parameters or the post-trained model in FL should be optimized with
    minimal loss by using a gradient approach, such as stochastic gradient descent
    (SGD) [45]. In basic FL, randomly selecting clients from a list of candidate clients
    is not the best method to achieve an optimal global model [7,8,33]. Local clients
    train the global model by using local data. This step is conducted by utilizing
    aggregated model updates before committing the model updates to the server for
    aggregating the final model. The global model is then adapted before being returned
    to each device for the subsequent iteration [8]. So, the convergence speed of
    the model can be affected by the number of participant clients, training iteration,
    resource allocation, data diversity, and aggregation method. In this process,
    hardware issues and data resources can significantly impact learning performance.
    In other words, end client nodes usually have different computation and communication
    capacities and are connected in an unstable environment. There is a risk of stragglers,
    which means that some clients with low-level resources are unable to complete
    their training within the deadline. Moreover, mislabeled and non-IID data [3,25]
    with different data quality are frequently gathered from clients due to sensor
    flaws and environmental restrictions, leading to various local learning shortcomings.
    To deal with these challenges, it is necessary to employ an efficient method to
    select appropriate clients during FL training. Therefore, greater understanding
    and research on the CS process are needed to optimize FL effectiveness and acceptable
    accuracy [8], leading to increased overall performance. For this, a comprehensive
    review of the CS process, methods, and categories will provide much-needed insight
    for the research community. So far, several review papers have been published
    on this topic, presenting proposals, methods, and practical examinations [6,50,51].
    However, a rigorous and well-defined SLR is required to classify and analyze the
    most important and latest research papers on this topic. Hence, an SLR of CS concepts,
    developments, and methods based on qualitative analysis from a design perspective
    is presented in this article for the first time. This review paper addresses the
    following RQs: (1) RQ1: Why and how can adopting an appropriate CS method optimize
    the overall performance of FL? (a) RQ1.1: In which aspects should FL be improved?
    (b) RQ1.2: How can CS help resolve the FL challenges? (2) RQ2: From the structural
    perspective, what are the main pros and cons of different CS methods? (a) RQ2.1:
    How can different methods be categorized in different terms? (b) RQ2.2: What challenges
    have been addressed with CS methods? By addressing the above-mentioned questions,
    this paper provides insights into current research gaps and future research directions.
    2.2. Related Surveys This subsection aims to summarize and discuss the most relevant
    survey work related to the RQs. As mentioned in Section 2.1, from the aspects
    of scale and training, there are two main types of FL, namely, cross-silo FL and
    cross-device FL. Cross-silo FL aims to foster collaboration among several organizations
    at a large scale, while cross-device FL focuses on ML across large populations,
    such as mobile devices [3,18,49]. This paper mostly focuses on cross-device FL;
    thus, this type of FL and its related publications are discussed. In different
    application domains of cross-device FL, such as IoT devices, mobile edge computing,
    and cloud computing, there are severe challenges like highly heterogeneous data,
    heterogeneous client configurations, privacy, and communication efficiency issues
    [18,38,52] among clients (all mobile or IoT devices). Mishandling these challenges
    can adversely affect the performance of FL. Hence, CS methods are used to help
    solve these challenges [6,34]. Employing an effective FL CS method, handling the
    heterogeneity of data and clients, reducing training overheads, guaranteeing privacy,
    efficient communication, strengthening robustness, and improving model accuracy
    can be achieved. Thus, the development of FL based on improved or new CS methods
    is increasingly being studied within the research community [18,51,52]. In Table
    1, different review papers are compared based on their main features and criteria.
    As listed in Table 1, there are three kinds of review papers written on this topic:
    Table 1. Main focuses and features of different surveys that exist in literature.
    (✕: Include criteria. ✓: Do not include criteria). ➢ Focusing on FL challenges
    without considering different CS methods: Li et al. [19], Zhang et al. [24], Liu
    et al. [27], Wen et al. [32], Zhang et al. [36], Nguyen et al. [53], Antunes et
    al. [54], Campos et al. [55], and Banabilah et al. [38] focus on FL challenges
    from the perspectives of IoT devices, IoT, privacy applications, 6G communication,
    privacy protection, intelligent healthcare, healthcare applications, intrusion
    detection in IoT, and edge computing respectively. ➢ Reviewing FL challenges and
    introducing CS as a solution without discussing its challenges: Lo et al. [18]
    examined the development and challenges of FL systems from the software engineering
    perspective. ➢ Focusing on the challenges of CS methods: Only two papers focus
    on CS and its importance for FL. In [41], only system and statistical homogeneity
    challenges are discussed without considering fairness, robustness, and privacy
    issues. In contrast, the authors in [6] briefly examines the critical challenges
    of CS methods extracted from current research, compares them to find the root
    causes of the challenges, and guides future research. However, it is not a comprehensive
    survey and does not contain data privacy issues or the design architecture of
    CS methods. Consequently, it is important to present a comprehensive and organized
    review that covers all of the criteria listed in Table 1. As stated, the literature
    on CS is relatively recent and has been advancing rapidly. Also, there is no thorough
    understanding of FL challenges and CS as a solution from the structural design
    lens to respond to these challenges. Moreover, the role of CS methods in improving
    convergence speed, model performance, decreasing communication costs, and attaining
    an optimal model has not been clearly understood. For addressing these research
    gaps, this review paper supplies an in-depth understanding of the CS process from
    the design perspective along with specifying the importance of CS methods as an
    effective solution for FL challenges. To be more specific, it demonstrates the
    significance of CS on the accuracy of an FL model through various techniques,
    including characteristics associated with each technique. This paper systematically
    demonstrates how CS can solve FL challenges, how it is evolving, and its challenges
    and opportunities. The aim of this is to assist practitioners in selecting the
    most appropriate CS method for their applications and to encourage investigators
    and researchers to gain a deeper understanding of this exciting research topic.
    This will undoubtedly shed light on the existing research gaps and future research
    directions. 3. Research Methodology An SLR is a comprehensive scientific method
    of investigating, determining, and evaluating research questions. It aims to determine,
    diagnose, and evaluate research responses corresponding to the specified RQ containing
    high-quality findings. Other than providing a thorough review of relevant studies,
    an SLR also determines current study gaps, supplies a basis for additional investigations,
    and elucidates new phenomena [56]. In this paper, after collating research papers
    through manual and automated searches using the SLR research methodology, the
    latest and most important literature on CS is categorized and analyzed. Figure
    1 summarizes the steps and methodology used in this study to produce this comprehensive
    SLR. The following subsections explain these steps in more detail. Figure 1. Conceptual
    Framework. A. Defining research questions. As the first step of a research methodology,
    it is required to define the RQs. In Figure 1, the RQs of this study are listed.
    B. Determine data literacy and keywords. To answer the research questions, it
    is essential to choose the best and most helpful and valid data sources [56,57].
    Here, the needed data were gathered from solid and well-known databases like IEEE,
    ACM, Springer, and Elsevier. Based on the RQs, a set of search queries, related
    abbreviations, and alternative synonyms such as “machine learning”, “federated
    learning”, “client selection”, “participant selection”, and “node selection” were
    used for gathering data from those databases. Conducting a keyword search yielded
    an initial pool of 130 resources. This study aims to encompass a comprehensive
    overview by incorporating scholarly publications from esteemed journals and reputable
    conference proceedings, ensuring the inclusion of high-quality academic work.
    C. Selecting studies based on inclusion and exclusion criteria. The inclusion
    criteria in this paper contain which type of research literature, from papers
    to technical reports, can be utilized for extracting data by searching specific
    terms [56]. For instance: The papers explicitly addressed challenges in FL related
    to CS. The papers were published in internationally recognized computer science
    journals and conferences. These publishers contribute to computer Science applications,
    and algorithms are used to structure the logic of their programs, perform computations,
    manipulate data, and control the flow of execution to simplify the CS process.
    The papers were written in English. Moreover, the studies irrelevant to the scope
    of this paper were excluded and are based on the following categories: Papers
    without evaluation results, such as white papers or short papers. Papers that
    provided background information on FL. Papers without peer review, such as theses.
    Papers not written in English. By applying rigorous inclusion and exclusion criteria,
    the number of resources was narrowed down to 86. D. Finalizing the source selection.
    First, the primary source selection was performed by reading the title and abstract
    of the papers. Then, the final selection from the shortlisted papers is made based
    on details of their content and contributions. A meticulous examination of the
    title and abstract of the remaining papers in accordance with the selection criteria
    resulted in a total of 80 papers. Finally, after an in-depth evaluation of the
    papers in the initially selected list, 69 papers emerged as the final selection
    that met all of the selection criteria. E. Data extraction from the selected sources
    In this step, the critical information of each paper was extracted and gathered,
    which contains their references, publication date, title, authors, datasets, applications,
    questions and sub-questions, criteria, merits, and demerits. F. Using study quality
    factor assessment. To assess the selected papers, three main quality factors were
    used, which are listed in Figure 1. This assessment guarantees that the steps
    taken up to now, i.e., steps 1–5, have been carried out correctly. G. Analyzing
    the extracted data. Table 2 categorizes the selected papers based on the RQs defined
    in this paper. In this table, it is clear where each paper falls within the RQs.
    As can clearly be seen, this review is a novel attempt to contribute significantly
    to the understanding of CS. Clearly, this survey outweighs the previously published
    studies in terms of scope, depth, and coverage, since it aims to answer all of
    the defined RQs at the same time. Table 2. A summary of the existing studies based
    on the defined RQs. 4. Discussing CS’s Impact on FL Challenges and Its Challenges
    4.1. FL Structure and Its Challenges In this part, the overall structure of FL
    along with the main challenges of FL are presented. As mentioned in Section 2,
    FL was developed due to the challenges of ML, including the lack of privacy in
    transferring data, its massive scale and heterogeneity, and non-uniform data distribution.
    The general flowchart of FL is shown in Figure 2 and Figure 3. The server (Figure
    2) and the client (Figure 3) are two significant parts of FL. These two significant
    parts are explained in the following. Figure 2. The general flowchart of the FL
    including server initialization, resource management, client selection, incentive
    mechanism, model aggregation, global model, encryption/decryption, model compression,
    and model and system evaluation. Figure 3. Client training includes data collection,
    pre-processing data, encryption/decryption compression, model training, model
    evaluation, model deployment, and audit training process. Central Server. The
    server is one of the key parts of FL. The server initializes the process by completing
    a foremost global model using a sample dataset generated by itself or by collecting
    data from clients [62]. In some FL systems like in [75], clients start the global
    model. Then, an encrypted and compressed global model is broadcasted to clients
    based on an examination of the available clients [50,51,63] or based on the participating
    clients’ performance in the last step [64]. After that, a trained local model
    can be collected from all clients or only the participating clients accordingly.
    The communication coordinator is an administrator that provides a channel between
    the server and multiple clients for communication [37]. It is also possible to
    collect local models either synchronously or asynchronously [57,66]. In contrast
    to synchronous, an asynchronous scheme means that clients do not need to wait
    for each other to synchronize. When the server receives all or part of the updates,
    it performs model aggregation. After that, clients are notified of the updated
    global model. In the end, the evaluation part assesses the system performance
    of the process. This process continues until convergence is reached. In addition
    to orchestrating the exchange of model parameters, FL also has other parts, especially
    a resource manager and a CS process [18]. The resource manager is to make the
    best use of resources. It is the administration system for the optimization of
    resource consumption and control of the allocated resources of clients. The result
    of this is reflected in the CS mechanism for selecting suitable clients to conduct
    model training and reaching desirable system performance [68]. In addition, clients
    may be motivated to participate through incentive mechanisms [71,72,73]. Clients.
    As another important part of FL, clients train local models at each iteration
    using their local data. To begin (see Figure 3), each client gathers and pre-processes
    its data through various steps, including cleaning, labeling, data augmentation,
    data transformation, feature extraction, data reduction, anomaly detection, feature
    fusion, and selection optimization [20]. Then, each client receives the global
    model and initiates the operations of decryption, decompression, and parameter
    extraction from the global model. This step is followed by performing local model
    training by clients. After being trained for multiple rounds [77], the model is
    evaluated by the client and audited as being complete. Model evaluation is to
    ensure that the model has reached the expected level of performance. This step
    is followed by model deployment and model inference. After this step, the model
    is compressed to acquire a sufficient level of performance and to decrease communication
    costs [63,72,74]. Encryption is applied to the local model before it is uploaded
    to secure the process and the data. Then, the local models are sent to the server
    to aggregate the results [78]. It is clear that FL has a comprehensive and coherent
    structure. Other than its advantages, FL also suffers from severe problems, which
    are briefly explained as follows: 4.1.1. Expensive and Inefficient Communication
    Communication is a fundamental problem in federated networks. Due to communication
    costs and privacy concerns in federated networks, data generated by each client
    node must remain local [6]. Instead of forwarding the complete dataset through
    the federated network for model fitting, clients transfer information or model
    updates repeatedly to the server during training. This means that several rounds
    of training are needed before the system converges to achieve the required level
    of accuracy. Hence, the federated network may be overloaded because of numerous
    clients sending their updates to the server. Moreover, network communication speed
    cannot be guaranteed because a federated network may contain many smartphone clients,
    which have limited communication bandwidth, energy, and power, and there are different
    transmission standards such as 3G, 4G, 5G, and Wi-Fi. As a primary solution, expensive
    communication can be employed to avoid overload and achieve high data transfer
    speed simultaneously. However, this is not desirable. As an alternative, a desirable
    solution is for a more efficient communication method to be developed and used.
    Hence, the design of a method with high communication efficiency is essential
    for practical FL [38]. So far, some suggestions to achieve this aim have been
    presented, including local updating techniques, compaction strategies, and decentralized
    training [6,28]. However, these solutions still have efficiency problems in terms
    of communication, and there is large room for further research. 4.1.2. Statistical
    Heterogeneity Statistical heterogeneity is the second challenge in FL. It refers
    to the distribution of data volume and class distribution variance among clients.
    It contains two factors: data quality and non-IID heterogeneity [18]. Variations
    in data quality can arise from diverse data samples used during training for each
    client in each iteration round [8,41]. Furthermore, each client owns a small portion
    of data, which it independently uses for training [58], so differences in unbalanced
    data classes (model parameters) result in fluctuated distribution reflecting non-uniform
    distribution [25] and local data overfitting, which are two issues that lead to
    non-IID. Model training latency and accuracy can be affected by these factors
    [6,34]. As a result, it is important when each client trains on local data independently
    to create a local model, and these models must be very flexible to reduce the
    statistical heterogeneity risk. Some methods have been suggested to control this,
    such as data modeling for heterogeneous datasets and a converged dataset for non-IID
    [53]. However, it is possible to design better solutions to balance accuracy and
    data heterogeneity efficiently. 4.1.3. Client Heterogeneity Differences in the
    client resources, such as computation, storage capabilities, and battery level,
    mean heterogeneity of clients, which is the third challenge in FL. These differences
    are due to various reasons. First, there may be differences in hardware, which
    affects the capacity of CPUs and memory that run AI models. Training models may
    take a long time since AI instances cannot fit into the memory of AI accelerators,
    or it is possible that AI model operators are not supported on devices [24,55].
    Battery power can be the second cause of differences among clients. The battery
    power level of clients depletes when running applications and taking part in the
    training process [59,60]. Due to the above-mentioned causes and network status
    [19], only a fraction of clients can be active simultaneously. Ignoring client
    resource capabilities affects dropouts of the model during the training process,
    leading to training deficiency, which impacts the accuracy of the model. So, FL
    should cater to the following considerations to reduce the risk of client heterogeneity:
    Expect an inferior portion of the participation. There is a need to consider this
    attribute specifically. Tolerate faults in heterogeneous hardware. It is a vital
    attribute of classical distributed systems to support fault tolerance, including
    Byzantine formalism failures [88]. Since some remote clients may drop out before
    completing training, fault tolerance becomes even more critical. For instance,
    suppose the failed clients have specific data properties. Ignoring such client
    failures, like in FedAvg [18], may lead to bias. FedAvg is difficult to analyze
    theoretically in such realistic scenarios and thus lacks convergence guarantees
    to characterize its behavior. Be sufficiently solid to drop clients in the transmission.
    As there is a risk of dropping clients during FL due to computational capability
    or poor network connection, the FL process should be solid enough even when encountering
    this issue [59]. Asynchronous communication. Due to client variability, they are
    also more exposed to stragglers [57]. Stragglers mean that some clients with low-level
    resources are unable to complete their training within the deadline. The use of
    this scheme, particularly in shared memory systems, is an attractive technique
    to mitigate stragglers [19,59], although they generally use boundary-delay assumptions
    to deal with staleness. Li et al. [39] also proposed a FedProx optimization method
    in FL to cope with heterogeneity, but it lacks formalization. Although asynchronous
    FL has been demonstrated to be more practical even with its restrictions [59],
    new solutions to ensure more expected performance are under-explored. Active device
    sampling. Each round of training in federated networks typically involves just
    a small number of clients. Nevertheless, most of these clients are passive in
    that round and each round does not aim to control which clients participate. As
    a result, as was explained, some techniques have been examined in recent studies.
    However, providing the mentioned attributes in a complete solution is of high
    importance. 4.1.4. Data Privacy An FL training process should keep user details
    private since FL aims to solve data privacy issues in ML [19,55]. As previously
    stated, FL is a step toward preserving the privacy of data generated on clients
    while transferring model changes instead of the raw data. Nevertheless, this communication
    may still disclose data and bring privacy risks. There are two privacy strategies
    in the FL structure, global privacy and local privacy [18,19]. Current strategies
    improve FL privacy by utilizing secure multiparty computation clients or differential
    privacy that preserve privacy at the client level rather than after data aggregation.
    These techniques mainly decrease the performance of the model or the efficiency
    of the design. In FL, the server may fail to aggregate the global model when clients
    upload untrustworthy and unreliable data. It is, therefore, crucial to find trustworthy
    and reliable clients in this scheme. A reputation measure was proposed in [28]
    to identify highly reliable clients and calculate their trustworthiness rating
    during the model update process. In summary, this section corresponds to RQ1.1
    and explains the significant challenges of FL in research. Theoretically and empirically,
    understanding and addressing these challenges are significant difficulties in
    FL approaches. 4.2. CS in FL As seen in Figure 2, the server initiates model training
    and orchestrates training rounds while clients carry out local model training.
    By choosing an appropriate CS method, suitable clients can be selected for evaluating
    the model and system performance [68]. Furthermore, Figure 4 shows the detailed
    CS process in a simple and categorized way. This provides a better and more general
    understanding of the CS process and its different parts and steps. The server
    sends a ticket to the clients for detecting and monitoring clients. The online
    clients who want to take part in the model training process respond to the server’s
    request. Then, the server computes the available resources and uses a specific
    strategy to choose participating clients. Resource allocation to the selected
    clients is conducted for the training process. After that, tasks are assigned
    to participants in two ways. First, use a hybrid algorithm and scheduling to repeat
    the chosen CS method until convergence is achieved. Second, the process is repeated
    for each iteration round. Scheduling the task can improve the system’s efficiency
    [26,85]. However, the implementation of this strategy is sometimes impossible,
    especially in a volatile environment. According to the explanations in this part,
    the mentioned challenges in the FL are in need of a solid solution. Focusing on
    the CS process can be a suitable approach to address the mentioned challenges
    in FL. As it is evident, RQ1.2 was addressed in this section. Figure 4. CS process:
    (a) The server sends a ticket to the clients for detecting and monitoring clients.
    (b)The available clients for model training respond. (c) The server computes the
    available resources. (d) The server selects a strategy to opt for participation.
    (e) Resource allocation is conducted, and tasks are assigned to participants.
    5. Pros and Cons of Different CS Methods 5.1. CS Methods The main aim of this
    part is to describe different CS methods. (1) Client selection methods based on
    the probability of selection: four CS methods based on the probability of selection
    at each round are presented as follows. Random selection There are several CS
    methods in FL, but randomly selecting clients is the conventional approach [86,87,88].
    Based on the FedAvg method [18], all clients will have the same probability of
    being selected for model training. In this method, aggregation is inefficient
    as this method ignores value differences among clients. In this method, each client
    trains its local model using its own data and then sends the updated model to
    the central server for aggregation. During aggregation, the central server simply
    averages the model updates from all clients and uses this average to update the
    global model. The inefficiency arises because FedAvg treats all client updates
    equally, regardless of the amount or quality of data each client has. Some clients
    might have more diverse or informative data, while others might have noisy or
    less relevant data. By blindly averaging all updates, valuable information from
    high-quality clients may be diluted or lost in the process. Additionally, this
    method does not consider the data heterogeneity of clients. The weak point of
    this method in a distributed computing environment is its high communication costs
    because the central server receives updates from distributed clients on a fixed
    bandwidth [5]. It is possible to save transportation costs by randomly selecting
    a part of the updated model parameters for transportation by random masking [33].
    However, it has restrictions, which we will discuss in more detail in the evaluation
    section. Greedy selection This method chooses clients with high-level quality
    grades and low expenses. It utilizes a heuristic method to characterize the quality
    rate of each client [33,85]. Each client employs a tiny subset of local data to
    train the global model and evaluate the FL platform model. Recently, this method
    has been widely used to evaluate the quality of budgeted incentive mechanisms
    in selecting the most influential clients for incentives [62]. In other words,
    this method selects the set of clients with the most considerable collaborative
    feedback. The FedCS algorithm proposed by Nisho [8] is mainly based on the greedy
    method. This algorithm is a typical example that is adjusted by picking the clients
    with the most significant average contribution instead of selecting the clients
    that complete the training in less time. This approach of CS prefers clients with
    high-level efficiency during each iteration training round. Then, it can effectively
    enhance the aggregation efficiency of FL models by completing the training model
    quickly and before the deadline. In this method, data collection is performed
    in FL regardless of existing clients in a federated network. As the amount of
    data varies significantly in different clients in FL, the data are non-IID in
    real-world datasets. Similar to the random method, the quality of client data
    is neglected [8,18]. Accordingly, they cannot reduce the number of clients selected
    with low-quality data, resulting in low-level accuracy for the global model and
    gradual convergence. Choosing superior clients accelerates global model convergence
    and improves global model accuracy along with keeping bandwidth boundaries. This
    is the primary objective of FL CS. Clustering selection In this method, clients
    that train the model are clustered according to their attribute similarities,
    including their resources, allocated data, characteristics, location, segment
    similarities, and gradient loss, to enhance the overall model efficiency and boost
    model training performance. In other words, k-center grouping is performed on
    the set of clients before training, and then the closest clients to the center
    client of each cluster are assigned to the cluster and the model training is conducted
    based on the clusters [13,89,90]. Multi-Armed Bandit (MAB) MAB is mainly used
    to get the root of repeated discovery situations in which a player (in the FL
    scenario, typically represented as the server) encounters a situation where it
    must choose from multiple arms (corresponding to the clients). The player honors
    the related reward (refer to model performance in FL) when an action is taken
    (choosing specific clients to participate in model training at each iteration
    round). Boosting the total prize and making sequential decisions simultaneously
    is the MAB’s primary goal. Players should examine the surroundings to gain more
    knowledge on each training round, recognize activities that boost the chance of
    achieving higher rewards, or exploit existing knowledge to execute the actions
    that reasonably worked in the past. This method has been used to design client
    scheduling [1,7,59] or in the CS process [91,92,93]. Three main categories arise
    from the proposed procedures to decrease the training latency in FL: Update compression
    (quantizing gradient is a solution for efficient communication). Over-the-air
    analysis [63]. Reducing transmission (periodic updates of model parameters to
    lessen the transmission expenses) [34,59]. To summarize, CS method categories
    are described in this section in response to RQ2.1. These methods have some merits
    and demerits that we will discuss in the evaluation section. 5.2. CS Side Effects
    This section describes the side effects of CS methods. Clearly, the implementation
    of CS methods improves the overall performance of FL in terms of client heterogeneity,
    statistical heterogeneity, and data quality. However, it is noteworthy that employing
    these methods may cause or intensify some side effects in FL. A brief explanation
    of these problems is as follows. Fairness: Fairness means that every client has
    an equal chance to be selected for training. When fairness is ignored, the server
    may prioritize the client with a different dataset size but in a shorter response
    time. This may significantly affect the training performance. So, clients with
    insufficient abilities have a lower chance of being selected to participate in
    the training process, which may lead to bias and low-level model accuracy [1,41].
    Fairness boosts the accuracy and speed of convergence of models by enabling clients
    with various datasets to participate in the FL [34,35,59]. Consequently, all end
    devices should be involved in the FL process to decrease model bias. Trustworthiness:
    Because the FL server is unaware of the local training procedure, malicious clients
    can launch attacks and manipulate the training outputs. A primary priority should
    be recognizing and removing malicious clients from the procedure [6]. Dynamic
    environment: This means that because of the existence of deficiencies, including
    high mobility, poor network conditions, and energy constraints, some clients might
    not be available to take part in model training [35,49,59]. Moreover, channel
    fading in wireless networks may result in losing some local model updates. Therefore,
    a dynamic condition with high-mobility devices and volatility including client
    population, client data, training status, and biased data [84] significantly impacts
    the performance of the CS process and FL. This section clarifies the most significant
    side effects in CS such as client heterogeneity, statistical heterogeneity, data
    quality, fairness, trustworthiness, and dynamic environment (addresses RQ.2.2).
    In Table 3, all of the findings and results of CS categories, along with each
    side effect, main characteristics, application, strategy of each source, and evaluation
    metrics of each work, are presented. It should be noted that the evaluation metrics
    are discussed in more detail in Appendix A. Table 3. CS Categories based on the
    probability of selection. 5.3. Overall Evaluation of Different CS Methods Clustering
    methods In typical dynamic FL training and clustering methods, FL clients display
    system and statistical heterogeneity. The main issue in data heterogeneity in
    clustering is non-IID data issues [10,13,28]. The clustering method can be based
    on training data [31,89,90] or based on the location of clients and the required
    skills and efficient collaboration among each other [2,13]. Some work performed
    clustering if necessary [28] and handled varying client populations. This provides
    distribution imbalance while its extent in conjunction with privacy strategies
    and compaction mechanisms is unclear. One work used the successive non-convex
    penalty (SNCP) approach as a performance evaluator, which can reduce communication
    costs [90]. However, it cannot handle outlier and noisy data. Some works [28,90]
    use multi-task learning in times when the clustering structure is ambiguous [31],
    but it goes with high communication and computation overheads. This challenge
    has been resolved in [10], and the mentioned method is only suitable for use in
    the risk functions context and evaluates the similarity of the loss value as a
    technique of secure data similarity evaluation. The authors in [13] address the
    divergence issues in class distributions by using a gradient-based binary permutation
    algorithm (GBP- CS) and tackle the issue of robust FL in a heterogeneous setting
    by having a functional convergence rate compared to FedAvg. Such methods are time-efficient
    models along with high-level efficiency. Greedy methods In greedy or dynamic methods,
    resource constraint issues [87,88] contain bandwidth allocation issues [5,40],
    communication cost issues [33,85], limited computational resources issues [8,42,85],
    and the energy consumption of selected clients [26,42], which can lead to low
    accuracy and high convergence time and latency. The authors in [40] proposed a
    novel perspective to resource allocation in WFLNs, realizing that learning rounds
    are temporally interdependent and have varying significance toward the final learning
    outcome. It is adaptive to varying network conditions, and it can enhance the
    training loss and model accuracy and reduce energy consumption. However, in this
    method, participation rounds of clients are limited because of the limited battery
    energy of clients. Clients in a wireless network are limited by finite wireless
    bandwidth in each iteration, with an adaptive choice to unstable phases of wireless
    channels. Although they reasoned that always picking the highest number of clients
    is not necessary, some other work [8,40] considers maximizing the number of the
    selected clients in each round to upload their local models before the deadline.
    In another paper, the authors offered a novel strategy [26] to choose fewer clients
    in earlier global iterations and more clients in later global iterations in the
    same period of training time. This can increase model accuracy and reduce training
    loss when compared to choosing more clients at first. Because it overlooks the
    local data quality of clients and cannot decrease the number of client selections
    with low-quality data, the global model needs to be more accurate, and convergence
    needs to be faster. Neither CS nor resource management solutions were discussed
    in terms of how they affect the convergence and accuracy of global models. Likewise,
    [87] ignores client data quality, so it is unable to decline client selections
    with low-level data quality and does not consider the clients’ waiting time leading
    to clients’ latency. However, it considers client channel conditions and the importance
    of their local model updates. The authors studied diverse scheduling models to
    select an appropriate participant client in the learning process at each round.
    In contrast, the authors in [42] prefer to choose high-data quality clients, ensuring
    system efficiency and prioritizing the clients who have suitable data rates rather
    than those with poor calculation and transmission capacities. So, it optimizes
    on-device data quality across clients while reducing delay, energy consumption,
    and packet size. Moreover, it provides a higher level of accuracy while improving
    convergence speed. Extremely dynamic scenarios were ignored in [8], where the
    average amount of resources and the required time for updating and uploading fluctuate
    dynamically. It assumes the scheduler has a pre-known local training time, which
    may only be realistic in some cases. It ignores client waiting time and undervalues
    the client’s latency in a global iteration. Moreover, transmission resource management
    and client data quality were neglected, and it could not decline the number of
    choices for clients with poor-quality data, leading to low global model accuracy
    and slow convergence. It only evaluates communication time, which accounts for
    a considerable amount of time for a training round. Random methods In random selection,
    which is the conventional and basic form of CS, resource constraint issues [87,88]
    contain bandwidth allocation issues [5], limited computational resources issues
    [42], and the energy consumption of selected clients [26,42], which can lead to
    low accuracy and high convergence time and latency. In [26], the authors offered
    a novel strategy to choose fewer clients in earlier global iterations and more
    clients in later global iterations in the same period of training. This can increase
    model accuracy and reduce training loss when compared to choosing more clients
    at first. Because it overlooks the local data quality of clients and cannot decrease
    the number of selections for clients with low-quality data, the global model needs
    to be more accurate, and convergence needs to be faster. Neither CS nor resource
    management solutions were discussed in terms of how they affect the convergence
    and accuracy of global models. Likewise, [87] ignores the client data quality,
    so it is unable to decline the clients selected with low-level data quality and
    does not consider the client waiting time, leading to client latency. However,
    it considers client channel conditions and the importance of their local model
    updates. The authors studied diverse scheduling models to select an appropriate
    participant client in the learning process at each round. In contrast, the authors
    in [42] prefer to choose high-data quality clients, ensuring system efficiency
    and prioritizing the clients who have suitable data rates rather than those with
    poor calculation of transmission capacities. So, it optimizes on-device data quality
    across clients while reducing delay, energy consumption, and packet size. Moreover,
    it provides a higher level of accuracy while improving convergence speed. MAB
    methods Multi-armed bandit-based method side effects are divided into four groups:
    dynamic wireless environment [93], client heterogeneity [7], data quality [52,91],
    and fairness [35,62,92]. In each sub-group, their main characteristic is the training
    latency [91,92,93]. To illustrate, authors in [93] proposed a CS algorithm based
    on the UCB policy and virtual queue technique (CS-UCB-Q). The method considers
    the availability of clients during FL training in the study because of the deep
    fade concern in wireless channels in both ideal and non-ideal strategies and unbalanced
    data in a volatile environment. However, the mentioned method and [1,7], cannot
    run asynchronously. In contrast, [91,92] can run asynchronously and provides a
    trade-off between training efficiency and fairness. A CS framework (AUCTION) as
    a model to obtain a root of fairness is suggested by [38], which employs a heuristic
    method to characterize the quality of each client and analyze the data quality
    challenges of each client in terms of the mislabeled and non-IID data. It is robust,
    adjustable, and scalable in diverse learning tasks and makes CS easy and flexible
    by automatically knowing procedures for variable client scales. Moreover, the
    research [38] develops a procedure network based on the encoder–decoder structure,
    which can be adjusted to dynamic modification clients and make sequential CS decisions
    to decrease RL searching space significantly. However, it did not consider the
    transmission expense of clients and computing latency to expand its CS functionality
    further. In another paper [35], a deadline-based aggregation model was offered
    to handle FL aggregation in a changing training environment, reaching faster convergence
    to fixed model accuracy. However, low-priority clients were denied training. Therefore,
    inequality selection does not guarantee data diversity on the global model aggregation.
    It ignores the local data quality of the clients, and it cannot decline the selections
    count for clients with low-quality data, resulting in low-level global model performance
    and slow convergence. The first research in mixing Lyapunov optimization and the
    C2MAB long-term constrained online scheduling issue is [1], which is a fairness-based
    CS while ensuring training efficiency and minimizing the average model exchange
    time when it is subject to a relatively flexible long-term fairness guarantee.
    It can handle unfair CS and large bias in data distribution, but it is unable
    to follow the theoretical analysis of the fairness for FL from the literature.
    It ignores the data quality factors, including mislabeling or non-IID, and it
    cannot find a way to trade-off between fairness and accuracy. It blindly considers
    fairness restrictions for each client while ignoring their contributions. Fairness
    quota metrics can severely impact training efficiency and should be assigned before
    training. Furthermore, it cannot run asynchronously. The authors of [62] introduce
    cumulative effective participation data (CEPD) as an optimization objective of
    volatile CS. They designed and implemented a CMAB model for learning efficient
    client participation and derived a finite constant upper bound on T-step regret
    based on UCBGS; however, they did not analyze the effect of policy fairness on
    training, nor the trade-off between fairness and overall training performance
    in a volatile FL. They also avoided focusing on selection adaptation when new
    clients are added. Overall, our findings indicate that MAB aims to minimize training
    latency. Considering an ideal and a non-ideal situation, it contains both local
    computation and data transmission times. The ideal scenario involves clients possessing
    IID datasets and always being available, whereas the non-ideal scenario involves
    clients being unavailable and the datasets being distributed non-IID. The primary
    purpose of the dynamic client sampling method is to improve the convergence rate.
    A non-convex training time minimization problem is developed by dynamic client
    sampling that gives an upper bound on convergence for arbitrary CS possibilities.
    Adopting such strategies can achieve the same target loss faster than the baseline.
    Using clustered sampling, different clients can be selected with different data
    allocations. An unbiased clustered sampling strategy for CS is offered that declines
    the weight variance of clients for the aggregating and provides unique client
    distribution. According to the authors, clustered sampling techniques were utilized
    for sample size and client similarity, so there is faster and better homogeneity
    with clustered sampling, especially for non-IID data. Table 4 summarizes the advantages
    and disadvantages of the mentioned methods that were extracted through RQ2. Table
    4. The advantages and disadvantages of each CS method. 6. Limitations and Research
    Possibilities To highlight motivations for future work, we first identify the
    limitations of current work and then discuss the critical potential points that
    should be considered for future work. This field of research is in its early stages,
    and there is limited research in literature. Hence, this work has the limitation
    of the number of reviewed publications. However, it should be considered that
    this is the first step to creating a comprehensive overview of this field. There
    are numerous unresolved concerns and issues surrounding CS in both the cross-silo
    and cross-device settings. This field of study presents numerous examination possibilities
    that need more in-depth analysis. In addition to developing high-performance CS
    algorithms for diverse application systems, existing work supposes the following
    issues as future open directions: Privacy and Communication: In the FL process,
    the communication between clients and parameter servers usually occurs over an
    impaired wireless channel, which introduces some research queries about privacy
    issues and how the updates can be transferred to a secure channel. Trade-off between
    metric factors: A considerable number of factors to improve model performance
    were used. However, different factors are not comparable. So, a need exists to
    balance factors for performance evaluation among various techniques for the same
    problem. For instance, selecting more clients in each training round boosts model
    performance and training efficiency but does not guarantee time efficiency, especially
    in a volatile environment. In the research that was reviewed in the paper, the
    rate of volatility in that space was unclear. This issue can be a potential research
    gap for future researchers. Asynchronous communication schemes: Regarding analysis
    approaches, asynchronous communication schemes for local data updates remain an
    open issue demanding additional examination. Communication resource handling:
    There is space to explore appropriate communication resource methods for allocating
    resources (same or different bandwidth, energy, and computational capacity) based
    on the network topology. This strategy can remarkably affect learning performance.
    This issue becomes essential when many client devices join the FL process. Remarkably,
    the training rate can be greatly reduced due to different client heterogeneity
    of computational capacities and data qualities. A favorable answer would be developing
    additional parts to encourage clients to use high-quality training data. Channel
    characteristics: Analyzing the network requirements impacts the accuracy of federated
    model training. It is a future examination direction, particularly in wireless
    communication, when noise, path loss, shadowing, and fading impairments exist.
    Available datasets for clients: The availability of client datasets is needed
    to obtain suitable training performance. Clients needed to use feature extraction
    for their local training. In this regard, one of the critical problems is the
    non-IID matter, potentially causing the local training to be highly divergent.
    Therefore, some solutions to cope with this matter need to be developed. 7. Conclusions
    This paper provides a comprehensive SLR of FL in IoT devices and CS methods and
    their challenges. FL faces severe challenges, including expensive and inefficient
    communication, statistical heterogeneity, poor data quality, privacy concerns,
    and client heterogeneity. Based on the reviewed literature, CS is a suitable solution
    to these challenges. To better understand the importance of CS in FL, a categorization
    of CS methods, including clustering, random selection, greedy selection, and multi-armed
    bandit was presented. However, these methods contain some side effects, such as
    fairness, dynamic environment, and trustworthiness issues. Hence, finding a suitable
    CS method is still an open problem, and further exploration is needed. As a result,
    based on this work, it is possible to classify existing CS methods, understand
    their current status, and plan and move to develop more desirable and efficient
    approaches. Funding This research received no external funding. Institutional
    Review Board Statement Not applicable. Informed Consent Statement Not applicable.
    Data Availability Statement Not applicable. Conflicts of Interest The authors
    declare no conflict of interest. Abbreviations AI Artificial intelligence MAB
    Multi-Armed Bandit AUC Area Under curve Macro-Acc Macro-Accuracy AUROC Area Under
    the ROC curve MEC Mobile edge computing CEPD Cumulative effective participation
    data Micro-Acc Micro-Accuracy CFL Clustered FL ML Machine learning CMAB Combinatorial
    multiarmed bandit MUEs Mobile user equipment CS Client selection non-IID Non-independent
    and -identical data FedAvg Federated averaging ROC Receiver Operating Characteristic
    FedCS Federated client selection RQs Research questions FL Federated Learning
    RS Recommendation systems FN False Negatives SGD Stochastic gradient descent FP
    False Positive THF 3-way hierarchical GBP- CS Gradient-based Binary Permutation
    Algorithm TN True Negative IIoT Industrial Internet of Things TP True Positive
    IoT Internet of Things MAB Multi-Armed Bandit IoV Internet of Vehicles WFLN Wireless
    FL network Appendix A The assessment of FL can be categorized into two distinct
    dimensions: model performance and system performance. The evaluation of model
    performance entails quantification through metrics convergence and accuracy. Accuracy
    is also allied with measures such as Recall, Precision, F1-Score, Micro-Acc, Micro-F1,
    Macro-Acc, and Macro-F1. These metrics serve as valuable methodologies to gauge
    the effectiveness of individual clients’ contributions to the overall FL system.
    The evaluation of model convergence is achievable through multiple facets, encompassing
    criteria such as training loss, the count of communication rounds, the number
    of local training epochs, and the establishment of formal convergence boundaries.
    Conversely, the assessment of system performance metrics directs its attention
    to parameters such as communication efficiency, computational efficiency, system
    heterogeneity, system scalability, and the capability to withstand faults [93,94].
    These metrics are elucidated in greater depth in the subsequent paragraph. ➢ Model
    performance metrics Accuracy Accuracy pertains to the proportion of correctly
    classified instances within the test set. Throughout the annals of machine learning
    research, accuracy has wielded considerable influence as a performance metric
    [28]. Accuracy= TP+TN FP+FN+TP+TN (A1) TP signifies the count of instances that
    have been accurately forecasted as positive by the model. TN quantifies the instances
    that have been correctly predicted as negative. FP delineates instances that have
    been erroneously categorized as positive. FN accounts for instances that have
    been inaccurately classified as negative by the model. Precision Precision assesses
    the veracity of positive predictions generated by the model. It involves the computation
    of the proportion of true positive predictions relative to the total instances
    that have been predicted as positive (sum of true positives and false positives).
    In essence, precision provides insight into the fraction of positive predictions
    that have been accurately determined. Elevated precision signifies that the model
    demonstrates a reduced frequency of false positive predictions [95]. Precision=
    TP FP+TP (A2) Recall Referred to as sensitivity or true positive rate, this metric
    gauge the model’s competence in apprehending all factual positive instances. It
    quantifies the correlation between true positive predictions and the entirety
    of actual positive instances (sum of true positives and false negatives). In simpler
    terms, recall provides insight into the percentage of positive instances that
    have been accurately anticipated as positive. A heightened recall signifies that
    the model adeptly identifies a significant portion of positive instances [95].
    Recall= TP+TN FP+TP (A3) F1-Score The F1-score represents the harmonic mean achieved
    by integrating precision and recall, amalgamating these two metrics into a solitary
    value to offer an equilibrium-based gauge of the model’s performance [28]. F1−score=2∗
    Precision×Recall Precision+Recall (A4) Micro-Acc Micro-Acc is an evaluation measure
    deployed in the context of multi-class classification endeavors. It serves to
    compute comprehensive accuracy by aggregating the accurate predictions across
    all classes. This approach conceptualizes the problem akin to a binary classification
    scenario, where the affirmative class signifies correct predictions, while the
    negatory class denotes incorrect ones. This metric accords equal significance
    to each individual instance [28]. Micro−Acc= TP1+TP2+…TPn TP1+TP2+…TPn++FP1+FP2+…+FPn
    (A5) TPi denotes the numerical representation of true positive instances exclusively
    associated with class I. FPi signifies the cumulative enumeration of false positive
    instances linked to class I. n denotes the entirety of classes that are presently
    being considered [28]. Macro-Acc Macro-Acc constitutes an additional assessment
    metric that finds application in the realm of multi-class classification endeavors.
    In contrast to micro-accuracy, which accords equal significance to each individual
    instance, macro-accuracy ascertains the mean accuracy for each distinct class.
    Subsequently, it computes the average of these accuracies pertaining to individual
    classes, thereby deriving a holistic evaluation of the model’s performance. Macro−Acc=
    Acc1+Acc2+…+Accn n (A6) Acci represents the accuracy pertaining to class I. n
    signifies the total count of classes encompassed [28]. Micro-F1 Micro-F1 is determined
    through an inclusive process encompassing all instances, along with their respective
    true positive, false positive, and false negative tallies across various classes.
    These data are subsequently employed to compute precision and recall. The ultimate
    Micro-F1 score materializes as the harmonic mean of micro-precision and micro-recall.
    Micro-F1 effectively addresses the challenge of class imbalance according to equal
    weight in all instances. This metric finds pertinence in scenarios wherein an
    overarching assessment of model performance across diverse classes is sought,
    with no bias towards larger classes. Micro−F1= 2∗(Micro−Precision∗Micro−Recall)
    Micro−Precision+Micro−Recall (A7) where Micro−Precision= TP_total TP_total+FP_total
    Micro−Recall= TP_total TP_total+FN_total TP_total: The cumulative count of true
    positives across all classes. FP_total: The cumulative count of false positives
    across all classes. FN_total: The cumulative count of false negatives across all
    classes [28]. Macro-F1 Macro-F1 is ascertained through an initial process involving
    the independent computation of F1-scores for individual classes, followed by the
    aggregation of these class-specific F1-scores to derive an average. Every class’s
    F1-score carries identical weightage in this computation, irrespective of the
    class’s magnitude. This metric equipped an equitable assessment of the model’s
    efficacy spanning all classes. It guarantees uniform consideration to each class,
    an attribute particularly advantageous when evaluating the model’s adeptness in
    dealing with smaller classes [28]. Macro−F1= (F1_class1+F1_class2+…+F1_classn)
    n (A8) where F1_classi= 2∗(Precision _class i ∗Recall_classi) (Precision_ class
    i +Recall_classi) Precision_classi corresponds to the precision value attributed
    to class I. Recall_classi pertains to the recall value pertaining to class I.
    n denotes the total count of classes in consideration. AUC One metric to consolidate
    the ROC curve into a single metric involves the calculation of AUROC, often denoted
    as AUC. This metric carries a well-established statistical interpretation, specifically
    defined as the probability that a randomly chosen instance from a particular class
    demonstrates a lower estimated likelihood of belonging to the opposing class in
    comparison to a randomly chosen instance from the opposing class [95]. AUC= ∑
    ∞ i=1 (TPRate[i]+TPRate[i−1])∗(FPRate[i]−FPRate[i−1]) 2 (A9) True Positive Rate[i]
    designates the true positive rate, also known as sensitivity, observed at the
    i-th threshold. False Positive Rate[i] signifies the false positive rate, denoted
    as 1 minus specificity, as assessed at the i-th threshold. Convergence speed By
    having a faster convergence speed on clients, local models adapt quickly to their
    respective datasets and contribute effectively to global model improvement. During
    the assessment of model performance, it encompasses the subsequent metrics: ▪
    Training time/training duration It denotes the actual time taken by each distinct
    client to conduct localized training utilizing its local dataset. In every training
    round, each client engages in the training of a model on its local data, thereby
    strengthening the model underlying parameters. This metric encompasses the cumulative
    time needed for executing multiple localized epochs on the client dataset. This
    metric serves as an assessment of the computational exertion entailed in the process
    of enhancing the model on each client [92]. ▪ Training loss It signifies the measurement
    of the discrepancy between the model projected outcomes and the factual ground
    truth during the localized training phase executed on each client. This metric
    of loss functions as an indicator of the model’s congruence with the intended
    target results and assumes a guiding role in the optimization procedure aimed
    at reducing the variance between predictions and factual values. ▪ Training round
    (number of local training epochs) Its emphasis lies in the procedure of revising
    model parameters on a particular dataset through repeated iterations. This represents
    a foundational concept within the realm of machine learning, extending its applicability
    to both conventional training methodologies and the domain of federated training.
    ➢ System performance metrics Convergence speed The concept of convergence speed
    observed at the server level indicates the rapidity with which the amalgamated
    global model, derived from client updates, progresses toward optimal performance.
    This aspect is related to the speed at which the aggregation procedure, completed
    at the server level, combines the diverse client model updates. A rapid convergence
    speed on the server emphasizes the proficient integration of diverse client contributions.
    This issue facilitates accelerated attainment of convergence for the global model.
    It contains the following metrics: ▪ Execution Time Execution time in FL encapsulates
    the complete duration required for an entire iteration round. An iteration round
    involves multiple phases, including distributing the global model to the client,
    conducting local training on each client, aggregating model updates, and generating
    a new global model. Execution time takes into account not only the training time
    on each client but also the time needed for communication, aggregation, and synchronization
    between the central server and clients [62,96]. ▪ Iteration count It pertains
    to the frequency of iterations through which the training process is iteratively
    executed across the clients. FL encompasses the cooperative training of a model
    across a multitude of devices while upholding the data’s localization on these
    devices as opposed to its centralization. The iteration count in federated learning
    encompasses the entire cycle of communication, local training, and model aggregation
    across all client devices. It refers to the number of times this complete cycle
    is repeated. Each iteration consists of distributing the global model to clients,
    clients performing local training, aggregating model updates, and generating a
    new global model. The iteration count represents the number of times this process
    is repeated until convergence [10]. Communication efficiency Assessing communication
    efficiency involves scrutinizing metrics such as the count of communication rounds,
    the tally of parameters, and the sizes of transmitted messages. Communication
    rounds facilitate data exchange between clients and servers in a training network.
    This measure provides a quantitative assessment of models trained with data from
    different clients [35]. Computational efficiency The evaluation of computational
    efficiency encompasses the examination of metrics including the duration of training.
    This assessment pertains to the computational resources essential for model training,
    encompassing aspects such as CPU and GPU utilization, memory consumption, and
    other hardware-related considerations to handle latency arising from non-IID data
    [62]. System scalability Indicates the capacity of a model to effectively manage
    growing quantities of data, workload, or users while maintaining its performance
    at a satisfactory level. The assessment of system scalability involves the analysis
    of its efficacy across an extensive array of clients, encompassing criteria such
    as performance outcomes, overall time taken, and aggregate memory usage [52].
    References Huang, T.; Lin, W.; Wu, W.; He, L.; Li, K.; Zomaya, A. An efficiency-boosting
    client selection scheme for federated learning with fairness guarantee. IEEE Trans.
    Parallel Distrib. Syst. 2020, 32, 1552–1564. [Google Scholar] [CrossRef] Asad,
    M.; Moustafa, A.; Rabhi, F.A.; Aslam, M. THF: 3-Way hierarchical framework for
    efficient client selection and resource management in federated learning. IEEE
    Internet Things J. 2021, 9, 11085–11097. [Google Scholar] [CrossRef] Ludwig, H.;
    Baracaldo, N. Federated Learning: A Comprehensive Overview of Methods and Applications;
    Springer: Berlin/Heidelberg, Germany, 2022. [Google Scholar] Briggs, C.; Fan,
    Z.; Andras, P. Federated learning with hierarchical clustering of local updates
    to improve training on non-IID data. In Proceedings of the 2020 International
    Joint Conference on Neural Networks (IJCNN), Glasgow, UK, 19–24 July 2020; pp.
    1–9. [Google Scholar] Chen, M.; Yang, Z.; Saad, W.; Yin, C.; Poor, H.V.; Cui,
    S. A Joint learning and communications framework for federated learning over wireless
    networks. IEEE Trans. Wirel. Commun. 2020, 20, 269–283. [Google Scholar] [CrossRef]
    Soltani, B.; Haghighi, V.; Mahmood, A.; Sheng, Q.Z.; Yao, L. A survey on participant
    selection for federated learning in mobile networks. In Proceedings of the 17th
    ACM Workshop on Mobility in the Evolving Internet Architecture, Sydney, NSW, Australia,
    21 October 2022; pp. 19–24. [Google Scholar] Xu, B.; Xia, W.; Zhang, J.; Quek,
    T.Q.S.; Zhu, H. Online client scheduling for fast federated learning. IEEE Wirel.
    Commun. Lett. 2021, 10, 1434–1438. [Google Scholar] [CrossRef] Nishio, T.; Yonetani,
    R. Client selection for federated learning with heterogeneous resources in mobile
    edge. In Proceedings of the ICC 2019—2019 IEEE International Conference on Communications
    (ICC), Shanghai, China, 20–24 May 2019; pp. 1–7. [Google Scholar] Li, L.; Fan,
    Y.; Tse, M.; Lin, K.-Y. A review of applications in federated learning. Comput.
    Ind. Eng. 2020, 149, 106854. [Google Scholar] [CrossRef] Ghosh, A.; Hong, J.;
    Yin, D.; Ramchandran, K. Robust federated learning in a heterogeneous environment.
    arXiv 2019, arXiv:1906.06629. [Google Scholar] Kang, J.; Yu, R.; Huang, X.; Wu,
    M.; Maharjan, S.; Xie, S.; Zhang, Y. Blockchain for secure and efficient data
    sharing in ve-hicular edge computing and networks. IEEE Internet Things J. 2018,
    6, 4660–4670. [Google Scholar] [CrossRef] Ye, D.; Yu, R.; Pan, M.; Han, Z. Federated
    learning in vehicular edge computing: A selective model aggregation approach.
    IEEE Access 2020, 8, 23920–23935. [Google Scholar] [CrossRef] Li, Z.; He, Y.;
    Yu, H.; Kang, J.; Li, X.; Xu, Z.; Niyato, D. Data heterogeneity-robust federated
    learning via group client selection in industrial IoT. IEEE Internet Things J.
    2022, 9, 17844–17857. [Google Scholar] [CrossRef] Rahman, M.A.; Hossain, M.S.;
    Islam, M.S.; Alrajeh, N.A.; Rahman, M.A.; Hossain, M.S.; Islam, M.S.; Alrajeh,
    N.A.; Muhammad, G. Secure and Provenance Enhanced Internet of Health Things Framework:
    A Blockchain Managed Federated Learning Approach. IEEE Access 2020, 8, 205071–205087.
    [Google Scholar] [CrossRef] Lian, X.; Zhang, C.; Zhang, H.; Hsieh, C.-J.; Zhang,
    W.; Liu, J. Can decentralized algorithms outperform centralized algo-rithms? a
    case study for decentralized parallel stochastic gradient descent. Adv. Neural
    Inf. Process. Syst. 2017, 30. Available online: https://proceedings.neurips.cc/paper_files/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf
    (accessed on 11 August 2023). Smith, V.; Forte, S.; Chenxin, M.; Taka, M.; Jordan,
    M.I.; Jaggi, M. Cocoa: A general framework for communication-efficient distributed
    optimization. J. Mach. Learn. Res. 2018, 18, 230. [Google Scholar] Zhang, X.;
    Hong, M.; Dhople, S.; Yin, W.; Liu, Y. FedPD: A Federated Learning Framework With
    Adaptivity to Non-IID Data. IEEE Trans. Signal Process. 2021, 69, 6055–6070. [Google
    Scholar] [CrossRef] Lo, S.K.; Lu, Q.; Wang, C.; Paik, H.-Y.; Zhu, L. A systematic
    literature review on federated machine learning: From a software engineering perspective.
    ACM Comput. Surv. (CSUR) 2021, 54, 1–39. [Google Scholar] [CrossRef] Li, T.; Sahu,
    A.K.; Talwalkar, A.; Smith, V. Federated learning: Challenges, methods, and future
    directions. IEEE Signal Process. Mag. 2020, 37, 50–60. [Google Scholar] [CrossRef]
    Khajehali, N.; Alizadeh, S. Extract critical factors affecting the length of hospital
    stay of pneumonia patient by data mining (case study: An Iranian hospital). Artif.
    Intell. Med. 2017, 83, 2–13. [Google Scholar] [CrossRef] [PubMed] Wu, W.; He,
    L.; Lin, W.; Mao, R.; Maple, C.; Jarvis, S.A. SAFA: A Semi-Asynchronous Protocol
    for Fast Federated Learning With Low Overhead. IEEE Trans. Comput. 2021, 70, 655–668.
    [Google Scholar] [CrossRef] Zhang, X.; Chen, X.; Liu, J.K.; Xiang, Y. DeepPAR
    and DeepDPA: Privacy Preserving and Asynchronous Deep Learning for Industrial
    IoT. IEEE Trans. Ind. Inform. 2019, 16, 2081–2090. [Google Scholar] [CrossRef]
    Hu, B.; Gao, Y.; Liu, L.; Ma, H. Federated Region-Learning: An Edge Computing
    Based Framework for Urban Environment Sensing. In Proceedings of the 2018 IEEE
    global communications conference (Globecom), Abu Dhabi, United Arab Emirates,
    9–13 December 2018; pp. 1–7. [Google Scholar] Zhang, T.; Gao, L.; He, C.; Zhang,
    M.; Krishnamachari, B.; Avestimehr, A.S. Federated Learning for the Internet of
    Things: Applications, Challenges, and Opportunities. IEEE Internet Things Mag.
    2022, 5, 24–29. [Google Scholar] [CrossRef] Cao, L. Beyond iid: Non-iid thinking,
    informatics, and learning. IEEE Intell. Syst. 2022, 37, 5–17. [Google Scholar]
    [CrossRef] Yu, L.; Albelaihi, R.; Sun, X.; Ansari, N.; Devetsikiotis, M. Jointly
    Optimizing Client Selection and Resource Management in Wireless Federated Learning
    for Internet of Things. IEEE Internet Things J. 2021, 9, 4385–4395. [Google Scholar]
    [CrossRef] Liu, Y.; Yuan, X.; Xiong, Z.; Kang, J.; Wang, X.; Niyato, D. Federated
    learning for 6G communications: Challenges, methods, and future directions. China
    Commun. 2020, 17, 105–118. [Google Scholar] [CrossRef] Sattler, F.; Mu, K.-R.;
    Samek, W. Clustered federated learning: Model-agnostic distributed multitask optimization
    under privacy constraints. IEEE Trans. Neural Netw. Learn. Syst. 2020, 32, 3710–3722.
    [Google Scholar] [CrossRef] [PubMed] Jiang, Y.; Kone, C.J.; Rush, K.; Kannan,
    S. Improving federated learning personalization via model agnostic meta-learning.
    arXiv 2019, arXiv:1909.12488. [Google Scholar] Liu, L.; Zhang, J.; Song, S.; Letaief,
    K.B. Client-edge-cloud hierarchical federated learning. In Proceedings of the
    ICC 2020 IEEE International Conference on Communications (ICC), Dublin, Ireland,
    7–11 June 2020; pp. 1–6. [Google Scholar] Ghosh, A.; Chung, J.; Yin, D.; Ramchandran,
    K. An Efficient Framework for Clustered Federated Learning. Adv. Neural Inf. Process.
    Syst. 2020, 33, 19586–19597. [Google Scholar] [CrossRef] Wen, J.; Zhang, Z.; Lan,
    Y.; Cui, Z.; Cai, J.; Zhang, W. A survey on federated learning: Challenges and
    applications. Int. J. Mach. Learn. Cybern. 2022, 14, 513–535. [Google Scholar]
    [CrossRef] Ji, S.; Jiang, W.; Walid, A.; Li, X. Dynamic Sampling and Selective
    Masking for Communication-Efficient Federated Learning. IEEE Intell. Syst. 2021,
    37, 27–34. [Google Scholar] [CrossRef] Lin, W.; Xu, Y.; Liu, B.; Li, D.; Huang,
    T.; Shi, F. Contribution-based Federated Learning client selection. Int. J. Intell.
    Syst. 2022, 37, 7235–7260. [Google Scholar] [CrossRef] Huang, T.; Lin, W.; Shen,
    L.; Li, K.; Zomaya, A.Y. Stochastic Client Selection for Federated Learning With
    Volatile Clients. IEEE Internet Things J. 2022, 9, 20055–20070. [Google Scholar]
    [CrossRef] Zhang, C.; Xie, Y.; Bai, H.; Yu, B.; Li, W.; Gao, Y. A survey on federated
    learning. Knowl.-Based Syst. 2021, 216, 106775. [Google Scholar] [CrossRef] Amiri,
    M.M.; Gu, D. Federated learning over wireless fading channels. IEEE Trans. Wirel.
    Commun. 2020, 19, 3546–3557. [Google Scholar] [CrossRef] Banabilah, S.; Aloqaily,
    M.; Alsayed, E.; Malik, N.; Jararweh, Y. Federated learning review: Fundamentals,
    enabling tech-nologies, and future applications. Inf. Process. Manag. 2022, 59,
    103061. [Google Scholar] [CrossRef] Li, T.; Sahu, A.K.; Zaheer, M.; Sanjabi, M.;
    Talwalkar, A.; Smith, V. Federated optimization in heterogeneous networks. Proc.
    Mach. Learn. Syst. 2020, 2, 429–450. [Google Scholar] Xu, J.; Wang, H. Client
    Selection and Bandwidth Allocation in Wireless Federated Learning Networks: A
    Long-Term Perspective. IEEE Trans. Wirel. Commun. 2020, 20, 1188–1200. [Google
    Scholar] [CrossRef] Fu, L.; Zhang, H.; Gao, G.; Wang, H.; Zhang, M.; Liu, X. Client
    selection in federated learning: Principles, challenges, and opportunities. arXiv
    2022, arXiv:2211.01549. [Google Scholar] [CrossRef] Saha, R.; Misra, S.; Chakraborty,
    A.; Chatterjee, C.; Deb, P.K. Data-centric client selection for federated learning
    over dis-tributed edge networks. IEEE Trans. Parallel Distrib. Syst. 2022, 34,
    675–686. [Google Scholar] [CrossRef] Telikani, A.; Rossi, M.; Khajehali, N.; Renzi,
    M. Pumps-as-Turbines’ (PaTs) performance prediction improvement using evolutionary
    artificial neural networks. Appl. Energy 2023, 330, 120316. [Google Scholar] [CrossRef]
    Khajehali, N.; Khajehali, Z.; Tarokh, M.J. The prediction of mortality influential
    variables in an intensive care unit: A case study. Pers. Ubiquitous Comput. 2021,
    27, 203–219. [Google Scholar] [CrossRef] Li, X.; Qu, Z.; Tang, B.; Lu, Z. FedLGA:
    Toward System-Heterogeneity of Federated Learning via Local Gradient Approximation.
    IEEE Trans. Cybern. 2023, 1–14. [Google Scholar] [CrossRef] Mothukuri, V.; Khare,
    P.; Parizi, R.M.; Pouriyeh, S.; Dehghantanha, A.; Srivastava, G. Federated-Learning-Based
    Anomaly Detection for IoT Security Attacks. IEEE Internet Things J. 2021, 9, 2545–2554.
    [Google Scholar] [CrossRef] Le, J.; Lei, X.; Mu, N.; Zhang, H.; Zeng, K.; Liao,
    X. Federated Continuous Learning With Broad Network Architecture. IEEE Trans.
    Cybern. 2021, 51, 3874–3888. [Google Scholar] [CrossRef] Zhang, Y.; Yang, Q.;
    An, D.; Li, D.; Wu, Z. Multistep Multiagent Reinforcement Learning for Optimal
    Energy Schedule Strategy of Charging Stations in Smart Grid. IEEE Trans. Cybern.
    2022, 53, 4292–4305. [Google Scholar] [CrossRef] [PubMed] Dennis, D.K.; Li, T.;
    Smith, V. Heterogeneity for the win: One-shot federated clustering. In Proceedings
    of the 38th International Conference on Machine Learning, Virtual, 18–24 July
    2021; pp. 2611–2620. [Google Scholar] Wang, S.; Tuor, T.; Salonidis, T.; Leung,
    K.K.; Makaya, C.; He, T.; Chan, K. Adaptive federated learning in re-source-constrained
    edge Computing systems. IEEE J. Sel. Areas Commun. 2019, 37, 1205–1221. [Google
    Scholar] [CrossRef] Mills, J.; Hu, J.; Min, G. Communication-Efficient Federated
    Learning for Wireless Edge Intelligence in IoT. IEEE Internet Things J. 2019,
    7, 5986–5994. [Google Scholar] [CrossRef] Deng, Y.; Lyu, F.; Ren, J.; Wu, H.;
    Zhou, Y.; Zhang, Y.; Shen, X. AUCTION: Automated and Quality-Aware Client Selection
    Framework for Efficient Federated Learning. IEEE Trans. Parallel Distrib. Syst.
    2021, 33, 1996–2009. [Google Scholar] [CrossRef] Nguyen, D.C.; Pham, Q.-V.; Pathirana,
    P.N.; Ding, M.; Seneviratne, A.; Lin, Z.; Dobre, O.; Hwang, W.-J. Federated Learning
    for Smart Healthcare: A Survey. ACM Comput. Surv. 2022, 55, 1–37. [Google Scholar]
    [CrossRef] Antunes, R.S.; Andre da Costa, C.; Kuderle, A.; Yari, I.A.; Eskofier,
    B. Federated learning for healthcare: Systematic review and architecture proposal.
    ACM Trans. Intell. Syst. Technol. TIST 2022, 13, 1–23. [Google Scholar] [CrossRef]
    Campos, E.M.; Saura, P.F.; Gonza, A.; Herna, J.L.; Bernabe, J.B.; Baldini, G.;
    Skarmeta, A. Evaluating federated learning for intrusion detection on the internet
    of things: Review and challenges. Comput. Netw. 2022, 203, 108661. [Google Scholar]
    [CrossRef] Xie, C.; Koyejo, S.; Gupta, I. Asynchronous federated optimization.
    arXiv 2019, arXiv:1903.03934. [Google Scholar] Fahmideh, M.; Grundy, J.; Ahmad,
    A.; Shen, J.; Yan, J.; Mougouei, D.; Wang, P.; Ghose, A.; Gunawardana, A.; Aickelin,
    U.; et al. Engineering blockchain-based software systems: Foundations, survey,
    and future directions. ACM Comput. Surv. 2022, 55, 1–44. [Google Scholar] [CrossRef]
    Roy, A.G.; Siddiqui, S.; Po, S.; Navab, N.; Wachinger, C. Braintorrent: A peer-to-peer
    environment for decentralized federated learning. arXiv 2019, arXiv:1905.06731.
    [Google Scholar] Cortes, C.; Lawrence, N.; Lee, D.; Sugiyama, M.; Garnett, R.
    Advances in neural information processing systems 28. In Proceedings of the 29th
    Annual Conference on Neural Information Processing Systems, Montreal, QC, Canada,
    7–12 December 2015. [Google Scholar] Yang, H.H.; Liu, Z.; Quek, T.Q.S.; Poor,
    H.V. Scheduling Policies for Federated Learning in Wireless Networks. IEEE Trans.
    Commun. 2019, 68, 317–333. [Google Scholar] [CrossRef] Triastcyn, A.; Faltings,
    B. Federated Generative Privacy. IEEE Intell. Syst. 2020, 35, 50–57. [Google Scholar]
    [CrossRef] Savazzi, S.; Nicoli, M.; Rampa, V. Federated Learning with Cooperating
    Devices: A Consensus Approach for Massive IoT Networks. IEEE Internet Things J.
    2020, 7, 4641–4654. [Google Scholar] [CrossRef] Yang, K.; Jiang, T.; Shi, Y.;
    Ding, Z. Federated Learning via Over-the-Air Computation. IEEE Trans. Wirel. Commun.
    2020, 19, 2022–2035. [Google Scholar] [CrossRef] Sattler, F.; Wiedemann, S.; Mu,
    K.-R.; Samek, W. Robust and communication-efficient federated learning from non-iid
    data. IEEE Trans. Neural Netw. Learn. Syst. 2019, 31, 3400–3413. [Google Scholar]
    [CrossRef] [PubMed] Song, Z.; Sun, H.; Yang, H.H.; Wang, X.; Zhang, Y.; Quek,
    T.Q.S. Reputation-Based Federated Learning for Secure Wireless Networks. IEEE
    Internet Things J. 2021, 9, 1212–1226. [Google Scholar] [CrossRef] Chen, Y.; Ning,
    Y.; Slawski, M.; Rangwala, H. Asynchronous Online Federated Learning for Edge
    Devices with Non-IID Data. In Proceedings of the 2020 IEEE International Conference
    on Big Data, Atlanta, GA, USA, 10–13 December 2020; pp. 15–24. [Google Scholar]
    Wei, X.; Li, Q.; Liu, Y.; Yu, H.; Chen, T.; Yang, Q. Multi-Agent Visualization
    for Explaining Federated Learning. IJCAI 2019, 6572–6574. Available online: https://www.ijcai.org/proceedings/2019/0960.pdf
    (accessed on 11 August 2023). Anh, T.T.; Luong, N.C.; Niyato, D.; Kim, D.I.; Wang,
    L.-C. Efficient Training Management for Mobile Crowd-Machine Learning: A Deep
    Reinforcement Learning Approach. IEEE Wirel. Commun. Lett. 2019, 8, 1345–1348.
    [Google Scholar] [CrossRef] Wang, G. Interpret federated learning with shapely
    values. arXiv 2019, arXiv:1905.04519. [Google Scholar] Yao, X.; Huang, T.; Wu,
    C.; Zhang, R.; Sun, L. Towards Faster and Better Federated Learning: A Feature
    Fusion Approach. In Proceedings of the 2019 IEEE International Conference on Image
    Processing (ICIP), Taipei, Taiwan, 22–25 September 2019; pp. 175–179. [Google
    Scholar] Sarikaya, Y.; Ercetin, O. Motivating Workers in Federated Learning: A
    Stackelberg Game Perspective. IEEE Netw. Lett. 2019, 2, 23–27. [Google Scholar]
    [CrossRef] Zhan, Y.; Li, P.; Qu, Z.; Zeng, D.; Guo, S. A Learning-Based Incentive
    Mechanism for Federated Learning. IEEE Internet Things J. 2020, 7, 6360–6368.
    [Google Scholar] [CrossRef] Zhang, W.; Lu, Q.; Yu, Q.; Li, Z.; Liu, Y.; Lo, S.K.;
    Chen, S.; Xu, X.; Zhu, L. Blockchain-Based Federated Learning for Device Failure
    Detection in Industrial IoT. IEEE Internet Things J. 2020, 8, 5926–5937. [Google
    Scholar] [CrossRef] Luping, W.; Wei, W.; Bo, L. Cmfl: Mitigating communication
    overhead for federated learning. In Proceedings of the IEEE 39th International
    Conference on Distributed Computing Systems (ICDCS), Dallas, TX, USA, 7–10 July
    2019; pp. 954–964. [Google Scholar] Bao, X.; Su, C.; Xiong, Y.; Huang, W.; Hu,
    Y. FL chain: A blockchain for auditable federated learning with trust and in-centive.
    In Proceedings of the 2019 5th International Conference on Big Data Computing
    and Communications (BIGCOM), QingDao, China, 9–11 August 2019; pp. 151–159. [Google
    Scholar] Zhao, Y.; Chen, J.; Zhang, J.; Wu, D.; Teng, J.; Yu, S. PDGAN: A Novel
    Poisoning Defense Method in Federated Learning Using Generative Adversarial Network.
    In Algorithms and Architectures for Parallel, Processing of the 19th International
    Conference, ICA3PP 2019 (Proceedings, Part I 19), Melbourne, VIC, Australia, 9–11
    December 2019; Springer: Cham, Switzerland, 2020; pp. 595–609. [Google Scholar]
    [CrossRef] McMahan, B.; Moore, E.; Ramage, D.; Hampson, S.; Arcas, B.A.Y. Communication-efficient
    learning of deep networks from decentralized data. In Proceedings of the 20th
    Artificial Intelligence and Statistics, Fort Lauderdale, FL, USA, 20–22 April
    2017; pp. 1273–1282. [Google Scholar] Hsu, T.-M.H.; Qi, H.; Brown, M. Measuring
    the effects of non-identical data distribution for federated visual classification.
    arXiv 2019, arXiv:1909.06335. [Google Scholar] Zhan, Y.; Li, P.; Guo, S. Experience-Driven
    Computational Resource Allocation of Federated Learning by Deep Reinforcement
    Learning. In Proceedings of the 2020 IEEE International Parallel and Distributed
    Processing Symposium (IPDPS), New Orleans, LA, USA, 18–22 May 2020; pp. 234–243.
    [Google Scholar] Kim, H.; Park, J.; Bennis, M.; Kim, S.-L. Block chained on-device
    federated learning. IEEE Commun. Lett. 2019, 24, 1279–1283. [Google Scholar] [CrossRef]
    Lu, Y.; Huang, X.; Dai, Y.; Maharjan, S.; Zhang, Y. Blockchain and Federated Learning
    for Privacy-Preserved Data Sharing in Industrial IoT. IEEE Trans. Ind. Inform.
    2019, 16, 4177–4186. [Google Scholar] [CrossRef] Weng, J.; Weng, J.; Zhang, J.;
    Li, M.; Zhang, Y.; Luo, W. DeepChain: Auditable and Privacy-Preserving Deep Learning
    with Blockchain-based Incentive. IEEE Trans. Dependable Secur. Comput. 2019, 18,
    2438–2455. [Google Scholar] [CrossRef] Shayan, M.; Fung, C.; Yoon, C.J.; Beschastnikh,
    I. Biscotti: A ledger for private and secure peer-to-peer machine learning. arXiv
    2018, arXiv:1811.09904. [Google Scholar] Shi, F.; Hu, C.; Lin, W.; Fan, L.; Huang,
    T.; Wu, W. VFedCS: Optimizing Client Selection for Volatile Federated Learning.
    IEEE Internet Things J. 2022, 9, 24995–25010. [Google Scholar] [CrossRef] Mohammed,
    I.; Tabatabai, S.; Al-Fuqaha, A.; El Bouanani, F.; Qadir, J.; Qolomany, B.; Guizani,
    M. Budgeted Online Selection of Candidate IoT Clients to Participate in Federated
    Learning. IEEE Internet Things J. 2020, 8, 5938–5952. [Google Scholar] [CrossRef]
    Shi, W.; Zhou, S.; Niu, Z. Device Scheduling with Fast Convergence for Wireless
    Federated Learning. In Proceedings of the ICC IEEE International Conference on
    Communications (ICC), Dublin, Ireland, 7–11 June 2020; pp. 1–6. [Google Scholar]
    Amiri, M.M.; Gu, D.; Kulkarni, S.R.; Poor, H.V. Convergence of update aware device
    scheduling for federated learning at the wireless edge. IEEE Trans. Wirel. Commun.
    2021, 20, 3643–3658. [Google Scholar] [CrossRef] Tan, X.; Ng, W.C.; Lim, W.Y.B.;
    Xiong, Z.; Niyato, D.; Yu, H. Reputation-Aware Federated Learning Client Selection
    based on Stochastic Integer Programming. IEEE Trans. Big Data 2022, 1–12. [Google
    Scholar] [CrossRef] Long, G.; Xie, M.; Shen, T.; Zhou, T.; Wang, X.; Jiang, J.
    Multi-center federated learning: Clients clustering for better per-sonalization.
    World Wide Web 2023, 26, 481–500. [Google Scholar] [CrossRef] Wang, S.; Chang,
    T.-H. Federated Matrix Factorization: Algorithm Design and Application to Data
    Clustering. IEEE Trans. Signal Process. 2022, 70, 1625–1640. [Google Scholar]
    [CrossRef] Qu, Z.; Duan, R.; Chen, L.; Xu, J.; Lu, Z.; Liu, Y. Context-Aware Online
    Client Selection for Hierarchical Federated Learning. IEEE Trans. Parallel Distrib.
    Syst. 2022, 33, 4353–4367. [Google Scholar] [CrossRef] Zhu, H.; Zhou, Y.; Qian,
    H.; Shi, Y.; Chen, X.; Yang, Y. Online Client Selection for Asynchronous Federated
    Learning With Fairness Consideration. IEEE Trans. Wirel. Commun. 2022, 22, 2493–2506.
    [Google Scholar] [CrossRef] Xia, W.; Quek, T.Q.S.; Guo, K.; Wen, W.; Yang, H.H.;
    Zhu, H. Multi-Armed Bandit-Based Client Scheduling for Federated Learning. IEEE
    Trans. Wirel. Commun. 2020, 19, 7108–7123. [Google Scholar] [CrossRef] Han, Y.;
    Li, D.; Qi, H.; Ren, J.; Wang, X. Federated learning-based computation offloading
    optimization in edge computing-supported internet of things. In Proceedings of
    the ACM Turing Celebration Conference-China, Chengdu China, 17–19 May 2019; pp.
    1–5. [Google Scholar] Tan, A.Z.; Yu, H.; Cui, L.; Yang, Q. Towards personalized
    federated learning. IEEE Trans. Neural Netw. Learn. Syst. 2022, 1–17. [Google
    Scholar] [CrossRef] Nilsson, A.; Smith, S. Evaluating the Performance of Federated
    Learning. Master’s Thesis, University of Gothenburg, Göteborg, Sweden, 2018. [Google
    Scholar]          Disclaimer/Publisher’s Note: The statements, opinions and data
    contained in all publications are solely those of the individual author(s) and
    contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
    disclaim responsibility for any injury to people or property resulting from any
    ideas, methods, instructions or products referred to in the content.  © 2023 by
    the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
    article distributed under the terms and conditions of the Creative Commons Attribution
    (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite
    MDPI and ACS Style Khajehali, N.; Yan, J.; Chow, Y.-W.; Fahmideh, M. A Comprehensive
    Overview of IoT-Based Federated Learning: Focusing on Client Selection Methods.
    Sensors 2023, 23, 7235. https://doi.org/10.3390/s23167235 AMA Style Khajehali
    N, Yan J, Chow Y-W, Fahmideh M. A Comprehensive Overview of IoT-Based Federated
    Learning: Focusing on Client Selection Methods. Sensors. 2023; 23(16):7235. https://doi.org/10.3390/s23167235
    Chicago/Turabian Style Khajehali, Naghmeh, Jun Yan, Yang-Wai Chow, and Mahdi Fahmideh.
    2023. \"A Comprehensive Overview of IoT-Based Federated Learning: Focusing on
    Client Selection Methods\" Sensors 23, no. 16: 7235. https://doi.org/10.3390/s23167235
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   1
    Scopus   1 Google Scholar   [click to view] Article Access Statistics Article
    access statistics Article Views 7. Jan 17. Jan 27. Jan 6. Feb 16. Feb 26. Feb
    7. Mar 17. Mar 27. Mar 0 500 1000 1500 2000 2500 For more information on the journal
    statistics, click here. Multiple requests from the same IP address are counted
    as one view.   Sensors, EISSN 1424-8220, Published by MDPI RSS Content Alert Further
    Information Article Processing Charges Pay an Invoice Open Access Policy Contact
    MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians
    For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum
    MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series
    Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications
    and newsletters from MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel,
    Switzerland) unless otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Comprehensive Overview of IoT-Based Federated Learning: Focusing on Client
    Selection Methods'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kunze J.E.
  - Mietzel T.
  - Freudenberg B.
  - Niemann A.
  citation_count: '0'
  description: Experience from more than one year of operation shows that IoT sensors
    for water level detection are capable of reliably detecting flood events. Compared
    to classic hydrometric gauges only minor measurement deviations and individual
    outliers occur. To address the time-consuming manual verification of data as the
    number of sensors increases, a toolbox for automated data quality assurance was
    created. It includes statistical and AI based methods and is capable of automatically
    correcting measurement errors of the IoT sensors.
  doi: 10.1007/s35147-023-1870-y
  full_citation: '>'
  full_text: '>

    "Continue with ads … Visit springerprofessional.de with ads and tracking enabled.
    You can revoke your consent at any time through our privacy policy.1 Accept and
    continue More information on ads and tracking is available in our privacy policy,
    the list of our partners and the privacy center. … or with contentpass Access
    springerprofessional.de and over 400 other websites as e.g. wirtschaftslexikon.gabler.de
    and gabler-banklexikon.de without banner ads, personalized tracking and video
    ads for only 3,99 € per month. Ad-free for 3,99 € per month Already a contentpass
    subscriber? Log in here 1This is tracking: through Information collected on your
    device (for example cookies) we and our up to 141 partners can modify our advertisements
    and content based on your user profile and measure the performance of our advertisements
    and content. We analyze the data on user behavior and preferences in order to
    optimise content and advertisements. Store and/or access information on a device
    Select basic ads Create a personalized ads profile Select personalized ads Create
    a personalized content profile Select personalized content Measure ad performance
    Measure content performance Apply market research to generate audience insights
    Develop and improve products Use limited data to select content Use precise geolocation
    data Actively scan device characteristics for identification Ensure security,
    prevent fraud, and debug Technically deliver ads or content Match and combine
    offline data sources Link different devices Receive and use automatically-sent
    device characteristics for identification Imprint Terms of Service Data Privacy
    Contact Further Information contentpass FAQ Menü Suche Anmelden MyJournal Alert
    Aktivieren Sie jetzt einen Alert für WASSERWIRTSCHAFT und erhalten Sie eine E-Mail
    zu jeder neuen Ausgabe mit einer Übersicht und direkter Verlinkung aller Inhalte.
    Alert aktivieren WASSERWIRTSCHAFT ERSCHIENEN IN: 01.08.2023 | Praxis Wege zur
    Echtzeitbewirtschaftung: Erfahrungen mit IoT-Sensorik in bestehenden Messnetzen
    verfasst von: Jan Erik Kunze, M. Sc., Dr.-Ing. Thorsten Mietzel, Benjamin Freudenberg,
    M. Sc., Prof. Dr.-Ing. André Niemann Erschienen in: WASSERWIRTSCHAFT | Ausgabe
    7-8/2023 Einloggen information Jetzt neu: KI-gestützte Suche! Markieren Sie Textabschnitte,
    um KI-gestützt weitere passende Inhalte zu finden. powered by Aktivieren Sie unsere
    intelligente Suche um passende Fachinhalte oder Patente zu finden. search-config
    KI-gestützte Suche Aus Zusammenfassung Erfahrungen aus mehr als einem Jahr Betrieb
    zeigen, dass IoT-Sensoren zur Wasserstandserfassung in der Lage sind, Hochwasserereignisse
    zuverlässig aufzuzeichnen. Gegenüber klassischen hydrometrischen Pegeln treten
    nur geringe Messabweichungen auf. Um einer zeitaufwändigen manuellen Überprüfung
    der Daten bei zunehmender Anzahl von Sensoren zu begegnen, wurde eine Toolbox
    zur automatisierten Datenqualitätssicherung erstellt. Sie umfasst statistische
    sowie KI-gestützte Verfahren und ist in der Lage, Messfehler der IoT-Sensoren
    automatisiert zu korrigieren. Anzeige Vorheriger Artikel Qualitative Hochwasservorhersage
    in Lenzkirch auf Basis von Bodenfeuchte- und Pegeldaten Nächster Artikel Autonomous
    Vehicles in der Hydrometrie - Ein Erfahrungsbericht Bitte loggen Sie sich ein,
    um Zugang zu Ihrer Lizenz zu erhalten. Jetzt einloggen Kostenlos registrieren
    Sie haben noch keine Lizenz? Dann Informieren Sie sich jetzt über unsere Produkte:
    Springer Professional \"Wirtschaft+Technik\" Online-Abonnement Mit Springer Professional
    \"Wirtschaft+Technik\" erhalten Sie Zugriff auf: über 102.000 Bücher über 537
    Zeitschriften aus folgenden Fachgebieten: Automobil + Motoren Bauwesen + Immobilien
    Business IT + Informatik Elektrotechnik + Elektronik Energie + Nachhaltigkeit
    Finance + Banking Management + Führung Marketing + Vertrieb Maschinenbau + Werkstoffe
    Versicherung + Risiko Jetzt Wissensvorsprung sichern! Jetzt informieren WasserWirtschaft
    Die Zeitschrift Zeitschrift WasserWirtschaft ist das Fachmagazin für Wasser- und
    Umwelttechnologie. 10x im Jahr bietet sie wissenschaftlich fundierte Informationen
    über u.a. Wasserbau- und Wasserkraft, Talsperren ... Jetzt informieren Springer
    Professional \"Technik\" Online-Abonnement Mit Springer Professional \"Technik\"
    erhalten Sie Zugriff auf: über 67.000 Bücher über 390 Zeitschriften aus folgenden
    Fachgebieten: Automobil + Motoren Bauwesen + Immobilien Business IT + Informatik
    Elektrotechnik + Elektronik Energie + Nachhaltigkeit Maschinenbau + Werkstoffe      Jetzt
    Wissensvorsprung sichern! Jetzt informieren Literatur Metadaten       Weitere
    Artikel der Ausgabe 7-8/2023 Zur Ausgabe Praxis KI-basiertes Vorhersagemodell
    für Kürzestfrist-Vorhersagen von Starkregen Praxis Grundinstandsetzung der linken
    Schleusenkammer der Schleuse Lauffen Praxis Die Konzeption von Pegelnetzen - Anthropogenes
    Erfordernis versus Prozesskenntnis Praxis Verdichtung des Pegelmessnetzes im Emscher-
    und Lippe-Einzugsgebiet Praxis Autonomous Vehicles in der Hydrometrie - Ein Erfahrungsbericht
    Wasser und Recht Wasser und Recht Firmeneintrag (ANZEIGE) LK Metallwaren GmbH
    - Wasseraufbereitung Firmendetails sehen » zur Fachgebietsseite Energie + Nachhaltigkeit
    Über uns: In eigener Sache Das Team Redaktionelles Leitbild Hilfe Referenzen Unsere
    Produkte: Einzelzugang Zugang für Unternehmen PatentFit MyAlerts Professional
    Book Archive MyNewsletter Carl Hanser Verlag - Bücher Rechtliche Informationen:
    Impressum AGB Datenschutzerklärung Cookies Cookies verwalten Verträge hier kündigen
    Zahlungsarten Weiterführende Links: RSS-Feeds Social Media Mediadaten Corporate
    Solutions Whitepaper Gabler Wirtschaftslexikon Gabler Banklexikon Springer Nature
    Logo © Springer Fachmedien Wiesbaden GmbH Version: 0.3538.0"'
  inline_citation: '>'
  journal: WasserWirtschaft
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Pathways to real-time management: experiences with IoT sensors in established
    measurement networks'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
