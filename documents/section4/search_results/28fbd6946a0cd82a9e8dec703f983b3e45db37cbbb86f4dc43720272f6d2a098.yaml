- analysis: '>'
  authors:
  - Rocha A.
  - Monteiro M.
  - Mattos C.
  - Dias M.
  - Soares J.
  - Magalhães R.
  - Macedo J.
  citation_count: '0'
  description: The Internet of Things (IoT) consists of heterogeneous devices such
    as wearables and monitoring devices that collect data to provide autonomous decision-making
    and smart applications. IoT technologies, such as the Internet of Medical Things
    (IoMT), have become gradually popular for medical purposes, combining IoT and
    medical devices to achieve good health and well-being. However, IoMT devices are
    often tight and have resource constraints, which leads to limited local data processing
    in the device. Edge computing provides access to additional computation and storage
    resources for IoMT devices, bringing intelligent processing closer to the data
    sources. This technology opens up great possibilities for IoMT applications, especially
    when combined with Artificial Intelligence (AI). Edge AI runs AI computations
    close to the IoT devices and users instead of centralized services such as cloud
    servers. This paper investigates the potential of Edge AI and IoMT. In this sense,
    this survey is the first work to further detail Edge AI and Machine Learning Operations
    in IoMT domains and wearable technology, thus contributing to the literature by
    comprehensively exploring the potential of ML strategies and operations at the
    network's edge and intelligence distribution. This study also presents a case
    study on heart anomaly detection.
  doi: 10.1016/j.compeleceng.2024.109202
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Comparison with other surveys 3.
    Edge AI for IoMT 4. Case study 5. Final considerations CRediT authorship contribution
    statement Declaration of competing interest Acknowledgments Data availability
    References Show full outline Figures (5) Tables (2) Table 1 Table 2 Computers
    and Electrical Engineering Volume 116, May 2024, 109202 Edge AI for Internet of
    Medical Things: A literature review☆ Author links open overlay panel Atslands
    Rocha a 1, Matheus Monteiro a 2, César Mattos b 1, Madson Dias b 3, Jorge Soares
    c 1, Regis Magalhães d 1, José Macedo b 1 Show more Add to Mendeley Share Cite
    https://doi.org/10.1016/j.compeleceng.2024.109202 Get rights and content Abstract
    The Internet of Things (IoT) consists of heterogeneous devices such as wearables
    and monitoring devices that collect data to provide autonomous decision-making
    and smart applications. IoT technologies, such as the Internet of Medical Things
    (IoMT), have become gradually popular for medical purposes, combining IoT and
    medical devices to achieve good health and well-being. However, IoMT devices are
    often tight and have resource constraints, which leads to limited local data processing
    in the device. Edge computing provides access to additional computation and storage
    resources for IoMT devices, bringing intelligent processing closer to the data
    sources. This technology opens up great possibilities for IoMT applications, especially
    when combined with Artificial Intelligence (AI). Edge AI runs AI computations
    close to the IoT devices and users instead of centralized services such as cloud
    servers. This paper investigates the potential of Edge AI and IoMT. In this sense,
    this survey is the first work to further detail Edge AI and Machine Learning Operations
    in IoMT domains and wearable technology, thus contributing to the literature by
    comprehensively exploring the potential of ML strategies and operations at the
    network’s edge and intelligence distribution. This study also presents a case
    study on heart anomaly detection. Previous article in issue Next article in issue
    Keywords Edge intelligenceEdge computingArtificial intelligenceMachine learning
    operationInternet of medical thingsSmart health 1. Introduction The Internet of
    Things (IoT) enables real-world devices to be connected to the Internet to monitor
    and control their status and the surrounding environment for automatized tasks
    and remote data access. By using embedded sensors, the smart devices monitor variables
    such as images, chemicals, and biosignals. Based on events and decisions, actuators
    such as electric motors and hydraulic cylinders can perform actions. IoT is a
    technology widely applied in several domains, such as agriculture and industry,
    in order to facilitate new services and improve results and production. IoT has
    been a disruptive technology providing valuable practices in the health domain
    resulting in innovative medical devices and new applications such as user attendance,
    emergency home care, fall detection, enhanced medical diagnosis, and activity
    recognition. Thus, IoT has extended to the Internet of Medical Things (IoMT).
    IoMT combines IoT technology and medical devices to achieve good and personalized
    health and well-being for society. Health is essential for human beings to lead
    a productive lifestyle. Medical devices of IoMT include embedded sensors such
    as electrocardiogram (ECG), body temperature, blood pressure (BP), glucose receptors,
    and mobile devices and wearables. Different types of wearables can be accessories,
    clothing, implanted devices, and even skin tattoos. According to Statista (The
    Statistics Portal for Market Data), the amount of data generated by all connected
    smart devices worldwide is forecasted to reach 79.4 zettabytes by 2025. There
    is a requirement for efficient data analysis methods due to the increasing volume
    and velocity of data generated by IoT devices. Currently, artificial intelligence
    (AI) techniques process data collected from IoMT devices, aiding in decision-making.
    This use of AI empowers devices to address problems and make choices in ways that
    resemble human thought processes. However, IoMT devices often lead to limited
    local data processing because they have resource constraints (CPU, memory). At
    the same time, IoMT applications often require a low-time response and handle
    sensitive data related to the individual’s health. Thus, they usually rely on
    different nodes to process their data using AI models. However, despite promising
    results, several approaches are reliant on cloud-based architectures. Cloud computing,
    a paradigm defined in 1997 [1], provides high-resource and on-demand power capacity
    and storage availability. However, this paradigm faces latency, energy, bandwidth,
    and privacy issues, particularly in health applications dealing with sensitive
    data. Since its emergence in 2009 [1], Edge Computing has addressed the previous
    challenges by providing computational resources in edge nodes, strategically located
    between IoMT devices and cloud data centers. This arrangement allows IoMT devices
    to access enhanced computation and storage for more sophisticated data processing,
    bridging the gap between the data source and the cloud. Examples of edge nodes
    are smartphones, gateways, single-board computers, drones, and servers. Edge computing
    promises low latency and bandwidth reduction since the edge nodes are near the
    data source devices. This aspect also facilitates acquiring context and location
    awareness. Other benefits are security and privacy because data is processed in
    edge nodes. Unlike cloud data centers, edge environments are distributed and have
    no single point of failure. Edge computing differs in terms of paradigms in its
    diverse distributed computing environment, such as Micro Data Centers (Microsoft),
    Cloudlets, Fog (Cisco), and Multi-access Edge Computing (European Telecommunications
    Standards Institute, ETSI). It is important to note these differences and understand
    how they can benefit various computing needs besides the characteristics, applications,
    and technologies associated with each paradigm. For instance, micro data centers
    refer to distributed clouds for mobile users that need a global hardware infrastructure
    with high-memory servers. Cloudlet is a small data center with enhanced mobility
    for casual and transient mobile device users. Fog Computing is a regular solution
    for IoT applications, which can use a gateway or powerful computing devices (servers,
    desktops) near the IoT devices to distribute computing and connect to the cloud.
    Mobile Edge Computing (MEC) was initially projected as remotely distributed data
    centers to meet the massive and increasing demand for mobile device processing.
    ETSI renamed MEC to Multi-access Edge Computing (MEC) to include non-mobile devices
    in wireless networks (Wi-Fi, 3G-6G) as well. Edge computing is a complementary
    paradigm of cloud computing. Their corroborative relationship originated the term
    Edge-to-Cloud Continuum [2]. In general, the Edge-to-cloud Continuum is organized
    in a three-level hierarchy: (i) source data nodes (IoT devices) responsible for
    collecting data; (ii) edge nodes responsible for processing data and making decisions;
    and (iii) cloud nodes responsible for processing data in a holistic view or when
    it is sophisticated and unfeasible to perform in edge nodes. This technology opens
    up possibilities for IoMT applications, especially when combined with AI. Artificial
    Intelligence at the Edge (Edge AI) [1], also known as Edge Intelligence (EI),
    emerged in 2021. It combines edge computing and artificial intelligence to distribute
    intelligence among devices. Edge AI is a promising solution since IoMT data can
    be analyzed in devices at the network’s edge. Instead of a centralized cloud service,
    Edge AI enables IoT devices, edge, and even cloud nodes to process their data
    with lightweight AI algorithms, thereby distributing intelligence. There have
    been several discussions about cloud independence in the literature, but the most
    common architecture for Edge AI systems includes the edge and cloud layers. This
    is discussed in detail in [3]. AI computations can be distributed across various
    levels, allowing for a combination of cloud and edge processing, where AI model
    training and inference collaboratively occur across these diverse levels. Besides
    a low real-time response to process data and make decisions, the Edge AI concept
    improves data security and the quality of user experience since it reduces problems
    involving network connection failures (intermittent in several cases) and delays.
    Evolving from DevOps and DataOps practices matured respectively by the software
    and data engineering communities, the so-called Machine Learning Operations (MLOps).
    MLOps are essential in Edge AI for IoMT applications, focusing on rapid development,
    deployment, and management of machine learning systems. MLOps architecture enables
    effective model management directly at data generation sources, reducing latency
    and ensuring real-time patient data analysis. It also supports collaborative model
    training on edge devices, preserving data confidentiality and adapting models
    to evolving patient conditions. Furthermore, MLOps address challenges in resource-limited
    edge environments, ensuring privacy, security, and adaptability in AI-based health
    solutions, crucial for healthcare applications. Briefly, the Internet of Medical
    Things benefits from several technologies, including the evolution of computers,
    the popularization of sensors and IoT, artificial intelligence, and edge computing.
    These technologies are often combined into the paradigm of Edge AI to create new
    applications in the IoMT domain. For example, some IoMT applications are remote
    diagnosis and monitoring health parameters such as blood oxygen, blood pressure,
    and blood sugar. These applications can be built into smart devices such as wristbands
    and smartwatches. Many recent IoMT applications, such as orthopedic screening,
    use image recognition and machine learning to achieve low response times and efficiency.
    Fig. 1 depicts the timeline of the relationship between the combined technologies
    to benefit the IoMT applications. Vast and diverse research has been conducted
    in the Edge AI domain. In contrast to prior surveys on Edge AI and related research
    areas (distributed intelligence or distributed machine learning), this study applies
    a detailed view of Edge AI for the IoMT domain. Download : Download high-res image
    (324KB) Download : Download full-size image Fig. 1. Timeline of the relationship
    between the technologies. To accomplish this objective, three formulated Research
    Questions (RQs) aim to answer in detail: 1. RQ1: Which intelligence distribution
    techniques in Edge AI are often used for the IoMT domain? 2. RQ2: How is ML training
    in most studies in the IoMT domain? 3. RQ3: What MLOps architectures and techniques
    are applied in the IoMT domain? In summary, the contributions of this study are
    in three aspects: 1. A review and theoretical and practical discussion of aspects
    of the state-of-the-art on Edge AI and IoMT, including distributed intelligence
    architectures, techniques, and ML training strategies. 2. A review and discussion
    of MLOps for edge computing that provides fast development and deployment of ML
    systems in edge environments. To the best of our knowledge, this survey is the
    first work that details further Edge AI and MLOps in e-health, IoMT, and wearable
    technology. 3. An introduction and proposal of an architecture of a case study
    for a case study on detecting heart anomalies, exploring the potential of Edge
    AI for IoMT applications. The paper is organized as follows. Section 2 presents
    a comparison with related surveys. This section is divided into two research areas:
    Edge AI and IoMT. The methodology adopted to select the papers is presented in
    Section 3. Section 3 also answers the formulated research questions and presents
    a comprehensive literature survey. This section is divided into two subsections:
    Distributed Intelligence Techniques and MLOps on Edge. Section 4 presents an IoMT
    case study. Finally, Section 5 concludes the paper, pointing to challenges and
    future research directions in this field. 2. Comparison with other surveys Several
    current works discuss the technologies and concepts involved or compile new Edge-AI-based
    solutions. However, a minority of those surveys compile solutions towards the
    IoMT applications domain. At the same time, the concepts of IoMT domain (sensors,
    applications, architecture, and platforms) have been compiled in recent surveys
    but do not usually involve approaches based on Artificial Intelligence on Edge
    (Edge AI). The following subsections review Edge AI and IoMT surveys and highlight
    their contribution, besides comparing them to this survey. The main objective
    is to explore a literature gap and investigate the potential of Edge AI approaches
    in IoMT applications. 2.1. Artificial Intelligence on Edge (Edge AI) Several researchers
    have conducted surveys on Edge AI as a potential alternative to IoT data processing
    for autonomous decision-making in recent years. Edge computing is essential to
    understanding architectures and the levels where edge intelligence is distributed.
    As discussed in [3], several works define Edge AI in two groups: (i) evolutionary,
    which defines Edge AI as the next stage of current edge computing where edge nodes
    process their data using AI algorithms, and (ii) revolutionary, where Edge AI
    is a new paradigm that combines novel and existing approaches, techniques, and
    tools such as edge computing, AI, approximate computing and cognitive science,
    to perform a fully distributed intelligence among the nodes. In both definition
    groups, edge computing is involved. Architecture. An important aspect is the architecture,
    where the devices process the data originated by IoT devices. For example, the
    studies [2], [3], [4] examine the relationship between the complementary paradigms
    of edge computing and cloud computing, emphasizing the synergy between edge and
    cloud levels (even a third level, called fog level) in the Edge-cloud Continuum
    to distribute lightweight ML algorithms and the evolution to Edge AI. These studies
    have also discussed the Edge AI approaches to deploying AI algorithms and models
    on edge nodes, typically resource-constrained devices. Work [5] presents an architecture
    that classifies edge levels based on their latency and resource constraints of
    the nodes. Edge AI and MLOps. The surveys [1], [4], [6], [7], [8], [9], [10] review
    important concepts for the optimization of ML models and deployment for resource-constrained
    edge nodes. For example, one key concept is federated learning training, a contemporary
    learning method that allows devices to learn ML models collaboratively. The work
    [6] focuses on mobile edge nodes in MEC architectures, while the studies [1],
    [8] expand the revised methods to devices in other variants of edge computing
    architectures (cloudlets, fog). The work [11] presents ML-based computation offloading
    mechanisms in the MEC paradigm considering other training perspectives: reinforcement
    learning and supervised and unsupervised learning. Besides, it also examines offloading
    metrics, evaluation tools, applications, and drawbacks. Studies on Edge AI have
    surveyed various aspects of AI computation. The study [12] discusses techniques
    for distributing ML across multiple nodes, such as hyperparameter optimization,
    ensemble methods (combination of multiple models), and topologies of data aggregation.
    Work [13] includes model partitioning, compression, scheduling, and other techniques.
    The survey [14] compiles the capacity of edge intelligence, including multisource,
    real-time, and context-aware fusion. The authors assume an effective edge intelligence
    system consists of primary functions: collection, communication, computing, caching,
    control, and collaboration. On the other hand, the study [15] summarizes the primary
    functions of Edge AI as caching, computing (training and inference), and offloading.
    The works [5], [16] investigated training activities performed in different locations
    (cloud, cloud-assisted edge, edge, IoT device) and recommended best practices.
    The work [16] focuses on techniques for deep learning (DL) at the edge, including
    model splitting, data parallelism, federated learning, and model adaptation. It
    also covers metrics for evaluating DL algorithms. The study [17] presents fundamental
    concepts of federated learning for data privacy preservation, including in healthcare,
    which is discussed briefly. Regarding resource management, it is important to
    explore the suitability of this kind of mechanism for Edge AI. The survey [18]
    presents training methods (federated, decentralized, offline, and online) and
    discusses which resource management categories (discovery, estimation, placement,
    orchestration, scheduling and allocation, provisioning, offloading, and load balancing)
    are more proper to use. Platforms, tools, virtualization. Regarding practical
    and operational aspects, the surveys [2], [3], [7], [8], [9], [12] compile tools
    (simulation, emulation, and deployment), techniques, algorithms, frameworks, hardware,
    and metrics for ML application in the Edge-to-Cloud Continuum. The authors [7]
    discuss several technologies to adapt edge to AI, such as hardware adaptation
    and containerization for hosting AI models. For the success of Edge AI, well-known
    challenges must be addressed, such as resource management, energy efficiency,
    communication, device failure, device heterogeneity, and data privacy [4], [13].
    Table 1 summarizes related surveys about AI on Edge. This survey primarily focuses
    on the following subjects when compiling related surveys: (i) Edge AI architecture,
    which refers to the infrastructure or architecture of the nodes that support the
    data processing; (ii) Edge AI, which involves AI computation tasks; (iii) Machine
    Learning Operations, if the work covers this research area ; (iv) Wearables or
    IoMT, if the work tackles the target application domain of this study; and (v)
    Platforms, Frameworks, Tools, or Virtualization, if the survey discusses practical
    aspects of Edge AI or not. Table 1 contains comprehensive information on each
    subject, including the column Wearables or IoMT. Some works investigate this topic
    extensively, while others do not (indicated by Yes or Not). Occasionally, some
    works only briefly mention or introduce this subject in a short section, referred
    to as Mention in Table 1. The contribution of this study is summarized as follows.
    The related surveys mentioned above discuss Edge AI from the perspective of AI
    strategies, key concepts, and practical aspects. A minority of related work [7],
    [8], [13], [17] mention briefly the health application domain as a use case for
    Edge AI. On the other hand, this study further reviews edge intelligence applications
    and features related to the IoMT and health domain. Moreover, while a limited
    number of surveys [9], [16] have covered only the deployment activity of ML models
    methodology at the edge, this survey covers all the methodology steps needed to
    develop, evaluate, deploy, monitor, and maintain ML models at the network edge.
    Additionally, it tackles practices and workflows of MLOps, with a particular emphasis
    on IoMT solutions. Table 1. Characterization of the collected surveys on Artificial
    Intelligence on Edge. Work Architecture Edge AI MLOps Wearables or IoMT Platforms,
    tools or virtualization [1] Variants of edge computing Yes No No Yes [2] Edge-to-Cloud
    Continuum Inference, Training distribution No No Yes [3] Edge-to-Cloud Continuum
    ML modeling inference and training No No Yes [4] Cloud versus Edge Yes No No Yes
    [5] Hierarchical edge Distributed ML No No No [6] MEC Federated learning No No
    No [7] Centralized versus Distributed Yes No Mention Yes [8] Edge-to-Cloud Continuum
    Edge Intelligence, Federated learning No Mention Yes [9] Edge-to-Cloud Continuum
    Distributed ML Deployment No Yes [10] Variants of edge computing Edge analytic
    No No Yes [11] MEC Training No No No [12] Centralized and Decentralized ML Distribution
    No No Yes [13] No Distributed ML No Mention Yes [14] No Information fusion at
    edge No No No [15] Centralized versus Edge Inference, Decentralized training No
    No Yes [16] Centralized and Distributed Training and Inference Deployment No No
    [17] Edge-to-Cloud Continuum Federated learning No Mention No [18] No Decentralized
    ML No No Yes This study Edge-to-Cloud Continuum Training and Inference Yes Yes
    Yes 2.2. IoMT and smart health IoMT is a powerful technology for improving traditional
    healthcare systems. Some key topics are considered to compare related work as
    follows. Architecture. Recent studies [19], [20], [21], [22] have conducted a
    comprehensive survey on IoT convergence for smart health, mainly concentrating
    on edge architectures, including wearables as a data source. The study [23] focuses
    on the multi-access edge computing (MEC) paradigm detailing diverse aspects of
    wearable technology on the data source level addressing, for example, security
    issues. Edge AI and MLOps. Edge computing is a widely accepted approach to analyzing
    real-time data near patients. However, using AI techniques in edge computing (Edge
    AI) is a new development. The study [21] provides a brief overview of the recent
    trend of deploying AI on the edge for processing health data. The survey [19]
    delves into edge optimization, medical signals fusion, and data processing lifecycle.
    The survey [24] investigates TinyML technology, including compression of neural
    networks based on architectures, design constraints, and performance metrics.
    The work [25] focuses on Edge AI for classification and prediction of health data,
    compiling proposals grouped on scenarios such as physiological, rehabilitation,
    skin disease and diet, epidemic prevention, and diabetes treatment. Combined other
    technologies. The integration of Edge AI with various technologies has shown great
    promise in the field of Internet of Medical Things (IoMT) solutions. One such
    technology is blockchain, which can be used to manage sensitive health data. By
    recording data in a sequential and immutable manner, blockchain provides enhanced
    data security, access management, and patient privacy in IoMT systems. The works
    [22], [25] discuss blockchain as a combined technology of IoMT system to safeguard
    patient’s data. Platforms, tools and virtualization. Concerning practical aspects
    (platforms, tools, visualizations, frameworks, hardware, and software aspects),
    The survey [24] investigates embedded ML, detailing hardware, software, and frameworks
    for wearable systems incorporating microcontroller units (MCUs) as edge inference
    devices. Besides, the work also presents a design flow of wearable devices as
    a TinyML technology. The study [25] presents platforms (Raspberry Pi, GPU, Arduino,
    smartphone, Intel Edison, and PC) as devices used in several health applications.
    Table 2 summarizes the comparison of related work about IoMT and smart health.
    This survey primarily focuses on the following subjects when compiling related
    surveys: (i) edge computing architecture, which refers to the infrastructure or
    architecture of the nodes that support the data processing; (ii) Platforms, Frameworks,
    Tools, or Virtualization, whether the survey discusses practical aspects of Edge
    AI or not; (iii) MLOps, if the work covers this research area; (iv) Edge AI, which
    involves AI computation tasks; and (iv) combined other technologies applied in
    the IoMT domain (e.g., blockchain), if the survey includes this topic. Table 2
    contains comprehensive information on each subject, including the column Edge
    AI. Some works investigate this topic extensively, while others do not (indicated
    by Yes or Not). Occasionally, some works focus only on techniques related to AI
    inference (do not investigate AI training techniques), referred to as Inference
    in the table. Notably, a concise set of related works [19], [24], [25] concentrates
    on Edge AI for IoMT applications. Besides, any mentioned related work does not
    include MLOps in their discussion. The main contribution of this study concerns
    discussing Edge AI, besides MLOps visions, for IoMT domains. Edge MLOps is important
    because it aims to develop and deploy high-quality ML systems in edge environments
    rapidly. To the best of our knowledge, this survey is the first work that details
    further Edge AI and MLOps in smart health, IoMT, or wearable technology. Considering
    the advantages and challenges of this research field, this survey will fully explore
    the potential of ML strategies, algorithms, and operations at the network’s edge.
    Table 2. Characterization of the collected surveys on IoMT and Smart health. Work
    Architecture Platforms or Virtualization MLOps Edge AI Combined other technologies
    [19] Yes No No Yes No [20] Yes No No No No [21] Yes Yes No No No [22] MEC No No
    No Blockchain [23] MEC, wearables No No No No [24] Microcontrollers as edge MCU
    No Yes No [25] Yes Yes No Yes Blockchain This study Yes Yes Yes Yes Yes 3. Edge
    AI for IoMT This section focuses on presenting the methodology and results compiled
    from this research. 3.1. Research goals and methodology This paper aims to answer
    3 research questions to review Edge AI and MLOPs approaches that have been applied
    to the IoMT and health domain: 1. RQ1: Which intelligence distribution techniques
    in Edge AI are often used for the IoMT domain? 2. RQ2: How is ML training in most
    studies in the IoMT domain? 3. RQ3: What MLOps architectures and techniques are
    applied in the IoMT domain? To achieve this objective, the first step was to construct
    a search query for study selection based on three formulated research questions
    in this study. Then, the second step was to conduct keyword searches on Scopus
    and ACM databases because they are the primary and most well-known libraries in
    the academic domain. The following search terms were: • (“artificial intelligence”
    OR “machine learning”) AND “edge computing” AND (IoMT OR health OR IoHT OR wearable)
    • (“edge intelligence” OR “edge AI”) AND (IoMT OR health OR IoHT OR wearable)
    The inclusion of a primary study in this survey depends on the assessment of 3
    research questions. However, there are specific criteria for excluding papers,
    such as studies that are not written in English, non-peer-reviewed papers, short
    papers (extended abstracts and posters), out-of-scope papers, theoretical works
    (without implementation or experiments), and surveys or review studies. After
    carefully analyzing the three research questions, a total of 10 papers out of
    56 were selected for this survey. Any duplicate or unavailable papers were not
    considered. The main findings are discussed as follows. 3.2. Distributed intelligence
    techniques To answer Research Question 1 (RQ1): “Which intelligence distribution
    techniques in Edge AI are often used for the IoMT domain?”, this study reviews
    edge inference and training strategies. The inference step refers to the process
    of predicting or classifying new real-time data in the proximity of data source
    devices. Designing models with low computational requirements for deployment on
    resource-constrained edge nodes through Model Compression and Inference Acceleration
    can help reduce the high resource consumption of AI models. To reduce the workload
    on the node and improve the inference performance, a portion or portions of the
    designed model can be partitioned and offloaded to another edge node or multiple
    nodes. This process is known as Model Partitioning. It is necessary to optimize
    the model while maintaining the baseline accuracy. The training step can be conducted
    using a single device or distributed multiple devices working collaboratively
    to train a shared model or algorithm. Cloud computing architectures have been
    the traditional infrastructure for AI computations related to IoT applications
    (including the health domain), such as ML training and inference tasks. Nevertheless,
    cloud architectures cannot properly perform these activities for IoMT applications,
    which require low latency or handling sensitive data. Recent works distribute
    intelligence (AI computations) at the different levels of the Edge-Cloud Continuum.
    The inference and training of AI models can be performed on different levels and
    combinations: (i) cloud only, (ii) cloud, edge, end device co-training, (iii)
    edge and end device, and (iv) end device only. These approaches with different
    levels and combinations are discussed as follows. Cloud only. The more common
    approach is to train ML models on the cloud level before running the data inference
    using these pre-trained models on the edge level (Fig. 2(i)). This approach is
    often because of the resource constraints of the edge nodes. This approach is
    presented in the studies [26], [27], [28]. Other studies [29], [30], [31], [32]
    combine pre-trained ML models on the cloud level with some pre-processing tasks
    performed on the edge level. This approach reduces communication costs at the
    cloud level. For example, the study [29] runs pre-processing on the edge level
    to detect anomalies in medical vitals signals (ECG, heart rate, and blood pressure)
    related to cardiac health. Edge nodes also run data inference. Download : Download
    high-res image (626KB) Download : Download full-size image Fig. 2. AI model training
    and inference on different levels and combinations. Cloud, edge, end devices.
    The second approach is training a machine learning model distributed on multiple
    devices at the edge and cloud in collaboration. In the studies [33], [34], edge
    and cloud devices collaboratively learn shared ML models without sharing data
    at the cloud level but just sharing crucial parameters (Fig. 2(ii)). The studies
    present federated learning involving user devices in training. Therefore, in this
    case, the end devices level (wearables, IoT devices) are included in this discussion.
    Edge only. The third approach involves training and inference tasks entirely in
    the edge nodes since these devices are improved regarding computational resources
    (Fig. 2(iii)). The study [35] explores this technique for human-centric IoT applications
    using different edge nodes with computational power. The study [33] evaluate three
    distributed models of AI computations performed in terms of execution time and
    accuracy: (i) training and inference edge only, (ii) pre-trained on cloud and
    inference on edge, (iii) distributed training (federated learning) and inference
    at the edge. The models have been tested using SVM and health databases. Their
    simulated experiments show the edge inference time is always faster than cloud
    time for all tested datasets for each proposed model. This is because of cloud
    communication costs. At the same time, pre-trained on the cloud is faster than
    training on the edge and distributed training. End device only. A new direction
    in the Edge AI research area is to run AI computation (training and inference)
    even in IoT devices (end devices) requiring advanced computing platforms (Fig.
    2(iv)). Recent IoT platforms are leveraging artificial intelligence accelerators
    and processors. To apply in IoMT applications, accelerators and processors should
    be used, and optimization and Tiny ML methods must be applied carefully to enable
    this approach, mainly to supply training activities. Deployment platforms. Since
    AI training and inference are moving to the edge, including IoT devices, it is
    interesting to consider the adopted platforms as edge devices because of their
    resource constraints. Historically, servers and smartphones have been considered
    edge-level technologies, as presented in the study [26]. On the other hand, due
    to their energy efficiency and low cost, a trend is the presence of resource-constrained
    nodes at this level, such as system-on-chip clusters and single-board computers.
    For example, the studies [27], [28], [29], [31] are edge architectures tailored
    (or tested) for single-board computers (Raspberry Pi and Odroid M1). Raspberry
    Pi is a popular platform for edge devices due to its low cost, versatility, and
    lightweight design. Choosing the most acceptable type of edge node requires knowledge
    about the data and ML models to be applied. The performance analysis helps to
    decide the suitable type of edge node. However, many proposals show that resource-constrained
    nodes have a good performance. For IoMT applications, the study [34] makes a performance
    comparison of edge nodes with low and high computational power, analyzing several
    ML strategies, such as federated, semi-supervised, transfer, and multi-task learning.
    In that context, Raspberry Pi outperforms a server for lung segmentation detection,
    but the server achieved slightly better results for COVID-19 detection. The study
    [35] emphasizes the importance of comprehending the initial data and signal characteristics
    to meet the cost–accuracy requirements. They conducted a performance evaluation
    to analyze the trade-off between the computational cost and the different classification
    models’ accuracy (KNN, RF, SVM, among others) for activity recognition applications.
    Raspberry Pi and Raspberry Zero devices were evaluated and achieved good results
    in terms of performance during the training and inference phases regarding time
    processing and accuracy. Heterogeneous technologies and resource consumption.
    The variety of deployment platforms is closely related to the expected heterogeneity
    of the devices across a distributed Edge AI solution. Beyond the aforementioned
    trade-off between computational cost and accuracy, the actual applicability of
    different technologies, especially communication technologies, plays an important
    role in this scenario. For example, the data collection in short distances may
    use Wi-Fi, Bluetooth, or ZigBee, while faster, wider and more flexible connections
    would require 5G or newer mobile technologies [6], [20]. Software and hardware
    heterogeneity is especially relevant in the context of computational offloading,
    where metrics such as energy consumed, computational latency, response time, total
    execution cost, quality of service (QoS), and quality of experience (QoE) are
    essential to monitor the performance of deployed applications [11]. Once again,
    there are several trade-offs to consider. For instance, higher QoS and QoE requirements
    could lead to higher power consumption. Importantly, while distributed intelligence
    approaches provide scalability and flexibility, they also may bring communication
    overhead and consequent cost increases. Strategies such as edge caching can alleviate
    the overall resource consumption [20]. IoMT sensors. Healthcare technology is
    making significant progress. IoMT applications depend on specific sensors to gather
    the required data. Heterogeneous sensors are embedded within various medical and
    IoT devices and wearables, offering a paradigm transformation in patient monitoring
    and data collecting. The most contemporary solutions even use data fusion, combining
    signals acquired from IoT sensors that get data directly from the patient and
    the environment to which the patient is exposed. For instance, the study [26]
    explores the integration of heart rate and accelerometer sensors for comprehensive
    activity tracking and stress monitoring. Study [27], a multifaceted approach,
    incorporates sensors for blood pressure, respiratory rate, consciousness state,
    temperature, and oxygen saturation to infer the clinical risk level of the patients.
    The study [28] deals with elderly patients, using ECG signals, temperature, relative
    humidity, heart rate, and oxygen saturation. The study [29] combines many input
    data and uses temperature, heart rate, blood pressure, respiratory rate, and oxygen
    saturation information to keep track of the patient’s cardiovascular health and
    to remotely monitor this data. The study [30] uses GPS, heart rate, insulin level,
    blood pressure, temperature, sweating, relative humidity, and accelerometer to
    analyze diabetes risk levels. The studies [31], [35] use a dataset of accelerometer
    information to classify user activities. The study [32] works with speech recognition
    and proposes context-aware task offloading based on wearable sensors, such as
    accelerometers, magnetometers, gyroscopes, and pedometers. The study [33] works
    with many different datasets and contexts, but regarding medical data, it uses
    blood pressure and blood glucose datasets to analyze problems like body fat, breast
    cancer, and diabetes. Finally, the study [34] uses radiography images to analyze
    COVID-19 and to do lung segmentation. Summary of IoMT applications. A Sankey Diagram
    (Fig. 3) compiles the research on Distributed Intelligence for IoMT Applications.
    A Sankey Diagram is a visualization technique that displays the flows of resources
    and represents their quantity. Fig. 3 depicts the compiled research on Distributed
    Intelligence for IoMT Applications (RQ1 answer) and represents the IoT sensors,
    cloud and edge platforms (discussed previously), AI models and intelligence distribution
    techniques found in the collected primary studies; discussed. Sankey Diagram displays
    comprehensive information on each subject. For example, it displays the recent
    IoMT applications domain whose distributed intelligence techniques are used to
    resolve issues related to human health. The works aim to use Edge AI to monitor
    stress levels [26], clinical risk [27], elderly patients [28], cardiovascular
    health [29], body fat, cancer, and diabetes [30], [33] activity recognition [31],
    [35], speech recognition [32], covid-19 and lung segmentation detection [34].
    Additionally, classification [26], [27], [28] and prediction [29] are the data
    analysis methods most required for IoMT applications that use Edge AI. Download
    : Download high-res image (979KB) Download : Download full-size image Fig. 3.
    Characterization of the collected works on distributed intelligence for IoMT applications.
    Note that the same work may follow multiple paths. 3.3. Model training in the
    IoMT domain To answer RQ2: “How is ML training in most studies in the IoMT domain?”
    The main findings compile training methods in the primary studies: cloud-pre-trained
    [26], [27], [28], [29], [30], [31], [32], [33], [35], embedded edge [33], [35],
    and federated learning [33], [34]. This study discusses the key ML training methods
    based on whether they are pre-trained (cloud), online training (edge), or federated
    training (edge and cloud) as follows. Centralized. Cloud pre-training is the most
    famous method in prior works about edge AI. Fig. 3 illustrates the most common
    cloud services used in the primary studies (for example, Amazon Service). In this
    approach, machine learning models are trained in cloud data centers and then deployed
    into edge devices to perform ML inference based on actual data coming from IoT
    devices. The benefit of this approach is the saving of limited edge resources,
    which may not be suitable for machine learning training tasks or in situations
    where training is performed in centralized mode due to the large amount of decentralized
    data arriving from geographically distant data. The shortcoming of this approach
    is keeping the performance of AI models, which were deployed on edge devices,
    updated or performing satisfactorily compared to the models in the cloud. Thus,
    monitoring and evaluating the performance of ML models is crucial to making decisions
    regarding retraining or updating the deployed ML model. Preserving data privacy
    is another concern since user data must be uploaded (and stored) to the cloud
    data center to be used for ML pre-training tasks. This aspect is very important
    since health applications deal with sensitive data from users: medical history,
    diseases, and clinical risks. Although cloud data centers leverage security mechanisms
    (authentication and confidentiality), transmitting data, unauthorized access,
    or changes to this kind of data would be problematic. Performing ML training activities
    on edge devices can be more efficient than transmitting data to a cloud data center
    for training, as it reduces communication overhead. However, to ensure the success
    of training on embedded edge devices, minimizing the number of trainable parameters
    is crucial to reduce the overall model size. Online learning. Despite the benefits
    of centralized training on the cloud, data frequency in edge environments is continuous.
    The pre-trained models are based on static datasets and cannot learn new knowledge
    when the training is over [8]. Online Learning is a machine learning method where
    a learner attempts to tackle some decision-making task by learning from a sequence
    of data instances one by one at each time and updates its parameters continuously
    as new data arrives [36]. In online learning, the method of updating ML parameters
    is continuous and incremental. It uses new data as it comes in, rather than processing
    data in batches. This means that models can adapt to changing and evolving data
    in real time, making them useful for scenarios where data is streaming continuously.
    Additionally, online learning algorithms require less memory than batch learning
    algorithms because they do not need to store the entire dataset in memory. They
    also handle concept drift, which occurs when there is a change in the underlying
    data distribution over time. The model can adapt to these changes and update its
    parameters accordingly. Therefore, online learning fits edge environments well
    because the user data flow is intense, constantly changing, and requires fewer
    computational resources than in batch or offline learning. Federated learning.
    FL [6] is a distributed and collaborative learning approach that aims to train
    sets and models located in decentralized and local positions and learn a shared
    model. This technique is based on the Distributed Selective Stochastic Gradient
    Descent (DSSGD). In federated learning, each edge device downloads from the cloud
    a generic global model for local training. After, the global model is improved
    with local edge data. Then, edge devices upload an encrypted gradient to the cloud.
    Finally, the average update of local models occurs in the cloud, and a renewed
    global model is transmitted to edge devices. The strong point of Federated Learning
    is the privacy-preserving technology of user data and saving bandwidth since it
    is a decentralized and local training method. Thus, data is not uploaded to cloud
    center data. Federated Learning can protect user privacy and provide personalized
    recommendations [8]. FL promises reduced energy consumption on devices since it
    is a collaborative method. The key shortcoming of federated learning is resource
    constraints since typical recent edge devices are smartphones, single-board computers,
    embedded sensors, and IoT devices. Thus, more than their resources are needed
    to train ML models. Another shortcoming is the high frequency of communication
    between edge devices operated in shared ML training because of the intermittent
    network connection, low bandwidth, and regular high costs. Besides, the success
    of the federated learning approach also depends on the number of available clients
    to train ML models, and mobile devices can be intermittently offline. 3.4. Machine
    learning operations on edge Including machine learning models in production environments
    is a challenge that has remained since the first studies in the field. Several
    difficulties in this task act as barriers for deploying models from a large portion
    of machine learning projects [37]. However, following the success of methodologies
    for continuous software development, a set of methods and techniques has been
    recently proposed with the objective of minimizing most of the existing problems.
    Such strategies also aim to implement improvements to production at the moment
    they are developed. The practice of adopting automation, monitoring tools, and
    methodologies at all stages of the development of machine learning solutions,
    including integration, testing, delivery, deployment, and infrastructure management,
    has been called Machine Learning Operations (MLOps) [38]. The main objective of
    MLOps is to achieve fast development and deployment of machine learning systems
    with high quality, reproducibility requirements, and tracking of all the parts
    of the end-to-end solution [39]. It is worth emphasizing that machine learning
    systems are built through a series of components that include not only the model
    and learning algorithm code. In [40], the authors emphasize that only a small
    portion of a real-world machine learning system comprises strict machine learning
    code. There are several other diverse components that turn such systems into complex
    structures to analyze and maintain. Section 3.4.1 reviews some of those components.
    As the demand for sophisticated ML capabilities increases, deploying ML systems
    at the network’s edge, closer to the data sources and end-users, has gained prominence.
    Edge computing offers advantages like reduced latency, improved privacy, and enhanced
    device autonomy in resource-constrained environments. This convergence of ML and
    edge computing introduces new challenges and opportunities in designing and managing
    ML systems. Despite MLOps use being more frequent in edge environments nowadays,
    developers need to handle additional issues. For instance, edge environments often
    present resource constraints, varying network connection quality, and node heterogeneity
    [41]. Thus, standard operational approaches initially developed for centralized
    systems must be adapted or extended. Section 3.4.2 reviews some MLOps workflows
    available in the literature, focusing on edge solutions and the IoMT domain. As
    previously mentioned, cloud and edge are part of a continuum of steps for data
    collecting, storing and processing, as well as making decisions. Thus, cloud and
    edge computing are complementary in IoT and IoMT solutions. Nevertheless, their
    intrinsic differences result in specificities in developing and deploying ML systems.
    In that sense, a comparative review and discussion between MLOps strategies over
    cloud and edge ML components is presented in Section 3.4.3. The following presentation
    focuses on general definitions, architectures, methodologies, and pipelines. For
    a comprehensive review of MLOps tools, the reader is referred to the recent work
    by [37]. 3.4.1. Machine learning system components Understanding the crucial components
    of an ML system and their implications when deployed at the edge is fundamental.
    As follows, the main components of an ML solution are described and their interaction
    are explored in the context of edge computing. Importantly, the intricacies of
    edge-based ML operations are emphasized in the face of the specificities of a
    decentralized architecture, the typical scenario where IoMT is deployed. Configuration.
    The set of all configuration parameters necessary for the system to function is
    not concentrated in a single specific step but permeates the entire process [42].
    In the context of edge computing, the configuration of ML systems takes on added
    significance, as it needs to account for the unique constraints and resource limitations
    inherent to edge devices. Fine-tuning configuration settings becomes essential
    to ensure optimal performance and resource utilization, enabling efficient and
    accurate ML model deployment at the edge [43]. For instance, in the presence of
    distinct edge devices, e.g., camera sensors and wearables, that interact with
    a given model, it is necessary to guarantee suitable configurations for every
    device, e.g., proper image resolution and transmission rate [44]. Data collection.
    The data gathering steps constitute a critical bottleneck of machine learning
    systems [45]. This component concerns the process of collecting and measuring
    information about variables in a given system, allowing others to answer relevant
    questions and evaluate results [42]. In line with edge computing, data collection
    becomes even more challenging as edge devices, especially IoMT sensors, often
    operate in resource-constrained environments with limited network connectivity.
    Efficiently gathering data at the edge while ensuring data quality and privacy
    becomes a significant consideration in the overall ML system design [46]. It is
    worth noting the importance of active learning [47] when dealing with distributed
    guided data collection locally in edge devices which enables faster data labeling
    and cleaning processes [48]. Feature engineering. The process of using domain
    knowledge to create features from raw data to feed learning models is called feature
    engineering. Data collection and feature engineering are fundamental components
    of the overall ML solution. In fact, most of the time spent running projects that
    involve end-to-end learning is used for preparing the data, which includes collecting,
    cleaning, analyzing, visualizing, and performing feature engineering [45]. Conventionally,
    every feature engineering task has been centralized in the cloud to leverage abundant
    computational resources [43]. Nevertheless, certain scenarios offer the possibility
    of executing this phase directly on the edge devices. In such instances, the feature
    engineering operations, including feature selection steps, require customization
    to align with the inherent constraints and capabilities of edge computing. This
    is necessary to ensure that relevant features are extracted efficiently while
    minimizing computational overhead and power consumption [49], usually a relevant
    bottleneck in the context of IoMT. Data verification. Data migrated from one source
    to another must be checked for correctness, ensuring no inconsistencies were introduced
    [50]. In edge computing, where intermittent connectivity and unreliable network
    conditions can be common, data verification becomes particularly important to
    ensure the integrity and accuracy of the data being transmitted between edge devices
    and centralized systems. Caching edge data on multiple edge servers is critical
    to provide reliable services, which raises the importance of having a continuous
    inspection for data integrity and localization of corrupted edge data [51]. Resource
    management. A deployed ML system should present a component responsible for identifying
    and accurately providing all the necessary infrastructure for the seamless operation
    of the ML models, as they require a wide availability of resources [50]. In edge
    computing, resource management becomes even more critical due to the limited computational
    power and storage capacity of edge devices. Efficiently allocating and managing
    resources on edge devices becomes essential to ensure that ML models can be effectively
    deployed and run at the edge. Due to rapidly changing environments and the distributed
    nature of edge computing, there have been several approaches to manage edge resources
    automatically [52]. Model analysis. It must be ensured that the available trained
    models follow what has been planned before going into production. Generally, this
    step is performed by applying extensive cross-validation experiments [53]. In
    the context of edge computing, the model analysis should also consider the model’s
    performance under varying edge conditions, such as low network bandwidth or intermittent
    connectivity, to assess the robustness and reliability of the models in real-world
    edge deployments. An important aspect of model analysis in edge devices is comparing
    the tradeoffs between accuracy and speed, or accuracy and latency [54]. This critical
    balance has brought the proposal of several model architectures [55] tailored
    towards resource-limited environments, such as IoMT [56], [57], [58]. Process
    management. In a production environment, process management plays a crucial role,
    especially as the ML system size and the number of components increase. It orchestrates
    individual processes and ensures seamless integration into customized solutions
    for each component. Thus, a dedicated process control and management component
    becomes essential to meet these demands effectively. In edge computing, process
    management, which can also be treated as AI life-cycle management, becomes more
    complex due to the distributed nature of edge systems, requiring effective coordination
    and synchronization between edge devices and centralized components [59]. The
    operations in the edge also involve network management, fault detection and handling,
    service management, and overall performance management [52]. Metadata management.
    Each and every execution of an ML system must be recorded in order to help users
    capture artifacts and possible comparisons and guarantee reproducibility. This
    component is responsible for capturing any information that can somehow be used
    later. Metadata management in edge computing becomes crucial to track and trace
    model versions deployed across multiple edge devices, enabling efficient model
    updates and version control in edge environments [59]. Additionally, the meta-information
    from local data at the edge must also be carefully managed to enable consistency
    across the system [60]. Serving infrastructure. When interacting with the end
    user, the ML system must properly handle the available infrastructure to efficiently
    answer incoming requests. This can include the usage of CPUs, GPUs, storage locations,
    and even network optimization for each solution [50]. In edge computing, serving
    infrastructure must be optimized to cater to the diverse hardware and network
    configurations of edge devices, ensuring low latency and responsive model inference
    at the edge [43]. Approaches commonly pursued at the edge to limit inference delay
    include model compression, parallelism, and service scheduling [61]. Monitoring.
    In ML systems, it is crucial to know that a trained model continues to work in
    production long after its release. This component is responsible for monitoring
    the model serving services, as well as the training and input data pipelines [40].
    In edge computing, real-time monitoring becomes essential to detect and address
    any performance degradation or anomalies in the model behavior, ensuring continuous
    and reliable operation [59]. The monitoring component also aids in determining
    when a new model is required by analyzing key performance indicators [41]. A model
    update is then performed by feeding recent data collected from the model in production
    [62]. IoMT applications, which are often personalized, require device-specific
    monitoring since fine-tuning results in slightly different models per user [63].
    3.4.2. MLOps workflows The complex interconnection between edge computing and
    IoMT is significantly influenced by the architectures and methodologies of MLOps.
    This convergence brings about a fundamental transformation in the utilization
    of medical data to improve healthcare and wellness outcomes. Central to this combination
    is a comprehensive MLOps workflow, which manages the complete lifecycle of ML
    models [64]. In the specific context of edge computing and IoMT, the MLOps architecture
    facilitates the seamless deployment, continuous monitoring, and effective management
    of ML models directly at the data generation source, i.e., close to the edge [65].
    This approach mitigates data transmission latency and ensures real-time analysis
    of patient data, thus enabling rapid responses and well-informed medical judgments
    [66]. Advanced techniques, such as federated learning, play a key role in enabling
    collaborative model training across dispersed edge devices while preserving the
    confidentiality of sensitive data without central aggregation. This effectively
    protects patient privacy [67]. Furthermore, the integration of continuous model
    refinement methodologies, such as online learning, guarantees the adaptability
    of these models to the evolution of patient conditions over time [68]. In essence,
    the combined influence of MLOps architectures and techniques increases the potentials
    of edge computing and IoMT [8]. In order to try to answer the RQ3: What MLOps
    architectures and techniques are applied in the IoMT domain?, it is of significant
    importance the discussion of methodologies aimed at providing guidance for data
    science projects. The CRISP-ML(Q) framework. The CRoss-Industry Standard Process
    model for the development of machine learning applications with Quality assurance
    methodology (CRISP-ML(Q), [69]) is a methodology that aims to address issues related
    to technical debt when deploying and maintaining ML systems as part of a product
    or service [69]. This framework is based on the well-known CRISP-DM, which defines
    a reference methodology for data mining projects [70]. In the CRISP-ML(Q), the
    project activities are organized in six phases: business understanding, data understanding,
    data preparation, modeling, evaluation, and deployment, as can be seen in Fig.
    4. These phases compose a cycle and include iterations to review previous steps
    if the success or completion criteria are not met. The result is a waterfall cycle
    with backtracking [71]. The main differences between CRISP-ML(Q) and CRISP-DM
    are: (i) CRISP-ML(Q) merges the first two phases (business understanding and data
    understanding) and creates the last one (monitoring and maintenance), and (ii)
    the quality assurance step is inserted at each phase and task of the process [69].
    The initial phase, business and data understanding, aims to define project objectives,
    create measurable success criteria via Key Performance Indicators (KPIs), and
    collect and analyze necessary data. Data preparation, also known as data engineering,
    involves readying data sets for modeling, addressing missing data, encoding variables,
    normalization, and other essential tasks. Download : Download high-res image (130KB)
    Download : Download full-size image Fig. 4. Illustration of the main CRISP-ML(Q)
    steps. Next, in the modeling phase, various ML models are developed and assessed
    based on the specific business problem, encompassing model selection, experimental
    design, and construction. These models are packaged into a repeatable pipeline
    for training. Evaluation, or offline testing, assesses trained models using unseen
    test data and appropriate metrics. It is worth noting the growing interest in
    explainable models to enhance confidence in decision-making [72]. The deployment
    phase integrates the model into existing software, provides predictive tools,
    and involves tasks like hardware definition and production environment evaluation.
    Once the model is in production, ongoing monitoring and maintenance are essential.
    Real data evaluation identifies retraining needs when performance falls below
    predefined thresholds set during the business and data understanding phase, potentially
    using incremental learning techniques [69], [70]. Automated pipelines. Each of
    the outlined procedural stages is amenable to integration within pipelines, thereby
    enabling partial or complete automation facilitated by a suite of specialized
    tools. Numerous studies have proffered innovative pipeline frameworks tailored
    for distinct problem domains, while some even serve as versatile templates. Illustratively,
    [39] exhibits an example of a pipeline architecture employed in forecasting electricity
    supply market trends, demonstrating practical implementation. Likewise, [73] advances
    a template founded upon canonical components of a conventional ML system. Hybrid
    workflows. Apart from fully cloud-based solutions, several hybrid workflows have
    been proposed by some authors [43], [48], [61], [65], [74]. In general, they advocate
    the distribution of tasks between the cloud and edge environments with the aim
    to address the limitations associated with cloud-exclusive approaches. The pipeline
    introduced by [61] presents an integrated MLOps framework that enables developers
    to schedule individual components of an ML workflow efficiently. The presented
    MLOps framework is an integrated system merging Kubeflow’s4 detailed ML workflow
    management with Kubernetes’5 automated deployment, administration, and scaling
    capabilities. The framework takes into account the requirements of each component
    in terms of computing and memory capacity, as well as network latency, and allows
    for an optimized and efficient scheduling of the components across the continuum
    while also preserving high utilization and energy efficiency with a minimal manual
    effort from the developers. In a similar approach, [48] introduces Edge Impulse,
    a comprehensive online framework that simplifies data aggregation, streamlines
    deep learning model training and supports deployment on embedded and edge computing
    devices. The authors in [48] highlight the challenges associated with the ML workflow
    and features of Edge Impulse that tackle those challenges. These instances typify
    ML pipeline proposals, wherein cloud and edge technologies are applied to remedy
    the aforementioned technical liabilities. The panorama of analogous solutions
    further extends through contributions such as [43], [65], [74] The previously
    outlined stages form a cohesive ML framework that systematically addresses complex
    business challenges through data-driven models. Recent studies, such as [39],
    [73], showcase the adaptability of this approach and the emergence of hybrid solutions.
    This is the path that has been pursued by some other authors, including [48],
    [61], [74], which indicates a movement towards optimizing resource use, combining
    both cloud and edge technologies. These innovations highlight the dynamism of
    the field and the potential for transformative impact through the integration
    of edge computing in practical ML scenarios. 3.4.3. Comparing cloud and edge MLOps
    in IoMT environments The previous sections highlighted some requirements and challenges
    when considering MLOps at the edge, for instance, in IoMT applications. Although
    still a recent research subject, the main principles, roles, components, workflows,
    and best practices of centralized MLOps at the cloud are growing mature [75].
    However, analyzing and adapting this body of knowledge from the angle of edge
    computing is an ongoing effort. Resources and infrastructure. Cloud deployment
    presents the advantage of presenting enough resources to handle complex ML pipelines,
    which are usually more straightforward to maintain, given the more homogeneous
    infrastructure. On the other hand, a cloud-based solution requires a large bandwidth
    capacity, may induce delayed responses, and hinders data privacy [9]. Edge computing
    limits communication demands and enables better handling of data privacy. Yet,
    due to the diversity of resources, features, and configurations in edge devices,
    customized frameworks are often required [41]. In the above context, when lightweight
    distributed modules are considered, such as IoT networks, microservices applications
    constitute a viable alternative [76]. This approach opposes a monolithic cloud-based
    service and aims for scalability and reduced latency. Containerization tools,
    such as Docker, are important ingredients of such a solution since they enable
    deploying and running applications in diverse environments [77]. Model training
    and inference. While cloud solutions can afford to train large models from centralized
    databases, techniques such as pre-training, federated learning, and online learning
    constitute important strategies to train and maintain models in the edge, especially
    in the context of personalized IoMT applications [78], [79]. Performing inference
    in resource-rich cloud systems is usually a simpler task than receiving requests
    and sending model predictions to the edge. To reduce computation and storage requirements,
    model pruning, low-rank approximations, model quantization, knowledge distillation,
    model partitioning, and edge caching are examples of techniques frequently pursued
    [8]. It is worth emphasizing that for both training and inference steps, the issues
    of energy efficiency and power consumption remain among the central challenges
    of IoMT applications [80]. Privacy and security. The concentration of large knowledge
    bases collected from multiple sources rises privacy and legal concerns in cloud-based
    AI services [81]. Distributed solutions at the edge are able to avoid sending
    sensitive raw data to untrusted data curators while still being able to extract
    patterns and provide useful information [82]. AI-based health solutions must also
    deal with security requirements in the cloud or at the edge. IoMT applications
    are exposed to several threats and attacks that require specific intrusion detection
    systems (IDSs), which must stay reliable even in the face of limited resources
    [83], [84], [85], [86]. The continuum. Since modern workflows have pursued hybrid
    solutions with cloud and edge components, especially in the IoMT domain, recognizing
    the differences between MLOps practices in both environments is important towards
    a more efficient overall solution. This is mostly relevant when distinct products
    and services require implementing components of the ML system in distinct positions
    due to particular constraints or requirements. Moreover, the same system component
    may present steps distributed across the edge-cloud continuum, which enables harvesting
    advantages and avoiding drawbacks of the target production environments. 4. Case
    study Considering the tools, operations, and architectures presented in this document,
    this section shows a case study and proposes an architecture to solve a well-known
    task in the IoMT context: detecting heart anomalies. The case study consists of
    collecting data for the personalized training of a model capable of receiving
    PPG data and generating an ECG signal to detect heart anomalies like atrial fibrillation
    and, in real-time, inform the patient or medical entities. Thus, the patient has
    the benefit of receiving a medical diagnosis in real time. ECG (Electrocardiogram)
    data is valuable information for detecting arrhythmia and atrial fibrillation.
    Despite this, collecting ECG data continuously can become a problem since the
    collection of this data is active, requiring the user to remain in specific conditions
    and sometimes to press or hold a button. Nowadays, smartwatches can collect ECG
    signals and PPG (photoplethysmogram) data. Collecting the PPG is easier and more
    convenient than the electrocardiogram, given the fact that it does not need user
    actuation. It is a passive data acquisition that uses a light sensor. Given the
    PPG measurement, the PP2ECG model reconstructs an ECG signal with PPG signals
    as input and ECG signals as target labels. Using a subject-based deep learning
    model [87], it is possible to create an ECG signal that represents the user’s
    condition and shows possible heart diseases inferred from the PPG data. Then,
    it is possible to use the built signal as the input for an ECG classification
    model and detect those anomalies. This approach requires a model specific to each
    patient, given that the correlation between ECG and PPG works differently for
    each user. Fig. 5 presents the Edge AI and MLOps steps performed on different
    levels of the Edge-to-Cloud Continuum regarding this case study that requires
    two AI models: a PPG2ECG model and an ECG classification model. The PPG2ECG model
    converts PPG signals to ECG data, while the ECG classification model is used to
    infer heart anomalies from the converted ECG data. As follows, it is given a description
    of the ML operations performed in each layer. End devices. The smartwatch collects
    data from a specific user’s ECG and PPG sensors and sends them to the Fog/Edge
    Nodes, which use the data to train a PPG2ECG model. The smartwatch can send data
    collected to Fog/Edge Nodes using Wi-Fi or Bluetooth networks. Next, the end device
    deploys the model and uses it to infer ECG data from new PPG data collected in
    real time. The ECG data itself is not enough to detect a heart anomaly. Therefore,
    an ECG classification model is required to complement the PPG2ECG model. Such
    a model is trained in the cloud and sent to the end device, which deploys and
    uses it to infer heart conditions from the ECG data reconstructed from the real-time
    PPG user data. Continuous monitoring and maintenance are required to ensure the
    availability and performance of the deployed models. Periodically, the edge device
    requests new collections of ECG and PPG data from the user to improve the PPG2ECG
    model. Download : Download high-res image (302KB) Download : Download full-size
    image Fig. 5. Architecture proposal for the case study of detecting heart anomalies.
    Fog/Edge nodes. The Fog/Edge nodes receive PPG and ECG data collected from a specific
    user and prepare such data to train a PPG2ECG model for reconstructing an ECG
    signal from the PPG data. The model is evaluated, and if a given threshold metric
    is satisfied, it is sent to the end device. Otherwise, the end device is notified
    to send more ECG and PPG user data for training a new model with a suitable metric
    value. Monitoring and maintenance are required to continuously get new data from
    users to improve their PPG2ECG models. The devices can communicate with each other
    using Wi-Fi, Bluetooth, or Ultra Wideband (UWB) networks. Cloud. This case study
    proposes preparing and using publicly available ECG datasets to train and evaluate
    an ECG classification model for detecting heart anomalies. The cloud layer is
    responsible for maintaining the business rules and, using public datasets, training,
    generating indicators, and offloading ECG classification models that will be taken
    to the final layer. 5. Final considerations The Internet of Medical Things (IoMT)
    actively transforms traditional healthcare systems, and Edge AI plays a critical
    role in this evolution by allowing the protection of sensitive patient data. As
    Edge AI operates on devices at the network’s edge, where resource availability
    is more constrained than in cloud environments, it necessitates a scholarly exploration.
    Despite the well-established applications and case studies available in the literature,
    this survey tackled some important gaps related to the required methodology to
    develop, evaluate, deploy, monitor, and maintain ML models at the edge. In that
    direction, diverse practices and workflows of the so-called MLOps field were covered,
    with focus on IoMT solutions and edge interactions. Besides, the potential of
    Edge AI for IoMT applications was approached by examining the methods of intelligence
    distribution, model training and inference, and the comparison between cloud and
    edge MLOps in IoMT environments. It emphasizes the importance of customized MLOps
    frameworks for edge devices due to the diversity of resources and configurations.
    The challenges and advantages of training and inference at the edge, including
    the need for energy efficiency and privacy, are thoroughly discussed. Furthermore,
    the document reviews the CRISP-ML(Q) framework as a methodology for developing
    machine learning applications with quality assurance. It also explores the comparison
    between centralized cloud deployment and distributed edge solutions, highlighting
    the considerations for privacy, security, and the continuum of edge-cloud workflows.
    In conclusion, the document provides a comprehensive analysis of the potential,
    challenges, and best practices of Edge AI for IoMT applications, shedding light
    on the complexities and opportunities of implementing MLOps in distributed edge
    environments. CRediT authorship contribution statement Atslands Rocha: Conceptualization,
    Methodology, Formal analysis, Writing – original draft, Visualization, Read and
    agreed to the published version of the manuscript. Matheus Monteiro: Writing—original
    draft, Visualization, Read and agreed to the published version of the manuscript.
    César Mattos: Conceptualization, Methodology, Formal analysis, Writing – original
    draft, Read and agreed to the published version of the manuscript. Madson Dias:
    Formal analysis, Writing – original draft, Visualization, Read and agreed to the
    published version of the manuscript. Jorge Soares: Writing – review & editing,
    Supervision, Read and agreed to the published version of the manuscript. Regis
    Magalhães: Writing – original draft, Writing – review & editing, Visualization,
    Supervision, Read and agreed to the published version of the manuscript. José
    Macedo: Resources, Writing – review & editing, Supervision, Read and agreed to
    the published version of the manuscript. Declaration of competing interest The
    authors declare that they have no known competing financial interests or personal
    relationships that could have appeared to influence the work reported in this
    paper. Acknowledgments Part of the results presented in this work were obtained
    through the project “CENTER OF EXCELLENCE IN ARTIFICIAL INTELLIGENCE – AI4WELLNESS”,
    funded by Samsung Eletrônica da Amazônia Ltda., at Federal University of Ceará,
    Brazil , under the Information Technology Law No. 8.248/91. Data availability
    No data was used for the research described in the article. References [1] Singh
    R., Gill S.S. Edge ai: A survey Internet Things Cyber-Phys Syst, 3 (2023), pp.
    71-92, 10.1016/j.iotcps.2023.02.004 View PDFView articleView in ScopusGoogle Scholar
    [2] Rosendo D., Costan A., Valduriez P., Antoniu G. Distributed intelligence on
    the Edge-to-Cloud Continuum: A systematic literature review J Parallel Distrib
    Comput, 166 (2022), pp. 71-94, 10.1016/j.jpdc.2022.04.004 URL https://www.sciencedirect.com/science/article/pii/S0743731522000843
    View PDFView articleView in ScopusGoogle Scholar [3] Barbuto V., Savaglio C.,
    Chen M., Fortino G. Disclosing edge intelligence: A systematic meta-survey Big
    Data Cogn Comput, 7 (2023), p. 44, 10.3390/bdcc7010044 View in ScopusGoogle Scholar
    [4] Hoffpauir K, Simmons J, Schmidt N, Pittala R, Briggs I, Makani S, Jararweh
    Y. A survey on edge intelligence and lightweight machine learning support for
    future applications and services. J Data Inf Qual http://dx.doi.org/10.1145/3581759.
    Google Scholar [5] Mwase C., Jin Y., Westerlund T., Tenhunen H., Zou Z. Communication-efficient
    distributed AI strategies for the IoT edge Future Gener Comput Syst, 131 (2022),
    pp. 292-308, 10.1016/j.future.2022.01.013 View PDFView articleView in ScopusGoogle
    Scholar [6] Singh A., Satapathy S.C., Roy A., Gutub A. AI-based mobile edge computing
    for IoT: Applications, challenges, and future scope Arab J Sci Eng, 47 (8) (2022),
    pp. 9801-9831, 10.1007/s13369-021-06348-2 View in ScopusGoogle Scholar [7] Douch
    S., Abid M.R., Zine-Dine K., Bouzidi D., Benhaddou D. Edge computing technology
    enablers: A systematic lecture study IEEE Access, 10 (2022), pp. 69264-69302,
    10.1109/ACCESS.2022.3183634 URL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132750947&doi=10.11092FACCESS.2022.3183634&partnerID=40&md5=88667ef450dd686759983878427de051
    View in ScopusGoogle Scholar [8] Su W., Li L., Liu F., He M., Liang X. AI on the
    edge: a comprehensive review, Springer Netherlands (2022), 10.1007/s10462-022-10141-4
    Google Scholar [9] Sarwar Murshed M.G., Murphy C., Hou D., Khan N., Ananthanarayanan
    G., Hussain F. Machine learning at the network edge: A survey ACM Comput Surv,
    54 (8) (2022), pp. 1-35, 10.1145/3469029 arXiv:1908.00080 Google Scholar [10]
    Nayak S, Patgiri R, Waikhom L, Ahmed A. A review on edge analytics: Issues, challenges,
    opportunities, promises, future directions, and applications. In: Digital communications
    and networks. http://dx.doi.org/10.1016/j.dcan.2022.10.016,. Google Scholar [11]
    Shakarami A, Ghobaei-Arani M, Shahidinejad A. A survey on the computation offloading
    approaches in mobile edge computing: A machine learning-based perspective. Comput
    Netw 182(August). http://dx.doi.org/10.1016/j.comnet.2020.107496. Google Scholar
    [12] Verbraeken J, Wolting M, Katzy J, Kloppenburg J, Verbelen T, Rellermeyer
    JS. A Survey on Distributed Machine Learning. ACM Comput Surv 53(2). http://dx.doi.org/10.1145/3377454,.
    Google Scholar [13] Filho C.P., Marques E., Chang V., Dos Santos L., Bernardini
    F., Pires P.F., Ochi L., Delicato F.C. A systematic literature review on distributed
    machine learning in edge computing Sensors, 22 (7) (2022), pp. 1-36, 10.3390/s22072665
    Google Scholar [14] Zhang Y., Jiang C., Yue B., Wan J., Guizani M. Information
    fusion for edge intelligence: A survey Inf Fusion, 81 (2021) (2022), pp. 171-186,
    10.1016/j.inffus.2021.11.018 View PDFView articleGoogle Scholar [15] Xu D, Li
    T, Li Y, Su X, Tarkoma S, Hui P. A survey on edge intelligence, CoRR abs/2003.12172
    arXiv:2003.12172 URL https://arxiv.org/abs/2003.12172. Google Scholar [16] Joshi
    P., Hasanuzzaman M., Thapa C., Afli H., Scully T. Enabling all in-edge deep learning:
    A literature review IEEE Access, 11 (2023), pp. 3431-3460, 10.1109/ACCESS.2023.3234761
    URL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147210326&doi=10.11092FACCESS.2023.3234761&partnerID=40&md5=ecbad1122c4ce3f54b1bb922c71e5fac
    View in ScopusGoogle Scholar [17] Boobalan P., Ramu S.P., Pham Q.V., Dev K., Pandya
    S., Maddikunta P.K.R., Gadekallu T.R., Huynh-The T. Fusion of federated learning
    and industrial Internet of Things: A survey Comput Netw, 212 (May) (2022), Article
    109048, 10.1016/j.comnet.2022.109048 arXiv:2101.00798 View PDFView articleView
    in ScopusGoogle Scholar [18] Iftikhar S., Gill S.S., Song C., Xu M., Aslanpour
    M.S., Toosi A.N., Du J., Wu H., Ghosh S., Chowdhury D., Golec M., Kumar M., Abdelmoniem
    A.M., Cuadrado F., Varghese B., Rana O., Dustdar S., Uhlig S. AI-based fog and
    edge computing: A systematic review, taxonomy and future directions Internet Things,
    21 (2023), Article 100674, 10.1016/j.iot.2022.100674 URL https://www.sciencedirect.com/science/article/pii/S254266052200155X
    View PDFView articleView in ScopusGoogle Scholar [19] Alshehri F., Muhammad G.
    A comprehensive survey of the Internet of Things (IoT) and AI-based smart healthcare
    IEEE Access, 9 (2021), pp. 3660-3678, 10.1109/ACCESS.2020.3047960 View in ScopusGoogle
    Scholar [20] Sun L., Sun L., Jiang X., Ren H., Ren H., Guo Y. Edge-cloud computing
    and artificial intelligence in internet of medical things: Architecture, technology
    and application IEEE Access, 8 (2020), pp. 101079-101092, 10.1109/ACCESS.2020.2997831
    View in ScopusGoogle Scholar [21] Greco L., Percannella G., Ritrovato P., Tortorella
    F., Vento M. Trends in IoT based solutions for health care: Moving AI to the edge
    Pattern Recognit Lett, 135 (2020), pp. 346-353, 10.1016/j.patrec.2020.05.016 View
    PDFView articleView in ScopusGoogle Scholar [22] Awad A.I., Fouda M.M., Khashaba
    M.M., Mohamed E.R., Hosny K.M. Utilization of mobile edge computing on the internet
    of medical things: A survey ICT Express, 9 (3) (2023), pp. 473-485, 10.1016/J.ICTE.2022.05.006
    View PDFView articleView in ScopusGoogle Scholar [23] Jin X., Li L., Dang F.,
    Chen X., Liu Y. A survey on edge computing for wearable technology Digit Signal
    Process: Rev J, 125 (2022), Article 103146, 10.1016/j.dsp.2021.103146 View PDFView
    articleView in ScopusGoogle Scholar [24] Diab M.S., Rodriguez-Villegas E. Embedded
    machine learning using microcontrollers in wearable and ambulatory systems for
    health and care applications: A review IEEE Access, 10 (June) (2022), pp. 98450-98474,
    10.1109/ACCESS.2022.3206782 View in ScopusGoogle Scholar [25] Amin S.U., Hossain
    M.S. Edge intelligence and Internet of Things in healthcare: A survey IEEE Access,
    9 (2021), pp. 45-59, 10.1109/ACCESS.2020.3045115 URL https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098753480&doi=10.11092FACCESS.2020.3045115&partnerID=40&md5=f9e13c14e5ad00459c922f1656a0f543
    View in ScopusGoogle Scholar [26] Sim S.-H., Paranjpe T., Roberts N., Zhao M.
    Exploring edge machine learning-based stress prediction using wearable devices
    2022 21st IEEE international conference on machine learning and applications,
    ICMLA (2022), pp. 1266-1273, 10.1109/ICMLA55696.2022.00203 View in ScopusGoogle
    Scholar [27] Pazienza A., Anglani R., Fasciano C., Tatulli C., Vitulano F. Evolving
    and explainable clinical risk assessment at the edge Evol Syst, 13 (3) (2022),
    pp. 403-422, 10.1007/s12530-021-09403-3 View in ScopusGoogle Scholar [28] Debauche
    O, Nkamla Penka JB, Mahmoudi S, Lessage X, Hani M, Manneback P, Lufuluabu UK,
    Bert N, Messaoudi D, Guttadauria A. RAMi: A New Real-Time Internet of Medical
    Things Architecture for Elderly Patient Monitoring. Information 13(9). http://dx.doi.org/10.3390/info13090423,
    URL. Google Scholar [29] Talha M., Mumtaz R., Rafay A. Paving the way to cardiovascular
    health monitoring using Internet of Medical Things and Edge-AI 2022 2nd international
    conference on digital futures and transformative technologies, iCoDT2 (2022),
    10.1109/ICoDT255437.2022.9787432 Google Scholar [30] Devarajan M., Subramaniyaswamy
    V., Vijayakumar V., Ravi L. Fog-assisted personalized healthcare-support system
    for remote patients with diabetes J Ambient Intell Humaniz Comput, 10 (10) (2019),
    pp. 3747-3760, 10.1007/s12652-019-01291-5 View in ScopusGoogle Scholar [31] Al-Rakhami
    M., Gumaei A., Alsahli M., Hassan M.M., Alamri A., Guerrieri A., Fortino G. A
    lightweight and cost effective edge intelligence architecture based on containerization
    technology World Wide Web, 23 (2) (2020), pp. 1341-1360, 10.1007/s11280-019-00692-y
    View in ScopusGoogle Scholar [32] Yang Y., Geng Y., Qiu L., Hu W., Cao G. Context-aware
    task offloading for wearable devices 2017 26th international conference on computer
    communication and networks, ICCCN (2017), pp. 1-9, 10.1109/ICCCN.2017.8038470
    View PDFView articleGoogle Scholar [33] Pattnaik B.S., Pattanayak A.S., Udgata
    S.K., Panda A.K. Advanced centralized and distributed SVM models over different
    IoT levels for edge layer intelligence and control Evol Intell, 15 (1) (2022),
    pp. 481-495, 10.1007/s12065-020-00524-3 View in ScopusGoogle Scholar [34] Ul Alam
    M, Rahmani R. Federated semi-supervised multi-task learning to detect covid-19
    and lungs segmentation marking using chest radiography images and raspberry pi
    devices: An internet of medical things application. Sensors 21(15). http://dx.doi.org/10.3390/s21155025.
    Google Scholar [35] Gómez-Carmona O., Casado-Mansilla D., Kraemer F.A., López-de
    Ipiña D., García-Zubia J. Exploring the computational cost of machine learning
    at the edge for human-centric Internet of Things Future Gener Comput Syst, 112
    (2020), pp. 670-683, 10.1016/j.future.2020.06.013 URL https://www.sciencedirect.com/science/article/pii/S0167739X20304106
    View PDFView articleView in ScopusGoogle Scholar [36] Hoi S.C.H., Sahoo D., Lu
    J., Zhao P. Online learning: A comprehensive survey Neurocomputing, 459 (2021),
    pp. 249-289, 10.1016/j.neucom.2021.04.112 URL https://www.sciencedirect.com/science/article/pii/S0925231221006706
    View PDFView articleView in ScopusGoogle Scholar [37] Symeonidis G., Nerantzis
    E., Kazakis A., Papakostas G.A. Mlops - definitions, tools and challenges 12th
    IEEE annual computing and communication workshop and conference, CCWC 2022, las
    vegas, NV, USA, January 26-29, 2022, IEEE (2022), pp. 453-460, 10.1109/CCWC54503.2022.9720902
    View in ScopusGoogle Scholar [38] Mäkinen S., Skogström H., Laaksonen E., Mikkonen
    T. Who needs mlops: What data scientists seek to accomplish and how can mlops
    help? 1st IEEE/ACM workshop on AI engineering - software engineering for AI, wAIN@iCSE
    2021, madrid, Spain, May 30-31, 2021, IEEE (2021), pp. 109-112, 10.1109/WAIN52551.2021.00024
    View in ScopusGoogle Scholar [39] Subramanya R., Sierla S., Vyatkin V. From devops
    to mlops: Overview and application to electricity market forecasting Appl Sci,
    12 (19) (2022), p. 9851, 10.3390/app12199851 View in ScopusGoogle Scholar [40]
    Breck E., Cai S., Nielsen E., Salib M., Sculley D. The ML test score: A rubric
    for ML production readiness and technical debt reduction 2017 IEEE international
    conference on big data (big data), IEEE (2017), pp. 1123-1132 CrossRefView in
    ScopusGoogle Scholar [41] Kolltveit A.B., Li J. Operationalizing machine learning
    models - A systematic literature review Proceedings - workshop on software engineering
    for responsible AI, SE4RAI, Vol. 2022 (2022), pp. 1-8, 10.1145/3526073.3527584
    View in ScopusGoogle Scholar [42] Sculley D., Holt G., Golovin D., Davydov E.,
    Phillips T., Ebner D., Chaudhary V., Young M., Crespo J., Dennison D. Hidden technical
    debt in machine learning systems Advances in neural information processing systems,
    Vol. 28 (2015), pp. 2503-2511 URL https://proceedings.neurips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html
    Google Scholar [43] Wu C., Brooks D., Chen K., Chen D., Choudhury S., Dukhan M.,
    Hazelwood K.M., Isaac E., Jia Y., Jia B., Leyvand T., Lu H., Lu Y., Qiao L., Reagen
    B., Spisak J., Sun F., Tulloch A., Vajda P., Wang X., Wang Y., Wasti B., Wu Y.,
    Xian R., Yoo S., Zhang P. Machine learning at facebook: Understanding inference
    at the edge 25th IEEE international symposium on high performance computer architecture,
    HPCA 2019, washington, DC, USA, February 16-20, 2019, IEEE (2019), 10.1109/HPCA.2019.00048
    Google Scholar [44] Min C, Mathur A, Acer UG, Montanari A, Kawsar F. Sensix++:
    Bringing mlops and multi-tenant model serving to sensory edge devices, arXiv preprint
    arXiv:2109.03947. Google Scholar [45] Roh Y., Heo G., Whang S.E. A survey on data
    collection for machine learning: A big data - AI integration perspective IEEE
    Trans Knowl Data Eng, 33 (4) (2021), pp. 1328-1347, 10.1109/TKDE.2019.2946162
    View in ScopusGoogle Scholar [46] Lv Z., Chen D., Lou R., Wang Q. Intelligent
    edge computing based on machine learning for smart city Future Gener Comput Syst,
    115 (2021), pp. 90-99, 10.1016/j.future.2020.08.037 View PDFView articleView in
    ScopusGoogle Scholar [47] Budd S., Robinson E.C., Kainz B. A survey on active
    learning and human-in-the-loop deep learning for medical image analysis Med Image
    Anal, 71 (2021), Article 102062 View PDFView articleView in ScopusGoogle Scholar
    [48] Hymel S, Banbury CR, Situnayake D, Elium A, Ward C, Kelcey M, Baaijens M,
    Majchrzycki M, Plunkett J, Tischler D, Grande A, Moreau L, Maslov D, Beavis A,
    Jongboom J, Reddi VJ. Edge impulse: An mlops platform for tiny machine learning,
    http://dx.doi.org/10.48550/arXiv.2212.03332 CoRR abs/2212.03332 arXiv:2212.03332.
    Google Scholar [49] Ding C., Zhou A., Liu X., Ma X., Wang S. Resource-aware feature
    extraction in mobile edge computing IEEE Trans Mob Comput, 21 (1) (2022), pp.
    321-331, 10.1109/TMC.2020.3007456 View in ScopusGoogle Scholar [50] Hazelwood
    K.M., Bird S., Brooks D.M., Chintala S., Diril U., Dzhulgakov D., Fawzy M., Jia
    B., Jia Y., Kalro A., Law J., Lee K., Lu J., Noordhuis P., Smelyanskiy M., Xiong
    L., Wang X. Applied machine learning at facebook: A datacenter infrastructure
    perspective IEEE international symposium on high performance computer architecture,
    HPCA, IEEE Computer Society (2018), pp. 620-629, 10.1109/HPCA.2018.00059 View
    in ScopusGoogle Scholar [51] Cui G., He Q., Li B., Xia X., Chen F., Jin H., Xiang
    Y., Yang Y. Efficient verification of edge data integrity in edge computing environment
    IEEE Trans Serv Comput, 15 (6) (2021), pp. 3233-3244 Google Scholar [52] Shahraki
    A., Ohlenforst T., Kreyß F. When machine learning meets network management and
    orchestration in edge-based networking paradigms J Netw Comput Appl, 212 (2023),
    Article 103558 View PDFView articleView in ScopusGoogle Scholar [53] Zaharia M.,
    Chen A., Davidson A., Ghodsi A., Hong S.A., Konwinski A., Murching S., Nykodym
    T., Ogilvie P., Parkhe M., Xie F., Zumar C. Accelerating the machine learning
    lifecycle with mlflow IEEE Data Eng Bull, 41 (4) (2018), pp. 39-45 URL http://sites.computer.org/debull/A18dec/p39.pdf
    Google Scholar [54] Chen J., Ran X. Deep learning with edge computing: A review
    Proc IEEE, 107 (8) (2019), pp. 1655-1674 CrossRefView in ScopusGoogle Scholar
    [55] Li Y., Yuan G., Wen Y., Hu J., Evangelidis G., Tulyakov S., Wang Y., Ren
    J. Efficientformer: Vision transformers at mobilenet speed Adv Neural Inf Process
    Syst, 35 (2022), pp. 12934-12949 Google Scholar [56] Dwivedy V., Shukla H.D.,
    Roy P.K. Lmnet: Lightweight multi-scale convolutional neural network architecture
    for covid-19 detection in iomt environment Comput Electr Eng, 103 (2022), Article
    108325 View PDFView articleView in ScopusGoogle Scholar [57] Datta Gupta K., Sharma
    D.K., Ahmed S., Gupta H., Gupta D., Hsu C.-H. A novel lightweight deep learning-based
    histopathological image classification model for iomt Neural Process Lett, 55
    (1) (2023), pp. 205-228 CrossRefView in ScopusGoogle Scholar [58] Ogundokun R.O.,
    Misra S., Akinrotimi A.O., Ogul H. Mobilenet-svm: A lightweight deep transfer
    learning model to diagnose bch scans for iomt-based imaging sensors Sensors, 23
    (2) (2023), p. 656 CrossRefView in ScopusGoogle Scholar [59] Rausch T., Hummer
    W., Muthusamy V., Rashed A., Dustdar S. Towards a serverless platform for edge
    AI Ahmad I., Sundararaman S. (Eds.), 2nd USeNIX workshop on hot topics in edge
    computing, hotEdge 2019, renton, WA, USA, July 9, 2019, USENIX Association (2019),
    pp. 1-7 URL https://www.usenix.org/conference/hotedge19/presentation/rausch CrossRefGoogle
    Scholar [60] Navaz A.N., Serhani M.A., El Kassabi H.T. Federated quality profiling:
    A quality evaluation of patient monitoring at the edge 2022 international wireless
    communications and mobile computing, IWCMC, IEEE (2022), pp. 1015-1021 CrossRefView
    in ScopusGoogle Scholar [61] Syrigos I., Angelopoulos N., Korakis T. Optimization
    of execution for machine learning applications in the computing continuum 2022
    IEEE conference on standards for communications and networking, CSCN, IEEE (2022),
    pp. 118-123 CrossRefView in ScopusGoogle Scholar [62] Bhattacharjee A., Chhokra
    A.D., Sun H., Shekhar S., Gokhale A., Karsai G., Dubey A. Deep-edge: An efficient
    framework for deep learning model update on heterogeneous edge 2020 IEEE 4th international
    conference on fog and edge computing, ICFEC, IEEE (2020), pp. 75-84 CrossRefView
    in ScopusGoogle Scholar [63] Zhu T., Kuang L., Daniels J., Herrero P., Li K.,
    Georgiou P. Iomt-enabled real-time blood glucose prediction with deep learning
    and edge computing IEEE Internet Things J, 10 (5) (2022), pp. 3706-3719 Google
    Scholar [64] Tamburri D.A. Sustainable mlops: Trends and challenges 22nd international
    symposium on symbolic and numeric algorithms for scientific computing, SYNASC
    2020, timisoara, Romania, September 1-4, 2020, IEEE (2020), pp. 17-23, 10.1109/SYNASC51798.2020.00015
    View in ScopusGoogle Scholar [65] Chen R., Pu Y., Shi B., Wu W. An automatic model
    management system and its implementation for AIOps on microservice platforms J
    Supercomput, 79 (10) (2023), pp. 11410-11426, 10.1007/s11227-023-05123-4 View
    in ScopusGoogle Scholar [66] Yumo L. An open-source and portable mLOps pipeline
    for continuous training and continuous deployment (Ph.D. thesis) Faculty of Science
    - University of Helsinki (2023) Google Scholar [67] Kairouz P., McMahan H.B.,
    Avent B., Bellet A., Bennis M., Bhagoji A.N., Bonawitz K.A., Charles Z., Cormode
    G., Cummings R., D’Oliveira R.G.L., Eichner H., Rouayheb S.E., Evans D., Gardner
    J., Garrett Z., Gascón A., Ghazi B., Gibbons P.B., Gruteser M., Harchaoui Z.,
    He C., He L., Huo Z., Hutchinson B., Hsu J., Jaggi M., Javidi T., Joshi G., Khodak
    M., Konečný J., Korolova A., Koushanfar F., Koyejo S., Lepoint T., Liu Y., Mittal
    P., Mohri M., Nock R., Özgür A., Pagh R., Qi H., Ramage D., Raskar R., Raykova
    M., Song D., Song W., Stich S.U., Sun Z., Suresh A.T., Tramèr F., Vepakomma P.,
    Wang J., Xiong L., Xu Z., Yang Q., Yu F.X., Yu H., Zhao S. Advances and open problems
    in federated learning Found Trends Mach Learn, 14 (1–2) (2021), pp. 1-210, 10.1561/2200000083
    View in ScopusGoogle Scholar [68] Xu J., Chen L., Ren S. Online learning for offloading
    and autoscaling in energy harvesting mobile edge computing IEEE Trans Cogn Commun
    Netw, 3 (3) (2017), pp. 361-373, 10.1109/TCCN.2017.2725277 View in ScopusGoogle
    Scholar [69] Studer S., Bui T.B., Drescher C., Hanuschkin A., Winkler L., Peters
    S., Müller K. Towards CRISP-ML(Q): A machine learning process model with quality
    assurance methodology Mach Learn Knowl Extr, 3 (2) (2021), pp. 392-413, 10.3390/make3020020
    View in ScopusGoogle Scholar [70] Wirth R, Hipp J. Crisp-dm: Towards a standard
    process model for data mining. In: Proceedings of the 4th international conference
    on the practical applications of knowledge discovery and data mining. Vol. 1,
    Manchester; 2000, p. 29–39. Google Scholar [71] Marbán O., Segovia J., Ruiz E.M.,
    Fernández-Baizán C. Toward data mining engineering: A software engineering approach
    Inf Syst, 34 (1) (2009), pp. 87-107, 10.1016/j.is.2008.04.003 View PDFView articleView
    in ScopusGoogle Scholar [72] Reddy S., Allan S., Coghlan S., Cooper P. A governance
    model for the application of AI in health care J Am Med Inform Assoc, 27 (3) (2020),
    pp. 491-497, 10.1093/jamia/ocz192 View in ScopusGoogle Scholar [73] Skogström
    H. The MLOps stack (2020) https://valohai.com/blog/the-mlops-stack/ Google Scholar
    [74] Antonini M., Pincheira M., Vecchio M., Antonelli F. An adaptable and unsupervised
    tinyml anomaly detection system for extreme industrial environments Sensors, 23
    (4) (2023), p. 2344, 10.3390/s23042344 View in ScopusGoogle Scholar [75] Kreuzberger
    D, Kühl N, Hirschl S. Machine learning operations (mlops): Overview, definition,
    and architecture. IEEE Access. Google Scholar [76] Hossain MD, Sultana T, Akhter
    S, Hossain MI, Thu NT, Huynh LN, Lee G-W, Huh E-N. The role of microservice approach
    in edge computing: Opportunities, challenges, and research directions. ICT Express.
    Google Scholar [77] Zhang H., Xiao J., Wang J., Yang H. Resource virtualization
    in edge computing: A review 2022 global conference on robotics, artificial intelligence
    and information technology, GCRAIT, IEEE (2022), pp. 123-127 CrossRefView in ScopusGoogle
    Scholar [78] Nguyen D.C., Ding M., Pathirana P.N., Seneviratne A., Li J., Poor
    H.V. Federated learning for internet of things: A comprehensive survey IEEE Commun
    Surv Tutor, 23 (3) (2021), pp. 1622-1658 CrossRefView in ScopusGoogle Scholar
    [79] Oyebode O., Fowles J., Steeves D., Orji R. Machine learning techniques in
    adaptive and personalized systems for health and wellness Int J Hum–Comput Interact,
    39 (9) (2023), pp. 1938-1962 CrossRefView in ScopusGoogle Scholar [80] Kakhi K.,
    Alizadehsani R., Kabir H.D., Khosravi A., Nahavandi S., Acharya U.R. The internet
    of medical things and artificial intelligence: trends, challenges, and opportunities
    Biocybern Biomed Eng, 42 (3) (2022), pp. 749-771 View PDFView articleView in ScopusGoogle
    Scholar [81] Thapa C., Camtepe S. Precision health data: Requirements, challenges
    and existing techniques for data security and privacy Comput Biol Med, 129 (2021),
    Article 104130 View PDFView articleView in ScopusGoogle Scholar [82] Vajar P.,
    Emmanuel A.L., Ghasemieh A., Bahrami P., Kashef R. The internet of medical things
    (iomt): a vision on learning, privacy, and computing 2021 international conference
    on electrical, computer, communications and mechatronics engineering, ICECCME,
    IEEE (2021), pp. 1-7 CrossRefGoogle Scholar [83] Koutras D., Stergiopoulos G.,
    Dasaklis T., Kotzanikolaou P., Glynos D., Douligeris C. Security in iomt communications:
    A survey Sensors, 20 (17) (2020), p. 4828 CrossRefGoogle Scholar [84] Hameed S.S.,
    Hassan W.H., Latiff L.A., Ghabban F. A systematic review of security and privacy
    issues in the internet of medical things; the role of machine learning approaches
    PeerJ Comput Sci, 7 (2021), Article e414 CrossRefGoogle Scholar [85] Rbah Y.,
    Mahfoudi M., Balboul Y., Fattah M., Mazer S., Elbekkali M., Bernoussi B. Machine
    learning and deep learning methods for intrusion detection systems in iomt: A
    survey 2022 2nd international conference on innovative research in applied science,
    engineering and technology, IRASET, IEEE (2022), pp. 1-9 CrossRefGoogle Scholar
    [86] Papaioannou M., Karageorgou M., Mantas G., Sucasas V., Essop I., Rodriguez
    J., Lymberopoulos D. A survey on security threats and countermeasures in internet
    of medical things (iomt) Trans Emerg Telecommun Technol, 33 (6) (2022), Article
    e4049 View in ScopusGoogle Scholar [87] Tang Q, Chen Z, Guo Y, Liang Y, Ward R,
    Menon C, Elgendi M. Robust reconstruction of electrocardiogram using photoplethysmography:
    A subject-based model. Front Physiol 13. http://dx.doi.org/10.3389/fphys.2022.859763.
    Google Scholar Cited by (0) ☆ Part of the results presented in this work were
    obtained through the project “CENTER OF EXCELLENCE IN ARTIFICIAL INTELLIGENCE
    – AI4WELLNESS”, funded by Samsung Eletrônica da Amazônia Ltda., at Federal University
    of Ceará, Brazil, under the Information Technology Law No. 8.248/91. 1 Ph.D. 2
    High School, Undergraduate Student. 3 M.Sc. 4 https://www.kubeflow.org/. 5 https://kubernetes.io/.
    View Abstract © 2024 Elsevier Ltd. All rights reserved. Recommended articles OCReP:
    An Optimally Conditioned Regularization for pseudoinversion based neural training
    Neural Networks, Volume 71, 2015, pp. 76-87 Rossella Cancelliere, …, Luca Rubini
    View PDF Enhancing urban economic efficiency through smart city development: A
    focus on sustainable transportation Computers and Electrical Engineering, Volume
    116, 2024, Article 109058 Haojie Jiang, Manjiang Xing View PDF Intensions and
    extensions of granules: A two-component treatment International Journal of Approximate
    Reasoning, Volume 169, 2024, Article 109182 Tamás Mihálydeák, …, Mihir K. Chakraborty
    View PDF Show 3 more articles About ScienceDirect Remote access Shopping cart
    Advertise Contact and support Terms and conditions Privacy policy Cookies are
    used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply."'
  inline_citation: '>'
  journal: Computers and Electrical Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Edge AI for Internet of Medical Things: A literature review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gill S.S.
  - Wu H.
  - Patros P.
  - Ottaviani C.
  - Arora P.
  - Pujol V.C.
  - Haunschild D.
  - Parlikad A.K.
  - Cetinkaya O.
  - Lutfiyya H.
  - Stankovski V.
  - Li R.
  - Ding Y.
  - Qadir J.
  - Abraham A.
  - Ghosh S.K.
  - Song H.H.
  - Sakellariou R.
  - Rana O.
  - Rodrigues J.J.P.C.
  - Kanhere S.S.
  - Dustdar S.
  - Uhlig S.
  - Ramamohanarao K.
  - Buyya R.
  citation_count: '1'
  description: Over the past six decades, the computing systems field has experienced
    significant transformations, profoundly impacting society with transformational
    developments, such as the Internet and the commodification of computing. Underpinned
    by technological advancements, computer systems, far from being static, have been
    continuously evolving and adapting to cover multifaceted societal niches. This
    has led to new paradigms such as cloud, fog, edge computing, and the Internet
    of Things (IoT), which offer fresh economic and creative opportunities. Nevertheless,
    this rapid change poses complex research challenges, especially in maximizing
    potential and enhancing functionality. As such, to maintain an economical level
    of performance that meets ever-tighter requirements, one must understand the drivers
    of new model emergence and expansion, and how contemporary challenges differ from
    past ones. To that end, this article investigates and assesses the factors influencing
    the evolution of computing systems, covering established systems and architectures
    as well as newer developments, such as serverless computing, quantum computing,
    and on-device AI on edge devices. Trends emerge when one traces technological
    trajectory, which includes the rapid obsolescence of frameworks due to business
    and technical constraints, a move towards specialized systems and models, and
    varying approaches to centralized and decentralized control. This comprehensive
    review of modern computing systems looks ahead to the future of research in the
    field, highlighting key challenges and emerging trends, and underscoring their
    importance in cost-effectively driving technological progress.
  doi: 10.1016/j.teler.2024.100116
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Early
    computing to modern computing: A vision 3. Evolution of computing paradigms: Technological
    drivers 4. Classification of computing: Paradigms, technologies and trends 5.
    Impact and performance criteria 6. Emerging trends in modern computing 7. Summary
    and conclusions CRediT authorship contribution statement Declaration of competing
    interest Acknowledgments Appendix. List of acronyms Data availability References
    Show full outline Cited by (3) Figures (3) Tables (6) Table 1 Table 2 Table 3
    Table 4 Table 5 Table 6 Telematics and Informatics Reports Volume 13, March 2024,
    100116 Modern computing: Vision and challenges Author links open overlay panel
    Sukhpal Singh Gill a, Huaming Wu b, Panos Patros c, Carlo Ottaviani d, Priyansh
    Arora e, Victor Casamayor Pujol f, David Haunschild g, Ajith Kumar Parlikad h,
    Oktay Cetinkaya i, Hanan Lutfiyya j, Vlado Stankovski k, Ruidong Li l, Yuemin
    Ding m, Junaid Qadir n, Ajith Abraham o p, Soumya K. Ghosh q, Houbing Herbert
    Song r, Rizos Sakellariou s, Omer Rana t, Joel J.P.C. Rodrigues u v…Rajkumar Buyya
    y Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.teler.2024.100116
    Get rights and content Under a Creative Commons license open access Highlights
    • We explore the evolution of computing paradigms & technological drivers (1960
    onward). • We offer a taxonomy of modern computing based on impact and performance
    criteria. • We classify computing based on paradigms, technologies, impact areas,
    and trends. • We identify open challenges and research directions for computing
    traits. • We introduce a hype cycle for modern computing systems, spotlighting
    emerging trends. Abstract Over the past six decades, the computing systems field
    has experienced significant transformations, profoundly impacting society with
    transformational developments, such as the Internet and the commodification of
    computing. Underpinned by technological advancements, computer systems, far from
    being static, have been continuously evolving and adapting to cover multifaceted
    societal niches. This has led to new paradigms such as cloud, fog, edge computing,
    and the Internet of Things (IoT), which offer fresh economic and creative opportunities.
    Nevertheless, this rapid change poses complex research challenges, especially
    in maximizing potential and enhancing functionality. As such, to maintain an economical
    level of performance that meets ever-tighter requirements, one must understand
    the drivers of new model emergence and expansion, and how contemporary challenges
    differ from past ones. To that end, this article investigates and assesses the
    factors influencing the evolution of computing systems, covering established systems
    and architectures as well as newer developments, such as serverless computing,
    quantum computing, and on-device AI on edge devices. Trends emerge when one traces
    technological trajectory, which includes the rapid obsolescence of frameworks
    due to business and technical constraints, a move towards specialized systems
    and models, and varying approaches to centralized and decentralized control. This
    comprehensive review of modern computing systems looks ahead to the future of
    research in the field, highlighting key challenges and emerging trends, and underscoring
    their importance in cost-effectively driving technological progress. Graphical
    abstract Download : Download high-res image (336KB) Download : Download full-size
    image Previous article in issue Next article in issue Keywords Modern computingEdge
    AIEdge computingArtificial IntelligenceMachine learningCloud computingQuantum
    computingComputing 1. Introduction The Internet, the expansive computational backbone
    of interactive machines, is largely responsible for the 21st century’s social,
    financial, and technological growth [1]. The growing reliance on the computing
    resources it encapsulates has pushed the complexity and scope of such platforms,
    leading to the development of innovative computing systems. These systems have
    genuinely improved the capabilities and expectations of computing equipment driven
    by rapid technical and user-driven evolution [2]. For instance, vintage mainframes
    combined centralized data processing and storage with transmission interfaces
    for user input. Due to advancements in clusters and packet-switching technologies,
    microchip gadgets, and Graphical User Interfaces (GUIs), technology originally
    shifted from big, centrally-run mainframe computers to Personal Computers (PCs).
    The globalization of network standards made it possible for interconnected networks
    worldwide to communicate and share data [3]. Businesses slowly combined sensor
    and actuator goals with built-in network connectivity by creating architectures
    and standards that submit tasks to remote pools of computing resources, such as
    memory, storage, and data processing [4]. As a result, newer models like the Internet
    of Things (IoT) and edge computing are now beginning to expand the reach of technology
    outside the confines of traditional network nodes [5]. Over the past six decades,
    computing models have fundamentally shifted to address the problems posed by the
    ever-evolving nature of our civilization and its associated computer system architectures
    [6]. The evolution of computing from mainframes to workstations to the cloud to
    autonomous and decentralized architectures, such as edge computing and IoT technologies,
    however, maintains identical core parts and traits that characterize their function
    [7]. Research in computing underpins all of them! Advancements in areas like security,
    computer hardware acceleration, edge computing, and energy efficiency typically
    serve as catalysts for innovation and entrepreneurship that span across various
    business domains [8]. While computing systems and other forms of system integration
    create new problems/opportunities, software frameworks have been developed to
    address them. Thus, middleware, network protocols, and safe segregation techniques
    must be continually developed and refined to support novel computing systems—and
    their innovative use cases. 1.1. Motivation By tracking the effect of computing
    systems on the community, this comprehensive study seeks to (a) establish the
    essential features and components of modern computing systems, (b) thoroughly
    assess the development of innovations and behavioral patterns that inspired the
    invention of these paradigms, and (c) recognize significant developments throughout
    the models, such as the integration of system design, the shifting between centralization
    and decentralization, and lags in model conceptualization and development. This
    investigation suggests that next-generation computing systems will facilitate
    the decentralization of computational services. This will be achieved via the
    composition of decentralized calculation tools with workload-specific targets
    for performance to create dramatically more complex structures. These will satisfy
    holistic operational demands, such as improved capacity and power accessibility.
    1.2. Related surveys and our contributions Computing being a rapidly growing topic,
    the time is right for a novel, forward-thinking study to summarize, improve, and
    integrate the existing and newly-generated information, and to explore possible
    trends and future viewpoints. Previously, Pujol et al. [9] provided a survey on
    distributed computing continuum systems that focused on business models. Further
    back in 2018, Buyya et al. [1] presented a manifesto on fundamental issues, developments,
    and impacts in cloud computing research. Meanwhile, Gill et al. [4] offered a
    visionary survey of advances in computing paradigms for fog, edge, and serverless
    computing. Further, Shalf [10] summarized the 2020 state of the art of technological
    roadmaps and their implications for the future of systems, including what a post-exascale
    system would entail. Finally, in 2021, Angel et al. [11] reviewed leading computational
    frameworks for cloud and edge computing, and showcased breakthroughs that had
    been brought about via the merging of Machine Learning (ML) with these models.
    In order to evaluate and identify the most pressing research issues of modern
    computing, we have developed the very first taxonomy of its type. We performed
    a gap analysis of the current surveys using several criteria, as shown in Table
    1, which underpinned the design of our work. Hence, our study uniquely contributes
    by (a) exploring the history of computing paradigm shifts with a focus on technology
    drivers, (b) providing a thorough taxonomy of computing systems, (c) introducing
    the hype cycle for modern computing systems with a focus on new trends, and (d)
    discussing the effects and cost-effective performance requirements of modern computing.
    The key contributions of this article are summarized as follows: Table 1. Comparison
    of this work with existing studies. Work [9] [1] [4] [10] [11] Our Work Year 2023
    2018 2022 2020 2021 2024 A Taxonomy of Modern Computing ✓ Evolution of Computing
    Paradigms (1960 to 2023) ✓ Classification of Computing Standalone vs. Networked
    Computing ✓ General Purpose vs. Specialized Computing ✓ Centralized vs. Decentralized
    Computing ✓ Computing Trends and Emerging Technologies ✓ ✓ ✓ ✓ ✓ ✓ Computational
    Methodologies: Parallel vs. Sequential Computing ✓ Traits of Computing Focus/
    Paradigms ✓ ✓ Technologies/ Impact Areas ✓ Trends/ Observations ✓ Impact and Performance
    Criteria Performance Metrics ✓ Efficiency Metrics ✓ Social Impact ✓ Security and
    Compliance ✓ Economic and Management ✓ Open Challenges and Future Directions ✓
    ✓ ✓ ✓ ✓ ✓ Emerging Trends in Modern Computing: Hype Cycle ✓ • It offers a concise
    overview of the transition from early to modern computing. • The study explores
    the evolution of computing paradigms, focusing on technological drivers (1960–2023).
    • Following a novel methodology, the article produces a taxonomy of modern computing
    based on traits of computing such as (1) focus or paradigms; (2) technologies
    or impact areas; and (3) trends or observations. • It presents a comprehensive
    classification of computing: (1) Standalone vs. Networked Computing; (2) General
    Purpose vs. Specialized Computing, (3) Centralized vs. Decentralized Computing,
    (4) Computing Trends and Emerging Technologies; and (5) Computational Methodologies:
    Parallel vs. Sequential Computing. • The study identifies the impact and performance
    criteria of modern computing in terms of performance metrics, efficiency metrics,
    social impact, security and compliance, and economics and management. • It provides
    an in-depth summary of computing traits and resources for further research. •
    The article identifies open challenges and research directions for the traits
    of computing. • Finally, it introduces the hype cycle for modern computing systems,
    spotlighting emerging trends. 1.3. Article organization The article is organized
    as follows: Section 2 offers a concise overview of the transition from early to
    modern computing. Section 3 explores the evolution of computing paradigms, focusing
    on technological drivers. Section 4 presents a classification of computing systems,
    and Section 5 examines the impact and performance criteria in modern computing.
    The article concludes in Section 7, summarizing computing-related technologies
    and trends through a hype cycle in Section 6. The list of acronyms used in this
    study is given in Appendix. 2. Early computing to modern computing: A vision Over
    the last six decades, advancements in computing systems have optimized the efficiency
    of the available hardware [12]. Over this time period, novel computing models
    and innovations have been developed and replaced the previous state-of-the-art,
    all of which incrementally contribute to the current technology status [2]. Fig.
    1 shows the transition from early computing to contemporary computing. Originally,
    a single system could only carry out a single task; hence, a user needed various
    systems working in tandem to achieve their desired tasks. However, to safely share
    information between computers – in order to overcome the problem of executing
    only one task at a time – a reliable communication mechanism is essential [13].
    To that end, our investigation unfolds across three key sections: Section 3 delves
    into the evolution of computing paradigms, emphasizing technological drivers.
    Section 4 offers a comprehensive classification of computing systems. The discussion
    in Section 5 revolves around the impact and performance criteria of modern computing.
    Section 6 introduces the hype cycle for modern computing systems, spotlighting
    emerging trends. Download : Download high-res image (2MB) Download : Download
    full-size image Fig. 1. Modern computing: A taxonomy. 3. Evolution of computing
    paradigms: Technological drivers Fig. 1 illustrates the progression of computing
    technology starting from the year 1960. 3.1. Client server In the year 1960, a
    centralized platform (a.k.a, distribution integration) was developed to share
    workloads (a.k.a., jobs) between the resource providers (i.e., server instances)
    and service consumers (i.e., customers) [12]. Supporting it, a networking system
    was utilized for communications between client devices and servers, and servers
    exchange resources for customers to perform their tasks using a load balancing
    mechanism [14]. Illustrative examples of the client–server model’s application
    include the Email and the World Wide Web (WWW). However, users in this configuration
    were unable to freely interact with one another. 3.2. Supercomputer A supercomputer
    is a powerful computer with extraordinary processing capability, such that it
    can handle complex calculations in several areas of science, including climate
    study, quantum physics, and molecular simulation [15]. Energy utilization and
    heat control in supercomputers endured as a key research problem throughout their
    growth in the 1960s [16]. Supercomputers, such as Multivac, HAL-9000, and Machine
    Stops, have been instrumental in underpinning/enabling dramatic technological
    advancements [14]. 3.3. Proprietary mainframe To handle massive amounts of data
    (including dealing with transactions, customer data analysis, and censuses), a
    high-speed machine with large computing power is required [17]. Virtualization
    on mainframes allows for increased efficiency, protection, and dependability.
    In the year 2017, IBM announced the newest version of its mainframe, the IBM z14
    [13]. Being built to support massive economic activity and despite their high
    price tag, mainframe computers deliver outstanding efficiency [14]. 3.4. Cluster
    computing Cluster computing is a method of increasing the efficiency of a computing
    system by utilizing several nodes to complete a single operation [18]. In order
    to coordinate various computing nodes, this type of technology requires a rapid
    Local Area Network (LAN) for exchanging information among them [19]. 3.5. Home
    PCs The early days of the Internet coincided with the flourishing of PC kept at
    one’s home [3]. The Internet was evolving into a foundational network, connecting
    local networks to the larger Internet using self-adaptive network protocols, such
    as Transmission Control Protocol/Internet Protocol (TCP/IP)—in contrast to the
    original Network Control Protocol (NCP)-based Advanced Research Projects Agency
    Network (ARPANET) mechanisms [2]. As a result, there was a sharp increase in the
    number of hosts on the Internet, which quickly overwhelmed centralized naming
    technologies like HOSTS.TXT. In the year 1985, the earliest publicly available
    version of a Domain Name System (DNS) was released for the Unix BIND system [20].
    This system translates hostnames into IP addresses. Pioneer Windows, Icons, Menus,
    and Pointers (WIMP)-based GUIs on computers, such as the Xerox Star and the Apple
    LISA, proved that customers could successfully use machines in their homes for
    tasks like playing video games and surfing the Internet [21]. 3.6. Open MPP/SMP
    Massive Parallel Processing (MPP) and Symmetric Multi-Processing (SMP) systems
    are the two most common forms of parallel computing platforms [16]. In an SMP
    setup, multiple processors run the same Operating System (OS) concurrently while
    sharing the rest of the hardware’s capacity (e.g., disc space and RAM). Naturally,
    resource pooling influences the computational speed of completing a given assignment.
    In an MPP scenario, the file system can be shared, while no other resources are
    pooled for use during task processing [14]. Incorporating more machines and their
    associated storage and RAM space, increases the ability to scale according to
    the Universal Scalability Law (an extension of Amdahl’s Law), assuming the proportion
    of work that can be parallelized and the interprocess communication penalty: (1)
    3.7. Grid computing This technology enables a group to work together towards the
    same objective by executing non-interactive, and largely IO-intensive tasks [19].
    Each application running on only one grid is a top priority [12]. In addition
    to allocating and managing resources, grid computing also offers a reliable architecture,
    as well as tracking and exploration support. 3.8. WWW The primary web browsers,
    websites, and web servers all came into existence in the later stages of the 1980s
    and early 1990s, underpinned by the development of Hyper Text Transport Protocol
    (HTTP) and Hyper Text Markup Language (HTML) [2]. The platform for the interconnected
    system of networks that makes up the WWW was made possible by the standardizing
    technology of TCP/IP network protocols. This allowed for a dramatic increase in
    the total number of servers linked to the Web and introduced Information Technology
    (IT) to the general public. Software applications were thus able to communicate
    with one another beyond address spaces and networking, e.g., via novel technologies
    like Remote Procedure Calls (RPCs) [22]. 3.9. Commodity clusters Commodity cluster
    computing employs several computers simultaneously, which can inexpensively execute
    user tasks [19]. In an effort to standardize their processes, several companies
    use open standards while building commodity computers [14]. This allowed immediate
    computing business needs to be met using ready-made processors. 3.10. Peer to
    peer (P2P) P2P is a distributed framework to share workloads or jobs amongst multiple
    peers; alternatively, computers and peers may interact with one another openly
    at the application layer [23]. With no mediator in the center, users of a peer-to-peer
    system can share resources like memory, CPU speed, and storage space. Peer-to-peer
    communication utilizes the TCP/IP protocol suite [24]. Interactive media, sharing
    file infrastructure, and content distribution are some of the most common use
    cases for P2P technology. 3.11. Web services The technology supporting web services
    enables the exchange of data between various Internet-connected devices in machine-understandable
    data formats, such as JavaScript Object Notation (JSON) and Extensible Markup
    Language (XML), over the WWW [25]. Commonly, web-based services operate as a connection
    between end users and database servers. 3.12. Service-Oriented Architecture (SOA)
    The SOA paradigm enables software elements to be reused and made compatible through
    advertised service designs/Application Programming Interfaces (APIs) [26]. It
    is normally easier to include services in new apps: the apps can be architected
    to adhere to standardized protocols and leverage consistent design patterns. This
    frees the software engineer from the burden of recreating or duplicating current
    features or figuring out how to link to and interoperate with current systems—e.g.,
    via using Software Development Kits (SDKs) that implement common functionalities,
    such as networking, retries, marshaling of data and error handling [27]. Each
    SOA API exposes the logic and data necessary to carry out a single, self-contained
    business operation (such as vetting the creditworthiness of a client, determining
    the loan’s due date, or handling an insurance application) [28]. The loose integration
    provided by the service’s design allows for the service to be invoked with limited
    knowledge of the underlying service implementation. 3.13. Virtualized clusters
    Virtualization enables a guest computer system to be implemented on top of a host
    computer system, which abstracts away the problem of physically supporting and
    maintaining multiple types/architectures of physical machines [19]. With a virtualized
    cluster, several Virtual Machines (VMs) may pool their resources to complete a
    single job. VM hypervisors, which execute the guest system on the host system,
    allow software-based virtualization to run either on top of an OS or directly
    (bare-metal) on hardware [14]. Costs and complexity are reduced, and a greater
    number of tasks may be completed with identical hardware by adopting a VM-based
    system. 3.14. High Performance Computing (HPC) system HPC is the computing method
    of choice when dealing with computationally intensive issues, which tend to arise
    in the domains of commerce, technology, and research [14], [19]. A scheduler in
    an HPC system manages accessibility to the various computing resources available
    for use in solving various issues [29]. HPC systems utilize a pooled set of resources,
    allowing them to perform workloads or tasks via the allocation of concurrent resources
    and online utilization of various resources. 3.15. Autonomic computing One of
    the first global initiatives to build computer systems with minimum human involvement
    to achieve preset goals was IBM’s autonomic computing program in 2006 [30]. It
    was mostly based on research on nerves, thinking, and coordination. Autonomic
    computing research examines how software-intensive systems may make choices and
    behave autonomously to achieve user-specified goals [4]. Control for closed- and
    open-loop systems has shaped autonomic computing [31]. Complex systems can have
    several separate control networks. 3.16. Mobile computing The term “mobile computing”
    is used to describe a wide range of IT components that give consumers mobility
    in their usage of computation, information, and associated equipment and capabilities
    [32]. An especially popular definition of “mobile” is accessing information while
    moving, when an individual is not confined to a fixed place. Accessibility at
    a fixed spot may also be thought of as mobile, especially if it is provided by
    hardware that consumers can move as needed but that remains in one place while
    functioning [33]. Mobile computing devices are becoming essential across industries,
    boosting efficiency and creativity in fields such as healthcare, retail, manufacturing,
    and the arts. 3.17. Cloud computing Software as a service (SaaS), platform as
    a service (PaaS), and infrastructure as a service (IaaS) are all examples of Internet-accessible
    web services [1]. Google Mail is an excellent instance of a SaaS product since
    it provides a wide range of useful features without the burden of installation
    and ongoing upkeep costs. PaaS providers like Microsoft provide a scalable environment
    where users can install their applications [34]. Amazon is a prime instance of
    an IaaS provider since it provides users with access to servers, networks, storage,
    and other hardware components necessary to run applications and other workloads
    efficiently and effectively. Using distant facilities for performing user operations
    (processing, administration, and storage of data) over the Internet is known as
    “cloud computing”, abbreviated as “XaaS”, where X = “I”, “P”, “S”, etc. Cloud
    computing enables the pooling of resources to reduce execution costs and enhance
    service accessibility [35]. There are four major types of cloud computing systems:
    public, private, hybrid, and communal. Dependability, safety, and cost-effectiveness
    are just a few examples of Quality of service (QoS) characteristics that should
    be considered while developing a successful cloud service. 3.18. IoT Controllers,
    gadgets, and detection devices are all examples of IoT devices that can communicate
    with one another over the WWW [5]. IoT has many potential uses in many different
    areas, including farming, medical treatment, climate prediction, logistics, home
    automation, and industrial automation [36]. 3.19. Fog computing This cutting-edge
    design makes extensive use of mobile devices, also known as fog nodes, which are
    utilized for data storage and processing, and rely on the web for inter-node connectivity
    [37]. The data plane and the control plane are the two main components of fog
    computing [38]. Although the control layer is a gateway component and determines
    the network’s layout, the data plane offers capabilities at the network’s edge
    to decrease delay and boost QoS [39]. Fog computing supports IoT gadgets such
    as smartphones, detectors, and health monitors. 3.20. Edge computing Edge computing
    is a method that delegates processing to dispersed edge devices for data processing
    and information exchange [40]. In addition, edge computing enhances QoS, decreases
    delay, and lowers transmitting expenses by computing huge volumes of data on gadgets
    at the edge rather than in the public cloud [41]. However, edge computing relies
    on a constantly available web connection to perform certain tasks in a timely
    manner, so it is best used for applications that can execute autonomously without
    centralized control for prolonged periods of time [42]. 3.21. Serverless computing
    The serverless computing paradigm eliminates the need to manage servers and other
    infrastructure components [43] centrally. Since serverless computing eliminates
    the need for software engineers to manage servers, it is expected to grow much
    faster. With serverless computing, hosting companies may easily handle infrastructure
    management and automatic provisioning [44]. Because of this, less effort and resources
    are needed to oversee the infrastructure. 3.22. Osmotic computing Osmotic computing
    is a growing idea that merges IoT, cloud, fog, and edge technology for the constantly
    changing administration of IT services. The dramatic increase in the size of resources
    in the network’s periphery is the primary force behind this trend. By defining,
    creating, and implementing a computing model, this paradigm focuses on methods
    to improve edge and cloud-based IoT services [45]. To manage resources and resolve
    data difficulties in IoT and data science, osmotic computing applies the fundamental
    concepts of the osmosis phenomenon in chemistry [46]. The primary objective of
    this computing model is to distribute workloads and efficiently use available
    resources among servers without degrading service delivery or efficiency. 3.23.
    Dew computing Dew computing is “a software-hardware organization model for computers
    situated in the cloud computing environment”, where a local machine complements
    and operates independently of cloud services [47]. Dew computing may bridge the
    gap between cloud and on-premises computing. Data and services stored in the cloud
    are accessible regardless of an Internet connection. The need for constant Internet
    access is the primary restriction on cloud and fog computing. Complementing fog
    and edge computing with considerable Internet reliance, an extra layer, including
    dew computing, is necessary to keep apps and services alive and functioning. Even
    if dew computing is not conducted entirely online, it nevertheless uses cloud
    computing and depends on collaboration for data and operations, for example, One
    Drive [48]. 3.24. Quantum computing Quantum computing is a radically different
    way to analyze knowledge and data. Several possibilities can be taken advantage
    of when processing information stored in the quantum states of quantum machines
    that are unavailable when analyzing information in a conventional fashion [49].
    The phenomena of quantum entanglement and superposition are two such examples.
    Because of quantum entanglement, it is difficult to offer a comprehensive description
    from the understanding of merely the component states, which is a defining characteristic
    of quantum systems. One definition of the term “superposition” is the potential
    of merging quantum states to create a new valid quantum state [50]. The primary
    purpose driving the effort to construct a quantum computer was the modeling of
    quantum systems; however, it was not until the identification of quantum algorithms
    capable of achieving realistic objectives that the enthusiasm for constructing
    such devices began to garner increasing scrutiny [51]. 4. Classification of computing:
    Paradigms, technologies and trends In this section, we discuss the different types
    of computing and classify them into different broad categories as shown in Fig.
    1. Table 2 briefly describes traits of computing that are used in this classification
    such as (1) focus or paradigms; (2) technologies or impact areas; and (3) trends
    or observations. Table 2. Summary of computing traits. Trait Description Focus/Paradigms
    We discuss well-established computing paradigms, from client–server to quantum
    computing, which have been explored in the last decade. Technologies/Impact Areas
    We cover key research that has grown over time by utilizing these well-established
    computing paradigms and how this has led to many breakthroughs in the underlying
    technology. Trends/Observations The new trends, such as large-scale machine learning,
    digital twins, edge AI, bitcoin currency, 6G & Beyond and quantum Internet and
    biologically-inspired computing, for the next generation of computing, have come
    to light due to these advances in computing paradigms and technology. 4.1. Standalone
    vs. Networked computing Standalone computing occurs when a computer is not connected
    to another computer in any way, whether through wired or WiFi connections [52].
    Multiple computers linked together form a network, a model that falls under networked
    computing. 4.1.1. Standalone computing In this section, we discuss the main focus
    or paradigms, technologies or impact areas, and various trends or observations
    within standalone computing. 4.1.1.1. Focus/paradigms The following are the main
    focus or paradigms for standalone computing: (1) PCs: Individuals use PCs, which
    leverage microprocessors designed for personal use. Before the PC, businesses
    had to operate computers by connecting several users’ terminals to a separate,
    massive mainframe system [3]. By the end of the 1980s, technical developments
    had enabled the construction of a compact computer that a person could purchase
    and use as a word processor or for various computing objectives [2]. (2) Embedded
    Systems: A computer (often a microcontroller or microprocessor) is built into
    (i.e., embedded in) the design of a device [53]. Most of the time, an individual
    does not even realize they are using a computer because there might not be any
    obvious hints of applications, data, or software [54]. The software that operates
    a microwave oven or an engine control unit of a contemporary vehicle are two instances
    of items with undetectable integrated systems. 4.1.1.2. Technologies/impact areas
    The key technologies and affected domains for standalone computing include: (1)
    Single-board Computers (SBCs): In an SBC, the CPU, I/O, memory, and various other
    components are all housed on one integrated circuit board; the quantity of memory
    is fixed; and there are no slots to be expanded for additional hardware [55].
    (2) Raspberry Pi 4: The Raspberry Pi is a family of tiny SBCs that have been developed
    to allow programming and computing capabilities to be available to all. The Raspberry
    Pi Model B became the inaugural board produced by the foundation behind the Raspberry
    Pi [55]. Due to its immense popularity, other variants have subsequently been
    developed, each with its own set of advantages. These include the Raspberry Pi
    computation component, which has been optimized for use in embedded systems [56].
    (3) NVIDIA Jetson Series: This is a line of Graphics Processing Units (GPUs) that
    includes the initial processors built with the explicit purpose of powering self-driving
    robots [57]. With up to 32 Tera Operations Per Second (TOPS) of Artificial Intelligence
    (AI) efficiency, these GPUs efficiently handle optical measurements, sensor fusion,
    positioning, visualization, obstacle detection, and path-planning, all of which
    are essential for the development of robotics [55]. The Jetson Xavier series focuses
    on creating specialized robots and edge robots, with several distinct manufacturing
    components. 4.1.1.3. Trends/observations The main trends and observations regarding
    standalone computing are: (1) Adoption of AI/ML: NVIDIA Jetson Nano, for instance,
    enables consumers to equip billions of low-power AI/ML systems with remarkable
    new features [58]. It paves the way for a wide variety of integrated IoT services,
    such as low-cost Network Video Recorders (NVRs), consumer automation, and analytics-rich
    gateways [55]. With its ready-to-try applications and enthusiastic software developer
    community, Jetson Nano serves as the ideal tool for beginning students to gain
    knowledge about AI and robotics in real-life situations. (2) Cybersecurity: Embedded
    systems are compact, specifically designed devices built to carry out a single
    task, frequently in real-time, while using as few resources as possible [54].
    Installing protective measures on these platforms to guard against dangers like
    unauthorized usage or fraudulent attacks drives the need for embedded security
    [59]. These safeguards are included in electrical components, firmware, and applications
    to achieve an all-encompassing defense. 4.1.2. Networked computing In this section,
    we discuss the main focus or paradigms, technologies or impact areas, and various
    trends or observations within networked computing. 4.1.2.1. Focus/paradigms The
    following are the main focus or paradigms for networked computing: (1) Networking/Connectivity:
    Servers in cloud computing underpin the services and APIs provided to internal
    and external clients. Communication on several levels, both inside and among data
    centers, is essential for effectively implementing cloud services [1]. Crucially,
    networking ensures that all parts can talk to one another in a safe, frictionless,
    effective, and adaptable way. Many developments and studies in networking during
    the past ten years have focused on the cloud [60]. For instance, Software-Defined
    Networking (SDN) and Network Function Virtualization (NFV) aim to construct adaptable,
    versatile, and programmable computer networks to lessen the financial and time
    commitments of cloud service providers [61]. Scalability challenges have spurred
    several current developments in network design for the Cloud Data Centers (CDCs),
    as well as the necessity for a flat addressing space, and the excess demand for
    machines. Notwithstanding these developments, numerous networking issues require
    a resolution. The excessive energy consumption of modern CDCs is a major issue
    [60]. Especially because it is a common practice in data centers to have all networking
    equipment active at all times. Furthermore, unlike computing servers, most network
    parts (including switches, hubs, and routers), cannot be energy-proportionate;
    features like hibernation during periods of low traffic and connection-rate adaptability
    are not built in by default [62]. Consequently, the design and execution of approaches
    and technologies that seek to minimize network energy usage and make it proportionate
    to the incoming load continue to be outstanding issues. QoS assurance presents
    another complex challenge within CDC networks [63]. Service Level Agreements (SLAs)
    in modern clouds focus mostly on computing and storage. There is currently no
    way to encapsulate network performance constraints like latency and bandwidth
    assurances without resorting to “best effort” because no abstraction or method
    guarantees performance isolation. Providing network connections across widely
    dispersed resources (in other words, installing a “virtual cluster” encompassing
    resources in an amalgamated cloud setting), exacerbates this difficulty [64].
    However, there are numerous open challenges to deliver reliability assurances
    for these networks—due to packages needing to navigate the (public) Internet,
    including resources in various locations [65]. 4.1.2.2. Technologies/impact areas
    The key technologies and affected domains for networked computing include: 4.1.2.2.1.
    Internet of things (IoT) Devices (a.k.a., things) that can detect, control, and
    communicate are now routinely integrated with continuous control and monitoring
    functions via the Internet [1]. These devices have become ubiquitous in modern
    society, found in homes, on public transport, along highways, and in vehicles.
    Because of this, IoT applications may function in many contexts and provide a
    sophisticated evaluation and administration of complicated relationships [66].
    As a result, IoT devices and services may solve problems in many application domains,
    such as digital health, facility administration systems, production, and transportation.
    IoT-based systems have to deal with limited processing power, memory, and storage
    space because (i) platforms are constantly changing, so devices that join a network
    have to be able to adapt to these changes; (ii) devices differ in how well they
    work with computers and what features they offer; and (iii) to ensure the safety
    of the IoT data that has been acquired, a federated system is needed [5]. These
    days, popular IoT use cases include medical care, smart cities, climate prediction,
    water supply management, and highway surveillance, all of which leverage the capabilities
    of cloud, serverless, fog, and edge computing for processing user data to meet
    QoS requirements [67]. • Healthcare: Among the many significant IoT applications
    is medical care, which is designed to treat conditions including heart attacks,
    diabetes, cancer, COVID-19, and influenza [68]. For instance, a patient’s heart
    condition may be instantly diagnosed using a variety of medical devices in an
    interconnected IoT and computing environment [69]. Additionally, modern technology
    like Virtual Reality (VR) or AI can enhance the present healthcare system in the
    fight against inevitable pandemics [70]. • Agriculture: In order to forecast variables
    like yield, rainfall, and crop quality, the agricultural industry is making use
    of modern technology to analyze a wide range of data pertaining to agriculture
    [71]. One use case is the development of cloud-based agricultural systems that
    can autonomously forecast the state of agriculture using data collected from a
    variety of IoT or edge sensors. Additionally, to facilitate automated farming,
    an iOS or Android application is created to handle the massive amounts of data
    and supply the information they need to the agriculturalists through their edge
    devices [72]. • Smart Home: Owners may optimize energy consumption and offer the
    necessary protection with the deployment of cameras through the implementation
    of smart homes, which allow them to operate their home devices from their cell
    phones [36]. For instance, a resource management approach that incorporates cloud
    and fog computing may be used for controlling edge devices utilizing a smartphone
    application, which in turn regulates the room’s humidity, lighting, surveillance
    systems, fans, and voltage, such as via sensors connected to different household
    devices [73]. • Traffic Management: IoT is crucial in the efficient management
    of traffic through the use of a number of sensors and controllers [74]. To identify
    potholes, for instance, an IoT-based intelligent transportation system is created.
    In addition, its efficiency was assessed using a range of machine learning approaches
    and performance metrics [75]. Additionally, data may be processed swiftly using
    fog and edge computing methodologies to notify about potholes early, thereby reducing
    the likelihood of mishaps. • Weather Forecasting: Through the use of cloud computing
    and the IoT, scientists and weather forecasters may better gather data to inform
    their work [76]. Scientists have long relied on visual observations, data storage,
    and the public presentation of meteorological factors like air quality and moisture
    to better understand and explain these phenomena [77]. The findings may be made
    using an IoT system that relies on sensors and can transmit the results to the
    cloud. Cloud services have long been relied upon by IoT applications to handle
    processing and permanent storage. Still, as the number of ‘things’ proliferates,
    such services are increasingly unable to keep up with the real-time demands of
    IoT gadgets [78]. This is due to the high quantity of data and the short reaction
    times required by systems that operate in the real world over wide geographical
    areas. By moving resource orchestration from servers to edge networks, fog/edge
    computing expands the capabilities of cloud systems: Set up as a series of nested
    “cloudlets” that may perform data intake, processing, and administration [79].
    Compared to cloud services, geographically localized solutions use less power
    and allow for more mobile resources by decreasing reaction times and increasing
    intake bandwidth through horizontal scalability. These features make fog/edge
    computing a potential future architecture for IoT applications since this architectural
    model allows for scalability on a logical and geographical scale with near-instantaneous
    response latency [32]. By aggregating information from implanted and mobile gadgets
    and establishing mobile area networks, smart e-health apps can track information
    about patients in a continuous fashion [80]. By performing tasks like healthcare
    equipment noise filtering, data reduction and fusion, and analytics that identify
    harmful patterns in patients’ well-being, smart gateways gather and interpret
    data from devices locally [81]. At the same time, longer-term patterns may be
    evaluated at cloud levels. In addition, IoT systems supported by fog computing
    may adjust their actions based on the information they receive from sensors. For
    example, if a heart attack is recognized by initial processing at the fog layer,
    the intelligent gateway gathering signals from the defibrillator may adaptively
    boost the sample size before the attack. Similarly, the Industrial Internet of
    Things (IIoT) benefits from integrating edge, fog and cloud layers to provide
    specific and nearly real-time actions [82]. Smart grids and energy management
    are central to the Internet of Energy (IoE) paradigm. Coarse-grained information
    on network health may be gathered from dispersed networks of energy producers
    that track power usage, generation and/or battery life. While ‘Smart-Meters’ may
    communicate energy needs to service providers on a more detailed scale, monitoring
    capacity, generation, and use [80]. Therefore, IoT is a foundational technology
    for future systems, like electric automobiles and decentralized power grids [33].
    In addition, the increased safety, reliability, and durability of electricity
    distribution that this type of grid may provide can better satisfy the evolving
    needs of consumers. In-depth surveys are a good resource for IoT researchers who
    want to explore more. 4.1.2.2.2. Software-defined network (SDN) SDN transcends
    traditional network paradigms by separating control logic from the underlying
    hardware and centralizing network management [83]. This innovative approach facilitates
    programmable network architectures and streamlines management by distinctly segregating
    network policies, hardware implementation, and traffic forwarding [84]. Integral
    to cloud computing, SDN enhances communication and automates configurations, revolutionizing
    network adaptability and resource utilization in diverse environments [85]. NFV
    is another approach that utilizes software programs to perform traditionally hardware-based
    networking tasks, such as DNS, load balancing, and intrusion detection. NFV not
    only lowers costs but also enhances the flexibility of network functions and service
    responsiveness. Furthermore, VM consolidation in a virtualized network can help
    reduce energy costs by minimizing the number of VMs in operation [86]. SDN-based
    cloud computing optimizes network virtualization while decreasing electricity
    consumption. Crucially, SDN increases the abstraction of physical assets and automates
    and optimizes the setup process [85]. Many questions still need to be answered
    by scholars and investigators. First, ensuring data safety during transit across
    multiple cloud data centers is absolutely necessary for SDN-based cloud computing
    [87]. Second, even if SDN-enabled cloud infrastructures may be replicated, the
    balance between cost and energy use remains. Deploying SDN-based cloud computing
    systems is necessary to offer an economical network virtualization service with
    lower energy costs and greater dependability [83]. Furthermore, this may also
    boost data distribution and outcome collection by utilizing methods inspired by
    AI-based models, allowing us to expand existing information connectivity in such
    SDN contexts to accommodate blockchain-based systems. 4.1.2.3. Trends/observations
    The main trends and observations regarding networked computing are as follows:
    4.1.2.3.1. Intelligent edge The IoT connects billions of new devices, generating
    massive amounts of information that, inevitably, proves challenging to process.
    Over 41.6 billion IoT gadgets are estimated to be in operation by the end of 2025
    [88]. Increasing numbers of products, including connected autos, smart meters,
    and in-store sensors, are being created and installed by companies to improve
    customer experience while generating enormous quantities of data [4]. Meanwhile,
    this emerging data must be gathered, managed, and processed immediately. What
    exactly will this mean? Edge and fog computing might be a method for moving ahead.
    In the coming years, edge computing is forecast to receive far greater focus than
    fog computing. In contrast to traditional cloud computing, which analyzes data
    at a remote data center, edge computing performs so locally. In fog computing,
    it is possible to execute a portion of the work in the cloud, while edge devices
    perform other aspects [89]. Since computing at the edge uses far less network
    bandwidth than conventional computing, the data exchanged among connected devices
    could take a long time. Computing it nearby, either on the gadget itself or within
    a local network, will be more cost- and energy-effective. On the contrary, edge
    computing may provide cloud computing with much-needed support in coping with
    the vast volumes of data created by the IoT and other connected devices [90].
    Emerging IoT devices create and transport data across the fog and edge, and their
    processing power is leveraged to carry out processes that could otherwise be performed
    in the cloud. Hence, managing these systems with fog and edge, IoT devices and
    support from the cloud requires distributing the intelligence along the computing
    tiers, which leads to edge intelligent [91]. The terms “fog” and “edge” allude
    to these novel network nodes for IoT devices. Thus, they aid businesses in reducing
    their reliance on the cloud by transmitting information to analytics platforms.
    Businesses can lessen their dependency on cloud platforms for data processing
    and thereby reduce latency across networks by implementing edge and fog solutions
    [92]. This will allow rapid evidence-based recommendations to assist them in their
    decision-making process. Nevertheless, once real-time processing is complete,
    edge devices must transfer data to the cloud for statistics to be performed on
    it [93]. A company’s communication network is largely concerned with enabling
    various remote apps and providing endless storage space, thanks to cloud computing,
    connectivity, and computing capacity. That will ultimately alter data processing
    at the edge in real-time, which is essential for optimal data utilization [90].
    Future-proof network infrastructures will need to accommodate an unprecedented
    influx of smart devices. For real-time intelligence, it is crucial to have the
    decision-making process located close to where the data is produced. Self-driving
    automobiles and self-sustainable, smart factory equipment, for instance, require
    to be making split-second decisions [94]. Further, airline sensors collect data
    on engine efficiency in real time, allowing for predictive maintenance before
    a plane ever takes off. Potential cost reductions might be considerable. The more
    business connections an organization has, the more processing power and intelligence
    it can provide. 4.1.2.3.2. 6G and beyond The advancement to Industry 5.0 and the
    foundation of a technology-driven economy hinge on the development of Beyond 5G
    (B5G) and 6G networks. As communication and technological advancements increase,
    international industrial sectors will increasingly depend on 5G and B5G networks
    to provide revolutionary services and applications that will inevitably require
    ultra-low latency, unprecedented reliability, and continuous mobility [95]. Meanwhile,
    underpinned by Moore’s law, mobile devices have been rapidly adopting systems-technology
    co-optimization (STCO) and related system-building approaches, which departs from
    the conventional system-on-a-chip (SoC) approach [96]. Through cloud-based principles,
    including utilizing functioning between and among data centers, connecting in
    a micro-service setting, and concurrently offering reliable services and applications,
    it is expected that B5G/6G networks will be able to serve a wide variety of applications
    [97]. Both B5G and 6G networks aim to enable the smooth and complete integration
    of many industries, including the IoT, aerial networks (also known as drones),
    satellite accessibility, and submerged connectivity [98]. To keep up with this
    astonishing expectation, the next generation of networks (B5G/6G) will largely
    rely on cutting-edge AI/ML technology for intelligent network operations and administration.
    B5G and 6G infrastructures are anticipated to provide computationally intensive
    applications and services paired with infrastructure shifts [99]. Edge computing
    has received a lot of interest and is being evaluated as an integrated service
    in 6G networks to enable the two fundamental changes in network infrastructure
    and network services. While many studies have focused on features like cache services
    and compute offloading methods, little is known about mobile edge computing implementation.
    The necessity of moving forward with a software-centric strategy from the network
    core to the wireless layer was emphasized in the first efforts that contributed
    significantly to the conception of 5G [100]. As with 5G networks, 6G networks
    will depend heavily on SDN, which, together with NFV, represents a departure from
    the conventional hardware-centric strategy [9]. The mobile edge computing paradigm
    also encourages moving the base station (BS) and the core network functions to
    different places. BS functions are moved upstream to the cloud, and core network
    functions are moved downstream to the devices. The resulting boundary between
    the BS and the end device might be seen as an “edge” or “fog” domain [73]. While
    cloud computing has made it possible for users to access richer and more complicated
    apps by tapping into the resources of a remote cloud server, an alternative technique
    is needed to meet the extremely delicate latency criteria stated for use cases
    in 5G and maybe 6G [101]. This heterogeneous network design directly results from
    the complicated traffic distributions in today’s wireless networks. A wireless
    access point (AP), a macro BS, and a small cell BS are just a few examples of
    network access nodes that may provide stable and smooth connections for mobile
    users [102]. These network access nodes provide edge computing at network edges
    with less delay. The design of diverse mobile edge computing networks has gained
    more and more interest due to the varied properties of network access nodes, such
    as coverage capability and power transmission [101]. Nevertheless, it is imperative
    to carefully plan for the offloading of computing tasks to many access nodes in
    a network [103], [104]. In a heterogeneous network design, intelligently distributing
    tasks and resources among different nodes can substantially boost system performance
    [55]. For instance, by collaborating, the edge and cloud can elevate IoT tasks’
    QoS. While cloud servers manage compute-intensive tasks well, edge servers excel
    at processing tasks demanding minimal data or low latency [41]. Strategically
    assigning tasks among edge servers can redistribute work from overburdened nodes
    to less active ones, thus accelerating execution times. 4.2. General purpose vs.
    Specialized computing Leveraging fit-for-purpose software and given enough time,
    general-purpose computing (which includes desktop PCs, laptops, mobile devices
    like tablets and smartphones, and even certain televisions) can handle just about
    any computation [105]. A CPU, memory, input/ output systems, and a bus are the
    main parts of any general-purpose computing system. In contrast, integrated computers,
    are used in intelligent systems, and are often referred to as “special-purpose”
    computing systems. 4.2.1. General-purpose computing In this section, we discuss
    the main focus, paradigms, technologies, and impact areas, as well as various
    trends and observations about general-purpose computing. 4.2.1.1. Focus/paradigms
    The following are the main focus areas and paradigms associated with general-purpose
    computing: (1) Von Neumann Architecture: A computing device with a Von Neumann
    architecture has its main components – the CPU, memory, and I/O – connected via
    a single bus [106]. The efficiency of computers was greatly enhanced by the advent
    of this architecture, which provided effective means of storing and executing
    instructions. The fundamental idea behind this design is that data and instructions
    are handled in the same way. In other words, the data being handled and the program
    instructions themselves share the same storage and processing resources: a memory
    address can contain either an instruction to be executed or data; the software
    execution pathways decide how to interpret it [107]. This design substantially
    simplifies the framework and features of a computer, making it more accessible
    to both software engineers and non-technical users. (2) Integrated Computing:
    Compatibility throughout cloud applications and services is commonly achieved
    by implementing software adaptors and libraries and deploying application containers
    for computing to facilitate mobility [108]. Nevertheless, there is still a variety
    of challenges that have existed since the beginning of cloud computing but, due
    to their complexities, have not been adequately resolved yet [60]. One of these
    challenges is encouraging cloud connectivity without mandating a baseline set
    of capabilities for all services; ideally, customers can combine complicated features
    from several providers. Another area of investigation is how to develop cloud
    interoperability middleware that can facilitate the offering of complex services
    by composing more straightforward services from multiple 3rd-party providers [109].
    Such a high degree of abstraction would empower users to make service decisions
    based on their requirements, such as price, turnaround time, privacy, etc. This
    brings up an additional key area that needs further study: the manner in which
    to allow user-level middleware (intercloud and hybrid clouds) to discover potential
    services for an ensemble without assistance from cloud service providers [110].
    A strategy that relies on cloud providers working together is unlikely to be successful
    because their financial goals lie in keeping all the features they offer to their
    consumers (i.e., they have no incentive to help due to the fact that just a portion
    of the offerings in an ensemble are themselves). Consequently, the middleware
    that allows the melody of services must address challenges at both of its connections:
    Firstly, the middleware should seamlessly and abstractly provide the service to
    cloud users. Secondly, for the consumers, a service might be implemented in its
    entirety by sub-services offered by one vendor (maybe leveraging a 3rd-part SaaS
    organization able to offer the functionality), or it might be acquired through
    composing multiple services from various providers [111]. The provider user interface
    makes it possible to access complex functions without requiring special cooperation
    from providers [109]. The widespread use of cloud compatibility can offer commercial
    and financial advantages to cloud manufacturers, but frequently integrated clouds
    (which were achieved via Cloud Federation) cannot be realized until such time
    [112]. This calls for the development of intercloud markets, distinctive approaches
    to invoicing and accounting, as well as novel cloud-suitable pricing systems.
    4.2.1.2. Technologies/impact areas The key technologies and affected domains for
    general-purpose computing include: 4.2.1.2.1. Programming models Clusters are
    a type of parallel or distributed computational system that consists of a group
    of interconnected standalone computers that work collectively as a single integrated
    computing resource. Clusters and grids are platforms that communicate with each
    other to serve as a single resource [113]. A multi-core parallel architecture
    describes this form of capability, which is based on specific functions. Conversely,
    cloud computing emerged on top of clusters to abstract leveraging their computing
    resources and coordinate enormous data sets. A programming model is tightly coupled
    to where data is transmitted to manage an application’s functions. Important metrics
    to remember while building a programming model are efficiency, adaptability, goal
    architecture, and code maintainability [114]. Data analytics software often handles
    massive data sets that require many phases of processing. Certain steps have to
    be carried out in order, while others are executed simultaneously across several
    nodes in a cluster, grid, or cloud. The capacity of algorithms to perform statistical
    analysis on huge amounts of data will be crucial to unlocking achievements in
    industrial advances and next-generation scientific discoveries [115]. With the
    exponential growth of data comes the difficulty of organizing massive data sets,
    which in turn increases their complexity due to the ways they connect with each
    other. Its many processes include moving, archiving, replicating, processing,
    and erasing data. Data life-cycle complexities can be reduced via solutions that
    automate and improve data management activities. It has been shown that two limitations
    affect the data life cycle [116]. The framework used is the first limitation,
    initially regarding how it operates on data derived from consumers and apps. The
    second limitation derives from the observation that data is spread over several
    systems and infrastructures. That is why big data applications need to be capable
    of communicating amongst various systems that deal with the data and the effects
    that information and occurrences might have. The focus of this work is the second
    limitation, the big data infrastructure itself, and it includes a comprehensive
    analysis of the programming models and settings necessary to overcome this limitation.
    A programming model is underpinned by how quickly and smoothly its data is manipulated.
    A few elements to consider when creating a programming model include operation,
    adaptability, target designs, and the simplicity of maintenance code modification
    procedures [117]. For the sake of service, it is sometimes necessary to sacrifice
    at least one of these aspects. The exchange of computation for data storage or
    transmission is a usual instance of algorithmic manipulation. These difficulties
    can be mitigated by employing parallel methods and technology. A software engineer
    might undoubtedly leverage many variants of the same technique to enable distinct
    performance adjustments on different hardware architectures. Modern computing
    clusters comprise nodes with more than one CPU, and their hardware designs range
    from tiny to super powerful. 4.2.1.2.2. Virtualization With virtualization, the
    original physical object is replaced with a virtual one. The OSs of server infrastructure,
    hard drives, and PCs are some of the most typical targets for virtualization in
    a data center. Thus, virtualization decouples higher-level software and OSs from
    the underlying computing system [118]. VMs are a key component of hardware virtualization,
    standing in for a “real” computer running an OS. Emulating a computer system is
    what VMs do. A hypervisor makes a copy of the underlying hardware so that several
    OSs can share the same resources [119]. Despite being around for half a century,
    VMs are experiencing a surge in popularity because of the rise of the mobile workforce
    and desktop PCs. Server virtualization, which employs a hypervisor to effectively
    “duplicate” the underlying hardware, is a primary use case for virtualization
    technology in the corporate world [120]. In a non-virtualized setting, the guest
    OS generally works in tandem with the hardware [121]. OSs may be virtualized and
    continue functioning as if running on hardware, giving businesses access to similar
    performance levels while reducing hardware costs [122]. The majority of guest
    OSs do not need full access to hardware; therefore, even if virtualization efficiency
    is lower than hardware efficacy, it remains preferred. This means firms are less
    reliant on a single piece of hardware and have more leeway to make necessary changes.
    Following the success of server virtualization, other sections of the data center
    have also begun to implement the same approach. Virtualization technology for
    OSs has been around for generations [123]. In this implementation, the software
    enables the hardware to run several OSs in parallel. Companies that want to adopt
    a cloud-like IT infrastructure should prioritize virtualization. Using server
    resources more effectively is one of the primary benefits of virtualizing a data
    center [124]. Thanks to virtualization, IT departments may use a single VM to
    host a wide variety of applications, workloads, and OSs, with the flexibility
    to add or subtract resources as required easily. The use of virtualization allows
    firms to expand readily. Organizations may better monitor resource utilization
    and react to shifting needs using such systems. 4.2.1.2.3. Multicore processors
    For improved performance and more efficient use of energy, integrated circuits
    with several processing cores, or “cores”, are becoming increasingly common. Furthermore,
    these processors enable more effective parallel processing and multithreading,
    allowing for simultaneous processing of numerous jobs [125]. A computer with a
    dual-core arrangement is functionally equivalent to one with two or more individual
    CPUs. Sharing a socket between two CPUs accelerates communication between them.
    The use of processors with multiple cores is one technique to enhance processor
    performance while surpassing the practical restrictions of semiconductor manufacturing
    and design. Using several processors helps prevent any potentially dangerous overheating
    [126]. Multicore processors are compatible with any up-to-date computer hardware
    architectures. These days, multicore processors are standard in desktop and portable
    computers. Nevertheless, the actual power and utility of these CPUs depend on
    software built to leverage parallelism [127]. Application tasks are broken up
    into many processing threads in a parallel strategy, distributing and managing
    them over multiple CPUs. 4.2.1.3. Trends/observations The main trends and observations
    regarding general-purpose computing are as follows: 4.2.1.3.1. Software systems
    Web-based computing and Software Engineering (SE) are closely related disciplines.
    For instance, service-oriented SE provides various advantages to the software
    creation procedure and app development by merging the greatest elements of services
    and the cloud. In contrast to cloud computing, which is concerned with effectively
    transmitting services to consumers using adaptable virtualization of resources
    and load balancing, service-oriented SE is concerned with architectural design
    (service searching and composition) [128]. Customers and developers are both essential
    to the evolution of hardware innovations, which is why software engineering is
    a crucial discipline [129]. With the help of distributed computing and virtualization,
    customers may set up automatically managed VMs and cloud services for their initiatives
    and applications. Thanks to cloud services, teams working on software may now
    more easily collaborate on the development, testing, and distribution of their
    products. Here are some scenarios in which cloud computing might improve software
    engineering: The production timeline can be compressed [130]. As a result of the
    availability of ample computing resources made possible by cloud computing and
    virtualization, software engineers no longer need to rely just on a single physical
    computer. The time it takes to install the required applications may be decreased
    by retrieving cloud services, indicating that development activities can be performed
    with increased parallelism thanks to cloud computing. Third, VMs and cloud instances
    may substantially improve the setup and delivery procedures. Using sufficient
    virtualization resources from a private or public cloud, developers can speed
    up the building and testing process, which is otherwise, extremely time-consuming
    [123]. To circumvent this, a simplified system for managing code versions is required.
    In software development, code branches are used for refining and adding features.
    With cloud computing, there is no need to invest in or lease expensive hardware
    only to store some code. A distributed software engineering team may access apps
    more easily in a cloud setting, and service quality can be enhanced through dynamic
    resource allocation. As a result, the software construction process is streamlined
    thanks to cloud computing, which eliminates the need for development servers to
    rely on specific physical computers [129]. Nevertheless, there are obstacles when
    merging software engineering and cloud computing. The majority of the difficulties
    are with moving the data. Because different cloud providers use various APIs to
    offer cloud services, migrating software and data from one cloud to another while
    avoiding vendor lock-in is challenging. Avoiding over-reliance on any one set
    of APIs is one way to fix this problem while building and releasing applications
    in the cloud. The problem of dependability and accessibility is another obstacle.
    If everything is moved to the cloud, it will be difficult to retrieve the data
    if the cloud is compromised by hackers or affected by an unexpected calamity.
    The engineering teams are responsible for creating a local backup of their work
    [131]. Cloud computing allows software engineering academics to study multinational
    software development. Several investigations have examined the feasibility of
    using cloud computing to lower operational, delivery, and software development
    expenses. Researchers have investigated the feasibility of replacing services
    with a cloud-based platform for student-to-student knowledge exchange and collaboration
    [132]. Software systems have been supplanted by systems running on the cloud to
    reduce expenditures and maximize the utilization of resources. The conventional
    data management techniques have become increasingly cumbersome in the past few
    years due to the rapid increase in available data. A new frontier for study in
    software engineering has opened up thanks to the IoT, Blockchain (the distributed
    ledger), and ML/AI, with data management being the primary challenge [133]. These
    studies also provide a springboard for further study and innovative approaches
    to cloud data management, leading to the development of advanced technologies
    like Cisco’s pioneering fog computing [134]. Enterprise software developers are
    creating an abstraction layer, or “Blockchain-as-a-Service”, and selling it to
    other businesses as a subscription service [4]. These numerous new fields rely
    significantly on software engineering, yet they could not exist without it. 4.2.1.3.2.
    Simulations The capacity to carry out research, analyze strengths and shortcomings,
    and demonstrate viability is hampered in new or emerging computing domains due
    to the lack of mature technology and sufficient infrastructure. In many cases,
    the time and resources needed to acquire the necessary physical resources make
    it impractical to conduct the necessary research [79]. An alternative approach
    that can approximate a physical environment is a simulator. Additionally, simulation
    offers the ability to test suggested hypotheses in lightweight and low-cost settings.
    Real-world testing of novel methods is difficult and expensive because of the
    time and effort required to gather the necessary hardware resources (particularly
    for large-scale tests) and create the necessary software applications and systems
    [135]. Investigators demonstrate the viability of their ideas by modeling and
    simulation, and then conduct tests to confirm their concepts in a monitored environment
    utilizing simulation tools. Simulation software provides a convenient setting
    for testing solutions to real-world issues by allowing users to experiment and
    see what happens [136]. If a commercially available simulator is inadequate for
    user needs, then researchers should consider building their own, complete with
    graphical user interfaces. This is especially true if users need to simulate components
    of emerging computer architectures [137]. Researchers could benefit greatly from
    using a simulator to formulate questions and analyze different theoretical frameworks
    in simulated setups, therefore stimulating more research and fostering the development
    of communities within the relevant field. 4.2.2. Specialized computing In this
    section, we discuss the main focus or paradigms, technologies or impact areas,
    and various trends or observations within specialized computing. 4.2.2.1. Focus/paradigms
    The following are the main focus or paradigms for Specialized Computing: 4.2.2.1.1.
    Reconfigurable computing The modern paradigm of reconfigurable computing enables
    hardware components to swiftly alter their configuration and functioning in response
    to changing processing needs. Reconfigurable computer devices, such as Field-Programmable
    Gate Arrays (FPGAs), can be reprogrammed to perform a variety of different functions
    [138]. The main function of reconfigurable computing is to fill the void among
    general-purpose CPUs and Application-Specific Integrated Circuits (ASICs) [19].
    It allows hardware to be optimized for efficiency, power efficiency, and flexibility
    by matching application requirements. Static and dynamic switching are the two
    primary modes of operation for reconfigurable technology. In static reconfiguration,
    the component settings are adjusted prior to the computer starting to compute.
    However, dynamic reconfiguration permits hardware changes to be made while the
    system is running, allowing for dynamic modifications to hardware behavior. 4.2.2.1.2.
    Domain-specific architectures As computing and the digital transformation spread
    to various use cases, such as cloud (AI/HPC), networking, edge, the IoT, and self-driving
    cars, highly domain-specific computational tasks are making it more likely that
    Domain-Specific Architectures (DSAs) can enable big performance gains [139]. Using
    ChatGPT and other comparable software that are powered by large language models
    – which are fundamental to achieving generative AI – provides greater specialization
    inside AI workloads at extremely high volume, which motivates further hardware
    specialization [81]. DSAs, or application-domain-specific hardware and software,
    have substantial market potential. As a result of their superior performance on
    tasks that profit from a significant amount of parallel computing, such as AI
    workloads (learning and predicting), GPUs and Tensor Processing Units (TPUs) are
    currently controlling a sizable portion of the data center market [140]. Meanwhile,
    accelerations of 15–50 times the original speed, depending on the workload, are
    not uncommon. In the automobile industry, minimal latency and high-performance
    inference are provided via tailor-made solutions from industry leaders. 4.2.2.1.3.
    Exascale computing To handle the massive computations required by convergent modeling,
    simulation, AI, and data analysis, an entirely novel type of supercomputer called
    exascale computing has emerged [2]. This is motivated by advanced computational
    needs in science and engineering. Exascale computing (also supercomputing) becomes
    essential to expedite the generation of knowledge. Researchers and technologists
    may employ data analysis driven by exascale supercomputing to expand the frontiers
    of our existing understanding and promote breakthrough ideas. Supercomputing capabilities
    are in high demand as the world moves towards exascale computing to ensure continued
    scientific and technological advancements, while our civilization’s technological
    and scientific frameworks are progressing quickly thanks to exascale computing
    [141]. The immense potential of these tools necessitates their careful operation,
    especially as cultures worldwide undergo rapid changes in their moral frameworks
    and their perceptions of what it means to live sustainably. As such, novel responses
    to formerly intractable issues are being uncovered thanks to exascale computing.
    Exascale supercomputers are prohibitively expensive to construct; thus academics
    and scientists rely on funding to lease them instead of buying their own [142].
    Exascale computing systems produce enormous quantities of heat because of the
    volume of data they process. They require extremely cold environments to be stored
    in or unique cooling mechanisms built into the systems and racks themselves for
    optimal performance. Differentiating them from other types of supercomputers and
    quantum computers, they are computer systems with the largest capacity and most
    powerful hardware [143]. To further our understanding of the universe, exascale
    computers can model elementary physical processes like the granular interactions
    of atoms. Quite a few sectors rely on this capacity to analyze, forecast, and
    construct the world of tomorrow: for instance, better predict the weather, investigate
    in detail the interaction between rain, wind, clouds, and various other atmospheric
    occurrences, analyze their effects on one another at a molecular level and so
    on. Mathematical formulas can be used to determine the millisecond-by-millisecond
    effects of all forces acting in a certain environment at a specific time [144].
    These seemingly trivial interactions rapidly generate billions of possible permutations,
    which need trillions of mathematical equations to calculate and analyze. This
    kind of speed is only achievable on an exascale machine. By studying the results
    of these computations, researchers can gain a deeper insight into the nature of
    our universe [143]. Exascale supercomputers, despite their challenges, can literally
    increase our understanding, enabling us to address the problems of the future.
    4.2.2.1.4. Analog computing A novel approach may minimize errors in ultra-fast
    analog optical neural networks. Larger and more complicated machine-learning models
    need stronger and more effective computing gear. However, standard digital computers
    are lagging. Compared to a digital neural network, an analog optical network’s
    performance in areas like image classification and voice recognition is comparable.
    However, its speed and energy efficiency far exceed those of its digital counterparts
    [145]. Nevertheless, hardware faults in these analog devices might impact the
    accuracy of calculations. One possible source of this inaccuracy is microscopic
    flaws in the hardware itself [146]. Errors tend to multiply rapidly in a complex
    optical neural network. Even when using error-correction approaches, due to the
    basic features of the components that make up an optical neural network, a certain
    degree of error is inescapable [147]. Conversely, the optical switches that make
    up the network’s architecture can reduce mistakes they typically accrue by adding
    a modest hardware component. 4.2.2.1.5. Neuromorphic computing When applied to
    AI, neuromorphic computing makes it possible for AI to learn and make decisions
    independently, significantly improving over the first generation of developing
    AI. To acquire abilities in areas like recognizing voice and sophisticated tactical
    games, including chess and Go, neuromorphic algorithms are now involved in deep
    learning [145]. Next-generation AI will imitate the human brain’s capacity to
    comprehend and react to circumstances instead of merely operating from formulaic
    algorithms [148]. When it comes to understanding what they have read, neuromorphic
    computing systems will seek out patterns and use their ‘common sense’ and the
    surrounding context. When Google’s Deep Dream AI was programmed to hunt for dog
    faces, it notably showed the limitations of algorithm-only computer systems [147]:
    Any images that it interpreted as having dog faces were transformed into dog faces.
    Third-generation AI computing attempts to simulate the elaborate structure of
    a living brain’s neural network [149]. This calls for AI with computing and analytic
    capabilities on par with the extremely efficient biological brain. To demonstrate
    their exceptional energy economy, human brains can surpass supercomputers using
    less than 20 watts of electricity. Spiking Neural Networks (SNN) are the AI equivalent
    of our synaptic neural network [150]. They leverage many layers of artificial
    neurons, and each spiking neuron may fire and interact with its neighbors in response
    to external inputs. Most AI neural network architectures follow the Von Neumann
    design [106], which divides the memory and computation into discrete nodes. Computers
    exchange information by reading it from memory, sending it to the CPU for processing,
    and then returning it to storage. This constant back-and-forth wastes a lot of
    time and effort. It causes a slowdown that becomes more noticeable while processing
    huge data sets. As a response, multiple neuromorphic devices can be utilized to
    supplement and improve the performance of traditional technologies, such as CPUs,
    GPUs, and FPGAs [146]. Low-power neurological systems may perform powerful activities,
    including learning, browsing, and monitoring. A practical instance would involve
    immediate voice recognition on mobile phones without the CPU needing to interact
    with the cloud. 4.2.2.2. Technologies/impact areas The key technologies and affected
    domains for Specialized Computing include: (1) Graphics Processing Unit (GPU):
    GPUs have rapidly risen in prominence as a crucial component of both home and
    enterprise computers [18]. A GPU is a special type of computer chip deployed in
    a variety of application domains, most notably the rendering of moving images.
    While GPUs are best recognized for their usage in gaming, they are also finding
    increasing application in the fields of creative creation and AI [151]. The initial
    purpose of GPUs was to speed up the display of 3D visuals. They improved their
    functionality as they got more adaptable and programmable over time. This paved
    the way for more complex lighting and shadow characteristics and photorealistic
    environments to be implemented by graphics developers. Additional engineers started
    using GPUs to drastically speed up various tasks in deep learning, HPC, and other
    fields [138]. (2) Compute Unified Device Architecture (CUDA): The demand for more
    powerful computers grows daily. As a result of constraints like size, climate,
    etc., vendors throughout the world are finding it difficult to make future improvements
    to CPUs [18]. Service providers that provide solutions in this kind of environment
    have begun to seek out performance improvements elsewhere. The use of GPUs for
    parallel processing is one option that enables significant speed gains [152].
    The total number of cores in a GPU is significantly greater than that of a CPU.
    Although CPUs are designed for sequential processing, offloading them to GPUs
    enables parallel processing. For general-purpose computing on NVIDIA’s GPUs, users
    can rely on CUDA, which allows for the execution of processes in parallel on the
    GPU without any specific order requirement [138]. Offloading compute-intensive
    activities to Nvidia’s GPU using CUDA is straightforward thanks to the library’s
    support for the popular C, C++, and Fortran programming languages [152]. CUDA
    is employed in scenarios needing extensive computational power or suitable for
    parallel processing to achieve high performance. Fields such as AI, healthcare
    analysis, science, digital transformation, cryptocurrency mining, and scientific
    modeling, among others, depend on CUDA technology. 4.2.2.3. Trends/observations
    The main trends and observations regarding Specialized Computing are as follows:
    Large-Scale ML: As big data grows, ML algorithms with many variables are needed
    to ensure that these models can handle very large data sets and make accurate
    predictions, including hidden features with many dimensions, middle representations,
    and selection functions [153]. The need for ML systems to train complicated models
    with millions to trillions of variables has increased as a result [154]. Distributed
    clusters of tens to hundreds of devices are often used for ML systems because
    they can handle the high computing needs of ML algorithms at these sizes. Yet,
    developing algorithms and software systems for these distributed clusters requires
    intensive analysis and design [155]. The latest advances in industrial-scale ML
    have focused on exploring new concepts and approaches for (a) highly specialized
    monolithic concepts for large-scale straight applications, such as different distributed
    topic models or regression models, and (b) for adaptable and readily programmable
    universally applicable distributed ML platforms such as GraphLab based on vertex
    programming and Petuum using a parameter-driven server [156]. It is widely acknowledged
    that knowledge of distributed system topologies and programming is essential;
    however, ML-rooted statistical and algorithmic discoveries can yield even more
    fruit for large-scale ML systems in the form of principles and techniques specific
    to distributed machine learning applications. These guidelines and techniques
    shed light on several crucial questions: • How to share an ML application among
    nodes? • How to connect machine-learning calculations with machine-to-machine
    dialog? • How should one proceed with having such a conversation? • What ought
    to be conveyed among machines? And, should they cover many big ML-related topics,
    from practical use cases to technical implementations to theoretical investigations
    [98]? Understanding how these concepts and tactics may be made effective, generally
    applicable, and easy to develop is the primary goal of large-scale ML systems
    studies, as is ensuring that scientifically validated accuracy and scalability
    assurances underpin them. 4.3. Centralized vs. Decentralized computing A central
    server controls and processes most of the data in a centralized network, whereas
    no single entity has influence over a decentralized network. 4.3.1. Centralized
    computing In this section, we discuss the main focus or paradigms, technologies
    or impact areas, and various trends or observations within centralized computing.
    4.3.1.1. Focus/paradigms The following are the main focus or paradigms for centralized
    computing: (1) Cloud Computing: The adoption of cloud computing, which revolutionized
    how end-users and software engineers interact with applications and computing
    systems, led to the rise of technology as the fifth utility [1]. Cloud computing
    was successfully accepted by giving consumers on-demand access to the computing
    power they want, the freedom to modify their resource consumption as needed, and
    the transparency of paying just whatever is being utilized. Business groups, regulatory
    bodies, and universities have all been quick to endorse it since it first appeared.
    Like contemporary society relying on essential utilities, the cloud has grown
    into the economy’s foundation by providing immediate utilization of subscription-driven
    computing resources [157]. As a result of using cloud technology, innovative companies
    can be launched quickly, existing ones can expand globally, advances in science
    can be sped up, and novel computing methods can be developed for ubiquitous and
    pervasive apps [34]. SaaS, PaaS, and IaaS have served as the three primary service
    models that have pushed uptake in the cloud thus far [35]. • Mobile Cloud Computing:
    To provide value to mobility consumers, network operators, and cloud service providers,
    mobile cloud computing integrates mobile devices, cloud computing, and communication
    networks. With the help of mobile cloud computing, a wide variety of handheld
    gadgets can run complex mobile apps. Under this paradigm, handling and storing
    data is done by servers rather than individual mobile devices [32]. Several advantages
    result from the use of mobile cloud computing apps based on this architecture:
    (i) battery life has significantly increased; (ii) there has been an increase
    in both the speed and size of data being stored and processed; (iii) the system’s
    emphasis on “store once, access anywhere” eliminates complex data synchronization;
    and (iv) stability and scalability have been dramatically enhanced. Nevertheless,
    inadequate network capacity is a significant challenge for mobile cloud computing
    [33]. Wireless mobile cloud services have capacity constraints in contrast to
    their cable counterparts. The spectrum of mobile devices offers a wide range of
    wavelengths. This has resulted in slower access speeds, as much as one-third in
    comparison to a wired network. Due to the increased likelihood of data loss on
    a wireless network, it is more challenging to recognize and deal with security
    risks on mobile devices than on desktop computers [158]. Customers frequently
    report issues with accessibility to services, including network outages, overcrowding
    on public transit, lack of coverage, etc. Customers may occasionally experience
    a low-energy signal, which slows down access and impacts data storage. Mobile
    cloud computing is employed on several OS-driven platforms, including Apple iOS,
    Android, and Windows Phone, resulting in network modifications that need cross-platform
    compatibility [159]. Mobile gadgets have a greater environmental impact due to
    their high energy consumption and low output [60]. As the use of mobile cloud
    computing grows, so does the problem of the increased drain on mobile devices’
    batteries. A device’s battery life is crucial for using its software and executing
    other tasks. Although the modified code is tiny in size, offloading uses more
    energy than running it locally [160]. • Green Cloud Computing: In the last few
    decades, Information and Communication Technology (ICT) has significantly evolved,
    drawing on technological advancements from the past two centuries. This evolution
    has elevated computing to the status of a fundamental service, akin to traditional
    utilities such as water, electricity, gas, and telephony, thereby establishing
    it as the fifth essential utility in modern society [161]. Modern cloud computing
    systems are becoming progressively large-scale and dispersed as more and more
    businesses and organizations have shifted their computing workload to the cloud—while
    others opt out of maintaining code altogether and instead leverage cloud-powered
    SaaS services. A cloud computing infrastructure of this magnitude not only offers
    more affordable and dependable services but also, increases energy effectiveness
    and reduces the global community’s carbon impact [162]. Every minor enhancement
    is much appreciated. In an effort to achieve zero carbon emissions, the community
    has recently been aggressively exploring a more sustainable version of cloud computing
    called green cloud computing to lessen reliance on fossil fuels and curb its carbon
    footprint [163]. Green cloud computing is a system that considers its constraints
    and goals to minimize energy consumption. Researchers are focusing on scheduling
    workloads and resources in light of carbon emissions, in order to increase the
    effectiveness of the resources used [164]. Additionally, forecasting problems
    with hardware and creating management systems to use hardware with varying degrees
    of dependability can maximize device lifetime and reuse. Further, utilizing micro-data
    centers – rather than standard server data centers – is a promising approach to
    boost efficiency and save costs. These facilities can accommodate future growth,
    serve huge populations, and dissipate heat effectively [165]. Furthermore, virtualization
    is another ecologically friendly technique that boosts the versatility of system
    resources. Through improved tracking and control, servers may pool their resources
    more effectively [166]. Innovations and practices that support sustainable development
    are constantly being developed as organizations rely more heavily on cloud services
    to enable “green cloud computing”. 4.3.1.2. Technologies/impact areas The key
    technologies and affected domains for centralized computing include: (1) Cloud
    Storage Technologies: Files and information stored in the cloud may be accessed
    from anywhere with a web connection or via secure network access. Transferring
    files to the cloud puts the responsibility for data security squarely on the shoulders
    of the cloud provider, rather than consumers. The service provider hosts, manages,
    and maintains the servers where user data is stored, and they also guarantee that
    users always have access to their files [167]. When compared to storing data on
    local discs or storage networks, cloud-based storage is a more affordable and
    scalable option. There is a limit to the quantity of information that can be stored
    on a hard disc. When users exhaust internal storage space, they must copy their
    data to removable media. The difference between on-premises storage networks and
    cloud storage is that the latter sends data to servers located in a remote data
    center. VMs, which are abstracted on top of an actual server, make up the vast
    majority of users’ servers [168]. Known as autoscaling, a cloud provider spins
    up more virtual servers as necessary to accommodate users’ ever-increasing storage
    demands. Files, blocks, and objects are the three primary categories of cloud
    storage, which are accessible in private, public, and hybrid cloud configurations.
    (2) Microservices: Microservices are a type of application architecture in which
    several autonomous services collaborate using simple APIs. A cloud-native software
    development method, microservice architecture separates an app’s main functionality
    into its own modules [169]. By compartmentalizing the app’s components, the development
    and operations teams may collaborate without interfering with each other. If several
    engineers can collaborate on the same project simultaneously, it takes less time
    to complete. This is in contrast with the monolith software architecture, which
    had been the standard for application development in the past [170]. All of an
    app’s features and services are tightly bound together and run as one seamless
    whole under a monolithic architecture [171]. The application’s architecture becomes
    more involved whenever new features are introduced, or existing ones enhanced.
    Because of this, optimizing a single feature inside the application requires disassembling
    the whole thing, which is a time-consuming and tedious process. This additionally
    necessitates that scaling the application as a whole is required if scaling any
    one process inside it—rather than just scaling out just that overloaded element
    [172]. Microservice architectures separate an app’s essential features into individual
    processes. To adapt to shifting business requirements, software engineering teams
    may develop and maintain new elements independently of the rest of the application.
    The monolith has been the standard for application development in the past. An
    application’s features and services are tightly bound together and run seamlessly
    under a monolithic architecture [173]. Microservices’ malleability might hasten
    the deployment of novel modifications, necessitating the development of novel
    patterns. In software engineering, a “pattern” is supposed to refer to any mathematical
    approach that is known to function. An “anti-pattern” is an erroneous pattern
    that is often applied to achieve a solution but often ends up causing even more
    problems. (3) Container Technologies: Given the advent of Docker, container technology
    has gained widespread use in the cloud computing sector, where it is used to efficiently
    execute user workloads [174]. Since containers are independent entities that may
    run without sharing data with other containers, this technology provides an inexpensive
    cloud environment for deploying applications. In a container, applications deployed
    on the same hardware server can share the same underlying resources while maintaining
    their own distinct processes [175]. Container technology leverages Linux kernel
    capabilities, such as libcontainer and control groups (cgroups). By utilizing
    cgroups and namespaces, Docker can operate containers independently within a host
    node, providing the container with its own dedicated set of runtime resources
    (including the host’s networked devices, disc space, memory, and CPU). In addition,
    namespaces provide for more efficient application deployment and development by
    separating the program’s perspective from the operating environment [176]. Furthermore,
    containerization becomes an example of creating, publishing, and running applications
    in an isolated way and is indicated as a Container as a Service (CaaS). There
    are three primary advantages of CaaS: (1) containers boot up in no time at all;
    (2) they consume fewer resources than VMs; and (3) many instances may be operated
    at once using container technology [177]. Recent investigations [178] into container
    technology reveal unanswered research questions. Firstly, containers are less
    secure than VMs since they share the kernel, but this is something that may be
    fixed in future versions with the help of Unikernel. Secondly, optimizing container
    performance is a time-consuming endeavor that requires buffer space. Swarm and
    Kubernetes are two examples of cutting-edge cloud computing tools that may be
    used for handling user-created QoS-based container clusters [179], [180]. Thirdly,
    because containers share the same computing/hardware resources, co-located tenants
    can suffer from unpredictable performance interference when the CPU Shares algorithm
    is used, and even worse, they can leak information enabling side-channel attacks
    to be performed by a malicious tenant [181]. (4) Serverless Computing: The use
    of serverless computing in the creation of apps for the cloud is gaining traction
    [182]. The goal of serverless computing is to ensure that only the most effective
    serverless technologies are deployed, reducing costs while increasing benefits
    [183]. Meanwhile, companies in all industries are adopting AI since it is the
    next generation of innovation. Due to these AI-driven platforms, we have been
    able to make more accurate, timely decisions [184]. They have altered the methods
    used to conduct business, communicate with customers, and assess company information.
    Complex ML systems can significantly affect developers’ output and efficiency
    [185]. However, switching to a serverless architecture may be able to solve many
    of the issues that engineers face. The serverless design ensures that the machine
    learning models are administered correctly and that all available resources are
    utilized efficiently. Developers will be able to devote a greater amount of time
    to training AI models rather than maintaining the server environment [186]. Creating
    ML algorithms is a common practice when confronting difficult problems. They perform
    tasks such as data analysis and preprocessing, model training, and AI model tuning
    [186]. Serverless computing running AI tasks will provide for reliable data storage
    and communication. 4.3.1.3. Trends/observations The main trends and observations
    regarding centralized computing are as follows: (1) AI-driven Computing: The fundamental
    benefit of autonomic computing is a reduced overall cost of ownership. Therefore,
    the cost of upkeep will be drastically reduced. The number of technicians required
    to keep everything running smoothly will go down as a result, too. Autonomous
    IT systems driven by AI will reduce the time and money needed for installation
    and upkeep while also improving IT system stability [4]. In accordance with higher-order
    benefits, businesses would be more capable of handling their operations with the
    help of IT systems that are able to adopt and execute directions based on their
    business plan and allow for adjustments in reaction to evolving circumstances.
    Using AI-based autonomic computing has several advantages, including reducing
    the expense and quantity of human labor needed to manage large server farms, which
    is made possible through server consolidation [187]. Using AI for self-driving
    computers will simplify system administration. As a result, computer systems will
    be greatly enhanced. Server load distribution is another potential use case since
    it allows for parallel data processing across several computers. Meanwhile from
    an energy perspective, analyzing the power grid in real-time allows for more cost-effective
    and long-term power policy decisions to be made [1]. There are benefits to using
    remote data centers instead of keeping data in-house. Despite the hefty upfront
    expenses, businesses may obtain AI technology relatively easily by paying a monthly
    fee on the cloud. When employing an AI-powered system, there may be no need for
    human involvement in data analysis [188]. Using AI in the cloud can potentially
    make businesses more effective, strategic, and insight-driven. AI can increase
    output by automating routine processes and data analysis without human intervention
    [74]. For instance, integrating AI technology with Google Cloud Stream statistics
    could enable real-time personalization, anomaly detection, and management scenario
    prediction [189]. As the number of cloud-based applications grows, it is essential
    to implement a system of rigorous data protection based on intelligence. Network
    security systems backed by AI-enabled traffic tracing and analysis; AI-enabled
    devices can sound an alarm as soon as an anomaly is detected. Such methods will
    ensure keeping sensitive data protected. (2) Net Zero Emissions: Several data
    center operators have committed to being carbon neutral by the year 2030 as sustainability
    becomes an increasingly hot subject in the industry [190]. But are these promises
    only a reaction to the possibility of legislation, or is it actually making progress?
    If business planes are a major contributor to global warming, how do they plan
    to cut their carbon footprint so rapidly? The data centers’ businesses in the
    United States use about as much power as the state of New Jersey [191]. If all
    of the power came from renewable resources, this level of demand would not be
    a problem. Liquid cooling and energy generation both require water, and a typical
    data center uses as much water as an urban area of 30,000 to 50,000 individuals
    [192]. Becoming a pioneer in sustainability might also bring up emerging markets.
    Companies are going to utilize green data centers to offset their carbon footprints
    as they grow and become more energy efficient and sustainable [193]. A car company,
    for instance, might employ emission-free data centers for all of its corporate
    services. Last but not least, adopting environmentally friendly practices may
    help businesses comply with environmental rules, avoid fines, and get access to
    attractive, low-interest, long-term capital investment possibilities [194]. 4.3.2.
    Decentralized computing In this section, we discuss the main focus or paradigms,
    technologies or impact areas, and various trends or observations within decentralized
    computing. 4.3.2.1. Focus/paradigms The following are the main focus or paradigms
    for decentralized computing: (1) Parallel Computing: Through the utilization of
    several processor cores, parallel computing can perform multiple tasks simultaneously.
    The ability to divide and conquer a work into smaller, more manageable chunks
    is what sets parallel computing apart from its serial counterpart [195]. Real-world
    events may be modeled and simulated effectively on parallel computing systems
    [196]. As processing and network speeds continue to increase at an exponential
    rate, adopting a parallel architecture is no longer just a nice-to-have. The IoT
    and big data will eventually require us to process terabytes of data simultaneously.
    Devices such as dual-core, quad-core, eight-core, and even 56-core CPUs utilize
    parallel computing. Therefore, although parallel computers are not brand new,
    this is the problem: These new technologies are spitting up ever-faster networks,
    and computer efficiency has surged 250,000 times in 20 years [197]. For instance,
    AI technologies will sift through more than 100 million patients’ heart rhythms
    in the medical sector alone, looking for signals of A-fib or V-tach, saving many
    lives in the process [196]. When the systems must slowly move through each procedure,
    they will not be able to complete it on time. As great as the potential is, parallel
    computing may be nearing the edge of what it can achieve with conventional processors.
    Parallel calculations may see significant improvements in the coming decade, thanks
    to quantum computers. In a current, unauthorized announcement, Google claimed
    to have achieved quantum supremacy [76], [198]. If it is accurate, then Google
    has created a machine that can perform in 4 min whatever would require the most
    capable supercomputer on the planet 10,000 years to achieve [51]. Quantum computing
    is a major step forward for parallel computation. Imagine it like this: Processing
    in a serial fashion does one task at a time. An 8-core simultaneous computer can
    do eight tasks simultaneously. There are fewer particles in the universe than
    there are qubits’ states in a 300-qubit quantum computer [198]. (2) Fog Computing:
    The proliferation of IoT devices and the effort needed for analyzing and storing
    enormous amounts of knowledge led to the development of fog computing as a complementary
    service to traditional cloud computing. Fog computing, which provides fundamental
    network functions, can back IoT apps that require a small response-time window
    [37]. Due to the dispersed, diverse, and constrained nature of the fog computing
    paradigm, it is challenging to spread IoT application operations effectively within
    fog nodes to meet QoS and Quality of Experience (QoE) limitations [39]. Vehicle-to-Everything
    (V2X), medical tracking, and manufacturing automation adopt fog computing as it
    delivers the ability to compute close to the consumer to match fast response demands
    for these applications. Due to the proliferation of IoT devices, these applications
    generate massive volumes of data. Cloud computing falls short of satisfying latency
    demands due to the transmission of data over long distances and network overload.
    Bridging data sources and CDCs, it sets up a network of gateways, routers, switches,
    and compute resources [199]. The use of fog computing enhances the capabilities
    of cloud computing due to its minimal latency and cost-effectiveness, as well
    as the decrease in bandwidth necessary for the transit of data. It is more secure
    to process confidential information locally at the fog nodes, and if/when needed,
    only submit trained models – not raw data – to intermediate nodes and eventually
    the cloud for aggregation, e.g., via federated learning [200]. These applications
    collect data from various IoT devices to deliver useful insights and deal with
    latency issues [201]. (3) P2P Network: This network is formed in its most basic
    form when two or more PCs are linked to one another and exchange resources without
    passing through a third computer that acts as a server [23]. A P2P network might
    be a spontaneous connection, which would consist of two or more computers linked
    together using a Universal Serial Bus for the purpose of file sharing. In a fixed
    infrastructure, P2P networking utilizes copper lines to connect six PCs located
    in a single workplace [24]. Alternately, a peer-to-peer network may be an ecosystem
    that is considerably larger in scale and is characterized by the use of specialized
    protocols and apps to establish direct links between consumers over the web. (4)
    Osmotic Computing: This model has become pervasive in various settings, from urban
    planning and healthcare to linked vehicles and Industry 4.0 [46]. It lays the
    groundwork for a system in which vehicles, pedestrians, and urban infrastructure
    interact and share real-time information to improve traffic flow. As more people
    use IoT applications housed in different types of networks (cloud, edge, and IoT),
    it is now clear that the providers who make up the IoT’s service ecosystem (data,
    service, network, and equipment) are all interconnected [48]. In this setting,
    buyers and sellers implicitly expect their data and services to be secure and
    trustworthy. There is no requirement for familiarity with the federated ecosystem
    (service, data, and network) for users of the IoT apps to connect with many applications
    using a web-based user interface [202]. Users send their information to application
    providers without realizing that those trusted suppliers may share that information
    with any third parties (such as a company that hosts analytics on the cloud or
    a company that provides the infrastructure for mobile devices). Security issues
    may arise for software due to the wide variety of computing devices available
    from different manufacturers and their presence in an untrusted realm with no
    overarching authority [203]. (5) Dew Computing: It stands out because of its near-complete
    independence from Internet access, its users’ physical closeness to servers, its
    low latency, outstanding speed, excellent user interface, and adaptability in
    terms of control available to users [204]. Instead of serving as a replacement
    for cloud computing, dew computing serves as a useful supplement. In the not-too-distant
    future, people throughout the globe might be able to limit their time spent online,
    increasing their efficiency and effectiveness. Countries have adopted measures
    to handle the influx of Internet users caused by the COVID-19 blackout. To lighten
    the Internet’s burden, video streaming services are reducing visual quality, while
    others just update their software outside of peak viewing times. The dew computer’s
    proximity to the user in the design means it can facilitate all electronic interactions
    with fewer steps and more efficient data transfer [204]. (6) Edge Computing: Since
    its origins in content delivery networks, distributed computing has matured into
    the mainstream as an edge computing paradigm that places resources near the client’s
    end. Big data is typically best stored in the cloud, whereas immediate information
    created by consumers and exclusively for the customer needs computing power and
    storage on the edge [40]. To accommodate growing mobile user needs, cloud providers
    have realized they must shift crucial processing to the device. With its high
    performance and low cost, edge computing is a key driver for AI. This can be the
    most helpful method to see how AI relates to edge computing. Due to the data-
    and compute-intensive characteristics of AI, edge computing aids AI-powered applications
    in resolving their technical problems. AI/ML systems consume large amounts of
    data to discover trends and provide trustworthy recommendations [205]. AI use
    cases that need video analysis face latency challenges and rising expenses due
    to the cloud-based transmission of high-definition video data. The delay and reliance
    on central processing in cloud computing are problematic when ML inputs, outputs,
    and (re-)training data must be handled in real-time. It is possible to perform
    computation and decisions at the edge, eliminating the need for costly backbone
    connections and allowing immediate action on the data. Client information regarding
    location is stored at the edge instead of in the cloud for security reasons. When
    data is streamed to the cloud, all relevant data and datasets are uploaded. Edge
    networks for computing have introduced several difficulties associated with infrastructure
    management because of their dispersed and intricate nature [206]. Managing resources
    efficiently requires carrying out several tasks. Examples include VM consolidation,
    resource optimization, energy efficiency, workload prediction, and scheduling.
    Resource management has historically relied on static, established guidelines,
    mostly based on operations research methodologies, even in dynamic, rapidly changing
    settings and in immediate situations. To deal with these issues, especially when
    choices must be made, AI-based solutions are being used more and more frequently.
    AI/ML methods have become increasingly common in the past few years [207]. However,
    selecting where on edge to carry out a task can be challenging, as it requires
    considering tradeoffs like the volume of data on edge servers and the ability
    to move users [208]. The cache has to anticipate the consumer’s next destination
    for it to build on the notion of mobility [209]. It is situated at a suitable
    edge to cut costs and energy consumption. Several different methods, including
    genetic algorithms, neural network models, and reinforcement learning, are utilized
    in this process. • Mobile Edge Computing: Mobile Edge Computing – now Multi-access
    Edge Computing (MEC) – expands its possibilities by introducing cloud computing
    to the web’s edge. Initially targeted solely on the edge nodes of mobile networks,
    MEC has since expanded its scope to include conventional networks and, ultimately,
    integrated networks. While typical cloud computing occurs on servers located far
    from the end-user and devices, MEC enables activities to be carried out at base
    stations, centralized controllers, and various other aggregating sites on the
    Internet [210]. MEC improves consumer QoE by redistributing cloud computing workloads
    to customers’ individual, on-premises servers, thus relieving congestion on mobile
    networks and lowering latency [211]. Innovative applications, services, and user
    experiences are being unlocked at a dizzying rate thanks to advances in edge data
    generation, collection, and analysis and in the transmission of data between devices
    and the cloud [212]. Because of this, MEC is accessible to consumers and businesses
    in a wide range of contexts and industries. Integrating MEC into a camera network
    improves the speed with which data may be stored and processed. With sufficient
    processing power and bandwidth, data may be immediately analyzed locally instead
    of being sent to a remote data center [213]. Self-driving automobiles and autonomous
    mobile robots (AMRs) are two examples of emerging technologies that require powerful
    ML to arrive at judgments rapidly. If such decisions take place in a remote data
    center, only seconds might be the distinction between nearly escaping failures
    and causing a tragedy [205]. Because the vehicle must avoid hitting pedestrians,
    animals, and other vehicles, judgments must be made on the vehicle. Machine-to-machine
    (M2M) communication will be essential to the success of 6G as the forthcoming
    generation of a global wireless standard and the technological advances that will
    emerge from it [101]. 4.3.2.2. Technologies/impact areas The key technologies
    and affected domains for decentralized computing include: 4.3.2.2.1. Distributed
    ledger technology The computing paradigms of fog, edge, and cloud are currently
    experiencing explosive growth in both the business and academic worlds. Security,
    confidentiality, and data integrity in these systems have become increasingly
    important as their practical applications have expanded [214]. Data loss, theft,
    and corruption from malicious software like ransomware, trojans, and viruses raise
    serious considerations in this area. For the system’s and most importantly, end-users’
    sake, it is crucial that data integrity be maintained, and that no data be delivered
    from an unauthenticated source. Medical care, innovative cities, transport, and
    monitoring are all examples of applications of critical importance where the margin
    for error is near zero [4]. • Blockchain: Because the majority of edge devices
    have limited computing and storage capacity, developing an appropriate system
    for data security, and preserving integrity is challenging. The IoT and other
    real-time systems have used blockchain technology for data security [134]. To
    store and monitor the worth of an asset over time, a blockchain is, in theory,
    a set of distributed ledgers. When new information is added to the system, it
    becomes a block with a Proof of Work (PoW). A PoW is a hash value that cannot
    be made without changing the PoW of the blocks that came before it in the ledger.
    Miners create and verify these PoWs while also mining blocks in the Fog network
    [215]. After a miner has completed the PoW, it broadcasts the newly created block
    into the network, where the other nodes check its legitimacy before joining it
    in the chain. Also, the fraudulent change of data in a blockchain will not work
    unless at least half of the copies of the data in question are changed individually
    by carrying out the same actions. With such a strict time constraint, modifying
    any data in the blockchain will be extremely difficult. Network nodes must offer
    route selection, preservation, financial services, and mining for the blockchain
    to function. Considering these challenges, numerous groups have worked to develop
    solid frameworks for combining blockchain and fog computing [133]. The majority
    of these systems employ a dynamic allocation mining technique in which the least-used
    nodes mine and validate the chains. In contrast, the remaining nodes are employed
    for load balancing, computation, and data collection [108], [216]. The blockchain
    on a large portion of the network is replicated at those nodes if a worker detects
    an issue in relation to blockchain manipulation or signature forging. Furthermore,
    blockchains offer public-key encryption with adaptive key exchange for further
    security. Blockchain is a deceptively simple central notion, but incorporating
    it into fog computing systems presents several challenges. Cost and upkeep are
    major factors surrounding storage capacity and scalability. Only complete nodes
    (nodes that can fully validate the transactions or blocks) store the whole chain,
    which still results in massive storage needs. Data anonymity and privacy issues
    are another blockchain shortcoming. Privacy is, therefore, not incorporated into
    the blockchain architecture; consequently, third-party tools are necessary for
    accomplishing these crucial requirements [217]. This might result in less efficient
    applications that demand more resources (both computationally and in terms of
    storage space) to run. There are still numerous unresolved issues and potential
    future developments for blockchains in IoT architectures [13]. Insufficient resources
    are the main barrier to excellent data protection and dependability. Because of
    resource limits, more complex encryption or key generation cannot be incorporated
    with these chains of data [218]. Only restricted encryption algorithms may be
    implemented. By considering resource limitations, more effective algorithms may
    be created. In high fault-rate scenarios, wherein the edge nodes are susceptible
    to attack at any time, modifying such chains is another essential approach [219].
    Network and I/O bandwidth needs are greatly increased due to the necessity of
    revalidating blocks and copying chains from the primary network. The majority
    of frameworks additionally use a master–slave architecture, which introduces a
    potential weak spot. In diverse settings, this is to be expected. The balance
    between cost and reliability must be meticulously evaluated when considering redundancy
    [132]. The blockchain flaws also continue to impact fog architectures. There is
    a need to develop efficient consensus techniques that can validate blocks with
    little block sharing and copying. Those curious might learn more about blockchain
    by reading an in-depth report on the topic. 4.3.2.2.2. Federated learning Data
    is needed for ML model training, testing, and validation. Information is stored
    in locations accessible by thousands or millions of users (devices). Rather than
    sharing the entire dataset required to train a model, federated devices only communicate
    the parameters specific to that device’s instance of the model. The parameter
    sharing mechanism is defined by the federated learning topology [220]. Each participant
    in a centralized topology contributes the parameters of the model to a centralized
    server, which then trains the centralized model and returns the trained parameters
    to each participant. Parameters are typically shared among a smaller group of
    peers in other configurations, including peer-to-peer or hierarchical ones. ML
    methods that require large or geographically dispersed data sets may benefit from
    federated learning. However, there is no universally applicable machine-learning
    solution [221]. Several unanswered questions remain about federated learning that
    researchers and developers are hard at work trying to address [222], [223]. There
    are a lot of opportunities for efficient communication in federated learning.
    This means the master server or other entities acquiring the parameters must be
    able to cope with occasional interruptions or delays in transmission. Getting
    all the federated devices to talk to each other and stay in sync is still an open
    issue [222]. There is typically a lack of transparency between federated parties
    and a central server regarding the computing capacity of the federated parties.
    However, it is still challenging to ensure that the training activities will operate
    on a diverse mix of devices [220]. Federated parties’ data sets might be quite
    varied in terms of data amount, reliability, and variety [224]. It is challenging
    to predict how statistically diverse the training data sets will be and how to
    protect against any detrimental effects this diversity may have. Efficient deployment
    of privacy-enhancing solutions is required to prevent data loss due to shared
    model parameters. 4.3.2.2.3. Bitcoin currency Transaction settlement using blockchain
    technology was initially proposed with the digital (crypto-) currency Bitcoin.
    The blockchain is a distributed ledger that verifies monetary transactions using
    PoW and may be configured to record anything of worth. Blockchains, including
    bitcoins and cryptocurrencies, are innovative in operating apps across networks
    [225]. Designers create smart contracts for Bitcoin money exchanges, which are
    subsequently carried out on blockchain VMs [226]. Blockchain relies on a decentralized,
    concurrency-agnostic runtime environment and consensus mechanism. Blocks of data
    may be disseminated across Bitcoin ledgers via a peer-to-peer network with no
    requirement for a centralized authority, thanks to the Bitcoin enabling network
    [226]. The data in the blockchain is certified by the members to keep it safe
    and open, and anybody is welcome to join the network. Cloud computing may use
    this property, and the security of cloud storage, in particular, can benefit from
    it. Cloud computing infrastructures enable the execution of complex applications
    and the handling of massive data sets. Centralized data centers coupled with Fog
    or IoT devices at the network edge cannot efficiently handle the enormous data
    storage required to deliver high-availability, real-time, low-latency services
    [227]. A distributed cloud design is required to deal with these problems instead
    of the more conventional network architecture. Blockchain technology, a fundamental
    element of distributed cloud systems, offers detailed control over resources by
    enabling their management through distributed apps [228]. It also allows for the
    tracking of resource usage, providing both customers and service providers with
    the means to verify that the agreed-upon QoS is being met. A marketplace is a
    platform where everyone may promote their computer resources while discovering
    what they require using AI-based techniques or models of prediction [229]. Blockchains,
    compared to cloud computing, offer fewer computer resources available to run distributed
    applications, such as less space for storing data, less powerful VMs, and a more
    unstable protocol. As a result, apps that are sensitive to delay and those that
    use a lot of resources need to find solutions to these problems [230]. Combining
    blockchain and cloud computing to develop a blockchain-based distributed cloud
    can provide novel advantages and solve current restrictions. Data moves closer
    to its owner and user through Blockchain’s distributed cloud, providing on-demand
    resources, security, and cost-effective access to infrastructure [231]. In the
    meantime, the high price and substantial consumption of electricity from clouds
    may be solved with a blockchain-based distributed cloud. Cloud storage security
    is another area where blockchain may play a role in the future [232]. By dividing
    user data into smaller pieces before storing it everywhere, it is possible to
    encrypt it further. A small portion of the data is accessible to the hacker, not
    the entire file. In addition to eradicating data-altering hackers from the network,
    a backup copy of the data may be used to restore any changes [229]. The use of
    quantum computers to circumvent the mathematical impossibility of modern encryption
    is one of their most publicized uses. In the meantime, many online publications
    have predicted the end of Bitcoin and other cryptocurrency use after Google stated
    it had achieved quantum supremacy. 4.3.2.3. Trends/observations The main trends
    and observations regarding decentralized computing are as follows: Serverless
    Edge Computing: Serverless’ ‘scale-to-zero’ feature, which releases unoccupied
    containers from the system, works well for energy-conscious IoT scenarios with
    load-inconsistent applications. On the other hand, fine-grained scaling (i.e.,
    at the function stage) is capable of handling extremely distinct needs and execution
    settings at the edge [185]. Many IoT applications rely on instances initiated
    by sensing or actuating, just like functions in serverless [102]. However, unlike
    serverless functions, IoT devices often sense or act only on rare occasions, whereas
    they sleep the majority of the time to conserve power. So, first, serverless appears
    to be an ideal paradigm of execution. However, combining serverless, edge computing,
    and IoT applications is challenging because serverless was originally designed
    for cloud environments, which do not have the same constraints as edge computing
    devices [233]. In light of this opportunity, it is essential to combine serverless,
    edge computing, and IoT applications to address this challenge. This is crucial
    to be addressed, as the fact is that although this adaptation looks needed and
    helpful, its practicality necessitates comprehensive inspections to avoid ramifications.
    4.3.3. Hybrid computing It involves combining both a centralized network and a
    decentralized network. In this section, we discuss the main focus or paradigms,
    technologies or impact areas, and various trends or observations within hybrid
    computing. 4.3.3.1. Focus/paradigms The following are the main focus or paradigms
    for hybrid computing: Fog-Cloud-Edge Orchestration: Increasingly, IoT technologies
    are required in daily life. Smart cities, automated manufacturing, virtual reality,
    and autonomous cars are just a few instances of the vast variety of sectors where
    the application of these technologies has been rising quickly [234]. This type
    of IoT application frequently necessitates access to heterogeneous distant, local,
    and multi-cloud compute resources, in addition to a globally dispersed array of
    sensors. The expanded Fog-Cloud-Edge orchestration paradigm is born from this.
    This new paradigm has made it a necessity to expand application-orchestration
    needs (i.e., self-service deployment and run-time administration) beyond the confines
    of a purely cloud-based infrastructure and across the full breadth of cloud or
    edge resources. Recent years have seen an increased focus on the research and
    advancement of orchestrating platforms in both business and academic settings
    as a means of meeting this need. 4.3.3.2. Technologies/impact areas The key technologies
    and affected domains for hybrid computing include: (1) Cryptocurrencies: Decentralized
    networks with powerful computational power were pioneered by cryptocurrencies.
    There is no centralized authority that controls the cryptocurrency market or issues
    new cryptocurrencies. Bitcoin, the first decentralized digital currency, was launched
    in 2009 and employs blockchain technology to record transactions and save user
    histories [235]. Blockchain Explorer and similar tools reveal Bitcoin’s decentralized
    network activity as it moves from one wallet to another, and they also reveal
    the activity of other cryptocurrency networks. There is no equivalent technology
    that would enable such transparency in the private banking business, nor would
    such a publication ever be made public. Decentralization design incorporates many
    additional features that make it hard for bad actors to forge bitcoin or steal
    from user accounts, such as synchronizing the blockchain across all machines on
    the network [236]. Bitcoin and other cryptocurrencies are required to function
    on decentralized networks: A blockchain does not have a central controlling computer
    or administrator. (2) Machine Economy: The emerging machine economy refers to
    the exchange of resources (such as power, data storage, processing power, currency,
    and network connections) in the upcoming global networks of computers [237]. Together,
    the data centers that power the cloud, the web, and monetary exchanges, form a
    network that will support the machines that power the future economy. This is
    the time when AI willfully conceals or exaggerates its powers. AI conceals and
    safeguards limited supplies to protect the crucial scarce resource of computation
    cycles used to generate AI insights. The organization is guarding the computation
    cycles used to generate AI insights, which are the most crucial scarce resource
    in this case. Lies, trickery, and barter to coax AI into parting with its limited
    resources will become an increasingly hot issue in the coming years [238]. To
    prevent itself from being overused, AI will have to resort to dishonest behavior.
    The machine economy is going to be among the most significant developments to
    come for human culture; and will be among the hottest topics of the emerging payment
    and AI technologies needed to fund future interstellar and interplanetary travel.
    4.3.3.3. Trends/observations The main trends and observations regarding hybrid
    computing are as follows: Distributed Computing Continuum: Emerging from the convergence
    of IoT, edge, fog, and cloud computing, Distributed Computing Continuum Systems
    (DCCS) represent a novel computing paradigm that harnesses the collective power
    and heterogeneity of these diverse computing tiers to address the demanding computational
    requirements of future applications [239]. These applications, ranging from autonomous
    vehicles and e-Health to smart cities, holographic communications, and virtual
    reality, demand unprecedented levels of computational power, low latency, and
    efficient data management. Achieving these stringent requirements necessitates
    seamless integration and collaborative operation among all computing tiers, transforming
    the underlying infrastructure into a unified, intelligent system. As exemplified
    by edge and fog computing, the underlying infrastructure of DCCS plays a pivotal
    role in determining its performance. This geographically distributed, heterogeneous,
    and resource-constrained infrastructure poses significant challenges, needing
    new approaches that can dynamically adapt to application and user demands [9].
    Cloud-centric methodologies, often tailored to cloud-specific assumptions, fall
    short in addressing the characteristics of edge, fog, and DCCS environments. To
    address these challenges, DCCS advocates for decentralized intelligence, empowering
    each component of the underlying infrastructure to make autonomous decisions based
    on its specific tasks and local conditions [240]. This approach leverages the
    concept of service level objectives (SLOs), well-established in cloud computing,
    to define the operational goals of each component of the system. By modularizing
    and distributing SLOs across the system, a DCCS can achieve scalable intelligence
    within its infrastructure. Further, incorporating the Markov Blanket concept into
    SLO management enables causal filtering, ensuring that only conditionally dependent
    variables are considered when making decisions. This selective filtering, coupled
    with causal inference or active inference, empowers each component to make informed
    decisions independently, adapting to its dynamic environment and the overall system’s
    requirements [241]. This loosely-coupled architecture fosters a resilient and
    adaptive DCCS, capable of catering to the diverse and evolving demands of future
    applications. 4.4. Computational methodologies: Parallel vs. Sequential computing
    Parallel computing implies a computer model wherein numerous tasks are completed
    concurrently, employing a number of processors or threads [242]. In this paradigm,
    many processes run concurrently and their outputs are pooled. Tasks can be conducted
    in parallel instead of sequentially, potentially reducing execution times. 4.4.1.
    Parallel computing In this section, we discuss the main focus or paradigms, technologies
    or impact areas, and various trends or observations within parallel computing.
    4.4.1.1. Focus/paradigms The following are the main focus or paradigms for parallel
    computing. Simultaneous Data Processing: In order to handle many parts of a task
    at once, parallel processing employs multiple processors, or CPUs. By breaking
    down large computations into smaller ones, systems may drastically speed up their
    execution [242]. Parallel processing is possible on current computers with multiple
    cores and on any machine with more than one CPU. Multi-core processors are embedded
    processors containing two or more CPUs for increased performance, lowered energy
    use, and more efficient handling of many tasks. Two to four cores are common in
    modern computers, with some models supporting up to 12. Modern computers commonly
    use parallel processing to complete complex processes and calculations. At the
    most basic level, sequential and parallel-serial processes differ in how registers
    are employed. Shift registers work in series, computing every bit one at a time,
    while registers with concurrent loading handle each bit of a word concurrently
    [243]. Using multiple functional units that can execute identical or distinct
    tasks in parallel enables the management of more complex parallel processing.
    4.4.1.2. Technologies/impact areas The key technologies and affected domains for
    parallel computing include: (1) ASICs: Application-Specific Integrated Circuits
    (ASICs) are integrated circuits designed for specific uses. As their name suggests,
    ASICs are limited to a single function. They provide a single function and are
    consistent throughout their service life [138]. ASICs are semiconductor devices
    and circuitry developed to carry out a particular task. In contrast to mainstream
    processors, including CPUs and GPUs, both the speed and the energy efficiency
    of ASICs are optimized to fit the needs of a specific application [244]. Their
    excellent performance, minimal energy use, and small form factor make them ideal
    for mass-produced goods that can afford the higher bespoke design costs. (2) FPGA:
    A Field Programmable Gate Array (FPGA) is a semiconductor that can be programmed
    to provide unique logic for use in both early system prototype design and the
    last version of a system to circumvent obsolescence [138]. In contrast to other
    bespoke or semi-custom integrated circuits, FPGAs can be easily reprogrammed by
    a software update to meet the changing requirements of the larger system they
    are integrated into, using hardware design languages, such Verilog and Very High-Speed
    Integrated Circuit Hardware Description Language (VHDL) [245]. Nowadays, most
    rapidly expanding applications are perfect fits for FPGAs, which include edge
    computing, AI, network security, 5G, industrial control, and automated machinery.
    4.4.1.3. Trends/observations The main trends and observations regarding parallel
    computing are as follows: (1) Neuro-symbolic AI: Advances in deep learning techniques
    have unlocked a few of AI’s enormous possibilities. Consequently, it is now obvious
    that these methods are at a breaking point and that such sub-symbolic or neuro-inspired
    solutions only function effectively for particular kinds of issues and are typically
    opaque to both analysis and comprehension [246]. However, symbolic AI methods,
    founded on rules, logic, and reasoning, perform significantly better in terms
    of openness, comprehensibility, authenticity, and reliability than sub-symbolic
    methods. A new path termed neuro-symbolic AI was recently recommended, integrating
    the effectiveness of sub-symbolic AI alongside the visibility of symbolic AI [247].
    This synergy has the potential to yield a new generation of AI devices and platforms
    that are both comprehensible and expansion-intolerant and can combine logic with
    learning in a generic fashion. (2) Scalability: The most important advantage of
    scalable design is improved efficiency, as well as the capacity to deal with sudden
    spikes in traffic or severe loads with little to no warning [248]. An application
    or online company may continue to operate smoothly during busy periods with the
    assistance of a scalable system, preventing businesses from incurring financial
    losses or suffering reputational harm [173]. If a system is organized into component
    services (for example, using the microservices system design), monitoring, updating
    features, troubleshooting, and scaling may become simpler tasks. 4.4.2. Sequential
    computing In this section, we discuss the main focus or paradigms, technologies
    or impact areas, and various trends or observations within sequential computing.
    4.4.2.1. Focus/paradigms The following are the main focus or paradigms for sequential
    computing: One-process-at-a-time execution: In the context of computing, sequential
    computing describes a paradigm in which operations are carried out in a certain
    order, with the output of one operation feeding into the data being the input
    of the subsequent one [249]. A single processor carries out all of the model’s
    tasks in the sequence specified by the code. 4.4.2.2. Technologies/impact areas
    The key technologies and affected domains for sequential computing include: Traditional
    Von Neumann Architecture: This architecture is a sequential computing-based concept
    for digital machines. This system includes a CPU, RAM, and I/O devices, all interconnected
    by a bus [250]. The CPU of a system based on the Von Neumann architecture processes
    instructions sequentially, feeding the output of one into the input channel of
    the subsequent one [107]. 4.4.2.3. Trends/observations The main trends and observations
    regarding sequential computing are as follows: (1) In-Memory Computing: In-memory
    computing is a method used to perform computations solely in memory (like RAM).
    This word usually refers to massive and complicated computations that must be
    executed on a cluster of computers using specialized systems software [249]. As
    a clustering system, the machines pool their RAM, so the computation is effectively
    done across machines and uses the combined RAM capacity of all the machines collectively.
    (2) Energy-efficiency: Power effectiveness and sustainability have emerged as
    major issues for HPC systems as their processing capacity increases [251]. To
    reduce electrical usage while increasing computational performance, scientists
    are inventing environmentally friendly hardware layouts, investigating innovative
    cooling strategies, and fine-tuning algorithms. The general efficiency of HPC
    systems is being improved by the development of energy-aware scheduling and utilization
    strategies. (3) Performance Optimization: Since single-processor efficiency can
    no longer develop at a rapid pace, the era of the single-microprocessor computer
    is coming to an end. It is time for a new era in computing when parallelism takes
    center stage and sequential computing takes a back seat [252]. There are still
    significant scientific and engineering obstacles to overcome, but now is a good
    moment to try new approaches to computer programming and hardware design. Various
    computer architectures have emerged, each tailored to certain performance and
    efficiency goals. The next wave of discoveries will certainly necessitate enhancements
    to computer hardware and software [253]. No one can say for sure if we will succeed
    in making parallel computing as mainstream and user-friendly as yesterday’s peak
    sequential single-processor computer systems in the field of computing. Innovative
    novel applications that motivate the computer business will slow down if parallel
    programming and associated software activities do not become popular, and if creativity
    slows down across the economy as a whole, many other sectors will suffer as well
    [121]. 4.5. Computing trends and emerging technologies New computing trends and
    emerging technologies continue to advance the field of computing, improving the
    adaptability, self-management, and sustainability of many types of industrial
    systems. 4.5.1. Advanced computing styles and trends In this section, we discuss
    advanced computing styles and trends and their related technologies and paradigms.
    4.5.1.1. Focus/paradigms The following are the main focus or paradigms for advanced
    computing styles: Quantum AI: Quantum computing is attractive because it is a
    unique innovation that can radically change AI and computing in general. In this
    section, we look into what quantum computing can do and how it can affect AI and
    the wider economy. The implications of this computing method might have far-reaching
    effects on several facets of our cultural and financial lives [4]. The widespread
    impact of AI suggests that combining it with quantum computing might unleash dramatic
    change in the field of AI [198]. Several algorithms that made it possible to do
    tasks previously thought impossible for conventional computers emerged in the
    wake of the foundational studies that formalized the notion of a quantum computer
    [254]. The development of Shor’s algorithm, an effective method for dividing enormous
    amounts of data, has bolstered research into quantum computing and quantum cryptography.
    Yet, existing cutting-edge technologies are not yet accurate enough to execute
    Shor’s algorithm successfully, which requires a degree of precision for performing
    register initialization, quantum operations on multiple qubits, and storing quantum
    states. It is also crucial to remember that quantum computers have particular
    limits [76]. The acceleration afforded by quantum computers grows exponentially
    compared to the amount of time a conventional computer takes (Grover’s method);
    hence, it is not predicted that it will effectively solve NP-hard efficiency issues.
    The benefits of quantum computing, such as quantum superposition and entanglement,
    typically vanish rapidly with the complexity and magnitude (i.e., the number of
    quantum systems involved) of the underlying hardware, making the process of designing
    a quantum computer non-trivial. Despite this, the curiosity of significant technologically
    advanced players (IBM, Microsoft, Google, Amazon, Intel, and Honeywell) has skyrocketed
    in the past few years, and a plethora of fresh startups have emerged to propose
    remedies for quantum computing using technologies as diverse as superconducting
    devices, encased ions, and integrated light circuits. Corporations like these
    are among the numerous that are investing in quantum research and development
    at the moment [255]. Although there are many obstacles to overcome, the Google
    AI team has achieved considerable strides in the past few years, gaining a quantum
    edge by developing Sycamore, a programmable quantum computer. Similarly, IBM has
    now launched the Eagle chip, the first quantum computer with more than 100 qubits
    of hardware [256]. This is only the beginning of an intensive research and development
    program, with the tech giant hoping to increase the number of qubits to over a
    thousand by 2024 [51]. But as was previously stated, protecting these devices
    from ambient noise is a significant constraint when trying to retain the subtle
    characteristics of composite quantum states while still allowing for coherence
    in quantum development. Because of this, a quantum computer’s components require
    ultra-low temperatures in the order of fractions of a Kelvin, which presents hurdles
    for both device design and material development [257]. 4.5.1.2. Trends/technologies
    The main trends and technologies regarding advanced computing styles are as follows:
    (1) Edge AI: Recent advancements in AI efficiency, the rise of IoT devices, and
    the emergence of edge computing have all unleashed the promise of edge AI. This
    has opened up previously unimaginable uses for edge AI, such as helping radiologists
    make diagnoses, assisting in driving cars and fertilizing crops [92]. Since its
    inception in the mid-1990s—paired with the emergence of content delivery networks
    that utilize edge servers positioned near users to stream online and gaming video—edge
    computing has been the subject of much discussion and adoption by professionals
    and businesses. Almost every sector today has tasks that may benefit from adopting
    edge AI. In truth, edge applications are driving the next generation of AI computing,
    which will improve people’s lives in various settings, such as at home, at work,
    at school, and on the road. AI at the edge refers to the application of AI to
    physical devices. In contrast to storing all of an organization’s data in a single
    centralized spot, such as a cloud provider’s data center or a private data warehouse,
    “Edge AI” allows for AI calculations to be performed close to the users at the
    network’s edge. Because the Internet is accessible all across the globe, any area
    might be thought of as its outskirts. Omnipresent traffic signals, autonomous
    equipment, and mobile phones are just a few examples. It might also be anything
    from a shop to a factory to a healthcare facility. Companies of all sizes strive
    to automate more of their processes because doing so improves productivity, effectiveness,
    and safety [258]. Computer software may aid with this through the ability to recognize
    patterns and dependably carry out identical tasks repeatedly. However, it is challenging
    to fully convey them in a system of algorithms and regulations because the world
    is unpredictable and human actions cover infinite circumstances. Today, as edge
    AI has progressed, robots and devices can work with the “intelligence” of human
    cognition no matter what they are. Intelligent IoT apps driven by AI may learn
    to adjust to novel circumstances and effectively complete identical or similar
    tasks [259]. Substantial progress in important areas has allowed for the practical
    deployment of AI models at the edge. Furthermore, developments in neural networks,
    along with other areas of AI, have laid the groundwork for universal ML [260].
    Many companies are finding that they can successfully train AI models and put
    them into action at the edge. AI in the periphery requires widely distributed
    computing resources. Recent advancements in enormously parallel GPUs are currently
    used to run neural networks. The development of devices connected to the IoT is
    partly responsible for the present age’s unparalleled surge in data volume [261].
    The development of sensors, smart cameras, robots, and other data-gathering equipment
    has made it possible to begin using AI models at the edge in nearly all facets
    of business. The increased speed, dependability, and security that 5G/6G is delivering
    to the battleground are also helping IoT use cases [118]. (2) Biologically-inspired
    Computing: The term “bio-inspired computing” refers to creating computer systems
    by drawing inspiration from the natural world. As an aside, computer science is
    also used to model and understand biological processes [145]. Computing architectures
    that take cues from nature can function as autonomous, flexible networks. Similarly,
    bio-inspired computing offers a fresh perspective on AI by building modular, self-improving
    systems [262]. Swarm intelligence refers to the ability of swarms of autonomous
    entities to generate intelligence by collaborating in ways reminiscent of the
    behavior of bees or ants. Biologists, software engineers, computer scientists,
    physicists, mathematicians, and geneticists all work together on the subject of
    bio-inspired computing [263]. Compared to their digital counterparts, biological
    systems have several distinct benefits. AI has advanced thanks to incorporating
    many concepts originally derived from natural processes into machine learning.
    Adaptable and responsive autonomous robots might be extremely useful in high-risk
    settings like conflict zones and hazardous clean-up activities [146]. Tasks like
    crop pollination might be performed by swarms of small robots. Bio-inspired technology
    is being used in cognitive modeling by developing artificial neural network systems
    based on neuron function within the brain. Training, growing, and collaborating
    on computer chips is becoming a reality [264]. When these nodes are linked by
    self-organizing wireless links, they form a system well adapted to modeling issues
    with several basic causes [263]. Self-learning and reconfigurable chips mean less
    time spent loading software and more time spent getting things done. Such systems
    might help explain the propagation of ideas through a community or construct a
    model of brain function that reflects true biological processes. The use of DNA
    in natural computing is a topic of current study. Data storage, covert messaging,
    and even computation are all possibilities that have been proposed by DNA bioinformatics
    studies DNA [265]. DNA molecules may also form practical structures by self-assembly.
    The computer hardware, such as switches, CPUs, and timers, might be replaced by
    biological components. It is already possible to employ some biological substances
    in electronics. Even internal cell programming for purposes like medication secretion
    is feasible. (3) Explainable Artificial Intelligence (XAI): Successful completion
    of computer engineering tasks depends on wise decision-making. Can workloads be
    reliably executed on an automated system? Is there any way to understand how the
    trained models came to their conclusions? Problems like this are typical and must
    be solved until any computer can be used in action [4]. Incorrect decision-making
    about such complicated and cutting-edge technology is costly in terms of resources
    and money. Many AI/ML implementations in computer systems have improved resource
    utilization and energy usage through better decision-making. However, the forecasts
    made by these AI/ML models for computing devices are still not usable, interpretable,
    or implementable. Such restrictions are a common problem for AI/ML models [266].
    Most current research has focused on clarifying how QoS is accomplished, even
    though QoS remains a top priority. Is there anything academics can do to help
    the IT industry move forward? Therefore, when attempting to make educated judgments
    on handling resources (a prime manifestation of AI for computing), a solid grounding
    in Explainable AI (XAI) and experience with XAI methods and tools is required
    [267]. Forecasting of resource and power consumption and SLA variances, as well
    as the implementation of promptly proactive action to resolve these concerns,
    are examples of the types of Explainable AI techniques that may be used. XAI forecasting
    algorithms must be correctly developed to make computing more practical, explicable,
    and deployable [268]. (4) Semantic Web and Decentralized Systems Integration:
    Fog computing has emerged as a software engineering culture and practice that
    combines at least five different technology types: IoT, AI, Cloud-to-Edge Computing,
    Blockchain, and Digital Twins [269]. Various recent projects have presented their
    vision of integration between the Semantic Web and decentralized systems, for
    example, networks based on Blockchain technologies [270]. Here, the main challenge
    is to achieve a new generation of trustworthy, sustainable, human-centric, performant,
    and scalable smart applications. (5) Quantum Internet: It is an ecosystem enabling
    quantum devices to communicate and share data in a setting that uses quantum physics’
    peculiar rules. In principle, this would grant the quantum Internet hitherto unattainable
    skills via standard web apps [59]. Quantum devices, such as a quantum computer
    or a quantum processor, may generate the quantum states of qubits, which can then
    be used to encode information. Sending qubits over a network of physically distinct
    quantum devices is, in essence, what the quantum Internet will be all about. Importantly,
    this will occur because of the strange characteristics of quantum states. That
    probably sounds like the conventional web [271]. However, if one wants to transmit
    qubits, then they need to use a quantum channel instead of a conventional one,
    which requires exploiting the peculiar behavior of quantum particles used to encode
    information onto quantum states. That requires to build up, and apply, relatively
    novel (or exotic) knowledge on the top of what is known about classical computing
    to effectively drive the possible evolution of quantum ecology into an effective
    quantum internet [272] [273] [254]. One could imagine that their favorite web
    browser will not have much in common with the quantum Internet [4]. 4.5.2. Industry
    and sustainability trends In this section, we discuss industry and sustainability
    trends and their related technologies and paradigms. 4.5.2.1. Focus/paradigms
    The following are the main focus or paradigms for industry and sustainability
    trends: Carbon-Neutral Computing: The expansion of the computer age is an important
    factor in the data center industry’s advancement; however, the push towards carbon
    neutrality is a more dramatic paradigm change and the industry’s biggest challenge
    to date. Large-scale cloud providers have pledged to attain zero emissions on
    all initiatives by 2030 [274]. The fight against climate change must include data
    centers. Everything from everyday conveniences like Internet banking and shopping
    to cutting-edge technologies like machine learning, quantum technology, and autonomous
    vehicles would be impossible without them. There is no denying of the ever-increasing
    need for data centers. Nevertheless, because of the damage they cause to the natural
    world, they also attract greater scrutiny [190]. A sustainable future with a zero-carbon
    footprint is possible because of these advancements in electricity, water effectiveness,
    and land utilization. Online conferences and handheld gadgets make it feasible
    for individuals to work from their homes and cut transit carbon emissions; however,
    each bit of data has a carbon footprint of its own [192]. Therefore, whereas electronic
    devices provide opportunities to enhance our oversight of water and materials
    and to support sustainable economic growth, simply sending a message provides
    for the challenging environmental impact of data. However, this may differ greatly
    depending on the spot and efficiency of the data centers that deal with traffic
    [193]. Crucially, as globalization brings online amenities to more societies,
    physical infrastructure, such as data centers, must grow to accommodate an increase
    in consumers, a majority of whom will be in regions around the globe that currently
    lack access to green power availability. 4.5.2.2. Trends/technologies The main
    trends and technologies regarding advanced computing styles are as follows: (1)
    Industry 4.0: The Fourth Industrial Revolution, or Industry 4.0, reshapes how
    goods are made, enhanced, and disseminated. Emerging innovations such as the IoT,
    cloud computing, analytics, and AI/ML are being incorporated into manufacturing
    facilities and processes [275]. Advanced sensors, software with embedded capabilities,
    and robots are used in these “smart industries” to gather information for more
    informed decision-making. When data from manufacturing operations is combined
    with data from Enterprise Resource Planning (ERP), supply chain, customer service,
    and other corporate systems, information that was previously kept separate can
    be seen and understood in completely new ways, which leads to even more value
    being created [276]. Improved efficiency and responsiveness to clients is made
    possible by the advent of technological innovations such as enhanced automation,
    predictive maintenance, and automatic optimization of process enhancements [277].
    To enter the fourth industrial revolution, the manufacturing sector must embrace
    the development of smart factories. The ability to see industrial assets in real-time
    and access preventative maintenance tools may be gained by analyzing the massive
    volumes of big data generated from sensors on the production line. Smart factories
    implementing cutting-edge IoT technology see gains in output and quality [278].
    Manufacturing inaccuracies and costs can be reduced by using AI-powered visual
    insights instead of traditional business models for human inspection. Quality
    assurance staff may monitor production operations from almost any location with
    minimal expenditure using a smartphone linked to the cloud. Companies may save
    money on costly repairs by identifying problems early on with the help of ML algorithms
    [49]. Any business operating in the industrial sector, from individual to process
    production and even in the energy and mining industries, may use the ideas and
    tools of Industry 4.0. (2) Digital Twins: A digital twin is a computerized model
    of and connected to a real-world object that may be used to test and improve its
    design, performance, and usability [279]. Smart sensors embedded in the object
    capture data in real-time, allowing a digital depiction of the asset to be produced
    [99]. The model may be used through an asset’s lifespan, from development and
    testing to actual usage, revamping and eventual retirement. To create a digital
    representation of a physical object, digital twins utilize many technologies.
    The term “IoT” describes the network of interconnected devices and the underlying
    infrastructure that enables them to exchange data and instructions with one another
    and the cloud as a whole. With gratitude to the introduction of affordable computer
    chips and high-bandwidth connectivity, one can now have trillions of gadgets hooked
    up to the global web. Digital twins use data from IoT sensors to replicate physical
    properties in a virtual form [280]. The information is sent into a system or panel
    to be viewed as it changes in real time. Studying, solving issues, and pattern
    recognition are just a few examples of the kinds of cognitive challenges that
    AI seeks to address [281]. AI/ML-based algorithms and statistical models let machines
    do tasks with little to no human help. They do this by relying on patterns of
    observation and inference. Machine learning techniques used in digital twins process
    enormous amounts of sensor data, allowing for the identification of data patterns.
    Optimization of performance, servicing, emissions outputs, and efficiency may
    all be gained using data insights provided by AI/ML [282]. There are several significant
    distinctions between digital twins and modeling: even though both leverage virtual
    model-based simulations, a digital twin maintains a two-way connection and can
    affect the physical object. Offline optimization and the design process are two
    common applications of simulation. Developers use simulators to test out different
    iterations of a product. On the contrary, digital twins are interactive and dynamically
    updated virtual worlds. Both their scope and their utility have increased. 4.5.3.
    Adaptive and self-managing systems In this section, we discuss adaptive and self-managing
    systems and their related technologies and paradigms. 4.5.3.1. Focus/paradigms
    The following are the main focus or paradigms for adaptive and self-managing systems:
    Autonomic Computing: IBM’s autonomic computing program was one of the earliest
    worldwide efforts to develop computing systems with little human intervention
    required to accomplish predetermined goals [30]. It was primarily based on findings
    about how human nerves and thinking work and how they are coordinated—bioinspiration,
    as discussed above. In autonomic computing, researchers explore how software-intensive
    systems can make decisions and act without human interaction to reach the (user-specified)
    “administration” objectives [283]. The concept of control for closed- and open-loop
    systems has significantly impacted the foundations of autonomic computing [31].
    Multiple independent control networks may coexist in practice inside complex systems.
    The integration of ML and AI to enhance resource utilization and efficiency at
    scale remains an important obstacle regardless of investigations into autonomic
    frameworks to handle computing resources, from a single resource (e.g., a web
    server) to resource groupings (e.g., several servers inside a CDC) [4]. Autonomous
    and self-managing systems can be implemented on a spectrum from fully automated
    to partially automated with human oversight through the use of AI/ML to improve
    the efficiency and performance of the computing systems. 4.5.3.2. Trends/technologies
    The main trends and technologies regarding adaptive and self-managing systems
    are as follows: SDN-NFV: The explosion of IoT devices and the concomitant flood
    of sensor data enable knowledge-driven IoT applications, including connected cities
    and smart agriculture [84]. To begin providing such services, one must develop
    a data-gathering method that is flexible enough to adapt to shifting conditions
    in the field. Network programmability (SDN or NFV) enables the easy reconfiguration
    of IoT networks [86]. Current SDN/NFV-based approaches in the IoT environment
    nevertheless fail owing to a shortage of knowledge of resources and overhead,
    as well as incompatibility with conventional protocols [1]. This void must be
    filled by prioritizing resource and power limitations in the creation of SDN/NFV-enabled
    IoT nodes and network protocols. Assigning traffic sources to those Virtual Network
    Functions (VNFs) across the most efficient paths, with sufficient energy and network
    reliability, may maximize the number of active NFV nodes [9]. Summary: Table 3
    lists a summary of open challenges and future directions in Paradigms/ Technologies/
    Impact Areas, along with recommendations for further reading. Table 4 lists the
    summary of Trends/Observations for modern computing along with the recommendations
    for future reading. Table 3. Summary of open challenges and future directions
    in Paradigms/Technologies/Impact areas along with further reading. Paradigms/Technologies/Impact
    areas Open challenges and future directions Further reading Cloud Computing What
    are the tradeoffs that need to be established between the various QoS requirements
    brought on by the large variety of IoT applications operating on cloud systems?
    ACM CSUR [1] Autonomic Computing What additional problems may be addressed by
    an autonomic computing expansion that is based on AI/ML as the number of IoT and
    scientific workloads increases? Elsevier IoT [4] Mobile Cloud Computing How would
    AI-based deep learning algorithms be used to anticipate the resource demands beforehand
    for diverse geographic resources needed for mobile cloud computing, requiring
    new strategies for provisioning and scheduling resources? ACM CSUR [60] Green
    Cloud Computing How can improved methods for effective data encoding for lower
    bandwidth usage and energy-effective transmission in data-intensive IoT devices
    make cloud computing more environmentally friendly? ACM CSUR [162] Fog Computing
    How can AI approaches be utilized to properly schedule tasks when working in locations
    with varying amounts of fog resources? Elsevier JPDC [39] & IEEE COMST [41] Edge
    Computing In what ways edge computing can be utilized to boost power and resource
    utilization, hence enhancing QoS? IEEE COMST [41], [206] Mobile Edge Computing
    How can novel resource provisioning and scheduling policies be developed for mobile
    edge computing that makes use of AI-based deep learning approaches to forecast
    the resource requirements beforehand for resources that are located in different
    locations? IEEE COMST [41], [213] & ACM CSUR [60] Serverless Computing How to
    reduce the cold start time and increase scalability using serverless edge computing?
    IEEE TSC [186] & ACM CSUR [183] Osmotic Computing How can osmotic computing improve
    resource availability or performance at the network edge while moving services
    from the data center to the edge for AI/ML-driven adaptive administration of microservices?
    ACM TOIT [46] Dew Computing How should dew computing allow a highly scalable method
    that can increase or reduce the real-time demands of performing operations at
    runtime via utilizing AI? Elsevier IoT [48] Programming Models How to select a
    programming model that efficiently gathers data when and where it is needed while
    keeping complexity low relative to the total number of processors at hand? Procedia
    Computer Science [115] Virtualization How can unbreakable security for VMs be
    ensured if consumers do not follow recommended practices when it comes to login
    credentials, installations, and other operations? ACM CSUR [122] IoT How to ensure
    that an SLA is upheld while responding to customer requests as quickly as possible
    using IoT applications? IEEE COMST [78] Integrated Computing How may QoS characteristics
    change if communication between layers in a fog-edge/cloud computing paradigm
    is improved? ACM CSUR [108] & Elsevier FGCS [112] Connectivity/ Networking How
    can satisfying the demand or need for network solutions enabling high performance,
    resilience, dependability, scalability, adaptability, and cybersecurity remain
    constant? ACM CSUR [60] & IEEE COMST [61] Container Technologies How can the QoS
    in data processing be enhanced by leveraging containers with virtualization? Springer
    JoS [174] & Wiley CCPE [178] Microservices How to handle errors, ensure data integrity,
    and communicate effectively amongst services in a distributed system using a microservice
    architecture? IEEE TSC [172] Software-defined Networks What are some ways in which
    SDN might help minimize power usage in cloud and edge computing? Wiley ETT [84]
    Distributed Ledger Technology (Blockchain) How can distributed ledger technology
    (Blockchain) be utilized to secure the data for IoT applications? IEEE COMST [108],
    [216] Federated Learning How could companies ensure privacy in federated learning
    services, which differ from learning in data centers in that users’ data is disclosed
    to third parties or the centralized server while exchanging model changes during
    the training stage? Elsevier KBS [222] & CIE [220] Software Engineering How can
    fault tolerance be improved in computing systems dynamically without manually
    writing the software code by utilizing AI to “automatically” diagnose and fix
    an error? Elsevier JSS [129] Distributed Computing Continuum Systems How can Distributed
    Computing Continuum Systems consider all computing tiers as a single system and
    optimize future applications in a decentralized manner? IEEE TKDE [239] Table
    4. Summary of Trends/Observation for modern computing along with future reading.
    Trends/ Observation Open challenges and future directions Further reading AI-driven
    Computing How to optimize the management of resources using the latest AI/ ML
    models in computing systems? Elsevier IoT [4] Large Scale Machine Learning How
    can businesses mitigate the risks associated with the proliferation of sensitive
    information that arise as a result of the proliferation of data produced by AI
    and ML systems? IEEE TKDE [155] Edge AI What strategies should be employed to
    oversee the simulation and information transmission among peripheral devices and
    other systems? What network infrastructures should be utilized to enable this
    communication? Elsevier IoTCPS [92] & ACM SIGCOMM [261] Bitcoin Currency How can
    computing be utilized to maximize the efficiency of computation or processing
    capacity usage in cryptocurrency for cloud mining? Elsevier JNCA [226] Industry
    4.0 How can AI, the cloud, and edge computing be used to do predictive analysis
    that involves company resources? IEEE COMST [275] Intelligent Edge How to deal
    with big problems that come up when designing system-level, algorithm-level, or
    architectural-level developments or innovations for integrated cognitive ability,
    like making decisions in real-time, keeping AI training and inference environmentally
    friendly, and deploying protection? IEEE COMST [88] XAI How can the forecasting
    of resource and power consumption and SLA variances, as well as the implementation
    of promptly proactive action, reduce SLA violations and enhance QoS using XAI?
    ACM CSUR [266] Exascale Computing How to make energy-efficient computing as power-hungry
    as the supercomputers that do calculations and transfer data within the computing
    environment nowadays? ACM CSUR [142] 6G and Beyond What role 6G may play in reducing
    latency and improving reaction times by transmitting data between edge devices
    at high speeds? IEEE COMST [98] Quantum AI What steps should be taken to build
    the AI cloud-based quantum computing infrastructures that are expected to be the
    foundation for our usage of quantum computers and simulators, which will supplement
    our existing classical computing hardware? Wiley SPE [51] Quantum Internet How
    can the benefits of quantum networking be preserved while integrating the quantum
    Internet into currently operating conventional technology that will have to exist
    alongside and communicate effortlessly with today’s Internet services? IEEE COMST
    [254] Analog Computing How is it that analog computers can do complicated computations
    faster and more accurately than their digital equivalents, which utilize ML methods?
    Nature Electronics [146] Neuromorphic Computing How might neuromorphic systems,
    which model the brain’s structure and function and use analog circuits to do AI
    tasks, pave the way for creating incredibly adaptable, self-learning machines?
    Nature Computational Science [149] Biologically-inspired Computing What can researchers
    take away from brain cells concerning ways to minimize the energy needed for computation,
    AI, and ML, given that these cells can easily combine smaller tasks to execute
    larger ones? Elsevier ESA [263] Digital Twins How can network digital twins aid
    in speeding up preliminary installations by preparing navigation, protection,
    digitization, and evaluation in simulation while offering the scalability and
    interoperability of complex networks? IEEE COMST [280] Net Zero Computing How
    can companies mitigate the negative ecological impact of their IT infrastructure
    by constructing environmentally friendly data centers and improving energy effectiveness,
    given that these centers use significant quantities of electricity and release
    enormous quantities of waste heat while also providing powerful computing services?
    IEEE COMST [190] 5. Impact and performance criteria In this section, we discuss
    the impact of contemporary computing and performance criteria. 5.1. Performance
    metrics We are considering QoS, SLA, autoscaling, and fault tolerance as performance
    metrics for computing systems. 5.1.1. QoS and SLA Predicting how a cloud computing
    system will work in real-time is a major difficulty, even if AI techniques are
    used [284]. The efficiency of a computer may be measured using QoS metrics, including
    execution time, cost, scalability, elasticity, latency, and dependability. A SLA,
    a legally binding contract between a cloud service consumer and provider, defines
    QoS standards and potential penalties should they be violated [285]. Today, various
    IoT applications can use blockchain and similar technologies. Each one has its
    own QoS factors that depend on its area, goal, and demand [286]. An SLA may also
    be assessed with a metric called SLA violation rate, which determines compensation
    in the event of an SLA breach by estimating the divergence of the real SLA compared
    to the needed (estimated or predicted) SLA [287]. Since compromized QoS in one
    cloud service may negatively impact the QoS of the entire computing system, QoS
    is becoming increasingly crucial while assembling cloud services. Provisioning
    the proper quantity and quality of cloud resources that will satisfy the QoS of
    an application’s price range, response time, and deadline is essential for providing
    an effective cloud service [288]. Consequently, cloud providers should guarantee
    to offer sufficient resources to minimize or reduce the SLA violation rate, allowing
    users’ workloads to be executed in accordance with their set time and cost constraints
    [289]. In that regard, the diversity of applications and their behaviors on different
    machines requires a tighter description of their needs to minimize SLA violation
    while not over-provisioning infrastructure [290]. QoS-aware resource management
    methods, which can determine and meet the QoS needs of a computing system, such
    as SLO-driven modeling and execution-reordering of web requests, are crucial to
    its success in the future [291]. Several research issues must be overcome before
    QoS can be attained effectively [292]. Initially, the execution time of an application
    is large, and its performance is diminished due to a lack of cloud resources during
    runtime—which can be compounded by transparent processes to the developer, such
    as garbage collection, magnifying the potential of inexplicable SLO violations
    [293]. Additionally, finding the requirement for effective SLA-aware resource
    management methods decreases the SLA violation rate and preserves the overall
    efficiency of the computing system. Finally, to reach the ultimate goal of having
    multiple clouds, there has to be a unified SLA standard across all cloud providers
    [294]. Since many IoT applications rely on cloud computing systems that employ
    AI-based supervised or unsupervised algorithms for learning or models for forecasting,
    it is imperative to determine the appropriate balance amongst various QoS needs.
    5.1.2. Autoscaling Thanks to the dynamic nature of the cloud, self-adapting techniques
    may be used to reduce resource costs without compromizing QoS [295]. Resource
    autoscaling, or strategy, reconfiguration, and provisioning, allows for self-additivity.
    Scientists have looked into autoscaling, or the dynamic modification of computational
    resources like VMs, for several reasons [123]. These include the desire to learn
    more about (a) horizontal changes, or the addition or removal of VMs; (b) vertical
    transformations, or the addition or removal of VM resources; (c) choice-making
    techniques, such as analytical modeling, control theory, and neural networks;
    and (d) utilizing a range of pricing models, such as on-demand. When it comes
    to latency-sensitive QoS requirements, the primary challenge for autoscaling methods
    is figuring out how to make a scaling decision quickly enough. AI prediction is
    the initial step towards making decisions in the quickest way possible [248].
    However, traditional ML may not be up to the task when it comes to IoT applications
    requiring real-time mistake correction due to a lack of autonomous error correction
    [296]. Also, the rise of latency-sensitive IoT apps and microservices that need
    responses in the range of milliseconds has made things worse while container-based
    solutions and burstable efficiency resources should make it possible to deploy
    and provision resources in the cloud quickly. To prevent a potentially disastrous
    situation, a smart car’s onboard computer constantly monitors data such as the
    vehicle’s speed, the location of other drivers and passengers, and the road conditions
    [297]. The cloud alone cannot answer this problem due to the instability and latency
    in connections between the cloud and users; instead, autoscaling techniques for
    IoT applications must take these factors into account [298]. The truth is that
    autoscaling needs to be made bigger because the cloud naturally gets in the way
    of Industry 4.0 ideas, like real-time management, and making decisions without
    a central authority. 5.1.3. Fault tolerance Providers of cloud computing services
    owe it to their customers to make such services available without interruption,
    regardless of what problems arise [299]. To meet the QoS standards of a computing
    system efficiently, fault tolerance approaches are employed. Software, hardware,
    and even networks may all go wrong when a computer system operates. In addition,
    fault resilience guarantees the reliability and accessibility of cloud services
    [4]. Timeout breakdowns, overload issues, and resource-lack failures are further
    examples of cloud dependability issues. A major breakdown has the potential to
    cause a cascade of failures in the system [300]. Several proactive and reactive
    fault tolerance approaches have been developed to cope with these kinds of failures.
    The most common method of handling faults in long-running processes is called
    “checkpointing”, and it involves preserving the current state after each modification
    [301]. Additionally, checkpoints are employed if there is a possibility of not
    beginning at the same position [1]. Replication-based resilience is another well-known
    method; it involves duplicating the nodes or jobs until they are completed. If
    a system is overloaded or malfunctioning, a task migration-based resilience solution
    can move the work to another computer. Computer systems must have autonomous resilience-aware
    resource management technology, reliability of service methods, and reliable information
    integrity (e.g., blockchain) to keep running. Reliability impacts QoS in cloud
    computing while still delivering it effectively. One of the biggest obstacles
    in cloud computing is figuring out how to deliver a secure and effective cloud
    service while cutting down on power consumption and emissions [302]. Cloud computing
    has built-in redundancy to maintain service availability, QoS, and performance
    guarantees. Resource management must consider varying failures and workload prototypes
    for medical care, urban planning, and agricultural applications to run well [71].
    Predicting failure in systems that use cloud computing is difficult and can impact
    the dependability of the system [301]. Predicting faults and achieving the requisite
    dependability of the cloud service while maintaining QoS necessitates several
    machine or deep learning approaches [13]. Replication-based fault tolerance solutions
    are effective for IoT applications because they reduce task delay and response
    time. A dependable cloud storage system that will offer an effective retrieval
    system for processing big data is also required to deal with big data applications
    [303]. 5.2. Efficiency metrics We are considering energy consumption, carbon footprint,
    and serviceability as efficiency metrics for computing systems. 5.2.1. Energy
    consumption Data collection and processing have risen exponentially during the
    last several years. This pattern has been pushing cloud systems to the limits
    of their computational and, by extension, energy consumption capacities [304].
    Annually, CDCs have increased their power use by around 20% to 25% [305]. This
    shift has led to the rise of decentralized computer architectures such as Fog
    and Edge. The latency and cost-effectiveness of cloud computing are all vastly
    improved by moving parts of its computation to distributed edge devices and networks.
    There nevertheless exist difficulties associated with this. Irregular energy supply,
    even without the power supply itself, presents significant issues for numerous
    highly critical and remote sensing applications. The ever-growing number of IoT
    devices and the data they produce have put networking’s ability to handle information,
    compute, and transfer data throughput to the test [162]. Meanwhile, smaller IoT
    devices are currently created with limited computing power, storage spaces, and
    energy. Hence, it is imperative to boost the performance of fog and edge nodes
    in the network. Sustainability in CDCs and minimizing their carbon impact have
    also become more pressing concerns. This must be accomplished without lowering
    the bar for QoS [306]. Notwithstanding the obstacles, there have been several
    advances in this area. Software, hardware, and transitional approaches have all
    been taken to the energy management problem. Approaches and techniques are being
    designed to optimize software efficiency, supported by computational models [306].
    One example is mobile edge computing offloading. Hardware-wise, particularly for
    the application, devices were designed to provide peak performance while minimizing
    energy consumption. Energy efficiency in Wireless Sensor Networks (WSNs) has been
    extensively researched [4]. Fog/edge-node sleep time scheduling, active resource
    management, and additional energy-saving strategies have all been used in the
    intermediate phase. There are still many unanswered questions and potential avenues
    for development when it comes to the effectiveness and longevity of fog, edge,
    and cloud infrastructures. Advanced algorithms for encoding data into fewer bits
    are explored to reduce transmitter power needs, which are crucial due to limited
    transmission bandwidth, more critical than direct CPU power needs. Despite the
    need for specialized hardware, encoding methods may be used by taking advantage
    of the universal encoders present in virtually all mobile devices [13]. Yet, it
    has become impossible to lower the ideal bandwidth due to the rising quantity
    of data exchange and loss. Preparing for CPU and data utilization in a way that
    minimizes heat generation requires modeling at the transistor level, which necessitates
    the development of 3D thermal simulation systems [75]. Lastly, the aim is to minimize
    power consumption to the point that the CPU and transceiver may be powered entirely
    by energy harvesting or scavenging approaches [307]. Consequently, the Fog/Edge
    network’s granularity may be decreased, leading to more widely scattered, overbearing,
    and resilient architectures. In various fields, like energy limits, blockchain
    algorithms might be studied with various versatile AI-based learning approaches
    for enhanced energy scheduling. 5.2.2. Carbon footprint End-user needs for applications
    and the resulting growth in storage in the Exabyte range will result in the first
    Exascale system by 2025, followed by a Zettascale system by 2035 [2]. While this
    is certainly something to be proud of, there are also many difficulties that come
    along with it. Keeping everything running requires massive amounts of energy,
    which poses a major obstacle. At the moment, over ten percent of the world’s power
    is used each year by the ICT sector [190]. The rebound effect, which leads to
    even higher demand and consumption, makes it counterproductive to create ever-larger
    systems by increasing efficiency. The next generation of autonomous system paradigms
    will likely place a greater emphasis on power and carbon footprints in light of
    climate change and the projected 1.5 °C rise in worldwide temperatures owing to
    emissions of carbon dioxide by 2100 [2]. This is not merely about lowering energy
    use per unit of processing, as is the case now, but also about more basic issues
    with systems that assume continuous stable power supplies, connectivity with sources
    of clean energy, and alternate techniques of minimizing energy usage [308]. The
    study and treatment of systems as living ecosystems rather than as collections
    of discrete components is a topic of great interest, and this includes the comprehensive
    integration of managing energy (asynchronous computation, power scaling, wake-on-LAN,
    air conditioning, etc.). 5.2.3. Serviceability/usability The fields of human–computer
    interaction and networked systems have yet to fully merge with each other. This
    closer synchronization would be especially helpful for cloud computing [1]. Despite
    significant work on resource management and the back-end associated concerns,
    accessibility is a vital component in lowering the costs of organizations investigating
    cloud services and infrastructure. Costs associated with labor might decrease
    since customers will receive superior service and increase their output [309].
    NIST’s Cloud Usability model addresses five dimensions of cloud usability: capability,
    personalization, reliability, security, and value, all of which have been highlighted
    as critical issues [310]. The term “capable” refers to the degree to which cloud
    service can fulfill the needs of its customers. With the assistance of personal
    customization options, individuals and businesses will have the capability to
    modify the visual style and adjust or eliminate features from interfaces for various
    services. Trustworthy, robust, and useful are attributes associated with possessing
    a system that fulfills its duties throughout state situations, is safely protected,
    and delivers value to customers accordingly. Current cloud initiatives have mostly
    concentrated on wrapping up sophisticated services into APIs that can be accessed
    by end users [309]. HPC Cloud is the most evident example. To make HPC applications
    more accessible and easier to use, researchers have developed several different
    services. In addition to being packaged as services, these systems provide Web
    interfaces through which their settings may be set and their input and output
    files managed. DevOps is another path associated with cloud usage that has gained
    popularity in recent years [311]. DevOps has increased the efficiency of both
    software engineers and administrators when it comes to developing and delivering
    remedies on the cloud. Cloud computing is important not only for creating brand
    new solutions AIOps and MLOps [312] but also, for streamlining the process of
    moving existing applications from onsite settings to adaptable, multi-tenant cloud
    services. 5.3. Social impact We are considering the digital divide, ethical AI,
    and digital humanism as social impact metrics for computing systems. 5.3.1. Digital
    divide Corporations in rural areas have significant challenges due to the difficulty
    of gaining a connection to broadband connectivity and, by extension, cloud-based
    resources [313]. Access to the web is one example of a long-standing infrastructural
    gap between urban and rural areas. There are a lot of companies that cannot expand
    and innovate because they lack access to new technology. Businesses in rural areas
    face another obstacle: the high cost of maintaining and upgrading on-premises
    IT infrastructure. Cloud computing’s main benefits are the ability to work together
    and think creatively. The cloud encourages teamwork by facilitating real-time,
    distributed collaboration. This greater collaboration encourages invention. As
    a result, rural enterprises may now compete on an equal basis with their metropolitan
    competitors [314]. Accessibility to data and fundamental information is also crucial.
    The benefit of using the cloud has increased significantly with the advent of
    generative AI. Comprehensive sales, marketing, and manufacturing capabilities
    are provided by core AI services, but these cannot be reproduced with human processing
    and can be too costly to install on-site for modest organizations. The proliferation
    of cloud computing has expanded business opportunities, but not equally. By utilizing
    the cloud, companies in rural areas may overcome the constraints of their physical
    location [315]. Cloud computing’s greater availability, decreased cost, scalable
    effectiveness, and improved cooperation may breathe new life into the rural economy
    and propel it towards long-term success. 5.3.2. Ethical AI AI systems require
    vast amounts of data, including details on businesses and their clients [316].
    The value of knowing the data owner surpasses that of having private information
    that cannot be linked to a specific person. When dealing with sensitive information,
    companies regularly face problems related to data security and regulatory compliance
    [317]. Autonomic computing using AI needs to take into account privacy rules and
    data protection. While AI has the potential to be a game-changer, it has not always
    been successful in achieving its aims. A hunt for answers by an AI may result
    in a flood of insensitive comments [318]. The vast number of AI decisions and
    the stakes involved make this field fraught with peril. Prior to expanding the
    use of this invention, it is crucial to develop accountability and ownership.
    5.3.3. Digital humanism The unavoidable consequences of digital colonization driven
    by business need a counter-force of digital humanism motivated by care for humanity
    and the Earth [319]. We have never been both so interdependent, yet so isolated.
    Modern digital systems allow for global communication. One no longer has to be
    in the same room as someone else to have a conversation, collaborate on a project,
    or just have fun with them. The cell phone is rapidly becoming an integral part
    of people’s daily lives all across the world. Connectivity between the developing
    world and the developed nations of the world is rapidly expanding, for both good
    and ill. These interconnections are causing conflicts that could have been prevented
    when individuals and ideas were separated by space. Western materialism and commerce
    meet Eastern spirituality and culture in the virtual world [320]. Therefore, although
    humans may all end up in the cloud at some point, the barriers of mutual respect
    and compassion that keep us from crashing into one another are more than frayed.
    Most modern digital accounting and tracking systems are used by private companies
    seeking to maximize profits at the expense of others, enriching a few elites at
    the expense of a much larger underclass [321]. In contrast, if the cloud could
    be utilized for humanity’s benefit, manufacturing and distribution might be dramatically
    enhanced. Controlled well, such instruments will allow for fine-tuning of many
    crucial societal functions, particularly at the subnational and neighborhood levels.
    5.4. Security and compliance We are considering data protection, privacy regulations,
    and resilience to attacks as security and compliance metrics for computing systems.
    5.4.1. Security, privacy and resiliency In recent years, there has been a dramatic
    change in academia and business towards the IoT, edge computing, and cloud computing
    in order to serve customers better. With this massive paradigm shift, comes a
    slew of problems and difficulties with protecting the confidentiality and safety
    of the information stored on these devices [322]. Edge computing’s many distinguishing
    features – its low latency, geographical dispersion, end-device accessibility,
    high processing power, variability, etc. – make it imperative that security and
    privacy mechanisms be both flexible and powerful [323]. In addition, creating
    universally compatible software platforms is challenging due to the wide variety
    of use cases and device types. Several elements become important in the research
    of these security and associated challenges in the cloud and fog computing models:
    End-user confidence and privacy; verification and validation of sources inside
    nodes; secure communications between sensor, compute, and broker nodes; detection
    and prevention of malicious attacks; secure, reliable and decentralized data storage,
    such as Blockchain [231]. Some of the problems that have already been addressed
    in this field include adaptive mutual authentication, identifying and retrieval
    of harmful or malfunctioning nodes, the detection and defense against assaults,
    the avoidance of harmful hazards, and the protection of user information from
    theft. Unmanned Aerial Vehicle (UAV)-aided computing devices can now maintain
    their anonymity while contributing to distributed frameworks in AI technology,
    such as computer vision and path learning, supporting data processing and decision-making
    [324]. Other efforts in fog forensics have also given digital evidence by recreating
    prior computer activities and identifying how these events contrast with cloud
    forensics in important ways. The past few years have seen significant progress
    in several key areas related to Fog Radio Access Networks (F-RANs), including
    mobility management, interference reduction, and resource optimization [325].
    Novel approaches have evolved for varied applications handling privacy challenges.
    Face recognition and resolution, vehicle crowd sensing, geographic location sensing
    and data processing, renewable node storage systems and data centers, and fog-based
    public cloud computing are promising new research areas. Prevention against data
    theft, attacks involving man-in-the-middle, confidentiality of users, location
    confidentiality, forward privacy, reliable user-level key management, and many
    other weaknesses have all been addressed through such efforts [4]. There are scaling
    issues with many fog/cloud privacy and security models that prevent them from
    fully applying to the next-generation edge computing transition [326]. Because
    of fog computing’s decentralized nature, numerous new security concerns, which
    are not an issue in the cloud, emerge in the fog layer and IoT devices. The deployment
    of authentication systems is hampered by the prevalence of threats such as advanced
    persistent threats (APT attacks), malware, distributed denial of service (DDoS)
    attacks, two-way communication, and micro-servers without hardware protection
    mechanisms in edge data centers [327]. Additionally, these studies show how the
    mobile edge computing architecture might change in the future. For example, edge
    nodes working together could make real-time encryption more efficient. The computational
    capacity of both edge and distant resources has not been completely used in previous
    efforts, and security flaws have been addressed from a restricted viewpoint. New
    phenomena appear when cloud-like capacities are distributed to the network’s periphery
    [231]. Edge data center collaboration, service migration on a local and global
    scale, end-user concurrency, QoS, real-time applications, load distribution, server
    overflow issues, stolen device detection, and dependable node interaction are
    all examples of such scenarios. Future studies can focus on new areas, such as
    evolving game-theoretical strategies to the privacy algorithms encouraged by adversarial
    attack scenarios, communication protocols in sensor cloud systems, and clustering
    model-based security evaluation (AI-based forecasting approaches), which can be
    investigated as potential solutions to these issues [236]. Mobile devices’ presence
    in these data centers should be taken into account by safeguarding systems. 5.5.
    Economic and management We are considering cost-efficiency, resource allocation,
    application design, computing economics, and data management under economics and
    management for computing systems. 5.5.1. Cost-efficiency Minimizing cloud expenditures
    while maximizing application performance and efficacy is the goal of cloud cost
    optimization, which entails striking a fine balance between technological standards
    and corporate goals [304]. Cost-effective cloud computing refers to the practice
    of utilizing cloud providers in the most economical way feasible to operate software,
    complete tasks, and create value for a company. Optimization as a practice varies
    from fundamental business management to challenging scientific and technical fields
    including operational research, statistical and data analysis, and modeling and
    prediction [316]. Corporations may maximize the return on their investments in
    cloud computing through cost optimization, which reduces wasteful expenditures
    and strengthens their operational effectiveness [328]. By avoiding economic hazards,
    aligning spending with company goals, and establishing a secure, scalable, and
    cost-effective cloud infrastructure, corporations can maximize the return on their
    investments in cloud computing. In general, efficient cloud cost management preserves
    essential resources against the risk of unanticipated expenditures and financial
    mismanagement. Changing to a cloud-native methodology involves more than just
    updating technology; it also necessitates a substantial adjustment in mindset
    [1]. Building scalable apps that make efficient use of resources requires developers
    to think in terms of the cloud from the start. To optimize cloud expenditures,
    a cloud-native application design requires an in-depth familiarity with the services
    and resources offered by different cloud service providers. Managed service options
    are superior to autonomous technologies since they require less effort and time
    investment [329]. A sophisticated knowledge of the user application’s demands,
    regulatory demands, and possible financial consequences is necessary to choose
    between a single and multi-cloud installation plan. An organization’s administration
    might be simplified by adopting a single-cloud approach, but doing so could leave
    it vulnerable to vendor lock-in and service restrictions [2]. Contrarily, a multi-cloud
    strategy can increase complexity in administration but has the ability to optimize
    costs, provide greater flexibility, and lessen the danger of vendor lock-in. Identifying
    which is the most economical and profitable implementation approach requires careful
    consideration of the specific features, pricing methods, and competencies of different
    cloud services. 5.5.2. Resource allocation The sheer size of today’s CDCs makes
    resource management in networked systems a formidable challenge. In large-scale
    distributed architectures, the variety of network devices, elements, and ways
    to connect raises the difficulty of resource management strategies [330]. Consequently,
    there is a necessity for innovative resource allocation methodologies that would
    add to the reliability and effectiveness of these systems while keeping them cost-effective
    and sustainable. While resource management is fundamental to distributed systems
    (be it the cloud, the IoT, or fog computing), additional guarantees are needed
    to ensure that these systems operate well in terms of latency, dependability,
    cost-effectiveness, and throughput [331]. The software layer is just one part
    of these larger systems, which also require consideration of networking, server
    architecture, and ventilation. By incorporating blockchain technology into operations
    like resource sharing and VM migration, cloud systems may be more secure [332].
    There is a pressing need to investigate novel approaches to managing computer
    system resources by taking a systemic perspective and using AI models. Moreover,
    experiment-driven strategies for examining methods to optimize resource management
    methods may be investigated [333]. Borg was opened up by Google as Kubernetes,
    which is an instance of a cluster management system that incorporates data abstraction
    into resource management. Users are freed from worrying about the nuts and bolts
    of resource management and may instead focus on composing cloud-native applications.
    Borg conceptually separates the whole cluster into cells, each housing a Borgmaster
    (controller) and a Borglet (which initiates and terminates tasks within the cell’s
    perimeter). The master node coordinates with the Borglets and processes RPCs from
    clients requesting actions like creating jobs or reading data [253]. This centralized
    design is very suitable for scaling. The primary benefit of this architecture
    is that operations that have already been started will continue to execute even
    if the master or a Borglet fails [334]. A system known as Mesos can facilitate
    the equitable distribution of commodity clusters. It coordinates the use of commodity
    clusters by many systems. The fundamental idea is to make use of available resources
    [335]. In this model, Mesos determines how many resources to give to every framework
    depending on the limitations associated with that framework, and the frameworks
    then choose which offers to take. Thus, scheduling choices must be made by frameworks.
    In addition, Mesos facilitates the creation of domain-specific frameworks (like
    Spark) that may greatly enhance performance. To schedule and manage available
    resources, YARN is used as a framework [1]. It enables services to ask for computing
    power at various topological levels, including individual servers, networks, and
    whole racks. The primary component in charge of allocation is YARN’s resource
    management. Similarly to Mesos, it enables several frameworks to collaborate on
    the same commodity clusters [334]. YARN’s integrated reliability masks the complexities
    of failure identification and recovery. • Heterogeneous Resources and Workloads:
    There is a lack of cohesion in the existing literature about managing resources
    and workloads in diverse cloud settings. As a result, there is no common setting
    in which cloud applications can make optimal use of heterogeneity in VMs, vendors,
    and hardware architectures [151]. Consequently, the initiative recommends an overarching
    program that takes into consideration diversity throughout. Effective solutions
    can be picked from a collection of workload and resource handling methods, depending
    on an application’s needs [336]. Heterogeneous memory control is necessary for
    this purpose. Modern memory control techniques rely heavily on hypervisors, thereby
    minimizing the potential advantages of heterogeneity. Recent calls for action
    have advocated for alternatives that focus on heterogeneity awareness in the guest
    OS. Another chasm is that between heterogeneity and abstraction [337]. Accelerator-specific
    languages and low-level programming initiatives are necessary for today’s programming
    paradigms to utilize hardware processors. Furthermore, such models allow for the
    creation of useful research software. As a result, service-oriented and user-driven
    applications on cloud platforms are hampered in their ability to take advantage
    of heterogeneity. Kick-starting an international community initiative to come
    up with an open-source, high-level programming language that is suitable for cutting-edge
    and creative Web-based applications in a heterogeneous setting is a worthwhile
    step to take [338]. Whenever fog computing matures and application migration occurs,
    such aids will be invaluable. 5.5.3. Application design By 2025, analysts predict
    61 billion connected devices will generate 40 percent of global data at the cost
    of $2.5 trillion [339]. Medical services, near-real-time traffic management systems,
    precise farming, intelligent towns and cities, etc., are just a few examples of
    IoT applications that are driving the need for improved processing capacity, data
    storage, confidentiality, security, and trustworthy communication. Additionally,
    as the data produced by these devices is used to resolve real-time challenges,
    credibility, uniformity, and accessibility of the data must be maintained. It
    is challenging to design such complex applications for IoT systems [340]. As a
    result, it is essential to develop application designs and architectures that
    are not only dependable and quick enough to deliver effective efficiency but additionally,
    scalable to manage massive amounts of data through these devices. These are the
    most important factors to consider when developing such apps for cloud environments.
    Firstly, a data packet’s latency is the time it takes to travel between an IoT
    device and the cloud before returning. For time-sensitive information, even a
    millisecond delay might have drastic consequences. For instance, having a crisis-sensing
    instrument that only sounds an alert after a disaster has already taken place
    is not a viable solution. Data needing immediate reaction should be analyzed as
    close as possible to the origin [341]. Secondly, if all this data is transferred
    to the cloud for storage and analysis, the resulting traffic will be massive,
    using up all available bandwidth. The distance between the device and the cloud
    also increases transmission latency, which slows down responses and reduces user
    experience. Therefore, some tasks must be transferred from the cloud to an edge
    server located between the Internet servers and the mobile device: such solutions
    better satisfy end-users’ requirements. By storing and processing certain IoT
    data directly on IoT devices, the fog computing model reduces the load on the
    cloud and keeps costs down. Large-scale, geographically dispersed applications
    that rely heavily on real-time data benefit from the fog’s consistency [342].
    Fog computing may be the most appropriate choice to enable effective IoT and provide
    reliable and safe services and resources to many IoT users. Big data analytics,
    IoT devices, fog, and edge computing have become the foundations for smart city
    programs worldwide [343]. In transport, fog computing is useful for several tasks,
    including vehicle-to-vehicle interaction, smart-sensor-based congestion control
    system management, driverless car management, and self-parking, among others.
    Furthermore, governments may employ these applications to make the lives of their
    residents safer and more environmentally friendly, making them a sustainable approach.
    Emergency services, such as those dealing with fires or natural disasters, can
    also benefit from this technology by receiving timely alerts about developing
    crises to help them make informed choices. Farming software that tracks weather
    and climatic data like rainfall, wind speed, and temperatures, makes it easier
    for farmers to reap a harvest. An IoT agriculture platform is suggested for cloud
    and fog computing, with applications including automated agricultural monitoring,
    visual inspection for pest control, and more efficient use of farm resources [340].
    Meanwhile, in the medical field, more and more people are using fitness trackers,
    blood pressure monitors, and heart rate monitors to track vital signs and gather
    data for medical analysis. Thanks to these innovations, physicians can check their
    patients’ health from afar, and patients have more say in their care and decisions.
    5.5.4. Computing economics There are several promising new avenues for study in
    the financial aspects of cloud computing. It is becoming clearer that the lower
    costs of container deployment can be used to handle real-time workloads [344].
    This is speeding up the switch from VMs to containers for cloud computing. • Cost-Effective
    Computing Models: In serverless computing, no billing for computing resources
    is made until a function is invoked. Processes executed in these lambda functions
    tend to be narrower in focus and designed for processing data streams. Whether
    or not serverless computing is beneficial for a given application depends on its
    projected runtime behavior and workload [1]. Averaged versus peak transaction
    rates; scaling the number of simultaneous operations on the infrastructure (i.e.,
    operating multiplies simultaneous functions with a growing number of consumers);
    and benchmark implementation of serverless functions across various backend hardware
    platforms [345]. Conversely, increased employment of fog and edge computing characteristics
    with cloud-based data centers gives tremendous study potential in cloud economics.
    • Economic Impact of Computing Technologies:It is possible to lower the expenses
    of running cloud services and infrastructure by combining reliable resources of
    the cloud with more ephemeral resources at the consumer’s edge. To make such technology
    accessible at the edge, nevertheless, it is anticipated that consumers will require
    some sort of inducement [157]. Expanding the cloud market to include new types
    of service providers is possible because of the accessibility of cloud and edge
    resources. Researchers call these intermediate facilities located between the
    conventional data center and the user-owned or provisioned resources, microdata
    centers [346]. The federation concept in computing allows for many microdata center
    operators to operate together to distribute workloads in a given region at desired
    pricing. 5.5.5. Data management Metadata handling for datasets is not given much
    attention in cloud IaaS and PaaS services for storing and information administration,
    which instead prioritize file, partially structured, and structured data separately.
    In contrast to traditional, organized data warehouses, proponents of “Data Lakes”
    advocate for businesses to store all their data in unstructured formats on the
    cloud, using services like Hadoop [1]. Nevertheless, using them might be difficult
    due to the absence of information for tracking and defining the origin and authenticity
    of the data. Throughout the past ten years, research archives have become exceptional
    in handling vast, varied datasets and the accompanying information that provides
    context for their usage. Collocating data and computing resources in a small number
    of strategically located data centers worldwide allow for economies of scale,
    a major advantage of CDCs [348]. Nevertheless, bandwidth restrictions across worldwide
    networks and delays in gaining access to data present obstacles [350]. This becomes
    an increasingly pressing issue as IoT and 5G mobile networks expand. However,
    the cloud providers’ access to private data and critical confidential information
    still poses a risk for businesses that need to guarantee strict privacy for their
    end-users. Likewise, there are no foolproof auditing techniques to prove that
    the cloud service provider has not obtained the data, even though regulatory measures
    are in place. In a hybrid setup, customers may handle confidential information
    under their watchful eye while still taking advantage of the advantages of public
    clouds, thanks to the proximity of private data centers to public CDCs connected
    by an independent high-bandwidth network. Furthermore, effective approaches to
    managing resource flexibility in such contexts should be explored [351]. In addition,
    it is preferable to have high-level programming abstractions and bindings to platforms
    that can allocate and oversee resources in these massively dispersed settings.
    Table 5. Summary of open challenges and future directions in the above-discussed
    impact of modern computing and performance criteria with future reading. Impact
    and performance criteria Open challenges and future directions Further reading
    QoS and SLA How can SLAs and QoS be preserved in real-time when cloud computing
    and edge resources and tasks are executed? ACM CSUR [287] and Wiley IJCS [289]
    Autoscaling How can it be ensured that computing resources need to meet SLAs and
    QoS are effectively autoscaled in real-time? ACM CSUR [295] Fault Tolerance How
    can reliable support be continuously provided with environmentally-friendly services?
    Elsevier SETA [300] Energy Consumption How can modern computing benefit from AI/ML
    to provide environmentally-friendly services and low energy consumption? Springer
    Cluster Computing [304] Carbon Footprint What technological advancements may decrease
    the impact of climate change and how could environmentally-friendly computing
    have a lower-carbon footprint? IEEE COMST [190] Serviceability What methodologies
    should be employed to develop and measure key performance indicators, also known
    as KPIs, in order to assess the success of initiatives that aim to make cloud
    computing more usable and secure? Wiley ETT [309] Digital Divide How does the
    use of the cloud help overcome the digital divide? Can ICTs help bridge the digital
    divide in infrastructural growth? Elsevier Telematics and Informatics [313] Ethical
    AI When designing and implementing AI in computing devices, what ethical concerns
    must be taken into account? Nature Machine Intelligence [347] Digital Humanism
    How may digital tools stimulate original thought and the independent thinking
    of individuals, and whether or not the synergy of these traits can promote a digital
    shift in the workplace? Elsevier Journal of Business Research [319] Security,
    Privacy & Resiliency What measures can be taken to ensure that personal information
    is protected and data is securely processed in the cloud when IoT apps collect
    and analyze massive amounts of data? IEEE COMST [323] Cost-Efficiency How can
    impending difficulties like the prohibitive cost of setting up and running big
    systems testing environments and the influence of global warming on the architecture
    of upcoming systems be overcome? Springer Cluster Computing [304] Resource Allocation
    What are the best practices for successfully provisioning cloud and edge resources
    for many IoT apps before scheduling such resources? ACM CSUR [333] Heterogeneous
    Workloads/ Resources How can the heterogeneity of resources and workloads impact
    the efficiency of a computing system at runtime? ACM CSUR [151] Application Design
    How can more efficient IoT apps be developed to make greater use of available
    computer power? ACM CSUR [201] Computing Economics How can businesses strengthen
    their CapEx (Capital Expenditure) and OpEx (Operational Expenditure) strategies
    by learning about the primary economic advantages of cloud computing in terms
    of return on investment (ROI), total cost of ownership (TCO), and relocation?
    Elsevier Telecommunications Policy [344] Data Management How can organizations
    make optimal use of AI/ML approaches for enormous amounts of data to ensure efficient
    data administration and analysis? Springer JBD [348] & ACM CSUR [349] Download
    : Download high-res image (706KB) Download : Download full-size image Fig. 2.
    Hype cycle for modern computing. Finally, with the IoT, deep learning, and blockchain
    all set to be housed on clouds, it is important to look at specialist data management
    services to ensure their success [352]. As indicated above, IoT will include a
    strengthened requirement to deal with streaming data, their effective storage,
    and a requirement to integrate data management on the edge effortlessly with administration
    in the cloud [38]. When unregulated edge devices are involved, integrity and authenticity
    become even more crucial. As the use of deep learning grows, it will become more
    important to be able to manage trained models well and make sure they can be quickly
    loaded and switched between to make online and distributed analytics applications
    possible [349]. Finally, blockchain and decentralized ledgers can improve data
    management and tracking by providing greater transparency and auditability. While
    initially used by the financial sector (of which cryptocurrencies are only one
    prominent example), these systems may be expanded to store other company data
    safely with an inherent auditing record. Summary: Table 5 lists the summary of
    open challenges and future directions in the above-discussed impact of modern
    computing and performance criteria, along with recommendations for future reading.
    6. Emerging trends in modern computing The advent of modern computing technology
    has made it possible to resolve several real-world issues, including delayed responses
    and low latency. It has facilitated the development of start-ups led by promising
    young minds from all over the world, providing access to massive computing capacity
    for tackling difficult issues and accelerating scientific advancement. Thanks
    to its ground-breaking improvements in efficiency in domains like neural networks,
    Natural Language Processing (NLP), and related applications, AI has been gaining
    popularity lately. Computing is a vital infrastructure for running AI services
    due to its enormous processing power, and AI has the potential to improve existing
    computing by making resource management effective. Several AI models rely on outside
    data sets and large-scale computer capacity, both of which might be easier to
    access with today’s computing systems. Currently, training advanced models of
    AI in large numbers is becoming even more crucial. Additionally, extensive application
    of AI in contemporary computer systems may be possible due to ground-breaking
    XAI research. In the decades to come, AI will place substantial stress on computing
    resources. To meet these demands, it is necessary to develop new approaches to
    research and methodology that make use of AI models to solve problems with adaptability,
    delay, and handling of resources and cybersecurity. Scalability and adaptability
    are two open issues that have not yet made full use of AI models as an economical
    way to boost the performance of computer applications. Our analysis has led us
    to categorize certain areas of computing into three separate maturity levels:
    a period of five to ten years, over a decade, and under five years. Several novel
    innovations are on the horizon that might significantly improve the utilization
    of modern computing, and the article has highlighted them all over the coming
    decade. Fig. 2 depicts the hype cycle for modern computing systems along with
    their new trends. Researchers extensively study computing paradigms and technologies,
    with edge AI and federated learning now dominating. New areas of study within
    computing, such as distributed computing continuum and AI-driven computing are
    just scratching the surface. Applications for computing in these domains may not
    mature for another five to 10 years. Quantum ML, sustainability, Net Zero Computing,
    XAI, and the quantum Internet are all expected to be in the spotlight for at least
    another decade. Digital twins, cybersecurity, edge intelligence, edge computing,
    and blockchain technology have generated an unprecedented level of excitement.
    They are expected to be completely built-in under five years with the help of
    modern technology. Machine Economics, In-Memory Computing, Bitcoin Currency and
    AIOps/MLOps have all reached their peak of inflated expectations for the following
    five to 10 years of noteworthy evolution. Significant progress needs to be made
    before biologically inspired computing, neuro-symbolic AI, analog computing, neuromorphic
    computing, 6G, and quantum computing can be considered hype-worthy. Cloud and
    fog computing has been trending heavily over the past few years, and that trend
    could persist for the next five to ten years. Table 6. List of acronyms. Abbreviation
    Description PCs Personal Computers DNS Domain Name System MPP Massive Parallel
    Processing AI Artificial Intelligence SMP Symmetric Multi Processing OS Operating
    System GUI Graphical User Interfaces IoT Internet of Things HTTP Hyper Text Transport
    Protocol HTML Hyper Text Markup Language WWW World Wide Web RPC Remote Procedure
    Calls JSON JavaScript Object Notation XML Extensible Markup Language SOA Service-Oriented
    Architecture CDC Cloud Data Centers HPC High Performance Computing IT Information
    Technology SaaS Software as a Service PaaS Platform as a Service IaaS Infrastructure
    as a Service SBC Single-board Computers SDN Software-Defined Networking NVF Network
    Function Virtualization IIoT Industrial Internet of Things QoS Quality of Service
    IoE Internet of Energy B5G Beyond 5G SLA Service-Level Agreement FPGA Field-Programmable
    Gate Arrays ASICs Application-Specific Integrated Circuits GPU Graphics Processing
    Units CUDA Compute Unified Device Architecture TPU Tensor Processing Units ICT
    Information and Communication Technology CaaS Container as a Service QoE Quality
    of Experience V2X Vehicle-to-Everything MEC Multi-access Edge Computing VM Virtual
    Machines M2M Machine-to-Machine PoW Proof of Work XAI Explainable Artificial Intelligence
    UAV Unmanned Aerial Vehicle DDoS Distributed Denial of Service STCO Systems-Technology
    Co-Optimization SoC System-on-a-Chip ML Machine Learning SLO Service Level Objective
    7. Summary and conclusions This research offers a comprehensive exploration of
    the evolution of modern computing systems over the past sixty years, tracking
    the transition from classical computers to quantum computing and examining their
    key components, such as physical architecture, conceptual units, and communication
    methods. We analyze the influence of conceptualization and physical models on
    the shift from centralized to decentralized structures, a significant change since
    the Internet’s inception. Developments in microcontroller architecture, operating
    system design, and networking infrastructure have given rise to ubiquitous computing
    models like the Internet of Things (IoT), pushing the boundaries of both physical
    and conceptual realms. The move towards specialized hardware and software, particularly
    in data-driven fields like AI, represents a shift from earlier focuses on system
    flexibility and adaptability. This article also addresses issues of accessibility
    and potential inequalities, emphasizing the need to ensure these technologies
    positively impact society and everyday life. Integrating recent advancements with
    ongoing challenges in the application of established technological trends, this
    work provides an in-depth analysis of the next wave of scientific research in
    computing. It summarizes current findings, acknowledges limitations, and outlines
    new trends and key challenges, considering the impact of emerging trends and envisioning
    future research paths in modern computing. This review aims to be a valuable resource
    for experts, technologists, and academics interested in the latest developments
    and future directions in the field of modern computing. CRediT authorship contribution
    statement Sukhpal Singh Gill: Writing – original draft, Validation, Methodology,
    Investigation, Formal analysis, Data curation, Conceptualization, Visualization,
    Writing – review & editing. Huaming Wu: Writing – review & editing, Writing –
    original draft, Conceptualization, Data curation, Formal analysis, Investigation,
    Methodology. Panos Patros: Writing – review & editing, Writing – original draft,
    Data curation, Conceptualization, Investigation, Methodology. Carlo Ottaviani:
    Writing – review & editing, Writing – original draft, Formal analysis, Conceptualization,
    Investigation. Priyansh Arora: Writing – original draft, Formal analysis, Conceptualization,
    Investigation, Methodology, Writing – review & editing. Victor Casamayor Pujol:
    Writing – original draft, Data curation, Conceptualization, Investigation, Methodology,
    Writing – review & editing. David Haunschild: Conceptualization, Formal analysis,
    Investigation, Methodology, Writing – original draft, Writing – review & editing.
    Ajith Kumar Parlikad: Writing – review & editing, Writing – original draft, Conceptualization,
    Investigation, Methodology. Oktay Cetinkaya: Writing – review & editing, Writing
    – original draft, Conceptualization, Investigation, Methodology. Hanan Lutfiyya:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology. Vlado Stankovski: Writing – review & editing, Writing – original
    draft, Conceptualization, Investigation, Methodology. Ruidong Li: Writing – review
    & editing, Writing – original draft, Conceptualization, Investigation, Methodology.
    Yuemin Ding: Writing – review & editing, Writing – original draft, Conceptualization,
    Investigation, Methodology. Junaid Qadir: Writing – review & editing, Writing
    – original draft, Conceptualization, Data curation, Formal analysis, Investigation,
    Methodology, Visualization. Ajith Abraham: Writing – review & editing, Writing
    – original draft, Conceptualization, Investigation, Methodology. Soumya K. Ghosh:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology. Houbing Herbert Song: Writing – review & editing, Writing – original
    draft, Methodology, Conceptualization, Investigation. Rizos Sakellariou: Writing
    – review & editing, Writing – original draft, Formal analysis, Conceptualization,
    Investigation, Methodology, Supervision. Omer Rana: Writing – review & editing,
    Writing – original draft, Conceptualization, Investigation, Methodology, Supervision.
    Joel J.P.C. Rodrigues: Writing – review & editing, Writing – original draft, Conceptualization,
    Investigation, Methodology. Salil S. Kanhere: Writing – review & editing, Writing
    – original draft, Conceptualization, Investigation, Methodology. Schahram Dustdar:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology, Supervision. Steve Uhlig: Writing – review & editing, Writing – original
    draft, Conceptualization, Investigation, Methodology, Supervision. Kotagiri Ramamohanarao:
    Writing – review & editing, Writing – original draft, Conceptualization, Investigation,
    Methodology, Supervision. Rajkumar Buyya: Writing – review & editing, Writing
    – original draft, Conceptualization, Formal analysis, Investigation, Methodology,
    Supervision, Visualization. Declaration of competing interest The authors declare
    that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgments
    We thank the Editor-in-Chief (Prof. Ke Xue) and anonymous reviewers for their
    insightful comments and recommendations to improve the overall quality and organization
    of the article. We would also like to express our gratitude to Neil Butler (CEO,
    CloudScaler, UK), Marco AS Netto (Microsoft Azure HPC, USA) and Manmeet Singh
    (University of Texas at Austin, USA) for their thoughtful remarks and valuable
    suggestions. Appendix. List of acronyms Table 6 shows the list of acronyms. Data
    availability No data was used for the research described in the article. References
    [1] Buyya R., et al. A manifesto for future generation cloud computing: Research
    directions for the next decade ACM Comput. Surv., 51 (5) (2018), pp. 1-38 Google
    Scholar [2] Lindsay D., et al. The evolution of distributed computing systems:
    from fundamental to new frontiers Computing, 103 (8) (2021), pp. 1859-1878 CrossRefView
    in ScopusGoogle Scholar [3] Yamashita R. History of personal computers in Japan
    Int. J. Parallel Emergent Distrib. Syst., 35 (2) (2020), pp. 143-169 CrossRefView
    in ScopusGoogle Scholar [4] Gill S.S., et al. AI for next generation computing:
    Emerging trends and future directions Int. Things, 19 (2022), Article 100514 View
    PDFView articleView in ScopusGoogle Scholar [5] Gubbi J., et al. Internet of Things
    (IoT): A vision, architectural elements, and future directions Future Gener. Comput.
    Syst., 29 (7) (2013), pp. 1645-1660 View PDFView articleView in ScopusGoogle Scholar
    [6] Muralidhar R., et al. Energy efficient computing systems: Architectures, abstractions
    and modeling to techniques and standards ACM Comput. Surv., 54 (11s) (2022), pp.
    1-37 CrossRefGoogle Scholar [7] Chakraborty A., et al. Journey from cloud of things
    to fog of things: Survey, new trends, and research directions Softw. - Pract.
    Exp., 53 (2) (2023), pp. 496-551 CrossRefView in ScopusGoogle Scholar [8] Beloglazov
    A., et al. Energy-aware resource allocation heuristics for efficient management
    of data centers for cloud computing Future Gener. Comput. Syst., 28 (5) (2012),
    pp. 755-768 View PDFView articleView in ScopusGoogle Scholar [9] Casamayor Pujol
    V., et al. Fundamental research challenges for distributed computing continuum
    systems Information, 14 (3) (2023), p. 198 CrossRefView in ScopusGoogle Scholar
    [10] Shalf J. The future of computing beyond Moore’s law Phil. Trans. R. Soc.
    A, 378 (2166) (2020), Article 20190061 CrossRefView in ScopusGoogle Scholar [11]
    Angel N.A., et al. Recent advances in evolving computing paradigms: Cloud, edge,
    and fog technologies Sensors, 22 (1) (2021), p. 196 CrossRefGoogle Scholar [12]
    Rimal B.P., et al. A taxonomy and survey of cloud computing systems 2009 Fifth
    International Joint Conference on INC, IMS and IDC, IEEE (2009), pp. 44-51 CrossRefView
    in ScopusGoogle Scholar [13] Gill S.S., et al. Transformative effects of IoT,
    blockchain and artificial intelligence on cloud computing: Evolution, vision,
    trends and open challenges Int. Things, 8 (2019), Article 100118 View PDFView
    articleView in ScopusGoogle Scholar [14] Flynn M.J. Very high-speed computing
    systems Proc. IEEE, 54 (12) (1966), pp. 1901-1909 View in ScopusGoogle Scholar
    [15] Kozyrakis C.E., et al. A new direction for computer architecture research
    Computer, 31 (11) (1998), pp. 24-32 View in ScopusGoogle Scholar [16] Casavant
    T.L., et al. A taxonomy of scheduling in general-purpose distributed computing
    systems IEEE Trans. Softw. Eng., 14 (2) (1988), pp. 141-154 View in ScopusGoogle
    Scholar [17] Yu J., et al. A taxonomy of workflow management systems for grid
    computing J. Grid Comput., 3 (2005), pp. 171-200 CrossRefView in ScopusGoogle
    Scholar [18] Owens J.D., et al. GPU computing Proc. IEEE, 96 (5) (2008), pp. 879-899
    View in ScopusGoogle Scholar [19] Compton K., et al. Reconfigurable computing:
    a survey of systems and software ACM Comput. Surv. (csuR), 34 (2) (2002), pp.
    171-210 View in ScopusGoogle Scholar [20] Wright S. Cybersquatting at the intersection
    of internet domain names and trademark law IEEE Commun. Surv. Tutor., 14 (1) (2010),
    pp. 193-205 CrossRefGoogle Scholar [21] Jansen B.J. The graphical user interface
    ACM SIGCHI Bull., 30 (2) (1998), pp. 22-26 CrossRefGoogle Scholar [22] Tay B.H.,
    et al. A survey of remote procedure calls Oper. Syst. Rev., 24 (3) (1990), pp.
    68-79 View in ScopusGoogle Scholar [23] Suryono R.R., et al. Peer to peer (P2P)
    lending problems and potential solutions: A systematic literature review Procedia
    Comput. Sci., 161 (2019), pp. 204-214 View PDFView articleView in ScopusGoogle
    Scholar [24] Schollmeier R., et al. Protocol for peer-to-peer networking in mobile
    environments Proceedings. 12th International Conference on Computer Communications
    and Networks (IEEE Cat. No. 03EX712), IEEE (2003), pp. 121-127 View in ScopusGoogle
    Scholar [25] Alonso G., et al. Web Services Springer (2004) Google Scholar [26]
    Perrey R., et al. Service-oriented architecture 2003 Symposium on Applications
    and the Internet Workshops, 2003. Proceedings, IEEE (2003), pp. 116-119 View in
    ScopusGoogle Scholar [27] Maffione V., et al. A software development kit to exploit
    RINA programmability 2016 IEEE International Conference on Communications (ICC),
    IEEE (2016), pp. 1-7 CrossRefGoogle Scholar [28] L. Resende, Handling heterogeneous
    data sources in a SOA environment with service data objects (SDO), in: Proceedings
    of the 2007 ACM SIGMOD International Conference on Management of Data, 2007, pp.
    895–897. Google Scholar [29] Mergen M.F., et al. Virtualization for high-performance
    computing Oper. Syst. Rev., 40 (2) (2006), pp. 8-11 CrossRefView in ScopusGoogle
    Scholar [30] Kephart J.O., et al. The vision of autonomic computing Computer,
    36 (1) (2003), pp. 41-50 View in ScopusGoogle Scholar [31] Singh S., et al. STAR:
    SLA-aware autonomic management of cloud resources IEEE Trans. Cloud Comput., 8
    (4) (2017), pp. 1040-1053 View in ScopusGoogle Scholar [32] Othman M., et al.
    A survey of mobile cloud computing application models IEEE Commun. Surv. Tutorials,
    16 (1) (2013), pp. 393-413 Google Scholar [33] AlAhmad A.S., et al. Mobile cloud
    computing models security issues: A systematic review J. Netw. Comput. Appl.,
    190 (2021), Article 103152 View PDFView articleView in ScopusGoogle Scholar [34]
    Anwar M.H., et al. Recommender system for optimal distributed deep learning in
    cloud datacenters Wirel. Pers. Commun. (2022), pp. 1-25 Google Scholar [35] Durao
    F., et al. A systematic review on cloud computing J. Supercomput., 68 (2014),
    pp. 1321-1346 CrossRefView in ScopusGoogle Scholar [36] Gill S.S., et al. ROUTER:
    Fog enabled cloud based intelligent resource management approach for smart home
    IoT devices J. Syst. Softw., 154 (2019), pp. 125-138 View PDFView articleView
    in ScopusGoogle Scholar [37] Iftikhar S., et al. AI-based fog and edge computing:
    A systematic review, taxonomy and future directions Int. Things (2022), Article
    100674 Google Scholar [38] Gill S.S., et al. Fog-based smart healthcare as a big
    data and cloud service for heart patients using IoT International Conference on
    Intelligent Data Communication Technologies and Internet of Things (ICICI) 2018,
    Springer (2019), pp. 1376-1383 CrossRefView in ScopusGoogle Scholar [39] Singh
    J., et al. Fog computing: A taxonomy, systematic review, current trends and research
    challenges J. Parallel Distrib. Comput., 157 (2021), pp. 56-85 View PDFView articleView
    in ScopusGoogle Scholar [40] Shi W., et al. Edge computing: Vision and challenges
    IEEE Int. Things J., 3 (5) (2016), pp. 637-646 View in ScopusGoogle Scholar [41]
    Walia G.K., et al. AI-empowered fog/edge resource management for IoT applications:
    A comprehensive review, research challenges and future perspectives IEEE Commun.
    Surv. Tutor., 26 (1) (2023), pp. 1-56 CrossRefGoogle Scholar [42] Khan W.Z., et
    al. Edge computing: A survey Future Gener. Comput. Syst., 97 (2019), pp. 219-235
    View PDFView articleView in ScopusGoogle Scholar [43] Jonas E., et al. Cloud programming
    simplified: A berkeley view on serverless computing (2019) arXiv preprint arXiv:1902.03383
    Google Scholar [44] Hassan H.B., et al. Survey on serverless computing J. Cloud
    Comput., 10 (1) (2021), pp. 1-29 CrossRefGoogle Scholar [45] A. Buzachis, et al.,
    Modeling and emulation of an osmotic computing ecosystem using osmotictoolkit,
    in: Proceedings of the 2021 Australasian Computer Science Week Multiconference,
    2021, pp. 1–9. Google Scholar [46] Neha B., et al. A systematic review on osmotic
    computing ACM Trans. Int. Things, 3 (2) (2022), pp. 1-30 CrossRefGoogle Scholar
    [47] Ray P.P. An introduction to dew computing: definition, concept and implications
    IEEE Access, 6 (2017), pp. 723-737 View in ScopusGoogle Scholar [48] Gushev M.
    Dew computing architecture for cyber-physical systems and IoT Int. Things, 11
    (2020), Article 100186 View PDFView articleView in ScopusGoogle Scholar [49] Qu
    Y., et al. A blockchained federated learning framework for cognitive computing
    in industry 4.0 networks IEEE Trans. Ind. Inform., 17 (4) (2020), pp. 2964-2973
    Google Scholar [50] Kovachy T., et al. Quantum superposition at the half-metre
    scale Nature, 528 (7583) (2015), pp. 530-533 CrossRefView in ScopusGoogle Scholar
    [51] Gill S.S., et al. Quantum computing: A taxonomy, systematic review and future
    directions Softw. - Pract. Exp., 52 (1) (2022), pp. 66-114 CrossRefView in ScopusGoogle
    Scholar [52] Gulliver S.R., et al. Pervasive and standalone computing: the perceptual
    effects of variable multimedia quality Int. J. Hum.-Comput. Stud., 60 (5–6) (2004),
    pp. 640-665 View PDFView articleView in ScopusGoogle Scholar [53] Ravi S., et
    al. Security in embedded systems: Design challenges ACM Trans. Embed. Comput.
    Syst. (TECS), 3 (3) (2004), pp. 461-491 CrossRefView in ScopusGoogle Scholar [54]
    De Micco L., et al. A literature review on embedded systems IEEE Latin Am. Trans.,
    18 (02) (2019), pp. 188-205 CrossRefView in ScopusGoogle Scholar [55] Basford
    P.J., et al. Performance analysis of single board computer clusters Future Gener.
    Comput. Syst., 102 (2020), pp. 278-291 View PDFView articleView in ScopusGoogle
    Scholar [56] Pajankar A. Raspberry pi supercomputing and scientific programming
    Ashwin Pajankar (2017) Google Scholar [57] Hwu T., et al. A self-driving robot
    using deep convolutional neural networks on neuromorphic hardware 2017 International
    Joint Conference on Neural Networks (IJCNN), IEEE (2017), pp. 635-641 View in
    ScopusGoogle Scholar [58] Süzen A.A., et al. Benchmark analysis of jetson tx2,
    jetson nano and raspberry pi using deep-cnn 2020 International Congress on Human-Computer
    Interaction, Optimization and Robotic Applications (HORA), IEEE (2020), pp. 1-5
    CrossRefGoogle Scholar [59] Kumar A., et al. Securing the future internet of things
    with post-quantum cryptography Secur. Priv., 5 (2) (2022), Article e200 Google
    Scholar [60] Ren J., et al. A survey on end-edge-cloud orchestrated network computing
    paradigms: Transparent computing, mobile edge computing, fog computing, and cloudlet
    ACM Comput. Surv., 52 (6) (2019), pp. 1-36 Google Scholar [61] Wang C., et al.
    Integration of networking, caching, and computing in wireless systems: A survey,
    some research issues, and challenges IEEE Commun. Surv. Tutor., 20 (1) (2017),
    pp. 7-38 View in ScopusGoogle Scholar [62] Ahmadabadi J.Z., et al. Star-quake:
    A new operator in multi-objective gravitational search algorithm for task scheduling
    in IoT based cloud-fog computing system IEEE Trans. Consum. Electron. (2023) Google
    Scholar [63] Asghari A., et al. Server placement in mobile cloud computing: a
    comprehensive survey for edge computing, fog computing and cloudlet Computer Science
    Review, 51 (2024), p. 100616 View PDFView articleView in ScopusGoogle Scholar
    [64] Bari M.F., et al. On orchestrating virtual network functions 2015 11th International
    Conference on Network and Service Management (CNSM), IEEE (2015), pp. 50-56 CrossRefView
    in ScopusGoogle Scholar [65] Cai Y., et al. Compute-and data-intensive networks:
    The key to the metaverse 2022 1st International Conference on 6G Networking (6GNet),
    IEEE (2022), pp. 1-8 View PDFView articleView in ScopusGoogle Scholar [66] Al-Masri
    E., et al. Energy-efficient cooperative resource allocation and task scheduling
    for Internet of Things environments Int. Things, 23 (2023), Article 100832 View
    PDFView articleView in ScopusGoogle Scholar [67] Sriraghavendra M., et al. DoSP:
    A deadline-aware dynamic service placement algorithm for workflow-oriented IoT
    applications in fog-cloud computing environments Energy Conservation Solutions
    for Fog-Edge Computing Paradigms, Springer (2022), pp. 21-47 CrossRefView in ScopusGoogle
    Scholar [68] Verma P., et al. FCMCPS-COVID: AI propelled fog–cloud inspired scalable
    medical cyber-physical system, specific to coronavirus disease Int. Things, 23
    (2023), Article 100828 View PDFView articleView in ScopusGoogle Scholar [69] Desai
    F., et al. HealthCloud: A system for monitoring health status of heart patients
    using machine learning and cloud computing Int. Things, 17 (2022), Article 100485
    View PDFView articleView in ScopusGoogle Scholar [70] Iftikhar S., et al. FogDLearner:
    A deep learning-based cardiac health diagnosis framework using fog computing Proceedings
    of the 2022 Australasian Computer Science Week, ACM (2022), pp. 136-144 CrossRefView
    in ScopusGoogle Scholar [71] Gill S.S., et al. IoT based agriculture as a cloud
    and big data service: the beginning of digital India J. Organ. End User Comput.
    (JOEUC), 29 (4) (2017), pp. 1-23 View in ScopusGoogle Scholar [72] Sengupta A.,
    et al. Mobile edge computing based internet of agricultural things: a systematic
    review and future directions Mob. Edge Comput. (2021), pp. 415-441 CrossRefView
    in ScopusGoogle Scholar [73] Iftikhar S., et al. Fog computing based router-distributor
    application for sustainable smart home 2022 IEEE 95th Vehicular Technology Conference:(VTC2022-Spring),
    IEEE (2022), pp. 1-5 Google Scholar [74] Bansal K., et al. DeepBus: Machine learning
    based real time pothole detection system for smart transportation using IoT Int.
    Technol. Lett., 3 (3) (2020), Article e156 View in ScopusGoogle Scholar [75] Tuli
    S., et al. IThermoFog: IoT-fog based automatic thermal profile creation for cloud
    data centers using artificial intelligence techniques Int. Technol. Lett., 3 (5)
    (2020), Article e198 View in ScopusGoogle Scholar [76] Singh M., et al. Quantum
    artificial intelligence for the science of climate change Artificial Intelligence,
    Machine Learning and Blockchain in Quantum Satellite, Drone and Network, CRC Press
    (2022), pp. 199-207 Google Scholar [77] Singh M., et al. Quantifying COVID-19
    enforced global changes in atmospheric pollutants using cloud computing based
    remote sensing Remote Sens. Appl.: Soc. Environ., 22 (2021), Article 100489 View
    PDFView articleView in ScopusGoogle Scholar [78] Stoyanova M., et al. A survey
    on the internet of things (IoT) forensics: challenges, approaches, and open issues
    IEEE Commun. Surv. Tutor., 22 (2) (2020), pp. 1191-1221 CrossRefView in ScopusGoogle
    Scholar [79] Mansouri N., et al. Cloud computing simulators: A comprehensive review
    Simul. Model. Pract. Theory, 104 (2020), Article 102144 View PDFView articleView
    in ScopusGoogle Scholar [80] Tuli S., et al. HealthFog: An ensemble deep learning
    based smart healthcare system for automatic diagnosis of heart diseases in integrated
    IoT and fog computing environments Future Gener. Comput. Syst., 104 (2020), pp.
    187-200 View PDFView articleView in ScopusGoogle Scholar [81] Gill S.S., et al.
    ChatGPT: Vision and challenges Int. Things Cyb.-Phys. Syst., 3 (2023), pp. 262-271
    View PDFView articleView in ScopusGoogle Scholar [82] Vila M., et al. Edge-to-cloud
    sensing and actuation semantics in the industrial Internet of Things Pervasive
    Mob. Comput., 87 (2022), Article 101699 View PDFView articleView in ScopusGoogle
    Scholar [83] Kreutz D., et al. Software-defined networking: A comprehensive survey
    Proc. IEEE, 103 (1) (2014), pp. 14-76 Google Scholar [84] Mekki T., et al. Software-defined
    networking in vehicular networks: A survey Trans. Emerg. Telecommun. Technol.,
    33 (10) (2022), Article e4265 View in ScopusGoogle Scholar [85] Son J., et al.
    A taxonomy of software-defined networking (SDN)-enabled cloud computing ACM Comput.
    Surv. (csuR), 51 (3) (2018), pp. 1-36 CrossRefGoogle Scholar [86] L. Poutievski,
    et al., Jupiter evolving: transforming google’s datacenter network via optical
    circuit switches and software-defined networking, in: Proceedings of the ACM SIGCOMM
    2022 Conference, 2022, pp. 66–85. Google Scholar [87] Kumar A., et al. A secure
    drone-to-drone communication and software defined drone network-enabled traffic
    monitoring system Simul. Model. Pract. Theory, 120 (2022), Article 102621 View
    PDFView articleView in ScopusGoogle Scholar [88] Wang X., et al. Convergence of
    edge computing and deep learning: A comprehensive survey IEEE Commun. Surv. Tutor.,
    22 (2) (2020), pp. 869-904 CrossRefView in ScopusGoogle Scholar [89] Zhang J.,
    et al. Mobile edge intelligence and computing for the internet of vehicles Proc.
    IEEE, 108 (2) (2019), pp. 246-261 CrossRefGoogle Scholar [90] Chen S., et al.
    Internet of things based smart grids supported by intelligent edge computing IEEE
    Access, 7 (2019), pp. 74089-74102 CrossRefView in ScopusGoogle Scholar [91] Pujol
    V.C., et al. Edge intelligence—Research opportunities for distributed computing
    continuum systems IEEE Internet Comput., 27 (4) (2023), pp. 53-74 CrossRefView
    in ScopusGoogle Scholar [92] Singh R., et al. Edge AI: a survey Int. Things Cyb.-Phys.
    Syst., 3 (2023), pp. 71-92 View PDFView articleView in ScopusGoogle Scholar [93]
    Jia Y., et al. Flowguard: An intelligent edge defense mechanism against IoT DDoS
    attacks IEEE Internet Things J., 7 (10) (2020), pp. 9552-9562 CrossRefView in
    ScopusGoogle Scholar [94] Yang B., et al. Edge intelligence for autonomous driving
    in 6G wireless system: Design challenges and solutions IEEE Wirel. Commun., 28
    (2) (2021), pp. 40-47 CrossRefView in ScopusGoogle Scholar [95] Liu F., et al.
    Integrated sensing and communications: Toward dual-functional wireless networks
    for 6G and beyond IEEE J. Selected Areas Commun., 40 (6) (2022), pp. 1728-1767
    CrossRefView in ScopusGoogle Scholar [96] Ishtiaq M., et al. Edge computing in
    IoT: A 6G perspective (2021) arXiv preprint arXiv:2111.08943 Google Scholar [97]
    Kumar A., et al. A drone-based networked system and methods for combating coronavirus
    disease (COVID-19) pandemic Future Gener. Comput. Syst., 115 (2021), pp. 1-19
    View PDFView articleView in ScopusGoogle Scholar [98] Shi Y., et al. Machine learning
    for large-scale optimization in 6g wireless networks IEEE Commun. Surv. Tutor.
    (2023) Google Scholar [99] Alkhateeb A., et al. Real-time digital twins: Vision
    and research directions for 6G and beyond IEEE Commun. Mag. (2023) Google Scholar
    [100] Ansar S.A., et al. Intelligent Fog-IoT Networks with 6G endorsement: Foundations,
    applications, trends and challenges 6G Enabled Fog Computing in IoT: Applications
    and Opportunities, Springer (2023), pp. 287-307 CrossRefGoogle Scholar [101] Akyildiz
    I.F., et al. 6G and beyond: The future of wireless communications systems IEEE
    Access, 8 (2020), pp. 133995-134030 CrossRefView in ScopusGoogle Scholar [102]
    Ghafouri S., et al. Mobile-kube: Mobility-aware and energy-efficient service orchestration
    on kubernetes edge servers 2022 IEEE/ACM 15th International Conference on Utility
    and Cloud Computing (UCC), IEEE (2022), pp. 82-91 CrossRefView in ScopusGoogle
    Scholar [103] Wu H., et al. Energy-efficient decision making for mobile cloud
    offloading IEEE Trans. Cloud Comput., 8 (2) (2020), pp. 570-584 CrossRefView in
    ScopusGoogle Scholar [104] Wu H., et al. Lyapunov-guided delay-aware energy efficient
    offloading in iIoT-mec systems IEEE Trans. Ind. Inform., 19 (2) (2023), pp. 2117-2128
    CrossRefView in ScopusGoogle Scholar [105] Owens J.D., et al. A survey of general-purpose
    computation on graphics hardware Comput. Graph. Forum, 26 (1) (2007), pp. 80-113
    CrossRefView in ScopusGoogle Scholar [106] Von Neumann J. John Von Neumann: Selected
    Letters American Mathematical Soc. (2005) Google Scholar [107] Kimovski D., et
    al. Beyond von neumann in the computing continuum: Architectures, applications,
    and future directions IEEE Internet Comput. (2023) Google Scholar [108] Yang R.,
    et al. Integrated blockchain and edge computing systems: A survey, some research
    issues and challenges IEEE Commun. Surv. Tutor., 21 (2) (2019), pp. 1508-1532
    CrossRefView in ScopusGoogle Scholar [109] Alsamhi S.H., et al. Computing in the
    sky: A survey on intelligent ubiquitous computing for uav-assisted 6g networks
    and industry 4.0/5.0 Drones, 6 (7) (2022), p. 177 CrossRefView in ScopusGoogle
    Scholar [110] Chen J., et al. Deep learning with edge computing: A review Proc.
    IEEE, 107 (8) (2019), pp. 1655-1674 CrossRefView in ScopusGoogle Scholar [111]
    Singh H., et al. Metaheuristics for scheduling of heterogeneous tasks in cloud
    computing environments: Analysis, performance evaluation, and future directions
    Simul. Model. Pract. Theory, 111 (2021), Article 102353 View PDFView articleView
    in ScopusGoogle Scholar [112] Botta A., et al. Integration of cloud computing
    and internet of things: a survey Future Gener. Comput. Syst., 56 (2016), pp. 684-700
    View PDFView articleView in ScopusGoogle Scholar [113] Cappello F., et al. Computing
    on large-scale distributed systems: XtremWeb architecture, programming models,
    security, tests and convergence with grid Future Gener. Comput. Syst., 21 (3)
    (2005), pp. 417-437 View PDFView articleView in ScopusGoogle Scholar [114] Andrews
    D., et al. Achieving programming model abstractions for reconfigurable computing
    IEEE Trans. Very Large Scale Integr. (VLSI) Syst., 16 (1) (2007), pp. 34-44 Google
    Scholar [115] Jackson J.C., et al. Survey on programming models and environments
    for cluster, cloud, and grid computing that defends big data Procedia Comput.
    Sci., 50 (2015), pp. 517-523 View PDFView articleView in ScopusGoogle Scholar
    [116] Cao C., et al. A novel multi-objective programming model of relief distribution
    for sustainable disaster supply chain in large-scale natural disasters J. Clean.
    Prod., 174 (2018), pp. 1422-1435 View PDFView articleView in ScopusGoogle Scholar
    [117] Butts M., et al. A structural object programming model, architecture, chip
    and tools for reconfigurable computing 15th Annual IEEE Symposium on Field-Programmable
    Custom Computing Machines (FCCM 2007), IEEE (2007), pp. 55-64 CrossRefView in
    ScopusGoogle Scholar [118] Shen X., et al. Holistic network virtualization and
    pervasive network intelligence for 6G IEEE Commun. Surv. Tutor., 24 (1) (2021),
    pp. 1-30 CrossRefView in ScopusGoogle Scholar [119] Jin S., et al. H-svm: Hardware-assisted
    secure virtual machines under a vulnerable hypervisor IEEE Trans. Comput., 64
    (10) (2015), pp. 2833-2846 View in ScopusGoogle Scholar [120] Mansouri Y., et
    al. A review of edge computing: Features and resource virtualization J. Parallel
    Distrib. Comput., 150 (2021), pp. 155-183 View PDFView articleView in ScopusGoogle
    Scholar [121] Zhang J., et al. Performance analysis of 3D XPoint SSDs in virtualized
    and non-virtualized environments 2018 IEEE 24th International Conference on Parallel
    and Distributed Systems (ICPADS), IEEE (2018), pp. 1-10 Google Scholar [122] Alam
    I., et al. A survey of network virtualization techniques for Internet of Things
    using SDN and NFV ACM Comput. Surv., 53 (2) (2020), pp. 1-40 Google Scholar [123]
    Xing Y., et al. Virtualization and cloud computing Future Wireless Networks and
    Information Systems: Volume 1, Springer (2012), pp. 305-312 CrossRefView in ScopusGoogle
    Scholar [124] A. Agache, et al., Firecracker: Lightweight virtualization for serverless
    applications, in: 17th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 20), 2020, pp. 419–434. Google Scholar [125] Blake G., et al. A survey of
    multicore processors IEEE Signal Process. Mag., 26 (6) (2009), pp. 26-37 View
    in ScopusGoogle Scholar [126] Gizopoulos D., et al. Architectures for online error
    detection and recovery in multicore processors 2011 Design, Automation & Test
    in Europe, IEEE (2011), pp. 1-6 CrossRefGoogle Scholar [127] Delgado R., et al.
    New insights into the real-time performance of a multicore processor IEEE Access,
    8 (2020), pp. 186199-186211 CrossRefView in ScopusGoogle Scholar [128] Piattini
    M., et al. Toward a quantum software engineering IT Prof., 23 (1) (2021), pp.
    62-66 CrossRefView in ScopusGoogle Scholar [129] Arvanitou E.-M., et al. Software
    engineering practices for scientific software development: A systematic mapping
    study J. Syst. Softw., 172 (2021), Article 110848 View PDFView articleView in
    ScopusGoogle Scholar [130] Althar R.R., et al. The realist approach for evaluation
    of computational intelligence in software engineering Innov. Syst. Softw. Eng.,
    17 (1) (2021), pp. 17-27 CrossRefView in ScopusGoogle Scholar [131] De Stefano
    M., et al. Software engineering for quantum programming: How far are we? J. Syst.
    Softw., 190 (2022), Article 111326 View PDFView articleView in ScopusGoogle Scholar
    [132] Sharma G., et al. Applications of blockchain in automated heavy vehicles:
    Yesterday, today, and tomorrow Autonomous and Connected Heavy Vehicle Technology,
    Elsevier (2022), pp. 81-93 View PDFView articleView in ScopusGoogle Scholar [133]
    Al-Jaroodi J., et al. Blockchain in industries: A survey IEEE Access, 7 (2019),
    pp. 36500-36515 CrossRefView in ScopusGoogle Scholar [134] Doyle J., et al. Blockchainbus:
    A lightweight framework for secure virtual machine migration in cloud federations
    using blockchain Secur. Priv., 5 (2) (2022), Article e197 Google Scholar [135]
    Jurado Perez L., et al. Simulation of scalability in cloud-based iot reactive
    systems leveraged on a wsan simulator and cloud computing technologies Appl. Sci.,
    11 (4) (2021), p. 1804 CrossRefGoogle Scholar [136] Buyya R., et al. A strategy
    for advancing research and impact in new computing paradigms Green Mobile Cloud
    Computing, Springer (2022), pp. 297-308 CrossRefView in ScopusGoogle Scholar [137]
    Brady C., et al. All roads lead to computing: Making, participatory simulations,
    and social computing as pathways to computer science IEEE Trans. Educ., 60 (1)
    (2016), pp. 59-66 Google Scholar [138] Ferraz O., et al. A survey on high-throughput
    non-binary LDPC decoders: ASIC, FPGA, and GPU architectures IEEE Commun. Surv.
    Tutor., 24 (1) (2021), pp. 524-556 Google Scholar [139] Jouppi N.P., et al. A
    domain-specific architecture for deep neural networks Commun. ACM, 61 (9) (2018),
    pp. 50-59 CrossRefView in ScopusGoogle Scholar [140] Cong J., et al. Customizable
    computing—from single chip to datacenters Proc. IEEE, 107 (1) (2018), pp. 185-203
    View in ScopusGoogle Scholar [141] Ji H., et al. Magnetic reconnection in the
    era of exascale computing and multiscale experiments Nat. Rev. Phys., 4 (4) (2022),
    pp. 263-282 CrossRefView in ScopusGoogle Scholar [142] Heldens S., et al. The
    landscape of exascale research: A data-driven literature analysis ACM Comput.
    Surv., 53 (2) (2020), pp. 1-43 CrossRefGoogle Scholar [143] Kim Y., et al. Evidence
    for the utility of quantum computing before fault tolerance Nature, 618 (7965)
    (2023), pp. 500-505 View in ScopusGoogle Scholar [144] Anzt H., et al. Preparing
    sparse solvers for exascale computing Phil. Trans. R. Soc. A, 378 (2166) (2020),
    Article 20190053 CrossRefView in ScopusGoogle Scholar [145] Zangeneh-Nejad F.,
    et al. Analogue computing with metamaterials Nat. Rev. Mater., 6 (3) (2021), pp.
    207-225 View in ScopusGoogle Scholar [146] Zhang W., et al. Neuro-inspired computing
    chips Nat. Electron., 3 (7) (2020), pp. 371-382 CrossRefView in ScopusGoogle Scholar
    [147] Zhao M., et al. Reliability of analog resistive switching memory for neuromorphic
    computing Appl. Phys. Rev., 7 (1) (2020) Google Scholar [148] Zador A., et al.
    Catalyzing next-generation artificial intelligence through neuroai Nat. Commun.,
    14 (1) (2023), p. 1597 View in ScopusGoogle Scholar [149] Schuman C.D., et al.
    Opportunities for neuromorphic computing algorithms and applications Nat. Comput.
    Sci., 2 (1) (2022), pp. 10-19 CrossRefView in ScopusGoogle Scholar [150] Morabito
    F.C., et al. Advances in AI, neural networks, and brain computing: An introduction
    Artificial Intelligence in the Age of Neural Networks and Brain Computing, Elsevier
    (2024), pp. 1-8 View PDFView articleCrossRefGoogle Scholar [151] Rosenfeld V.,
    et al. Query processing on heterogeneous CPU/GPU systems ACM Comput. Surv., 55
    (1) (2022), pp. 1-38 CrossRefGoogle Scholar [152] Sanders J., et al. CUDA by Example:
    An Introduction to General-Purpose GPU Programming Addison-Wesley Professional
    (2010) Google Scholar [153] Tuli S., et al. Predicting the growth and trend of
    COVID-19 pandemic using machine learning and cloud computing Int. Things, 11 (2020),
    Article 100222 View PDFView articleView in ScopusGoogle Scholar [154] Lwakatare
    L.E., et al. Large-scale machine learning systems in real-world industrial settings:
    A review of challenges and solutions Inf. Softw. Technol., 127 (2020), Article
    106368 View PDFView articleView in ScopusGoogle Scholar [155] Wang M., et al.
    A survey on large-scale machine learning IEEE Trans. Knowl. Data Eng., 34 (6)
    (2020), pp. 2574-2594 CrossRefGoogle Scholar [156] M.N. Angenent, et al., Large-scale
    machine learning for business sector prediction, in: Proceedings of the 35th Annual
    ACM Symposium on Applied Computing, 2020, pp. 1143–1146. Google Scholar [157]
    Buyya R., et al. Cloud computing and emerging IT platforms: Vision, hype, and
    reality for delivering computing as the 5th utility Future Gener. Comput. Syst.,
    25 (6) (2009), pp. 599-616 View PDFView articleView in ScopusGoogle Scholar [158]
    Malik S.U., et al. EFFORT: Energy efficient framework for offload communication
    in mobile cloud computing Softw. - Pract. Exp., 51 (9) (2021), pp. 1896-1909 CrossRefView
    in ScopusGoogle Scholar [159] Jin X., et al. A survey of research on computation
    offloading in mobile cloud computing Wirel. Netw., 28 (4) (2022), pp. 1563-1585
    CrossRefView in ScopusGoogle Scholar [160] Patros P., et al. Toward sustainable
    serverless computing IEEE Internet Comput., 25 (6) (2021), pp. 42-50 CrossRefView
    in ScopusGoogle Scholar [161] Masdari M., et al. Green cloud computing using proactive
    virtual machine placement: challenges and issues J. Grid Comput., 18 (4) (2020),
    pp. 727-759 CrossRefView in ScopusGoogle Scholar [162] Gill S.S., et al. A taxonomy
    and future directions for sustainable cloud computing: 360 degree view ACM Comput.
    Surv., 51 (5) (2018), pp. 1-33 CrossRefGoogle Scholar [163] Shu W., et al. Research
    on strong agile response task scheduling optimization enhancement with optimal
    resource usage in green cloud computing Future Gener. Comput. Syst., 124 (2021),
    pp. 12-20 View PDFView articleView in ScopusGoogle Scholar [164] Zhou Q., et al.
    Energy efficient algorithms based on VM consolidation for cloud computing: comparisons
    and evaluations 2020 20th IEEE/ACM International Symposium on Cluster, Cloud and
    Internet Computing (CCGRID), IEEE (2020), pp. 489-498 CrossRefView in ScopusGoogle
    Scholar [165] Mansour R.F., et al. Design of cultural emperor penguin optimizer
    for energy-efficient resource scheduling in green cloud computing environment
    Cluster Comput., 26 (1) (2023), pp. 575-586 CrossRefView in ScopusGoogle Scholar
    [166] Singh M., et al. Dynamic shift from cloud computing to industry 4.0: Eco-friendly
    choice or climate change threat IoT-Based Intelligent Modelling for Environmental
    and Ecological Engineering: IoT Next Generation EcoAgro Systems, Springer (2021),
    pp. 275-293 CrossRefView in ScopusGoogle Scholar [167] W. Zeng, et al., Research
    on cloud storage architecture and key technologies, in: Proceedings of the 2nd
    International Conference on Interaction Sciences: Information Technology, Culture
    and Human, 2009, pp. 1044–1048. Google Scholar [168] Hota M., et al. Leveraging
    cloud-native microservices architecture for high performance real-time intra-day
    trading: A tutorial 6G Enabled Fog Computing in IoT: Applications and Opportunities,
    Springer (2023), pp. 111-129 CrossRefGoogle Scholar [169] Kumar M., et al. Qos-aware
    resource scheduling using whale optimization algorithm for microservice applications
    Softw. - Pract. Exp. (2023) Google Scholar [170] Ghofrani J., et al. Challenges
    of microservices architecture: A survey on the state of the practice ZEUS, 2018
    (2018), pp. 1-8 View in ScopusGoogle Scholar [171] Song C., et al. ChainsFormer:
    A chain latency-aware resource provisioning approach for microservices cluster
    International Conference on Service-Oriented Computing, Springer (2023), pp. 197-211
    CrossRefView in ScopusGoogle Scholar [172] Al-Doghman F., Mothers AI-enabled secure
    microservices in edge computing: Opportunities and challenges IEEE Trans. Serv.
    Comput. (2022) Google Scholar [173] Xu M., et al. CoScal: Multifaceted scaling
    of microservices with reinforcement learning IEEE Trans. Netw. Serv. Manag., 19
    (4) (2022), pp. 3995-4009 CrossRefView in ScopusGoogle Scholar [174] Bentaleb
    O., et al. Containerization technologies: Taxonomies, applications and challenges
    J. Supercomput., 78 (1) (2022), pp. 1144-1181 CrossRefView in ScopusGoogle Scholar
    [175] A. Barbalace, et al., Edge computing: The case for heterogeneous-ISA container
    migration, in: Proceedings of the 16th ACM SIGPLAN/SIGOPS International Conference
    on Virtual Execution Environments, 2020, pp. 73–87. Google Scholar [176] Golec
    M., et al. BioSec: A biometric authentication framework for secure and private
    communication among edge devices in IoT and industry 4.0 IEEE Consum. Electron.
    Mag., 11 (2) (2020), pp. 51-56 Google Scholar [177] Struhár V., et al. Real-time
    containers: A survey 2nd Workshop on Fog Computing and the IoT (Fog-IoT 2020),
    Schloss Dagstuhl-Leibniz-Zentrum für Informatik (2020) Google Scholar [178] Casalicchio
    E., et al. The state-of-the-art in container technologies: Application, orchestration
    and security Concurr. Comput.: Pract. Exper., 32 (17) (2020), Article e5668 View
    in ScopusGoogle Scholar [179] Zhong Z., et al. A cost-efficient container orchestration
    strategy in kubernetes-based cloud computing infrastructures with heterogeneous
    resources ACM Trans. Int. Technol. (TOIT), 20 (2) (2020), pp. 1-24 CrossRefView
    in ScopusGoogle Scholar [180] Mallikarjunaradhya V., et al. An overview of the
    strategic advantages of AI-powered threat intelligence in the cloud J. Sci. Technol.,
    4 (4) (2023), pp. 1-12 Google Scholar [181] P. Patros, et al., Investigating resource
    interference and scaling on multitenant PaaS clouds, in: Proceedings of the 26th
    Annual International Conference on Computer Science and Software Engineering,
    2016, pp. 166–177. Google Scholar [182] Kounev S., et al. Toward a definition
    for serverless computing Leibniz-Zentrum fur Informatik (2021) Google Scholar
    [183] Shafiei H., et al. Serverless computing: a survey of opportunities, challenges,
    and applications ACM Comput. Surv., 54 (11s) (2022), pp. 1-32 CrossRefGoogle Scholar
    [184] Golec M., et al. Qos analysis for serverless computing using machine learning
    Serverless Computing: Principles and Paradigms, Springer (2023), pp. 175-192 CrossRefView
    in ScopusGoogle Scholar [185] M.S. Aslanpour, et al., Serverless edge computing:
    vision and challenges, in: Proceedings of the 2021 Australasian Computer Science
    Week Multiconference, 2021, pp. 1–10. Google Scholar [186] Li Y., et al. Serverless
    computing: state-of-the-art, challenges and opportunities IEEE Trans. Serv. Comput.,
    16 (2) (2022), pp. 1522-1539 CrossRefView in ScopusGoogle Scholar [187] Kumar
    M., et al. AI-based sustainable and intelligent offloading framework for iIoT
    in collaborative cloud-fog environments IEEE Trans. Consum. Electron. (2023) Google
    Scholar [188] Iftikhar S., et al. TESCO: Multiple simulations based AI-augmented
    Fog computing for QoS optimization 2022 IEEE Smartworld, Ubiquitous Intelligence
    & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing,
    Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta),
    IEEE (2022), pp. 2092-2099 CrossRefView in ScopusGoogle Scholar [189] Firouzi
    F., et al. The convergence and interplay of edge, fog, and cloud in the AI-driven
    Internet of Things (IoT) Inf. Syst., 107 (2022), Article 101840 View PDFView articleView
    in ScopusGoogle Scholar [190] Cao Z., et al. Toward a systematic survey for carbon
    neutral data centers IEEE Commun. Surv. Tutor., 24 (2) (2022), pp. 895-936 CrossRefView
    in ScopusGoogle Scholar [191] Siddik M.A.B., et al. The environmental footprint
    of data centers in the United States Environ. Res. Lett., 16 (6) (2021), Article
    064017 CrossRefView in ScopusGoogle Scholar [192] Senthilkumar A., et al. Enhancement
    of R600a vapour compression refrigeration system with MWCNT/TiO2 hybrid nano lubricants
    for net zero emissions building Sustain. Energy Technol. Assess., 56 (2023), Article
    103055 View PDFView articleView in ScopusGoogle Scholar [193] Kurniawan T.A.,
    et al. Decarbonization in waste recycling industry using digitalization to promote
    net-zero emissions and its implications on sustainability J. Environ. Manag.,
    338 (2023), Article 117765 View PDFView articleView in ScopusGoogle Scholar [194]
    Wilkinson R., et al. Environmental impacts of earth observation data in the constellation
    and cloud computing era Sci. Total Environ., 909 (2024), Article 168584 View PDFView
    articleView in ScopusGoogle Scholar [195] Bhardwaj A.K., et al. HEART: Unrelated
    parallel machines problem with precedence constraints for task scheduling in cloud
    computing using heuristic and meta-heuristic algorithms Softw. - Pract. Exp.,
    50 (12) (2020), pp. 2231-2251 CrossRefView in ScopusGoogle Scholar [196] Fox G.C.,
    et al. Parallel Computing Works! Elsevier (2014) Google Scholar [197] Wu H., et
    al. A multi-dimensional parametric study of variability in multi-phase flow dynamics
    during geologic CO2 sequestration accelerated with machine learning Appl. Energy,
    287 (2021), Article 116580 View PDFView articleView in ScopusGoogle Scholar [198]
    Gill S.S. Quantum and blockchain based serverless edge computing: A vision, model,
    new trends and future directions Int. Technol. Lett. (2021), Article e275 Google
    Scholar [199] Nayeri Z.M., et al. Application placement in fog computing with
    AI approach: Taxonomy and a state of the art survey J. Netw. Comput. Appl., 185
    (2021), Article 103078 View PDFView articleView in ScopusGoogle Scholar [200]
    Patros P., et al. Rural AI: Serverless-powered federated learning for remote applications
    IEEE Internet Comput., 27 (2) (2023), pp. 28-34 CrossRefView in ScopusGoogle Scholar
    [201] Mahmud R., et al. Application management in fog computing environments:
    A taxonomy, review and future directions ACM Comput. Surv., 53 (4) (2020), pp.
    1-43 View in ScopusGoogle Scholar [202] Ruggeri A., et al. An innovative blockchain-based
    orchestrator for osmotic computing J. Grid Comput., 20 (2022), pp. 1-17 Google
    Scholar [203] Gill S.S., et al. SECURE: Self-protection approach in cloud resource
    management IEEE Cloud Comput., 5 (1) (2018), pp. 60-72 CrossRefView in ScopusGoogle
    Scholar [204] Ahammad I., et al. A review on cloud, fog, roof, and dew computing:
    Iot perspective Int. J. Cloud Appl. Comput. (IJCAC), 11 (4) (2021), pp. 14-41
    CrossRefView in ScopusGoogle Scholar [205] Mao Y., et al. A survey on mobile edge
    computing: The communication perspective IEEE Commun. Surv. Tutorials, 19 (4)
    (2017), pp. 2322-2358 View in ScopusGoogle Scholar [206] Luo Q., et al. Resource
    scheduling in edge computing: A survey IEEE Commun. Surv. Tutor., 23 (4) (2021),
    pp. 2131-2165 CrossRefView in ScopusGoogle Scholar [207] Cao K., et al. An overview
    on edge computing research IEEE Access, 8 (2020), pp. 85714-85728 CrossRefView
    in ScopusGoogle Scholar [208] Kotsehub N., et al. FLoX: Federated learning with
    FaaS at the edge 2022 IEEE 18th International Conference on E-Science (E-Science)
    (2022), pp. 11-20 CrossRefView in ScopusGoogle Scholar [209] Almurshed O., et
    al. Adaptive edge-cloud environments for rural AI 2022 IEEE International Conference
    on Services Computing (SCC) (2022), pp. 74-83 CrossRefView in ScopusGoogle Scholar
    [210] Abbas N., Zhang Y., Taherkordi A., Skeie T. Mobile edge computing: A survey
    IEEE Internet Things J., 5 (1) (2017), pp. 450-465 View in ScopusGoogle Scholar
    [211] Du J., et al. Computation energy efficiency maximization for NOMA-based
    and wireless-powered mobile edge computing with backscatter communication IEEE
    Trans. Mob. Comput. (2023), pp. 1-16 View in ScopusGoogle Scholar [212] Mach P.,
    et al. Mobile edge computing: A survey on architecture and computation offloading
    IEEE Commun. Surv. Tutorials, 19 (3) (2017), pp. 1628-1656 View in ScopusGoogle
    Scholar [213] Siriwardhana Y., et al. A survey on mobile augmented reality with
    5G mobile edge computing: Architectures, applications, and technical aspects IEEE
    Commun. Surv. Tutor., 23 (2) (2021), pp. 1160-1192 CrossRefView in ScopusGoogle
    Scholar [214] Golec M., et al. BlockFaaS: Blockchain-enabled serverless computing
    framework for AI-driven IoT healthcare applications J. Grid Comput., 21 (4) (2023),
    p. 63 View in ScopusGoogle Scholar [215] Zheng Z., et al. Blockchain challenges
    and opportunities: A survey Int. J. Web Grid Serv., 14 (4) (2018), pp. 352-375
    CrossRefView in ScopusGoogle Scholar [216] Gai K., et al. Blockchain meets cloud
    computing: A survey IEEE Commun. Surv. Tutor., 22 (3) (2020), pp. 2009-2030 CrossRefView
    in ScopusGoogle Scholar [217] Moqurrab S.A., et al. A deep learning-based privacy-preserving
    model for smart healthcare in internet of medical things using fog computing Wirel.
    Pers. Commun., 126 (3) (2022), pp. 2379-2401 CrossRefView in ScopusGoogle Scholar
    [218] Golec M., et al. Aiblock: Blockchain based lightweight framework for serverless
    computing using ai 2022 22nd IEEE International Symposium on Cluster, Cloud and
    Internet Computing (CCGrid), IEEE (2022), pp. 886-892 CrossRefView in ScopusGoogle
    Scholar [219] Kumar M., et al. Blockchain inspired secure and reliable data exchange
    architecture for cyber-physical healthcare system 4.0 Int. Things Cyber-Phys.
    Syst. (2023) Google Scholar [220] Li L., et al. A review of applications in federated
    learning Comput. Ind. Eng., 149 (2020), Article 106854 View PDFView articleView
    in ScopusGoogle Scholar [221] Yang J., et al. A federated learning attack method
    based on edge collaboration via cloud Softw. - Pract. Exp. (2022) Google Scholar
    [222] Zhang C., et al. A survey on federated learning Knowl.-Based Syst., 216
    (2021), Article 106775 View PDFView articleView in ScopusGoogle Scholar [223]
    Jiang W., et al. Federated split learning for sequential data in satellite–terrestrial
    integrated networks Inf. Fusion, 103 (2024), Article 102141 View PDFView articleView
    in ScopusGoogle Scholar [224] Kairouz P., et al. Advances and open problems in
    federated learning Found. Trends Mach. Learn., 14 (1–2) (2021), pp. 1-210 CrossRefView
    in ScopusGoogle Scholar [225] Wu G., et al. Privacy-preserving offloading scheme
    in multi-access mobile edge computing based on MADRL J. Parallel Distrib. Comput.,
    183 (2024), Article 104775 View PDFView articleView in ScopusGoogle Scholar [226]
    Ferdous M.S., et al. A survey of consensus algorithms in public blockchain systems
    for crypto-currencies J. Netw. Comput. Appl., 182 (2021), Article 103035 View
    PDFView articleView in ScopusGoogle Scholar [227] Manimuthu A., et al. A literature
    review on Bitcoin: Transformation of crypto currency into a global phenomenon
    IEEE Eng. Manag. Rev., 47 (1) (2019), pp. 28-35 CrossRefView in ScopusGoogle Scholar
    [228] Xu J., et al. A survey of blockchain consensus protocols ACM Comput. Surv.
    (2023) Google Scholar [229] Wang X., et al. Blockchain intelligence for internet
    of vehicles: Challenges and solutions IEEE Commun. Surv. Tutor. (2023) Google
    Scholar [230] Rahardja U., et al. GOOD, bad and dark bitcoin: a systematic literature
    review Aptisi Trans. Technopreneurship (ATT), 3 (2) (2021), pp. 115-119 View in
    ScopusGoogle Scholar [231] Golec M., et al. IFaaSBus: A security-and privacy-based
    lightweight framework for serverless computing using IoT and machine learning
    IEEE Trans. Ind. Inform., 18 (5) (2021), pp. 3522-3529 Google Scholar [232] Qu
    G., et al. ChainFL: A simulation platform for joint federated learning and blockchain
    in edge/cloud computing environments IEEE Trans. Ind. Inform., 18 (5) (2022),
    pp. 3572-3581 CrossRefView in ScopusGoogle Scholar [233] Golec M., et al. HealthFaaS:
    AI based smart healthcare system for heart patients using serverless computing
    IEEE Internet Things J. (2023) Google Scholar [234] Svorobej S., et al. Orchestration
    from the cloud to the edge The Cloud-to-Thing Continuum: Opportunities and Challenges
    in Cloud, Fog and Edge Computing, Springer International Publishing (2020), pp.
    61-77 CrossRefView in ScopusGoogle Scholar [235] Härdle W.K., et al. Understanding
    cryptocurrencies J. Financ. Econom., 18 (2) (2020), pp. 181-208 CrossRefView in
    ScopusGoogle Scholar [236] Weichbroth P., et al. Security of cryptocurrencies:
    A view on the state-of-the-art research and current developments Sensors, 23 (6)
    (2023), p. 3155 CrossRefView in ScopusGoogle Scholar [237] Schweizer A., et al.
    To what extent will blockchain drive the machine economy? Perspectives from a
    prospective study IEEE Trans. Eng. Manage., 67 (4) (2020), pp. 1169-1183 CrossRefView
    in ScopusGoogle Scholar [238] Khan M., et al. A review of distributed ledger technologies
    in the machine economy: challenges and opportunities in industry and research
    Proc. CIRP, 107 (2022), pp. 1168-1173 View PDFView articleView in ScopusGoogle
    Scholar [239] Dustdar S., et al. On distributed computing continuum systems IEEE
    Trans. Knowl. Data Eng., 35 (4) (2022), pp. 4092-4105 Google Scholar [240] Donta
    P.K., et al. Exploring the potential of distributed computing continuum systems
    Computers, 12 (10) (2023), p. 198 CrossRefView in ScopusGoogle Scholar [241] Morichetta
    A., et al. A roadmap on learning and reasoning for distributed computing continuum
    ecosystems IEEE International Conference on Edge Computing (EDGE), Institute of
    Electrical and Electronics Engineers (IEEE) (2021), pp. 25-31 CrossRefView in
    ScopusGoogle Scholar [242] Beasley C.J., et al. A new look at simultaneous sources
    Seg Technical Program Expanded Abstracts 1998, Society of Exploration Geophysicists
    (1998), pp. 133-135 CrossRefView in ScopusGoogle Scholar [243] Aminizadeh S.,
    et al. The applications of machine learning techniques in medical data processing
    based on distributed computing and the Internet of Things Comput. Methods Programs
    Biomed. (2023), Article 107745 View PDFView articleView in ScopusGoogle Scholar
    [244] Petrou L., et al. The first family of application-specific integrated circuits
    for programmable and reconfigurable metasurfaces Sci. Rep., 12 (1) (2022), p.
    5826 View in ScopusGoogle Scholar [245] Murray K.E., et al. Vtr 8: High-performance
    cad and customizable fpga architecture modelling ACM Trans. Reconfigurable Technol.
    Syst. (TRETS), 13 (2) (2020), pp. 1-55 CrossRefGoogle Scholar [246] Hitzler P.,
    et al. Neuro-Symbolic Artificial Intelligence: The State of the Art IOS Press
    (2022) Google Scholar [247] Gaur M., et al. Knowledge-infused learning: A sweet
    spot in neuro-symbolic ai IEEE Internet Comput., 26 (4) (2022), pp. 5-11 CrossRefView
    in ScopusGoogle Scholar [248] Du J., et al. Computation energy efficiency maximization
    for intelligent reflective surface-aided wireless powered mobile edge computing
    IEEE Trans. Sustain. Comput. (2023) Google Scholar [249] Cuadrado J., et al. Intelligent
    simulation of multibody dynamics: space-state and descriptor methods in sequential
    and parallel computing environments Multibody Syst. Dyn., 4 (2000), pp. 55-73
    View in ScopusGoogle Scholar [250] Zhang Y., et al. Transparent computing: Spatio-temporal
    extension on von Neumann architecture for cloud services Tsinghua Sci. Technol.,
    18 (1) (2013), pp. 10-21 View in ScopusGoogle Scholar [251] Jiang Q., et al. Adaptive
    scheduling of stochastic task sequence for energy-efficient mobile cloud computing
    IEEE Syst. J., 13 (3) (2019), pp. 3022-3025 CrossRefView in ScopusGoogle Scholar
    [252] Bufistov D., et al. A general model for performance optimization of sequential
    systems 2007 IEEE/ACM International Conference on Computer-Aided Design, IEEE
    (2007), pp. 362-369 CrossRefView in ScopusGoogle Scholar [253] Aslanpour M.S.,
    et al. Performance evaluation metrics for cloud, fog and edge computing: A review,
    taxonomy, benchmarks and standards for future research Int. Things, 12 (2020),
    Article 100273 View PDFView articleView in ScopusGoogle Scholar [254] Singh A.,
    et al. Quantum internet—applications, functionalities, enabling technologies,
    challenges, and research directions IEEE Commun. Surv. Tutor., 23 (4) (2021),
    pp. 2218-2247 CrossRefView in ScopusGoogle Scholar [255] De Leon N.P., et al.
    Materials challenges and opportunities for quantum computing hardware Science,
    372 (6539) (2021), p. eabb2823 View in ScopusGoogle Scholar [256] Smith K.N.,
    et al. Scaling superconducting quantum computers with chiplet architectures 2022
    55th IEEE/ACM International Symposium on Microarchitecture (MICRO), IEEE (2022),
    pp. 1092-1109 CrossRefView in ScopusGoogle Scholar [257] Spivey R.F., et al. High-stability
    cryogenic system for quantum computing with compact packaged ion traps IEEE Trans.
    Quant. Eng., 3 (2021), pp. 1-11 CrossRefGoogle Scholar [258] Nandhakumar A.R.,
    et al. EdgeAISim: A Toolkit for Simulation and Modelling of AI Models in Edge
    Computing Environments Meas.: Sensors (2023) Google Scholar [259] Xue M., et al.
    DDPQN: An efficient DNN offloading strategy in local-edge-cloud collaborative
    environments IEEE Trans. Serv. Comput., 15 (2) (2022), pp. 640-655 CrossRefView
    in ScopusGoogle Scholar [260] Lee Y.-L., et al. Techology trend of edge AI 2018
    International Symposium on VLSI Design, Automation and Test (VLSI-DAT), IEEE (2018),
    pp. 1-2 Google Scholar [261] Ding A.Y., et al. Roadmap for edge ai: A dagstuhl
    perspective ACM SIGCOMM Comput. Commun. Rev., 52 (1) (2022), pp. 28-33 CrossRefView
    in ScopusGoogle Scholar [262] Murugesan D., et al. Comparison of biologically
    inspired algorithm with socio-inspired technique on load frequency control of
    multi-source single-area power system Applied Genetic Algorithm and Its Variants:
    Case Studies and New Developments, Springer (2023), pp. 185-208 CrossRefGoogle
    Scholar [263] Kar A.K. Bio inspired computing–a review of algorithms and scope
    of applications Expert Syst. Appl., 59 (2016), pp. 20-32 View PDFView articleView
    in ScopusGoogle Scholar [264] Xu M., et al. esDNN: deep neural network based multivariate
    workload prediction in cloud computing environments ACM Trans. Int. Technol. (TOIT),
    22 (3) (2022), pp. 1-24 CrossRefView in ScopusGoogle Scholar [265] Denkena B.,
    et al. Reprint of: Gentelligent processes in biologically inspired manufacturing
    CIRP J. Manuf. Sci. Technol., 34 (2021), pp. 105-118 View PDFView articleView
    in ScopusGoogle Scholar [266] Dwivedi R., et al. Explainable AI (XAI): Core ideas,
    techniques, and solutions ACM Comput. Surv., 55 (9) (2023), pp. 1-33 CrossRefGoogle
    Scholar [267] Tosun A.B., et al. Histomapr™: An explainable AI (XAI) platform
    for computational pathology solutions Artificial Intelligence and Machine Learning
    for Digital Pathology: State-of-the-Art and Future Challenges, Springer (2020),
    pp. 204-227 CrossRefView in ScopusGoogle Scholar [268] Arrieta A.B., et al. Explainable
    artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges
    toward responsible AI Inf. Fusion, 58 (2020), pp. 82-115 Google Scholar [269]
    Kochovski P., et al. Trust management in a blockchain based fog computing platform
    with trustless smart oracles Future Gener. Comput. Syst., 101 (2019), pp. 747-759
    View PDFView articleView in ScopusGoogle Scholar [270] Shkembi K., et al. Semantic
    web and blockchain technologies: Convergence, challenges and research trends J.
    Web Semant., 79 (2023), Article 100809 View PDFView articleView in ScopusGoogle
    Scholar [271] Córcoles A.D., et al. Challenges and opportunities of near-term
    quantum computing systems Proc. IEEE, 108 (8) (2019), pp. 1338-1352 Google Scholar
    [272] Pirandola S., et al. Physics: unite to build a quantum internet Nature,
    532 (7598) (2016), pp. 169-171 CrossRefView in ScopusGoogle Scholar [273] Wehner
    S., et al. Quantum internet: a vision for the road ahead Science, 362 (6412) (2018),
    p. eaam9288 View in ScopusGoogle Scholar [274] Seto K.C., et al. From low-to net-zero
    carbon cities: The next global agenda Ann. Rev. Environ. Resour., 46 (2021), pp.
    377-415 CrossRefView in ScopusGoogle Scholar [275] Aceto G., et al. A survey on
    information and communication technologies for industry 4.0: State-of-the-art,
    taxonomies, perspectives, and challenges IEEE Commun. Surv. Tutor., 21 (4) (2019),
    pp. 3467-3501 CrossRefView in ScopusGoogle Scholar [276] Aceto G., et al. Industry
    4.0 and health: Internet of things, big data, and cloud computing for healthcare
    4.0 J. Ind. Inf. Integr., 18 (2020), Article 100129 View PDFView articleView in
    ScopusGoogle Scholar [277] Teoh Y.K., et al. IoT and fog computing based predictive
    maintenance model for effective asset management in industry 4.0 using machine
    learning IEEE Internet Things J. (2021) Google Scholar [278] Zheng T., et al.
    The applications of Industry 4.0 technologies in manufacturing context: a systematic
    literature review Int. J. Prod. Res., 59 (6) (2021), pp. 1922-1954 CrossRefView
    in ScopusGoogle Scholar [279] Yu W., et al. Energy digital twin technology for
    industrial energy management: Classification, challenges and future Renew. Sustain.
    Energy Rev., 161 (2022), Article 112407 View PDFView articleView in ScopusGoogle
    Scholar [280] Mihai S., et al. Digital twins: A survey on enabling technologies,
    challenges, trends and future prospects IEEE Commun. Surv. Tutor. (2022) Google
    Scholar [281] Wang Y., et al. A survey on digital twins: architecture, enabling
    technologies, security and privacy, and future prospects IEEE Internet Things
    J. (2023) Google Scholar [282] Kor M., et al. An investigation for integration
    of deep learning and digital twins towards construction 4.0 Smart Sustain. Built
    Environ., 12 (3) (2023), pp. 461-487 CrossRefView in ScopusGoogle Scholar [283]
    Singh S., et al. Qos-aware autonomic resource management in cloud computing: a
    systematic review ACM Comput. Surv., 48 (3) (2015), pp. 1-46 Google Scholar [284]
    Morichetta A., et al. Demystifying deep learning in predictive monitoring for
    cloud-native SLOs 2023 IEEE 16th International Conference on Cloud Computing (CLOUD)
    (2023), pp. 1-11 CrossRefGoogle Scholar [285] Wright S.A. Performance modeling,
    benchmarking and simulation of high performance computing systems Future Gener.
    Comput. Syst., 92 (2019), pp. 900-902 View PDFView articleView in ScopusGoogle
    Scholar [286] Materwala H., et al. QoS-SLA-aware adaptive genetic algorithm for
    multi-request offloading in integrated edge-cloud computing in internet of vehicles
    Veh. Commun., 43 (2023), Article 100654 View PDFView articleView in ScopusGoogle
    Scholar [287] Sharma Y., et al. SLA management in intent-driven service management
    systems: A taxonomy and future directions ACM Comput. Surv. (2023) Google Scholar
    [288] Khan S., et al. Guaranteeing end-to-end QoS provisioning in SOA based SDN
    architecture: A survey and open issues Future Gener. Comput. Syst., 119 (2021),
    pp. 176-187 View PDFView articleView in ScopusGoogle Scholar [289] Dilek S., et
    al. QoS-aware IoT networks and protocols: A comprehensive survey Int. J. Commun.
    Syst., 35 (10) (2022), Article e5156 View in ScopusGoogle Scholar [290] Pujol
    V.C., et al. Towards a prime directive of SLOs 2023 IEEE International Conference
    on Software Services Engineering (SSE) (2023), pp. 61-70 CrossRefView in ScopusGoogle
    Scholar [291] P. Patros, et al., SLO request modeling, reordering and scaling,
    in: Proceedings of the 27th Annual International Conference on Computer Science
    and Software Engineering, 2017, pp. 180–191. Google Scholar [292] Singh S., et
    al. The journey of qos-aware autonomic cloud computing IT Prof., 19 (2) (2017),
    pp. 42-49 View in ScopusGoogle Scholar [293] Patros P.o. Investigating the effect
    of garbage collection on service level objectives of clouds 2017 IEEE International
    Conference on Cluster Computing (CLUSTER), IEEE (2017), pp. 633-634 CrossRefView
    in ScopusGoogle Scholar [294] Zeng X., et al. SLA management for big data analytical
    applications in clouds: A taxonomy study ACM Comput. Surv., 53 (3) (2020), pp.
    1-40 Google Scholar [295] Qu C., et al. Auto-scaling web applications in clouds:
    A taxonomy and survey ACM Comput. Surv., 51 (4) (2018), pp. 1-33 View in ScopusGoogle
    Scholar [296] Lorido-Botran T., et al. A review of auto-scaling techniques for
    elastic applications in cloud environments J. Grid Comput., 12 (2014), pp. 559-592
    CrossRefView in ScopusGoogle Scholar [297] Singh P., et al. RHAS: robust hybrid
    auto-scaling for web applications in cloud computing Cluster Comput., 24 (2) (2021),
    pp. 717-737 CrossRefView in ScopusGoogle Scholar [298] T. Heinze, et al., Auto-scaling
    techniques for elastic data stream processing, in: Proceedings of the 8th ACM
    International Conference on Distributed Event-Based Systems, 2014, pp. 318–321.
    Google Scholar [299] Gill S.S., et al. Holistic resource management for sustainable
    and reliable cloud computing: An innovative solution to global challenge J. Syst.
    Softw., 155 (2019), pp. 104-129 View PDFView articleView in ScopusGoogle Scholar
    [300] Bharany S., et al. Energy efficient fault tolerance techniques in green
    cloud computing: A systematic survey and taxonomy Sustain. Energy Technol. Assess.,
    53 (2022), Article 102613 View PDFView articleView in ScopusGoogle Scholar [301]
    Gill S.S., et al. Failure management for reliable cloud computing: a taxonomy,
    model, and future directions Comput. Sci. Eng., 22 (3) (2018), pp. 52-63 View
    in ScopusGoogle Scholar [302] Gill S.S., et al. Tails in the cloud: a survey and
    taxonomy of straggler management within large-scale cloud data centres J. Supercomput.,
    76 (2020), pp. 10050-10089 CrossRefView in ScopusGoogle Scholar [303] Gill S.S.
    A manifesto for modern fog and edge computing: Vision, new paradigms, opportunities,
    and future directions Operationalizing Multi-Cloud Environments: Technologies,
    Tools and Use Cases, Springer (2021), pp. 237-253 Google Scholar [304] Katal A.,
    et al. Energy efficiency in cloud computing data centers: a survey on software
    technologies Cluster Comput., 26 (3) (2023), pp. 1845-1875 CrossRefView in ScopusGoogle
    Scholar [305] Masanet E., et al. Recalibrating global data center energy-use estimates
    Science, 367 (6481) (2020), pp. 984-986 CrossRefView in ScopusGoogle Scholar [306]
    Iftikhar S., et al. HunterPlus: AI based energy-efficient task scheduling for
    cloud–fog computing environments Int. Things, 21 (2023), Article 100667 View PDFView
    articleView in ScopusGoogle Scholar [307] Tuli S., et al. HUNTER: AI based holistic
    resource management for sustainable cloud computing J. Syst. Softw., 184 (2022),
    Article 111124 View PDFView articleView in ScopusGoogle Scholar [308] Schneider
    T., et al. Harnessing AI and computing to advance climate modelling and prediction
    Nature Clim. Change, 13 (9) (2023), pp. 887-889 CrossRefView in ScopusGoogle Scholar
    [309] Hartmann M., et al. Edge computing in smart health care systems: Review,
    challenges, and research directions Trans. Emerg. Telecommun. Technol., 33 (3)
    (2022), Article e3710 View in ScopusGoogle Scholar [310] Baek H.J., et al. Enhancing
    the usability of brain-computer interface systems Comput. Intell. Neurosci., 2019
    (2019) Google Scholar [311] Miraz M.H., et al. Adaptive user interfaces and universal
    usability through plasticity of user interface design Comp. Sci. Rev., 40 (2021),
    Article 100363 View PDFView articleView in ScopusGoogle Scholar [312] Diaz-de
    Arcaya J., et al. A joint study of the challenges, opportunities, and roadmap
    of mlops and aiops: A systematic survey ACM Comput. Surv., 56 (4) (2023), pp.
    1-30 CrossRefGoogle Scholar [313] Celik I. Exploring the determinants of artificial
    intelligence (Ai) literacy: Digital divide, computational thinking, cognitive
    absorption Telemat. Inform., 83 (2023), Article 102026 View PDFView articleView
    in ScopusGoogle Scholar [314] Gill S.S., et al. Transformative effects of ChatGPT
    on modern education: Emerging Era of AI chatbots Int. Things Cyber-Phys. Syst.,
    4 (2024), pp. 19-23 View PDFView articleView in ScopusGoogle Scholar [315] Le
    Roux C., et al. Can cloud computing bridge the digital divide in South African
    secondary education? Inf. Dev., 27 (2) (2011), pp. 109-116 CrossRefView in ScopusGoogle
    Scholar [316] Arce C.G.M., et al. Optimizing business performance: Marketing strategies
    for small and medium businesses using artificial intelligence tools Migr. Lett.,
    21 (S1) (2024), pp. 193-201 Google Scholar [317] Qadir J., et al. Toward accountable
    human-centered AI: rationale and promising directions J. Inf., Commun. Ethics
    Soc., 20 (2) (2022), pp. 329-342 CrossRefView in ScopusGoogle Scholar [318] Munn
    L. The uselessness of AI ethics AI Ethics, 3 (3) (2023), pp. 869-877 CrossRefGoogle
    Scholar [319] Scuotto V., et al. The digital humanism era triggered by individual
    creativity J. Bus. Res., 158 (2023), Article 113709 View PDFView articleView in
    ScopusGoogle Scholar [320] Schaap J., et al. ‘Gods in world of warcraft exist’:
    Religious reflexivity and the quest for meaning in online computer games New Media
    Soc., 19 (11) (2017), pp. 1744-1760 CrossRefView in ScopusGoogle Scholar [321]
    Magni D., et al. Digital humanism and artificial intelligence: the role of emotions
    beyond the human–machine interaction in society 5.0 J. Manag. History (2023) Google
    Scholar [322] Yu Q., et al. Lagrange coded computing: Optimal design for resiliency,
    security, and privacy The 22nd International Conference on Artificial Intelligence
    and Statistics, PMLR (2019), pp. 1215-1225 CrossRefView in ScopusGoogle Scholar
    [323] Olowononi F.O., et al. Resilient machine learning for networked cyber physical
    systems: A survey for machine learning security to securing machine learning for
    CPS IEEE Commun. Surv. Tutor., 23 (1) (2020), pp. 524-552 Google Scholar [324]
    Liu Z., et al. Efficient dropout-resilient aggregation for privacy-preserving
    machine learning IEEE Trans. Inf. Forensics Secur., 18 (2022), pp. 1839-1854 CrossRefView
    in ScopusGoogle Scholar [325] Samriya J.K., et al. Secured data offloading using
    reinforcement learning and Markov decision process in mobile edge computing Int.
    J. Netw. Manag., 33 (5) (2023), Article e2243 View in ScopusGoogle Scholar [326]
    Ullah I., et al. Privacy preserving large language models: Chatgpt case study
    based vision and framework (2023) arXiv preprint arXiv:2310.12523 Google Scholar
    [327] Kim H., et al. Resilient authentication and authorization for the internet
    of things (IoT) using edge computing ACM Trans. Int. Things, 1 (1) (2020), pp.
    1-27 Google Scholar [328] Delacour C., et al. Energy-performance assessment of
    oscillatory neural networks based on VO devices for future edge AI computing IEEE
    Trans. Neural Netw. Learn. Syst. (2023) Google Scholar [329] Quan Z., et al. A
    historical review on learning with technology: From computers to smartphones Encyclopedia
    of Information Science and Technology, Sixth Edition, IGI Global (2025), pp. 1-21
    CrossRefGoogle Scholar [330] Mijuskovic A., et al. Resource management techniques
    for cloud/fog and edge computing: An evaluation framework and classification Sensors,
    21 (5) (2021), p. 1832 CrossRefGoogle Scholar [331] Singh S., et al. A survey
    on resource scheduling in cloud computing: Issues and challenges J. Grid Comput.,
    14 (2016), pp. 217-264 View in ScopusGoogle Scholar [332] Hong C.-H., et al. Resource
    management in fog/edge computing: a survey on architectures, infrastructure, and
    algorithms ACM Comput. Surv., 52 (5) (2019), pp. 1-37 View in ScopusGoogle Scholar
    [333] Jamil B., et al. Resource allocation and task scheduling in fog computing
    and internet of everything environments: A taxonomy, review, and future directions
    ACM Comput. Surv., 54 (11s) (2022), pp. 1-38 CrossRefGoogle Scholar [334] Raju
    A., et al. A comparative study of spark schedulers’ performance 2019 4th International
    Conference on Computational Systems and Information Technology for Sustainable
    Solution (CSITSS), IEEE (2019), pp. 1-5 Google Scholar [335] Henning S., et al.
    Benchmarking scalability of stream processing frameworks deployed as microservices
    in the cloud J. Syst. Softw., 208 (2024), Article 111879 View PDFView articleView
    in ScopusGoogle Scholar [336] Feng J., et al. Heterogeneous computation and resource
    allocation for wireless powered federated edge learning systems IEEE Trans. Commun.,
    70 (5) (2022), pp. 3220-3233 CrossRefView in ScopusGoogle Scholar [337] Garofalo
    A., et al. A heterogeneous in-memory computing cluster for flexible end-to-end
    inference of real-world deep neural networks IEEE J. Emerg. Sel. Top. Circuits
    Syst., 12 (2) (2022), pp. 422-435 CrossRefView in ScopusGoogle Scholar [338] Wu
    H., et al. Collaborate edge and cloud computing with distributed deep learning
    for smart city internet of things IEEE Internet Things J., 7 (9) (2020), pp. 8099-8110
    CrossRefView in ScopusGoogle Scholar [339] Kumar V. Digital enablers The Economic
    Value of Digital Disruption: A Holistic Assessment for CXOs, Springer (2023),
    pp. 1-110 View in ScopusGoogle Scholar [340] Sha K., et al. A survey of edge computing-based
    designs for IoT security Digit. Commun. Netw., 6 (2) (2020), pp. 195-202 View
    PDFView articleView in ScopusGoogle Scholar [341] Sequeiros J.B., et al. Attack
    and system modeling applied to IoT, cloud, and mobile ecosystems: Embedding security
    by design ACM Comput. Surv., 53 (2) (2020), pp. 1-32 Google Scholar [342] Kaur
    A., et al. The future of cloud computing: opportunities, challenges and research
    trends 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics
    and Cloud)(I-SMAC) I-SMAC, IEEE (2018), pp. 213-219 CrossRefView in ScopusGoogle
    Scholar [343] Sebastian A., et al. Memory devices and applications for in-memory
    computing Nature Nanotechnol., 15 (7) (2020), pp. 529-544 CrossRefView in ScopusGoogle
    Scholar [344] Vu K., et al. ICT as a driver of economic growth: A survey of the
    literature and directions for future research Telecommun. Policy, 44 (2) (2020),
    Article 101922 View PDFView articleView in ScopusGoogle Scholar [345] Tesfatsion
    L. Agent-based computational economics: Overview and brief history Artif. Intell.,
    Learn. Comput. Econ. Finance (2023), pp. 41-58 CrossRefView in ScopusGoogle Scholar
    [346] Vairetti C., et al. Analytics-driven complaint prioritisation via deep learning
    and multicriteria decision-making European J. Oper. Res., 312 (3) (2024), pp.
    1108-1118 View PDFView articleView in ScopusGoogle Scholar [347] Jobin A., et
    al. The global landscape of AI ethics guidelines Nat. Mach. Intell., 1 (9) (2019),
    pp. 389-399 CrossRefGoogle Scholar [348] Hariri R.H., et al. Uncertainty in big
    data analytics: survey, opportunities, and challenges J. Big Data, 6 (1) (2019),
    pp. 1-16 Google Scholar [349] Cao L. Data science: a comprehensive overview ACM
    Comput. Surv., 50 (3) (2017), pp. 1-42 View in ScopusGoogle Scholar [350] Daniel
    B.K. Big data and data science: A critical review of issues for educational research
    Br. J. Educ. Technol., 50 (1) (2019), pp. 101-113 CrossRefView in ScopusGoogle
    Scholar [351] Donta P.K., et al. Governance and sustainability of distributed
    continuum systems: a big data approach J. Big Data, 10 (1) (2023), pp. 1-31 Google
    Scholar [352] ur Rehman M.H., et al. The role of big data analytics in industrial
    Internet of Things Future Gener. Comput. Syst., 99 (2019), pp. 247-259 View PDFView
    articleView in ScopusGoogle Scholar Cited by (3) CloudAISim: A toolkit for modelling
    and simulation of modern applications in AI-driven cloud computing environments
    2023, BenchCouncil Transactions on Benchmarks, Standards and Evaluations Show
    abstract PREDICTION OF CRYPTOCURRENCY PRICES USING LSTM, SVM AND POLYNOMIAL REGRESSION
    2024, arXiv Quantum Computing: Vision and Challenges 2024, arXiv © 2024 The Authors.
    Published by Elsevier B.V. Recommended articles Secure Internet of medical Things
    (IoMT) based on ECMQV-MAC authentication protocol and EKMC-SCP blockchain networking
    Information Sciences, Volume 654, 2024, Article 119783 Qinyong Lin, …, D. Paulraj
    View PDF Smarter eco-cities and their leading-edge artificial intelligence of
    things solutions for environmental sustainability: A comprehensive systematic
    review Environmental Science and Ecotechnology, Volume 19, 2024, Article 100330
    Simon Elias Bibri, …, Alexandre Alahi View PDF Cloud-based non-destructive characterization
    Non-Destructive Material Characterization Methods, 2024, pp. 727-765 Arash Heidari,
    …, Akira Otsuki Show 3 more articles Article Metrics Captures Readers: 29 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Telematics and Informatics Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Modern computing: Vision and challenges'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Solayman H.E.
  - Qasha R.P.
  citation_count: '0'
  description: The Internet of Things (IoT) has great potential to be adopted by applications
    covering several smart domains, as it consists of a set of physically linked objects
    that can be accessed through the internet. Virtualisation techniques play an important
    role in the field of IoT, especially for provisioning and orchestrating IoT applications
    to overcome the heterogeneity and diversity of the IoT components and environments
    that host the applications. Recently, container virtualisation became the preferred
    technique for IoT applications due to providing execution isolation, portability,
    lightweight deployment, and reduced design time as compared with hypervisor-based
    virtualisation. This survey presents a comprehensive study of provisioning and
    orchestrating the distributed IoT applications in different environments like
    Edge and Cloud Computing, and how containers can be used proficiently for provisioning
    and orchestrating IoT applications in these environments.
  doi: 10.1504/IJCSM.2023.135042
  full_citation: '>'
  full_text: '>

    "Login Help Sitemap Home For Authors For Librarians Orders Inderscience Online
    News Home Full-text access for editors On the use of container-based virtualisation
    for IoT provisioning and orchestration: a survey by Haleema Essa Solayman; Rawaa
    Putros Qasha International Journal of Computing Science and Mathematics (IJCSM),
    Vol. 18, No. 4, 2023  Abstract: The Internet of Things (IoT) has great potential
    to be adopted by applications covering several smart domains, as it consists of
    a set of physically linked objects that can be accessed through the internet.
    Virtualisation techniques play an important role in the field of IoT, especially
    for provisioning and orchestrating IoT applications to overcome the heterogeneity
    and diversity of the IoT components and environments that host the applications.
    Recently, container virtualisation became the preferred technique for IoT applications
    due to providing execution isolation, portability, lightweight deployment, and
    reduced design time as compared with hypervisor-based virtualisation. This survey
    presents a comprehensive study of provisioning and orchestrating the distributed
    IoT applications in different environments like Edge and Cloud Computing, and
    how containers can be used proficiently for provisioning and orchestrating IoT
    applications in these environments. Online publication date: Tue, 28-Nov-2023
    The full text of this article is only available to individual subscribers or to
    users at subscribing institutions.   Existing subscribers: Go to Inderscience
    Online Journals to access the Full Text of this article. Pay per view: If you
    are not a subscriber and you just want to read the full contents of this article,
    buy online access here. Complimentary Subscribers, Editors or Members of the Editorial
    Board of the International Journal of Computing Science and Mathematics (IJCSM):
    Login with your Inderscience username and password:     Username:        Password:          Forgotten
    your password?  Want to subscribe? A subscription gives you complete access to
    all articles in the current issue, as well as to all articles in the previous
    three years (where applicable). See our Orders page to subscribe. If you still
    need assistance, please email subs@inderscience.com    Keep up-to-date Our Blog
    Follow us on Twitter Visit us on Facebook Our Newsletter (subscribe for free)
    RSS Feeds New issue alerts Return to top Contact us About Inderscience OAI Repository
    Privacy and Cookies Statement Terms and Conditions Help Sitemap © 2024 Inderscience
    Enterprises Ltd."'
  inline_citation: '>'
  journal: International Journal of Computing Science and Mathematics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'On the use of container-based virtualisation for IoT provisioning and orchestration:
    a survey'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Urblik L.
  - Kajati E.
  - Papcun P.
  - Zolotova I.
  citation_count: '0'
  description: There is a rapid increase in the number of edge devices in IoT solutions,
    generating vast amounts of data that need to be processed and analyzed efficiently.
    Traditional cloud-based architectures can face latency, bandwidth, and privacy
    challenges when dealing with this data flood. There is currently no unified approach
    to the creation of edge computing solutions. This work addresses this problem
    by exploring containerization for data processing solutions at the network’s edge.
    The current approach involves creating a specialized application compatible with
    the device used. Another approach involves using containerization for deployment
    and monitoring. The heterogeneity of edge environments would greatly benefit from
    a universal modular platform. Our proposed edge computing-based framework implements
    a streaming extract, transform, and load pipeline for data processing and analysis
    using ZeroMQ as the communication backbone and containerization for scalable deployment.
    Results demonstrate the effectiveness of the proposed framework, making it suitable
    for time-sensitive IoT applications.
  doi: 10.3390/s23177662
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sensors All Article Types Advanced   Journals
    Sensors Volume 23 Issue 17 10.3390/s23177662 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Academic Editor Peter Chong
    Subscribe SciFeed Recommended Articles Related Info Links More by Authors Links
    Article Views 1080 Table of Contents Abstract Introduction Background Proposed
    Streaming ETL Framework Testing of the Framework Discussion Conclusions Author
    Contributions Funding Conflicts of Interest Abbreviations References share Share
    announcement Help format_quote Cite question_answer Discuss in SciProfiles thumb_up
    Endorse textsms Comment first_page settings Order Article Reprints Open AccessArticle
    A Modular Framework for Data Processing at the Edge: Design and Implementation
    by Lubomir Urblik *, Erik Kajati , Peter Papcun and Iveta Zolotova * Department
    of Cybernetics and Artificial Intelligence, Faculty of EE & Informatics, Technical
    University of Kosice, 042 00 Kosice, Slovakia * Authors to whom correspondence
    should be addressed. Sensors 2023, 23(17), 7662; https://doi.org/10.3390/s23177662
    Submission received: 28 July 2023 / Revised: 26 August 2023 / Accepted: 2 September
    2023 / Published: 4 September 2023 (This article belongs to the Special Issue
    Feature Papers in Communications Section 2023) Download keyboard_arrow_down     Browse
    Figures Review Reports Versions Notes Abstract There is a rapid increase in the
    number of edge devices in IoT solutions, generating vast amounts of data that
    need to be processed and analyzed efficiently. Traditional cloud-based architectures
    can face latency, bandwidth, and privacy challenges when dealing with this data
    flood. There is currently no unified approach to the creation of edge computing
    solutions. This work addresses this problem by exploring containerization for
    data processing solutions at the network’s edge. The current approach involves
    creating a specialized application compatible with the device used. Another approach
    involves using containerization for deployment and monitoring. The heterogeneity
    of edge environments would greatly benefit from a universal modular platform.
    Our proposed edge computing-based framework implements a streaming extract, transform,
    and load pipeline for data processing and analysis using ZeroMQ as the communication
    backbone and containerization for scalable deployment. Results demonstrate the
    effectiveness of the proposed framework, making it suitable for time-sensitive
    IoT applications. Keywords: containerization; edge computing; data processing
    framework; Kubernetes; Docker 1. Introduction The proliferation of devices at
    the edge of the network, year-on-year increments in computing power, more energy-saving
    devices, and small form-factor devices are creating new kinds of technological
    challenges and are generating a significant volume of data that were not anticipated
    when the cloud-based computing paradigm was developed. The volume of data being
    processed, the prioritization of processes, and the requirements for a low response
    critical for some applications have led to shifting computing resources as close
    as possible to the sources of these data. Edge devices may be consuming as well
    as producing data, so it is necessary to move some aspects of the infrastructure
    closer. Edge computing and cloud computing are not mutually exclusive. The issue
    is about extending and offloading demand from remote servers and reducing the
    load on the global network [1]. The Internet of Things (IoT) plays a significant
    role in this computing paradigm. Efficient data processing has become a critical
    aspect of IoT applications, enabling better monitoring, analysis, decision-making,
    and automation of various applications [2]. However, efficiently processing and
    managing vast amounts of data poses significant challenges, particularly regarding
    latency, bandwidth, and privacy. Edge computing is an emerging paradigm that aims
    to address these challenges by processing data closer to their source, reducing
    the need for data to travel long distances to centralized data centers [3]. This
    approach results in lower latency, reduced bandwidth and consumption, and improved
    data protection. However, efficiently deploying and managing applications at the
    edge remains a complex task [4]. Containerization has proven to be a powerful
    technology for deploying and managing applications. It offers improved scalability,
    portability, and resource utilization. Containers enable lightweight, isolated
    environments for running applications, making it easier to manage and scale applications
    across heterogeneous edge computing infrastructures [5,6,7]. In addition, Infrastructure
    as Code (IaC) allows for the streamlined management of infrastructure resources,
    enabling consistent and repeatable deployments. Using IaC, developers and operators
    can automate the provisioning and management of infrastructure resources, reducing
    the likelihood of human error and increasing the efficiency of the deployment
    process [8]. The rapid development of dedicated edge devices brings new opportunities
    as the performance of the devices increases. The variety in architectures, platforms,
    performance, and power of these devices presents a challenge in ensuring the compatibility
    of created solutions. The lack of standardized communication between parts of
    the solutions also poses a problem. To ensure the reusability of created solutions
    and compatibility with as many devices as possible, we propose a novel framework
    based on Docker containers. The framework employs a pipeline approach to data
    processing while providing an easy way to modify the pipeline steps. This work
    describes the implementation of the framework using a few custom-built services.
    By employing the pipeline approach, the framework allows for easy modification
    and extension of the services offered, providing a foundation for other solutions.
    2. Background This section provides an overview of the key concepts and technologies
    underpinning the proposed system, laying the foundation for a deeper understanding
    of the subsequent sections. 2.1. Containerization and Its Benefits Containerization
    is a lightweight virtualization technique that runs applications in isolated environments.
    A container image packages an application with all its dependencies, allowing
    it to be executed consistently across different platforms and environments. Docker
    is one of the most widely used containerization platforms, providing a robust
    ecosystem of tools and services for building, deploying, and managing containers.
    As shown in Figure 1, many required dependencies are omitted in containerized
    applications [9]. Figure 1. Architectures of virtual machines and containers.
    2.2. Container Orchestration Container orchestration is about managing and automating
    multiple containerized applications running on a cluster of machines instead of
    containerization, which focuses on creating isolated environments for applications
    and their dependencies. While containerization enables applications to run consistently
    across different platforms and environments, container orchestration ensures that
    these applications are efficiently deployed, scaled, and managed to meet high
    availability, resilience, and load-balancing demands. Kubernetes is an open-source
    container orchestration platform that simplifies the management of containerized
    applications across clusters of machines. Kubernetes provides various features,
    such as automatic scaling, rolling updates, and self-healing, to ensure that applications
    remain highly available and resilient [10]. 2.3. Edge Computing and Its Importance
    Rather than relying solely on centralized data centers or cloud infrastructure,
    edge computing is a distributed computing paradigm that moves data processing,
    storage, and analytics closer to the data source, such as IoT devices and sensors.
    This approach addresses several of the challenges associated with large-scale
    IoT deployments and offers multiple benefits: Reduced latency; Improved bandwidth
    utilization; Improved privacy; Improved scalability and efficiency. Reduced latency
    is one of the key benefits of edge computing, as data are processed closer to
    their source, resulting in faster response times for applications that rely on
    real-time decisions. This is particularly important in scenarios such as autonomous
    vehicles, industrial automation, healthcare, and smart cities, where low-latency
    responses are critical to the safety, efficiency, and overall performance of the
    system [11]. Improved bandwidth utilization also comes into play as edge computing
    reduces the need to send raw data across the network to a central processing facility.
    By processing data locally, we can reduce network traffic and communication costs
    by minimizing the data transmitted over the network [12]. In addition, this approach
    improves privacy and security by allowing sensitive data to be processed and stored
    locally without leaving a local firewall. This helps protect data from potential
    breaches and leaks [13]. The system can handle increased data volume and workload
    better by distributing computing tasks across multiple edge nodes. Furthermore,
    by performing computations at the edge, data centers and cloud infrastructures
    can offload a portion of their workload, resulting in lower energy consumption
    and a reduced environmental impact [14]. 2.4. Infrastructure as Code Infrastructure
    as Code is an approach that automates the management and deployment of infrastructure
    using code and configuration files rather than through manual processes or custom
    scripts. IaC enables developers to define, version, and maintain infrastructure
    components consistently and repeatably, similar to how software applications are
    developed and maintained. The key benefits are consistency and repeatability,
    automation, and reduced costs. Depending on the specific tool, IaC can use declarative
    or imperative methods to create and manage the infrastructure [15]. 2.5. Real-Time
    Data Processing Tools Real-time data processing tools are crucial for modern applications
    that analyze and react to data in motion. These tools enable large volumes of
    data to be processed with low latency, ensuring that insights can be generated
    faster and decisions can be made quicker. In IoT and edge computing, real-time
    data processing tools enable rapid decision-making, anomaly detection, and dynamic
    system adaptation [16]. ETL processes are essential aspects of data processing.
    They involve extracting data from different sources, transforming it into a desired
    format, and loading it into a target system for further analysis or storage [17].
    Streaming ETL is an extension of traditional ETL processes. It is specifically
    designed to handle continuous data streams in real time. Streaming ETL enables
    continuous data ingestion, transformation, and output with minimal latency. Unlike
    batch-based ETL, data are extracted, transformed, and loaded regularly. This approach
    is especially suited for modern applications with time-critical requirements like
    the IoT, event-driven architectures, and real-time analytics [18]. Streaming ETL
    processes have three primary components, as seen in Figure 2. First, data ingestion
    involves continuously consuming data from various sources, such as IoT devices
    or application events. Data ingestion components utilize streaming technologies
    like Apache Kafka or other messaging systems to receive and buffer data streams.
    Second, data transformation processes transform the ingested data in real time
    according to specified rules and logic. Transformation may include data cleansing,
    enrichment, aggregation, format conversions, or normalization. Lastly, data output
    involves loading transformed data immediately into target systems for further
    analysis, storage, or visualization. Output systems may include databases, data
    warehouses, or other analytics platforms, depending on the specific requirements
    of the application [19]. Figure 2. Streaming ETL data flow diagram. In this section,
    we will mention some works related to our article that served as either direct
    or indirect inspiration and solved similar problems or utilized the same technologies
    as our team. This section is divided into multiple subsections based on the main
    focus of the work. 2.6. Edge Computing Al-Rakhami et al. [20] describe running
    regularized extreme learning machine neural networks in a containerized environment
    on an inexpensive edge device. The proposed framework divides the components into
    layers, which communicate using REST API to provide abstraction and independence
    to each component. The study focused on machine learning applications in an edge
    environment, specifically the classification of a person’s movement. The subjects
    were wearing multiple accelerometers and gyroscopes, the data from which were
    sent to a Raspberry Pi, which then processed and classified these data into five
    movement categories. Kristiani et al. [21] created an air quality monitoring system
    using Docker, Kubernetes, and OpenStack. The devices collected data from the sensors
    and used MQTT to send them to the edge, where it was processed and saved, and
    in the case of an abnormal value, an alarm was sent using MQTT. The data were
    then sent to the cloud for long-term storage, analysis, and visualization. Docker
    created a unified environment for all edge devices and made deployment easier
    as the services were containerized and deployed across all devices. Ren et al.
    [22] describe the advantages of edge computing with a focus on personal computing
    services. Their experiments compare various communication methods, namely Wi-Fi,
    BLE, 4G, and wired Internet, and their combinations. The tests are divided into
    three categories: edge only, edge and cloud, and cloud only. Their results show
    a decrease in latency whenever edge computing is used, especially whenever machine
    learning is applied, as the processing time is much shorter than the data transmission
    time. González et al. [23] describe a data analysis pipeline used for biomedical
    images. The raw images are taken from imaging devices, such as microscopes or
    cameras, and with the application of AI, cells are detected and selected. Afterward,
    each cell is analyzed, the results of which are aggregated and saved. This pipeline
    approach gives the researchers more time for other tasks, such as experiments
    or research, as the data processing is almost entirely automated. Abdellatif et
    al. [24] describe the various uses of edge computing in smart health. These include
    the detection of emergencies, such as falls, by using cameras or accelerometers
    situated in the room or on the patient. Another is a patient data aggregator,
    which uses various sensors spread across the patient’s body to measure vital signs.
    These are then sent to a nearby hub for processing and storage. Many heart-related
    emergencies are detectable by sensors long before the patient feels them, so latency
    becomes an important factor, and having a nearby edge node improves the quality
    of services that come with a direct-to-cloud connection. Khan et al. [25] surveyed
    the various use cases of edge computing in smart cities. The expected rise of
    autonomous vehicles, which require a lot of processing power, can lead to quicker
    and better responses to traffic accidents. By placing edge nodes near the roads,
    they can contact the emergency units if a nearby vehicle has an accident. They
    can also evaluate the accident’s seriousness and appropriately relay the information
    to the rescuers [26]. Another possible use case is the detection of forest fires
    using unmanned aerial vehicles, such as drones. By either placing an edge node
    nearby or directly on the drone, we can ensure a much higher quality of service
    and, therefore, faster and better detection of possible fires. The UAVs can also
    serve as a communication infrastructure for rescue units in places where regular
    infrastructure is not sufficient [27,28]. The current parking infrastructure suffers
    from inefficient management, as drivers often have to drive all over and look
    for an empty spot. Some parking garages include distance sensors to detect where
    a car is present in a space, but these can be tricked with a shopping cart or
    cannot detect a parked motorcycle. By employing various artificial intelligence
    methods, such as neural networks, we can detect vehicles and their license plates
    and navigate the drivers to the nearest empty parking space [29,30]. Feng et al.
    [31] mention several possible edge computing applications in smart grids. As the
    voltage and frequency of electricity change dynamically, regulating these parameters
    requires real-time monitoring, which is a good fit for edge computing. Maintainers
    can also use these edge nodes, as they can provide accurate and real-time information
    about the current state of the infrastructure and help locate faults. Analyzing
    the data collected by the sensors connected to the distribution network can help
    owners forecast load or demand better. Using only the cloud is not feasible in
    such a case, as the amount of data collected would overwhelm the network and require
    less granularity, possibly resulting in a less accurate model. By collecting these
    data at the edge and then sending the results of computations to the cloud, the
    granularity is preserved, and higher accuracy can be achieved. Meani et al. [32]
    proposed the utilization of edge computing in smart retail. By collecting data
    about users, such as demographics, and time-related data, such as time of year,
    time of day, and time spent in specific areas of the shopping center, in combination
    with center-related data, such as layout, paths, areas of interest, and the mapping
    of the stores, an application can offer personalized offers targeted at a single
    user. As shopping centers provide free Wi-Fi to users, communication between their
    mobile devices and the edge components occurs faster than with the cloud. This
    increases data throughput at a lower latency, leading to faster total processing
    time. Smart farming is another area that benefits significantly from adopting
    edge computing. Connecting the devices to the Internet might be problematic on
    bigger fields, as the signal might not reach them. Placing a few devices along
    the field that connect can help with this connectivity, but it will also lead
    to an increase in latency as the number of hops required to reach the cloud will
    increase. This becomes a non-problem if the processing takes place on the devices
    themselves. Gireest et al. [33] created one such example: a Raspberry Pi is used
    with cheap, off-the-shelf sensors and actuators to create an automated irrigation
    system. As the temperature and humidity change, so must the irrigation. Otherwise,
    water might be wasted or insufficient. A more efficient system can be created
    by continuously measuring these values and adapting to the changes, leading to
    better yields. Similarly to the detection of fires, drones can be used to monitor
    the fields, as described by Oghaz et al. [34]. By placing an edge computing module,
    such as an SBC, onto the drone, we can monitor the crops in real time and apply
    various detection algorithms. After a drone detects a diseased plant, it can move
    closer to the plant, pull it out, or spray it with a pesticide to prevent further
    spread. Moreover, if the drone is insufficient, it can mark the diseased plant
    for another robot or person to care for. Such monitoring can also monitor plant
    growth and predict yield and profit. Many nutrient deficiencies are also detectable
    from the air. Another possible use of UAVs in farming is the collection of data
    from the sensors, as these can be placed far out of reach. A drone regularly flies
    over these points and collects all the data without the devices having to send
    the data all the time. One of the main limitations of edge computing is the power
    available to these devices. Typical cloud servers can consume hundreds or even
    thousands of watts of power. Pomsar et al. [35] describe the devices available
    for AI applications at the edge. Their study focused on devices with low power
    requirements: less than 40 W. The study also highlights another problem with edge
    computing: the variety of devices available. The performance of the devices in
    the study ranges from 0.472 tera operations per second (TOPS) to 32 TOPS, a difference
    of almost 7000% between the weakest and the strongest devices. 2.7. Frameworks
    Pääkkönen et al. [36] propose a reference architecture for machine learning development
    and deployment in edge environments as an extension of the traditional Big Data
    reference architecture. Compared with a more traditional approach, which uses
    high-performance computers, edge environments may be constrained by size, performance,
    or energy consumption. This presents new challenges when developing machine learning
    solutions for edge, and requires the different processing and preprocessing tasks
    to be split between multiple environments and devices to achieve the best possible
    results. We consider this reference architecture an excellent starting point as
    it encompasses almost every part of edge computing. We find it lacks specific
    details on how the specific tasks and services should be created and deployed
    to the devices. This might lead to an incompatibility between different layers
    of the solution. Bao et al. [37] propose a federated learning framework for use
    in edge-cloud collaborative computing, as the combination of federated learning
    and edge computing solves the privacy and security problems inherent in many areas,
    such as medical data. Computation offloading is widely used in the cloud-edge
    collaborative architecture, and the application of federated learning can take
    advantage of it to prolong the battery life of mobile devices. Another possible
    application is caching on the edge. To learn user preferences based on their age,
    gender, occupation, etc., the use of centralized learning becomes unavailable
    due to privacy concerns. By moving the learning to the edge, we can tailor the
    experiences to the users, as we can access their personal data without sharing
    it anywhere. This framework is focused on federated learning, which is an important
    part of edge computing but is not the only part. The framework could be extended
    to include other parts, such as data preprocessing, cleaning, or filtering. Rong
    et al. [38] propose an industrial edge-cloud collaborative computing platform
    for building and deploying IoT applications. This platform utilizes a pipeline-based
    model for streaming data from IoT devices. The problem of heterogeneity in edge
    devices is solved by an abstraction that declares the properties and behaviors
    of devices. They also provide pre-implemented functions, which can be used as
    steps in the pipeline. The M:N relationship between devices and pipelines allows
    us to use multiple pipelines for a single device or connect multiple devices to
    a single pipeline. The effectiveness of this solution is described in a real-world
    example where multiple cameras were deployed to detect sewage dumping. The authors
    focused on AI for edge computing, more specifically, computer vision and the application
    of AI to video streams. We consider their pipeline approach an excellent tool,
    as it removes the need to redeploy the entire application in the case of minor
    changes. The authors make no mention of compatibility with various edge devices,
    which might prove problematic. They also train the models in the cloud, which
    sparks privacy concerns. Lalanda et al. [39] created a modular platform aimed
    at smart homes that has since been expanded to other areas, such as smart manufacturing
    or smart building. This platform allows the connection of various devices, the
    development of data processing modules, and a dedicated simulator to test devices
    and modules. Unlike our framework, which is based on containerization and Docker,
    this solution is based on OSGi [40], a dynamic module system for Java. The modules
    can be created inside a dedicated IDE containing several wizards that help during
    the development. After creating a module, it can be deployed directly from the
    IDE to a remote platform. The framework displays great modularity but is limited
    to only one programming language: Java. We consider this a drawback, as many data-related
    and machine learning tasks are performed in Python. It can also lead to device
    incompatibility as it requires the Java Runtime. Xu et al. [41] describe an edge
    computing platform that allows connected devices to communicate through HTTP or
    CoAP protocols. The platform allows bidirectional communication to obtain data
    from the devices or send commands, such as changing the parameters, to the devices.
    To connect a device, an object containing the basic information is created, containing
    values such as device name, protocol to be used, device address, port number,
    etc. When a device is connected, value descriptors must be set, which contain
    basic information about the value, such as type, min, max, and default value.
    This approach allows heterogeneous devices to be connected to a unified platform.
    The platform also allows for the creation of commands, which can contain parameters,
    expected return values, and descriptions. The abstraction of the connected devices
    provides a great way to ensure compatibility. The authors have focused only on
    the connection of devices and not the processing of the data exchanged between
    them. With some expansion and combination with a data processing framework, their
    method can serve as a great building block for edge computing solutions. Trakadas
    et al. [42] propose a meta-OS, named RAMOS, aimed at edge computing. The authors
    consider the current hierarchical approach to the edge-fog-cloud continuum a barrier
    to the cooperation and collaboration of devices in IoT solutions. Their proposed
    solution, built on top of existing operating systems, aims to swap this approach
    for a more decentralized and distributed architecture. Unlike a more traditional
    approach in which cloud nodes take on the manager role, the authors propose multi-agent
    collaboration to achieve complex tasks without needing a central manager. By decentralizing
    the decision-making processes, the solution becomes more resilient to outages
    as it eliminates multiple points of failure. RAMOS aims to disrupt the current
    business models by returning the data back to producers. According to the authors,
    the current data balance between cloud and edge is 80–20%, highlighting the proliferation
    of cloud computing in the IoT. The authors envision a unified system, spanning
    everything from simple MCUs to high-performance servers, all using RAMOS. The
    nodes themselves are divided into two categories: atoms and molecules. Atoms are
    more simplistic nodes, providing basic computing capabilities and services. Molecules
    are more advanced nodes consisting of several Atoms and can provide the full functionality
    of RAMOS. These nodes advertise their capabilities, such as computing resources,
    services, and storage, using Agents. The Scheduler then looks at a list containing
    the Agents and decides where and how to process the data. RAMOS depends on a very
    high level of abstraction to unify the variety of devices used in the IoT. Implementing
    such an abstract solution may prove challenging due to different architectures,
    communication protocols and standards, and the sheer variety of available devices.
    Nevertheless, the potential of such a solution is immense and could transform
    the current approach to IoT solutions to be more collaborative, cooperative, and
    resource-efficient. Srirama et al. [43] describe a fog computing framework named
    FogDEFT, which uses containerization to solve the problem of heterogeneity in
    IoT solutions. This framework aims to utilize the OASIS Topology and Orchestration
    Specification for Cloud Applications (TOSCA) modeling language in fog applications.
    Although traditionally aimed at cloud applications utilizing services from known
    providers, the authors were able to extend TOSCA to the fog. The services themselves
    use Docker to solve the problems of different operating systems. To solve the
    problem of different hardware platforms, the authors utilize Buildx, a tool provided
    by Docker that allows an image to be built for multiple platforms, i.e., AMD64
    and ARM64. The framework was used for climate control in a convention center.
    The solution consisted of multiple Arduino MCUs, Raspberry Pi SBCs, and an AMD64
    server. One of the main advantages of cloud computing is access to virtually infinite
    resources. Recent years have seen a massive increase in the application of AI
    and ML in solutions. While powerful, these solutions tend to be demanding on the
    hardware. Edge devices cannot currently compete and require a slightly different
    approach. TinyML is a category of machine learning aimed at edge devices, which
    are constrained by the available performance, power, or size. The models created
    using this approach consume less energy and achieve better performance and comparable
    results when running on low-power devices such as SBCs. Lootus et al. [44] created
    a containerized framework for the deployment and monitoring of TinyML applications,
    named Runes, at the edge. One of the main drawbacks of AI at the edge is the need
    for the optimization of used hardware. As previously mentioned, the differences
    in available computing power in edge devices are immense. This extends to services
    available on these devices. The authors’ framework aims to optimize the created
    applications and ensure their compatibility with devices before deployment. The
    deployment of these applications is similar to Docker. The configuration is described
    in a Runefile, similar to a Dockerfile, which contains the basic information about
    the requirements of the image. It also contains the instructions that are executed
    when creating a Rune container. When deploying a new container, the framework
    first determines whether the device satisfies all requirements, such as communication
    protocols or connected devices. After a device is deemed capable, the container
    is created. To deploy Runes to multiple devices, the authors provide another tool
    named Hammer. This tool allows remote deployment to any connected device running
    RunicOS. We consider this work impressive, but the authors limit their framework
    to only AI/ML applications. There is no mention of any possible preprocessing
    to be performed using this framework, which limits the potential of this platform.
    Edge computing provides an excellent opportunity for IoT solutions, as relying
    solely on the cloud may prove difficult in many situations. The works described
    in this chapter provide a great look at the potential of edge computing. However,
    this potential will not be fully utilized without a reference architecture. Due
    to the sheer number and variety of edge devices, a unified approach to this problem
    must be taken. During our research, we have found several key points that we consider
    vital to this problem: Containerization: By packing together all the required
    libraries and settings, we can remove the headache of finding a version compatible
    with our device. Container orchestration: Tools like Kubernetes allow us to remotely
    manage the containers running on our devices. Data format unification: The sheer
    number of IoT devices brings communication challenges. Different devices use different
    data formats to convey their data to the edge or cloud. By ensuring that the data
    are formatted before being processed, we can solve this problem. Modularity: Due
    to the wide array of IoT applications, the system should apply to as many of them
    as possible. An easy-to-modify framework that allows the developers to modify
    existing processing tasks and seamlessly add new ones will be necessary. The table
    comparing the mentioned related works and our framework using the factors we consider
    important is shown in Table 1. The “x” denotes that the paper concerns itself
    with this category, the “-” that it does not. Table 1. Comparison of the mentioned
    frameworks. 3. Proposed Streaming ETL Framework To address the challenges of real-time
    data processing in IoT applications, we present a comprehensive Streaming ETL
    framework. The design incorporates partially decentralized communication using
    the ZMQ event bus, an approach similar to [31], and MQTT broker, facilitating
    communication between the Streaming ETL services and the IoT devices. The modular
    system can be easily scaled thanks to a partially decentralized architecture.
    Containerization technology, which provides a consistent environment for the application
    and its dependencies, plays a key role in ensuring the modularity and portability
    of the framework [41]. While simplifying deployment and management, this feature
    allows for smooth integration with infrastructure. As a result, the proposed framework
    can be easily adapted to different use cases and requirements, demonstrating its
    versatility in the face of diverse IoT application needs. Furthermore, combining
    ZMQ and MQTT communication technologies enables efficient data transfer and processing
    even in high-volume or limited networked scenarios. The lightweight nature of
    MQTT makes it well-suited for constrained environments and low-bandwidth networks
    [38], while ZMQ’s asynchronous messaging capabilities provide reliable and high-performance
    communication between ETL services. Both chosen communication technologies use
    publish-subscribe architecture, contributing to the system’s modularity. The proposed
    architecture design with the event bus and event platform can be seen in Figure
    3. Figure 3. Architecture of the proposed framework. A robust and flexible infrastructure
    that can adapt to the dynamic demands of IoT applications can be achieved by combining
    Kubernetes and Terraform. With its declarative approach, Terraform enables seamless
    infrastructure management and versioning, paving the way for rapid development
    and deployment of the solution. Using Terraform with Kubernetes ensures that infrastructure
    changes can be applied consistently and reliably across environments, simplifying
    the transition from development to production [21]. This combination of technologies
    also promotes more manageable and maintainable infrastructure by encouraging the
    adoption of IaC practices. By treating infrastructure as code, the system’s configuration
    can be versioned and tested, increasing confidence in the stability of the deployed
    solution. Overall, the integration of Kubernetes and Terraform provides a solid
    foundation for the Streaming ETL framework, ensuring its adaptability, reliability,
    and efficiency in meeting the diverse needs of IoT applications. Our programming
    language of choice for the Streaming ETL services’ development was Python. However,
    using the ZMQ event bus in our framework adds an extra layer of modularity, allowing
    the integration of components to be written in different programming languages
    such as C, C++, Java, JavaScript, Go, C#, and many others. This language-agnostic
    approach allows future researchers and developers to leverage the strengths of
    different programming languages when building individual components, increasing
    the flexibility and adaptability of the overall system. As a result, the system
    can be easily extended by adding more services (subscribers and/or publishers)
    into the ETL ZMQ event bus, as can be seen in Figure 4, or can be customized to
    meet the unique requirements of different IoT applications and environments. By
    incorporating this level of modularity and versatility into the implementation,
    our framework becomes more robust and capable of handling the complex and evolving
    challenges associated with IoT data processing. Figure 4. Streaming ETL ZMQ eventbus.
    In the following sections, we will look at the details of each service we have
    implemented as part of our Streaming ETL framework. We will discuss their functionalities,
    used technologies, and how they work together to provide efficient real-time data
    processing tools for not only IoT applications. In this project, we have developed
    a standardized framework for real-time data processing that can be customized
    for different applications. By following this framework, the user can create a
    data processing pipeline tailored to their specific needs, taking advantage of
    containerization, edge computing, automated infrastructure on Kubernetes, and
    efficient communication protocols. 3.1. Input Data Transformation Service The
    input data transformation service is an essential part of this framework. The
    data formats most commonly used in IoT solutions include JSON, raw values in either
    numerical or string form, and binary data. To ensure that all these types are
    usable in our framework, they must first be transformed into a unified format.
    The transformer processes raw data that are collected from various IoT devices.
    The service takes incoming data, applies transformation rules, and converts them
    into a standardized JSON format that can be easily consumed by subsequent components
    in the system, such as filtering and data analysis services. Data fields are standardized
    to general names during the transformation process, and measured values are rounded
    to two decimal places. After transformation, data are sent to the ZMQ event bus,
    making them available for other services. Standardization is the first step toward
    an autonomous AI algorithm capable of selecting and managing processing tasks
    to be applied to the data. This service can receive data from MQTT devices via
    the MQTT broker, which we call push devices, or from REST API clients, which we
    call fetch devices. For REST API clients, we implemented a modular client that
    fetches data from the endpoint every n seconds, where n is configured in the environment
    variables of this service. By standardizing data, the input data transformation
    service increases the overall efficiency and interoperability of the system, ensuring
    a seamless integration with other back-end services. 3.2. Filtering Mechanism
    Reducing the volume of data stored in the database is the responsibility of the
    filtering service. This is performed by applying pre-defined filters to the transformed
    data, ensuring that only relevant and necessary information is forwarded to subsequent
    components in the system. By filtering data, the service helps optimize storage
    and processing requirements and reduce network bandwidth consumption. This efficiency
    is particularly important in IoT applications, where devices can generate massive
    amounts of data, but not all may be relevant or useful. In addition, the service
    integrates modular filtering algorithms. Implemented filters include a value change
    filter that only sends data to the ZMQ event bus for further processing when a
    new value differs from the previous one. Another filter deals with numerical values
    with a set precision. In this case, data are only passed to the ZMQ event bus
    for further processing if a new value has changed by more than the set precision.
    These filters can be easily extended and adapted to suit different data processing
    requirements and scenarios. 3.3. Analyzing Service Analyzing service is the next
    component in the Streaming ETL system, designed to perform analysis on transformed
    data to extract valuable insights. This service has three components: Redis Logger,
    Redis Consumer, and Redis Streams. These elements and dataflow can be seen in
    Figure 5. Figure 5. Dataflow of analysis service. Redis Logger reads transformed
    data from the ZMQ event bus and sends them to Redis Streams, an in-memory data
    structure for managing and processing real-time data streams. Redis Streams provides
    efficient stream processing capabilities, making it an ideal choice for IoT applications
    that require rapid analysis of large volumes of data. Redis Consumer reads data
    from Redis Streams and analyzes the last n records. The analysis includes calculating
    the slope and rate of change of values such as temperature and humidity. These
    insights can then be used to support decision-making. It is important to state
    that within the analyzing service, any number of Redis consumers can be incorporated
    to meet the requirements of each specific application as needed. The service enables
    faster decision-making based on analyzed data by using Redis Streams for real-time
    data analysis. 3.4. MongoDB Time Series and Logging Service MongoDB Time Series
    and logging service represent the final components of the Streaming ETL system,
    responsible for storing processed data from IoT devices. This service uses the
    MongoDB Time Series database, a specialized data storage solution designed to
    handle time-based data with high ingestion rates and large volumes. By utilizing
    a time series database, the system can efficiently store, index, and query large
    volumes of time-stamped data, making it easier to perform historical analysis
    and identify trends over time. In addition, the MongoDB Time Series provides flexibility
    to handle different data types commonly found in IoT applications, such as temperature,
    humidity, and many other sensor readings. The logging service reads filtered data
    from the ZMQ event bus and stores them in the MongoDB Time Series database. In
    addition to transformed and filtered data, the logging service also includes information
    about the location of the device, the device ID, and metadata about the processes
    performed on the received data. This additional information provides valuable
    context for understanding and analyzing IoT data, enabling more informed decision-making
    and a better understanding of overall system performance. 3.5. Deploying Framework
    to Kubernetes Cluster We deployed the containerized framework on a laboratory
    edge server running the Proxmox hypervisor. This is an open-source virtualization
    platform based on Debian GNU/Linux. Proxmox uses Kernel-based Virtual Machine
    (KVM) and Linux Containers (LXC) technologies to manage and create virtual machines
    and containers. We created two virtual machines that form a Kubernetes cluster
    consisting of a manager node and a worker node. The manager node is responsible
    for the management of the cluster and the coordination of its activities. In contrast,
    the worker node runs containerized ETL services, MongoDB Time Series database,
    and MQTT broker. Instead of using Dockershim to connect Kubernetes to the Docker
    container engine, we chose CRI-O, a Kubernetes Container Runtime Interface (CRI)
    implementation. This was performed because CRI-O is specifically designed and
    optimized to meet the requirements of Kubernetes. The full architecture of our
    framework is shown in Figure 6. Figure 6. Full diagram of our framework. 4. Testing
    of the Framework To evaluate our framework, we have focused on analyzing the round-trip
    time (RTT) metric across ETL services within a series of tests. The tests included
    different numbers of measurements (10, 100, and 1000) and different time delays
    (1 ms, 10 ms, and 100 ms) between measurements. These parameters were selected
    to simulate different scenarios, such as a short burst with 10 values and 1 ms
    time delay or a long burst with 1000 measurements and 1 ms time delay. The burst,
    1 ms time delay, can simulate an industrial sensor that requires an instant response
    to measured values. The longer delay can simulate commercial IoT sensors, such
    as temperature sensors, where the increased delay does not cause any problems.
    We expected the RTT to decrease as the time delay decreased due to the dynamic
    frequency boosting of modern CPUs. To test the framework, we have chosen two different
    approaches that allowed us to verify the efficiency and performance of our framework
    on different modern hardware platforms. First, we tested the framework on a 64-bit
    ARM processor and then on a 64-bit x86 processor. We allocated eight CPU cores
    and 8192 MB of RAM in both environments for the test environment. The chosen architecture
    for testing the framework can be seen in Figure 7. Since we focused on testing
    ETL services within our proposed framework, we omitted actual IoT devices from
    our test environment because of the lack of computing resources and limited control
    over data transfer rates. For this reason, we designed a test container that could
    simulate an IoT device and allow us to adjust the speed and volume of the data
    feed as needed. The test container generates and sends data to the ZMQ event bus
    and then receives them to calculate RTT. We also omitted the MQTT broker and decided
    not to include the analysis service in the test as it does not play a primary
    role in data processing and depends on a required application. This proposed architecture
    allowed us to perform load tests on the designed services and ZMQ communication
    while giving us control over parameters we could change for different test scenarios.
    Figure 7. Communication between our containers. 4.1. The Test Environment For
    our testing, we took two different approaches, which allowed us to compare the
    effectiveness and performance of two different hardware platforms. The first was
    a 64-bit ARM CPU, and the second was a 64-bit x86 CPU. This was performed to ensure
    compatibility of our framework with the two most common CPU architectures used
    in edge computing. We also wanted to test whether there were measurable differences
    in the performance of our framework between them. The test scenarios were also
    selected to see how they dealt with various workloads. Table 2 shows the different
    platforms used in our testing. The Apple M1 CPU uses the ARM big.LITTLE architecture
    [45], employing high-performance cores (CPU-p), named Firestorm, and energy-efficient
    cores (CPU-e), named Icestorm. This architecture allowed a seamless switching
    of tasks between these different cores. Table 2. Hardware used in testing. 4.2.
    Tests on ARM64 with 100 ms Delay Test No. 1 consisted of 10 measurements of RTT
    across ETL services with a 100 ms delay between measurements. The median of this
    set of measurements was 5.392 ms, and the average was 5.729 ms, implying that
    the measurements’ distribution was approximately symmetrical. The standard deviation
    was 1.265 ms, implying that most values were close to the average. Other details
    are shown in Table 3. From the plot in Figure 8, we conclude that there was no
    increased activity in efficient cores. The performance cores worked at high frequencies,
    regardless of the test. Figure 8. ARM64 Test No. 1: Round trip time across services,
    10 times with a 100 ms delay. Table 3. The results of measurements. Test No. 2
    consisted of 100 measurements of RTT across ETL services with a 100 ms delay between
    measurements. The median was 5.313 ms, and the average was 5.055 ms, implying
    that most measurements were close to the average. The standard deviation was 1.965
    ms, which means that the values of the measurements are significantly different
    from the average and thus the distribution of the measurements is quite scattered.
    Other details can be seen in Table 3. From the plot in Figure 9, we can see that
    in some periods of the test, the RTT time is significantly reduced due to the
    higher clock speed of the efficient cores. In these intervals, the efficient cores
    were at their maximum clock speed. The performance cores operate at high frequencies,
    regardless of the test. Figure 9. ARM64 Test No. 2: Round trip time across ETL
    services, 100 times with 100 ms delay. Test No. 3 consisted of 1000 measurements
    of RTT with a delay of 100 ms between measurements, where the median was 5.208
    ms, and the average was 4.932 ms, indicating that most measurements were close
    to the average. The standard deviation was 1.505 ms, indicating a slight scatter
    of measurements and confirming that the distribution of measurements was approximately
    normal. The other data can be seen in Table 3. The plot in Figure 10 did not show
    a significant improvement in RTT time during the higher clock speeds of the efficient
    cores over the longer measurement period, and the performance cores operated at
    high frequencies independently of the test. Figure 10. ARM64 Test No. 3: Round
    trip time across ETL services, 1000 times with 100 ms delay. 4.3. Tests on ARM64
    with 10 ms Delay For the following tests, we have decided to omit the plots as
    they were too similar to the plots in previous tests and therefore had little
    value. Test No. 4 consisted of 10 measurements of RTT with a delay of 10 ms between
    measurements. The median was 5.189 ms, and the average was 5.036 ms, indicating
    that most measurements were close to the average. The standard deviation of the
    set of measurements was 1.177 ms, indicating that the distribution of measurements
    was slightly scattered. The other data can be seen in Table 3. We found a lower
    median in test No. 4 compared to test No. 1, where the difference was 0.203 ms,
    indicating a smaller RTT metric. Test No. 5 consisted of 100 measurements of RTT
    with a delay of 10 ms between measurements. The median was 4.692 ms, and the average
    was 4.492 ms, indicating that most measurements were close to the average. The
    standard deviation of the set of measurements was 1.175 ms, indicating that the
    values of the measurements were slightly different from the average. Thus, the
    distribution of the measurements is slightly scattered. The other data can be
    seen in Table 3. Compared to test No. 2, we observed a median time lower by 0.621
    ms in test No. 5, which could indicate a lower RTT metric. Test No. 6 consisted
    of 1000 measurements of RTT with a delay of 10 ms between measurements. The median
    was 4.561 ms, and the average was 4.218 ms, indicating that most measurements
    were close to the average. The standard deviation of the set of measurements was
    1.449 ms, indicating that the values of the measurements were slightly different
    from the average. The other data can be seen in the table (Table 3). We observed
    a 0.647 ms lower median time in test No. 6 compared to test No. 3, which could
    indicate lower RTT metrics. 4.4. Tests on ARM64 with 1 ms Delay Test No. 7 consisted
    of 10 measurements of RTT with a delay of 1 ms between measurements. The median
    was 4.352 ms, and the average was 4.653 ms, indicating that most measurements
    were close to the average. The standard deviation of the measurements was 0.837
    ms, indicating that the measurements differed little from the average. Thus, the
    distribution of the measurements was poorly dispersed. The other data can be seen
    in Table 3. From the plot in Figure 11, we conclude that the efficient cores had
    no increased activity during the test. The performance cores were operating at
    high frequencies, regardless of the test. Compared to test No. 4, the median in
    test No. 7 was 0.837 ms lower, which could indicate better RTT metrics. Figure
    11. ARM64 Test No. 7: Round trip time across ETL services, 10 times with 1 ms
    delay. Test No. 8 consisted of 100 measurements of RTT with a delay of 1 ms between
    measurements. The median was 1.144 ms, and the mean was 1.852 ms, with a standard
    deviation of 1.617 ms, indicating a slight scatter in the measurements. The other
    data can be seen in Table 3. In the plot in Figure 12, we can see a significant
    decrease in RTT values at the maximum clock frequency of the efficient cores.
    The performance cores worked at high frequencies, regardless of the test. We observed
    a 3.548 ms lower median in test No. 8 compared to test No. 5, which could indicate
    a lower RTT metric. Figure 12. ARM64 Test No. 8: Round trip time across ETL services,
    100 times with 1 ms delay. Test No. 9 consisted of 1000 measurements of RTT with
    a delay of 1 ms between measurements. The median was 2.504 ms, and the average
    was 2.428 ms, indicating that most measurements were close to the average. The
    standard deviation was 1.013 ms, indicating that the measurement values are slightly
    different from the average, and thus the distribution of measurements is slightly
    scattered. The plot in Figure 13 shows the timescale improvement in RTT values
    during the maximum clock frequency of the efficient cores. The performance cores
    operated at high frequencies, regardless of the test. Compared to test No. 6,
    we observed a lower median in test No. 9 (difference of 2.057 ms). This may indicate
    lower RTT metric values. See Table 3 for more details. Figure 13. ARM64 Test No.
    9: Round trip time across ETL services, 1000 times with 1 ms delay. 4.5. Tests
    on AMD64 with 100 ms Delay Test No. 1 consisted of 10 RTT measurements across
    ETL services with a delay of 100 ms between measurements. The median of the measurements
    was 2.616 ms, and the mean was 2.719 ms, indicating that the distribution of measurements
    was approximately symmetric. The standard deviation was 0.488 ms, indicating that
    most measurements were close to the average. Other data can be seen in Table 3.
    From the graph in Figure 14, it can be concluded that there was no significantly
    increased activity in the CPU cores during the test. Figure 14. AMD64 Test No.
    1: Round trip time across ETL services, 10 times with 100 ms delay. Test No. 2
    consisted of 100 measurements of RTT across ETL services with a delay of 100 ms
    between measurements. The median of this set of measurements was 2.563 ms, and
    the mean was 2.571 ms, indicating that the distribution of measurements was approximately
    symmetric. The standard deviation was 0.239 ms, indicating that most measurements
    were close to the average. Other data can be seen in Table 3. From the graph in
    Figure 15, it can be concluded that there was an increase in the activity of the
    CPU cores during the test. Figure 15. AMD64 Test No. 2: Round trip time across
    ETL services, 100 times with 100 ms delay. Test No. 3 consisted of 1000 measurements
    of RTT communication across ETL services with a delay of 100 ms between measurements.
    The median of this set of measurements was 2.529 ms, and the mean was 2.565 ms,
    indicating that the distribution of measurements was approximately symmetric.
    The standard deviation was 0.233 ms, indicating that most measurements were close
    to the average. Other data can be seen in Table 3. Test No. 3 showed increased
    activity of the cores during the test. The graph in Figure 16 shows that the clock
    speed of the cores increased at the beginning of the test. Figure 16. AMD64 Test
    No. 3: Round trip time across ETL services, 1000 times with 100 ms delay. 4.6.
    Tests on AMD64 with 10 ms Delay For the following tests, we decided to omit the
    plots as they were too similar to the plots in previous tests and therefore had
    little value. Test No. 4 was performed with a delay of 10 ms between measurements
    and consisted of 10 measurements. The mean RTT was 2.162 ms, the median was 2.001
    ms, and the standard deviation was 0.538 ms. Compared to test No. 1, where the
    delay was 100 ms, we can see that a lower delay between measurements resulted
    in lower RTTs. The other data can be seen in Table 3. Test No. 5 consisted of
    100 measurements of RTT communication across ETL services with a delay of 10 ms
    between measurements. The median of this set of measurements was 2.092 ms, and
    the mean was 2.170 ms, indicating that the distribution of measurements was approximately
    symmetric. The standard deviation was 0.391 ms, indicating that most measurements
    were close to the average. Other data can be seen in Table 3. Compared with the
    second test, we can see that a delay of 10 ms showed lower RTT values. Test No.
    6 consisted of 1000 measurements of RTT communication across ETL services with
    a delay of 10 ms between measurements. The median of this set of measurements
    was 1.955 ms, and the average was 1.991 ms, indicating that the distribution of
    measurements was approximately symmetric and most measurements were close to the
    average, as shown by the low standard deviation value of 0.231 ms. Other data
    can be seen in Table 3. Test No. 6 showed better results than Test No. 3. At a
    delay of 10 ms, the median RTT value was slightly better (1.955 ms versus 2.529
    ms). 4.7. Tests on AMD64 with 1 ms Delay Test No. 7 consisted of 10 measurements
    of RTT across ETL services with a delay of 1 ms between measurements. The median
    of this set of measurements was 1.582 ms, and the mean was 1.798 ms, indicating
    that the distribution of measurements was approximately symmetric. The standard
    deviation was 0.660 ms. Other data can be seen in Table 3. From the graph in Figure
    17, it can be concluded that the activity of the CPU cores increased during the
    test. Test No. 7, with a delay of 1 ms, had a median RTT of 1.582 ms, while Test
    No. 4, with a delay of 10 ms, had a median RTT of 2.001 ms. This means that decreasing
    the interval between sending data also decreased RTT values. Figure 17. AMD64
    Test No. 7: Round trip time across ETL services, 10 times with 1 ms delay. Test
    No. 8 consisted of 100 measurements of RTT across ETL services with a delay of
    1 ms between measurements. The median of this set of measurements was 1.395 ms,
    and the mean was 1.444 ms, indicating that the distribution of measurements was
    approximately symmetric. The standard deviation was 0.244 ms, indicating that
    most measurements were close to the average. The results of this test are similar
    to test No. 7, with a delay of 1 ms, which was conducted with fewer measurements.
    The other data can be seen in Table 3. The graph in Figure 18 shows a significant
    increase in the clock speed of the processor cores during the test. Comparing
    with test No. 5, we can see that a lower median RTT of 1.395 ms was achieved in
    test No. 8 compared to 2.092 ms in test No. 5. Figure 18. AMD64 Test No. 8: Round
    trip time across ETL services, 100 times with 1 ms delay. Test No. 9 consisted
    of 1000 measurements of RTT communication across ETL services with a delay of
    1 ms between measurements. The median of this set of measurements was 1.425 ms,
    and the mean was 1.475 ms, indicating that the distribution of measurements was
    approximately symmetric. The standard deviation was 0.317 ms, indicating that
    most measurements were close to the average. Other data can be seen in Table 3.
    Compared with previous tests, it can be seen that a short delay resulted in the
    lowest average value of RTT. Comparing tests No. 6 and No. 9, we can observe lower
    mean and median values in test No. 9. However, in test No. 9, the maximum value
    of RTT is significantly higher. In the graph in Figure 19, we can see an increase
    in the clock speed of the CPU cores during the test, which was fairly stable throughout
    the test. Figure 19. AMD64 Test No. 9: Round trip time across ETL services, 1000
    times with 1 ms delay. 5. Discussion Our test results provide valuable insight
    into the performance and efficiency of ETL services across different hardware
    platforms and configurations. Using a simulated IoT device as a test container
    allowed us to focus on data processing speeds and communication between ETL services
    while maintaining control over transmission speed and other test parameters. In
    addition, the decision to test the framework on ARM64 and x86 64-bit processors
    allowed us to explore the performance and compatibility of our framework across
    different modern hardware architectures. The results show that the proposed framework
    performs well under varying conditions, with the RTT metric remaining within acceptable
    limits throughout the tests. The tests also show that the framework can handle
    a range of data transfer speeds and measurement volumes, demonstrating its potential
    for scalability and adaptability to different IoT deployment scenarios. However,
    it is important to note that, while the test environment is comprehensive, it
    does not cover all possible scenarios. Further testing with real IoT devices and
    different network conditions may reveal additional challenges and potential optimizations
    for the ETL services. Figure 20 shows a significant difference in performance
    between the ARM64 and AMD64 processors in our evaluation of the RTT metrics. As
    the number of measurements increases, the ARM64 processor shows a greater decrease
    in median RTT than the AMD64 processor. This could be due to the differences in
    the architecture of the two processors. On the other hand, the AMD64 processor
    shows consistently low latency across all test scenarios, highlighting its suitability
    for real-time computing applications that require minimal latency. In addition,
    the data show that as the time delay between measurements decreases, the median
    RTT values for both processors tend to decrease, suggesting improved performance
    with more frequent requests. Overall, these results provide valuable insight into
    the performance of the two processors in real-time computing applications and
    can help select the appropriate hardware for specific use cases. Figure 20. Median
    values for different architectures and delays. The last test on both architectures
    shows that even when reaching the highest CPU frequencies, the processing is not
    fast enough to process the data before new data are acquired. This is shown in
    the results, where the mean and the median increased. The processing queue will
    keep lengthening, and the processing times will increase. The proposed framework
    shows promising performance and efficiency in processing and communication between
    ETL services. Further research and testing with real-world IoT devices, networks,
    and analytical services are required to better understand the framework’s potential
    for use in different IoT scenarios. The testing was performed on both ARM64 and
    AMD64 CPUs, as SBCs do not use a unified CPU architecture, and we wanted to ensure
    that our framework was viable for both architectures. Further testing is required
    to check if the differences between architectures transfer to smaller devices.
    Several research challenges still need to be addressed. The many devices used
    in edge environments make task offloading and load balancing difficult. For latency-sensitive
    tasks, the task offloading mechanism has to consider the distance to available
    nodes, their current load, and their potential performance. Load balancing is
    also a more complex task as the devices’ performance can vary greatly. Compatibility
    also needs to be considered, as the devices may not be capable of completing all
    the required tasks, and the system, therefore, has to consider the capabilities
    of other nodes [46]. Another research challenge arises in mobile edge computing,
    where the edge devices or the users connected to them move. The solution must
    predict the movement and ensure that the services are deployed to the devices
    when needed. The services also need to be deployed quickly. An example of this
    might be a shopping center with an unevenly spread-out crowd. As a large group
    of users moves, the solutions must scale the services up. It also needs to scale
    the services down when no longer needed, leaving room for other services to be
    deployed [47]. The security in edge computing also needs to be addressed, as the
    devices themselves may not support the security tools available for other devices.
    The variety of devices requires a particular approach to ensuring that all the
    devices are protected. Even a single vulnerable device endangers the whole network.
    The devices are also more prone to physical attacks where the attacker gains physical
    access to the devices. The data need to be protected on their entire journey from
    the sensor to the cloud, and they can be potentially intercepted at any step [48].
    6. Conclusions Edge computing is a fast-growing field hindered by a lack of standardization.
    The variety in edge devices collecting and generating the data proves to be an
    obstacle. This article focused on creating a modular framework that allows developers
    to modify and create data processing tasks on the edge. We first looked at the
    tools that can help develop and deploy edge computing solutions like Docker, Kubernetes,
    and Terraform. We then described the parts of our data processing pipeline and
    the architecture of our framework that brought the pipeline together. Using containerization
    technology, the tasks can be easily deployed, scaled, and monitored using an orchestrator.
    The pipeline approach we have selected allows us to have better control over the
    processing performed. It also allows us to reuse existing parts in new pipelines,
    leading to shorter development times. We consider the following to be the main
    strengths of our framework: compatibility—using Docker, we can deploy our framework
    to a wide array of SBCs, Mini-PCs, or servers; modularity—new data processing
    tasks can be easily added to the processing pipeline; agnosticism—our framework
    is not tied to any programming language. Parts of the pipeline can use different
    languages or versions of the same language. We have also created a unified testing
    platform that can be used to evaluate and compare the performance of edge devices.
    In our testing, we focused on the RTT metric in our pipeline. This was tested
    on both the ARM64 and AMD64 platforms, and both were compatible. We have found
    that increasing the frequency of sending data leads to decreased processing time.
    The testing was performed using a synthetic sensor capable of simulating different
    sensors and generating synthetic data. This tool will be expanded to include different
    scenarios and serve as a standardized tool for criteria evaluation. In the future,
    we plan on extending the framework with a graphical user interface (GUI) similar
    to Apache Airflow. This approach will provide an easy way to modify the data processing
    pipeline and allow even non-programmers to use the framework in their solutions.
    We also plan on creating more modules that will be included with our framework
    and can be directly used or modified. These modules will include additional data
    analytics, anomaly detection, and machine learning methods. Including advanced
    monitoring tools, such as Grafana, is also planned. The user will have better
    and more user-friendly access to their data with tables and graphs. The extension
    of the framework to include cloud nodes is also in our scope, as we have previously
    mentioned that edge computing and cloud computing are not competing technologies
    but rather complement each other. Author Contributions Conceptualization and methodology,
    L.U. and E.K.; software and original draft preparation, L.U.; review and editing,
    L.U., E.K., P.P. and I.Z.; supervision, P.P.; project administration and funding
    acquisition, I.Z. All authors have read and agreed to the published version of
    the manuscript. Funding This research was funded by APVV grant ENISaC—Edge-eNabled
    Intelligent Sensing and Computing, number APVV-20-0247. Conflicts of Interest
    The authors declare no conflict of interest. The funders had no role in the study’s
    design; in the collection, analyses, or interpretation of data; in the writing
    of the manuscript; or in the decision to publish the results. Abbreviations The
    following abbreviations are used in this manuscript: IoT Internet of Things IaC
    Infrastructure as Code ETL Extract, transform, load MQTT Message Queuing Telemetry
    Transport ZMQ ZeroMQ REST Representational state transfer API Application programming
    interface HTTP Hypertext Transfer Protocol CoAP Constrained Application Protocol
    BLE Bluetooth Low Energy AI Artificial intelligence UAV Unmanned aerial vehicle
    SBC Single-board computer KVM Kernel-based Virtual Machine LXC Linux Containers
    CRI Container Runtime Interface RTT Round-trip time References Pan, J.; McElhannon,
    J. Future edge cloud and edge computing for internet of things applications. IEEE
    Internet Things J. 2018, 5, 439–449. [Google Scholar] [CrossRef] Krishnamurthi,
    R.; Kumar, A.; Gopinathan, D.; Nayyar, A.; Qureshi, B. An Overview of IoT Sensor
    Data Processing, Fusion, and Analysis Techniques. Sensors 2020, 20, 6076. [Google
    Scholar] [CrossRef] Sulieman, N.A.; Ricciardi Celsi, L.; Li, W.; Zomaya, A.; Villari,
    M. Edge-Oriented Computing: A Survey on Research and Use Cases. Energies 2022,
    15, 452. [Google Scholar] [CrossRef] Vaño, R.; Lacalle, I.; Sowiński, P.; S-Julián,
    R.; Palau, C.E. Cloud-Native Workload Orchestration at the Edge: A Deployment
    Review and Future Directions. Sensors 2023, 23, 2215. [Google Scholar] [CrossRef]
    [PubMed] Watada, J.; Roy, A.; Kadikar, R.; Pham, H.; Xu, B. Emerging trends, techniques
    and open issues of containerization: A Review. IEEE Access 2019, 7, 152443–152472.
    [Google Scholar] [CrossRef] Bentaleb, O.; Belloum, A.S.; Sebaa, A.; El-Maouhab,
    A. Containerization technologies: Taxonomies, applications and challenges. J.
    Supercomput. 2021, 78, 1144–1181. [Google Scholar] [CrossRef] Hossain, M.D.; Sultana,
    T.; Akhter, S.; Hossain, M.I.; Thu, N.T.; Huynh, L.N.; Lee, G.W.; Huh, E.N. The
    role of microservice approach in edge computing: Opportunities, challenges, and
    research directions. ICT Express 2023. [Google Scholar] [CrossRef] Teppan, H.;
    Fla, L.H.; Jaatun, M.G. A survey on infrastructure-as-code solutions for cloud
    development. In Proceedings of the 2022 IEEE International Conference on Cloud
    Computing Technology and Science (CloudCom), Bangkok, Thailand, 13–16 December
    2022. [Google Scholar] [CrossRef] IBM. What Is Containerization? Available online:
    https://www.ibm.com/topics/containerization (accessed on 12 June 2023). Casalicchio,
    E. Container Orchestration: A survey. Syst. Model. Methodol. Tools 2018, 221–235.
    [Google Scholar] [CrossRef] Zhang, K.; Leng, S.; He, Y.; Maharjan, S.; Zhang,
    Y. Mobile edge computing and networking for green and low-latency internet of
    things. IEEE Commun. Mag. 2018, 56, 39–45. [Google Scholar] [CrossRef] Cao, K.;
    Liu, Y.; Meng, G.; Sun, Q. An overview on Edge computing research. IEEE Access
    2020, 8, 85714–85728. [Google Scholar] [CrossRef] Satyanarayanan, M. The emergence
    of Edge Computing. Computer 2017, 50, 30–39. [Google Scholar] [CrossRef] Premsankar,
    G.; Di Francesco, M.; Taleb, T. Edge computing for the internet of things: A case
    study. IEEE Internet Things J. 2018, 5, 1275–1284. [Google Scholar] [CrossRef]
    Morris, K. Infrastructure as Code: Managing Servers in the Cloud; O’Reilly Media:
    Sebastopol, CA, USA, 2016. [Google Scholar] Wang, J.; Yang, Y.; Wang, T.; Sherratt,
    R.S.; Zhang, J. Big Data Service Architecture: A Survey. J. Internet Technol.
    2020, 21, 393–405. [Google Scholar] AWS. What Is ETL (Extract, Transform, Load).
    Available online: https://aws.amazon.com/what-is/etl/ (accessed on 12 June 2023).
    Simitsis, A.; Skiadopoulos, S.; Vassiliadis, P. The History, Present, and Future
    of ETL Technology. In Proceedings of the 25th International Workshop on Design,
    Optimization, Languages and Analytical Processing of Big Data (DOLAP) Co-Located
    with the 26th International Conference on Extending Database Technology and the
    26th International Conference on Database Theory (EDBT/ICDT 2023), Ioannina, Greece,
    28 March 2023; Available online: https://ceur-ws.org/Vol-3369/invited1.pdf (accessed
    on 12 June 2023). Pareek, A.; Khaladkar, B.; Sen, R.; Onat, B.; Nadimpalli, V.;
    Lakshminarayanan, M. Real-time ETL in Striim. In Proceedings of the International
    Workshop on Real-Time Business Intelligence and Analytics, Rio de Janeiro, Brazil,
    27 August 2018. [Google Scholar] [CrossRef] Al-Rakhami, M.; Gumaei, A.; Alsahli,
    M.; Hassan, M.M.; Alamri, A.; Guerrieri, A.; Fortino, G. A lightweight and cost
    effective edge intelligence architecture based on Containerization Technology.
    World Wide Web 2019, 23, 1341–1360. [Google Scholar] [CrossRef] Kristiani, E.;
    Yang, C.T.; Huang, C.Y.; Wang, Y.T.; Ko, P.C. The implementation of a cloud-edge
    computing architecture using OpenStack and Kubernetes for Air Quality Monitoring
    Application. Mob. Netw. Appl. 2020, 26, 1070–1092. [Google Scholar] [CrossRef]
    Ren, L.; Zhang, Q.; Shi, W.; Peng, Y. Edge-based personal computing services:
    Fall detection as a pilot study. Computing 2019, 101, 1199–1223. [Google Scholar]
    [CrossRef] González, G.; Evans, C.L. Biomedical image processing with containers
    and Deep Learning: An Automated Analysis Pipeline. BioEssays 2019, 41, 1900004.
    [Google Scholar] [CrossRef] Abdellatif, A.A.; Mohamed, A.; Chiasserini, C.F.;
    Tlili, M.; Erbad, A. Edge computing for Smart Health: Context-aware approaches,
    opportunities, and challenges. IEEE Netw. 2019, 33, 196–203. [Google Scholar]
    [CrossRef] Khan, L.U.; Yaqoob, I.; Tran, N.H.; Kazmi, S.M.; Dang, T.N.; Hong,
    C.S. Edge-computing-enabled smart cities: A comprehensive survey. IEEE Internet
    Things J. 2020, 7, 10200–10232. [Google Scholar] [CrossRef] Chen, L.; Englund,
    C. Every second counts: Integrating edge computing and service oriented architecture
    for automatic emergency management. J. Adv. Transp. 2018, 2018, 7592926. [Google
    Scholar] [CrossRef] Narang, M.; Xiang, S.; Liu, W.; Gutierrez, J.; Chiaraviglio,
    L.; Sathiaseelan, A.; Merwaday, A. UAV-assisted edge infrastructure for challenged
    networks. In Proceedings of the 2017 IEEE Conference on Computer Communications
    Workshops (INFOCOM WKSHPS), Atlanta, GA, USA, 1–4 May 2017. [Google Scholar] [CrossRef]
    Avgeris, M.; Spatharakis, D.; Dechouniotis, D.; Kalatzis, N.; Roussaki, I.; Papavassiliou,
    S. Where there is fire there is smoke: A scalable edge computing framework for
    early fire detection. Sensors 2019, 19, 639. [Google Scholar] [CrossRef] Tang,
    C.; Wei, X.; Zhu, C.; Chen, W.; Rodrigues, J.J. Towards smart parking based on
    Fog Computing. IEEE Access 2018, 6, 70172–70185. [Google Scholar] [CrossRef] Tandon,
    R.; Gupta, P.K. Optimizing Smart Parking System by using Fog Computing. Adv. Comput.
    Data Sci. 2019, 724–737. [Google Scholar] [CrossRef] Feng, C.; Wang, Y.; Chen,
    Q.; Ding, Y.; Strbac, G.; Kang, C. Smart Grid encounters Edge computing: Opportunities
    and applications. Adv. Appl. Energy 2021, 1, 100006. [Google Scholar] [CrossRef]
    Meani, C.; Paglierani, P.; Ropodi, A.; Stasinopoulos, N.; Tsagkaris, K.; Demestichas,
    P. Enabling Smart Retail through 5G Services and Technologies. Architecture 2018,
    2, 3. [Google Scholar] T, G.K.; Shashank, K.V. Smart farming based on ai, edge
    computing and IOT. In Proceedings of the 2022 4th International Conference on
    Inventive Research in Computing Applications (ICIRCA), Coimbatore, India, 21–23
    August 2022. [Google Scholar] [CrossRef] Maktabdar Oghaz, M.; Razaak, M.; Kerdegari,
    H.; Argyriou, V.; Remagnino, P. Scene and environment monitoring using aerial
    imagery and deep learning. In Proceedings of the 2019 15th International Conference
    on Distributed Computing in Sensor Systems (DCOSS), Santorini Island, Greece,
    29–31 May 2019. [Google Scholar] [CrossRef] Pomsar, L.; Brecko, A.; Zolotova,
    I. Brief overview of edge ai accelerators for energy-constrained edge. In Proceedings
    of the 2022 IEEE 20th Jubilee World Symposium on Applied Machine Intelligence
    and Informatics (SAMI), Poprad, Slovakia, 19–22 January 2022. [Google Scholar]
    [CrossRef] Pääkkönen, P.; Pakkala, D. Extending reference architecture of Big
    Data Systems towards machine learning in edge computing environments. J. Big Data
    2020, 7, 25. [Google Scholar] [CrossRef] Bao, G.; Guo, P. Federated learning in
    cloud-edge collaborative architecture: Key Technologies, applications and challenges.
    J. Cloud Comput. 2022, 11, 94. [Google Scholar] [CrossRef] Rong, G.; Xu, Y.; Tong,
    X.; Fan, H. An edge-cloud collaborative computing platform for building AIoT applications
    efficiently. J. Cloud Comput. 2021, 10, 36. [Google Scholar] [CrossRef] Lalanda,
    P.; Hamon, C. A service-oriented edge platform for cyber-physical systems. CCF
    Trans. Pervasive Comput. Interact. 2020, 2, 206–217. [Google Scholar] [CrossRef]
    IBM. The OSGi Framework. 2013. Available online: https://www.ibm.com/docs/en/was/8.5.5?topic=applications-osgi-framework
    (accessed on 15 June 2023). Xu, R.; Jin, W.; Kim, D.H. Knowledge-based Edge Computing
    framework based on CoAP and HTTP for enabling heterogeneous connectivity. Pers.
    Ubiquitous Comput. 2020, 26, 329–344. [Google Scholar] [CrossRef] Trakadas, P.;
    Masip-Bruin, X.; Facca, F.M.; Spantideas, S.T.; Giannopoulos, A.E.; Kapsalis,
    N.C.; Martins, R.; Bosani, E.; Ramon, J.; Prats, R.G.; et al. A reference architecture
    for cloud—Edge meta-operating systems enabling cross-domain, data-intensive, ML-assisted
    applications: Architectural overview and key concepts. Sensors 2022, 22, 9003.
    [Google Scholar] [CrossRef] Srirama, S.N.; Basak, S. Fog computing out of the
    box with FOGDEFT framework: A case study. In Proceedings of the 2022 IEEE 15th
    International Conference on Cloud Computing (CLOUD), Barcelona, Spain, 10–16 July
    2022. [Google Scholar] [CrossRef] Lootus, M.; Thakore, K.; Leroux, S.; Trooskens,
    G.; Sharma, A.; Ly, H. A VM/containerized approach for scaling tinyml applications.
    arXiv 2022, arXiv:2202.05057. [Google Scholar] Arm. Big.LITTLE. Available online:
    https://www.arm.com/technologies/big-little (accessed on 8 June 2023). Qiu, T.;
    Chi, J.; Zhou, X.; Ning, Z.; Atiquzzaman, M.; Wu, D.O. Edge computing in industrial
    internet of things: Architecture, advances and challenges. IEEE Commun. Surv.
    Tutor. 2020, 22, 2462–2488. [Google Scholar] [CrossRef] Kong, X.; Wu, Y.; Wang,
    H.; Xia, F. Edge computing for internet of everything: A survey. IEEE Internet
    Things J. 2022, 9, 23472–23485. [Google Scholar] [CrossRef] Singh, A.; Satapathy,
    S.C.; Roy, A.; Gutub, A. AI-based Mobile Edge Computing for IOT: Applications,
    challenges, and future scope. Arab. J. Sci. Eng. 2022, 47, 9801–9831. [Google
    Scholar] [CrossRef]   Disclaimer/Publisher’s Note: The statements, opinions and
    data contained in all publications are solely those of the individual author(s)
    and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
    disclaim responsibility for any injury to people or property resulting from any
    ideas, methods, instructions or products referred to in the content.  © 2023 by
    the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
    article distributed under the terms and conditions of the Creative Commons Attribution
    (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite
    MDPI and ACS Style Urblik, L.; Kajati, E.; Papcun, P.; Zolotova, I. A Modular
    Framework for Data Processing at the Edge: Design and Implementation. Sensors
    2023, 23, 7662. https://doi.org/10.3390/s23177662 AMA Style Urblik L, Kajati E,
    Papcun P, Zolotova I. A Modular Framework for Data Processing at the Edge: Design
    and Implementation. Sensors. 2023; 23(17):7662. https://doi.org/10.3390/s23177662
    Chicago/Turabian Style Urblik, Lubomir, Erik Kajati, Peter Papcun, and Iveta Zolotova.
    2023. \"A Modular Framework for Data Processing at the Edge: Design and Implementation\"
    Sensors 23, no. 17: 7662. https://doi.org/10.3390/s23177662 Note that from the
    first issue of 2016, this journal uses article numbers instead of page numbers.
    See further details here. Article Metrics Citations No citations were found for
    this article, but you may check on Google Scholar Article Access Statistics Article
    access statistics Article Views 7. Jan 17. Jan 27. Jan 6. Feb 16. Feb 26. Feb
    7. Mar 17. Mar 27. Mar 0 250 500 750 1000 1250 For more information on the journal
    statistics, click here. Multiple requests from the same IP address are counted
    as one view.   Sensors, EISSN 1424-8220, Published by MDPI RSS Content Alert Further
    Information Article Processing Charges Pay an Invoice Open Access Policy Contact
    MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians
    For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum
    MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series
    Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications
    and newsletters from MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel,
    Switzerland) unless otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Modular Framework for Data Processing at the Edge: Design and Implementation'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Shen Z.
  - Jin J.
  - Tan C.
  - Tagami A.
  - Wang S.
  - Li Q.
  - Zheng Q.
  - Yuan J.
  citation_count: '3'
  description: 'Space-air-ground integrated networks (SAGINs) are key elements for
    facilitating high-speed seamless connectivity to the devices/users in infrastructure-less
    environments, where the traditional terrestrial networks are critically infeasible
    or uneconomical to be fully deployed. This article comprehensively surveys the
    advanced computing technologies that support the utilization of SAGINs for infrastructure-less
    environments. The advanced computing technologies refer to the emerging computing
    techniques, tools, and the processes that can be utilized to support SAGINs in
    handling the increasing computing tasks. The main contents include: (1) background
    of SAGINs, (2) typical use cases of SAGINs in infrastructure-less environments,
    (3) advanced computing technologies to assist SAGINs to meet the requirements
    of various services in infrastructure-less environments, (4) the related practical
    initiatives, and (5) the open research challenges and the future research directions.'
  doi: 10.1145/3606018
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Journal Home Just Accepted Latest
    Issue Archive Authors Editors Reviewers About Contact Us HomeACM JournalsACM Computing
    SurveysVol. 56, No. 1A Survey of Next-generation Computing Technologies in Space-air-ground
    Integrated Networks SURVEY SHARE ON A Survey of Next-generation Computing Technologies
    in Space-air-ground Integrated Networks Authors: Zhishu Shen , Jiong Jin , Cheng
    Tan , Atsushi Tagami , + 4 Authors Info & Claims ACM Computing SurveysVolume 56Issue
    1Article No.: 23pp 1–40https://doi.org/10.1145/3606018 Published:28 August 2023Publication
    History 0 citation 1,311 Downloads eReaderPDF ACM Computing Surveys Volume 56,
    Issue 1 Previous Next Abstract 1 INTRODUCTION 2 BACKGROUND 3 USE CASES 4 TRENDS
    AND REQUIREMENTS 5 EMERGING COMPUTING TECHNOLOGIES 6 PRACTICAL PROJECTS AND TRIALS
    7 RESEARCH CHALLENGES AND FUTURE DIRECTIONS 8 CONCLUSION Footnotes REFERENCES
    Index Terms Recommendations Comments Skip Abstract Section Abstract Space-air-ground
    integrated networks (SAGINs) are key elements for facilitating high-speed seamless
    connectivity to the devices/users in infrastructure-less environments, where the
    traditional terrestrial networks are critically infeasible or uneconomical to
    be fully deployed. This article comprehensively surveys the advanced computing
    technologies that support the utilization of SAGINs for infrastructure-less environments.
    The advanced computing technologies refer to the emerging computing techniques,
    tools, and the processes that can be utilized to support SAGINs in handling the
    increasing computing tasks. The main contents include: (1) background of SAGINs,
    (2) typical use cases of SAGINs in infrastructure-less environments, (3) advanced
    computing technologies to assist SAGINs to meet the requirements of various services
    in infrastructure-less environments, (4) the related practical initiatives, and
    (5) the open research challenges and the future research directions. Skip 1INTRODUCTION
    Section 1 INTRODUCTION In the past decade, the heterogeneous data generated by
    massive devices has triggered the rapid development of the telecommunication industry.
    The next-generation data communication networks are expected to accommodate increasing
    data traffic demands for various associated applications, such as those generated
    from the Internet of Things (IoT) devices, which are transforming the digital
    world and driving a new industrial revolution [149]. These IoT devices comprising
    low-cost sensing modules are not only deployed in cities to improve the quality
    of citizens’ lives and cities’ sustainability, but are also expected to be applied
    to remote services such as precision agriculture [211] that monitor infrastructure-less
    environments. Currently, the deployments of terrestrial communications in infrastructure-less
    environments are still critical due to the geographical and economic restrictions,
    e.g., the traditional cellular base stations (BSs) are unrealistic to be fully
    deployed in rural areas [86]. The infrastructure-less environments refer to the
    areas where network communication infrastructure and its associated functions
    such as data storage, electricity supply, and system maintenance are unreliable
    or even unavailable. It is significant to seek a solution to extend the coverage
    of the current mobile communications to satisfy the requirements of various services
    provided in vast remote/rural areas. With the development of telecommunication
    technologies, especially those related to wireless communications, current commercial
    fifth generation (5G) networks have achieved significant advances beyond the long-term
    evolution (LTE) in terms of bandwidth, capacity, and latency [144]. To meet the
    seamless connectivity demands of the future digital society in era of next-generation
    beyond the fifth generation/sixth generation (B5G/6G) wireless network technologies,
    satellite-air-ground integrated networks (SAGINs)1 that interconnect the components
    from space, aerial, and ground networks have attracted increasing attention since
    their potential to establish next-generation mobile networks [91, 200]. Compared
    with the traditional terrestrial networks, SAGINs have the potential to enhance
    network coverage and throughput by leveraging various communication components
    in the non-terrestrial networks. These components include satellites, high altitude
    platform stations (HAPS), and unmanned aerial vehicles (UAVs) that can ensure
    uninterrupted connectivity to a large number of devices in infrastructure-less
    environments. Despite the potential advantages brought by SAGINs in providing
    IoT services in infrastructure-less environments, the following practical challenges
    still need to be addressed [20, 76, 91, 133]: • Restrictions on the available
    resources to be utilized: Besides the shortage of networking infrastructure in
    infrastructure-less environments on the ground site, the onboard computing, communication,
    storage, and power resources provided by SAGINs are also limited for the flying
    components such as UAVs and satellites. • Dynamics of SAGINs systems: The complexity
    of spatial–temporal dynamics of SAGINs associated with the network traffic, network
    topology, and heterogeneous resources availability should be considered. • Requirements
    of IoT services in infrastructure-less environments: Quality of Service (QoS)
    in terms of data communication (e.g., data delivery delay and reliability) and
    data computing (e.g., data processing performance) is indispensable when providing
    IoT service instances for the given applications in infrastructure-less environments.
    SAGINs have become a prominent area of research within the field of computer science,
    as evidenced by the numerous recently published research articles on the topic.
    Table 1 summarizes the state-of-the-art survey and tutorial articles. In general,
    these articles can be classified into three categories: Table 1. Work Year Ground
    NW Aerial NW Space NW IoT Use Case Computing Technology Testbed Highlights [91]
    2018 ✓ ✓ ✓ Network architecture, network design, and communication resources allocation
    [79] 2019 ✓ ✓ Cloud computing, edge computing, heterogeneous resource allocation
    Network architecture and wireless communication technologies [203] 2019 ✓ ✓ MEC
    Wireless communication technologies (integration of UAVs into cellular networks)
    Show More Table 1. Existing Comprehensive Surveys with Their Primary Focus NW:
    Network. • Design of network architecture and networking technologies for SAGINs:
    For example, Reference [91] covers the network architecture development, network
    cross-layer design, and communication resource allocation including spectrum allocation
    and handover management, along with the analysis and optimization of network communication
    performance. Zhang et al. investigated the satellite and mobile technologies including
    radio interface, network management, and security [209]. • Communication technologies
    of UAVs: For example, Reference [203] is a tutorial article that overviews the
    recent advanced UAV communications technologies. It addresses the issues such
    as aerial-terrestrial network interference and demonstrates how to integrate UAVs
    into the forthcoming communication networks. • Communication technologies of satellite
    networks: For example, Centenaro et al. summarized the recent communication technologies,
    standards, and open challenges related to the satellite IoT services [20]. It
    focuses on 5G communication solutions such as open radio access network (O-RAN)
    and network slicing. It also summarizes the recent industrial applications and
    use cases in the area of satellite IoT. As described in Table 1, the existing
    survey and tutorial articles mainly focus on the investigation of SAGINs from
    the aspects of network communication technologies, while the advanced computing
    technologies that are essential to realizing data-driven IoT services on SAGINs
    have not been comprehensively surveyed yet. In addition, several articles have
    previously explored some related computing technologies including MEC and AI for
    enhancing communication performance, such as reducing communication latency and
    improving bandwidth utilization. In this article, the computing technologies involve
    the specialized hardware or software technologies to be applied to SAGINs to process
    the massive computing tasks efficiently. In particular, this article also encompasses
    networking technologies that enable various computing elements to communicate
    with each other and access the heterogeneous resources provided by SAGINs. Despite
    the progress in realizing high-speed communication globally, how to cope with
    the aforementioned practical challenges by computing technologies still needs
    to be addressed. To tackle these challenges, this article investigates the potential
    introduction effectiveness of emerging computing technologies in SAGINs, particularly
    the computing technologies that realize the integration of computing to networking
    and communication functions for SAGINs including computing architecture, computing
    policy, and hardware. This article includes a discussion regarding the computing
    technologies that can be applied to SAGINs for executing the computation tasks
    with various service requirements integrally. It also focuses on the computing
    solution that integrates intelligence into the network from centralized facilities
    to massive end users/devices, in which a user-centric distributed network architecture
    is essential in infrastructure-less environments. In essence, this article is
    a complementary survey covering the recent advancements in emerging computing
    technologies including computing architecture, computing policy, and hardware
    that can be utilized in SAGINs for infrastructure-less environments. It overviews
    the future trends and challenges in realizing data-driven IoT services for SAGINs
    in the era of B5G/6G. The roadmap of this article is summarized in Fig. 1. Fig.
    1. Fig. 1. Roadmap of this article. The main contributions of this article are
    as follows: • We review the recent progress of SAGINs and summarize the state-of-the-art
    solutions that utilize SAGINs for infrastructure-less environments, along with
    the associated use cases. • We identify the trends and requirements of the current
    solutions in SAGINs for infrastructure-less environments and introduce five emerging
    computing technologies to fulfill the requirements. • We summarize the latest
    developments of practical projects and trials empowered by advanced computing
    technologies and outline the potential research challenges with further research
    directions. The remainder of this article is organized as follows: Section 2 overviews
    the background of SAGINs and infrastructure-less environments, along with the
    visions of SAGINs for infrastructure-less environments. Section 3 introduces four
    typical use cases associated with SAGINs served in infrastructure-less environments,
    followed by the trends and technical requirements highlighted in Section 4. Section
    5 reviews the emerging breakthrough computing technologies that are essential
    to meet the aforementioned requirements. Section 6 introduces the related practical
    initiatives. Section 7 discusses the research challenges and future directions.
    Finally, Section 8 concludes this article. The major abbreviations used in this
    article are listed in Table 2. Table 2. Table 2. List of Major Abbreviations Skip
    2BACKGROUND Section 2 BACKGROUND This section first introduces the background
    of SAGINs and infrastructure-less environments and then reviews the vision of
    SAGINs for infrastructure-less environments. 2.1 Satellite-air-ground Integrated
    Networks (SAGINs) Fig. 2 illustrates a typical architecture of SAGINs comprising
    three heterogeneous networks: ground networks, aerial networks, and space networks.
    Each network provides various resources including those for communication, computation,
    and storage. These networks can be operated independently or cooperatively for
    providing comprehensive services and global anytime-anywhere network access [91,
    200]. The characteristic features of typical SAGINs components are summarized
    in Table 3. Table 3. Characteristics Cellular (5G) UAVs/HAPS LEOs MEOs GEOs Altitude
    - 100 m–50 km 160–2,000 km 2,000–35,000 km 35,786 km Range ∼ 100 km 5–200 km 100–500
    km 100–500 km 200–1,000 km RTT ∼ 1 ms ∼ 3 ms 30–50 ms 135–250 ms 560–800 ms Show
    More Table 3. Comparison of Typical Components in SAGINs (Based on References
    [90, 91, 166, 195, 196, 209]) 1 1 Cost of a 5G non-standalone Evolved Packet Core
    for up to 50,000 subscribers [34]. See more information regarding 5G infrastructure
    cost in Reference [123]. 2 2 UAVs available for sale (2022/10) at https://www.amazon.com/.
    3 3 A CubeSat satellite can be lower than 1k USD [140], while the cost of SpaceX
    Starlink satellite is already well below 500k USD [160]. 4 4 The total cost of
    the option and certain related expenditure is between $220M and $250M USD for
    Inmarsat-5 satellite [67]. 5 5 Currently, the majority of satellites only provide
    minimum computing capability for satellite operations. Recently, several LEO satellites
    with powerful computing capability have been launched. See more details in Section
    6. Fig. 2. Fig. 2. Typical architecture of SAGINs. 2.1.1 Ground Networks. Ground
    networks cover the data communication on the ground via the terrestrial communication
    systems such as cellular networks, optical fiber networks, local area networks
    (LANs), and mobile ad hoc networks. As the core sector of ground networks, cellular
    networks provide wireless data communication links to massive users over wide
    land areas, especially the densely populated areas. With rapid advances in wireless
    technologies such as massive multiple-input multiple-output (MIMO) [78] and network
    slicing [207], the commercialization of 5G cellular networks that provide high
    network speed with low latency has already been realized in 2019. Research communities
    are now shifting their attentions towards the B5G/6G mobile networks, which are
    expected to bring transformational changes to the telecommunications industry
    [51]. 2.1.2 Aerial Networks. Wireless communication services in the middle layer
    of SAGINs are provided by aerial networks, which deploy aircraft such as UAVs
    and HAPS. Compared with the ground networks installed with multiple BSs, aerial
    networks could offer wireless access services with a wider coverage on a regional
    basis with low-cost and easy deployment [91]. Specifically, with the explosive
    growth in global UAV production in the past decade, UAVs have been introduced
    for diverse objectives such as enhancing ground networks coverage, providing data
    communication to the ground vehicles, and evacuating victims from disaster areas
    [5]. The use of UAVs is anticipated to enable wireless communication and high-speed
    transmissions to a variety of device types [203]. Meanwhile, due to the unique
    properties of the stratosphere, a HAPS can stay at a quasi-stationary position
    and play a vital role in achieving ubiquitous connectivity. Recent advancements
    in solar panel efficiency, lightweight composite materials, autonomous avionics,
    and antennas have made HAPS a viable component in aerial networks [73]. 2.1.3
    Space Networks. Space networks are composed of satellite systems, which include
    both the satellites and their corresponding terrestrial infrastructure, such as
    ground stations and network operation centers. The satellites can be classified
    based on their orbits as geostationary earth orbit (GEO), medium earth orbit (MEO),
    and low earth orbit (LEO): GEO satellites travel at exactly the same rate as the
    Earth, which makes GEO satellites appear to be stationary over a fixed position.
    A GEO satellite covers a large area of the Earth so three equally spaced GEO satellites
    can provide near global coverage with the exception of the polar regions. MEO
    satellites such as Inmarsat-P and Odyssey are commonly used for navigation and
    mobile communications [209]. Compared with GEO and MEO satellites, LEO satellites
    can achieve higher speed data communication with low latency (see Table 3). With
    the development of satellite manufacturing and launching technologies, massive
    number of LEO satellites are being launched into space to construct LEO satellite
    constellations. Multiple enterprises, such as SpaceX, OneWeb, and Amazon, have
    planned to launch large-scale LEO satellite constellations that include thousands
    of satellites to provide high-throughput broadband services across a global area
    [76]. Meanwhile, the market of LEO CubeSat designed for exploring new space technologies
    is also experiencing rapid growth. The LEO CubeSat provides a complementary connectivity
    solution to the existing terrestrial networks including those for the machine-to-machine
    (M2M) and the IoT services in very sparsely populated areas [140]. 2.2 Infrastructure-less
    Environments Providing universal and affordable Internet access around the world
    is one of the important targets included in sustainable development goals (SDGs)
    declared by the United Nations (UN) [165]. The 5G wireless communication is expected
    to provide networking services with high transmission speed and low latency. Current
    commercial 5G services are mainly delivered to the population density area, e.g.,
    city center with sufficient network infrastructure and connectivity services.
    However, deploying terrestrial infrastructure for providing Internet service in
    infrastructure-less environments still face practical challenges caused by the
    geographical and economic restrictions. In this article, we define infrastructure-less
    environments as the areas where network communication infrastructure and its associated
    functions such as data storage, electricity supply, and system maintenance are
    unreliable or even unavailable. It is worth noting that infrastructure-less environments
    are not only the cases for rural/remote areas in some developing/less-developed
    countries or ocean, but also for some areas in developed countries. For example,
    in 2017, the Internet coverage in Australia was only available to approximately
    31% of its homeland area, not to mention any form of connection to the electrical
    grids [7]. In the US, network operators prioritize urban tower density over ubiquitous
    geographic coverage, and thus users from less populated areas have to compete
    much fewer communication resources (cell towers) for the communication services,
    especially for the rural broadband services [211]. Obviously, the development
    and maintenance of network infrastructure in infrastructure-less environments
    have been stagnant due to the high cost and less return to establish traditional
    cellular networks to serve the sparsely populated and clustered remote rural areas.
    In addition to providing broadband services to the subscribers, it is also essential
    to establish connections for massive IoT devices placed in infrastructure-less
    environments. By utilizing various IoT network architectures including wireless
    sensor networks (WSNs), wireless fidelity networks, and wireless mesh networks
    [128], applications such as precision agriculture, environmental monitoring, and
    wildlife monitoring can be implemented effectively in infrastructure-less environments.
    More details regarding the related typical use cases are summarized in Section
    3. For the remote monitoring services, it is necessary to ensure the collected
    data from the IoT networks can be transferred to cloud server via Internet [8].
    Nevertheless, various factors, especially the lack of network infrastructure,
    impede the spread of remote monitoring services in infrastructure-less environments.
    2.3 SAGINs for Infrastructure-less Environments The future B5G/6G networks are
    expected to provide seamless services continuity and reliability in the global
    scale coverage with low-cost deployments and a low environmental impact [51].
    As one of the core components of B5G/6G networks, SAGINs are strong candidates
    to surpass the traditional communication networks to provide data connectivity
    in infrastructure-less environments [22, 39, 195]. The basic element of SAGINs
    for infrastructure-less environments is the economic backhaul network that provides
    diverse services to the end-user/devices located in the vast remote/rural areas.
    Recently developed low-power wide area network (LPWAN) technologies including
    long range wide area network (LoRaWAN) can realize long distance data connectivity
    (up to 50 km). These technologies are highly demanded for data communication in
    infrastructure-less environments, where the local infrastructure only provides
    limited energy resources [66]. To solve the coverage issue in vast remote/rural
    areas, HAPS and UAVs are deployed to provide backhaul connectivity [126]. For
    example, using inexpensive drones to fetch the data from IoT gateways on the ground
    and then transfer the collected data to the core network as a multi-hop communication.
    Meanwhile, the use of satellites has already proven to be a valuable solution
    in supporting the terrestrial networks and offload tasks to alleviate the communication
    burden [76]. Setup a satellite backhaul is a cost-effective solution when compared
    to deploying a fiber-based network, even for supplying a link bandwidth of 100/50
    Mbps in the first 15 years of network operation [195]. Developments of machine
    type communication (MTC) or M2M communication for global wide area including remote/rural
    areas via satellites are currently a hot segment for satellite providers and funding
    agencies. The advantages of utilizing SAGINs for IoT services are three-way: First,
    SAGINs can facilitate the communications between local IoT nodes and data processing
    devices (e.g., remote servers and data centers), enabling the exchange of data
    and analysis results. Second, SAGINs can enable coordination and collaboration
    among various IoT devices, allowing them to collaboratively support the given
    services such as natural environmental monitoring (see Section 3.2). Finally,
    with the advancements of the emerging computing technologies, SAGINs are expected
    to be evolved to support more complex data analytics by evaluating the collected
    massive data with advanced algorithms in specialized hardware. Therefore, while
    local processing of IoT data can be useful, SAGINs should also be considered as
    essential complementary components that can provide additional benefits and capabilities
    to various IoT services, especially for the cases in infrastructure-less environments.
    By providing a wide-area network connection for terrestrial terminals, SAGINs
    can facilitate the data communication and processing of IoT devices/users located
    in infrastructure-less environments. Taking the application of wildfires detection
    in infrastructure-less environments as an example, the existing solutions [15]
    include: • Satellite imaging: Since its low cost and potential to take high-resolution
    images, satellite-based forest monitoring and fire detection is a popular approach.
    However, fire detection at early stages is not possible by using this method due
    to the fast spread of the fire areas. Moreover, the quality of satellite images
    is highly dependent on the weather conditions. • Local sensing: Mounting surveillance
    cameras and infrared sensors on a ground tower or UAVs is another solution. However,
    installing ground surveillance stations is expensive, while mounting cameras on
    UAVs faces the battery limitation issue during their flights and data processing.
    SAGINs, which integrate the components of ground, aerial, and space-based networks,
    are anticipated to provide solutions to the issues discussed above: Initially,
    a massive number of low-cost, simple-structured, and self-powered IoT devices
    are deployed in the local areas for wildfire monitoring by sending fire detection
    signals. The UAVs regularly collect the sensors’ signals at the respective location
    and enter an alarm mode if a certain amount of warning messages from sensors is
    observed. Meanwhile, satellites can also be used to scan the suspicious areas
    for assisting in the verification on the fire alarm raised by the collected observations.
    In general, SAGINs have the capability of monitoring wildfires in real time and
    detecting potential fire alarms before they develop into more severe situations,
    which makes SAGINs highly effective for detecting and mitigating the impact of
    wildfires. The use of SAGINs for data communication is just one aspect of their
    potential; they also offer computing capabilities for processing data and controlling
    networks, which will be crucial as the number of IoT devices and network diversity
    continue to grow rapidly. This is particularly important in infrastructure-less
    environments where resources are limited and eco-friendliness is a priority [95].
    Specifically, the limitations exist in the computing and communication resources
    of SAGINs, which depend on various factors including the number of nodes in the
    network, available bandwidth, and processing capability of the nodes. Based on
    the real-world implementation/testbed, the following examples demonstrate the
    limitations of computing and communication resources in various layers of SAGINs:
    • Ground layer: Zheng et al. developed an energy-sustainable fog system for mobile
    web services in a national park of 40,000 square kilometers [214], which is a
    typical infrastructure-less environment with limited network communication infrastructure.
    The communication between different fog nodes on this platform is accomplished
    through the Nanobeam, which has a maximum communication range of only 5 kilometers.
    Moreover, a variety of environmental obstacles need to be further considered.
    The platform is powered by a solar system of a 120-Watt solar panel and a 60-AH
    battery to minimally satisfy the daily utilization of a single fog node without
    requiring any charge. In addition, the computing resources that the platform can
    provide are also limited. Specifically, it uses Raspberry Pi to implement the
    information processing function of data computing. For instance, the performance
    of the latest Raspberry Pi 4 Model B is a 1.5 GHz quad-core ARM Cortex-A72 CPU,
    which is unsuitable for various big data processing tasks. • Space layer: The
    data processing capability of current satellites is relatively limited compared
    to the ground-based computing systems. This is primarily due to the challenges
    such as power consumption constraints, limitations on heat dissipation, and the
    physical size of the satellite. As a result, most satellite systems are primarily
    used for satellite operation and data communication purposes. Tiansuan is a recently
    launched open satellite research platform that combines edge computing capabilities
    with remote sensing applications (more details can be found in Section 6). Currently,
    the satellites in orbit in Tiansuan constellation have deployed Atlas 200DK. The
    hardware parameters of Atlas 200DK are as follows: CPU resource capacity is 4/8
    TFLOPS, the storage resource capacity of satellite nodes is 1/2 TB [172]. It is
    essential for SAGINs to dynamically allocate heterogeneous resources among different
    nodes based on their current needs and availability, which improves the overall
    network performance. SAGINs have the potential to become the key element involving
    data creation, collection, computing, and communication for the IoT services provided
    in infrastructure-less environments. The usage of SAGINs needs to be further exploited
    for achieving superior service performance. Skip 3USE CASES Section 3 USE CASES
    SAGINs have already been a key supplement to traditional terrestrial networks
    in terms of data communication in modern mobile networks. For example, SAGINs
    have already been utilized to provide various IoT services in smart cities such
    as surveillance services [74] and autonomous vehicles [156]. Apart from these,
    SAGINs take an important role in various use cases: Table 4 summarizes the recent
    use cases supported by SAGINs, especially the applications provided in infrastructure-less
    environments. It is significant to mention that besides the existing data communication
    function, data sensing empowered by onboard sensors (e.g., received signal strength
    sensors [12] and remote sensing [116, 151]) and data analysis enabled by various
    computing technologies can be addressed for diverse monitoring purposes. Table
    4. Work Year Target Focus IE 1 1 Data Sensing Data Communication Data Analysis
    Testbed [213] 2019 UAVs Emergency networks design ✓ ✓ [103] 2020 UAVs Ocean monitoring
    ✓ ✓ [116] 2020 Satellites Agriculture areas monitoring ✓ ✓ ✓ ✓ Show More Table
    4. Summary of Recent Use Cases Supported by SAGINs 1 1 IE: Infrastructure-less
    Environments As depicted in Fig. 3, this section summarizes the following four
    typical use cases applied to SAGINs for infrastructure-less environments where
    restrictions exist in data processing, cellular networks, electrical supply, and
    information storage: Fig. 3. Fig. 3. Typical use cases that use SAGINs for infrastructure-less
    environments. 3.1 Smart/Precision Agriculture As one of the primary economic industries,
    agriculture plays an important role in global economic development. Smart agriculture
    is an emerging agricultural production paradigm for realizing agricultural intelligence
    and automation [199], while precision agriculture aims to guide decision-making
    over on-site data that assists the management of the crops growing for better
    yield and quality [187]. Both of them are empowered by the advanced technologies
    including cloud computing and WSNs. Nevertheless, there is an urgent need to realize
    prompt remote monitoring and sensing systems over swathes of land daily in agriculture
    [113]. For these agricultural applications, SAGINs can be used for assisting data
    collection and monitoring over vast remote/rural areas. For example, smart/precision
    agriculture services have incorporated the use of UAVs due to their capability
    of capturing diverse data from various ranges, even in unfavorable weather conditions
    [134]: Vasisht et al. developed an IoT platform named FarmBeats for precision
    agriculture, monitoring food storage and animal shelters. In FarmBeats, the purpose
    of using UAVs is to capture the video data reflecting the farm status and transfer
    the recorded video to the gateway, where data processing is executed by incorporating
    the sensing data from the farm and video captured by drones [167]. Other UAV-based
    solutions for precision agriculture include crop stress management, crop yield
    management, and weed management. More details can be found in Reference [113].
    Realizing a sustainable and accurate land monitoring is essential for agriculture
    applications, especially for those served in infrastructure-less environments.
    Analyzing the satellite images that indicate the status of wide geographic areas
    can help improve the service performance of agriculture applications: Cai et al.
    integrated the satellite and climate data to build statistical models based on
    the machine learning methods. The obtained models are then used for wheat yield
    prediction across Australia [17]. Ngyuen et al. established a streaming data processing
    pipeline to collect satellite streaming data. Based on the proposed pipeline,
    a deep neural network (DNN)-based approach is used to analyze the collected data
    for monitoring the rice cultivation areas in Vietnam [116]. 3.2 Natural Environmental
    Monitoring Environmental monitoring that deals with the issues such as air/water
    pollution, climate changes, and hazardous radiation has become an essential role
    for realizing human sustainable development. The development of environmental
    monitoring applications has been enhanced by the availability of large-scale and
    cost-effective environmental sensors provided through WSNs. These sensors offer
    various types of information to create a more comprehensive view of environmental
    conditions. Satellite systems enable the collection and transmission of various
    sensing data regarding the environmental status of the observed infrastructure-less
    environments. For example, Xiao et al. assessed the ground air quality data and
    the satellite-retrieved aerosol optical depth (AOD) data from Aqua and Terra satellites
    to predict particulate matter 2.5 (PM2.5) concentrations in vast territory of
    China [192]. To monitor the offshore wind farm where the turbines are usually
    located in a hard-to-reach environment that requires five maintenance visits per
    year on average, Ullah et al. proposed a solution that uses a local mMTC gateway
    with a backbone over satellite communication, by which reliable communication
    can be realized [164]. Furthermore, cooperating multiple low-cost satellites can
    enhance the data measurement and analysis performance. For example, Ruf et al.
    proposed a radar remote sensing paradigm that utilizes the global positioning
    system (GPS) satellite constellations gathering much more frequent measurements
    for the spaceborne Earth environmental monitoring including wind speed measurements
    and soil moisture monitoring [139]. The functions of remote sensing, data communication,
    integration, and analysis offered by SAGINs can also be utilized for wildlife
    monitoring and assessment, such as wildlife migration monitoring, habitat mapping,
    and endangered species tracking, which can assist in natural resource management
    and conservation activities. For example, the spread of invasive grasses is critical,
    which brings huge unexpected damages to ecosystems and local economies. To detect
    and monitor the invasive grasses in Saguaro National Park in US, Elkind et al.
    analyzed 2 meter resolution DigitalGlobe WorldView-2 satellite imagery with 12
    centimeter resolution UAVs imagery and classified buffelgrass on cloud computing
    platform (Google Earth Engine) [42]. A similar approach utilizing WorldView-2
    satellite imagery for analyzing the Tamarix distribution changes in Grand Canyon
    National Park in US can be found in Reference [14]. The obtained results provide
    valuable directions for local land managers to track the status over the region.
    By using the UAV-based canopy height model, Solano et al. assessed the forest
    size distribution and analyzed their spatial pattern in the Pollino National Park
    in Italy, where no human influence occurred in at least the past 70 years due
    to its remote location [151]. It is worth noting that the satellite’s link capacity
    is limited for transmitting large volumes of environmental data. To increase the
    reliability and availability of satellite link for data transmission, Giuliano
    et al. proposed a fiber-to-the-cabinet (FttC)-based access network solution backhauling
    WSNs to avoid sensor devices to incorporate transmission facilities to directly
    connect to the satellite. Meanwhile, to accommodate latency-sensitive applications,
    local processing of sensor data can be conducted at the gateway located close
    to the sites where the data is collected [53]. 3.3 Disaster Management and Prevention
    It is doubtless that implementing efficient disaster reduction solutions is essential
    to reduce the risks caused by natural disaster phenomena such as earthquake and
    flood. The data collection and communication capabilities provided by SAGINs,
    particularly from the aerial and space networks, allows status measurements and
    evaluation over large-scale phenomena that occur in a vast area. For example,
    mapping and monitoring the floods disaster can be realized by analyzing data obtained
    from multiple satellites equipped with various onboard sensors [143]. Collaboratively
    assessing the remote sensing images observed from satellite, airborne, and ground-based
    facilities could enhance the performance of earthquake disaster information assessment
    [38]. Rapidly establishing a reliable and resilient network is one of the highest
    priorities to conduct disaster management, such as rescue operations in the affected
    areas, especially for those where communication base stations are no longer functioning.
    SAGINs can be utilized as a promising solution for establishing emergency communications:
    UAVs can be deployed to detect incidents on the road, provide the rescue teams
    with precise locations of accidents, and plot the fastest path for interventions
    [122]. To extend the communication coverage of UAVs, multi-hop UAV relaying can
    be utilized for establishing the communication path between the disaster areas
    and outside, with the optimization of UAVs hovering positions [213]. Using satellite
    constellation networks empowered by software defined network (SDN) can realize
    large-scale data evacuating for data located in possibly isolated terrestrial
    systems, i.e., moving parts of data from one data center to others [100]. 3.4
    Ocean Exploration Ocean exploration is used to recognize and manage the changes
    occurring in marine biodiversity, resources, and habitats for sustainable global
    conservation and development [71, 112]. Unlike ground areas, it is challenging
    to densely install massive communication devices such as base stations across
    the ocean, which covers nearly 71% of the Earth’s surface. SAGINs have been introduced
    as a solution to address the challenges of establishing a reliable and efficient
    data communication network in maritime propagation environments: To realize agile
    broadband maritime coverage, shore-based terrestrial-based stations are deployed
    on the high-altitude spots such as mountains along the coastlines. Meanwhile,
    satellites are utilized to achieve communication coverage on a global scale, while
    UAVs can be readily deployed in a flexible manner to provide reliable broadband
    communication services [85, 86, 178]. Specifically, a maritime UAV can fly in
    close proximity to a vessel in the ocean, in order to improve the quality of communication.
    Undersea/underwater wireless sensor networks (UWSNs) are used to acquire quasi/real-time
    data for ocean observation missions. UWSNs are a collection of underwater sensors
    that work similarly to their ground-based counterparts. These sensors monitor
    specific areas of interest and associated events and transmit data using multi-hop
    acoustic communication. The data is then delivered to surface-level sonobuoys,
    which act as sinks, and are stationed on the sea’s surface [33]. In addition,
    autonomous underwater vehicles (AUVs) that travel along the sensors can also be
    used for data collection and transmission to the sonobuoys [103]. For example,
    for UWSNs in underwater pipeline monitoring, deploying AUVs can improve the data
    delivery ratio and reduce the delivery delay as well [69]. In the context of maritime
    communication, UAVs/satellites can act as intermediary nodes that facilitate the
    data transfer between offshore sites and onshore locations where data analysis
    is conducted [103]. Yang et al. designed a multi-layer framework empowered by
    cloud computing for ocean observations, where two data processing algorithms are
    introduced to achieve multi-sensor data fusion for improving data processing quality
    [197]. To improve the performance of ocean exploration services, it is important
    to incorporate advanced computing technologies into SAGINs due to the critical
    conditions of operating in ocean environments. Skip 4TRENDS AND REQUIREMENTS Section
    4 TRENDS AND REQUIREMENTS In the near future, the amount of diverse information
    on networks will continue to grow exponentially, particularly with the influx
    of data from numerous IoT devices. It is crucial to exploit the full range of
    capabilities that SAGINs provide to meet the diverse requirements of various services,
    as summarized below: 4.1 Performance Metrics It is significant to measure the
    overall performance of the IoT services provided by SAGINs. The network metrics
    include latency, jitter, throughput, packet loss, and so on [117], while some
    other parameters such as sustainability, availability (i.e., service coverage)
    and data processing accuracy [150] should also be considered while delivering
    diverse services in infrastructure-less environments. Table 5 summarizes various
    performance metrics adopted in the state-of-the-art research. It is obvious that
    the metrics of communication performance such as latency and throughput are essential
    for the study of heterogeneous resource allocation involving the assignment of
    communication bandwidth [37, 46, 58]. Meanwhile, the metrics such as coverage
    and loss (including data loss, packet loss and path loss) are associated with
    the network topology design [164, 175]. In the realm of data processing, the metrics
    such as data processing accuracy play a significant role in validating the performance
    of a given data processing scheme [153, 197]. Table 5. Work Year Target Focus
    Latency Throughput Loss Utilization Coverage Reliability Accuracy [63] 2019 UAVs
    Scheduling and trajectory optimization ✓ ✓ [197] 2020 Satellites Environmental
    information monitoring ✓ ✓ ✓ [37] 2020 Satellites Resource allocation ✓ ✓ ✓ ✓
    Show More Table 5. Summary of Performance Metrics Related to SAGINs To reduce
    the overall size of data traversed throughout the network, it is necessary to
    exploit the resources provided by the components deployed on the ground site,
    such as MEC in cellular mobile networks directly connected to the Internet via
    the gateway of a radio access network (RAN). Although resource control and orchestration
    over massive MEC servers is applicable for services provided to the areas with
    densely distributed users, the deployment and maintenance of terrestrial networks
    is still economically unrealistic in the vast remote/rural areas [212, 214]. Moreover,
    due to the lack of computational resources provided by the elements in the space
    networks, enormous IoT data collected from infrastructure-less environments have
    to be transferred to the data center at a far distance for data processing and
    analysis. As a result, when providing IoT services in infrastructure-less environments,
    it is critical to realize an efficient network congestion control that guarantees
    satisfactory performance by understanding the complex correlation between network
    topology, routing, and input service tasks. 4.2 System Integration With the evolution
    of communication systems, the mobile networks are becoming service-specific to
    enable various smart IoT services. When designing a service provision scheme,
    it is important to take into account the integration of the system and the impact
    of its network functions on the performance of the overall IoT service provision.
    For example, the selection of multiple communication paths for transferring the
    arriving service tasks could affect some significant network metrics such as service
    latency. On the contrary, it is necessary to meet various QoS requirements of
    the IoT services, e.g., the processing priority should be given to the services
    requiring with higher QoS [76]. Although the scale of recent networks has experienced
    unprecedented growth, there are still existing gaps on the requirements between
    network performance and IoT service provision [59]. Compared with the traditional
    terrestrial networks, SAGINs involve heterogeneous data processing components
    including flying components with a dynamic changing position, which increases
    the complexity of service computing scheme design for system integration. Traditional
    centralized network control based on a scheduled contact plan [80] might not be
    feasible for efficient management over numerous data processing components involving
    large-scale UAVs and LEO satellite constellations. This is because, compared with
    the case in traditional satellite networks involving a limited number of satellites,
    the routing problem in large-scale UAVs/satellites networks is much more complex
    due to dynamic changes in the network environments, such as the time-varying status
    of inter-satellite links (ISLs) and motions of massive satellite. Moreover, the
    communication and computation resources provided by the aerial and satellite networks
    are also limited. Due to the insufficient energy and the limited computing and
    storage capacities, it is still impractical for the current remote sensing satellite
    to perform sensing, computing, and communication functions simultaneously. For
    example, the computing capacity may be affected by the space environment such
    as extreme temperature, sunspots, and electromagnetic interference [59]. Realizing
    the integration of these functions for the network endogenous intelligent perception
    and computing adaptive ability is still challenging for current SAGINs. 4.3 Data-driven
    Paradigm SAGINs are expected to process various types of data collected from multiple
    sources. Taking the use case of smart agriculture as an example, for realizing
    real-time monitoring and management, the potential data includes the sensing data
    collected by ground-based sensor networks and the image or video data captured
    by UAV/satellites [113]. A frequent high-resolution images/videos collection process
    will generate a large amount of data to be processed by SAGINs. To realize data-driven
    paradigm is one of the core objectives for the next-generation communication networks
    that accommodate various IoT applications [24]. The data-driven paradigm herein
    indicates a computational analysis process to produce predictive outcomes from
    the collected big data. This process needs to integrate network operation and
    service provision. A comprehensive data-driven paradigm is expected to achieve
    seamless real-time information acquisition, transmission, interpretation like
    data analysis, and action such as feedback control. Compared with the current
    terrestrial networks, SAGINs aim to achieve superior data processing performance
    due to its ability to collect abundant data globally for analyzing. To alleviate
    the insufficiency of data processing resource available at the remote/rural areas,
    several solutions have been devised to maximize the onboard resources of UAVs/satellites
    for data processing, in addition to their current capabilities for data communication
    and storage. For example, to minimize the cost of onboard data storage, remote
    sensing satellites can perform data preprocessing on the raw data gathered from
    various sources. This data preprocessing may entail integrating, cleaning, and
    eliminating redundant data. It is significant to develop a data-driven paradigm
    that can handle the various IoT services while accommodating their diverse characteristics.
    This includes addressing the varying computing and energy requirements associated
    with different data processing tasks. 4.4 Economic Issues When evaluating the
    expenses associated with SAGINs, it is crucial to take into account not only the
    costs of operating and upgrading existing terrestrial network infrastructure,
    but also the expenses related to installing and maintaining large aerial or satellite
    components. Although the cost for GEO satellite launching and operation is extremely
    expensive in the last several decades, the production and launching cost of a
    LEO satellite have already been attractive to the associated industries. As a
    result, several LEO constellation projects have been launched for providing global
    broadband communications services. Furthermore, HAPS and UAVs require much lower
    deployment and operation cost, since neither terrestrial structure nor rocket
    launch is needed [209]. Moreover, in terms of network control and management,
    SDN-enabled SAGINs can be expected to simplify the network management and achieve
    cost-effective network upgrade/evolution [205]. It is worth noting that the advancement
    of SAGINs is still in its infancy. As a result, the communication and computation
    resources currently available through SAGINs are still restricted, particularly
    those derived from aerial or satellite networks. Hence, while distributing the
    diverse resources offered by SAGINs, it is crucial to take into account the operational
    and rental expenses associated with these valuable resources. This becomes particularly
    important when handling massive IoT service tasks. 4.5 Energy Issues In the light
    of the massive IoT devices expected to be deployed in vast areas including infrastructure-less
    environments, creating a green IoT ecosystem is one of the significant objectives
    to construct a sustainable environment for human beings. Besides the energy consumed
    by operating massive network components, the energy consumption is also linked
    to the factors related to task communication and computation executed at these
    components, which is in portion to the amount of assigned tasks conducted at the
    respective components and data transmission rate [61]. Realizing an energy-efficient
    network operation is always a high priority for SAGINs. The IoT devices installed
    in infrastructure-less environments are often with different battery status and
    channel conditions. In some cases, the overall service performance might be degraded
    due to the disability of some low-battery IoT devices for data acquisition and
    exchange [145]. Moreover, due to the lack of data processing components available
    on the ground site, the energy consumption of SAGINs is largely in relation to
    the volume of data transmitted in the aerial and space networks [26]. For UAVs
    and HAPS, due to their limited energy available for the flight usage, it is significant
    to design the flight paths that minimize energy consumption generated by motion
    while achieving other essential objectives associated with data collection, communication,
    and processing. Recent advancements in satellite onboard digital processing technologies
    are expected to assist the innovative next-generation transmission strategies.
    For example, by introducing flexible routing, channelization, and beamforming,
    the energy efficiency of satellite communication can be significantly enhanced
    [76]. LEO satellites also face energy challenges due to their limited ability
    to generate and store sufficient power to carry out the intended missions. Typically,
    the power supply of LEO satellites relies on solar panels and battery cells [46].
    Specifically, a satellite can be powered by solar panels while it is exposed to
    the sun, otherwise, it can only rely on the battery cells, e.g., during the eclipse
    seasons. Due to the limited number of charging and discharging times of battery
    cells, unrestrained use of energy on a satellite will accelerate the aging of
    satellite batteries and increase the failure probability [92]. Moreover, the need
    to balance power generation with other constraints such as weight, volume, and
    cost should also be considered within the launch budgets. Efforts to enhance the
    energy efficiency of SAGINs through transmission strategies have undergone extensive
    research [70, 76, 138]. However, it is equally important to effectively harness
    the data processing capabilities of various components for efficient energy consumption.
    Specifically, performing the data preprocessing/processing at the intermediate
    components can reduce the amount of data transmitted throughout SAGINs, which
    further reduces the energy consumed by network operations. Skip 5EMERGING COMPUTING
    TECHNOLOGIES Section 5 EMERGING COMPUTING TECHNOLOGIES This section reviews the
    emerging computing technologies that have been/expected to be introduced to SAGINs
    to meet the requirements of various data services. 5.1 Cloud Computing and Edge
    Computing Fig. 4 illustrates two computing solutions that utilize the limited
    computational resources provided by the components of SAGINs: realizing cloud
    computing functions at the GEO satellites and introducing edge computing capability
    to the LEO satellites. In this case, when a LEO satellite receives a data processing
    task, it can either utilize its edge computing capability to preprocess the task
    or transfer it to a neighboring satellite with abundant data computation resources
    for processing. Then, the processed data can be returned to a designated data
    center via ground stations. Meanwhile, the processed data can also be stored at
    the GEO satellites that function as cloud data centers, by which secure data storage
    can be realized without building large, energy-intensive, and ground-based data
    centers [124]. Fig. 4. Fig. 4. Cloud computing and edge computing for SAGINs.
    Currently, B5G/6G networks are designed to natively integrate communication, computing,
    and sensing capabilities to facilitate the centralized intelligence in the cloud,
    which enables workload distribution across devices, networks, and data centers.
    However, unlike the network infrastructure deployed in smart cites equipped with
    abundant data communication and computation resources, it is still critical to
    deploy the network facilities with high computing capability in infrastructure-less
    environments. The massive satellites operated at different orbits can extend the
    connectivity between the cloud data center regions and remote/rural areas. Azure
    space is a project designed to “connect to the cloud anywhere on the planet.”
    The new Azure modular data center is anticipated to receive high-speed and low-latency
    satellite broadband [111]. Furthermore, LyteLoop, a startup, is planning to store
    massive amounts of data by moving it continuously between satellites, i.e., the
    data storage resides on beams of light transmitted between satellites instead
    of in huge and power-hungry cloud servers on the Earth [43]. However, edge computing
    has attracted increasing attention from researchers in the telecommunication field
    owing to its capability of providing various data processing resources in the
    vicinity of end devices. The efficacy of edge computing in enhancing data processing
    performance for real-time analysis has been demonstrated across a range of applications,
    including industrial IoT [129], autonomous driving [94], and smart building [146].
    As a key component to construct 5G mobile network, MEC that enables the extension
    of cloud computing to the edge of networks leveraging mobile BSs has already been
    widely deployed in densely populated areas. Nevertheless, it is not feasible to
    deploy massive base stations in infrastructure-less environments due to economic
    restrictions [86]. The edge computing capability of SAGINs can be partially achieved
    through the implementation of aerial networks or space networks at the component
    level, as shown in Fig. 4. Table 6 summarizes the cloud computing and edge computing
    solutions utilizing SAGINs. This table evaluates each work based on the requirements
    outlined in Section 4. To fully capitalize on the advantages offered by cloud
    computing and edge computing, it is crucial to have an integrated system for managing
    various data services. This integration can lead to improved performance metrics,
    which includes minimizing latency and reducing task completion time. For example:
    Chen et al. demonstrated the benefits of introducing edge-cloud computing to multiple-UAV
    network for improving QoS performance including latency, operation cost reduction,
    and energy efficiency [25]. Yu et al. compared various edge computing technologies
    suitable for SAGINs, which includes orbital edge computing, aerial edge computing,
    and the traditional terrestrial edge computing. The authors also proposed a SAGIN-based
    framework empowered by edge computing to support Internet of Vehicles services
    in remote areas [202]. Zhang et al. summarized several computation offloading
    schemes for deploying MEC servers to satellite networks, by which QoS performance
    including user-perceived delay and energy-efficiency can be enhanced [212]. Similarly,
    Xie et al. proposed a SAGIN-based architecture integrating edge computing networks.
    This architecture leverages satellite edge computing clusters to enable computation
    offloading between user devices and ground data centers [193]. Several developed
    platforms that implement onboard edge computing on satellites are summarized in
    Section 6. Table 6. Work Year Purpose CC 1 1 EC 1 1 Performance Metrics System
    Integration Data-driven Paradigm Economic Energy [25] 2019 UAV Swarm ✓ ✓ ✓ (latency)
    ✓ ✓ ✓ ✓ [212] 2019 Satellite networks ✓ ✓ ∗ ✓ (latency, resource utilization)
    ✓ ✓ ✓ [193] 2020 Satellite networks ✓ ∗ ✓ (latency, throughput) ✓ ✓ ✓ Show More
    Table 6. Summary of Cloud Computing and Edge Computing Solutions Utilizing SAGINs
    1 1 CC: Cloud computing, EC: Edge computing. ∗ It indicates that onboard edge
    computing capability is available at UAVs/satellites. ∗∗ It indicates that onboard
    cloud computing capability is available at satellites. 5.2 Service Provision Service
    computing is a traditional broad research area that contains multiple research
    branches such as Web services, service-oriented architecture, and cloud computing.
    It covers the whole life cycle of service provision: services deployment, discovery,
    composition, migration, delivery, coordination, recommendation, and monitor.2
    In the case of SAGINs scenarios, space service computing acts as a new function
    for service computing [59]. It aims to provide on-demand services across all domains
    for people, things, platforms, environments, and data through the seamless coordination
    of the components in ground, aerial, and space networks. However, the inherent
    characteristics of SAGINs make space service computing to be a challenging topic
    due to the following reasons: First, SAGINs have hybrid architecture with fixed
    infrastructures on the ground and mobile ones over the air and in space, which
    makes service coordination nontrivial. Second, the heterogeneous network protocols
    create natural barriers for different network domains. It is also challenging
    to realize inter-domain cooperation. Third, satellites and UAVs have limited power,
    computing, and storage capabilities, and thus the expected upcoming computation-intensive
    applications like computer vision-based Earth observation may be hardly executed
    on a single node. This necessitates the collaboration among satellites, UAVs,
    and HAPS to achieve optimized scalability and efficiency in SAGINs. Recent research
    on SAGINs has primarily focused on communication or network levels, with few studies
    specifically addressing the service level. Guo et al. presented a service coordination
    framework for SAGINs and proposed service coordination methods at different granularity
    (i.e., fine-grained, medium-grained, and coarse-grained) to guarantee different
    QoS requirements [58]. Li et al. investigated a dynamic service placement scheme
    on the satellites to realize robustness-aware service coverage with constrained
    resources [83]. Wang et al. proposed a reconfigurable service provision framework
    based on service function chaining (SFC) for SAGINs [174]. Zhang et al. developed
    a service function chain mapping path selection method based on delays prediction
    and a network slice scheme for SAGINs [206]. Lyu et al. proposed an online service
    provisioning framework to provision virtualized and micro services to users efficiently
    and flexibly [102]. They also proposed an online spectrum resource slice method
    for isolated vehicular services provisioning. While there have been efforts to
    explore service provision frameworks or micro services within SAGINs, the challenge
    of realizing efficient and effective service provision remains unresolved in current
    literature. 5.3 Computing and Networking Convergence To create B5G/6G communication
    networks, achieving a deep convergence of computing and networking is crucial
    [152], by which SAGINs and the services can be jointly optimized as a single integrated
    system. The use of computing capabilities offered by aerial and space networks
    can serve as a valuable solution for SAGINs in managing the growing volume of
    services required in infrastructure-less environments. To support these distributed
    systems, the interoperable convergence of networking and computation is essential
    [9, 152]. For example, the data communication requires path computation, in which
    the execution depends on the implementation policy and server-side processing.
    Duan et al. investigated how to integrate emerging networking technologies into
    cloud computing and edge computing for improving resource usage efficiency and
    realizing better service provision [40]. As a main driving force for network cloudification,
    network function virtualization (NFV) abstracts the computing and networking infrastructures
    as a common virtualization layer, by which virtual resources can be leveraged
    for realizing virtual network functions. For SAGINs, it is necessary to consider
    how to enable high/medium-performance computation architecture to fit the flying
    components like UAVs/satellites with a constrained energy budget, i.e., the flight
    time of UAVs is limited due to battery and fuel capacity, while the maintenance
    of satellites is on a software basis. To extend the operational lifetime of SAGINs,
    it is crucial to reduce communication distance and data volume. However, this
    reduction comes at the cost of increasing computational expenses. Aiming an optimal/acceptable
    tradeoff between the communication and the computation energy, Thammawichai et
    al. proposed a mixed-integer optimization formulation for hierarchical clustering-based
    self-organizing UAV networks incorporating data aggregation [161]. To realize
    multiple moving targets surveillance utilizing multi-UAV networks, the data needs
    to be efficiently transmitted to UAVs or ground devices with higher computational
    capabilities without compromising the overall networking performance [55]. From
    the viewpoint of satellite networking functionalities, network virtualization
    adopting software defined network (SDN) should also consider the computing and
    forwarding rules to properly route the arriving traffic flow [13]. Carver et al.
    explored the computing and networking solutions for a space cloud platform in
    the space domain [18]. The authors stressed the importance of the decision in
    allocating the computing and storage resources on-site or off-site, while also
    emphasizing the need to consider the cost of data transmission. The authors emphasized
    the importance of the decision in allocating the computing and storage resources
    locally or off-site, while the cost of data transmission should not be neglected.
    The operation of network systems such as SAGINs involves distributed computing
    processing that is utilized across multiple components including end devices located
    on the ground and satellites orbiting in space. To achieve the convergence of
    computing and networking, it is crucial to make use of resources provided by different
    devices and networks. As a result, there has been significant research on managing
    heterogeneous resources in SAGINs, as summarized in Table 7. A typical SAGINs
    resource management problem assigns the available resources, such as computation
    (e.g., VM), communication (e.g., bandwidth), and power to schedule the arriving
    tasks for the given services. The operations of task scheduling and resource allocation
    are performed based on the current metric function value considering the tradeoff
    among various criteria such as energy efficiency, throughput, and communication
    latency. The existing work mainly focuses on the assignment over communication,
    computation, caching, power resources, or multiple of them as a joint resource
    assignment problem. Meanwhile, Table 7 also covers the trajectory management problem,
    which involves considering various environmental factors that may impact their
    movement or flight path. Table 7. Work Year Research Focus Target Latency Energy
    Throughput Cost Resource Utilization [28] 2019 Computational resource and task
    scheduling SAGINs ✓ ✓ ✓ [127] 2019 Networking, caching, and computational resources
    management Satellites ✓ [82] 2020 Task offloading UAVs ✓ Show More Table 7. Summary
    of Recent Representative Work on Heterogeneous Resource Management for SAGINs
    Efficient computing offloading schemes are crucial in managing heterogeneous resources
    for IoT applications deployed in infrastructure-less environments. Computation
    offloading enables IoT end devices to offload computation tasks to devices like
    edge servers that possess the ability to perform edge computing [65, 89]. A communication
    path is set up for a given task using the communication and computation resources
    available across various components of SAGINs. This allows for leveraging the
    computational capabilities of a device through which the path traverses, facilitating
    efficient computing offloading. When designing this scheme, it is important to
    take into account a range of objectives such as minimizing latency, maximizing
    energy efficiency, and ensuring reliability. For example, Cheng et al. proposed
    a joint algorithm including virtual machine (VM) and task scheduling for SAGINs
    to realize an efficient computing offloading at IoT devices and UAVs [28]. Li
    et al. focused on UAV-assisted MEC with the objective to optimize computation
    offloading with minimum energy consumption [82]. Tang et al. proposed a computation
    offloading scheme for LEO satellite network empowered by edge computing [158].
    Regarding the work on power allocation for SAGINs, Liu et al. introduced a power
    allocation optimization for satellite-IoT devices communications utilizing non-orthogonal
    multiple access (NOMA) [97]. An integrated power allocation scheme on caching,
    computing, and communication for terrestrial-satellite systems can be found in
    Reference [47]. 5.4 Artificial Intelligence (AI) As an intellectual backbone to
    support the advanced computing technology, AI has attracted increasing attention
    due to its ability to achieve intelligent processes in a dynamic computing environment.
    Especially, due to the rising quantity of data services that need to be processed
    by SAGINs, the volume of data transmitted among the ground, air, and space is
    expanding rapidly. Processing vast amounts of data using traditional techniques
    is no longer suitable, whereas deep learning algorithms are capable of efficiently
    handling high-dimensional data of this magnitude. As a consequence, deep learning
    techniques have been widely used for providing big data analytics over various
    network services including terrestrial network monitoring and management [198].
    The potential of implementing AI–based procedures in real-world satellite communication
    operations is summarized in Reference [168]. The effectiveness of deep learning
    technology is demonstrated in the automation of related operations. Meanwhile,
    AI can also facilitate the efficient allocation of heterogeneous resources, such
    as those for computation, communication, and power, as summarized in Table 8.
    Taking the example of the work on satellites/UAVs networks empowered by AI, Wu
    et al. proposed a deep supervised learning architecture of CNN to facilitate the
    real-time decisions on caching and trajectory of UAVs for assisting terrestrial
    vehicular networks [188]. Wang et al. used DNN for resource scheduling optimization
    that minimizes the transmission time by jointly optimizing power allocation and
    time slot assignment in satellite systems [171]. Table 8. Work Year Target Purpose
    1 1 AI Approach Latency Throughput Energy Cost Other Criteria [188] 2020 UAVs
    TM & BAM CNN ✓ ✓ [93] 2020 UAVs TM & BAM DQN ✓ ✓ System reward related to QoS
    [37] 2020 Satellites CRM & BAM DQN ✓ ✓ ✓ System utilization Show More Table 8.
    Summary of AI-based Solutions for Different Purposes in SAGINs 1 1 TM: Trajectory
    Management, CRM: Computational Resource Management, BAM: Bandwidth Allocation
    Management, PAM: Power Allocation Management. Among multiple AI solutions, reinforcement
    learning algorithms such as Q-learning, deep Q networks (DQN), and deep deterministic
    policy gradient algorithm (DDPG) are emerging solutions due to their ability to
    train the network by learning from its past experiences and dynamically allocate
    resources based on the current network conditions. Moreover, these algorithms
    can lead to a more optimal allocation of resources for achieving better network
    performance and QoS for the given services, since they involve multi-objective
    optimization, such as jointly maximizing throughput and minimizing latency for
    decision-making. In this field, Liu et al. proposed a DQN-based algorithm for
    improving the QoS for UAVs to serve the mobile terminal users [93]. In Reference
    [37], DQN is applied to enhance resource utilization in intercommunication between
    various satellite systems. Tsuchida et al. proposed a Q-learning algorithm to
    realize an efficient power control for the communication between ground and LEO
    constellation satellites [163]. Other work that utilizes reinforcement learning
    for SAGINs can be found in References [16, 62, 72]. To achieve the desired AI-based
    operations, it is essential to maximize the utilization of the constrained computational
    resources available through SAGINs. 5.5 Hardware Implementing the capability for
    gathering, storing, and analyzing data within elements of SAGINs can effectively
    enable various information services in infrastructure-less environments. In Reference
    [98], a RESTful service model is applied to each sensor, where all sensors and
    devices are organized by the programmable logic controller (PLC) control unit
    and IoT service adapter. The onboard computer such as Raspberry Pi is a suitable
    commercial solution to establish information services in ground networks. A platform
    combined with Arduino Nano for sensors and Raspberry Pi 2 for computing services
    was proposed to monitor soil moisture through data communication between sensors
    and edge devices [68]. Meanwhile, Albanese et al. proposed a smart trap for pest
    detection running a lightweight DNN on Raspberry Pi 3 [4]. Furthermore, the Raspberry
    Pi 3 with long-distance communication bridges is able to achieve mobile web services
    in infrastructure-less environments due to the low energy consumption [214]. The
    requirement of computing capability in ground networks is increasing dramatically,
    since more AI approaches will play a significant role in information processing.
    The hardware associated with the components in aerial and space networks also
    needs to be developed to cope with this trend. Current commercially available
    UAVs can be equipped with multiple high-resolution cameras and empowered by onboard
    data processing capabilities such as data calibration and image matching [189].
    The onboard data processing plays an essential role in UAVs for various services
    including remote sensing, while the emerging field programmable gate arrays (FPGAs)
    and graphics processing units (GPUs) are expected to be designed to be lightweight
    with low energy consumption and adaptable to miniaturized UAVs for onboard processing.
    Moreover, high-performance computing (e.g., the level of cloud computing) is required
    to process the collected data [191]. With the rapid development of commercial
    space launches and modular satellites, deploying a satellite at space is becoming
    more accessible. Enhancing the computational capability to realize sophisticated
    computational intelligence on a satellite is essential for improving the data
    processing services in infrastructure-less environments. Realizing deep learning
    in space can improve the operation of a satellite, e.g., utilizing on-device satellite
    imaging can save at least half of power and improve the transmission latency [77].
    Rapuano et al. reported an in-orbit demonstration of a CNN-based inference on
    LEO satellites applied to hyperspectral images processing [132]. More practical
    projects regarding the recently developed satellite platforms with highly computational
    capability are summarized in Section 6. Meanwhile, due to the harsh and unpredictable
    conditions of the space environments, the satellites must be highly energy-efficient.
    Williams et al. designed an on-chip, fully digital energy monitor for a low-power
    energy monitoring solution that provides just enough performance for intermittent
    computation use cases such as energy harvesting satellites [185]. With a similar
    objective, Resch et al. proposed a non-volatile processing-in-memory architecture
    supporting minimal overhead checkpointing for intermittent computing, and the
    effectiveness of the proposal has been verified in a wide range of temperatures
    with a natural resilience to radiation [135]. Skip 6PRACTICAL PROJECTS AND TRIALS
    Section 6 PRACTICAL PROJECTS AND TRIALS The current operating components in aerial
    and space networks are mostly used for providing data collection and communication
    while the onboard data processing capability is still limited. For example, CryoSat-2
    satellite, launched on April 2010, is used to measure the global environmental
    changes while the measured data are transferred to the computing device on the
    ground for further sophisticated data processing [121, 162]. Empowered by the
    emerging computing technologies summarized in Section 5, an increasing number
    of research focuses on designing the UAVs/satellites for providing massive computational
    resources in the aerial/space networks for SAGINs. This section summarizes the
    practical projects and trails regarding SAGINs, especially for those enabling
    the implementation of AI at the aerial/space networks. Table 9 summarized the
    adopted computing technologies mentioned in Section 5. The primary objective of
    remote sensing satellites such as Landsat9 and NOAA-20 is to acquire sensor data
    and transmit it to ground stations. Consequently, there is less emphasis on having
    high computing power for processing large volumes of data. Meanwhile, the integration
    of cutting-edge computing and networking technologies, along with AI advancements,
    is imperative for communication satellites like Starlink. This will not only enhance
    the overall fleet’s performance but also ensure uninterrupted connectivity for
    their consumers. Recently developed satellite platforms are empowered by onboard
    edge computing due to multiple advantages such as realizing real-time data processing
    and decreasing the amount of data that needs to be transmitted to ground sites,
    by which the costs associated with data transmission can be reduced. More details
    can be found in the latter part of this section. Table 9. Name On-board Edge Computing
    Service Provision Computing & Networking Convergence Artificial Intelligence Hardware
    Landsat 9 [118] ✓ NOAA-20 [180] ✓ ✓ ✓ Starlink ✓ ✓ ✓ ✓ Show More Table 9. Computing
    Technologies Adopted by Representative Projects and Trails Regarding SAGINs 6.1
    Datasets A dataset can provide a structured and organized collection of data that
    can be analyzed to obtain meaningful insights into the domain of this article.
    As a basic solution, several commonly used simulators such as NS-3 and OMNET+
    can be used for generating data for diverse purposes in SAGINs. More details regarding
    the related toolkits can be found in the next subsection. Meanwhile, several open
    datasets regarding the 5G/B5G communications have already been published, such
    as the 5G end-to-end emulation dataset to support 5G network automation [125]
    and the channel estimation dataset for mobile communications [19]. In addition,
    it is essential for SAGINs to fulfill the requirements of various IoT applications.
    A summary of datasets of IoT applications can be found in Reference [99]. In terms
    of big data processing, the real-world data collected by UAVs and SAGINs are also
    vital for the study of computing technologies in SAGINS. Table 10 outlines the
    existing datasets that are pertinent to the scope of this topic. This table includes
    the remote sensing data collected and the flight information of the UAVs and the
    current operating satellites. Table 10. Name Source Target Data Feature Data Size
    1 1 Additional Information Stanford Drone Dataset [137] Drone video 6 categories
    69 GB 60 aerial UAV videos over Stanford campus and bounding boxes FloodNet [131]
    Drone image 10 land-cover categories 12 GB 2,343 images captured by drone (DJI
    Mavic Pro quadcopters) after Hurricane Harvey DOTA-v1.0 [190] Satellite image
    15 common object categories 35 GB 2,806 aerial images. Latest version is DOTA-v2.0
    Show More Table 10. Summary of Existing Datasets Related to SAGINs 1 1 For some
    large-size datasets, this value indicates the compressed file size downloaded
    from the source file sharing site. 6.2 Toolkit The widely known network simulators
    primarily designed for the research/educational usage include NS-2, NS-3, OMNET+,
    NetSim, and optimized network engineering tool (OPNET), which are capable of emulating
    large-scale network simulations including those for SAGINs. These simulators not
    only provide rich models for traffic generation, protocols, and channels, but
    also support development on the software to simulate the computing paradigms such
    as edge computing. As a commercial simulation software for the aerospace field,
    systems tool kit (STK) has been widely utilized in the related studies [106, 110].
    For more non-commercial simulation tools/testbeds specific to SAGINs, Nakama et
    al. proposed a testbed that utilizes machine learning algorithms to procedurally
    generate, scale, and place 3D models to create realistic environments, which allows
    various UAVs and automatic generation [115]. Giuliari et al. developed a LEO satellite
    constellation network routing simulator that can provide deployment design of
    LEO satellite and ground stations [54]. SatEdgeSim is a toolkit for evaluating
    the satellite edge computing environments. It can customize network resource configuration
    and offloading task deployment strategies and also coordinate with STK to customize
    satellite orbit parameters [183]. Cheng et al. presented a SAGINs simulation platform
    that supports the mobility traces and protocols of various networks. Several types
    of controllers are implemented to this platform for network functions optimization
    such as those for access control and resource orchestration [29]. 6.3 Computing
    Platforms for UAVs Current UAV processing units are integrated within the system
    and used to provide flight controller functions, such as controlling the UAV’s
    altitude and mobility, interacting with the sensing and communication modules,
    and other processing. The choice of onboard processor significantly influences
    the computing capabilities of a UAV’s platform. The potential candidate for onboard
    processor includes Raspberry Pi 4 Model B, Nvidia Jetson Nano/TX2, et al. [186].
    Advanced technologies including edge computing and AI can assist the UAVs to provide
    abundant services such as delivery systems, civil infrastructure inspection, precision
    agriculture, and aerial wireless base station [109]. For example, Qualcomm Flight
    Pro is a small-sized drone development platform designed for AI applications.
    It offers advanced connectivity options, integrated perception sensors, and high-performance
    processing system powered by Qualcomm Snapdragon 820 processor, which can be applied
    to various academic and industrial usages [147, 155]. 6.4 Φ-Sat-1 Mission As one
    of the focus experiments of the European Space Agency (ESA) for promoting the
    development of disruptive innovative technologies for the onboard Earth observation
    missions, the ϕ -Sat-1 is the first-ever European onboard AI inference on a dedicated
    chip, which aims to exploit DNN capability for Earth observation. In detail, the
    mission involved the demonstrations of the robustness of the Intel Movidius Myriad
    2 hardware accelerator against ionizing radiation and the development of a Cloudscout
    segmentation neural network run on Myriad 2. This development aims to identify,
    classify the objects in the cloudy images, assessed by the Hyperscount-2 hyperspectral
    sensors. This mission is the first official attempt to successfully implement
    a deep convolutional neural network directly inferencing on a dedicated accelerator
    onboard a satellite, opening a new era of discovery and commercial applications
    driven by the deployment of onboard AI computing technology [52]. 6.5 Orbital
    Edge Computing (OEC) The research group of Carnegie Mellon University proposed
    an OEC architecture that supports edge computing at camera-equipped nano-satellite
    to realize onboard sensed data processing including machine learning and inference
    algorithms, by which the system processing latency can be significantly reduced
    [35, 36]. OEC makes each independent satellite to be an autonomous system, by
    which transforms a constellation of nano-satellites into a sophisticated, orbital
    sensing and data processing infrastructure. Further information on the OEC-based
    satellite design and deployment case studies can be found in Reference [101].
    Alba Orbital and Carnegie Mellon University also planned to launch the OEC (PocketQube
    aboard Alba Cluster 4) in 2022 via SpaceX’s Falcon 9 rocket. 6.6 OrbitsEdge OrbitsEdge
    is a US commercial company that aims to provide high-performance computing solutions
    that bridge the worlds of aerospace and information technology. OrbitsEdge works
    with various companies to provide radiation shielding and thermal management systems
    that allow computers designed for terrestrial applications to function in orbit
    [184]. OrbitsEdge’s first two SatFrame pathfinder satellites will support 18-inch-deep
    hardware with production designs capable to grow to support full-sized 36-inch-deep
    hardware. Onboard Satframe-1 and Satframe-2 will be HPE EL8000 servers. Currently,
    the exact setups for hardware are still being worked out, with different configurations
    to be implemented onboard each satellite to test and verify various CPUs and other
    hardware. 6.7 Tiansuan Tiansuan is an open satellite research platform that was
    initiated by Beijing University of Posts and Telecommunications and Spacety, which
    is a commercial satellite company in China [177]. It aims to bridge the research
    gap by enabling the global academic community to conduct experiments on real satellites.
    The experiments would support various areas include but not limited to 6G core
    network systems, Internet of data, satellite operating system, federated learning
    and AI acceleration, and onboard service capability. Tiansuan plans three phases:
    6 satellites in the first phase, 24 satellites in the second phase, and 300 satellites
    in the last phase. As of May 2023, Tiansuan has launched five satellites Baoyun,
    Lize1, Innovation Raytheon, Yuanguang, and BUPT-1. Tisansuan has deployed many
    on-orbit tests or experiments on the two launched satellites, such as 5G lightweight
    core network, cloud-native satellite, Quick User Datagram Protocol Internet Connection
    (QUIC) protocol verification, and diagnostics over Internet Protocol (DoIP) protocol
    verification. Specifically, Tiansuan conducts the worldwide first deployment test
    of a 5G lightweight core network system on the satellite successfully and performs
    video calls based on the session initiation protocol via the satellite-borne core
    network [194]. Baoyun is the first cloud-native satellite that realizes “Cloud-Edge
    Synergy” in space by the coordination of the ground station (cloud) and the satellite
    (edge). It supports satellite-ground collaborative inference applications based
    on KubeEdge and its AI extension, which improves up to 50% accuracy of in-orbit
    image detection and reduces up to 90% data transmission from the satellite. Tiansuan
    also verifies the connectivity and effectiveness of the Internet of data link
    between the satellite and the ground station. Skip 7RESEARCH CHALLENGES AND FUTURE
    DIRECTIONS Section 7 RESEARCH CHALLENGES AND FUTURE DIRECTIONS This section identifies
    several technical challenges and future directions that enhance SAGINs’ computing
    capability and performance for infrastructure-less environments. 7.1 Collaborative
    In-network Computing To cope with the massive data generated from infrastructure-less
    environments, it is crucial to utilize the computational resources provided by
    each component within SAGINs. As mentioned, powerful AI algorithms can be deployed
    at SAGINs to enhance the data-processing performance, reducing the amount of data
    traversed in the network. However, conducting deep learning algorithms requires
    massive data for analysis, while frequent update of models and related parameters
    are needed as well. Various platforms that perform DNN inference have stringent
    energy consumption, computation, and memory cost limitations [154]. Efficient
    collaborative data computing can be achieved by taking advantage of the connectivity
    between various network segments, especially for SAGINs that have different components
    connected from these segments. Federated learning [84] can link multiple computing
    devices into a decentralized system that utilizes the data collected from individual
    device to assist the training process of AI model, i.e., each device collaborative
    trains a shared model by analyzing the accessible massive datasets. The benefits
    of introducing federated learning to MEC has been discussed in terms of introducing
    collaborative training of deep learning models to enable MEC optimization [88].
    In addition to the factors in the traditional terrestrial networks, the intermittent
    and unpredictable participation of the large-scale flying components in SAGINs
    also impacts the overall performance. Chen et al. investigated the potential ways
    to introduce federated learning in LEO satellite constellation, i.e., implementing
    federated learning to the satellites acting as relay nodes or servers, by which
    the communication overheads and energy consumption can be reduced compared with
    the current solutions [23]. Recent emerging edge intelligence [216] is taking
    shape so AI services can occur close to the data source where data is captured.
    Incorporating edge intelligence to SAGINs can improve the agility of various big
    data services and leverage multiple resources located at the edge of the network
    [210]. Wei et al. proposed a satellite IoT edge intelligent computing architecture.
    It utilizes the computational resources from the cloud to the edge of the network
    to decompose the deep learning tasks. A simulation based on STK analyzes how edge
    intelligence plays a significant role in satellite IoT’s target detection in terms
    of end-to-end delay and throughput [182]. To enhance the performance brought by
    edge intelligence, the authors in Reference [130] proposed a framework that organically
    integrates SAGINs and federated learning. In this framework, aerial computing
    elements such as UAVs can collaboratively train an effective learning model output
    by federated learning. As quoted in the paper, it is significant to consider a
    joint optimization of UAV’s location, resource allocation, and training parameters
    in a decentralized strategy for the system flexibility and robustness. 7.2 Computational
    Resource Utilization Despite the rapid development of manufacturing technologies
    in the past decade, the computational resources provided by SAGIN components remain
    valuable, particularly for space components such as satellites. Upgrading computational
    capabilities in these components is crucial, especially when compared to those
    deployed on the ground [193]. It is therefore important to enhance the utilization
    of these valuable computational resources. The execution on the tasks for the
    same/comparable services with similar input data often yields the same processing
    outputs, thus resulting in redundant computation processes. Computation reuse
    [57] is a emerging paradigm that reuses the existing execution results from previous
    services/functions for the forthcoming computations, by which the utilization
    of the computational resources can be enhanced to meet the application QoS requirements.
    For IoT services provided by SAGINs, a large number of IoT devices on the ground
    site might offload massive similar tasks to various components in SAGINs. For
    example, for the smart/precision agriculture mentioned in Section 3.1, the sensing
    data (such as those for temperature monitoring) collected from the same place
    during a period of time tend to be similar with no anomalous results. To this
    end, utilizing computation reuse can eliminate the execution on duplicate information
    and thus reduce the task execution time and save the computational resource. Recently,
    Al Azad et al. proposed a framework that enables pervasive computation reuse at
    the edge computing devices while imposing marginal overheads on user devices and
    the operation of the network infrastructure [2]. This framework uses unified mechanisms
    for task naming, by which a lightweight identification for similar tasks can be
    conducted. Then such tasks are forwarded towards the computation nodes for capitalizing
    the benefits of computation use. Bellal et al. presented a computation reuse architecture
    for edge computing, which can reuse previous computations while scheduling dependent
    incoming computation offloading tasks [10]. In terms of data privacy, Nour et
    al. designed an architecture that incorporated federated edge computing: It selects
    the appropriate participating devices with quality data to improve the data processing
    performance and reduce communication cost while deploying computation reuse to
    satisfy incoming tasks with less-to-no computation [119]. In the design of a computation
    offloading scheme for SAGINs, it is advisable to incorporate computation reuse
    through in-network computing [104] and cooperative computing [145]. By adopting
    these techniques, a significant improvement in the utilization of computational
    resources across different network elements in SAGINs can be expected. 7.3 Powerful
    Computing Capability A direct solution to enhancing data processing performance
    across the network is to equip each SAGIN component with powerful data computing
    capabilities. As a forceful solution, quantum computing technology based on the
    fundamentals of quantum mechanics is attracting increasing attention from various
    academic and industrial institutions, where inspiring outcomes have been achieved
    recently [60]. For example, a processor with programmable superconducting qubits
    is developed to create quantum states on 53 qubits, which corresponds to a computational
    state-space of dimension 2 53 . The developed processor takes around 200 seconds
    to sample one instance of a quantum circuit a million times, while a state-of-the-art
    classical supercomputer would cost approximately 10,000 years [6]. In the field
    of SAGINs, recent progress focuses on realizing quantum communication that provides
    a highly secure information sharing between two nodes located far away: In 2021,
    a quantum key distribution (QKD)-based network combined over 700 optical fibers
    with two ground-to-satellite links to achieve high-speed communications over a
    total distance of 4,600 kilometers [27]. Meanwhile, quantum optimization inspired
    by quantum computing can be introduced to solve complex tasks for SAGINs. Vista
    et al. utilized quantum annealing to optimize scheduling for UAV-enabled IoT networks
    [170]. Li et al. proposed a reinforcement learning algorithm inspired by the collapse
    phenomenon and amplitude amplification in quantum computation theory to tackle
    the UAV’s trajectory planning problem for UAV-aided wireless networks [87]. It
    is necessary to create SAGINs that are able to interconnect multiple quantum servers
    with the aim of achieving an extraordinary superior computational capability.
    Chiti et al. designed an SDN-based LEO satellite constellation architecture for
    controlling the quantum satellite backbone. This paper reports the evaluations
    that attempt to interconnect two quantum computers through a satellite backbone
    [30]. In consideration of the fact that distributed quantum computing can rely
    on complex communication patterns, a generalized solution should be addressed
    for the future development of SAGINs. 7.4 Robust and Scalable Computing Architecture
    Besides the improvement in the computing capability offered by independent components,
    the establishment of a robust and scalable computing architecture is also essential
    for SAGINs to cope with massive computing tasks efficiently. Containerization
    is a revolutionary approach that facilities the load balancing and server consolidation
    for improving resource utilization in an energy-efficient manner [181]. The advantage
    of containerized deployment of micro-services has been verified in the cloud computing
    services [169]. Furthermore, by performing proper configuration tuning and implementation
    optimization, the docker-based containerization can be deployed to the resource-limited
    network edge devices like Raspberry Pi, where the overhead can be reduced with
    excellent flexibility. The docker-based containerization also enables an easy
    deployment to the existing environments [11]. Nevertheless, besides the development
    of the related management and supportive tools [181], it is also necessary to
    further explore its feasibility on SAGINs, in which a large-scale deployment consisting
    of massive distributed and coordinated components is required. As a computation
    offloading model to improve resource efficiency, serverless computing is also
    an attractive option to both cloud computing and edge computing services. Serverless
    computing owns unique features including rapid auto-scaling, strong isolation,
    fine-grained billing options, and accessibility to massive service systems that
    autonomously handle resource management decisions [105]. Ko et al. proposed an
    energy-efficient task offloading system based on serverless computing for industrial
    IoT that aims to reduce transmission latency. This system can be optimized by
    using a constrained Markov decision process formulation [75]. Cicconetti et al.
    developed a task dispatch framework utilizing serverless edge computing to minimize
    the response time and guarantee both short-term and long-term fairness. Evaluation
    results verify the effectiveness of the framework in handling fast-changing loads
    and network conditions [32]. When designing a task allocation algorithm to deploy
    the serverless computing to SAGINs, it is essential to consider the service characteristics
    in SAINGs and the usage of micro-services with serverless function in each SAGINs’
    component. 7.5 Data Security in Computing Environments Integrated by space, aerial,
    and ground layers, SAGINs are expected to achieve autonomous data collection,
    exchange, and processing across different network segments to bring great convenience
    to users. However, SAGINs also suffer from security issues such as severe privacy
    invasion, reliability issues, and security breaches of SAGINs’ devices [56]. As
    a distributed network system that interconnects heterogeneous devices or nodes
    across different layers, SAGINs are highly compatible with blockchain technology.
    Blockchain is a public list of records that are linked and secured using cryptography
    hash. The underlying cryptography mechanism enables the records in the blockchain
    to resist against modification and achieves trusted data transfer across users
    without a third party. However, to meet the high-performance demands of SAGINs,
    blockchain technology faces challenges such as high computational complexity,
    latency of transactions, and low scalability. Besides, other security and privacy
    requirements, such as mutual authentication, authenticity, integrity, confidentiality,
    availability, and non-repudiation should be assured [179]. Satija et al. proposed
    a high-throughput blockchain named as Blockene to improve the scalability of blockchain
    platform and reduce its resource usage [142]. Its nodes can participate as first-class
    members in consensus while running on devices as lightweight as smartphones. A
    novel split-trust design has been presented with a new security model to achieve
    good performance. Blockchain technology can also be applied to edge computing
    environments. It can fairly and efficiently allocate storage resources on edge
    devices, by which the scalability can be enhanced. Recently, a block storage allocation
    scheme was proposed for quick retrieval of missing blocks [64]. The proposed blockchain
    system can also reach mining consensus with low energy consumption in edge devices
    with a new Proof-of-stake (PoS) mechanism, which is a consensus mechanism used
    in the blockchain networks to validate transactions and create new blocks Another
    important issue that expects to be resolved for SAGINs is how to tackle the threat
    of GPS spoofing attacks. GPS is one of the fundamental services of SAGINs for
    providing positioning and time services to billions of end-users/devices in various
    environments including infrastructure-less environments. However, GPS spoofing
    attacks have also become a growing threat to GPS-dependent systems including aerial/space
    networks in SAGINs [41]. To cope with those attacks, Sathaye et al. presented
    a single antenna GPS receiver capable of tracking legitimate GPS satellite signals
    and estimating the true location even against strong adversaries [141]. The 5G
    new radio (5G-NR) architecture has been developed to enable state-of-the-art positioning
    techniques. The positioning mechanism must not be subverted by adversarial interference,
    such as distance manipulation attacks. Recently, V-Range [148] has been developed
    for UAV networks to execute secure ranging operations resilient to both distance
    enlargement and reduction attacks. Similar strategies should also be considered
    when establishing the future SAGINs. Skip 8CONCLUSION Section 8 CONCLUSION SAGINs
    are expected to play an important role in the forthcoming B5G/6G mobile systems
    in providing satisfactory data service to various applications sourced from the
    massive devices located in vast areas. It is significant to coordinate the components
    of the ground networks, aerial networks, and space networks to enrich SAGINs to
    diverse use cases in infrastructure-less environments. This article signifies
    the emerging computing technologies to meet the needs for maximum utilization
    of the comprehensive functions provided by SAGINs, especially for those to process
    the big data generated from massive IoT devices. It provides a fundamental and
    technical insight on SAGINs to seek a solution on not only the data communication,
    but also the data processing including collaborative onboard data processing empowered
    by the advanced computing technologies such as cloud/edge computing and AI. This
    article also highlights the visionary points towards the gradual development of
    enhancing computing capability on SAGINs to cope with the upcoming big data services.
    The extensive studies covered by this article are expected to give insights into
    the future development of SAGINs towards the next-generation telecommunication
    networks. Footnotes 1 Some other terms including space-aerial-terrestrial integrated
    network [209], hybrid satellite-terrestrial network [45], space-terrestrial integrated
    network [200], integrated terrestrial-aerial-space network [44] also indicate
    a same/similar architecture as SAGINs. In this article, we refer to all of these
    terms as SAGINs, which represent the networks integrating satellite systems, aerial
    networks, and terrestrial communications with the aim to provide various global
    services such as broadband communications and IoT services. Footnote 2 http://tab.computer.org/tcsvc/index.html.
    Footnote REFERENCES [1] Abdu Tedros Salih, Kisseleff Steven, Lagunas Eva, and
    Chatzinotas Symeon. 2021. Flexible resource optimization for GEO multibeam satellite
    communication system. IEEE Trans. Wirel. Commun. 20, 12 (2021), 7888–7902. DOI:
    Reference [2] Azad Md Washik Al and Mastorakis Spyridon. 2022. Reservoir: Named
    data for pervasive computation reuse at the network edge. In Proceedings of the
    IEEE International Conference on Pervasive Computing and Communications (PerCom’22).
    141–151. DOI: Reference [3] Ridhawi Ismaeel Al, Bouachir Ouns, Aloqaily Moayad,
    and Boukerche Azzedine. 2021. Design guidelines for cooperative UAV-supported
    services and applications. Comput. Surv. 54, 9 (2021). DOI: Reference [4] Albanese
    Andrea, Nardello Matteo, and Brunelli Davide. 2021. Automated pest detection with
    DNN on the edge for precision agriculture. IEEE J. Emerg. Select. Topics Circ.
    Syst. 11, 3 (2021), 458–467. Reference [5] Alzahrani Bander, Oubbati Omar Sami,
    Barnawi Ahmed, Atiquzzaman Mohammed, and Alghazzawi Daniyal. 2020. UAV assistance
    paradigm: State-of-the-art in applications and challenges. J. Netw. Comput. Applic.
    166 (2020), 102706. DOI: Reference [6] Arute Frank, Kunal Arya, Ryan Babbush,
    Dave Bacon, Joseph C. Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando
    G. S. L. Brandao, David A. Buell, Brian Burkett, Yu Chen, Zijun Chen, Ben Chiaro,
    Roberto Collins, William Courtney, Andrew Dunsworth, Edward Farhi, Brooks Foxen,
    Austin Fowler, Craig Gidney, Marissa Giustina, Rob Graff, Keith Guerin, Steve
    Habegger, Matthew P. Harrigan, Michael J. Hartmann, Alan Ho, Markus Hoffmann,
    Trent Huang, Travis S. Humble, Sergei V. Isakov, Evan Jeffrey, Zhang Jiang, Dvir
    Kafri, Kostyantyn Kechedzhi, Julian Kelly, Paul V. Klimov, Sergey Knysh, Alexander
    Korotkov, Fedor Kostritsa, David Landhuis, Mike Lindmark, Erik Lucero, Dmitry
    Lyakh, Salvatore Mandrà, Jarrod R. McClean, Matthew McEwen, Anthony Megrant, Xiao
    Mi, Kristel Michielsen, Masoud Mohseni, Josh Mutus, Ofer Naaman, Matthew Neeley,
    Charles Neill, Murphy Yuezhen Niu, Eric Ostby, Andre Petukhov, John C. Platt,
    Chris Quintana, Eleanor G. Rieffel, Pedram Roushan, Nicholas C. Rubin, Daniel
    Sank, Kevin J. Satzinger, Vadim Smelyanskiy, Kevin J. Sung, Matthew D. Trevithick,
    Amit Vainsencher, Benjamin Villalonga, Theodore White, Z. Jamie Yao, Ping Yeh,
    Adam Zalcman, Hartmut Neven, and John M. Martinis. 2019. Quantum supremacy using
    a programmable superconducting processor. Nature 574 (2019), 505–510. DOI: Reference
    [7] (ACCC) Australian Competition and Consumer Commission. 2017. Domestic Mobile
    Roaming Declaration Inquiry - Draft Decision. Technical Report. 1–90. Reference
    [8] Bacco Manlio, Brunori Gianluca, Ferrari Alessio, Koltsida Panagiota, and Toli
    Eleni. 2020. IoT as a digital game changer in rural areas: The DESIRA conceptual
    approach. In Proceedings of the Global Internet of Things Summit (GIoTS). 1–6.
    DOI: Reference [9] Beck Micah, Moore Terry, Luszczek Piotr, and Danalis Anthony.
    2019. Interoperable convergence of storage, networking, and computation. In Proceedings
    of the Future of Information and Communication Conference (FICC’19). 1–15. Reference
    [10] Bellal Zouhir, Nour Boubakr, and Mastorakis Spyridon. 2021. CoxNet: A computation
    reuse architecture at the edge. IEEE Trans. Green Commun. Netw. 5, 2 (2021), 765–777.
    DOI: Reference [11] Bellavista Paolo and Zanni Alessandro. 2017. Feasibility of
    fog computing deployment based on docker containerization over RaspberryPi. In
    Proceedings of the International Conference on Distributed Computing and Networking
    (ICDCN’17). DOI: Reference [12] Bisio Igor, Garibotto Chiara, Haleem Halar, Lavagetto
    Fabio, and Sciarrone Andrea. 2021. On the localization of wireless targets: A
    drone surveillance perspective. IEEE Netw. 35, 5 (2021), 249–255. DOI: Reference
    1Reference 2 [13] Boero Luca, Bruschi Roberto, Davoli Franco, Marchese Mario,
    and Patrone Fabio. 2018. Satellite networking integration in the 5G ecosystem:
    Research trends and open challenges. IEEE Netw. 32, 5 (2018), 9–15. DOI: Reference
    [14] Bransky Nathaniel, Sankey Temuulen, Sankey Joel B., Johnson Matthew, and
    Jamison Levi. 2021. Monitoring tamarix changes using WorldView-2 satellite imagery
    in Grand Canyon National Park, Arizona. Rem. Sens. 13, 5 (2021). DOI: Reference
    [15] Bushnaq Osama M., Chaaban Anas, and Al-Naffouri Tareq Y.. 2021. The role
    of UAV-IoT networks in future wildfire detection. IEEE Internet Things J. 8, 23
    (2021), 16984–16999. DOI: Reference [16] Cai Ting, Zhihua Yang, Yufei Chen, Wuhui
    Chen, Zibin Zheng, Yang Yu, and Hong-Ning Dai. 2022. Cooperative data sensing
    and computation offloading in UAV-assisted crowdsensing with multi-agent deep
    reinforcement learning. IEEE Trans. Netw. Sci. Eng. 9, 5 (2022), 3197–3211. DOI:
    Reference [17] Cai Yaping, Kaiyu Guan, David Lobell, Andries B. Potgieter, Shaowen
    Wang, Jian Peng, Tianfang Xu, Senthold Asseng, Yongguang Zhang, Liangzhi You,
    and Bin Peng. 2019. Integrating satellite and climate data to predict wheat yield
    in Australia using machine learning approaches. Agric. Forest Meteorol. 274 (2019),
    144–159. DOI: Reference [18] Carver Brett, Esposito Timothy, and Lyke James. 2019.
    Cloud-based computation and networking for space. In Proceedings of the SPIE DEFENSE
    + COMMERCIAL SENSING, Vol. 11015. 221–236. DOI: Reference [19] Catak Ferhat Ozgur,
    Kuzlu Murat, Catak Evren, Cali Umit, and Guler Ozgur. 2022. Defensive distillation-based
    adversarial attack mitigation method for channel estimation using deep learning
    models in next-generation wireless networks. IEEE Access 10 (2022), 98191–98203.
    DOI: Reference [20] Centenaro Marco, Costa Cristina E., Granelli Fabrizio, Sacchi
    Claudio, and Vangelista Lorenzo. 2021. A survey on technologies, standards and
    open challenges in satellite IoT. IEEE Commun. Surv. Tutor. 23, 3 (2021), 1693–1720.
    DOI: Reference 1Reference 2Reference 3 [21] Chandra Ranveer, Swaminathan Manohar,
    Chakraborty Tusher, Ding Jian, Kapetanovic Zerina, Kumar Peeyush, and Vasisht
    Deepak. 2022. Democratizing data-driven agriculture using affordable hardware.
    IEEE Micro 42, 1 (Jan.2022), 69–77. Retrieved from https://www.microsoft.com/en-us/research/publication/democratizing-data-driven-agriculture-using-affordable-hardware/.
    Reference [22] Chaoub Abdelaali, Giordani Marco, Lall Brejesh, Bhatia Vimal, Kliks
    Adrian, Mendes Luciano, Rabie Khaled, Saarnisaari Harri, Singhal Amit, Zhang Nan,
    Dixit Sudhir, and Zorzi Michele. 2022. 6G for bridging the digital divide: Wireless
    connectivity to remote areas. IEEE Wirel. Commun. 29, 1 (2022), 160–168. DOI:
    Reference [23] Chen Hao, Xiao Ming, and Pang Zhibo. 2022. Satellite-based computing
    networks with federated learning. IEEE Wirel. Commun. 29, 1 (2022), 78–84. DOI:
    Reference 1Reference 2 [24] Chen Min, Qian Yongfeng, Hao Yixue, Li Yong, and Song
    Jeungeun. 2018. Data-driven computing and caching in 5G networks: Architecture
    and delay analysis. IEEE Wirel. Commun. 25, 1 (2018), 70–75. DOI: Reference [25]
    Chen Wuhui, Liu Baichuan, Huang Huawei, Guo Song, and Zheng Zibin. 2019. When
    UAV swarm meets edge-cloud computing: The QoS perspective. IEEE Netw. 33, 2 (2019),
    36–43. DOI: Reference 1Reference 2 [26] Chen Yali, Ai Bo, Niu Yong, Zhang Hongliang,
    and Han Zhu. 2021. Energy-constrained computation offloading in space-air-ground
    integrated networks using distributionally robust optimization. IEEE Trans. Vehic.
    Technol. 70, 11 (2021), 12113–12125. DOI: Reference [27] Chen Yu-Ao, Qiang Zhang,
    Teng-Yun Chen, Wen-Qi Cai, Sheng-Kai Liao, Jun Zhang, Kai Chen, Juan Yin, Ji-Gang
    Ren, Zhu Chen, Sheng-Long Han, Qing Yu, Ken Liang, Fei Zhou, Xiao Yuan, Mei-Sheng
    Zhao, Tian-Yin Wang, Xiao Jiang, Liang Zhang, Wei-Yue Liu, Yang Li, Qi Shen, Yuan
    Cao, Chao-Yang Lu, Rong Shu, Jian-Yu Wang, Li Li, Nai-Le Liu, Feihu Xu, Xiang-Bin
    Wang, Cheng-Zhi Peng, and Jian-Wei Pan. 2021. An integrated space-to-ground quantum
    communication network over 4,600 kilometres. Nature 589 (2021), 214–219. DOI:
    Reference [28] Cheng Nan, Lyu Feng, Quan Wei, Zhou Conghao, He Hongli, Shi Weisen,
    and Shen Xuemin. 2019. Space/aerial-assisted computing offloading for IoT applications:
    A learning-based approach. IEEE J. Select. Areas Commun. 37, 5 (2019), 1117–1129.
    DOI: Reference 1Reference 2 [29] Cheng Nan, Quan Wei, Shi Weisen, Wu Huaqing,
    Ye Qiang, Zhou Haibo, Zhuang Weihua, Shen Xuemin, and Bai Bo. 2020. A comprehensive
    simulation platform for space-air-ground integrated network. IEEE Wirel. Commun.
    27, 1 (2020), 178–185. DOI: Reference [30] Chiti Francesco, Fantacci Romano, Picchi
    Roberto, and Pierucci Laura. 2022. Mobile control plane design for quantum satellite
    backbones. IEEE Netw. 36, 1 (2022), 91–97. DOI: Reference [31] Chiu Mang Tik,
    Xu Xingqian, Wei Yunchao, Huang Zilong, Schwing Alexander G., Brunner Robert,
    Khachatrian Hrant, Karapetyan Hovnatan, Dozier Ivan, Rose Greg, Wilson David,
    Tudor Adrian, Hovakimyan Naira, Huang Thomas S., and Shi Honghui. 2020. Agriculture-vision:
    A large aerial image database for agricultural pattern analysis. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR’20).
    2825–2835. DOI: Reference [32] Cicconetti Claudio, Conti Marco, and Passarella
    Andrea. 2021. A decentralized framework for serverless edge computing in the internet
    of things. IEEE Trans. Netw. Serv. Manag. 18, 2 (2021), 2166–2180. DOI: Reference
    [33] Coutinho Rodolfo W. L., Boukerche Azzedine, and Loureiro Antonio A. F.. 2019.
    Underwater Networks for Ocean Monitoring: A New Challenge for Topology Control
    and Opportunistic Routing. Springer International Publishing, Cham, 571–601. DOI:
    Reference [34] Dano M.. 2021. Here’s how much a 5G network really costs. Retrieved
    from https://www.lightreading.com/open-ran/heres-how-much-5g-wireless-network-really-costs/d/d-id/769114.
    Reference [35] Denby Bradley and Lucia Brandon. 2019. Orbital edge computing:
    Machine inference in space. IEEE Comput. Archit. Lett. 18, 1 (2019), 59–62. DOI:
    Reference 1Reference 2 [36] Denby Bradley and Lucia Brandon. 2020. Orbital edge
    computing: Nanosatellite constellations as a new class of computer system. In
    Proceedings of the International Conference on Architectural Support for Programming
    Languages and Operating Systems. 939–954. DOI: Reference [37] Deng Boyu, Jiang
    Chunxiao, Yao Haipeng, Guo Song, and Zhao Shanghong. 2020. The next generation
    heterogeneous satellite communication networks: Integration of resource management
    and deep reinforcement learning. IEEE Wirel. Commun. 27, 2 (2020), 105–111. DOI:
    Navigate to [38] Ding Xiang, Wang Xiaoqing, Dou Aixia, Ding Ling, Yuan Xiaoxiang,
    and Wang Shuming. 2021. The development of rapid earthquake disaster assessment
    system based on space-air-ground integrated earth observation. In Proceedings
    of IEEE International Geoscience and Remote Sensing Symposium (IGARSS’21). 8456–8459.
    DOI: Reference [39] Dixit Sudhir, Bhatia Vima, Khanganba Sanjram, and Agrawal
    Anuj. 2022. 6G: Sustainable Development for Rural and Remote Communities. Springer
    Singapore. DOI: Reference [40] Duan Qiang, Wang Shangguang, and Ansari Nirwan.
    2020. Convergence of networking and cloud/edge computing: Status, challenges,
    and opportunities. IEEE Netw. 34, 6 (2020), 148–155. DOI: Reference [41] Eldosouky
    AbdelRahman, Ferdowsi Aidin, and Saad Walid. 2020. Drones in distress: A game-theoretic
    countermeasure for protecting UAVs against GPS spoofing. IEEE Internet Things
    J. 7, 4 (2020), 2840–2854. DOI: Reference [42] Elkind Kaitlyn, Sankey Temuulen
    T., Munson Seth M., and Aslan Clare E.. 2019. Invasive buffelgrass detection using
    high-resolution satellite and UAV imagery on Google Earth Engine. Remote Sens.
    Ecol. Conserv. 5, 4 (2019), 318–331. DOI: Reference [43] Etherington Darrell.
    2021. LyteLoop raises $40 million to launch satellites that use light to store
    data. Retrieved from https://techcrunch.com/2021/02/09/lyteloop-raises-40-million-to-launch-satellites-that-use-light-to-store-data/.
    Reference [44] Fadlullah Zubair Md and Kato Nei. 2021. On smart IoT remote sensing
    over integrated terrestrial-aerial-space networks: An asynchronous federated learning
    approach. IEEE Netw. 35, 5 (2021), 129–135. DOI: [45] Fang Xinran, Feng Wei, Wei
    Te, Chen Yunfei, Ge Ning, and Wang Cheng-Xiang. 2021. 5G Embraces satellites for
    6G ubiquitous IoT: Basic models for integrated satellite terrestrial networks.
    IEEE Internet Things J. 8, 18 (2021), 14399–14417. DOI: [46] Fraire Juan A., Nies
    Gilles, Gerstacker Carsten, Hermanns Holger, Bay Kristian, and Bisgaard Morten.
    2020. Battery-aware contact plan design for LEO satellite constellations: The
    Ulloriaq case study. IEEE Trans. Green Commun. Netw. 4, 1 (2020), 236–245. DOI:
    Reference 1Reference 2Reference 3 [47] Fu Shu, Gao Jie, and Zhao Lian. 2020. Integrated
    resource management for terrestrial-satellite systems. IEEE Trans. Vehic. Technol.
    69, 3 (2020), 3256–3266. DOI: Reference 1Reference 2 [48] Fujita Aito, Sakurada
    Ken, Imaizumi Tomoyuki, Ito Riho, Hikosaka Shuhei, and Nakamura Ryosuke. 2017.
    Damage detection from aerial images via convolutional neural networks. In Proceedings
    of the IAPR International Conference on Machine Vision Applications (MVA’17).
    5–8. DOI: Reference [49] Garnot Vivien Sainte Fare and Landrieu Loic. 2021. Panoptic
    segmentation of satellite image time series with convolutional temporal attention
    networks. Proceedings of the IEEE/CVF International Conference on Computer Vision
    (ICCV’21). 4852–4861. Reference [50] Geraci Giovanni, Garcia-Rodriguez Adrian,
    Azari M. Mahdi, Lozano Angel, Mezzavilla Marco, Chatzinotas Symeon, Chen Yun,
    Rangan Sundeep, and Renzo Marco Di. 2022. What will the future of UAV cellular
    communications be? A flight from 5G to 6G. IEEE Commun. Surv. Tutor. 24, 3 (2022),
    1304–1335. DOI: Reference [51] Giordani Marco, Polese Michele, Mezzavilla Marco,
    Rangan Sundeep, and Zorzi Michele. 2020. Toward 6G networks: Use cases and technologies.
    IEEE Commun. Mag. 58, 3 (2020), 55–61. DOI: Reference 1Reference 2 [52] Giuffrida
    Gianluca, Luca Fanucci, Gabriele Meoni, Matej Batič, Léonie Buckley, Aubrey Dunne,
    Chris van Dijk, Marco Esposito, John Hefele, Nathan Vercruyssen, Gianluca Furano,
    Massimiliano Pastena, and Josef Aschbacher. 2022. The Φ -Sat-1 mission: The first
    on-board deep neural network demonstrator for satellite earth observation. IEEE
    Trans. Geosci. Rem. Sens. 60 (2022), 1–14. DOI: Reference 1Reference 2 [53] Giuliano
    Romeo, Mazzenga Franco, and Vizzarri Alessandro. 2019. Satellite-based capillary
    5G-mMTC networks for environmental applications. IEEE Aerosp. Electron. Syst.
    Mag. 34, 10 (2019), 40–48. DOI: Reference [54] Giuliari Giacomo, Klenze Tobias,
    Legner Markus, Basin David, Perrig Adrian, and Singla Ankit. 2020. Internet backbones
    in space. SIGCOMM Comput. Commun. Rev. 50, 1 (2020), 25–37. Reference [55] Gu
    Jingjing, Su Tao, Wang Qiuhong, Du Xiaojiang, and Guizani Mohsen. 2018. Multiple
    moving targets surveillance based on a cooperative network for multi-UAV. IEEE
    Commun. Mag. 56, 4 (2018), 82–89. DOI: Reference [56] Guo Hongzhi, Li Jingyi,
    Liu Jiajia, Tian Na, and Kato Nei. 2022. A survey on space-air-ground-sea integrated
    network security in 6G. IEEE Commun. Surv. Tutor. 24, 1 (2022), 53–87. DOI: Reference
    1Reference 2 [57] Guo Peizhen, Hu Bo, Li Rui, and Hu Wenjun. 2018. FoggyCache:
    Cross-device approximate computation reuse. In Proceedings of the ACM Annual International
    Conference on Mobile Computing and Networking (MobiCom’18). 19–34. DOI: Reference
    [58] Guo Yan, Li Qing, Li Yuanzhe, Zhang Ning, and Wang Shangguang. 2021. Service
    coordination in the space-air-ground integrated network. IEEE Netw. 35, 5 (2021),
    168–173. DOI: Reference 1Reference 2Reference 3 [59] Guo Yan and Wang Shangguang.
    2021. Challenges and opportunities in space service computing. In Proceedings
    of the IEEE International Conference on Services Computing (SCC’21). 44–51. DOI:
    Reference 1Reference 2Reference 3 [60] Gyongyosi Laszlo and Imre Sandor. 2019.
    A survey on quantum computing technology. Comput. Sci. Rev. 31 (2019), 51–71.
    DOI: Reference [61] He Meilin, Zhong Lei, Tan Huidong, Qu Ying, and Lai Junyu.
    2020. A novel edge computing server selection strategy of LEO constellation broadband
    network. In Proceedings of IEEE World Congress on Services. 275–280. DOI: Reference
    [62] He Ying, Wang Yuhang, Yu F. Richard, Lin Qiuzhen, Li Jianqiang, and Leung
    Victor C. M.. 2021. Efficient resource allocation for multi-beam satellite-terrestrial
    vehicular networks: A multi-agent actor-critic method with attention mechanism.
    IEEE Trans. Intell. Transport. Syst. 23, 3 (2021), 2727–2738. DOI: Reference [63]
    Hu Xiaoyan, Wong Kai-Kit, Yang Kun, and Zheng Zhongbin. 2019. UAV-assisted relaying
    and edge computing: Scheduling and trajectory optimization. IEEE Trans. Wirel.
    Commun. 18, 10 (2019), 4738–4752. DOI: Reference [64] Huang Yaodong, Zhang Jiarui,
    Duan Jun, Xiao Bin, Ye Fan, and Yang Yuanyuan. 2019. Resource allocation and consensus
    on edge blockchain in pervasive edge computing environments. In Proceedings of
    the IEEE International Conference on Distributed Computing Systems (ICDCS’19).
    IEEE, 1476–1486. DOI: Reference [65] Huda S. M. Asiful and Moh Sangman. 2022.
    Survey on computation offloading in UAV-enabled mobile edge computing. J. Netw.
    Comput. Applic. 201, C (2022), 26. DOI: Reference [66] Ikpehai Augustine, Adebisi
    Bamidele, Rabie Khaled M., Anoh Kelvin, Ande Ruth E., Hammoudeh Mohammad, Gacanin
    Haris, and Mbanaso Uche M.. 2019. Low-power wide area network technologies for
    internet-of-things: A comparative review. IEEE Internet Things J. 6, 2 (2019),
    2225–2240. DOI: Reference [67] Inmarsat. 2013. Inmarsat to purchase fourth Inmarsat-5
    satellite from Boeing. Retrieved from https://www.inmarsat.com/en/news/latest-news/corporate/2013/inmarsat-to-purchase-fourth-inmarsat-5-satellite-from-boeing.html.
    Reference [68] Izolan Patric Lincon Ramirez, Rossi Fábio Diniz, Hohemberger Rumenigue,
    Konzen Marcos Paulo, Rodrigues Guilherme da Cunha, Saquette Luiza Rodrigues, Temp
    Daniel Chaves, Lorenzon Arthur Francisco, and Luizelli Marcelo Caggiani. 2020.
    Low-cost fog computing platform for soil moisture management. In Proceedings of
    the International Conference on Information Networking (ICOIN’20). 499–504. Reference
    [69] Jawhar Imad, Mohamed Nader, Al-Jaroodi Jameela, and Zhang Sheng. 2019. An
    architecture for using autonomous underwater vehicles in wireless sensor networks
    for underwater pipeline monitoring. IEEE Trans. Industr. Inform. 15, 3 (2019),
    1329–1340. DOI: Reference [70] Jia Ziye, Sheng Min, Li Jiandong, Zhou Di, and
    Han Zhu. 2021. Joint HAP access and LEO satellite backhaul in 6G: Matching game-based
    approaches. IEEE J. Select. Areas Commun. 39, 4 (2021), 1147–1159. DOI: Reference
    [71] Jiang Shengming. 2021. Networking in oceans: A survey. Comput. Surv. 54,
    1, Article 13 (Jan.2021), 33 pages. DOI: Reference [72] Jung Soyi, Yun Won Joon,
    Kim Joongheon, and Kim Jae-Hyun. 2021. Infrastructure-assisted cooperative multi-UAV
    deep reinforcement energy trading learning for big-data processing. In Proceedings
    of the IEEE International Conference on Information Networking (ICOIN’21). IEEE,
    159–162. Reference [73] Kurt Gunes Karabulut, Khoshkholgh Mohammad G., Alfattani
    Safwan, Ibrahim Ahmed, Darwish Tasneem S. J., Alam Md Sahabul, Yanikomeroglu Halim,
    and Yongacoglu Abbas. 2021. A vision and framework for the high altitude platform
    station (HAPS) networks of the future. IEEE Commun. Surv. Tutor. 23, 2 (2021),
    729–779. DOI: Reference 1Reference 2 [74] Kim Hyunbum, Mokdad Lynda, and Ben-Othman
    Jalel. 2018. Designing UAV surveillance frameworks for smart city and extensive
    ocean with differential perspectives. IEEE Commun. Mag. 56, 4 (2018), 98–104.
    DOI: Reference [75] Ko Haneul, Pack Sangheon, and Leung Victor C. M.. 2021. Performance
    optimization of serverless computing for latency-guaranteed and energy-efficient
    task offloading in energy harvesting industrial IoT. IEEE Internet Things J. 10,
    3 (2021), 1897–1907. DOI: Reference [76] Kodheli Oltjon, Eva Lagunas, Nicola Maturo,
    Shree Krishna Sharma, Bhavani Shankar, Jesus Fabian Mendoza Montoya, Juan Carlos
    Merlano Duncan, Danilo Spano, Symeon Chatzinotas, Steven Kisseleff, Jorge Querol,
    Lei Lei, Thang X. Vu, and George Goussetis. 2021. Satellite communications in
    the new space era: A survey and future challenges. IEEE Commun. Surv. Tutor. 23,
    1 (2021), 70–109. DOI: Navigate to [77] Kothari Vivek, Liberis Edgar, and Lane
    Nicholas D.. 2020. The final frontier: Deep learning in space. In Proceedings
    of the International Workshop on Mobile Computing Systems and Applications. 45–49.
    DOI: Reference [78] Larsson Erik G., Edfors Ove, Tufvesson Fredrik, and Marzetta
    Thomas L.. 2014. Massive MIMO for next generation wireless systems. IEEE Commun.
    Mag. 52, 2 (2014), 186–195. DOI: Reference [79] Li Bin, Fei Zesong, and Zhang
    Yan. 2019. UAV communications for 5G and beyond: Recent advances and future trends.
    IEEE Internet Things J. 6, 2 (2019), 2241–2263. DOI: Reference [80] Li Jian, Lu
    Hancheng, Xue Kaiping, and Zhang Yongdong. 2019. Temporal Netgrid Model-based
    dynamic routing in large-scale small satellite networks. IEEE Trans. Vehic. Technol.
    68, 6 (2019), 6009–6021. DOI: Reference [81] Li Kai, Ni Wei, Yuan Xin, Noor Alam,
    and Jamalipour Abbas. 2022. Deep-graph-based reinforcement learning for joint
    cruise control and task offloading for aerial edge internet of things (EdgeIoT).
    IEEE Internet Things J. 9, 21 (2022), 21676–21686. DOI: Reference [82] Li Mushu,
    Cheng Nan, Gao Jie, Wang Yinlu, Zhao Lian, and Shen Xuemin. 2020. Energy-efficient
    UAV-assisted mobile edge computing: Resource allocation and trajectory optimization.
    IEEE Trans. Vehic. Technol. 69, 3 (2020), 3424–3438. DOI: Reference 1Reference
    2 [83] Li Qing, Wang Shangguang, Ma Xiao, Sun Qibo, Wang Houpeng, Cao Suzhi, and
    Yang Fangchun. 2022. Service coverage for satellite edge computing. IEEE Internet
    Things J. 9, 1 (2022), 695–705. DOI: Reference 1Reference 2 [84] Li Tian, Sahu
    Anit Kumar, Talwalkar Ameet, and Smith Virginia. 2020. Federated learning: Challenges,
    methods, and future directions. IEEE Signal Process. Mag. 37, 3 (2020), 50–60.
    DOI: Reference [85] Li Xiangling, Feng Wei, Chen Yunfei, Wang Cheng-Xiang, and
    Ge Ning. 2020. Maritime coverage enhancement using UAVs coordinated with hybrid
    satellite-terrestrial networks. IEEE Trans. Commun. 68, 4 (2020), 2355–2369. DOI:
    Reference [86] Li Xiangling, Feng Wei, Wang Jue, Chen Yunfei, Ge Ning, and Wang
    Cheng-Xiang. 2020. Enabling 5G on the ocean: A hybrid satellite-UAV-terrestrial
    network solution. IEEE Wirel. Commun. 27, 6 (2020), 116–121. DOI: Reference 1Reference
    2Reference 3 [87] Li Yuanjian, Aghvami A. Hamid, and Dong Daoyi. 2021. Intelligent
    trajectory planning in UAV-mounted wireless networks: A quantum-inspired reinforcement
    learning perspective. IEEE Wirel. Commun. Lett. 10, 9 (2021), 1994–1998. DOI:
    Reference [88] Lim Wei Yang Bryan, Luong Nguyen Cong, Hoang Dinh Thai, Jiao Yutao,
    Liang Ying-Chang, Yang Qiang, Niyato Dusit, and Miao Chunyan. 2020. Federated
    learning in mobile edge networks: A comprehensive survey. IEEE Commun. Surv. Tutor.
    22, 3 (2020), 2031–2063. DOI: Reference [89] Lin Hai, Zeadally Sherali, Chen Zhihong,
    Labiod Houda, and Wang Lusheng. 2020. A survey on computation offloading modeling
    for edge computing. J. Netw. Comput. Applic. 169 (2020), 102781. DOI: Reference
    [90] Lin Xingqin, Hofström Björn, Wang Y.-P. Eric, Masini Gino, Maattanen Helka-Liina,
    Rydén Henrik, Sedin Jonas, Stattin Magnus, Liberg Olof, Euler Sebastian, Muruganathan
    Siva, Löwenmark Stefan Eriksson, and Khan Talha. 2021. 5G New Radio Evolution
    Meets Satellite Communications: Opportunities, Challenges, and Solutions. Springer
    International Publishing, 517–531. DOI: Reference [91] Liu Jiajia, Shi Yongpeng,
    Fadlullah Zubair Md., and Kato Nei. 2018. Space-air-ground integrated network:
    A survey. IEEE Commun. Surv. Tutor. 20, 4 (2018), 2714–2741. DOI: Navigate to
    [92] Liu Jiahao, Zhao Baokang, Xin Qin, Su Jinshu, and Ou Wei. 2021. DRL-ER: An
    intelligent energy-aware routing protocol with guaranteed delay bounds in satellite
    mega-constellations. IEEE Trans. Netw. Sci. Eng. 8, 4 (2021), 2872–2884. DOI:
    Reference [93] Liu Qian, Shi Long, Sun Linlin, Li Jun, Ding Ming, and Shu Feng.
    2020. Path planning for UAV-mounted mobile edge computing with deep reinforcement
    learning. IEEE Trans. Vehic. Technol. 69, 5 (2020), 5723–5728. DOI: Reference
    1Reference 2 [94] Liu Shaoshan, Liu Liangkai, Tang Jie, Yu Bo, Wang Yifan, and
    Shi Weisong. 2019. Edge computing for autonomous driving: Opportunities and challenges.
    Proc. IEEE 107, 8 (2019), 1697–1716. DOI: Reference [95] Liu Xilong and Ansari
    Nirwan. 2019. Toward green IoT: Energy solutions and key challenges. IEEE Commun.
    Mag. 57, 3 (2019), 104–110. DOI: Reference [96] Liu Xiao, Liu Yuanwei, and Chen
    Yue. 2021. Machine learning empowered trajectory and passive beamforming design
    in UAV-RIS wireless networks. IEEE J. Select. Areas Commun. 39, 7 (2021), 2042–2055.
    DOI: Reference [97] Liu Xin, Zhai Xiangping Bryce, Lu Weidang, and Wu Celimuge.
    2021. QoS-guarantee resource allocation for multibeam satellite industrial internet
    of things with NOMA. IEEE Trans. Industr. Inform. 17, 3 (2021), 2052–2061. DOI:
    Reference [98] Liu Zheng, Beibei Yu, Zhibo Chen, Jiqin Peng, Qiang Feng, Qing
    Liu, and Cheng Xie. 2018. A case study of service-centric IoT model for rural
    sewage disposal. In Proceedings of the IEEE International Conference on e-Business
    Engineering (ICEBE’18). 133–138. DOI: Reference [99] Lohiya Ritika and Thakkar
    Ankit. 2021. Application domains, evaluation data sets, and research challenges
    of IoT: A systematic review. IEEE Internet Things J. 8, 11 (2021), 8774–8798.
    DOI: Reference [100] Lourenco Rafael B. R., Figueiredo Gustavo B., Tornatore Massimo,
    and Mukherjee Biswanath. 2019. Data evacuation from data centers in disaster-affected
    regions through software-defined satellite networks. Comput. Netw. 148 (2019),
    88–100. DOI: Reference [101] Lucia Brandon, Denby Brad, Manchester Zachary, Desai
    Harsh, Ruppel Emily, and Colin Alexei. 2021. Computational nanosatellite constellations:
    Opportunities and challenges. GetMobile: Mob. Comput. Commun. 25, 1 (June2021),
    16–23. DOI: Reference [102] Lyu Feng, Wu Fan, Zhang Yongmin, Xin Jiang, and Zhu
    Xueling. 2020. Virtualized and micro services provisioning in space-air-ground
    integrated networks. IEEE Wirel. Commun. 27, 6 (2020), 68–74. DOI: Reference [103]
    Ma Ruofei, Wang Ruisong, Liu Gongliang, Chen Hsiao-Hwa, and Qin Zhiliang. 2020.
    UAV-assisted data collection for ocean monitoring networks. IEEE Netw. 34, 6 (2020),
    250–258. DOI: Reference 1Reference 2Reference 3 [104] Mai Tianle, Yao Haipeng,
    Guo Song, and Liu Yunjie. 2021. In-network computing powered mobile edge: Toward
    high performance industrial IoT. IEEE Netw. 35, 1 (2021), 289–295. DOI: Reference
    [105] Mampage Anupama, Karunasekera Shanika, and Buyya Rajkumar. 2022. A holistic
    view on resource management in serverless computing environments: Taxonomy and
    future directions. Comput. Surv. (2022), 1–34. DOI: Reference [106] Marcano Néstor
    J. Hernández, Bartle Hannes, and Jacobsen Rune Hylsberg. 2020. Patch antenna arrays
    beam steering for enhanced LEO nanosatellite communications. In Proceedings of
    the IEEE Wireless Communications and Networking Conference (WCNC’20). 1–6. DOI:
    Reference [107] Martinez Gonzalo J., Dubrovskiy Grigoriy, Zhu Shangyue, Mohammed
    Alamin, Lin Hai, Laneman J. Nicholas, Striegel Aaron D., Pragada Ravikumar V.,
    and Castor Douglas R.. 2021. An open, real-world dataset of cellular UAV communication
    properties. In Proceedings of the International Conference on Computer Communications
    and Networks (ICCCN’21). 1–6. DOI: Reference [108] Masaracchia Antonino, Nguyen
    Long D., Duong Trung Q., Yin Cheng, Dobre Octavia A., and Garcia-Palacios Emiliano.
    2020. Energy-efficient and throughput fair resource allocation for TS-NOMA UAV-assisted
    communications. IEEE Trans. Commun. 68, 11 (2020), 7156–7169. DOI: Reference [109]
    McEnroe Patrick, Wang Shen, and Liyanage Madhusanka. 2022. A survey on the convergence
    of edge computing and AI for UAVs: Opportunities and challenges. IEEE Internet
    Things J. 9, 17 (2022), 15435–15459. DOI: Reference [110] Miao Jiansong, Wang
    Pengjie, Yin Haoqiong, Chen Ningyu, and Wang Xianghe. 2019. A multi-attribute
    decision handover scheme for LEO mobile satellite networks. In Proceedings of
    the IEEE 5th International Conference on Computer and Communications (ICCC’19).
    938–942. DOI: Reference [111] Microsoft. Azure Space. 2023. Retrieved from https://azure.microsoft.com/en-us/solutions/space.
    Reference [112] Miloslavich Patricia, Sophie Seeyave, Frank Muller-Karger, Nicholas
    Bax, Elham Ali, Claudia Delgado, Hayley Evers-King, Benjamin Loveday, Vivian Lutz,
    Jan Newton, Glenn Nolan, Ana C. Peralta Brichtova, Christine Traeger-Chatterjee,
    and Edward Urban. 2019. Challenges for global ocean observation: The need for
    increased human capacity. J. Operat. Oceanograph. 12, sup2 (2019), S137–S156.
    DOI: Reference [113] Mukherjee Anandarup, Misra Sudip, and Raghuwanshi Narendra
    Singh. 2019. A survey of unmanned aerial sensing solutions in precision agriculture.
    J. Netw. Comput. Applic. 148 (2019), 102461. DOI: Reference 1Reference 2Reference
    3 [114] Na Zhenyu, Zhang Mengshu, Wang Jun, and Gao Zihe. 2020. UAV-assisted wireless
    powered Internet of Things: Joint trajectory optimization and resource allocation.
    Ad Hoc Netw. 98 (2020), 102052. DOI: Reference [115] Nakama Justin, Parada Ricky,
    Matos-Carvalho João P., Azevedo Fábio, Pedro Dário, and Campos Luís. 2021. Autonomous
    environment generator for UAV-based simulation. Appl. Sci. 11, 5 (2021). DOI:
    Reference [116] Nguyen Thanh Tam, Hoang Thanh Dat, Pham Minh Tam, Vu Tuyet Trinh,
    Nguyen Thanh Hung, Huynh Quyet-Thang, and Jo Jun. 2020. Monitoring agriculture
    areas with satellite images and deep learning. Appl. Soft Comput. 95 (2020), 106565.
    DOI: Reference 1Reference 2Reference 3 [117] Niephaus Christian, Kretschmer Mathias,
    and Ghinea Gheorghita. 2016. QoS provisioning in converged satellite and terrestrial
    networks: A survey of the state-of-the-art. IEEE Commun. Surv. Tutor. 18, 4 (2016),
    2415–2441. DOI: Reference [118] Niroumand-Jadidi Milad, Legleiter Carl J., and
    Bovolo Francesca. 2022. River bathymetry retrieval from Landsat-9 images based
    on neural networks and comparison to SuperDove and Sentinel-2. IEEE J. Select.
    Topics Appl. Earth Observ. Rem. Sens. 15 (2022), 5250–5260. DOI: Reference [119]
    Nour Boubakr, Cherkaoui Soumaya, and Mlika Zoubeir. 2021. Federated learning and
    proactive computation reuse at the edge of smart homes. IEEE Trans. Netw. Sci.
    Eng. 9, 5 (2021), 3045–3056. DOI: Reference [120] Scientists Union of Concerned.
    2023. UCS satellite database. Retrieved from https://www.ucsusa.org/resources/satellite-database.
    Reference [121] Olsen Nils, Albini Giuseppe, Bouffard Jerome, Parrinello Tommaso,
    and Toffner-Clausen Lars. 2020. Magnetic observations from CryoSat-2: Calibration
    and processing of satellite platform magnetometer data. Earth, Planets Space 72,
    48 (2020). DOI: Reference [122] Oubbati Omar, Lakas Abderrahmane, Lorenz Pascal,
    Atiquzzaman Mohammed, and Jamalipour Abbas. 2019. Leveraging communicating UAVs
    for emergency vehicle guidance in urban areas. IEEE Trans. Emerg. Topics Comput.
    9, 2 (072019), 1070–1082. DOI: Reference [123] Oughton Edward J. and Frias Zoraida.
    2018. The cost, coverage and rollout implications of 5G infrastructure in Britain.
    Telecommun. Polic. 42, 8 (2018), 636–652. DOI: Reference [124] Peng Cong, He Yuanzhi,
    Zhao Shanghong, Song Lingyang, and Deng Boyu. 2022. Integration of data center
    into the distributed satellite cluster networks: Challenges, techniques, and trends.
    IEEE Netw. (2022), 1–8. DOI: Reference 1Reference 2 [125] Phung Chi-Dung, Yellas
    Nour-El-Houda, Ruba Salah Bin, and Secci Stefano. 2022. An open dataset for Beyond-5G
    data-driven network automation experiments. In Proceedings of the International
    Conference on 6G Networking (6GNet’22). 1–4. DOI: Reference [126] Qin Yujie, Kishk
    Mustafa A., and Alouini Mohamed-Slim. 2022. Drone charging stations deployment
    in rural areas for better wireless coverage: Challenges and solutions. IEEE Internet
    Things Mag. 5, 1 (2022), 148–153. DOI: Reference [127] Qiu Chao, Yao Haipeng,
    Yu F. Richard, Xu Fangmin, and Zhao Chenglin. 2019. Deep Q-learning aided networking,
    caching, and computing resources allocation in software-defined satellite-terrestrial
    networks. IEEE Trans. Vehic. Technol. 68, 6 (2019), 5871–5883. DOI: Reference
    [128] Qiu Tie, Chen Ning, Li Keqiu, Atiquzzaman Mohammed, and Zhao Wenbing. 2018.
    How can heterogeneous internet of things build our future: A survey. IEEE Commun.
    Surv. Tutor. 20, 3 (2018), 2011–2027. DOI: Reference [129] Qiu Tie, Chi Jiancheng,
    Zhou Xiaobo, Ning Zhaolong, Atiquzzaman Mohammed, and Wu Dapeng Oliver. 2020.
    Edge computing in industrial internet of things: Architecture, advances and challenges.
    IEEE Commun. Surv. Tutor. 22, 4 (2020), 2462–2488. DOI: Reference [130] Qu Yuben,
    Dong Chao, Zheng Jianchao, Dai Haipeng, Wu Fan, Guo Song, and Anpalagan Alagan.
    2021. Empowering edge intelligence by air-ground integrated federated learning.
    IEEE Netw. 35, 5 (2021), 34–41. DOI: Reference [131] Rahnemoonfar Maryam, Chowdhury
    Tashnim, Sarkar Argho, Varshney Debvrat, Yari Masoud, and Murphy Robin Roberson.
    2021. FloodNet: A high resolution aerial imagery dataset for post flood scene
    understanding. IEEE Access 9 (2021), 89644–89654. DOI: Reference [132] Rapuano
    Emilio, Meoni Gabriele, Pacini Tommaso, Dinelli Gianmarco, Furano Gianluca, Giuffrida
    Gianluca, and Fanucci Luca. 2021. An FPGA-based hardware accelerator for CNNs
    inference on board satellites: Benchmarking with Myriad 2-based solution for the
    CloudScout case study. Rem. Sens. 13, 8 (2021). DOI: Reference [133] Ray Partha
    Pratim. 2021. A review on 6G for space-air-ground integrated network: Key enablers,
    open challenges, and future direction. J. King Saud Univ. - Comput. Inf. Sci.
    34, 9 (2021). DOI: Reference 1Reference 2 [134] Maddikunta Praveen Kumar Reddy,
    Hakak Saqib, Alazab Mamoun, Bhattacharya Sweta, Gadekallu Thippa Reddy, Khan Wazir
    Zada, and Pham Quoc-Viet. 2021. Unmanned aerial vehicles in smart agriculture:
    Applications, requirements, and challenges. IEEE Sensors J. 21, 16 (2021), 17608–17619.
    DOI: Reference [135] Resch Salonik, Khatamifard S. Karen, Chowdhury Zamshed I.,
    Zabihi Masoud, Zhao Zhengyang, Cilasun Husrev, Wang Jian-Ping, Sapatnekar Sachin
    S., and Karpuzcu Ulya R.. 2022. Energy efficient and reliable inference in nonvolatile
    memory under extreme operating conditions. ACM Trans. Embed. Comput. Syst. 21,
    5 (2022), 1–36. DOI: Reference [136] Rinaldi Federica, Maattanen Helka-Liina,
    Torsner Johan, Pizzi Sara, Andreev Sergey, Iera Antonio, Koucheryavy Yevgeni,
    and Araniti Giuseppe. 2020. Non-terrestrial networks in 5G & beyond: A survey.
    IEEE Access 8 (2020), 165178–165200. DOI: Reference [137] Robicquet Alexandre,
    Sadeghian Amir, Alahi Alexandre, and Savarese Silvio. 2016. Learning social etiquette:
    Human trajectory understanding in crowded scenes. In Proceedings of the European
    Conference on Computer Vision (ECCV’16). 549–565. DOI: Reference [138] Ruan Yuhan,
    Li Yongzhao, Wang Cheng-Xiang, and Zhang Rui. 2018. Energy efficient adaptive
    transmissions in integrated satellite-terrestrial networks with SER constraints.
    IEEE Trans. Wirel. Commun. 17, 1 (2018), 210–222. DOI: Reference [139] Ruf Christopher
    S., Chew Clara, Lang Timothy, Morris Mary G., Nave Kyle, Ridley Aaron, and Balasubramaniam
    Rajeswari. 2018. A new paradigm in earth environmental monitoring with the CYGNSS
    small satellite constellation. Scient. Rep. 29, 1 (2018), 1–13. DOI: Reference
    [140] Saeed Nasir, Elzanaty Ahmed, Almorad Heba, Dahrouj Hayssam, Al-Naffouri
    Tareq Y., and Alouini Mohamed-Slim. 2020. CubeSat communications: Recent advances
    and future challenges. IEEE Commun. Surv. Tutor. 22, 3 (2020), 1839–1862. DOI:
    Reference 1Reference 2 [141] Sathaye Harshad, LaMountain Gerald, Closas Pau, and
    Ranganathan Aanjhan. 2022. SemperFi: Anti-spoofing GPS receiver for UAVs. In Proceedings
    of the Network and Distributed Systems Security Symposium (NDSS’22). 1–17. Reference
    [142] Satija Sambhav, Mehra Apurv, Singanamalla Sudheesh, Grover Karan, Sivathanu
    Muthian, Chandran Nishanth, Gupta Divya, and Lokam Satya. 2020. Blockene: A high-throughput
    blockchain over mobile devices. In Proceedings of the USENIX Symposium on Operating
    Systems Design and Implementation (OSDI’20). 567–582. Reference [143] Schumann
    Guy J.-P., Brakenridge G. Robert, Kettner Albert J., Kashif Rashid, and Niebuhr
    Emily. 2018. Assisting flood disaster response with earth observation data and
    products: A critical assessment. Rem. Sens. 10, 8 (2018). DOI: Reference [144]
    Shafi Mansoor, Andreas F. Molisch, Peter J. Smith, Thomas Haustein, Peiying Zhu,
    Prasan De Silva, Fredrik Tufvesson, Anass Benjebbour, and Gerhard Wunder. 2017.
    5G: A tutorial overview of standards, trials, challenges, deployment, and practice.
    IEEE J. Select. Areas Commun. 35, 6 (2017), 1201–1221. DOI: Reference [145] Shang
    Bodong, Yi Yang, and Liu Lingjia. 2021. Computing over space-air-ground integrated
    networks: Challenges and opportunities. IEEE Netw. 35, 4 (2021), 302–309. DOI:
    Reference 1Reference 2Reference 3 [146] Shen Zhishu, Jin Jiong, Zhang Tiehua,
    Tagami Atsushi, Higashino Teruo, and Han QingLong. 2022. Data-driven edge computing:
    A fabric for intelligent building energy management systems. IEEE Industr. Electron.
    Mag. 16, 2 (2022), 44–52. DOI: Reference [147] Simpson Todd. 2021. Real-time drone
    surveillance system for violent crowd behavior Unmanned Aircraft System (UAS)
    – Human Autonomy Teaming (HAT). In Proceedings of the IEEE/AIAA Digital Avionics
    Systems Conference (DASC’21). 1–9. DOI: Reference [148] Singh Mridula, Roeschlin
    Marc, Ranganathan Aanjhan, and Capkun Srdjan. 2022. V-range: Enabling secure ranging
    in 5G wireless networks. In Proceedings of the Network and Distributed System
    Security Symposium (NDSS’22). 1–18. Reference [149] Sisinni Emiliano, Saifullah
    Abusayeed, Han Song, Jennehag Ulf, and Gidlund Mikael. 2018. Industrial internet
    of things: Challenges, opportunities, and directions. IEEE Trans. Industr. Inform.
    14, 11 (2018), 4724–4734. DOI: Reference [150] Sodhro Ali Hassan, Mohammad S.
    Obaidat, Qammer H. Abbasi, Pasquale Pace, Sandeep Pirbhulal, Ansar-Ul-Haque Yasar,
    Giancarlo Fortino, Muhammad Ali Imran, and Marwa Qaraqe. 2019. Quality of service
    optimization in an IoT-driven intelligent transportation system. IEEE Wirel. Commun.
    26, 6 (2019), 10–17. DOI: Reference [151] Solano Francesco, Praticò Salvatore,
    Piovesan Gianluca, and Modica Giuseppe. 2021. Unmanned aerial vehicle (UAV) derived
    canopy gaps in the old-growth beech forest of Mount Pollinello (Italy): Preliminary
    results. In Proceedings of International Conference on Computational Science and
    Its Applications (ICCSA’21). 126–138. DOI: Reference 1Reference 2Reference 3 [152]
    Song Liang, Hu Xing, Zhang Guanhua, Spachos Petros, Plataniotis Konstantinos,
    and Wu Hequan. 2022. Networking systems of AI: On the convergence of computing
    and communications. IEEE Internet Things J. 9, 20 (2022), 20352–20381. DOI: Reference
    1Reference 2 [153] Soylu Mehmet Evren and Bras Rafael L.. 2022. Global shallow
    groundwater patterns from soil moisture satellite retrievals. IEEE J. Select.
    Topics Appl. Earth Observ. Rem. Sens. 15 (2022), 89–101. DOI: Reference 1Reference
    2 [154] Sze Vivienne, Chen Yu-Hsin, Yang Tien-Ju, and Emer Joel S.. 2017. Efficient
    processing of deep neural networks: A tutorial and survey. Proc. IEEE 105, 12
    (2017), 2295–2329. DOI: Reference [155] Tagliabue Andrea and How Jonathan P..
    2022. Output feedback tube MPC-guided data augmentation for robust, efficient
    sensorimotor policy learning. In Proceedings of the IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS’22). 8644–8651. DOI: Reference [156] Tan
    Liang, Yu Keping, Lin Long, Cheng Xiaofan, Srivastava Gautam, Lin Jerry Chun-Wei,
    and Wei Wei. 2022. Speech emotion recognition enhanced traffic efficiency solution
    for autonomous vehicles in a 5G-enabled space–air–ground integrated intelligent
    transportation system. IEEE Trans. Intell. Transport. Syst. 23, 3 (2022), 2830–2842.
    DOI: Reference 1Reference 2 [157] Tan Tiao, Zhao Ming, and Zeng Zhiwen. 2022.
    Joint offloading and resource allocation based on UAV-assisted mobile edge computing.
    ACM Trans. Sensor Netw. 18, 3, Article 36 (Apr.2022), 21 pages. DOI: Reference
    [158] Tang Qingqing, Fei Zesong, Li Bin, and Han Zhu. 2021. Computation offloading
    in LEO satellite networks with hybrid cloud and edge computing. IEEE Internet
    Things J. 8, 11 (2021), 9164–9176. DOI: Reference 1Reference 2 [159] Tang Yao,
    Cheung Man Hon, and Lok Tat-Ming. 2022. Delay-tolerant UAV-assisted communication:
    Online trajectory design and user association. IEEE Trans. Vehic. Technol. 71,
    12 (2022), 13137–13151. DOI: Reference [160] Teslarati. 2019. SpaceX announces
    second Starlink satellite launch in two weeks. Retrieved from https://www.teslarati.com/spacex-starlink-satellite-launch-second-announcement/.
    Reference [161] Thammawichai Mason, Baliyarasimhuni Sujit P., Kerrigan Eric C.,
    and Sousa João B.. 2018. Optimizing communication and computation for multi-UAV
    information gathering applications. IEEE Trans. Aerosp. Electron. Syst. 54, 2
    (2018), 601–615. DOI: Reference [162] Tilling Rachel L., Ridout Andy, and Shepherd
    Andrew. 2018. Estimating Arctic sea ice thickness and volume using CryoSat-2 radar
    altimeter data. Adv. Space Res. 62, 6 (2018), 1203–1225. DOI: Reference [163]
    Tsuchida Hikaru, Kawamoto Yuichi, Kato Nei, Kaneko Kazuma, Tani Shigenori, Uchida
    Shigeru, and Aruga Hiroshi. 2020. Efficient power control for satellite-borne
    batteries using q-learning in low-earth-orbit satellite constellations. IEEE Wirel.
    Commun. Lett. 9, 6 (2020), 809–812. DOI: Reference 1Reference 2 [164] Ullah Muhammad
    Asad, Mikhaylov Konstantin, and Alves Hirley. 2022. Enabling mMTC in remote areas:
    LoRaWAN and LEO satellite integration for offshore wind farm monitoring. IEEE
    Trans. Industr. Inform. 18, 6 (2022), 3744–3753. DOI: Navigate to [165] Nations
    United. 2022. Sustainable Development Goals. Retrieved from https://www.un.org/sustainabledevelopment/.
    Reference [166] Vaezi Mojtaba, Azari Amin, Khosravirad Saeed R., Shirvanimoghaddam
    Mahyar, Azari M. Mahdi, Chasaki Danai, and Popovski Petar. 2022. Cellular, wide-area,
    and non-terrestrial IoT: A survey on 5G advances and the road towards 6G. IEEE
    Commun. Surv. Tutor. 24, 2 (2022), 1117–1174. DOI: Reference 1Reference 2 [167]
    Vasisht Deepak, Kapetanovic Zerina, Won Jongho, Jin Xinxin, Chandra Ranveer, Sinha
    Sudipta, Kapoor Ashish, Sudarshan Madhusudhan, and Stratman Sean. 2017. FarmBeats:
    An IoT platform for data-driven agriculture. In Proceedings of the USENIX Symposium
    on Networked Systems Design and Implementation (NSDI’17). 515–529. Retrieved from
    https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/vasisht.
    Reference [168] Vázquez Miguel Ángel, Henarejos Pol, Pappalardo Irene, Grechi
    Elena, Fort Joan, Gil Juan Carlos, and Lancellotti Rocco Michele. 2021. Machine
    learning for satellite communications operations. IEEE Commun. Mag. 59, 2 (2021),
    22–27. DOI: Reference 1Reference 2 [169] Venkateswaran Sreekrishnan and Sarkar
    Santonu. 2021. Fitness-aware containerization service leveraging machine learning.
    IEEE Trans. Serv. Comput. 14, 6 (2021), 1751–1764. DOI: Reference [170] Vista
    Francesco, Iacovelli Giovanni, and Grieco Luigi Alfredo. 2021. Quantum scheduling
    optimization for UAV-enabled IoT networks. In Proceedings of the ACM CoNEXT Student
    Workshop. 19–20. DOI: Reference [171] Wang Anyue, Lei Lei, Lagunas Eva, Chatzinotas
    Symeon, and Ottersten Björn. 2021. Dual-DNN assisted optimization for efficient
    resource scheduling in NOMA-enabled satellite systems. In Proceedings of the IEEE
    Global Communications Conference (GLOBECOM’21). 1–6. DOI: Reference [172] Wang
    Chao, Li Qing, Sun Qibo, and Wang Shangguang. 2022. Multi-dimensional resource
    optimal allocation scheme for Tiansuan constellation. In Proceedings of the International
    Conference on Wireless Communications and Signal Processing (WCSP’22). 865–870.
    DOI: Reference [173] Wang Dawei, Fan Tingxiang, Han Tao, and Pan Jia. 2020. A
    two-stage reinforcement learning approach for multi-UAV collision avoidance under
    imperfect sensing. IEEE Robot. Autom. Lett. 5, 2 (2020), 3098–3105. DOI: Reference
    [174] Wang Guangchao, Zhou Sheng, Zhang Shan, Niu Zhisheng, and Shen Xuemin. 2020.
    SFC-based service provisioning for reconfigurable space-air-ground integrated
    networks. IEEE J. Select. Areas Commun. 38, 7 (2020), 1478–1489. DOI: Reference
    [175] Wang Pengfei, Di Boya, and Song Lingyang. 2021. Multi-layer LEO satellite
    constellation design for seamless global coverage. In Proceedings of the IEEE
    Global Communications Conference (GLOBECOM’21). 01–06. DOI: Reference 1Reference
    2 [176] Wang Peng, Zhang Jiaxin, Zhang Xing, Yan Zhi, Evans Barry G., and Wang
    Wenbo. 2020. Convergence of satellite and terrestrial networks: A comprehensive
    survey. IEEE Access 8 (2020), 5550–5588. DOI: Reference [177] Wang Shangguang,
    Li Qing, Xu Mengwei, Ma Xiao, Zhou Ao, and Sun Qibo. 2021. Tiansuan constellation:
    An open research platform. In Proceedings of the IEEE International Conference
    on Edge Computing (EDGE’21). 94–101. DOI: Reference 1Reference 2 [178] Wang Yanmin,
    Feng Wei, Wang Jue, and Quek Tony Q. S.. 2021. Hybrid satellite-UAV-terrestrial
    networks for 6G ubiquitous coverage: A maritime communications perspective. IEEE
    J. Select. Areas Commun. 39, 11 (2021), 3475–3490. DOI: Reference [179] Wang Yuntao,
    Su Zhou, Ni Jianbing, Zhang Ning, and Shen Xuemin. 2022. Blockchain-empowered
    space-air-ground integrated networks: Opportunities, challenges, and solutions.
    IEEE Commun. Surv. Tutor. 24, 1 (2022), 160–209. DOI: Reference [180] Wang Zhuosen,
    Román Miguel O., Kalb Virginia L., Miller Steven D., Zhang Jianglong, and Shrestha
    Ranjay M.. 2021. Quantifying uncertainties in nighttime light retrievals from
    Suomi-NPP and NOAA-20 VIIRS day/night band data. Rem. Sens. Environ. 263 (2021),
    112557. DOI: Reference [181] Watada Junzo, Roy Arunava, Kadikar Ruturaj, Pham
    Hoang, and Xu Bing. 2019. Emerging trends, techniques and open issues of containerization:
    A review. IEEE Access 7 (2019), 152443–152472. DOI: Reference 1Reference 2 [182]
    Wei Junyong and Cao Suzhi. 2019. Application of edge intelligent computing in
    satellite internet of things. In Proceedings of the IEEE International Conference
    on Smart Internet of Things (SmartIoT’19). 85–91. DOI: Reference [183] Wei Junyong,
    Cao Suzhi, Pan Siyan, Han Jiarong, Yan Lei, and Zhang Lei. 2020. SatEdgeSim: A
    toolkit for modeling and simulation of performance evaluation in satellite edge
    computing environments. In Proceedings of the International Conference on Communication
    Software and Networks (ICCSN’20). 307–313. DOI: Reference [184] Werner Debra.
    2022. Living on the edge: Satellites adopt powerful computers. SpaceNews Mag.
    (2022). Retrieved from https://spacenews.com/living-on-the-edge-satellites-adopt-powerful-computers/.
    Reference 1Reference 2 [185] Williams Harrison, Moukarzel Michael, and Hicks Matthew.
    2021. Failure sentinels: Ubiquitous just-in-time intermittent computation via
    low-cost hardware support for voltage monitoring. In Proceedings of the ACM/IEEE
    Annual International Symposium on Computer Architecture (ISCA’21). 665–678. DOI:
    Reference [186] Wilson A. N., Kumar Abhinav, Jha Ajit, and Cenkeramaddi Linga
    Reddy. 2022. Embedded sensors, communication technologies, computing platforms
    and machine learning for UAVs: A review. IEEE Sensors J. 22, 3 (2022), 1807–1826.
    DOI: Reference [187] Wrigley Colin, Batey Ian, and Miskelly Diane. 2017. Grain
    quality: The future is with the consumer, the scientist and the technologist.
    In Cereal Grains (Second Edition). Woodhead Publishing, 691–725. DOI: Reference
    [188] Wu Huaqing, Lyu Feng, Zhou Conghao, Chen Jiayin, Wang Li, and Shen Xuemin.
    2020. Optimal UAV caching and trajectory in aerial-assisted vehicular networks:
    A learning-based approach. IEEE J. Select. Areas Commun. 38, 12 (2020), 2783–2797.
    DOI: Reference 1Reference 2 [189] Wu Xin, Li Wei, Hong Danfeng, Tao Ran, and Du
    Qian. 2022. Deep learning for unmanned aerial vehicle-based object detection and
    tracking: A survey. IEEE Geosci. Remote Sens. Mag. 10, 1 (2022), 91–124. DOI:
    Reference [190] Xia Gui-Song, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie,
    Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. 2018. DOTA: A large-scale
    dataset for object detection in aerial images. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR’18). 3974–3983. DOI: Reference
    [191] Xiang Tian-Zhu, Xia Gui-Song, and Zhang Liangpei. 2019. Mini-unmanned aerial
    vehicle-based remote sensing: Techniques, applications, and prospects. IEEE Geosci.
    Remote Sens. Mag. 7, 3 (2019), 29–63. DOI: Reference [192] Xiao Qingyang, Chang
    Howard H., Geng Guannan, and Liu Yang. 2018. An ensemble machine-learning model
    to predict historical PM2.5 concentrations in China from satellite data. Environ.
    Sci. Technol. 52, 22 (2018), 13260–13269. DOI: Reference [193] Xie Renchao, Tang
    Qinqin, Wang Qiuning, Liu Xu, Yu F. Richard, and Huang Tao. 2020. Satellite-terrestrial
    integrated edge computing networks: Architecture, challenges, and open issues.
    IEEE Netw. 34, 3 (2020), 224–231. DOI: Reference 1Reference 2Reference 3 [194]
    Xing Ruolin, Ma Xiao, Zhou Ao, Dustdar Schahram, and Wang Shangguang. 2023. From
    earth to space: A first deployment of 5G core network on satellite. China Commun.
    20, 4 (2023), 315–325. DOI: Reference [195] Yaacoub Elias and Alouini Mohamed-Slim.
    2020. A key 6G challenge and opportunity–connecting the base of the pyramid: A
    survey on rural connectivity. Proc. IEEE 108, 4 (2020), 533–582. DOI: Reference
    1Reference 2Reference 3 [196] Yang Guang, Lin Xingqin, Li Yan, Cui Hang, Xu Min,
    Wu Dan, Rydén Henrik, and Redhwan Sakib Bin. 2018. A telecom perspective on the
    internet of drones: From LTE-Advanced to 5G. ArXiv abs/1803.11048 (2018), 1–8.
    Reference [197] Yang Jiachen, Wen Jiabao, Wang Yanhui, Jiang Bin, Wang Huihui,
    and Song Houbing. 2020. Fog-based marine environmental information monitoring
    toward ocean of things. IEEE Internet Things J. 7, 5 (2020), 4238–4247. DOI: Reference
    1Reference 2Reference 3 [198] Yang Lixuan and Rossi Dario. 2021. Quality monitoring
    and assessment of deployed deep learning models for network AIOps. IEEE Netw.
    35, 6 (2021), 84–90. DOI: Reference [199] Yang Xing, Shu Lei, Chen Jianing, Ferrag
    Mohamed Amine, Wu Jun, Nurellari Edmond, and Huang Kai. 2021. A survey on smart
    agriculture: Development modes, technologies, and security and privacy challenges.
    IEEE/CAA J. Autom. Sinic. 8, 2 (2021), 273–302. DOI: Reference [200] Yao Haipeng,
    Wang Luyao, Wang Xiaodong, Lu Zhou, and Liu Yunjie. 2018. The space-terrestrial
    integrated network: An overview. IEEE Commun. Mag. 56, 9 (2018), 178–185. DOI:
    Reference 1Reference 2 [201] Yao Zhuohui, Cheng Wenchi, Zhang Wei, and Zhang Hailin.
    2021. Resource allocation for 5G-UAV-based emergency wireless communications.
    IEEE J. Select. Areas Commun. 39, 11 (2021), 3395–3410. DOI: Reference [202] Yu
    Shuai, Gong Xiaowen, Shi Qian, Wang Xiaofei, and Chen Xu. 2022. EC-SAGINs: Edge-computing-enhanced
    space–air–ground-integrated networks for internet of vehicles. IEEE Internet Things
    J. 9, 8 (2022), 5742–5754. DOI: Reference 1Reference 2 [203] Zeng Yongs, Wu Qingqing,
    and Zhang Rui. 2019. Accessing from the sky: A tutorial on UAV communications
    for 5G and beyond. Proc. IEEE 107, 12 (2019), 2327–2375. DOI: Reference 1Reference
    2Reference 3 [204] Zhang Liang and Ansari Nirwan. 2021. Optimizing the operation
    cost for UAV-aided mobile edge computing. IEEE Trans. Vehic. Technol. 70, 6 (2021),
    6085–6093. DOI: Reference [205] Zhang Ning, Zhang Shan, Yang Peng, Alhussein Omar,
    Zhuang Weihua, and Shen Xuemin Sherman. 2017. Software defined space-air-ground
    integrated vehicular networks: Challenges and solutions. IEEE Commun. Mag. 55,
    7 (2017), 101–109. DOI: Reference [206] Zhang Peiying, Yang Pan, Kumar Neeraj,
    and Guizani Mohsen. 2022. Space-air-ground integrated network resource allocation
    based on service function chain. IEEE Trans. Vehic. Technol. 71, 7 (2022), 7730–7738.
    DOI: Reference [207] Zhang Shunliang. 2019. An overview of network slicing for
    5G. IEEE Wirel. Commun. 26, 3 (2019), 111–117. DOI: Reference [208] Zhang Shangwei,
    Liu Jiajia, Zhu Yajie, and Zhang Jing. 2021. Joint computation offloading and
    trajectory design for aerial computing. IEEE Wirel. Commun. 28, 5 (2021), 88–94.
    DOI: Reference [209] Zhang Shunliang, Zhu Dali, and Wang Yongming. 2020. A survey
    on space-aerial-terrestrial integrated 5G networks. Comput. Netw. 174 (2020),
    107212. DOI: Navigate to [210] Zhang Tiehua, Shen Zhishu, Jin Jiong, Zheng Xi,
    Tagami Atsushi, and Cao Xianghui. 2021. Achieving democracy in edge intelligence:
    A fog-based collaborative learning scheme. IEEE Internet Things J. 8, 4 (2021),
    2751–2761. DOI: Reference [211] Zhang Yaguang, Love David J., Krogmeier James
    V., Anderson Christopher R., Heath Robert W., and Buckmaster Dennis R.. 2021.
    Challenges and opportunities of future rural wireless communications. IEEE Commun.
    Mag. 59, 12 (2021), 16–22. DOI: Reference 1Reference 2 [212] Zhang Zhenjiang,
    Zhang Wenyu, and Tseng Fan-Hsun. 2019. Satellite mobile edge computing: Improving
    QoS of high-speed satellite-terrestrial networks using edge computing techniques.
    IEEE Netw. 33, 1 (2019), 70–76. DOI: Reference 1Reference 2Reference 3 [213] Zhao
    Nan, Lu Weidang, Sheng Min, Chen Yunfei, Tang Jie, Yu F. Richard, and Wong Kai-Kit.
    2019. UAV-assisted emergency networks in disasters. IEEE Wirel. Commun. 26, 1
    (2019), 45–51. DOI: Reference 1Reference 2 [214] Zheng Qiushi, Jin Jiong, Zhang
    Tiehua, Li Jianhua, Gao Longxiang, and Xiang Yong. 2019. Energy-sustainable fog
    system for mobile web services in infrastructure-less environments. IEEE Access
    7 (2019), 161318–161328. DOI: Reference 1Reference 2Reference 3 [215] Zhou Conghao,
    Wu Wen, He Hongli, Yang Peng, Lyu Feng, Cheng Nan, and Shen Xuemin. 2021. Deep
    reinforcement learning for delay-oriented IoT task scheduling in SAGIN. IEEE Trans.
    Wirel. Commun. 20, 2 (2021), 911–925. DOI: Reference [216] Zhou Zhi, Chen Xu,
    Li En, Zeng Liekang, Luo Ke, and Zhang Junshan. 2019. Edge intelligence: Paving
    the last mile of artificial intelligence with edge computing. Proc. IEEE 107,
    8 (2019), 1738–1762. DOI: Reference [217] Zhu Xiangming, Jiang Chunxiao, Kuang
    Linling, Zhao Zhifeng, and Guo Song. 2020. Two-layer game based resource allocation
    in cloud based integrated terrestrial-satellite networks. IEEE Trans. Cognit.
    Commun. Netw. 6, 2 (2020), 509–522. DOI: Reference Index Terms A Survey of Next-generation
    Computing Technologies in Space-air-ground Integrated Networks Computing methodologies
    Artificial intelligence Distributed computing methodologies Networks Network algorithms
    Network architectures Recommendations Space-Air-Ground Integrated Network: A Survey
    Space-air-ground integrated network (SAGIN), as an integration of satellite systems,
    aerial networks, and terrestrial communications, has been becoming an emerging
    architecture and attracted intensive research interest during the past years.
    Besides ... Read More High-Bandwidth Optical Interconnect Technologies for Next-Generation
    Server Systems High-bandwidth optical interconnect is a key technology for high-performance
    computing. This article describes a proposed architecture for next-generation
    blade servers and optical transceivers required to realize the architecture. It
    describes the key ... Read More The Photonic Technologies Impact on the Next Generation
    Network This paper describes the possible impact of photonic technologies on the
    next-generation network. With the explosion of the Internet (IP), the capacity
    demand is increasing exponentially, which exceeds Moor''s law. The next-generation
    IP network should ... Read More Comments 150+ References View Issue’s Table of
    Contents Footer Categories Journals Magazines Books Proceedings SIGs Conferences
    Collections People About About ACM Digital Library ACM Digital Library Board Subscription
    Information Author Guidelines Using ACM Digital Library All Holdings within the
    ACM Digital Library ACM Computing Classification System Digital Library Accessibility
    Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect
    Contact Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library
    is published by the Association for Computing Machinery. Copyright © 2024 ACM,
    Inc. Terms of Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: ACM Computing Surveys
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Survey of Next-generation Computing Technologies in Space-air-ground Integrated
    Networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gore R.
  - Banerjea S.
  - Tyagi N.
  citation_count: '0'
  description: Internet of Things has been a popular technology in recent years for
    deploying a large-scale, smart environment monitoring application across the country,
    using fog computing and the cloud. However, most locations of the developing countries
    suffer from power outages and limited network connectivity. Moreover, varied population
    in different locations, may lead to either frequent or rare changes in the state
    of the monitored environment. Due to these stochastic conditions, there may be
    substantial increase in service time of the application with unnecessary battery
    and resource consumption of the fog node. For efficient utilization of fog node
    resources in dynamic environment conditions, an event-driven information fusion
    framework is proposed using docker containerization technology and MQTT (Message
    Queuing Telemetry Transport) as application layer protocol. The proposed framework
    provides resilience to the application in the presence of stochastic conditions
    and auto-scales edge intelligence with minimum scaling operations. The performance
    of the framework is validated on a smart sanitation system use-case application,
    proposed for autonomous monitoring of restrooms deployed in Indian rural and semi-urban
    environment, and the experimental result show deviation of 2.8% in average response
    time of the application with average accuracy of 98.9%.
  doi: 10.1002/ett.4804
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Transactions on Emerging Telecommunications Technologies RESEARCH ARTICLE
    Full Access An event-driven fusion framework with auto-scaling of edge intelligence
    for resilient smart applications in developing countries Rajasi Gore,  Shashwati
    Banerjea,  Neeraj Tyagi First published: 26 May 2023 https://doi.org/10.1002/ett.4804
    SECTIONS PDF TOOLS SHARE Abstract Internet of Things has been a popular technology
    in recent years for deploying a large-scale, smart environment monitoring application
    across the country, using fog computing and the cloud. However, most locations
    of the developing countries suffer from power outages and limited network connectivity.
    Moreover, varied population in different locations, may lead to either frequent
    or rare changes in the state of the monitored environment. Due to these stochastic
    conditions, there may be substantial increase in service time of the application
    with unnecessary battery and resource consumption of the fog node. For efficient
    utilization of fog node resources in dynamic environment conditions, an event-driven
    information fusion framework is proposed using docker containerization technology
    and MQTT (Message Queuing Telemetry Transport) as application layer protocol.
    The proposed framework provides resilience to the application in the presence
    of stochastic conditions and auto-scales edge intelligence with minimum scaling
    operations. The performance of the framework is validated on a smart sanitation
    system use-case application, proposed for autonomous monitoring of restrooms deployed
    in Indian rural and semi-urban environment, and the experimental result show deviation
    of 2.8% in average response time of the application with average accuracy of 98.9%.
    1 INTRODUCTION Smart applications are being developed worldwide using Internet
    of Things (IoT) technology.1 The cloud2 has unlimited computational power for
    processing the sensed data and perform information fusion to provide useful insights.
    With increased data communication due to multiple IoT applications being deployed
    over the network every day, the network bandwidth is being burdened causing frequent
    bottlenecks and delays. To reduce data over the network, most of the data processing
    is performed at the network edge with the help of fog computing. Numerous smart
    applications have adopted fog-cloud architecture to monitor multiple domains such
    as city, healthcare and agriculture and real-time IoT applications have benefited
    from fog computing through edge intelligence for faster decision-making.3 1.1
    Motivation Real-time IoT applications deployed for monitoring environment in the
    developed countries have good network connectivity, continuous power supply and
    thus, service time is not affected by dynamic environmental factors. Moreover,
    cost of deployment is not an issue and multiple fog devices can be deployed to
    provide prompt response. However, deploying smart applications to monitor location
    in a developing country is a challenge. These regions suffer from network issues
    and frequent power outages. Moreover, the developing countries prefer low-cost
    deployment of the smart applications which results in minimal number of fog devices
    to handle data at the network edge. In such a scenario, providing timely and accurate
    service is a challenge.4 To manage application services in a stochastic environment,
    the fog device needs to be made resilient, adapting to current stochastic conditions
    and provide resource-efficient, timely service. Furthermore, different monitored
    locations have varied population that is dynamic throughout the day. Population
    dynamics directly affect the rate of change of environment state. If there is
    high number of people, the state of monitored environment would change more frequently.
    For example, railway station having multiple platforms will have high population
    rush and therefore, will require continuous monitoring and fast decision-making
    compared to railway station with one platform that usually have low rush. To conserve
    fog node resources of applications dealing with stochastic environment conditions,
    the application need to auto-scale intelligence considering current state change
    rate of the environment rather than running same model for all the locations irrespective
    of the rush. 1.2 Objectives In a generic fog-cloud architecture of a real-time
    IoT application, the fog node preprocesses the data and cloud runs intelligence
    to make decisions. However, in case of limited network, the network delay increases,
    affecting the timeliness of the real-time IoT application. Thus, in case of limited
    network connectivity, the fog node needs to preprocess as well as take decisions
    rather than communicating data to the cloud. In case of power cut, the fog node
    runs on battery and battery life is limited. The fog node has to handle incoming
    data such that it is fast as well as accurate in decision-making, utilizing minimal
    resources for increasing fog node operational time. Thus, the first objective
    of the article is to detect stochastic events like limited network or power cuts
    at the fog node in real-time and accordingly switch fog node operations from preprocessing,
    to providing edge intelligence, to resource conservation, making the application
    resilient by maintaining timeliness as well as accuracy of the application. The
    switching operation should be fast such that no incoming critical data is missed.
    To run intelligence at the fog node having limited computational power, the edge
    intelligence has to be auto-scaled depending on the current population in the
    monitored environment. Docker5 has been one of the most used containerization
    technology for implementing intelligence efficiently on fog devices. The docker
    containers are easy to auto-scale according to the incoming data rate at the cloud
    and on multiple edge devices.6 Most of the applications have dedicated one fog
    node to auto-scale containers across multiple fog nodes for processing. While
    handling medium or low rate of state change, these devices become under-utilized
    but equally consume power, increasing the cost of deployment. Thus, second objective
    of the work is to auto-scale edge intelligence on stand-alone fog node, adapting
    to the dynamic environment state change, in a reactive manner as well as in a
    proactive manner, providing minimum number of up-scales and down-scales along
    with efficient resource utilization as well as fast scaling. 1.3 Contributions
    To address the above objectives, an event-driven information fusion framework
    is proposed, that runs using docker containerization technology on the fog-cloud
    architecture, to provide resilience to the application and auto-scale edge intelligence
    according to the change of environment state. Moreover, the framework uses Message
    Queuing Telemetry Transfer (MQTT) protocol7 for event-driven data communication
    between docker microservices in the fog node as well as with the cloud, as it
    is light-weight and provides reliable communication in case of limited network.
    Overall, the contributions of this work are as follows: An event-driven information
    fusion framework is proposed with a stand-alone fog node and a cloud for a low-cost
    IoT application, deployed for monitoring rural and semi-urban locations of developing
    countries, suffering from limited network connectivity and frequent power cuts.
    The fog node runs an event-driven controller, to provide resilience to the application,
    detecting intermittent connectivity issues and frequent power outages and re-configuring
    data processing on fog-cloud architecture in real-time. An efficient information
    fusion model is proposed for providing edge intelligence in the presence of limited
    network connectivity and power outages. State change of the monitored environment
    with respect to dynamic population rush is modeled using queuing theory and a
    novel auto-scaling approach is proposed using combination of statistical and probability
    based estimation algorithms for lightweight, resource efficient, and timely service
    on the edge device. To validate the proposed event-driven information fusion framework
    with auto-scaling edge intelligence, a smart sanitation system application is
    proposed as a use-case that monitors public toilets established across India under
    “Clean India Mission.” Prototype of the use-case application is deployed in the
    institute lavatories and different environment conditions are simulated for evaluating
    the performance of the proposed fusion framework. Rest of the article is structured
    as follows. Related works are discussed in Section 5, discussing real-time applications
    running edge-intelligence in Section 6, real-time applications with battery powered
    fog nodes are discussed in Section 7 and applications running docker containers
    on fog nodes are discussed in Section 8. Section 10 provides a system model in
    Section 10 and information fusion framework in Section 11 of the proposed smart
    sanitation system application considered as a use-case. The proposed fog-based
    event-driven information fusion framework, designed on the use-case application,
    is discussed in Section 16. Experiments are carried out in Section 24 to validate
    the proposed framework. Section 32 concludes the article. 2 RELATED WORKS The
    survey is divided into four parts. First part discusses current IoT frameworks
    that have implemented computing continuum for real-time data processing. Next
    two part discusses applications proposed for handling stochastic environment conditions.
    The research works that proposed edge intelligence for real-time monitoring smart
    applications to handle limited network connections is reviewed in second part.
    Third part discusses applications that proposed solutions for battery-powered
    fog nodes. Literature works that have used docker containerization technology
    for running edge intelligence are highlighted in the fourth part. State-of-the-art
    in each part of the survey is presented in the following subsections. 2.1 Edge-fog-cloud
    computing continuum Multiple research works have proposed fog-cloud computing
    frameworks for IoT applications to provide real-time service to end users. Sánchez-Gallegos
    et al.8 proposed an edge-fog-cloud framework to process health data to provide
    continuous reliability, security, delivery and efficiency. They have implemented
    parallelism and in-memory storage. However, the implementation cost of the framework
    is high with fog and edge devices having high storage of 1 TB and the network
    is assumed to be good for implementing parallel analysis of sensitive data over
    edge, fog and cloud. Neha Agarwal9 proposed an edge-fog-cloud framework where
    edge device preprocesses the data, fog device balance the network load and process
    some of the data locally and the cloud auto-scales virtual machines to handle
    data workload forwarded by fog layer. Load of data on the cloud is distributed
    to edge devices, however, the cloud is main component of the architecture providing
    centralized point of failure and high network cost. Demirel et al.10 presented
    a fog-cloud framework having wearable device with low RAM as edge device. They
    have implemented first layer of CNN classifier on edge device and following layers
    on fog and cloud device for real-time detection of normal/abnormal heartbeats.
    Normal heartbeats are detected fast on edge device, however, time- critical abnormal
    heartbeats are further processed on fog-cloud using CNN model. Dependency on cloud
    for time-critical medical assistance is not feasible in case of limited network
    with delayed information communicated to analytical model distributed over edge-fog-cloud
    devices. Das et al.11 proposed an edge-fog-cloud framework that analyse the patient''s
    health on fog device and if report is good it is forwarded to edge device, and
    if report is abnormal, the data is processed at the cloud and cloud calls the
    ambulance in case of emergency. With cloud handling time-critical patients and
    emergency, in case of high network bandwidth utilization or limited network connectivity,
    the response can be delayed, delaying handling of emergency situations fast. In
    such cases, fog devices need to be made intelligence to handle such cases near
    to the patients. Diamanti et al.12 presented a framework that distribute tasks
    from over utilized edge devices to upper fog layers using multi-dimensional contract
    theory modeling, efficiently utilizing resource limited and battery powered edge
    devices. The model deals with edge and fog devices in close proximity and edge
    devices communicate with each other for task offloading, increasing network burden
    within a specific location. Moreover, they have not considered stochastic events
    that can hamper the timeliness of the task processing. Alharbi et al.13 presented
    an edge-fog-cloud framework for agriculture system to reduce CO emission caused
    by transferring of data to and from the cloud. The framework follows bottom-up
    approach as incoming data is processed at the edge, and if the data load increases,
    the processing is shifted to fog and then the cloud, reducing data on network
    and providing real-time service. The framework has assumed good network connectivity
    and the implementation cost is high with edge devices on LAN connected via passive
    optical network. In this work, the proposed framework deals with stochastic environment
    conditions and adjusts its task processing accordingly following top (cloud) to
    down (fog device) approach with minimal implementation cost for feasible deployment
    of large-scale applications in developing countries. 2.2 Framework running edge-intelligence
    As data transmission time increases during limited network connectivity, the data
    is analysed at the fog node using edge-intelligence rather than the cloud. Martín
    C et al.14 proposed an edge/fog/cloud framework for structural health monitoring.
    The architectural components of the framework are run as microservices. Docker
    swarm is used to place these microservices at the cloud, fog or edge depending
    on the load. Haseeb et al.15 developed an application where sensors select a network
    to intelligently route data depending on the data bytes and distance. The intelligence
    is applied using deep learning model and the sensors are trained for intelligent
    routing of their data. Rahman et al.16 presented an intrusion detection system
    application running over edge devices. The edge devices run parallel machine learning
    algorithms to handle every incoming data and detect malicious activity. The machine
    learning algorithm runs on different data features and then the results are combined
    by the ML model. Zhen et al.17 have implemented a deep neural network model on
    edge device for ECG based heartbeat detection. They have compared the performance
    with HealthFog. HealthFog has an edge broker to decide at what edge device the
    data has to be sent. In the proposed application, gateways decide where to send
    data for analysis. The Raspberry Pi behaves as a gateway and normal PC is made
    an edge device. Taneja et al.18 developed a microservice-based fog-cloud model
    for herd management in remote farms. The application uses a pedometer rather than
    WiFi which has 2 km of transmission range to handle internet issues in remote
    farms. A powerful PC is taken as a fog node for analysing data at the edge with
    flexible microservices between cloud and edge device. Renewable energy from solar
    panels is used to charge the pedometers. Advantages and disadvantages of these
    edge-intelligence frameworks is presented in Table 1. In this article, fog-cloud
    framework is proposed with stand-alone fog device running edge-intelligence as
    a microservice. The fog device is taken as Raspberry Pi, that acquire data from
    low-cost sensors and edge-intelligence is light-weight. Thus, the proposed framework
    is cost-effective for deployment in developing countries as well as provide fast
    service with stand-alone fog node minimizing network bandwidth utilization. TABLE
    1. Comparison of related edge-intelligence frameworks. Research Framework Edge
    device Advantages Disadvantages 14 Fog-cloud with multiple fog devices devices
    with microservices Normal PC Reduce service latency High network bandwidth consumption,
    high deployment cost 15 Cloud with sensors as edge devices Sensors Redundant path
    availability during network disruptions Heavy edge-intelligence on sensors 16
    Fog-cloud with multiple edge devices Normal PC Parallel intelligence, fast service
    High network bandwidth consumption, high deployment cost 17 Multiple fog devices
    and edge broker Raspberry Pi and normal PC Reduced service latency High network
    bandwidth consumption, high deployment cost 18 Fog-cloud with stand-alone fog
    device with microservices Normal PC Reduced service latency, direct data transmission
    Costly sensors to deal with network issue 2.3 Battery-powered fog nodes In case
    of power cuts, the fog node battery will drain while processing the data in real-time.
    Applications that are dependent on battery of fog nodes and have no alternatives,
    may have huge impact on timeliness of the service until the battery is replenished.
    Real-time applications that have used fog computing and has considered battery-powered
    fog nodes are as follows. Sangaiah et al.19 presented a fusion framework on edge
    device using a deep learning model and fuzzy logic to track elephants around the
    boundary of forest. The multi-sensors used are acoustic sensor, camera sensor
    and vibration sensor. The data from these sensors is fused using fuzzy cognitive
    maps. The model is tested in different weather conditions: summer, winter, and
    rainy and calculated false rates. Fang et al.20 have proposed an application using
    renewable energy from solar panels to charge limited battery Raspberry Pi. The
    charging of fog node is predicted ahead of time using a deep learning model before
    offloading services, to maintain quality of service and energy efficiency of the
    application. Ferrández-Pastor et al.21 have implemented an edge-based intelligence
    for energy management of smart buildings. The energy sources in the smart building
    are of two types: first is electricity and second is renewable energy from wind
    energy and solar energy. Two separate classifiers, K-nearest neighbors and decision
    tree, are deployed on the edge device for each type of energy source, respectively.
    They have taken into consideration the environmental factors such as non-windy
    and non-sunny days in modeling of the application. Alonso et al.22 presented an
    application that provides automated livestock farming with solar panels as the
    renewable energy source. They have utilized edge computing to handle data analysis
    at the edge over cloud. Gupta et al.23 developed a sustainable application for
    vehicular health monitoring creating a lightweight ANN model optimized with genetic
    algorithm. Rather than deploying multiple sensors, the vehicle health is detected
    based on sound signals. Smartphones are chosen as an edge device to analyse the
    sound data with an optimized ANN model rather than sending data to the cloud.
    Advantages and disadvantages of these research works is presented in Table 2.
    In the proposed framework, the battery of Raspberry Pi, considered as fog node
    in the use-case application, is not charged with renewable energy sources, thus,
    the application does not have a high deployment cost and no dependence on environmental
    factors. The proposed fusion framework has fast and lightweight edge-intelligence
    model and no data transmission to the cloud during power cut, to increase the
    duration of fog node operation. TABLE 2. Comparison of related battery-powered
    fog computing frameworks. Research Framework Battery alternative Advantages Disadvantages
    19 Fog computing No Application suitable for all seasons Failure to on-time replenishment
    of battery due to weather conditions may lead to stopped tracking 20 Fog-cloud
    computing Solar panel Battery is charging even in case of power cut High cost,
    battery will run out fast in case of cloudy sky and continuous data transmission
    to the cloud 21 Fog computing Solar panel and wind mill Application is resilient
    in case of power cut, cloudy sky as well as non windy weather High cost, dependency
    on environmental factors of the location 22 Fog computing Solar panel No data
    transmission to cloud, saving fog node battery High deployment cost, Non-sunny
    days will impact the battery life and application service as no alternative as
    Cloud used for analysis 23 Smart phones as fog devices No Reduced service latency,
    cost effective Heavy computational model that will consume battery faster, requires
    constant charging of battery 2.4 Containerized fog computing Multiple research
    works have used docker containers to run edge intelligence as micro-service on
    fog node. Al-Rakhami et al.24 proposed an IoT application for identifying human
    activity. Raspberry Pi is chosen as a fog node device for analysing data at the
    edge. RELM classification algorithm is trained for analysis and its docker image
    is created to be run on Raspberry Pi. They experimented on the consumption of
    the network bandwidth with respect to the increasing number of sensors while analysing
    data at the edge. Divya and Sri25 deployed an IoT application performing vision
    based detection using edge computing. They have used a compressed neural network
    learning algorithm to analyse the data. The intelligence is deployed on the edge
    device using docker containers. They have used transfer learning methods and thus,
    the docker image with newly trained models are replaced on the edge device with
    ease and without corruption. Li and Gui26 proposed a framework for industrial
    application having evolving machine learning algorithms. They have used docker
    containers for seamless updating of machine learning models on edge devices. They
    have proposed an algorithm for updating without interrupting the analysis process
    and evaluated the container switching overhead. Jin et al.27 used docker containers
    for distributed training of machine learning models on edge devices. Moreover,
    any new device is updated with the current docker image to be part of coordinated
    training. These works run machine learning model on a single container over edge
    device. However, these works have not studied running of edge intelligence micro-service
    handling varied high or low data rates with optimized use of limited resources
    of fog node. Few research works have proposed auto-scaling of micro-services to
    provide edge intelligence on varied data rates at the fog node. Gand et al.,28
    Bali et al.,29 Lee et al.30 have proposed auto-scaling of containers on fog devices
    based on fuzzy logic, knowledge base and deep Q network respectively. Gand et
    al.28 have proposed fuzzy logic for an uncertain environment that updates the
    membership function according to data load and take scaling decisions reactively.
    The incoming data rate is classified as low, average and high and accordingly
    the scales are decided. This is a continuous reactive process where the parameters
    of membership function is adjusted and fuzzy service decides the scaling and records
    message round trip time between the application and the vehicle. However, the
    model has not taken into consideration the uncertainty in incoming data that can
    affect parameter adjustment. Moreover, the response time with the mobile vehicles
    may get affected in case of network connectivity issue resulting in faulty adjustment
    of membership function. Bali et al.29 have created knowledge base and presented
    threshold based rules to scale containers within multiple edge devices and auto-scaling
    decision is reactive. Moreover, they have deployed a separate node as manager
    node to perform scaling. The architecture deployed for scaling is costly and has
    high network bandwidth utilization. Lee et al.30 have deployed a deep learning
    model based on a recurrent neural network that trains in run-time and decides
    scaling of the application on multiple edge devices. They have proposed a fog
    cloud framework for handling data in existing traffic and provided vertical as
    well as horizontal, reactive auto-scaling in multiple edge devices and within
    edge devices during additional traffic, respectively. The model is heavy to be
    trained in real-time on edge devices having limited computational power. This
    article has proposes a vertical, lightweight, and proactive auto scaling method
    using queuing theory-based estimation algorithm for intelligently scaling containers
    (microservices running edge-intelligence) within a stand-alone fog device with
    efficient resource-utilization. The proposed scaling creates and removes a minimum
    possible number of replicas by estimating future distribution of load rather than
    reactive scaling decisions which can be resource costly for edge devices. 3 USE-CASE:
    SMART SANITATION SYSTEM APPLICATION For developing a resilient fusion framework
    in stochastic countryside conditions, smart sanitation system application is proposed
    as a use-case. The application aims to continuously monitor cleanliness, maintenance,
    and resource availability of public toilets built by the government of India across
    the country under the “Clean India Mission” using IoT technology. 3.1 System model
    Figure 1 shows the system model of the use-case application. In the figure, two
    sensors are used to capture the complete context of the toilet. A gas sensor is
    used to keep track of ammonia concentration within the toilet and collects data
    in ppm. A motion sensor is used to track the toilet access rate for determining
    the rush. The sensor represents detected motion as 1 and no motion as 0. The gas
    sensor and motion sensor are connected to Arduino Uno. Arduino Uno only communicates
    the sensor data when there is a toilet access that means when motion sensor data
    is 1. Thus, the transmission energy of the sensor is conserved. A feedback app
    is developed for users to rate the toilet usage. The ratings are provided for
    cleanliness, maintenance and experience in the range from 1 to 5. Rating 1 means
    very poor, rating 2 represents poor, rating 3 denotes average, rating 4 means
    good and rating 5 indicate excellent. The fog node collects sensed environment
    data from public toilets situated in its vicinity (blue line). Each fog node preprocesses
    the data and forward the data to the cloud (black dotted line). A private cloud
    is deployed to analyse the incoming fused information from each of the restroom
    locations. The cloud is made intelligent to analyse data generated from each toilet
    and make decisions. If the cloud predicts whether a toilet requires cleaning,
    maintenance, or resources, it notifies the cleaner, maintenance department or
    the store room respectively (red line). FIGURE 1 Open in figure viewer PowerPoint
    System model of the proposed smart sanitation system application with fog nodes
    acquiring sensor data from toilets in their vicinity, fog nodes publishing the
    data to cloud for decision-making and depending on the decision, cloud notifies
    respective cleaners, maintenance department, and store room. 3.2 Default information
    fusion framework As per the system model, the fusion framework of the proposed
    application has three modules: data preprocessing, data analysis, and decision
    notification. Working of each module is discussed in detail in the following subsections.
    3.2.1 Data preprocessing This module is placed at the fog node and performs preprocessing
    on incoming gas sensor data, motion sensor data and feedback app data. Gas sensor
    data is directly used without preprocessing. From motion sensor data, the current
    access rate of the restroom is deduced. Average feedback rating is derived from
    the data collected from the feedback app. The ammonia value from gas sensor data,
    access rate from motion sensor data and average feedback rating is fused in time-series
    and communicated to the data analysis module of the cloud using MQTT publish/subscribe
    model with MQTT broker placed at the cloud. 3.2.2 Data analysis In this module,
    an adaptive neuro-fuzzy inference system (ANFIS) is implemented as an analytical
    model at the cloud for intelligent decision-making. The learning model classifies
    the toilet state with respect to provided information as input. The toilet state
    determination is fuzzy and thus, five categories are defined in which the toilet
    state can be in. The analytical model is trained for accurate category classification.
    If the category is classified as 1, the decision is “Clean and Maintenance Required”.
    If the category is 2, the decision is “Mild and Resources Required.” If the classified
    toilet category is 3, the decision is “Smelly.” The decision is “Stinky” if the
    toilet state is classified in category 4. If the toilet state is in category 5,
    the decision is “Pungent.” The decisions taken by this module for each of the
    toilet locations is communicated to the decision notification module. 3.2.3 Decision
    notification This module is placed in the cloud for notifying the cleaners, maintenance
    department or store room as per the decision made by the ANFIS model. If the toilet
    category is 1, the maintenance department within the location vicinity is contacted
    immediately. If the category of toilet state comes out to be 2, store rooms are
    contacted for resources such as mug and soap. The cleaner is notified only once
    in case of toilet state in category 3. Alert notification is sent to the cleaner
    periodically after every 5 min, if the toilet state is in category 4. In case
    of toilet state in category 5, the cleaner is notified continuously until the
    toilet state does not change from category 5. Next section discuss the proposed
    framework on the use-case application and the notations used in the modeling of
    the proposed framework are summarized in Table 3. TABLE 3. Notations used in the
    article and their description. Symbol Description Response time of support mode
    Incoming data count Communication latency between devices and fog node Preprocessing
    time of data at fog node Communication latency between fog device and the cloud
    Processing time by analytical model running at the cloud Processing time taken
    by cloud to notify decisions External factor based added time in communication
    between device and the fog node External factor based added time in communication
    between fog device and the cloud The arrival probability distribution Probability
    distribution of the service Number of servers running in parallel Discipline of
    the queue like first come first serve, priority or service in random order Buffer
    limit which is finite or infinite Calling source capacity Number of toilets Arrival
    rate of every toilet Departure rate of every toilet Cumulative arrival rate from
    toilets Service time Inter-arrival rate of data from toilets at time Number of
    containers List of current arrival rate List of number of containers with respect
    to current arrival rate Likelihood of containers in Mean of prior probability
    Standard deviation of prior probability Previous number of containers For measuring
    hour Time taken in making scaling decision. Time taken by fog node to analyse
    data Processing time taken by fog node to notify decisions Array for storing access
    rate of each location Location that has high rush Function that extract critical
    data from n data and can range from 4 PROPOSED FOG-BASED EVENT-DRIVEN INFORMATION
    FUSION FRAMEWORK An event-driven information fusion framework is proposed, depicted
    in Figure 2, to make the use-case application resilient in stochastic environment
    conditions such as limited network connectivity and power outage. The power outage
    or limited network connectivity is assumed as events in the proposed framework
    due to their unusual occurrences that hampers normal operation of the application.
    According to the detected event, the fog node switch between three modes: support,
    stand-alone and power-saver. Fog node is in support mode, a default mode, when
    the network connectivity is good and there is continuous electric supply. In case
    of limited network connectivity with continuous electric supply, the fog node
    switch to stand-alone mode. If the restroom location faces power cuts irrespective
    of good or weak network, the fog node runs in power-saver mode. Therefore, to
    perform information fusion in three different modes, the proposed framework has
    four modules: event-driven mode controller, data preprocessing, data analysis,
    and decision notification. Working of data preprocessing, data analysis, and decision
    notification modules are same as use-case application (refer to Section 11 respectively),
    with some changes in their placement and functionality to adapt according to the
    event detected. Event-driven mode controller module implements decision logic
    to detect event in real-time and accordingly switch modes of the fog node. At
    different mode of the fog node, data analysis, and decision notification modules
    are placed either at fog device or at cloud. Different modes of the fog node are
    described in Section 16 and the working of event-driven mode controller module
    is described in Section 22. FIGURE 2 Open in figure viewer PowerPoint Modes and
    corresponding placement of modules in the proposed fog-based event-driven information
    fusion framework for managing application in stochastic countryside conditions.
    4.1 Modes of the fog node Fog node switch modes depending on the detected stochastic
    conditions in real-time. Functionality of data preprocessing module, data analysis
    module, and decision notification module changes with respect to each mode. Each
    mode of the fog node with placement and functionality of the three modules (data
    preprocessing module, data analysis, and decision notification) is discussed as
    follows. 4.1.1 Support mode Fog node functions in this mode in case of good network
    connectivity and no power cuts. It supports cloud in processing and analysing
    data coming from multiple geographical toilet locations. The data from toilet
    is acquired and preprocessed at the fog node and communicated to the cloud for
    analysis and decision-making. Information fusion model in case of support mode
    of fog node is shown in Figure 3. FIGURE 3 Open in figure viewer PowerPoint Proposed
    information fusion framework during support mode (default mode 0). The modules
    of information fusion framework in this mode functions as discussed in Section
    11. This mode is default mode of the fog node and it changes into stand-alone
    mode or power-saving mode as per the change in monitored environment. When the
    monitoring environment gets stable, the fog node resumes operation in this mode.
    The response time of the information fusion model is calculated as (1) In support
    mode, and is 0 due to good network connectivity. Value of 0.1 is multiplied with
    as it is assumed that 10% of data will have a cleaning decision to notify either
    cleaners, maintenance department, or store room. 4.1.2 Stand-alone mode The fog
    node switches to this mode in case when the locations handled by the fog node
    suffer from weak network connectivity and no power cut. In this mode, the data
    analysis module is implemented on the fog device and the toilet is monitored from
    the network edge. Thus, the fog node preprocesses as well as analyse the incoming
    toilet data. The decisions are notified by fog node itself. The model in stand-alone
    mode is shown in Figure 4. FIGURE 4 Open in figure viewer PowerPoint Proposed
    information fusion framework during stand-alone mode (mode 1). Fog node has limited
    resources compared to the cloud. Furthermore, fog node can be any device with
    heterogeneous configurations and platform. In the use-case application, Raspberry
    Pi is chosen as fog node. To efficiently utilize limited resources of the fog
    node and have ease of deployment on any fog node hardware with heterogeneous resources,
    the data analysis module is implemented using docker containerization technology.
    Data analysis module is slowest and heavy on resources compared to other two modules
    and require scaling as per incoming data for maintaining framework''s response
    time. Containers enables micro-service architecture where data analysis module
    forms a separate service. A micro-service architecture is easy to scale according
    to incoming data with efficient resource utilization. With respect to current
    toilet rush, docker container running data analysis module is auto-scaled dynamically,
    providing multiple micro-servers for analysing the incoming toilet data in parallel,
    maintaining the service time of the framework. Auto-scaled containers form a multi-processing
    architecture, where each container has separate memory to process the data and
    if one container fails, the data and service of other containers is not affected
    as opposed to multi-threading. Multi-threading31 or goroutines32 that are lightweight
    solutions for scaling analysis module, do not execute incoming data sequentially
    which can impact real-time output and decision-making of the framework. Moreover,
    containers enable easy horizontal scaling to multiple devices if a stand-alone
    device is not sufficient. A docker image of trained ANFIS model is used as used
    as data analysis module running as micro-service on fog node. The preprocessing
    module preprocesses the incoming data from gas sensor, motion sensor and feedback
    app. From motion sensor data, access rate is calculated. Access rate denotes the
    current toilet rush and average access time denotes average time taken by the
    people using the restroom. The access rate is used to calculate number of micro-servers
    required to analyse the data for maintaining the current response time. To dynamically
    determine number of micro-servers (docker containers), the incoming data on fog
    node from toilet locations is modeled using queuing theory.33 A queuing system34
    is represented as . The fog node is modeled as M/D/1 queue having finite buffer.
    The incoming time-series toilet data in the system is served by analysis module,
    running as micro-servers, in first come first serve basis. The source of data
    is population which is infinite. The arrival rate at each location follows Markovian
    distribution as the current arrival is random and is independent of the last arrival.
    The micro-service time of the analytical model is constant for every incoming
    data. Thus, the service probability distribution is deterministic. Moreover, the
    service is non-preemptive. Another property of the system is that the buffer is
    limited but the requests cannot be kept waiting for long in the buffer. The arrival
    rate is denoted as and the departure rate is denoted as at every toilet location.
    Cumulative arrival rate from toilets is denoted as . Service time is constant
    and is denoted as which is equal to .35 Inter-arrival rate of data from toilets
    at time , represented as , is calculated from the cumulative arrival rate as per
    the following equation. (2) For reactive auto-scaling of the docker containers
    running as micro-servers, the value of has to be dynamically updated. The value
    of is evaluated based on average inter-arrival time in toilet locations and deterministic
    service time on docker container. The equation to calculate required number of
    microservices at time is as follows:36 (3) Value of is known by the queuing system
    and is calculated as response rate of the implemented ANFIS model on docker container.
    The time-series data is not buffered in queue for more than 1 s to maintain timeliness
    of the system. The containers are dynamically auto-scaled as per the inter-arrival
    rate of time-series data from “N” locations. The will differ due to variability
    in the arrival rate at certain locations. Most of the time, of location is a constant
    value and there can be chances of its increase or decrease as per temporal aspects
    of the rural and semi-urban regions as shown in Figure 5. FIGURE 5 Open in figure
    viewer PowerPoint Cumulative arrival rate at fog node in stochastic environment
    having normal distribution. Depending on the stochastic cumulative arrival rate,
    the number of containers required are predicted. Figure 6 depicts the distribution
    of number of containers predicted in the presence of stochastic arrival rate.
    The predicted scale of containers also follows a normal distribution. Depending
    on the arrival rate, the predicted containers maintain the service time of the
    fog node. It is assumed that, with every arrival, there is a change in environment
    state. In presence of stochastic state changes due to population dynamics in the
    countryside, there will be mix of frequent/infrequent scaling requirement throughout
    the day. The container running and stopping every time with respect to value of
    may increase power consumption. Thus, a novel algorithm is proposed that decides
    on number of micro-service containers depending on the arrival rate reactively
    as well as proactively. The proposed algorithm has combination of estimation algorithms
    that keeps an optimum trade-off between processing time and number of containers.
    The algorithm estimates the chance of a container to be utilized in near future
    and accordingly takes the decision during down-scale. The algorithm adapts the
    dynamic nature of the locations every hour as the data rate can change every hour.
    Moreover, on some days, toilet locations may have near to zero access. Another
    possibility is to have high arrival rate during first half of the day and no or
    minimal arrival during second half of the day. Thus, the algorithm scales according
    to the arrival rate during each hour rather than running same algorithm the whole
    day. The arrival rate between hours may be different and thus, probability distribution
    of toilet access at every hour is taken in consideration rather than the probability
    distribution of toilet access the whole day. Thus, auto-scaling algorithm resets
    every hour by considering the arrivals as new distribution for auto-scaling. FIGURE
    6 Open in figure viewer PowerPoint Predicted containers with respect to arrival
    rate having normal distribution. The algorithm initializes three arrays: , and
    . has list of current arrival rate and has list of number of containers with respect
    to current arrival rate . is used to keep values in the initialized arrays on
    an hourly basis. calculated from motion sensor data. Number of containers required
    for current arrival rate is calculated from Equation (3) and saved in variable
    . If current value of is greater than previous, the containers are up-scaled.
    If value of is less than previous value, the containers has to be down-scaled.
    determines the data size of the arrays and is incremented as the data arrives
    in an hour. For value of greater than 100, maximum likelihood estimation is used
    to decide value of for down-scaling from the list of in rate_array. For greater
    than 4 and less than 100, or if data is critical, Median based thresholding is
    used on to determine value of to down-scale. If data is non-critical or the counter
    is less than 4, Bayesian inference is used on and is created to determine the
    value of to down-scale. Algorithm 1. Algorithm for the proposed auto-scaling method
    The estimation methods used for auto-scaling in real-time with efficient trade-off
    are: Median based thresholding: The threshold value to downscale the containers
    is decided by median. Median is one of the popular tools in statistics and probability.
    It selects middle value from series of data. The series of data chosen in the
    proposed algorithm is stored in an array named having number of containers instantiated
    on every incoming arrival. The middle value in the sorted size array is chosen
    as a sample for down-scale threshold. Median based thresholding is non-parametric
    estimation method as it does not know distribution of incoming arrival rates.
    Thus, it is useful when there are very few arrivals in the system. Moreover, it
    is fast. Data coming with higher arrival rates can be down-scaled using median
    as a threshold. Assuming there are values in array where each value is the number
    of containers required with every data, the values are sorted and the median is
    calculated as:37 (4) Maximum likelihood estimation: This technique is applied
    when there are sufficient number of samples with known probability distribution
    as normal. From these samples, parameters and can be derived. Likelihood function
    determines the probability of each sample in the distribution with respect to
    the parameters. Likelihood on each sample x from distribution having parameters
    is calculated as probability P(x). In a normal distribution, the sample x having
    maximum likelihood is same as . However, due to stochastic behavior of the system,
    the maximum likelihood will change with shift in distribution of the arrival rates.
    Maximum likelihood estimation is carried on normal distribution of arrival rates
    stored in array and has two steps:38 Joint probability: Each sample in rate_array
    have probability density function with and . The function is denoted as: (5) Probabilities
    of all the samples in are added to find joint probability distribution. The joint
    distribution has to be differentiated to find parameters and when the distribution
    is maximum. (6) Log likelihood: The Equation (6) is differentiated by taking logarithm
    of the joint density function. The Equation (6) is updated to calculate log likelihood
    as: The estimated and by differentiation of log likelihood function is used to
    estimate arrival rate having maximum likelihood. Number of containers required
    is calculated from estimated arrival rate. The calculated value of containers
    from estimated arrival rate is used to down-scale containers. According to the
    calculated value of x having maximum likelihood, the containers are down-scaled.
    Bayesian inference: This technique is used to estimate parameters of data distribution
    based on Bayes theorem. Bayes theorem estimates probability of an event based
    on probability of prior related event using conditional probability. For samples
    of data, the Bayesian inference equation is:39 (7) where, P() is a prior belief
    of the parameters of the distribution. P() is the likelihood of each sample in
    with respect to the parameters defined in prior belief. P() is a posterior probability
    of parameters, estimated from prior belief of parameters and likelihood of the
    samples. Data in is chosen as samples to estimate parameters in this technique.
    The likelihood of containers in is calculated via estimated parameters and stored
    in . Using likelihood values, posterior probability of the containers is measured.
    During downscale, the container with maximum posterior probability is selected.
    The range of containers compared for getting maximum posterior is between up-scaled
    container value to the container value calculated for down-scale. Suppose is the
    up-scaled container value and is the calculated down-scale container value and
    , then the container chosen for downscale is calculated via following equation.
    (8) With every new data, likelihood and posterior of each container in is calculated
    and prior is updated with the new posterior for next arrival. These three estimation
    methods require data to be stored for estimation. With unknown parameters that
    are mean and standard deviation of the current arrival rate, the scaling is based
    on median based thresholding. During thresholding, the arrival rate samples are
    collected for determining the parameters. Once the sufficient samples are collected,
    the parameters of the arrival rate are calculated. If the mean of arrival rate
    is average, the scaling is performed using maximum likelihood estimation. If the
    mean is lower than the average, estimation time can be longer while reducing the
    number of scaling. Bayesian inference is used to scale in case of low arrival
    rate as it provides a better estimate but it is slow. However, scaling is performed
    using Bayesian inference only on non-critical data. If the mean arrival rate is
    average and parameters are unknown, the median based thresholding is used to scale.
    In case of average arrival rate with large number of collected data, the parameters
    are calculated. Scaling is performed via maximum likelihood estimation method.
    In case of critical data, median based thresholding is applied as it is faster
    than other two. Rather than auto-scaling over incoming toilet data on daily basis,
    the proposed algorithm auto-scales over toilet data collected at hourly basis,
    as rural toilets may have rush during certain hours one day, rush in the morning
    hours or evening hours of next day, or no rush throughout the other day. With
    auto-scaling and on-time analysis, the response time of the information fusion
    model is calculated as (9) The deviation in response time in switching from support
    mode to stand-alone mode is calculated from Equations (1) and (9) is as follows:
    (10) In this calculation, is assumed to be 100 times slower of and is assumed
    same in both modes. is added to the deviation and it is kept minimum as possible.
    However, if is not considered, additional delay time in analysis would have been
    added depending on the data in the system and the queue. Computational complexity
    of the proposed auto-scaling algorithm (Algorithm 1) is as follows. Calculation
    of current (line 10), calculation of c from current (line 12), calculation of
    current time (line 40), increment of value (line 42), determining value of (lines
    20, 26, and 32), upscaling (line 15) and downscaling of containers (lines 35 and
    37) takes constant time . Appending c to (line 13), calculating median (line 24),
    calculation of and (line 29) and calculation of posterior probability (line 33)
    takes . Sorting takes . Appending and (line 11), calculation of ML (line 18) takes
    . Total time complexity of the proposed algorithm is . Thus, the time complexity
    of auto-scaling is loglinear. Memory is consumed by three single-dimensional arrays
    , and and therefore the space complexity is linear. Since, the input values to
    the algorithm is reset at an hourly basis, thus, the incoming sensor values may
    not get large enough for processing, maintaining near-real time analysis of the
    toilet states with auto-scaling of loglinear time complexity. 4.1.3 Power-saver
    mode The fog node switch to this mode in case of power cuts. During power cuts,
    the IoT devices run over battery power and power cut duration is unknown. If every
    captured data is transmitted to the fog node, which adds to energy consumption
    of sensors. Thus, the sensing unit collects and communicates data in case of detected
    motion (motion data as 1). The fog node has limited resources as well as limited
    battery life. Thus, utilization of system and network resources has to be minimum
    for keeping the fog node working for longer in case of power cuts. A fusion framework
    is proposed running data preprocessing module, data analysis module and decision
    notification module on fog node where data preprocessing module fuses incoming
    sensor data, filters fused data of critical location and data analysis module
    deployed on docker container performs fast analysis and does not auto-scale. Thus,
    in this mode, the toilets having high access rate are prioritized for analysis,
    reducing data communication within modules of the framework on fog node. Data
    preprocessing module finds which toilets are in critical state through fusion,
    having high rush or high ammonia value or low feedback value, and if no toilet
    is filtered as critical, the preprocessing module does not communicate any fused
    data to data analysis module, saving resource consumption of the fog node for
    time being. The working of proposed framework in power-saver mode is shown in
    Figure 7. FIGURE 7 Open in figure viewer PowerPoint Proposed information fusion
    framework during power-saver mode (mode 2). The functionality of preprocessing
    module in this mode of operation is depicted in Algorithm 2. The algorithm follows
    threshold-based data filtering on incoming data to fetch critical data. With thresh-holding,
    the locations generating huge amount of data due to high access rate, is given
    priority compared to locations having low rate of change of environment. The locations
    with high access rate are classified as critical locations and access rate of
    the critical locations are saved in array. Sensed data from such locations are
    filtered based on thresh-hold value. In the use-case application, if the gas value
    greater than 30 or feedback data less than 3, the complete data is filtered as
    critical data. The critical data from critical location is processed by data analysis
    module. The critical location is decided in run-time as access of some of the
    locations may change with respect to time of the day. For example, in morning,
    one location is highly accessed whereas in the evening, the same location generates
    no data. Time complexity of the proposed algorithm is constant and maximum of
    as the access rate is being saved on specific index of array, representing critical
    location. All other statements in Algorithm 2 takes time. Space complexity of
    the algorithm is constant as it does not change with the change in data input
    rate. Algorithm 2. Algorithm of critical location data filter module An analytical
    module has to be run on fog node that consumes minimal system resources to conserve
    battery. The system performance of each trained model on Raspberry Pi running
    over docker containers is depicted in Table 4. TABLE 4. CPU and memory utilization
    of trained machine learning models implemented on Raspberry Pi using docker containers.
    Machine learning models Gauss SVM KNN ANFIS MLP RF CPU utilization (%) 26.16 25.53
    25.44 25.83 25.34 25.26 Memory utilization (%) 4.97 5.22 5.11 4.53 4.61 5 According
    to the table, ANFIS has minimum memory consumption with MLP the next. Among ANFIS
    and MLP, MLP has minimum CPU utilization. Memory consumption of MLP can be reduced
    by reducing number of layers for training, affecting the accuracy. However, CPU
    utilization of ANFIS model cannot be decreased. Thus, MLP is implemented in data
    analysis module to analyse toilet data in case of power cuts. A single docker
    container runs the MLP trained model for analysis and the containers do not auto-scale
    with rush. According to the classified category by MLP, the fog node takes decisions
    and notifies cleaners or the maintenance department or the store room. The response
    time of the information fusion model is calculated as (11) The deviation in response
    time in switching from support mode to power-saver mode is calculated from Equations
    (1) and (11) is as follows: (12) 4.2 Event-driven mode controller The mode in
    which fog node will operate is decided by the event-driven mode controller module.
    The controller handles switching modes of the fog node based on detected events
    in toilet locations. When a person accesses a toilet location, generated data
    from sensors and feedback app is communicated to fog node. The controller acquires
    the data and check the current environment condition for processing the data.
    Two conditions are taken into consideration: network connectivity between fog
    node and the cloud and battery status of the fog node. Network connectivity is
    determined by ping command with cloud. The ping command provides round trip time
    (RTT) between two hosts. Therefore, is the RTT between fog node and cloud. If
    the packet RTT is high or the ping results in “network host is unreachable” message,
    network is considered weak. If the network is good, the fog node mode does not
    change from support mode (default mode 0). During weak network, the fog node generates
    the event and the controller changes the mode of fog node to stand-alone mode
    (mode 1). Moreover, battery status determines power outage in the area. If the
    status is charging/charged, there is power supply otherwise the fog node runs
    on battery with discharging status. If the battery status is discharging, fog
    node generates event. The controller changes the fog node mode to power-saver
    mode (mode 2) irrespective of network connectivity issue. Nevertheless, if the
    power status is charging/charged and the network connectivity is good at both
    ends of fog node, the fog node continues processing in support mode (default mode
    0). Table 5 shows the three modes of the fog node based on different conditions
    of network connectivity and battery status. TABLE 5. Power outage and network
    connectivity status and corresponding mode of fog node. Battery status Network
    connectivity (fog-cloud) Mode Mode value Charging/charged Strong Support mode
    0 Charging/charged Weak Stand-alone mode 1 Discharging - Power-saver mode 2 An
    MQTT broker is placed at the fog node to communicate data with the cloud. The
    same broker is used to communicate data to fusion modules depending on the current
    mode decided by the controller. MQTT is a light-weight, serverless application
    protocol that follows publish/subscribe architecture and is suitable for a fast
    event-driven communication between microservices. MQTT is compared with other
    existing event-driven technologies in Table 6. Moreover, MQTT based event-driven
    micro-service architecture is popular at the cloud for real-time processing.44
    In this work, the event-driven communication is proposed for each mode and within
    microservices performing data analysis, running on a single-device fog node. The
    event-driven communication between each module of each mode at the fog node is
    depicted in Figure 8. In the figure, the fog node publishes limited network connectivity
    event on topic “/network” and event of discharging battery on topic “/battery.”
    The controller publishes the data to broker and the preprocessing module in each
    mode running on docker container is subscribed to the broker. Three topics are
    created as “/support,” “/standalone,” and “/power” for each of the mode of operation.
    Depending on the current mode of fog node decided by the controller, it publishes
    the sensor data to the topic of respective mode. The analysis and decision notification
    modules, each running on separate docker container, are subscribed to the respective
    topic of the respective mode. For example, the preprocessing module of support
    mode publish the preprocessed data on topic “/support/analyse.” The analysis module
    running on the cloud is subscribed to topic “/support/analyse.” Thus, the topics
    are represented as where is current mode, either “support,” “standalone,” or “power,”
    decided by the the event-driven mode controller module, is either “analyse” or
    “decision.” The broker performs topic-based filtering on published data from different
    modules according to the current fog node mode. Therefore, in the proposed fusion
    framework, the event-driven mode controller module decides the mode, and accordingly
    the data is communicated to the running microservices of data preprocessing, data
    analysis, and decision notification modules via MQTT protocol. This provides fast
    switching of modes according to the detected events, making the use-case application
    resilient. TABLE 6. Comparison of MQTT with other existing event-driven technologies.
    Event-driven technology Advantage Disadvantage MQTT Advantage Apache Kafka40 A
    fault-tolerant streaming event data storage and processing framework It is a pull
    based system which is demand-driven, continuous polling with the broker for the
    data is an overhead and data has to be stored until fetched It is push based system,
    which is preferable for event-driven information fusion where event is detected
    and accordingly data is pushed to specific consumer (framework module) Apache
    Pulsar41 A distributed pub/sub messaging that handles streaming data and supports
    message queuing Running zookeeper and bookkeeper is resource overhead over fog
    node MQTT broker is lightweight on fog node, suitable even in power-saver mode
    gRPC42 A remote procedure call framework created by google for fast cross-platform
    communication between micro-services and follows binary, two way communication
    The data is communicated using google-based protobuf format and follows HTTP 2.0
    protocol, transforming every incoming data with respect to protocol and data format
    is time and resource costly for real-time application MQTT runs on popular HTTP
    1.1 and communicates incoming data of any format RabbitMQ43 A software-based broker
    running AMQP protocol for high performance enterprise communication between devices
    and has advanced functioning Header size is 8 byte and does not guarantee data
    delivery during network disconnections Header size is 2 bytes, lightweight packets,
    guarantees delivery suitable for poor network connectivity FIGURE 8 Open in figure
    viewer PowerPoint Event-driven communication between different modules of the
    proposed information fusion framework using MQTT topic-based routing. Gas sensor,
    motion sensor are attached to Arduino Uno and Arduino Uno collects data from the
    sensors. NodeMCU is attached to Arduino Uo to communicate data to Raspberry Pi
    for preprocessing on 802.11 wireless network. Information from feedback app is
    communicated to Raspberry Pi, where it is preprocessed and fused with preprocessed
    sensor data. Thus, hardware required to implement the framework for one toilet
    location are gas sensor (300 INR), motion sensor (239 INR), Arduino Uno kit (1350
    INR), NodeMCU for wireless communication (1000 INR), Raspberry Pi (3000 INR),
    and battery of 9 volt (100 INR). Hence, total implementation cost of the proposed
    framework for single toilet location is 2889 INR and one Raspberry Pi as fog node
    handling multiple toilet locations costs to 3100 INR. In the proposed framework,
    Raspberry Pi gets two MQTT packets acquired by event driven mode controller. In
    mode 0, event-driven mode control communicates the data to preprocessing module
    that fuses the data and further communicates it to the cloud for analysis and
    cloud sends the analyzed data to decision notification through MQTT (as shown
    in Figure 8). The decision notification module publish the decision for subscribers
    using MQTT protocol. Therefore, total 6 MQTT packets are communicated on network
    in mode 0 with 1 packet communicated within cloud and 1 packet within fog. In
    mode 1 and mode 2, the data is analyzed and the decision is published for subscribers
    at the fog node itself. Thus, in these modes, 3 of the packets are communicated
    within device. Moreover, in mode 1, with one up-scaled container, MQTT packet
    number within device is increased by 1 and with down-scale, MQTT packet is decremented
    by 1. Each MQTT packet on network has overhead of MQTT header of 2 bytes, TCP
    header of 40 bytes, IP header of 20 bytes, 802.11 frame header of 36 bytes with
    actual information. MQTT packet communicated within device use Ethernet 802.3
    instead of 802.11 whose header size is 18 bytes. Furthermore, for each data communication,
    MQTT exchange total of 6 control packets between publisher-broker-subscriber.
    Thus, information overhead in mode 0 is 6 *[4*(2+40+20+36) +2*(2+40+20+18)] which
    is equal to 3312 bytes. Information overhead in mode 1 and mode 2 is which equals
    to 3204 bytes. With auto-scaling to containers and value of is greater than 1,
    the information overhead in mode 1 increases by bytes where is equal to that is
    480 bytes overhead per up-scaled container. Next section discuss experiments conducted
    on the use-case to validate the proposed framework in terms of accuracy as well
    as timeliness of the application dealing with stochastic conditions. 5 EXPERIMENTS
    AND RESULTS This section validates the effectiveness of the proposed event-driven
    fusion framework in maintaining timely and accurate service when dealing with
    stochastic environment. Multiple experiments are conducted to verify the classification
    performance, timeliness of the application running the proposed framework as well
    as system performance of the fog node. For performing experiments, a prototype
    of smart sanitation system application is developed with Raspberry Pi 3 model
    B taken as a fog node. The institute lavatories have gas and motion sensors embedded
    on Arduino Uno device that collects restroom data and communicates it to the fog
    node. A mosquito MQTT broker runs on fog device at port 1883. The fog node is
    installed with docker containerization technology to run preprocessing module,
    data analysis module and decision notification module. Docker image is created
    having trained ANFIS and MLP models for analysis. The docker containers of the
    the cloud and the fog node are subscribed to port 1883 for preprocessing, analysing
    published data with respect to detected events. A docker container is dedicated
    to auto-scale the analysis-specific containers within the fog node. Docker image
    of the proposed auto-scaling algorithm is created using python 3.9 libraries.
    Four experiments are carried out to measure the performance of the proposed event-driven
    fusion framework. Experiment 1 in Section 24 compares response time of the proposed
    information fusion framework for the use-case application switching modes in presence
    of limited network and power cuts with default fusion framework (refer Section
    11). Experiment 2 in Section 25 evaluates the proposed auto-scaling algorithm
    in terms of total scaling count and total scaling time with respect to different
    toilet access rate. The performance of Raspberry Pi as fog node in power-saver
    mode is evaluated in experiment 3 in Section 26. Experiment 4 in Section 27 studies
    deviation in response time of the application running proposed fusion framework
    in presence of mixed stochastic events of limited network and power outage. 5.1
    Experiment 1: Comparison with default framework In this experiment, the effect
    on the response time of the application in case of good network, limited connectivity,
    and power cuts is calculated. The experiment is performed with nine different
    scenarios to measure the deviation in response time of the application running
    proposed framework with respect to response time of the application in case of
    default mode. The situations of rural and semi-urban regions considered for the
    experiment are as follows: 10% of the day, network connectivity is limited. 30%
    of the day, network connectivity is limited. 50% of the day, network connectivity
    is limited. 10% of the day, power cuts occur. 30% of the day, power cuts occur.
    50% of the day, power cuts occur. 10% of the day, power cuts occur and 20% of
    the day, network connectivity is limited. 20% of the day, power cuts occur and
    10% of the day, network connectivity is limited. 25% of the day, power cuts occur
    and 25% of the day, network connectivity is limited. The response time of the
    application in each scenario is depicted in Figure 9. Results in Figure 9A–C corresponds
    to situations 1, 2, and 3 respectively. Figure 9A–C shows deviation in response
    time with mode 0 and mode 1 having network delay between fog and cloud as 0.05,
    0.15, 0.2, and 0.4 s respectively to handle limited network delays. Figure 9D–F
    depicts power cut scenario for situations 4, 5, and 6 respectively. During power
    cuts, the proposed fusion framework switch to mode 2. As seen in the graphs, during
    power cut duration, the response time fluctuates as per incoming critical data.
    Overall, the average response (depicted by black dotted line) during power cut
    is faster in mode 2 with respect to response time in mode 0. Fast response means
    computational resources are used for lesser amount of time, saving energy of the
    fog node. Figure 9G–I depicts scenario for situations 7, 8, and 9 respectively.
    With limited network induced delays, the average response time increases in case
    of power cuts. However, it is lower than the response time with mode 0 running
    in scenarios 7, 8, and 9. FIGURE 9 Open in figure viewer PowerPoint Response time
    of use-case application with different modes of operation in case of limited network
    and power outages. Tables 7 and 8 reports the response time range, in case of
    good network and in case of situations 1, 2, 3, with fog node in default framework
    (mode 0) and in proposed framework with mode 0 and mode 1, respectively. The values
    are calculated with respect to arrival rates of 50, 100, and 250 events/min. The
    response time of proposed framework in different situations with varied delays
    (6.66, 7.76, 8.86 s), have minimal deviation from response time of use-case application
    in good network (6.12 s) (refer Table 7). Moreover, the range of response time
    in default framework increases by 4, 8, and 16 s, with increase in rate of limited
    network delay by 10%. Whereas, in proposed framework, the response time is constant
    in presence of delays. Furthermore, with increasing delay, the average response
    time in proposed framework reduces compared to average response time in default
    framework. Therefore, the response time of the application performing fusion with
    proposed framework, has better performance compared to response time of fusion
    performed with default framework in presence of scenarios 1, 2, and 3. TABLE 7.
    Performance of application in default mode. Good network (%) Limited network (%)
    Events/minute Delay (s) Response time (s) 100 0 50 0 6.12 100 12.24 250 30.6 90
    10 50 [0.05–0.4] [6.37–8.12] 100 [12.73–16.25] 250 [31.91–40.56] 70 30 50 [0.05–0.4]
    [6.87-12.114] 100 [13.73–24.23] 250 [34.35–60.6] 50 50 50 [0.05–0.4] [7.37–16.11]
    100 [14.74–32.23] 250 [36.85–80.6] TABLE 8. Performance of application with proposed
    fusion framework in stand-alone mode 1. Good network (%) Limited network (%) Events/minute
    Delay (s) Response time (s) 90 10 50 [0.05–0.4] 6.66 100 13.33 250 33.30 70 30
    50 [0.05–0.4] 7.76 100 15.531 250 38.82 50 50 50 [0.05–0.4] 8.86 100 17.72 250
    44.3 Table 9 shows the measured response time for 10%, 30%, and 50% power outage
    in a day with fog node running proposed fusion framework with mode 0 and mode
    2 The values are calculated for arrival rates of 50, 100, and 250 events per minute.
    As seen in the table, with high power cut duration, there is slightly increase
    in the response time. Moreover, the response time has constant increase with increasing
    events/minute and has response time equal to time taken by fusion in default framework
    running in good network connectivity (refer Table 7). TABLE 9. Performance of
    application with proposed fusion framework in power-saver mode 2. Good network
    (%) Power outage (%) Events/minute Response time (s) 90 10 50 6.11 100 12.23 250
    30.57 70 30 50 6.264 100 12.53 250 31.32 50 50 50 6.36 100 12.72 250 31.81 Thus,
    the results show that performance of application with proposed fusion framework
    for the use-case application switching mode 1 and mode 2 as per detected event,
    has better performance than default framework of use-case application in dealing
    with the listed nine scenarios of events. 5.2 Experiment 2: Auto-scaling edge
    intelligence in stand-alone mode (Mode 1) In experiment 1, the proposed fusion
    framework is validated in dealing with stochastic environment conditions such
    as limited network connectivity and power cuts. This experiment evaluates the
    performance of the proposed auto-scale algorithm implemented on fog node, handling
    limited network connectivity in mode 1 with dynamic data rate determined by frequency
    of change in toilet state (considered as an event). The performance of the auto-scale
    algorithm is determined through total number of upscale and downscale with unknown
    dynamic event arrivals. The algorithms compared for pro-active scaling are statistical
    based (mean and median) and probability based (maximum likelihood estimation and
    Bayesian inference). Response time of median, maximum likelihood estimation and
    Bayesian inference is depicted in Figure 10. Figure 11 shows upscale and downscale
    count of each algorithm for incoming number of events as 100, 1000, 10 000, and
    50 000. Figure 11A,C,E,G represent upscale and Figure 11B,D,F,H represent downscale
    count for 100, 1000, 10 000, and 50 000 events respectively. As seen in the figure,
    at 100 events, Bayesian inference model performs better than other algorithms.
    After 100 events, maximum likelihood estimation method performs better than other
    algorithms. After 1000 events, statistical median performs close to maximum likelihood
    estimation and Bayesian inference. Thus, in case when there are fewer incoming
    events throughout the day, Bayesian inference performs well. Moreover, maximum
    likelihood needs to have samples to estimate the distribution parameter. For higher
    number of events, maximum likelihood estimation is good for auto-scaling. With
    unknown distribution parameter and high rate of event handling, median based statistical
    method performs better. FIGURE 10 Open in figure viewer PowerPoint Response time
    of Bayesian inference, median, maximum likelihood estimation with input of 500
    events. FIGURE 11 Open in figure viewer PowerPoint Performance comparison of algorithms
    for pro-active auto-scaling in terms of number of upscale and downscale for stochastic
    event arrival rate. Different scenarios are considered to prove the performance
    of proposed algorithm, scaling on hourly basis. The proposed algorithm is compared
    with running individual algorithms scaling on arrival rate of daily basis. The
    algorithms chosen for comparison are Bayesian inference for low arrival rate,
    median based thresholding for low, average, or high arrival rate with unknown
    data distribution and maximum likelihood estimation for low, average, or high
    arrival rate estimating distribution parameters and on daily basis. Table 10 depicts
    the up-scale and down-scale values of the proposed auto-scaling approach compared
    with auto-scaling performed by individual algorithms. Event arrival rate at the
    fog node is varied to have different situations for estimating the performance.
    Range means number of toilet users, represented as an event for simplicity as
    they change the environment state, arrived every hour as between 0 and value .
    When =10, it means maximum 10 events are generated per hour. Thus, in such case,
    the arrival rate is low. In this scenario, the proposed algorithm is compared
    with all three individual algorithms. With to , the arrival rate is assumed to
    be average. Value of greater than 100, it means more than 100 users arrived for
    using toilet which is considered as high arrival rate for countryside. In case
    of average and high arrival rates, the proposed algorithm is compared with maximum
    likelihood estimation and median. Three hybrid situations are considered. Hybrid
    1 considers during first 12 h and during next 12 h of the day. Hybrid 2 considers
    , , and for every 8-h period of the day. This means first 8 h has event arrival
    rate between 0 and 50 per hour, next 8 h having 0 to 500 events/hour and last
    8 h of the day has rate between 0 and 100 events/hour. Hybrid 3 considers , ,
    and after every four hours of the day. With every situation, the individual algorithm
    performs differently. In , median performs better than MLE whereas with , MLE
    performs better than median. In hybrid situations, MLE performs better in hybrid
    1 and hybrid 3, whereas median performs better in hybrid 2. Thus, individual algorithms
    may not provide consistent performance in presence of stochastic events. In all
    the situations except in arrival rate of Range (1,10), number of upscale and downscale
    is minimum in the proposed pro-active auto-scaling approach. In range (0,10),
    the upscale and downscale in the proposed approach is close to Bayesian approach
    and 1.5 times fast in scaling containers compared to Bayesian approach. Thus,
    the proposed approach is efficient in proactively as well as reactively handling
    dynamic state of change of environment at the network edge in minimum time and
    with minimum number of scaling operations. TABLE 10. Performance of proposed algorithm
    in terms of up-scale and down-scale count handling stochastic arrival of events.
    Arrivals/hour Algorithm Upscale count Down scale count Range (0, 10) Bayesian
    estimation 22 22 Maximum likelihood estimation 66 66 Median 39 39 Proposed algorithm
    30 34 Range (0, 50) Maximum likelihood estimation 254 254 Median 222 222 Proposed
    algorithm 204 213 Range (0, 100) Maximum likelihood estimation 579 579 Median
    511 511 Proposed algorithm 455 454 Range (0, 500) Maximum likelihood estimation
    490 490 Median 654 654 Proposed algorithm 427 428 Hybrid 1 (100, 10) Maximum likelihood
    estimation 274 274 Median 306 306 Proposed algorithm 248 248 Hybrid 2 (50, 500,
    100) Maximum likelihood estimation 908 908 Median 853 853 Proposed algorithm 814
    824 Hybrid 3 (10, 500, 100, 50) Maximum likelihood estimation 706 706 Median 1230
    1230 Proposed algorithm 617 626 5.3 Experiment 3: Fog node performance in power-saver
    mode (Mode 2) This experiment studies the performance of fog node in power-saver
    mode (mode 2) using MLP as edge analytical model for the use-case application.
    The performance is compared with the classification performance of fog node using
    ANFIS as edge analytical model. Figure 12 depicts confusion matrices of the edge
    analytical models where Figure 12A is the confusion matrix of ANFIS model running
    as edge analytical model in mode 0. Figure 12B–E are confusion matrices of ANFIS
    and MLP models analysing together in proposed framework in good network scenarios
    having power cuts as 10%, 30%, 50%, and 90% respectively. FIGURE 12 Open in figure
    viewer PowerPoint Confusion matrices of edge analytical model in mode 1 and mode
    2 of proposed fusion framework. The system and classification performance of fog
    node performing fusion in mode 2 is calculated in Table 11. The system performance
    is measured as CPU utilization and memory utilization of proposed fusion framework
    on Raspberry Pi. The classification performance is depicted in terms of accuracy
    in classifying toilet state in five categories. The performance is compared to
    fusion performed using ANFIS model at edge in case of power cuts. The CPU utilization
    is reduced approximately by 4% as the duration of power cut increases. Moreover,
    memory utilization increases by 0.2% and becomes constant on 1.1% with increase
    in power cut duration. Accuracy increases by 1% with increase in power cut duration
    to 50% of the day. After 50%, the accuracy reduces gradually. However, the accuracy
    using ANFIS and MLP together is higher than the accuracy of ANFIS alone for classification.
    Thus, MLP used in proposed framework in power-saver mode (mode 2) provides good
    accuracy classifying data as well as utilizes less fog node resources compared
    to ANFIS model running at edge in default framework. TABLE 11. System and classification
    performance of fog node in mode 0 and mode 2. Good network (%) Power outage (%)
    CPU utilization (%) Memory utilization (%) Accuracy(%) 100 0 96.4 0.9 98.4 90
    10 95.8 1 99.4 70 30 95.2 1.1 99.2 50 50 94.4 1.1 99.3 10 90 92.8 1.1 98.8 5.4
    Experiment 4: Timeliness of the proposed framework This experiment studies deviation
    in average response time with respect to incoming events in case of limited network
    and power cut. The average response time of the application by the proposed fusion
    framework and respective deviation from response time during good network for
    10 000 events is quantified in Table 12. According to the experiment, the average
    deviation from response time of 10 000 events with proposed fusion framework in
    presence of stochastic environment is 2.8%. Figure 13 depicts average response
    time for events handled by the proposed fusion framework. In the figure, yellow
    line represents the response time of application having good network connectivity.
    The time taken by the application running in mode 0 to respond in case of limited
    network, is depicted by blue line. The response time of the application running
    in mode 0 in case of power cuts, is depicted by green line. Red line represents
    the response time of the proposed fusion framework to handle incoming events in
    stochastic environment with switching modes. The application with the proposed
    framework is tested for 10 000 events. The average response time is represented
    by blue dotted lines, green dotted lines, and red dotted lines respectively. TABLE
    12. Deviation in response time of proposed fusion framework in mode 1 and mode
    2 in presence of stochastic conditions as events compared to response time in
    mode 0 with good network connectivity. Good network (%) Limited network (%) Power
    outage (%) Average response time Deviation (%) 100 0 0 0.1224 0 90 10 0 0.1260
    2.94 70 30 0 0.1309 6.94 90 0 10 0.1230 0.49 70 0 30 0.122 −0.32 70 20 10 0.1286
    5.06 70 10 20 0.1246 1.71 FIGURE 13 Open in figure viewer PowerPoint Average response
    time of application with proposed fusion framework in different scenarios of limited
    network and power outages. Figure 13A,B depicts average response time in application
    facing less connectivity issues and application suffering more connectivity issue
    respectively. Average response time of the proposed framework in mode 0 depicted
    by blue dotted line has shifted up with huge margin as the network connectivity
    duration increases. However, average response time by the proposed fusion framework
    in mode 1 has slightly shifted upwards with increase in connectivity issue. Additionally,
    the average response time in mode 1 is close to yellow line which ensures the
    deviation in response time is small running in mode 1, dealing with network connectivity
    issue. Figure 13C,D shows average response time in application suffering less
    duration of power cuts and long duration of power cuts respectively. In Figure
    13C, the response time is calculated using ANFIS model running at network edge
    during power cut. Figure 13D depicts the response time of proposed fusion framework
    running in mode 2. As seen in both the figures, the green dotted line shifts upward
    in case of more power cuts. However, the red dotted line has shifted downwards
    with increase in power cut duration. Thus, the proposed fusion framework running
    in mode 2 of fog node, provides fast response with increase in duration of power
    cuts. This experiment validates that the response time of the application is not
    much affected with the proposed fusion framework in case of power cuts. Figure
    13E,F depicts response time of application in case of both limited network as
    well as power cuts. The former figure represents response time in large limited
    network duration and small power cut duration. The latter figure plot responses
    in large power cut duration and small limited network duration. The red dotted
    line goes downwards in the latter figure confirming that there is minimum effect
    on response time, with proposed framework switching modes, in presence of both
    network connectivity issue and power cuts. Thus, the proposed event-driven information
    fusion framework maintains response time of the system in stochastic presence
    of limited network connectivity or power cuts or both and confirms the resiliency
    of the proposed framework. 5.5 Experiment 5: Computational complexity This experiment
    studies time and space complexity of the proposed framework. Time complexity is
    calculated by executing “time” command on each module (preprocessing, data analysis,
    and decision notification) and summing it up to calculate total time taken to
    execute all the statements of the framework. Since complexity varies with input
    data size, the framework is executed on input data ranging from 1 to 50 000. First
    graph of Figure 14 shows the time complexity of the proposed framework having
    input data on x axis and corresponding execution time the framework instruction
    takes on y axis in seconds. According to the graph, the execution time of the
    framework doubles with doubling of input data. FIGURE 14 Open in figure viewer
    PowerPoint Computational complexity of the proposed framework. Furthermore, space
    complexity is calculated by determining memory utilization of the proposed framework
    with respect to increase in input data size. Second graph of Figure 14 shows space
    complexity of the proposed framework with constant memory utilized in megabytes
    on y axis even with increased input data on x axis. The reason behind constant
    space complexity is that the input data is executed in real time and not stored
    on fog node or cloud. Thus, according to empirical analysis of the proposed framework,
    the time complexity is linear with and space complexity is constant with . 5.6
    Experiment 6: Scalability analysis Fog node will scale differently at different
    modes of operation with maximum scaling at mode 0 as it only preprocesses the
    incoming data and minimum scaling at mode 1 due to multiple containers providing
    edge analytics. For each event, preprocessing at mode 0 takes 0.03. At mode 1,
    preprocessing and auto-scaling takes average response time of 0.031 s with analytical
    model running on container having average response time of 0.003 s. At mode 2
    fog node runs one docker container analysing only critical locations with average
    response time of 0.002 s, conserving battery power with average response time
    of the framework for each event as 0.032. Thus, to evaluate the robustness of
    the proposed framework, the scalability of fog node is determined at mode 0, mode
    1, and mode 2 in terms of events per minute, memory and CPU utilization is shown
    in Table 13. TABLE 13. Total events handled by fog node at each mode of operation.
    Mode Events/minute Memory (MB) CPU utilization (%) 0 1980 71 25  1 [1764–1860]
    [161–831] [50–98.5] 2 1875 95 50 Figure 15 depicts the number of events processed
    at fog node with auto-scaled container in mode 1. According to Figure 15, the
    maximum number of containers up to which edge analytics can be scaled is 8 as
    Raspberry pi 3 memory is 1 GB. Thus, the proposed framework is robust in three
    modes of operation scaling to maximum of 1980 events/min in mode 0, 1860 events/min
    in mode 1 and 1875 events/min in mode 2 with optimal CPU and memory utilization.
    FIGURE 15 Open in figure viewer PowerPoint Scalability analysis of the proposed
    framework in three modes with respect to events/minute. 5.7 Experiment 7: Energy
    consumption Fog node switches to different modes depending on detected event and
    scale according to toilet rush. During mode 0 and mode 1, there is continuous
    electric supply and fog node switch to mode 2 in case of power cut. The default
    power consumption of Raspberry Pi is 13.85 W. With data preprocessing (mode 0),
    power consumption rises to 18 W. Running container on Raspberry Pi consumes 14.1
    W. Preprocessing the data and running container analyzing the data using single
    container consumes 20 W (mode 2). Auto-scaling containers for analysis on preprocessed
    data consumes 22 W of battery (mode 1). Figure 16 shows energy consumption of
    framework in three modes. According to the figure the average energy consumption
    per day is 0.48 kWh and per month is 14.4 kWh. Raspberry Pi 3 Model B consumes
    about 260 mA of current at 5.0 V (which is about 1.3–1.4 W) when no USB devices
    are connected to it and it is in idle state (Figure 16). (13) FIGURE 16 Open in
    figure viewer PowerPoint Energy consumption of the proposed framework in three
    modes daily and monthly. Converting kWh to Ah (dividing kWh by voltage of 5.0
    V), 4 Ah of energy is drawn from a discharging battery in mode 2. According to
    Equation (13), the total hours Raspberry Pi as fog node in use-case application
    can operate, continuously preprocessing and analyzing data, in mode 2 is 15 h.
    Depending on critical locations being analyzed, the fog node can be alive for
    long time unless the power cut is more than a day and there is no charging at
    all. 6 CONCLUSION Developing countries suffer from limited network connectivity
    as well as frequent power cuts. Current real-time edge intelligence-based information
    fusion frameworks are not suitable to withstand such stochastic environment conditions.
    To handle stochastic environment conditions efficiently, an event-driven information
    fusion framework is proposed. In the proposed framework, the fog node runs in
    three different modes: mode 0, mode 1, and mode 2. A novel auto-scaling technique
    is proposed to scale containers running edge analytical model, reactively as well
    as proactively, as per dynamic environment state change. The framework is developed
    on smart sanitation system application considered as a use-case. The experimental
    results shows that the proposed information fusion framework performs efficiently
    in presence of short or long duration of power cuts and limited network connectivity,
    with minimal deviation in response time and maintains the classification accuracy.
    Furthermore, the proposed auto-scaling technique is fast, resource efficient and
    has minimum number of upscales and downscales. Limitation of the proposed framework
    is that it is only applicable to the IoT applications that are monitoring large-scale
    locations. Moreover, the performance of application can be affected in case of
    no power or limited network connectivity for multiple days, as fog node batteries
    may die out. In such cases, multiple fog nodes need to be deployed as backup or
    the fusion framework need to be light-weight for increasing fog node lifetime
    in such scenarios. In future work, the performance of fog node, with switching
    modes and scaling containers, in case of no network connectivity or electricity
    or in the presence of calamities, would be studied. FUNDING INFORMATION This work
    is fully funded by the Ministry of Education (MoE), Government of India. CONFLICT
    OF INTEREST STATEMENT The authors declare no potential conflict of interests.
    Open Research REFERENCES Volume34, Issue8 August 2023 e4804 Figures References
    Related Information Recommended Fog Computing Model for Evolving Smart Transportation
    Applications M. Muzakkir Hussain,  Mohammad Saad Alam,  M.M. Sufyan Beg Fog and
    Edge Computing: Principles and Paradigms, [1] A Smart Payment Transaction Procedure
    by Smart Edge Computing Smart Edge Computing: An Operation Research Perspective,
    [1] Towards a data‐driven IoT software architecture for smart city utilities Yogesh
    Simmhan,  Pushkara Ravindra,  Shilpa Chaturvedi,  Malati Hegde,  Rashmi Ballamajalu
    Software: Practice and Experience Multi-objective metaheuristic optimization-based
    clustering with network slicing technique for Internet of Things-enabled wireless
    sensor networks in 5G systems B. Gracelin Sheena,  N. Snehalatha Transactions
    on Emerging Telecommunications Technologies Lesson 28: Developing Web Applications
    with WebSockets Java® Programming: 24‐Hour Trainer, [1] Download PDF Additional
    links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms of Use About Cookies Manage
    Cookies Accessibility Wiley Research DE&I Statement and Publishing Policies Developing
    World Access HELP & SUPPORT Contact Us Training and Support DMCA & Reporting Piracy
    OPPORTUNITIES Subscription Agents Advertisers & Corporate Partners CONNECT WITH
    WILEY The Wiley Network Wiley Press Room Copyright © 1999-2024 John Wiley & Sons,
    Inc or related companies. All rights reserved, including rights for text and data
    mining and training of artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: Transactions on Emerging Telecommunications Technologies
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An event-driven fusion framework with auto-scaling of edge intelligence for
    resilient smart applications in developing countries
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wang R.
  - Wang Y.
  - Yin P.
  - Qi J.P.
  - Sun Y.T.
  - Li Q.
  - Zhang Y.D.
  - Zhang M.K.
  citation_count: '0'
  description: 'With the rapid arrival of the Internet of Everything era, massive
    data resources are generated on edge sides, causing problems such as large network
    load, high energy consumption, and privacy security in traditional distributed
    training based on cloud computing. Edge computing sinks computing power resources
    to the edge side, forming a collaborative computing system that integrates “cloud,
    edge, and end,” which can meet the basic needs of real-time operations, intelligence,
    security, and privacy protection. With the help of edge computing capabilities,
    edge intelligence effectively promotes the intelligent development of the edge
    side, which has become a popular topic. Through our research, we found that edge
    collaborative intelligence is currently in a stage of rapid development. At this
    stage, several deep learning models are combined with edge computing, and many
    edge collaborative intelligent processing solutions have exploded, such as distributed
    training in edge computing scenarios, federated learning, and distributed collaborative
    reasoning based on technologies such as model cutting and early exit. The combination
    of a shallow breadth learning system and virtualization technology allows for
    quick implementation of edge intelligence, which considerably improves service
    quality and user experience and makes services more intelligent. As a key link
    of edge intelligence, edge intelligence collaborative training aims to assist
    or implement the distributed training of machine learning models on the edge side.
    However, in an edge computing scenario, the distributed training of the model
    must coordinate several edge nodes, and many challenges remain. Therefore, by
    fully investigating the existing research foundation of edge intelligent collaborative
    training, we focus on the challenges and solutions of edge intelligent collaborative
    training in edge scenarios such as equipment heterogeneity, limited equipment
    resources, and unstable network environments. This paper introduces and summarizes
    the overall architecture and core modules of edge intelligent collaborative training.
    The overall architecture mainly focuses on the interaction framework between edge
    devices. In terms of whether there is a central server role, it can be divided
    into two categories: parameter server centralized architecture and fully decentralized
    parallel architecture. The core module of edge intelligent collaborative training
    mainly focuses on the problem of collaborative training of a large number of edge
    devices for neural network models to update parameters. In terms of the role of
    parallel computing in model training, it is divided into data parallelism and
    model parallelism. Finally, the many challenges and prospects of edge collaborative
    training are analyzed and summarized.'
  doi: 10.13374/j.issn2095-9389.2022.09.26.004
  full_citation: '>'
  full_text: '>

    "VISIT DOI.ORG DOI NOT FOUND 10.13374/j.issn2095-9389.2022.09.26.004 This DOI
    cannot be found in the DOI System. Possible reasons are: The DOI is incorrect
    in your source. Search for the item by name, title, or other metadata using a
    search engine. The DOI was copied incorrectly. Check to see that the string includes
    all the characters before and after the slash and no sentence punctuation marks.
    The DOI has not been activated yet. Please try again later, and report the problem
    if the error continues. WHAT CAN I DO NEXT? If you believe this DOI is valid,
    you may report this error to the responsible DOI Registration Agency using the
    form here. You can try to search again from DOI.ORG homepage REPORT AN ERROR DOI:
    URL of Web Page Listing the DOI: Your Email Address: Additional Information About
    the Error: More information on DOI resolution: DOI Resolution Factsheet The DOI
    Handbook Privacy Policy Copyright © 2023 DOI Foundation. The content of this site
    is licensed under a Creative Commons Attribution 4.0 International License. DOI®,
    DOI.ORG®, and shortDOI® are trademarks of the DOI Foundation."'
  inline_citation: '>'
  journal: Gongcheng Kexue Xuebao/Chinese Journal of Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Survey of edge–edge collaborative training for edge intelligence
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Serrano M.
  - Khorsand B.
  - Soldatos J.
  - Troiano E.
  - Neises J.
  - Kranas P.
  - Perakis K.
  - Mamelli A.
  - Elicegui I.
  - Kyriazis D.
  - Makridis G.
  - Sanchez G.
  - Cugurra M.
  citation_count: '0'
  description: 'This third and final part of the INFINITECH book series begins by
    providing a definition for Fintech, namely: the use of technology to underpin
    the delivery of financial services. The book further discusses why Fintech is
    the focus of industry nowadays as the waves of digitization and the way financial
    technology (FinTech) and insurance technology (InsuranceTech) are rapidly transforming
    the financial and insurance services industry. The book also introduces technology
    assets that follow the Reference Architecture (RA) for BigData, IoT and AI applications.
    Moreover, the series of assets includes the domain area where applications from
    the INFINITECH innovation project and the concept of innovation for the financial
    sector are described. Further described is the INFINITECH Marketplace and its
    components including details of available assets, as well as a description of
    solutions developed in INFINITECH.'
  doi: 10.1561/9781638282334
  full_citation: '>'
  full_text: '>

    "About Us Alerts Contact Ordering Info Help Log in Home FnTs Journals Books NowOpen
    Technical Financial Innovation, Solving the Interoperability Problems of Europe:
    The INFINITECH Legacy By Martín Serrano, NUIG-Insight, Ireland, martin.serrano@insight-centre.org
    | Bardia Khorsand, NUIG-Insight, Ireland | John Soldatos, INNOV, Cyprus | Ernesto
    Troiano, GFT, Italy | Juergen Neises, FTSG, Germany | Pavlos Kranas, LXS, Spain
    | Kostis Perakis, UBI, Greece | Alessandro Mamelli, HPE, Italy | Ignacio Elicegui,
    ATOS, Spain | Dimosthenis Kyriazis, UPRC, Greece | George Makridis, UPRC, Greece
    | Gisela Sanchez, FI, France | Marina Cugurra, GFT, Italy Publication Date: 03
    Jul 2023 Suggested Citation: Martín Serrano, Bardia Khorsand, John Soldatos, Ernesto
    Troiano, Juergen Neises, Pavlos Kranas, Kostis Perakis, Alessandro Mamelli, Ignacio
    Elicegui, Dimosthenis Kyriazis, George Makridis, Gisela Sanchez, Marina Cugurra
    (2023), \"Technical Financial Innovation, Solving the Interoperability Problems
    of Europe: The INFINITECH Legacy\", Boston-Delft: now publishers, http://dx.doi.org/10.1561/9781638282334
    Downloaded: 8405 times Description This third and final part of the INFINITECH
    book series begins by providing a definition for Fintech, namely: the use of technology
    to underpin the delivery of financial services. The book further discusses why
    Fintech is the focus of industry nowadays as the waves of digitization and the
    way financial technology (FinTech) and insurance technology (InsuranceTech) are
    rapidly transforming the financial and insurance services industry. The book also
    introduces technology assets that follow the Reference Architecture (RA) for BigData,
    IoT and AI applications. Moreover, the series of assets includes the domain area
    where applications from the INFINITECH innovation project and the concept of innovation
    for the financial sector are described. Further described is the INFINITECH Marketplace
    and its components including details of available assets, as well as a description
    of solutions developed in INFINITECH. Table of Contents Frontmatter Pages i-xxvii
    1. INFINITECH Reference Architecture Overview Pages 1-5 2. Innovative Technologies
    for the Financial Sector Pages 6-77 3. INFINITECH Technology Assets Pages 78-92
    4. INFINITECH Continuous Integration/Continuous Deployment Tools Pages 93-101
    5. INFINITECH Tools and Techniques for Management of Data Sets Pages 102-108 6.
    INFINITECH Reference Testbed and Sandbox Pages 109-127 7. Lessons Learnt from
    INFINITECH Pilots Pages 128-147 8. Conclusions Pages 148-151 9. References Pages
    152-205 Download Chapters ISBN: 978-1-63828-232-7 232 pp. Price: $115.00 Buy Book
    (hb) ISBN: 978-1-63828-233-4 232 pp. Open Access (.pdf) This is published under
    the terms of CC BY-NC Copyright © 2024 now publishers inc. Boston - Delft"'
  inline_citation: '>'
  journal: 'Technical Financial Innovation, Solving the Interoperability Problems
    of Europe: The INFINTECH Legacy'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Technical financial innovation, solving the interoperability problems of
    Europe: The INFINTECH legacy'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Serrano M.
  - Khorsand B.
  - Soldatos J.
  - Troiano E.
  - Neises J.
  - Kranas P.
  - Perakis K.
  - Mamelli A.
  - Elicegui I.
  - Kyriazis D.
  - Makridis G.
  - Sanchez G.
  - Cugurra M.
  citation_count: '0'
  description: In this second part of the INFINITECH book series, which is a series
    of three books, the basic concepts for Fintech referring to the diversity in the
    use of technology to underpin the delivery of financial services are reviewed.
    The demand and the supply side in the financial sector are demonstrated, and further
    discussed is why Fintech is the focus of industry nowadays and the meaning for
    waves of digitization. Financial technology (FinTech) and insurance technology
    (InsuranceTech) are rapidly transforming the financial and insurance services
    industry. An overview of Reference Architecture (RA) for BigData, IoT and AI applications
    in the financial and insurance sectors (INFINITECH-RA) is also provided. Moreover,
    the book reviews the concept of innovation and its application in INFINITECH,
    and innovative technologies provided by the project for financial sector practical
    examples.
  doi: 10.1561/9781638282310
  full_citation: '>'
  full_text: '>

    "About Us Alerts Contact Ordering Info Help Log in Home FnTs Journals Books NowOpen
    Methods and Design Principles for Financial Innovation, Explaining the Supply
    Side for Interoperability in Finance- and Insurance-Tech: The INFINITECH Applications
    and Solutions By Martín Serrano, NUIG-Insight, Ireland, martin.serrano@insight-centre.org
    | Bardia Khorsand, NUIG-Insight, Ireland | John Soldatos, INNOV, Cyprus | Ernesto
    Troiano, GFT, Italy | Juergen Neises, FTSG, Germany | Pavlos Kranas, LXS, Spain
    | Kostis Perakis, UBI, Greece | Alessandro Mamelli, HPE, Italy | Ignacio Elicegui,
    ATOS, Spain | Dimosthenis Kyriazis, UPRC, Greece | George Makridis, UPRC, Greece
    | Gisela Sanchez, FI, France | Marina Cugurra, GFT, Italy Publication Date: 03
    Jul 2023 Suggested Citation: Martín Serrano, Bardia Khorsand, John Soldatos, Ernesto
    Troiano, Juergen Neises, Pavlos Kranas, Kostis Perakis, Alessandro Mamelli, Ignacio
    Elicegui, Dimosthenis Kyriazis, George Makridis, Gisela Sanchez, Marina Cugurra
    (2023), \"Methods and Design Principles for Financial Innovation, Explaining the
    Supply Side for Interoperability in Finance- and Insurance-Tech: The INFINITECH
    Applications and Solutions\", Boston-Delft: now publishers, http://dx.doi.org/10.1561/9781638282310
    Downloaded: 5584 times Description In this second part of the INFINITECH book
    series, which is a series of three books, the basic concepts for Fintech referring
    to the diversity in the use of technology to underpin the delivery of financial
    services are reviewed. The demand and the supply side in the financial sector
    are demonstrated, and further discussed is why Fintech is the focus of industry
    nowadays and the meaning for waves of digitization. Financial technology (FinTech)
    and insurance technology (InsuranceTech) are rapidly transforming the financial
    and insurance services industry. An overview of Reference Architecture (RA) for
    BigData, IoT and AI applications in the financial and insurance sectors (INFINITECH-RA)
    is also provided. Moreover, the book reviews the concept of innovation and its
    application in INFINITECH, and innovative technologies provided by the project
    for financial sector practical examples. Table of Contents Frontmatter Pages i-xxi
    1. FINTECH Services Pages 1-13 2. Applications and Services for the Financial
    Sector Pages 14-19 3. INFINITECH Implemented Solutions Pages 20-145 4. INFINITECH
    Conclusions Pages 146-151 5. References Pages 152-205 Download Chapters ISBN:
    978-1-63828-230-3 226 pp. Price: $115.00 Buy Book (hb) ISBN: 978-1-63828-231-0
    226 pp. Open Access (.pdf) This is published under the terms of CC BY-NC Copyright
    © 2024 now publishers inc. Boston - Delft"'
  inline_citation: '>'
  journal: 'Part II: Methods and Design Principles for Financial Innovation, Explaining
    the Supply Side for Interoperability in Finance- and Insurance-Tech: The INFINITECH
    Applications and Solutions'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Part II: Methods and design principles for financial innovation, explaining
    the supply side for interoperability in finance- and insurance-tech: The INFINITECH
    applications and solutions'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Serrano M.
  - Khorsand B.
  - Soldatos J.
  - Troiano E.
  - Neises J.
  - Kranas P.
  - Perakis K.
  - Mamelli A.
  - Elicegui I.
  - Kyriazis D.
  - Makridis G.
  - Sanchez G.
  - Cugurra M.
  citation_count: '1'
  description: In this first part of the INFINITECH book series, which is a series
    of three books, the principles of the modern economy that lead to make the modern
    financial sector and the FinTech's the most disruptive areas in today's global
    economy are discussed. INFINITECH envision many opportunities emerging for activating
    new channels of innovation in the local and global scale while at the same time
    catapult opportunities for more disruptive user-centric services. At the same
    time, INFINITECH is the result of a sharing vision from a representative global
    group of experts, providing a common vision and identifying impacts in the financial
    and insurance sectors.
  doi: 10.1561/9781638282297
  full_citation: '>'
  full_text: '>

    "About Us Alerts Contact Ordering Info Help Log in Home FnTs Journals Books NowOpen
    Concepts and Design Thinking Innovation Addressing the Global Financial Needs:
    The INFINITECH Way Foundations By Martín Serrano, NUIG-Insight, Ireland, martin.serrano@insight-centre.org
    | Bardia Khorsand, NUIG-Insight, Ireland | John Soldatos, INNOV, Cyprus | Ernesto
    Troiano, GFT, Italy | Juergen Neises, FTSG, Germany | Pavlos Kranas, LXS, Spain
    | Kostis Perakis, UBI, Greece | Alessandro Mamelli, HPE, Italy | Ignacio Elicegui,
    ATOS, Spain | Dimosthenis Kyriazis, UPRC, Greece | George Makridis, UPRC, Greece
    | Gisela Sanchez, FI, France | Marina Cugurra, GFT, Italy Publication Date: 03
    Jul 2023 Suggested Citation: Martín Serrano, Bardia Khorsand, John Soldatos, Ernesto
    Troiano, Juergen Neises, Pavlos Kranas, Kostis Perakis, Alessandro Mamelli, Ignacio
    Elicegui, Dimosthenis Kyriazis, George Makridis, Gisela Sanchez, Marina Cugurra
    (2023), \"Concepts and Design Thinking Innovation Addressing the Global Financial
    Needs: The INFINITECH Way Foundations\", Boston-Delft: now publishers, http://dx.doi.org/10.1561/9781638282297
    Downloaded: 10186 times Description In this first part of the INFINITECH book
    series, which is a series of three books, the principles of the modern economy
    that lead to make the modern financial sector and the FinTech’s the most disruptive
    areas in today’s global economy are discussed. INFINITECH envision many opportunities
    emerging for activating new channels of innovation in the local and global scale
    while at the same time catapult opportunities for more disruptive user-centric
    services. At the same time, INFINITECH is the result of a sharing vision from
    a representative global group of experts, providing a common vision and identifying
    impacts in the financial and insurance sectors. Table of Contents Frontmatter
    Pages i-xxvi 1. INFINITECH and The Global Financial Sector Pages 1-5 2. INFINITECH
    Way Foundations Pages 6-21 3. Reference Architectures Analysis Pages 22-37 4.
    INFINITECH Data Pack Pages 38-48 5. INFINITECH Technologies, Data and Processes
    Pages 49-80 6. INFINITECH Way Foundations Impact on Fintech and Insurance Pages
    81-94 7. Conclusions Pages 95-97 8. References Pages 98-151 Download Chapters
    ISBN: 978-1-63828-228-0 184 pp. Price: $115.00 Buy Book (hb) ISBN: 978-1-63828-229-7
    184 pp. Open Access (.pdf) This is published under the terms of CC BY-NC Copyright
    © 2024 now publishers inc. Boston - Delft"'
  inline_citation: '>'
  journal: 'Part I: Concepts and Design Thinking Innovation Addressing the Global
    Financial Needs: The INFINTECH Way Foundations'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Part I: Concepts and design thinking innovation addressing the global financial
    needs: The INFINTECH way foundations'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wang X.
  - Guo P.
  - Li X.
  - Gangopadhyay A.
  - Busart C.E.
  - Freeman J.
  - Wang J.
  citation_count: '1'
  description: Cloud computing has become a major approach to help reproduce computational
    experiments. Yet there are still two main difficulties in reproducing batch based
    Big Data analytics (including descriptive and predictive analytics) in the cloud.
    The first is how to automate end-to-end scalable execution of analytics including
    distributed environment provisioning, analytics pipeline description, parallel
    execution, and resource termination. The second is that an application developed
    for one cloud is difficult to be reproduced in another cloud, a.k.a. vendor lock-in
    problem. To tackle these problems, we leverage serverless computing and containerization
    techniques for automated scalable execution and reproducibility, and utilize the
    adapter design pattern to enable application portability and reproducibility across
    different clouds. We propose and develop an open-source toolkit that supports
    1) fully automated end-to-end execution and reproduction via a single command,
    2) automated data and configuration storage for each execution, 3) flexible client
    modes based on user preferences, 4) execution history query, and 5) simple reproduction
    of existing executions in the same environment or a different environment. We
    did extensive experiments on both AWS and Azure using four Big Data analytics
    applications that run on virtual CPU/GPU clusters. The experiments show our toolkit
    can achieve good execution performance, scalability, and efficient reproducibility
    for cloud-based Big Data analytics.
  doi: 10.1109/TCC.2023.3245081
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Cloud Co...
    >Volume: 11 Issue: 3 Reproducible and Portable Big Data Analytics in the Cloud
    Publisher: IEEE Cite This PDF Xin Wang; Pei Guo; Xingyan Li; Aryya Gangopadhyay;
    Carl E. Busart; Jade Freeman; Jianwu Wang All Authors 2 Cites in Papers 291 Full
    Text Views Abstract Document Sections I. Introduction II. Background III. Overview
    of Reproducible and Portable Data Analytics in the Cloud IV. Data Modeling and
    Storage for Reproducibility V. Automated Big Data Analytics in the Cloud Towards
    Reproducibility Show Full Outline Authors Figures References Citations Keywords
    Metrics Abstract: Cloud computing has become a major approach to help reproduce
    computational experiments. Yet there are still two main difficulties in reproducing
    batch based Big Data analytics (including descriptive and predictive analytics)
    in the cloud. The first is how to automate end-to-end scalable execution of analytics
    including distributed environment provisioning, analytics pipeline description,
    parallel execution, and resource termination. The second is that an application
    developed for one cloud is difficult to be reproduced in another cloud, a.k.a.
    vendor lock-in problem. To tackle these problems, we leverage serverless computing
    and containerization techniques for automated scalable execution and reproducibility,
    and utilize the adapter design pattern to enable application portability and reproducibility
    across different clouds. We propose and develop an open-source toolkit that supports
    1) fully automated end-to-end execution and reproduction via a single command,
    2) automated data and configuration storage for each execution, 3) flexible client
    modes based on user preferences, 4) execution history query, and 5) simple reproduction
    of existing executions in the same environment or a different environment. We
    did extensive experiments on both AWS and Azure using four Big Data analytics
    applications that run on virtual CPU/GPU clusters. The experiments show our toolkit
    can achieve good execution performance, scalability, and efficient reproducibility
    for cloud-based Big Data analytics. Published in: IEEE Transactions on Cloud Computing
    ( Volume: 11, Issue: 3, 01 July-Sept. 2023) Page(s): 2966 - 2982 Date of Publication:
    15 February 2023 ISSN Information: DOI: 10.1109/TCC.2023.3245081 Publisher: IEEE
    Funding Agency: SECTION I. Introduction Reproducibility is increasingly required
    by the research community, funding agencies, and publishers [1]. By reproducing
    an existing computational experiment and obtaining consistent results, we can
    have more confidence in the research. Further, besides reproducing the exact process,
    it is also valuable to explore how the experiment behaves with different input
    datasets, execution arguments, and environments. Cloud computing has been a major
    approach for reproducibility [2] because cloud services can be leveraged to provision
    data, software, or hardware needed in reproduction. For instance, paper [3] summarized
    13 aspects that cloud computing can help with reproducibility. In this paper,
    we mainly address the following challenges in cloud-based reproducibility. First,
    it is still difficult to achieve end-to-end automated Big Data analytics execution
    and reproduction in the cloud. The end-to-end automation should support scale-up
    and scale-out of distributed hardware environment, software environment provisioning,
    data and configuration storage for each execution, resource termination after
    execution, execution history query and reproducibility of existing executions
    in the same environment or a different cloud environment. Second, because the
    services provided by each service provider such as AWS and Azure are proprietary,
    an application developed for one cloud cannot run in another cloud, which is a
    well-known vendor lock-in challenge. Two scientific problems to be studied by
    tackling challenges are: 1) what is a proper abstraction and design for better
    reproducibility support from both user and toolkit perspectives, 2) what is a
    more efficient way to achieve cloud-based reproducibility for Big Data analytics.
    We note our work only supports batch based processing Big Data analytics jobs,
    including descriptive and predictive analytics, not interactive jobs like database
    queries. Based on the above challenges and scientific problems, we propose an
    approach and corresponding open-source toolkit [4] for Reproducible and Portable
    Big Data Analytics in the Cloud (RPAC). Our contributions are summarized as follows.
    Our proposed approach and toolkit integrate serverless computing techniques to
    automate end-to-end batch based Big Data analytics execution. Tasks of Big Data
    analytics execution (resource provisioning, application execution, data storage
    and resource termination) are encapsulated as cloud functions and automatically
    triggered by proper events. With the full automation support, users can re-run
    the exact execution or run the application with different configurations, including
    different scale-out and scale-up factors, via only one command. Our RPAC toolkit
    supports both AWS and Azure cloud environments. For easy reproducibility, we make
    proper data modeling and abstraction. It first separates essential information
    required for reproducibility and detailed information required by each cloud provider.
    Following the separation of concerns principle, it further separates the essential
    information into three categories (resources, application, personal) for easy
    reconfiguration. The essential information will also be automatically stored in
    the cloud by our toolkit as authentic recording of the execution. Later, the storage
    URL can be published and shared as the single source to reproduce the historical
    execution. To deal with the vendor lock-in challenge, on top of the above abstractions,
    we propose a Cloud Agnostic Application Model (CAAM) to support execution and
    reproducibility portability with different cloud providers. CAAM abstracts the
    application out of its cloud specific logic, and allows reproducing executions
    in another cloud via only minimal configuration changes from the user. We benchmark
    both CPU-based and GPU-based Big Data analytics applications using our RPAC toolkit.
    We measure the overhead of data storing for reproducibility. We also did extensive
    experiments to benchmark three applications on different cloud providers in terms
    of execution performance, scalability and reproducibility efficiency. The rest
    of the paper is organized as follows. In Section II, we briefly introduce related
    techniques our work is built on. Section III provides an overview of our proposed
    approach. Three main parts of our approach, namely data modeling, automated execution
    and reproduction of Big Data analytics in the cloud are explained in Sections
    IV, V and VI, respectively. Experiments and benchmarking results are discussed
    in Section VII. We compare our work with related studies in Section VIII and conclude
    in Section IX. SECTION II. Background A. Big Data Analytics To deal with increasing
    data volumes in data analytics, many platforms have been proposed to achieve parallelization
    of the analytics in a distributed environment. We explain three such platforms
    that our work is built on for reproducibility. As one of the most popular Big
    Data platform, Spark [5] follows and extends the MapReduce paradigm [6] and achieve
    parallelism by distributing input data among many parallel tasks of the same function.
    To run an application, Spark employs a master process on one node and a worker
    process on each of other nodes so the worker processes can take tasks from the
    master process and run them in parallel. Similar to Spark, a Dask [7] application
    is composed as a task graph that can be distributed within one computer or a distributed
    computing environment. Dask employs a similar master-worker framework for task
    scheduling. Horovod [8], as a popular software framework for distributed learning,
    provides data parallel deep learning optimized for GPU-based data analytics. For
    coordinating execution between distributed processes on GPU, Horovod can use Message
    Passing Interface (MPI) for communicating data with high performance. The CUDA-aware
    MPI is commonly used in HPC to build applications that can scale to multi-node
    computer clusters [9]. B. Reproducibility There have been many definitions of
    reproducibility and similar terms like replicability and repeatability [10], [11].
    Unfortunately, these definitions are not very consistent, some even contradict
    with each other [1]. Here, we simply define reproducibility as a capability that
    obtains consistent results using the same computational steps, methods, and code.
    As paper [12] said, containerization is one of the valid and common solutions
    for the reproducible software deployment problem of scientific pipelines. For
    cloud-based reproducibility, it studies how to re-execute an existing application
    in the cloud [3]. We categorize reproducibility support into four ways: 1) rerun
    exactly the same application with the same hardware and software environment,
    2) reproduce with a different application configuration to know how the application
    performs with different datasets or arguments, 3) reproduce with different cloud
    provider hardware environment (virtual machine type and number, etc.) within the
    same cloud provider to test scale-up and scale-out; and 4) reproduce with a different
    cloud provider to avoid vendor lock-in problem. Our toolkit is built to support
    all four types of reproduction. C. Serverless Computing As a recent cloud-based
    execution model, serverless computing provides a few advantages. First, it responds
    to user service requests without maintaining back-end servers in the cloud. Second,
    it employs Function as a Service (FaaS) architecture that allows customers to
    develop separate functions directly rather than standalone cloud applications.
    As explained in [13], each application logic/pipeline is split into functions
    and application execution is based on internal or external events. All major cloud
    providers offer serverless services, including AWS Lambda, Azure Functions and
    Google Cloud Functions. SECTION III. Overview of Reproducible and Portable Data
    Analytics in the Cloud In this section, we provide an overview of how our proposed
    approach achieves reproducible and portable data analytics in the cloud. With
    the approach and corresponding open-source toolkit RPAC for reproducible and portable
    data analytics in the cloud, users can easily re-run previous experiments with
    the same or different setups including environments, application arguments, input
    data and cloud providers. Our approach is built on top of serverless computing
    and we adopt a new way of utilizing serverless computing for large scale computations.
    So we will explain first how to use serverless large scale computations, then
    how to use serverless for Big Data analytics reproducibility. A. Serverless Based
    Reproducibility As shown in Fig. 1, our proposed approach has two parts: 1) first
    execution of an application, and 2) reproduction of the existing execution from
    historical configurations. Both first execution and reproduction are automated
    via serverless-based approach shown on the right. Fig. 1. The overview of our
    proposed approach for reproducible and portable data analytics in the cloud. Show
    All In the beginning, there is no execution history for querying and reproducing.
    Clients need to prepare configurations to generate the pipeline file for the whole
    execution. The configuration includes all configurable setups, for example, the
    application-based information like application programs, arguments, input data,
    and the cloud-based information like virtual cluster type, size, network setting,
    memory, with personal credentials. Our toolkit will take this information to create
    an executable pipeline for a target cloud. With this pipeline, the data analytics
    application will execute in the cloud environment, output its results to the storage,
    and automatically terminate resources once the execution finishes. We will explain
    in detail how we leverage serverless techniques for automated Big Data analytics
    in Section V. After an application is executed, clients can reproduce it based
    on its execution history. Our RPAC toolkit will generate a pipeline file based
    on the execution history and reproduction configurations. If the client wants
    to reproduce an existing execution with the exact environment and configuration,
    the pipeline file within the execution history can be used directly by our toolkit
    for reproducibility. If the client chooses to reproduce existing execution within
    the same cloud, but with a different environment or application, our toolkit will
    combine changed configurations of cloud resources or applications with the historical
    execution information to generate a new pipeline file. If the client prefers reproducing
    existing execution on a different cloud, our toolkit will provide cloud service
    mapping and implementations of functions in the target cloud. With user-provided
    personal information and historical execution, a new pipeline will be generated
    for the target cloud. Finally, with the pipeline file executable by cloud serverless
    services, the data analytics will be reproduced in cloud automatically. Details
    of how our approach achieves reproducibility will be explained in Section VI.
    We would like to note the serverless pipeline used here is different from most
    other workflow or pipeline definitions such as [12], [14], [15], [16]. These definitions
    only include the processing steps and their dependencies. They do not describe
    how to provision hardware and software environments because they assume these
    environments are ready before pipeline execution. Our serverless pipeline includes
    the full execution life cycle including hardware and software provisioning, Big
    Data analytics, execution export and resource release. Our pipeline does not describe
    internal processing steps, but could be integrated with traditional pipelines
    as internal logic description in its Function 2: conduct Big Data analytics. B.
    Serverless Based Large Scale Application in the Cloud Traditionally, serverless
    computing is used to execute serverless pipelines and the functions defined in
    each pipeline directly via cloud services like AWS CloudFormation. In this case,
    the computation is executed following the pipeline without using any additional
    cloud resources. Because of the memory and CPU limit for serverless functions,
    this approach can only handle computations whose resource requirements are light.
    For instance, OpenWhisk [17] is an event-based serverless computing cloud platform,
    which allows users to implement their own OpenWhisk APIs for the connections between
    the event source and trigger, the trigger rule, and the computation actions. Different
    from the above way to use serverless, we leverage serverless computing and its
    FaaS to achieve reproducibility for Big Data analytics in the cloud. The main
    difference is that we use the serverless pipeline as a way to orchestrate and
    manage additional cloud resources for heavy workloads while each step is wrapped
    as a function. In this way, both the serverless pipeline and its functions do
    not execute heavy commands directly. Instead, each function''s execution only
    submits commands from serverless to the additional cloud resources. Then when
    the function is triggered, the commands will be transferred to the additional
    cloud services and be executed as background processes so they can return without
    waiting for the finish of the commands. As shown in Fig. 2, the serverless pipeline
    listens to events sent by our toolkit or other cloud services. By mapping the
    triggers mentioned in the event with the trigger rule associated with each serverless
    function, it knows which serverless function will be involved based on the received
    event. For instance, Function 2 will be triggered when the pipeline receives an
    event as SoftwareEnvReady. For each function''s execution, it only submits commands
    from serverless to additional cloud services such as AWS EC2, so the resource
    and time limits for serverless functions will not be violated. Also, all major
    cloud providers including AWS and Azure, only enforce time limits for serverless
    functions, not serverless pipelines. So serverless pipelines are capable of large-scale
    computations that might take a long time. Serverless pipeline is also a reasonable
    choice from a budgetary cost perspective because serverless service is charged
    by the number of function invocations and the duration it takes to execute, not
    the deployment time of the pipeline. Fig. 2. The interaction between our proposed
    toolkit and cloud resources. Show All SECTION IV. Data Modeling and Storage for
    Reproducibility To achieve easy configurability by users and future reproducibility
    across cloud providers, we categorize data based on their usage and employ different
    levels of data abstraction. Specifically, the data model contains three parts:
    abstract request information, executable request information and execution history
    information. We believe the data model can serve as a reference model for different
    reproducibility toolkits. A. Abstract Request Information To avoid learning specific
    specifications and templates for specific clouds, we extract minimal information
    a user has to provide for application execution or reproduction. Further, as shown
    in the upper part of Fig. 3, we categorize the information into three separate
    key-value based configuration files where ini is used as file extension to distinguish
    them from other file types used by our toolkit. Specifically, resources.ini stores
    hardware and software resources information such as virtual machine type, virtual
    instance number, docker image URL and Big Data engine; application.ini records
    the program URI of the application, program arguments, and input dataset URIs
    of the program; personal.ini contains the cloud credential information such as
    SSH key location and cloud credential info (which can also be provided at runtime
    for security concerns). We use three different files so only a subset of files
    needs to be edited for each type of reproducibility shown in Fig. 1. A complete
    and formal listing of the information can be found at Fig. 4 using syntax of Backus–Naur
    form (BNF) [18]. Fig. 3. Data modeling and abstraction for reproducibility. Show
    All Fig. 4. Normative form for abstract request information. Show All B. Executable
    Request Information We separate information that is required for actual cloud-based
    application execution into four files and use json as the file extension. Such
    files have to follow specifications set by each cloud. For such files, our RPAC
    toolkit generates them automatically based on corresponding abstract ini file(s)
    mentioned above. The first file is resources.json which describes hardware and
    software environment info. This file has to be changed if the cloud provider is
    switched. The resources.json will be generated based on the above resources.ini
    file, the cloud type and the type of Big Data analytics. Another file is application.json
    which contains application specific information and will be generated by our toolkit
    based on the above application.ini file and the cloud type. Similarly, personal.json
    can be generated from personal.ini. As shown in Fig. 3, by combining resources.json,
    application.json, personal.json and four cloud-specific serverless functions shown
    in Fig. 1, we get pipeline.json that describes the execution logic of the serverless
    application. Our RPAC toolkit contains template json files and serverless function
    implementations so they can be reused for different data analytics applications.
    We illustrate how to map from abstract requirements in Section IV-A to an executable
    cloud specific serverless pipeline and its implemented functions in Fig. 5. The
    pipeline file is generated by the three abstract request information provided
    by users, which transfers the stateless configurations to executable cloud-specific
    information. The abstract request information is first transferred to the executable
    request information while missing parameters can be filled with their default
    values. All parameters in the executable request information are also sorted out
    based on the cloud-specific schema. By combining with corresponding serverless
    functions, the cloud-specific executable request information, like pipeline $\\_$
    aws.json, will be generated and executed in our RPAC toolkit. Each serverless
    function listens to the upcoming events. If a received event (e.g.,HardwareEnvReady
    in Fig. 5) matches, the associated function (e.g., SoftwareEnvSetup() in Fig.
    5) will be triggered. At the end of the function execution, a new event (e.g.,
    SoftwareEnvReady in Fig. 5) will be returned to trigger the downstream functions.
    Fig. 5. Mapping from abstract requirement to executable cloud specific serverless
    pipeline and functions. Show All The differences between the two types of request
    information are summarized below. Abstract request information, as a user-friendly
    abstraction, contains the minimal information a user has to provide for application
    execution or reproduction. In comparison, the executable request information describes
    the execution logic of the serverless application, which is required for actual
    cloud-based application execution. Our RPAC toolkit will generate the executable
    request information based on the corresponding abstract information during the
    execution. Next, we will explain how the files are used for automated execution
    in detail in Section V-A and how they are reused or transformed for reproduction
    in Section VI. C. Execution History Information Execution history information
    is critical to share each execution for later analysis and reproduction. As illustrated
    in Fig. 6, we classify execution history related data into three categories and
    store them separately. The first category is execution log metadata, such as timestamps,
    duration, cost, and status, which are stored in the database for query. Key-value
    based execution parameters including analytics command line and arguments are
    also stored for easy comparison among executions. This metadata information is
    unique for each execution, not required for reproducibility, but useful for later
    analysis such as finding the fastest execution time of the same application on
    different clouds or cloud resources. For information that can be referred from
    external resources, such as input datasets, output files and configuration files
    used for the execution, only their URLs are stored in the database. The second
    category is object based storage of each execution information for reproducibility.
    Two items are stored for each execution: 1) abstract request information (resources.ini,
    personal.ini and application.ini in Config.zip), 2) execution output datasets
    in Result.zip. Only abstract request information, not cloud specific information,
    is stored by our RPAC toolkit in order to minimize storage overhead. These data
    are compressed, categorized and stored in cloud object storage services such as
    AWS S3 and Azure Blob storage so a unique URL could be obtained for each execution.
    Because the data has the complete information to achieve reproducibility, the
    URL could be easily published as public records following the Research Object
    framework [19] so it can be referred to via a DOI identifier later as the single
    source for reproducibility. The third category is shared object storage of input
    datasets. It is stored separately so that multiple executions with the same input
    data only need one object storage. Also, cloud storage services like AWS S3 and
    Azure Blob storage allow automatic versioning so minor changes of input datasets
    do not require a fully separate storage. Fig. 6. Data modeling of execution history
    information. Show All SECTION V. Automated Big Data Analytics in the Cloud Towards
    Reproducibility To achieve easy reproducibility, the execution should be as automated
    as possible to minimize manual operations during reproduction phase. Also, an
    execution should be easily configurable for different scalability factors, application
    parameters, even cloud providers. In this section, we discuss our techniques to
    achieve fully automated Big Data analytics in the cloud so an application can
    be executed and later reproduced using only one command. A. Serverless and Docker-Based
    Execution Automation We leverage serverless computing to achieve overall analytics
    pipeline description and execution, and docker for software environment setup.
    Serverless computing offers a few advantages for reproducibility: 1) it saves
    costs because we do not need to maintain a server in the cloud especially for
    cases reproduction does not happen frequently; 2) its FaaS model allows us to
    design and implement separate functions required for automated execution/reproduction;
    3) its event-based function composition and execution eliminates the requirement
    of a separate workflow/pipeline software which is needed for many traditional
    workflow-based reproducibility [1]. As explained in Section II-C, serverless computing
    offers templates to describe cloud service resources required by the application,
    structured application pipeline, and event-based execution. Each component in
    the application pipeline is implemented as a serverless function and triggered
    by the events it listens to. So the pipeline binds cloud services with the specific
    event in order to trigger the corresponding serverless function. In addition,
    we can package complicated software dependencies required for an application via
    docker. The details of the automation are illustrated in Fig. 7. Fig. 7. The system
    sequence diagram for automated execution of Big Data analytics in serverless framework.
    Show All After receiving the user request, RPAC execution automation starts with
    pipeline generation and submission. Based on configurations, RPAC generates corresponding
    pipeline files, deploys its serverless functions, and uploads these configurations
    to the storage except client personal information. RPAC then submits this pipeline
    to the cloud and starts serverless execution. The serverless pipeline starts with
    the on-demand hardware environment provisioning (step a in Fig. 7) via cloud manager
    services (such as CloudFormation for AWS and Deployment Manager for Azure). The
    hardware provisioning is more like an on-demand resource request service that
    is a prerequirement for all serverless functions. So we put the hardware provisioning
    at the beginning of the serverless pipeline. To conduct Big Data analytics, we
    also need to create a virtual cluster by specifying the type and sub-type of virtual
    machines, the number of virtual machines, network security groups, etc. Cloud
    manager services allow the information to be submitted based on their semi-structured
    specification such as JSON and YAML. RPAC will send a reply once the pipeline
    file is submitted. The remaining steps of the automated pipeline execution are
    done via four cloud functions. On top of the virtual hardware environment provisioned,
    the next automation step is to deploy the required software to run the application
    (step b in Fig. 7). It is achieved by the first serverless function, which pulls
    required docker file and starts it. After the hardware and software environments
    are provisioned, it is ready to execute applications. The second serverless function
    in Fig. 7 executes the application by deploying user application (e.g., download
    application codes and unzip them) and running its commands with proper parameters
    (input data, application specific arguments, etc.). The third function exports
    all addresses of stored files to cloud database and object storage for future
    query and reproduction. After the storage completes, a termination event is sent
    to the last function, which terminates all cloud resources. At this time, the
    whole pipeline is fully executed, and the client is able to check and query information
    stored in the database and object storage. All these functions are triggered automatically
    when they receive corresponding events. The cloud manager services mentioned before
    can help the client manually send events to the serverless function from cloud
    console. In order to achieve full automation, these events can also be delivered
    to the target function by cloud event handling services (such as EventBridge for
    AWS and Event Grid for Azure) using a pre-defined event rule. Each serverless
    function needs to set up an event rule which specifies what type/property of event
    can trigger this function. For example, the rule of execution export function
    (step d in Fig. 7) requires the event source from object storage with a rule-defined
    prefix, like export. Besides serverless-based execution, our RPAC toolkit also
    supports cloud SDK-based execution to allow flexible client modes. Their differences
    are summarized in Table I. The cloud SDK mode is designed based on the cloud-specific
    software development toolkit (SDK). SDK facilitates the creation of applications
    by having a compiler, debugger and a software framework based on its functionality.
    The implementation of this SDK-based mode contains cloud application programming
    interfaces (APIs) for pipeline management. For example, AWS Boto python SDK can
    be invoked to describe the status of EC2 using ec2.describe $\\_$ instances().
    The execution can be automated by a periodical status pulling loop. This SDK-based
    mode requires programming knowledge and a complete understanding of the data analytics
    pipeline, so that developers become preferred users since they can either run
    the application in a fully automated way or step-wise execution for debugging
    purposes. By supporting different execution modes, users can make flexible choices.
    In comparison, serverless based approach is fully automated and more efficient
    because only the execution is managed via internal event triggering. No communications
    between client and cloud are needed once the pipeline is submitted. TABLE I Comparison
    of Two Execution Modes of RPAC Toolkit for Execution and Reproducibility B. Scalable
    Execution for Three Parallel Frameworks In this section, we discuss how our approach
    supports scalable execution via the three parallel frameworks in Section II-A,
    namely Spark-based, Dask-based, and Horovod-based analytics. The first two utilize
    virtual CPU clusters and the third utilizes virtual GPU clusters. By specifying
    the virtual machine type and number, cloud services can provision a cluster hardware
    environment. However, software dependencies, process coordination, and even access
    permission may differ for different Big Data analytics. Because of these differences,
    each framework requires its own resources.json and implementation of the first
    serverless function shown in Fig. 7. To reproduce Big Data analytics, one important
    part is to record and reuse original Big Data engine configurations. Paper [20]
    uses separate files to record Spark memory configuration for reproducibility.
    Similar to this approach, we set these configurations by recording the information
    via command line arguments or original Big Data engine configuration files. Big
    data engine''s configurations can be modified in application reproduction by users
    in application.ini file, such as changing –driver-memory 60 g –executor-memory
    60 g for Spark engine. Additional Big Data engine configurations are set up via
    separate files like spark-env.sh in the $SPARK/conf folder. Our toolkit supports
    storing such files in the cloud so they can be reused in reproduction. Beyond
    the listed frameworks, the additional parallel frameworks can also be deployed
    by updating the docker images’ address in application.ini. RPAC will setup this
    parallel framework in the second serverless function of Fig. 7 and execute analytics
    within the new environment. Spark-Based Big Data Analytics on Virtual CPU Nodes.
    We provide Spark-based parallel framework via the docker-based Spark engine virtual
    cluster provisioned by direct cloud services like AWS EMR with additional cloud
    resources like virtual network, container service and file system. By default
    setting, the resource manager like YARN NodeManager initiates the environment
    from a pulled docker image, and allocates one virtual instance as the master while
    others as workers. With serverless based pipeline execution, our toolkit enables
    automated execution management on master and execution computation on workers
    defined by serverless function handlers/implementations. Since Big Data analytics
    utilizes many compute nodes with complex computation proprieties, it is important
    to make sure availability and reliability during cloud execution. To achieve a
    secure and stable scalable execution, we control the access permission of master
    and workers by using the network security group. During Big Data analytics, our
    pipeline assigns one group for the master and another group for workers, and only
    enables TCP/UDP inbound and outbound rules within them. Also, for computation
    reliability, the Big Data analytics pipeline only allows client SSH permission
    for the master security group. Dask-Based Big Data Analytics on Virtual CPU Nodes.
    Besides Spark, our RPAC toolkit also supports CPU-based parallel analytics by
    using Dask as the resource manager in the virtual cluster. Different from Spark
    which has dedicated cloud services (such as EMR in AWS), Dask environment can
    only be provisioned by regular virtual machine services (such as EC2 in AWS).
    Each virtual instance in the cluster initiates one docker container and our pipeline
    assigns one of the containers to be the Dask scheduler and others to be workers.
    Same with the security group setup with Spark-based analytics, we divide the client
    access between scheduler and workers for execution reliability. During execution,
    different from AWS EMR service which automatically initiates Spark processes after
    hardware provisioning, our RPAC toolkit needs to start Dask processes on both
    scheduler and worker containers during software provisioning before executing
    Big Data analytics on virtual CPU nodes. Besides, same as Spark-based cloud services,
    the client can also produce interactive visualizations based on Dask diagnostic
    dashboard in our framework, by using the public DNS name (public IP) of the scheduler
    instance with its dashboard port. Horovod-Based Big Data Analytics on Virtual
    GPU Nodes. To provide a GPU-based parallel framework, we leverage Horovod and
    regular virtual machine services for analytics. The RPAC toolkit executes multi-instance
    GPU-based data analytics within our pre-built Docker containers, involving a shared
    file system and a customized port number for the SSH daemon. In order to categorize
    functionality between different instances, we set one of them as the primary worker
    and others as secondary workers. Within the container, the primary worker runs
    the MPI parallel command for data analytics execution while secondary workers
    listen to that specific port. SECTION VI. Reproduce Big Data Analytics in the
    Cloud In this section, we discuss how to achieve different levels of reproducibility
    within the same cloud and across different cloud providers. To achieve reproducibility,
    the user only needs to provide the URL of a historical execution stored in cloud
    storage (more in Section IV) and her own configurations. We will explain how our
    framework and RPAC toolkit support different ways of reproducibility summarized
    in Section II-B. A. Reproducibility in the Same Cloud Reproduction With the Same
    Environment and Configuration. This type of reproducibility is simplest because
    it is the same with the first execution as long as we can retrieve the information
    used from execution history. As illustrated by the first item in reproducibility
    phase of Fig. 1, by retrieving resources.ini and application.ini from execution
    history and providing proper personal.ini, our RPAC toolkit can rerun the experiment
    the same way it was executed for the first time. Reproduction in a Different Environment.
    Reproduction in a different environment means the virtual environment configuration
    needs to be changed from a historical execution, which is often useful for scale-up
    and scale-out experiments. As illustrated by the second reproducibility item in
    Fig. 1, a new resources.ini needs to be provided explaining the new environment
    setup (mostly virtual machine type and number). Then our RPAC toolkit can use
    it to generate a new executable resources.json and run the experiment in the same
    cloud. Reproduction With a Different Application Configuration. Reproduction with
    a different application configuration is often useful to run the application with
    a different dataset and/or application argument. As illustrated by the third reproducibility
    item in Fig. 1, a new application.ini needs to be provided explaining the new
    application setup. Then our toolkit can use it to generate a new executable application.json
    and run the experiment in the same cloud. We note that the last two reproductions
    can be easily combined for the requirements of running an application with different
    configurations and a different environment. To support it, a new resources.ini
    and a new application.ini should be provided. Algorithm 1: Cloud Agnostic Application
    Model (CAAM). B. Cross-Cloud Reproducibility We discuss how the client achieves
    reproducibility with a different cloud provider. As illustrated in the fourth
    way of reproduction in Fig. 1, by providing cloud service mapping and corresponding
    serverless function implementation, our toolkit can transform the general-purpose
    configurations in execution history into a new executable pipeline file for another
    cloud. To extend the reproducibility to another cloud, by leveraging the adapter
    pattern [21], we propose a portable Cloud Agnostic Application Model (CAAM) in
    order to solve the vendor lock-in and interoperability problem for Big Data analytics,
    which is shown in Algorithm 1. When CAAM receives resources.ini, application.ini
    and proper personal.ini, CloudAdapter() invokes each vendor specific method of
    different cloud. It means as long as there is an adaptee class written for the
    cloud provider, by calling the CloudAdapter() with this cloud provider, the provided
    general-purpose configurations will be transformed to the executable request information
    of the target cloud based on its specification sets and execution requirements.
    By combining the compatible information of resources, application and personal,
    CAAM generates the overall executable pipeline.json and starts to execute the
    data analytics. As shown in Algorithm 1, each cloud adaptee needs to implement
    how to get its resources.json based on resources.ini from execution history, parallel
    framework and service mapping shown in Table II. After all json files are ready,
    AWS uses GetAwsPipeline(AwsConfig) to generate pipeline file, while Azure uses
    GetAzurePipeline (AzureConfig) for generation. With CAAM, client directly calls
    CloudAdapter() with a specific adaptee method to execute data analytics with one
    general-purpose configuration. By calling Pipeline.execution(), the generate()
    method in corresponding cloud adaptee will generate the pipeline file and execute
    the Big Data analytics in cloud. Particularly, adaptee is in a modular design
    that can be injected into, removed from, or replaced within CAAM at any time.
    TABLE II Cloud Service Used by Reproducible and Portable Data Analytics Extensibility
    on Cross-Cloud Reproduction. Our reproducible and portable Big Data analytics
    can be easily extended to additional clouds because most services from different
    cloud providers can be mapped to each other. Table II lists all cloud services
    provided by Amazon AWS, Microsoft Azure and Google Cloud for data analytics. Our
    toolkit currently only implements cross-cloud reproducibility between AWS and
    Azure. Extension to Google Cloud can be done by adding an additional cloud specific
    adaptee and providing corresponding service mapping with function implementation.
    SECTION VII. Evaluation We implement the reproducible and portable cloud computing
    and open-source it on GitHub at [4]. Two CPU-based analytics applications (cloud
    retrieval and causality discovery) and one GPU-based analytics application (domain
    adaptation) are tested in our experiments. All benchmark evaluations are developed
    on two cloud providers, Amazon AWS and Microsoft Azure. Seven metrics are used
    to evaluate our work which include data analytics metrics like execution time,
    budgetary cost, cost-performance ratio, and cloud reproducibility metrics like
    overhead. Table III lists the exact cloud resources we use for each data analytics.
    For executing the application with a larger dataset, additional storage like AWS
    Elastic Block Store (EBS) is also been attached during the resource initialization.
    One variation is in scale-out of AWS CPU-based evaluation. We use c5d.large cluster
    for Dask-based analytics, but c5d.xlarge cluster for Spark-based analytics because
    AWS EMR requires more computational capability. TABLE III Comparison of Cloud
    Resources A. Benchmark Analytics and Datasets To benchmark our toolkit''s functionality
    comprehensively, we employ four applications and each uses a separate Big Data
    framework in Section II-A. Cloud Retrieval. Cloud property retrieval is an important
    task in remote sensing and Atmospheric science. We used the implementation of
    paper [22] for our first application. It trains a Random Forest machine learning
    model for cloud mask and cloud thermodynamic-phase retrieval from satellite observations.
    Dask framework is used for execution parallelization. The Docker image we built
    is hosted on DockerHub public repository, with Python 3.6 and sklearn 0.24.2.
    Total datasets are around 0.5 GB. Causality Discovery. In order to discover the
    cause-effect relationships in a system with the increasing volume and dimensionality
    of available data, the two-phase scalable and hybrid causality discovery is proposed
    by Guo et al. [23]. As a Big Data analytics, we use the Spark application with
    Hadoop in the cloud virtual cluster. The Docker image we built is hosted on DockerHub
    public repository, with Python 3.7 and R 3.4. The data in our execution is 200,000
    rows of simulated five variable time-series records, which is around 10 MB. Domain
    Adaptation. Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge
    learned from a labeled source domain to an unlabeled target domain. We use the
    UDA implementation designed by Sun et al. [24] that solves the problem of the
    unlabeled target domain. To move this data analytics to the cloud, we use the
    virtual cluster with Pytorch GPU acceleration and Horovod with MPI. The Docker
    image we built is hosted on DockerHub public repository, with Python 3.6, CUDA
    10.1 and cuDNN 7. The data we used is the public Office dataset containing 31
    object categories in two domains: Amazon and Webcam, which is around 50 MB in
    total. Satellite Collocation. Because there are many satellites orbiting the Earth,
    it is valuable to integrate and/or compare their measurements. Satellite collocation
    provides a way to pair measurements from two satellite sensors that observe the
    same location quasi-simultaneously. We implemented and parallelized the method
    in [25] to generate collocated data from two satellites. Like the cloud retrieval
    application, we use Dask framework for execution parallelization. The Docker image
    we built is hosted on DockerHub public repository, with Python 3.8, Pandas 1.5.0
    and H5py 3.7.0. The two satellites we used in the experiment include the ABI passive
    sensing data product from NOAA Geostationary Operational Environmental Satellites
    (GOES-16+) [26] and the CALIOP active sensing data product from NASA Cloud-Aerosol
    Lidar and Infrared Pathfinder Satellite Observations (CALIPSO) satellite [27].
    The total input data volume is 1.1 TB. B. Evaluation Metrics Even though there
    have been many studies on reproducibility, as stated in this recent survey paper
    [28], there are still no agreed metrics that can quantitatively measure reproducibility
    and compare different reproducible toolkits. The survey paper thinks performance,
    scalability and efficiency are possible metrics, but no concrete metric definition
    was provided. In this work, to promote fair comparison, we provide our own definition
    of performance, scalability and efficiency for cloud based reproducibility, which
    results in seven metrics (namely m1 to m7 listed below). 1) Execution Performance
    Metrics Following paper [29], we measure execution performance of our cloud based
    application via m1: execution time, m2: budgetary cost and m3: performance-price
    ratio (PPR). We first record execution time for each data analytics benchmark.
    The execution time is the wall-clock time of analytics pipeline (as shown in Fig.
    7), which includes pipeline file preparation, cloud resources deployment and initialization,
    data analytics execution, execution history upload, and termination. Budgetary
    cost contains bill usages for all resources used in each data analytics benchmark,
    which mainly includes the virtual cluster, container, network, database, and object
    storage with read and write request usage. Regarding the performance-price ratio
    (PPR), it evaluates the performance of each analytics considering the execution
    time with cost. We use the same formula used in [30] for PPR by calculating the
    product of execution time and budgetary cost. Lower PPR is more desirable excluding
    other factors. 2) Cloud Scalability Metrics We evaluate the scalability of our
    work for both m4: scale-up (vertical scaling) and m5: scale-out (horizontal scaling).
    Cloud scale-up is achieved by utilizing more resources within an existing computation
    system to reach a desired state of performance. In our evaluation, scale-up is
    set in a single virtual machine by having more threads in Dask-based analytics,
    more executor cores in Spark-based analytics, or more GPUs in Horovod-based analytics.
    For cloud retrieval, we fix the number of threads for each Dask worker, and utilize
    the number of workers from 1 to 8 during evaluation. It is the same in domain
    adaptation, except by increasing more threads for GPUs rather than CPUs. For causality
    discovery, because of the EMR setup, we launch only one worker in each virtual
    instance and allocate only one executor in this worker. To scale up, we use one
    executor with increasing the numbers of vCPUs of this executor for parallel execution.
    In real world scale-up, it is undesired to launch a powerful instance but only
    use its partial computational capability. In order to have a fair comparison,
    we additionally measure scale-up cost by usage, which times the budgetary cost
    of one instance by the percentage of CPU that is actually used. It simulates scale-up
    scenarios that use more and more powerful machines. Cloud scale-out is usually
    associated with a distributed architecture, which is achieved by adding additional
    computational capacity to a cluster. In our evaluation, scale-out is set by increasing
    more virtual machines in an existing cluster. For cloud retrieval and domain adaptation,
    we deploy only one worker process per instance, and increase the number of instances
    from 1 to 8 during evaluation. For causality discovery, we instead use one CPU
    core in each executor, and increase the number of workers by adding virtual instances.
    3) Reproducibility Efficiency Metrics For reproducibility, a metric m6: reproducibility_overhead
    is used to understand how much overhead it brings by supporting reproducibility
    during execution. Since reproducibility support is achieved by storing application
    configuration and execution history, we calculate the ratio between additional
    execution time caused by reproducibility data storage and the execution time of
    execution without reproducibility support. The lower the overhead ratio is, the
    better. As we mentioned in Section V-A, an SDK-based pipeline execution mode has
    also been proposed for Big Data analytics. Since both SDK-based and serverless-based
    approaches can be achieved automatically, we also measure m7: reproducibility_efficiency
    to compare their execution time with reproducibility. C. Benchmarking for Execution
    Performance and Scalability In this section, we first assess the cloud scalability
    of our RPAC toolkit based on three metrics: m1: execution time, m2: budgetary
    cost, and m3: performance-price ratio. In m4: scale-up evaluation, the execution
    is analyzed by gradually utilizing more resources in one instance. In m5: scale-out,
    the evaluation is achieved by gradually adding additional instances of the same
    type in one cluster. Next, we will explain our benchmarking results of the four
    applications. 1) Scalability Evaluation for the Cloud Retrieval Application The
    cloud retrieval m4: scale-up and m5: scale-out evaluations are shown in Fig. 8.
    As illustrated in Fig. 8(a), the m1: execution time decreases when the number
    of executors increases in both AWS and Azure with similar trends. The m2: budgetary
    cost as shown in Fig. 8(b), however, decreases in m4: scale-up and increases in
    m5: scale-out when the number of executors increase. The reason is that in cluster
    scale-up, the same resources were used while their execution time was decreasing;
    and in cluster scale-out, the costs saved by less execution time costs were less
    than the costs increased with additional resources. If only calculating the cost
    by usage for m4: scale-up case, its trends become similar to those of m5: scale-out.
    Combining cost and time, as illustrated in Fig. 8(c), the m3: PPR in m4: scale-up
    and scale-up by usage decrease when the numbers of executors increase. However,
    the m3: PPR first decreases but later increases a little bit in m5: scale-out
    cases. The figure also shows AWS achieves better m3: PPR than Azure, and m4: scale-up
    achieves better m3: PPR than m5: scale-out. So the best m3: PPR for the Dask-based
    Big Data application with virtual CPU nodes is achieved by m4: scale-up of application
    with more executors in AWS. Fig. 8. Scalability evaluation of RPAC toolkit for
    the cloud retrieval application: scale-up (circle and square mark) and scale-out
    (triangle and diamond mark) for AWS and Azure. Dashed line: cost value calculated
    by its usage. Show All 2) Scalability Evaluation for the Causality Discovery Application
    Because Azure HD-Insight cluster does not support Docker-based Spark computation,
    we only focus on the evaluation of causality discovery for AWS, which is shown
    in Fig. 9. The trends for this application are very similar to those for the previous
    application since they both are CPU-based. As illustrated in Fig. 9(a), the m1:
    execution time for both m4: scale-up and m5: scale-out decreases dramatically
    by at most 80% when the parallelism increases. This change of time appears more
    significant in causality discovery compared with what is in cloud retrieval. For
    the m2: budgetary cost in Fig. 9(b), when the parallelism increases, the m4: scale-up
    decreases, while both m5: scale-out and scale-up by usage increase with similar
    trends. For all three metrics in Fig. 9(c), The m3: PPR decreases when the numbers
    of executors increase. As a result, it is better to use a larger number of executors
    in the Spark-based Big Data analytics with virtual CPU nodes. Fig. 9. Scalability
    evaluation of RPAC toolkit for the causality discovery application: scale-up (circle
    mark) and scale-out (triangle mark) for AWS. Dashed line: cost value calculated
    by its usage. Show All 3) Scalability Evaluation for the Domain Adaptation Application
    For domain adaptation, the evaluations are shown in Fig. 10. Because the maximal
    number of GPUs in one instance is 4 for Azure, we compare m4: scale-up only from
    1 GPU to 4 GPUs. As illustrated in Fig. 10(a), same with the findings from other
    data analytics, the m1: execution time decreases when the numbers of GPUs increase
    in both AWS and Azure. The m2: budgetary cost in Fig. 10(b), also have the same
    regularity compared with CPU-based analytics. For m3: PPR, as illustrated in Fig.
    10(c), more GPUs lead to better ratios for m4: scale-up and worse ratios for scale-up
    by usage. For m5: scale-out, m3: PPR first gets worse and then improves a little
    bit. But still launching with only 1 instance can have the best m3: PPR for both
    AWS and Azure execution. Fig. 10. Scalability evaluation of RPAC toolkit for the
    domain adaptation application: scale-up (circle and square mark) and scale-out
    (triangle and diamond mark) for AWS and Azure. Dashed line: cost value calculated
    by its usage. Show All 4) Scalability Evaluation for the Satellite Collocation
    Application The above three applications already show the effectiveness of RPAC
    for parallel frameworks in different clouds. We further evaluate the satellite
    collection application with over 1 TB input data on AWS, and its longest total
    execution time is over 25 hours. As shown in Fig. 11, the m1: execution time in
    Fig. 11(a) of all m4: scale-up experiments decrease around 1 to 2 hours compared
    with all m5: scale-out experiments in the same parallelism setting. Thus, parallel
    execution in one VM with scale-up deployment is preferred, since m5: scale-out
    generates more communication overheads between different nodes. For the m2: budgetary
    cost as illustrated in Fig. 11(b), when the number of executors increases, the
    m4: scale-up gets a more reasonable price while the m5: scale-out becomes more
    expensive. Different with previous findings, m2: budgetary cost of m5: scale-out,
    m4: scale-up and scale-up by usage change very dramatically by at most 75% when
    the parallelism changes. The reason is that the execution time of the Big Data
    application is much longer than others. Combining cost and time, as illustrated
    in Fig. 11(c), the m3: PPR of m4: scale-up is decrease when the numbers of executors
    increase. The m3: PPR of scale-up by usage and m5: scale-out are first decrease
    but later increase a little bit. As a result, the better parallelism strategy
    for the Big Data application is using more executors in m4: scale-up deployment.
    Fig. 11. Scalability evaluation of RPAC toolkit for the satellite collocation
    application: scale-up (circle mark) and scale-out (triangle mark) for AWS. Dashed
    line: cost value calculated by its usage. Show All D. Benchmarking for Reproducibility
    Efficiency In this section, we assess the efficiency of reproducibility for RPAC
    toolkit in the first three applications. We first evaluate the overhead caused
    by serverless-based reproducibility, then we compared the efficiency between serverless-based
    and SDK-based approaches. 1) Efficiency Comparison for Reproducibility Support
    We first measure the m6: reproducibility_overhead of our applications with and
    without reproducibility support. For each application, we measure the AWS scale-up
    with 4 parallelisms, run each experiment 10 times and collect all results in a
    box-plot shown in Fig. 12. From the figure, we can see having reproducibility
    support did not cause much overhead, which is less than 0.01 hours, for all applications.
    The overhead percentage caused by reproducibility for cloud retrieval (CR), causality
    discovery (CD), and domain adaptation (DA), are 1.28%, 3.58%, and 2.17%, respectively.
    Besides, the time range of GPU-based analytics is larger than both CPU-based analytics,
    which means the execution time of GPU-based computation is more unstable than
    CPU-based one. Fig. 12. The box-plots and its relative difference for application
    execution time with and without reproducibility support. Show All We utilize a
    statistical hypothesis test approach, called T-test [31], to determine whether
    the execution time with and without reproducibility support differ statistically.
    T-test determines a possible conclusion from two different hypotheses. By calculating
    the corresponding p-value [32], we can measure the probability that an observed
    difference has occurred just by random chance. Hypothesizing that the reproduce
    execution provides some overhead over the execution without reproducibility, we
    calculate the p-value for the two sample t-test with equal variance. The p-values
    of CR, CD and DA, turn out to be 0.4968, 0.3193 and 0.3634. Since these are not
    less than p=0.05 , we fail to reject the null hypothesis of the tests. As the
    result, we do not have sufficient evidence to say that the average execution time
    between the two species (with and without reproducibility support) is different
    for all three applications. 2) Comparison With SDK-Based Reproduction Besides
    the serverless-based approach, as shown in Table I, we also implemented an SDK-based
    automatic execution mode which is achieved by periodical status pulling. In order
    to explore their difference, we evaluate the m7: reproducibility_efficiency of
    these two approaches with the same applications. Same with the previous measurement
    setting, we run each experiment 10 times and collect all results in a box-plot
    as illustrated in Fig. 13. For SDK-based approach, the time window for each status
    pulling is set to 10 s. The figure shows that serverless-based approach is more
    efficient than SDK-based approach, and the percentage of overhead reduction for
    CR, CD and DA, are 25.92%, 28.24% and 29.41%, respectively. The time range of
    serverless-based approach is larger than SDK-based one especially in GPU-based
    analytics. The reason is that, in SDK-based approach, the execution status monitoring
    could be delayed with periodical pulling. With the serverless function and event
    trigger, serverless-based approach enables Big Data analytics to be measured more
    efficiently and with less noise. Fig. 13. The box-plots and its relative difference
    for application execution time with serverless-based and SDK-based approach. Show
    All We also use T-test to determine whether the execution time using serverless
    approach and SDK-based approach differs statistically. Hypothesize that the serverless-based
    approach provides some efficient benefit over SDK-based approach. The p-values
    of CR, CD and DA, turn out to be 9.31e−15 , 6.00e−14 and 2.19e−06 . Since these
    p-values are less than 0.05, we can reject the null hypothesis of the tests. The
    serverless-based approach is indeed providing statistically significant efficient
    benefit compared with SDK-based approach. SECTION VIII. Related Work There have
    been many studies on cloud-based reproducibility. Some of them [35], [40], [41],
    [42], [43] only study its conceptual frameworks. In this section, we only discuss
    those having actual systems/toolkits. As shown in Table IV, we categorize related
    work into four groups based on their systems’ capabilities. Besides, we also selected
    two most related works to compare in detail. The comparison is shown in Table
    V where the first one also leverages serverless computing and the second is one
    of the most recent work on cloud based reproducibility. TABLE IV Comparison of
    Related Work for Cloud-Based Reproducibility TABLE V Detailed Comparison Among
    Serverless Computing Related Work A. General Comparison With Related Work Among
    the related studies in Table IV, nearly all related approaches achieve the software
    environment provision for reproducibility. However, approaches in group 1 mainly
    use the archived or containerized software environment, which limits the scope
    of applicability and lacks support for maintaining hardware configurations within
    cloud. Additionally, they monitor execution status by system commands or cloud
    APIs based periodical pulling which is less efficient than event-based execution
    in our work. For example, ReproZip [33] tracks system commands and zips collected
    information along with all the used system files together for reproducibility.
    CARE [34] reproduces a job execution by monitoring and archiving all the material
    required to re-execute operations. For related work in group 2, their proposed
    approaches encapsulate the code dependencies and software in virtual machine images
    or graphs, and enable history retrieval for reproduction. For instance, WSSE [41]
    proposes to generate digital data and source code snapshots to be reproduced and
    distributed within a cloud-computing provider. The Tapis [16] open-source API
    platform was proposed for accomplishing distributed computational experiments
    in a secure, scalable, and reproducible way. With the implemented pipeline with
    the Python API, the containerized applications can be submitted, scheduled and
    executed as tasks using a traditional HPC batch scheduler such as SLURM. AMOS
    [40] uses a VM containing a set of tools previously installed to implement a mechanism
    that initializes and configures VMs on demand. However, this reproduction is more
    like a history repetition, which is designed for verification and validation of
    history execution. They also provide configurable environment variables for automatic
    resource deployment in a single cloud, but do not support cross-cloud reproducibility.
    Instead, our proposed RPAC uses a data abstraction for information needed for
    reproducibility and transforms resource configurations used in one cloud into
    those in another cloud. For related studies in Table IV, group 3''s capabilities
    are closest to ours. These approaches rely on annotated information provided by
    a user to assign workflow, and software/hardware environment. For example, PRECIPE
    [45] provides APIs to access both AWS and private cloud. However, users need to
    call the functions in order and have to manually terminate resources after the
    experiment is done, so it does not support automated end-to-end execution and
    reproducibility. The whole execution has to wait at the client side before the
    next function can be called. On the contrary, RPAC serverless event triggering
    enables fewer communications between client and cloud, which improves the efficiency
    for cloud analytics. Chef [46] achieves virtual execution environment launching
    and termination via designed knife commands. Chef client is installed in virtual
    machines to run the pipeline within the virtual machines. So some internal steps
    of the application can be executed within the virtual machines via its pipeline.
    However, Chef does not support full automation since its user has to wait at client
    side to manually terminate resources after the experiment is done. Apt [47] uses
    user-provided profiles, which consists of a cluster with a control system to instantiate
    encapsulated experiment environments, for repeating historical research. From
    this information, they deduce the required execution resources in cloud and then
    re-provision or configure them through their own APIs. In comparison, we use a
    serverless-based pipeline and follow cloud function APIs provided by cloud providers
    so the execution/reproduction process can be managed by the cloud without communications
    with toolkit. Also, their fully created annotations, even in cross-cloud reproduction,
    rely heavily on the users instead of execution history. Our work abstracts information
    required by users from information in execution history, users only need to provide
    minimal information to reproduce. Our toolkit will transform user-provided information
    into executable pipeline. For automated execution and reproducibility, none of
    these approaches can achieve full automation including resources and software
    provisioning, analytics execution and termination. RPAC''s event-based automation
    is the one-command execution that achieves a more efficient cloud computation
    and reproduction. For cross-cloud reproducibility, we further use the adapter
    pattern model to achieve the configuration mapping without taking all inputs from
    the user. B. Detailed Comparison With Most Related Work As shown in Table V, Apache
    OpenWhisk [14], [17] is an open-source, distributed Serverless cloud platform.
    In their serverless design, functions are explicitly defined in terms of the event,
    trigger, and action, which are implemented by users. Events are generated from
    event sources, which often indicate changes in data or carry data themselves.
    The trigger is defined by specifying its name and parameters (key-value pairs).
    It is associated with an action. The action is defined as functions (code snippets),
    which encapsulate application logic to be executed in response to events. Before
    deployment, users need to initiate the cluster in the cloud and provide the hostname
    and port of the Kubernetes cluster to the toolkit. No resource termination option
    when deployment finishes. Openwhisk has three deployment options. 1) OpenWhisk
    can be deployed using Helm charts on any Kubernetes provisioned from a public
    cloud provider. 2) The deployment can be achieved by OpenWhisk REST API or OpenWhisk
    CLI. 3) Use the cloud-defined CLI on a cloud provider that already provisions
    Apache OpenWhisk as a service, which is only supported by IBM cloud as of now.
    For parallel execution, Openwhisk does not provide direct parallel framework support.
    To enable scalable execution, users need to initiate a cloud cluster with the
    parallel software environment and prepare docker images with a parallel framework.
    Then users need to implement the parallel logic in the pipeline''s action. OpenWhisk
    enables the deployment on different clouds with Helm charts on any Kubernetes.
    However, an application for one cloud cannot be redeployed in another cloud, unless
    users 1) initiate instances on another cloud with the Kubernetes cluster, 2) rewrite
    the events and triggers in the pipeline, and 3) provide the new hostname and port
    of Kubernetes cluster to the toolkit and redeploy the pipeline. Another example
    is NeuroCAAS [44]. NeuroCAAS provides formatted pipelines, called blueprints,
    in a public code repository and defines a resource bank that can make hardware
    available through pre-specified instances in one specific cloud. The users are
    able to update the blueprint with new configurations and upload its new version
    to the public repository for deployment and reproduction. The users need to provide
    the blueprint''s repository address for automated deployment, execution and reproduction.
    By default, NeuroCAAS fixes a single instance type per analysis in order to facilitate
    reproducibility. With the blueprint, datasets, and configuration files for one
    analysis, NeuroCAAS achieves reproducibility for corresponding analyses with the
    same environment and configuration. For parallel execution, NeuroCAAS does not
    provide direct parallel framework support. The logic of parallel processing must
    be explicitly scripted and implemented in the blueprint by users. SECTION IX.
    Conclusion Reproducibility is an important way to gain the confidence of new research
    contributions. In this paper, we study how to achieve cloud-based reproducibility
    for Big Data analytics. By leveraging serverless, containerization and adapter
    design pattern techniques, our proposed approach and RPAC toolkit can achieve
    reproducibility, portability and scalability for Big Data analytics. Our experiments
    show our toolkit can achieve good scalability and low overhead for reproducibility
    support for both AWS and Azure. For future work, we will mainly focus on the following
    three aspects. First, we will optimize the executions in terms of time, cost or
    ratio by mining execution history, and further optimize the overhead of reproducibility
    via better data abstraction, modeling and storage. Second, we will extend our
    work to easily publish data analytics as public records following the Research
    Object framework [19] so they can be referred via DOI identifiers later. Third,
    we will study how to utilize execution history data to achieve automated execution
    optimization based on users’ objectives (time, cost or ratio) and datasets. Authors
    Figures References Citations Keywords Metrics More Like This Resting-State fMRI
    Functional Connectivity: Big Data Preprocessing Pipelines and Topological Data
    Analysis IEEE Transactions on Big Data Published: 2017 Application of Cloud Computing
    and Big Data in Accounting Software 2023 Eighth International Conference on Science
    Technology Engineering and Mathematics (ICONSTEM) Published: 2023 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Cloud Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Reproducible and Portable Big Data Analytics in the Cloud
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zhou N.
  - Zhou H.
  - Hoppe D.
  citation_count: '6'
  description: Containers improve the efficiency in application deployment and thus
    have been widely utilised on Cloud and lately in High Performance Computing (HPC)
    environments. Containers encapsulate complex programs with their dependencies
    in isolated environments making applications more compatible and portable. Often
    HPC systems have higher security levels compared to Cloud systems, which restrict
    users' ability to customise environments. Therefore, containers on HPC need to
    include a heavy package of libraries making their size relatively large. These
    libraries usually are specifically optimised for the hardware, which compromises
    portability of containers. Per contra, a Cloud container has smaller volume and
    is more portable. Furthermore, containers would benefit from orchestrators that
    facilitate deployment and management of containers at a large scale. Cloud systems
    in practice usually incorporate sophisticated container orchestration mechanisms
    as opposed to HPC systems. Nevertheless, some solutions to enable container orchestration
    on HPC systems have been proposed in state of the art. This paper gives a survey
    and taxonomy of efforts in both containerisation and its orchestration strategies
    on HPC systems. It highlights differences thereof between Cloud and HPC. Lastly,
    challenges are discussed and the potentials for research and engineering are envisioned.
  doi: 10.1109/TSE.2022.3229221
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Software...
    >Volume: 49 Issue: 4 Containerization for High Performance Computing Systems:
    Survey and Prospects Publisher: IEEE Cite This PDF Naweiluo Zhou; Huan Zhou; Dennis
    Hoppe All Authors 4 Cites in Papers 928 Full Text Views Abstract Document Sections
    I. Introduction II. Concepts and Technologies for Containerisation III. Container
    Engines and Runtimes for HPC Systems IV. Container Orchestration V. Research Challenges
    and Vision Show Full Outline Authors Figures References Citations Keywords Metrics
    Footnotes Abstract: Containers improve the efficiency in application deployment
    and thus have been widely utilised on Cloud and lately in High Performance Computing
    (HPC) environments. Containers encapsulate complex programs with their dependencies
    in isolated environments making applications more compatible and portable. Often
    HPC systems have higher security levels compared to Cloud systems, which restrict
    users’ ability to customise environments. Therefore, containers on HPC need to
    include a heavy package of libraries making their size relatively large. These
    libraries usually are specifically optimised for the hardware, which compromises
    portability of containers. Per contra , a Cloud container has smaller volume and
    is more portable. Furthermore, containers would benefit from orchestrators that
    facilitate deployment and management of containers at a large scale. Cloud systems
    in practice usually incorporate sophisticated container orchestration mechanisms
    as opposed to HPC systems. Nevertheless, some solutions to enable container orchestration
    on HPC systems have been proposed in state of the art. This paper gives a survey
    and taxonomy of efforts in both containerisation and its orchestration strategies
    on HPC systems. It highlights differences thereof between Cloud and HPC. Lastly,
    challenges are discussed and the potentials for research and engineering are envisioned.
    Published in: IEEE Transactions on Software Engineering ( Volume: 49, Issue: 4,
    01 April 2023) Page(s): 2722 - 2740 Date of Publication: 14 December 2022 ISSN
    Information: DOI: 10.1109/TSE.2022.3229221 Publisher: IEEE Funding Agency: SECTION
    I. Introduction Containers have been widely adopted on Cloud systems. Applications
    together with their dependencies are encapsulated into containers [1], which can
    ensure environment compatibility and enable users to move and deploy programs
    easily among clusters. Containerisation is a virtualisation technology [2]. Rather
    than creating an entire operating system (called guest OS) on top of a host OS
    as in a Virtual Machine (VM), containers only share the host kernel, which makes
    containers more lightweight than VMs. Containers on Cloud are often dedicated
    to run micro-services [3] and one container mostly hosts one application or a
    part of it. High Performance Computing (HPC) systems are traditionally employed
    to perform large-scale financial, engineering and scientific simulations [4] that
    demand low latency (e.g., interconnect) and high throughput (e.g., the number
    of jobs completed over a specific time). To satisfy different user requirements,
    HPC systems normally provide predefined modules with specific software versions
    that users can switch by loading or unloading the modules with the desired packages
    [5]. This approach requires assistance of system administrators and therefore
    limits increasing user demands for environment customisation. On a multi-tenant
    environment as on HPC systems, especially HPC production systems, installation
    of new software packages on-demand by users is restricted, as it may alter the
    working environments of existing users and even raise security risks. Module-enabled
    software environments are also inconvenient for dynamic Artificial Intelligence
    (AI) software stacks [6]. Big Data Analytics hosted on Cloud are compute-intensive
    or data-intensive, mainly due to deployments of AI or Machine Learning (ML) applications,
    which demand extremely fast knowledge extraction in order to make rapid and accurate
    decisions. HPC-enabled AI can offer optimisation of supply chains, complex logics,
    manufacturing, simulation and underpin modelling to solve complex problems [7].
    Typically, AI applications have sophisticated requirements of software stacks
    and configurations. Containerisation not only enables customised environments
    on HPC systems, but also brings research reproducibility into practice. Containerised
    applications can become complex, e.g., thousands of separate containers may be
    required in production, and containers may require network isolation among each
    other for security reasons. Sophisticated strategies for container orchestration
    [8] have been developed on Cloud or big-data clusters to meet such requirements.
    HPC systems, per contra, lack features of efficiency in container scheduling and
    management (e.g. load balancing and auto container scaling), and often provide
    no integrated support for environment provisioning (i.e., infrastructure, configurations
    and dependencies). There have been numerous studies on containerisation and container
    orchestration on Cloud [2], [9], [10], [11], [12], [13], [14], [15], however,
    there is no comprehensive survey on these technologies and techniques for HPC
    systems existing as of yet. This article: Investigates state-of-the-art works
    in containerisation on HPC systems and underscores their differences with respect
    to the Cloud; Introduces the representative orchestration frameworks on both HPC
    and Cloud environments, and highlights their feature differences; Gathers the
    related studies in the integration of container orchestration strategies on Cloud
    into HPC environments; Discusses the challenges and envisions the potential directions
    for research and engineering. The rest of the paper is organised as follows. First,
    Section II introduces the background on containerisation technologies and techniques.
    Key technologies of state-of-the-art container engines (Section III) and orchestration
    strategies (Section IV) are presented, and the feature differences thereof between
    HPC and Cloud systems are discussed. Next, Section V describes research challenges
    and the vision. Lastly, Section VI concludes this paper. SECTION II. Concepts
    and Technologies for Containerisation The main differences between containerisation
    technologies on Cloud and HPC systems are in terms of security and the types of
    workloads. The HPC applications tend to require more resources as to not only
    CPUs, but also the amount of memory and network speed. HPC communities have, therefore,
    developed sophisticated workload managers to leverage hardware resources and optimise
    application scheduling. Since the typical applications on Cloud differ significantly
    from those in HPC centres with respect to the sizes, execution time and requirements
    of the availability of hardware resources [16], the management systems on Cloud
    are evolved to include architectures different from those on HPC systems. Research
    and engineering on containerisation technologies and techniques for HPC systems
    can be classified into two broad categories: Container engines/runtimes; Container
    orchestration. In the first category, various architectures of container engines
    have been developed which vary in usage of namespaces (see Section II-A), image
    formats and programming languages. The research in the latter category is still
    in its primitive stage, which will be discussed in Section IV. A. Containerisation
    Concepts Containerisation is an OS-level virtualisation technology [17] that provides
    separation of application execution environments. A container is a runnable instance
    of an image that encapsulates a program together with its libraries, data, configuration
    files, etc. [1] in an isolated environment, hence it can ensure library compatibility
    and enables users to move and deploy programs easily among clusters. A container
    utilises the dependencies in its host kernel. The host merely needs to start a
    new process that is isolated from the host itself to boot a new container [18],
    thus making container start-up time comparable to that of a native application.
    In contrary, a traditional VM loads an entire guest kernel (simulated OS) into
    memory, which can occupy gigabytes of storage space on the host and requires a
    significant fraction of system resources to run. VMs are managed by hypervisor
    which is also known as Virtual Machine Monitor (VMM) that partitions and provisions
    VMs with hardware resources (e.g., CPU and memory). The hypervisor gives the hardware-level
    virtualisation [19], [20]. Fig. 1 highlights the architecture distinction of VMs
    and containers. It is worth noting that containers can also run inside VMs [21].
    Besides portability, containers also enable reproducibility, i.e. once a program
    has been defined inside the container, its included working environment remains
    unchanged regardless of its running occurrences. Nevertheless, the shared kernel
    strategy presents an obvious pitfall: a Windows containerised application cannot
    execute on Unix kernels. Obviously, this should not become an impediment to its
    usage as Unix-like OS are often the preference for HPC systems. Fig. 1. Structure
    comparison of VMs and containers. On the VM side, the virtualisation layer often
    appears to be hypervisor while on the container side it is the container runtimes.
    Show All HPC applications are often highly optimised for processor architectures,
    interconnects, accelerators and other hardware aspects. Containerised applications,
    therefore, need to compromise between performance and portability. The studies
    have shown that containers can often achieve near-native performance [18], [22],
    [23], [24], [25], [26], [27] (see Section III-B). Linux has several namespaces
    [28] that isolate various kernel resources: mount (file system tree and mounts),
    PID (process ID), UTS (hostname and domain name), network (e.g., network devices,
    ports, routing tables and firewall rules), IPC (inter-process communication resources)
    and user. The last namespace is an unprivileged namespace that grants the unprivileged
    process access to traditional privileged functionalities under a safe context.
    More specifically, the user namespace allows to map user ID (UID) and group ID
    (GID) from hosts to containers, meaning that a user having UID 0 (root) inside
    a container can be mapped to a non-root ID (e.g., 100000) outside the container.
    Cgroups (Control Groups) is another namespace that is targeted to limit, isolate
    and measure resource consumption of processes. Cgroups is useful for a multi-tenant
    setting as excess resource consumption of certain users will be only adverse to
    themselves. One application of Linux namespaces is the implementation of containers,
    e.g., Docker, the most widely-used container engine, uses namespaces to provide
    the isolated workspace that is called container. When a container executes, Docker
    creates a set of namespaces for that container. B. Docker There are multiple techniques
    that realise the concept of containers. Docker is among the most popular ones
    [27]. After its appearance in 2013, various container solutions aimed for HPC
    have emerged [22]. Docker, initially based on LXC [29], is a container engine
    that supports multiple platforms, i.e., Linux, OSX and Windows. A Docker container
    image is composed of a readable/writable layer above a series of read-only layers.
    A new writable layer is added to the underlying layers when a new Docker container
    is created. All changes that are made to the running container, such as writing
    new files, modifying or deleting existing files, are written to this thin writable
    container layer. Docker adopts namespaces including Cgroups to provide resource
    isolation and resource limitation, respectively. Table I highlights the usage
    of namespaces with respect to Docker and a list of container engines targeted
    for HPC environments. TABLE I Linux Namespace Supports for HPC-Targeted Container
    Engines (Section III) and Docker in the Year of 2022 Docker provides network isolation
    and communication by creating three types of networks: host, bridge and none.
    The bridge network is the default Docker network. The Docker engine creates a
    subset or gateway to the bridged network. This software bridge allows Docker containers
    to communicate within the same bridged network; meanwhile, isolates the containers
    from a different bridged network. Containers in the same host can communicate
    via the default network by the host IP address. To communicate with the containers
    located on a different host, the host needs to allocate ports on its IP address.
    Managing ports brings overhead which can intensify at scale. Dynamically managing
    ports can solve this issue which is better handled by orchestration platforms
    as introduced in Section IV-B. Docker is widely adopted in Cloud where users often
    have root privileges. The root privilege is required to execute the Docker application
    and its Daemon process that provides the essential services. Originally running
    Docker with root permission brings some advantages to Cloud users. For instance,
    users can run their applications and alternative security modules to provide separation
    among different allocations [30]; users can also mount host filesystems to their
    containers. Root privilege can cause security issues. Therefore, the latest updates
    of Docker engine start to support rootless daemon and enable users to execute
    containers without root. Nevertheless, other security concerns still persist.
    For instance, usage of Unix socket can be changed to TCP socket which will grant
    an attacker a remote control to execute any containers in the privileged mode.
    Additionally, rootless Docker does not run out of box, system administrators need
    to carefully set the namespaces of hosts to separate resources and user groups
    in order to guarantee security. Hence HPC centres that typically have high security
    requirements are still reluctant to enable the Docker support on their systems.
    SECTION III. Container Engines and Runtimes for HPC Systems This section first
    reviews the state-of-the-art container engines/runtimes designed for HPC systems
    and compares the major differences with the mainstream Cloud container engine,
    i.e., Docker. Next, Section III-B shows the performance evaluation of the reviewed
    HPC container engines. A. State-of-The-Art Container Engines and Runtimes A list
    of representative container engines and runtimes for HPC systems is given in this
    section. They differ in functional extent and implementation, however, also hold
    some similarities. Tables I and II summarise the feature differences and similarities
    between Docker and a list of main HPC container engines. TABLE II Comparison of
    Docker With the List of Container Engines for HPC Systems 1) Shifter Shifter [31]
    is a prototypical implementation of container engine for HPC developed by NERSC.
    It utilises Docker for image building workflow. Once an image is built, users
    can submit it to an unprivileged gateway which injects configurations and binaries,
    flattens it to an ext4 file system image, and then compresses to squashfs images
    that are copied to a parallel filesystem on the nodes. In this way, Shifter insulates
    the network filesystem from image metadata traffic. Root permission of Docker
    is naturally deprived from Shifter that only grant user-level privileges. Existing
    directories can be also mounted inside Shifter image by passing certain flags.
    As an HPC container engine, Shifter supports MPICH that is an implementation of
    the Message Passing Interface (MPI) [32], [33] standard. To enable accelerator
    supports such as GPU without compromising container portability, Shifter runtime
    swaps the built-in GPU driver of a Shifter container with an ABI (Application
    Binary Interface) compatible version at the container start-up time. 2) Charliecloud
    Charliecloud [28] runs containers without privileged operations or daemons. Charlicloud
    can convert a Docker image into a tar file and unpacks it on the HPC nodes. Installation
    of Charliecloud does not require root permission. Such non-intrusive mechanisms
    are ideal for HPC systems. Charliecloud is considered to be secure against shenanigans,
    such as chroot escape, bypass of file and directory permission, privileged ports
    bound to all host IP addresses or UID set to an unmapped UID [15]. MPI is supported
    by Charliecloud. Injecting host files into images is used by Charliecloud to solve
    library compatibility issues, such as GPU libraries that may be tied to specific
    kernel versions. 3) Singularity Singularity is the most-widely used HPC container
    engine in academia and industry. Singularity [34] was specifically designed from
    the outset for HPC systems. Contrasting with Docker, it gives the following merits
    [23]: Running with user privileges and no daemon process. Only user privileges
    are required to execute Singularity applications. Acquisition of root permission
    is only necessary when users want to build or rebuild images, which can be performed
    on their own working computers. Unprivileged users can also build an image from
    a definition file with a few restrictions by ”fake root” in Singularity, however,
    some methods requiring to create block devices (e.g., /dev/null) may not always
    work correctly in this way; Seamless integration with HPC systems. Singularity
    natively supports GPU, MPI and InfiniBand [16]. No additional network configurations
    are expected in contrast with Docker containers; Portable via a single image file
    (SIF format). On the contrary, Docker is built up on top of layers of files. Two
    approaches are often used to execute MPI applications using Singularity, i.e.,
    hybrid model and bind model. The former compiles MPI binaries, libraries and the
    MPI application into a Singularity image. The latter binds the container on a
    host location where the container utilises the MPI libraries and binaries on the
    host. The latter model has a smaller image size since it does not include compiled
    MPI libraries and binaries in the image. Utilising the host libraries is also
    beneficial to application performance, however, the version of MPI implementation
    that is used to compile the application inside the container must be compatible
    with the version available on the host. The hybrid model is recommended, as mounting
    storage volumes on the host often require privileged operations. Most Docker images
    can be converted to singularity images directly via simple command lines (e.g.
    docker save, singularity build). Singularity has quickly become the ipso facto
    standard container engine for HPC systems. 4) SARUS SARUS [35] is another container
    engine targeted for HPC systems. SARUS relies on runc1 to instantiate containers.
    runc is a CLI (Command-Line Interface) tool for spawning and running containers
    according to the OCI (Open Container Initiative) specification. Different from
    the aforementioned engines, the internal structure of SARUS is based on the OCI
    standard (see Section V-A2). As shown in Fig. 2, the CLI component takes the command
    lines which either invoke the image manager component or the runtime component.
    The latter instantiates and executes containers by creating a bundle that comprises
    a root filesystem directory and a JSON configuration file. The runtime component
    then calls runc that will spawn the container processes. It is worth noting that
    functionalities of SARUS can be extended by calling customised OCI hooks, e.g.,
    MPI hook. Fig. 2. The internal structure of SARUS. OCI hooks include MPI hook.
    Show All 5) UDocker UDocker2 is a Python wrapper for the Docker container, which
    executes only simple Docker containers in user space without the acquisition of
    root privileges. UDocker provides a Docker-like CLI and only supports a subset
    of Docker commands, i.e., search, pull, import, export, load, save, create and
    run. It is worth noting that UDocker neither makes use of Docker nor requires
    its presence on the host. It executes containers by simply providing a chroot-like
    environment over the extracted container. 6) Other HPC Container Engines More
    and more HPC container engines are being developed, this section gives an overview
    of some that are targeted for special use cases. Podman [36] makes use of the
    user namespace to execute containers without privilege escalation. A Podman container
    image comprises layers of read-write files as Docker. It adopts the same runtime
    runc as in SARUS and Docker. The runtime crun, which is faster than runc, is also
    supported. A notable feature of Podman is as its name denotes: the concept of
    pod. A pod groups a set of containers that collectively implements a complex application
    to share namespaces and simplify communication. This feature enables the convergence
    with the Kubernetes [37] environment (Section IV-B1), however, requires advanced
    kernel features (e.g., version 2 Cgroups and user-space FUSE). These kernel features
    are not yet compatible with network filesystems to make full use of the rootless
    capabilities of Podman and consequently restrain its usage from HPC production
    systems [38]. Similar to UDocker, Socker [39] is a simple secure wrapper to run
    Docker in HPC environments, more specifically SLURM (Section IV-A2). It does not
    support the user namespace, however, it takes the resource limits imposed by SLURM.
    Enroot3 from NVIDIA can be considered as an enhanced unprivileged chroot. It removes
    much of the isolation that the other container engines normally provide but preserves
    filesystem separation. Enroot makes use of user and mount namespaces. B. Performance
    Evaluation for HPC Container Engines This section only selects the representative
    works as given in Table III, rather than exhausting the literature, to show the
    performance of containers that are specifically targeted for HPC systems in terms
    of CPU, memory, disk (I/O), network and GPU. Table VI lists the benchmarks utilised
    in these work. Overall, the container startup latency can be high on the Cloud.
    This startup overhead is caused by building containers from multiple image layers,
    setting read-write layers and monitoring containers [27]. An HPC container is
    composed of a single image or directory (with exception to Podman) and monitoring
    is performed by HPC systems. TABLE III Overview of the Related Work on Container
    Performance Evaluation in Terms of CPU, Memory, Disk, Network and GPU on HPC Systems
    TABLE IV The List of HPC Benchmarks Mentioned in Section III-B The work in [24],
    utilising the IMB [42] benchmark suite and HPCG [43] benchmarks, proved that little
    overhead of network bandwidth and CPU computing overhead is caused by Singularity
    when dynamically linking vendor MPI libraries in order to efficiently leverage
    advanced hardware resources. With the Cray MPI library, Singularity container
    achieved 99.4% efficiency of native bandwidth on a Cray XC [44] HPC testbed when
    running the IMB benchmark suite. However, the efficacy drastically drops to 39.5%
    with Intel MPI. Execution time evaluated with the HPCG benchmarks, indicated that
    the performance penalty caused by Singularity is negligible with Cray MPI, though
    the overhead can reach 18.1% with Intel MPI. The performance degradation with
    Intel MPI is mostly because of the vendor-tuned MPI library which does not leverage
    hardware resources from a different vendor, e.g., interconnect. Hu et al. [23]
    evaluated the Singularity performance in terms of CPU capacity, memory, network
    bandwidth and GPU with Linpack benchmarks [45] and four typical HPC applications
    (i.e., NAMD [46], VASP [47], WRF [48] and AMBER [49]). Singularity provides close
    to native performance on CPU, memory and network bandwidth. A slight overhead
    (4.135%) is shown on NVIDIA GPU. Muscianisi et al. [41] illustrated the performance
    impact of Singularity with the increasing number of GPU nodes. The evaluation
    was carried out on CINECA''s GALILEO systems with TensorFlow [50] applications.
    The results again demonstrated that the container environments caused negligible
    performance overhead. The work by Hale et al. [18] presented the CPU performance
    of Shifter with HPGMG-FE (MPI implementation) benchmarks [51] on Cray XC30 (192
    cores, 24 cores per compute node) where the performance margin between Shifter
    container and bare metal is unnoticeable. Comparison is also given for MPI with
    implementation in C++ and Python using a custom benchmark. The authors observed
    that it could take over 30 minutes to import the Python modules when running natively
    with 1,000 processes. Each process of a Python application imports modules from
    the filesystem on each node. Accesses to many small files on an HPC filesystem
    using many processes can be extremely slow comparing with the accesses to a few
    large files. The containerised benchmark has already included all the modules
    in its image that is mounted as a single file on each node, therefore, Shifter
    container outperforms the native execution in this case. Bahls [40] also evaluated
    the execution time of Shifter on Cray XC and Cray XE/XK systems exploiting Cray
    HSN (High Speed Network). Their results showed that Shifter gave comparable performance
    to bare metal. The study in [22] compared the performance of Shifter and Singularity
    against bare metal in terms of computation time using two biological use cases
    on three types of supercomputer CPU architectures: Intel Skylake, IBM Power9 and
    Arm-v8. Containerised applications can scale at the same rate as the bare-metal
    counterparts. However, the authors also observed that with a small number of MPI
    ranks, containers should be built as generic as possible, per contra, when it
    comes to a large number of cores, containers need to be tuned for the hosts. Without
    performance comparison with bare-metal applications, the work in [27] studied
    the CPU, memory, network and I/O performance of Charliecloud, Podman and Singularity.
    All the containers behave similarly with respect to the CPU and memory usage.
    Charliecloud and Singularity have comparable I/O performance. Charliecloud incurs
    large overhead on Lustre''s MDS (Metadata Server) and OSS (Object Storage Server)
    due to its bare tree structure. Comparing with the structures of shared layers
    (as in Docker), this structure needs to access a large number of individual files
    from the image tree from Lustre. Consequently, it causes network overhead when
    data is transmitted from the client node over the network at container start-up
    time. Similarly, as Singularity is stored as a single file on Lustre, a large
    amount of data needs to be loaded at starting point resulting in a data transmission
    spike on network. SARUS has shown strong scaling capability on Cray XC systems
    with hybrid GPU and CPU nodes [35]. The performance difference between SARUS and
    bare metal is less than 0.5% up to 8 nodes and 6.2% up to 256 nodes. No specific
    metrics are given in terms of GPU, though GPU has been used as accelerators. C.
    Section Highlights Containers are introduced to HPC systems, as they enable environment
    customisation for users, which offers the solutions to application compatibility
    issues. This is particularly important on HPC systems that are typically inflexible
    for environment modifications. Notably, HPC container engines are designed to
    meet the high-security requirements on HPC systems. Multiple prevailing engines
    have been described in this section, they share some common features: Non-root
    privileges; Often can convert Docker images to their own image formats; Supports
    of MPI that are typical HPC applications; Use host network rather than pluggable
    network drivers. Yet differences exist in their image formats. Layered image format
    is seen in Docker (UDocker wraps Docker image layers to a local directory), which
    is executed by pulling the image layers that have not been previously downloaded
    on the host. HPC container images are stored in a single directory or file which
    can be transferred to the compute nodes easily avoiding the pulling operations
    that require network access. HPC container engines show various ways to incorporate
    well-tuned libraries targeting for the hosts in order to achieve optimised performance,
    e.g., OCI hooks (SARUS), injecting host files into images (Charliecloud). Section
    III-B aims to give examples that can provide general advices on how to build the
    container images to maximise performance. Clearly, performance loss can occur
    in certain cases which are summarised in the second column of Table III. SECTION
    IV. Container Orchestration Orchestration under the context herein means automated
    configuration, coordination and management of Cloud or HPC systems. In theory,
    HPC workload manager can be also addressed as orchestrator, however, this article
    takes the former term as it is the custom terminology that has been long-used
    and widely understood in the HPC area. The driving factors that push HPC workload
    managers and Cloud orchestrators to be developed in different directions can be
    multiple. This will be discussed at the end of this section (Section IV-D). However,
    first it is important to understand the mechanisms of HPC workload managers (Section
    IV-A) and Cloud orchestrators (Section IV-B). Mostly, container orchestration
    for HPC systems either relies on the orchestration strategies of the existing
    Cloud orchestrators or exploits the mechanisms of current HPC workload managers
    or software tools. This point will be depicted in Section IV-C. A. Workload Managers
    for HPC Systems Cloud aims to exploit economy of scale by consolidating applications
    into the same hardware [16] and the hardware resources can be easily extended
    based on user demands. In contrast, HPC centres have large-scale hardware resources
    available and reserve computing resources exclusively for users. Table V underscores
    the main differences between HPC workload managers and Cloud orchestrators. A
    typical HPC system is managed by a workload manager. A workload manager comprises
    a resource manager and a job scheduler. A resource manager [52] allocates resources
    (e.g., CPU and memory), schedules jobs and guarantees no interference from other
    user processes. A job scheduler determines the job priorities, enforces resource
    limits and dispatches jobs to available nodes [53]. TABLE V Comparison of HPC
    Workload Managers (Section IV-A) and Cloud Orchestrators (Section IV-2) HPC workload
    managers incorporate a big family, such as PBS [54], Spectrum LSF [55], Grid Engine
    [56], OAR [57] and Slurm [58]. Slurm and PBS are two main-stream workload managers.
    The workload managers shares some common features: a centralised scheduling system,
    a queuing system and static resource management mechanisms, which will be detailed
    in this section. 1) PBS PBS stands for Portable Batch System which includes three
    versions: OpenPBS, PBS Pro and TORQUE. OpenPBS is open-source and TORQUE is a
    fork of OpenPBS. PBS Pro is dual-licensed under an open-source and commercial
    license. The structure of a TORQUE-managed cluster consists of a head node and
    many compute nodes as illustrated in Fig. 3 where only three compute nodes are
    shown. The head node (coloured in blue in Fig. 3) controls the entire TORQUE system.
    A pbs_server daemon and a job scheduler daemon are located on the head node. The
    batch job is submitted to the head node (in some cases, the job is first submitted
    to a login node and then transferred to the head node). A node list that records
    the configured compute nodes is maintained on the head node. The architecture
    of this kind as shown in Fig 3 represents the fundamental cluster structure of
    main-stream HPC workload managers. The procedure of job submission on TORQUE is
    briefly described as follows: The job is submitted to the head node by the command
    qsub. A job is normally written in the format of a PBS script. A job ID is returned
    to the user as the standard output of qsub. Fig. 3. TORQUE structure. pbs_server
    , scheduler and pbs_mom are the daemons running on the nodes. Mother superior
    is the first node on the node list (on step4). Show All The job record, which
    incorporates a job ID and the job attributes, is generated and passed to pbs_server
    . pbs_server transfers the job record to the job scheduler daemon. The job scheduler
    daemon adds the job into a job queue and applies a scheduling algorithm to it
    (e.g., FIFO: First In First Out) which determines the job priority and its resource
    assignment. When the scheduler finds the list of nodes for the job, it returns
    the job information to pbs_server . The first node on this list becomes the mother
    superior and the rest are called sister MOMs or sister nodes. pbs_server allocates
    the resources and passes the job control as well as execution information to the
    pbs_mom daemon installed on the mom superior node instructing to launch the job
    on the assigned compute nodes. The pbs_mom daemons on the compute nodes manage
    the execution of jobs and monitor resource usage. pbs_mom will capture all the
    outputs and direct them to stdout and stderr which are written into the output
    and error files and are copied to the designated location when the job completes
    successfully. The job status (completed or terminated) will be passed to pbs_server
    by pbs_mom . The job information will be updated. In TORQUE, nodes are partitioned
    into different groups called queues . In each queue, the administrator sets limits
    for resources such as walltime and job size. This feature can be useful for job
    scheduling in a large HPC cluster where nodes are heterogeneous or certain nodes
    are reserved for special users. This feature is commonly seen in HPC workload
    managers. TORQUE has a default scheduler FIFO, and is often integrated with a
    more sophisticated job scheduler, such as Maui [59]. Maui is an open source job
    scheduler that provides advanced features such as dynamic job prioritisation,
    configurable parameters, extensive fair share capabilities and backfill scheduling.
    Maui functions in an iterative manner like most job schedulers. It starts a new
    iteration when one of the following conditions is met: (1) a job or resource state
    alters; (2) a reservation boundary event occurs; (3) an external command to resume
    scheduling is issued; (4) a configuration timer expires. In each iteration, Maui
    follows the below steps [60]: Obtain resource records from TORQUE; Fetch workload
    information from TORQUE; Update statistics; Refresh reservations; Select jobs
    that are eligible for priority scheduling; Prioritise eligible jobs; Schedule
    jobs by priority and create reservations; Backfill jobs. Despite an abundance
    of algorithms, only a few scheduling strategies are practically in use by job
    schedulers. Backfilling scheduling [61] allows jobs to take the reserved job slots
    if this action does not delay the start of other jobs having reserved the resources,
    thus allowing large parallel jobs to execute and avoiding resource underutilisation.
    Differently, Gang scheduling [62] attempts to take care of the situations when
    the runtime of a job is unknown, allowing smaller jobs to get fairer access to
    the resources. Both scheduling strategies are also seen in SLURM and backfilling
    can be also found in LSF. 2) SLURM The structure of a SLURM (Simple Linux Utility
    for Resource Management) [58] managed cluster is composed of one or two SLURM
    servers and many compute nodes. Its procedure of job submission is similar to
    that of TORQUE. Fig. 4 illustrates the structure of SLURM. Its server hosts the
    slurmctld daemon which is responsible for cluster resource and job management.
    SLURM servers and the corresponding slurmctld daemons can be deployed in an active/passive
    mode in order to provide services of high reliability for computing clusters.
    Each compute node hosts one instance of the slurmd daemon, which is responsible
    for job staging and execution. There are additional daemons, e.g., slurmdbd which
    allows to collect and record accounting information for multiple SLURM-managed
    clusters and slurmrestd that can be used to interact with SLURM through a REST
    API (RESTful Application Programming Interface). The SLURM resource list is held
    as a part of the slurm.conf file located on SLURM server nodes, which contains
    a list of nodes including features (e.g., CPU speed and model, amount of memory)
    and configured partitions (named queue in PBS) including partition names, list
    of associated nodes and job priority. Fig. 4. SLURM structure. Show All Both PBS
    and SLURM have little (if at all) dedicated supports for container workloads.
    Containers are only scheduled as conventional HPC workloads, e.g lacking of load-balancing
    supports. 3) Spectrum LSF IBM platform Load Sharing Facility (LSF), targeted for
    enterprises, is designed for distributed HPC deployments. LSF is based on the
    Utopia job scheduler [55] developed at the University of Toronto. Its Session
    Scheduler runs and manages short-duration batch jobs, which enables users to submit
    multiple tasks as a single LSF job, consequently reduces the number of job scheduling
    decisions. Session Scheduler can efficiently share resources regardless of job
    execution time and can make thousands of scheduling decisions per second. These
    capabilities create a focus on throughput which is often critical for HPC workloads.
    Fig. 5 illustrates the structure of LSF. Its license scheduler allows to make
    policies that control the way software licenses are shared among users within
    an organisation. Jobs are submitted via the command line interface, API or IBM
    platform application centre. Job submission carries similar procedure as in TORQUE.
    Fig. 5. Spectrum LSF structure. Show All LSF supports container workloads: Docker,
    Singularity and Shifter. LSF configures container runtime control in the application
    profile4 that is managed by the system administrator. Users do not need to consider
    which containers are used for their jobs, instead only need to submit their jobs
    to the application profile and LSF automatically manages the container runtime
    control. Section IV-C3 elaborates this feature in more details. B. Orchestration
    Frameworks on Cloud Cloud clusters often include orchestration mechanisms to coordinate
    tasks and hardware resources. Cloud has evolved mature orchestrators to manage
    containers efficiently. Container orchestrators can offer [11], [15], [37]: Resource
    limit control. Reserve a specific amount of CPUs and memory for a container, which
    restrains interference from other containers and provides information for scheduling
    decisions; Scheduling. It determines the policies that optimise the placement
    of containers on nodes; Load balancing. It distributes workloads among container
    instances; Health check. It verifies if a faulty container needs to be destroyed
    or replaced; Fault tolerance. It allows to maintain a desired number of containers;
    Auto-scaling. It automatically adds and removes containers. Additionally, a container
    orchestrator should also simplify networking, enable service discovery and support
    continuous deployment [63]. 1) Kubernetes Kubernetes originally developed by Google
    is among the most popular open-source container orchestrators, which has a rapidly
    growing community and ecosystem with numerous platforms being developed upon it.
    The architecture of Kubernetes comprises a master node and a set of worker nodes.
    Kubernetes runs containers inside pods that are scheduled to run either on master
    or worker nodes. A pod can include one or multiple containers. Kubernetes provides
    its services via deployments that are created by submission of yaml files. Inside
    a yaml file, users can specify services and computation to perform on the cluster.
    A user deployment can be performed either on the master node or the worker nodes.
    Kubernetes is based on a highly modular architecture which abstracts the underlying
    infrastructure and allows internal customisation, such as the deployment of software-defined
    networks or storage solutions. It also supports various big-data frameworks, such
    as Hadoop MapReduce [64], Spark [65] and Kafka [66]. Kubernetes incorporates a
    powerful set of tools to control the life cycle of applications, e.g., parameterised
    redeployment in case of failures and state management. Furthermore, it supports
    software-defined infrastructures5 [67] and resource disaggregation [68] by leveraging
    container-based deployments and particular drivers (e.g., Container Runtime Interface
    driver, Container Storage Interface driver and Container Network Interface driver)
    based on standardised interfaces. These interfaces enable the definition of abstractions
    for fine-grain control of computation, states and communication in multi-tenant
    Cloud environments along with optimal usage of the underlying hardware resources.
    Kubernetes incorporates a scheduling system that permits users to specify different
    schedulers for each job. The scheduling system makes the decisions based on two
    steps before the actual scheduling operations: Node filtering. The scheduler locates
    the node(s) that fit(s) the workload, e.g., a pod is specified with node affinity,
    therefore, only certain nodes can meet the affinity requirements or some nodes
    may not include enough CPU resources to serve the request. Normally the scheduler
    does not traverse the entire node list, instead it selects the one/ones detected
    first. Node priority calculation. The scheduler calculates a score for each node,
    and the highest scoring node will run that pod. Kubernetes has started being utilised
    to assist HPC systems in container orchestration (Section IV-C). 2) Docker Swarm
    Docker Swarm [69] is built for the Docker engine. It is a much simpler orchestrator
    comparing with Kubernetes, e.g., it offers less rich functionalities, limited
    customisations and extensions. Docker Swarm is hence lightweight and suitable
    for small workloads. In contrast, Kubernetes is heavyweight for individual developers
    who may only want to set up an orchestrator for simplistic applications and perform
    infrequent deployments. Nevertheless, Docker Swarm still has its own API, and
    provides filtering, scheduling and load-balancing. API is a strong feature commonly
    used in Cloud orchestrators, as it enables applications or services to talk to
    each other and provides connections with other orchestrators. The functionalities
    of Docker Swarm may be applied to perform container orchestration on HPC systems
    as detailed in Section IV-C3. 3) Apache Mesos and YARN Apache Mesos [70] is a
    cluster manager that provides efficient resource isolation and sharing across
    distributed applications or frameworks. Mesos removes the centralised scheduling
    model that would otherwise require to compute global schedules for all the tasks
    running on the different frameworks connected to Mesos. Instead, each framework
    on a Mesos cluster can define its own scheduling strategies. For instance, Mesos
    can be connected with MPI or Hadoop [71]. Mesos utilises a master process to manage
    slave daemons running on each node. A typical Mesos cluster includes 3 ∼ 5 masters
    with one acting as the leader and the rest on standby. The master controls scheduling
    across frameworks through resource offers that provide resource availability of
    the cluster to slaves. However, the master process only suggests the amount of
    resources that can be given to each framework according to the policies of organisations,
    e.g fair sharing. Each framework rules which resources or tasks to accept. Once
    a resource offer is accepted by a framework, the framework passes Mesos a description
    of the tasks. The slave comprises two components, i.e., a scheduler registered
    to the master to receive resources and an executor process to run tasks from the
    frameworks. Mesos is a non-monolithic scheduler which acts as an arbiter that
    allocates resources across multiple schedulers, resolves conflicts, and ensures
    fair distribution of resources. Apache YARN (Yet Another Resource Negotiator)
    [72] is a monolithic scheduler which was developed in the first place to schedule
    Hadoop jobs. YARN is designed for long-running batch jobs and is unsuitable for
    long-running services and short-lived interactive queries. Mesosphere Marathon6
    is a container orchestration framework for Apache Mesos. Literature has seen the
    usage of Mesos together with Marathon in container orchestration on HPC systems
    as detailed in Section IV-C3. 4) Ansible Ansible [73] is a popular software orchestration
    tool. More specifically, it handles configuration management, application deployment,
    cloud provisioning, ad-hoc task execution, network automation and multi-node orchestration.
    The architecture of Ansible is simple and flexible, i.e., it does not require
    a special server or daemons running on the nodes. Configurations are set by playbooks
    that utilise yaml to describe the automation jobs, and connections to other nodes
    are via ssh. Nodes managed by Ansible are grouped into inventories that can be
    defined by users or drawn from different Cloud environments. Ansible is adopted
    by the SODALITE framework (Section IV-C4) as a key component to automatically
    build container images. 5) OpenStack OpenStack [74] is mostly deployed as infrastructure-as-a-service
    (IaaS)7 [75] on Cloud. It can be utilised to deploy and manage cloud-based infrastructures
    that support various use cases, such as web hosting, Big Data projects, software
    as a service (SaaS) [76] delivery and deployment of containers, VMs or bare-metal.
    It presents a scalable and highly adaptive open source architecture for Cloud
    solutions and helps to leverage hardware resources [77]. It also manages heterogeneous
    compute, storage and network resources. Together with its support of containers,
    container orchestrators such as Docker Swarm, Kubernetes and Mesos, Openstack
    enables the possibilities to quickly deploy, maintain, and upgrade complex and
    highly available infrastructures. OpenStack is also used in HPC communities to
    provide IaaS to end-users, enabling them to dynamically create isolated HPC environments.
    Academia and industry have developed a plethora of Cloud orchestrators. This article
    only reviews the ones that are mostly relevant to the HPC communities and the
    ones that have seen their usage in container orchestration for HPC systems, and
    the rest is out of the scope herein. C. Bridge Orchestration Strategies Between
    HPC and Cloud There are numerous works in literature [11], [78], [79], [80] on
    container orchestration for Cloud clusters, however, they are herein out of the
    scope. This section reviews the works that have been performed on the general
    issues of bridging the gap between conventional HPC and service-oriented infrastructures
    (Cloud). Overall, the state-of-the-art works on container orchestration for HPC
    systems fall into four categories as illustrated in Fig. 6. Added functionalities
    to HPC workload managers. It relies on workload managers for resource management
    and scheduling; meanwhile adopts additional software such as MPI for container
    orchestration. Fig. 6. The four types of container orchestration on HPC systems.
    Show All Connector between Cloud and HPC. Containers are scheduled from Cloud
    clusters to HPC clusters. This architecture isolates the HPC resources from Cloud
    so as to ensure HPC environment security; meanwhile offers application developments
    with flexible environments and powerful computing resources. Cohabitation. Workload
    managers and Cloud orchestrators co-exist on an HPC cluster, such as IBM LSF-Kubernetes.
    This gives a direction for the provision of HPC resources as services. In practice,
    the HPC workload managers and Cloud orchestrators do not coexist in one cluster.
    Meta-orchestration. An additional orchestrator is implemented on top of the Cloud
    orchestrator and HPC workload manager. There are pros and cons of the above four
    categories, which are outlined in Table VI. In addition, a research and engineering
    trend [12], [30], [81], [82], [83] is to move HPC applications to Cloud, as Cloud
    provides flexible and cost-effective services which are favoured by small-sized
    or middle-sized business. Beltre et al. [84] proposed to manage HPC applications
    by Kubernetes on a Cloud cluster with powerful computing resources and InfiniBand,
    which demonstrated comparable performance in containerised and bare-metal environments.
    The approach of this kind may be extended to HPC systems, however, remains unpractical
    for HPC centres to completely substitute their existing workload managers. TABLE
    VI A List of the Related Work on Container Orchestration for HPC Systems 1) Added
    Functionalities to WLM A potential research direction is to complement workload
    managers with container orchestration or make use of the existing HPC software
    stacks. Wofford et al. [85] simply adopt Open Runtime Environment (orted) reference
    implementation from Open MPI to orchestrate container launch suitable for arbitrary
    batch schedulers. Julian et al. [86] proposed their prototype for container orchestration
    in an HPC environment. A PBS-based HPC cluster can automatically scale up and
    down as load demands by launching Docker containers using the job scheduler Moab
    [98]. Three containers serve as the front-end system, scheduler (it runs PBS and
    Moab inside) and compute node (launches pbs_mom daemon, see Section IV-A1). More
    compute node containers are scheduled when there is no sufficient number of physical
    nodes. Unused containers are destroyed via external Python scripts when jobs complete.
    This approach may offer a solution for resource elasticity on HPC systems (Section
    V-B6). Similarly, an early study [87] described two models that can orchestrate
    Docker containers using an HPC workload manager. The former model launches a container
    to behave as one compute node which holds all assigned processes, whilst the latter
    boots a container per process by MPI launchers. The latter work seems to be outdated
    as to MPI applications which can be now automatically scaled with Singularity
    support. 2) Connector Between Cloud and HPC Cloud technologies are evolving to
    be able to support complex applications of HPC, Big Data and AI. Nevertheless,
    the applications with intensive computation and high inter-processor communication
    could not scale well, particularly due to the lack of low latency networks (e.g.,
    InfiniBand) and the usage of network virtualisation for network isolation. A research
    and development trend is to converge HPC and Cloud in order to take advantage
    of the resource management and scheduling of both HPC and Cloud infrastructures
    with minimal intrusion to HPC environments. Furthermore, the software stack and
    workflows in Cloud and HPC are usually developed and maintained by different organisations
    and users with various goals and methodologies, hence a connector between HPC
    and Cloud systems would bridge the gap and solve compatibility problems. Zhou
    et al. [88], [89], [90], [91] described the design of a plugin named Torque-Operator
    that serves as the key component to its proposed hybrid architecture. The containerised
    AI applications are scheduled from the Kubernetes-managed Cloud cluster to the
    TORQUE-managed HPC cluster where the performance of the compute-intensive or data-demanding
    applications can be significantly enhanced. This approach is less intrusive to
    HPC systems, however, its architecture shows one drawback: the latency of the
    network bridging the Cloud and HPC clusters can be high, when a large amount of
    data needs to be transferred in-between. DKube8 is a commercial software that
    is able to execute a wide range of AI/ML components scheduled from Kubernetes
    to SLURM. The software comprises a Kubernetes plugin and a SLURM Plugin. The former
    is represented as a hub that runs MLOps (Machine Learning Operations) management
    and associated Kubernetes workloads, while the latter connects to SLURM. 3) Cohabitation
    Liu et al. [92] showed how to dynamically migrate computing resources between
    HPC and OpenStack clusters based on demands. At a higher level, IBM has demonstrated
    the ability to run Kubernetes pods on Spectrum LSF where LSF acts as a scheduler
    for Kubernetes. An additional Kubernetes scheduler daemon needs to be installed
    into the LSF cluster, which acts as a bridge between LSF and the Kuberentes server.
    Kubelet will execute and manage pod lifecycle on target nodes in the normal fashion.
    IBM released LSF connector to Kubernetes, which makes use of the core LSF scheduling
    technologies and Kubernetes API functionalities. Kubernetes needs to be installed
    in a subset of the LSF managed HPC cluster. This architecture allows users to
    run Kubernetes and HPC batch jobs on the same infrastructure. The LSF scheduler
    is packed into containers and users submit jobs via kubectl. The LSF scheduler
    listens to the Kubernetes API server and translates pod requests into jobs for
    the LSF scheduler. This approach can add additional heavy workloads to HPC systems,
    as Kubernetes relies deployments of services across clusters to perform load balancing,
    scheduling, auto scheduling, etc. Piras et al. [93] implemented a method that
    expanded Kubernetes clusters with HPC clusters through Grid Engine. Submission
    is performed by PBS jobs to launch Kubernetes jobs. Therefore, HPC nodes are added
    to Kubernetes clusters by installing Kubernetes core components (i.e., kubeadm
    and Kubelet) and Docker container engine. On HPC, especially HPC production systems
    in HPC centres, adding new software packages that require using root privileges
    can cause security risks and alter the working environments of current users.
    The security issues will be further elaborated in Section V-A4. Khan et al. [1]
    proposed to containerise HPC workloads and install Mesos and Marathon (Section
    IV-B3) on HPC clusters for resource management and container orchestration. Its
    orchestration system can obtain the appropriate resources satisfying the needs
    of requested services within defined Quality-of-Service (QoS) parameters, which
    is considered to be self-organised and self-managed meaning that users do not
    need to specifically request resource reservation. Nevertheless, this study has
    not shown insight into novel strategies of container orchestration for HPC systems.
    Wrede et al. [94] performed their experiments on HPC clusters using Docker Swarm
    as the container orchestrator for automatic node scaling and using C++ algorithmic
    skeleton library Muesli [99] for load balance. Its proposed working environment
    is targeted for Cloud clusters. Usage of Docker cannot be easily extended to HPC
    infrastructures especially to HPC production systems due to the security risks.
    4) Meta-Orchestration Croupier [95] is a plugin implemented on Cloudify9 server
    that is located at a separate node in addition to the nodes that are managed by
    an HPC workload manager and a Cloud orchestrator. Croupier establishes a monitor
    to collect the status of every infrastructure and the operations (e.g., status
    of the HPC batch queue). Croupier together with Cloudify, can orchestrate batch
    applications in both HPC and Cloud environments. Similarly, Di Nitto et al. [96]
    presented the SODALITE10 framework by utilising XOpera11 to manage the application
    deployment in heterogeneous infrastructures. Colonnelli et al. [97] presented
    a proof-of-concept framework (i.e., Streamflow) to execute workflows on top of
    the hybrid architecture consisting of Kubernetes-managed Cloud and OCCAM [100]
    HPC cluster. D. Section Highlights HPC workload managers and Cloud orchestrators
    have distinct ways to manage clusters mainly because of their types of workloads
    and hardware resource availabilities. Table V summaries the differences of key
    features between HPC workload managers and Cloud Orchestrators. Typical HPC jobs
    are large workloads with long but ascertainable execution time and large throughput.
    HPC jobs are often submitted to a batch queue within a workload manager where
    jobs wait to be scheduled from minutes to days. Per contra, job requests can be
    granted immediately on Cloud as resources are available on demand. Batch-queuing
    is insufficient to satisfy the needs of Cloud communities: most of jobs are short
    in duration and the Cloud services are persistently long-running programs. Most
    of the HPC workload managers support Checkpointing that allows applications to
    save the execution states of a running job and restart the job from the checkpointing
    when a crash happens. This feature is critical for an HPC application with execution
    time typically from hours to months. Because it enables the application to recover
    from error states or resume from the state when it was previously terminated by
    the workload manager when its walltime limit had been reached or resource allocation
    had been exceeded. In contrary, jobs on Cloud, which are often micro-service programs,
    are usually relaunched in case of failures [101]. A container orchestrator offers
    an important property, i.e., container status monitoring. This is practical for
    long-running Cloud services, as it can monitor and replace unhealthy containers
    per desired configuration. HPC systems do not offer the equivalence of container
    pod which bundle performance monitoring services with the application itself as
    in Cloud systems [13]. Additionally, HPC workload managers often do not provide
    capabilities of application elasticity or necessary API at execution time, however,
    these capabilities are important for task migration and resource allocation changes
    at runtime on Cloud [102]. Section IV-C has reviewed the approaches to address
    the issues of container orchestration on HPC systems, which are summarised in
    Table VI. Overall, a container orchestrator on its own does not address all the
    requirements of HPC systems [3], as a result cannot replace existing workload
    managers in HPC centres. An HPC workload manager lacks micro-service support and
    deeply-integrated container management capabilities in which container orchestrators
    manifest their efficiency. SECTION V. Research Challenges and Vision The distinctions
    between Cloud and HPC clusters are diminishing, especially with the trend of HPC
    Clouds in industry [103]. HPC Cloud is becoming an alternative to on-premise HPC
    clusters for executing scientific applications and business analytics models [16].
    Containerisation technologies help to ease the efforts of moving applications
    between Cloud and HPC. Nevertheless, not all applications are suitable for containerisation.
    For instance, in the typical HPC applications such as weather forecast or modelling
    of computational fluid dynamics, any virtualisation or high-latency networks can
    become the bottlenecks for performance. Containerisation in HPC still faces challenges
    of different folds (Section V-A). Interest in using containers on HPC systems
    is mainly due to the encapsulation and portability that yet may trade off with
    performance. In practice, containers deployed on HPC clusters often have large
    image size and as a result each HPC node can only host a few containers that are
    CPU-intensive and memory-demanding. In addition, implementation of AI frameworks
    such as TensorFlow and PyTorch [104] typically also have large container image
    size. Architecture of HPC containers should be able to easily integrate seamlessly
    with HPC workload managers. The research directions (Section V-B) which can be
    envisioned are not only to adapt the existing functionalities from Cloud to HPC,
    but to also explore the potentials of containerisation so as to improve the current
    HPC systems and applications. A. Challenges and Open Issues Although containerisation
    enables compatibility, portability and reproducibility, containerised environments
    still need to match the host architecture and exploit the underlying hardware.
    The challenges that containerisation faces on HPC systems are in three-fold: compatibility,
    security and performance. Some issues still remain as open questions. Table VII
    summarises the potential solutions to the research challenges and the open questions
    that will be discussed in this section. TABLE VII Overview of Research Challenges
    and Potential Solutions 1) Library Compatibility Issues Mapping container libraries
    and their dependencies to the host libraries can cause incompatibility. Glibc
    [105], which is an implementation of C standard library that provides core supports
    and interfaces to kernel features, can be a common library dependency. The version
    of Glibc on the host may be older or newer than the one in the container image,
    consequently introducing symbol mismatches. Additionally, when the container OS
    (e.g., Ubuntu 18.04) and the host OS are different (e.g., CentOS 7), it is likely
    that some kernel ABI are incompatible, which may lead to container crashes or
    abnormal behaviours. This issue can also occur to MPI applications. As a result
    users must either build an exact version of the host MPI or have the privilege
    to mount the host MPI dependency path into the container. A research direction
    to handle library mismatches between container images and hosts is to implement
    a container runtime library at a lower level. For instance, Nvidia implemented
    the library libnvidia-container12 that manages driver or library matching at container
    runtime, i.e., using a hook interface to inject and/or activate the correct library
    versions. However, the libnvidia-container library can be only applied to Nividia
    GPUs. A significant modification of this library code is likely to be needed in
    order to be adapted for other GPU suppliers. In practice, such a compatibility
    layer would also require supports from different HPC interconnect and accelerator
    vendors. 2) Compatibility Issues of Container Engines and Images Not all Docker
    images can be converted by HPC container engines to their own formats. Moreover,
    to reuse HPC container implementations between container engines, users need to
    learn different container command lines to build the corresponding images, which
    further complicates adoption of containers for HPC applications. This issue calls
    for container standardisation. OCI is a Linux foundation project that designs
    open standards for container image formats (a filesystem bundle or rootfs) and
    multiple data volume [106]. Some guidelines were proposed in [63], i.e., a container
    should be: Not bound to higher-level frameworks, e.g., an orchestration stack;
    Not tightly associated with any particular vendor or project; Portable across
    a wide variety of OSs, hardware, CPUs, clusters, etc. Unfortunately, this standard
    cannot guarantee that the runtime hooks built for one runtime can be used by another.
    For example, container privileges (e.g., mount host filesystems) assumed by one
    container runtime may not be translated to unprivileged runtimes (e.g., not all
    HPC centres have mount namespace enabled) [107]. 3) Kernel Optimisation In general,
    containers are forbidden by the host to install their own kernel modules for the
    purpose of application isolation [108]. This is a limitation for the applications
    requiring kernel customisation, because the kernels of their HPC hosts cannot
    be tuned and optimised. Shen et al. [108] proposed an Xcontainer to address this
    issue by tuning the Linux kernel into library OS that supports binary compatibility.
    This functionality is yet to be explored in HPC containers. 4) Security Issues
    Containers face three major threats [109]: Privilege Escalation. Attackers gain
    access to hosts and other containers by breaking out of their current containers.
    Denial-of-Service (DoS). An attack causes services to become inaccessible to users
    by disruption of a machine or network resources. Information Leak. Confidential
    details of other containers are leaked and utilised for further attacks. Multiple
    or many containers share a host kernel, therefore, one container may infect other
    containers. In this case, a container does not reduce attack surfaces, but rather
    brings multiple instances of attack surfaces. For example, starting from version
    V3.0, Singularity has added Cgroups support that allows users to limit the resources
    consumed by containers without the help from a batch scheduling system (e.g.,
    TORQUE). This feature helps to prevent DoS attacks when a container seizes control
    of all available system resources which prohibits other containers from operating
    properly. Execution of HPC containers (including the Docker Engine starting from
    v19.03) does not require root privileges on the host. Containers in general adopt
    namespaces to isolate resources among users and map a root user inside a container
    to a non-root user on the host. The User namespace nevertheless is not a panacea
    to resolve all problems of resource isolation. User exposes code in the kernel
    to non-privileged users, which was previously limited to root users. A container
    environment is generated by users, and it is likely that some software inside
    a container may be embedded with security vulnerabilities. Root users inside a
    container may escalate their privileges via application level vulnerability. This
    can bring security issues to the kernel that does not account for mapped PIDs/GIDs.
    This issue can be addressed in two ways: (1) avoiding root processes inside HPC
    containers; (2) installing container engines with user permission instead of sudo
    installation. Security issues of the user namespace continue to be discovered
    even in the latest version of Linux kernels. Therefore, many HPC production centres
    have disabled the configuration of this namespace, which prevents usage of almost
    any state-of-the-art HPC containers. How to address the risks of using namespaces
    still remains an open question. 5) Performance Degradation GPU and accelerators
    often require customised or proprietary libraries that need to be bound to container
    images so as to leverage performance. This operation is at the cost of portability
    [107]. It is de facto standard to utilise the optimised MPI libraries for HPC
    interconnects, such as InfiniBand and Slingshot [110], and it is likely that the
    container performance degrades in a different HPC infrastructure [22] (see Section
    III-B). There is no simple solution to address this issue. Another example presented
    in [111] identified the performance loss due to increasing communication cost
    of MPI processes. This occurs when the number of containers (MPI processes running
    inside containers) rises on a single node, e.g., point to point communication
    (MPI_Irecv, MPI_Isend), polling of pending asynchronous messages (MPI_Test) and
    collective communication (MPI_Allreduce). B. Research and Engineering Opportunities
    Research studies should continue working on solutions to the open question identified
    in Section V-A. This section discusses current research and engineering directions
    that are interesting, yet still need further development. This section also identifies
    new research opportunities that yet need to be explored. The presentation of this
    section is arranged from short-term vision to long-term efforts. Table VIII summarises
    the potentials discovered in literature and the prospects given by the authors.
    TABLE VIII Future Directions of Research and Engineering 1) Containerisation of
    AI in HPC Model training of AI/DL applications can immensely benefit from the
    compute power (GPU or CPU), storage and security [112] of HPC clusters in addition
    to the superior GPU-aware scheduling and features of workflow automation provided
    by workload managers. The trained models are subsequently deployed on Cloud for
    scalability at low cost and on HPC for computation speed. Exploiting HPC infrastructures
    for ML/DL training is becoming a topic of increasing importance [113]. For example,
    Fraunhofer13 has developed the software framework Carme14 that combines established
    open source ML and Data Science tools with HPC backends. The execution environments
    of the tools are provided by predefined Singularity containers. AI applications
    are usually developed with high-level scripting languages or frameworks, e.g.,
    TensorFlow and PyTorch, which often require connections to external systems to
    download a list of open-source software packages during execution. For instance,
    an AI application written in Python cannot be compiled into an executable that
    has included all the dependencies ready for execution as in C/C++. Therefore,
    the developers need flexibility to customise the execution environments. Since
    HPC environments, especially on HPC production systems, are often based on closed-source
    applications and their users have restricted account privileges and security restrictions
    [6], deployment of AI applications on HPC infrastructures is challenging. Besides
    the predefined module environments or virtual environments (such as Anaconda),
    containerisation can be an alternative candidate, which enables easy transition
    of AI workloads to HPC while fully taking advantage of HPC hardware and the optimised
    libraries of AI applications without compromising security. Huerta et al. [114]
    recommend three guidelines for containerisation of AI applications for HPC centres:
    Provide up-to-date documentation and tutorials to set up or launch containers.
    Maintain versatile and up-to-date base container images that users can clone and
    adapt, such as a container registry (see Section V-B2). Give instructions on installation
    or updates of software packages into containers. The AI program depends on distributed
    training software, such as Horovod [115], which then depends on system architecture
    and specific versions of software packages such as MPI. Increasing amount of new
    software frameworks are being developed using containerisation technologies to
    facilitate deployment of AI applications on HPC systems. Further research is still
    needed to improve scalability and enable out-of-box usage. 2) HPC Container Registry
    Container registry is a useful repository to provide pre-built container images
    that can be accessed easily either by public or private users by pulling images
    to the host directly. It is portable to deploy applications in this way on Cloud
    clusters. Accesses to external networks are often blocked in HPC centres, so users
    need to upload images onto the clusters manually. One solution is to set up a
    private registry within the HPC centres that offer pre-built images suitable for
    the targeted systems and architectures. A container registry is also a way to
    ensure container security. It is a good security practice to ensure that images
    executed on the HPC systems are signed and pulled from a trusted registry. Scanning
    vulnerabilities on the registry should be regularly performed. To simplify usage,
    the future work can enable HPC workload managers to boot the default containers
    on the compute nodes (by pulling images from the private registry) which match
    the environments with all the required libraries and configuration files of user
    login nodes where users implement their own workflows and submit their jobs. The
    jobs should be started without user awareness of the presence of containers and
    without additional user intervention. 3) Linux Namespace Guidelines The set of
    Linux namespaces used within an implementation depends on the policies of HPC
    centres [116]. HPC centres should provide clear instructions on the availabilities
    of namespaces. For example, different user groups may have different namespaces
    enabled or disabled. A minimal set of namespaces should be enabled for a general
    user group: mount and user, which are suitable for node-exclusive scheduling.
    PID and Cgroups should be provided to restrict resource usage and enforce process
    privacy, which are useful for shared-node scheduling. Advanced use cases may require
    additional sets of namespaces. When users submit the container jobs, workload
    managers can start the containers with appropriate namespaces enabled. 4) DevOps
    DevOps aims at integrating efforts of development (Dev) and operations (Ops) to
    automate fast software delivery while ensuring correctness and reliability [117],
    [118]. This concept is influential in Cloud Computing and has been widely adopted
    in industry, as DevOps tools minimise the overhead of managing a large amount
    of micro-services. In HPC environments, typical applications have large workloads,
    hence the usage of DevOps should concentrate on research reproducibility. Nevertheless,
    the off-the-shelf DevOps tools are not well fitted for HPC environments, e.g.,
    the dependencies of MPI applications are too foreign for the state-of-the-art
    DevOps tools. A potential solution is to develop HPC-specific DevOps tools for
    the applications that are built and executed on on-premise clusters [16]. Unfortunately,
    HPC environments are known to be inflexible and typical HPC applications are optimised
    to leverage resources, thereby generation of DevOps workflows can be restricted
    and slow. Such obstacles can be overcome by containerisation, which may provision
    DevOps environments. For instance, Sampedro et al. [119] integrate Singularity
    with Jenkins [120] that brings CICD15 practices into HPC workflows. Jenkins is
    an open-source automation platform for building and deploying software, which
    has been applied at some HPC sites as a general-purpose automation tool. 5) Middleware
    System A middleware system, which bridges container building environments with
    HPC resource managers and schedulers, can be flexible. A middleware system can
    be either located on an HPC cluster or connect to it with secured authentication.
    The main task of the middleware is to perform job deployment, job management,
    data staging and generating non-root container environments [121]. Different container
    engines can be swiftly switched, optimisation mechanisms can be adapted to the
    targeted HPC systems and workflow engines [122] can be easily plugged in. Middleware
    systems can be a future research direction that provides a portable way to enable
    DevOps in HPC centres. 6) Resource Elasticity One major difference between resource
    management on HPC and Cloud is the elasticity [123], i.e., an HPC workload manager
    runs on a fixed set of hardware resources and the workloads of its jobs at any
    point can not exceed the resource capacity, while Cloud orchestrators can scale
    up automatically the hardware resources to satisfy user needs (e.g., AWS spot
    instances). Static reservation is a limitation for efficient resource usages on
    HPC systems [124]. One future direction of containerisation for HPC systems can
    work towards improvement of the elasticity of HPC infrastructure, which can be
    introduced to its workload manager. In [123], the authors presented a novel architecture
    that utilises Kubernetes to instantiate the containerised HPC workload manager.
    In this way, the HPC infrastructure is dynamically instantiated on demand and
    can be served as a single-tenant or multi-tenant environment. A complete containerised
    environments on HPC system may be impractical and much more exploration is still
    needed. 7) Moving Towards Minimal OS Containers may be utilised to partially substitute
    the current HPC software stack. Typical compute nodes on HPC clusters do not contain
    local storage (e.g., hardware disk), therefore lose states after reboots. The
    compute node boots via a staged approach [116]: (1) a kernel and initial RAM disk
    are loaded via a network device; (2) a root filesystem is mounted via the network.
    In a monolithic stateless system, modification of the software components often
    requires system rebooting to completely activate the functions of updates. Using
    containerised software packages on top of a minimal OS (base image) on the compute
    nodes, reduces the number of components in the kernel image, hence decreasing
    the frequency of node reboots. Furthermore, the base image of reduced size also
    simplifies the post-boot configurations that need to run in the OS image itself,
    consequently the node rebooting time is minimised. Additionally, when a failure
    occurs, a containerised service can be quickly replaced without affecting the
    entire system. Long-term research is required on HPC workload managers to control
    the software stack and workloads that are partially native and partially containerised.
    Moreover, it needs to explored whether containerisation of the entire OS on HPC
    systems is feasible. SECTION VI. Concluding Remarks This paper presents a survey
    and taxonomy for the state-of-the-art container engines and container orchestration
    strategies specifically for HPC systems. It underlines differences of containerisation
    on Cloud and HPC systems. The research and engineering challenges are also discussed
    and the opportunities are envisioned. HPC systems start to utilise containers
    as thereof reduce environment complexity. Efforts have been also made to ameliorate
    container security on HPC systems. This article identified three points to increase
    the security level: (1) set on-site container registry, (2) give Linux namespaces
    guidelines (3) and remove root privilege meanwhile avoid permission escalation.
    Ideally, HPC containers should require no pre-installation of container engines
    or installation can be performed without root privileges, which not only meets
    the HPC security requirements but also simplifies the container usability. Containers
    will continue to play a role in reducing the performance gap and deployment complexity
    between on-premise HPC clusters and public Clouds. Together with the advancement
    of low-latency networks and accelerators (e.g., GPUs, TPUs [125]), it may eventually
    reshape the two fields. Containerised workloads can be moved from HPC to Cloud
    so as to temporarily relieve the peak demands and can be also scheduled from Cloud
    to HPC in order to exploit the powerful hardware resources. The research and engineering
    trend are working towards implementation of the present container orchestrators
    within HPC clusters, which however still remains experimental. Many studies have
    been devoted to container orchestration on Cloud, however, it can be foreseen
    that the strategies will be eventually introduced to HPC workload managers. In
    the future, it can be presumed that containerisation will play an essential role
    in application development, improve resource elasticity and reduce complexity
    of HPC software stacks. ACKNOWLEDGMENTS The authors would like to thank Dr. Joseph
    Schuchart for proof-reading the contents. Authors Figures References Citations
    Keywords Metrics Footnotes More Like This Implementing a Hybrid Virtual Machine
    Monitor for Flexible and Efficient Security Mechanisms 2010 IEEE 16th Pacific
    Rim International Symposium on Dependable Computing Published: 2010 I/O for Virtual
    Machine Monitors: Security and Performance Issues IEEE Security & Privacy Published:
    2008 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Software Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Containerization for High Performance Computing Systems: Survey and Prospects'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Du X.
  - Dastmalchi F.
  - Ye H.
  - Garrett T.J.
  - Diller M.A.
  - Liu M.
  - Hogan W.R.
  - Brochhausen M.
  - Lemas D.J.
  citation_count: '4'
  description: 'Background: Liquid chromatography-high resolution mass spectrometry
    (LC-HRMS) is a popular approach for metabolomics data acquisition and requires
    many data processing software tools. The FAIR Principles – Findability, Accessibility,
    Interoperability, and Reusability – were proposed to promote open science and
    reusable data management, and to maximize the benefit obtained from contemporary
    and formal scholarly digital publishing. More recently, the FAIR principles were
    extended to include Research Software (FAIR4RS). Aim of review: This study facilitates
    open science in metabolomics by providing an implementation solution for adopting
    FAIR4RS in the LC-HRMS metabolomics data processing software. We believe our evaluation
    guidelines and results can help improve the FAIRness of research software. Key
    scientific concepts of review: We evaluated 124 LC-HRMS metabolomics data processing
    software obtained from a systematic review and selected 61 software for detailed
    evaluation using FAIR4RS-related criteria, which were extracted from the literature
    along with internal discussions. We assigned each criterion one or more FAIR4RS
    categories through discussion. The minimum, median, and maximum percentages of
    criteria fulfillment of software were 21.6%, 47.7%, and 71.8%. Statistical analysis
    revealed no significant improvement in FAIRness over time. We identified four
    criteria covering multiple FAIR4RS categories but had a low %fulfillment: (1)
    No software had semantic annotation of key information; (2) only 6.3% of evaluated
    software were registered to Zenodo and received DOIs; (3) only 14.5% of selected
    software had official software containerization or virtual machine; (4) only 16.7%
    of evaluated software had a fully documented functions in code. According to the
    results, we discussed improvement strategies and future directions.'
  doi: 10.1007/s11306-023-01974-3
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Metabolomics Article Evaluating LC-HRMS
    metabolomics data processing software using FAIR principles for research software
    Review Article Published: 06 February 2023 Volume 19, article number 11, (2023)
    Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Metabolomics Aims and scope Submit manuscript Xinsong Du, Farhad Dastmalchi, Hao
    Ye, Timothy J. Garrett, Matthew A. Diller, Mei Liu, William R. Hogan, Mathias
    Brochhausen & Dominick J. Lemas  4239 Accesses 3 Citations 3 Altmetric Explore
    all metrics Abstract Background Liquid chromatography-high resolution mass spectrometry
    (LC-HRMS) is a popular approach for metabolomics data acquisition and requires
    many data processing software tools. The FAIR Principles – Findability, Accessibility,
    Interoperability, and Reusability – were proposed to promote open science and
    reusable data management, and to maximize the benefit obtained from contemporary
    and formal scholarly digital publishing. More recently, the FAIR principles were
    extended to include Research Software (FAIR4RS). Aim of review This study facilitates
    open science in metabolomics by providing an implementation solution for adopting
    FAIR4RS in the LC-HRMS metabolomics data processing software. We believe our evaluation
    guidelines and results can help improve the FAIRness of research software. Key
    scientific concepts of review We evaluated 124 LC-HRMS metabolomics data processing
    software obtained from a systematic review and selected 61 software for detailed
    evaluation using FAIR4RS-related criteria, which were extracted from the literature
    along with internal discussions. We assigned each criterion one or more FAIR4RS
    categories through discussion. The minimum, median, and maximum percentages of
    criteria fulfillment of software were 21.6%, 47.7%, and 71.8%. Statistical analysis
    revealed no significant improvement in FAIRness over time. We identified four
    criteria covering multiple FAIR4RS categories but had a low %fulfillment: (1)
    No software had semantic annotation of key information; (2) only 6.3% of evaluated
    software were registered to Zenodo and received DOIs; (3) only 14.5% of selected
    software had official software containerization or virtual machine; (4) only 16.7%
    of evaluated software had a fully documented functions in code. According to the
    results, we discussed improvement strategies and future directions. Similar content
    being viewed by others Dissemination and analysis of the quality assurance (QA)
    and quality control (QC) practices of LC–MS based untargeted metabolomics practitioners
    Article 12 October 2020 Data standards can boost metabolomics research, and if
    there is a will, there is a way Article Open access 17 November 2015 Reference
    materials for MS-based untargeted metabolomics and lipidomics: a review by the
    metabolomics quality assurance and quality control consortium (mQACC) Article
    Open access 09 April 2022 1 Introduction Metabolomics is the systematic study
    of small molecules (metabolites) within cells, biofluids, tissues, or organisms,
    and it has been widely used in clinical studies (Zhang et al., 2020). Liquid Chromatography
    – High Resolution Mass Spectrometry (LC-HRMS) is a popular data acquisition technique
    due to its high sensitivity and specificity (Zhang et al., 2020). Data processing
    is the first computational step of a metabolomics study and is very important
    for downstream analysis, such as statistical analysis and data interpretation
    (Du et al., 2022). Therefore, our study focused on LC-HRMS metabolomics data processing
    steps. The FAIR Principles – Findability, Accessibility, Interoperability, and
    Reusability – were proposed to promote open science and reusable data management,
    and to maximize the benefit obtained from contemporary and formal scholarly digital
    publishing. To date, numerous software has been developed for LC-HRMS metabolomics
    data processing (Spicer et al., 2017) such as XCMS (Smith et al., 2006; Tautenhahn
    et al., 2012), MZmine (Pluskal et al., 2010), and MS-DIAL (Tsugawa et al., 2015);
    however, there is limited information on the FAIRness of LC-HRMS metabolomics
    data processing software. To enhance the propensity of data and other digital
    objects for sharing and reuse by humans and at scale by machines, the FAIR Principles
    (Findable, Accessible, Interoperable, and Reusable) originated in the Netherlands
    during the 2014 Lorentz Workshop “Jointly Designing a Data FAIRport”. Following
    consultation via the Future of Research Communications and e-Scholarship (FORCE11),
    the FAIR Principles with 15 detailed guiding principles were published by Wilkinson
    et al., in 2016 (Wilkinson et al., 2016). With such an arresting and rhetorically
    useful acronym, the FAIR Principles have gained greater uptake than earlier encapsulation
    of these ideas (Directorate-General for Research and Innovation (European Commission),
    2018). To date, FAIR Principles have been implemented in several areas such as
    precision oncology data sharing and bioinformatics data management (Mayer et al.,
    2021; Vesteghem et al., 2020). The adoption of FAIR Principles does not only require
    FAIR data, but also other FAIR digital objects such as software (Chue Hong et
    al., 2022). FAIR for Research Software (FAIR4RS) Working Group (Katz, Barker,
    et al., 2021) was established in 2020 to develop community-endorsed FAIR Principles
    for Research Software (Chue Hong et al., 2022). In the released FAIR4RS Principles,
    “Findable” (F) means the software and its metadata can be easily found by humans
    and machines; “Accessible” (A) means the software and its metadata can be retrieved
    via standardized protocols; “Interoperable” (I) means software can interoperates
    with other software; and “Reusable” (R) means software can be understood, modified,
    built upon, or incorporated into other software (Chue Hong et al., 2022). Clinical
    studies conducted with FAIR research software are associated with better transparency,
    reproducibility, and reusability (Barker et al., 2022), leading to more useful
    research and better translational potential (Goodman et al., 2016). However, the
    FAIR4RS Principles is a general description that only depicts a continuum of features
    that moves a research software closer to that goal (Wilkinson et al., 2019). Since
    FAIR speaks to machine-actionable operations, FAIR digital objects should be amenable
    to unambiguous forms of validation and evaluation, and a practical interpretation
    of the FAIR4RS Principles and the development of detailed evaluation guidelines
    are needed for the implementation (Wilkinson et al., 2019). In the field of LC-HRMS
    metabolomics data processing, a detailed implementation solution for FAIR4RS Principles
    has not been investigated yet. The purpose of this study is to implement FAIR4RS
    Principles in the evaluation of LC-HRMS metabolomics data processing software.
    Through a systematic review, we identified relevant LC-HRMS metabolomics data
    processing software. Next, we evaluated them using criteria related to FAIR4RS
    Principles, which were based on published papers regarding best practices of research
    software and internal discussions (XD, MAD, HY, DJL). Each of the criteria was
    also mapped to the corresponding FAIR4RS Principles. To remove potential ambiguity,
    detailed evaluation guidelines were also developed and refined through discussion.
    The goal of our analysis is focused on identifying strategies to improve the FAIRness
    of software for LC-HRMS metabolomics data processing. 2 Methods 2.1 Study selection
    process As illustrated by Fig. 1, we followed PRISMA guidelines for literature
    review (Liberati et al., 2009). Steps include keyword search, duplicate removal,
    title-abstract scanning, full-text review, and tool extraction. Search databases
    include Web of Science (WOS), PubMed, and Embase. The literature search strategy
    was established by consulting a librarian. As shown by Table S1, search terms
    have words related to LC-HRMS metabolomics, software, reproducibility, and LC-HRMS
    data processing. During the Boolean search, we restricted search fields to title
    and abstract, as well as subject headings for some keywords, and only studies
    published during the past 5 years as of 2021 August were included. During the
    paper screening, Covidence software was employed to help with the process (Covidence
    - Better Systematic Review Management, n.d.). We extracted potentially related
    software from eligible papers based on sentences around their names in the paper
    or descriptions in the tool’s paper if the tool has a publication. Then, the software
    was selected and evaluated based on eligibility and evaluation criteria via reading
    all their available documentation, publications, and code repositories. Notably,
    each paper was screened by two reviewers and each software was evaluated by two
    reviewers. All criteria could be considered as a question like “does the paper/software
    has a specific feature”. When there is a disagreement between the two reviewers,
    the reviewer said the paper/software had the feature that would need to show evidence
    and persuade the other reviewer, then the final label of a specific criterion
    was decided based on the result of the persuasion. Detailed guidelines for resolving
    discrepancies were illustrated in Table S3. Fig. 1 Consort diagram for the literature
    review and the computational tool extraction. The screening has two phases: literature
    screening and tool screening. During literature screening, 1396 papers published
    in the past 5 years were obtained through keyword search, 70 papers were finally
    included as relevant ones. In tool screening phase, 122 potentially relevant software
    were extracted from the 70 papers. We added two software that were recommended
    by an expert but were not mentioned in the 70 papers. All 124 software were reviewed
    in more detailed by reading other available resources online such as documentations
    and code repositories. 61 software were finally considered eligible for final
    FAIR4RS review. Full size image 2.2 Inclusion and exclusion criteria 2.2.1 Related
    steps We refer LC-HRMS metabolomics data processing to all steps after data acquisition
    and before statistical analysis. We documented 13 relevant steps for each software
    tool. A detailed list and descriptions of the steps are included in the supplementary
    material Table S2 along with literature references (Clasquin et al., 2012; DeFelice
    et al., 2017; FillPeaks-Methods, n.d.; Liu et al., 2020; Mayer et al., 2013; Smith
    et al., 2006; Zhou et al., 2012). We created a controlled vocabulary for these
    steps based on ontologies and literature. The steps were categorized into four
    main categories: 1. Data preparation: Steps included in this category happen immediately
    after data acquisition. These steps make the data usable with computational software
    in the lab. File format conversion and parameter optimization are the only two
    steps involved. 2. Feature generation: This category represents processes of generating
    features and their intensity values in the peak table. Nine steps are included
    in this category: mass detection, chromatogram building, deconvolution, peak grouping,
    retention time alignment, peak filling, ion annotation, and batch effect correction.
    3. Quality control: Evaluating the analytical variability of the data to make
    sure the acquired data have good quality for downstream analysis. 4. Metabolite
    annotation: The process of determining the identity or chemical structure of a
    metabolite underlying a group of peaks, methods that are used to enhance the proximation
    result are also included in this category. 2.2.2 Paper screening The goal of paper
    screening was to select papers that mentioned or used software related to LC-HRMS
    metabolomics data processing. Therefore, during title-abstract scanning, we excluded
    studies based on the following criteria: 1. We excluded papers that were not written
    in English. 2. We excluded papers whose instrumental analysis did not include
    LC-HRMS. For example, studies used gas chromatography-mass spectrometry (GC-MS)
    or nuclear magnetic resonance (NMR). 3. We excluded papers that were not about
    metabolomics, such as papers focusing on proteomics. 4. We excluded papers that
    did not evaluate computational steps. For instance, some studies only focused
    on sample processing steps. 5. We excluded reviews that did not describe specific
    LC-HRMS metabolomics software. During the full-text reading stage, we used information
    provided within the paper and used the following exclusion criteria for further
    filtering: 1. The paper did not include a computational tool. 2. No tool in the
    paper was about metabolomics. 3. Some software in the paper was about metabolomics
    but not related to LC-HRMS data processing steps. 2.2.3 Tool screening For initial
    screening, the relevance of a certain tool was judged according to its context
    in the paper. Software for which at least one reviewer thought it could perform
    related steps was included. During tool screening, we further investigated whether
    a tool had a certain function based on its documentation. For example, if a tool
    did not contain a tutorial regarding how to do quality control, we would think
    it did not have the function; this applied even if a researcher might figure out
    a way to do quality control with the tool. Then, the software was further filtered
    by reading publicly available documentation, publications, and code repositories.
    The exclusion criteria were: 1. The tool was no longer available. 2. The tool
    was produced by a commercial company. 3. Functions included in the tool were not
    related to the steps in Table S2. 4. The tool was an extension of an existing
    tool and did not have its webpage or documentation. 2.3 Information gathering
    process To extract information manually, the full texts of selected articles were
    read by two reviewers. Relevant software included in these articles was deduplicated
    and evaluated by two reviewers for general information - name, first release date,
    supported operating systems, major programming languages, literature citation,
    supported mode (i.e., web-based/standalone/plug-in), and open-source or not -
    about the tool and specific FAIR4RS related criteria. Data processing steps a
    software tool can perform were extracted by going through the documentation as
    well as literature containing relevant information (Li et al., 2018; Misra, 2018,
    2021; Misra et al., 2017; Misra & Mohapatra, 2019; O’Shea & Misra, 2020; Stanstrup
    et al., 2019). To ensure the correctness of the annotation of steps that a software
    tool can perform, we first did the annotation by ourselves, then contacted all
    senior authors of included software via emails for confirmation, correction, and
    comments. We also sent a follow-up email to software authors that did not respond
    within five days. The software authors we contacted were recorded in Table S4.
    FAIR4RS-related evaluation criteria were extracted by going through publications
    related to research software best practices, and additional criteria were added
    based on internal discussions (XD, MAD, HY, DJL). Each selected evaluation criterion
    was then assigned to one or more FAIR4RS categories through discussions between
    two bioinformaticians (XD and MAD) and one reproducibility expert (HY). The detailed
    evaluation guideline for each criterion was created and refined through discussion
    between the two reviewers (XD and FD) to make sure the description was not ambiguous.
    We screened the latest version of each tool at the time of evaluation, the time
    spanned from October 2021 to August 2022. During the evaluation process, each
    criterion was assigned a value of “TRUE”, “FALSE” or “NA” based on publicly available
    documentation and code repositories. “TRUE” represents that the tool met the criterion;
    “FALSE” meant the tool did not meet the criterion; “NA” meant the criterion does
    not apply to the tool. For example, if the tool did not have the command-line
    option, then the criterion of “include help command” would be not applicable (NA).
    2.4 Synthesis of results The percentage of criteria fulfillment of each tool was
    calculated by dividing the number of “TRUE”s by the number of applicable criteria
    of the tool (i.e. ignoring any NA values). We also calculated the percentage of
    software that met each criterion, which was calculated by dividing the number
    of “TRUE”s by the number of software that could apply the criterion. For example,
    a tool may have 45 applicable criteria out of the total 47 criteria, and 35 criteria
    were labeled “TRUE”, then the percentage of criteria fulfillment would be 35/45*100%=77.8%.
    This can identify FAIR4RS-related criteria that merit further attention from the
    authors of each tool as well as future developers. 3 Results 3.1 Paper screening
    As illustrated in Fig. 1, the initial search generated 1396 records and 954 articles
    after removing duplicates. The title-abstract screening reduced the number to
    87 articles. During the full-text screening, 17 articles were excluded: 4 did
    not include a computational tool; 8 mentioned computational software but they
    were not about metabolomics; and 5 included metabolomics software but the software
    was not for LC-HRMS data processing. Paper screening record exported from Covidence
    is included in the supplementary material Table S4. 3.2 Tool screening We extracted
    122 potentially related software from the 70 eligible papers, then added two relevant
    software (EI-MAVEN, MS-FLO) we knew but were not mentioned in the 70 papers. That
    said, the two software were not mentioned in the notes produced by reviewers (Table
    S5 -> sheet “full_text_included” -> column “Notes”) during the full-text review.
    The number of software was reduced to 61 after reviewing their documentation in
    detail. In terms of function annotation, senior authors of 49 out of the 61 included
    software responded to our inquiry emails. General information and detailed evaluation
    results of the 61 selected software (Adusumilli & Mallick, 2017; Agrawal et al.,
    2019; Alonso et al., 2011; Broeckling et al., 2014; Brunius et al., 2016; Bueschl
    et al., 2017; Cai et al., 2015; Capellades et al., 2016; Chokkathukalam et al.,
    2013; Chong & Xia, 2018; Clasquin et al., 2012; Creek et al., 2012; Davidson et
    al., 2016; De Livera et al., 2018; DeFelice et al., 2017; Del Carratore et al.,
    2019; Dührkop et al., 2019; Fischer et al., 2022; Franceschi et al., 2014; Gatto
    et al., 2021; Giacomoni et al., 2015; Guo et al., 2021; Helmus et al., 2021; Huan
    & Li, 2015a, b; Huang et al., 2014; F. Huber, Ridder, et al., 2021; Hughes et
    al., 2014; Jaitly et al., 2009; Ji et al., 2017, 2019; Kantz et al., 2019; Kuhl
    et al., 2012; Kutuzova et al., 2020; Li et al., 2017; Libiseller et al., 2015;
    Liggi et al., 2018; Lommen, 2009; Loos, 2016; Mahieu et al., 2016; Müller et al.,
    2020; Olivon et al., 2018; Palarea-Albaladejo et al., 2018; Pang et al., 2021;
    Pluskal et al., 2010; Protsyuk et al., 2018; Ridder et al., 2012; Ross et al.,
    2020; Röst et al., 2016; Ruttkies et al., 2016; Shen et al., 2019; Smith et al.,
    2006; Tautenhahn et al., 2012; Teo et al., 2020; Tsugawa et al., 2015, 2016; Uppal
    et al., 2013, 2017; Weber & Viant, 2010; Yu et al., 2009; Zhou et al., 2014) was
    included in the supplementary material Table S5. Venn diagrams regarding technical
    properties were illustrated in the supplementary material Figure S1. In the 63
    excluded software, 8 were excluded because they were not available, 24 were excluded
    due to their association with commercial companies, 28 were excluded because functions
    were not relevant, and 3 were excluded because they were not independent (i.e.,
    an extension of another selected tool). Here “independent” means the tool has
    its documentation or webpage. For example, we consider MetaboAnalystR a different
    tool from MetaboAnalyst since MetaboAnalystR has its own GitHub page and webpage.
    However, we considered CSI:FinderID part of SIRIUS since we did not find CSI:FinderID
    to have its documentation at the time we did our evaluation. Similarly, MSnBase
    and CAMERA are included in the xcms software tool, but they were considered “independent”
    in our study since they had their webpage and documentation. All excluded software
    and detailed reasons for exclusion were shown in the supplementary material Table
    S6. 3.3 FAIR4RS evaluation FAIR4RS evaluation results were illustrated in Fig.
    2 and Fig. 3. As illustrated in Table 1, we summarized 47 FAIR4RS-related criteria
    in total, among which 41 were from the literature regarding research software
    best practices, and 6 were based on internal discussions (XD, MAD, HY, DJL). At
    the time of our evaluation, the minimum, first quartile, median, third quartile,
    and maximum percentages of the fulfillment of criteria were about 21.6%, 39.5%,
    47.7%, 57.9%, and 71.8%. Two software (OpenMS and patRoon) fulfilled no less than
    70% applicable criteria. Ten software met 60-70% applicable criteria, they were
    MSnbase, EI-MAVEN, smartPeak, Spec2Vec, SIRIUS, ProteoWizard-msConvert, W4M, xcms,
    MetaboAnalystR, and MAGMa. Additionally, we found seven software (OpenMS, patron,
    EI-MAVEN, W4M, MetaboAnalystR, MS-DIAL, MetaboAnalyst) could perform the greatest
    number of steps (9 out of 13) involved in LC-HRMS metabolomics data processing.
    Three software were powered by artificial intelligence (AI) techniques (Spec2Vec,
    DeepMASS, and No_NAME), and they fulfilled 62.2%, 47.4%, and 26.3% of our evaluation
    criteria. We also found most of the included software was open source except MetaboAnalyst,
    MetAlign, and XCMS-Online, and none of the closed-source software was ranked within
    the first quartile of all included software. Figure 4 represented the percentage
    of criteria fulfilled in each category. Notably, some criteria were related to
    multiple FAIR4RS categories. An overview of the evaluation results in terms of
    each category is illustrated below: 1. Findability: We identified eight criteria
    that were related to findability. Among the eight criteria, “Have web-based documentation”
    had the best fulfillment of 100%. “Register to Zenodo and get DOI” was associated
    with the least fulfillment of ~ 6% and “Include any semantic annotation with controlled
    vocabulary underlying ontologies in the documentation” had the lowest fulfillment
    of 0%. 2. Accessibility: We identified four criteria that were corresponding to
    accessibility. Among the four criteria, “Code has version control” had the greatest
    fulfillment of over 80%. However, “Provide official software containerization
    or virtual machine” had ~ 14% fulfillment, and “Register to Zenodo and get DOI”
    was associated with the least fulfillment of ~ 6%. 3. Interoperability: We identified
    12 criteria that were about interoperability. Among the 12 criteria, “Use conventional
    input and output” and “Explain file format in documentation if a new format is
    created” had the best fulfillment of 100%. Nevertheless, “Have fully documented
    functions in code” had ~ 17% fulfillment, “Provide official software containerization
    or virtual machine” had ~ 14% fulfillment, “Document exit status” had ~ 2% fulfillment,
    and “Include any semantic annotation with controlled vocabulary underlying ontologies
    in the documentation” had 0% fulfillment. 4. Reusability: We identified 41 criteria
    that were related to reusability. Among the 41 criteria, three criteria (“Use
    conventional input and output”, “Have user documentation”, and “Explain file format
    in documentation if a new format is created”) had 100% fulfillment. However, we
    found 13 criteria with a fulfillment of < 20%: “Have FAQ in documentation”, “Have
    developer documentation”, “Have information on potential errors and warnings as
    well as ways to resolve them in documentation”, “Have fully documented functions
    in code”, “Have an output log file containing loaded modules/dependencies during
    the execution”, “Have code coverage assessment”, “Provide official software containerization
    or virtual machine”, “Have a searchable forum”, “Have comment of creator’s email
    in code”, “Have historical contribution record in documentation”, “Have comment
    of creation date in code”, “Have automated code quality checks”, “Document exit
    status”, and “Include any semantic annotation with controlled vocabulary underlying
    ontologies in the documentation”. 3.4 FAIRness over time Table S5 had information
    of the release time of every included tool. We observed that there was only one
    included tool was released in 2006 (xcms), 2008 (MZmine), and 2021 (DaDIA), and
    no tool was released in 2007. Multiple included software was associated with a
    release time within each year between 2009 and 2020. Figure 5 is a scatter plot
    representing the change of FAIRness through time. Pearson’s correlation (Schober
    et al., 2018) indicated: (1) there was no significant (p < 0.05) FAIRness improvement
    across time; (2) findability has a trend of decrease; (3) all four categories
    were positively correlated with a statistical significance, and interoperability
    and reusability had the highest coefficient. To investigate what factors might
    contribute to the decrease of findability, we did a multiple linear regression
    using criteria related to findability and found four factors were significantly
    and positively correlated to findability: (1) have a name; (2) have a citation
    guide in documentation; (3) register to Zenodo and get DOI; (4) have dependencies
    and version numbers in documentation. Table 1 Information about evaluation criteria
    Full size table Fig. 2 Summary of basic results. Functions associated with each
    tool were displayed on the left side. Percentages of evaluating criteria that
    a software tool fulfilled are shown in the middle of the figure. The right panel
    indicates the percentage of criteria fulfillment of each tool. Additionally, names
    of software powered by AI techniques are highlighted in red, and software that
    are closed source are highlighted in black. Full size image Fig. 3 Line chart
    reflecting the criteria fulfillment of each category. X-axis represents tool names,
    ranked by overall percentage of fulfillment from left to right. Y-axis on the
    left side stands for percentage of fulfilled criteria. Overall fulfillment is
    represented by the black solid line. Fulfillments of findability, accessibility,
    interoperability, and reusability categories are represented by blue, yellow,
    green, and red dash line separately. Software are ranked by their release time,
    which is represented by the grey line along with the secondary y-axis. Full size
    image Fig. 4 Percentage of software that fulfilled each evaluation criterion.
    X-axes are values for percentage of fulfillment, and y-axes list all evaluation
    criteria. There are 4 categories and 47 criteria in total. (A) Blue bars represent
    findability related criteria; (B) yellow bars represent criteria related to accessibility;
    (C) green bars represent criteria related to interoperability; and (D) red bars
    stand for criteria regarding reusability. Full size image Fig. 5 FAIRness trend
    across time. X-axis are years representing selected software? first release times,
    y-axis stands for the averaged %fulfillment of software released in each year.
    (A) represents the relationship between findability and tool release times; (B)
    represents the relationship between accessibility and tool release times; (C)
    represents the relationship between interoperability and tool release times; (D)
    represents the relationship between reusability and tool release times; (E) represents
    the overall FAIR4RS criteria fulfillment and tool release times. Results of Pearson?s
    correlation are also included in each sub-figure; (F) includes information regarding
    correlations among categories. Full size image Table 2 Multiple linear regression
    for criteria related to findability Full size table 4 Discussion The application
    of FAIR Principles in the metabolomics field has largely focused on data sharing
    and management (Mendez et al., 2019; Rocca-Serra & Sansone, 2019; Savoi et al.,
    2021). More recently, the FAIR Data Principles have been translated into research
    software (Hasselbring et al., 2020; Katz, Gruenpeter, et al., 2021; Lamprecht
    et al., 2020), and the first version of FAIR4RS Principles was released in May
    2022 (Chue Hong et al., 2022). In this study, we used FAIR4RS-related criteria
    to evaluate 61 selected data processing software, including 58 open-source software
    and 3 closed-source software. Most (41 out of 61) included criteria were related
    to reusability, which is consistent with previous findings (Wolf et al., 2021),
    and we also found the four categories were positively related to each other. Notably,
    open-source code is not required by FAIR Principles (Katz, Gruenpeter, et al.,
    2021), thus, it was not one of our evaluation criteria. To date, no study has
    been conducted to investigate how to implement FAIR4RS Principles in the metabolomics
    field. Therefore, our study extends previous work by implementing FAIR4RS Principles
    to assess LC-HRMS metabolomics data processing software. 4.1 FAIRness improvement
    strategies The primary findings from our evaluation revealed that semantic annotation
    of key information was associated with 0% fulfillment among all evaluated software,
    notably, the criterion is related to findability, interoperability, and reusability.
    Semantic annotation of key information includes functions, input and output data
    type, and format. Describing key information using controlled vocabulary underlying
    an ontology makes it readable and discoverable by both humans and machines (Lamprecht
    et al., 2020). Although we noticed that many software such as XCMS has been registered
    to bio.tools (Ison et al., 2019) that include semantic annotation of software,
    none of the official documentation mentioned this feature at the time of our evaluation.
    Notably, semantic annotation does not only improve the findability of the tool
    but also enables machines to find other similar software based on the annotation
    (Lamprecht et al., 2021). Semantic annotation of data processing software can
    be used to find relevant software to develop workflows automatically (Lamprecht
    et al., 2021), such systems have been used for analyzing proteomics data (Kasalica
    et al., 2021; Palmblad et al., 2019) and DNA sequence data (Zheng et al., 2015).
    Therefore, adding semantic annotation information to the official documentation
    may improve the FAIRness of LC-HRMS metabolomics data processing software dramatically.
    We also found three other criteria that were related to multiple FAIR categories
    for which only a small percentage of software was compliant. Firstly, only about
    6% of software was registered to Zenodo and received a digital object identifier
    (DOI). As explained in Table 1, unlike other identifiers such as GitHub page URL
    or the DOI assigned by Bioconductor, Zenodo assigns each version of the software
    a persistent and distinct DOI (Berrios et al., n.d.; van de Sandt et al., 2019),
    facilitating findability and accessibility. Notably, we also found the criterion
    of registering software to Zenodo contributes to the decrease in findability over
    time. The FAIR4RS Working Group created a community on Zenodo to ensure the FAIRness
    of research outputs (Chue Hong et al., 2022). Notably, GitHub now has an integration
    of Zenodo as a third-party tool to help users with referencing and citing content
    (Referencing and Citing Content, n.d.). The low percentage of fulfillment (6%)
    is also expected since we observed most of the included software are R packages
    (39 out of 61 as illustrated in Figure S1). Unlike other popular repositories
    for R packages such as GitHub, R-Force, CRAN, and Bioconductor (Decan et al.,
    2015); users cannot install an R package deposited to Zenodo in the console using
    “install.packages”. Therefore, to improve software FAIRness, we recommend future
    developers deposit packages to both Zenodo and an R repository. We also recommend
    Zenodo developers provide a wrapper or guide regarding how to install R packages
    using a Zenodo URL. Additionally, we found only about 15% of software provided
    an official containerized version, which is relevant to accessibility, interoperability,
    and reusability. Providing a containerized version enables users to execute the
    software on different machines smoothly and without worrying about the installation
    of dependencies (Georgeson et al., 2019; Senington et al., 2018). In terms of
    R-based software, containerization including only a single package may not be
    that useful since users may want to use multiple packages for the analysis. The
    RofMassSpectrometry Initiative (Rainer et al., 2022; RforMassSpectrometry, n.d.)
    and metaRbolomics Toolbox in Bioconductor (Stanstrup et al., 2019) are currently
    two popular ecosystems for R-based metabolomics software. Therefore, a community-wide
    containerized ecosystem including all commonly-used and mature R-based software
    for the metabolomics community would be very helpful. Another poorly fulfilled
    criterion is “fully documented functions in code”, which is related to interoperability
    and reusability. Documenting input, output, and errors that a function may raise
    makes it easier for users to inspect the software and learn how the software can
    interact with other software (Lee, 2018). Therefore, in addition to semantic annotation,
    our results also demonstrate the FAIRness of LC-HRMS metabolomics data processing
    software also needs: (1) registering to Zenodo and getting version controlled
    DOIs, (2) providing official software containerization or virtual machine, and
    (3) providing fully documented functions in code. Advances in academic publishing
    of software have required checklists to ensure the quality of submissions, such
    as the Journal of Open Source Software (Review Checklist — JOSS Documentation,
    n.d.). Our results revealed that 59 out of 61 software were associated with a
    peer-reviewed publication. Checklists have been used to promote transparent reporting
    of metabolomics studies (Considine & Salek, 2019; Fiehn et al., 2007; Goodacre
    et al., 2007; Snyder et al., 2014; Sumner et al., 2007). Our recent work proposed
    a checklist to promote reproducible computational analysis of clinical metabolomics
    research (Du et al., 2022). The evaluation guideline in this study can also be
    used as a checklist by journals for promoting FAIR4RS Principles. A recent study
    shows that a direct request from the editor for research resource identifiers
    (RRID) is a more effective way for users to provide RRID than merely writing it
    in the journal’s author instructions document (Menke et al., 2020). Similarly,
    we expect a direct request from the journal editor regarding some criteria in
    our evaluation guideline will be a more effective way for FAIRness improvement
    than just providing a checklist in the author’s guideline. Additionally, some
    journals such as Analytical Chimica Acta now require an assessment of the tool
    by an independent advanced user, who is not a software developer or bioinformatician
    (Analytica Chimica Acta | Journal | ScienceDirect.Com by Elsevier, n.d.). In summary,
    our evaluation guideline can be used during peer-review as a checklist to improve
    the FAIRness of software associated with a scientific publication. 4.2 Strengths,
    limitations, and future works Our study has several strengths. We followed PRISMA
    guidelines to extract literature in the past 5 years (from 2016 to 2021) that
    might include a metabolomics data processing tool. Our evaluation checklist synthesized
    FAIR4RS-related criteria from existing literature regarding best software practices,
    including reproducible software development practices (Heil et al., 2021; Jiménez
    et al., 2017; Ram, 2013), best practices for scientific software (Hunter-Zinck
    et al., 2021; Leprevost et al., 2014; Zhao et al., 2012), best software documenting
    practices (Karimzadeh & Hoffman, 2018; Lee, 2018), best practices for command-line
    software (Georgeson et al., 2019; Seemann, 2013), and best software testing practices
    (Aghamohammadi et al., 2021). We also filtered out criteria that were not required
    by FAIR4RS Principles such as open-source (Katz, Gruenpeter, et al., 2021). Therefore,
    our checklist is more comprehensive than existing checklists, and more focused
    on FAIR4RS Principles. Additionally, our checklist is more specific than the general
    FAIR4RS Principles and with a detailed evaluation guideline. We used multiple
    reviewers to mitigate subjective bias. We extracted related software from selected
    papers, and read their publicly available documentation, publications, and code
    repositories for a comprehensive FAIRness evaluation. The FAIR4RS criteria were
    synthesized from the literature and additional criteria were added via discussions
    among authors (XD, MAD, HY, DJL), and the criteria can also be used to guide future
    FAIR LC-HRMS metabolomics software development. Each criterion was assigned to
    one or more FAIR4RS categories based on a discussion between two bioinformaticians
    (XD and MAD) and one reproducibility expert (YH). During the evaluation, two reviewers
    (XD and FD) went through the official documentation, publications, and code repositories
    of the 61 included data processing software based on an agreed evaluation guideline.
    More importantly, our evaluation method leads to a meaningful result. The result
    indicates that findability decreases over time, which is expected since people
    usually try to publish first and then build up nicer documentation afterward when
    resources are available, which means it takes time after the release for findability
    to increase. Although we only evaluated non-commercial LC-HRMS metabolomics data
    processing software, the criteria may also be suitable for evaluating all types
    of software. To date, this is the first study to provide an implementation solution
    for using FAIR4RS Principles in the metabolomics field. However, the results of
    this study should be considered along with some limitations of our experimental
    design. First, our evaluation was based only on publicly available materials including
    documentation and code. Secondly, we only checked the software’s official documentation
    but did not check the software’s machine-readable metadata (e.g., XCMS has machine-readable
    metadata in the software ontology (Malone et al., 2014)), which might be added
    by experts other than the software’s authors. Additionally, we considered all
    evaluation criteria equally important without weighting them. Furthermore, we
    included only software that was mentioned in metabolomics papers or metabolomics
    software review papers published in 2016–2021, thus, the list of tools may not
    be completed and some relevant software published very recently might not be included,
    such as MS2DeepScore (F. Huber, van der Burg, et al., 2021) and MS2Query (Jonge
    et al., 2022). The controlled vocabulary and categorization of data processing
    functions shown in Table S2 were produced by looking through literature (Clasquin
    et al., 2012; De Vos et al., 2007; FillPeaks-Methods, n.d.; Libiseller et al.,
    2015; Liu et al., 2020; Mayer et al., 2013; Smith et al., 2006; Vitale et al.,
    2022; Zhou et al., 2012) and discussing internally, and we admit that such a categorization
    is never easy. So, our function categorization may not be perfect and recognized
    by the entire metabolomics community, and software authors may have a slightly
    different understanding of our categorization and descriptions when responding
    to our inquiry. For instance, one software author emailed us saying functions
    like mass precision enhancement (Huber et al., 2022) should be part of the data
    processing steps. Notably, our study only focuses on evaluating the FAIRness of
    software using relevant qualitative properties, but FAIRness is just one of many
    considerations when selecting the software for a research workflow. For example,
    some software authors responded to us by saying the speed of data processing,
    the capability to process large-scale data, and the efficiency of hardware usage
    are very important aspects (Gatto et al., 2021). We did not use real-world data
    to get results using the included software and make a comparison. Our main focus
    is the software itself, and we did not consider factors (e.g., the type of the
    mass spectrometer) that may affect the development of the software. Additionally,
    it is essential to assess the quantitative performance of software using annotated
    LC-HRMS dataset and library (Hao et al., 2018). The FAIRness of the LC-HRMS dataset,
    spectral library, and software for metabolite proximation are important for the
    FAIRness of the entire workflow. Therefore, a future review regarding studies
    and available datasets for such benchmarking would be a very useful resource for
    FAIR LC-HRMS metabolomics workflows. 5 Conclusion We presented a comprehensive
    FAIRness evaluation for existing non-commercial LC-HRMS metabolomics data processing
    software. We evaluated 61 qualified software with 47 criteria related to FAIR4RS.
    Our result indicated that no software had perfect FAIR4RS compliance (i.e., fulfilled
    100% applicable criteria). The maximum of criteria fulfillment was 71.7%, meaning
    all evaluated software had considerable space for improvement. We also identified
    criteria that were poorly fulfilled for each category along with detailed strategies
    for improvement. We believe our study can serve as a guideline to create FAIR
    research software for LC-HRMS metabolomics data processing software. References
    Adusumilli, R., & Mallick, P. (2017). Data Conversion with ProteoWizard msConvert.
    Methods in Molecular Biology. (Clifton N J), 1550, 339–368. https://doi.org/10.1007/978-1-4939-6747-6_23.
    Article   CAS   Google Scholar   Aghamohammadi, A., Mirian-Hosseinabadi, S. H.,
    & Jalali, S. (2021). Statement frequency coverage: a code coverage criterion for
    assessing test suite effectiveness. Information and Software Technology, 129,
    106426. https://doi.org/10.1016/j.infsof.2020.106426. Article   Google Scholar   Agrawal,
    S., Kumar, S., Sehgal, R., George, S., Gupta, R., Poddar, S., Jha, A., & Pathak,
    S. (2019). El-MAVEN: A Fast, Robust, and User-Friendly Mass Spectrometry Data
    Processing Engine for Metabolomics. Methods in Molecular Biology (Clifton, N.J.),
    1978, 301–321. https://doi.org/10.1007/978-1-4939-9236-2_19 Alonso, A., Julià,
    A., Beltran, A., Vinaixa, M., Díaz, M., Ibañez, L., Correig, X., & Marsal, S.
    (2011). AStream: an R package for annotating LC/MS metabolomic data. Bioinformatics,
    27(9), 1339–1340. https://doi.org/10.1093/bioinformatics/btr138. Article   CAS   PubMed   Google
    Scholar   Analytica Chimica Acta | Journal | ScienceDirect.com by Elsevier. (n.d.).
    Retrieved September 16, from https://www.sciencedirect.com/journal/analytica-chimica-acta
    Barker, M., Chue Hong, N. P., Katz, D. S., Lamprecht, A. L., Martinez-Ortiz, C.,
    Psomopoulos, F., Harrow, J., Castro, L. J., Gruenpeter, M., Martinez, P. A., &
    Honeyman, T. (2022). Introducing the FAIR principles for research software. Scientific
    Data, 9(1), https://doi.org/10.1038/s41597-022-01710-x. Berrios, D. C., Beheshti,
    A., & Costes, S. V. (n.d.). FAIRness and Usability for Open-access Omics Data
    Systems. 10. Broeckling, C. D., Afsar, F. A., Neumann, S., Ben-Hur, A., & Prenni,
    J. E. (2014). RAMClust: a Novel feature clustering method enables spectral-matching-based
    annotation for Metabolomics Data. Analytical Chemistry, 86(14), 6812–6817. https://doi.org/10.1021/ac501530d.
    Article   CAS   PubMed   Google Scholar   Brunius, C., Shi, L., & Landberg, R.
    (2016). Large-scale untargeted LC-MS metabolomics data correction using between-batch
    feature alignment and cluster-based within-batch signal intensity drift correction.
    Metabolomics: Official Journal of the Metabolomic Society, 12(11), 173. https://doi.org/10.1007/s11306-016-1124-4.
    Article   CAS   PubMed   Google Scholar   Bueschl, C., Kluger, B., Neumann, N.
    K. N., Doppler, M., Maschietto, V., Thallinger, G. G., Meng-Reiterer, J., Krska,
    R., & Schuhmacher, R. (2017). MetExtract II: a Software suite for stable isotope-assisted
    untargeted metabolomics. Analytical Chemistry, 89(17), 9518–9526. https://doi.org/10.1021/acs.analchem.7b02518.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Cai, Y., Weng, K.,
    Guo, Y., Peng, J., & Zhu, Z. J. (2015). An integrated targeted metabolomic platform
    for high-throughput metabolite profiling and automated data processing. Metabolomics,
    11(6), 1575–1586. https://doi.org/10.1007/s11306-015-0809-4. Article   CAS   Google
    Scholar   Capellades, J., Navarro, M., Samino, S., Garcia-Ramirez, M., Hernandez,
    C., Simo, R., Vinaixa, M., & Yanes, O. (2016). geoRge: a computational Tool to
    detect the Presence of stable isotope labeling in LC/MS-Based untargeted metabolomics.
    Analytical Chemistry, 88(1), 621–628. https://doi.org/10.1021/acs.analchem.5b03628.
    Article   CAS   PubMed   Google Scholar   Chokkathukalam, A., Jankevics, A., Creek,
    D. J., Achcar, F., Barrett, M. P., & Breitling, R. (2013). mzMatch–ISO: an R tool
    for the annotation and relative quantification of isotope-labelled mass spectrometry
    data. Bioinformatics, 29(2), 281–283. https://doi.org/10.1093/bioinformatics/bts674.
    Article   CAS   PubMed   Google Scholar   Chong, J., & Xia, J. (2018). MetaboAnalystR:
    an R package for flexible and reproducible analysis of metabolomics data. Bioinformatics,
    34(24), 4313–4314. https://doi.org/10.1093/bioinformatics/bty528. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Chue Hong, N. P., Katz, D. S., Barker, M., Lamprecht,
    A. L., Martinez, C., Psomopoulos, F. E., Harrow, J., Castro, L. J., Gruenpeter,
    M., Martinez, P. A., Honeyman, T., Struck, A., Lee, A., Loewe, A., van Werkhoven,
    B., Jones, C., Garijo, D., Plomp, E., & Genova, F. (2022). … WG, R. F. FAIR Principles
    for Research Software (FAIR4RS Principles). https://doi.org/10.15497/RDA00068
    Clasquin, M. F., Melamud, E., & Rabinowitz, J. D. (2012). LC-MS Data Processing
    with MAVEN: A Metabolomic Analysis and Visualization Engine. Current Protocols
    in Bioinformatics / Editoral Board, Andreas D. Baxevanis … et Al.], 0 14, Unit14.11.
    https://doi.org/10.1002/0471250953.bi1411s37 Considine, E. C., & Salek, R. M.
    (2019). A Tool to encourage Minimum Reporting Guideline Uptake for Data Analysis
    in Metabolomics. Metabolites, 9(3), E43. https://doi.org/10.3390/metabo9030043.
    Article   CAS   Google Scholar   Covidence—Better systematic review management.
    (n.d.). Covidence. Retrieved April 6, from https://www.covidence.org/ Creek, D.
    J., Jankevics, A., Burgess, K. E. V., Breitling, R., & Barrett, M. P. (2012).
    IDEOM: an Excel interface for analysis of LC-MS-based metabolomics data. Bioinformatics
    (Oxford England), 28(7), 1048–1049. https://doi.org/10.1093/bioinformatics/bts069.
    Article   CAS   PubMed   Google Scholar   Davidson, R. L., Weber, R. J. M., Liu,
    H., Sharma-Oates, A., & Viant, M. R. (2016). Galaxy-M: a Galaxy workflow for processing
    and analyzing direct infusion and liquid chromatography mass spectrometry-based
    metabolomics data. GigaScience, 5(1), 10. https://doi.org/10.1186/s13742-016-0115-8.
    Article   CAS   PubMed   PubMed Central   Google Scholar   De Livera, A. M., Olshansky,
    G., Simpson, J. A., & Creek, D. J. (2018). NormalizeMets: assessing, selecting
    and implementing statistical methods for normalizing metabolomics data. Metabolomics,
    14(5), 54. https://doi.org/10.1007/s11306-018-1347-7. Article   CAS   PubMed   Google
    Scholar   De Vos, R. C., Moco, S., Lommen, A., Keurentjes, J. J., Bino, R. J.,
    & Hall, R. D. (2007). Untargeted large-scale plant metabolomics using liquid chromatography
    coupled to mass spectrometry. Nature Protocols, 2(4), https://doi.org/10.1038/nprot.2007.95.
    Decan, A., Mens, T., Claes, M., & Grosjean, P. (2015). On the Development and
    Distribution of R Packages: An Empirical Analysis of the R Ecosystem. Proceedings
    of the 2015 European Conference on Software Architecture Workshops, 1–6. https://doi.org/10.1145/2797433.2797476
    DeFelice, B. C., Mehta, S. S., Samra, S., Čajka, T., Wancewicz, B., Fahrmann,
    J. F., & Fiehn, O. (2017). Mass Spectral feature list optimizer (MS-FLO): a Tool
    to minimize false positive peak reports in untargeted liquid Chromatography–Mass
    Spectroscopy (LC-MS) data Processing. Analytical Chemistry, 89(6), 3250–3255.
    https://doi.org/10.1021/acs.analchem.6b04372. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Del Carratore, F., Schmidt, K., Vinaixa, M., Hollywood,
    K. A., Greenland-Bews, C., Takano, E., Rogers, S., & Breitling, R. (2019). Integrated
    Probabilistic Annotation: a bayesian-based annotation method for metabolomic profiles
    integrating biochemical connections, isotope patterns, and Adduct Relationships.
    Analytical Chemistry, 91(20), 12799–12807. https://doi.org/10.1021/acs.analchem.9b02354.
    Article   CAS   PubMed   Google Scholar   Directorate-General for Research and
    Innovation (European Commission). (2018). Turning FAIR into reality: final report
    and action plan from the European Commission expert group on FAIR data. Publications
    Office of the European Union. https://doi.org/10.2777/1524. Du, X., Aristizabal-Henao,
    J. J., Garrett, T. J., Brochhausen, M., Hogan, W. R., & Lemas, D. J. (2022). A
    checklist for reproducible computational analysis in clinical Metabolomics Research.
    Metabolites, 12(1), https://doi.org/10.3390/metabo12010087. Dührkop, K., Fleischauer,
    M., Ludwig, M., Aksenov, A. A., Melnik, A. V., Meusel, M., Dorrestein, P. C.,
    Rousu, J., & Böcker, S. (2019). SIRIUS 4: a rapid tool for turning tandem mass
    spectra into metabolite structure information. Nature Methods, 16(4), https://doi.org/10.1038/s41592-019-0344-8.
    Article 4. Fiehn, O., Sumner, L. W., Rhee, S. Y., Ward, J., Dickerson, J., Lange,
    B. M., Lane, G., Roessner, U., Last, R., & Nikolau, B. (2007). Minimum reporting
    standards for plant biology context information in metabolomic studies. Metabolomics,
    3(3), 195–201. https://doi.org/10.1007/s11306-007-0068-0. Article   CAS   Google
    Scholar   fillPeaks-methods: Integrate areas of missing peaks in xcms: LC-MS and
    GC-MS Data Analysis. (n.d.). Retrieved April 6, from https://rdrr.io/bioc/xcms/man/fillPeaks-methods.html
    Fischer, D., Panse, C., & Laczko, E. (2022). cosmiq: Cosmiq - COmbining Single
    Masses Into Quantities (1.28.0). Bioconductor version: Release (3.14). https://doi.org/10.18129/B9.bioc.cosmiq
    Franceschi, P., Mylonas, R., Shahaf, N., Scholz, M., Arapitsas, P., Masuero, D.,
    Weingart, G., Carlin, S., Vrhovsek, U., Mattivi, F., & Wehrens, R. (2014). MetaDB
    a Data Processing Workflow in untargeted MS-Based Metabolomics experiments. Frontiers
    in Bioengineering and Biotechnology, 2, 72. https://doi.org/10.3389/fbioe.2014.00072.
    Article   PubMed   PubMed Central   Google Scholar   Gatto, L., Gibb, S., & Rainer,
    J. (2021). MSnbase, efficient and elegant R-Based Processing and visualization
    of raw Mass Spectrometry Data. Journal of Proteome Research, 20(1), 1063–1069.
    https://doi.org/10.1021/acs.jproteome.0c00313. Article   CAS   PubMed   Google
    Scholar   Georgeson, P., Syme, A., Sloggett, C., Chung, J., Dashnow, H., Milton,
    M., Lonsdale, A., Powell, D., Seemann, T., & Pope, B. (2019). Bionitio: demonstrating
    and facilitating best practices for bioinformatics command-line software. GigaScience,
    8, giz109. https://doi.org/10.1093/gigascience/giz109. Article   PubMed   PubMed
    Central   Google Scholar   Giacomoni, F., Le Corguillé, G., Monsoor, M., Landi,
    M., Pericard, P., Pétéra, M., Duperier, C., Tremblay-Franco, M., Martin, J. F.,
    Jacob, D., Goulitquer, S., Thévenot, E. A., & Caron, C. (2015). Workflow4Metabolomics:
    a collaborative research infrastructure for computational metabolomics. Bioinformatics,
    31(9), 1493–1495. https://doi.org/10.1093/bioinformatics/btu813. Article   CAS   PubMed   Google
    Scholar   Goodacre, R., Broadhurst, D., Smilde, A. K., Kristal, B. S., Baker,
    J. D., Beger, R., Bessant, C., Connor, S., Capuani, G., Craig, A., Ebbels, T.,
    Kell, D. B., Manetti, C., Newton, J., Paternostro, G., Somorjai, R., Sjöström,
    M., Trygg, J., & Wulfert, F. (2007). Proposed minimum reporting standards for
    data analysis in metabolomics. Metabolomics, 3(3), 231–241. https://doi.org/10.1007/s11306-007-0081-3.
    Article   CAS   Google Scholar   Goodman, S. N., Fanelli, D., & Ioannidis, J.
    P. A. (2016). What does research reproducibility mean? Science Translational Medicine,
    8(341), 341ps12-341ps12. Guo, J., Shen, S., Xing, S., & Huan, T. (2021). DaDIA:
    Hybridizing Data-Dependent and Data-Independent Acquisition Modes for Generating
    High-Quality Metabolomic Data.Analytical Chemistry, 93(4),2669–2677. https://doi.org/10.1021/acs.analchem.0c05022
    Hao, L., Wang, J., Page, D., Asthana, S., Zetterberg, H., Carlsson, C., Okonkwo,
    O. C., & Li, L. (2018). Comparative evaluation of MS-based Metabolomics Software
    and its application to preclinical Alzheimer’s Disease. Scientific Reports, 8(1),
    https://doi.org/10.1038/s41598-018-27031-x. Hasselbring, W., Carr, L., Hettrick,
    S., Packer, H., & Tiropanis, T. (2020). From FAIR research data toward FAIR and
    open research software. It - Information Technology, 62(1), 39–47. https://doi.org/10.1515/itit-2019-0040.
    Article   Google Scholar   Heil, B. J., Hoffman, M. M., Markowetz, F., Lee, S.
    I., Greene, C. S., & Hicks, S. C. (2021). Reproducibility standards for machine
    learning in the life sciences. Nature Methods, 18(10), 1132–1135. https://doi.org/10.1038/s41592-021-01256-7.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Helmus, R., ter Laak,
    T. L., van Wezel, A. P., de Voogt, P., & Schymanski, E. L. (2021). patRoon: open
    source software platform for environmental mass spectrometry based non-target
    screening. Journal of Cheminformatics, 13(1), 1. https://doi.org/10.1186/s13321-020-00477-w.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Huan, T., & Li, L.
    (2015a). Counting missing values in a metabolite-intensity data set for measuring
    the analytical performance of a metabolomics platform. Analytical Chemistry, 87(2),
    1306–1313. https://doi.org/10.1021/ac5039994. Article   CAS   PubMed   Google
    Scholar   Huan, T., & Li, L. (2015b). Quantitative metabolome analysis based on
    Chromatographic Peak Reconstruction in Chemical isotope labeling liquid chromatography
    Mass Spectrometry. Analytical Chemistry, 87(14), 7011–7016. https://doi.org/10.1021/acs.analchem.5b01434.
    Article   CAS   PubMed   Google Scholar   Huang, X., Chen, Y. J., Cho, K., Nikolskiy,
    I., Crawford, P. A., & Patti, G. J. (2014). X13CMS: Global Tracking of Isotopic
    Labels in untargeted metabolomics. Analytical Chemistry, 86(3), 1632–1639. https://doi.org/10.1021/ac403384n.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Huber, C., Nijssen,
    R., Mol, H., Philippe Antignac, J., Krauss, M., Brack, W., Wagner, K., Debrauwer,
    L., Vitale, M., Price, C. J., Klanova, E., Molina, J. G., Leon, B., Pardo, N.,
    Fernández, O., Szigeti, S. F., Középesy, T., Šulc, S., Čupr, L., & Lommen, P.,
    A (2022). A large scale multi-laboratory suspect screening of pesticide metabolites
    in human biomonitoring: from tentative annotations to verified occurrences. Environment
    International, 168, 107452. https://doi.org/10.1016/j.envint.2022.107452. Article   CAS   PubMed   Google
    Scholar   Huber, F., Ridder, L., Verhoeven, S., Spaaks, J. H., Diblen, F., Rogers,
    S., & van der Hooft, J. J. J. (2021). Spec2Vec: improved mass spectral similarity
    scoring through learning of structural relationships. PLOS Computational Biology,
    17(2), e1008724. https://doi.org/10.1371/journal.pcbi.1008724. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Huber, F., van der Burg, S., van der Hooft, J. J. J.,
    & Ridder, L. (2021). MS2DeepScore: a novel deep learning similarity measure to
    compare tandem mass spectra. Journal of Cheminformatics, 13(1), 84. https://doi.org/10.1186/s13321-021-00558-4.
    Article   PubMed   PubMed Central   Google Scholar   Hughes, G., Cruickshank-Quinn,
    C., Reisdorph, R., Lutz, S., Petrache, I., Reisdorph, N., Bowler, R., & Kechris,
    K. (2014). MSPrep—Summarization, normalization and diagnostics for processing
    of mass spectrometry–based metabolomic data. Bioinformatics, 30(1), 133–134. https://doi.org/10.1093/bioinformatics/btt589.
    Article   CAS   PubMed   Google Scholar   Hunter-Zinck, H., de Siqueira, A. F.,
    Vásquez, V. N., Barnes, R., & Martinez, C. C. (2021). Ten simple rules on writing
    clean and reliable open-source scientific software. PLOS Computational Biology,
    17(11), e1009481. https://doi.org/10.1371/journal.pcbi.1009481. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Ison, J., Ienasescu, H., Chmura, P., Rydza, E., Ménager,
    H., Kalaš, M., Schwämmle, V., Grüning, B., Beard, N., Lopez, R., Duvaud, S., Stockinger,
    H., Persson, B., Vařeková, R. S., Raček, T., Vondrášek, J., Peterson, H., Salumets,
    A., Jonassen, I., & Brunak, S. (2019). The bio.tools registry of software tools
    and data resources for the life sciences. Genome Biology, 20(1), 164. https://doi.org/10.1186/s13059-019-1772-6.
    Article   PubMed   PubMed Central   Google Scholar   Jaitly, N., Mayampurath,
    A., Littlefield, K., Adkins, J. N., Anderson, G. A., & Smith, R. D. (2009). Decon2LS:
    an open-source software package for automated processing and visualization of
    high resolution mass spectrometry data. Bmc Bioinformatics, 10(1), 87. https://doi.org/10.1186/1471-2105-10-87.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Ji, H., Xu, Y., Lu,
    H., & Zhang, Z. (2019). Deep MS/MS-Aided structural-similarity scoring for unknown
    metabolite identification. Analytical Chemistry, 91(9), 5629–5637. https://doi.org/10.1021/acs.analchem.8b05405.
    Article   CAS   PubMed   Google Scholar   Ji, H., Zeng, F., Xu, Y., Lu, H., &
    Zhang, Z. (2017). KPIC2: an effective Framework for Mass Spectrometry-Based Metabolomics
    using pure Ion Chromatograms. Analytical Chemistry, 89(14), 7631–7640. https://doi.org/10.1021/acs.analchem.7b01547.
    Article   CAS   PubMed   Google Scholar   Jiménez, R. C., Kuzak, M., Alhamdoosh,
    M., Barker, M., Batut, B., Borg, M., Capella-Gutierrez, S., Chue Hong, N., Cook,
    M., Corpas, M., Flannery, M., Garcia, L., Gelpí, J. L., Gladman, S., Goble, C.,
    González Ferreiro, M., Gonzalez-Beltran, A., Griffin, P. C., Grüning, B., & Crouch,
    S. (2017). Four simple recommendations to encourage best practices in research
    software. F1000Research, 6, ELIXIR-876. https://doi.org/10.12688/f1000research.11407.1
    de Jonge, N. F., Louwen, J. R., Chekmeneva, E., Camuzeaux, S., Vermeir, F. J.,
    Jansen, R. S., Huber, F., & van der Hooft, J. J. J. (2022). MS2Query: Reliable
    and Scalable MS2 Mass Spectral-based Analogue Search (p. 2022.07.22.501125). bioRxiv.
    https://doi.org/10.1101/2022.07.22.501125 Kantz, E. D., Tiwari, S., Watrous, J.
    D., Cheng, S., & Jain, M. (2019). Deep neural networks for classification of LC-MS
    spectral peaks. Analytical Chemistry, 91(19), 12407–12413. https://doi.org/10.1021/acs.analchem.9b02983.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Karimzadeh, M., & Hoffman,
    M. M. (2018). Top considerations for creating bioinformatics software documentation.
    Briefings in Bioinformatics, 19(4), 693–699. https://doi.org/10.1093/bib/bbw134.
    Article   PubMed   Google Scholar   Kasalica, V., Schwämmle, V., Palmblad, M.,
    Ison, J., & Lamprecht, A. L. (2021). APE in the Wild: Automated Exploration of
    Proteomics Workflows in the bio.tools Registry. Journal of Proteome Research,
    20(4), 2157–2165. https://doi.org/10.1021/acs.jproteome.0c00983. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Katz, D. S., Barker, M., Chue Hong, N. P., Castro,
    L. J., & Martinez, P. A. (2021, June 28). The FAIR4RS team: Working together to
    make research software FAIR. 2021 Collegeville Workshop on Scientific Software
    - Software Teams (Collegeville2021). Zenodo. https://doi.org/10.5281/zenodo.5037157
    Katz, D. S., Gruenpeter, M., & Honeyman, T. (2021). Taking a fresh look at FAIR
    for research software. Patterns, 2(3), 100222. https://doi.org/10.1016/j.patter.2021.100222.
    Article   PubMed   PubMed Central   Google Scholar   Kuhl, C., Tautenhahn, R.,
    Böttcher, C., Larson, T. R., & Neumann, S. (2012). CAMERA: an Integrated strategy
    for compound Spectra extraction and annotation of Liquid Chromatography/Mass Spectrometry
    Data Sets. Analytical Chemistry, 84(1), 283–289. https://doi.org/10.1021/ac202450g.
    Article   CAS   PubMed   Google Scholar   Kutuzova, S., Colaianni, P., Röst, H.,
    Sachsenberg, T., Alka, O., Kohlbacher, O., Burla, B., Torta, F., Schrübbers, L.,
    Kristensen, M., Nielsen, L., Herrgård, M. J., & McCloskey, D. (2020). SmartPeak
    automates targeted and quantitative Metabolomics Data Processing. Analytical Chemistry,
    92(24), 15968–15974. https://doi.org/10.1021/acs.analchem.0c03421 Article   CAS   PubMed   Google
    Scholar   Lamprecht, A.-L., Garcia, L., Kuzak, M., Martinez, C., Arcila, R., Martin
    Del Pico,E., Dominguez Del Angel, V., van de Sandt, S., Ison, J., Martinez, P.
    A., McQuilton,P., Valencia, A., Harrow, J., Psomopoulos, F., Gelpi, J. L., Chue
    Hong, N., Goble,C., & Capella-Gutierrez, S. (2020). Towards FAIR principles for
    research software.Data Science, 3(1), 37–59. https://doi.org/10.3233/DS-190026
    Lamprecht, A. L., Palmblad, M., Ison, J., Schwämmle, V., Manir, M. S. A., Altintas,
    I., Baker, C. J. O., Amor, A. B. H., Capella-Gutierrez, S., Charonyktakis, P.,
    Crusoe, M. R., Gil, Y., Goble, C., Griffin, T. J., Groth, P., Ienasescu, H., Jagtap,
    P., Kalaš, M., Kasalica, V., & Wolstencroft, K. (2021). Perspectives on automated
    composition of workflows in the life sciences (10:897). F1000Research. https://doi.org/10.12688/f1000research.54159.1
    Lee, B. D. (2018). Ten simple rules for documenting scientific software. PLOS
    Computational Biology, 14(12), e1006561. https://doi.org/10.1371/journal.pcbi.1006561.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Leprevost, F. V., Barbosa,
    V. C., Francisco, E. L., Perez-Riverol, Y., & Carvalho, P. C. (2014). On best
    practices in the development of bioinformatics software. Frontiers in Genetics,
    5, 199. https://doi.org/10.3389/fgene.2014.00199. Article   PubMed   PubMed Central   Google
    Scholar   Li, B., Tang, J., Yang, Q., Li, S., Cui, X., Li, Y., Chen, Y., Xue,
    W., Li, X., & Zhu, F. (2017). NOREVA: normalization and evaluation of MS-based
    metabolomics data. Nucleic Acids Research, 45(W1), W162–W170. https://doi.org/10.1093/nar/gkx449.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Li, Z., Lu, Y., Guo,
    Y., Cao, H., Wang, Q., & Shui, W. (2018). Comprehensive evaluation of untargeted
    metabolomics data processing software in feature detection, quantification and
    discriminating marker selection. Analytica Chimica Acta, 1029, 50–57. https://doi.org/10.1016/j.aca.2018.05.001.
    Article   CAS   PubMed   Google Scholar   Liberati, A., Altman, D. G., Tetzlaff,
    J., Mulrow, C., Gøtzsche, P. C., Ioannidis, J. P. A., Clarke, M., Devereaux, P.
    J., Kleijnen, J., & Moher, D. (2009). The PRISMA statement for reporting systematic
    reviews and meta-analyses of studies that evaluate healthcare interventions: explanation
    and elaboration. Bmj, 339, b2700. https://doi.org/10.1136/bmj.b2700. Article   PubMed   PubMed
    Central   Google Scholar   Libiseller, G., Dvorzak, M., Kleb, U., Gander, E.,
    Eisenberg, T., Madeo, F., Neumann, S., Trausinger, G., Sinner, F., Pieber, T.,
    & Magnes, C. (2015). IPO: a tool for automated optimization of XCMS parameters.
    Bmc Bioinformatics, 16(1), 118. https://doi.org/10.1186/s12859-015-0562-8. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Liggi, S., Hinz, C., Hall, Z., Santoru, M. L., Poddighe,
    S., Fjeldsted, J., Atzori, L., & Griffin, J. L. (2018). KniMet: a pipeline for
    the processing of chromatography–mass spectrometry metabolomics data. Metabolomics,
    14(4), 52. https://doi.org/10.1007/s11306-018-1349-5. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Liu, Q., Walker, D., Uppal, K., Liu, Z., Ma, C., Tran,
    V., Li, S., Jones, D. P., & Yu, T. (2020). Addressing the batch effect issue for
    LC/MS metabolomics data in data preprocessing. Scientific Reports, 10(1), 13856.
    https://doi.org/10.1038/s41598-020-70850-0. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Lommen, A. (2009). MetAlign: Interface-Driven, Versatile Metabolomics
    Tool for Hyphenated full-scan Mass Spectrometry Data Preprocessing. Analytical
    Chemistry, 81(8), 3079–3086. https://doi.org/10.1021/ac900036d. Article   CAS   PubMed   Google
    Scholar   Loos, M. (2016). enviPick: Peak Picking for High Resolution Mass Spectrometry
    Data (1.5). https://CRAN.R-project.org/package=enviPick Mahieu, N. G., Spalding,
    J. L., & Patti, G. J. (2016). Warpgroup: increased precision of metabolomic data
    processing by consensus integration bound analysis. Bioinformatics, 32(2), 268–275.
    https://doi.org/10.1093/bioinformatics/btv564. Article   CAS   PubMed   Google
    Scholar   Malone, J., Brown, A., Lister, A. L., Ison, J., Hull, D., Parkinson,
    H., & Stevens, R. (2014). The Software Ontology (SWO): a resource for reproducibility
    in biomedical data analysis, curation and digital preservation. Journal of Biomedical
    Semantics, 5(1), 25. https://doi.org/10.1186/2041-1480-5-25. Article   PubMed   PubMed
    Central   Google Scholar   Mayer, G., Montecchi-Palazzi, L., Ovelleiro, D., Jones,
    A. R., Binz, P. A., Deutsch, E. W., Chambers, M., Kallhardt, M., Levander, F.,
    Shofstahl, J., Orchard, S., Vizcaíno, J. A., Hermjakob, H., Stephan, C., Meyer,
    H. E., Eisenacher, M., & HUPO-PSI Group. (2013). &. The HUPO proteomics standards
    initiative- mass spectrometry controlled vocabulary. Database: The Journal of
    Biological Databases and Curation, 2013, bat009. https://doi.org/10.1093/database/bat009
    Mayer, G., Müller, W., Schork, K., Uszkoreit, J., Weidemann, A., Wittig, U., Rey,
    M., Quast, C., Felden, J., Glöckner, F. O., Lange, M., Arend, D., Beier, S., Junker,
    A., Scholz, U., Schüler, D., Kestler, H. A., Wibberg, D., Pühler, A., & Turewicz,
    M. (2021). Implementing FAIR data management within the German Network for Bioinformatics
    infrastructure (de.NBI) exemplified by selected use cases. Briefings in Bioinformatics,
    22(5), bbab010. https://doi.org/10.1093/bib/bbab010. Article   PubMed   PubMed
    Central   Google Scholar   Mendez, K. M., Pritchard, L., Reinke, S. N., & Broadhurst,
    D. I. (2019). Toward collaborative open data science in metabolomics using Jupyter
    Notebooks and cloud computing. Metabolomics, 15(10), 125. https://doi.org/10.1007/s11306-019-1588-0.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Menke, J., Roelandse,
    M., Ozyurt, B., Martone, M., & Bandrowski, A. (2020). The rigor and transparency
    Index Quality Metric for assessing Biological and Medical Science Methods. IScience,
    23(11), 101698. https://doi.org/10.1016/j.isci.2020.101698. Article   PubMed   PubMed
    Central   Google Scholar   Misra, B. B. (2018). New tools and resources in metabolomics:
    2016–2017. ELECTROPHORESIS, 39(7), 909–923. https://doi.org/10.1002/elps.201700441.
    Article   CAS   PubMed   Google Scholar   Misra, B. B. (2021). New software tools,
    databases, and resources in metabolomics: updates from 2020. Metabolomics, 17(5),
    49. https://doi.org/10.1007/s11306-021-01796-1. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Misra, B. B., Fahrmann, J. F., & Grapov, D. (2017).
    Review of emerging metabolomic tools and resources: 2015–2016. ELECTROPHORESIS,
    38(18), 2257–2274. https://doi.org/10.1002/elps.201700110. Article   CAS   PubMed   Google
    Scholar   Misra, B. B., & Mohapatra, S. (2019). Tools and resources for metabolomics
    research community: a 2017–2018 update. ELECTROPHORESIS, 40(2), 227–246. https://doi.org/10.1002/elps.201800428.
    Article   CAS   PubMed   Google Scholar   Müller, E., Huber, C. E., Brack, W.,
    Krauss, M., & Schulze, T. (2020). Symbolic aggregate approximation improves gap
    filling in high-resolution Mass Spectrometry Data Processing. Analytical Chemistry,
    92(15), 10425–10432. https://doi.org/10.1021/acs.analchem.0c00899. Article   CAS   PubMed   Google
    Scholar   Olivon, F., Elie, N., Grelier, G., Roussi, F., Litaudon, M., & Touboul,
    D. (2018). MetGem Software for the generation of Molecular Networks based on the
    t-SNE algorithm. Analytical Chemistry, 90(23), 13900–13908. https://doi.org/10.1021/acs.analchem.8b03099.
    Article   CAS   PubMed   Google Scholar   O’Shea, K., & Misra, B. B. (2020). Software
    tools, databases and resources in metabolomics: updates from 2018 to 2019. Metabolomics,
    16(3), 36. https://doi.org/10.1007/s11306-020-01657-3. Article   CAS   PubMed   Google
    Scholar   Palarea-Albaladejo, J., Mclean, K., Wright, F., & Smith, D. G. E. (2018).
    MALDIrppa: quality control and robust analysis for mass spectrometry data. Bioinformatics,
    34(3), 522–523. https://doi.org/10.1093/bioinformatics/btx628. Article   CAS   PubMed   Google
    Scholar   Palmblad, M., Lamprecht, A. L., Ison, J., & Schwämmle, V. (2019). Automated
    workflow composition in mass spectrometry-based proteomics. Bioinformatics, 35(4),
    656–664. https://doi.org/10.1093/bioinformatics/bty646. Article   CAS   PubMed   Google
    Scholar   Pang, Z., Chong, J., Zhou, G., de Lima Morais, D. A., Chang, L., Barrette,
    M., Gauthier, C., Jacques, P., Li, S., & Xia, J. (2021). MetaboAnalyst 5.0: narrowing
    the gap between raw spectra and functional insights. Nucleic Acids Research, 49(W1),
    W388–W396. https://doi.org/10.1093/nar/gkab382. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Pluskal, T., Castillo, S., Villar-Briones, A., & Orešič,
    M. (2010). MZmine 2: modular framework for processing, visualizing, and analyzing
    mass spectrometry-based molecular profile data. Bmc Bioinformatics, 11(1), 395.
    https://doi.org/10.1186/1471-2105-11-395. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Protsyuk, I., Melnik, A. V., Nothias, L. F., Rappez, L., Phapale, P.,
    Aksenov, A. A., Bouslimani, A., Ryazanov, S., Dorrestein, P. C., & Alexandrov,
    T. (2018). 3D molecular cartography using LC-MS facilitated by Optimus and ’ili
    software. Nature Protocols, 13(1), 134–154. https://doi.org/10.1038/nprot.2017.122.
    Article   CAS   PubMed   Google Scholar   Rainer, J., Vicini, A., Salzer, L.,
    Stanstrup, J., Badia, J. M., Neumann, S., Stravs, M. A., Hernandes, V., Gatto,
    V., Gibb, L., S., & Witting, M. (2022). A modular and expandable ecosystem for
    Metabolomics Data Annotation in R. Metabolites, 12(2), https://doi.org/10.3390/metabo12020173.
    Article 2. Ram, K. (2013). Git can facilitate greater reproducibility and increased
    transparency in science. Source Code for Biology and Medicine, 8(1), 7. https://doi.org/10.1186/1751-0473-8-7.
    Article   PubMed   PubMed Central   Google Scholar   Referencing and citing content.
    (n.d.). GitHub Docs. Retrieved December 30, from https://ghdocs-prod.azurewebsites.net/en/repositories/archiving-a-github-repository/referencing-and-citing-content
    Review checklist—JOSS documentation. (n.d.). Retrieved April 28, from https://joss.readthedocs.io/en/latest/review_checklist.html
    RforMassSpectrometry. (n.d.). Retrieved January 14, from https://www.rformassspectrometry.org/
    Ridder, L., van der Hooft, J. J. J., Verhoeven, S., de Vos, R. C. H., van Schaik,
    R., & Vervoort, J. (2012). Substructure-based annotation of high-resolution multistage
    MSn spectral trees. Rapid Communications in Mass Spectrometry, 26(20), 2461–2471.
    https://doi.org/10.1002/rcm.6364. Article   CAS   PubMed   Google Scholar   Rocca-Serra,
    P., & Sansone, S. A. (2019). Experiment design driven FAIRification of omics data
    matrices, an exemplar. Scientific Data, 6(1), https://doi.org/10.1038/s41597-019-0286-0.
    Romano, J. D., & Moore, J. H. (2020). Ten simple rules for writing a paper about
    scientific software. PLOS Computational Biology, 16(11), e1008390. https://doi.org/10.1371/journal.pcbi.1008390.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Ross, D. H., Cho, J.
    H., Zhang, R., Hines, K. M., & Xu, L. (2020). LiPydomics: a Python Package for
    Comprehensive Prediction of lipid Collision Cross sections and Retention Times
    and Analysis of Ion Mobility-Mass Spectrometry-Based Lipidomics Data. Analytical
    Chemistry, 92(22), 14967–14975. https://doi.org/10.1021/acs.analchem.0c02560.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Röst, H. L., Sachsenberg,
    T., Aiche, S., Bielow, C., Weisser, H., Aicheler, F., Andreotti, S., Ehrlich,
    H. C., Gutenbrunner, P., Kenar, E., Liang, X., Nahnsen, S., Nilse, L., Pfeuffer,
    J., Rosenberger, G., Rurik, M., Schmitt, U., Veit, J., Walzer, M., & Kohlbacher,
    O. (2016). OpenMS: a flexible open-source software platform for mass spectrometry
    data analysis. Nature Methods, 13(9), 741–748. https://doi.org/10.1038/nmeth.3959.
    Article   CAS   PubMed   Google Scholar   Ruttkies, C., Schymanski, E. L., Wolf,
    S., Hollender, J., & Neumann, S. (2016). MetFrag relaunched: incorporating strategies
    beyond in silico fragmentation. Journal of Cheminformatics, 8(1), 3. https://doi.org/10.1186/s13321-016-0115-9.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Savoi, S., Arapitsas,
    P., Duchêne, É., Nikolantonaki, M., Ontañón, I., Carlin, S., Schwander, F., Gougeon,
    R. D., Ferreira, A. C. S., Theodoridis, G., Töpfer, R., Vrhovsek, U., Adam-Blondon,
    A. F., Pezzotti, M., & Mattivi, F. (2021). Grapevine and wine metabolomics-based
    guidelines for FAIR data and Metadata Management. Metabolites, 11(11), 757. https://doi.org/10.3390/metabo11110757.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Schober, P., Boer,
    C., & Schwarte, L. A. (2018). Correlation coefficients: appropriate use and interpretation.
    Anesthesia & Analgesia, 126(5), 1763–1768. https://doi.org/10.1213/ANE.0000000000002864.
    Article   Google Scholar   Seemann, T. (2013). Ten recommendations for creating
    usable bioinformatics command line software. GigaScience, 2(1), 15. https://doi.org/10.1186/2047-217X-2-15.
    Article   PubMed   PubMed Central   Google Scholar   Senington, R., Pataki, B.,
    & Wang, X. V. (2018). Using docker for factory system software management: experience
    report. Procedia CIRP, 72, 659–664. https://doi.org/10.1016/j.procir.2018.03.173.
    Article   Google Scholar   Shen, X., Wang, R., Xiong, X., Yin, Y., Cai, Y., Ma,
    Z., Liu, N., & Zhu, Z. J. (2019). Metabolic reaction network-based recursive metabolite
    annotation for untargeted metabolomics. Nature Communications, 10(1), https://doi.org/10.1038/s41467-019-09550-x.
    Smith, C. A., Want, E. J., O’Maille, G., Abagyan, R., & Siuzdak, G. (2006). Matching,
    and Identification. Analytical Chemistry, 78(3), 779–787. https://doi.org/10.1021/ac051437y.
    XCMS: Processing Mass Spectrometry Data for Metabolite Profiling Using Nonlinear
    Peak Alignment,. Snyder, M., Mias, G., Stanberry, L., & Kolker, E. (2014). Metadata
    Checklist for the Integrated Personal OMICS Study: Proteomics and Metabolomics
    experiments. OMICS: A Journal of Integrative Biology, 18(1), 81–85. https://doi.org/10.1089/omi.2013.0148.
    Article   CAS   PubMed   Google Scholar   Spicer, R., Salek, R. M., Moreno, P.,
    Cañueto, D., & Steinbeck, C. (2017). Navigating freely-available software tools
    for metabolomics analysis. Metabolomics: Official Journal of the Metabolomic Society,
    13(9), 106. https://doi.org/10.1007/s11306-017-1242-7. Article   CAS   PubMed   Google
    Scholar   Stanstrup, J., Broeckling, C. D., Helmus, R., Hoffmann, N., Mathé, E.,
    Naake, T., Nicolotti, L., Peters, K., Rainer, J., Salek, R. M., Schulze, T., Schymanski,
    E. L., Stravs, M. A., Thévenot, E. A., Treutler, H., Weber, R. J. M., Willighagen,
    E., Witting, M., & Neumann, S. (2019). The metaRbolomics Toolbox in Bioconductor
    and beyond. Metabolites, 9(10), https://doi.org/10.3390/metabo9100200. Article
    10. Sumner, L. W., Amberg, A., Barrett, D., Beale, M. H., Beger, R., Daykin, C.
    A., Fan, T. W. M., Fiehn, O., Goodacre, R., Griffin, J. L., Hankemeier, T., Hardy,
    N., Harnly, J., Higashi, R., Kopka, J., Lane, A. N., Lindon, J. C., Marriott,
    P., Nicholls, A. W., & Viant, M. R. (2007). Proposed minimum reporting standards
    for chemical analysis Chemical Analysis Working Group (CAWG) Metabolomics Standards
    Initiative (MSI). Metabolomics: Official Journal of the Metabolomic Society, 3(3),
    211–221. https://doi.org/10.1007/s11306-007-0082-2. Article   CAS   PubMed   Google
    Scholar   Tautenhahn, R., Patti, G. J., Rinehart, D., & Siuzdak, G. (2012). XCMS
    Online: a web-based platform to process untargeted metabolomic data. Analytical
    Chemistry, 84(11), 5035–5039. https://doi.org/10.1021/ac300698c. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Teo, G., Chew, W. S., Burla, B. J., Herr, D., Tai,
    E. S., Wenk, M. R., Torta, F., & Choi, H. (2020). MRMkit: Automated Data Processing
    for large-scale targeted Metabolomics Analysis. Analytical Chemistry, 92(20),
    13677–13682. https://doi.org/10.1021/acs.analchem.0c03060. Article   CAS   PubMed   Google
    Scholar   Tsugawa, H., Cajka, T., Kind, T., Ma, Y., Higgins, B., Ikeda, K., Kanazawa,
    M., VanderGheynst, J., Fiehn, O., & Arita, M. (2015). MS-DIAL: Data-independent
    MS/MS deconvolution for comprehensive metabolome analysis. Nature Methods, 12(6),
    523–526. https://doi.org/10.1038/nmeth.3393. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Tsugawa, H., Kind, T., Nakabayashi, R., Yukihira, D., Tanaka, W., Cajka,
    T., Saito, K., Fiehn, O., & Arita, M. (2016). Hydrogen rearrangement rules: computational
    MS/MS fragmentation and structure elucidation using MS-FINDER Software. Analytical
    Chemistry, 88(16), 7946–7958. https://doi.org/10.1021/acs.analchem.6b00770. Article   CAS   PubMed   PubMed
    Central   Google Scholar   Uppal, K., Soltow, Q. A., Strobel, F. H., Pittard,
    W. S., Gernert, K. M., Yu, T., & Jones, D. P. (2013). xMSanalyzer: automated pipeline
    for improved feature detection and downstream analysis of large-scale, non-targeted
    metabolomics data. Bmc Bioinformatics, 14(1), 15. https://doi.org/10.1186/1471-2105-14-15.
    Article   PubMed   PubMed Central   Google Scholar   Uppal, K., Walker, D. I.,
    & Jones, D. P. (2017). xMSannotator: an R Package for Network-Based annotation
    of high-resolution Metabolomics Data. Analytical Chemistry, 89(2), 1063–1067.
    https://doi.org/10.1021/acs.analchem.6b01214. Article   CAS   PubMed   PubMed
    Central   Google Scholar   van de Sandt, S., Nielsen, L. H., Ioannidis, A., Muench,
    A., Henneken, E., Accomazzi, A., Bigarella, C., Lopez, J. B. G., & Dallmeier-Tiessen,
    S. (2019). Practice meets Principle: Tracking Software and Data Citations to Zenodo
    DOIs (arXiv:1911.00295). arXiv. https://doi.org/10.48550/arXiv.1911.00295 Vesteghem,
    C., Brøndum, R. F., Sønderkær, M., Sommer, M., Schmitz, A., Bødker, J. S., Dybkær,
    K., El-Galaly, T. C., & Bøgsted, M. (2020). Implementing the FAIR Data Principles
    in precision oncology: review of supporting initiatives. Briefings in Bioinformatics,
    21(3), 936–945. https://doi.org/10.1093/bib/bbz044. Article   CAS   PubMed   Google
    Scholar   Vitale, C. M., Lommen, A., Huber, C., Wagner, K., Garlito Molina, B.,
    Nijssen, R., Price, E. J., Blokland, M., van Tricht, F., Mol, H. G. J., Krauss,
    M., Debrauwer, L., Pardo, O., Leon, N., Klanova, J., & Antignac, J. P. (2022).
    Harmonized Quality Assurance/Quality control provisions for nontargeted measurement
    of urinary pesticide biomarkers in the HBM4EU Multisite SPECIMEn Study. Analytical
    Chemistry, 94(22), 7833–7843. https://doi.org/10.1021/acs.analchem.2c00061. Article   CAS   PubMed   Google
    Scholar   Weber, R. J. M., & Viant, M. R. (2010). MI-Pack: increased confidence
    of metabolite identification in mass spectra by integrating accurate masses and
    metabolic pathways. Chemometrics and Intelligent Laboratory Systems, 104(1), 75–82.
    https://doi.org/10.1016/j.chemolab.2010.04.010. Article   CAS   Google Scholar   Wilkinson,
    M. D., Dumontier, M., Aalbersberg, I. J., Appleton, G., Axton, M., Baak, A., Blomberg,
    N., Boiten, J. W., da Silva Santos, L. B., Bourne, P. E., Bouwman, J., Brookes,
    A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T.,
    Finkers, R., & Mons, B. (2016). The FAIR Guiding Principles for scientific data
    management and stewardship. Scientific Data, 3(1), Article 1. https://doi.org/10.1038/sdata.2016.18
    Wilkinson, M. D., Dumontier, M., Sansone, S. A., Bonino da Silva Santos, L. O.,
    Prieto, M., Batista, D., McQuilton, P., Kuhn, T., Rocca-Serra, P., Crosas, M.,
    & Schultes, E. (2019). Evaluating FAIR maturity through a scalable, automated,
    community-governed framework. Scientific Data, 6(1), https://doi.org/10.1038/s41597-019-0184-5.
    Wolf, M., Logan, J., Mehta, K., Jacobson, D., Cashman, M., Walker, A. M., Eisenhauer,
    G., Widener, P., & Cliff, A. (2021). Reusability First: Toward FAIR Workflows.
    2021 IEEE International Conference on Cluster Computing (CLUSTER), 444–455. https://doi.org/10.1109/Cluster48925.2021.00053
    Yu, T., Park, Y., Johnson, J. M., & Jones, D. P. (2009). ApLCMS—adaptive processing
    of high-resolution LC/MS data. Bioinformatics, 25(15), 1930–1936. https://doi.org/10.1093/bioinformatics/btp291.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Zhang, X., Li, Q.,
    Xu, Z., & Dou, J. (2020). Mass spectrometry-based metabolomics in health and medical
    science: a systematic review. RSC Advances, 10(6), 3092–3104. https://doi.org/10.1039/C9RA08985C.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Zhao, J., Gómez-Pérez,
    J., Belhajjame, K., Klyne, G., García-Cuesta, E., Garrido, A., Hettne, K., Roos,
    M., Roure, D. D., & Goble, C. (2012). Why workflows break—Understanding and combating
    decay in Taverna workflows. 2012 IEEE 8th International Conference on E-Science.
    https://doi.org/10.1109/eScience.2012.6404482 Zheng, C. L., Ratnakar, V., Gil,
    Y., & McWeeney, S. K. (2015). Use of semantic workflows to enhance transparency
    and reproducibility in clinical omics. Genome Medicine, 7(1), 73. https://doi.org/10.1186/s13073-015-0202-y.
    Article   PubMed   PubMed Central   Google Scholar   Zhou, B., Xiao, J. F., Tuli,
    L., & Ressom, H. W. (2012). LC-MS-based metabolomics. Molecular BioSystems, 8(2),
    470–481. https://doi.org/10.1039/c1mb05350g. Article   CAS   PubMed   Google Scholar   Zhou,
    R., Tseng, C. L., Huan, T., & Li, L. (2014). IsoMS: automated processing of LC-MS
    data generated by a chemical isotope labeling metabolomics platform. Analytical
    Chemistry, 86(10), 4675–4679. https://doi.org/10.1021/ac5009089. Article   CAS   PubMed   Google
    Scholar   Download references Acknowledgements The authors thank Biswapriya Misra,
    Ph.D., for his constructive remarks and useful suggestions for the study. The
    authors also sincerely thank Bailey Ballard and Jianming (Jennifer) Wang for their
    help in the process of title-abstract screening. We would like to express a special
    thank to all software authors that took the time out of their busy schedule to
    respond our emails and provide thoughtful feedback regarding the annotation of
    software functions. Funding Research reported in this publication was supported
    by the University of Florida Informatics Institute Fellowship Program. Research
    reported in this publication was also supported by Southeast Center for Integrated
    Metabolomics at the University of Florida, the National Institute of Diabetes
    and Digestive and Kidney Diseases (K01DK115632), the University of Florida Clinical
    and Translational Science Institute (UL1TR001427). The content is solely the responsibility
    of the authors and does not necessarily represent the official views the University
    of Florida Informatics Institute, Southeast Center for Integrated Metabolomics
    at the University of Florida, University of Florida Clinical and Translational
    Science Institute, or the National Institutes of Health. Author information Authors
    and Affiliations Department of Health Outcomes and Biomedical Informatics, University
    of Florida College of Medicine, Gainesville, FL, USA Xinsong Du, Farhad Dastmalchi,
    Matthew A. Diller, Mei Liu, William R. Hogan & Dominick J. Lemas Health Science
    Center Libraries, University of Florida, Florida, USA Hao Ye Department of Pathology,
    Immunology and Laboratory Medicine, College of Medicine, University of Florida,
    Florida, USA Timothy J. Garrett Department of Biomedical Informatics, College
    of Medicine, University of Arkansas for Medical Sciences, Little Rock, USA Mathias
    Brochhausen Department of Obstetrics and Gynecology, University of Florida College
    of Medicine, Florida, Gainesville, United States Dominick J. Lemas Center for
    Perinatal Outcomes Research, University of Florida College of Medicine, Gainesville,
    United States Dominick J. Lemas Contributions XD conceived the idea, designed
    the study, participated in the paper review, participated in software evaluation,
    participated in designing software evaluation criteria and assigning criteria
    to corresponding FAIR4RS categories as a major contributor, drafted the original
    version of the categorization and description of LC-HRMS metabolomics data processing
    steps, performed and programmed for all analysis and visualization, interpreted
    results, prepared the original draft, and revised the manuscript. FD participated
    in the paper review and software evaluation. HY provided literature search terms
    and databases, participated in designing software evaluation criteria and assigned
    criteria to corresponding FAIR4RS categories. TJL provided expertise regarding
    study design, revised the categorization and description of LC-HRMS metabolomics
    data processing steps, and revised the manuscript. MAD participated in designing
    software evaluation criteria and assigning criteria to corresponding FAIR4RS categories.
    ML provided expertise regarding statistical analysis. WRH revised the manuscript.
    MB revised the manuscript. DJL conceived the idea, provided expertise regarding
    study design, provided expertise regarding data analysis and visualization, interpreted
    results, and revised the manuscript. Corresponding author Correspondence to Dominick
    J. Lemas. Ethics declarations Conflict of interest The authors declare that they
    have no competing interests. Additional information Publisher’s Note Springer
    Nature remains neutral with regard to jurisdictional claims in published maps
    and institutional affiliations. Electronic supplementary material Below is the
    link to the electronic supplementary material. Supplementary Material 1 Supplementary
    Material 2 Supplementary Material 3 Supplementary Material 4 Rights and permissions
    Springer Nature or its licensor (e.g. a society or other partner) holds exclusive
    rights to this article under a publishing agreement with the author(s) or other
    rightsholder(s); author self-archiving of the accepted manuscript version of this
    article is solely governed by the terms of such publishing agreement and applicable
    law. Reprints and permissions About this article Cite this article Du, X., Dastmalchi,
    F., Ye, H. et al. Evaluating LC-HRMS metabolomics data processing software using
    FAIR principles for research software. Metabolomics 19, 11 (2023). https://doi.org/10.1007/s11306-023-01974-3
    Download citation Received 08 December 2022 Accepted 20 January 2023 Published
    06 February 2023 DOI https://doi.org/10.1007/s11306-023-01974-3 Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords FAIR principles Metabolomics Research software Liquid chromatography-mass
    spectrometry Open science Reproducibility Use our pre-submission checklist Avoid
    common mistakes on your manuscript. Sections Figures References Abstract Introduction
    Methods Results Discussion Conclusion References Acknowledgements Funding Author
    information Ethics declarations Additional information Electronic supplementary
    material Rights and permissions About this article Advertisement Discover content
    Journals A-Z Books A-Z Publish with us Publish your research Open access publishing
    Products and services Our products Librarians Societies Partners and advertisers
    Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy
    choices/Manage cookies Your US state privacy rights Accessibility statement Terms
    and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic
    Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024
    Springer Nature"'
  inline_citation: '>'
  journal: Metabolomics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Evaluating LC-HRMS metabolomics data processing software using FAIR principles
    for research software
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Berkaoui A.
  - Gahi Y.
  citation_count: '0'
  description: Businesses today recognize the tremendous value of data and seek to
    proactively leverage it to drive growth and success. Therefore, they adopt a data-driven
    approach, collecting, analyzing and using massive volumes of data from multiple
    sources. Data-driven companies need to invest in advanced technologies such as
    data and analytics platforms, to unlock the full potential of their data to gain
    significant competitive advantage.Accordingly, the technology innovations supported
    these business trends over the years by continuously releasing new platforms,
    patterns and architectures capable of scaling and managing large data volumes.
    Apache Hadoop emerges as one of these platforms offering distributed storage and
    processing running on physical commodity hardware and capable of scaling to support
    petabytes of data. However, this kind of platforms when deployed in an enterprise
    datacenter, are usually confronted with the growing usage of virtualization as
    a core component of infrastructure provisioning.This paper addresses infrastructure
    considerations of Hadoop deployment within enterprise's datacenters. It covers
    deployment scenarios on both physical and virtual servers with respect to prerequisites,
    limitations and constraints of each one of them. Furthermore, it suggests a hybrid
    approach with enterprise level capabilities usually required in datacenters. Finally,
    the suggested infrastructure choices are exposed and discussed with perspectives
    and future research areas.
  doi: 10.1109/FiCloud58648.2023.00012
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2023 10th International Confe... High Availabale
    Hadoop Deployment Modes with Enterprise-Level capabilities Publisher: IEEE Cite
    This PDF Abdellah Berkaoui; Youssef Gahi All Authors 16 Full Text Views Abstract
    Document Sections I. Introduction II. Big Data Platforms Landscape III. Hadoop
    as an Enterprise Data Platform IV. Hadoop Deployment On Virtualized Infrastructure
    V. Hadoop Disaster Recovery Scenarios Authors Figures References Keywords Metrics
    Abstract: Businesses today recognize the tremendous value of data and seek to
    proactively leverage it to drive growth and success. Therefore, they adopt a data-driven
    approach, collecting, analyzing and using massive volumes of data from multiple
    sources. Data-driven companies need to invest in advanced technologies such as
    data and analytics platforms, to unlock the full potential of their data to gain
    significant competitive advantage.Accordingly, the technology innovations supported
    these business trends over the years by continuously releasing new platforms,
    patterns and architectures capable of scaling and managing large data volumes.
    Apache Hadoop emerges as one of these platforms offering distributed storage and
    processing running on physical commodity hardware and capable of scaling to support
    petabytes of data. However, this kind of platforms when deployed in an enterprise
    datacenter, are usually confronted with the growing usage of virtualization as
    a core component of infrastructure provisioning.This paper addresses infrastructure
    considerations of Hadoop deployment within enterprise''s datacenters. It covers
    deployment scenarios on both physical and virtual servers with respect to prerequisites,
    limitations and constraints of each one of them. Furthermore, it suggests a hybrid
    approach with enterprise level capabilities usually required in datacenters. Finally,
    the suggested infrastructure choices are exposed and discussed with perspectives
    and future research areas. Published in: 2023 10th International Conference on
    Future Internet of Things and Cloud (FiCloud) Date of Conference: 14-16 August
    2023 Date Added to IEEE Xplore: 29 January 2024 ISBN Information: DOI: 10.1109/FiCloud58648.2023.00012
    Publisher: IEEE Conference Location: Marrakesh, Morocco SECTION I. Introduction
    In recent years, the world witnessed an exponential rise in digital channels through
    which businesses can interact with customers [1]. While companies launched these
    initiatives, they became naturally open to a goldmine that materialized as petabytes
    of data were captured daily through these new channels. Today, tremendous amounts
    of data are constantly generated by governments, organizations, people, and even
    objects [2]. Consequently, many organizations reacted positively to this opportunity
    and initiated their digital transformation journey to avoid missing this strategic
    shift. These amounts of Data are precious from a business perspective, and companies
    are becoming progressively aware of it [3]. Accordingly, data can tell us about
    an enterprise''s processes and activities, drive corporate decisions, closely
    monitor customer behavioral pattems, and thus reveal new exciting business opportunities.
    To make effective use of this data, the IT infrastructure should be capable of
    handling this massive flow throughout its whole lifecycle. Therefore, enterprises
    need to have a Data platform as an enabler for these ambitions, which would be
    positioned as a central repository where data is unified, secured, and governed
    to serve diverse use cases capable of unveihng new opportunities and creating
    business value. Although the use of big data can provide significant benefits,
    it comes with various challenges and constraints that traditional systems often
    can''t handle. A big data platform should be capable of handling vast amounts
    of data while satisfying core requirements, namely low latency, security, cost
    rationalization, and scalability. Therefore, enterprises need to invest in cutting-edge
    technologies to take full advantage of the potential of big data while managing
    the associated requirements and risks. Fortunately, the IT offers evolved over
    the years to develop new paradigms and architectures, notably distributed systems,
    capable of fulfilling these requirements and handling related constraints. One
    of the platforms offering distributed storage and processing capabilities is Apache
    Hadoop. With a rich ecosystem of software components and libraries, Hadoop is
    a good fit for data initiatives within enterprises at an industrial scale [4].
    Therefore, its deployment within enterprise''s data centers regarding their different
    constraints and requirements should be considered a critical success factor to
    any data initiative with Apache Hadoop at its heart. Apache Hadoop''s philosophy
    regarding infrastructure deployment is based on bare metal servers offering low-cost
    and redundant storage on physical commodity servers [5]. In contrast to this configuration,
    infrastructure virtualization imposes itself, on the other hand, as a general
    approach widely used among enterprises for data center design and implementation
    [6]. With hardware virtualization, enterprises can efficiently manage IT resources
    with less technical staff and capabilities such as minimized downtime, faster
    provisioning, and more significant business continuity and disaster recovery response.
    This paper addresses the cohabitation of Apache Hadoop infrastructure deployment
    philosophy and virtualization as a key technology used within data centers. It
    covers the prerequisites of Hadoop deployment on top of virtual servers concerning
    diverse dimensions, namely cluster topology, storage, compute, and network. Additionally,
    this paper suggests a hybrid approach that offers enterprise-level capabilities
    and manages the Hadoop cluster lifecycle while using both virtual enterprise infrastructure
    and bare metal servers. The rest of this paper is organized as follows; Section
    1 presents an overview of big data tools and argues the relevance of the choice
    of Hadoop as an enterprise data platform. Section 2 presents the building blocks
    of Hadoop by highlighting its storage and processing paradigms and infrastructure
    deployment philosophy. Afterward, section 3 covers deployment scenarios with a
    recommendation for a hybrid infrastructure approach that uses Bare Metal servers
    while preserving required enterprise-level deployment capabilities. Finally, potential
    improvements to this work are highlighted as perspectives that future contributions
    could address. After all, we conclude by providing key findings and results of
    this paper. SECTION II. Big Data Platforms Landscape To better use data generated
    daily, enterprises must invest in their IT infrastructure and create a centralized
    data platform that offers storage, management, and compute power, enabling business
    value creation across data use cases. Accordingly, the enterprise data platform
    should be capable of ingesting, storing, and managing massive amounts of data,
    launching large-scale computing and analysis on them, and finally serving data
    products to end users while offering crosscutting services such as security, platform
    management, and governance [7]. Apache Hadoop, with its distributed storage and
    compute and its rich ecosystem of software components, could be a great fit to
    implement these capabilities in an enterprise context. The following section presents
    an overview of the leading big data platforms, highlighting Hadoop''s advantages
    over the available options. A. Big Data platforms overview With the rise of business
    opportunities related to big data, several systems have been designed over the
    years to handle data volume, speed, complexity, and security constraints. Table
    I presents the core capabilities of an enterprise big data platform with a mapping
    of the tools corresponding to each. The tools that are part of the Apache Hadoop
    ecosystem or are easily integrated with it are highlighted in the listings below:
    TABLE I Big Data Platforms Overview According to this overview, enterprises can
    build their data platforms as separate building blocks, each corresponding to
    the core required capabilities. In contrast to this approach, Hadoop presents
    itself as a rich and dynamic ecosystem that comes out of the box with integrated
    components. It is a natural accelerator for setting up such a platform concerning
    time-to-market constraints. In addition, Hadoop offers excellent flexibility and
    allows companies to choose the tools best suited to their specific needs. It can
    also integrate third-party tools for processing, visualization, data integration,
    etc. Moreover, Hadoop is open-source software, which means it is free to use and
    modify according to business needs. It is also supported by an active community
    of developers, ensuring the system is regularly updated and improved. Figure 1
    presents the Hadoop ecosystem as an enterprise data platform for the capabilities
    discussed above: Fig. 1. Hadoop ecosystem capabilities Show All The idea behind
    Apache Hadoop started with two founding publications that appeared respectively
    in 2003 and 2004. The first paper addressed the issue of distributed storage through
    the suggestion of a distributed filesystem, namely the Google File System, which
    provides storage of files across a large cluster of commodity hardware [8]. The
    second founding paper suggested a distributed programming model called MapReduce,
    which offered a parallel, distributed processing algorithm on large datasets [9].
    Therefore, Apache Hadoop was created, evolved over the years, and made several
    improvements to reach its current position as one of the leading enterprise data
    platforms used today [10]. SECTION III. Hadoop as an Enterprise Data Platform
    This section will cover the concepts related to Hadoop storage and processing
    paradigms and their Deployment constraints within enterprise data centers. A.
    Distributed storage: HDFS Hadoop Distributed File System is the storage layer
    of Hadoop. It is a logical file system with the notion of directories containing
    files. Physically, the files are cut into blocks of bytes and distributed over
    the nodes of a cluster [11]. The blocks of each file are replicated to ensure
    redundancy in the event of failure and to facilitate the co-location of processing
    and data. Regarding architecture, HDFS defines two main components: Name nodes
    containing metadata about blocs'' location across the cluster and Data nodes holding
    storage of these blocs. Figure 2 presents HDFS''s logical architecture: Fig. 2.
    HDFS logical architecture Show All B. Distributed processing: MapReduce/Yarn MapReduce
    is a distributed compute algorithm which works in two stages. The first stage,
    called Map, consists of distributing treatments as close as possible to the data
    they use. The second one, Reduce, aggregates the Map stages'' results [12]. During
    the Map step, the algorithm discovers which nodes the input data is stored as
    sets of contiguous HDFS blocks with the host data node for each. The processing
    code is sent to all the host data nodes: each node executes the code in parallel
    on its splits. At the end of the Map stage, we would have as many intermediate
    results as there are splits involved. It remains to combine them to obtain the
    result of the calculation: this is the Reduce step, which produces the result
    files. Figure 3 summarizes the sequence of performed actions in a MapReduce job:
    Fig. 3. MapReduce job actions Show All C. Hadoop deployment constraints The Hadoop
    cluster has a Master-Slave architecture composed of master and slave nodes with
    different responsibilities: Master Nodes: run resiliently, the core services of
    Hadoop such as HDFS Name Node, YARN [13] resource manager, and various other services.
    Slave Nodes: run the storage and compute services such as HDFS Data Node, YARN
    node manager, and temporary containers created by processing jobs. Gateway Nodes:
    run peripheral services that interact with the cluster without constituting its
    core, such as spaces for buffer storage and access channels through protocols
    like FTP and SSH. Admin Nodes: run administration tools that allow operations
    teams to apply configuration changes, restart services, or fix production issues
    To deploy a Hadoop cluster efficiently, the question regarding the underlying
    infrastructure should be carefully addressed to fulfill the requirements of its
    nodes and the services running on top of them. Theoretically, Apache Hadoop was
    designed to run on top of commodity bare metal hardware offering low-cost and
    redundant storage while guaranteeing scalability beyond thousands of servers.
    However, it is easier for many organizations to provide virtual servers as virtualization
    is widely used within data centers [14]. Conversely, many precautions should be
    taken, especially regarding performance, when deploying Hadoop on a virtual infrastructure.
    The nature of Hadoop workloads, mainly based on HDFS and MapReduce, which require
    massive network communications between nodes, CPU intensive compute, and several
    disk I/O solicitations, could be visibly slowed down by the overhead of virtualization
    software used in this kind of infrastructure [15]. In the next section, this paper
    will address the cohabitation of Hadoop and virtualization by giving a deeper
    assessment regarding the prerequisites of Hadoop deployment on top of virtual
    infrastructure according to diverse dimensions, namely cluster topology, storage,
    compute, and network. SECTION IV. Hadoop Deployment On Virtualized Infrastructure
    Before addressing Apache Hadoop cluster deployment on top of a virtualized infrastructure,
    we present the relevant theory and terminology related to virtualization usage
    within enterprise information systems in this part. Infrastructure virtualization
    is a technology that allows the creation of multiple simulated environments or
    dedicated resources from a single physical system. Technically, it relies on software,
    called a hypervisor, directly connected to the hardware, which allows fragmenting
    of a single system into several separate secure and isolated environments called
    virtual machines [16]. In addition to servers and compute as infrastructure building
    blocks, we can distinguish several tiers of virtualization: Storage virtualization:
    Consists of abstracting storage layer by providing centrally hosted storage arrays
    such as Storage Area Network. Network virtualization: Consists of creating virtual
    networks on a single physical web to separate data and simplify network management.
    As Apache Hadoop clusters grow and, consequently, their requirements in terms
    of infrastructure investments and management, the natural question that comes
    to mind is whether they can leverage virtualization benefits. Theoretically, the
    answer is yes, as virtualized infrastructure offers all needs regarding server
    provisioning, compute, storage, and network capabilities. However, there are some
    constraints to consider when using Hadoop in a virtualized environment, especially
    regarding performance and resource consumption. In the next part, we will address
    the prerequisites of running Hadoop on top of virtual infrastructure by managing
    various rules related to cluster topology, compute, storage, and network. For
    the remaining parts of this paper, we will consider using VMware as a virtualization
    platform and Hadoop with Cloudera distribution. A. Cluster topology Cluster topology
    refers to how master and slave nodes and the services running inside them, especially
    name nodes and data nodes, are distributed across virtual machines and their underlying
    physical hosts. Inadequate distribution of roles or collating them on several
    virtual machines running on the same physical host might lead to a loss of cluster
    resilience and consequently cause an inconsistent data state. This distribution
    should be carefully considered while deploying a Hadoop cluster on virtual infrastructure.
    The first requirement is that Name nodes should be deployed on virtual machines
    provisioned on separate physical servers. By applying this rule, we ensure that
    the loss of a physical host will not lead to blocking read and write operations
    on HDFS. Figure 4 illustrates a bad placement of name nodes, on which an eventual
    loss of physical host N ∘ 2 will cause the cluster to lose its quorum majority
    and thus block any HDFS writes: Fig. 4. Bad placement of VMs in a Hadoop cluster
    Show All To achieve this, we must leverage DRS (Distributed Resource Scheduler)
    rules offered by VMware so that there is a strong negative affinity between Name
    Node VMs [17]. This will ensure no master node is provisioned or migrated to the
    same physical host as other master nodes. The second requirement covers resilience
    regarding Data Nodes storage by ensuring that replication of data blocks must
    be done on separate physical servers. The separation of data block replicas to
    VMs on different hosts is achieved through a feature called HVE (Hadoop Virtualization
    Extensions) [17]. By activating HVE for HDFS within a Hadoop cluster, we ensure
    it is fully aware of the topology of the underlying virtual infrastructure and
    thus prevent data loss in case of any physical host or disk failure. Furthermore,
    we should be vigilant about some virtualization features, such as the automated
    migration of virtual machines. For example, in the case ofVMware, VMotion is intended
    to guarantee zero downtime migration of virtual machines from one server to another,
    which offers great flexibility in running workloads. However, this configuration
    could lead to significant data loss if it is applied to virtual machine hosting
    data nodes if they use directly attached disks to physical hosts [17]. B. Storage
    Hadoop was designed to offer distributed storage over data nodes of the cluster.
    It is recommended that each node is backed by DAS (Direct Attached Storage) disks
    holding natively redundant file blocks, thanks to HDFS replication mechanisms.
    However, in the case of virtualized infrastructure, enterprise data centers frequently
    rely on remotely attached storage solutions such as SAN (Storage Area Network),
    in which storage resources are mutualized. At the same time, they appear to be
    logically connected to the VM instances [18]. Several requirements should be considered
    to ensure the cohabitation of these two different storage approaches. The first
    requirement covers the high bandwidth disk access required by HDFS and MapReduce
    workloads. It is generally recommended that 100 Megabytes to 150 Megabytes per
    second be available as I/O bandwidth for each core [19]. Additionally, it is not
    recommended to use RAID (Redundant Array of Independent Disks) configurations
    with Hadoop [19]. RAID disks offer redundancy and protection against data loss
    by implementing mechanisms such as stripping, which is slower for I/O operations
    performance, especially when considering Hadoop replication mechanisms that provide
    similar functionalities. C. Compute Regarding compute virtuahzation, vCPUs can
    share computing resources, such as memory and CPU, between multiple virtual machines
    [20]. This can be useful for optimizing resource usage and reducing costs by allowing
    multiple nodes to run on a single physical hardware. In a Hadoop cluster, compute
    related to MapReduce jobs is usually performed through ephemeral YARN containers
    created using Data Nodes resources. Therefore, to ensure the proper running of
    these jobs, physical CPUs on the vSphere host should not be overloaded. A viable
    approach is to use as many vCPUs configured across all virtual machines on a host
    server as there are physical cores on the host server [19]. In addition, activating
    hyperthreading for the Hadoop cluster is not recommended, as this feature will
    create multiple virtual compute threads for each physical core, which could slow
    down MapReduce jobs performance [19]. Regarding compute sizing, the recommended
    rule is two cores per disk for Data Nodes and one core per disk for the other
    nodes [17]. Regarding memory, we should know that 6% of available physical memory
    is reserved for the vSphere virtualization layer at the host level beyond the
    total memory required by virtual machine instances [19]. Therefore, we should
    avoid over-commitment situations in which the configured VM memory exceeds the
    available physical memory on the physical host [19]. Also, it is recommended to
    have at least 8Go per core and to deactivate memory pagination to have an optimal
    performance by setting vm.swapiness parameter to 0 [17]. D. Network Network performance
    is at the heart of any distributed system, and Hadoop falls perfectly under this
    rule. For example, in a MapReduce job, as highlighted in previous sections, tasks
    are distributed over the cluster, which could lead to massive data transfer between
    nodes, especially in the shuffle and sort steps. This same fact also applies even
    in normal HDFS I/O operations, which result in the transfer and replication of
    file blocks over the network. Therefore, network bandwidth is a crucial factor
    that needs to be carefully considered to guarantee the optimal performance of
    Hadoop jobs and operations. Initially, when designing the network, traffic between
    VMs within the Hadoop cluster should be separated from consumers related to other
    VM instances. There should be no dependency on a single network card or a connection
    to a single physical switch [19]. Consequently, redundant paths for VM-to-VM traffic
    should be planned. Furthermore, enough port density must be provided to accommodate
    the Hadoop cluster nodes, which require ports to create a realistic spine-leaf
    topology providing ISL (Inter-Switch Link) bandwidth [17]. Regarding network bandwidth,
    it is recommended to use dedicated switches for the Hadoop cluster and ensure
    that all physical servers are connected to a TOR(Top ofRack) switch [19]. Also,
    we should use a network with a bandwidth of at least 10 GB per second to connect
    servers running virtualized Hadoop workloads and provide between 200 MB per second
    and 600 MB per second of aggregate network bandwidth per core [19]. E. Hadoop
    Recommended Infrastructure Deployment In Hadoop large-scale deployments, it is
    recommended to use bare metal servers as cluster nodes with DAS storage. The reasons
    behind this recommendation are mainly related to performance and cost factors.
    In fact, bare metal and DAS storage offer superior performance compared to virtualization
    environments and network-attached storage systems in which disk I/O solicitations
    would go over the network instead of directly attaching to the related cluster
    node. Furthermore, bare metal and DAS storage are cheaper than virtualized environments
    with network-attached storage systems [21]. However, bare metal deployment comes
    also with several operational constraints when considering the lifecycle and daily
    operations related to such a platform. For example, the infrastructure provisioning
    timeline is generally longer in bare metal environments than in virtualized ones,
    which could be very restrictive when considering time-to-market constraints. Therefore,
    careful management should be applied regarding the Hadoop cluster''s capacity
    planning, especially given this platform''s continuously growing storage and compute
    requirements. Furthermore, data center managers should be aware of the operational
    burden that comes under their responsibility and thus refresh the skills of their
    operating teams regarding the management of bare metal infrastructures. We will
    make two levels of distinction between production and non-production environments
    and between core and edge nodes in a typical Hadoop deployment. Given the low
    criticality compared to production environments, we suggest a hybrid approach
    in which all non-production environments could be deployed on top of virtual machines.
    We also consider that edge nodes could be deployed on virtual machines, even in
    production environments. Therefore, we suggest a deployment approach that better
    uses both virtualization and physical infrastructure benefits. In addition to
    this deployment, the Hadoop platform should fulfill business continuity and disaster
    recovery requirements along with RTO (Recovery Time Objective) and RPO (Recovery
    Point Objective) fixed by the business owners according to their functional requirements.
    SECTION V. Hadoop Disaster Recovery Scenarios The hybrid deployment approach suggested
    in the previous section allows us to use the benefits of virtualization technology
    better while guaranteeing optimal performance for production environments. However,
    this deployment must be completed by some enterprise-level capabilities required
    for such a critical platform, especially business continuity and disaster recovery
    aspects. In this section, we cover three scenarios of disaster recovery policies
    while highlighting each of their strengths, weaknesses, and constraints. A. Mirroring
    cluster In this scenario, we have an active Hadoop cluster responsible for data
    ingestion from different sources and exposition to other consumers. In addition
    to the active cluster, a passive cluster is also provisioned in the DR data center
    according to a passive-active configuration. These two clusters are synchronized
    through batch scripts responsible for HDFS data blocks replication. For example,
    DistCp (Distributed Copy) [22] command or even proprietary tools related to specific
    distributions, such as Cloudera Replication Manager, could be used to achieve
    this replication between the two clusters. Figure 5 illustrates an architecture
    schema of the mirroring cluster scenario: Fig. 5. Cluster mirroring scenario architecture
    Show All With the mirroring cluster scenario, the RPO objective is highly tied
    to the frequency of the replication jobs. If the replication jobs are done daily,
    we would have, at most, a data loss of one day. In addition, this scenario offers
    flexibility and more control over replication mechanisms. However, it requires
    well-designed failover and fallback procedures and processes to smoothly switch
    between the two clusters in case of hardware or software failure. To illustrate
    the implementation of this scenario, we present an algorithmic approach based
    on a replication policy materialized as a configuration file consumed by DistCp
    replication scripts. The configuration file shows a list of HDFS directories necessary
    to rebuild the secondary cluster state and should be integrated into the replication
    jobs. In addition to HDFS raw data, the replication jobs should implement additional
    steps necessary to align the two clusters'' states. For example, platform configuration,
    metadata, and access policies are crucial elements that should be replicated to
    ensure the proper running of the standby cluster. Fortunately, the corresponding
    components of the Hadoop ecosystem offer API endpoints that could be used to import
    elements, such as Apache Ranger access policies or Apache Atlas metadata, and
    therefore align the two clusters properly. Figure 6 presents the suggested algorithmic
    approach for the cluster mirroring scenario: Fig. 6. Mirroring cluster implementation
    example Show All B. Stretched cluster In the stretched cluster scenario, we need
    to provision one cluster spread over three distant data centers to provide redundancy
    and fault tolerance. The cluster components must be spread over the data centers
    for strong negative affinity rules. These rules should be applied to guarantee
    that in the event of hardware failure or any disaster in the first data center,
    the second one can have the quorum majority and thus run generally without any
    business interruption. In addition, the enterprise should have a third data center
    as a witness in this scenario. This site should be configured to be independent
    of the other sites in the cluster and to be able to operate independently and
    ensure failover decisions in the event of an outage or disaster at the main sites
    of the cluster. Figure 7 illustrates an architecture schema of the stretched cluster
    scenario: Fig. 7. Stretched cluster scenario architecture Show All Even though
    the stretched cluster allows an enterprise to implement a DR policy for its Hadoop
    cluster, this scenario comes with numerous constraints relative to its implementation.
    On the one hand, this scenario requires significant investment in three different
    data centers, which may be hard to justify for some enterprises that don''t need
    very high fault tolerance for their data platform. On the other hand, this scenario
    requires a high network bandwidth between the sites that 10 Gbps which could be
    very restrictive for different workloads. C. Dual ingest cluster Regarding the
    dual ingest scenario, we must provide two clusters in an active-active configuration
    of different data centers. Both clusters are responsible for data capture, processing,
    and exposition to other consumers. Therefore, they need to be continuously monitored
    by operations teams to ensure that they are running correctly since any downtime
    or failure in one cluster could easily lead to data integrity issues. In addition
    to the technical monitoring of the two clusters and the services running on top
    of them, we need to provide custom scripts that perform data integrity verifications
    of HDFS blocks and alert in case of any desynchronization between the two instances.
    Figure 8 illustrates an architecture schema of the dual ingest scenario Fig. 8.
    Dual ingest scenario architecture Show All The dual ingest scenario presents many
    advantages, such as a very low RPO, as the two clusters are supposed to have the
    same image regarding data and running workloads. Furthermore, as both clusters
    would be active, it is easier to implement failover and failback mechanisms in
    case of any failure or disaster. However, this scenario comes with numerous constraints
    related, for instance, to the considerable investment required to set up two clusters
    with the same configuration and maintain them in the same way across their lifecycle.
    Also, the scenario requires setting up the dual ingest mechanisms for both batch
    and real-time data with a cautious consideration to avoid data drift between the
    two clusters. D. Disaster recovery scenarios summary Although the three presented
    DR recovery scenarios for Hadoop could be implemented, their feasibility is tightly
    dependent on business requirements in terms of RPO and RTO objectives as well
    as the cost that the enterprise could afford according to the criticality of their
    data platform. The stretched cluster and the dual ingest could be very restrictive
    given that the first requires considerable investments in data centers and network
    bandwidth between them and that the second presents a high risk of data drift.
    Therefore, the screen mirroring scenario seems to be the most rational, even though
    it requires custom data replication and clear failback and failover procedures
    between the two sites. To conclude, Table II gives a summary of the three scenarios:
    TABLE II Hadoop Disaster Recovery Scenarios Conclusion and Future Work In this
    paper, we addressed Apache Hadoop infrastructure deployment within enterprise
    data centers with respect to their constraints and requirements regarding scalability
    and resilience. As virtualization is widely used today, given its flexibility,
    the main question that this paper tries to cover is whether Hadoop can be deployed
    on top of virtual machines. Even though its deployment on virtualized infrastructure
    is theoretically a viable option, Hadoop has its resilience features and was initially
    designed to scale over a cluster of commodity hardware. Therefore, we covered
    in this paper its deployment on physical infrastructure while preserving enterprise-level
    business continuity and disaster recovery capabilities. As a technology stack,
    we started with VMware as a virtualization technology and Cloudera with Hadoop
    3.x as a target distribution. Regarding Hadoop deployment on virtual machines,
    we followed an approach driven by the different virtualization tiers, namely virtual
    machines deployment, storage, compute, and network. For each of these tiers, we
    covered the guidelines and restrictions that need to be followed to properly function
    Hadoop with minimized risk on the stored data and the running workloads. Hence,
    the requirements presented by this paper were issued from other research papers
    and recommendations gathered from Cloudera and VMware official documentation.
    This paper also recommended an infrastructure deployment using a hybrid approach
    in which non-Production environments and edge nodes are deployed on virtual machines.
    In contrast, the core production nodes run on physical servers. Additionally,
    we covered three disaster recovery scenarios that could apply to the Hadoop cluster.
    The covered Dr scenarios are the screen mirroring, the stretched cluster, and
    the dual ingest. For each of these scenarios, we presented strengths and weaknesses
    and concluded with a recommendation for the first one regarding cost and data
    integrity factors. As a potential area of work for this contribution, we suggest
    to experiment the screen mirroring deployment, and especially support the replication
    algorithms with an empiric approach. Moreover, we suggest to cover the growing
    trend towards containerized and cloud environments [23][24] given the advantages
    they offer in terms of infrastructure flexibility and scalability. This future
    contribution would shed light on the cohabitation of Apache Hadoop with these
    environments while leveraging the best benefits of both Apache Hadoop and containerized
    environments. Authors Figures References Keywords Metrics More Like This Virtual
    Fog: A Virtualization Enabled Fog Computing Framework for Internet of Things IEEE
    Internet of Things Journal Published: 2018 Strategies in Smart Service Systems
    Enabled Multi-sided Markets: Business Models for the Internet of Things 2015 48th
    Hawaii International Conference on System Sciences Published: 2015 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings - 2023 International Conference on Future Internet of Things
    and Cloud, FiCloud 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: High Availabale Hadoop Deployment Modes with Enterprise-Level capabilities
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Golis T.
  - Dakić P.
  - Vranić V.
  citation_count: '0'
  description: The rapid growth and widespread adoption of containerization technologies,
    such as Docker, and the increasing popularity of Kubernetes as a container orchestration
    platform have significantly shaped the landscape of modern software development
    and deployment. Containers have created a paradigm shift in application packaging,
    offering a consistent and portable environment across diverse infrastructure setups.
    Meanwhile, Kubernetes has emerged as the dominant choice for managing containerized
    applications at scale, providing features such as automated scaling, load balancing,
    and fault tolerance. However, despite the numerous advantages containers and Kubernetes
    offer, deploying applications to a Kubernetes cluster often poses challenges due
    to the steep learning curve and complex configuration requirements. To address
    these challenges, this paper aims to introduce a comprehensive learning tool that
    automates and simplifies the deployment process on Kubernetes clusters.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: CEUR Workshop Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Automatic Deployment to Kubernetes Cluster by Applying a New Learning Tool
    and Learning Processes
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bartlett L.
  - Kabir M.A.
  - Han J.
  citation_count: '0'
  description: Business Process Management (BPM) has emerged as a fundamental aspect
    of modern business, revolutionizing task execution and operational efficiency.
    This study explored the integration of BPM, virtualization, and work design to
    enhance organizational performance and productivity. The objective is to contribute
    to the ongoing dialogue on the combined impact of these elements on BPM systems
    and their applicability in contemporary office settings. Through a systematic
    literature review, 136 journal articles were examined and selected from 2,248
    articles matching the search criteria. This review reveals major gaps in the current
    literature and identifies opportunities for further research and investigation.
    These findings underscore the potential significance of integrating virtualization
    and work design in BPM systems to enhance flexibility, scalability, and agility.
    Organizations can effectively respond to dynamic business needs and market conditions
    by leveraging virtual resources, thus eliminating the constraints of physical
    proximity. We provide a reflective discussion linking theoretical understanding
    with empirical evidence from literature. Our analysis revealed promising avenues
    for future research, emphasizing the role of usability in BPM system design and
    its impact on task accomplishment. This systematic literature review underscores
    the role of virtualization and work design in BPM system design. We found that
    both components not only enhanced the performance and effectiveness of BPM systems,
    but also improved flexibility, scalability, and user experience. A holistic approach
    to BPM system design has emerged as crucial, encompassing process modelling, automation,
    workflow management, integration, analytics, reporting, governance, and continuous
    improvement. Despite the evident benefits, our review identified distinct challenges
    such as managing system complexity, ensuring security, navigating resistance to
    change, and harmonizing technology with human elements. Our analysis underscores
    avenues for research that have not yet been thoroughly explored and opportunities
    to further extend knowledge in this field.
  doi: 10.1109/ACCESS.2023.3323445
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 11 A Review
    on Business Process Management System Design: The Role of Virtualization and Work
    Design Publisher: IEEE Cite This PDF Luke Bartlett; Muhammad Ashad Kabir; Jun
    Han All Authors 821 Full Text Views Open Access Comment(s) Under a Creative Commons
    License Abstract Document Sections I. Introduction II. Methodology III. Findings
    IV. Discussion and Future Research V. Conclusion Authors Figures References Keywords
    Metrics Abstract: Business Process Management (BPM) has emerged as a fundamental
    aspect of modern business, revolutionizing task execution and operational efficiency.
    This study explored the integration of BPM, virtualization, and work design to
    enhance organizational performance and productivity. The objective is to contribute
    to the ongoing dialogue on the combined impact of these elements on BPM systems
    and their applicability in contemporary office settings. Through a systematic
    literature review, 136 journal articles were examined and selected from 2,248
    articles matching the search criteria. This review reveals major gaps in the current
    literature and identifies opportunities for further research and investigation.
    These findings underscore the potential significance of integrating virtualization
    and work design in BPM systems to enhance flexibility, scalability, and agility.
    Organizations can effectively respond to dynamic business needs and market conditions
    by leveraging virtual resources, thus eliminating the constraints of physical
    proximity. We provide a reflective discussion linking theoretical understanding
    with empirical evidence from literature. Our analysis revealed promising avenues
    for future research, emphasizing the role of usability in BPM system design and
    its impact on task accomplishment. This systematic literature review underscores
    the role of virtualization and work design in BPM system design. We found that
    both components not only enhanced the performance and effectiveness of BPM systems,
    but also improved flexibility, scalability, and user experience. A holistic approach
    to BPM system design has emerged as crucial, encompassing process modelling, automation,
    workflow management, integration, analytics, reporting, governance, and continuous
    improvement. Despite the evident benefits, our review identified distinct challenges
    such as managing system complexity, ensuring security, navigating resistance to
    change, and harmonizing technology with human elemen... (Show More) This image
    shows the PRISMA flow chart demonstrating the article selection process used during
    this systematic literature review. Published in: IEEE Access ( Volume: 11) Page(s):
    116786 - 116819 Date of Publication: 10 October 2023 Electronic ISSN: 2169-3536
    DOI: 10.1109/ACCESS.2023.3323445 Publisher: IEEE SECTION I. Introduction Business
    Process Management (BPM) has emerged as a fundamental aspect of business, transforming
    how tasks are streamlined and executed [1]. Researchers, remote designers, and
    logistics personnel have found notable success in cooperative work structures,
    often supported by basic tools, such as calendars, emails, or business social
    networks [2]. However, the rapid advancement of technology is driving the transition
    from these traditional platforms to more robust BPM software that offers features
    such as workflow chains, deadline tracking, and audit capabilities. Operational
    efficiency and work quality, which are the prime determinants of business success,
    are significantly enhanced by a well-implemented BPM system [3]. It streamlines
    processes, reduces errors, and mitigates wastage. Furthermore, BPM has been recognized
    for fostering regulatory compliance, supporting the digital revolution, promoting
    agility, and enabling improved delivery of goods and services to consumers [4].
    Thus, businesses may gain competitive advantage by implementing customer-centric
    software interfaces. Data management, which is the bedrock of many business operations,
    has also been greatly affected by digital advancements [5]. Efficient organization
    and management of voluminous data is now achievable, with larger companies often
    leveraging SQL Server databases and smaller entities using applications such as
    Microsoft Access [3]. These tools are essential for companies of all sizes because
    they facilitate data collection and storage for future decision-making purposes.
    For instance, databases enable businesses to operate e-commerce websites and manage
    user-login information. Such technological infrastructure, which is an integral
    component of BPM, can significantly transform a company’s operations. The pivotal
    role of technology in business has been increasingly evident since the technological
    revolution, which has had an even more profound impact than the preceding industrial
    revolution [3]. Almost every aspect of business operations, from manufacturing
    and finance to security and human resources, has been reshaped significantly by
    technological advancements. Computing technology has long served as a crucial
    productivity tool for businesses, with advancements continuously altering the
    work processes to enhance productivity [6]. For instance, the transition from
    typewriters to word processors necessitated new skills and workflows but resulted
    in substantial productivity gains [7]. This symbiotic relationship between work
    design and productivity improvement was particularly evident in the BPM domain
    [8]. With the advent of BPM systems, businesses can automate many manual tasks,
    improving workflow organization, integration, and optimization [9]. This transformation
    allows business processes to be driven by the BPM system rather than solely by
    human operators. Virtualization, which involves dividing resources into multiple
    execution environments, has also gained prominence in the business world [10].
    Combined with the BPMS, it provides the resources and instructions necessary to
    complete the workflows. Advances in virtual reality (VR), mixed reality (MR),
    and collaborative virtual environments (CVEs) offer new ways of viewing and interacting
    with the physical environment [11]. Figure 1 offers a visual representation of
    these foundational elements of the data-management process and their interplay
    in a typical business setting. FIGURE 1. Foundations of data management process.
    Show All Modern BPM system design, deeply rooted in computer and management science
    [12], significantly impacts daily work, leading to considerations of work design
    as a function of business process automation. The usability of a system also plays
    a vital role in ensuring effective and efficient task completion [13]. The concept
    of usability also has relevance in the context of BPM, with the potential to positively
    influence task accomplishment [13]. The amalgamation of BPM, virtual resources,
    and work design raises pertinent questions regarding their combined impact on
    BPM systems and applicability in contemporary office settings. While some systematic
    literature reviews related to business process management exist [14], [15], [16],
    these reviews have focused on other issues such as sustainability, the state of
    digital transformation in business processes, and business management processes
    in specific industries. Reference [14] conducted a systematic review of digital
    transformation in business process management, focusing on the current state of
    digital transformation to inform the basis of transforming business processes.
    However, this study emphasized Big Data, the Internet of Things, and Blockchain
    technologies, leaving out aspects of virtualization and impacts on modern business
    environments. Reference [16] focused on integrating sustainability into business
    process management by exploring the design and evaluation of visualization to
    encourage sustainable practices. Another systematic review [15] focused on business
    process management systems in port processes, exploring the analysis, modelling,
    implementation, execution, control, and optimization of business processes in
    port environments. In contrast to these existing reviews, this review investigates
    the incorporation of system design and visualization in business process management
    across different industries, clearly outlining how beneficial systems design and
    visualization are to improve business process management. The objective of this
    paper is to critically examine the current body of knowledge concerning the integration
    of BPM, virtualization, and work design, identify gaps and limitations in the
    existing literature, and address pertinent research questions, such as how these
    elements interrelate and collectively impact BPM system design. Our analysis focuses
    on the benefits and challenges of combining these three elements within BPM systems,
    which may influence a system’s adaptability, usability, and response to business
    needs and market changes. A total of 129 journal articles were reviewed from a
    larger pool of 2,248 articles that matched the search criteria. This review identified
    major gaps in the current literature in relation to BPM, virtualization, and work
    design. The findings show that there is room for better understanding of the relationship
    between BPM, virtualization, and work design. In our discussion, we highlight
    the impact of usability on the BPM system design as a promising area for further
    research. The goal is to highlight areas for further exploration and provide valuable
    insights for researchers and academics. The remainder of this paper is organized
    as follows. Section II outlines the methodologies used to conduct this systematic
    literature review. Section III discusses our findings and key themes in detail.
    Section IV provides a reflective discussion linking theoretical understanding
    with empirical evidence obtained from the literature, identifies literature gaps,
    and proposes directions for future research. Finally, Section V summarizes the
    key findings and their implications for the field, and reinforces the future directions
    suggested in the discussion. SECTION II. Methodology The current research employed
    a systematic review of the relevant literature to enhance the understanding of
    virtualization and work design concepts in the context of BPM system design. This
    encompasses an examination of the benefits and limitations associated with the
    integration of these concepts. The research additionally examined the correlation
    between virtualization and work design, and their influence on the design of business
    process management systems. The decision to conduct a systematic review instead
    of a meta-analysis or narrative review is based on two primary justifications.
    First, it was intended to ensure methodological rigor in mapping the extant academic
    literature on virtualization and work design in the context of BPM system design.
    Second, as portrayed in empirical studies, this study aimed to establish a basis
    for clarifying the association between virtualization and work design, and their
    effects on BPM system design [4]. This systematic literature review follows a
    fundamental approach to establish an evidence base by comprehensively identifying,
    assessing, and interpreting all pertinent research related to a specific research
    inquiry, subject matter, or phenomenon of significance [17]. This systematic review
    adheres to explicit procedures and principles to ensure transparency and reproducibility
    [17] and to the preferred reporting items for systematic reviews and meta-analyses
    (PRISMA) [18]. A. Research Process The systematic review procedure was divided
    into three distinct stages to ensure its successful and efficient implementation.
    These stages encompass the process of planning, executing the review, and finally
    reporting the findings of the review [18]. During the first phase of the study,
    a set of research questions was developed that were deemed crucial in achieving
    the objective of the study. Consequently, these questions were used to identify
    the most suitable search terms and keywords, and the databases that would subsequently
    be utilized for conducting the literature search. The Google Scholar, Scopus,
    and Science Direct databases were used in the search process to identify relevant
    studies for this review. The review entailed identifying relevant studies from
    these databases using the selection criteria. The data obtained from studies that
    met the inclusion criteria were subsequently extracted for analysis. Finally,
    an in-depth analysis of the results was performed to address the research questions
    in the current study. The findings were subsequently organized and presented based
    on themes, followed by a brief discussion and suggestions for future research.
    B. Research Questions and Search Strategy To facilitate the search for eligible
    articles, it was essential to identify and define the research questions that
    this systematic review would focus on. As a result, the present study formulated
    the following questions to facilitate attaining the systematic review objective.
    How does the relationship between BPM, virtual resources, and work design impact
    the practical application of BPM systems in the modern office environment? What
    are virtualization and work design roles in BPM system design, including the advantages
    and limitations of incorporating these concepts? These research questions aim
    to determine the influence of virtualization and work design on business process
    management (BPM) and their consequential effects on these systems. Additionally,
    the review aimed to identify areas of research that have yet to be explored to
    inform future advancements in business process management and encourage additional
    empirical investigations in this area. This study aims to consolidate the fragmented
    literature on virtualization and work design in Business Process Management (BPM)
    by gathering a comprehensive collection of articles that address the topic. The
    review adhered to established management practices by exclusively incorporating
    peer-reviewed scholarly journals into the analysis [19]. The search strategy encompassed
    a comprehensive exploration of pertinent scholarly databases, which included,
    but were not restricted to, the online platforms of educational establishments
    that host peer-reviewed literature. The search was be executed utilizing a blend
    of key terms encompassing “Business Process Management System Design,” “virtualization,”
    and “work design.” The retrieved outcomes were screened for relevance to the research
    subject matter and the inclusion criteria. Table 1 provides a detailed description
    of the five distinct search stages and terms used in this process. TABLE 1 Search
    Terms C. Selection Criteria This systematic literature review aimed to present
    pertinent data, conduct comparisons, and perform statistical analyses. Therefore,
    only pertinent information was considered. The present investigation utilized
    precisely defined eligibility criteria to identify the pertinent literature. The
    present study used scholarly articles from the Google Scholar, Scopus, and Science
    Direct databases, representing a large subset of available databases to be meaningful.
    The inclusion criteria for articles were limited to those published in English
    between 2000 and 2023. The study will employ inclusion criteria based on the occurrence
    of the terms “Business Process Management System Design,” “virtualization,” and
    “work design” in the title, abstract, or keywords of the article. The exclusion
    criteria will encompass non-inclusion of the terms above in the title, abstract,
    or keywords, coupled with the presence of extraneous information. The selection
    process for peer-reviewed articles involved the assessment of their adherence
    to the inclusion criteria. Articles that satisfied these criteria were evaluated
    based on the presence of an abstract, a minimum length of four pages, and a focus
    on the impact of virtualization and work design on business process management
    systems. No papers that discussed the topic areas investigated in this review
    but did not concentrate on the same outcomes were considered [18]. The flow chart
    in the figure below shows how the articles were selected for analysis in this
    systematic literature review. D. Data Extraction and Synthesis The objective of
    the literature review was to investigate and amalgamate prior research on the
    subject matter, ascertain deficiencies in knowledge, and formulate suggestions
    for prospective research. Furthermore, this research aims to offer a thorough
    and evaluative examination of the extant literature concerning the function of
    virtualization and work design in advancing a business process management (BPM)
    system. The study used a methodological literature search and analysis approach,
    as outlined in Figure 2. The chosen documents were assessed using thematic analysis.
    Subsequently, the analysis phase identified and categorized the principal themes,
    concepts, and theories derived from relevant literature [19]. This involved inferring
    the relevance of the derived concepts to the research questions and objectives.
    This literature review aims to critically analyze current research on incorporating
    virtualization and work design in developing Business Process Management Systems
    (BPMSs). This research endeavor aims to identify the principal themes, concepts,
    and theories that arise from existing literature, which will serve as a foundation
    for constructing a conceptual framework. FIGURE 2. PRISMA flow chart demonstrating
    the article selection process. Show All The criteria for determining eligible
    publications were established according to the review question. Consulting scholarly
    articles on systematic literature reviews facilitated the identification of the
    procedural steps involved in conducting systematic reviews [18]. The present investigation
    involved multiple phases, and the delineation of each of the research stages facilitated
    the acquisition and analysis of data. The procedures were partitioned into distinct
    categories: information sources, search methodology, selection procedure, data-gathering
    procedure, and data analysis. E. Risk of Biasness Several potential sources of
    bias were identified in this study. The search for studies in scientific repositories
    was subject to bias owing to the idiosyncratic nature of each repository, with
    some databases not permitting the application of pre-established criteria. Additionally,
    there is a potential risk of receiving documentation outside the scope of Business
    Process Management. Furthermore, there was an uneven distribution of documents
    across years and scientific repositories. The data selection process exhibited
    bias after the application of the established criteria. Exclusion criteria were
    applied to ensure that only publications meeting the minimum page requirements
    were included in the review, thereby maintaining a high level of quality. The
    use of diverse criteria resulted in the exclusion of several publications. Had
    these publications been incorporated, it could have facilitated a more equitable
    distribution and reduced the likelihood of bias among all items under investigation
    [19]. Furthermore, this review identified bias in data reporting. Nevertheless,
    utilizing a unified methodology in research is crucial in mitigating this potential
    hazard. Potential bias during the research process, including data collection
    and publication selection, was assessed. The pertinent aspects of the methodology
    were discussed, and various techniques were revised until they could be integrated
    effectively. The potential for bias was mitigated, as the data were collected
    across nearly all categories of the study [17]. Furthermore, the established eligibility
    criteria ensured high rigour and certainty in the collected data. The criteria
    and methodologies were revised as required to establish a cohesive approach for
    implementation. The consistent approach employed in the review process facilitated
    the acquisition of the most pertinent data, thus mitigating the potential for
    bias. An iterative review and data analysis were required to ensure the consistency
    and reliability of the information obtained. The present investigation employed
    rigorous systematic research and data collection techniques, and thoroughly examined
    and evaluated all relevant publications. Persistent practice of conducting methodological
    examinations and evaluating the acquired data was consistently ensured. The analyses
    aimed to minimize existing biases. Examining reference publications in literary
    reviews facilitated the comprehension of the appropriate methods for organizing,
    processing, and presenting acquired data and information [19]. SECTION III. Findings
    A. BPM System Design BPM has become essential for modern organizations to improve
    efficiency and optimize their workflows. The framework of BPM system design involves
    several key components and considerations that are critical for designing a successful
    BPM system [20]. The first step in the framework of BPM system design is process
    modelling. The process involves identifying, defining, and modelling business
    processes using process modelling notations, such as BPMN. The goal clearly and
    precisely represents each process including its inputs, outputs, activities, and
    decision points. By defining processes in this way, organizations can understand
    how their operations work and identify areas for improvement. The next step is
    the process automation. Another study established that this process entails automating
    the methods defined in the previous step [21]. This requires the selection of
    an appropriate BPM software tool that enables process execution, monitoring, and
    management [1]. Process automation can significantly improve efficiency by eliminating
    manual tasks, reducing errors, and enabling processes to be executed quickly and
    accurately. Workflow management is another critical component of BPM system design
    [22]. This step involves assigning user tasks, tracking progress, and routing
    between different process steps. Workflow management allows organizations to coordinate
    work across teams and ensure that processes are executed promptly and efficiently
    [22]. Integration with other systems is also crucial for the BPM System Design.
    Organizations must integrate their BPM systems with other systems to enable data
    exchange, communication, and coordination between systems. This could include
    integration with legacy systems, enterprise resource planning (ERP) systems, or
    third-party applications [6]. Integration enables data to flow between systems,
    ensuring that all procedures can access the information they need to operate efficiently.
    Analytics and reporting are critical components of BPM system design. Organizations
    must monitor process performance and identify areas for improvement [23]. BPM
    systems provide analytics and reporting capabilities that allow process owners
    to monitor process performance, identify bottlenecks, and make data-driven decisions
    for process improvements. Governance and compliance are critical components of
    BPM system design. BPM systems must provide features for governance and compliance,
    including security and access control, audit trials, and regulatory compliance
    checks [24]. In addition, organizations can avoid legal and financial risks by
    promptly executing processes. Finally, the design of the BPM system should support
    continuous improvements. It involves providing tools for process analysis, simulation,
    and optimization [25]. These tools enable organizations to identify improvement
    areas and simulate the impact of process changes before implementation [26]. Generally,
    the framework of BPM system design provides a structured approach for designing
    and implementing BPM systems. By following this framework, the study noted that
    public administrators and organizations could improve process efficiency, agility,
    and overall business performance [26]. Process modelling, process automation,
    workflow management, integration with other systems, analytics and reporting,
    governance and compliance, and continuous process improvement are critical components
    of BPM system design. Consequently, organizations adopting a BPM system can improve
    their operations, reduce costs, and increase customer satisfaction [27]. With
    the rapid pace of technological advancement and changing customer expectations,
    the importance of the BPM system design will continue to grow [24]. Hence, organizations
    that invest in designing and implementing effective BPM systems are better positioned
    to compete and thrive in the modern business landscape. 1) Development of BPM
    BPM has been developed as a catalyst for optimizing activities and operations
    at the workplace by providing a systematic approach to identifying, analyzing,
    designing, implementing, monitoring, and controlling business processes. BPM started
    as a quality management initiative in the 1980s with the introduction of Total
    Quality Management (TQM), Six Sigma, and Continuous Improvement (CI) methodologies
    [28]. These approaches emphasize the importance of process improvement in improving
    business performance. The study noted that in the 1990s, a new generation of BPM
    tools emerged that combined workflow management capabilities with process modelling,
    simulation, and analysis tools. These tools enable organizations to visualize
    their business processes and identify opportunities for improvement. A range of
    factors have driven the development of BPM [29]. These factors include the increasing
    complexity of business operations and need for greater efficiency and cost-effectiveness.
    In addition, these factors rely on growing demand for agility and flexibility
    in response to changing market conditions [30]. Consequently, BPM provides a systematic
    framework for identifying and addressing bottlenecks, inefficiencies, and other
    issues that affect business performance. In a changing business environment with
    dynamic demands, BPM must be flexible for an effective response [26]. Linking
    the BPM theory to the business process by assessing non-traditional methods in
    work design allows for its practical application [31]. However, the definition
    of business management and Information Technology (IT) terminology often needs
    to be more broadly agreed upon within academic or corporate spheres. Hence, explaining
    various concepts, such as BPM, becomes challenging. BPM entails the methods, tools,
    technologies, management, analysis, and automation of processes by entities to
    achieve competitive and strategic advantages [32]. Thus, BPM adoption is integral
    to upgrading a firm’s business operations, and its successful execution is vital.
    The process of implementing BPM systems and practices within an organization involves
    a systematic approach to identifying, designing, executing, monitoring, and optimizing
    business processes to achieve business objectives [33]. The study noted that BPM
    adoption is a critical component of digital transformation for organizations,
    enabling them to streamline operations, reduce costs, and increase customer satisfaction.
    Technological advancements have played a significant role in the development of
    the BPM [34]. The availability of process modelling tools, workflow engines, and
    integration platforms provided organizations with the necessary infrastructure
    to manage their processes more efficiently and automatically [35]. Additionally,
    the growing demand for organizational agility and responsiveness to changing customer
    needs and market conditions has led to the adoption of BPM. A management framework
    is a critical enabler of strategic alignment and operational excellence [36].
    It gives organizations a holistic view of their processes, allowing them to identify
    improvement opportunities and make data-driven decisions [37]. Generally, BPM
    is developed as a catalyst for optimizing activities and operations in the workplace.
    It achieves this by providing organizations with a structured and systematic approach
    to process improvement [36]. The process is supported by technology and is driven
    by the need for agility and customer centricity. The following section discusses
    the modern BPM systems. 2) Modern BPM Systems BPM has undergone significant changes
    in evolution from traditional systems to modern BPM system designs. Conventional
    BPM systems, such as those used by Romanian industrial service companies, focus
    on optimizing processes and using technology to manage and streamline workflow
    [38]. These systems are primarily aimed at increasing efficiency and reducing
    costs. However, with the advancement of technology, the need for modern BPM system
    design has emerged [34]. Modern BPM systems have evolved to incorporate additional
    features and capabilities beyond traditional workflow management. These systems
    are designed to help organizations become more agile, flexible, and responsive
    to environmental changes. Technological advances and changes in entities’ economic,
    social, and political environments have impacted BPM practices. Traditional BPM
    systems focus on process optimization and automation, whereas modern systems focus
    on digital transformation. Modern BPM systems aim to leverage the latest digital
    technology. These include Artificial Intelligence (AI), Robotic Process Automation
    (RPA), and Machine Learning (ML). These technologies enable entities to leverage
    their capabilities, drive innovation, enhance customer experiences, and streamline
    operations [39]. Hence, modern BPM systems are customer centric. They aim to create
    a seamless experience for customers and provide personalized services. Modern
    BPM systems are designed to facilitate customer interaction across multiple channels
    and provide a unified view of customer data. The advent and ready availability
    of virtual resources change how a company looks at resourcing processes [40].
    Modern BPM systems are cloud based and provide greater flexibility and scalability.
    These systems are designed to operate on a subscription-based model, and can be
    accessed anywhere and anytime. Hence, modern BPM systems are agile and adaptable,
    designed to respond to changes in the environment, and quickly adapt to new business
    requirements. These advancements affect how work is designed, the employee’s role
    as a process is automated, and resources are virtualized. Therefore, BPM theory
    must expand beyond automation to include work design and virtual resources for
    architectural considerations. In addition, the BPM systems built according to
    these theories must be adapted to expand the approach. The study notes that many
    firms need help with the BPM because of the limited understanding of this system
    design [41]. Hence, they must still undertake BPM initiatives. It concludes that
    BPM is a term that describes the process-oriented philosophy in managing organizational
    activities as well as the business-IT functions of the organization. Progress
    in modern BPM systems is used to improve the efficiency and reliability of processes,
    as virtual resources can be used to simulate real-world scenarios. Modern BPM
    systems can also support work design, which involves designing and arranging work
    activities to optimize performance and efficiency [42]. These systems can also
    provide a centralized platform for process management and monitoring, allowing
    businesses to monitor and analyze the performance of their processes in real time.
    Such information can be used to identify areas for improvement and implement changes
    that can enhance process performance [25]. Moreover, modern BPM systems are designed
    to be seamlessly integrated with other systems and applications. This integration
    ensures that data are shared across the organization, leading to more informed
    decision-making. Hence, modern BPM systems have evolved from traditional process
    optimization and automation methods to digital transformation, customer-centric,
    cloud-based, agile, and integration. These systems are designed to give organizations
    the tools to become more agile, flexible, and responsive to environmental changes.
    Generally, modern BPM systems improve process management to improve efficiency
    and performance, and remain competitive in today’s fast-paced business environment
    [27]. Over the years, BPM systems have undergone significant development and improvement,
    leading to modern techniques designed to meet the current dynamic and rapidly
    changing business environment. Table 2 presents some recent studies on the development
    of BPM, focusing on virtualization and work design. This provides valuable insight
    into the impact of practical integration. Moreover, these studies support the
    need for further research for broad contributions to the development of BPM system
    design. TABLE 2 Studies on the Impact of Virtualization and Work Design on the
    Development of BPM System Design Modern BPM systems provide a holistic view of
    business processes. Thus, it enables area identification for improvements and
    changes to increase efficiency and reduce costs. Another essential aspect is their
    ability to automate business processes, reduce the need for manual intervention,
    and free employees to focus on more important tasks. Another critical feature
    of modern BPM systems is their ability to be integrated with other technologies.
    These technologies include customer relationship management (CRM) systems, ERP
    systems, and supply chain management (SCM) systems [23]. Integration allows organizations
    to manage their operations effectively (see Table 3). Thus, the risk of data silos
    was reduced. Simultaneously, the information shared between different departments
    in real time is enhanced with higher efficiency. TABLE 3 Technology With the Capacity
    to Integrate Into BPM Systems 3) Key Steps in Adopting BPM System Design The adoption
    of BPM is a complex process that involves several key steps. The first step in
    BPM adoption is to identify the business processes that need improvement. It involves
    understanding the organization’s goals and objectives as well as the methods that
    are currently in place. Organizations can use process-mapping tools to map their
    existing processes and identify areas for improvement. Once the organization has
    identified the techniques that need improvement, the next step is to define the
    process objectives [55]. Process objectives are specific, measurable, and achievable
    goals that an organization wants to achieve through BPM adoption. These objectives
    should be aligned with the organization’s overall goals and objectives. The third
    step in BPM adoption is to design new processes to help the organization achieve
    its objectives. This entails identifying process improvements, streamlining processes,
    and eliminating redundant tasks. Process modelling tools can create new techniques
    and simulate their impact on organizations [38]. Once the new processes have been
    designed, the organization must implement them. It encompasses training employees
    on the latest procedures, updating existing systems, and integrating new functions
    into the organization’s operations. Organizations may need to modify their operational
    structures to support new processes. The fifth step in BPM adoption involves monitoring
    and measuring the performance of the new processes. It includes tracking key performance
    indicators (KPIs) such as cycle time, process defects, and customer satisfaction.
    Organizations can use dashboards and other monitoring tools to track KPIs and
    identify areas for improvement. The final step in BPM adoption is to analyze and
    optimize the new processes. It uses the data collected from the monitoring and
    measurement stages to identify areas for improvement and make changes to the new
    methods. Organizations can use process improvement tools to identify areas for
    optimization and implement process changes. Generally, BPM adoption is a complex
    process involving several key steps, including identifying business processes
    and defining process objectives. The process entails designing new strategies,
    implementing new procedures, monitoring and measuring performance, and analyzing
    and optimizing processes [55]. Consequently, organizations adopting BPM can achieve
    improved efficiency, increased agility, cost savings, enhanced customer satisfaction,
    and many other benefits. By following these key steps, organizations can successfully
    implement BPM and achieve operational excellence. 4) Application of BPM System
    Design Across Industries BPM systems continue to evolve, incorporating new technologies,
    such as AI and ML. Consequently, BPM systems became more integrated with the ability
    to interact with other methods, such as customer relationship management (CRM)
    and enterprise resource planning (ERP) systems [56]. This integration allowed
    organizations to streamline their operations and processes, reduce manual intervention,
    and improve efficiency. Currently, BPM systems are widely used across various
    industries, including manufacturing, finance, healthcare, and government. These
    systems have become critical components of organizations’ operations, helping
    them manage complex processes, improve efficiency, and achieve operational excellence.
    BPM systems are also increasingly focusing on customer experience, with integrated
    features such as process analytics and customer feedback. The history and evolution
    of BPM system design across industries have been marked by steady technological
    advancements, increased integration with other systems, and a focus on improving
    customer experience [38]. Consequently, BPM systems have become essential to organizations’
    operations and processes, helping them achieve operational excellence and goals.
    As technology evolves, BPM systems are likely to play a key role in organizations’
    operations and processes for many years [55]. Although BPM system design has common
    principles, each industry’s specific challenges and needs result in differences
    in creating BPM systems. a: Application of BPM System Design in the Manufacturing
    Industry The use of the BPM system design in the manufacturing industry has evolved
    significantly over the past several decades [57]. In the early days of BPM systems,
    manufacturing organizations primarily focused on automating individual processes,
    such as order fulfilment and production schedules. These early BPM systems were
    limited in scope and functionality and were often implemented using manual methods
    and spreadsheets. However, the emergence of the Internet and an unprecedented
    surge in technological applications have revolutionized the industry. Consequently,
    BPM significantly impacts a company’s supply chain management (SCM) by improving
    operational efficiency, positive productivity, and manageable costs [58]. One
    example of the impact of BPM on SCM is order fulfilment (see Figure 3). BPM systems
    automate the receiving, processing, and fulfilling of customer orders. As a result,
    it helps organizations reduce manual intervention, minimize errors, and improve
    order accuracy. BPM systems also provide real-time visibility into order-fulfilment
    processes, allowing organizations to identify and resolve any issues that may
    arise quickly. FIGURE 3. Common input and output processes of manufacturing. Show
    All b: Application of BPM System Design in the it Industry The IT industry has
    come a long way since its inception as a disorganized emerging sector. Over the
    years, the industry has transformed into a modern integral part of the global
    economy through proper BPM practices [59]. BPM systems enable organizations to
    automate and streamline their operations and processes, reduce manual intervention,
    and improve efficiency. These systems also allow organizations to better manage
    their resources and optimize their processes, which helps improve competitiveness.
    As the IT industry continued to evolve, BPM systems became increasingly integrated
    with other systems such as CRM and ERP [56]. The IT industry is one of the most
    significant and integral parts of the global economy, contributing significantly
    to economic growth and employment. This transformation has been driven by the
    widespread adoption of BPM systems, allowing organizations to achieve operational
    excellence and goals. The significant ways in which BPM has been integrated into
    the industry include automating IT processes, streamlining workflows, enhancing
    communication, improving decision-making, enhancing compliance, improving customer
    satisfaction, and managing change. BPM solutions in the IT industry typically
    offer features such as process modelling and design, process execution, process
    monitoring and optimization, and process collaboration and communication [34].
    These tools help organizations gain visibility into their processes, identify
    bottlenecks and inefficiencies, and make real-time improvements. c: Application
    of BPM System Design in the Medical Industry In this industry, BPM can be used
    to improve the efficiency, accuracy, and effectiveness of patient care and administrative
    processes [60]. The healthcare industry uses unique processes that require high
    precision, speed, and efficiency. Therefore, BPM principles are applied in the
    healthcare industry to improve patient care quality, reduce costs, and increase
    efficiency. Another way that BPM principles are involved in the medical sector
    is through clinical decision support systems (CDSSs) [61]. CDSSs use algorithms
    and data analysis to help healthcare providers make informed decisions regarding
    patient care. BPM principles help optimize these systems, ensuring that they provide
    relevant and accurate information in real time. As a result, it helps healthcare
    providers make informed decisions quickly and confidently, improving the quality
    of patient care in emergency cases, such as the COVID-19 pandemic [62]. The medical
    industry also applies BPM principles to enhance supply chain management. BPM principles
    help optimize the flow of drugs and medical supplies from manufacturers to healthcare
    providers. As a result, it helps reduce waste, improve inventory management, and
    reduce healthcare costs. By using BPM principles to optimize the supply chain,
    the healthcare industry can ensure that patients have access to the medications
    and supplies they need when they need them. Hence, the healthcare industry can
    ensure that patients receive high-quality care, while reducing costs and improving
    efficiency [60]. BPM principles have led to significant innovation, which has
    improved practice in the industry. These steps include the following. Clinical
    Process Improvement: BPM can streamline and standardize clinical processes such
    as patient diagnosis, treatment, and follow-up care. It helps to ensure that patients
    receive consistent, high-quality care and reduces the risk of errors and adverse
    events. Electronic Health Record (EHR) Management: The BPM can optimize EHR systems
    commonly used in healthcare organizations to manage patient information. By automating
    and streamlining the process of collecting, storing, and retrieving patient data,
    BPM can improve healthcare delivery efficiency and accuracy. As a result, EHR
    ensure that healthcare providers have access to accurate and up-to-date information
    about their patients, thereby improving patient outcomes. Complaint Handling System:
    A complaint handling system is essential for quality management and patient satisfaction
    in the medical industry [63]. A complaint handling system aims to allow patients,
    families, and healthcare providers to express their concerns and receive prompt
    and effective responses. BPM affects the effectiveness and efficiency of the usability
    of such a system. It enables complaint registration, assessment, resolution, feedback,
    and intervention success, monitoring, and evaluation. Supply Chain Management:
    BPM can manage the supply chain of medical goods and services by procuring raw
    materials to deliver finished products to customers. This includes managing the
    flow of materials, information, and payments between suppliers, manufacturers,
    distributors, and healthcare providers. Patient Scheduling and Appointments: BPM
    can automate and streamline scheduling patient appointments and managing waiting
    lists. This can help improve patient satisfaction, reduce wait times, and increase
    operational efficiency. Regulatory Compliance: BPM can be used to ensure that
    healthcare organizations comply with various regulations, such as the HIPAA and
    the Affordable Care Act [64]. By automating and streamlining data privacy and
    security processes, the BPM can help reduce the risk of regulatory violations
    and protect patients’ personal information. d: Application of BPM System Design
    in the Finance Industry The financial industry is highly regulated and complex
    [59]. Therefore, efficient operational processes are required. BPM systems are
    designed to help organizations optimize their operations and improve their efficiency.
    In the finance industry, BPM systems can be designed to address a range of processes
    including loan processing, risk management, and compliance management [28]. Loan
    processing is a critical process that can benefit from BPM systems. A BPM system
    can be designed to automate the loan-processing workflow, reduce the need for
    manual intervention, and minimize the risk of errors [65]. This system can be
    designed to capture customer data, assess loan eligibility, and manage the loan
    approval process. It helps improve the speed and accuracy of loan processing and
    reduces the time required to complete loan applications. Risk management is another
    process that can benefit from the BPM system in the finance industry. BPM systems
    can be designed to monitor and manage risk, ensuring that the finance organization
    operates within acceptable risk parameters [28]. This includes monitoring market
    risk, credit risk, and operational risk. BPM systems can be designed to track
    and manage risk metrics, and alert risk managers when a potential risk event occurs.
    It helps reduce the risk of financial loss and ensures that the organization operates
    within regulatory compliance requirements. Compliance management is another critical
    process that can benefit from the BPM system in the finance industry. BPM systems
    can be designed to automate compliance management, ensuring that the organization
    adheres to regulatory requirements [66]. This study noted that this includes monitoring
    and managing regulatory compliance, such as anti-money laundering (AML) regulations
    and the Foreign Account Tax Compliance Act (FATCA). BPM systems can be designed
    to capture and analyze data and alert compliance managers when a potential compliance
    issue arises. As a result, it helps reduce the risk of non-compliance and ensures
    that the organization operates within the law [66]. Moreover, automation of back-end
    processes enables accounts payable, accounts receivable, and payment processing.
    The BPM system design helps to streamline these processes by automating routine
    tasks, reducing errors, and improving the speed and accuracy of transactions [67].
    As a result, it enables finance organizations to process transactions faster,
    reducing the time required to complete back-end processes and improving customer
    experience. Generally, the finance industry benefits significantly from the use
    of BPM systems. BPM systems can be designed to improve efficiency, reduce costs,
    and minimize the risk of errors. By automating key processes, BPM systems can
    help finance organizations operate more effectively and efficiently [1]. The study
    notes that areas with the opportunity to implement such automation include loan
    processing, risk management, and compliance management. e: Application of BPM
    System Design in the Retail Industry The retail industry constantly changes, and
    retailers continuously seek ways to improve their operations and customer experiences.
    The BPM system design is one way to achieve this goal [68]. An example of the
    unique use of the BPM system design in the retail industry is the implementation
    of omni-channel retailing. Omni-channel retailing is a customer-focused approach
    that integrates all customer touchpoints, including online and offline channels
    [69]. The BPM system design helps optimize these channels, ensuring that customers
    have a seamless shopping experience, regardless of their medium. Consequently,
    it helps retailers improve customer satisfaction and increase sales. Another example
    of a unique application of the BPM system design in the retail industry is the
    implementation of predictive analytics [68]. Predictive analytics uses data analysis
    to predict future trends and customer behavior. Retailers use the BPM system design
    to optimize predictive analytics systems, ensuring that they provide relevant
    and accurate information to staff. This helps retailers make informed decisions
    about inventory management, marketing strategies, and customer engagement [70].
    The BPM is also used to improve in-store operations. Retailers use the BPM system
    to automate cash management, inventory management, and customer service processes.
    Walmart is a leading global chain store that leverages the BPM system design to
    optimize its supply chain processes [71]. The BPM system design helps Walmart
    manage the flow of goods from suppliers to stores, ensuring that products are
    available to customers when needed [72]. Walmart’s use of the BPM system design
    has enabled the company to reduce waste, improve inventory management, and increase
    efficiency. Consequently, it has helped Walmart remain a market leader in the
    retail industry. Another pioneer in BPM system design in the retail industry is
    Amazon. Amazon uses a BPM system design to automate the order-fulfilment processes.
    The BPM system design helps Amazon to manage the flow of orders from customers
    to fulfilment centers, reducing the time required to complete orders and improving
    customer experience [72]. Amazon’s use of the BPM system design has enabled the
    company to remain a market leader in the retail industry. Hence, they deliver
    products to customers quickly and efficiently [73]. The BPM system design helps
    to optimize these processes, reduce errors, and improve the speed and accuracy
    of transactions. Consequently, it allows retailers to reduce costs, improve customer
    satisfaction, and increase sales. f: Application of BPM System Design By the Government
    The government faces unique challenges and needs that require innovative and tailored
    BPM system designs. For example, one unique way the government uses the BPM system
    design is by automating the procurement processes [74]. Procurement processes
    can be complex and time consuming. Such complexities can be solved using the BPM
    system design, which can streamline these processes, reduce errors, and improve
    the speed and accuracy of transactions. Consequently, it enables the government
    to purchase goods and services more efficiently, reduce costs, and improve the
    quality of public services. Another unique way in which the government uses the
    BPM system design is by implementing case management systems [75]. Case-management
    systems are designed to track and manage complex cases related to benefits, immigration,
    and criminal justice. The BPM system design helps optimize these systems, ensuring
    that they provide relevant and accurate information to citizens and staff. Consequently,
    it allows the government to improve case resolution times, reduce costs, and improve
    the quality of public services. This system helps governments to ensure that procurement
    processes are transparent, efficient, and compliant with government regulations
    [74]. By using the BPM system design, governments can reduce the risk of fraud
    and corruption, improve the quality of public procurement, and reduce the costs
    of public goods and services [66]. By using the BPM system design, the government
    can improve efficiency, reduce costs, and improve the quality of public services
    [76]. In addition, the government uniquely uses the BPM system design to enhance
    the quality of its public delivery foundation and framework. The BPM system design
    manages public health programs, public works projects, and general procurement
    processes. Governments can use BPM system design to enhance the efficiency, transparency,
    and compliance of public services [66]. This improves public health outcomes,
    the delivery of public works projects, and the quality of public procurement.
    5) Global Attitudes Toward BPM System Design Although BPM system design has gained
    widespread acceptance globally, there are varying attitudes toward its implementation
    and use [77]. The first attitude toward the BPM system design is that it is necessary
    for organizational success. BPM systems enable businesses to streamline processes,
    reduce errors, and increase their productivity [78]. BPM systems are widely embraced
    in countries such as the United States with a strong focus on productivity and
    efficiency. Entities can perceive BPM as a way to reduce costs and improve their
    operational efficiency [57]. The use of BPM systems is prevalent in industries,
    such as healthcare, finance, and manufacturing. Another attitude toward the BPM
    system design is that it is a tool for driving innovation [79]. BPM systems enable
    organizations to automate processes and focus on more innovative tasks. Attitudes
    are particularly prevalent in countries with a strong focus on technology and
    innovation. These countries see the BPM systems as a way to improve their global
    competitiveness. The third attitude toward the BPM system design is that it is
    a tool for compliance [63]. BPM systems are considered a way to ensure regulatory
    compliance. Organizations observe regulations by BPM systems, enabling them to
    comply with legal requirements, while improving operational efficiency [57]. This
    attitude is particularly prevalent in banking, insurance, and healthcare industries.
    Another attitude toward the BPM system design is that it is a tool for customer
    experience [78]. BPM systems are seen as a way to improve customer experience,
    as entities enhance their focus on customer service. These systems enable organizations
    to provide more efficient and personalized services. This attitude is particularly
    prevalent in retail and hospitality industries. Generally, while there are varying
    attitudes toward the BPM system design, it is widely accepted as a critical tool
    for organizational success [36]. BPM systems are essential for businesses to compete
    in global markets. It is necessary to improve operational efficiency, drive innovation,
    ensure regulatory compliance, and enhance customer experience. As such, organizations
    must embrace BPM system design as a critical component of their digital transformation
    strategy [4]. B. Concept of Virtualisation Virtualization refers to the creation
    of a virtual version of a physical device or resource, such as a computer, network,
    or storage. Several pioneers, such as IBM, VMware, and Microsoft, have broadly
    expanded this innovation. These companies first introduced the concept of virtualization
    in the 1960s [80]. IBM developed the first virtualization software in the late
    1960s: mainframe computers. Another critical innovation was VMware, which revolutionized
    the industry in the late 1990s with its x86 virtualization technology [81]. The
    study notes that Microsoft followed suit with its Hyper-V virtualization technology
    in 2008. It also notes that the history of virtualization has seen several advancements,
    with the development of virtual machine technology in the late 1990s being a key
    milestone. The adoption of virtualization in business drives the efficiency and
    flexibility of IT systems and reduces costs. Companies have widely adopted virtualization
    technology for several reasons. First, virtualization allows entities to manage
    and automate their IT systems more efficiently and flexibly, reducing downtime
    and improving response time [82]. In addition, such capabilities reduce costs
    by enabling firms to run multiple virtual machines on a single physical server,
    thereby reducing the need for multiple physical servers [83]. Finally, virtual
    machines can be isolated from the underlying physical infrastructure to enhance
    security and privacy (Figure 4). FIGURE 4. Illustration of virtualization by files
    interacting with cloud computing and virtual machine for storage purposes. Show
    All Over time, virtualization technology has advanced, allowing organizations
    to create virtual versions of a broader range of resources and use them innovatively.
    In addition, it has led to the development of related concepts, such as cloud
    computing and containerization. Cloud computing has enabled virtualization technology
    to create virtualized computing resources that can be delivered over the internet
    [84]. For containerization, virtualization technology has been allowed to create
    isolated settings for applications and services, improving efficiency and flexibility.
    Critical technologies in virtualization include [85] the following. Hypervisor:
    This is the core technology behind virtualization. The hypervisor, a virtual machine
    monitor, creates virtual machines and manages their interactions with underlying
    physical hardware. Virtual Machine (VM): A virtual machine is a software-based
    version of a computer that functions as a physical machine. VMs allow multiple
    operating systems to run on a single physical device, making it possible to run
    various applications and procedures simultaneously. Network virtualization: This
    technology creates virtualized networks that allow multiple virtual machines to
    share the same physical network infrastructure. It reduces hardware costs, increases
    scalability, and improves the network performance. Storage virtualization: This
    technology abstracts physical storage devices into a pool of virtual storage devices,
    making it possible to allocate and manage storage resources dynamically. It enables
    organizations to optimize storage utilization and improve data protection and
    availability. Management Tools: Organizations require specialized management tools
    to manage and monitor virtualized systems and applications. These tools provide
    a single control point for managing the virtual infrastructure, including provisioning,
    monitoring, and reporting. 1) Development of Virtualisation Virtualization has
    played a significant role in the development of modern BPM systems. Innovation
    has existed for many years, but its use in BPM systems is relatively recent [86].
    This study notes that it has become an essential IT infrastructure component and
    has transformed how businesses operate and manage their systems. Virtualization
    allows businesses to create a virtual environment in which multiple operating
    systems can run on a single physical device [85]. It provides a more flexible,
    scalable, and cost-effective approach for managing IT resources than traditional
    computing models. The introduction of virtualization in BPM systems has been driven
    by the need for organizations to improve their business processes [86]. Hence,
    they can remain competitive in an increasingly challenging business environment.
    Thus, it has played a significant role in developing modern BPM systems, providing
    organizations with a powerful tool for improving the efficiency and reliability
    of their business processes. With the continued growth of virtualization technology,
    its use in BPM systems is likely to continue growing [11]. Creating virtual environments
    for process execution has several benefits. This allows the simulation of real-world
    scenarios and testing process designs before implementation [83]. Thus, it increases
    the efficiency and reliability of the assessment process at reduced costs associated
    with the testing and implementation tasks. Many business functions are outsourced
    as it becomes more common to integrate remote staff into core work functions.
    The step of bringing geographically distributed groups of workers together for
    collaboration through technology-mediated communication is called a global virtual
    team [77]. These Global Virtual Teams have unique needs in terms of employee engagement
    and work design. Another study found that virtual teams performed less effectively
    than face-to-face teams when performing complex tasks [87]. In these tests, work
    design was not considered a factor. Several studies have investigated the impact
    of BPM and workflow automation [88]. Their preliminary results from an investigation
    into 16 business processes from six different organizations showed several improvements.
    They asserted that business process improvement is measured in terms of lead time,
    service time, wait time, and resource utilization. Based on preliminary results,
    they predicted that significant improvements in these parameters would be expected
    for almost all the investigated business processes. However, this study did not
    include virtual resources or explore the impact of BPM activities on work design.
    Simulations proved to be a valuable way to validate the initial measurements.
    However, it cannot consider technological advances over time or their impact on
    staff roles and work design. The key components of virtualization are discussed
    in the following section. a: Hypervisor A hypervisor, which is a virtual machine
    monitor, is a software layer that enables multiple virtual instances of operating
    systems [81]. It enhances applications and other computing resources for running
    on a single physical machine. The hypervisor allocates and utilizes resources,
    such as CPU, memory, and storage. This ensured that each virtual machine was isolated
    from the others. The hypervisor is a critical component of virtualization technology
    that enables the efficient and secure utilization of physical resources and provides
    a virtualized environment for multiple virtual machines. There are two types of
    hypervisors: types 1 and 2. Type 1 hypervisors run directly on the hardware of
    the host machine and are known as bare-metal hypervisors [81]. They provide high
    performance and efficiency and are tightly integrated with the underlying hardware.
    Type 1 hypervisors are often used in enterprise datacenters and cloud computing
    environments. Type 2 hypervisors run on top of a host operating system and are
    known as hosted hypervisors [81]. They are less efficient than type 1 hypervisors
    because they rely on the host operating system for hardware access. Type 2 hypervisors
    are often used in desktop virtualization and testing environments. b: Virtual
    Machines A virtual machine (VM) is a software emulation of a physical device that
    runs an operating system and applications [89]. Virtual machines are created and
    managed by a hypervisor, which is a key component of virtualization technology.
    Hence, the hypervisor allocates physical resources such as the CPU, memory, and
    storage from the underlying hardware and creates a virtual environment for the
    virtual machine [81]. The virtual machine is installed with an operating system
    and applications similar to a physical machine. They are widely used in enterprise
    data centers, cloud computing, and other environments. Virtual machines provide
    several benefits, including Isolation: Each virtual machine is isolated from other
    devices and underlying hardware, providing a secure environment for running applications.
    Resource utilization: Multiple virtual machines can run on a single physical machine,
    allowing for the efficient utilization of resources such as CPU, memory, and storage.
    Flexibility: Virtual machines can be created, deleted, and moved between physical
    devices quickly, providing flexibility in managing computing resources. Testing
    and development: Virtual machines can be used for this purpose by allowing developers
    to test applications in different operating system environments without requiring
    physical hardware. c: Virtual Disks Virtual disks are a vital component of virtualization
    technology that allow virtual machines to access storage resources in a virtualized
    environment [89]. A virtual disk is a file stored on a physical disk that acts
    as a virtual hard drive for a virtual machine. It can be created by a hypervisor
    or operating system running on a virtual machine [83]. The hypervisor manages
    virtual disks and can be configured to have different sizes and storage types.
    They can be stored in different types of storage media, including local disks,
    network-attached storage, and storage area networks. Virtual disks are a crucial
    component of virtualization technology that provide a flexible and efficient way
    to manage storage resources in a virtualized environment [90]. The study noted
    that they are widely used in enterprise datacenters, cloud computing, and other
    settings. Virtual disks provide several benefits, including: Flexibility: Virtual
    disks can be easily created, revised, and moved between physical storage devices,
    thus providing flexibility in managing storage resources. Isolation: Each virtual
    machine has a virtual disk that provides isolation from other virtual machines
    and the underlying hardware. Performance: Virtual disks can be optimized by configuring
    parameters such as block size, caching, and access patterns. Snapshots: Virtual
    disks can be used to create images that are point-in-time copies of the disk.
    This allows administrators to easily roll back to the previous state of the virtual
    machine. d: Virtual Network A virtual network is a crucial component of virtualization
    technology that enables virtual machines to communicate with one another [91].
    In addition, these resources can communicate with different physical and virtual
    machines in a virtualized environment. In a virtual network, virtual machines
    are connected to virtual switches managed by a hypervisor. The virtual switches
    are then connected to physical switches and routers, allowing the virtual machines
    to communicate with other devices on the network [80]. Virtual networks are a
    vital component of virtualization technology that provide a flexible and efficient
    way to manage network resources in a virtualized environment. Consequently, they
    are widely used in enterprise data centers, cloud computing, and other computing
    environments [92]. Virtual networks provide several benefits, including: Isolation:
    Each virtual network is isolated from other virtual and physical networks, thereby
    providing a secure environment for virtual machines to communicate. Flexibility:
    Virtual networks can be easily created, resized, and configured, providing flexibility
    in managing network resources. Scalability: Virtual networks can be scaled up
    or down to meet the changing needs of a virtual environment. Performance: Virtual
    networks can be optimized by configuring parameters such as bandwidth, latency,
    and quality of service. e: Management Tools Management tools are essential virtualization
    technologies [93]. This enables administrators to manage and monitor virtual machines,
    networks, and other features. Management tools are critical for virtualization.
    Administrators can efficiently monitor virtualized environments [22]. As a result,
    they are widely used in enterprise data centers, cloud computing, and other computing
    environments Management tools provide a wide range of features and capabilities,
    including: Virtual machine management: Management tools enable administrators
    to create, configure, and manage virtual machines, including provisioning virtual
    disks, configuring virtual networks, and managing virtual machine templates. Performance
    monitoring: Management tools enable administrators to monitor the performance
    of virtual machines, virtual networks, and other virtualization components, including
    metrics such as CPU and memory usage, disk I/O, and network bandwidth. Resource
    allocation: Management tools enable administrators to allocate resources to virtual
    machines and other virtualization components, including setting CPU and memory
    reservations and limits. Automation: Management tools enable administrators to
    automate daily tasks such as deploying virtual machines, configuring virtual networks,
    and managing virtual machine backups. Reporting and analysis: Management tools
    enable administrators to generate reports and analyze virtualization performance,
    usage, and trends. 2) Modern Virtual Resources Modern corporate environments are
    more data-driven than ever. The availability of digital resources has resulted
    in a nearly limitless supply of processing power that can be made available within
    short timeframes. Studies have investigated the impact of virtual resources on
    workflow scheduling applications [94]. This study examines the commercial application
    of running business-IT functions in virtual environments compared to traditional
    ones. Organizations’ return on investment is considered when investing in BPM
    [74]. These commercial considerations have a direct effect on adoption. Aspects
    such as following proper guidelines and optimizing workflow design in business
    operations impact the efficiency of systems. When followed, these aspects result
    in an efficient, meaningful, and logical workflow [35]. After investigating the
    opportunities for automation in real-world scenarios, it was established that
    data processing is a common element that affects the reliability of a workflow
    system [53]. Owing to computer processing power for business operations, automation
    for a modern organization is more data-oriented than when it is more physical.
    Virtual resources provide a unique opportunity for trigger-based automation. The
    same triggers commonly used in BPM workflows can trigger changes in a virtual
    environment [11]. A helpful link between workflow triggers, a key component of
    BPM systems, and virtual resources exists, but does not form part of this research.
    The virtual resources are not limited to computer processing [11]. This study
    established that offsite and offshore labor forces are becoming increasingly available
    to businesses to perform specific tasks. Another study investigated the commercial
    impact of BPM, focusing on the automation of business processes and relocation
    of business processes to low-wage countries [95]. This study examined cost reduction
    and productivity differences between locations. They found that automation and
    sourcing opportunities are usually evaluated independently, which can result in
    suboptimal commercial returns. These findings show that the interdependencies
    between automation and sourcing need to be adequately considered and present a
    decision model to support these considerations. Thus, the availability of virtual
    resources is changing how people and organizations think about BPM and automation.
    Researchers conducted a longitudinal study that moved beyond the simulations by
    assessing work processes over time [22]. The results showed that only 50% of the
    organizations successfully implemented a workflow management system for at least
    one of the business processes they had targeted. In addition, the successfully
    implemented processes indicated significant productivity improvement, showing
    the assurance of benefits from workflow automation through BPM. The impact of
    work design should have been considered, but the critical success factors of implementing
    support management and cultural changes are discussed. However, this study lacks
    quantitative evaluations based on empirical and experimental research. The initial
    process executions result in a higher effort than the subsequent process changes.
    In addition, the impact of domain knowledge on implementation efforts was significant,
    supporting the notion that work design is substantial [96]. This study asserted
    that more experimental research is needed to investigate the impact of virtualization.
    3) Key Steps in Adopting Virtualization Virtualization adoption involves several
    key steps that must be followed to ensure successful implementation. The first
    step in virtualization adoption is to assess the organization’s needs. This includes
    evaluating the current IT infrastructure, identifying improvement areas, and determining
    the benefits of virtualization [97]. Organizations must also consider the cost
    of virtualization adoption and whether they align with their overall goals and
    objectives [10]. The second step in virtualization adoption is to determine a
    virtualization strategy. It entails deciding which type of virtualization is most
    suitable for the organization, whether server virtualization, desktop virtualization,
    or application virtualization [91]. Organizations must consider the infrastructure
    required to support the virtualization solution and the technical expertise required.
    The third step in virtualization adoption is implementation planning. This involves
    creating a detailed project plan outlining the steps to implement the virtualization
    solution [40]. The project plan should also include a timeline, budget, and list
    of resources required. Involving all stakeholders in the planning process is essential
    to ensure buy-in and support [98]. The fourth step in virtualization adoption
    is to deploy a virtualization solution. Installing the necessary software and
    hardware and configuring the virtualization environment entail the installation
    of essential software and hardware [92]. Organizations must also consider security
    and backup solutions to protect their virtual environments [99]. The fifth step
    in virtualization adoption is to train staff. This includes training IT staff
    on the virtualization solution and its operation [11]. The study established that
    ensuring that staff members have the necessary skills to operate and maintain
    a virtualization environment is essential. The final step in virtualization adoption
    is to monitor and manage the performance. It involves tracking KPIs, such as system
    availability, user experience, and resource utilization [38]. Organizations can
    use monitoring tools and dashboards to track KPIs and identify areas for improvement.
    Virtualization is a complex process involving several key steps [100]. These steps
    include assessing organizational needs, determining a virtualization strategy,
    planning implementation, deploying the virtualization solution, training staff,
    and monitoring and managing performance. Consequently, organizations that adopt
    virtualization can achieve improved efficiency, increased agility, cost savings,
    enhanced security, and many other benefits [97]. Hence, organizations can successfully
    implement virtualization and transform their IT infrastructure. 4) Adoption of
    Virtualization Across Industries The adoption of virtualization across industries
    has been widespread and varied in recent years. The pioneering industries using
    this technology are IT companies. Virtualization has been widely adopted by the
    IT industry to improve efficiency, reduce costs, and increase agility. This has
    enabled companies to run multiple virtual machines on a single physical server,
    thereby reducing the need for physical hardware and increasing the utilization
    of existing resources. In the data management age and the need to scale storage
    systems and security, other industries such as medicine, finance, retail, education,
    and the government are adopting it [24]. a: Adoption of Virtualization in the
    it Industry Virtualization in the information technology (IT) industry has a rich
    history. It has significantly impacted how organizations operate and manage their
    IT resources. The concept of virtualization can be traced back to the 1960s when
    the idea of time sharing was introduced [59]. However, until the late 1990s and
    the early 2000s, virtualization began to gain widespread traction as a technology
    for managing IT resources. The adoption of virtualization was driven by the need
    to use hardware resources better, reduce costs, and increase the agility of IT
    systems. Virtualization has evolved and improved in recent years. Today, it is
    widely recognized as a critical technology in modern data centers. Most companies
    have adopted virtualization technology, with virtual machines (VMs) being the
    most commonly used [81]. Virtualization has enabled organizations to run multiple
    virtual machines on a single physical server, reducing the need for physical hardware
    and increasing the utilization of existing resources. It has also facilitated
    the creation of new and innovative IT solutions such as cloud computing, transforming
    how organizations manage and deliver IT services [101]. In terms of impact, virtualization
    has enabled organizations to increase efficiency, reduce costs, improve security,
    and increase the availability of IT services. It has also allowed organizations
    to respond more quickly to changing business requirements, reduce downtime, and
    improve business continuity. b: Adoption of Virtualization in the Medical Industry
    Research on the adoption of virtualization in the medical industry has shown that
    virtual consultation and telemedicine are becoming more common. Progress is enabling
    patients to receive medical advice and treatment from the comfort of their homes.
    It has improved access to care and reduced travel, thus saving time and money
    for patients and healthcare providers [56]. Patients can receive medical advice
    from their own homes, reducing the need for travel and making healthcare more
    accessible. As convenience from this trend increases, it is gradually becoming
    easier for physicians and patients to make workable schedules with the lowest
    cost of travel and related expenses. In Nigeria, patients can efficiently and
    safely access healthcare needs by digitizing medical services. In addition, technology
    has improved healthcare delivery efficiency with less need for administrative
    tasks and walk-in [102]. Additionally, better health outcomes have significantly
    increased, as patients can receive medical advice and treatment more quickly and
    efficiently. Further research is needed to ensure that virtualization is safe
    and responsible for the continuous use and protection of private data. Nevertheless,
    the benefits of virtualization in the medical industry have been widely recognized,
    and its use is expected to continue to grow in the coming years. Its adoption
    in the industry has positively impacted patient care and medical education while
    reducing costs and improving efficiency. c: Adoption of Virtualization in the
    Finance Industry Virtualization has played a significant role in revolutionizing
    the finance industry in recent years. Its adoption in finance began in the early
    2000s and quickly became an essential tool for many financial organizations [103].
    Some early pioneers of virtualization in the finance industry were Citigroup,
    JPMorgan Chase, and the Bank of America. These financial companies saw the potential
    for virtualization to improve service delivery [37]. Thus, by developing reliable
    IT infrastructure that creates value for customers, companies could serve customers
    while reducing the costs associated with credit or account statement preparation
    paperwork. The research noted that service innovations were significantly impacted
    by virtualization capabilities. With virtualization, financial organizations have
    reduced IT infrastructure costs and improved their applications’ efficiency and
    performance. Virtualization has also allowed organizations to respond quickly
    and efficiently to changes in their business environment. As a result, businesses
    can rapidly deploy new applications and services or scale back their IT infrastructure,
    as needed. Therefore, the virtual desktop infrastructure (VDI) is a critical example
    of virtualization technology in the finance industry [89]. The study noted that
    VDI allows financial organizations to provide their employees access to their
    desktop environment from any device, anywhere, at any time. Consequently, it has
    made it possible for employees to be more productive and efficient, as they no
    longer need to be tied to a physical desktop or location. Server virtualization
    is another example of virtualization in the finance industry. Server virtualization
    allows financial organizations to run multiple virtual servers on a single physical
    server, thereby reducing the hardware needed and costs [97]. This has been imperative
    in the finance industry, where many organizations have increased their data center
    efficiency by up to 15% [104]. In addition, the study noted that the technology
    infrastructure could be optimized to consume less energy through better optimization
    of algorithms targeted to function with less power demand. Over the years, several
    visionaries have played a vital role in adopting and impacting virtualization
    in the finance industry. One such visionary is Simon Crosby, a co-founder and
    the former CTO of XenSource [105]. Crosby was one of the early pioneers of virtualization
    and was instrumental in helping financial organizations understand the potential
    benefits of virtualization. Another visionary in the finance industry is the VMware
    CEO Pat Gelsinger. Another study noted that Gelsinger was instrumental in bringing
    virtualization to the mainstream [106]. Moreover, he helped establish VMware as
    one of the leading virtualization companies in the world. He has continued to
    drive the company forward, and his vision has helped shape the future of virtualization
    in the finance industry. d: Adoption of Virtualization in the Retail Industry
    Virtualization has significantly impacted the retail industry, transforming how
    retailers operate and interact with customers. Adopting virtualization technology
    in the industry has made retailers seek new ways to improve their operations and
    enhance their customer experience. Cloud computing is a critical example of the
    virtualization technology used in the retail sector [107]. The study noted that
    cloud computing allows retailers to move their IT infrastructure and applications
    to virtual environments. Thus, they can be accessed and managed anywhere in the
    environment. Consequently, it has allowed retailers to reduce their IT costs and
    improve the scalability and reliability of their applications. Another example
    of virtualization technology in retail is virtual reality (VR) and augmented reality
    (AR). VR and AR have revolutionized how retailers interact with customers, allowing
    them to provide immersive experiences and enhance their shopping experiences [108].
    Retailers can use VR and AR to showcase products, demonstrate product features
    and benefits, and create interactive experiences to engage and excite customers.
    Over the years, several visionaries have played a vital role in the adoption and
    impact of virtualization in the retail industry. One such visionary was Jeff Bezos,
    the founder and CEO of Amazon. Bezos was an early pioneer of e-commerce after
    leading the bookselling sector into the digital marketplace [109]. The study noted
    that Bezos was instrumental in transforming the retail industry through his vision
    and leadership. Satya Nadella, Microsoft’s CEO, is another visionary in the retail
    sector. Nadella is a critical driver of Microsoft’s cloud-computing strategy [90].
    His vision has helped bring technology to the forefront of the retail industry.
    He also strongly advocated using VR and AR in retail. In addition, Nadella has
    worked to make these technologies accessible and affordable to retailers of all
    sizes. Virtualization has profoundly impacted the retail industry and continues
    to play a critical role in shaping its future. This technology has allowed retailers
    to improve their operations and enhance their customer experiences. Moreover,
    it has paved the way for innovations and advancements in the future. e: Adoption
    of Virtualization in the Education Industry Virtualization in the education industry
    can provide numerous benefits for scaling and improving the quality of language
    education. Access to technology has improved as virtualization has enabled students
    to access technology and resources that may not be available in traditional classroom
    settings [110]. It can help improve the quality of language education and support
    students in achieving their language-learning goals. Flexibility has become an
    integral benefit of virtualizing education. It allows students to access language
    courses anywhere and anytime, thus providing greater flexibility and convenience
    [40]. This can help to increase student engagement and improve learning outcomes.
    Flexibility is another benefit of cost effectiveness. Eliminating the need for
    physical classrooms and other infrastructure has reduced the costs of language
    education. This can help make language education more accessible and affordable
    for a broader range of students. In addition, the scalability level in this transformation
    of traditional education has enabled educators to reach a much larger audience
    regardless of geographic location. The language bias problem is solved by innovating
    language-translating AI and having translators that safeguard the quality standard
    of the deployed education content. This process has enabled educational needs
    to be met from a growing demand [111]. This study shows that using AI improves
    interactive systems, indicating reliable access to language education for students
    worldwide. Furthermore, this capability has enabled language educators to customize
    language courses to meet students’ specific needs and learning styles. This improves
    the effectiveness of digital education by ensuring that students receive the support
    they need to succeed. Collaboration is another benefit of virtualization in the
    industry [11]. This has enabled students to collaborate and interact with each
    other and language educators in real time, regardless of geographic location.
    It can help build communities and foster a sense of belonging among students,
    thereby improving their motivation and engagement. Virtualization can potentially
    transform the educational industry and enhance the quality of language education.
    Such progress can be achieved by providing greater flexibility, affordability,
    scalability, customization, and collaboration. f: Adoption of Virtualization By
    the Government Virtualization has had a significant impact on the way governments
    operate, transforming the way they provide services to citizens and managing their
    operations. The adoption of virtualization technology by the government began
    in the early 2000s [74]. Governments have sought new ways to improve their operations
    and enhance the delivery of public services. Cloud computing is a critical example
    of virtualization technology in use by the government. This study established
    that cloud computing capacity could allow governments to move their IT infrastructure
    and applications to a cloud-based environment. They can be accessed and managed
    anywhere and at any time. This has allowed governments to reduce their IT costs,
    improve the scalability and reliability of their applications, and better serve
    the needs of citizens. Another example of virtualization technology from this
    study is the government’s use of virtual resources for online meetings and teleconferencing.
    Virtual conferences and teleconferencing have allowed governments to collaborate
    and communicate more effectively without the need for travel. Virtual meetings
    and teleconferencing have made these technologies accessible and affordable for
    governments of all sizes. Virtual meetings and teleconferencing are critical to
    global cooperation [112]. The infrastructure has led continental intergovernmental
    bodies, such as the African Union (AU), to reform their diplomacy. The study noted
    that while virtual conferencing has brought challenges to cooperation among countries,
    it is an opportunity to reassess the founding of the AU to reimagine its continental
    diplomacy. These virtual technologies have allowed intergovernmental organizations
    to connect worldwide without travelling. Thus, it reduced travel costs and made
    it easier for officials to work together to achieve their goals. For example,
    the United Nations (UN) and its umbrella agencies, bodies, and related committees
    have had to adapt to the realities of the COVID-19 pandemic through videoconferencing
    [113]. The study noted that the advancement of diplomatic relations among nations
    during intergovernmental meetings was achieved using virtual conferences and teleconferencing.
    Thus, it has ensured improved communication and collaboration between member countries.
    In addition, it has allowed for more effective addresses of regional issues to
    promote cooperation. Similarly, the European Union (EU)has been using virtual
    meetings and teleconferencing to improve communication and collaboration among
    its member countries and promote regional integration [114]. Hence, this method
    has improved communication and collaboration between various departments and field
    offices. Moreover, its organs and governmental and nongovernmental partners have
    increasingly become capable of leading and cooperating more effectively when responding
    to humanitarian crises. 5) Global Attitudes Toward Virtualization Global attitudes
    toward virtual resources vary depending on country, cultural background, and individual
    experiences. In general, there is growing recognition of the importance and convenience
    of virtual resources. Increasing awareness of virtualization resources and their
    potential downsides and risks. In developed countries, access to technology and
    the Internet has led to widespread use of virtual resources for education, work,
    communication, and entertainment [91]. This study established that virtual resources
    are essential tools for modern life in these countries. In less developed countries,
    access to technology may be limited. However, there is still growing recognition
    of the potential of virtual resources to improve access to information, education,
    and economic opportunities. The growth of decentralized finance (Defi) as a disruptive
    financial technology has recently soared owing to the widespread use of cryptocurrencies
    in the global digital economy. Defi technology has led to the rise of virtualization
    in developing and less-developed nations [115]. It is easier and quicker to conduct
    transactions using these technologies. Consequently, attitudes toward virtual
    resources have become a topic of interest for many researchers [116]. Studies
    have found that virtual resources, such as social media, significantly influence
    people’s attitudes. A survey by the Pew Research Centre found that most adults
    in the United States primarily use technology as a news source for major health
    issues. Many people have used the Internet to assess how leaders have reacted
    to the COVID-19 pandemic [117]. A vital point of interest during the pandemic
    was the use of internet access by nurses for entertainment, news, and personal
    and professional development [118]. The use of virtual resources shows similar
    trends in favorability across Europe. A European Commission (EU) study found that
    most Europeans rely on the internet for valuable scientific information [119].
    This study found that digital resources can be used to inform people regarding
    scientific data and related insights. Such utility of virtual resources is realizable
    by proper and comprehensive adaptation of virtualization. As people find the Internet
    to be filled with various resources, according to the study, the ability to compete
    and trustworthy resources can foster reliable research [50]. Another study found
    that people were more likely to trust virtual resources from established sources
    [120]. These include well-known websites and significant technological companies.
    An online study of 357 social media users, especially Facebook, showed positive
    attitudes toward virtual news resources. People recognize the benefits of trusted
    sources in their development. However, misinformation is prevalent because of
    concerns regarding the adverse effects of exploiting virtual news resources to
    spread misinformation [121]. In addition, these attitudes are likely to continue
    to evolve as technology and the use of virtual resources continue to change. In
    addition, regardless of location, concerns about privacy, security, and the potential
    for addiction and negative impacts on mental health are increasingly being raised
    regarding virtual resources. Thus, global attitudes toward virtual resources are
    complex and nuanced, reflecting the benefits and challenges of this rapidly evolving
    technological area. C. Concept of Work Design The theory of work design is evolving
    and could be a good fit for some gaps in the BPM literature. Another study investigated
    the evolution of work design, contesting the belief that work design applications
    are limited to the manufacturing industry. Instead, they note that a global shift
    from manufacturing economies to service and knowledge economies has dramatically
    altered the nature of work in organizations [122]. This study proposes that work
    design theory and research are changing and shows two emerging viewpoints on work
    design: relational and proactive perspectives. Relational perspectives assert
    that increased interactions between co-workers and service recipients impact how
    jobs, roles, and tasks become more socially embedded. Comparatively, theoretical
    perspectives identify the importance of employees taking the initiative to predict
    and change how work is performed based on increasing market volatility and dynamism.
    These two concepts complement the BPM concepts, such as rule-based workflow automation.
    Work design is a consequence of process-improvement practices related to manufacturing
    [123]. Work design is critical for process improvement in the manufacturing industry.
    It involves designing and implementing workflows and processes that align with
    the business goals and strategies. In manufacturing, work design results from
    process improvement practices that streamline operations, reduce waste, and improve
    the efficiency and quality. The focus on process improvement in manufacturing
    is driven by the need to remain competitive in ever-evolving markets. Manufacturers
    constantly seek ways to reduce costs, improve product quality, and increase productivity.
    Work design plays a crucial role in achieving these goals, as it helps ensure
    that processes are optimized to meet the specific needs and requirements of the
    manufacturing environment. To achieve effective work design in manufacturing,
    it is essential to consider various factors, such as equipment and technology
    availability, human resources, and environmental impact. Process improvement practices
    in manufacturing often involve the adoption of new technologies, such as automation
    and robotics, which can significantly affect work design. These technologies can
    improve the speed and efficiency of the processes. However, they require careful
    consideration and integration into the existing workflows and processes. Another
    study discussed BPM theory and attempted to identify critical success factors
    in BPM projects [124]. This study proposes a framework for identifying these factors
    and measuring their impact, as this determines BPM failure. They also provide
    a method for identifying and measuring an organization’s BPM maturity, positing
    that this directly impacts the identified success factors. As they apply to BPM,
    the common theme among these three theories is the need for continual change.
    Organizations must change and adapt to new business practices, market conditions,
    technological advances, and the results of BPM program implementation. Critical
    success factors include transitioning from generalist workers to high specialization,
    appointment of process owners, and training and empowerment of employees. These
    factors represent work design elements that must be redesigned for the successful
    implementation of business process automation. The study does not investigate
    the use of virtual resources but cites leveraging innovation and best practices
    as a critical success factor. This strongly indicates that further research on
    work design and virtual resources would be beneficial. The work design theory
    can be expanded to include knowledge management [125]. Hence, they are more relevant
    to modern business environments. Their study challenges the traditional view that
    employees passively perform their assigned tasks. Instead, they investigate the
    types of proactive behaviors of individual employees at work and explore the impact
    of greater participation of employees in BPM and knowledge management processes.
    It builds on the finding of [122] a proactive perspective. While further research
    and expansion of work design theory are needed, this study clearly shows the link
    between work design and efficiency. 1) Foundations of Work Design Work design
    has been widely discussed and researched in the business and management fields.
    It refers to the planning, creation, and execution of tasks and activities essential
    for organizational operations and maintenance. It is a systematic approach for
    designing and optimizing work systems that includes the design of work tasks,
    processes, and systems integrated with organizational goals and objectives (see
    Figure 5). Traditional work design has evolved from Taylorist work design principles,
    which focus on efficiently using resources to maximize output and minimize costs
    [126]. In conventional work design, work is viewed as a series of tasks that can
    be broken down into smaller components and analyzed to determine the most efficient
    way to perform them. This approach led to the creation of standardized work procedures,
    defined roles and responsibilities, and machine-like operations to increase efficiency
    and productivity. However, the focus was on efficiency and productivity rather
    than on the satisfaction and well-being of workers. FIGURE 5. Illustration of
    proactive perceptiveness in work design. Show All The foundation of work design
    is to understand the coordination, performance, and relationships between tasks
    and activities in order to achieve goals. Work design aims to create an effective
    and efficient process that meets the needs of an organization and its stakeholders
    [8]. Workplace design has a long history in industrial and organizational psychology.
    In the early 20th century, researchers and practitioners focused on standardizing
    and optimizing work processes using mathematical models and time and motion studies
    to improve efficiency [127]. This work has led to the development of scientific
    management, a method of work design that focuses on the best ways to perform specific
    tasks. Over time, the focus of work design has shifted toward understanding the
    social and psychological aspects of work. Researchers have studied the impact
    of work design on employee satisfaction, motivation, and well-being and how these
    factors can affect the performance and success of an organization. This shift
    in focus led to the development of a human-centered approach to work design, which
    emphasized the importance of involving employees in the work design process and
    considering their needs and preferences. Traditionally, work design has focused
    on creating jobs and systems to improve work efficiency and effectiveness. However,
    in the age of technology, work design has expanded to encompass the use of technology
    to optimize work processes and systems. This was achieved by developing modern
    BPM systems that integrate virtualization, automation, and work design [21]. Thus,
    the traditional approach is continuously challenged by the advancement of technology
    and changing nature of work. The shift toward knowledge-based work, the increasing
    use of automation and digitalization, and the changing demographics of the workforce
    have led to the need for a more flexible and adaptive approach to work design
    [125]. In the context of BPM, modern workplace design must consider the use of
    virtualization, the integration of technology, and the need for collaboration
    and teamwork. 2) Work Design in the Age of Technology In the age of technology,
    work design refers to the creation, development, and maintenance of work systems,
    processes, and practices to improve productivity and efficiency. According to
    Parker and Grote, with the advent of technology, work design has taken on a new
    meaning, characterized by integrating technology into the working environment.
    Technology integration into work design has revolutionized work performance and
    increased efficiency and productivity. For example, automating processes through
    software, robotic systems, and machine learning has allowed businesses to streamline
    and standardize work processes. This reduces the time and effort required to complete
    tasks. Technology has also allowed it to gather, analyze, and act on data in real
    time. Hence, it provides a more comprehensive view of operations and enables businesses
    to make informed decisions. Workers should be placed at the center of investigating
    tools and methods that assist human labor in mitigating associated risks [128].
    This study established that workflow systems often ignore workers, which hinders
    effective work performance. They examined previously documented case studies and
    analyses of more recent workflow implementations. While there were cases that
    showed an initial disruption of work, changes to the design of the work and the
    configuration of the BPM toolset were able to remedy an issue. They also noted
    that these impacts can be minimized by effective knowledge management and staff
    empowerment. Interestingly, while this analysis does not explicitly include an
    investigation into work design, it does refer to changes that need to be made
    in the flow of work and system design. Another study conducted a statistical analysis
    of 324 companies by applying the critical success factors to BPM identified in
    a previous case study [129]. This step followed the operationalization of these
    factors by combining them into logical practices. They show that following these
    vital practices can improve the operationalization and adoption of BPM, which
    they describe as the business process orientation of the organization. They found
    that one of these critical practices was directly related to employee training
    and work practices. They noted that business processes traditionally expend time
    and resources dedicated to having management make decisions for workers. Organizations
    learn to automate these decision steps and alter employee practices to follow
    process decisions, rather than constantly referring to a supervisor. It follows
    the critical methods of employee training and empowerment. This study established
    the need for a work design to be considered. However, it falls short of investigating
    work design theory itself. The use of case studies to identify critical success
    factors and statistical analyses to identify essential practices were effective.
    This allowed them to combine both qualitative and quantitative research practices.
    These findings are now challenging to see process improvement practices from a
    broader range of business activities and BPM tools [130]. In the last five years,
    researchers have challenged the traditional work design theory. It is not necessarily
    a top-down process conducted by management because of the impact of external influences
    outside the control [131]. Thus, the work design can be completed systemically
    and modified as part of a dynamic business process. 3) Work Design in the Future
    of Artificial Intelligence (AI) Technological advancements have recently influenced
    work design, particularly the development of computers and information systems.
    Work design systems have become increasingly sophisticated owing to the emergence
    of artificial intelligence (AI). AI would be preferable in production as companies
    work toward achieving higher optimization of processes with an error-free rate
    [132]. This is because AI produces more during a significantly shorter period.
    AI will have lower fixed costs than human labor, requiring employment benefits
    and having time downs. AI’s fixed price is lower than that of human labor [133].
    Hence, this makes this futuristic technology preferable for productivity input,
    given significant advancements in robotics and ML technology for the future of
    work. Thus, AI can potentially transform the design and performance of a work.
    Virtualization, in particular, has the potential to revolutionize work design
    by allowing organizations to create virtual environments for modelling and testing
    work processes before execution. Work design foundations are rooted in understanding
    how work is performed and the relationships between tasks [122]. However, over
    time, the focus of work design has shifted toward a worker-centered approach,
    considering employee satisfaction and well-being. Recently, it has been influenced
    by technological advancements such as virtualization in BPM systems. Human labor
    is becoming more expensive, with a limited capacity to deliver productivity at
    levels that do not reduce the cost of business. Thus, organizations are researching
    other ways to meet these expectations. AI research and development are becoming
    a promising future for organizations, especially with regard to work design [134].
    AI in the future of workplace design has been a topic of considerable interest
    and debate in recent years. Integrating AI technology into business processes
    and work design can significantly change how work is performed and organized.
    The impact of AI on work design is expected to be both positive and negative.
    It depends on various factors, such as the nature of the task, level of AI integration,
    and skills and expertise of the workforce. One of the key benefits of AI in work
    design is its increased efficiency and productivity. AI technologies such as machine
    learning and natural language processing can automate routine tasks, freeing workers
    to focus on more complex and value-adding activities [111]. This can lead to reduced
    operational costs, improved output quality, increased worker satisfaction, and
    reduced turnover. In addition, it solves the problem of human labor, where efficiency
    and productivity increase until they become limited. Thus, it would require more
    people who must be highly skilled to solve this challenge at an increased production
    cost. However, the integration of AI into the work design raises several concerns.
    Some workers may feel threatened by potential job loss [135]. In addition, other
    workers may feel resentful about being replaced by machines. There are also ethical
    concerns regarding the use of AI in decision-making, particularly in areas where
    human judgment is critical. In addition to the benefits of increased efficiency
    and productivity, technology has enabled work design to be more adaptive and flexible.
    Work processes can evolve quickly as new technologies become available or business
    needs change [30]. This has resulted in greater agility and resilience under changing
    market conditions, allowing companies to respond quickly to new opportunities
    and challenges. Despite these benefits, integrating technology into work design
    has raised new challenges. One of the biggest challenges is ensuring that work
    systems remain user-friendly and accessible to employees regardless of their technical
    ability. In addition, the rapid pace of technological change can make work systems
    obsolete quickly, requiring businesses to continually invest in new technologies
    to stay competitive. In conclusion, work design in the age of technology has brought
    about significant changes in the business environment. This has increased efficiency,
    productivity, and adaptability. However, it has also created new challenges that
    businesses must address to stay competitive. Hence, ensuring the successful integration
    of AI into work design is essential by carefully considering the implications
    and risks associated with this technology. Developing broad and clear strategies
    for implementing AI technologies responsibly and ethically is vital [136]. It
    may involve collaboration with workers and other stakeholders and developing training
    programs and other support mechanisms to help workers adapt to new technologies.
    The impact of AI on work design is likely to be significant and wide ranging.
    It depends on various factors, including the nature of the task, level of AI integration,
    and skills and expertise of the workforce [137]. Therefore, organizations must
    strategically and responsibly integrate AI into work design to ensure that the
    benefits of AI are realized, and the potential risks are mitigated. 4) Key Steps
    in Adoption of Work Design Adopting work design principles can lead to numerous
    benefits for organizations. These benefits include increased productivity, employee
    satisfaction, and overall organizational performance [8]. The first step in adopting
    work design is to understand the current work environment. It includes identifying
    the tasks that employees perform, the resources available, and the challenges
    they face. In addition, organizations must identify the pain points and areas
    where employees struggle to effectively perform their tasks. Once the current
    work environment is understood, the next step is to define work objectives and
    goals. The action includes identifying the KPIs the organization wants to achieve
    through the work design adoption process [127]. These KPIs may include productivity,
    employee satisfaction, or customer satisfaction. The third step is to analyze
    the work processes. Organizations must identify the steps involved in performing
    each task and determine how to streamline these steps to make the process more
    efficient. Achieving this will entail eliminating unnecessary steps and automating
    the steps involved. Another step is to create job roles and responsibilities that
    align with work objectives and goals. Additionally, the step involves organizations
    defining the skills, knowledge, and experience required for each job role and
    each employee’s responsibilities. The stage consists of creating job descriptions
    and defining performance expectations for each job role. The fifth step is to
    design the work environment. This action involves creating a physical and virtual
    environment that enables employees to perform their tasks effectively [22]. This
    may include providing employees with the necessary tools and resources, such as
    computers, software, and equipment, and designing a workspace that supports collaboration
    and productivity. The final step is to implement and monitor the adoption of work
    design. Organizations need to implement new work designs and monitor the results
    to ensure that the work objectives and goals are achieved [88]. Hence, this may
    involve collecting data on KPIs, monitoring employee feedback, and adjusting work
    design as needed. Generally, work design adoption can benefit organizations, including
    increased productivity, employee satisfaction, and overall organizational performance.
    Thus, understanding the current work environment, defining work objectives and
    goals, analyzing work processes, creating job roles and responsibilities, designing
    the work environment, and implementing and monitoring work design adoption is
    vital [22]. These steps enable organizations to optimize their work environments
    and achieve their goals. Hence, these steps are critical to making work design
    adoption effective in allowing organizations to succeed in an increasingly competitive
    business environment. 5) Work Design Across Industries Work design optimizes processes,
    reduces waste, and improves efficiency and quality across various industries.
    Thus, the importance of work design is not limited to a single sector but is applicable
    across a wide range of industries [8]. These sectors include manufacturing, medicine,
    retail, finance, and government. a: Application of Work Design in the Manufacturing
    Industry Work design is a critical component of process improvement practices
    aimed at streamlining operations and reducing manufacturing costs. Effective work
    design in this sector often involves the integration of new technologies, such
    as automation and robotics, to improve the speed and efficiency of processes [57].
    Thus, it is crucial to optimize operations, reduce waste, and improve efficiency
    and quality. Effective work design in the manufacturing industry requires careful
    consideration of various factors, including equipment and technology availability,
    human resources, and environmental impacts [138]. In recent years, the integration
    of new technologies, such as automation and robotics, has significantly impacted
    work design in manufacturing. These technologies can substantially improve the
    speed and efficiency of processes. However, they require careful consideration
    and integration into the existing workflows and processes. An essential application
    of work design in the manufacturing industry is the adoption of lean methodologies
    [139]. This study established that lean methodologies are process improvement
    practices that reduce waste, improve quality, and increase efficiency. Work design
    is a critical component of lean methodologies, as it helps ensure that processes
    are optimized to meet the specific needs and requirements of the manufacturing
    environment. For example, lean methodologies could involve the elimination of
    non-value-adding activities [139]. Other benefits include standardizing processes
    and integrating new technologies such as automation and robotics. Another critical
    application of work design in the manufacturing industry is the integration of
    Industry 4.0 technologies [82]. Industry 4.0, such as the Internet of Things (IoT),
    AI, and ML, are revolutionizing the manufacturing industry. These technologies
    can improve efficiency, reduce costs, and significantly enhance quality. However,
    they require careful consideration and integration into the existing workflows
    and processes. Therefore, work design is a critical component of integrating Industry
    4.0, technologies in manufacturing, as it helps to ensure that these technologies
    are aligned with business goals and strategies. Work design is a critical component
    of process improvement in the manufacturing industry. Effective work design in
    manufacturing requires careful consideration of various factors, including integrating
    new technologies such as automation and robotics, to achieve optimal results.
    In addition, adopting lean methodologies and integrating Industry 4.0 technologies
    are critical applications of work design in the manufacturing industry to reduce
    waste, improve quality, and increase efficiency. b: Application of Work Design
    in the Medical Industry Healthcare Sector Work design plays a crucial role in
    improving patient outcomes and reducing costs in the health care sector. For example,
    effective work design in healthcare can involve streamlining patient flow and
    implementing process improvement practices, such as lean methodologies to reduce
    waste and improve quality [98]. Effective work design in the healthcare industry
    requires careful consideration of various factors, including patient needs, regulatory
    requirements, and their impact on healthcare professionals. In recent years, the
    integration of new technologies such as electronic health records (EHRs) has significantly
    impacted work design in healthcare [140]. These tools can significantly improve
    the speed and efficiency of these processes. However, they require careful consideration
    and integration into the existing workflows and processes. An essential application
    of work design in the healthcare industry is the adoption of lean methodologies.
    For example, lean methods can eliminate non-value-adding activities, standardize
    processes, and integrate new technologies, such as EHRs and telemedicine [25].
    Another critical application of work design in the healthcare industry is the
    integration of patient-centered care practices. Patient-centered care practices
    focus on patients’ needs and preferences, and aim to improve patient outcomes
    and satisfaction [60]. Work design is critical to patient-centered care practices,
    as it helps optimize processes to meet patients’ needs and requirements. For example,
    patient-centered care practices can involve the integration of patient feedback
    into process design, standardizing patient flow, and implementing process improvement
    practices. Hence, work design is critical for process improvement in the healthcare
    industry. Effective work design in healthcare requires careful consideration of
    various factors, including integrating new technologies such as EHRs, to achieve
    optimal results. Adopting lean methodologies and integrating patient-centered
    care practices are vital applications of work design in the healthcare industry.
    Its value is realized by improving patient outcomes, reducing costs, and increasing
    efficiency. Pharmaceutical Sector Work design plays a crucial role in the pharmaceutical
    industry, impacting the quality of products and overall efficiency of pharmaceutical
    organizations. Effective work design in the pharmaceutical industry requires careful
    consideration of various factors, including product safety, compliance with regulations,
    and impact on pharmaceutical professionals. In recent years, the integration of
    new technologies such as AI and IoT has significantly impacted work design in
    the pharmaceutical industry [82]. These technologies can considerably improve
    the speed and efficiency of these processes. However, they require careful consideration
    and integration into the existing workflows and processes. One essential application
    of work design in the pharmaceutical industry is the adoption of six-sigma methodologies.
    Six Sigma is a data-driven process improvement methodology that reduces defects
    and improves quality [123]. The study noted that work design is a critical component
    of Six Sigma. It is also established that work design ensures that processes are
    optimized to meet the specific needs and requirements of the pharmaceutical industry.
    Another crucial application of work design in the pharmaceutical industry is the
    improvement of the product development processes. Effective work design in the
    pharmaceutical industry can involve optimizing product development processes to
    reduce cycle times, improve product quality, and increase overall efficiency.
    c: Application of Work Design in the Retail Industry Proper work design frameworks
    are critical for the success of the retail industry. They enable organizations
    to optimize processes, improve efficiency, and enhance customer experience. The
    impact of proper work design frameworks in the retail sector can be significant
    and wide ranging [70]. Its effects are felt across all business areas, from the
    customer experience to the bottom line. First, this study noted that proper work
    design frameworks in the retail industry could improve customer experience. Organizations
    can streamline the customer journey by evaluating and optimizing processes, reducing
    wait times, and improving the overall experience. Thus, this can increase customer
    satisfaction and loyalty, positively impacting the bottom line through repeated
    business and positive word-of-mouth recommendations. Another impact of proper
    work design frameworks in the retail industry is the increased efficiency. Retailers
    can eliminate waste and reduce the time and resources required to complete tasks
    by evaluating and optimizing processes in their work design [141]. Hence, they
    can increase productivity, lower operating costs, and improve profitability. Additionally,
    proper work design frameworks can help organizations identify and prioritize opportunities
    for automation, which can further improve efficiency and reduce costs. In addition,
    implementing suitable work design frameworks can help organizations stay competitive
    in the ever-changing retail industry [70]. Retailers can quickly adapt to market
    and customer demand changes by continually evaluating and optimizing processes.
    It allows organizations to stay ahead of the curve and remain competitive in the
    face of growing competition. Hence, executing proper work design frameworks is
    critical for the success of the retail industry. By improving customer experiences,
    increasing efficiency, and helping organizations stay competitive, appropriate
    work design frameworks can significantly impact the bottom line and overall success
    of retail organizations. d: Application of Work Design in the Finance Industry
    Work design has profoundly impacted the finance industry, affecting various aspects
    from banking to insurance and everything in between, such as credit scoring. Thus,
    enhancing the industry’s talent management model is a reliable way to enhance
    competitiveness. This approach is reliable for ensuring an effective work design.
    It provides a structured approach for organizations to achieve this, especially
    in the banking sector. This has impacted the banking industry by improving the
    efficiency and effectiveness of customer-facing processes. By optimizing the processes,
    banks can reduce wait times, improve customer experiences, and increase overall
    customer satisfaction [78]. Work design has also helped banks streamline internal
    processes, thereby reducing the time and resources required to complete tasks.
    Thus, the ease of executing tasks such as assessing and approving loans has improved
    their bottom line, leading to increased productivity and profitability. Firms
    can efficiently conduct accurate and effective credit-scoring processes. Optimizing
    processes by adopting technology in their work design reduces the time and resources
    required to assess creditworthiness and customer experience [78]. Financial risks
    and reports have become easier to implement through proper auditing and assurance
    mechanisms. Other economic players, such as insurance firms, can rely on these
    reports to evaluate claims and policies to prepare comprehensive and accurate
    settlements. More reforms in the industry have become worker-centric, motivating
    and encouraging employees to perform better. e: Application of Work Design By
    the Government Government work design refers to defining, structuring, and executing
    work within government organizations. Over the years, work design in the government
    sector has undergone significant changes primarily driven by technological advancements,
    political landscape changes, and public service delivery expectations [76]. In
    the early days of government work design, tasks were performed manually, and employees
    were mainly responsible for clerical and administrative tasks. However, with the
    advent of new technologies, work design in government has become more sophisticated,
    and the focus has shifted from manual to automated processes. For instance, computerization
    and the Internet have enabled government organizations to streamline processes,
    increase efficiency, and improve the quality of service delivery. One example
    of the impact of work design on government is the adoption of e-governance. It
    refers to the use of information and communication technologies to provide public
    services electronically. E-governance has revolutionized how government agencies
    interact with citizens, significantly reducing the time and costs associated with
    manual processes [76]. In addition, the study showed that e-governance enables
    citizens to access public services online. These services include applying for
    passports, registering businesses, and paying taxes. Hence, it improves service
    delivery quality and increases citizen satisfaction. Another example of work design
    by the government is the adoption of agile methodologies. Agile methodologies
    are a set of practices and principles aimed at increasing the efficiency and flexibility
    of software development projects [26]. In the government sector, this study established
    that agile methodologies were applied to improve the design and delivery of public
    services. For instance, government organizations can use agile methodologies to
    prioritize and manage projects, allocate resources, and respond to changing requirements.
    Government work design has undergone significant changes over the years driven
    by advancements in technology, changes in the political landscape, and shifts
    in public service delivery expectations. Thus, the impact of proper work design
    frameworks in the government sector is significant, with improvements in efficiency,
    quality of service delivery, and citizen satisfaction [76]. However, there is
    still much work to be done to ensure that work design in the government continues
    to evolve and respond to the needs of citizens. 6) Global Attitudes Toward Work
    Design Work design is a crucial aspect of any workplace as it impacts employee
    satisfaction, productivity, and, ultimately, the success of a business. Thus,
    it is essential to assess how entities view work designs globally [21]. The study
    examines global attitudes toward work design for potential improvement of practices
    by one organization through comparison. Global attitudes toward work design consider
    factors such as importance, standard procedures, and their impact on employee
    satisfaction and productivity. The importance of work design varies globally depending
    on cultural and economic factors. For example, in developed countries, work design
    is often viewed as a critical component of job satisfaction and career growth
    [74]. Comparatively, in developing countries where employment opportunities may
    be limited, work design is seen as a means of securing employment and generating
    income. Work design is important and depends on the industry and job type. For
    example, work design is essential for fostering creativity, collaboration, and
    innovation in creative fields such as advertising and design. Standard work design
    practices vary globally; however, there are several universal principles. First,
    work design should involve job analysis, which includes the identification of
    job tasks, responsibilities, and requirements. Second, work design should aim
    to reduce physical strain, monotony, and workload imbalances. This can be achieved
    through job rotation, enlargement, or enrichment. Finally, work design should
    be flexible, allowing employees to adjust their work schedules to accommodate
    their personal and family responsibilities. Work design has a significant effect
    on employee satisfaction and productivity. A well-designed work environment leads
    to higher job satisfaction and engagement, reduced absenteeism, and increased
    productivity [8]. The effect of work design on productivity is particularly crucial
    in the current economic climate, where companies are looking for ways to improve
    efficiency and reduce costs. Work design can also impact employee retention rates,
    as employees who are satisfied with their jobs and work environments are likely
    to stay with the company. In conclusion, work design is a critical aspect of any
    workplace that impacts employee satisfaction, productivity, and ultimately, the
    success of a business. While attitudes toward work design vary globally, several
    universal principles are crucial for creating a well-designed work environment
    [21]. Companies that prioritize work design and create a flexible and engaging
    work environment are more likely to retain employees, boost productivity, and
    succeed in the long run. D. Research on the Relationship Between BPM, Virtualization,
    and Work Design The relationship among BPM, virtualization, and work design is
    complex and multifaceted [48]. BPM is a systematic approach to optimize and automate
    business processes. Simultaneously, virtualization involves creating virtual environments
    to run business processes. Work design refers to creating work systems to ensure
    that they effectively and efficiently meet organizational goals. Virtualization
    and BPM are closely related, focusing on optimizing and automating business processes
    [92]. Thus, virtualization creates environments that allow organizations to run
    business processes more efficiently and cost-effectively. BPM provides a framework
    for optimizing and automating these processes, enabling organizations to improve
    the speed and accuracy of transactions. Combining virtualization and BPM enables
    organizations to create a seamless and integrated environment for running business
    processes, reducing costs, and improving efficiency. Work design is also closely
    related to BPM and virtualization, as it is critical for ensuring that work systems
    are practical and efficient [97]. Technology is reshaping work and pushing it
    into the digital universe [21]. Hence, it is increasingly vital to reassess the
    work design with technological advancement. Work design is continually analyzing
    and optimizing work systems in virtualization and aligning this technology with
    organizational goals and meeting the needs of employees and customers. Hence,
    incorporating proper BPM system design and virtualization into work design will
    ensure that organizations improve their work environment design [142]. Moreover,
    this enhances their ability to meet the needs of employees and customers. Thus,
    the relationship among BPM, virtualization, and work design is complex and multifaceted.
    BPM and virtualization are closely related and focus on optimizing and automating
    business processes. At the same time, work design is critical in ensuring that
    work systems are practical and efficient. By combining BPM, virtualization, and
    work design, organizations can create an integrated and seamless environment for
    running business processes, improving efficiency, and reducing costs. 1) Influence
    of Virtualization on Work Design for Efficient BPM System Designs Virtualization
    concepts play a significant role in shaping work design in organizations [97].
    Virtualization technology has revolutionized how organizations manage their IT
    systems, allowing greater efficiency, flexibility, and security. Virtualization
    has also impacted work design, creating new opportunities for organizations to
    optimize their work processes and systems [100]. Cloud computing is a vital virtualization
    concept that influences the work design. Cloud computing has enabled organizations
    to use virtualization technology to create virtualized computing resources that
    can be delivered over the Internet. Cloud computing has enhanced access to computing
    resources on demand without requiring physical infrastructure [81]. It has transformed
    work design by enabling organizations to access computing resources from anywhere,
    at any time, allowing for greater flexibility and efficiency in work processes.
    Another virtualization concept that influences work design is containerization.
    Containerization refers to the use of virtualization technology to create isolated
    environments for applications and services. Another study found that this technology
    allows organizations to run multiple applications and services on a single physical
    server, improving efficiency and reducing costs [143]. Furthermore, containerization
    has enabled firms to deploy and manage applications and services more flexibly
    and scalable, allowing for more significant innovation in work design. Virtualization
    continually impacts the adoption of improved infrastructure in work design to
    respond better to utilizing virtual machines’ capacities [144]. The process allows
    organizations to run multiple virtual machines on a single physical server, reducing
    the costs of relying on multiple physical servers. This will enable organizations
    to optimize their work processes while saving time and space and reducing the
    vast management responsibilities required before integrating and adopting virtual
    resources. Virtual machines have enabled organizations to create virtual versions
    of physical devices and resources, allowing greater flexibility and innovation
    in work design [100]. Virtualization has played a crucial role in workplace design
    by creating environments that support business processes. The advancement of this
    technology and related concepts will likely continue to shape the future of organizational
    work design. This allows firms to use technology to develop new work designs and
    improve efficiency and effectiveness. Generally, modern BPM systems integrate
    virtualization and work design to enable organizations to streamline their work
    processes and improve productivity. This is achieved by automating work processes,
    reducing manual intervention, and allowing workers to focus on higher-level tasks
    [145]. The virtualization process in work design enables organizations to create
    flexible and scalable systems that respond to changing business needs and demands.
    The process improves value after integration into the work design because of its
    unlimited benefits, allowing firms to develop efficient and effective work processes
    [100]. These steps enable the agility of business responses to changing market
    demand. As technology evolves, work design is expected to become crucial in optimizing
    work processes and systems. 2) Experience Working With BPM System Designs The
    design of BPM systems has become a critical issue for organizations as it influences
    the overall effectiveness and success of BPM initiatives. One key aspect of the
    experience of working with the BPM system design is user satisfaction. Several
    studies have shown that user interface quality and ease of use are essential factors
    in determining user satisfaction with BPM systems. For example, a survey [38]
    found that most BPM users were satisfied with their systems, the most important
    being the ease of use and level of control over the process. This finding is supported
    by that of another study [146]. User satisfaction was positively correlated with
    the level of control over the process and ease of use of the BPM system. Another
    critical aspect of the experience of working with the BPM system design is the
    level of user engagement. One study [147] found that user engagement was a critical
    factor in the success of BPM initiatives. Engaged users are more likely to adopt
    and use the system effectively. This finding is supported by a study by [146],
    which found that user engagement is positively correlated with satisfaction. In
    addition to user satisfaction and engagement, the level of automation and integration
    with other systems are essential factors in the experience of working with BPM
    system designs. For example, one study [148] found that the level of automation
    was positively correlated with the level of user satisfaction and the success
    of the BPM initiative. Likewise, the study [55] found that integrating with other
    systems was positively correlated with the level of user engagement and the success
    of the BPM initiative. Another study surveyed 219 individuals working with the
    BPM system on a daily basis [149]. The research sought to investigate several
    areas of BPM, including applications used, level of maturity, realized benefits,
    and possible outsourcing opportunities. Outsourcing opportunities were included,
    representing one of the few studies that provides virtual resources as a primary
    consideration. As with previous research in this review, BPM is an ongoing consideration
    for organizations and must constantly change and evolve to provide strategic and
    operational benefits. Business process automation alone can provide a short-term
    competitive advantage. Still, it must be accompanied by more extensive organizational
    changes to deliver long-term benefits. A key finding was that most organizations
    did not achieve BPM maturity. Hence, while BPM practices are becoming more common,
    they are still in their infancy and very much considered in the formative stages
    of development. However, this insight into virtual resources is limited by outsourcing
    [149]. It does not include any other type of virtual resources. Over 25% of respondents
    attempted outsourcing. Its value varies according to the area in which the business
    is located. Respondents cited concerns about losing core competencies and corporate
    culture, and a lack of understanding of which business processes could benefit
    from outsourcing. This indicates that systems and work design changes would need
    to be in place to ensure that these concerns are addressed to take advantage of
    outsourcing. This research falls short of linking work design and virtual resources
    with BPM as a solution to some of the concerns raised by their study. Despite
    these concerns, BPM is evolving as more organizations reach increasing growth
    and understanding of its adoption, execution, and maintenance. Generally, the
    experience of working with BPM system designs is influenced by various factors,
    including user satisfaction, engagement, automation, and integration with other
    systems [8]. These findings suggest that organizations should focus on designing
    easy-to-use BPM systems. In addition, these techniques should provide a high level
    of control to users and be well integrated with other systems to ensure their
    success. In conclusion, the experience of working with BPM system designs is an
    important issue for organizations. Some studies have shown that the levels of
    user satisfaction, user engagement, automation, and integration with other systems
    are critical factors in determining the success of BPM initiatives. The results
    of these studies provide valuable insights for organizations seeking to improve
    their BPM systems and enhance their overall experience of working with these systems.
    3) Evolving Intersection of Organisational Culture and BPM System Design A study
    was conducted to explore the notion that organizational culture directly impacts
    BPM projects and their effectiveness [65]. This study found that factors external
    to processing operations can have a fundamental impact and should be considered
    when designing a BPM system. Further research is needed to explore the effects
    of external factors, such as work design, on BPM systems. Another study investigated
    the impact of changes in the business environment during the implementation of
    BPM and knowledge management projects. Human resources are critical success factors
    [79]. Specifically, these implementations were more successful when the process
    supported the workers’ ideas and actions, enabling them to participate [128].
    This indicates that the work design is a critical element in the practical application
    of the BPM and warrants further theoretical investigation. The relationship between
    organizational culture and BPM system design has been changing rapidly with the
    evolution of technology. Therefore, the intersection of these two aspects has
    become increasingly complex and intertwined. Thus, the impact of BPM system design
    on organizational culture and vice versa is becoming more pronounced. One of the
    critical studies in this area is the research paper by [65], which explores the
    impact of BPM system design on organizational culture. This study found that BPM
    system design can significantly impact corporate culture, particularly its impact
    on work processes, work design, and work behavior. For instance, the study found
    that BPM system design can influence the structure and norms of work processes,
    work design, and work behavior, shaping the organizational culture. Another study
    highlighting the impact of BPM system design on corporate culture [36] found that
    BPM system design can positively impact corporate culture by promoting teamwork,
    communication, and employee collaboration. Moreover, this study found that a well-designed
    BPM system can help improve organizational efficiency, transparency, and accountability,
    positively impacting corporate culture. The BPM evolved since its conception in
    the 1990s [129]. Early adopters have seen their responses in various industries
    and economic conditions, including bull and bear markets, and a global recession.
    Throughout the reviewed literature, an adaptable answer is vital for organizations
    to benefit from BPM system designs. Such agility is essential, as organizations
    are continuously impacted by legislative, competitive, economic, and social forces.
    Thus, their BPM practices must be flexible in order to provide them with competitive
    and strategic advantages. The rise and increasing level of adoption of virtual
    resources by organizations is undeniable and must be included in BPM considerations.
    Likewise, work design must be considered as BPM changes how organizations utilize
    employees. The consequences of work design theory are tangible, and the literature
    supports further investigation of the modernization of traditional concepts [150].
    Technology and modern business practices seem to innovate and change faster than
    research can conduct [55]. As such, there are significant gaps in the literature,
    and often, one component may be covered in the literature, but not in combinations.
    For example, the limited literature covers work design, virtual resources, and
    BPM. However, they were covered separately to varying degrees. As organizations
    achieve greater maturity in their BPM practices, greater insight has been available
    to researchers. The research suggested in this proposal follows from the reviewed
    literature. The reviewed studies have shown that BPM and work design theory must
    be updated to remain relevant to modern business. They also show that virtual
    resources are increasingly being utilized in ways that directly impact business
    processes, employees, and culture. However, there is a lack of research combining
    all these elements. Thus, further investigation is needed into the need for work
    design and virtual resources to be an architectural consideration of business
    process automation [150]. Today, the BPM system design can also impact the development
    of an organization’s strategy and decision-making processes. Research has shown
    that the integration of BPM system design and organizational culture can result
    in the creation of a culture of continuous improvement. For instance, integrating
    BPM system design and organizational culture can help organizations identify and
    address problems and opportunities for improvement [65]. This led to a continuous
    improvement cycle. The intersection between corporate culture and BPM system design
    is becoming increasingly complex and vital. The BPM system design can significantly
    impact organizational culture, including its impact on work processes, work design,
    work behavior, administrative efficiency, transparency, and accountability. Additionally,
    integrating BPM system design and corporate culture can create a culture of continuous
    improvement, leading to improved decision-making processes and better organizational
    outcomes. 4) Impact on BPM System Design From Dimensions of Work Design Work design
    has four main dimensions: organizations rely on to ensure cohesive, effective,
    and efficient cooperation among employees and stakeholders. These dimensions include
    job crafting, skill variety, autonomy, and feedback [151] (Figure 6). In today’s
    dynamic business environment, organizations must ensure that their employees’
    work optimizes their performance, job satisfaction, and organizational outcomes.
    Achieving these goals requires organizations to consider different dimensions
    of work design, including job crafting, task variety, autonomy, and feedback [8].
    Job crafting refers to the process by which employees proactively modify their
    job tasks and relationships with others to improve the fit between their skills
    and the demands of their roles. Job crafting was positively related to job satisfaction,
    engagement, and performance. Job crafting promotes the development of expertise,
    which can positively impact organizational outcomes [127]. In the context of the
    BPM system design, job crafting can help employees identify tasks that are more
    aligned with their skills and interests. This leads to increased engagement and
    job satisfaction. Furthermore, job crafting can enable employees to identify areas
    of the BPM system that can be improved, thereby leading to increased efficiency
    and effectiveness. FIGURE 6. Dimensions of work design. Show All Task variety,
    as a dimension of work design, refers to the degree to which a job involves various
    activities that require different skills and knowledge. Task variety was positively
    related to job satisfaction, engagement, and performance [8]. In addition, task
    variety has been shown to promote learning and development, which can positively
    impact organizational outcomes. In the BPM system design, task variety can enable
    employees to develop various skills and knowledge, thereby increasing job satisfaction
    and performance [125]. Additionally, task variety can ensure that employees are
    exposed to different aspects of the BPM system, leading to increased knowledge
    and understanding. Autonomy, another dimension of work design, refers to how employees
    control their work tasks, schedules, and methods. Independence was positively
    related to job satisfaction, engagement, and performance. Autonomy has also been
    shown to promote creativity and innovation, which can positively impact organizational
    outcomes [135]. In the context of the BPM system design, independence can enable
    employees to work on tasks aligned with their skills and interests, leading to
    increased job satisfaction and performance. In addition, freedom can promote creativity
    and innovation, enabling employees to identify and implement improvements in the
    BPM system. Finally, feedback refers to the degree to which employees receive
    information about their work performance and their impact on the organization.
    Feedback was positively related to job satisfaction, engagement, and performance
    [135]. Furthermore, feedback has been shown to promote learning and development,
    which can positively impact organizational outcomes. In the context of the BPM
    system design, feedback can enable employees to identify areas of the BPM system
    that require improvement, leading to increased efficiency and effectiveness [137].
    Hence, feedback would allow employees to understand the impact of their work on
    the organization, leading to increased job satisfaction and engagement. Taken
    together, the work design dimensions can have a significant effect on BPM system
    design. By incorporating job crafting, task variety, autonomy, and feedback into
    the design of the BPM system, organizations can promote employee engagement, job
    satisfaction, and performance. In addition, by facilitating learning and development,
    creativity, and innovation, and identifying areas for improvement, organizations
    can ensure that the BPM system is continuously optimized to meet the organization’s
    needs [141]. Thus, the dimensions of work design, including job crafting, task
    variety, autonomy, and feedback, can significantly impact BPM system design. Organizations
    can promote employee engagement, job satisfaction, and performance by incorporating
    these dimensions into the design of a BPM system. Meanwhile, they can ensure that
    the system is continuously optimized to meet the organization’s needs. As such,
    organizations should consider the dimensions of work design when designing. SECTION
    IV. Discussion and Future Research A. Discussion of the Findings This review explores
    virtualization and work design integration in BPM system design by analyzing the
    results of existing relevant studies in this field. This supports the essential
    role of BPM systems in supporting organizational processes and achieving business
    goals [21]. The findings of this study show that integrating virtualization and
    work design into BPM system design can enhance the flexibility, scalability, and
    agility of BPM systems. This will enable organizations to respond more effectively
    to changing business needs and market conditions. 1) Scalability One of the key
    benefits of integrating virtualization and work design in BPM system design is
    the ability to provide greater flexibility and scalability [41]. Virtualization
    technology allows resources to be allocated dynamically, enabling organizations
    to respond quickly to changing business demands. Work design principles can also
    be integrated into the BPM system design to ensure that tasks are assigned to
    maximize efficiency and reduce the risk of bottlenecks. One benefit derived from
    this principle is integrating virtualization, and work design in BPM system design
    is the ability to enhance agility [26]. Virtualization technology can enable organizations
    to rapidly deploy new systems and applications. Simultaneously, work design principles
    can be used to ensure that the processes are designed to be as flexible as possible.
    It can enable organizations to respond more effectively to market changes, customer
    demands, and new business opportunities. 2) Alignment to Strategy Studies have
    established that businesses need to develop a robust governance framework to effectively
    manage their BPM systems [125]. This study noted that a well-designed governance
    framework can ensure that the BPM system is aligned with the business’s strategic
    objectives and operates efficiently and effectively. Companies can achieve their
    goals by establishing clear roles and responsibilities. In addition, businesses
    can create policies and procedures and develop performance metrics to measure
    the efficacy of a BPM system [41]. Integrating virtualization and work design
    in BPM system design could produce an effective tool to drive business strategies
    to action. As such, businesses that invest in integrating virtualization and work
    design into their BPM system design will be prepared to compete in the modern
    business landscape [27]. 3) Resource Allocation One of the key findings of this
    research is that virtualization can greatly enhance BPM system design by enabling
    more flexible and efficient resource allocation. Thus, virtual machines would
    allow organizations to allocate resources on demand and reduce the costs of maintaining
    physical servers [5]. Moreover, virtualization allows for effortless scalability
    and disaster recovery, which are critical for ensuring business continuity in
    the face of unexpected disruptions. Work design integration can improve employee
    productivity and job satisfaction by enabling flexible work arrangements and task
    allocations [8]. 4) Culture & Engagement The study identified key factors contributing
    to successful virtualization and work design integration through a literature
    review and case study analysis. These include technical capabilities, organizational
    culture, and employee engagement [27]. Organizations can create a more engaged
    and motivated workforce by designing work processes around the capabilities and
    preferences of individual employees. In addition, work design integration can
    enable more efficient collaboration and knowledge sharing, which are essential
    for achieving BPM system goals [27]. Generally, the potential benefits of integrating
    virtualization and work design in BPM system design are significant [152]. Organizations
    that embrace these approaches will likely improve productivity, scalability, and
    employee engagement outcomes. B. Recommendations and Future Research Virtualization
    and work design integration in BPM system design enables organizations to achieve
    greater agility, flexibility, and efficiency while improving employee engagement
    and satisfaction. However, its successful implementation requires careful attention.
    The identified focus domains entail technical, organizational, and cultural factors,
    and ongoing monitoring and evaluation to achieve the desired outcomes [135]. Thus,
    future research should explore the long-term impact of virtualization and work
    design integration on BPM system performance, and the factors that influence the
    adoption and sustainability of these approaches. 1) Emerging Technologies Further
    studies could also investigate the use of emerging technologies, such as AI and
    blockchain, to enhance BPM system design and implementation [153]. This could
    include an empirical study incorporating diverse sample sizes to increase the
    study’s external validity compared to the currently available literature. This
    would allow for a more accurate representation of various firms’ BPM practices
    and experiences in different regions. 2) Case Studies of Active Businesses Another
    opportunity is that future research could be conducted in a real-world setting
    to complement the simulated environment used in most current literature. It would
    provide a more accurate assessment of the impact of virtualization and work design
    on BPM in an actual work environment. 3) Leadership, Culture & Engagement Finally,
    future research could explore the effects of other factors on BPM, such as the
    role of leadership, organizational culture, and employee engagement. These factors
    can be incorporated into the design of the BPM systems to improve their effectiveness
    and efficiency. Although this review offers valuable insights into the influence
    of virtualization and work design on BPM, there is ample room for further exploration.
    Future research can build on the findings of this study and expand the scope of
    knowledge on BPM. SECTION V. Conclusion The literature review highlighted the
    vital role of virtualization and work design in BPM system design. Virtualization
    and work design are essential components of BPM system design that can enhance
    the performance and effectiveness of these systems. Virtualization technology
    can improve the flexibility and scalability of BPM systems. By contrast, work
    design can improve employee satisfaction, productivity, and performance. The literature
    review shows that the BPM system design should be approached as a comprehensive
    process. This approach includes process modelling, automation, workflow management,
    integration, analytics and reporting, governance and compliance, and continuous
    process improvement. These components are critical for the successful development
    of BPM systems. This review also highlights the challenges in designing and implementing
    effective BPM systems. These challenges include managing complexity, ensuring
    system security, overcoming resistance to change, and balancing the technological
    and human factors. The literature review provides insights and best practices
    for addressing these challenges and designing BPM systems that meet the needs
    of modern organizations. Overall, this literature review emphasizes the importance
    of a holistic approach to BPM system design. Organizations that invest in designing
    effective BPM systems can improve their business processes, reduce costs, and
    increase customer satisfaction. Virtualization and work design are critical components
    of BPM system design that can help organizations achieve this goal. Authors Figures
    References Keywords Metrics More Like This A propaedeutic on logic for object-oriented
    systems analysis and design Proceedings of HICSS-29: 29th Hawaii International
    Conference on System Sciences Published: 1996 The System Analysis and Design Program
    IRE Transactions on Education Published: 1962 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'A Review on Business Process Management System Design: The Role of Virtualization
    and Work Design'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Srivastava S.
  - Gupta B.K.
  - Tandon D.
  - Tiwari K.
  - Raj A.
  - Agarwal M.
  citation_count: '0'
  description: Serverless computing has gained significant popularity in recent years
    due to its scalability, cost efficiency, and simplified development process. In
    a serverless environment, functions are the basic units of computation that are
    executed on-demand, without the need for provisioning and managing servers. However,
    efficiently triggering serverless functions remains a challenge, as traditional
    methodologies often suffer from latency, Time limit and scalability issues and
    the efficient execution and management of serverless functions heavily rely on
    effective triggering mechanisms. This research paper explores various design considerations
    and proposes a novel approach for designing efficient triggering mechanisms in
    serverless environments. By leveraging our proposed methodology, developers can
    efficiently trigger serverless functions in a variety of scenarios, including
    event-driven architectures, data processing pipelines, and web application backend.
  doi: 10.17762/ijritcc.v11i7.7846
  full_citation: '>'
  full_text: '>

    "Register Login International Journal on Recent and Innovation Trends in Computing
    and Communication Home Editorial Board Call For Papers For Authors Special Issue
    Archives Peer Review About Us Search HOME ARCHIVES VOL. 11 NO. 7 (2023) ARTICLES
    A Novel Approach for Triggering the Serverless Function in Serverless Environment
    Article Sidebar PDF DOI: https://doi.org/10.17762/ijritcc.v11i7.7846 Keywords:
    Serverless Computing, AWS, AWS Lambda, Cloud Computing, Serverless Functions Main
    Article Content Shashank Srivastava Author, DCSIS, IOT, Shri Ramswaroop Memorial
    University,Lucknow, Uttar Pradesh, India Bineet Kumar Gupta DCSIS, IOT, Shri Ramswaroop
    Memorial University,Lucknow, Uttar Pradesh, India Dheeraj Tandon Department of
    Computer Science and Engineering, SRM Institute of Science & Technology, Delhi
    - NCR Campus, Modinagar –Ghaziabad, Uttar Pradesh, India Kartikesh Tiwari DCSIS,
    IOT, Shri Ramswaroop Memorial University,Lucknow, Uttar Pradesh, India Anshita
    Raj DCSIS, IOT, Shri Ramswaroop Memorial University,Lucknow, Uttar Pradesh, India
    Megha Agarwal DCSIS, IOT, Shri Ramswaroop Memorial University,Lucknow, Uttar Pradesh,
    India Abstract Serverless computing has gained significant popularity in recent
    years due to its scalability, cost efficiency, and simplified development process.
    In a serverless environment, functions are the basic units of computation that
    are executed on-demand, without the need for provisioning and managing servers.
    However, efficiently triggering serverless functions remains a challenge, as traditional
    methodologies often suffer from latency, Time limit and scalability issues and
    the efficient execution and management of serverless functions heavily rely on
    effective triggering mechanisms. This research paper explores various design considerations
    and proposes a novel approach for designing efficient triggering mechanisms in
    serverless environments. By leveraging our proposed methodology, developers can
    efficiently trigger serverless functions in a variety of scenarios, including
    event-driven architectures, data processing pipelines, and web application backend.
    Article Details How to Cite Srivastava, S. ., Gupta, B. K. ., Tandon, D. ., Tiwari,
    K. ., Raj, A. ., & Agarwal, M. . (2023). A Novel Approach for Triggering the Serverless
    Function in Serverless Environment. International Journal on Recent and Innovation
    Trends in Computing and Communication, 11(7), 200–209. https://doi.org/10.17762/ijritcc.v11i7.7846
    More Citation Formats Issue Vol. 11 No. 7 (2023) Section Articles References Bhamare,
    S., & Patil, V. (2020). Function Placement and Triggering Techniques in Serverless
    Computing: A Survey. International Journal of Computer Applications,pp. 975-980.
    Mao, H., & Wang, J. (2019). Fine-Grained Function Triggering Mechanism in Serverless
    Computing. IEEE Transactions on Cloud Computing, 7(2), pp. 446-459. Zhang, H.,
    Huang, H., & Zhou, H. (2021). Intelligent Function Placement in Serverless Computing.
    IEEE Transactions on Services Computing, 14(4), pp. 656-669. P.G. López, M. Sánchez-Artigas,
    G. París, D.B. Pons, Á.R. Ollobarren, D.A. Pinto, Comparison of faas orchestration
    systems, in: 2018 IEEE/ACM International Conference on Utility and Cloud Computing
    Companion (UCC Companion), IEEE, 2018, pp. 148–153. D. Barcelona-Pons, P. García-López,
    A. Ruiz, A. Gómez-Gómez, G. París, M. Sánchez-Artigas, FaaS orchestration of parallel
    workloads, in: Proceedings of the 5th International Workshop on Serverless Computing,
    in: WOSC ’19, Association for Computing Machinery, New York, NY, USA, 2019, pp.
    25–30, http://dx.doi.org/10.1145/3366623.3368137 Chen, Y., & Chen, L. (2020).
    Survey and Taxonomy on Function Placement Strategies in Serverless Computing.
    IEEE Access, 8, 178530-178545. Shin, D., Yoo, S., & Woo, J. (2021). An Event-Driven
    Function Placement Algorithm for Optimizing Function Invocation Latency in Serverless
    Computing. Sensors, 21(1), 278. N.W. Paton, O. Díaz, Active database systems,
    ACM Comput. Surv. 31 (1) (1999) 63–103. C. Mitchell, R. Power, J. Li, Oolong:
    asynchronous distributed applications made easy, in: Proceedings of the Asia-Pacific
    Workshop on Systems, ACM, 2012, p. 11. S. Han, S. Ratnasamy, Large-scale computation
    not at the cost of expressiveness, in: Presented as Part of the 14th Workshop
    on Hot Topics in Operating Systems, 2013. A. Geppert, D. Tombros, Event-based
    distributed workflow execution with EVE, in: Middleware’98, Springer, 1998, pp.
    427–442. Bent AL-Huda Sahib Ghetran, Enas Abdul Hafedh Mohammed. (2023). Bayes
    Estimation of Parameters of the Kibble-Bivariate Gamma Distribution Under A Precautionary
    Loss Function for Fuzzy Data Using Simulation. International Journal of Intelligent
    Systems and Applications in Engineering, 11(2s), 373–380. Retrieved from https://ijisae.org/index.php/IJISAE/article/view/2733
    W. Chen, J. Wei, G. Wu, X. Qiao, Developing a concurrent service orchestration
    engine based on event-driven architecture, in: OTM Confederated International
    Conferences’’ on the Move to Meaningful Internet Systems’’, Springer, 2008, pp.
    675–690. W. Binder, I. Constantinescu, B. Faltings, Decentralized orchestration
    of composite web services, in: 2006 IEEE International Conference on Web Services
    (ICWS’06), IEEE, 2006, pp. 869–876. G. Li, H.-A. Jacobsen, Composite subscriptions
    in content-based publish/subscribe systems, in: ACM/IFIP/USENIX International
    Conference on Distributed Systems Platforms and Open Distributed Processing, Springer,
    2005, pp. 249–269. D. Dai, Y. Chen, D. Kimpe, R. Ross, Trigger-based incremental
    data processing with unified sync and async model, IEEE Trans. Cloud Comput. (2018).
    P. Soffer, A. Hinze, A. Koschmider, H. Ziekow, C. Di Ciccio, B. Koldehofe, O.
    Kopp, A. Jacobsen, J. Sürmeli, W. Song, From event streams to process models and
    back: Challenges and opportunities, Inf. Syst. 81 (2019) 181–200. I. Baldini,
    P. Cheng, S.J. Fink, N. Mitchell, V. Muthusamy, R. Rabbah, P. Suter, O. Tardieu,
    The serverless trilemma: Function composition for serverless computing, in: Proceedings
    of the 2017 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and
    Reflections on Programming and Software, Onward! 2017, 2017, pp. 89–103. Schad,
    J., Dittrich, J., & Quiané-Ruiz, J. A. (2018). Towards Function-as-a-Service:
    Perspectives on Serverless Computing. ACM Queue, 16(2), 70-111. B. Carver, J.
    Zhang, A. Wang, Y. Cheng, In search of a fast and efficient serverless DAG engine,
    2019, arxiv preprint arXiv:1910.05896. S. Joyner, M. MacCoss, C. Delimitrou, H.
    Weatherspoon, Ripple: A practical declarative programming framework for serverless
    compute, 2020, arxiv preprint arXiv:2001.00222. Prof. Parvaneh Basaligheh. (2020).
    Mining Of Deep Web Interfaces Using Multi Stage Web Crawler. International Journal
    of New Practices in Management and Engineering, 9(04), 11 - 16. Retrieved from
    http://ijnpme.org/index.php/IJNPME/article/view/94 M. Malawski, A. Gajek, A. Zima,
    B. Balis, K. Figiela, Serverless execution of scientific workflows: experiments
    with hyperflow, aws lambda and google cloud functions, Future Generation Comput.
    Syst. (ISSN: 0167- 739X) 110 (2020) 502–514, http://dx.doi.org/10.1016/j.future.2017.10.029,
    https://www.sciencedirect.com/science/article/pii/S0167739X1730047X. A. Jangda,
    D. Pinckney, Y. Brun, A. Guha, Formal foundations of serverless computing, Proc.
    ACM Program. Lang. 3 (OOPSLA) (2019) 1–26. E. Van Eyk, J. Grohmann, S. Eismann,
    A. Bauer, L. Versluis, L. Toader, N. Schmitt, N. Herbst, C. Abad, A. Iosup, The
    SPEC-RG reference architecture for FaaS: From microservices and containers to
    serverless platforms, IEEE Internet Comput. (2019). Mark White, Thomas Wood, Carlos
    Rodríguez, Pekka Koskinen, Jónsson Ólafur. Exploring Natural Language Processing
    in Educational Applications. Kuwait Journal of Machine Learning, 2(1). Retrieved
    from http://kuwaitjournals.com/index.php/kjml/article/view/168 S. Fouladi, F.
    Romero, D. Iter, Q. Li, S. Chatterjee, C. Kozyrakis, M. Zaharia, K. Winstein,
    From laptop to lambda: Outsourcing everyday jobs to thousands of transient functional
    containers, in: 2019 USENIX Annual Technical Conference (USENIX ATC 19), USENIX
    Association, Renton, WA, 2019, pp. 475–488, URL https://www.usenix.org/conference/atc19/presentation/
    fouladi. S. Burckhardt, C. Gillum, D. Justo, K. Kallas, C. McMahon, C.S. Meiklejohn,
    Serverless workflows with durable functions and netherite, 2021, arXiv: 2103.00033
    Fernández, P., Tordsson, J., & Elmroth, E. (2019). Event-Driven Function Placement
    in Serverless Computing. IEEE International Conference on Cloud Engineering (IC2E),
    32-37. Lu, Q., & Zhang, Z. (2020). Serverless Function Placement with Data Locality
    Optimization in Edge Computing Environment. Future Generation Computer Systems,
    108, 136-147. Wang, W., Zhang, S., & Liu, J. (2021). A Trigger-Based Serverless
    Function Placement Algorithm for Edge Computing. Journal of Parallel and Distributed
    Computing, 154, 139-148. Castro, P., & Cheng, L. T. (2019). Serverless computing:
    Present and future trends. Journal of Systems and Software, 157, 110381. Shahrad,
    M., Yarom, Y., & Falkner, N. (2019). Function Placement in Serverless Computing:
    From User Requirements to Serverless Frameworks. IEEE International Conference
    on Cloud Computing Technology and Science (CloudCom), 329-336. Bineet Kumar Gupta,
    et. al. “Integrated hesitant fuzzy-based decision-making framework for evaluating
    sustainable and renewable energy” in International Journal of Data Science and
    Analytics, ISSN 2364-4168, July 2023, Vlume 7(1), pp-1-12 https://doi.org/10.1007/s41060-023-00426-4,
    Kwame Boateng, Machine Learning-based Object Detection and Recognition in Autonomous
    Systems , Machine Learning Applications Conference Proceedings, Vol 3 2023. Bineet
    Kumar Gupta and Satya Bhushan“Containerization and its Architectures: A Study
    ”Advances in Distributed Computing and Artificial Intelligence Journal Regular
    Issue, Vol. 11 N. 4 (2022), pp-395-409, eISSN: 2255-2863, DOI: https://doi.org/10.14201/adcaij.28351
    Praveen Kumar Singh, Neeraj Kumar & Bineet Kumar Gupta \"Smart Card ID: An Evolving
    and Viable Technology\" International Journal of Advanced Computer Science and
    Applications (IJACSA), ISSN: 2158-107X, Volume 9 (3), pp. 114-124, April 2018.
    [34] N. Kumar & B. K. Gupta, E-Health Approach to Stipulate The Diabetic Patient
    Care and Management”, Value Health in The Journal of international Society for
    Pharmaeconomics and Outcomes Research, Volume 19, Issue 3, Page A211, May 2016,
    https://doi.org/10.1016/j.jval.2016.03.1281 N. Kumar, B.K. Gupta, V. Sharma, V.
    Dixit, and S.K. Singh, “E-Health: Stipulation of mobile phone technology in adolescent
    Diabetic Patient Care” Paediatric Diabetes, Jon Wily & Sons A/A, Volume 14(18),
    October 2013, ISSN P: 1399-543X, O: 1399-5448, p-90, 90. (DOI:10.1111/pedi.12075).
    Announcements Call for Papers March 7, 2024 Call for Papers for the New Issue.
    Last Date of Submission: April 30th, 2024 Imp. Announcement April 15, 2022 Dear
    Authors, We are feeling proud congratulations to all the contributors of IJRITCC.
    Because The \"International Journal on Recent and Innovation Trends in Computing
    and Communication\" has been accepted for Scopus. Citation Index Citation Indices
    All Since 2018 Citation 5854 3996 h-index 28 23 i10-index 119 72 Acceptance Rate
    (By Year) Year Rate 2019 12.6% 2018 18.3% 2017 16.9% 2016 18.8% 2015 22.9% 2014
    28.9% 2013 26.1% Important Links Home Aims and Scope Call for Papers Instructions
    for Authors Editorial Board Archive Download Ethics & Policies Publication Ethics
    and Publication Malpractice Statement Plagiarism Policy Copyright, Grants and
    Ownership Declaration Refund Policy Open Access Overview Open Access License Permissions
    Downloads Paper Template Indexed by Make a Submission Make a Submission Most Viewed
    Articles Today Research Paper on Basic of Artificial Neural Network 2713 A study
    and Comparative Analysis of HUL and ITC 2135 Access Android Device Using The FatRat
    and Metasploit 1681 Lift Control System Based on PLC 1356 Aspect-Based Sentiment
    Analysis using Machine Learning and Deep Learning Approaches 1298 Contact Us:
    Auricle Global Society of Education and Research Y-18-A, Near Sanskar Play School,
    Sudarshana Nagar, Bikaner, Rajasthan (India). Pin 334003 : editor@ijritcc.org
    Quick Links: Author''s Guideline Reviewers Guideline Peer Review and Publication
    Policy FAQ''s Privacy Policy Refund and Cancellation Policy Terms and Conditions"'
  inline_citation: '>'
  journal: International Journal on Recent and Innovation Trends in Computing and
    Communication
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Novel Approach for Triggering the Serverless Function in Serverless Environment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Blanco D.F.
  - Le Mouel F.
  - Lin T.
  - Escudie M.P.
  citation_count: '0'
  description: Over the last few decades, automotive embedded Information and Communication
    Technology (ICT) systems have been used to enhance vehicle performance and enrich
    peopleâ's driving experience, increasing the panel of software features within
    them. However, even though until now automakers have kept up with the innovation
    pace in terms of the functionalities that have been offered to passengers, the
    majority of automakers' efforts have concentrated on bringing in these new functionalities
    by adding an unceasingly larger set of ECUs. All of this has been done without
    evolving any of the embedded software architecture consequently, due to budgetary
    constraints, legislative limitations, retro-compatibility problems, and a lack
    of awareness of the trending IT innovation. This unbalanced progress has then
    led to a substantial increase in in-vehicle architectural complexity, which has
    become a major concern for automakers nowadays as it makes the vehicle repairing
    process more complex, decreases software traceability and clashes with the objective
    of having higher business flexibility, modularity, and dynamicity within the vehicles.
    In this paper, we are going to go through literature, both academic and industrial,
    and propose a comprehensive study into automotive system transformation. We begin
    by giving a detailed analysis of the causes of evolution under five axes - i.e.,
    society, business, industry, application, and technical. Then, we discuss the
    convergence of cars and software life cycles and propose a three-layered analysis
    of automotive ICT systems consisting of architecture design, software pipelines,
    and run-time management. Finally, we are going to propose certain detailed guidelines
    on the evolution perspectives for automotive systems through deriving from the
    convergence of advances in IT, as well as current and future automotive environmental
    constraints.
  doi: 10.1109/ACCESS.2023.3294256
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 11 A Comprehensive
    Survey on Software as a Service (SaaS) Transformation for the Automotive Systems
    Publisher: IEEE Cite This PDF David Fernández Blanco; Frédéric Le Mouël; Trista
    Lin; Marie-Pierre Escudié All Authors 1 Cites in Paper 943 Full Text Views Open
    Access Comment(s) Under a Creative Commons License Abstract Document Sections
    I. Introduction II. Literature Overview III. Use-Case Scenario: Software in the
    Automotive Industry IV. Causes of Evolution V. Automotive Application Life-Cycle:
    Conver-Gence and Open Issues Show Full Outline Authors Figures References Citations
    Keywords Metrics Footnotes Abstract: Over the last few decades, automotive embedded
    Information and Communication Technology (ICT) systems have been used to enhance
    vehicle performance and enrich peopleâ’s driving experience, increasing the panel
    of software features within them. However, even though until now automakers have
    kept up with the innovation pace in terms of the functionalities that have been
    offered to passengers, the majority of automakers’ efforts have concentrated on
    bringing in these new functionalities by adding an unceasingly larger set of ECUs.
    All of this has been done without evolving any of the embedded software architecture
    consequently, due to budgetary constraints, legislative limitations, retro-compatibility
    problems, and a lack of awareness of the trending IT innovation. This unbalanced
    progress has then led to a substantial increase in in-vehicle architectural complexity,
    which has become a major concern for automakers nowadays as it makes the vehicle
    repairing process more complex, decreases software traceability and clashes with
    the objective of having higher business flexibility, modularity, and dynamicity
    within the vehicles. In this paper, we are going to go through literature, both
    academic and industrial, and propose a comprehensive study into automotive system
    transformation. We begin by giving a detailed analysis of the causes of evolution
    under five axes - i.e., society, business, industry, application, and technical.
    Then, we discuss the convergence of cars and software life cycles and propose
    a three-layered analysis of automotive ICT systems consisting of architecture
    design, software pipelines, and run-time management. Finally, we are going to
    propose certain detailed guidelines on the evolution perspectives for automotive
    systems through deriving from the convergence of advances in IT, as well as current
    and future automotive environmental constraints. In this paper, we are going to
    go through literature, both academic and industrial, and propose a comprehensive
    study into automotive system transformation. We begin by g...View more Published
    in: IEEE Access ( Volume: 11) Page(s): 73688 - 73753 Date of Publication: 11 July
    2023 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2023.3294256 Publisher: IEEE
    Funding Agency: CCBY - IEEE is not the copyright holder of this material. Please
    follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain
    full-text articles and stipulations in the API documentation. SECTION I. Introduction
    As the cities and societies evolve to be more connected, accessible, and flexible,
    the automotive industry and the current perception of mobility services and vehicle
    ownership are following the same path (cf. §IV). Automobiles have become a part
    of a bigger and more complex network in which cooperative services and cloud-based
    applications combined with enhanced onboard computing capabilities are paving
    the way for innovation, moving towards a fully connected autonomous vehicle. Over
    the last few decades, Electronic Control Units (ECUs) have been used to ameliorate
    vehicle performances and enrich the driving experience by adding a multitude of
    new innovative services such as Over-The-Air Update Frameworks, Anti-Lock Braking
    Systems, embedded GPS systems, automated parking systems, and so on. These changes
    have shifted the value of digital features in vehicles by up to 35- 40% in today’s
    market [1], thus, becoming, for most people, a differential factor when it comes
    to buying a new vehicle. Henceforth, this trend is expected to continue increasing
    substantially in the coming years, making embedded vehicle architectures more
    flexible from a business point of view, enhancing the quantity of applications
    and customisation possibilities, and opening the existing collaboration frontiers
    of the automotive sector to new-coming IT companies such as Google, Facebook,
    Microsoft, or Amazon. However, even if until now the automotive industry has managed
    to keep pace with innovation in terms of the different functionalities offered
    to the passengers, all the efforts have been focused on launching these new functionalities
    by adding an unceasingly larger set of ECUs without evolving all the embedded
    software architecture consequently due to budgetary constraints, legislative limitations,
    retro-compatibility problems or a general lack of awareness regarding trending
    IT innovation [2]. This unbalanced progress has then, as we can see in Buckl et
    al. [3], considerably increased the complexity of the system and cost for both
    developing and maintaining new services, while complicating the path to evolve
    application development methodology and bring in new innovative software-related
    business proposals. In this context, adapting the current Electrical/Electronic
    (E/E) and Software architectures to the upcoming requirements, without forgetting
    about the already present base, has become a vital necessity, but also a major
    challenge for the automotive sector. As could not be otherwise, multiple industrial
    and academic researchers center themselves nowadays on trying to stage the phases
    of this transformation. On the one hand, if we look more closely at the industrial
    research initiatives, we can appreciate an increasing emergence of new standardization
    groups and projects such as SOAFEE, [4], Eclipse Software-Defined Vehicle (SDV),
    [5], Eclipse Automotive, [6], Android Automotive, [7], MIH EV, [8], eSync, [9]
    or Connected Vehicle Systems Alliance (COVESA) [10], previously called GENIVI
    Alliance, trying to establish some inter-company basis for the architectural transformation.
    However, these solutions are still at an early stage and no clear standards have
    been proposed as of yet. On the other hand, if we take an in-depth look at the
    current academic research contributions, considerable effort has been put into
    the economical and organisational aspects of software architecture transformation
    [11], the benefits and applicability of upcoming paradigms [12], [13], [14], [15],
    the evolution of embedded E/E architecture [16], [17], and the evolution of network
    architecture, either around communication protocols used in automotive embedded
    systems [18], [19], [20], [21], [22] or network evolvability and adaptability
    [23], [24], [25]. However, as we will detail in §II, there is a considerable research
    gap with regard to the study of the automotive ICT architectures as a whole (i.e.
    both software and E/E architecture, software integration pipelines and run-time
    control services) as well as the relationships between its layers, which makes
    it more difficult to apply the contributions to the automotive environment. Thus,
    in this paper, we will research the different, complete, or partial, software
    architecture related proposals from 1986 to early 2023, and give comprehensive
    guidelines for automotive architecture transformation, focusing on the analysis
    of software and system development life cycle (i.e., design, development, integration,
    testing, maintenance, etc.) and the whole panel of evolution causes, including
    societal evolutions, technical and application evolutions, business evolutions,
    etc. In addition, this paper has a second objective regarding converging the technical
    advancements of IT (especially those of Software as a Service (SaaS) / Platform
    as a Service (PaaS) [26] in Cloud Computing) and the constraints and needs, old
    or new, of the automotive systems in terms of flexibility, safety, security, dynamicity,
    etc. Hence, the main contributions of the paper are summarised as follows: A detailed
    analysis of the causes of evolution under five axes: societal evolutions (cf.
    §IV-A), business evolutions (cf. §IV-B), automaker design process evolutions (cf.
    §IV-C), the evolution of application profiles and requirements (cf. §IV-D), and
    technical causes of evolution (cf. §IV-E). An analysis of the IT and automotive
    literature around a three macro-theme classification deriving from the convergence
    of the vehicle product and software life cycles (cf. §V). Thus, these three macro-themes
    are Architecture Design (cf. §VI), Software Pipelines (cf. §VII) and Run-time
    Management (cf. §VIII). The presentation of our complete vision for future automotive
    software architecture models, evolution perspectives and future work for the automotive
    systems deriving from the convergence between advances in IT and current and future
    automotive environments and constraints (cf. §IX). The remainder of this paper
    is organised as follows: First, Section II gives a high-level analysis of the
    current literature research trends. Then, Section III presents a detailed scenario
    of the uses of software in the automotive industry to illustrate the resulting
    challenges of this transformation. Subsequently, Section IV details the causes
    leading to this unprecedented automotive IT evolution. After that, Section V discusses
    the convergence on software life cycle between the vehicles product and software
    life cycles, proposing a new division in three macro-themes and their associated
    open issues, that will be explained in more details later in Section VI - Architecture
    design, Section VII - Software Pipelines and Section VIII - Run-time Management.
    After that, Section IX will summarise all the studies in a detailed literature-based
    architecture evolution proposal, as well as detailing future associated works.
    Finally, Section X will present the conclusions of our study. SECTION II. Literature
    Overview Many articles discuss the distinct aspects of software transformation
    within automotive systems, both on-board and off-board. In this section, we want
    to present the different main research trends in automotive ICT systems as well
    as the evolution of their presence in renamed IT and automotive conferences1 and
    journals.2 The results of this study of over 600 papers spanning the last decade,
    including some of those presented throughout this paper, are presented in Fig.
    1. Thus, in the following paragraphs of this section we will investigate each
    of these research subjects one by one, presenting their most representative works.
    After that, we will precisely position the scope of our survey with regard to
    the rest of the state of the art. FIGURE 1. Popularity of the different academic
    research topics interest in the automotive domain since 2015. Show All From Fig.
    1, we can highlight that the sectors whose interest has increased more over the
    last years are those concerning the V2X (either V2C, V2V or V2I) and the enhanced
    ADAS systems, closely followed by those focusing on software development & architecture
    paradigms, which is coherent with the evolution of the business and user requirements
    and interests presented later on this paper (cf. §IV-B). For the rest of the section,
    we will give an overview of each of the categories, giving some references that
    illustrate them. First, the energy consumption and energy efficiency of vehicles
    has been addressed both at a low-level manner [27], [28], [29], [30] by seeking
    to optimize the consumption of different parts of the vehicle (i.e. engine, telecommunications
    unit…) and, also, at a higher level manner through route optimization, collaborative
    coordination, smart parking optimizations… [31], [32], [33], [34], [35]. Secondly,
    the software related functional advancements with regard to mobility, the context
    heterogeneity and the integration of the vehicle with the users, the network and
    other smart-city infrastructures, have been addressed separately depending on
    whether the application runs exclusively within the vehicle [36], [37], in coordination
    with other vehicles or infrastructure (V2X) [38], [39], or in coordination with
    the Cloud servers (V2C) [40], [41], either the central or edge nodes. Moreover,
    applications are usually grouped by their final objective such as entertainment
    (IVI) [42], [43] or driving assistance (ADAS) [44], [45]. Besides, several contributions
    appeared to target the evolution of software development methodologies, with the
    adaptation of the classic V-cycles and the agile manifest [46], [47], [48] and
    philosophies [49], [50]. Third, Security and Safety properties have been widely
    covered, both isolated and combined, through multiple surveys. In-vehicle has
    been addressed from a purely software design perspective in [51], [52], and [53],
    but also from a hardware point of view in [54], [55], and [56]. Similarly, security
    has also been covered domain by domain, first focusing on the network [57], [58],
    [59], [60], then on the application design [61], [62]. However, despite the fact
    that these two properties are usually studied separately, they are often addressed
    as an ensemble with regard to system threat analysis and risk assessment [63],
    [64]. Finally, the Hardware (E/E) architecture evolution, including more centralized
    and performant architectures, has been discussed in multiple surveys before [65],
    [66], [67], [68], [69], however, we haven’t found any contribution that goes into
    detail about the hardware characteristics of the nodes, only the way to distribute
    them. However, despite these advances, little information can be found concerning
    the study of both the E/E and Software automotive architectures as a whole, while,
    to the best of our knowledge, the study of integration between these features
    through the vehicle life cycle and the interactions in between remains completely
    unexplored. Thus, in this paper, we will focus on covering these missing spots,
    as well as surveying each of the mechanisms independently, in order to give an
    extensive view of the transformation progress and options in the automotive sector.
    In addition, we will add supplementary reviews of proposals from the IT sector,
    which leads this digital transition, all the while keeping in mind the importance
    of safety and security concerns required by automotive legislation and certificates.
    Finally, we will try to propose a theoretical reference for architecture and guidelines
    for fully dynamic and connected automotive architectures. This is one of the key
    development trends of the sector and will be explained in more detail in §IV.
    SECTION III. Use-Case Scenario: Software in the Automotive Industry In this section,
    we provide a detailed use-case scenario (cf. Fig. 2) depicting the complexity
    of the software life cycle through all the vehicle software life-cycle phases
    (i.e., Development, Deployment, and Maintenance). This is a simplification of
    the life cycle presented in §V, leaving all the economic aspects aside. First,
    it is worth noting that this scenario was elaborated based on the literature [70],
    [71] and from the innovation reports from several automaker groups (e.g., STELLANTIS,
    General Motors, Toyota, BMW…), who all share similar internal processes. FIGURE
    2. Software life-cycle through the vehicle states use case scenario. Show All
    To fully illustrate and highlight the complexity of this cycle, we are going to
    focus on the most frequent case, an application developed by a supplier (or content
    provider) dependent on specific hardware, shared between multiple car models within
    the same automaker, and that provides recent updates regularly. Some example applications
    would be the rear camera controller application or the lidar & radar collection
    application. In this case, the study market tendencies (cf. Fig. 2–4), automaker
    demands (cf. Fig. 2–3), and the analysis of the standards (cf. Fig. 2–2) and regulations
    (cf. Fig. 2–1) are carried out by the supplier before developing (cf. Fig. 2–5)
    the product. After that, the supplier will work directly with the automaker to
    integrate everything (cf. Fig. 2–6,7,8) into the rest of the in-vehicle architecture,
    elaborating the different installations, testing and monitoring instructions specific
    to this application and for each necessary update, in order to adhere to the security
    certificates and legislative constraints. The cycle continues after with the deployment
    phase when these applications are deployed, first, in the vehicle production phase
    (cf. Fig. 2–9) using local fast installation tooling (cf. Fig. 2–10) and will
    be updated remotely (or locally in a garage) for the subsequent updates (cf. Fig.
    2–11,12). After being successfully installed and tested (Fig. 2–13), the monitoring
    and data collection instructions for this application will be added to the global
    maintenance tool (cf. Fig. 2–14), thus, being able to manage its correct functioning
    on the road. However, this cycle does not always take place like this. The party
    developing the software can also be the same automaker or even an independent
    company wanting to sell a product instead of a supplier under a contract. Besides,
    the software can be deployed at distinct stages (i.e., Over-The-Air on the road,
    locally in an after-sales garage or on the production line) and with a periodic
    update that can range from never updating the software once installed to updating
    it monthly. This causes a hard-to-handle heterogeneity on the integration, certification,
    installation, and maintenance pipelines, since every application is designed by
    a different organisation without following a standardised procedure that, in turn,
    making the in-vehicle systems more complex. Note that, in this environment, the
    heterogeneity on the different vehicle variants and user preferences also has
    a negative impact on the complexity of the system, especially whenever deploying
    new software. However, even though, up until now, current innovation pace has
    kept up with user expectations, given the recent evolution of user requirements
    (cf. §IV-A), this is no longer the case. The role of the vehicle is changing and,
    thus, business dynamicity (cf. §IV-B) needs to evolve along with it. In this context,
    and as we mentioned in the introduction, vehicles will need to be able to add
    all these new applications with different profiles (cf. §IV-D), coming from both
    internal and third party application stores, remotely and with a higher dynamicity
    (cf. §IV-C), without being limited by the lack of network or hardware capacities,
    upcoming standards and regulations, integration constraints, nor safety and security
    needs. This evolution involves a major change in perspective within the vehicle
    industry. Extra effort will be needed to keep pace with with the software and
    E/E architecture needs and to simplify software development, delivery, and maintenance,
    while all the time keeping in mind that critical safety and security levels must
    be respected (cf. §IV-E). However, as the need for defining standardised pipelines
    for application deployment and maintenance is a past challenge for the IT industry
    [72], [73], this problem has been widely addressed in research on the cloud computer
    field, first for homogeneous [74], [75], and more recently heterogeneous environments
    [76]. Thus, the automotive industry should take advantage of these advancements
    to make its ICT systems and software life cycle management less complex. On the
    other hand, despite the lack of modern software standards, from this use case
    we can also highlight that the Automotive industry maintains strong fundamentals
    regarding how to manage the integration of legacy systems and highly heterogeneous
    applications coming from completely different suppliers, all the while keeping
    the system safe and secure [77], [78]. Hence, over the last few decades, in-vehicle
    software and systems have focused on safety and security and not on dynamicity
    and flexibility, which is a recent trend for the automotive industry. Maintenance
    and integration costs are nowadays too high, raising the need for a review into
    in-vehicle ICT systems. SECTION IV. Causes of Evolution As we said before, the
    use of software has enabled vehicles to become more connected, autonomous, and
    efficient. However, the transformation of the automotive sector is not solely
    due to technological advancements, but rather a combination of factors. This section
    will provide a detailed analysis of the causes of evolution in the automotive
    industry, specifically focusing on societal factors, business factors, automaker
    design process factors, application profiles and requirements factors, and technical
    factors. The objective of this section is to give a better understanding of the
    driving forces behind the transformation of the automotive sector and how they
    will influence the future of the automotive sector. A. Societal Evolutions In
    recent years, and as we previously stated, cities and societies have evolved to
    be more connected, accessible, and flexible and, with it, the role of the vehicle
    and the behavior of the new generations of customers have drastically shifted.
    Even though the nature of these societal changes are multifaceted, at a high level
    of abstraction, we can highlight some of the principal interesting changes such
    as a rising interest in mobility instead of car ownership (cf. §IV-A1), a decreasing
    interest in driving while in the vehicle (cf. §IV-A2), a shift in the general
    perception of the vehicle (cf. §IV-A3), and a convergence toward a greener vehicle
    production and usage model (cf. §IV-A4). These four principal interests converge
    into the industrial guiding megatrends for vehicle transformation C.A.S.E. (cf.
    Fig. 3). FIGURE 3. Automotive technology mega-trends described through C.A.S.E
    (Connected Autonomous Shared Electrified) acronym. Show All 1) From Car Ownership
    to Occasional Use The way young generations feel about vehicles in developed countries,
    mostly those in urban areas, has progressively shifted due to multiple factors.
    Among these factors, we can include the development of urban transportation networks
    (bus, subway, train, bicycles, taxis…) in most cities in developed countries [79],
    [80], increasing car usage restrictions [81], a lack of parking space [82], [83],
    the environmental impact of using and producing private transport [84], [85],
    elevated maintenance [86], [87], gasoline [88], insurance costs [89], etc…Indeed,
    studies [90], [91] show that interest in car ownership has drastically decreased
    among the young population strata. For example, new vehicle purchases per-capita
    for 18- to 54-year-olds in the US has declined more than 15% within the last 20
    years. Studies [92], [93], [94] show that financial issues, a lack of need for
    a vehicle, mostly in urban areas, and a rising concern for the environment are
    the major causes that delay or rule out having a car or even a driving license.
    Moreover, studies suggest that car use has reached its peak in the last decade
    [95], [96] leading to a phenomena of voluntary demotorisation and reduced car
    ownership, particularly in urban areas [97], [98]. Thus, as car ownership is considered
    to be more and more of a struggle, motivated by the aforementioned factors, among
    the younger population [99], on-demand car-ride services (such as Uber, Lyft or
    Bolt), carpooling services (such as BlaBlaCar or Via) and car-sharing services
    (such as Citiz, Ouicar or Zipcar) [100], [101], [102], have experienced a rapid
    growth. This growth has led some traditional automaker companies to join this
    Mobility-as-a-Service market trend, such as STELLANTIS with the acquisition of
    Share Now [103] or BMW with Drive Now [104]. This trend of using vehicles as punctual,
    flexible, shared resources within the smart-city ecosystem is expected to keep
    growing from its current 8% of total global miles travelled to over 26% in 2030
    [94]. Having a car population consisting of a high percentage of publicly shared
    vehicles would enhance the value of some already present research topics such
    as dynamic driver profiling [105], pay-as-you-go applications [106] or dynamic
    software-defined vehicle systems [107], all the while redirecting the business’s
    focus towards mobility instead of vehicle production (cf. §IV-C). However, other
    studies [108] highlight the extent to which giving up car ownership can be a stigmatising
    factor for rural areas [92] or developing countries [109], in which there are
    no real mobility alternatives because of a lack of infrastructure or country instability.
    2) Decreasing Interest in Driving In addition to the changes presented previously
    in this section, which suggest a fall in car ownership among younger generations,
    people also seem to have changed their behaviour related to car use. Driven by
    the Fear of Missing Out (FoMO) [110] phenomenon, new generations dislike more
    and more tasks in which they have to invest all their attention for a long period
    of time, such as driving or the process of buying the car itself [99]. In addition
    to that, teleworking democratisation [111] and rising real estate prices within
    urban areas [112], [113] are leading to an increase in the home-to-work distance
    people are willing to assume, which can be see with the recent exodus towards
    peri-urban or rural areas. This distance increase is especially remarkable withing
    the lower strata of society [114], [115]. As the travel distance increases, which
    increases as well the massiveness of daily traffic jams [116] and road stress,
    it also deteriorates the image of driving. Which has lead to what is perhaps one
    of the biggest differences between the older and recent generations with regards
    to driving, whilst previous generations perceive driving as a fun activity, recent
    generations perceive driving as a chore [99], something perfunctory. In addition,
    with the evolution of network coverage and the connectivity and the IT capabilities
    of smart-phones, laptops and the vehicles themselves [16], [117], numerous opportunities
    for virtual relationships, online shopping and other remote activities, such as
    working or entertainment, so many activities can now be carried out without physically
    moving, even when on the road [118]. Therefore, the possibility of multitasking
    with your smartphone while going to work, school, or elsewhere is viewed as one
    of the main reasons for taking public transport [119] and is a desired feature
    for drivers [120], [121]. Semi-and-full-autonomous vehicles will therefore help
    increase traffic efficiency and travel time, allowing drivers to multitask while
    on the road, while increasing traffic and energy efficiency [122], etc. 3) Shift
    on the Vehicle Symbolism Since the invention of cars during the last few decades
    of the XIXth century to the early XXth century, vehicles were a rare asset [123]
    and were viewed as the ultimate symbol of wealth, freedom, independence and autonomy
    [118]. This turned them into a capital status symbol all around the world. However,
    as mass production spread, focusing on functional cost-efficient vehicles further
    down the class hierarchy, mere ownership lost its ability to convey distinction.
    Thus, a clear distinction appears between luxury vehicles, which are nowadays
    affordable for a small percentage of the population and are still viewed as status
    symbols worldwide [123], and functional cars. Today, given the advances in mobile
    phones and internet, smartphones arguably provide as much freedom as cars, offering
    instantaneous access to information, family, friends, shopping…which, also helped
    by progressively better urban transport availability, is responsible for fewer
    trips being made, and, therefore, for the decrease in ownership interest [123]
    unless we are talking about vehicle necessity (i.e. family needs [124], uncovered
    rural areas [93], etc.). Hence, we can say that vehicle perception is changing
    from being a status symbol, towards utility [125]. However, in developing countries,
    given the elevated price of vehicles and the poor public transport infrastructures,
    vehicles are still rare and considered by the populations both a necessity and
    a status symbol [126], [127], [128]. Moreover, users are questioning identification
    as they now identify less with communities or brands and therefore want their
    car to be more unique and tailored to their individual needs [129], [130], [131].
    According to this concept, products and services should not be adapted to consumer
    segments, but rather to the preferences of individual consumers. However, customisation
    also has a negative effect on customer acceptance, as each product needs to be
    adapted for a specific personality [132], which brings higher costs. Therefore,
    a balance needs to be found between consumer acceptance and satisfaction of individual
    customisation. This challenge is even bigger when we consider the previous trends
    of shared driving or carpooling services. 4) Through a Greener Vehicle Production
    and Usage The 5th and 6th IPCC Climate Change Report [133], [134], as well as
    the Paris Agreement Goals adopted at COP21 [135], expose the unbalanced footprint
    within the different main sectors of activity, with regard to resource needs,
    biodiversity impact, energy consumption, GHG emissions, etc. From all the previously
    cited factors, we are going to focus in detail on the most frequently studied
    GHG emissions. The transport sector accounted for 16.2% of the total GHG emitted
    per year, with the largest part being(11.9%) produced directly from road transport
    [133], the majority of that coming from passenger travel (i.e., cars, motorcycles
    and buses). However, this statistic only shows the impact of vehicles once they
    are on the road, completely ignoring the costs associated with the impact on the
    rest of the vehicle product life cycle (i.e. production phase, maintenance, end
    of life, etc.) [136], [137], [138]. On one hand, with regard exclusively to the
    usage-related impact, technological breakthroughs in the internal combustion engine
    and migration to other power sources such as electricity or hydrogen fuel are
    driving down the emissions from vehicle driving [139], [140], [141], [142]. However,
    in order for these changes to be sustainable in the long term, further technical
    and societal advancements are required in subjects such as electricity decarbonisation
    [143], [144] or battery recycling / reuse [141], [145]. On the other hand, with
    regard to vehicle lifecycle itself, studies show contrasting results on whether
    extending the vehicle lifetime would positively impact its footprint. Those in
    favor of extending it contend that by extending their lifetime, we would decrease
    the production of new vehicles [146], in turn, helping to reduce the C O 2 incidental
    from manufacturing new products. This argument also goes along with extending
    vehicle hardware / software repairability and upgradability meaning users would
    not need to change their vehicles once they are no longer functioning at an optimum
    level. Contrariwise, those against extending it [147], [148], [149], [150] claim
    that slowing the pace of product replacement will leave many older, less energy-efficient
    products in society, which will increase CO2 emissions incidental to product usage.
    However, in this case, what comes after the end of life of the vehicles is of
    critical importance increasing the need for higher re-use and shifting from linear
    to circular economies to be sustainable in the long time [151]. As may be the
    case, both these options rely on higher software and hardware compatibility and
    a separate management of hardware and software life cycles, moving closer to plug
    & play vehicle dynamics. Finally, the critical environmental situation does not
    seem to be a differential factor for people when choosing whether to buy a vehicle
    or not [93]. As we said before, the need to have a car is the principal deciding
    factor for car ownership. However, despite it not being the main factor for consumers,
    it is a critical point for both Governments and Automakers, with multiple brands
    being transformed to full electric and many subsidies proposed by developed countries
    to motivate the population to move towards greener engines. B. Business Evolutions
    As society and user interests evolve through time to become a highly connected,
    autonomous, shared and electrified ecosystem (cf. §IV-A), new business opportunities
    arise [152], transforming the whole business ecosystem, which is critical for
    the traditional automotive manufacturers and suppliers in order to adapt to the
    current trends to maintain or better their current market position [152], [153],
    [154]. In this section, we propose an analysis of the business ecosystem focusing
    separately on two issues, first on product transformation opportunities (cf. §IV-B1)
    and then, on the company strategic aspects (cf. §IV-B2). 1) Product Transformation
    First, we need to highlight the shift in perceptions of the customer. While until
    now, the customer’s role was limited to buying what Automotive manufacturers were
    offering, studies on co-creation processes [155], [156] have shown that adding
    digital interactions with the customers, integrating them into the processes of
    product design, is essential to rapidly sense and respond to changing customer
    needs, which has become vital for the survival of organisations in the digital
    age [157]. Secondly, if we take a closer look at the business consequences of
    societal transformations (cf. §IV-A), both shared-mobility services and autonomous
    driving mean a substantial change in the value proposition. On one hand, shared
    mobility changes the scope from delivering a product (the vehicle) towards also
    delivering a service (mobility). This shift in scope has had significant implications
    [154], [158], [159], [160], [161] on both the automaker’s revenue model, which
    will likely be altered as these services comprise of pay-per-use pricing schemes,
    and the partnership model which has needed to be extended towards public transport
    providers. Moreover, Mobility as a Service redefines the traditional 1:1 relationship
    between vehicle and customer leading to an n:n type of relationship, which is
    a potential available business market for new innovative contributions. On the
    other hand, autonomous driving also substantially changes the value proposal as
    the customer no longer needs to drive the vehicle. Hence, while on the road, there
    is an available spot for innovation in both entertainment, comfort and, potentially,
    in many other sectors. Finally, we feel it important to highlight that, due to
    the aforementioned reasons, the integration of the vehicle into Smart City environments
    and enhanced connectivity evolution trends, vehicles are a major innovation target
    for new IT-related services [162], [163], [164], whether in entertainment, comfort,
    driving assistance etc. These new software-based business opportunities will become
    a new important form of revenue for both automakers and suppliers, relying once
    again on subscriptions and pay-per-use economic models. In this context, software
    deployment systems, such as OTA updates, have an even greater importance than
    before. Moreover, these services tend to produce a large amount of data [165],
    [166] that needs to be stored and that can be analysed and used for other business
    purposes, such as optimisation of vehicle diagnostic processes, custom advertising
    etc. and that can be exploited economically as is done in multiple other domains
    [167], [168]. 2) Strategic Aspects As software transformation opens the pre-established
    automaker industry borders and enlarges its evolution perspectives and market
    opportunities [152], [153], [169], powerful non-traditional players, such as IT
    companies like Apple or Google, progressively enter into the automotive ecosystem,
    enforcing competition for both suppliers and the automakers themselves. Thus,
    traditional players see their position threatened as this transformation implies
    a major update, not only of their products, but also their internal processes,
    product life cycles and development methodologies (cf. §IV-C). Besides, traditional
    players may no longer have the full in-house competences to develop these new
    IT-related products and services, needing to restructure their workforce or collaborate
    with other IT-centered companies. Thus, in order to preserve the stability within
    their companies and comply with legislation, fully restructuring a company workforce
    and internal processes cannot be immediately done, making inter-company co-operation
    and outsourcing crucial in the coming years [170], [171], [172], [173], [174].
    C. Automotive Design Process Evolutions Motivated by societal evolution trends
    (cf. §IV-A) and business evolution perspectives (cf. §IV-B), automakers need to
    update their internal development and design processes to match the dynamicity
    required by the digital age, as they now have a decoupled life cycle between software
    and hardware and an unexploited innovation panel to cover. In this section, we
    will first (a) reevaluate the design principles on which automakers are based,
    and then (b) revisit the progress on their updates in development methodology.
    It is worth noting that with regard to these points, we will compare these advances
    to those in the IT industry and comment on their integration possibilities. 1)
    Design Process Re-Evaluation: Top-Down VS Bottom-Up As depicted in Fig. 4, a software
    block can be seen as a mapping between functional (business, user, etc.) requirements
    (at the top level of abstraction) and the technical bases (at the bottom level
    of abstraction) that make them possible [175], [176]. This mapping is complex
    and is done through multiple levels of specifications, certificates, algorithms,
    concepts etc., that can be done through multiple solutions. This mapping can be
    done from either side [177], [178], [179], either with the functional requirements
    mapped directly over the technical layer, which is normally called top-down and
    is the preferred model within the industry, or the technical advancements converted
    into a product afterwards, which is usually the case for the IT sector. If we
    focus once again on Fig. 4, the functional requirements at the top represent long-term
    wishes and dreams whilst the bottom side is made up from the existing real options.
    Therefore, someone who strongly focuses on the top part of this figure would often
    fail to deliver a result that can be considered feasible, needing to delay and
    re-adapt their products, which is often the case in the actual Automotive sector.
    On the other hand, someone focusing on the bottom part would struggle to stay
    within budget requirements, producing lots of impractical and redundant things.
    Thus, the key to long-term success is to keep both perspectives in balance, maintaining
    both strategic and operational aspects with similar priority. This way, a balanced
    or hybrid top-down-bottom-up design approach will combine the requirements from
    both sides to quickly produce useful results and continuously adapt to the changing
    demands by evolving this solid operational foundation [178], [180], [181]. However,
    this balance is complicated to establish in the automotive industry, since the
    time scale for the inter-layer propagation is often delayed due to certificates,
    legislation validation etc. Therefore, a full revisit of the traditional automotive
    development methods is needed to enable them to get in line with this trend. FIGURE
    4. Software life-cycle through the vehicle states. Show All 2) Revisiting Software
    Development Methodology Classical, traditional or sequential development method-
    ologies (such as the V-Model [182] or the Waterfall model [183]) have been wandering
    around IT and industry for the last decades [182], [184], [185]. These models
    rely on a clear statement: all processes involved during the development of a
    product are phase-to-phase dependent on each other, needing to end the precedent
    process before starting the next one. Furthermore, these models also focus on
    offering detailed documentation after each of the phases to enhance action traceability.
    However, as these models were designed for short end-to-end industrial projects,
    they have considerable flaws in long-term/complex projects in which requirements
    may evolve overtime. Therefore, as a first step through more adaptable development
    methodologies, some multi-stage sequential models appeared (such as Rapid-Application
    Development [186]) which tried to solve the lack of adaptability by separating
    the whole set of requirements into sub-modules, meaning that each cycle was smaller
    and easier to manage. However, even though these proposals were able to address
    the dynamicity problems of sequential models for medium / small projects, they
    added elevated planning costs and parallel development issues. Consequently, as
    a way to unify all multi-stage incremental models and offer a more flexible and
    cost-effective paradigm, a group of developers released, in 2001, the Agile Manifesto,
    from which most of the modern development models, such as eXtreme Programming
    [187], Scrum [188], or Kanban [189] have derived. Despite the variations between
    them, these models are nowadays dominant within most IT companies worldwide and
    are characterized by the same principles of flexibility, dynamicity, cost effectiveness,
    adaptation to constant user and client feedback and fast development. However,
    as these models are mostly designed for software, they don’t focus on specific
    automotive safety and quality standards (such as ISO/IEC 15504, ISO 21434 Cybersecurity,
    or ISO 26262 Safety SW), and thus need to be revisited by the automotive industry.
    More specifically, if we focus now on the automotive domain, as the traditional
    V-Model was not designed with critical safety and quality in mind, and as Automakers
    tended to focus on the core competencies when developing cars as a whole, needing
    to outsource a considerable part of the work, they set up Automotive SPICE (ASPICE)
    [46]. This was was started as an initiative to determine the capability of the
    suppliers to fit these high-end requirements and standards [47], [190]. ASPICE
    can simply be seen as an extension of the V-Model compliant with the previously
    mentioned automotive standards and certificates. However, as the ASPICE standard
    was defined prior to the software boom in the automotive systems, it still has
    an archaic vision of software dynamicity and high coupling between the hardware
    and software life cycles, which makes the software innovation more complicated
    [191]. Thus, in this context, new initiatives [46], [191] are pushing towards
    a merger between IT-oriented Agile models and strong automotive requirements,
    however there is not yet a clear standard proposal for automakers to follow. D.
    Evolution of Application Profiles and Requirements If we map together now all
    the societal and business changes (cf. §IV-B & IV-A), the upcoming and mostly
    research-driven functional evolutions (cf. §II), as well as those already deployed
    in vehicles (cf. §III), we are able to propose a simple application profile classification
    using four distinct categories: (1) Static Driving Real-Time Services (SDRTS),
    (2) Collaborative (V2V/V2X) Services, (3) HMI/ Infotainment/Data Collection Services,
    and (4) Remote services (V2C). Table 1 shows a detailed description of each of
    the categories based on their network-related, software-related and hardware-related
    requirements and characteristics. Static Driving Real-Time Services (SDRTS): This
    service category was originally made up of a group of small signal-based and safety-critical
    applications (highly prone to jitters and latency) which were used to handle everything
    related to the driving of the vehicle. However, with time, these services have
    been added on top of higher-level complex services that complement the easy control
    tasks with higher driving enhancement mechanisms (ADAS). These services are normally
    statically orchestrated since they are needed for the vehicle to work properly
    and safely, which means they are running most of the time. Collaborative (V2V
    / V2X) Services: This category consists of highly dynamic collaborative services
    computed simultaneously in multiple vehicles and infrastructures depending on
    the environment. These services, coming mainly from the newly shared driving use
    cases, need high system dynamicity and flexibility since they may need to be put
    into motion at any moment depending on the vehicle-fleet status. Among these services,
    we can also find those that are not directly related to driving, which have more
    relaxed constraints, such as vehicle infrastructure log collection or global traffic
    optimization algorithms. HMI / Infotainment / Data Collection Services: This third
    category comprises the most varied set of services and, as you can see in Table
    1, we have split them into 3 different subcategories: (i) those focusing on passenger
    comfort, which are normally low-complex functions such as winding the car window
    up or down, changing the position of the mirrors…(ii) those focusing on the Infotainment,
    and therefore those that are closer to the classic IT services, needing normally
    high networking and computing capabilities and (iii) those related to data collection,
    either with business, technical or legislation purposes. This last sub-category
    is similar to the one in the Remote Services category but keeps the data stored
    locally instead of in the cloud. Remote services (V2C): This last kind is composed
    of all the services that have a direct interaction with the cloud, whether it’s
    an offloaded cloud function, in which case the on-board local function will not
    need many resources, or if it’s a remotely triggered function, such as tele-diagnosis,
    vehicle configuration… TABLE 1 Automotive Software Application Profiles Detailed
    Characteristics Therefore, if we now combine the requirements expressed in Table
    1 with the aforementioned research evolution perspectives (cf. §II), we can hypothesise
    about future technical requirements and any blocking points associated. The incessant
    integration of V2X functions raises questions about a higher connectivity of the
    vehicle with its surroundings and, at the same time, the need for software flexibility
    and dynamicity to be able to efficiently orchestrate highly dynamic services whenever
    they are needed, without over-consuming resources along the way. On the other
    hand, the evolution of the ADAS mechanisms, being suddenly based on complex neuronal
    networks and combining services from different domains, confirm the need for higher
    computing processors and more powerful onboard networking. Nonetheless, as more
    and more ADAS safety critical functions appear, so does the need for inter-service
    isolation, strong control services and standardised network middle-wares. If we
    look now at the last category, the software development & architecture paradigms,
    it raises, once again, the need for standardisation and flexibility, all the while
    respecting the safety and security needs of the vehicle. E. Technical Causes of
    Evolution Now, we have exposed the diverse needs coming from new application advancements,
    societal changes, business opportunities and design & development process evolutions,
    we can oversee the technical conflicts needing urgent evolution within the current
    on-board systems. To begin, we need to remind you that embedded ICT architecture
    is often classified in two layers: hardware architecture and software architecture.
    While the hardware architecture is composed of all the Electrical/Electronic (E/E)
    resources deployed in the vehicle i.e., ECUs, Power Sources, Communication supports…-,
    the software architecture includes all the high-level factors -such as business
    strategy, human dynamics, design, development cycle…- and all software specific
    details - such as its granularity, interactions, control services, execution environment
    or system security. Fig. 5 shows a high-level simplified vision of each of these
    layers in current embedded vehicle architecture, and how they interact with the
    cloud support layer. FIGURE 5. Simple overview of the current Vehicle embedded
    & Cloud arch. Show All 1) Hardware Architecture Evolution Conflicts With the raising
    levels of vehicle automation cars have progressively needed more computational
    power, which has ended up with a massive increase in the number of ECUs, which
    leads to unmanageable system complexity and is a struggle to keep growing at the
    same pace the application demands. Current cars are equipped with a mix 70 to
    100 restrained ECUs, either Microcontrollers (MCUs) or Microprocessors (MPUs),
    connected through a highly heterogeneous set of communication supports, both wired
    (CAN, LIN, FlexRay, Ethernet…) and wireless (Bluetooth, LTE-M…) [66], [192], [193],
    [194]. In addition to the previous problems, the heterogeneity and limited capacities
    of this infrastructure also present an obstacle regarding resource efficiency,
    space harness efficiency, and energy consumption. Thus, it seems of the utmost
    importance that the automotive industry reduces the number of ECUs, by merging
    various mixed-critical applications into multi-core SOCs and standardizing the
    network connections. Moreover, the lack of E/E standards between car models [195],
    even within the same company, complicates the reuse and optimization of the development
    processes, both in terms of time, energy, and money, while also having a negative
    impact on the planet. Adding to this problem, we can also highlight the lack of
    clear long-term architectural strategy [196] and the need to integrate black-box
    solutions coming from a wide set of suppliers [66], which hinders the definition
    of homogeneous standards for all in-vehicle archs. Finally, as a way to keep the
    vehicle updated through the years, and to use less energy and resources while
    having less of a negative impact on the environment, vehicles need to solve the
    current compatibility problems and integration costs linked to plugging new hardware
    into its original embedded systems [197], allowing them to add the new trending
    innovations, reinforcing their original ICT systems. 2) Software Architecture
    Evolution Conflicts As the trend is to evolve through a more flexible and customizable
    architecture, in which each driver could set-up their own vehicle preferences,
    use some pay-as-you-go subscriptions to core vehicle functionalities and move
    with their own profile from their personal vehicle to other vehicles, such as
    their work vehicle or even a rental vehicle, the embedded software architecture
    still has a lot to go through. To begin with, one of the main problems affecting
    this evolution is high coupling in the current vehicle ICT systems between the
    hardware and the software [12], [198], both for internally developed software
    applications and those coming from suppliers. This coupling also hinders testing
    requirements, as testing is required for the integration of each software repeatedly
    for each different hardware variant, as well as limiting the software reuse, resulting
    in a higher dev. cost. In addition to that, another serious problem of the current
    software architecture is linked to the possibility of installing new software
    on user’s demand in a safe and secure way [199], [200], [201], [202]. This problem
    derives from the aforementioned E/E system heterogeneity, as well as from a lack
    of standardized middleware and control services, the complexity orchestration
    in such environments, and the current testing infrastructure and legislation constraints.
    Besides, the use of virtualization, which may relax this integration stress, is
    under-exploited, being used exclusively for security and business segregation
    concerns. Another stumbling block for the current software architecture evolution
    is the integration of the legacy software components into this new deployment
    phase, which is critical to ensure an affordable transition from the current to
    the new generation ICT systems. Finally, from a network point of view, current
    architecture needs to find a way to standardize the communication for all software
    blocks, enabling it to guarantee Quality-of-Service (QoS) and safety deadlines
    despite the software dynamicity [21], [203], all the while remaining focused on
    the pricing gap between network cables. Furthermore, we cannot forget the interaction
    with the new support infrastructures such as the V2X [204], [205] or the V2C [206],
    [207], [208], [209], either in Classic, Fog or Edge clouds, which can also be
    used to run in-vehicle functions, acting as a support, while also being a more
    resource efficient manner to run the necessary soft. However, vehicles need to
    evolve their connectivity capabilities to be able to match strict deadlines as
    well as dynamically integrate the available external resources collaboratively
    into their orchestrator. SECTION V. Automotive Application Life-Cycle: Conver-Gence
    and Open Issues Due to the rapid integration of software into modern automobiles,
    the convergence of automotive product and software life cycles has become increasingly
    important in recent years. As a result, the traditional linear approach to product
    development has been changed to integrate both hardware and software processes.
    This section details the result of this convergence between the automotive product
    and software life cycles, including the challenges that arise from said convergence.
    We will also examine the open issues that are yet to be addressed in order to
    achieve a successful integration of hardware and software in the automotive industry.
    A. Convergence of Automotive Product and Software Life-Cycles In this subsection,
    we will delve into the convergence of the automotive product and software life
    cycles. We will examine how the traditional product development process is being
    transformed by the incorporation of software and how new methodologies are being
    developed to integrate hardware and software. Furthermore, in Fig. 6, we propose
    a merger between these two life cycles, with the objective of completing the previous
    use-case scenario (cf. §III). FIGURE 6. Convergence between vehicle product and
    software life-cycles. Show All Thus, if we focus first on surveying the vehicle
    life cycle from a product point of view, we expand on the literature proposals
    made in [21], [78], [210], and [211] and also integrate the participants and phases
    linked to the distribution and end-of-life of the vehicle. Hence, as you can see
    in Fig. 6, the vehicle life cycle is composed of six different participants -
    i.e., automakers (or vehicle manufacturer), suppliers, logistics companies (transports,
    distributors…), car owners, maintenance and recycling plants - and represented
    as a finite-state machine model, where each state represents a phase in the life
    cycle of the vehicle - i.e. design, manufacturing, transporting, selling, operation,
    maintenance and end-of-life disposal. It is worth noting that for each of these
    states, a player is associated with and has exclusive access to the vehicle during
    this phase. Moreover, in Fig. 6, you can also see a high-level mapping of the
    automotive software life cycle through the different phases of the product life
    cycle, firstly, consisting of an architecture design phase, a second software
    development and pipelines phase, and a final run-time management phase. However,
    even though this mapping makes it easier to understand how the software life cycle
    is set out through the manufacturing process, it is still unclear in which part
    the different blocks of the vehicle software (i.e., the software architecture,
    the software integration pipelines and the run-time management / control services)
    are implemented or interact with each other. Thus, to complete this section, and
    as we want to look at the automotive systems mainly from an IT point of view,
    we will structure the main part of the survey following these three sections:
    A first architecture design trend (cf. §VI) including all the changes grouping
    the architectural evolutions for all four; E/E architecture (cf. §VI-A), Software
    architecture (cf. §VI-B), Network architecture (cf. §VI-C) and Internet-of-Vehicles
    platform (cf. §VI-D). A second trend concerning all the software integration pipeline
    evolutions (cf. §VII) for both initial vehicle production and on-the road management.
    In this second phase we will detail the changes in the development (cf. §VII-A),
    delivery (cf. §VII-B), deployment (cf. §VII-C), testing (cf. §VII-D), and orchestration
    (cf. §VII-E) processes, allowing the software to be dynamically integrated into
    any vehicle software context. A final trend concerning all the run-time management
    services (cf. §VIII) that ensure the proper functioning of the systems, as well
    as the compliance with the different safety and security certificates and legislation.
    In this section and taking into account that the number of possible Control Services
    is too extensive, we will then focus on the mechanisms of tackling the security
    and safety of the in-vehicle and off-board systems and software and those centred
    on the data. B. Open Issues Prior to beginning the detailed state-of-the-art analysis,
    in this subsection, Table 2 summarises the open issues that need to be addressed
    through automotive software transformation. This table includes the issues concerning
    all four networking, architecture, control, and application and indicates in which
    of the three macro-trends of this survey (i.e., Architecture in §VI, Software
    Integration Pipelines in §VII and Control Services §VIII) each of them will be
    addressed, or partially addressed. However, note that, as there are plenty of
    issues, this table contain those targeting mostly the global architecture and,
    although other smaller or less general issues do exist, they are not mentioned
    in this table but presented individually in the concerned section. TABLE 2 Summary
    of the Open Issues That Need to be Addressed Through Software Transformation.
    Classification Made From the Aggregated Data From the Papers [22], [48], [65],
    [71], [192], [194], [212], [213], [214], [198], [215], [216], [217], [218], [219],
    [220], [221], [222] SECTION VI. Architecture Design Traditionally, in the automotive
    sector, all the levels of architecture design were grouped under the definition
    of E/E Architecture, which was seen as a convergence of electronics hardware,
    network communications, software applications and wiring into one integrated system.
    However, this definition is shallow and not precise enough for the current context,
    in which software and off-board architectures are steadily growing in importance.
    Thus, in this section and as a way to narrow this definition, and the brief one
    in §IV-E, we will split the architecture design into four distinct categories
    - i.e., Electric/Electronic Architecture, Software Architecture, Network Architecture,
    and Internet-of-Vehicles Platform. For each subsection, we will go through an
    extensive analysis of the state-of-the-art features and initiate a discussion
    about the evolution perspectives and threats. Moreover, for each of these categories,
    we have classified their sub-parts according to their main properties - i.e.,
    processing, in-vehicle communication, energy supply and hardware modularity &
    reusability for the E/E Architectures, software re-use, dynamicity, flexibility,
    safety, security, maintainability & complexity for the software architectures,
    standardisation and data-management for the network architectures and cloud interaction,
    Vehicle-to-Vehicle, Vehicle-to-Everything interactions for the IoV platform. A.
    Electric/Electronic (E/E) Architectures To fully address the challenges facing
    E/E architectures we are going to conduct a four-layered analysis starting by
    taking an in-depth look at the state of technology presentation of the Electronic
    control units (ECUs) possibilities, to address the processing capabilities of
    the system. This is then followed by another analysis of network cables, targeting,
    this time, the in-vehicle communication properties. Then, we will focus on the
    external energy, delving deeper into the power supply infrastructures and, finally,
    we will focus on the hardware modularity and reusability by giving an extensive
    analysis of the automotive hardware reference architectures and their evolutions.
    As you will see all through this section it is worth noting that E/E evolutions
    are especially limited with regard to security & safety certificates (such as
    ISO 26262 [223] or ISO 21434 [224]). 1) Processing Properties: ECU Evolution Given
    the previously presented context, automakers are forced to evolve their traditional
    ECUs so that they can deliver more performance, functionality, and flexibility,
    all the while reducing component cost and size, lowering power consumption and
    footprint, as well as meeting the stringent requirements regarding reliability,
    robustness etc. All of this must be done despite the punishing operating conditions
    as described in the standards, legislation, and certificates. In addition, they
    are required to operate correctly, without needing to be changed until they have
    functioned under high material stress conditions (i.e., rain, with elevated temperatures…)
    over many years. In order to accurately choose the new set of ECUs for their new
    generation on-board systems, automakers have a wide range of choices to choose
    from depending on the target functionalities that the systems need to provide.
    Firstly, and the most cost-effective and simplest option, we have micro-controllers
    (MCUs). Micro-controllers typically use on-chip highly constrained flash memories
    in which its main program is stored and executed. This guarantees higher energy
    efficiency as well as a shorter start-up period [232], [233], [234]. Most MCUs
    available on the market have few Mega Bytes of Program memory, which is undoubtedly
    a limiting factor when trying to operate with more complex applications. However,
    these kinds of boards are interesting for low-functional use-cases that need low
    energy consumption and a fast/resilient start, such as the applications within
    §IV-D - Static Driving Real-time Services > Low complex services. Some examples
    from this category that are dominating the automotive market are the Infineon
    Aurix [235], [236], the Renesas RH850 [237], [238] or the NXP S32K [239]. On the
    other hand, if we now turn to more complex and high-end boards, we have microprocessors
    (MPUs), which don’t have the same memory constraints as the MCUs. They separate
    non-volatile memory such as NAND or serial flash from volatile memory, traditionally
    DRAM. In the former they store programs and data, whereas in the latter the start-up
    files are loaded [232], [233], [234]. These kinds of nodes are then, given their
    higher complexity and number of interfaces, slower to start and have a higher
    energy consumption, however they do allow significantly more complex functions
    to run. These MPUs are often complemented with other extra chip-sets (as is also
    done less often for MCUs), higher computing, network or graphic capabilities becoming
    Systems-on-Chip (SoCs), or clustered together with other MPUs and MCUs from Chiplets.
    These boards will then become an interesting choice for both general purpose functions
    and functions with specific needs, even being able to substitute MCUs in most
    cases, thanks to tools such as virtualisation. Further in-depth analysis will
    be carried out on this topic in this paper in §7.3. If we focus on different features,
    some widespread examples would be the NVIDIA Jetson AGX Xavier [240] for High-Performance
    Computing (HPCs), Qualcomm Gen2/3/4 [241] for Graphical Processing Units (GPUs)
    or Mobileye 6-Series [242] for AI-based functions relying on Neuronal Network
    Processing (NPUs). Thus, summing up, there is a wide variety of potential ECUs
    available from different suppliers, already compliant with legislation and certificates,
    and even in academic research [243], [244], [245], that could be used for the
    new software enhanced automotive systems. However, their specifications must be
    carefully chosen considering the trade-off between application/system needs and
    system monetary and energetic efficiency. It is worth noting that there are other
    factors that we have not yet discussed and that need to be considered when choosing
    an ECU, such as the Instruction Set Architecture (ISA) supported (i.e., ARM, RISC-V
    or CISC-based) [246], [247], [248], the price gaps between different suppliers,
    production availability [249], etc. 2) In-Vehicle Communication: Network Cables
    Evolution As the capacities of the in-vehicle ECUs evolve to satisfy the application
    needs, more data will get produced within the in-vehicle systems for both inter-service
    collaboration and interactions with the off-board architectures (such as Cloud,
    infrastructure, or other vehicles - cf. §VI-D). Therefore, in this context, the
    network architecture and stack (cf. §VI-C), need to be revisited in order to extend
    the bandwidth requirements, match the application latency constraints, and establish
    new standards, all the while once again keeping both the costs and energy consumption
    down. Note that, Table 3 summarises the technology panel for network cables. TABLE
    3 Network Cable Standard Detailed Comparison. Classification Made From the Aggregated
    Data From the Papers [225], [226], [227], [228], [229], [230], [231] First, with
    regard to the network bandwidth bottleneck issues, it seems clear that Ethernet
    offers considerably higher options, being able to offer data-rates of multiple
    GBit/s in comparison with the 20 MBit/s limit of CAN XL, which is the most performant
    solution available for signal-based networks (i.e., CAN, LIN or FlexRay). Moreover,
    Ethernet has a bigger community and a wider choice panel within IP higher layer
    protocols, which means it is a better fit at an application level, especially
    when managing complex data-structures. However, in order to profit from these
    high speeds, Ethernet needs a network switch, which significantly increases the
    cost of the system, which is already higher for Ethernet cables than for signal-based
    cables, and energy consumption [228]. If enough Ethernet ports are available in
    ECUs, then it may be able to host the Ethernet switch, however, this solution
    will be less performant than a dedicated switch. Secondly, if we focus now on
    the impact on the processing workloads of both Ethernet and CAN [225], which is
    the most widespread signal-based network, studies show that even though Eth. communication
    requires significantly more processor than CAN communication for the interface
    initialisation, it is considerably more efficient at transmitting, receiving and
    unpacking messages. Thus, Ethernet has a higher initial cost, which impacts the
    time within which an ECU is considered fully available, but once the interface
    initialisation is done, the system will struggle less to match the application
    latency constraints. Note that both technologies are more than capable of communicating
    within an acceptable safety range of few μs for small/medium sized messages. However,
    only Eth. maintains an acceptable level of efficiency for long messages. Furthermore,
    even though both technologies are appropriate for resilient applications, Ethernet
    guarantees a 10-fold lower error occurrence than CAN. Moreover, if we look further
    now into the energy consumption issues, CAN comm. shows clear advantages with
    almost 50% lower power consumption than Ethernet [225]. However, this energy efficiency
    gap is less significant as the package size increases, as the Ethernet interface
    can enter idle or sleep mode for a longer time, or if we use mechanisms such as
    PoDL [227], [229] whose efficiency and maturity is higher for Ethernet than for
    CAN. Finally, and as the transition from the legacy to the new generation systems
    will happen progressively, it is important to survey the techniques that will
    help to simplify this transition. Thus, as CAN is the biggest representative of
    the signal-based networks, it’s worth mentioning some papers that present how
    to bring CAN closer to the IP world. Among these papers, and as you can see in
    Table 3, some proposals target encapsulating the IP package over the CAN frame
    [231], for best-effort traffic use cases, or adding bridges to make the encapsulation
    from CAN to Ethernet [250]. Finally, it is also interesting to take under consideration
    [251] and [252], which study the integration of CAN networks with Time-Sensitive
    Networking (TSN) and Ethernet for real-time scheduling. 3) External Energy: Power
    Supply Platforms Evolution With the recent advancements in Electric & Hybrid Vehicles,
    most automakers now have their own modular global power supply platforms to abstract
    the energetic aspects from the E/E design process itself. This way, E/E designers
    can make use of the electric platform simply on a dynamic power-on-demand basis
    to keep the power supply controlled, standardised, and decoupled from the rest
    of the E/E choices. These modular platforms are usually made up from the chassis
    of the vehicle including the battery, motor, and power electric system, and are
    standard and can be reused independently from the vehicle model. Examples of these
    platforms would be Hyundai Electric Global Modular Platform (E-GMP) [253], Volkswagen
    Group Premium Platform Electric (PPE) [254], [255], STELLANTIS STLA SMALL/MED/LARGE/FRAME
    [256], Toyota New Global Architecture (e-TNGA) [257], Volkswagen Modular Electric-drive
    Toolkit (MEB) [258] or Renault-Nissan Common Module Family (CMF) [258]. Furthermore,
    some suppliers, such as Foxconn, with their MIH initiative [8], also try to provide
    solutions for these kinds of power platforms. Note that as these platforms are
    at an early stage, they act in such a way that they furnish energy to the ECUs,
    however, this is expected to evolve in the near future, more in line with a power-on-demand
    basis. Moreover, as more and more power-hungry functions, ECUs and wires are used
    within cars, designing new techniques to reduce the energetic impact on the on-board
    IT systems seems crucial. Considering their negative impact on vehicle autonomy
    and, with it, some techniques such as ECU degraded operation mode [259], [260],
    partial network wake-up [261], [262], [263] or highly dynamic software contexts
    (cf. §VII) will become more important in the coming following years. 4) Hardware
    Modularity and Re-Usability: Reference Architec-Ture Evolution As we said in §IV-E,
    as more and more functionalities were added to the car, the in-vehicle systems
    needed more computational power, which then signified a massive increase in the
    number of embedded ECUs. This has led to unmanageable system complexity, making
    it complicated to keep growing at the same pace as application demands with regard
    to computing, network or energy consumption [16], [17], [264], [265]. Today, most
    of the automakers’ E/E architectures are already in compliance with the E/E Domain
    Vehicle reference architecture. However, there are still over a hundred low functional
    MCUs, connected through a highly limited and heterogeneous set of CAN, LIN, FlexRay
    and, to a lesser degree, Ethernet. If we look further into the progression perspectives
    set out in the previous state-of-technology analysis and the research trends with
    regard to upcoming in-vehicle architectures (i.e. Zonal Architectures, also known
    as ZoA), presented in Fig. 7, there exists a clear trend. They move through more
    centralised architectures, with fewer high-end function target ECUs both for general
    purposes and specific uses, - i.e., NPUs, GPUs, HPUs etc. interconnected, at least
    in the network backbone, through Ethernet to reduce latencies and enhance system
    bandwidth. FIGURE 7. On-going transitions for automotive E/E architectures. Show
    All Thus, if we delve further into detail, the now low-functional ECUs would be
    grouped into higher-end SoCs depending on the functions that they will be handling
    (e.g., a GPU for the ECU handling the on-board screen, an HPC for the ECU in which
    ADAS will more than likely be deployed…). These SoCs can then be separated into
    smaller, more dynamic, virtual ECUs thanks to techniques such as virtualisation
    (cf. §VII-C), offering more optimal communication management interfaces, multi-threading,
    fault tolerance tooling, and, most importantly, significantly reducing system
    complexity and coupling. On the other hand, as far as the cables interconnecting
    the ECUs are concerned, migrating from signal-based networks, (i.e., CAN, LIN
    and FlexRay) to data-centred networks (i.e., Ethernet) [22], shall relax the bandwidth
    and latency stress, allowing for the establishment of homogeneous communication
    interfaces and standards, reducing once again system complexity [17], [216], [266].
    Finally, with regard to the cost & energy efficiency, by drastically decreasing
    the quantity of ECUs and cables and not having to maintain all the architecture
    powered on a constant basis, we should reduce the costs/energy consumption or,
    at least, compensate for the increase in new ECU and wire evolutions. Furthermore,
    as the vehicle becomes progressively more integrated with smart-city and cloud
    infrastructure (cf. §VI-D), some studies [267], [268], [269] suggest that a combination
    of resources including cloud, infrastructures and vehicles operating together
    as a hive (i.e. Seamless Architecture) should be the next step in the evolution
    of Automotive E/E architectures. B. Software Architectures Now that we have gone
    over the evolution perspectives of the E/E architecture, the software architecture
    beneath needs to be revisited so as to benefit from these new system capacities
    in the most optimal way possible. Thus, in this section, we will begin focusing
    on the main reference architectures coming from an IT point of view, focusing
    on properties such as software reuse, dynamicity, and flexibility. Subsequently,
    we will go over the automotive software architectures and proposals, trying to
    highlight the safety and security aspects that are considered to be vital requirements
    for these systems. Finally, we will discuss the convergence of both, trying to
    evaluate how we can combine the aforementioned IT-related properties with automotive
    needs and how does it impact on the maintainability and complexity of the system.
    1) It Software Reference Architectures: Zoom on Re-Use, Dynamicity and Flexibility
    If we begin with Software Product Line (SPL) [270], [271], which was, to the best
    of our knowledge, the first modern reference architecture targeted to simplify
    and standardise the software development and deployment process, we have to highlight
    that this reference architecture was inspired by the classic industrial production
    lines and individual products, that interact between themselves through a trade-off
    of the base concept around which these architectures rotate: the variability,
    which is used to classify each of the features detailing an application as a commonality
    or a variation. Fig. 8 depicts the structure of SPLs, which consists of three
    phases: (1) Scoping, in which the company determines what to develop (i.e., which
    products will be part of the product line and what are their commonalities and
    variants). (2) Domain engineering, in which the objective is to design the underlying
    system architecture by characterising the inter-application reusability and pre-scheduling
    their data exchanges. Furthermore, in this phase, the system will also produce
    different test-cases, scenarios, documentation, and specifications for each individual
    product. Finally, (3) Application engineering, in which the output of the two
    precedent phases will be analysed, the inter-application conflicts resolved, and
    the development of the final architecture and applications will be done through
    the creation of reusable templates that will be further tested and integrated
    into the architecture, trying to maintain them so that they are reusable for future
    iterations or projects. FIGURE 8. Theoretical software reference architecture
    schematic figures. Show All Secondly, Framework Components Arch. (FCA) [270],
    [271], relies on the definition of the framework itself, which is basically a
    tool or a set of tools that provide ready-made components or solutions that can
    be easily used and customised in order to speed up the development and integration
    of new software applications. Therefore, the FCA ref. arch. proposes a sort of
    configurable black box, that already provides all the classes that contain application
    logic, which can be directly initiated or parameterised to adapt themselves to
    the needs of the application. However, in addition to these default parameterization
    possibilities, FCA frameworks ensure their extensibility by defining a clear set
    of interfaces to add components that were not initially planned. Thus, as we can
    see in Fig. 8c, this ref. arch. is composed of a component system framework that
    provides application invariant generic services close to the middleware layer,
    and a component application framework which provides highly customizable interfaces
    for the developers to implement their business-specific software over it, hence,
    enhancing the intelligence of the architecture and its middle-wares. To sum up,
    FCA paradigm offers a large set of supporting functions that allow the user to
    only implement the business applications required and forget about the rest of
    the system control mechanisms, which are developed once and are afterwards common
    to all the applications. As the obvious evolution of the previously mentioned
    FCA, the Service-Oriented Architectures (SOA) [272], [273] try to solve the aforementioned
    architectural problems in a similar manner: by enhancing the intelligence of the
    infrastructure and offering a common management layer for all the different applications
    running over it. These architectures base themselves on the existence of a fault
    tolerance service bus or middleware used by all the different services for either
    simple data passing or inter-service requesting. This will act as an abstraction
    layer between the services and the physical position of their instances, enhancing
    the flexibility and, through simple replication, the fault tolerance of the system
    [274], [275]. Furthermore, and as you can see in Fig. 8c, a typical SOA will be
    made up of (1) service providers, those who generate and share the data in the
    system, (2) service consumers, also called service requester sometimes, who are
    those using this data to operate, (3) a fault-tolerant common comm. bus or middleware,
    (4) a set of control services running constantly and trying to exploit, coordinate
    and ensure that both the services and the infrastructure themselves are functioning
    well, dynamically adapting to system constraint changes over time [18], [276].
    Finally, this paradigm also focuses on the importance of reusing and making the
    services as modular and inter-operable as possible, while always establishing
    an elevated level of standards and pipelines to be respected by the developers.
    In the same scope, Event-driven Architectures (EDA), as you can see in the Fig.
    8e, are a variant of the previously presented FCA. However, unlike SOA, which
    focuses on the services and their behaviour and interactions, the EDA reference
    architecture will focus on ensuring the real-time reactivity, implementing for
    it an asynchronous communication bus instead of the classic synchronous bus of
    SOA [277], [278]. This way, EDA would increase even more the flexibility and agility
    of SOA, relaxing further the software integration process. For the final variant
    we will review the FCA, Micro-services Architecture (MSA) [279], [280], [281]
    which is built over a collection of smaller, normally mono-functional, independent
    service units with well-defined interfaces. The goal of this ref. arch. is to
    exploit to the maximum the reusability and decoupling of software chunks between
    themselves, allowing for the possibility to to independently deploy and scale
    the different micro-services dynamically depending on the demands of the rest
    of the services [282], [283]. This flexibility when developing new services, together
    with the continuous growing collection of the already well-defined and deployed
    micro-services in the infrastructure, help developers to work faster and reduce
    the integration time, which then enables them to home in on the specific business
    needs. On the other hand, Multi-Agent Architectures (MAA) this time try to solve
    the aforementioned problems by enhancing the intelligence of each software application
    instead of the infrastructure itself. MAA synthesizes contributions from different
    areas, including artificial intelligence, software engineering and distributed
    computing to address developing systems that are composed by many dynamically
    interacting components [270], each of them having their own distributed control
    mechanisms to coordinate each other. Between these mechanisms we can account for
    some automotive critical mechanisms such as neighbour discovery, fault tolerance,
    network abstraction or consensus [284]. In this paradigm, and as you can see in
    Fig. 8b, this architecture model relies on the concept of agents, which are pieces
    of software able to coordinate themselves in sub-sets so as to establish the necessary
    control mechanisms without the need for a centralised service to intervene. However,
    this solution, even if it is a better fit in some cases, implies a redevelopment
    of the control services each time for each individual piece of software, which
    is not ideal for a highly variant and participative environment such as vehicle
    embedded architecture. To conclude, having already presented in detail all the
    main reference software architectures, we will compare them and comment on the
    interesting points of each one for future on-board architectures. Moreover, we
    will lean on Fig. 9 which visually depicts the similarities and differences between
    the different paradigms, and Table 4, which proposes a multi-criteria comparison
    of the reference architectures with regard to all four communication, software,
    architecture and business centred characteristics. On the one hand, if we isolate
    all three MAA, SPL and FCA, we can highlight that by making the control services
    exclusive to each application, as it is for MAA, we make the application development,
    maintenance, and integration considerably more difficult, having more coupling
    and less flexibility than SPL and FCA. However, this paradigm is interesting as
    ensuring the autonomy between the infrastructure and applications can be fascinating
    in some situations, as is ensuring the fault-tolerance of the control services
    layer in the FCA variants. In addition, we can also highlight that by the SPL
    focusing on optimising the software production through templates has a positive
    effect on decreasing software complexity and granularity and enhancing software
    reuse but won’t be enough to address the complete set of previously exposed issues
    without adding some extra control layers or mixing it with other reference architectures.
    On the other hand, all three of SOA, EDA and MSA match the future architecture
    levels of abstraction, security, and flexibility with slight differences in their
    focus. Thus, SOA paradigms will propose a higher tolerant and reliable call-driven
    environment, while MSA focuses less on these aspects but offers higher flexibility
    and collaboration. By reducing the granularity of the systems, and EDA focuses
    on implementing these control services in an asynchronous and more reactive manner.
    Furthermore, we can then imagine that future automotive software reference architecture
    could benefit from many of these paradigms. Using SPL to reduce development complexity
    and establish standards, SOA to manage all safety, security, scheduling, and criticality
    that is needed by embedded vehicle architectures, and even EDA or Micro-services
    to reduce the complexity of the applications or ensure network reactivity of time-constrained
    services. TABLE 4 Exhaustive Comparison of the Aforementioned Software Architecture
    Paradigms FIGURE 9. Outlook of the studied software architecture paradigms. Show
    All 2) Automotive Software Architectures: Safety and Security In this subsection,
    we will be taking an in-depth look at the automotive contributions around software
    arch., with SOA being the one that we focus on the most. Note that Table 5 summarises
    the main contributions around Software Architectures in the automotive sector,
    breaking each of them down into sub-domain categories depending on all the subjects
    addressed in the paper. TABLE 5 Classification of the Most Interesting Automotive
    SOA (Complete or Partial) Propositions If we take a detailed look at this table,
    we can observe that, at least for now, there are few contributions concerning
    full software reference architectures or design patterns within the automotive
    industry and that we can identify four distinct categories of papers. Firstly,
    there are those that focus on software architectures and zoom in on the E/E properties,
    then those giving special importance to communications and networking within the
    architecture. Subsequently, there are those that focus on application characteristics
    and granularity, and finally, those focusing on the initial stages, going extensively
    through the context and use cases to justify architectural evolution. Furthermore,
    the run-time management of the infrastructures, done through the control services
    and which will be addressed in §VIII, are often evoked but are never the focal
    point of the paper, leaving the focus on this field considerably behind when compared
    to others. Finally, despite the existence of papers covering software characteristics
    and relations we haven’t found any paper on how to properly manage its life cycle
    (install / deploy / testing / monitoring), which is a crucial part of any success
    when implementing SOA architecture (cf. §VII). Moreover, in industry, AUTOSAR
    SOME-IP is the biggest representative of SOA in the automotive sector. While already
    being the principal OS standard for the constrained ECUs of vehicles, AUTOSAR
    released SOME-IP: a SOA-like architecture quite similar to the one proposed in
    Leguay et al. [303] and, in the same manner, focusing exclusively on the networking
    capabilities of the architecture and introducing interesting features such as
    service discovery and network abstraction to the legacy of AUTOSAR-based ECUs.
    This approach does not address the lack of control services and flexibility in
    the automotive embedded architectures nor their low dynamicity and adaptability
    that, as we said before, are key features for the future of the automotive industry.
    However, it’s a good a good foundation from where to start and can be complemented
    with the addition of control services over it and other features to achieve the
    aforementioned capabilities. In this same scope, other proprietary OEMs, and companies
    (GuardKnox, BMW, TESLA…) have developed or offer SOA frameworks, however, the
    lack of papers detailing their structure or justifying their choices makes their
    evaluation and realistic positioning in the ecosystem impossible. It should also
    be noted that we will be comparing AUTOSAR’s SOME-IP solution to other IT solutions
    in the next section. 3) It and Automotive Software Architecture Convergence: Maintainability
    and Complexity To cover the lack of maturity of the automotive software architecture
    research proposals, we are now going to focus on the perspectives of both the
    classic high-end cloud computing sector, which was the precursor of SOA, and some
    lightweight SOA implementations from the IoT sector, comparing them over an eight-vertex
    criteria deriving from §IV and the open issues in §V. This evaluation is discussed
    through the whole section going one by one through the remarkable representative
    solutions and summarised in Table 6, which can be found at the end of the section.
    It is worth noting that, as these solutions are not targeting the automotive sector,
    we cannot evaluate if they comply with the traditional automotive safety & security
    certificates & standards, which must surely be worked on if finally integrated
    into the automotive in-vehicle systems. TABLE 6 Comparison Summary of the IT SOA
    State-of-the-Art. Classification Made From the Aggregated Data From the Papers
    [304], [305], [306], [307], [308], [303], [309], [310] If we start with solutions
    that are targeting cloud computing, the first proposal worth mentioning is IBM
    Websphere [304], [305], [306]. As we can see in Fig. 10, the core of this architecture
    is the connectivity services, which provide the infrastructure to manage the Enterprise
    Service Bus. These connectivity services provide three major resources: transport
    services, event services, and mediation services. Directly linked to the connectivity
    services, we can find business logic services that define what the application
    needs to develop, as well as control services, which are in charge of managing
    the interaction between services. In addition to that, this architecture also
    provides development services, modelling, testing, and maintaining the system,
    business innovation and optimisation services for research and development, sand-boxing,
    IT management services for security, network, virtualisation, system management
    and, finally, infrastructure services to manage hardware resource dependencies.
    FIGURE 10. IBM WebSphere (left) & OASIS FERA (right) architecture schemes. Show
    All Secondly, we have OASIS FERA [307] (Federated Enterprise Reference Architecture)
    that presents a set of principles and guidelines for the development of an SOA
    application with loosely coupled collaboration for Federated Architectures.3 This
    model, as we can see in Fig. 10, maps the Federated Architecture formed with semi-autonomous
    decentralised systems into the SOA concept architecture. In this implementation,
    the federated systems interfere with the rest of the federated systems, SOA control
    services and external/developer access APIs through a centralised fault tolerance
    ESB. Thirdly, we are going to focus on a solution that concentrates on integrating
    the development and operation cycle into the SOA architecture: the Process-Embedded
    Service-Oriented Infrastructure (PESOI) [308]. As this solution integrates the
    development process into the infrastructure, this simplifies the redevelopment
    of the applications to add new features on run-time. This model then implements
    a control service layer based on the development cycle of the application, in
    which, we can count (1) modelling services, to offer high SOSE support and services
    to help the users to construct the architecture model, the process model and the
    policy model of the target application, (2) verification and validation services,
    (3) a dynamic re-composition manager, in charge of taking the model and picking
    up proper services from the service repositories to compose the desired applications,
    (4) run-time data collection, (5) Analysis, and (6) dynamic workflow definition
    services in order to monitor the system, adapt and optimise execution after the
    applications are assembled and deployed (cf. Fig. 11). Thus, this system is able
    to monitor the behaviour of the service and dynamically reconfigure it for better
    performance or reliability (or even replace the service) at run-time, which is
    an very important feature for the automotive industry. FIGURE 11. PESOI architecture
    scheme. Show All This time, focusing in IoT-like resource restrained devices as
    in the automotive industry, we have Lenguay et al. [303], in which a proposal
    and implementation of multi-level SOA-based architecture is described. This is
    for heterogeneous and dynamic wireless sensor nodes with limited computing, battery,
    and communication capacity. Afterwards this solution will then create both a new
    protocol and routing algorithm allowing for dynamic discovery (cf. Fig. 12), network
    abstraction, fault-tolerance, as well as invoking services in other nodes. However,
    the absence of nodes with higher computing capabilities to implement a more complex
    control layer considerably holds back the capacity and possible uses of the system.
    FIGURE 12. Lenguay et al. [303] architecture scheme. Show All Finally, within
    the same scope, in Wang et al. [309] they propose a Service-Oriented model for
    heterogeneous MPSoCs, suggesting a new 3-layered hierarchical model, as we can
    see in Fig. 13. Thus, the three layers are, in order from the furthest from the
    hardware, to the closest; (1) the Services Layer, which offers an API for deployment,
    run-time analysis mechanisms, an orchestrator and scheduler and some data collection
    services, (2) the Servants Layer, which is in charge of the error management of
    the different services running and the fault-tolerant communication support among
    them, and finally, (3) the Physical Layer, which is responsible for providing
    the requested physical resources to the different service. Therefore, this solution
    implements a completely dynamic SOA in which the resources and services are allocated
    to need and run-time analysis techniques, aiming to reduce hazards and to optimise
    the execution of the applications. FIGURE 13. Wang et al. [309] architecture scheme.
    Show All To conclude, if you compare all the previous detailed solution descriptions,
    we can highlight some layers that are common to all of them. To begin with, we
    have the Fault tolerant Communication Layer, usually named Enterprise Service
    bus, and which will looked at in §VI-C. This is probably the most important layer
    as it enhances the flexibility of the architecture and cuts the hardware specific
    couplings, therefore increasing the reusability of the software. Secondly, now
    that the communication between services has been ensured, the second point we
    can highlight is the software development pipelines, that we will study in detail
    in §VII, covering the software installation, orchestration, deployment, and testing
    within the vehicles. Finally, and more extensive than the two previous layers,
    we have the control services, to be looked at further in §VIII, which are very
    varied but share the common goal of ensuring that the software installed and the
    communication channel are both functioning well through services such as the service
    registry, orchestrators and schedulers, the status checkers etc. However, we need
    to add some mechanisms to those ones, allowing them to compensate for the heterogeneity
    either in terms OS, security and safety constraints, and resources, all of which
    are inherent to the automotive sector. Furthermore, as detailed in Table 6, we
    can see that the more complex and complete the control services and architecture
    proposition are, the lower the maintenance and integration costs. However, this
    signifies a high initial cost that, considering the life cycle of the vehicles,
    should be rapidly compensated for by simpler maintenance and integration cycles.
    C. Network Architectures Given the advances of both E/E (cf. §VI-A) and Software
    architecture (cf. §VI-B), we now need to adapt the network stack so as to fully
    profit from the new bandwidth enhancements thanks to the higher presence of Ethernet,
    the new simple E/E architecture board availability, and to match the requirements
    necessary to have fault-tolerant data-based communication middleware (or Enterprise
    Service Bus). Thus, in this section we will start by (1) surveying how the network
    architecture is standardised, by analysing application-level network protocols
    to be used for this homogeneous data-based communication middleware over IP. After
    that, (2) we will discuss the rest of the tooling that allows us to enable data-oriented
    communications over this middleware, which is a key point for a flexible and dynamic
    abstract communication ESB. Finally, (3) we will discuss how the legacy software
    components can be integrated into this framework, which is mandatory for the transition.
    1) Standardisation: Network Protocols To begin the study, if we start with the
    solutions from the IT domain, where the SOA architectures are already in place,
    HTTP REST [311], [312] is highlighted. HTTP REST is a communication protocol that
    defines an ensemble of constraints to be used when creating a web service to establish
    easy interoperability between services on the internet. This protocol follows
    a request/response communication pattern in star topology. In this host-centric
    protocol, the central server will listen for client requests that, on reception,
    will invoke the different functions available. HTTP implements lots of interesting
    mechanisms such as TLS security, object compatible payloads, standard response,
    and error codes. However, the fact that this protocol needs a centralised server
    to offer its services, linked to the lack of environment service discovery or
    single point of error, failure prevention mechanisms might be a problem when dealing
    with safety constrained systems such as vehicle IT architecture. In the same scope,
    if we keep looking at web service oriented communications protocols we can find
    other interesting protocols such as SOAP [313], [314] or WebSocket [315]. SOAP
    first appeared two years before HTTP REST and provides mostly the same functionalities.
    The main difference is that SOAP requests are written in a custom XML (or YAML)
    SOAP format then wrapped in an HTTP message while REST directly uses the HTTP
    as the application layer protocol. This provides higher bandwidth and processing
    consumption for the SOAP protocol if compared to REST, which in the context of
    embedded systems, is a critical point that makes REST more suitable. On the other
    hand, WebSocket approaches this problem as a lower-end solution. WebSocket is
    based on the concepts of socket and port and will use the IP address from the
    device and their application port details in order to establish a bi-directional
    communication channel, whereas HTTP REST only works as a unidirectional channel.
    WebSocket is therefore ideal for real-time scalable communication with high data
    exchange between services but is less performant when handling lots of small requests
    from different clients. At the same time, as WebSocket is a very low-level restful
    protocol that manages the mapping itself between services, it needs to know a
    lot of infrastructure details. This lack of abstraction might limit the possibilities
    of a flexible SOA arch. in which the services might change their location over
    time. However, it could be interesting when used in some situations since the
    comm. cost is lower than REST and the bidirectional socket can offer some advantages.
    To reduce overheads and computing requirements of these protocols, the Constraint
    RESTful Environments (CoRE) working group of IETF developed CoAP [316]. CoAP is
    a protocol like HTTP REST but with the peculiarity that the headers, methods,
    and status codes are all binary encoded and that it runs over UDP instead of TCP.
    This reduces the package overhead but, unfortunately, also transmission reliability.
    To deal with the lack of reliability of UDP, CoAP implements a functional layer
    to re-transmit lost or corrupted packages. At the same time, in order to reduce
    the resource consumption of the protocol even more, they created a new mechanism
    to reduce the message exchange to retrieve data. While in HTTP REST the data retrieving
    always implies 2 messages (a HTTP GET request and a HTTP RESPONSE), in CoAP, with
    the first HTTP GET request, you can add a header flag (observe) to continue receiving
    changes to the requested resource from the server. Therefore, whenever this resource
    changes, the server will send the information to the client without the need for
    an HTTP GET Request. This implementation was the first step to asynchronous messaging
    over HTTP, getting closer to a publish/subscribe communication pattern. Asynchronous
    messaging allows systems to increase the flexibility of their architecture, which
    makes this communication pattern rather more suitable for resource constrained
    devices and non-ideal network conditions than the classic request/response paradigm.
    Between the asynchronous messaging protocols we can state that three of them stand
    out from the rest: MQTT [317], AMQP [318], XMPP [319] and DDS [320], [321]. MQTT
    runs over TCP (which ensures its reliability) with the peculiarity that it has
    an exceedingly small header and, thus, extremely low package overhead, making
    it one of the most prominent solutions in constrained environments. MQTT follows
    the publish/subscribe paradigm with the peculiarity that any communication is
    centralised through a Broker that will handle message persistence and ensure its
    arrival to the correct host. However, since it was designed to be the more lightweight
    possibility, MQTT’s weak point is a lack of security and the potential single
    point of failures. In the same scope, being aware of the security deficiencies
    of MQTT, there also exist alternatives such as AMQP and XMPP. Even if both implement
    strong data protection and authentication, XMPP is a bit more secure than AMQP
    because of the inclusion of stricter security, authentication, privacy, and access
    control protocol extensions. However, the fact that, at the same time, XMPP offers
    request/response and publish/subscribe communication services and that they use
    an inefficient XML payload formatting, makes the resource consumption of XML high
    compared to both AMQP and MQTT- This will complicate its use on lossy low-power
    wireless networks and low-specification processors. Even though XMPP is slightly
    more secure than AMQP, this second protocol has some other interesting points.
    As one of the main problems in MQTT was the threat that a broker failure would
    prevent the entire system from working, AMQP developed a new async. peer-to-peer
    mechanism that makes the pub./sub. pattern more flexible and robust, eliminating
    the threat coming from the centralised broker single point of failure. However,
    as in XMPP, all these features denote an increase in network, power consumption,
    processing, and memory reqs., making it a potential problem if implemented in
    highly constrained devices. Finally, if we look at other domain specific solutions
    we can find OPC UA [322], [323] (coming from the industry 4.0) and SOME/IP [324],
    [325] (in the automotive industry). To begin, OPC UA offers both publish/subscribe
    and request/response mechanisms based on TCP/IP. OPC UA, and as is required for
    industrial context, focuses on real time and safety of machine-to-machine communications.
    As this protocol is widely adopted in the industry 4.0, it offers interesting
    interoperability options that can lead to new business models and interactions
    between industry and vehicles. Secondly, SOME/IP is an open standard messaging
    protocol formalised by AU- TOSAR as a reaction to the inclusion of software-oriented
    architectures in the automotive system. SOME/IP offers a complete communication
    standard including 3 communication patterns (publish/subscribe, request/response
    and fire and forget (request without response)) that communicate via a centralised
    broker over TCP/IP. At the same time, it offers a service discovery protocol to
    deal with any abstraction between network and data providers and some basic security
    features. One final positive characteristic of AUTOSAR is the fact that it has
    already been widely adopted by the automotive OEMs which eases the transition
    to a software-oriented architecture. However, AUTOSAR has the same problems as
    the rest of the broker centralised communication protocols, as well as the fact
    that it does not handle data objects or serialise a method invocation payload.
    Hence, if we look further into detail in Table 7, it seems clear that the combination
    of multiple communication protocols can be useful to cover all the different automotive
    use cases. To begin, among the communication protocols that include publish/subscribe
    patterns, the most remarkable solutions would be MQTT, SOME/IP, DDS, AMQP and
    ROS2. TABLE 7 Application Layer Network Stack Comparison If we try to use the
    most resource efficient solution, despite security issues, MQTT offers the lowest
    resource consumption and compatibility for Android, Austosar Adaptive and Linux.
    In addition to that, MQTT is a widespread solution in the IT world, which is an
    advantage when customers are looking for new solutions at a lower price. From
    another point of view, if what we seek is a solution that is compatible with both
    the old ECUs and the new ones, SOME/IP, as it’s the AUTOSAR solution, is the only
    one capable of ensuring compatibility with both AUTOSAR Classic and the other
    Linux-based operating systems. However, SOME/IP is far from being optimal in terms
    of other criteria, having low security and higher development and maintenance
    costs than MQTT. From a security point of view, both solutions offer basic authentication,
    encryption, and TLS techniques. However, other communication protocols like ROS2,
    AMQP and DDS have stronger additional mechanisms in exchange for a higher resource
    consumption. Finally, if we look from a flexibility point of view, once again
    DDS, AMQP and ROS2, as they are P2P publish/subscribe solutions, offer a higher
    flexibility on the system design. On the other hand, if we shift our focus to
    communication protocols following the request/response scheme, we have HTTP REST,
    WebSocket, CoAP, OPC UA and SOME/IP. Among these solutions, HTTP REST is the most
    frequently used in the IT world, which presents an advantage in terms of development
    tools and support. At the same time, it is compatible with all but AUTOSAR Classic
    and offers decent security and flexibility at an assumable resource cost. In the
    same scope, CoAP is an optimisation of HTTP REST for constrained environments.
    However, with the libraries and optimisations made in REST by the community, the
    resource consumption of REST is only slightly superior to CoAP. However, if resource
    efficiency is a priority, WebSocket is the most interesting solution. Nevertheless,
    this protocol needs low-level knowledge of the infrastructure, which blocks the
    network abstraction concept of SOA unless an abstraction connection manager is
    implemented over this network stack at application level. After that, when focusing
    on security, it is clear that OPC UA is the most adequate solution. However, the
    development and maintenance costs and the resource consumption of the protocol
    are higher than the other ones. Finally, as in the case of the publish/subscribe
    paradigm, SOME/IP is the only fully compatible solution with the AUTOSAR classic
    and Linux-based ECUs. 2) Data-Management: Through a Data-Centric Communication
    Middleware As the aforementioned evolution trends of both E/E and software architecture
    suggest an increasing break in domain boundaries (i.e., ADAS, Power-train…), a
    democratization of Ethernet as the main transport support, a dynamic and seamless
    perception of the software location, as well as a steady rise the complexity of
    deterministic. Whilst deterministic comm. is a key challenge to match real-time
    constraints of the automotive sector [326], something that we will discuss in
    depth later in §VI-C3, in this section we are going to focus on the migration
    from static network mapping to more flexible and dynamic data-centric mechanisms.
    Remember that, in the FCA sub-variants, and specifically in SOA, systems are represented
    as a set of services that are either data producers, data consumers or both at
    the same time. Today, as the in-vehicle systems and development processes mostly
    ECU / signal-based development approaches, the complexity and data maintenance
    of both signal and bus message databases requires a lot of time and money [327].
    The high software coupling to both network and hardware means that multiple functions,
    such as driver presence, are implemented several times for specific customer functions
    on different ECUs when they could easily be communal functions provided as a service
    and reused by all ECUs. Moreover, nowadays, we cannot underestimate that an unnecessarily
    high bus load can be observed because of signals being sent without receivers
    on most of the CAN buses [328]. Therefore, the implementation of an Ethernet-based
    data centric middleware will not only allow for a reduction in overall system
    complexity and comply with the flexibility and dynamicity proposed by the software
    architecture evolution trends (cf. §VI-B), but it will also help cope with the
    problems of function reuse and unnecessary network consumption. For that matter,
    the only source of communication overhead will be the one added by the middleware,
    which is predictable and not rooted in inadequate tooling or insufficient communication
    management processes. This shall help maintain the system determinism. On the
    one hand, if we look closer at the proposals surrounding data-centric communication
    middleware for the automotive industry, what stands out is that most of them rely
    on the aforementioned DDS tack, with the publish/subscribe comm. pattern playing
    a central role in this matter. The DDS functions either alone with some containerisation
    and custom tooling [328], or as an extension of a network layer for either SOME-IP
    [327] or ROS 2 [329]. As a complement of these studies, [326] presents, in a theoretical
    way, a study of the desired properties for future data-centric real-time comm.
    middleware, focusing on how resources are shared, priorities managed, and data
    consistency can be ensured. On the other hand, with the objective currently not
    only being about adding a coordination layer but, also, to change the host- centric
    philosophy TCP/IP and move towards an information centric philosophy, some papers
    [330], [331] chose to delegate the network abstraction to a lower OSI level. Thus,
    the network routing protocols of this solution will no longer generate routing
    tables containing the addresses of the connected devices, but instead of the data
    that these connected devices can provide [332]. However, since there is no widespread
    standard as of yet, Information Centric Architecture implementation has been approached
    in diverse ways (Data-Oriented Network Architecture, PubSub, Named Data Networking…).
    Even though different implementations have small subtleties, they all focus on
    the data (Named Data Object) and its storage, mobility, authenticity, integrity,
    and security [332], [333]. ICN approaches can be seen as an ensemble of data publishers
    and data requesters connected through the NDO they share and the abstraction routing
    mechanisms. However, even though the potential of this new network arch. paradigm
    is interesting, the maturity of the technology makes its integration into the
    in-vehicle systems more complicated. 3) Latency: With Regard to App. Constraints
    and Deadlines As the comm. capabilities of most of the ECUs within the in-vehicle
    arch. rise more and more, having a precise view of traffic flows, the nature of
    the delays (called “latency”) and variation in travel time (called “jitter”) has
    become crucial for both understanding system interactions and matching safety
    requirements in the different apps. The control and prioritisation of traffic
    flows can be done at multiple levels - i.e., app. level, data-link level, hardware
    level… At the application level, this feature can be done either by adding time-sensitive
    extensions for the network stack protocols [334], [335], such as those in Table
    7, or by implementing some high-level global-schedulers [336]. However, all these
    solutions mean additional system constraints, such as needing to use a common
    network stack, which has a negative impact on system flexibility. Moreover, as
    the scheduling takes place during the application layer, it adds further delays
    for decapsulating the packages, which also increments the jitter as it strongly
    depends on the CPU use of the nodes. On the other hand, hardware-level, real-time
    traffic flow control can keep treatment delays down and maintain network stack
    flexibility, solving the main problems that arise from application level real-time
    communication services. Some examples of these are presented in [337] and [338].
    However, these solutions are suddenly too expensive to be implemented within cars
    and have a real risk of buffer overflow or of ignoring some traffic norms due
    to its prioritisation. Besides, these solutions limit the dynamicity and adaptability
    of the system since the hardware and software don’t follow the same timescale
    in their life cycle. Therefore, being aware of these problems in the automotive
    industry, both research and industry position themselves by carrying out this
    traffic-flow control directly over the data-link layer, following the standards
    described in IEEE 802.1Qca,AB,Qbv,Qci…by the IEEE Time Sensitive Networking (TSN)
    standardisation group [339], [340], [341], [342]. TSN, therefore, proposes a set
    of standards featuring time synchronisation, traffic scheduling, gate control,
    frame preemption…that enables it to have a fine grain control over traffic prioritisation,
    by reserving specific bandwidths for each traffic, and allows for a precise calculation
    of latency and jitter. However, even though nowadays TSN is still elevated, it
    will certainly be deployed cheaper over MAC in the next years. Note that TSN can
    be complemented with higher level application QoS solutions [343], [344], [345],
    or even other techniques such as DPDK [346], [347] to overlap the kernel and enhance
    network bandwidth capabilities. D. IoV Platforms The Internet of Vehicles (IoV)
    is a network born from the integration of vehicles to the smart city and whose
    objective is to interconnect cars (V2V), pedestrians(V2P), cloud computing infrastructure
    (V2C) and parts of the urban infrastructure (V2I), such as charging stations,
    roads, traffic lights, etc. to make transportation more autonomous, safer, faster,
    more efficient, more eco-friendly, etc. In this section, we will split the IoV
    platform into two categories, i.e., Vehicle-to-Cloud (V2C) and Vehicle-to-Everything
    (V2X), and subsequently detail the advances and evolution perspectives of both.
    1) Vehicle-to-Cloud (V2C) As the levels of autonomy and functionality of vehicles
    increase, vehicles are expected to execute a wide range of computations over short
    periods. Given the limited on-board battery capability and computation capacity
    of the in-vehicle systems, offloading, the more power-sapping and time-consuming
    computation tasks to other more powerful servers, from the Cloud Architectures,
    has significantly improved the performance of many applications, such as intelligent
    driving, cruise-control assistance or log collection [348], [349], [350], [351].
    Cloud computing is therefore defined, by the US National Institute for Standards
    and Technology (NIST) in [352], as a model that enables ubiquitous, convenient,
    and on-demand network access to a shared pool of configurable computing resources
    (e.g., networks, servers, storage, applications and services) that can be rapidly
    provided and released with minimal mgmt effort or service provider interaction.
    Moreover, the latest advances in the Cloud comp. sector, driven by the Internet-of-Things
    explosion, allow us to complete this definition as there are now more possibilities
    concerning where to place and how to manage Cloud nodes. If we begin by focusing
    on the location of these nodes, Table 8 summarises the four big trends regarding
    this matter. First, we can highlight that Central Cloud Comp. nodes, which are
    those traditionally used by most of the industry nowadays, are placed in data
    centers extremely far from the vehicles, which has a negative impact on the latency
    and energy consumption for this solution. However, they also have the advantage
    of working globally and offering mostly unlimited resources and, thus, they are
    preferable when being used for global non-real-time overly complex calculus or
    data storage. Following on closely, Edge Comp. nodes are placed in smaller data
    centers at the edge of the network, which tries to even the trade-off between
    latency and quantity of resources. On the other hand, Fog, and Mist comp., the
    first based in local antennas or buildings and the second in the IoT surroundings,
    have a similar objective of scarifying some calculus performance to gain considerably
    in latency. These nodes are worth using when the calculus does not have exceptionally
    long to run, as the gain in latency will compensate for the restrained computing
    capabilities. However, it is worth noting that all three, Edge, Fog, and Mist
    computing solutions, have limited mobility support, which means that we would
    need to add high-level mechanisms, such as data or function prefetching, to make
    the data follow the vehicles when in movement. TABLE 8 Comparison of Cloud Computing
    Solutions According to the Distance Where They are Placed On the other hand, if
    we now focus on the nature and mgmt. of how the cloud computing platform is used,
    we need to highlight several different modes - i.e., On-premises, Infrastructure-as-a-Service
    (IaaS) [354], [371], Platform-as-a-Service (PaaS) [372], Function-as-a-Service
    (FaaS) [373] and Software-as-a-Service (SaaS) [374], [375]. As cloud computing
    architectures are often split into nine distinct levels - i.e., network, storage,
    servers, virtualisation, operating system, container technologies, run-time, application,
    and data - we are also going to rely on this definition to characterise the different
    management modes. Fig. 14 defines, for each of the previously introduced modes,
    which part is carried out by the Automakers and which by the Cloud provider. Note
    that, the more things that are dealt with by the Automakers (On-premises / IaaS),
    the more complex and time-prone to manage it is, but also more configurability
    is offered. On the other hand, by letting the cloud provider manage most of the
    layers, Automakers would gain in agility, while also easing the complexity of
    the system, thus being able to focus their efforts on more business-related issues.
    Furthermore, if we look closer at the network stack used for V2C communication,
    studies [376], [377] show that this communication normally takes place through
    standard LTE/5G wireless networks. FIGURE 14. Cloud computing management possibilities.
    Show All In conclusion if we give a brief overview of current Cloud computing
    use in the automotive sector, as you can see once again in Table 8, most of the
    Automakers use On-premises/IaaS/SaaS based Central Cloud Computing solutions.
    However, this trend is beginning to reverse due to academia proposals suggesting
    infrastructures closer to the car, which should most likely happen in the industry
    in the following years, while maintaining the Central Cloud Infrastructures for
    overly complex calculus and big-data storage. Finally, with regard to the management
    mode, all management modes that are already in use should continue to coexist
    as they have done until now, integrating little by little both PaaS and FaaS into
    the ecosystem. 2) Vehicle-to-Everything (V2X) Vehicle-to-Everything communication
    is essential to maintain a shared information layer throughout all the vehicles,
    pedestrians, and infrastructure. In this layer, the position, status, trajectories,
    and obstacles for all smart-city participants are grouped together, with the objective
    of increasing road safety, helping users and vehicles throughout their journey,
    adapting the energy supply, parking spaces, etc. Thus, in this section, we are
    going to explain in detail some of the main V2X use cases, i.e., vehicle-to-Vehicle
    (V2V), Vehicle-to-Pedestrian (V2P) and Vehicle-to-Infrastructure (V2I). a: Vehicle-to-Vehicle
    (V2V) Architectures As V2V encompasses a wide range of functionalities and use
    cases, in this section, we will order them according to the distance and communication
    technology used to interact with one another. If we begin with those solutions
    of short V2V ranges that are the most widespread. Their standardisation relies
    on the extension of the WIFI standard (IEEE 802.11a) to support the ad-hoc mode
    (IEEE 802.11p [378]) made back in 1999 by the American Federal Communication Commission
    (FCC), allowing for the use of highly dynamic Vehicular Ad-hoc NETworks (VANET).
    This protocol can then control any high mobility conditions caused by high speeds
    coming from multi-path reflections, and Doppler shifts obstructions [379], [380],
    thanks to techniques such as the Distributed Congestion Control (DCC) mechanism
    or the Multiple Access with Collision Avoidance mechanism (CSMA-CA). Therefore,
    in these networks the vehicles will constantly transmit and receive information
    from all other vehicles within their transmission range. VANETS are suddenly being
    used for either safety-oriented applications, such as driving assistance [381]
    (i.e., lane changing, emergency braking or cooperative collision avoidance), information
    [382] (i.e., speed limit or road work areas), and warnings [383] (i.e., post-crash
    notifications, road conditions or collision alerts), non-safety-oriented applications
    for some traffic controls [384], [385], and some comfort and infotainment applications
    [386], all the while being aware of the limited data-rate and security limitations
    of the protocol. In this same scope, with the objective of updating the VANET
    standard and adapting to the data-centric vehicle enhancements, a new protocol
    appears that makes a few changes to the IEEE 802.11p extension. This protocol
    is known as Dedicated Short Range Communication (DSRC) [387], [388] protocol and,
    among its main improvements we can highlight: the addition of network congestion
    control for high-density networks, a significant reduction in communication overhead
    (which remains unsuitable for big-data ultra-low latency applications), a fair
    management algorithm for transmission coordination between vehicles to reduce
    the network noise, and the addition of additional security mechanisms. Some examples
    of applications running DSRC are lane change detection [389] or emergency collision
    avoidance [390]. However, both DSRC and VANET share their bandwidth not only with
    other vehicles but with several applications from the smart-city, which is a limiting
    factor for them when trying to achieve higher data-rates or SLA ratios. In addition,
    the security of these two protocols is weak compared to the more powerful LTE-based
    protocols available. Thus, in order to explore other uncongested frequencies,
    some studies [391], [392], [393] focus on the adoption of an emergent technology
    known as Visible Light Communication (VLC). This targets a completely unexplored
    communication method: light. Some studies have shown interesting possibilities
    arising from this technology for short range driving assistance applications in
    V2V clusters; however, this technology is still not mature enough and most of
    the research is dedicated to increasing its performance, decreasing the noise
    from other lights or the sun, compensating for weather conditions, etc. Moreover,
    VLC, as well as previous standards, cannot guarantee service availability, as
    they need other vehicles in proximity to enable their services. On the other hand,
    with the V2V services needing higher availability despite the presence of vehicles
    nearby, higher security or, simply a higher data-rate, academia [379], [394],
    [395] suggests 5G must be supported by future V2V architectures, assisting the
    current standards when they are not able to match the requirements of the application.
    5G aims to support wireless communications with high reliability, ultra-low latency,
    and ultra-high throughput, offering several interesting features such as Proximity
    Services (ProSE), and allowing for awareness to detect nearby devices without
    continuously emitting data, or data managing services. Literature suggests that
    5G networks shall be increasingly used in the coming years for various existing
    and new use-cases such as global traffic management, V2V update spreading, infotainment
    services, etc. b: Vehicle-to-Pedestrian (V2P) Architectures Pedestrians, cyclists
    and motorists, who are usually grouped under Vulnerable Road Users (VRUs), account
    for a big part of traffic fatalities (e.g., in 2020 USA National Highway Traffic
    Safety Administration (NHTSA) declared 1,674 pedestrian and 355 cyclist fatalities,
    compared with 10,626 traffic fatalities, which represents around 20% of the country’s
    traffic fatalities [396])). V2P architectures englobe both crash prevention architectures
    and those assisting both VRUs and vehicles to increase their travel efficiency
    [397], [398], [399], often named convenience applications (i.e., ride-sharing,
    green light for bicycles or travel information for VRUs). Typically, V2P architectures
    involve periodic exchanges of messages among vehicles and VRUs, either directly,
    through ad-hoc communication technologies such as VANET or DSRC, or indirectly,
    through infrastructure such as cellular technology. It is normal that VRUs smartphones
    often play a key role in the mechanism. In addition, these architectures often
    operate in three distinct phases: detection, tracking and trajectory prediction,
    and action. Some examples of V2P architectures are [400], [401], and [402]. c:
    Vehicle-to-Infrastructure (V2I) Architectures Communication between vehicles and
    infrastructure [403], [404], [405] encompasses the interactions between vehicles
    (V2R) and roads, charging stations (V2G), and houses and buildings (B2H & B2B).
    On the one hand, Vehicle-to-Road (V2R) [406], [407], [408] architectures, which
    have been already slightly addressed by the V2P indirect mechanisms, aim to facilitate
    the next step for driverless vehicles by implementing mechanisms for Advanced
    Driver Assistance (ADAS), such as traffic light cycle details, potential road
    hazard alerts, vehicle congestion monitoring, global traffic organisation mechanisms,
    etc. On the other hand, all three V2G, V2H & V2B [409], [410], [411], [412] focus
    on different sources of power, mostly EV vehicles, discussing charging station
    coverage, energy eff. of the charging process itself or the parallel connection
    between the vehicle and the charging network for other vehicle maint. purposes.
    At this juncture, it is worth noting that, some of the aforementioned protocols,
    such as OPC-UA (cf. §VI-C), will grow in importance in the near future. In this
    section, we have covered the main architectural aspects of the automotive in-vehicle
    ICT architectures (i.e., hardware, software, network, and external architectures).
    However, the architecture design is, as we saw in §V, just one part of the life
    cycle. In the next section, we will focus on how the elements that we have just
    presented interact with each other and the different data fluxes within these
    architectures by delving deeper into the software pipelines. SECTION VII. Software
    Delivery Pipelines Studies [218], [219], [220] have shown that, given the big
    changes coming to automotive industry motivated by both industry and society (cf.
    §IV), the frequency of both new system/application releases and updates will significantly
    increase in the next five to ten years, making application life cycles far more
    dynamic than nowadays. This acceleration of software in the automotive industry
    has already started, leading to a higher level of faults in software components
    which accounted for almost half of the vehicle recalls last year [1], the highest
    level for the last few decades. Thus, as a counter effect, bug fixes and security
    threats correction will also carry more weight than nowadays, which could lead
    to even higher software dynamicity and, with it, an increase in the need to revisit
    the software delivery pipelines, either centralised, through V2C, or distributed,
    through V2X (mostly V2V). Moreover, as cars move further towards software customisation,
    these faults will inevitably increase, as the number of software and hardware
    context variants rises. Therefore, being able to ensure and define a dynamic and
    flexible way to bring and update the systems and software, both during the production
    phase, garage and on the road, is essential so as to address the challenges of
    future architectures. In this section, we will go through all of the software
    delivery pipelines, which for us are composed of five independent parts; development
    (cf. §VII-A), delivery (cf. §VII-B), deployment (cf. §VII-C), testing (cf. §VII-D)
    and orchestration (cf. §VII-E). It should also be taken into consideration that
    the inter-service data-based communication has already been covered in §VI-C by
    the communication middleware. Moreover, unlike in the previous section, we will
    focus now on the sub-features of each part instead of on their properties. A.
    Development In this section, we take an in-depth look at the first part of software
    delivery pipelines, software development. We will start by extensively focusing
    on the definition of software and what it consists of. Afterwards, as the development
    is at a very multifaceted stage, we will go through the software development methodologies,
    focusing on how they are adapting to on-going evolutions. 1) Software Characteristics
    As software packages can take many forms in the automotive systems, depending
    on their final target node or purpose, first we need to clarify which kinds are,
    for us, those that are the most important and, thus, those that we are going to
    use as the base for the following sections. It is also worth noting that these
    packages are usually composed of source code files, configuration instructions,
    testing instructions, various meta-data files containing their certificate, dependencies
    etc. The five kinds of software are as follow: Application Configuration packages
    are composed of, as indicated by their name, a set of run-time/post- installation
    parameter changes. These parameters include examples such as driver profile changes,
    deep learning algorithm optimisations, or parameter updates for regulatory compliance.
    These updates strongly condition vehicle behaviour of the vehicle without requiring
    any software changes, Due to this, these update types do not require the relevant
    software components to shut down, only for the vehicle to stop. Security / Key
    packages are composed of configurations allowing us to ensure the security of
    the on-board systems. These are usually done during vehicle production, for the
    initial set of rules and keys, and are only updated as and when there is a case
    of a newly discovered security threat. Firmware packages which include main system
    software that controls the underlying hardware. Thus, to both install and update
    these packages, a complete restart and re-flash of the ECU are required. Afterwards,
    the soft. must be fully tested and, if there are any errors, switched back to
    the previous firmware version through the use of techniques such as dual banking.
    This form of updates is used in Actuators and Sensors. Software packages including
    the installation of application components. These packages may contain the whole
    software or a partial/incremental software chunk, also known as Delta ( Δ ) software
    packages. Note that the size of the partial updates is typically close to the
    aforementioned Firmware packages. It is also important to bear in mind that for
    both full and delta software packages, the installation process must be performed
    when the vehicle is shut down. This process must also be tested afterwards and
    allowed to roll back if the system does not operate properly following the update.
    SOTA usually takes place over Unix-like systems, typically in infotainment or
    telematics ECUs. Media file packages, which include some multi-media files such
    as Global Navigation Satellite Systems (GNSS) maps, custom images, sounds, or
    videos for the In-Vehicle Infotainment (IVI). Note that these updates are considerably
    heavier than those described above. However, some academic proposals [413], [414],
    [415] suggest various solutions to decrease their size by splitting them into
    more-periodic incremental packages or by narrowing the package content for the
    user interests, e.g. downloading only your country for GNSS, only your language
    for audio/video/images etc. Moreover, it is also worth noting that one of the
    key challenges facing software development in the automotive industry is the need
    for elevated levels of reliability and safety. Automotive software must be able
    to perform reliably under a wide range of conditions, including extreme temperatures,
    humidity, and vibration. Thus, multiple mechanisms are integrated during the development
    phase to match these strong safety requireents. 2) Software Development Methodologies
    Traditional or sequential software development methodologies have been in around
    the industry over the last few decades [182], [184], [185]. These models, as we
    said in §IV-C, base themselves on a clear statement: all processes involved during
    the development of a product are phase-to-phase dependent on each other, needing
    to end th previous process before starting the next. These proposals also ensure
    to offer detailed documentation after each of the phases to enhance the action
    traceability. From traditional dev. methodologies, we can highlight two; the first
    one is the V-model, which is a classic sequential model, whereas the second one,
    Rapid Application Dev. (RAD) model is in-between the classic sequential models
    and the modern agile philosophy. V-model [182] (cf. Fig. 16), where V stands for
    verification and validation, is one of the most widespread industrial development
    methodologies thanks to its simplicity and robustness for projects where the initial
    set of requirements stays static, which has generally been the case for traditional
    industries until recently. However, even if the V-model has evolved through the
    years, nowadays as there are so many different variants we are going to focus
    on a standard classic variant. V-model is an extension of another classic model,
    the waterfall model [183], in which the progress of a project is seen as flowing
    steadily downwards through the phases of conception, initiation, analysis, design,
    implementation, testing, and maintenance, and it proposes a review after each
    phase so as to evaluate whether to continue or discard the project. Further adding
    to this, V-model adds product testing in parallel with each corresponding phase
    of dev. This way, the V-model still benefits from a fast set-up and management
    simplicity due to the rigidity and previously fixed specific deliverables and
    review process from the waterfall model. Parallel and exhaustive testing through
    each of the phases enhance the chances of success of the project, while simultaneously
    saving a lot of time which allows the dev. teams to re-adapt code that has already
    been tested and validated, in turn avoiding the downward flow of defects. However,
    the flaws in long/complex projects from these models, in which requirements may
    evolve over time, have a negative impact on the adaptability and evolvability
    of the software and limits the adaptability and innovation capacity desired by
    the evolution of the users and business requirements. FIGURE 15. IoV Interactions
    for smart cities. Show All FIGURE 16. The V-model & the Rapid Application Development
    (RAD) model basic conceptual schemes. Show All On the other hand, one of the first
    proposals we have that works through a multi-stage software development methodology
    is RAD [186] (cf. Figure 16). The RAD development process starts then by splitting
    the complete set of requirements into sub-modules, this way each cycle becomes
    smaller and easier to manage. Also, in this model, the first functional version
    of the software will come with the first module, with all subsequent releases
    viewed as new functionalities. This allows the client to evaluate the viability
    of the solution sooner, create new functionalities that become essential at different
    moments, etc. In this model, each iteration is thought to take up to 2 or 3 months.
    Furthermore, this software development model shall only be applied if the project
    is divided into exceedingly small and independent sub-modules. This way it will
    reduce global development time, increase the reusability of the components, and
    reduce the chance of rollback and integration issues. However, this model needs
    developers with high-level planning, modelling, and developing skills, who are
    able to reduce time and costs through automated code generation and software reuse.
    This solution is not considered suitable for parallel development nor teams with
    a high number of staff. Nonetheless, it brings interesting opportunities with
    regard to software reuse and incremental development, both of which are key points
    for increasing innovation pace, all the while reducing development costs. In 2001,
    and as s a way to unify all the aforementioned methods and develop a common and
    more flexible paradigm, a group of developers released the Agile Manifesto and,
    with it, the Agile development model, which is once again based on Incremental
    model logic. In Agile’s approach to software development, work is carried out
    in small phases, based on collaboration, adaptive planning, prompt delivery, continuous
    improvement, regular customer feedback, and frequent redesign, all of which result
    in the development of software increments that are delivered in successive iterations
    in response to the ever-changing customer requirements. Within the models deriving
    from the Agile manifesto, we can find several methodologies including Extreme
    Programming (XP), Scrum, Kanban, Lean, FDD (Feature-Driven Development), Crystal,
    DSDM (Dynamic Systems Development Method) etc. However, in this paper we are only
    going into detail about the first three, which in turn, are the most widespread
    of them all. Scrum [188], is the most common representative of the agile-based
    software development methodologies. It focuses on offering high project dynamicity
    and total flexibility all the while keeping the resources, budget, and delivery
    constraints low. To achieve this flexibility, Scrum signifies having a cross-functional
    team where every person contributes towards the best design solution and defines
    a clear set of roles so as to split the tasks efficiently. These roles are as
    follows: the Product Owner, who is responsible for defining, prioritising, and
    communicating the project requirements and its evolution; the Scrum Master, who
    manages the day-to-day team interactions and removes impediments to development
    and helps improve the process, development team and software product being developed;
    and the Development team, which is responsible for executing the tasks allocated
    within the deadlines. However, to maintain control and to fully profit from the
    team and client feedback, Scrum proposes splitting the development into small
    periods (which shall be the in the form of two weeks) called Sprints after which
    a new version will be deployed and a meeting with high-level managers and clients
    is set so as to evaluate and correct any necessary details as fast as possible.
    Also, in order to reduce futile meeting time, and as the planning is done by the
    PO, Scrum proposes 15-to-30-minute structured reunions on a daily basis in which
    the product development team members communicate and evaluate the progress status
    of software development and briefly discuss any blocking points or obstacles.
    However, it also has some drawbacks such as the lack of a definitive end-date,
    which often leads to scope creep, an elevated risk of failure if the individuals
    are not committed or cooperative enough, or the frustration of the development
    team when a task is blocked for a lengthy period pending client approval. Unfortunately,
    these drawbacks can compromise the speedy and efficient advance of the project
    as a whole. As the objective is to adapt the Agile Manifesto to smaller / less
    cross-functional teams, XP [187], this bases itself on simplicity and continuous
    iterations with the clients. In this methodology, requirements are presented as
    scenarios by users then split into a series of small tasks. As the scenarios are
    extremely specific, XP aims to implement where strictly necessary for each feature,
    making it larger when required by the customer’s needs. For that, the XP development
    process starts by developing and agreeing on the acceptance tests through feedback
    loops and only starts the real development once this is done. However, the cost
    effectiveness of this programming methodology relies on well-defined processes,
    version management and constant, clean, and understandable coding and refactoring.
    Finally, with the current objective being the enhancement of the task workflow
    visualisation and limiting the parallel work in progress during software development
    processes, we have Kanban [189]. This methodology aims to facilitate the delivery
    of software products just-in-time, and therefore maximise productivity. In short,
    influenced by the IT sector and attractive flexibility, dynamicity, cost effectiveness,
    adaptation to constant user and incremental client feedback, and the fast development
    capacities brought by the agile manifest, automotive industry software development
    methodology is abandoning little by little traditional solutions. However, in
    order to fully profit from these upcoming opportunities, it once again raises
    questions about the capacity of the vehicles to dynamically adapt in a safe and
    secure manner, as well as their ability to run all these new-coming features on
    highly restrained and heterogeneous embedded nodes ECUs without exploding the
    complexity of the system. B. Delivery Nowadays, software deployment in the Automotive
    sector is highly conditioned by the place in which this software deployment takes
    place, with different mechanisms and constraints existing whether it takes place
    in production after-sales, or in maintenance garages [416], [417], [418]. Furthermore,
    this deployment process takes place through different means depending on the size
    and safety impact of the packages to be installed. On the one hand, software and
    configurations for small software factory/garage deployment can be deployed physically,
    through the OBD-II port [419], [420], [421], or through the USB port for bigger-sized
    packages both in production, garages and after-sales. On the other hand, for all
    low-to-medium sized packages, this deployment can also be done directly and remotely
    with Over-The-Air (OTA) [422], [423], which is currently being implemented by
    most Automakers as it allows for the saving of millions of dollars, minimising
    repair delays, and reducing the environmental impact deriving from update campaigns.
    However, even though until now, the OTA update frameworks are not systematically
    used for all update any size and safety issues they may be caused if dealt with
    incautiously must be considered, studies continue to show [1], [424] that OTA
    importance will likely keep growing in the near future. In this subsection, we
    will begin by briefly going through the deployment possibilities and limitations
    of physical deployment means, - i.e., OBD-II and USB deployment, as they are already
    widely used by all automakers and, thus, overly complicated to change due to the
    economic impact that it would imply. Subsequently, we are going to focus hard
    on the state of the art of Over-The-Air software delivery, which is currently
    the main challenge faced by software deployment. 1) Physical Software Deployment
    Frameworks Even though The standard OBD-II port was first conceived to gain access
    to on-board diagnostics, it still allows automakers and other third parties to
    develop software updates, and even software embedded dongles, which interact safely
    and naturally with in-vehicle systems. Even though these packages that are to
    be installed rely on, as they also will do for most of the cases both in USB and
    OTA deployment, certificates, basic encryption libraries and a pre-loaded set
    of keys to secure this software transaction and prevent most of the attacks [324],
    [421]. Physical updates have some interesting capabilities as they allow, prior
    to having a full on-board system up and running and configured, the deployment
    of basic software and configuration, including the initial set of proprietary
    keys, which enables the rest of the software to have a secure update. Moreover,
    even though the OBD-II port data rate is considerably low, USB port allows us
    to bring in big Media file packages [425], [426], [427], such as GNSS maps, in
    an easier way than if we were using unstable OTA connections. It is worth noting
    that, OBD-II is nowadays considered to be the preferred technology for on-production
    software/configuration deployment, since only a really basic set of configurations
    are deployed in this phase due to time restrictions (cf. Fig. 17) and given the
    economic cost of changing the safety/quality control tooling already implemented
    and deployed in all factories and garages. Thus, in order for OTA deployment frameworks
    to be successfully integrated into the chain, they need to preserve, at least
    for now, the standards allowing the use of pre-existing OBD-II-based diagnostic/safety
    tooling. FIGURE 17. Factory software delivery process. Show All 2) Over-the-Air
    Software Deployment Frameworks OTA software deployment frameworks are an attractive
    attack surface for malicious users, as there is no need to directly access the
    vehicle hardware as you needed to do with the physical software deployment frameworks.
    These malicious users can operate multiple attacks on OTA software deployment
    frameworks, such as the classic Denial of Service and man-in-the-middle attacks
    [290]. It is worth noting that, if malicious software is introduced within a vehicle,
    it will be a critical threat for the passengers’ safety and, potentially, a vector
    to access other devices connected to the cars, such as smartphones, Bluetooth
    headsets etc., which in turn will have an enormous impact on the passenger’s privacy,
    or even their economy. In this subsection, we will delve into the state-of-the-art
    OTA deployment frameworks and evaluate them according to package security, authenticity,
    and integrity preservation. The first group of solutions guarantees the authenticity
    of data using symmetric encryption, asymmetric encryption, or both. In these solutions,
    the integrity of the data is reliant upon decryption. Interesting examples that
    represent symmetric key-based OTA frameworks include Mahmud et al. [300] and Mansour
    et al. [293]. In both studies, a secure software update framework is detailed,
    based on sharing an initial set of link keys among automakers, vehicles, and software
    suppliers, which is then used for encrypting both software and communications.
    In addition, they propose other mechanisms to enhance security and complex transmission
    traceability, such as time-hopping randomisation [300], or detecting potential
    errors and malicious behaviour, such as remote diagnostic tooling [293]. However,
    even if the computing requirements needed to perform the encryption are low, as
    are the set of keys is directly included in the vehicle by automakers, the impact
    of a key being compromised, thus making it impossible to link a message to its
    sender directly, poses a significant threat that does not match the requirements
    of safety and security-critical frameworks. In addition, neither implements a
    content integrity verification mechanism, therefore making it impossible to detect
    maliciously modified packages if an authorised key is compromised. Hence, to solve
    the problems linked to the risk of compromising a static set of symmetric keys
    and to improve message traceability, Steger et al. [294] propose a solution in
    which an asymmetric key is used to secure unicast communication, in addition to
    a symmetric multi-cast key from the service centre to several cars, thus enabling
    parallel updates. In this case, the only keys that will be shared are the public
    keys, making identity fraud difficult. However, this solution is again at potential
    risk of key dangers since the triggering action is centralised and there is no
    distributed network to ensure software package authenticity Similarly, the approach
    proposed by Mayilsamy et al. [297] involves combining asymmetric encryption and
    a well-known cryptography field, steganography. Their study proposes a solution
    that integrates software files encrypted by an asymmetric encryption algorithm
    (RSA in this case) hidden along the edge region of the cover image of the update
    using steganography. This self-verifiable stego-image would then be adequate for
    safe storage and transmission. However, the danger of long RSA keys (2048 bit
    in this case) being compromised remains an open challenge, and the costs and storage
    needs associated with using stego images are also unsuitable for the highly restrained
    nodes within the automotive industry. Further considering the integrity of package
    content instead of heuristic solutions for security during the transmission, we
    are going to outline discuss hash-based solutions. Based on this technique, Nilsson
    and Larson [295] propose a secure OTA firmware update protocol for connected vehicles
    based on dividing software and then hashing and encrypting each chunk. However,
    this division and hashing process appears inefficient in terms of computing and
    energy consumption compared to using software packages as a whole. Nilsson et
    al. [296] propose an alternative infrastructure in which a trusted portal calculates
    the hash of the whole software package and places it at the end of the message
    so the receiver can check the veracity of the message. However, in both approaches,
    having a centralised authority in charge of distributing keys is highly vulnerable
    to attack, a single point of failure, and identity usurpation attacks. Within
    the same scope as [296], we can consider the approach proposed by Kuppusamy et
    al. [290]. This solution secures key storage at a lower level by using Secure
    Hardware Module technology to handle key management. Their study also proposes
    an OTA framework that distributes updated software to ECUs in the form of images
    (containing collections of code and data) and metadata (containing image-related
    files such as the size of the file, the image hash, creation date, author, etc.).
    In addition, Kuppusamy et al. [290] suggest having different keys introduced in
    the hardware to verify the encryption of different files. However, this solution
    is vulnerable to rollback attacks due to a lack of proper verification mechanisms
    during software update installation. This system also suffers from the same issue
    as those in the previously discussed approaches of being centralised rather than
    distributed. Through optimising hashing solutions by adding distributed middleware
    and some new optimisation mechanisms, some studies propose using blockchain-based
    solutions. In the blockchain, software packages are linked to each other immutably
    and spread through the different nodes integrating the system; in this case, it
    is always possible to use a simple majority vote approach to detect malicious
    or erratic introductions. Thus, this technology guarantees data integrity, complete
    traceability, and higher consistency and security than the aforementioned techniques.
    However, there are also some drawbacks-for example, blockchain-based solutions
    do not allow, or barely allow, for the modification of past data, rely on the
    secrecy of private user keys, require substantial amounts of storage and significant
    computational resources (for the Proof of Work and other cryptographic mechanisms).
    On top of that, their relationship regarding future legislation and regulations
    remains uncertain. Nonetheless, in recent years, increasing efforts have been
    expended to develop new techniques which reduce the resource consumption of this
    solution and allow its deployment in the IoT world, such as pruning and new, less
    resource-consuming proofs. In the automotive sector, multiple papers have proposed
    the use of blockchain-based OTA update frameworks. However, most of these, such
    as Steger et al. [298], are based on the Proof-of-Work mechanism, while others
    such as Witanto et al. [428] and Mtetwa et al. [429] focus on other parts of the
    implementation such as peer-to-peer exchange or transmission details, respectively.
    However, the resource consumption constraints of embedded vehicle architecture
    suggest that Proof-Of-Work algorithms are unsuitable for such systems, hence indicating
    a key limitation to these proposals. Additionally, all the cited blockchain solutions
    follow traditional public blockchain schemes, which we consider unpractical in
    the automotive software development context, where the blockchain publishers are
    varied and with different interests (i.e., companies that need to use this chain
    as a mean to sell their software, internal automaker developers…). Therefore,
    other solutions such as Blanco et al. [430] take these factors into consideration
    and focus on reducing the computing, energetic and storage needs of blockchain-based
    software deployment frameworks by proposing a proof-of-authority based multi-provider
    collaborative software deployment framework that is both public to publish and
    privately managed by the Automakers. C. Deployment As the software has already
    been safely deployed in the vehicles by this stage, in this section we are going
    to focus on the software deployment/install process. Firstly, we are going to
    look at the Firmware deployment process, then, less specifically at a single hardware,
    the OS deployment process and, finally, making an abstraction of the hardware
    resources below, the software virtualisation deployment process. 1) Firmware Even
    though Firmware started, mostly for MCUs, as a simple combination of a hardware
    device and some computer instructions and data residing in the read-only memory
    (ROM) of the hardware device [431], nowadays, firmware is stored in the Electrically
    Erasable Programmable Read-Only Memory (EEPROM), which is divided into program
    and data memory, which, in turn, enables it to be updated after being deployed.
    With regard to the normal operation cycle, traditionally, the program memory contains
    an application to run a flash-loader which contains the instructions on how to
    re-flash a new application to the application memory, and, finally, a boot manager,
    which will determine whether it is operating in flash-loader mode or application
    mode, always choosing application mode if a valid application is available. Furthermore,
    the data memory will contain a flash-driver agent which will be able to coordinate
    a new installation or update with the flash-loader. Thus, when wanting to install
    a new firmware or update the already installed one, as you can see in Fig. 18
    the boot manager will start on flash-loader mode, which will trigger the flash-driver.
    Once the flash-driver is active it orchestrates the installation of the new firmware
    application with the application memory. Once installed, it will re-boot in application
    mode. It is worth noting that if the node has no EEPROM memory, or if it is too
    small, the flash-driver will be loaded onto the Random Access Memory (RAM) or
    directly with the flash-loader [431], [432], [433]. FIGURE 18. Schematic view
    of the Firmware flashing process. Show All Moreover, in the automotive sector,
    as firmware usually handles safety-critical components, some techniques need to
    be implemented over the traditional mechanism. The first technique we want to
    focus on is the Secure Boot feature whose objective is to increase protection
    tampering attacks. This feature complements the mechanisms presented in the §VII-B
    by loading an initial set of keys to the ROM memory that are only known by the
    Automaker and used to guarantee software integrity [434], [435], [436]. Furthermore,
    with the objective now of reducing installation time and increasing safety by
    always keeping a backup of the previous firmware version, we have A/B, also called
    dual-banking, mechanisms. This is when the ECU contains two identical partitions
    so that one can be updated and tested while the other is running. This helps to
    make the process as seamless and problem-free as possible. This way, once the
    firmware is installed and tested in the idle partition, the car only needs to
    restart the MCU to boot up the new application memory, while leaving the other
    one as a backup [436], [437]. Finally, the last extra-mechanism that we want to
    highlight concerns the delta-firmware updates [436], [438], [439], which are used
    to reduce the size of the patch, consuming less bandwidth and storage within the
    cars. The former is extremely important considering that MCUs are mostly connected
    through low data-rate CAN networks nowadays (cf. §VI-A). In delta updates, differential
    compression algorithms, such as bsdiff [440], rsync [441] or xdelta [442], are
    used to create a patch that only contains the part of the software that has changed.
    Even though these advancements speed up and enhance the security and safety of
    firmware flashing, it is always necessary to, at least, restart the MCU, which
    is not possible to be done during run-time, which, therefore becomes a complicated
    factor for in-vehicle dynamicity and flexibility. In addition, firmware updates
    are, by definition, highly coupled to hardware and thus, considerably harder to
    reuse. Moreover, firmware installation does not support, to the best of our knowledge,
    multiple simultaneous applications deployed at the same time. 2) Operating Systems
    Operating systems, similar in many ways to Firmware, but more complex and less
    hardware specific, are a set of software that normally runs in kernel mode, providing
    the application developers with a clean abstract set of resources that can be
    exploited and used to manage hardware resources associated with enhancing the
    capabilities of the devices. Appearing in the 1950s for mainframes, their evolution
    and flexibility has led to a complete palette of choices depending on system constraints
    (processing, memory management, networking…) and preferences (real-time support,
    security and safety, power consumption…). In this part, we will mainly focus on
    the Operating Systems that are able to run in resource-constrained devices, which
    therefore excludes classic PC Operating Systems such as Ubuntu or Windows and
    the server-oriented OSs. This, combined with the aforementioned automotive requirements
    and limitations, will orientate our research towards those deployed in (1) IoT
    / sensor nodes, (2) embedded systems or (3) automotive nodes, while also carrying
    out a survey of those focusing on (4) real-time. In order to compare the different
    systems with each other, throughout this section we will be bearing in mind certain
    aspects such as OS architecture and kernel, scheduler characteristics, programming
    model, languages permitted, real-time support, memory management techniques, security
    and safety evaluation, power consumption, and multimedia support (Audio/Video).
    An extensive ranking of all the OSs mentioned in this section is detailed in Table
    10. TABLE 9 Comparison of the Over-the-Air Frameworks Found in the State-of-the-Art
    TABLE 10 Exhaustive Comparison of the Different Operating Systems Analysed a:
    IoT / Sensor Node Operating Systems Sensor nodes and IoT are small-powered computers
    with built-in radio, suddenly deployed in high quantities that operate simple
    calculus and possess poor memory capacity. These systems run small, but complete,
    operating systems that normally follow event-driven programming models in order
    to respond to external events or to periodically take measurements based on internal
    interruptions. The major problem concerning this system is how to extend the battery
    lifetime as much as possible because of the high number of sensors and the difficult
    physical access for substitution. Thus, these kinds of OS are simple and small,
    implementing sleep-awake periods and other energy efficiency techniques. Among
    these solutions we can include TinyOS [443], Contiki [444], MantisOS [445] and
    LiteOs [446]. First, TinyOS is an open-source non-Linux based OS that was explicitly
    designed for this use case: low-end IoT devices. This solution has a monolithic
    kernel and uses a component-based arch. that depends on the requirements of the
    application. This reduces the size of the code needed to set up the hardware.
    In addition to that, this solution follows an event-driven programming concurrency
    model which consists of split-phase interfaces, deferred computation, and asynchronous
    events. TinyOS also implements other techniques for updating and efficiently scheduling
    soft. on demand. However, this solution implements poor security mechanisms to
    keep the OS overhead to the minimum. Following the same logic of lightweight non-Linux
    OS, we have the Contiki option: a modular architecture with an unchangeable kernel
    that allows the app. modules and subsystems, which will make up the Contiki system,
    to be downloaded. In addition to that, it adds a system of polling to check update
    availability, some slightly safer security mechanisms (TLS, DTLS available) and
    the possibility to support both event-driven programming models and multi-threading.
    However, the energy consumption of this solution is slightly higher than TinyOS.
    After that, regarding a cross Linux/Windows environment solution, we can turn
    our attention to MantisOS. MantisOS is a microkernel-based solution that does
    not follow the event-driven programming model it (only allows multi-threading).
    Adding more security compared with the past two solutions and various semaphores
    to deal with the inter-thread comm. (which implies higher energy cons.), MantisOS
    presents itself as an interesting solution for applications with more sensitive
    data (or with higher criticality). However, none of these solutions comply with
    the norms without adding extra security and safety mechanisms. Finally, we have
    LiteOS, which is the most used Linux-based solution over sensor nodes. This Operating
    System was conceived to provide an UNIX-like environment for IoT developers and
    to supply programmers with familiar programming paradigms such as a hierarchical
    file system developed using LiteC and a UNIX-like shell. Thus, just as with linux,
    it has multi-threading capabilities and a higher security and safety level (complying
    with ASIL D) than its predecessors. The best part of this solution is its power
    consumption, which is rather similar to TinyOS. b: Embedded Systems Operating
    Systems Embedded systems are computer systems that have a dedicated function within
    a larger mechanical or electronic system. These kinds of systems usually don’t
    allow users to install new software and have strong real-time computing constraints.
    In addition to that, as these systems form part of a larger system, safety and
    security is sometimes required. Thankfully, as user software installations are
    not allowed, the possibility of having untrusted software running on your system
    is reduced, which acts as the first security barrier. In this type of OSs, we
    should focus onQNX Neutrino [447], WindRiver VxWorks [448], [449], Embedded Linux
    (uCLinux) [450], Android Things [451] and Raspbian [452]. To begin, uCLinux is
    an open-source Linux-based OS that extends the classic Linux kernel to enable
    its operation in micro-controllers without MMU (memory management unit). As in
    the classic Linux, uCLinux follows a monolithic architecture to which you can
    add different CPU platforms and file systems that are more adapted to embedded
    solutions. At the same time, uCLinux supports multi-threading programming, a wide
    set of networking and communication protocols as well as using a priority-based
    preemptive scheduler to manage call executions. However, the security mechanisms
    that uCLinux implements are not completely secure and the energy consumption of
    the Operating System could be lower, despite the kernel sleep mode possibility.
    Then we have Raspbian, Android Things and QNX Neutrino, all of which are based
    on the classic Linux Kernel. Despite their similarity to uCLinux, all these solutions
    need to have, or benefit strongly from having, an MMU to operate under. Raspbian
    is a solution conceived for RPi hardware that has lower energy consumption and
    higher security while operating on their specific boards. However, when it operates
    over other kinds of hardware, its performance and energy consumption are mostly
    similar to those of uCLinux. With even safer security mechanisms, we have QNX
    Neutrino. QNX Neutrino is a solution focused on the security and reliability for
    the automotive and medical industry. However, this linux-based solution is less
    optimal in terms of energy consumption, but it is compliant with the automotive
    ISO 26262 D level without needing any modifications. Finally, we have the leader
    of the smartphone market, Android who have presented Android Things to conquer
    the OS ecosystem of the embedded system by offering the development tools, the
    wide android store, and the strength of the big Android developing community.
    This port of Android manages to keep power consumption low, even lower than the
    solutions that have already been presented, despite the security of the system
    which was already representative of the Smartphone Android OS. Finally, as the
    only remarkable non-Linux embedded OS, we have WindRiver VxWorks. Widely used
    by the aeronautical, automobile and telecommunications industry, this system is
    an alternative if you need a real-time safe (ASIL D) operating system without
    an excessively high energy consumption. c: Automotive Operating Systems Currently
    targeting those Operating Systems designed or adapted to recognise the Automotive
    on-board node needs and use-cases, we can find Autosar Classic OS [453] and Autosar
    Adaptive OS [454], [455]. These target devices that don’t require visual interaction
    with the user and Windows Embedded Automotive 7 [456] and Android Automotive OS
    [457] for the vehicle entertainment system. For starters, nowadays Autosar Classic
    is the most widespread OS choice for the OEMs, which are progressively transitioning
    to Autosar Adaptive. Autosar Classic has a decent energy consumption and lots
    of interesting features that have been developed over the years to adapt to upcoming
    automotive needs. However, the biggest features missing from Autosar Classic,
    and that are already covered by Autosar Adaptive, are the file mgmt. system, the
    security mechanisms, and the lack of compatibility with the incoming Linux libraries.
    To conclude, as a way to conquer the IVI systems in the vehicle, Android (Android
    Automotive OS) and Windows (Windows Embedded Automotive) both launched their OS
    for entertainment in the automotive industry. However, since Android had already
    won the smartphone market and the Google Play Store brings a lot of new apps and
    developers, it also won the automotive market, leaving windows embedded automotive
    as a dead-end project. As you can see in Table 10, it is worth noting that even
    though some of the aforementioned technologies have not yet been targeting the
    automotive systems, Automakers have managed to implement them in the on-board
    systems. d: Real-Time Operating Systems (RTOS) As their name implies, these Operating
    systems focus on the real-time reactivity of the nodes. Many of these are found
    in industrial process controls, avionics, military, and similar application areas.
    Thus, these systems must provide absolute guarantees that a certain action will
    occur by a certain time. Even if real-time constraints will also appear within
    some of the embedded system and automotive OSs, these are not as precise time-wise
    as RTOS. In this category, we can find various solutions focusing on low-end devices
    such as FreeRTOS [458], RIOT [459], Mynewt [460] and others that focus on medium/high-end
    devices such as Nucleus RTOS [461], Green Hills Integrity [462] and TizenRT [463].
    As one of the main representative OSs for low-end device RTOS, FreeRTOS is an
    open-source non-Linux real-time OS for low-end devices. It supports an extensive
    variety of hardware architectures, which makes it an excellent choice for heterogeneous
    environments. This solution is based on a rather small micro-kernel that supports
    multi-threading and includes a preemptive scheduler. Therefore, the small size
    of the kernel makes this solution very scalable, simple, easy to use, highly portable
    and with an incredibly low energy consumption. However, the security of this Operating
    System relies only on a lightweight OpenSSL substitute: WolfSSL, which means it
    is indirectly adapted to the automotive environment requirements without extra
    development work. However, with the appearance of the SafeRTOS extension, it gains
    ASIL D security and safety pre-certification, making its adaptation to the automotive
    industry much easier. In the same scope, we have RIOT which is another open-source
    non-Linux based OS. RIOT supports many interesting functionalities such as hardware
    abstraction, interruption handling, memory management, IPC and good reliable synchronisation,
    performance, scalability, and an energy consumption similar to FreeRTOS. On the
    other hand, if we are searching for an open-source Linux-based real-time representative
    solution, there is Apache Mynewt OS. This solution aims to bring a linux environment
    (and the developing advantage that this implies) to low-end devices that have
    limited memory and storage capabilities and that need to operate for a long time
    under power constraints. With the same level of functionalities, the consumption
    of this solution is similar to the ones aforementioned, however, the granular
    power control mechanism that it implements allows us to reduce consumption a great
    deal when some functionalities are not useful for the application. However, this
    solution has poor security capabilities that must be reworked and redesigned in
    order to bring this solution to the automotive world. If we consider RTOS for
    higher-end IoT devices, we have Siemens Nucleus RTOS. This OS was designed for
    industry (medical, aerospace, automotive…) and thus matches all the security requirements
    imposed by the ISO 26262 D level. It also offers interesting features such as
    a power manager, 64-bit support, support for heterogeneous computing multi-core
    System on a Chip (SOC) processors, and some domain partitioning and isolation
    techniques. All of this makes this OS an interesting solution for the automotive
    high-end IoT nodes. In the same range, we have GreeHills Integrity OS. This Linux-based
    solution offers scalable run-time environments with secure partitions, safe (ASIL
    D), secure embedded multi-core virtualisation, fast boot and advanced development
    tools that lower development costs and reduce the time to market. Finally, we
    have Samsung’s solution: Tizen (for high-end devices) and TizenRT (for low-end
    devices). Tizen was designed and introduced by Samsung for their mobile devices,
    wearable devices, and smart TVs. Therefore, the energy consumption and multimedia
    capabilities of the OS are remarkably interesting for the automotive industry.
    However, as they are not designed for a safe-critical domain, the security capabilities
    are far from optimal. To sum up, if we look at Table 10, we can see that most
    of the operating systems matching both the consumption and security requirements
    are Linux-based, which helps to decrease the development complexity. For the low-end
    sensor nodes on the system LiteOS, if real time is not required, FreeRTOS and
    Autosar Adaptive show themselves to be the most complete solutions. However, due
    to its compatibility with Autosar Classic and all the current infrastructure,
    Autosar Adaptive seems like the best choice for, at least, the transition to a
    more centralised architecture. If we look now at higher-end solutions, we have
    more choice. Nucleus RTOS (if a non-Linux system is preferred), Integrity OS,
    VxWorks and QNX Neutrino are safe and secure Operating Systems designed for the
    automotive industry. All of them will be adequate and the choice will depend mostly
    on the pricing and support that the companies are willing to offer to the OEMs.
    On the other hand, Android Automotive OS is an interesting solution for the IVI
    since the application store and development community is by far the most active.
    However, its safety is lacking, and it needs extra development for its conception
    errors to be compensated. 3) Software Virtualisation Thus, in order to make the
    installation and development of new applications more flexible, dynamic, reusable,
    and simple, we usually put the applications over an Operating System, as they
    are no longer firmware applications but software applications, which is already
    the case for the Automaker MPU nodes. Even though we usually use firmware for
    the MCUs due to their restrained resources, there are other possibilities as the
    micro-kernel OSs presented above in Table 10, such as RIOT, MantisOS or FreeRTOS.
    Moreover, when installing the software over an operating system, there are other
    mechanisms on top of this to increase system flexibility and adaptability even
    more. The most common way to do this is virtualisation, which presents itself
    as a technology able to combine or divide computing resources. This allows them
    to present one or many operating environments using methodologies like hardware
    and software partitioning or aggregation, partial or complete machine simulation,
    emulation, time-sharing etc. In other words, visualisation allows a single ECU
    of any kind to run sets of code independently and in isolation from other sets.
    Note that virtualisation is already a critical feature in other sectors such as
    cloud computing or artificial intelligence and provides higher resource orchestration,
    easier environment maintenance, less expensive sand-boxing, the ease with which
    to maintain multiple execution environments with low resource consumption, techniques
    to facilitate the deployment of apps multiple systems, etc. Even if most of the
    visualisation techniques present similar advantages and environments to the end
    user, the levels of abstraction in which they operate, and the underlying architecture
    tend to vary extensively. Therefore, depending on the abstraction in which they
    operate, the virtualisation techniques can be classified, as set out in Table
    11, into the following: (1) virtualisation at the instruction set architecture
    level, (2) at the hardware abstraction layer, (3) at the operative system level,
    (4) at the library level and (5) at the application level. Finally, we will also
    address the virtualisation evolution perspectives over (6) MCU nodes.- In addition,
    as you can see, the layer in which the virtualisation is applied implies a quite
    different performance for the solution, being able to identify patterns within
    all the solutions at each level. TABLE 11 Exhaustive Comparison of the Different
    Virtualisation Solutions a: Virtualisation at ISA Level ISA Level virtualisation
    consists of emulating an ISA completely with middleware. Then, an emulator will
    transmit the instructions to a set of native instructions and then execute them
    on the available hardware. This technique presents advantages when dealing with
    multiple platforms since there is no binding between the guest and the host operative
    system. However, the performance of this solution is far from optimal. Among the
    virtualisation solutions that come from this logic, Bochs [464], Crusoe [465],
    QEMU [466] and Bird [467] all stand out. It is worth noting that the inefficiency
    of this solution comes from the overhead added by the system call translator,
    with each of the aforementioned solutions having diverse ways to approach this
    problem. The studies show that QEMU and Crusoe are those with the highest level
    of efficiency. b: Virtualisation at HAL Level These solutions try to exploit the
    similarities between the architectures of the guest and host platforms so as to
    cut down on interpretation latency. Thus, this technique helps to map virtual
    resources to physical resources and uses the native hardware for computations
    in the virtual machine. This mapping and the resource management process is managed
    by a control software called Hypervisor. There are multiple ways to implement
    HAL virtualisation, however, in this paper, we will focus on the two most widespread:
    Full-Virtualisation, in which the guest OS are fully independent, both from the
    rest of the host and guest OSs, and are built either on the top of the physical
    hardware (a - Type I or Bare-Metal Hypervisor) or on top of an existing OS (b
    - Type II or Hosted Virtualisation), and (c -) Para-virtualisation, in which the
    guest OS kernel must be modified to act as a bridge between the different apps.
    and the hardware resources. Type I or Bare-Metal Hypervisor. In Type-I hypervisors,
    the hypervisor is placed directly over the hardware. It is small as its main task
    is sharing and managing hardware resources between the different OS running in
    the machine. The major advantages of this solution are that it achieves a performance
    at almost 100% of its capabilities and that any existing problem in one of the
    different guest OS running on the hypervisor will never affect any other guest.
    We are going to analyse and consider the following as representative solutions
    to this model,Linux KVM [468], VMware ESXi [469], XEN [470] and Hyper-V [471].
    Type II or Hosted Virtualisation. In Type-II hypervisors, the hypervisor runs
    over a host Operating System instead of directly over the hardware. Even though
    it runs over a host OS, the hypervisor can support other OSs above it. This technique
    is easier and faster to configure and allows for better specification policies.
    However, the performance of these kinds of hypervisors is slightly slower than
    Type I (while still performing better than emulators). Another disadvantage of
    this technique is that the guest OSs are dependent on the host OS for its operations
    and thus, any problem with the host will affect all the other hypervisor guests
    too. We are going to review the following as representative solutions to this
    model: VirtualBox [472], VMware Workstation Player [469], Xvisor [473] and Linux-VServer
    [474]. Para-virtualisation. In para-virtualisation, as previously stated, the
    guest OS kernels need to be modified so as to function as a bridge between the
    other guest OSs, the applications, and the hardware resources. In that way, guests
    are able to recognise the presence of the para-virtualisation hypervisor and communicate
    through it with the hardware resources in a more efficient manner. To enhance
    this efficiency, the hypervisor will distribute control over the different highly
    requested hardware materials (hard disk, network interface, CPU…) among all the
    guests. Therefore, by installing a set of drivers, the guest OS that has control
    of a component (in this case OS-1) is able to efficiently redistribute and schedule
    the call executions from the other concurrent guests (in this case OS-2). Para-virtualisation
    technologies are more efficient and performant than any of the aforementioned
    virtualisation techniques. However, their efficiency comes from creating new intra-guest
    dependencies and reducing the isolation between concurrent operative systems,
    therefore creating new hardware / software coupling constraints and possible security
    vulnerabilities. Multiple projects such as XEN or VMware have para-virtualisation
    solutions. However, since they share the same logic concerning the solutions presented
    before, by adding the concept of system call redistribution they all see their
    performance increase as well as their intra-guest isolation decrease. Another
    interesting para-virtualisation solution is Lguest [475]. This offers a lightweight
    hypervisor built into Linux kernel and that does not provide any fancy features
    that other hypervisors do, but it does allow for an easy development and test
    environment. c: Virtualisation at OS Level If we now review virtualisation the
    Operative System Level, these solutions tend to have a high degree of isolation,
    as well as support for different OSs and applications without requiring rebooting
    and can carry out complicated dual boot setup procedures with minimal risk and
    easy maintenance. In OS level virtualisation, the kernel allows for the existence
    of isolated user space instances. Such instances can be Containers, Zones, Virtual
    Private Servers, virtual environments, virtual kernels or jails. A program running
    inside one of these instances will only be aware of the content and devices assigned
    to it and not all the resources available in the whole machine. If we focus on
    the main implementation, Containers run over a host OS (Linux-based normally)
    which needs a containerisation control engine. Contrary to the classic hypervisor-based
    solutions, this technique does not need the guests to run their own OS but it
    is the host kernel that allows for multiple isolated user-space instances to which
    the necessary libraries and bins are added. This is much more performant, with
    a lower boot time while being considerably more lightweight. In addition to that,
    this technology implements diverse isolation mechanisms, resource-management features
    that limit the impact the containers have on the others, network abstraction mechanisms
    etc. This technique allows for the execution of mono-functional apps. in isolated
    environments at a reasonable resource cost. However, containerisation is less
    flexible than full virtualisation since you cannot have different host OS and
    guest OS for your apps. Finally, with regard to security, it is a misconceived
    thought that containers ensure security boundaries like VMs do. Containerisation
    is more like packaging and delivery mechanisms and the security of the system
    relies not on the containers but on the host OS. Containerisation is a trending
    subject these days and therefore, multiple, and heterogeneous solutions appear.
    The most widespread solutions are are Docker [476], Linux Containers (LXC & LXD)
    [477], [478], Rkt [479], [480], Windows Containers [481] and Podman [482]. d:
    Virtualisation at Library Level This technique relies on the principle that applications
    are programmed using a set of APIs exported by a group of user-level library implementations.
    Such libraries are designed to hide the OS related specific details so as to keep
    it simpler for programmers to use. Thus, in this technique, virtualization is
    done above the OS layer by producing a different virtual environment through exposing
    different but equivalent binary interfaces to be able to emulate the application
    binary interfaces and application program interfaces needed to run an application.
    Examples of these applications. are WINE [483], WABI [484], LxRun [485] and Visual
    MainWin. e: Virtualisation at the Application Level In this technique, the idea
    is to create a virtual device at application level that can behave like a machine
    to a set of applications, just like any other machine. This virtual machine includes
    registers, stack…and the byte code that the application requires to run together
    with this abstract machine. Subsequently, the abstract machine will interpret
    the compiled byte-code into machine instructions. By using this technique, applications
    can be run in multiple different machines and distributions with the same expected
    behaviour. In other words, these applications are aligned with the philosophy
    of Write Once Run Anywhere. The main representative technologies of these virtualisation
    techniques are JVM [486], Microsoft.NET CLI [487] and Parrot [488]. These three
    technologies profit from the compiling process to generate files containing VM
    commands that run the code for Java (JVM), C++ (.NET CLI) and Perl (Parrot). As
    these machines are very finely grained, the performance is close to the natively
    compiled program if used correctly but it can considerably slow the performance
    if not developed according to the recommendations. f: MCU Virtualisation All the
    aforementioned techniques consume, each one in its own measure, a high amount
    of processing time due to the need to virtually emulate hardware. That, in turn,
    poses a problem for the current automotive embedded architecture, mainly composed
    of low-cost micro-controllers. Until now, in these resource-constrained environments,
    the partitioning of mixed-criticality systems has been mainly implemented through
    federated architectures (by adding a physical separation of several subsystems
    across different MCUs). However, with the exponential rise in functions, this
    physical separation becomes impractical and inefficient in terms of size, weight,
    power, and cost. Therefore, the concept of virtualization becomes interesting
    to ensure safe and security-critical functions that operate at a lower complexity
    and cost. Hence, the increasing demand of MCU-based virtualization is leading
    the academic researchers and industrial developers to put significant effort into
    developing lightweight virtualization solutions. This way, we can find some interesting
    solutions such as, ARM TrustZone Assisted Hypervisor [489], [490], PRPL-HYPERVISOR
    for MIPS micro-controllers [491] and Femto-Containers [492]. D. Testing As more
    safety-critical car functions rely on software (i.e., self-driving functions,
    V2X…), testing the new upcoming applications in real live traffic has become dangerous
    and has already caused fatalities. Thus, in this context, exhaustive virtual testing
    offers a more cost-efficient and safer alternative compared to operational tests
    [502]. This virtual testing can be done in multiple ways, however, for this paper,
    we are going to focus on three of them. We are going to rank them from the most
    to least decorrelated with regard to the vehicle environment, and they are as
    follows: shadow-mode testing, sand-boxing, and model-based simulations. First
    of all, model-based simulations allow for the development of high-level test models
    than can be used from the early stages of the development process, being able
    to integrate not only the software constraints, but also the electrical and mechanical
    aspects entwined. These models allow us to find a common functional understanding
    and validation early in the design phase, all the while improving the communication
    within development and engineering teams, reducing the time-to-market through
    component reuse, and strong validation prior to the implementation. Model- based
    simulation provides a development process from requirements-to-code, ensuring
    that the implemented systems are complete and behave as expected. Some examples
    of these testing techniques are [493] and [494]. However, as the number of apps
    and variants rise, creating suitable complete test scenarios is becoming increasingly
    laborious and difficult to ensure that they are complete in all situations. Nonetheless,
    to deal with this complexity, a recent research trend suggested using automatic
    test-case generation based on search-based procedural contents [502], [503] or
    machine learning analysis from real situations [504], [505], [508]. If we look
    closer at model-based automatic testing in the IT sector, we believe it is fundamental
    to highlight the possibilities that exist that help reinforce learning as a way
    to continuously revisit the tests, adapting granularly to the incoming applications
    and consequently enhancing the on-board system safety and security [509], [510].
    Secondly, the objective of sand-boxing is to create a close-to-reality software
    environment, making it easier to both test and develop new software, integrating
    these environments much easier with classic CI/CD tooling such as [511], [512],
    [513]. Even though sand-boxing does not seem to have the same importance now in
    the automotive systems as it does in the IT sector, there are some interesting
    examples in literature [495], [496], [497], [498]. Moreover, sand-boxing will
    not only help to ease the development and testing of new applications but also
    to make the automotive sector easier to access for new players. However, just
    like it did for model-based simulations, the incessantly increasing number of
    applications and variants makes it more and more complex to cover all the test
    cases within the sand-boxing environment, taking longer and longer to test all
    the available possibilities. Finally, shadow mode testing, first introduced by
    Tesla on their vehicle fleet, and which relies heavily on virtualisation tools.
    Shadow mode consists of introducing a software to be tested directly in the car,
    but in an isolated environment, in which this software can collect all the necessary
    inputs of all the software in the environment without intervening in the operation
    of the car [514]. This way, the software tested registers the actual behaviour
    and what the system may have reacted under real-case circumstances. Thus, considering
    the magnitude of the car fleets, this mechanism allows us to rapidly cover most
    of the different application combinations, behaviours, and software contexts,
    while also detecting errors that could not have been foreseen otherwise. Moreover,
    these statistics that have been gathered can later be used to refine the simulation
    testing phase or as part of the previously presented reinforced learning mechanisms.
    Even though, this technique has barely been explored until now [499], [500], [501],
    if compared with the precedent techniques, we believe it can be a key-mechanism
    to lessen future automotive integration and development complexity. On the other
    hand, addressing not how the testing criterion is defined but if the software
    has been successfully integrated into the in-vehicle systems, the verification
    & validation mechanisms mainly consist of [506], [507], and [507], simply following
    a testing procedure, which is compliant with the safety and security certifications,
    that comes with the installation package. In addition, this testing is not only
    done after installation but also throughout the control service §VIII over time
    to detect any potential unexpected failures, allowing us to bring the car to a
    safe-state mechanism, even if this state usually implies a temporary reduction
    of the functionalities. It is worth noting that this process is similar for testing
    software in factories, after-sales, and on the road. E. Orchestration Having already
    gone through how to safely deploy, install and test the in-vehicle software, the
    next step, and the one we are addressing in this section, is how to decide into
    which of the ECUs in the system each application shall be deployed. Contrary to
    the precedent sections, in which each application could be treated separately,
    orchestration must be evaluated from a global point of view in order to guarantee
    that we individually fulfill, in the most optimal way, all application resource
    requirements (i.e., computing, storage, bandwidth…) but also collectively all
    the functional latency and safety requirements for every functionality [515].
    For example, when we deploy an ADAS function, such as the Anti-lock Braking System
    (ABS), the function dealing with this functionality needs to be able to command
    braking under extremely low-latency constraints, which might not be possible if
    the distance to the software dealing with brake management is too far away. In
    brief, it can be said that the overall performance of in-vehicle systems critically
    depends on the different service resource allocation. Until now, as the number
    of applications to be installed was very restrained and the update / new functionality
    delivery periodicity followed long-time periods, this application orchestration
    was done by the Automaker’s workforce during the design phase [216]. They directly
    assigned the software components to a certain computing unit and then testing
    and certification of the correct operation of this given mapping was carried out.
    Furthermore, this mapping was manually revisited by the Automakers when a new
    functionality had a significant impact on the system. However, revisiting and
    certifying this software mapping study for each variant or significant system
    update is time-consuming and, as the number of applications and functionality
    delivery periodicity rise, cannot be maintained over time. As you can see in Fig.
    21, the inclusion of dynamic context-dependent functionalities, such as the V2X
    functions [516] punctually used and depending on the driving situation, has meant
    that the traditional approach of statically orchestrating hardware and network
    resources is no longer optimal for all the different vehicle states and environmental
    variations. It is important to note that this is even more noticeable if we think
    of more futuristic cases such as seamless inter-vehicle cloud assisted architectures
    [517], [518], [519] in which any vehicle could be requested to run another function
    of any other vehicles at any given time. Therefore, in this context, it seems
    that going from static orchestration to dynamic orchestration should solve the
    dynamicity and variability issues, as was already the case for the Cloud Computing
    sector. FIGURE 19. Virtualisation examples. Show All FIGURE 20. Virtualisation
    examples. Show All FIGURE 21. In-vehicle applications dynamicity levels. Show
    All On the one hand, we begin with those proposals targeting the orchestration
    of resources with networking as decisive criteria. First, Khalili et al. [520]
    study the radio resource allocation in a multi-cell orthogonal frequency-division
    multiple access (OFDMA) cellular network, focusing on locally minimising energy
    consumption through a variable relaxation, memorisation and minimisation (MM)
    method. On a similar scope, targeting once again a centralised infrastructure
    dealing with multiple parallel requests, Kuang et al. [521] investigate the joint
    problem of partial task offloading and resource allocation between mobile devices
    and the Edge Computing infrastructures with the objective of minimising the execution
    delay and energy consumption from the network point of view and proposes a multi-layered
    algorithm combining genetics and flow-shop scheduling for the upper layer and
    convex optimisation techniques for the lower level. If we take a more in-depth
    look at those contributions, targeting the Automotive sector, Li et al. [522]
    and Khan et al. [516] both address the issues of existing competition for comm.
    among mobile users when offloading tasks to the Edge, the first through a simplification
    and an auction based offloading algorithm, and the second through a multi-factor
    prioritisation function and a budgetary scheduler. With the same objective, but
    now within intelligent transportation systems, Mittal et al. [517] proposes a
    centralised multi-factor prioritisation mechanism that calculates the offloading
    orchestration based on vehicular movements along urban roads and their expected
    trajectory. Finally, and as the only proposal that we have found with reference
    to in-vehicle networks, Hu et al. [523] study a holistic scheduling problem for
    handling real-time apps. in time-triggered in-vehicle networks, proposing a new,
    highly-flexible scheduling algorithm called Unfixed Start Time, and, to compensate
    for the conflicts with both a rescheduling and a backtracking mechanism. On the
    other hand, now we are going to focus on the proposals that use the computing
    characteristics of the nodes as decisive criteria. Note that, to the best of our
    knowledge, there are no automotive-specific contributions on this matter, but
    we can learn from the domain of cloud / edge computing. First, Xu et al. [524]
    propose a computation offloading method for cloud-edge computing in which a non-dominated
    sorting genetic algorithm is employed to address the multi-objective optimisation
    problem that will help reduce both the execution delay and energy consumption.
    Secondly, Yu et al. [525] consider the scenario where multiple mobile users offload
    duplicated computation tasks to the network edge, sharing the computation results
    among themselves to develop fine- grained collaborative offloading strategies
    with caching enhancements. This is done to minimise the overall execution delay
    on the mobile terminal side, through coalition game formation algorithms. Finally,
    Xu et al. [526] study service catching and replication using a mobile edge heterogeneous
    computing arch. through Lyapunov’s optimisation and Gibbs’s sampling to reduce
    the delay and spatial demand coupling of the systems. Finally, we now focus on
    the contributions mixing both computing and network as decisive criteria. First
    of all, if we start with those not conceived for the automotive industry, the
    first, and in our opinion most interesting proposal comes from is Gholami et al.
    [515]. This proposes a framework for resource orchestration for micro-services
    based 5G applications in a dynamic, heterogeneous, multi-tiered computer and network
    fabrics by analysing the end-to-end application requirements. Moreover, Wang et
    al. [530] and Xing et al. [531] also target this issue, firstly through an alternating
    direction method of multipliers to reduce the computation complexity and, secondly,
    through a heuristic scheme based task assignment algorithm. Shifting our focus
    now to consensus and security, Guo et al. [528] use blockchain technology for
    adaptive resource allocation and computation offloading in future wireless networks,
    where the blockchain works as an overlaid system to provide management, offloading
    consensus, and control functions. Finally, between those specifically targeting
    the automotive industry, even though they target the external networking and not
    the in-vehicle systems, Vu et al. [527] address the popularity and load balancing
    issues of Fog nodes depending on their location and contents. They propose a dynamic
    resource orchestration scheme which harmonises resource allocation through maximum
    weight matching for connected vehicles. This migrates the offloaded services among
    fog nodes depending on the computing and network resources available and the location
    of the fog node latency-wise. Secondly, Qian et al. [529] investigate the non-orthogonal
    multiple access (NOMA) enabled multi-access edge computing platforms, with the
    objective of minimising a system-wise cost that accounts for the overall delay
    in finishing total computation workload. Thirdly, making use of artificial intelligence,
    Wang et al. [532] use deep reinforcement learning techniques and a federated learning
    framework with the mobile edge systems to optimise the mobile edge computing,
    caching, and communication. Furthermore, Zhou et al. [533] tackle this problem
    where they once again use alternating direction method of multipliers algorithms.
    SECTION VIII. Run-Time Management Having addressed the challenges from both architecture
    and software delivery pipelines, we now need to focus on the services that ensure
    that the systems and all of the software deployed on them run smoothly. These
    services are called, as we mentioned in §VI-B, control services. However, as there
    is an infinitude of possible control services, we are going focus on those we
    feel are a priority to comply with the automotive requirements, mostly in terms
    of safety and security. Therefore, this section is composed of three parts; the
    first is about data-centred control services, the second is to do with security-centred
    control services and the last one focuses on safety-centred control services.
    A. Data-Centred Control Services These control services allow for the collection,
    analysis, and distribution of data to optimise the performance, efficiency or
    even safety of the vehicle. In this section, first of all, we will go over the
    state-of-the-art data collection solutions proposed in both the automotive and
    IT sectors, then move towards how this collected data is shared and used. Finally,
    we will focus on how to safely store this data. Note that Table 14 summarises
    all the data-centred contributions listed in each subsection to give a graphic
    outlook of the section results. TABLE 12 Summary of the Testing Mechanisms TABLE
    13 Exhaustive Comparison of the Different Dynamic Orchestration & Scheduling Techniques
    Analyzed TABLE 14 Summary of the Solutions Around Data-Centred Control Services
    1) Data Collection The collection of vehicle data is a promising research field,
    being one of the main motivations for the increase of in-vehicle cameras, software,
    and sensors that gather valuable info. about cars and driving patterns. In this
    first subsection, we will be focusing on how this new data is being collected
    by vehicles, as well as the context in which it is being collected. Afterwards,
    we will briefly go through data collection in the IT sector to illustrate the
    rest of the panel choice. To illustrate the advancement of these control services
    in the automotive sector, we focus on one of the most advanced data collection
    use cases: digital forensics, which targets the collection of data to enhance
    traceability and accountability of delinquent activities. Thus, in this context,
    we highlight Alexakos et al. [534], whose paper tirelessly goes through the challenges
    of data collection and security and proposes an attack attribution and forensics
    readiness tool for the IoV systems. For other purposes, we can find other interesting
    contributions such as Sheeny et al. [535], who target the collection of automotive
    sensors for weather profiling, Xun et al. [536] who propose a framework to use
    data collection for driver profiling authentication, Major et al. [537] and Kocić
    et al. [538] focusing on using data collection for ADAS purposes such as the generation
    of cross-vehicle omni-view middleware for autonomous vehicles, Llorca et al. [539]
    who use data collection for traffic monitoring purposes, Giobergia et al. [540]
    and Tiedmann et al. [541] who propose using this data for some predictive maintenance
    frameworks and, finally, Beier et al. [165] who suggest making use of data collection
    for environmental purposes. Finally, Rahim et al. [542] summarise, in a more general
    way, how new services might interact with the data collection services and how
    they would access this data. On the other hand, if we focus now on the proposals
    coming from the IT sector, data-collection is addressed for IoT in Liu et al.
    [543], Li et al. [544] and Mo et al. [545] and more generally for mobile sensor
    networks and distributed systems respectively in Nguyen [546] and Tuor et al.
    [547]. It is also extensively addressed regarding fog/edge/cloud computing in
    Khaliq et al. [548] and Wang et al. [549]. Finally, one solution of data-collection
    for monitoring purposes that is worth mentioning is from Jiang et al. [550], which
    proposes a push/pull/alarm monitoring framework for micro-services architecture,
    being able to cover both system and software. 2) Data Sharing As the number of
    connected cars grows, the amount of data produced will increase respectively.
    This will be used to make vehicles safer, more fuel-efficient, with better in-vehicle
    navigation and with higher traffic congestion management capabilities. Moreover,
    this data is also expected to enable better diagnostics, maint. and monitoring,
    as well as a higher understanding of the security and safety issues which will
    help automakers detect and develop better services and options for their customers
    based on their real habits. In this case, with regard to automotive systems, the
    solutions we found tackle this subject either by focusing on the privacy aspects
    involved in the data-sharing process, such as Xiong et al. [554], the security
    aspects of the sharing transaction, such as Rathee et al. [555] or Su et al. [556],
    or the interactions with the cloud infrastructure, such as Chen et al. [558] and
    [557] or Fu et al. [559]. Furthermore, within the last category, many solutions
    focus both on data collection and data sharing, such as Waltereit et al. [551],
    Feng et al. [552], and Sathe and Deshmukh [553]. On the other hand, with regard
    to the IT domain, we can highlight multiple surveys focusing on security through
    the data sharing process, which are either focused on encryption characteristics
    - i.e., Hidayat and Mahardiko [560] or Eltayieb et al. [561] - or on the use of
    blockchain technology with this purpose, both in cloud computing Shen et al. [562]
    or Ge et al. [563], or in other security-critical domains such as medicine - i.e.,
    Jin et al. [564] or Cheng et al. [565]. Finally, we would like to highlight some
    papers that focus on the efficiency of data-sharing with regard to data placement
    and replications Du et al. [566] or Shen et al. [567]. 3) Data Storage Having
    already detailed the immense value of data collection and sharing both intra-and-inter
    vehicles and the increasing amount of data produced, in this section we aim to
    discuss how to safely store this data, as it might be interesting for the development
    of future models, the creation of new business opportunities, etc. In this context,
    first of all we look at the study of automotive systems, Guo et al. [568], Li
    et al. [569], and Vinzenz and Eggendorfer [570] propose ways to securely store
    and preserve data authenticity through distributed storage systems such as blockchain
    [568]. Moreover, we can find many other data-storage proposals centred around
    the advantages of Blockchain when ensuring data authenticity and enhancing traceability
    in contexts other than digital forensics, such as Mohammad et al. [571] which
    surveys the blockchain challenges for data-collection in the automotive sector,
    Jabbar et al. [572] who propose a solution for inter-vehicle communication, Morano
    et al. [573] who suggest a solution for driver data collection or Jain et al.
    [574] who discuss which future directions this promising technology may take in
    the future. In addition, it is worth noting note that all the blockchain-based
    solutions presented before, such as those concerning software delivery (cf. §VII-B),
    could also be also included in this section with minor data-structure adjustments.
    Finally, with regard to the contributions on the IT domain, many contributions
    tackle the data storage in cloud computing - i.e., Tabrizchi and Kuchaki Rafsanjani
    [575], Wu et al. [576] or Odun-Ayo et al. [577] - or in IoT systems - i.e., Wang
    et al. [578] or Khalaf and Abdulsahib [579]. Moreover, we can also point out many
    contributions concerning the form of storage either using the blockchain - i.e.,
    Li et al. [580] or Liang et al. [581] -, classic databases - i.e., Győrödi et
    al. [582] or Lv et al. [583] - or non-relational databases - i.e., Bradshaw et
    al. [584] or Yang et al. [585]. B. Security-Centred Control Services Security
    has been shown to be one of the most desirable properties for automotive systems.
    Thus, in this section, we will discuss, firstly, two of the most important security-centred
    run-time control services - i.e., Access Control Policies and State Management.
    Afterwards, we will go on to discuss security framework evolutions and threats.
    1) Access Control Policies Access control policies are important in automotive
    in-vehicle architectures to ensure that only authorised users can access and use
    certain features and functions of the vehicle. These policies can help to guarantee
    the safety and security of the driver and passengers, as well as prevent unauthorised
    use or theft of the vehicle. Traditionally, the access control model can be divided
    into three categories: Attribute-based access control (ABAC), role-based access
    control (RBAC), and access control tech. based on usage control (UCON). Firstly,
    if we focus on the solutions found in RBAC and ABAC models, which is the second
    most common in the automotive domain, we can find many proposals. Among the ABAC
    models, we can highlight; Kumar et al. [586], who propose a tree-based certificate
    access management framework for large-scale communications within connected vehicles,
    Kim et al. [587], who suggest an integration of ABAC models for AUTOSAR, Rumez
    et al. [588], who propose a general discussion of ABAC integration in automotive
    systems or Farooq et al. [589] who discuss the use of hardware for ABAC authentication
    through RFID chips. In addition, others also focus on introducing dynamicity properties
    to the ABAC models either for the new generation of rules, as proposed by Gupta
    et al. [590], or for the resolution of ABAC conflicts, as discussed in Asif et
    al. [591] and Lachmund and Hengst [592]. On the other hand, for the RBAC model,
    Veitas and Delaere [593] are a good representative of the contributions in this
    scope regarding data confidentiality, integrity, and availability with the objective
    of recovering an exact situation following the occurrence of an event or on demand.
    In addition to these, from the IT sector, we can highlight many surveys on this
    matter such as Ren et al. [594], Qiu et al. [595], Hu et al. [596] or Zhu et al.
    [597] or even certain papers focusing on the combination of both RBAC and ABAC
    as in Long and Yan [598] or Al-Alaj et al. [599]. On the other hand, those based
    on UCON are focused on regulating the usage of the resources, rather than just
    guaranteeing, or denying access to them. In a UCON access control policy, access
    to a resource is granted based on whether the user’s request satisfies a set of
    usage conditions, rather than just a set of static access control rules. This
    way, UCON policies allow the definition of more fine-grained and context-aware
    rules, guaranteeing higher flexibility and dynamicity. Some examples of UCON access
    control frameworks in the automotive industry are Terzi et al. [600], which discusses
    a way of dealing with identity management and authorization through the authenticity
    characteristics of blockchain, or Kaur et al. [601], Lupu and Lupu [602] and Chen
    and Yin [603], that propose using bio-metric parameters for access control. On
    the other hand, from the IT sector, we can highlight an exhaustive survey published
    by Guoping and Wentao [604], some contributions surrounding the use of blockchain
    for access control such as Maesa et al. [605], Ali et al. [606] or Tang et al.
    [607] and, finally, a variety of contributions regarding flexibility and dynamicity
    of the generation of complex access rules in Jajodia et al. [608] and Lachmund
    and Hengst [592]. 2) State Management State management in automotive in-vehicle
    systems refers to the process of managing and tracking of the different states
    or modes of the system as it interacts with the driver, passengers, and external
    environment. In simpler terms, it involves the control and coordination of contrasting
    functions, features, and components of the in-vehicle system. Some examples of
    states that can be managed in automotive in-vehicle systems include: Power states:
    These states are related to the power management of the electronic systems of
    the vehicle. For example, the system may need to manage the power state of the
    infotainment system, air conditioning system, or other subsystems based on battery
    level or user preferences. Some examples of this are shown in Wang and Gladwin
    [609] or Zhong et al. [610]. Driver assistance states: In-vehicle systems may
    have various modes for driver assistance, such as adaptive cruise control, lane-keeping
    assistance, or emergency braking. State management is crucial in ensuring that
    these systems are functioning correctly and providing the right level of assistance
    to the driver. Some examples of this kind of state management can be found in
    Al-Saadi et al. [611], Sajadi-Alamdari et al. [612] or Iglesias et al. [613].
    Communication states: Many modern in-vehicle systems support communication protocols
    such as Blue- tooth, Wi-Fi, or cellular networks. Managing the state of these
    communication systems is crucial for ensuring reliable connectivity and data transfer
    between the vehicle and external devices. Some examples of this can be seen in
    Joshi et al. [614] or Hontani and Higuchi [615]. User interface states: The in-vehicle
    system may have different states for its user interface, such as menu navigation,
    input methods, or screen layouts. Managing these states is essential to provide
    a smooth and intuitive user experience. Some examples of this can be seen in Braun
    et al. [616] and [617], Urooj et al. [618] or Kern and Schmidt [619]. Therefore,
    in general, state management in automotive in-vehicle systems requires a combination
    of hardware and software solutions. This can involve sensors, controllers, and
    software algorithms that work together to monitor and control distinct aspects
    of the system. Some examples of more general approaches for state management,
    this time coming from non-specific IT solutions, can be found in Nabi et al. [620],
    Sharma and Santharam [621] or Ahmed et al. [622]. 3) Cyber-Security Threats and
    Frameworks The enhanced connectivity and software capabilities of new generations
    of vehicle has widened the range of cyber-attacks, and this has become one of
    the major challenges for Automakers each time a new functionality is introduced
    to keep the system secure and able to guarantee the passengers’ safety. Thus,
    multiple papers have researched possible attacks, exploits and vulnerabilities
    of future vehicles; Teichmann et al. [623], Cui et al. [624], Parkinson et al.
    [625], Humayed et al. [626], Buinevich and Vladyko [627], Gupta et al. [628],
    Sommer et al. [629] and Bazzi et al. [630]. Other papers such as Chowdhury et
    al. [631], Pisarov and Mester [632], Jang and Shin [633] and Pascale et al. [634]
    further extend this threat analysis while at the same time also proposing some
    countermeasures. Furthermore, cybersecurity is also a trending topic in the IT
    sector from which we can highlight; Zhuang et al. [635] tackling cybersecurity
    in smart grid systems, Shaukat et al. [636] using machine learning for enhancing
    system security, Nikander et al. [637] in agriculture, and the one which the closest
    to the automotive domain, Andrade et al. [638] in IoT. C. Safety-Centred Control
    Services The main control services targeting in-vehicle software safety are Fault
    tolerance and Load balancing, with both being extensively addressed in the Automotive
    sector, as well as in IT which was already required by the current systems. Moreover,
    if we consider safety with regard to passenger-basis, we also need to include
    system monitoring and diagnostics. 1) Fault Tolerance On the one hand, fault tolerance
    allows the systems to continue operating properly when one or more failures occur
    within some of its components. Fault tolerance has been generally researched in
    the Automotive sector in Cheng et al. [639] and Bhat [640], heterogeneous systems
    are focused on in Lex et al. [641], deep neuronal networks are detailed in Malekzadeh
    [642], a conceptual threat approach is looked at in Gräßler et al. [643], and,
    then specifically for the Autosar systems in Bhat et al. [644]. In addition, in
    the IT sector, it has been widely addressed concerning two similar sectors, such
    as cloud computing in Kumari and Kaur [645] and Bharany et al. [646] and distributed
    systems in Gupta and Vaidya [647] and Xiao et al. [648]. 2) Load Balancing On
    the other hand, load balancing is the process of distributing a set of tasks or
    net workload over a set of computing units or cables, with the aim of making their
    overall system more efficient. In IT, we highlight some static algorithms such
    as Round Robin, MIN-MIN or MAX-MIN, described in Kaur and Luthra [649] and Mesbahi
    and Rahmani [650], and some dynamic algorithms such as the honeybee foraging behaviour
    algorithm, throttled algorithms or ant colony algorithms, described respectively
    in Thapliyal and Dimri [651], Patel and Rajawat [652] and, Nishant et al. [653].
    Load balancing is also surveyed in the Automotive sector between both vehicles
    and cloud (V2C) in Liu et al. [654], Majumder et al. [655], vehicles and vehicles
    (V2V) Wu et al. [656], and within a single vehicle architecture Ahmed et al. [657]
    and Syed et al. [658]. 3) System and Software Monitoring Software and system monitoring
    in cars is an essential part of modern vehicle technology. With the increasing
    complexity of car systems and the growing importance of software in controlling
    these systems, monitoring has become a critical aspect of ensuring vehicle safety
    and performance. This topic has barely been explored within the automotive sector,
    with Yemelyanov et al. [659], Charette [660], and Hameed et al. [661] and [662]
    providing the most representative and traditional proposals. Moreover, other contributions
    spend time explaining how to use machine learning and artificial intelligence
    for software monitoring such as Norouzi et al. [663], Kuutti et al. [664] and
    Georgakos et al. [665] or discussing the effects of software aging Parnas [666]
    or Costa et al. [667]. On the other hand, from IT, we can highlight some papers
    about software monitoring in cloud computing, such as Aktas [668], Spring [669],
    Tamburri et al. [670] or Shao et al. [671], in IoT, such as Maksymyuk et al. [672]
    or Razzag [673], or even more safety critical domains such as Industry from Rakhmonov
    and Kurbonov [674] or He et al. [675]. 4) Vehicle Diagnostics In modern cars,
    system diagnostics are typically performed by the onboard diagnostic (OBD) system.
    The OBD-II system uses a series of sensors and diagnostic codes to detect malfunctions
    and issues with various components within the car. When a problem is detected,
    the system generates a diagnostic trouble code (DTC) that can be read using a
    scan tool. This makes it easier for technicians to diagnose and repair problems
    with a car. OBD-II systems are typically located under the dashboard, near the
    steering column. Some examples of this are discussed in McCord [676], Malekian
    et al. [420] or Rimpas et al. [677]. Furthermore, and more recently, as a way
    to reduce the environmental impact of hardware repairs, to detect future problems
    sooner, and to speed up garage visits, many solutions explore how to carry out
    vehicle diagnostics remotely such as Mesgarpour et al. [678] or Singh et al. [679].
    Others, extend this idea by introducing machine learning for analysing as detailed
    in Theissler et al. [680] or technologies such as digital twin in Bhatti et al.
    [681]. SECTION IX. Discussion Society and, more extensively, vehicles follow more
    connected, accessible, and flexible perspectives, increasingly integrating software-based
    functionalities, enhancements, and optimisations. However, as we showed in previous
    sections, adapting the architecture, delivery pipelines and run-time management
    services, without forgetting about the already existing base, has become not only
    a major necessity, but also a major challenge for the automotive sector. Thanks
    to these enhancements, vehicles will not only be more prone to innovation but
    also considerably less complex, easing the costs of maintenance, integration and,
    even, development. The results presented in the earlier sections are numerous
    and diversified, covering different disciplines and topics. We understand how
    important it is to work on future in-vehicle and off-board ICT systems and it
    raises numerous open questions and challenges, including technical, societal,
    and economical issues. Our research provides us with the scope to complete this
    paper by give some engineering guidelines (cf. Fig. 22) about how to combine all
    the aforementioned sections, completing the results of the research in the previous
    Fig. 5 in §IV, which can be completed with Fig. 22 and to which we can comment
    layer by layer: FIGURE 22. Simple overview of our vision, based on the survey
    results, of future vehicle embedded & cloud architecture. Show All Architecture
    Design: If we focus first on the E/E architecture, it seems clear that in the
    coming years there is a lot of work to go through in terms of how to organise
    the nodes within our architectures. It is an open challenge to decide whether
    it is better to go through zonal architectures or central architectures like Tesla’s,
    as well as discussing the fundamental issue regarding which kind of boards (i.e.,
    GPU, HPCs…) to use for each case [66]. In addition, there are also various concerns
    regarding the integration and cohabitation of legacy and new software in terms
    of hardware, as it must be able to support both [231]. Finally, further research
    is needed on the analysis of other ISA level architectures (RISC-V, CISC, ARM…)
    to match safety, cost, and performance constraints [246]. Secondly, regarding
    software architecture, further work needs to be done on how to combine the different
    paradigms so as to fully profit from the advantages arising from all of them [682].
    Moreover, the adaptation and implementation of an IT-like complete complex software
    architecture should be done so as to evaluate its performance and overhead for
    the automobile sector. Thirdly, with regard to network architecture, the key research
    point will be the scheduling and the respect of the latencies in a data-centred
    communication middleware [18]. Finally, IoV architecture needs to be explored
    much more, as most of the current contributions don’t delve deep enough into the
    implementation and do not profit enough from the possibilities of integrating
    both smart cities and vehicles [683]. Software Delivery Pipelines: First, with
    the enhanced networking capabilities of vehicles, software delivery faces a new
    revolution, needing to integrate OTA within their classic software management
    cycle, and then focusing on how to ensure the safety, security and data authenticity
    becomes of vital importance, as the scope of this mechanism and its attack surface
    are both enormous is huge [430]. Secondly, further research is needed on software
    deployment so as to, first of all pick the appropriate Operating system and, secondly,
    choose the right installation method that will favor flexibility, isolation… [492].
    Thirdly, testing faces many open issues with regard to dynamic certification and
    the testing process itself that also needs further work [506]. Finally, additional
    research needs to be done on how to orchestrate, both efficiently and dynamically,
    the in-vehicle applications depending on the software context and also the nearby
    users [518]. Run-Time Management Services: Firstly, with regard to data-centred
    control services, further research is needed to integrate the new data collected
    from the development of new services [534] and, as the data has high privacy implications,
    additional work is also needed concerning data privacy and authenticity [554].
    Moreover, with regard to security-centred control services, the new technologies
    added to the vehicle have several implications concerning the security issues
    of the whole system, and thus, much effort has been made to find and solve [684]
    them. Finally, safety-centred control services need further integration with the
    modern technologies, in order to apply current knowledge to the new applications
    and cases [639]. A. Future Trends As the automotive industry is constantly evolving,
    there are several areas that are likely to see significant dev. in the coming
    years. Here are a few potential areas of focus for future work in the automotive
    industry. 1) Electric Vehicles (EVs) The transition to electric vehicles is already
    underway, but there is still a lot of work to be done to make EVs more affordable,
    efficient, and convenient. Future work in this area might focus on developing
    new battery technologies [141], improving charging infrastructure [685], [686],
    and increasing the range of electric vehicles [687]. 2) Autonomous Vehicles Self-driving
    cars are becoming more common, but there are still technical and regulatory hurdles
    that need to be overcome before they become mainstream. Future work in this area
    might focus on developing more advanced sensors and software [688], [689], as
    well as working with regulators to establish guidelines for autonomous vehicles
    [690]. 3) Sustainability As concerns about climate change and environmental impact
    grow, there is a growing focus on making cars more sustainable. Future work in
    this area might include developing more fuel-efficient engines [691], increasing
    the use of renewable materials in car construction, reducing the environmental
    impact of manufacturing processes [692], [693], and changing car sharing possibilities
    [160], [694]. 4) Personalization Consumers are increasingly looking for cars that
    can be customised to fit their individual needs and preferences. Future work in
    this area might focus on developing more modular car designs that can be easily
    modified to meet specific customer requirements [695], as well as developing new
    interfaces and technologies that allow drivers to customise their driving experience
    [696], [697]. 5) New Use-Cases Furthermore, other work will be done on creating
    new services that interact with the current unexplored or barely explored smart
    city, such as: Predictive maintenance: By analysing data from various sensors
    and systems in a vehicle, software can help predict when maintenance is needed,
    reducing downtime and repair costs [698], [699], [700]. Emergency response & Healthcare
    services: Vehicles can be fitted with emergency response equipment, such as defibrillators,
    oxygen tanks, and first-aid supplies, to create mobile emergency response units
    that can quickly respond to medical emergencies or other crises [701], [702].
    This can be particularly useful in rural or underdeveloped areas where access
    to healthcare is limited. Tourism: Vehicles can be used to provide guided tours,
    either through traditional tour companies or through new models, such as self-driving
    tour buses or mobile audio guides [703]. Urban farming: Vehicles can be transformed
    into mobile urban farms, providing fresh produce to urban areas where access to
    fresh food is limited [704]. Education & Mobile offices: Vehicles can be adapted
    to provide educational services, such as mobile classrooms or libraries, providing
    educational resources to underdeveloped communities [705]. Moreover, with with
    the rise of remote work, many people are looking for ways to work from anywhere.
    Vehicles can be fitted with workspace features, such as Wi-Fi, power outlets,
    and comfortable seating, to create mobile offices that allow people to work while
    on the go [706], [707]. Delivery services: As e-commerce continues to grow, there
    is an ever-increasing demand for delivery services. Vehicles can be adapted to
    provide last-mile delivery services, either through traditional delivery companies
    or through new delivery models, such as autonomous vehicles [708]. Entertainment:
    Vehicles can be fitted with entertainment features, such as high-end sound systems,
    large displays, and gaming consoles, to create mobile entertainment centers that
    can be used for events, road trips, or even as mobile movie theatres [709], [710].
    SECTION X. Conclusion Software in the automotive system is becoming increasingly
    important and is a major factor for clients when it comes to deciding which vehicle
    to buy. However, even if until now the automotive industry has kept up with the
    innovation pace in terms of functionalities offered to the passengers, much effort
    has been made to bring about these new functionalities by adding an unceasingly
    larger set of ECUs without evolving all the embedded software architecture consequently
    due to budgetary constraints, legislative limitations, retro-compatibility problems,
    or lack of awareness regarding trending IT innovation. This unbalanced progress
    has then, as we can see in §IV, considerably increased the complexity and cost
    of the system for developing and maintaining new services, hardening the path
    for evolving the application development methodology and bringing about new innovative
    software-related business proposals. Our paper has provided a comprehensive overview
    of the current state-of-the-art technologies in Software-as-a-Service transformation
    for automotive systems. Through an extensive review of literature, we have compared
    our paper to others in the same scope and identified the unique contributions
    of our work. Our use-case scenario highlights the importance of software in the
    automotive industry and the challenges faced by automakers when adopting Software-as-a-Service
    solutions. We have identified societal, business, design process, application
    profiles and requirements, and technical factors as the main catalysts of evolution
    towards Software-as-a-Service in the automotive industry. The automotive application
    life cycle and its convergence with Software-as-a-Service have been discussed
    in detail. We have highlighted open issues in this area, including the need for
    standardisation, security, and scalability. The architecture design, software
    delivery pipelines, and run-time management services have been examined in detail,
    providing a comprehensive understanding of the technical aspects of Software-as-
    a-Service for the automotive industry. In conclusion, our research has provided
    a comprehensive understanding of Software-as-a-Service transformation for the
    automotive industry. The discussion and future works section identifies key areas
    for future research and gives our engineering insights by converging all the survey
    information in one theoretical high-level architecture proposal. Overall, our
    work contributes to the ongoing efforts to transform the automotive industry through
    the adoption of Software-as-a-Service solutions. Authors Figures References Citations
    Keywords Metrics Footnotes More Like This Current Status and Future Implementation
    of Information and Communication Technologies (ICT) in the Retail Business 2021
    Zooming Innovation in Consumer Technologies Conference (ZINC) Published: 2021
    Effects of Information and Communication Technology Usage by Individuals, Businesses,
    and Government on Human Development: An International Analysis IEEE Access Published:
    2019 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Comprehensive Survey on Software as a Service (SaaS) Transformation for
    the Automotive Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wang R.
  - Qi J.
  - Chen L.
  - Yang L.
  citation_count: '4'
  description: 'At present, the continuous change of information technology along
    with the dramatic explosion of data quantity makes the cloud computing solutions
    face many problems such as high latency, limited bandwidth, high carbon footprint,
    high maintenance cost, and privacy concerns. In recent years, the emergence and
    rapid development of edge computing has effectively alleviated such dilemmas,
    sinking user demand processing to the edge and avoiding the flow of massive data
    in the network. As a typical scenario of edge computing, edge intelligence is
    gaining increasing attention, in which one of the most important stages is the
    inference phase. Due to the general low performance of resources in edge computing,
    collaborative inference through resources is becoming a hot topic. By analyzing
    the trends of edge intelligence development, we conclude that collaborative inference
    at the edge is still in the increasing phase and has not yet entered a stable
    phase. We divide edge-edge collaborative inference into two parts: Intelligent
    methods and collaborative inference architecture, based on a thorough investigation
    of edge collaborative inference. The involved key technologies are summarized
    vertically and organized from the perspective of dynamic scenarios. Each key technology
    is analyzed in more detail, and the different key technologies are compared horizontally
    and analyzed on the application scenarios. Finally, we propose several directions
    that deserve further studying in collaborative edge inference in dynamic scenarios.'
  doi: 10.7544/issn1000-1239.202110867
  full_citation: '>'
  full_text: '>

    "中国精品科技期刊 CCF推荐A类中文期刊 计算领域高质量科技期刊T1类                   所有                   标题                   作者                   关键词                   摘要                   DOI                   栏目                   地址                   基金                   中图分类号                  高级检索
    首页 期刊介绍 编委会 期刊在线 作者中心 审稿中心 联系我们 English 文章导航 >  计算机研究与发展  > 2023  >  60(2) : 398-414.  >
    DOI: 10.7544/issn1000-1239.202110867 引用本文: 王睿, 齐建鹏, 陈亮, 杨龙. 面向边缘智能的协同推理综述[J].
    计算机研究与发展, 2023, 60(2): 398-414. DOI: 10.7544/issn1000-1239.202110867 Citation:
    Wang Rui, Qi Jianpeng, Chen Liang, Yang Long. Survey of Collaborative Inference
    for Edge Intelligence[J]. Journal of Computer Research and Development, 2023,
    60(2): 398-414. DOI: 10.7544/issn1000-1239.202110867 面向边缘智能的协同推理综述  智能化辅读 王睿1,
    2, ,  齐建鹏1,  陈亮1,  杨龙1 1. 北京科技大学计算机与通信工程学院　北京　100083 2. 北京科技大学顺德研究生院　广东佛山　528300
    基金项目: 国家自然科学基金项目（62173158,72004147） 详细信息 Survey of Collaborative Inference for
    Edge Intelligence Wang Rui1, 2, ,  Qi Jianpeng1,  Chen Liang1,  Yang Long1 1.
    School of Computer and Communication Engineering, University of Science and Technology
    Beijing, Beijing 100083 2. Shunde Graduate School of University of Science and
    Technology Beijing, Foshan, Guangdong 528300 Funds: This work was supported by
    the National Natural Science Foundation of China (62173158,72004147). 摘要 摘要 HTML全文
    图(7) 表(2) 参考文献(128) 相关文章 施引文献(14) 资源附件(0) 摘要: 近年来，信息技术的不断变革伴随数据量的急剧爆发，使主流的云计算解决方案面临实时性差、带宽受限、高能耗、维护费用高、隐私安全等问题.
    边缘智能的出现与快速发展有效缓解了此类问题，它将用户需求处理下沉到边缘，避免了海量数据在网络中的流动，得到越来越多的关注. 由于边缘计算中资源性能普遍较低，通过资源实现协同推理正成为热点.通过对边缘智能发展的趋势分析，得出边缘协同推理目前仍处于增长期，还未进入稳定发展期.
    因此，在对边缘协同推理进行充分调研的基础上，将边缘协同推理划分为智能化方法与协同推理架构2个部分，分别对其中涉及到的关键技术进行纵向归纳整理，并从动态场景角度出发，对每种关键技术进行深入分析，对不同关键技术进行横向比较以及适用场景分析.最后对动态场景下的边缘协同推理给出值得研究的若干发展方向.   关键词:
    边缘计算  /  边缘智能  /  机器学习  /  边缘协同推理  /  动态场景   Abstract: At present, the continuous
    change of information technology along with the dramatic explosion of data quantity
    makes the cloud computing solutions face many problems such as high latency, limited
    bandwidth, high carbon footprint, high maintenance cost, and privacy concerns.
    In recent years, the emergence and rapid development of edge computing has effectively
    alleviated such dilemmas, sinking user demand processing to the edge and avoiding
    the flow of massive data in the network. As a typical scenario of edge computing,
    edge intelligence is gaining increasing attention, in which one of the most important
    stages is the inference phase. Due to the general low performance of resources
    in edge computing, collaborative inference through resources is becoming a hot
    topic. By analyzing the trends of edge intelligence development, we conclude that
    collaborative inference at the edge is still in the increasing phase and has not
    yet entered a stable phase. We divide edge-edge collaborative inference into two
    parts: Intelligent methods and collaborative inference architecture, based on
    a thorough investigation of edge collaborative inference. The involved key technologies
    are summarized vertically and organized from the perspective of dynamic scenarios.
    Each key technology is analyzed in more detail, and the different key technologies
    are compared horizontally and analyzed on the application scenarios. Finally,
    we propose several directions that deserve further studying in collaborative edge
    inference in dynamic scenarios.   Keywords: edge computing  /  edge intelligence  /  machine
    learning  /  edge collaborative inference  /  dynamic scenario                           PDF下载
    ( 1629 KB) XML下载 导出引用 点击查看大图 图(7)  /  表(2) 计量 文章访问数:  452 HTML全文浏览量:  55 PDF下载量:  224
    被引次数: 14 出版历程 收稿日期:  2021-08-25 修回日期:  2022-04-14 网络出版日期:  2023-02-10 发布日期:  2021-08-25
    刊出日期:  2023-01-31 分享             办公地点：北京中关村科学院南路6号 中国科学院计算技术研究所350房间 通信地址：北京2704信箱《计算机研究与发展》编辑部（100190）
    电　　话：（010）62620696(FAX);（010）62600350 E-mail　：crad@ict.ac.cn 网　　址：https://crad.ict.ac.cn/
    微　　信：J-CRAD 邮件订阅 RSS 微信订阅号 （实时资讯） 微信服务号 （同步网站） 微信视频号 （视频分享） CCF （扫码入会） 版权所有 ©
    《计算机研究与发展》编辑部 本系统由北京仁和汇智信息技术有限公司开发  "'
  inline_citation: '>'
  journal: Jisuanji Yanjiu yu Fazhan/Computer Research and Development
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Survey of Collaborative Inference for Edge Intelligence
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Karamolegkos P.
  - Kiourtis A.
  - Karabetian A.
  - Voulgaris K.
  - Poulakis Y.
  - Mavrogiorgou A.
  - Filippakis M.
  - Kyriazis D.
  citation_count: '1'
  description: The use of systems to calculate mathematical operations has facilitated
    people to automate processes in the corporate sector. Such systems lie behind
    everything from calculating the final amount on a grocery receipt, to complex
    mathematical operations involving finding behaviors in a business's customers.
    However, when a company or organization has a large amount of data on which to
    perform mathematical operations, the procedure becomes time-consuming, whereas
    to execute mathematical operations on entire datasets, one typically needs strong
    programming skills. In this paper, a service called MathBlock is analyzed that
    is able to be used as a language agnostic mathematical expression parser and executioner,
    on batch data. MathBlock consists of four types of functions, including arithmetic,
    comparison, logical, and statistical. To evaluate the applicability of MathBlock,
    an experiment is carried out on the mentioned service as a proof of concept. This
    experimentation uses batch and synthetic data, covering the domains of maritime
    and healthcare, with the aim of performing mathematical operations through MathBlock.
    The derived results showcase that MathBlock can assist users on their need to
    calculate and gather results for many different datasets. Overall, it can be clearly
    stated that through MathBlock the challenge of the need to perform arithmetic,
    logical, comparison and statistical operations on different datasets to get results
    in an automated manner is well addressed, whereas additional experimentation with
    datasets from multiple domains should take place in order to conclude to more
    concrete and reliable results.
  doi: 10.1109/ICCRD56364.2023.10080594
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Manage Preferences IEEE.org
    IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account Personal
    Sign In Browse My Settings Help Access provided by: University of Nebraska - Lincoln
    Sign Out All Books Conferences Courses Journals & Magazines Standards Authors
    Citations ADVANCED SEARCH Conferences >2023 15th International Confe... MathBlock:
    Performing Complex Mathematical Operations on Synthetic Data Publisher: IEEE Cite
    This PDF Panagiotis Karamolegkos; Athanasios Kiourtis; Andreas Karabetian; Konstantinos
    Voulgaris; Yannis Poulakis; Argyro Mavrogiorgou; Michael Filippakis All Authors
    1 Cites in Paper 66 Full Text Views Abstract Document Sections I. Introduction
    II. Related Work III. Mathblock IV. Mathblock Experimentation V. Discussion and
    Concluding Remarks Authors Figures References Citations Keywords Metrics Abstract:
    The use of systems to calculate mathematical operations has facilitated people
    to automate processes in the corporate sector. Such systems lie behind everything
    from calculating the final amount on a grocery receipt, to complex mathematical
    operations involving finding behaviors in a business''s customers. However, when
    a company or organization has a large amount of data on which to perform mathematical
    operations, the procedure becomes time-consuming, whereas to execute mathematical
    operations on entire datasets, one typically needs strong programming skills.
    In this paper, a service called MathBlock is analyzed that is able to be used
    as a language agnostic mathematical expression parser and executioner, on batch
    data. MathBlock consists of four types of functions, including arithmetic, comparison,
    logical, and statistical. To evaluate the applicability of MathBlock, an experiment
    is carried out on the mentioned service as a proof of concept. This experimentation
    uses batch and synthetic data, covering the domains of maritime and healthcare,
    with the aim of performing mathematical operations through MathBlock. The derived
    results showcase that MathBlock can assist users on their need to calculate and
    gather results for many different datasets. Overall, it can be clearly stated
    that through MathBlock the challenge of the need to perform arithmetic, logical,
    comparison and statistical operations on different datasets to get results in
    an automated manner is well addressed, whereas additional experimentation with
    datasets from multiple domains should take place in order to conclude to more
    concrete and reliable results. Published in: 2023 15th International Conference
    on Computer Research and Development (ICCRD) Date of Conference: 10-12 January
    2023 Date Added to IEEE Xplore: 30 March 2023 ISBN Information: ISSN Information:
    DOI: 10.1109/ICCRD56364.2023.10080594 Publisher: IEEE Conference Location: Hangzhou,
    China SECTION I. Introduction The data analysis process is responsible for finding
    hidden patterns, rules, and information from a given set of data [1]. According
    to Statista [2], the total amount of data consumed globally has increased to 64.2
    Zettabytes in 2020, 79 Zettabytes in 2021, and is expected to increase more than
    180 Zettabytes by 2025. All this data is referred to as Big Data, which is defined
    as large volumes of data collected from various sources and in various formats
    [3]. Such data are widely known to obey to some specific characteristics (Vs of
    the data), which nowadays mainly refer to data Volume (i.e., data size), Variety
    (i.e., data format), Velocity (i.e., data production rate), Veracity (i.e., size
    of data authenticity), Validity (i.e., data validity), Volatility (i.e., time
    of data validation), and Value (i.e., data usefulness in terms of analysis) [3].
    Although Big Data is essentially data, the traditional techniques and technologies
    of their analysis do not seem to perform efficiently, due to the complexity of
    the aforementioned characteristics. The three major motives for Big Data technology
    implementations are: (i) minimize hardware costs, (ii) check the value of Big
    Data before committing significant company resources, and (iii) reduce processing
    costs [4]. It is easily visible that companies are trying to handle these large
    amounts of data while minimizing their expenses. This is mostly possible due to
    the modern technologies and techniques that are built upon the field of Cloud
    Computing [5]. Big Data that are being able to be analyzed by a data analysis
    process are divided as (i) batch data deriving from ready-to-use datasets that
    require some processing or analytic activities, and (ii) streaming data deriving
    from live sources that are constantly streaming information [6]. Batch and streaming
    data can be either real data or synthetic data. As described in [7] synthetic
    data are data obtained from a generative process that learns the properties of
    the real data. More specifically, synthetic data are data that are trying to represent
    real data, without necessarily being real. Synthetic data are commonly used for
    training and testing of many data analysis processes [8], since through synthetic
    data more accurate Machine Learning (ML) models'' training can be provided, leading
    to better decision-making systems and results of high-reliability and trustworthiness.
    Also, the use of systems to calculate mathematical operations has helped people
    to automate processes in the corporate sector. Such systems lie behind everything
    from calculating the final amount on a grocery receipt, to complex mathematical
    operations involving finding behaviors in a business''s customers [9]. To support
    such systems and to give a language agnostic solution for the execution of mathematical
    expressions, some tools have been built, giving to their users the ability to
    compose and execute simple mathematical expressions, which are furtherly described
    in Section II. However, most of the time, these technologies require a human user
    to interact with them. Furthermore, the datasets must be handed one by one to
    the aforementioned systems, usually by human hand. Moreover, while these systems
    automate some of the methods for computing operations with datasets, they do not
    fully automate, and as a result, the entire process is semi-automated rather than
    entirely automated. To address the gaps of the aforementioned tools, in this paper,
    a service called MathBlock is being provided and analyzed. This service can be
    used as a language agnostic mathematical expression parser and executioner, on
    batch data. In simpler words, MathBlock is a program that can get as an input
    (i) many different datasets as batch data, (ii) a mathematical expression, as
    well as (iii) some needed metadata, to output a result dataset having values calculated
    based on the given expression. MathBlock can operate using different aspects of
    the mathematical and language agnostic systems mentioned, as described in the
    upcoming Sections. The rest of this paper is structured as follows. Section II
    presents a review of related research works. These are based on topics such as
    batch data, synthetic data, orchestration systems, mathematical operations systems,
    object storages, and workflow management systems. Section III covers the functionality
    of MathBlock, analyzing its different functions and operations. It also describes
    how it can be used by other users and services in addition to this analysis. Section
    IV covers the two experiments carried out in MathBlock as a proof of concept,
    considering the maritime and healthcare sector. In both experiments, synthetic
    data is employed, and MathBlock is used to test the reliability of the synthetic
    data based on the provided real data. Section V contains the paper''s discussion
    and concluding remarks, referring to the outcomes of the experiments performed
    using MathBlock, as well as to its limitations and challenges. SECTION II. Related
    Work As already discussed, MathBlock has been implemented considering various
    modern tools and research works, related to its goals and implementation methods.
    These related works are analyzed in the following sub-sections. A. Batch Data
    As described, batch data derive from datasets that require some processing or
    analytic activities. These data can be collected from many different sources as
    they can originate from Big Data environments [6]. In the referenced research,
    different architectures for Big Data Batch Processing are explained and analyzed.
    An analysis of different environments and architectures is performed, using the
    MapReduce process, with the aim of producing results regarding the efficiency
    of each different architecture. B. Synthetic Data ML algorithms must be trained
    and validated using diverse datasets, including datasets with known patterns and
    distributions when creating models from sensor data [10]. Synthetic data can be
    created for initial testing and validation of novel machine learning techniques
    [11]. Researches [12] [13] are reviewing the concept of synthetic data. In the
    referenced paper [12], the authors are reviewing different methods of making synthetic
    data. With the advent of high dimensionality, accurate identification of relevant
    data features has become critical in real-world scenarios. The significance of
    feature selection in this context cannot be overstated, and various methods have
    been developed. Several synthetic datasets are used for this purpose, with the
    goal of evaluating the efficacy of feature selection algorithms in the presence
    of many irrelevant features, noise in the data, redundancy and interaction between
    characteristics, and a low sample-to-feature ratio. Research [13] is taking the
    discussion of synthetic data one step further. It is evaluating the safety of
    using synthetic data in terms of the protection of the real data involved in making
    the generated data. C. Orchestration Systems Orchestration systems are services
    for accessing processes that are in a queue and executing them one by one. The
    project called Diastema uses such a system to perform processes which are provided
    in a graph [14]. This graph is built by the users of the mentioned platform using
    a low code user interface. It then proceeds to the orchestration system which
    is responsible for accessing the constructed graph. When the graph is accessed,
    each of its nodes is executed. Each node is referred to as a ‘job’ of the node.
    Orchestration systems are not the only ones recommended for running processes
    as workflows. In addition to orchestration systems, there are also choreography
    systems. In [15] a comparison of these two is made, as well as the usage environments
    of each one is analyzed. Orchestration systems seem to be preferable to choreography
    systems, in environments where there is no waiting for a process to be executed
    outside of the system. MathBlock is a system in which all processes are predefined.
    This means that it does not expect anything to be completed outside of its own
    system. Therefore, the handling it performs on the graph given as input is ideal,
    as explained in Section III. D. Systems for Mathematical Operations Systems have
    been built which solve mathematical operations at the level of simple values up
    to entire matrices. Reference [16] shows a system which can perform mathematical
    operations involving integrals and differential equations. It does this by making
    use of devices, namely metadevices. It demonstrates that metadevices can perform
    generalized matrix inversions and serve as the foundation for the gradient descent
    approach for solving a wide range of issues. Finally, a general upper constraint
    on the solution convergence time demonstrates the potential of such metadevices
    for stationary iterative systems. Other tools that are able to handle complex
    mathematical operations are MatLab and Octave [17]. These tools are giving to
    their users the ability to compute data using traditional and ML practices. Also,
    programming languages such as Python can be used for analytic purposes as well
    [17]. E. Object Storages Many kinds of systems use object storages, with the purpose
    of managing various data or sets of data as files. Reference [18] is exploring
    the concept of object storages. In this survey, all the key differences and similarities
    of using a Network File System(NFS) and an object storage are mentioned. One of
    the object storages mentioned is MinIO. This study concludes that object storages
    are a practice that is a cloud native solution in contrast to NFS. However, although
    object storages are cloud native, they have fewer features than NFS. Project [19]
    is using object storages to collect JavaScript Object format Notation (JSON) files
    and save them as-is in a cloud infrastructure, proving the cloud-native use of
    the system called MinIO as an object storage. F. Workflow Management Systems Reference
    [19] is analyzing a serverless data pipeline for a cloud computing environment.
    The particular system that is mentioned uses tools like MinIO as object storage.
    Within the document, it is mentioned how the pipeline is executed. This method
    uses a choreography system to execute specific processes after being invoked by
    another process. The order of the execution of the processes is as follows: (i)
    acquire data from the source, (ii) compress data and send to the cloud, (iii)
    decompress the data, (iv) detect data type, (v) convert data into JSON, (vi) store
    the data in object storage, and (vii) send relevant notifications to the end users.
    Another cloud-based data pipeline platform is analyzed in [20]. Unlike the previous
    related work, this system uses an orchestrator. Therefore, each of its processes
    is executed when the previous running processes finish, as opposed to the invoking
    that was done in the previous project. The Amazon Web Service (AWS) environment
    is used for the purpose of performing analyzes on COVID-19 data. In the experimentation
    presented in the referred article, data from reliable sources are used as well
    as the user interface of the administrator and the researchers participating in
    the system is presented. The system ML4ProFlow in [21] can manage workflow task
    using graphs. The referred document depicts the continuous development of the
    mentioned system, as a framework that combines the following components: (i) it
    manages execution environments, (ii) it defines processing modules that prioritize
    reusability and cross-platform compatibility, and (iii) it includes benchmarking
    automation to facilitate developers in the implementation and analysis of modules
    and their combinations. These three framework components are given, and their
    usage is shown. G. Advancements Beyond the Related Work The MathBlock service
    combines all the literature developed above, going beyond the current state of
    the art analysis. MathBlock seems to innovate the way mathematical operations
    are managed by combining techniques related to managing processes on entire datasets.
    First, batch data is used, in order to use the data as datasets. These data, as
    well as their results, are stored in the form of files (objects) within an object
    storage. In order to extract results, but also to access each mathematical operation
    of an expression, algorithms are used that concern the mentioned Workflow Management
    Systems, their related algorithms, as well as the mentioned orchestration systems.
    Furthermore, MathBlock is able to be used with containerization technologies,
    so that it can be easily scaled to computing clusters [22]. Thus, MathBlock can
    also be provided in Big Data ecosystems [23]–[25], whereas providing this service
    is achievable using technologies such as Docker and Kubernetes [26]. To conclude,
    MathBlock has gone through experiments using synthetic data, to extract relevant
    results. SECTION III. Mathblock A. Brief Description of the Service MathBlock
    is a microservice that may be operated in Kubernetes environments using tools
    like Docker. The Hypertext Transfer Protocol (HTTP) is used to communicate with
    the service. MathBlock receives as input a JSON object with information about
    the execution that must be performed. The service can receive various datasets
    from an object storage as well as simple values on which to do mathematical operations.
    The imported JSON object contains all the information about the dataset and the
    values it should utilize. The JSON item can also be represented as a graph, which
    the service can access and utilize to conduct the given mathematical operations.
    MathBlock returns a single column from a created dataset. This column holds the
    outcomes of every mathematical operation done on each row of the imported dataset.
    Fig. 1 presents the described architecture of the service. Fig. 1. Mathblock architecture.
    Show All B. Mathematical Operations as a Graph A protocol has been created that
    must be followed in order to use the service. As previously stated, MathBlock
    communicates with users and other programs over HTTP. The HTTP request to MathBlock
    must include a JSON body with the attributes provided in Table I. Table I. Json
    body attributes There should be a JSON attribute corresponding to a unique identification,
    as indicated in Table I. This identification is used to grant the user of the
    service access to the execution''s progress status. In more detail, during the
    execution of MathBlock, the status of the given execution is registered in a database.
    The status starts with the value “in progress” and ends when the execution is
    completed with the value “completed”. Thus, users of the service can make relevant
    HTTP calls whenever they wish and be informed about the status of the mathematical
    expression they wish to be executed. According to Table I, the user must additionally
    supply the locations where MathBlock may find the datasets to be utilized, as
    well as the locations where the user wants the result to be located. These locations
    must refer to paths within the object storage system that is being used. The most
    important part of the JSON object given to the service, is the last attribute
    shown in Table I. The “function” attribute is using a JSON object to specify what
    the service should do in its execution. Table II shows the attributes that the
    “function” JSON should include. Table II. Function attributes As indicated in
    Table II, “args” in each JSON Object must have two pieces of information as attributes.
    The first attribute, “arg_id” can take any integer value. The second attribute
    is either “feature” or “value”. If the property is “feature”, its value must be
    a string containing the name of the feature header that the service must use as
    input for the next imported path in the series, which is referred to Table I “inputs”
    attribute. In the instance when the property is “value”, the value must be a mathematical
    number or a Boolean value (true or false). If the attribute used is “value”, the
    value is still delivered as a string. The “expression” attribute of Table II gives
    to the service the expression to execute. To accomplish this, each JSON object
    listed in its value, should have the attributes showcased in Table III. Each JSON
    object of the “expression” list is called a MathBlock node. There are two types
    of nodes: (i) argument nodes, and (ii) operation nodes. Table III. Mathblock node
    attributes As shown in Table III, each MathBlock node has a unique identifier
    (in the “step” attribute) that may be used to reference other nodes via the “next”
    and “from” attributes. If a node does not originate from another, the “from” property
    must be set to zero (0). In addition, if a node has no following node to execute,
    the “next” attribute must be set to zero (0). In essence, nodes with no originating
    nodes are the MathBlock''s inputs, whereas nodes with no next node represent the
    outcome. Finally, Table IV is showing the information that each MathBlock node
    includes in the attribute “info”. As shown, there must be a “kind” attribute.
    This attribute is referring to the type of the node. On the other hand, there
    can be only one of the attributes “arg_id” and “name”. If the node''s type is
    “arg” then the attribute “arg_id” must be used. If the node''s type is “operation”
    then the attribute “name” must be used. The operations allowed to be used as values
    for the attribute “name” are the following: (i) “addition”, (ii) “subtraction”,
    (iii) “division”, (iv) “multiplication”, (v) “logarithm”, (vi) “power”, (vii)
    “logical_and”, (viii) “logical_or”, (ix) “logical_not”, (x) “equal”, (xi) “not_equal”,
    (xii) “less_or_equal”, (xiii) “greater_or_equal”, (xiv) “greater_than”, (xv) “less_than”,
    (xvi) “mean_value”, (xvii) “variance_value”, and (xviii) “amount of”. Table IV.
    Node''s information attributes C. Parsing of the Mathematical Operations Before
    explaining how each of the mathematical operations are executed by the MathBlock
    service, it is important to mention how the analyzed graph is parsed. In fact,
    each operation is performed as the graph is accessed, but the way that the operation
    is executed is completely separate to the parsing algorithm used. First, when
    MathBlock is called by a user or application, a new thread is built using the
    programming technique of multithreading. In this way it is possible to run many
    different imported graphs in a pseudo-parallel way (but in real parallel if a
    Kubernetes environment is used). After the execution status is given as “in progress”,
    the last step of preparation before the parsing of the graph is starting. This
    step is the initialization of a dictionary, which stores as keys all the “arg_id”
    mentioned above. The value of each key is a list with two values. The two values
    of the list are determined according to the type of argument: (i) in the case
    that the argument is a simple numeric or Boolean value, then the first position
    of the list is the literal “value”, and the second is the given value from the
    user, (ii) in the event that the argument is a column from a dataset that exists
    in the object storage, then the first value is the corresponding path within the
    object storage and the second value is the given feature that the user wishes
    to use as input. After the above, the graph is getting parsed by the service.
    In Fig. 2, the parsing processes are depicted using an activity diagram and the
    Unified Modeling Language (UML). Fig. 2. Parsing algorithm. Show All As represented
    in Fig. 2, the first step of the algorithm is the isolation of the expression
    given by the user. The service then makes a serial search for MathBlock nodes
    that do not come from other nodes. That is, in this step, all named starting nodes
    are searched. The graph within the algorithm acts as an inverted tree. The starting
    nodes are the leaves of the tree while the root of the tree is the result of the
    MathBlock service. Therefore, the result can also be called an ending node. The
    ending node is unique, unlike the starting nodes which are at least one. In the
    next step, an iterative process starts for each starting node. In each iteration
    loop, the recursion technique is used to access all subsequent nodes of the current
    node. A tree with one root, many leaves, and many branches will necessarily have
    its branches joined together. All the branches join and eventually end up in the
    trunk of the tree. Therefore, keeping the latter in mind, it is easy to establish
    that at some point two starting nodes will have some common next node in which
    the two theoretical branches are merging. MathBlock''s parsing algorithm controls
    this condition so that it performs math operations only when all previous operations
    have been performed. So as MathBlock parses the graph, a loop terminates if it
    ends up merging with another branch of the tree whose execution is not complete.
    But a recursion loop will continue to access the graph in case the other branches
    it encounters have terminated their loop. So, it is certain that the last loop
    will be able to access all the nodes that are unions of other nodes. D. Execution
    of the Mathematical Operations When MathBlock parses a node, it performs the operation
    specified within it. In this sub-section, we will go through what each node does
    in detail. However, before proceeding with the required analysis, it is necessary
    to define how MathBlock stores the results. These outcomes are stored in the object
    storage, which uses MinIO. The outcome of an operation can be a complete portion
    of the expression used or even the ultimate result of a user call. The service
    ensures that every time it accesses a node, there is a corresponding object storage
    path that includes the information that the expression''s next nodes must use.
    If this path already exists, it does not create a new one. When this path does
    not exist, it creates it in an ordered manner and stores a Comma-separated Values
    (CSV) file within it. There is only one column labeled as “result” in the aforementioned
    file, and its rows include all of the results for the presently completed node.
    As previously stated, there are two types of nodes. If a node is an argument type,
    MathBlock utilizes the attribute “arg_id” to look it up in the dictionary it created
    before beginning the graph parsing. After the service finds the argument, there
    are two possible outcomes, depending on whether the argument is a single value
    or a whole column of a dataset. If it is a basic value, the service simply generates
    an exported file that has only the value entered by the user in the first row.
    MathBlock marks this path as a “value path” after it saves it. If, on the other
    hand, the given input represents a feature of a dataset, MathBlock does not create
    a new path because it already exists. As indicated in the previous sub-section,
    the user provides the route as input when invoking MathBlock. In the two scenarios
    described, but also when the node is an operation, the service records the path
    for this node. This notation aids the following nodes in determining which results
    to act on. MathBlock can perform several operations, all of which have already
    been stated. Moving on, the generation of the exported files in the object storage
    is described. The analysis performed in the relevant operations only applies to
    the circumstance in which all the inputs to each operation are whole columns from
    prior results or user inputs. If all of an operation''s inputs are simply values,
    the extracted result is the operation of the two values and the extracted file
    contains the result as a value. However, if one of an operation''s inputs is a
    column and the remaining inputs contain at least one simple value, all the values
    are turned into columns. This conversion is based on the column that has the most
    rows in the specified columns. Simple values are translated into columns in which
    all rows have the same value as the input simple value. The size of the created
    column is the same as the size of the column with the most rows. The operation
    is then executed in the following manner, as it has now been translated into operation
    inputs that are only columns. When an operation has only columns as inputs, the
    service first retrieves the data from the object storage. It then organizes the
    data into arrays and begins processing them in memory. It is simple to compute
    the results. The outcomes are generated and placed in a new array based on the
    operation that was called. For example, if the supplied operation is addition,
    a loop is simply utilized to add the data by two. The procedure is applied for
    the other mathematical expressions as well. Finally, the new path is created within
    the object storage and the results are stored there. The execution of a statistical
    operation is the final case that can occur during the execution of MathBlock operations.
    These operations are the following: (i) “mean_value”, (ii) “variance_value”, and
    (iii) “amount_of’. They are known as statistical operations because they find
    statistical measures on a dataset feature, such as the mean value. MathBlock stores
    the result as a simple value in object storage after computing the information
    required to complete these operations. There are instances when an operation is
    unable to complete the tasks assigned to it. These instances are as follows: (i)
    a statistical operation cannot be calculated (e.g., the operation is to the “variance
    value” and there is only one row), (ii) different-sized column inputs are provided,
    and (iii) an attempt is made to execute an illegal operation (e.g., division by
    zero). In each of these scenarios, the service returns the result “None”. As a
    result, if an extracted value is “None”, it signifies that no action could be
    performed, and no meaningful result could be generated in its place. E. Construction
    of Functions Using Operations MathBlock enables its users to conduct a variety
    of operations on the datasets they wish to process or analyze. The first operations
    discovered to be useable by MathBlock users are the ones already described. They
    let the user to perform mathematical, comparative, logical, and statistical procedures.
    However, the various functions available to users do not end there. A user can
    create his/her own functions using the graph that MathBlock utilizes to conduct
    operations. These functions can employ all of MathBlock''s primary operations
    several times. The outcomes of two such small functions are discussed in the following
    chapter. In this chapter. some examples of the functions that can be created are
    given. A user can find the number of values in a dataset that are more than a
    user-specified threshold by using the “greater than” and “amount of” operations.
    This can be accomplished by commencing with the “greater than” operation and specifying
    the value the user is looking for. This operation will produce boolean results
    with values of “true” or “false”. Then, using the “amount of”, one can identify
    and count all “true” values. Many algebraic identities can be established by employing
    the operations “power”, “addition” and “subtraction”. A user can generate the
    difference of squares, sum of squares, and the squared difference using these.
    Of course, this is still utilized in the cubic difference and other related identities.
    As it is clear, a user can create any function he/she desires. In the same manner
    that one can write operations he/she wants to do as functions on a piece of paper,
    one can work by constructing the MathBlock graph. As a result, the user''s imagination
    serves as the limit. SECTION IV. Mathblock Experimentation A. Experimentation
    Method Experimentation was carried out on the MathBlock service to demonstrate
    how it works. More specifically, data from the maritime [27] and healthcare [28]
    sectors were utilized. The feature “speed” is utilized for Maritimes data and
    the feature “age” is used for healthcare data in the aforementioned experiment.
    Both approaches demonstrate how MathBlock can be used. As a result, it is easy
    to grasp how it works and when the service is valuable to a user. The aforementioned
    characteristics were used to generate synthetic data on them. The experiment employs
    MathBlock to determine whether the method used to generate the synthetic data
    approximates the real data. This is accomplished using mean values, as discussed
    in each experiment. Before going over each experiment, it is a good idea to go
    over how the synthetic data were generated. First, the features to be utilized
    were chosen, notably “age” and “speed”, as previously described. Following that,
    the mean value and standard deviation of the two specified features were discovered.
    The obtained values were then used to create their respective Normal Distributions.
    Then, as much random data as the data contained in each corresponding genuine
    characteristic was generated. The Maritimes data contained 38994 distinct rows,
    whereas the healthcare data contained 1575 distinct rows. Because the two datasets
    have a high number of records, the mean and variance values will be similar in
    both examples of fake data generation. The mean value of the created Maritimes
    data was close to zero. As a result, the random data contained negative values.
    These values were converted to their corresponding absolute positive values. In
    the Maritimes data, the mean will deviate by a percentage and should theoretically
    be much bigger if MathBlock functions properly. B. Maritimes Experimentation Table
    V displays a sample of the real data that was used for the Maritimes data testing.
    As previously stated, the “speed” feature was used. Table VI depicts a sample
    of the created synthetic column. After entering the data into the object storage,
    the graph that MathBlock will use was written. This graph uses the two object
    storage path inputs where the real and synthetic data were stored. The graph was
    given the inverted tree expression illustrated in Fig. 3. Fig. 3. Maritimes inverted
    tree expression. Show All More specifically, MathBlock was asked to calculate
    the mean values of the real and synthetic data. After obtaining these statistical
    metrics, the mean value of the synthetic data should be subtracted from the mean
    value of the real data. Because the last operation being the root of the tree,
    it will be the outcome of the experiment. Since several numbers in the specific
    synthetic data were utilized with their absolute value, the real mean is predicted
    to be lower than the synthetic one, and hence the result should be fairly negative.
    Table V. Real maritimes dataset Table VI. Synthetic maritimes dataset C. Healthcare
    Experimentation Table VII shows a sample of the real data utilized for healthcare
    data testing. The “age” feature was utilized, as previously indicated. Table VIII
    shows a sample of the synthetic column that was constructed. The graph that MathBlock
    will employ was written once the data was entered into the object storage. This
    graph makes advantage of the two object storage path inputs, which contain both
    the real and the synthetic datasets. The inverted tree expression shown in Fig.
    4 was applied to the graph. Fig. 4. Healthcare inverted tree expression. Show
    All MathBlock was instructed to first subtract the values of the synthetic data
    from the real data. MathBlock is then asked to calculate the mean value of the
    newly computed column. The process used in this and earlier experiments from a
    mathematical point of view is the same. However, in this experiment, the use of
    a column operation is done as a proof of concept. Table VII. Real healthcare dataset
    Table VIII. Synthetic healthcare dataset D. Experimentation Results In the example
    of Maritimes data, Fig. 5 depicts the speed of each of the 200 first vessels in
    the dataset as a blue line and the speed reported by the synthetic data as an
    orange line. It is clear that the mean value of the synthetic data is higher.
    This experiment resulted with the negative number “-4.29” according to MathBlock.
    So, the outcome was as planned, and the service functioned flawlessly. The two
    mean values were initially recorded as simple values in the object storage and
    then simply subtracted. The second experiment appears to be more interesting.
    Fig. 6, graphically visualizes the first 200 magnitudes of the real ages with
    the blue line, while also displaying the magnitudes of the synthetic ages with
    the orange line. MathBlock first performed the subtraction of the two as requested
    by the received graph. The result of the subtraction is a whole column whose values
    are represented in Fig. 7. After that, the graph is asked to find the mean value
    of these new values. Specifically, the value placed in the object storage by the
    service is the number “0.16”. This number is very small, and it shows that the
    synthetic data was quite close to the real data. As a result, the answer of MathBlock
    is right in the second experiment as well. Fig. 5. Maritimes real and synthetic
    speeds. Show All Fig. 6. Healthcare real and synthetic ages. Show All Fig. 7.
    Healthcare age difference. Show All SECTION V. Discussion and Concluding Remarks
    This paper investigated the functionality of the MathBlock service. First, certain
    fundamental principles were presented to introduce the user to the topic being
    discussed. Following, some references to relevant studies on batch data, synthetic
    data, orchestration systems, mathematical operations systems, object storages,
    and workflow management systems were provided. MathBlock was then examined in
    terms of how to utilize it and how it functions. In addition, two experiments
    were performed as proof of concept utilizing synthetic data. Finally, several
    graphs were used to evaluate and discuss the outcomes. It is undeniable that MathBlock
    is an essential tool for anyone who needs to execute mathematical, logical, and
    comparison operations on various datasets. It can use a variety of features with
    no restrictions on the type of each one. It can also be implemented as a microservice
    in scalable systems due to its ability to be utilized by other apps and users
    over the HTTP protocol, being a novel service that can be used as a language-independent
    mathematical library. Furthermore, it can be used in low-code environments such
    as the previously stated [24], which can construct the explained graph automatically
    for the users. Such systems are going to take computer science to the next level
    by making technology and resources available globally via the cloud. The service
    under consideration has a diverse set of potential users. These users could be
    data analysts who want to run complex mathematical operations on their datasets.
    Furthermore, the service can be used by scientists from other fields who do not
    have special understanding of data analysis (e.g., students, healthcare practitioners,
    and marketing specialists). However, the service is not only able to be used by
    humans since MathBlock will be especially beneficial in systems that can automatically
    generate the graph given to them. Because the HTTP protocol is one of the most
    widely used, it facilitates experimentation and integration with other systems.
    However, it is necessary to mention the limitations of the service, which must
    be addressed. Three significant constraints have been identified: (i) If the input
    data contains values that cannot conduct an operation with another value, the
    outcome is “None”. In such a circumstance, a user may wish to be notified by the
    MathBlock system to make a change. (ii) The system uses arrays to manage the values
    of the columns it calculates. The use of arrays has the critical limitation that
    the size of memory used cannot exceed the memory accessible to the system. If
    this occurs, an error will occur, and the calculating procedure will be terminated.
    (iii) Even with the necessary documentation, constructing the graph is tough.
    It is advantageous to have a system that can automate the graph construction process.
    Nevertheless, MathBlock can be enhanced through several actions, which are also
    among our next steps. The ability to calculate mathematical expressions on streaming
    data [29] is the first and most significant feature that would be good to add
    to the service. This would allow such a technology to be used in far more flexible
    ways in sensor-enabled settings. It will also be beneficial to include more mathematical
    and statistical operations such as “max”, “min”, “sum”, and others. It would also
    offer an added value in the case that such a system could employ mathematical
    limits and integrals as well. Finally, it would make sense to include operations
    that are already used by MathBlock but are not immediately visible. For example,
    using the “power” function with the value “0.5”, a user can get the square root
    of any values in a dataset. This situation, however, does not arise to a man who
    is accustomed to representing the square root by its mathematical notation. As
    a result, adding the operation “root” would be a welcomed addition. ACKNOWLEDGMENT
    The research has been financed by the European Union and Greek national funds
    through the Operational Program Competitiveness, Entrepreneurship and Innovation,
    under the call RESEARCH - CREATE - INNOVATE (project codes: RESPECT - T2EDK-03741,
    DIASTEMA - T2EDK-04612). Authors Figures References Citations Keywords Metrics
    More Like This Optimal assignment of research and development projects in a large
    company using an integer programming model IEEE Transactions on Engineering Management
    Published: 1965 A program of research on the research and development process
    IEEE Transactions on Engineering Management Published: 1964 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 15th International Conference on Computer Research and Development,
    ICCRD 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'MathBlock: Performing Complex Mathematical Operations on Synthetic Data'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Borisaniya B.
  - Kumar S.
  citation_count: '0'
  description: In the past decade, it has been observed that the number of devices
    is increasing at an exponential rate all around the world. In such a case, there
    is a need for networking across the globe, irrespective of real-time constraints.
    As a result, long-range communication gets affected on a large scale. In this
    context, the Internet of Things (IoT) environment provides a platform where different
    types of devices coexist to perform various operations. Since the IoT devices
    at the physical layer have very small memory footprints, it becomes extremely
    difficult for them to perform heavy computation locally. The cloud provides a
    platform to reduce this limitation. The integration of the IoT with the cloud
    can be considered a cost-effective solution for businesses, wherein there is a
    flexibility for managing and analyzing the data gathered by the connected devices.
    Moreover, fog computing plays a major role in nullifying the gap between the cloud
    and IoT devices. This chapter discusses the promising role of an IoT environment
    to fulfill the needs of real-time computation and communication in the current
    era. Additionally, the idea behind utilizing cloud and fog computing, their architectural
    frameworks, design, implementation potential, and applications with respect to
    the IoT environment are also discussed. Finally, the significance of integrating
    IoT, cloud and fog computing is explored with the use of a few case studies.
  doi: 10.1201/9781003298335-13
  full_citation: '>'
  full_text: '>

    "Access Provided By:University of Nebraska-Lincoln T&F eBooks ‍ Advanced Search
    Login About Us Subjects Browse Products Request a trial Librarian Resources What''s
    New!! HomeComputer ScienceAlgorithms & ComplexityBig Data, Cloud Computing and
    IoTIoT Network Used in Fog and Cloud Computing Chapter IoT Network Used in Fog
    and Cloud Computing ByBhavesh Borisaniya, Saurabh Kumar Book Big Data, Cloud Computing
    and IoT Edition 1st Edition First Published 2023 Imprint Chapman and Hall/CRC
    Pages 20 eBook ISBN 9781003298335 Share ABSTRACT In the past decade, it has been
    observed that the number of devices is increasing at an exponential rate all around
    the world. In such a case, there is a need for networking across the globe, irrespective
    of real-time constraints. As a result, long-range communication gets affected
    on a large scale. In this context, the Internet of Things (IoT) environment provides
    a platform where different types of devices coexist to perform various operations.
    Since the IoT devices at the physical layer have very small memory footprints,
    it becomes extremely difficult for them to perform heavy computation locally.
    The cloud provides a platform to reduce this limitation. The integration of the
    IoT with the cloud can be considered a cost-effective solution for businesses,
    wherein there is a flexibility for managing and analyzing the data gathered by
    the connected devices. Moreover, fog computing plays a major role in nullifying
    the gap between the cloud and IoT devices. This chapter discusses the promising
    role of an IoT environment to fulfill the needs of real-time computation and communication
    in the current era. Additionally, the idea behind utilizing cloud and fog computing,
    their architectural frameworks, design, implementation potential, and applications
    with respect to the IoT environment are also discussed. Finally, the significance
    of integrating IoT, cloud and fog computing is explored with the use of a few
    case studies. Previous Chapter Next Chapter Your institution has not purchased
    this content. Please get in touch with your librarian to recommend this.  To purchase
    a print version of this book for personal use or request an inspection copy  GO
    TO ROUTLEDGE.COM  Policies Privacy Policy Terms & Conditions Cookie Policy Journals
    Taylor & Francis Online Corporate Taylor & Francis Group Help & Contact Students/Researchers
    Librarians/Institutions Connect with us Registered in England & Wales No. 3099067
    5 Howick Place | London | SW1P 1WG © 2024 Informa UK Limited"'
  inline_citation: '>'
  journal: 'Big Data, Cloud Computing and IoT: Tools and Applications'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: IoT Network Used in Fog and Cloud Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Srivastava A.
  - Kumar N.
  citation_count: '1'
  description: Cloud computing has become a growing technology and has received wide
    acceptance in the scientific community and large organizations like government
    and industry. Due to the highly complex nature of VM virtualization, lightweight
    containers have gained wide popularity, and techniques to provision the resources
    to these containers have drawn researchers towards themselves. The models or algorithms
    that provide dynamic scalability which meets the demand of high performance and
    QoS utilizing the minimum number of resources for the containerized cloud have
    been lacking in the literature. The dynamic scalability facilitates the cloud
    services in offering timely, on-demand, and computing resources having the characteristic
    of dynamic adjustment to the end users. The manuscript has presented a technique
    which has exploited the queuing model to perform the dynamic scalability and scale
    the virtual resources of the containers while reducing the finances and meeting
    up the user’s Service Level Agreement (SLA). The paper aims in improving the usage
    of virtual resources and satisfy the SLA requirements in terms of response time,
    drop rate, system throughput, and the number of containers. The work has been
    simulated using Cloudsim and has been compared with the existing work and the
    analysis has shown that the proposed work has performed better
  doi: 10.14569/IJACSA.2023.0140150
  full_citation: '>'
  full_text: '>

    "HOME ABOUT US JOURNALS CONFERENCES CONTACT US Home Call for Papers Indexing Submit
    your Paper Guidelines Fees Current Issue Archives Editors Reviewers Subscribe
    DOI: 10.14569/IJACSA.2023.0140150 PDF Queueing Model based Dynamic Scalability
    for Containerized Cloud Author 1: Ankita Srivastava Author 2: Narander Kumar International
    Journal of Advanced Computer Science and Applications(IJACSA), Volume 14 Issue
    1, 2023. Abstract and Keywords How to Cite this Article {} BibTeX Source Abstract:
    Cloud computing has become a growing technology and has received wide acceptance
    in the scientific community and large organizations like government and industry.
    Due to the highly complex nature of VM virtualization, lightweight containers
    have gained wide popularity, and techniques to provision the resources to these
    containers have drawn researchers towards themselves. The models or algorithms
    that provide dynamic scalability which meets the demand of high performance and
    QoS utilizing the minimum number of resources for the containerized cloud have
    been lacking in the literature. The dynamic scalability facilitates the cloud
    services in offering timely, on-demand, and computing resources having the characteristic
    of dynamic adjustment to the end users. The manuscript has presented a technique
    which has exploited the queuing model to perform the dynamic scalability and scale
    the virtual resources of the containers while reducing the finances and meeting
    up the user’s Service Level Agreement (SLA). The paper aims in improving the usage
    of virtual resources and satisfy the SLA requirements in terms of response time,
    drop rate, system throughput, and the number of containers. The work has been
    simulated using Cloudsim and has been compared with the existing work and the
    analysis has shown that the proposed work has performed better. Keywords: Cloud
    computing; scalability; containers; containerized cloud models; queueing model            Copyright
    Statement: This is an open access article licensed under a Creative Commons Attribution
    4.0 International License, which permits unrestricted use, distribution, and reproduction
    in any medium, even commercially as long as the original work is properly cited.
    Upcoming Conferences Future of Information and Communication Conference (FICC)
    2024 4-5 April 2024 Berlin, Germany Computing Conference 2024 11-12 July 2024
    London, United Kingdom IntelliSys 2024 5-6 September 2024 Amsterdam, The Netherlands
    Future Technologies Conference (FTC) 2024 14-15 November 2024 London, United Kingdom
    BACK TO TOP COMPUTER SCIENCE JOURNAL About the Journal Call for Papers Submit
    Paper Indexing OUR CONFERENCES Computing Conference Intelligent Systems Conference
    Future Technologies Conference Communication Conference HELP & SUPPORT Contact
    Us About Us Terms and Conditions Privacy Policy © The Science and Information
    (SAI) Organization Limited. All rights reserved. Registered in England and Wales.
    Company Number 8933205. thesai.org"'
  inline_citation: '>'
  journal: International Journal of Advanced Computer Science and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Queueing Model based Dynamic Scalability for Containerized Cloud
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Alyas T.
  - Tabassum N.
  - Iqbal M.W.
  - Alshahrani A.S.
  - Alghamdi A.
  - Shahzad S.K.
  citation_count: '5'
  description: Kubernetes, a container orchestrator for cloud-deployed applications,
    allows the application provider to scale automatically to match the fluctuating
    intensity of processing demand. Container cluster technology is used to encapsu-late,
    isolate, and deploy applications, addressing the issue of low system reliabil-ity
    due to interlocking failures. Cloud-based platforms usually entail users define
    application resource supplies for eco container virtualization. There is a constant
    problem of over-service in data centers for cloud service providers. Higher operating
    costs and incompetent resource utilization can occur in a waste of resources.
    Kubernetes revolutionized the orchestration of the container in the cloud-native
    age. It can adaptively manage resources and schedule containers, which provide
    real-time status of the cluster at runtime without the user’s contribution. Kuber-netes
    clusters face unpredictable traffic, and the cluster performs manual expansion
    configuration by the controller. Due to operational delays, the system will become
    unstable, and the service will be unavailable. This work proposed an RBACS that
    vigorously amended the distribution of containers operating in the entire Kubernetes
    cluster. RBACS allocation pattern is analyzed with the Kuber-netes VPA. To estimate
    the overall cost of RBACS, we use several scientific benchmarks comparing the
    accomplishment of container to remote node migra-tion and on-site relocation.
    The experiments ran on the simulations to show the method’s effectiveness yielded
    high precision in the real-time deployment of resources in eco containers. Compared
    to the default baseline, Kubernetes results in much fewer dropped requests with
    only slightly more supplied resources.
  doi: 10.32604/iasc.2023.028815
  full_citation: '>'
  full_text: '>

    "Submit LOGIN REGISTER Home Academic Journals Books & Monographs Conferences Language
    Service News & Announcements About Home/ Journals/ IASC/ Vol.35, No.1, 2023/ 10.32604/iasc.2023.028815
    Submit a Paper Propose a Special lssue Table of Content Abstract Introduction
    Literature Review Proposed Methodology Experimental Results Conclusion References
    Open Access ARTICLE Resource Based Automatic Calibration System (RBACS) Using
    Kubernetes Framework Tahir Alyas1, Nadia Tabassum2, Muhammad Waseem Iqbal3,*,
    Abdullah S. Alshahrani4, Ahmed Alghamdi5, Syed Khuram Shahzad6 1 Department of
    Computer Science, Lahore Garrison University, Lahore, 54000, Pakistan 2 Department
    of Computer Science & IT, Virtual University of Pakistan, Lahore, 54000, Pakistan
    3 Department of Software Engineering, The Superior University, Lahore, 54000,
    Pakistan 4 Department of Computer Science & Artificial Intelligence, College of
    Computer Science & Engineering, University of Jeddah, 21493, Saudi Arabia 5 Department
    of Software Engineering, College of Computer Science and Engineering, University
    of Jeddah, 21493, Saudi Arabia 6 Department of Informatics & Systems, University
    of Management & Technology, Lahore, 54000, Pakistan * Corresponding Author: Muhammad
    Waseem Iqbal. Email: Intelligent Automation & Soft Computing 2023, 35(1), 1165-1179.
    https://doi.org/10.32604/iasc.2023.028815 Received 18 February 2022; Accepted
    24 March 2022; Issue published 06 June 2022 View Full Text Download PDF Abstract
    Kubernetes, a container orchestrator for cloud-deployed applications, allows the
    application provider to scale automatically to match the fluctuating intensity
    of processing demand. Container cluster technology is used to encapsulate, isolate,
    and deploy applications, addressing the issue of low system reliability due to
    interlocking failures. Cloud-based platforms usually entail users define application
    resource supplies for eco container virtualization. There is a constant problem
    of over-service in data centers for cloud service providers. Higher operating
    costs and incompetent resource utilization can occur in a waste of resources.
    Kubernetes revolutionized the orchestration of the container in the cloud-native
    age. It can adaptively manage resources and schedule containers, which provide
    real-time status of the cluster at runtime without the user’s contribution. Kubernetes
    clusters face unpredictable traffic, and the cluster performs manual expansion
    configuration by the controller. Due to operational delays, the system will become
    unstable, and the service will be unavailable. This work proposed an RBACS that
    vigorously amended the distribution of containers operating in the entire Kubernetes
    cluster. RBACS allocation pattern is analyzed with the Kubernetes VPA. To estimate
    the overall cost of RBACS, we use several scientific benchmarks comparing the
    accomplishment of container to remote node migration and on-site relocation. The
    experiments ran on the simulations to show the method’s effectiveness yielded
    high precision in the real-time deployment of resources in eco containers. Compared
    to the default baseline, Kubernetes results in much fewer dropped requests with
    only slightly more supplied resources. Keywords Docker; container; virtualization;
    cloud resource; kubernetes Cite This Article T. Alyas, N. Tabassum, M. Waseem
    Iqbal, A. S. Alshahrani, A. Alghamdi et al., \"Resource based automatic calibration
    system (rbacs) using kubernetes framework,\" Intelligent Automation & Soft Computing,
    vol. 35, no.1, pp. 1165–1179, 2023. BibTex EndNote RIS    This work is licensed
    under a Creative Commons Attribution 4.0 International License , which permits
    unrestricted use, distribution, and reproduction in any medium, provided the original
    work is properly cited. We recommend Security Monitoring and Management for the
    Network Services in the Orchestration of SDN-NFV Environment Using Machine Learning
    Techniques Nasser Alshammari et al., Computer Systems Science and Engineering,
    2024 Container Instrumentation and Enforcement System for Runtime Security of
    Kubernetes Platform with eBPF Songi Gwak et al., Intelligent Automation & Soft
    Computing, 2023 Vertical Pod Autoscaling in Kubernetes for Elastic Container Collaborative
    Framework Mushtaq Niazi et al., CMC-Computers, Materials & Continua, 2022 Service
    Level Agreement Based Secured Data Analytics Framework for Healthcare Systems
    S. Benila et al., Intelligent Automation & Soft Computing, 2022 Pedestrian and
    Vehicle Detection Based on Pruning YOLOv4 with Cloud-Edge Collaboration Huabin
    Wang et al., CMES-Computer Modeling in Engineering & Sciences, 2023 Real-Time
    Scheduling on Dynamic Resources in a Fog Computing Environment Vrinda Kochar et
    al., Comparative Politics, 2017 Integration of Communication and Navigation Technologies
    toward LEO-Enabled 6G Networks: A Survey Yihai Liao et al., Selections from Space:
    Science & Technology, 2023 Requirement-Oriented TT&C Method for Satellite Based
    on BDS-3 Short-Message Communication System Chao Li et al., Selections from Space:
    Science & Technology, 2023 Learning-Based Run-Time Power and Energy Management
    of Multi/Many-Core Systems: Current and Future Trends Amit Kumar Singh et al.,
    Comparative Politics, 2017 AUTOSIM: Automated Urban Traffic Operation Simulation
    via Meta-Learning Yuanqi Qin et al., IEEE/CAA Journal of Automatica Sinica, 2023
    Powered by Downloads Citation Tools 1218 View 582 Download 0 Like 6 Related articles
    Alfalfa (Medicago sativa L.) production in soil at different bulk densities under
    controlled conditions Martínez-Rubin de Celis E, E Rivas-Robles,... Effect of
    cell size and cytokinins on growth of petunia plants Lagoutte S, M Divo de Sesar,
    F... Rotational Motion of Micropolar Fluid Spheroid in Concentric Spheroidal Container
    M. Krishna Prasad, G. Manpreet... Electromagnetic Levitation Part I: Theoretical
    and Experimental Considerations Sayavur I. Bakhtiyarov, Dennis... Simulation of
    Sloshing with the Volume of Fluid Method M.H. Djavareshkian, M. Khalili"'
  inline_citation: '>'
  journal: Intelligent Automation and Soft Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Resource Based Automatic Calibration System (RBACS) Using Kubernetes Framework
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Meyer V.
  - da Silva M.L.
  - Kirchoff D.F.
  - De Rose C.A.F.
  citation_count: '4'
  description: 'Cloud computing allows several applications to share physical resources,
    yielding rapid provisioning and improving hardware utilization. However, multiple
    applications contending for shared resources are susceptible to interference,
    which might lead to significant performance degradation and consequently an increase
    in Service Level Agreements violations. In previous work, we started to analyze
    resource contention and its impact on performance degradation and hardware utilization.
    Then, we created an interference-aware application classifier based on machine
    learning techniques and evaluated it comparing two classification strategies:
    (i) unique, when a single classification is performed over the entire applications’
    execution; and (ii) segmented, when the classification is carried out over multiple
    static-defined intervals. Moving towards a dynamic scheduling solution, we combine
    and improve on previous work findings and, in this work, we present IADA, a full-fledged
    dynamic interference-aware cloud scheduling architecture for latency-sensitive
    workloads. Our approach consists in improving on a segmented interference classification
    of applications to a dynamic classification scheme based on workload variations.
    Aiming at using the available resource more efficiently and respecting Quality
    of Services requirements, the proposed architecture was developed supported by
    machine learning techniques, heuristics, and a bayesian changepoint detection
    algorithm for online inference. We conducted a set of real and simulated experiments,
    utilizing a developed extension of CloudSim Toolkit to analyze and compare the
    proposed architecture efficiency with related studies. Results evidenced that
    IADA reduces by 25%, on average, the overall performance degradation.'
  doi: 10.1016/j.jss.2022.111491
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Background and state-of-the-art
    3. Dynamic interference-aware scheduling architecture 4. Evaluation and results
    5. Related work 6. Conclusion and future directions CRediT authorship contribution
    statement Declaration of Competing Interest Acknowledgments References Vitae Show
    full outline Cited by (7) Figures (15) Show 9 more figures Tables (4) Table 1
    Table 2 Table 3 Table 4 Journal of Systems and Software Volume 194, December 2022,
    111491 IADA: A dynamic interference-aware cloud scheduling architecture for latency-sensitive
    workloads☆ Author links open overlay panel Vinícius Meyer, Matheus L. da Silva,
    Dionatrã F. Kirchoff, Cesar A.F. De Rose Show more Share Cite https://doi.org/10.1016/j.jss.2022.111491
    Get rights and content Highlights • A novel dynamic interference-aware scheduling
    architecture. • Experiments with an extension of a cloud simulator platform. •
    Comparison with related schedulers found in the literature. • Evaluation of the
    general architecture overhead to validate its usability. Abstract Cloud computing
    allows several applications to share physical resources, yielding rapid provisioning
    and improving hardware utilization. However, multiple applications contending
    for shared resources are susceptible to interference, which might lead to significant
    performance degradation and consequently an increase in Service Level Agreements
    violations. In previous work, we started to analyze resource contention and its
    impact on performance degradation and hardware utilization. Then, we created an
    interference-aware application classifier based on machine learning techniques
    and evaluated it comparing two classification strategies: (i) unique, when a single
    classification is performed over the entire applications’ execution; and (ii)
    segmented, when the classification is carried out over multiple static-defined
    intervals. Moving towards a dynamic scheduling solution, we combine and improve
    on previous work findings and, in this work, we present IADA, a full-fledged dynamic
    interference-aware cloud scheduling architecture for latency-sensitive workloads.
    Our approach consists in improving on a segmented interference classification
    of applications to a dynamic classification scheme based on workload variations.
    Aiming at using the available resource more efficiently and respecting Quality
    of Services requirements, the proposed architecture was developed supported by
    machine learning techniques, heuristics, and a bayesian changepoint detection
    algorithm for online inference. We conducted a set of real and simulated experiments,
    utilizing a developed extension of CloudSim Toolkit to analyze and compare the
    proposed architecture efficiency with related studies. Results evidenced that
    IADA reduces by 25%, on average, the overall performance degradation. Previous
    article in issue Next article in issue Keywords Interference-awareResource managementDynamic
    workloadsMachine learningCloud computing 1. Introduction Due to the promise of
    unlimited computing resources and the pay-per-use model, many internet-based applications
    have started to target cloud computing infrastructures as an attracting solution
    (Meyer et al., 2019a). Cloud environments provide on-demand resources through
    the benefits of the virtualization technology for users to execute many services
    (Alboaneen et al., 2021). Such technology reduces operational costs in data centers
    by minimizing the number of hardware in use and increasing their utilization by
    loading more than one virtual machine (VM) instance on the same physical machine
    (PM). However, several cloud-applications contending for shared resources can
    produce cross-application interference, which may lead to considerable performance
    degradation and consequently to an increase in Service Level Agreement (SLA) violations
    (Meyer et al., 2021b). When scheduling users’ services, cloud providers have to
    carefully place the VMs to the hosts in a way that the objectives from both providers
    and users would be optimized, which is a non-trivial task. In previous work (Meyer
    et al., 2020), it has been presented that when there exists more than one application
    using the same resource (CPU, memory, network, cache, or disk), depending on the
    most stressed one, the resulting performance degradation index is different. For
    instance, by comparing applications contending for distinct hardware resources,
    we have observed that disk-intensive is the one that presents the highest interference
    levels, subsequently producing a substantial performance degradation among applications.
    To deeply mitigate this problem, Ludwig et al. (2019) created an interference
    classification based on levels for placement policies to improve resource utilization.
    However, their classification method was developed using fixed and empirically-defined
    thresholds. Aiming at tackling this issue, in previous work (Meyer et al., 2020),
    we introduced an interference-aware application classifier which is assisted by
    machine learning techniques to automatically define interference levels, and based
    on that, classify the applications. When compared to related studies, our classification
    approach demonstrates an improvement in placement decisions efficiency by 23%,
    on average, reducing resource consumption and also performance degradation at
    the application level. Likewise (Ludwig et al., 2019), our classification strategy
    created an interference level label for each application over its entire execution.
    Even though the resource utilization presented overall improvements, we believed
    that just a single interference label for the entire application’s execution cycle
    does not represent accurately its behavior, especially when abrupt changes may
    occur. Within a scenario with dynamic workloads, the hardware utilization may
    vary significantly, generating distinct interference rates throughout the application’s
    execution and, consequently, directly affecting the application’s degradation.
    Therefore, in previous work (Meyer et al., 2021b), we proposed a classification
    scheme that analyzes interference indexes’ changes over the applications to evaluate
    the benefits of static-defined segmented scheduling. Preliminary results revealed
    an improvement in resource utilization efficiency by 27%, on average, when applying
    our classification approach in cloud infrastructures. With these results, we have
    noticed we started moving towards dynamic interference-aware scheduling. To advance
    a step forward in this direction, we combine and improve on previous work findings
    and, in this work, we present IADA, an interference-aware scheduling architecture
    for dynamic workloads in cloud computing environments. The main goal is to analyze
    applications workloads, and based on the interference they generate, make dynamic
    scheduling decisions, in real-time. IADA has three principal components that profiles,
    analyzes, and performs scheduling decision. Aiming at using the available resource
    more efficiently and respecting Quality of Services (QoS) requirements, the proposed
    architecture was built supported by machine learning techniques, heuristics, and
    a bayesian changepoint detection algorithm for online inference. We compare our
    solution with related work using real workloads patterns and results show that
    IADA reduces the resulting performance degradation by 26% in real experiments,
    and by 24% in simulated ones. Concretely, the main contributions of this work
    are: • We proposed a resource scheduling architecture observing cross-application
    interference aspects for dynamic workloads. Unlike previous work, which has tackled
    partial issues, in this study, we present a full scheduling architecture solution
    targeting real production systems. • We used an online bayesian changepoint detection
    (OBCD) algorithm to find time-points automatically to perform classification and
    scheduling decisions. This specific topic was considered a gap found in previous
    work (Meyer et al., 2021b). Since we did not know how to find the best moments
    to run classification and scheduling actions, we used a static-defined interval
    scheme to analyze our strategy. Therefore, we included an OBCD algorithm as a
    new feature in the proposed architecture to overcome this issue. • We create an
    optimized version of Ludwig et al. (2019) Simulated Annealing heuristic to tackle
    dynamic scheduling aspects. The original version, presented by Ludwig et al. (2019),
    was built upon static interference models, and to apply it in a dynamic scenario,
    we had to perform some modifications. • We developed an extension for the CloudSim
    toolkit to execute interference-aware scheduling, using real case provisioning
    requirements and constraints, making it available in a GitHub repository1 to allow
    reproducibility. • We conducted a set of real and simulated experiments to analyze
    and compare the proposed architecture efficiency. The remaining of this document
    is organized as follows: Section 2 discusses related work and background material.
    Section 3 introduces the proposed interference-aware scheduling architecture,
    its functionalities, and capabilities. Section 4 describes an evaluation performed
    to compare our solution with related studies and its results. Section 5 introduces
    related work in the literature. Finally, Section 6 depicts conclusions and future
    directions. 2. Background and state-of-the-art This section outlines the state-of-the-art
    concepts intrinsic to the work. Firstly, we present an overview of resource management
    and virtualization technologies. Secondly, we characterize interference and its
    impact on performance. Lastly, we introduce the dynamic workload application concept.
    2.1. Resource management and virtualization In data centers, orchestration systems
    need highly elastic and scalable infrastructures that allow the dynamic allocation
    of different resources (such as compute, storage, networking, software, or a service)
    in the right location and, in short, times, enable the deployment of applications
    (Tosatto et al., 2015). The elasticity in cloud environments is obtained by abstracting
    physical resources from an underlying layer through virtualization. There are
    different virtualization technologies, but the two most relevant in the cloud
    computing landscape are Hardware virtualization and System-level virtualization:
    • Hardware virtualization (Hypervisors) abstracts the underlying hardware layers
    to enable complete operating systems to run inside the hypervisor as if they were
    an application. Paravirtualization solutions (Xen2) and hardware virtualization
    solutions (KVM3), in combination with hardware-specific support, integrated into
    modern CPU (Intel VT-x and AMD-V), can achieve a low level of overhead due to
    the new layer added between the virtual instance and the hardware. • System-level
    virtualization (Containers) is based on fast and lightweight process virtualization
    and allows to tie up an entire application with its dependencies in a virtual
    container that can run on every Linux distribution. It provides its users an environment
    as close as possible to a standard Linux distribution. Since containers are more
    lightweight than VMs, the same host can achieve higher densities with containers
    than with VMs. This approach has radically decreased both the start-up time of
    instances and the processing and storage overhead, which are typical drawbacks
    of Hypervisor-based virtualization (Rosen, 2014). Containerization is the state-of-art
    virtualization of the cloud platform (Merkel, 2014). Containers only need seconds
    to bootstrap, initiate, versus minutes for a regular VM (Zhang et al., 2019) (seen
    in Table 1). Container technologies effectively virtualize the operating system
    and are becoming popular in cloud computing. By encapsulating runtime contexts
    of software components and services, containers improve portability and efficiency
    for cloud application deployment (Hu et al., 2020, Pahl et al., 2019). In addition,
    one container can be scaled out/in within a minute, and consequently can react
    immediately when encountering possible unforeseen crashes. Therefore, containers
    are capable of tolerating fluctuating stress and reducing overhead  (Scheepers,
    2014), the features that autoscaling coincidentally needs. Due to the characteristics
    presented above, we used container technology to implement the virtualization
    layer in this work. In the next paragraphs we present the main container solutions
    available in the literature, namely OpenVZ,4 Linux-VServer,5 LXC,6 and Docker.7
    Table 1. Comparison between container and virtual machine (Zhang et al., 2019).
    Performances Kinds of virtualization Empty Cell Container Virtual Machine Size
    Megabytes Hundreds Megabytes Start time Seconds Minutes Management overhead Low
    High Portability High Low OpenVZ is developed on top of kernel namespaces, allowing
    an isolated subset of resource to each container. It uses PID and IPC namespaces
    to reach isolation between processes from different contexts. OpenVZ also implements
    network namespaces. Moreover, it also provides different network operation modes,
    such as Route-based, Bridge-based and Physical-based. The main distinction between
    them lies at operation layer. While Route-based works in Layer 3 (network layer),
    Bridge-based works in Layer 2 (data link layer) and Physical-based in Layer 1
    (physical layer). In the Physical-based mode, it is possible to assign a real
    network device (such as eth0) to a container, improving the network performance
    (OpenVZ, 2022). Instead of using namespaces, Linux-VServer implements its own
    kernel mechanisms to provide process, network and CPU isolation. The system limits
    the scope of the file system from different processes through the traditional
    chroot system call and prohibits unwanted communications between them by using
    a technique called global PID space. Since it is impossible to re-instantiate
    processes with the same PID, Linux-VServer does not implement usual virtualization
    techniques, such as live migration, checkpoint and resume. Also, it does not virtualize
    the network layer, so that all network subsystems are shared among the containers
    and also with the host system (Xavier et al., 2014). Like OpenVZ, LXC uses kernel
    namespaces to guarantee isolation among container instances. LXC implements PID,
    IPC, File System and Network namespaces. Furthermore, it also offers different
    types of network configurations, namely: Route-based and Bridge-based. Resource
    management is only performed via cgroups. With cgroups it is possible to define
    network configurations, limiting the CPU usage and accomplishing isolation among
    processes from different containers contexts. By default, LXC adopts the CFQ scheduler
    to control I/O operations (Menage, 2022). Similar to LXC, Docker shares Linux
    cgroups, namespaces, and the Linux kernel. Although Docker was originally developed
    over LXC, over time Docker incorporates its own environment engine, called libcontainer.
    Different from LXC, where each container contains its own operating system, Docker
    provides an environment consisting of only one guest operating system, on which
    all processes run packed in containers, with each application having its own isolated
    environment (Docker Engine Overview, 2022). A Docker application container packages
    a single process or application, while a LXC system container simulates a full
    operating system and allows users to run multiple processes simultaneously. Docker
    provides separate components, while LXC delivers a full solution of libraries,
    applications, databases, and so on. In addition, it is possible to use LXC to
    create different user spaces and isolate all processes belonging to each userspace,
    which is not what docker is intended for. In addition, LXC performs live migration
    without modifications. Therefore, due to all cited advantages, LXC was the container
    implementation used in this work. 2.2. Performance interference With the resource
    sharing techniques evolution, each cluster node can host several applications.
    However, when multiple services intensively use a specific resource simultaneously,
    resource contention issues will occur. This problem is labeled as performance
    interference and may lead to severe performance degradation (Chen et al., 2015).
    Virtualization technologies and server consolidation are the main drivers of high
    resource utilization in modern data centers (Meyer et al., 2019b). The authors
    of Jersak and Ferreto (2016) state that applications are affected by virtual machines
    that use the same resource intensively in the same physical machine and each resource
    is affected differently. CPU intensive applications led to performance degradation
    of 14%. Memory and disk I/O intensive applications, the performance degradation
    was as high as 90%. Therefore, it is clear that performance interference is a
    problem, and the performance degradation varies depending on the most used resource.
    Not only hardware virtualization is affected by performance interference, but
    container-based environments are as well. Disk-intensive applications running
    over containers promote performance degradation that uses different resources
    intensively. The authors of Xavier (2019) have tested a bunch of co-hosted workload
    combinations. While some of these combinations led to performance degradation
    up to 38%, there are those which cause no interference indexes. In Shah et al.
    (2013), the authors claim that mapping performance data related to shared resources
    onto time slices can establish the simultaneity of application usage across jobs,
    which can be indicative of inter-application interference. In some cases, inter-application
    interference causes performance degradation by up to 50%. Both works (Xavier,
    2019, Shah et al., 2013) focus on analyze the interference from co-hosted applications
    in cluster environments. While (Xavier, 2019) presents a novel interference instrumentation
    tool, Shah et al. (2013) introduces an approach to correlate the performance behavior
    of applications running side by side. In this work, to accomplish our goal, we
    use both strategies (interference instrumentation and performance analysis), and
    on top of that, we include algorithms to efficiently schedule applications across
    cluster nodes. 2.3. Dynamic workload applications In data centers, applications
    may present a variety of workload patterns, and QoS demands. Non-interactive batches
    are an example that requires completion time, while transactional web services
    are concerned with throughput guarantees. Different application workloads require
    a diverse type and amount of resources. For instance, batch jobs tend to be relatively
    stable, unlike latency-sensitive, which tends to be highly unpredictable and bursty
    in nature (Garg et al., 2014). Besides, latency-sensitive applications can include
    short latency-critical user-facing tasks, responding to web requests, for example.
    Also, this workloads’ type can be characterized by short deadlines in the order
    of tens of milliseconds (Chen et al., 2017). Multi-tenancy services need to efficiently
    manage resources within and among data centers taking time-varying demands into
    account (Iqbal et al., 2018). Their workload is not deferrable, and this means
    that every time a request is received, the response should be generated immediately
    afterward. Consequently, such applications must perform real-time scheduling of
    the load, ensuring the quality of requests flow (Toosi et al., 2017). This kind
    of application presents an unpredictable intensity variation of resource utilization
    at run time due to the user’s different request patterns and periodicity (Iqbal
    et al., 2018). Therefore, latency-sensitive applications and multi-tenant services
    are ideal candidates for evaluating interference effects suffered by dynamic workloads
    and will be considered target applications in this work. Garg et al. (2014) creates
    a scheduling mechanism to guarantee the meeting of users’ QoS requirements, according
    to SLAs specifications. They state that it is important to be aware of different
    types of SLAs and the mix of workloads for better resource provisioning. Results
    present an improvement in reducing SLA violations. Sampaio et al. (2015) address
    the resource allocation issues running different application workloads types (CPU-,
    and network-intensive ones). After performing experiments with synthetic workloads,
    results indicate that the authors’ strategy can fulfill contracted SLAs of real-world
    scenarios while reducing energy expenses. In Ebadifard and Babamir (2021), authors
    developed an autonomous load balancing method to alleviate the communication overhead
    among servers. Based on the resources, requests are divided into CPU-bound and
    I/O-bound. Results using dynamic workloads indicate that this proposed algorithm
    can distribute the workload among them equally and allocate requests to appropriate
    VMs based on the required resources, decreasing the communication overhead. In
    Daraje and Shaikh (2021), authors developed a hybrid approach combining vertical
    and horizontal scaling to increase resource utilization and better adapt to user
    requests. The results demonstrate that the proposed approach is more efficient
    in comparison with the existing ones. Works (Garg et al., 2014, Sampaio et al.,
    2015, Ebadifard and Babamir, 2021, Daraje and Shaikh, 2021) propose scheduling
    strategies to improve the use of computational resources. Although these works
    have the same goal as we have, we apply an interference-aware scheduling, not
    just considering resource capacities in our solution. Besides, we developed our
    solution on top of a container-based cluster, while they explored traditional
    VMs. Furthermore, rather than just evaluating our work through simulation, as
    they did, we also performed experiments in a physical environment to validate
    the simulation and show the scalability of our solution. 2.4. Scheduling approaches
    To evaluate the efficacy of the proposed architecture, we compared our solution
    to three schedulers from the literature: EVEN, CIAPA, and Segmented: • EVEN implements
    the EvenScheduler algorithm, which is the Apache Storm8 default scheduler. This
    algorithm distributes computation tasks across nodes in a Round-Robin manner (Al-Sinayyid
    and Zhu, 2020). When tasks are scheduled, this approach counts all available slots
    on each node and places application instances to be scheduled one at a time to
    each node while keeping the order of nodes constant. Although not interference-aware,
    we have decided to use this method as a baseline because Apache Storm is a well-known
    framework that processes real-time data, like cloud multi-tenant systems, which
    are the target applications in this work. In this case, applications are placed
    into the cluster nodes in a round-robin fashion, meaning that they are not moved
    during the experiment execution. • CIAPA (Ludwig et al., 2019) evaluates the profile
    of the application workloads and uses a static interference classification with
    three levels. Its classification is static, done only one-time in the beginning
    of the execution using an average of an application generated interference over
    the entire execution. The definition of interference levels is done used fixed
    thresholds that are empirically defined. In a first phase, applications are placed
    in a round-robin manner, and after a 10-min interval, the collected data is analyzed
    and only one scheduling movement is done, not changing the placement of the applications
    after that. • Segmented (Meyer et al., 2021b) applies a pseudo-dynamic interference
    classification with levels, similar to CIAPA. The difference is that the Segmented
    scheduler arbitrarily divides the applications’ executions into four parts (proportional),
    and based on that division, classifies the generated interference of an application
    per segment, considering some level of change during execution. This allows the
    first placement to be changed three times during execution. The goal of this approach
    is to classify applications’ interference considering the workload variability,
    not only using a simple average over the entire execution. 3. Dynamic interference-aware
    scheduling architecture Performance interference is known to adversely impact
    QoS properties of applications and dynamic service demands with workload profiles
    further raise the challenges for cloud service providers in managing resources
    on-demand to satisfy SLAs while minimizing operational costs (Nathuji et al.,
    2010). Therefore, any solution that addresses these challenges requires an approach
    that should account for the workload variability and the performance interference
    (Shekhar et al., 2018). Due to the dynamic nature of the process, some questions
    come up, such as: How to classify applications in real-time based on the interference
    they generate? When to execute the classification? When to schedule them and how
    to tradeoff migration costs? Finding a solution that comprehensively covers all
    the mentioned issues is not a straightforward task. Recently proposed approaches
    present significant improvements regarding interference classification and dynamic
    scheduling strategies. However, there are still gaps in the state-of-the-art.
    One example, is the lack of a complete scheduling architecture that automatically
    handles dynamic workloads, finding the best intervals to classify and schedule
    applications among cluster nodes. Besides, this architecture should address performance
    interference aspects without earlier workload know-how and with no user mediation.
    Standing for the concept that dynamic interference-based scheduling algorithms,
    which analyzes workload variations over time, could improve even more resource
    utilization, and consequently reduce SLA violations, in this section, we propose
    IADA, a full-fledged dynamic interference-aware cloud scheduling architecture
    for latency-sensitive workloads. This architecture aims to efficiently schedule
    applications based on the interference they generate, without user intervention
    and with no previous workload knowledge. The main goal is to analyze hardware
    events and supported by classification, time interval detection, and scheduling
    algorithms, to find the best applications’ placement set at runtime. In the next
    subsections, we introduce the proposed architecture, describing its capabilities
    and functionalities in detail. 3.1. Proposed architecture Usually, interference-aware
    task schedulers are performed by combining three main steps (Chiang and Huang,
    2011, Zhu and Tung, 2012, Bu et al., 2013, Zhang et al., 2014, Xavier, 2019, Wang
    et al., 2019): (i) profiling queued tasks based on their resource needs; (ii)
    predicting the performance interference; and (iii) scheduling the task on the
    best-suited node, which is the node that causes the lowest performance interference
    effects. Since we are interested in scheduling real-time applications based on
    the workload variability, we decide to use a reactive approach. For this reason,
    we adjusted the prediction step by splitting it into two more ones: (ii-A) classifying
    interference and (ii-B) analyzing the best time intervals from applications at
    runtime in order to perform the scheduling (next) step. Therefore, to build a
    dynamic interference-aware scheduling architecture, we used four main components,
    presented as follows: (i) a profiler that reads hardware metrics; (ii) a technique
    that gets significant workload changes, based on profiling data at runtime; (iii)
    an interference classification method supported by a combination of machine learning
    techniques; and (iv) a scheduling algorithm that interprets all data generated
    by the previous components and makes efficient placement decisions. The choice
    of using a reactive technique is normally adopted before applying a proactive
    one, however as a matter of fact we intend to investigate and compare proactive
    approaches as well in the future. To perform the proposed architecture, all these
    aforementioned components were assigned in a node that works over the entire computational
    environment analyzing and executing scheduling decisions, referred here as Node
    Manager. Also, an interference profiler module is executed inside each cluster
    node, profiling all applications and sending all data to the Node Manager. First,
    these metrics are received and analyzed by the Data Analyzer component, which
    is responsible for examining and finding abrupt changes in the application’s behavior.
    After, these metrics are also sent to the Classifier component that has the duty
    of classifying each application in a given period, defined by the previous component,
    into interference levels. Then, the Scheduler module performs hardware orchestration
    decisions by running an algorithm based on generated data from the two previous
    components. Fig. 1 presents an overview of the proposed architecture, distinguishing
    each layer. The Node Manager is continually monitoring and analyzing potentially
    interference information from the cluster infrastructure. It is worth noting that
    the Profiler module is always monitoring the entire infrastructure while feeding
    the Data Analyzer and Classifier components. While both mentioned modules analyze
    and classify the received data, when they find there is room to make scheduling
    decisions, they send that information to the Scheduler module to apply it over
    the cluster infrastructures. Fig. 2 depicts the architecture data flow, where
    it is possible to observe how collected metrics are processed through each component.
    Download : Download high-res image (348KB) Download : Download full-size image
    Fig. 1. System architecture. This cycle will always run while there exists more
    than one application running in the cluster. To clarify, let us take an example:
    suppose that application1 starts on Node1, together with application2. In a given
    moment, application1 and application2 become to contend for CPU (or another resource,
    i.e.), and the Node Manager perceives it and decides to migrate the container
    which runs application2 to Node2. This scheduling action aims to use the resources
    more efficiently, improving QoS and consequently reducing SLA violations. In the
    next subsections, each component is presented in detail. Download : Download high-res
    image (80KB) Download : Download full-size image Fig. 2. Architecture data flow.
    3.1.1. Interference profiler Profiling runtime applications is not a straightforward
    task, given that different tasks may burst arbitrary resources, causing variation
    in resource consumption. In addition, an intrusive profiler can induce the performance
    of applications and compromise the reliability thereof. The literature presents
    works that care about resource contention aspects among applications in a simple
    way, existing or not (Ludwig et al., 2019). Also, a number of application profiling
    mechanisms, ranging from kernel-based (Linux Trace Toolkit Project Page, 2002)
    to runtime (Urgaonkar et al., 2003) profiling that uses especially linked libraries,
    have been proposed in the past. Recently, a tool called IntP (Xavier, 2019) has
    been developed, an open-source system-level monitoring tool which analyzes selected
    architectural counters and operating systems data structures to estimate the stress
    an application puts on each hardware’s subsystem and consequently infer the potential
    interference it could generate in other applications hosted in the same physical
    machine. Different from tools that apply a more high-level approach using micro
    benchmarks and application metrics, IntPs low level instrumentation enables a
    more accurate prediction of the performance degradation that results from contention
    on shared resources, with less monitoring overhead. IntP is subdivided into modules
    responsible for each access method on specific resources at the infrastructure
    level and outcomes the percentage of their utilization relative to the total system
    capacity, for each running application. This isolated measurement provides analytical
    information to infer how much an application could potentially interfere with
    other applications consolidated in the same physical machine, so that conflicts
    can be avoided by the scheduler. More specifically, the tool provides the following
    metrics: • netp - physical network; • nets - network queue; • blk - disk; • mbw
    - memory bandwidth; • llcmr - last-level cache miss rate; • llcocc - last-level
    cache occupation; • cpu - CPU utilization; IntP is used in this work to profile
    applications at runtime (every second), so it is possible to perform an analysis
    on how interference is potentially hurting consolidated applications over time,
    and trigger a new scheduling operation if needed. 3.1.2. Time-series analysis
    We aim to evaluate the influence of application interference over time, but dealing
    with dynamic workloads at runtime is a challenging task. Whether we wish to perform
    scheduling decisions in an online trend, time is an important factor that must
    now be considered in our model. For example, to perform dynamic scheduling actions
    on the fly, it is necessary to define at what moments our architecture will execute
    them. As already mentioned, in previous work (Meyer et al., 2021b), we moved a
    step forward and created a static-defined time interval scheme to start analyzing
    segmented scheduling, and preliminary results presented a considerable improvement
    in hardware efficiency. Since we are interested in accomplishing automatic scheduling
    decisions based on interference levels generated across applications, we need
    to carry out a statistical time-series analysis that deals with the profiled data
    and points trend patterns out. However, some questions come into play when working
    with time series, such as: Is this data stationary? Is there seasonality? To work
    around these questions, we performed an online change point analysis (Pagotto,
    2019) aiming at determining the time points with the most significant behavior
    changes, considered crucial for analyzing and classifying the profiled data, and
    subsequently, performing scheduling actions. Change points are abrupt variations
    in the generative parameters of a data sequence. Online detection of change points
    is useful in modeling and prediction of time series in application areas such
    as finance, biometrics, and robotics (Pagotto, 2019). A time series consists of
    multiple assessments of a specific outcome measure, at group level, at regularly
    spaced time intervals. The “interruption” or “change point” of the time series
    is an identifiable real-world event. Since IntP profiles each application in an
    isolated manner and outcomes multiple metrics (different resources) from each
    one, we first had to reduce its dimensionality. To do that, we apply the Principal
    Component Analysis (PCA) (R Core Team, 2019) over each application. PCA is a dimensionality-reduction
    method that is often used to reduce the dimensionality of datasets, by transforming
    a set of variables into a smaller one that still contains most of the valuable
    information in the large dataset. In our case, we decided to reduce seven metrics
    profiled from IntP to only one for each application. Depending on the order the
    algorithm sort those metrics, the PCA outcome changes. So, to find out what is
    the best order to arrange interference metrics, we have performed several tests
    and decided to place those metrics in an order that follows its performance degradation
    priority. In previous work (Meyer et al., 2021b), we introduced such priority
    order, presenting that when there is resource contention incidence, some hardware
    components present more elevated performance degradation indexes than others,
    so that we decided to apply PCA with the following resource order: disk, memory,
    cpu, cache, and network. This means that performance degradation caused by disk
    resource contention is bigger than caused by network, for example. If there will
    be no disk usage, PCA takes the next resource in the queue order, memory in this
    case. If there will be no memory utilization, the next resource will be considered
    as the principal, and so on. After reducing the profiled data from each application
    to a single dimension, observing its performance degradation priority, we apply
    the Online Change Point Detection (OCPD) function, from R Package (Pagotto, 2019),
    overall applications’ metrics. Such a technique provides an implementation of
    Bayesian online change point detection that handles multivariate data, computing
    the set of change points with the highest probability in an online way (updating
    the results with each incoming point). This method outputs a list of change points
    over time (x-axis) during running the model, in an online fashion. The entire
    process of reducing and analyzing profiled data is depicted in Fig. 3. To present
    a simple use case example, we run an experiment adopting Node-Tiers.9 This tool
    is a multi-tier benchmark that allows fine-grained personalization of resource
    utilization. Node-Tiers stresses the computer system in various selectable ways
    and was designed to exercise various physical subsystems of a computer through
    web requests. This tool explores the web applications concept (client–server)
    and allows the creation of workload variations. First, we choose two memory-intensive
    applications from the Node-Tiers suite, then we created a synthetic workload for
    each one. On purpose, each workload produced an interval with a high-load request
    rate: (A) between 60 and 120 s; and (B) between 180 e 240 s, accordingly Table
    2. Download : Download high-res image (140KB) Download : Download full-size image
    Fig. 3. Data scheme of data profiling (IntP), dimensionality reduction (PCA),
    and discovering change points over time (OCPD). Both applications (A and B) were
    executed together while profiled with IntP, and the results are presented at the
    top of Fig. 4. Table 2. A and B applications’ workloads behavior. Intervals (s)
    A (req/s) B (req/s) 0–60 100 100 61–120 200 100 121–180 100 100 181–240 100 200
    241–300 100 100 The collected metrics passed through the PCA phase and then produced
    A and B resulting data, depicted at the bottom of the same Figure. Finally, this
    data was submitted to the OCPD function, returning the moments where both applications
    presented abrupt behavior changes (60 s, 120 s, 180 s, and 240 s), seen in the
    same image. It is possible to observe that OCPD handles multiple applications
    due to its multivariate characteristics, being a good candidate function for our
    architecture. Download : Download high-res image (347KB) Download : Download full-size
    image Fig. 4. Profiled data (IntP) from A and B execution (top); PCA resulting
    data along with found OCPD change points (bottom). 3.1.3. Interference classification
    A number of techniques have been proposed regarding interference classification,
    such as: collaborative filtering (Delimitrou and Kozyrakis, 2013), decision-tree
    (Moreno et al., 2013, Javadi and Gandhi, 2017), major interference source (Kumar
    and Setia, 2017, Devarajan et al., 2018) and resources historic mean (Caglar et
    al., 2014, Caglar et al., 2016). The authors from Ludwig et al. (2019), the most
    closely related to our study, developed a scheduling model that considers interference
    levels among applications to increase resource usage. Even though the authors’
    approach increases the state-of-the-art in the scheduling resource field, the
    proposed classification was developed with fixed thresholds, empirically defined.
    Aiming at finding out alternatives to minimize interference overhead effects over
    scheduling decisions, in previous work (Meyer et al., 2020, Meyer et al., 2021b),
    we have proposed a classifier that quantifies cross-application interference in
    levels over time, standing for the concept that an interference classifier method
    that better represents the workload variability improves hardware utilization.
    The main purpose of our classification method is to return the hardware resources’
    interference produced by applications, within a time slice, to a given degree.
    This is achieved by exploiting the combination of two different machine learning
    algorithms: (i) SVM for classification and (ii) K-Means for clustering. Initially,
    SVM receives interference data from applications, collected each second by IntP,
    and those metrics are classified and stored into resource queues for their respective
    classes: memory, CPU, disk, network, and cache. Subsequently, K-Means quantifies
    values for each queue and returns their interference level for a specific period.
    More specifically, we adopted four interference levels: (i) absent, when there
    is no interference incidence; (ii) low; (ii) moderate; and (iv) high. Both machine
    learning algorithms use a training dataset, previously defined, to assist their
    decisions. More details about the classification method are presented in Meyer
    et al. (2021a). Download : Download high-res image (363KB) Download : Download
    full-size image Fig. 5. Segmented TPC-H static interference classification. To
    facilitate the visualization, a Loess function was applied to smooth short-term
    variations in each resource. Resources labels that changed are shown in bold in
    the bottom plot. IntP metrics that do not suffer any interference were not depicted.
    The proposed ML-based interference classifier dynamically defines thresholds and
    assigns interference levels for each resource used by the monitored applications
    for a particular time slice, without the need for user intervention. This classification
    process is repeated until the end of the execution, characterizing the dynamicity
    of our approach, where interference levels are reevaluated regularly, accordingly
    to OCPD function (seen in Section 3.1.2), so that we are able to better react
    to significant changes in the workload. To present an example, we use a decision
    support benchmark called TPC-H.10 This benchmark evaluates the performance of
    various decision support systems by the execution of sets of queries against a
    standard database under controlled conditions. Also, we create an increased workload,
    starting with a low load and gradually going to a high load. This workload execution
    was profiled with IntP, arbitrarily divided into four segments, and each one was
    classified by our approach. The classification result is depicted in Fig. 5. It
    is possible to notice that there are resources that do not change their labels,
    for instance, memory, disk, and network. Since they keep their interference metrics
    at the same level, on average, with no expressive variation, their labels are
    maintained. On the other hand, also some resources do change their labels, which
    are the CPU and Cache cases. The CPU has a smooth increase in its behavior, moving
    from moderate to high label. In addition, Cache keeps its labels with the highest
    levels while executing, changing from high to moderate, and from moderate to high
    interference levels again. This highlights that, due to the dynamic workload nature,
    the application execution present different interference labels during its execution.
    3.1.4. Scheduling algorithm Scheduling consists of ordering running jobs across
    available computational resources (Hu et al., 2020, Thamsen et al., 2020). To
    do this with interference awareness, first, the Node Manager pulls the tasks into
    available node slots. After, it profiles the interference from each application
    and, based on the information generated on previous components, suits them on
    the best candidate nodes that minimize the overall performance interference. IADA
    is an architecture that relies mainly on a reactive approach so that the applications
    are constantly profiled and when the OCPD technique finds significant workload
    variations, the most recent interval data is used to perform scheduling decisions.
    The applications start at time zero and they are monitored continuously every
    second. The time-series analysis evaluates the data and returns , which is the
    point found by the OCPD function with the greatest representativeness in the workload
    variation of the applications (according to Section 3.1.2). When is found, the
    classification module generates an interference label for each application resource
    running in each container. The interval between and is defined as . When a is
    found, the scheduling is performed based on the most recent data, which means,
    the last outcome. The traditional view for real-time scheduling problems focuses
    on how to find a feasible schedule for an application set. However, the scheduling
    of a given application set is not a straightforward task. With the rapid increase
    in the use of powerful cloud systems, an efficient task scheduling policy, which
    deals with the assignment of tasks to resources, is required to reduce performance
    degradation. Task scheduling is an established NP-Hard optimization problem that
    can be effectively tackled with meta-heuristic algorithms (Chhabra et al., 2020).
    Taking this statement into account, we decided to use a heuristic algorithm to
    solve our problem. Ludwig et al. (2019) tested many heuristics to schedule applications
    with interference awareness and concluded that Simulated Annealing (SA) presented
    the best overall results. So, in this work, we decided to apply a modified SA
    algorithm that addresses interference-aware aspects. The SA algorithm is an optimization
    method that mimics the slow cooling of metals, which is characterized by a progressive
    reduction in the atomic movements that reduce the density of lattice defects until
    a lowest-energy state is reached (Kirkpatrick et al., 1983). Similarly, the simulated
    annealing algorithm generates a new potential solution to the problem by altering
    the current state, according to a predefined criterion. The new state solution
    is then based on the satisfaction criterion and may be accepted even if they do
    not lead to an improvement in the objective function. Since our architecture moves
    applications among cluster nodes at runtime, we developed an algorithm based on
    SA to find the best applications’ arrangement set in order to minimize performance
    degradation. The algorithm 1 presents how our architecture scheduling policy works.
    Download : Download high-res image (171KB) Download : Download full-size image
    Initially, the algorithm creates an application set , in which each container
    receives one application instance to execute. All containers are distributed among
    cluster nodes by a function that receives a set of physical machines and a set
    of applications to be executed. Every SA iteration generates one new solution
    that is compared to the best solution at that point. This new solution is generated
    by the Random Swap Function, presented in Algorithm 2. Download : Download high-res
    image (115KB) Download : Download full-size image This function relies on a randomized
    approach, in which the function has a 50% chance of swapping random applications
    in the cluster and a 50% chance of swapping the application of the cluster node
    with the highest score to the cluster node with the lowest score. After finding
    the new solution, if presents an interference degradation index lower than the
    current one, the algorithm replaces the current (best) solution with the new one.
    To compare both solutions, we create a function InterferenceScore() that analyzes
    the interference levels in and returns a total interference score, which is calculated
    by using the function seen in Eq. (1). (1) The total interference score is the
    result of the sum of all interference scores from each cluster node, where represents
    the hosts’ number in the environment. Each cluster node has its own interference
    score as well, this score is calculated with a function demonstrated by Eq. (2).
    (2) where, denotes the applications’ number running in each cluster node, ranging
    from to (total number of applications). If there exists less than 2 applications
    running in a cluster node, it will not generate interference incidence in that
    specific node and consequently will return a zero-score, since only one or no
    one application does not cause interference. Finally, the application interference
    score is calculated by Eq. (3). (3) All resource interference metrics (cpu, memory,
    disk, network, and cache) were measured and allocated into an level . Depending
    on the level they are set, the interference overhead index value varies, according
    to Fig. 6. To find these Interference Degradation Indexes (IDI), first, we ran
    applications with each resource-intensive (e.g. CPU-intensive, memory-intensive,
    and so on) in isolation and took the average response time. After, we ran each
    one again co-hosted with one more application instance at a specific level (low,
    moderate, and high), according to the classifier method, and found the average
    response time from both. With those metrics we discovered how much each resource
    degraded at each interference level by using Eq. (4). (4) Download : Download
    high-res image (124KB) Download : Download full-size image Fig. 6. Interference
    degradation index by resource. To illustrate this scenario for memory, let us
    take an example: We executed a memory intensive application in isolation, which
    resulted in 23.2 ms. While co-hosting with a Low-intensive memory application,
    the runtime increased to 24.4 ms, which gives IDI of 1.10. When co-hosting with
    a Moderate-intensive application, the response time increases to 39.2 ms, resulting
    in an IDI of 1.69. Finally, when co-hosting with a High-intensive memory application,
    the response time increased to 41.5 ms, resulting in an IDI of 1.79. Another important
    aspect that is analyzed in the SA algorithm, is the number of migrations done
    with the newly generated solution. If the number of migrations performed in the
    new solution is bigger than the best solution, this new solution is disregarded
    and another one is considered. The migrations number is taken with the help of
    the function, as seen in algorithm 1. 4. Evaluation and results In this section,
    we describe how the experiments were conducted, the scope, and the limits of the
    project. Also, the details about workload, application, and the computational
    environment adopted in this work are discussed. 4.1. Application and workload
    To investigate applications that present dynamic workload (unpredictable load
    variation) by stressing different hardware resources, Node-Tiers (seen in Section
    3.1.2) has been adopted. This tool explores the latency-sensitive application’s
    concept (client–server) and allows the creation of workload variations. The goal
    is to stress hardware resources in many ways (distinct resources) through many
    latency-sensitive applications from this suite benchmark, increasing and decreasing
    the request arrival rate, executing scheduling decisions at runtime, handling
    changes in the workload. Node-Tiers tool also provides an intensive-data pressure
    to the target server/cluster. Its disk and memory applications stress hardware
    resources likewise real intensive-data workloads do. To create a most realistic
    scenario, we evaluated our architecture using three real-world workload traces.
    The first one is from the Wikimedia project, found in Wikipedia11 traces. Specifically,
    we collected the page view statistics for the main page in the English language
    for the month of January 2021. The second one is from Alibaba Open Cluster Trace,12
    this one is sampled from one of Alibaba production clusters. There are both online
    services and batch workloads, and we collected only information from Sigma, the
    online service scheduler. The last one is from NASA13 dataset, consisting of all
    web requests made to the 1998 World Cup Web site between April 30, 1998, and July
    26, 1998. Although this particular workload is not considered as a newer one,
    it remains being adopted by recent studies (Mallikharjuna Rao and Rama Satish,
    2022, Radhika and Sudha Sadasivam, 2021, Chhetri et al., 2021). In addition to
    these workloads being widely used in related work in the resource management field,
    they were chosen also because they are not drawn from synthetic functions or independent
    distributions, but rather represent real-world traces that exhibit the realistic
    patterns of a workload resulting from user-, program- and operating system behaviors.
    In addition, they are lengthy traces obtained over extended periods of time, allowing
    us to evaluate if the proposed architecture can cope with dynamic real time applications,
    while improving resource utilization. 4.2. Experiments scenarios To explore the
    efficiency of our architecture, we dived all experiments into two phases: first,
    we use a real-scenario with a small number of machines to ensure all proposed
    steps work correctly together and to guarantee the simulation phase outcomes are
    in accordance to the reality, reflecting reliable results; and second, based on
    the previous phase, we build a simulated environment, to test our architecture
    with a bigger number of cluster nodes, and consequently, more applications. In
    the next sections we describe how each phase was performed and its results. 4.2.1.
    Real experiments To run our experiments within a real testbed, we used a cluster
    called Pantanal that belongs to the LAD Laboratory14 from PUCRS. This cluster
    has Dell PowerEdge R740xd nodes, each one equipped with: 2x Intel Xeon Gold 5118
    Processor, 300 GB DDR4 RAM Memory, 1TB Hard Drive, and 4x Gigabit Ethernet Interface.
    Also, as the Node Manager, we used one Dell Optiplex 990 outside of the cluster,
    equipped with 8 GB of RAM, and one Core i5 processor. To stress different resources
    subsystems (CPU, memory, disk, network, and cache), we used different applications
    from the Node-Tiers suite. The server-side was performed over the cluster, while
    the client-side was configured on a single computer, using Artillery15, a load
    stress testing tool. The server-side applications were executed inside containers,
    more specifically one container per application. Since containers present many
    benefits concerning traditional virtual machines (seen in Section 2.1), we decide
    to adopt LXC/LXD containers as target virtualization technology, allowing us to
    schedule the applications with live migration facilities across the cluster nodes
    with Checkpoint/Restore In Userspace (CRIU16) functionalities. All pieces of equipment
    were connected through a Gigabit Ethernet Network. We ran four applications inside
    each cluster node, and each one was submitted to a period of 2-h workload trace
    (randomly chosen), mixing the elected datasets and creating greater variation
    among application workloads. To evaluate the proposed architecture, we compared
    our work to the references presented in the related work section that apply similar
    scheduling strategies and target the same type of environments and applications
    so that a direct comparison to our results is possible, namely CIAPA (Ludwig et
    al., 2019) and Segmented (Meyer et al., 2021b). We also included EVEN – not cited
    in related work section because it does not consider interference aspects – that
    uses a round-robin scheduling strategy, as a baseline. This strategy is widely
    applied nowadays by schedulers such as Apache Storm, for example. For this experiment
    phase, we used four nodes of the Pantanal cluster, each one executing four Node-tiers
    applications, totaling 16 applications. When using latency-sensitive applications,
    the response time (latency) metric quantifies how long the user must wait for
    a response to a query, regardless of the quality of the response. Together with
    data quality metrics, latency metrics provide the best indication of the end-user
    experience under normal conditions and during outages (Broadwell, 2004). For this
    reason, we decided to use the Average Response Time as the main performance metric
    in this work, which represents the total latency for the test divided by the number
    of requests submitted to the server-side by the users-side. The response time
    was collected from each application during the entire experiment with Artillery,
    and their total average is presented in Fig. 7. It is worth noting that in all
    experiments, our proposed architecture presented the best results, improving in
    average the response time by 26% when compared to CIAPA, EVEN, and Segmented scheduling
    approaches. Also, it is interesting to note that EVEN scheduler reaches the higher
    response time indexes (worst results), as predicted since this scheduling strategy
    is not interference-optimized. Download : Download high-res image (70KB) Download
    : Download full-size image Fig. 7. Average Response Time from real experiments
    phase. By running 16 applications with a mix of workloads variations, IADA detected
    12-time points with an expressive (global) behavior change. Consequently, 12 periods
    were classified and each one provoked scheduling actions. As mentioned before,
    the Data Analyzer component uses a bayesian change point detection to find workload
    behavior modification, but this does not imply that all applications, in all analyzed
    intervals, had their interference labels modified, exchanging their interference
    degree (levels). The applications only have their labels modified if the workload
    variation has an abrupt alteration. To give an example of how much the interference
    in a node is affected by scheduling actions, we collected the average interference
    generated in Node1 and Node2 within a given period, every second, while a scheduling
    rearrangement was performed, and presented in Fig. 8. This image illustrates the
    average of interference generated by the 8 applications running in Node1 and Node2
    before and after a scheduling movement. The red dashed line demonstrates the exact
    moment the scheduling was executed. Download : Download high-res image (576KB)
    Download : Download full-size image Fig. 8. Average interference indexes in Node1
    and Node2 while performing a scheduling action. Applications which have migrated
    across nodes are shown in bold. *This image presents only data from 2 of 4 nodes
    used in the real experiment. By looking at the interference measures, it is possible
    to perceive two interesting facts: (i) after the scheduling, Node1 and Node2 exchanged
    app2 (disk intensive) and app6 (cpu intensive) applications (in bold at the image);
    and (ii) after this rearrangement, Node1 had its disk interference ratios considerably
    reduced while Node2 had its disk ratios increased. Also, Node2 had its cpu interference
    indexes reduced while Node1 had its overall cpu usage increased. In general, it
    is possible to observe that this scheduling operation provided a balance across
    interference indexes, improving the resources’ usage and reducing applications’
    response time. Therefore, these results show that a technique that analyzes frequently
    the generated interference over time is able to reduce the overall system’s overhead,
    using the infrastructure more efficiently, and consequently, improving QoS requirements.
    Also, this experiments show that the proposed architecture presents interesting
    and trustworthy outcomes, and they will be used to calibrate and perform the simulation
    experiments, presented in the next section. 4.3. Simulated experiments In order
    to carry out experiments closer to a real scenario, a large physical machine set
    is necessary, and to have more flexibility to perform different host arrangements,
    we decide to scale our approach out through simulation as well. First, we search
    in the literature for tools that simulate cloud infrastructures (Lim et al., 2009,
    Kliazovich et al., 2012, nez et al., 2012, Calheiros et al., 2011). After exploring
    each simulation tool, we conclude no one of them offers an environment that handles
    interference aspects from applications. Then, we have confirmed that CloudSim
    (Calheiros et al., 2011) is the most widely spread cloud simulator and by far
    also the most sophisticated. It is developed as an add-on-top of the grid network
    simulator GridSim (Casanova, 2001). CloudSim is a completely customizable tool
    that supports modeling, creation of one or more VMs, and mapping tasks to appropriate
    virtual machines. This gives CloudSim the ability to handle a complex simulation
    environment. It mainly targets application developers or testers as it gives the
    ability to configure several variables such as the number of users, data centers,
    and cloud resources along with the location of both users and data centers. Besides
    many other studies extending CloudSim, such as Beloglazov and Buyya, 2012, Guérout
    et al., 2013, Xavier et al., 2017 and Krzywda et al. (2020), there is one in particular
    that supports Container as a Service (CaaS), namely ContainerCloudSim (Piraghaj
    et al., 2017). This extension provides a platform for modeling and simulating
    containerized cloud computing environments. Therefore, it is the most fitting
    simulation tool to use nowadays and the one we have chosen to extend with applications
    interference features. We have developed the CloudSimInterference plug-in, a trace-driven
    extension to the CloudSim simulation tool. First, each application was monitored
    in a physical machine (previously), and all IntP metrics were kept and used as
    input to our workload simulations. This means that we did not use the workload
    trace itself, but rather the resulting resource utilization levels from a real
    scenario. Since we measured all interference levels from all resources (seen in
    Section 3.1.4), we used those metrics to perform scheduling actions instead of
    generating them within the simulation tool. Because bandwidth sharing is not considered
    by default in CloudSim, we introduced a migration degradation overhead to overcome
    this limitation. This overhead was measured in isolated physical machines, considering
    several migration scenarios, with many different workloads. Each time the simulator
    performs a migration, this overhead is added to the total interference cost (TotalIntScore),
    being considered in the simulated experiments. To implement our simulation plugin,
    we have made many classes modifications in ContainerCloudSim. First, we extend
    the containerCloudlet() class to InterferenceContainerCloudlet(). This class is
    responsible for representing the application behavior, and we include all Interference
    metrics measured with IntP inside them, each one keeps the information from each
    application trace generated from real execution. Another major modification, was
    the integration with the R algorithms to perform the OCPD and ML functions, presented
    in the Sections 3.1.2 Time-series analysis, 3.1.3 Interference classification.
    To perform this integration, we include the JRI (Java-R-Integration) library on
    the java side and the rJava library on the R side. Also, we extended containerDatacenter()
    class to InterferenceContainerDatacenter(), including several functions to handle
    the modifications done with interference metrics utilization. To generate a considerable
    number of applications (InterferenceContainerCloudlets) for the simulation experiments,
    we have executed several hours of each workload trace (seen in Section 4.1) with
    five applications instances from Node-Tiers suite, stressing the main hardware
    resources (cpu, memory, disk, net, and cache). After, we randomly divided those
    execution traces collected with IntP into two-hour segments to use as input data
    in our simulation experiments. To run the simulated experiments, we use four different
    arrangements of cluster node numbers and application instances, presented in Table
    3. As mentioned before, we used real experiments to calibrate our simulator. To
    produce more reliable results, compatible with real case scenarios, we applied
    the same number of applications in each cluster node, as done in practical experiments
    phase (four application instances per node). All results shown in this section
    display the average over 10 simulation trials with 95% confidence interval level.
    Fig. 9 presents the results from the simulated experiment’s phase. Table 3. Hosts/application
    arrangements used in simulated experiments. Hosts Applications 6 24 12 48 24 96
    48 192 It is noteworthy that, in all experiments, IADA achieves the best results
    (lower indexes). When compared to EVEN scheduler, our proposed architecture reached
    a reduction of 37% on average in the response time. Not surprisingly, EVEN reaches
    the worst results as well, similar to the real experiments, because this scheduler
    was not developed based on interference-awareness. Compared to the CIAPA approach,
    IADA reduced the average response time in 21% and, in contrast to Segmented, the
    state-of-the-art strategy, IADA obtained a reduction of 14% in the average response
    time. Download : Download high-res image (128KB) Download : Download full-size
    image Fig. 9. Average Response Time from simulated experiments phase in each host
    arrangement (6, 12, 24, and 48). To analyze how much the response time varies
    over time, we took the average response time in each scheduled interval, during
    the experiments running with 24 nodes (96 applications), and compare them with
    EVEN, CIAPA, and Segmented schedulers’ results. These results are presented in
    Fig. 10. It is interesting to observe that EVEN scheduler places the applications
    at the beginning of the execution and after that, they are not rearranged anymore.
    That is the reason its representation in the image is a straight line, depicting
    the total average in the entire execution. Something similar happens with the
    CIAPA strategy, within the first 300 s the interference metrics are collected
    and analyzed, and after that just one placement decision is taken, not being executed
    again, and its representation also is a straight line, representing the total
    average in the entire execution. In the Segmented approach, the application execution
    was divided into four segments, and at the end of each one, scheduling actions
    were taken. This strategy improves the overall response time when compared to
    EVEN and CIAPA strategies because it adjusts the infrastructure to applications,
    taking into account the variability of workloads (Meyer et al., 2021b), even considering
    only a few segments (four in this case). Download : Download high-res image (99KB)
    Download : Download full-size image Fig. 10. Average Response Time in each scheduled
    interval from the 24-nodes experiment. In general, IADA reaches the lowest response
    time rates (best results). However, there was an interval that presented worst
    results than CIAPA’s scheduler, for example, depicted in point A. This happened
    because IADA relies on a reactive approach, using the most recent data and not
    a general view to make scheduling decisions, so that in interval A the workload
    had an abrupt behavior change, not presenting the best scheduling arrangement,
    but still is considered as an acceptable result. There were intervals that IADA
    touches CIAPA’s outcomes, which were the B and C intervals’ cases. Also, the major
    response time reduction can be seen in the interval D, reaching an average improvement
    of 57% in relation to EVEN, CIAPA, and Segmented scheduling approaches. It is
    important to highlight that the proposed scheduling architecture adjusts applications
    over the hardware based on the workload oscillation, in real-time, and in a reactive
    manner. So far, the outcomes found in this work support our idea that an interference-aware
    dynamic scheduling architecture designed to observe workloads tends to reduce
    the overhead generated by cross-application interference over the system, and
    consequently utilizing the available hardware resources more efficiently. 4.4.
    Overhead evaluation Considering the dynamic nature of the problem, it was necessary
    to run some preliminary steps in order to make the scheduling decisions. As mentioned
    in Section 3, those steps are profiling applications, analyzing time-series data,
    and performing interference classification with ML techniques. After that, with
    a heuristic-oriented algorithm, it was possible to execute scheduling actions.
    All these techniques put some overhead pressure on the cluster system, as well
    as on the Node Manager. To examine these aspects, in the next sections, we performed
    some analysis to find out if the overhead generated by the proposed architecture
    could make its use infeasible. 4.4.1. Migration When running experiments within
    the real scenario, we performed many container migrations across the cluster.
    In terms of hardware resource usage, the overhead rate created by a single LXC/LXD
    migration can be considered low over the entire computational environment. To
    present how much this operation affects the system, we executed one container
    migration and profiled LXC/LXD processes with IntP. Fig. 11 illustrates the hardware
    utilization while performing a single container migration across the cluster.
    In our experiments the mean migration time was about 18 s, depending on the resource
    the applications is stressing more, it can take more or less time to conclude
    this operation. Download : Download high-res image (112KB) Download : Download
    full-size image Fig. 11. Resources’ behavior while running a container migration
    across cluster nodes. However, when the number of container migrations increases,
    the resulting overhead can considerably increase as well. So, such operations
    should be minimized as much as possible. As mentioned in Section 3.1.4, IADA uses
    a heuristic to find the best applications’ set to schedule applications over the
    cluster. We developed an optimized version of CIAPA (Ludwig et al., 2019) scheduler
    algorithm, taking the number of migrations into account when deciding the best
    scheduling actions. IADA scheduling algorithm was developed to dynamically deals
    with workload behavior changes, adjusting its decisions considering the most recent
    data and its behavior. So that it is important to keep the number of migration
    operations at the minimum, and for this reason, the amount of migrations operations
    is contemplated as a quality measure to decide if the new solution created is
    better than the actual one. Considering IADA automatically finds the best moments
    to classify data intervals, and based on that, it runs scheduling decisions, in
    all experiments we also observe the number of intervals found by the Data Analyzer
    component and how many migration actions were performed. These metrics are presented
    in Table 4 for each host arrangement. Observing this table, it is evident that
    the more applications IADA is controlling, the more intervals will be found. The
    reason this happens is that the more data (more applications with distinct workload
    patterns) the proposed architecture is analyzing, the greater the amount of information
    to be processed, consequently increasing the dynamism in the environment and generating
    greater optimization opportunity. Of course, this is also highly dependent on
    the variation of workloads, but since we are monitoring dynamic applications,
    these are not unexpected outcomes. Looking at the number of migrations performed,
    it is noticeable that the more applications running in the cluster, the greater
    the number of migrations as well, practically following a linear trend with applications’
    number. At the first look, the number of migrations performed with 192 applications
    seems to be exaggerated, but when dividing the number of migrations by the interval
    (Mig./interval), it is possible to notice that the number of migrations makes
    sense, being proportional in relation to the number of hosts, almost one migration
    per host per interval, demonstrating a reasonable outcome. Table 4. Number of
    intervals found by Data Analyzer component and how many migrations were performed
    per host arrangement. Hosts App. Intervals Migrations Mig./Interval 6 24 18 115
    6 12 48 23 245 11 24 96 24 712 30 48 192 27 1323 49 4.4.2. Machine learning According
    to Section 3.1.4, when is defined, then is established. After, this data interval
    is sent to the Classifier component so that depending on the data quantity (interval
    length), this process time can vary. On average, in our experiments, each application
    took about 1 s to be classified. To improve the performance of this proceeding,
    we used the doParallel library (Corporation and Weston, 2020) from R packages.
    This library can be adopted to send tasks (encoded as function calls) to each
    of the processing cores on a machine in parallel. This is done by using a function
    that distributes the tasks to multiple processors. After, this function gathers
    the responses up from each process call and it returns a list of responses, which
    is the same length as the list or vector of input data (one return per input item).
    To speed the classification process up, we performed the ML training phase previously,
    generating .RDA files. These files are the results from the R programming language
    within the training dataset phase so that it is not necessary to execute the model
    training phase each time the classification process is performed. Depending on
    the number of applications running inside the cluster and the length of data sent
    to the Classifier component, the ML analysis can take longer to be performed.
    In our experiments, the largest classified interval took less than 25 s, which
    is very reasonable, since each scheduling decision was not performed within less
    than a 20-s interval, that is the meantime to migrate containers across the cluster
    nodes. To measure the overhead of the classification process even with a high
    number of running applications, we performed a scalability experiment with this
    component. Therefore, we created a set of application workloads with varied interval
    lengths between 30 and 600 s, testing short, medium, and long periods. For each
    interval, we gradually increased the number of running application workloads (24,
    48, 96, and 192) to ensure that the execution time of the classification (y axis)
    is not significant even for many applications (colors) executing in a long interval
    of time (x axis), what would invalidate its use in a dynamic environment. Fig.
    12 presents these results. When looking at this figure, it is possible to observe
    the classification follows a linear trend, which is already expected since the
    classification is not a distributed process, and at certain times we are allocating
    more tasks than the number of cores our Node Manager owns. It is interesting to
    notice that it takes less than 30 s to accomplish the classification of 192 application
    workloads with a 10-min interval length, meaning the biggest application quantity
    with the largest period in this experiment. This result can be considered acceptable
    if the target applications do not have workloads with extreme behavior patterns
    variability. Download : Download high-res image (249KB) Download : Download full-size
    image Fig. 12. Classification makespan from a set of applications with different
    interval lengths. (For interpretation of the references to color in this figure
    legend, the reader is referred to the web version of this article.) Summing it
    up, if there will be performed an expressive number of applications or the length
    of the monitored period will be increased, it could be necessary to adopt a different
    machine with more computational power (more CPUs) than ours as Node Manager, in
    order to ensure the architecture works correctly. 4.4.3. Profiler To enforce IntP
    does not input a considerable overhead over the hardware, we have run an experiment
    with NAS Parallel Benchmarks (NPB),17 which are a small set of programs designed
    to help evaluate the performance of parallel supercomputers. First, we ran BT.d,
    CG.c, DC.b, EP.d, LU.c, MG.d, and UA.c algorithms without any profiler. After,
    we ran each one again with IntP profiling them. Each execution was performed 10
    times and the resulting meantime of them are presented in Fig. 13. Since IntP
    core works with low-level kernel events instrumentation, in our experiments, when
    it was enabled, its execution practically did not generate overhead. Meaning that
    IntP plays a non-intrusive role over the entire system. Download : Download high-res
    image (129KB) Download : Download full-size image Fig. 13. NAS Parallel Benchmark
    (NPB) algorithms’ executions with and without IntP profiling them. 5. Related
    work Virtualization technology enables highly scalable services to be easily delivered
    by cloud providers within different contexts. However, many real applications
    present dynamic workloads and need to be managed to avoid interference problems,
    minimizing performance degradation in resource-shared environments. Recently,
    several research efforts were conducted addressing interference-aware scheduling
    strategies, and in this section, we present them and provide a contrast to our
    work. Bu et al. (2013) introduce a task scheduling strategy to mitigate interference
    and meanwhile preserve task data locality for MapReduce applications. The authors’
    strategy includes an interference-aware scheduling policy, based on a task performance
    prediction model, and an adaptive delay scheduling algorithm for data locality
    improvement. Results show that the authors’ proposal is able to achieve a speedup
    of 1.5 to 6.5 times for individual jobs and yield an improvement of up to 1.9
    times in system throughput in comparison with four other MapReduce schedulers.
    Zhang et al. (2014) propose two schedulers: one in the virtualization layer designed
    to minimize interference on high priority interactive services, and one in the
    Hadoop framework that helps batch processing jobs meet their own performance deadlines.
    The evaluation shows that both schedulers allow a mixed cluster to reduce web
    response times by more than tenfold while meeting more Hadoop deadlines and lowering
    total task execution times by 6.5%. Chen et al. (2015) present CloudScope, a system
    that diagnoses interference for multi-tenant cloud sources. It employs a discrete-time
    Markov Chain model for the online prediction of performance interference of co-resident
    VMs. The interference-aware scheduler improves virtual machine performance by
    up to 10% compared to the default scheduler, achieving an average error of 9%.
    The authors also claim that the hypervisor reconfiguration can improve network
    throughput by up to 30%. Melo Alves et al. (2018) have developed an interference-aware
    virtual machine placement strategy for HPC applications in cloud computing. The
    authors’ approach implements a method that predicts interference levels in order
    to minimize the number of used physical machines. Results presented that the authors’
    method reduced interference by more than 40%, using the same hardware set. Shekhar
    et al. (2018) present an online, data-driven approach to build runtime predictive
    performance models. The predictive online models are then used in dynamically
    adapting to the workload variability by vertically auto-scaling co-located applications
    such that performance interference is minimized and QoS properties of latency-sensitive
    applications are met. A comparison with a representative latency-sensitive application
    reveals up to 39.46% lower tail latency than reactive approaches. Wang et al.
    (2019) developed data-driven analytical models to estimate the effect of interference
    among multiple Apache Spark jobs on job execution time in virtualized cloud environments.
    Experimental results show that the scheduling algorithm reduces the average execution
    time of individual jobs (between 47 and 26%) and the total execution time (2 to
    13%). Ludwig et al. (2019) propose placement algorithms based on interference
    levels for different workload scenarios. As a result, they achieve a 10% reduction
    in response time compared to interference strategies. Evolving the author’s efforts,
    in previous work (Meyer et al., 2021b), we propose a machine learning-driven classification
    scheme for dynamic interference-aware resource scheduling in cloud computing environments.
    We have presented how a classification approach, that better represents the workload
    variations, affects resource scheduling. By analyzing how hardware resources react
    to different applications, we explored distinct interference classification formats
    and evaluate their efficiency, taking the dynamic nature of cloud workloads into
    account. Then, we applied an interference-aware application classifier (Meyer
    et al., 2020) based on machine learning techniques and compare it with related
    work, adopting a variety of workload patterns. Preliminary results revealed an
    improvement of 27% hardware utilization efficiency when applying our classification
    approach in cloud data centers. Our work differs from related studies due to evolution
    of technology and the update of operating systems and Kernels versions. Such evolution
    makes possible to extract information from hardware in a manner that was not possible
    in the recent past. One example of that, is the advanced feature developed by
    Intel, called Intel Cache Monitoring Technology (CMT), now it is entirely viable
    to collect information about the usage of the cache by applications running inside
    any piece of equipment. This is something essential since multi-thread architectures
    are in an exponential growth within the computer market. This technology allows
    us to use an ID denominated Resource Monitoring ID (RMID) to metrify the number
    of threads scheduled among the operation system. For each thread, there is one
    ID associated with it, therefore, those metrics can be collected within an MSR
    interface. Something that could not be possible before the creation of this technology.18
    Bu et al. (2013) study is limited to analyze only CPU from hypervisor through
    xentop counters and disk metrics through linux iostat from hadoop workloads. Similar
    to Bu et al. (2013) work, Zhang et al. (2014) proposes ILA, in which only CPU
    and disk counters are monitored from hadoop applications. Likewise our work, Chen
    et al. (2015) includes in Cloudscope strategy some different components to distribute
    systems’ responsibilities. Also, authors’ approach profiles specific virtual machines
    characteristics, such as VCPUs and VNICs. The main difference from ours architecture
    is that we run applications over containers instead of traditional virtual machines,
    as mentioned before, this kind of virtualization presents many benefits over the
    traditional method, such as low management overhead and portability (Zhang et
    al., 2019). Melo Alves et al. (2018) work utilizes a slowdown factor that measures
    the applications’ time and how much (in percent) each one increased its time regarding
    isolated execution. Also, an average period is calculates over each host to compare
    them with each other. Our work is different in the sense that we apply interference
    levels instead of the raw percent of performance degradation, and on top of that
    we also use automatic techniques that outcome the interference levels, with no
    user intervention. Similar to our work, Shekhar et al. (2018) investigate the
    interference generated by container-based instances with workload variations.
    However, our work is distinctive in the following ways: (i) instead of focusing
    on a single server, our proposed approach is performed over a distributed architecture
    (cluster). Further, our proposed architecture focuses exclusively on applications
    that have dynamic workloads, such as latency-sensitive applications, while authors
    mix them with batch-job ones. Wang et al. (2019) are interested in improving Apache
    Spark jobs’ makespan. Since this type of application is workflow-oriented, they
    present multiple jobs’ stages, and the authors analyze each stage separately,
    stating that each one has different (specific) resources behavior. The authors
    consider only execution time, CPU usage, disk I/O rate, and network I/O rate,
    not observing cache and memory metrics, which our approach does. 6. Conclusion
    and future directions Cloud service providers offer many services through virtualization
    techniques to users over the Internet. As many virtual machines (VMs) run on the
    same computational node, they share physical resources, and consequently there
    exists great opportunity to produce resource contention, which results in applications’
    performance degradation. Therefore, how to place VMs to reduce performance degradation
    and guarantee QoS requirements is still a challenging task. Recently, many related
    studies have proposed strategies to tackle these issues, but none of them consider
    cross-application interference aspects to dynamically make scheduling decisions.
    In previous work (Meyer et al., 2020, Meyer et al., 2021b) we proposed a machine
    learning-driven classification scheme for dynamic interference-aware resource
    scheduling in cloud computing environments. It has been presented how a classification
    approach, that deals better with workload variability, affects resource scheduling.
    Preliminary results revealed an improvement in resource utilization efficiency
    by 27%, on average, when applying this classification approach in cloud scenarios.
    In this work, we further analyze this topic and develop IADA, a full-fledged interference-aware
    scheduling architecture for dynamic workloads in clouds. IADA combines and improves
    different techniques studied in previous work, including machine learning, bayesian
    algorithms, and heuristics to find abrupt changes in applications’ workload behavior,
    classifying and placing them in a way that minimizes the overall resource contention.
    We compare our solution with close-related studies in this field using real workloads
    from NASA, Wikimedia, and Alibaba Open Cluster Trace datasets and results show
    that IADA reduces the resulting performance degradation by 26% when compared to
    EVEN, CIAPA, and Segmented scheduling approaches in real experiments, and by 24%
    in simulated experiments. Moreover, we also performed and presented an overhead
    analysis under the Migration, Machine Learning, and Profiler techniques used by
    IADA and we have concluded that: the scheduling algorithm was developed and optimized
    to reduce as much as possible the number of migrations. Our solution presents
    reasonable numbers of scheduling actions per interval, keeping the general overhead
    at a acceptable rate; the machine learning techniques used generate a layer of
    overhead, but in our experiments, these indexes are considered acceptable. However
    depending on the number of nodes and applications the architecture is going to
    control, the resulting overhead could be bigger than ours, and consequently, the
    Node Manager might be resized; the chosen profiler (IntP) practically does not
    put any overhead pressure over the system, since this tool was built to analyze
    hardware events at the kernel layer. In future work, we expect to evaluate proactive
    scheduling approaches by applying machine learning prediction algorithms and comparing
    them with the current work. The goal is to analyze how much the performance degradation
    can be reduced by forecasting the workload variability and anticipating the hardware
    resource’s arrangement within a dynamic scheduling architecture. CRediT authorship
    contribution statement Vinícius Meyer: Conceptualization, Methodology, Software,
    Investigation, Data curation, Writing. Matheus L. da Silva: Review, Validation,
    Writing. Dionatrã F. Kirchoff: Review, Validation, Writing. Cesar A.F. De Rose:
    Conceptualization, Methodology, Validation, Supervision. Declaration of Competing
    Interest The authors declare that they have no known competing financial interests
    or personal relationships that could have appeared to influence the work reported
    in this paper. Acknowledgments This study was financed in part by the Coordenação
    de Aperfeiçoamento de Pessoal de Nível Superior - Brazil (CAPES) - Finance Code
    001. This work has been partially supported by the project ‘‘GREEN-CLOUD: Computação
    em Cloud com Computação Sustentável ” (#16/2551-0000 488-9), from FAPERGS and
    CNPq Brazil, program PRONEX 12/2014. Also, this work was achieved in cooperation
    with HP Brasil Indústria e Comércio de Equipamentos Eletrônicos LTDA using incentives
    of Brazilian Informatics Law (Law n°8.2.48 of 1991). References Al-Sinayyid and
    Zhu, 2020 Al-Sinayyid A., Zhu M. Job scheduler for streaming applications in heterogeneous
    distributed processing systems J. Supercomput. (2020), 10.1007/s11227-020-03223-z
    Google Scholar Alboaneen et al., 2021 Alboaneen D., Tianfield H., Zhang Y., Pranggono
    B. A metaheuristic method for joint task scheduling and virtual machine placement
    in cloud data centers Future Gener. Comput. Syst., 115 (2021), pp. 201-212, 10.1016/j.future.2020.08.036
    View PDFView articleView in ScopusGoogle Scholar Beloglazov and Buyya, 2012 Beloglazov
    A., Buyya R. Optimal online deterministic algorithms and adaptive heuristics for
    energy and performance efficient dynamic consolidation of virtual machines in
    cloud data centers Concurr. Comput.: Pract. Exper., 24 (13) (2012), pp. 1397-1420,
    10.1002/cpe.1867 View in ScopusGoogle Scholar Broadwell, 2004 Broadwell P.M. Response
    time as a performability metric for online services Report No. UCB//CSD-04-1324,
    Computer Science Division (EECS),University of California, Berkeley, California
    94720 (2004), pp. 1-49 Google Scholar Bu et al., 2013 Bu X., Rao J., Xu C.-z.
    Interference and locality-aware task scheduling for MapReduce applications in
    virtual clusters Proceedings of the 22Nd International Symposium on High-Performance
    Parallel and Distributed Computing, HPDC ’13, ACM, New York, NY, USA (2013), pp.
    227-238, 10.1145/2493123.2462904 View in ScopusGoogle Scholar Caglar et al., 2014
    Caglar F., Shekhar S., Gokhale A.S. Towards a performance interference-aware virtual
    machine placement strategy for supporting soft real-time applications in the cloud
    REACTION 2014, 3rd IEEE International Workshop on Real-Time and Distributed Computing
    in Emerging Applications, Proceedings, Rome, Italy. December 2nd, 2014 (2014),
    pp. 15-20 View in ScopusGoogle Scholar Caglar et al., 2016 Caglar F., Shekhar
    S., Gokhale A., Koutsoukos X. Intelligent, performance interference-aware resource
    management for IoT cloud backends 2016 IEEE First International Conference on
    Internet-of-Things Design and Implementation (IoTDI) (2016), pp. 95-105, 10.1109/IoTDI.2015.36
    View in ScopusGoogle Scholar Calheiros et al., 2011 Calheiros R.N., Ranjan R.,
    Beloglazov A., De Rose C.A.F., Buyya R. CloudSim: a toolkit for modeling and simulation
    of cloud computing environments and evaluation of resource provisioning algorithms
    Softw. - Pract. Exp., 41 (1) (2011), pp. 23-50, 10.1002/spe.995 Google Scholar
    Casanova, 2001 Casanova H. Simgrid: a toolkit for the simulation of application
    scheduling 1st IEEE/ACM International Symposium on Cluster Computing and the Grid
    (2001), pp. 430-437 View in ScopusGoogle Scholar Chen et al., 2017 Chen S., GalOn
    S., Delimitrou C., Manne S., Martínez J.F. Workload characterization of interactive
    cloud services on big and small server platforms 2017 IEEE International Symposium
    on Workload Characterization (IISWC) (2017), pp. 125-134, 10.1109/IISWC.2017.8167770
    Google Scholar Chen et al., 2015 Chen X., Rupprecht L., Osman R., Pietzuch P.,
    Franciosi F., Knottenbelt W. CloudScope: Diagnosing and managing performance interference
    in multi-tenant clouds 2015 IEEE 23rd International Symposium on Modeling, Analysis,
    and Simulation of Computer and Telecommunication Systems (2015), pp. 164-173,
    10.1109/MASCOTS.2015.35 View in ScopusGoogle Scholar Chhabra et al., 2020 Chhabra
    A., Singh G., Kahlon K.S. Multi-criteria HPC task scheduling on iaas cloud infrastructures
    using meta-heuristics Cluster Comput. (2020) Google Scholar Chhetri et al., 2021
    Chhetri M.B., Forkan A.R.M., Vo Q.B., Nepal S., Kowalczyk R. Exploiting heterogeneity
    for opportunistic resource scaling in cloud-hosted applications IEEE Trans. Serv.
    Comput., 14 (6) (2021), pp. 1739-1750, 10.1109/TSC.2019.2908647 View in ScopusGoogle
    Scholar Chiang and Huang, 2011 Chiang R.C., Huang H.H. TRACON: Interference-aware
    scheduling for data-intensive applications in virtualized environments Proceedings
    of 2011 International Conference for High Performance Computing, Networking, Storage
    and Analysis, SC ’11, ACM, New York, NY, USA (2011), pp. 47:1-47:12, 10.1145/2063384.2063447
    Google Scholar Corporation and Weston, 2020 Corporation M., Weston S. doparallel:
    Foreach parallel adaptor for the ‘parallel’ package (2020) R package version 1.0.16.
    URL https://CRAN.R-project.org/package=doParallel Google Scholar Daraje and Shaikh,
    2021 Daraje M., Shaikh J. Hybrid resource scaling for dynamic workload in cloud
    computing 2021 IEEE International Conference on Mobile Networks and Wireless Communications
    (ICMNWC) (2021), pp. 1-6, 10.1109/ICMNWC52512.2021.9688556 Google Scholar Delimitrou
    and Kozyrakis, 2013 Delimitrou C., Kozyrakis C. Paragon: QoS-aware scheduling
    for heterogeneous datacenters SIGPLAN Not., 48 (4) (2013), pp. 77-88, 10.1145/2499368.2451125
    Google Scholar Devarajan et al., 2018 Devarajan H., Kougkas A., Challa P., Sun
    X. Vidya: Performing code-block I/O characterization for data access optimization
    2018 IEEE 25th International Conference on High Performance Computing (HiPC) (2018),
    pp. 255-264, 10.1109/HiPC.2018.00036 View in ScopusGoogle Scholar Docker Engine
    Overview, 2022 Docker Engine Overview, ., 2022. URL https://docs.docker.com/engine/.
    Google Scholar Ebadifard and Babamir, 2021 Ebadifard F., Babamir S.M. Utonomic
    task scheduling algorithm for dynamic workloads through a load balancing technique
    for the cloud-computing environment Cluster Comput., 24 (2) (2021), pp. 1075-1101,
    10.1007/s10586-020-03177-0 View in ScopusGoogle Scholar Garg et al., 2014 Garg
    S.K., Toosi A.N., Gopalaiyengar S.K., Buyya R. SLA-based virtual machine management
    for heterogeneous workloads in a cloud datacenter J. Netw. Comput. Appl., 45 (2014),
    pp. 108-120, 10.1016/j.jnca.2014.07.030 View PDFView articleView in ScopusGoogle
    Scholar Guérout et al., 2013 Guérout T., Monteil T., Costa G.D., Calheiros R.N.,
    Buyya R., Alexandru M. Energy-aware simulation with DVFS Simul. Model. Pract.
    Theory, 39 (2013), pp. 76-91 View PDFView articleView in ScopusGoogle Scholar
    Hu et al., 2020 Hu Y., Zhou H., de Laat C., Zhao Z. Concurrent container scheduling
    on heterogeneous clusters with multi-resource constraints Future Gener. Comput.
    Syst., 102 (2020), pp. 562-573, 10.1016/j.future.2019.08.025 View PDFView articleView
    in ScopusGoogle Scholar Iqbal et al., 2018 Iqbal W., Erradi A., Mahmood A. Dynamic
    workload patterns prediction for proactive auto-scaling of web applications J.
    Netw. Comput. Appl., 124 (2018), pp. 94-107, 10.1016/j.jnca.2018.09.023 View PDFView
    articleView in ScopusGoogle Scholar Javadi and Gandhi, 2017 Javadi S.A., Gandhi
    A. DIAL: Reducing tail latencies for cloud applications via dynamic interference-aware
    load balancing 2017 IEEE International Conference on Autonomic Computing (ICAC)
    (2017), pp. 135-144, 10.1109/ICAC.2017.17 View in ScopusGoogle Scholar Jersak
    and Ferreto, 2016 Jersak L.C., Ferreto T. Performance-aware server consolidation
    with adjustable interference levels 31st Annual ACM Symposium on Applied Computing,
    SAC ’16, ACM, New York, NY, USA (2016), pp. 420-425, 10.1145/2851613.2851625 View
    in ScopusGoogle Scholar Kirkpatrick et al., 1983 Kirkpatrick S., Gelatt C.D.,
    Vecchi M.P. Optimization by simulated annealing Science, 220 (4598) (1983), pp.
    671-680, 10.1126/science.220.4598.671 View in ScopusGoogle Scholar Kliazovich
    et al., 2012 Kliazovich D., Bouvry P., Khan S.U. GreenCloud: a packet-level simulator
    of energy-aware cloud computing data centers J. Supercomput., 62 (2012), pp. 1263-1283,
    10.1007/s11227-010-0504-1 View in ScopusGoogle Scholar Krzywda et al., 2020 Krzywda
    J., Meyer V., Xavier M.G., Ali-Eldin A., Östberg P., De Rose C.A.F., Elmroth E.
    Modeling and simulation of qos-aware power budgeting in cloud data centers 2020
    28th Euromicro International Conference on Parallel, Distributed and Network-Based
    Processing (PDP) (2020), pp. 88-93, 10.1109/PDP50117.2020.00020 View in ScopusGoogle
    Scholar Kumar and Setia, 2017 Kumar R., Setia S. Interface aware scheduling of
    tasks on cloud 2017 4th International Conference on Signal Processing, Computing
    and Control (ISPCC) (2017), pp. 654-658, 10.1109/ISPCC.2017.8269758 View in ScopusGoogle
    Scholar Lim et al., 2009 Lim S., Sharma B., Nam G., Kim E.K., Das C.R. MDCSim:
    A multi-tier data center simulation, platform 2009 IEEE International Conference
    on Cluster Computing and Workshops (2009), pp. 1-9, 10.1109/CLUSTR.2009.5289159
    Google Scholar Linux Trace Toolkit Project Page, 2002 Linux Trace Toolkit Project
    Page, ., 2002. URL https://www.opersys.com/LTT/. Google Scholar Ludwig et al.,
    2019 Ludwig U.L., Xavier M.G., Kirchoff D.F., Cezar I.B., De Rose C.A.F. Optimizing
    multi-tier application performance with interference and affinity-aware placement
    algorithms Concurr. Comput.: Pract. Exper. (2019), p. e5098, 10.1002/cpe.5098
    e5098 cpe.5098 View in ScopusGoogle Scholar Mallikharjuna Rao and Rama Satish,
    2022 Mallikharjuna Rao K., Rama Satish A. A comprehensive study on workloads in
    cloud computing Bianchini M., Piuri V., Das S., Shaw R.N. (Eds.), Advanced Computing
    and Intelligent Technologies, Springer Singapore, Singapore (2022), pp. 505-514
    CrossRefView in ScopusGoogle Scholar Melo Alves et al., 2018 Melo Alves M., Teylo
    L., Frota Y., Drummond L.M.A. An interference-aware virtual machine placement
    strategy for high performance computing applications in clouds 2018 Symposium
    on High Performance Computing Systems (WSCAD) (2018), pp. 94-100, 10.1109/WSCAD.2018.00024
    Google Scholar Menage, 2022 Menage P. Control groups definition, implementation
    details, examples and api (2022) URL https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt
    Google Scholar Merkel, 2014 Merkel D. Docker: Lightweight linux containers for
    consistent development and deployment Linux J., 2014 (239) (2014) Google Scholar
    Meyer et al., 2020 Meyer V., Kirchoff D.F., da Silva M.L., César D.R.A.F. An interference-aware
    application classifier based on machine learning to improve scheduling in clouds
    2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based
    Processing (PDP) (2020), pp. 80-87, 10.1109/PDP50117.2020.00019 View in ScopusGoogle
    Scholar Meyer et al., 2021a Meyer V., Kirchoff D.F., da Silva M.L., De Rose C.A.F.
    Interference-aware application classifier for dynamic scheduling in cloud infrastructures
    (2021), 10.24433/CO.3183391.v1 https://www.codeocean.com/ Google Scholar Meyer
    et al., 2021b Meyer V., Kirchoff D.F., Da Silva M.L., De Rose C.A. ML-driven classification
    scheme for dynamic interference-aware resource scheduling in cloud infrastructures
    J. Syst. Archit., 116 (2021), Article 102064, 10.1016/j.sysarc.2021.102064 View
    PDFView articleView in ScopusGoogle Scholar Meyer et al., 2020 Meyer V., Ludwig
    U.L., Xavier M.G., Kirchoff D.F., De Rose C.A.F. Towards interference-aware dynamic
    scheduling in virtualized environments Job Scheduling Strategies for Parallel
    Processing, Springer International Publishing, Cham (2020), pp. 1-24 CrossRefView
    in ScopusGoogle Scholar Meyer et al., 2019a Meyer V., Righi R.R., Rodrigues V.F.,
    Costa C.A.D., Galante G., Both C. Pipel: Exploiting resource reorganization to
    optimize performance of pipeline-structured applications in the cloud Int. J.
    Comput. Syst. Eng. (2019), 10.1504/IJCSYSE.2019.10015444 Google Scholar Meyer
    et al., 2019b Meyer V., Xavier M., Kirchoff D., Righi R., De Rose C.F. Performance
    and cost analysis between elasticity strategies over pipeline-structured applications
    Proceedings of the 9th International Conference on Cloud Computing and Services
    Science - CLOSER, SciTePress, INSTICC (2019), pp. 404-411, 10.5220/0007729004040411
    View in ScopusGoogle Scholar Moreno et al., 2013 Moreno I.S., Yang R., Xu J.,
    Wo T. Improved energy-efficiency in cloud datacenters with interference-aware
    virtual machine placement 2013 IEEE Eleventh International Symposium on Autonomous
    Decentralized Systems (ISADS) (2013), pp. 1-8, 10.1109/ISADS.2013.6513411 Google
    Scholar Nathuji et al., 2010 Nathuji R., Kansal A., Ghaffarkhah A. Q-clouds: Managing
    performance interference effects for qos-aware clouds Proceedings of the 5th European
    Conference on Computer Systems, EuroSys ’10, ACM, New York, NY, USA (2010), pp.
    237-250, 10.1145/1755913.1755938 View in ScopusGoogle Scholar nez et al., 2012
    nez A.N., L. V.-P.J., Caminero A.C., Castañé G.G., Carretero J., Llorente I.M.
    iCancloud: A flexible and scalable cloud infrastructure simulator J. Supercomput.,
    10 (2012), pp. 185-209, 10.1007/s10723-012-9208-5 View in ScopusGoogle Scholar
    OpenVZ, 2022 OpenVZ, ., 2022. URL https://openvz.org/. Google Scholar Pagotto,
    2019 Pagotto A. ocp: Bayesian online changepoint detection (2019) R package version
    0.1.1. URL https://CRAN.R-project.org/package=ocp Google Scholar Pahl et al.,
    2019 Pahl C., Brogi A., Soldani J., Jamshidi P. Cloud container technologies:
    A state-of-the-art review IEEE Trans. Cloud Comput., 7 (3) (2019), pp. 677-692,
    10.1109/TCC.2017.2702586 View in ScopusGoogle Scholar Piraghaj et al., 2017 Piraghaj
    S.F., Dastjerdi A.V., Calheiros R.N., Buyya R. ContainerCloudSim: An environment
    for modeling and simulation of containers in cloud data centers Softw. - Pract.
    Exp., 47 (4) (2017), pp. 505-521, 10.1002/spe.2422 View in ScopusGoogle Scholar
    R Core Team, 2019 R Core Team S.F. R: A Language and Environment for Statistical
    Computing R Foundation for Statistical Computing, Vienna, Austria (2019) URL https://www.R-project.org/
    Google Scholar Radhika and Sudha Sadasivam, 2021 Radhika E., Sudha Sadasivam G.
    A review on prediction based autoscaling techniques for heterogeneous applications
    in cloud environment Mater. Today Proc., 45 (2021), pp. 2793-2800, 10.1016/j.matpr.2020.11.789
    International Conference on Advances in Materials Research - 2019 View PDFView
    articleView in ScopusGoogle Scholar Rosen, 2014 Rosen R. Linux containers and
    the future cloud (2014) URL https://www.linuxjournal.com/content/linux-containers-and-future-cloud
    Google Scholar Sampaio et al., 2015 Sampaio A.M., Barbosa J.G., Prodan R. PIASA:
    A power and interference aware resource management strategy for heterogeneous
    workloads in cloud data centers Simul. Model. Pract. Theory, 57 (2015), pp. 142-160,
    10.1016/j.simpat.2015.07.002 View PDFView articleView in ScopusGoogle Scholar
    Scheepers, 2014 Scheepers M.J. Virtualization and containerization of application
    infrastructure : A comparison 21st Twente Student Conference on IT (2014), pp.
    1-7 Google Scholar Shah et al., 2013 Shah A., Wolf F., Zhumatiy S., Voevodin V.
    Capturing inter-application interference on clusters IEEE International Conference
    on Cluster Computing (CLUSTER) (2013), pp. 1-5, 10.1109/CLUSTER.2013.6702665 Google
    Scholar Shekhar et al., 2018 Shekhar S., Abdel-Aziz H., Bhattacharjee A., Gokhale
    A., Koutsoukos X. Performance interference-aware vertical elasticity for cloud-hosted
    latency-sensitive applications 2018 IEEE 11th International Conference on Cloud
    Computing (CLOUD) (2018), pp. 82-89, 10.1109/CLOUD.2018.00018 View in ScopusGoogle
    Scholar Thamsen et al., 2020 Thamsen L., Verbitskiy I., Nedelkoski S., Tran V.T.,
    Meyer V., Xavier M.G., Kao O., De Rose C.A.F. Hugo: A cluster scheduler that efficiently
    learns to select complementary data-parallel jobs Schwardmann U., Boehme C., B.
    Heras D., Cardellini V., Jeannot E., Salis A., Schifanella C., Manumachu R.R.,
    Schwamborn D., Ricci L., Sangyoon O., Gruber T., Antonelli L., Scott S.L. (Eds.),
    Euro-Par 2019: Parallel Processing Workshops, Springer International Publishing,
    Cham (2020), pp. 519-530 CrossRefView in ScopusGoogle Scholar Toosi et al., 2017
    Toosi A.N., Qu C., de Assunção M.D., Buyya R. Renewable-aware geographical load
    balancing of web applications for sustainable data centers J. Netw. Comput. Appl.,
    83 (2017), pp. 155-168, 10.1016/j.jnca.2017.01.036 Google Scholar Tosatto et al.,
    2015 Tosatto A., Ruiu P., Attanasio A. Container-based orchestration in cloud:
    State of the art and challenges 2015 Ninth International Conference on Complex,
    Intelligent, and Software Intensive Systems (2015), pp. 70-75, 10.1109/CISIS.2015.35
    View in ScopusGoogle Scholar Urgaonkar et al., 2003 Urgaonkar B., Shenoy P., Roscoe
    T. Resource overbooking and application profiling in shared hosting platforms
    SIGOPS Oper. Syst. Rev., 36 (SI) (2003), pp. 239-254, 10.1145/844128.844151 Google
    Scholar Wang et al., 2019 Wang K., Khan M.M.H., Nguyen N., Gokhale S. Design and
    implementation of an analytical framework for interference aware job scheduling
    on apache spark platform Cluster Comput., 22 (1) (2019), pp. 2223-2237, 10.1007/s10586-017-1466-3
    View in ScopusGoogle Scholar Xavier, 2019 Xavier M.G. Data Processing With Cross-application
    Interference Control via System-level Instrumentation (Ph.D. thesis) Pontifical
    Catholic University of Rio Grande do Sul, Porto Alegre, Brazil (2019) Google Scholar
    Xavier et al., 2014 Xavier M.G., Neves M.V., Rose C.A.F.D. A performance comparison
    of container-based virtualization systems for MapReduce clusters 2014 22nd Euromicro
    International Conference on Parallel, Distributed, and Network-Based Processing
    (2014), pp. 299-306, 10.1109/PDP.2014.78 View in ScopusGoogle Scholar Xavier et
    al., 2017 Xavier M.G., Rossi F.D., Rose C.A.F.D., Calheiros. R.N., Gomes D.G.
    Modeling and simulation of global and sleep states in ACPI-compliant energy-efficient
    cloud environments Concurr. Comput.: Pract. Exper., 29 (4) (2017), Article e3839,
    10.1002/cpe.3839 e3839 cpe.3839 View in ScopusGoogle Scholar Zhang et al., 2014
    Zhang W., Rajasekaran S., Wood T., Zhu M. MIMP: Deadline and interference aware
    scheduling of hadoop virtual machines 2014 14th IEEE/ACM International Symposium
    on Cluster, Cloud and Grid Computing (2014), pp. 394-403, 10.1109/CCGrid.2014.101
    View in ScopusGoogle Scholar Zhang et al., 2019 Zhang F., Tang X., Li X., Khan
    S.U., Li Z. Quantifying cloud elasticity with container-based autoscaling Future
    Gener. Comput. Syst., 98 (2019), pp. 672-681, 10.1016/j.future.2018.09.009 View
    PDFView articleGoogle Scholar Zhu and Tung, 2012 Zhu Q., Tung T. A performance
    interference model for managing consolidated workloads in qos-aware clouds 2012
    IEEE Fifth International Conference on Cloud Computing (2012), pp. 170-179, 10.1109/CLOUD.2012.25
    View in ScopusGoogle Scholar Cited by (7) Perphproctor: Qos-Aware Scheduling Based
    on Co-Location Forecasting on Shared Clusters 2024, SSRN Effective Local Search
    for Priority-Constrained Job Scheduling in Cloud 2023, SSRN Delay and size-dependent
    priority-aware scheduling for IoT-based healthcare traffic using heterogeneous
    multi-server priority queueing system 2023, IET Communications Performance interference
    of co-allocated applications: a systematic literature review 2023, Research Square
    A Hybrid Edge-Cloud Computing Approach for Energy-Efficient Surveillance Using
    Deep Reinforcement Learning 2023, Proceedings - International Conference on Technological
    Advancements in Computational Sciences, ICTACS 2023 Analysis of Clustering Effects
    In Cloud Workload Forecasting 2023, 2023 14th International Conference on Computing
    Communication and Networking Technologies, ICCCNT 2023 View all citing articles
    on Scopus Vinícius Meyer received his bachelor’s degree in Computer Engineering
    from the Univates University in 2014 and his master’s degree in Applied Computing
    from the Unisinos University in 2016. Currently, he is a Ph.D. student in Computer
    Science at the Pontifical Catholic University of Rio Grande do Sul (PUCRS), working
    mainly with dynamic resource scheduling based on cross-application interference.
    His research interests are Distributed Systems, Cloud Computing, Machine Learning
    and Simulated Environments. Matheus L. Da Silva received the B.S. degree from
    University from Passo Fundo, Brazil, in 2017, and the master’s degree in computer
    science from the Pontifical Catholic University of Rio Grande do Sul, Brazil,
    in 2020, where he is currently pursuing the Ph.D. degree with the Computer Science
    Graduate Program. His research interests include Parallel and Distributed Processing
    and Edge Computing. Dionatrã F. Kirchoff was born in Brazil in 1990. He received
    the B.E degree from the Faculdade Meridional (IMED, Passo Fundo, Brazil, 2013).
    He holds a specialist degree in Governance of Information Technology based on
    international standards from the University of Vale do Rio dos Sinos (UNISINOS,
    São Leopoldo, Brazil, 2015). Also, he has an M.Sc. in Computer Science from the
    Pontifical University Catholic of Rio Grande do Sul (PUCRS, Porto Alegre, Brazil,
    2019). Since 2019 he is a Ph.D. candidate at the same university. His main areas
    of research interest are Resource Management, Cloud Computing, and Machine Learning.
    Cesar A. F. De Rose has a B.Sc. degree in Computer Science from PUCRS, a M.Sc.
    in Computer Science from PGCC/UFRGS and a Doctoral degree from Karlsruhe Institute
    of Technology (KIT - Karlsruhe, Germany). In 1998 he joined the School of Technology
    at PUCRS as an associate professor and member of the Resource Management and Virtualization
    Group (Full Professor since 2012). His research interests include several aspects
    of resource management, including dynamic provisioning and allocation, monitoring
    and profiling techniques, scheduling and optimization in parallel and distributed
    environments (Cluster, Grid, Cloud) and virtualization. In 2009 he founded PUCRS
    High Performance Computing Laboratory (LAD-PUCRS) being nowadays senior researcher.
    ☆ Editor: J.C. Duenas. 1 https://github.com/ViniciusMeyer/CloudSimInterference.
    2 https://xenproject.org/. 3 https://www.linux-kvm.org/. 4 https://openvz.org/.
    5 http://www.linux-vserver.org. 6 https://linuxcontainers.org/. 7 http://www.docker.com.
    8 https://storm.apache.org/. 9 https://github.com/uillianluiz/node-tiers. 10 http://www.tpc.org/tpch/.
    11 https://dumps.wikimedia.org/other/analytics/. 12 https://github.com/alibaba/clusterdata/tree/master/cluster-trace-v2018.
    13 ftp://ita.ee.lbl.gov/html/contrib/. 14 https://www.pucrs.br/ideia/lablad/.
    15 https://artillery.io/. 16 https://criu.org/. 17 https://www.nas.nasa.gov/publications/npb.html.
    18 https://github.com/intel/intel-cmt-cat. View Abstract © 2022 Elsevier Inc.
    All rights reserved. Recommended articles Self-adaptive mobile web service discovery
    framework for Dynamic Mobile Environment Journal of Systems and Software, Volume
    184, 2022, Article 111120 Salisu Garba, …, Nor Azizah Saadon View PDF Code-quality
    evaluation scheme for assessment of student contributions to programming projects
    Journal of Systems and Software, Volume 188, 2022, Article 111273 Hsi-Min Chen,
    …, Chyi-Ren Dow View PDF A comprehensive empirical investigation on failure clustering
    in parallel debugging Journal of Systems and Software, Volume 193, 2022, Article
    111452 Yi Song, …, Xi Wu View PDF Show 3 more articles Article Metrics Citations
    Citation Indexes: 7 Captures Readers: 8 View details About ScienceDirect Remote
    access Shopping cart Advertise Contact and support Terms and conditions Privacy
    policy Cookies are used by this site. Cookie settings | Your Privacy Choices All
    content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Journal of Systems and Software
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'IADA: A dynamic interference-aware cloud scheduling architecture for latency-sensitive
    workloads'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chou D.
  - Jiang M.
  citation_count: '55'
  description: Data-driven network intrusion detection (NID) has a tendency towards
    minority attack classes compared to normal traffic. Many datasets are collected
    in simulated environments rather than real-world networks. These challenges undermine
    the performance of intrusion detection machine learning models by fitting machine
    learning models to unrepresentative "sandbox"datasets. This survey presents a
    taxonomy with eight main challenges and explores common datasets from 1999 to
    2020. Trends are analyzed on the challenges in the past decade and future directions
    are proposed on expanding NID into cloud-based environments, devising scalable
    models for large network data, and creating labeled datasets collected in real-world
    networks.
  doi: 10.1145/3472753
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Journal Home Just Accepted Latest
    Issue Archive Authors Editors Reviewers About Contact Us HomeACM JournalsACM Computing
    SurveysVol. 54, No. 9A Survey on Data-driven Network Intrusion Detection SURVEY
    OPEN ACCESS SHARE ON A Survey on Data-driven Network Intrusion Detection Authors:
    Dylan Chou , Meng Jiang Authors Info & Claims ACM Computing SurveysVolume 54Issue
    9Article No.: 182pp 1–36https://doi.org/10.1145/3472753 Published:08 October 2021Publication
    History 19 citation 4,591 Downloads View all FormatsPDF ACM Computing Surveys
    Volume 54, Issue 9 Previous Next Abstract References Cited By Index Terms Recommendations
    Comments Skip Abstract Section Abstract Data-driven network intrusion detection
    (NID) has a tendency towards minority attack classes compared to normal traffic.
    Many datasets are collected in simulated environments rather than real-world networks.
    These challenges undermine the performance of intrusion detection machine learning
    models by fitting machine learning models to unrepresentative “sandbox” datasets.
    This survey presents a taxonomy with eight main challenges and explores common
    datasets from 1999 to 2020. Trends are analyzed on the challenges in the past
    decade and future directions are proposed on expanding NID into cloud-based environments,
    devising scalable models for large network data, and creating labeled datasets
    collected in real-world networks. References Shaza Merghani Abdelrahman and Ajith
    Abraham. 2014. Intrusion detection using error correcting output code based ensemble.
    In International Conference on Hybrid Intelligent Systems. 181–186. R. Abdulhammed,
    M. Faezipour, A. Abuzneid, and A. AbuMallouh. 2019. Deep and machine learning
    approaches for anomaly-based intrusion detection of imbalanced network traffic.
    IEEE Sensors Lett. 3, 1 (2019), 1–4. Adebayo O. Adetunmbi, Samuel O. Falaki, Olumide
    S. Adewale, and Boniface K. Alese. 2008. Network intrusion detection based on
    rough set and k-nearest neighbour. Int. J. Comput. ICT Res. 2, 1 (2008), 60–66.
    Show All References Cited By View all Zhang J, Gong B, Waqas M, Tu S and Chen
    S. (2023). Many-Objective Optimization Based Intrusion Detection for in-Vehicle
    Network Security. IEEE Transactions on Intelligent Transportation Systems. 24:12.
    (15051-15065). Online publication date: 1-Dec-2023. https://doi.org/10.1109/TITS.2023.3296002
    Sharifian Z, Barekatain B, Quintana A, Beheshti Z and Safi-Esfahani F. (2023).
    Sin-Cos-bIAVOA. Expert Systems with Applications: An International Journal. 228:C.
    Online publication date: 15-Oct-2023. https://doi.org/10.1016/j.eswa.2023.120404
    Fu J, Wang L, Ke J, Yang K and Yu R. (2023). GANAD: A GAN-based method for network
    anomaly detection. World Wide Web. 26:5. (2727-2748). Online publication date:
    1-Sep-2023. https://doi.org/10.1007/s11280-023-01160-4 Show All Cited By Index
    Terms A Survey on Data-driven Network Intrusion Detection Computing methodologies
    Machine learning General and reference Document types Surveys and overviews Security
    and privacy Intrusion/anomaly detection and malware mitigation Network security
    Recommendations Enhancing byte-level network intrusion detection signatures with
    context CCS ''03: Proceedings of the 10th ACM conference on Computer and communications
    security Many network intrusion detection systems (NIDS) use byte sequences as
    signatures to detect malicious activity. While being highly efficient, they tend
    to suffer from a high false-positive rate. We develop the concept of contextual
    signatures as an ... Read More An artificial intelligence membrane to detect network
    intrusion We propose an artificial intelligence membrane to detect network intrusion,
    which is analogous to a biological membrane that prevents viruses from entering
    cells. This artificial membrane is designed to monitor incoming packets and to
    prevent a ... Read More Agent-oriented network intrusion detection system using
    data mining approaches Most of the existing commercial Network Intrusion Detection
    System (NIDS) products are signature-based but not adaptive. In this paper, an
    adaptive NIDS using data mining technology is developed. Data mining approaches
    are used to accurately capture the ... Read More Comments 150+ References View
    Issue’s Table of Contents Footer Categories Journals Magazines Books Proceedings
    SIGs Conferences Collections People About About ACM Digital Library ACM Digital
    Library Board Subscription Information Author Guidelines Using ACM Digital Library
    All Holdings within the ACM Digital Library ACM Computing Classification System
    Digital Library Accessibility Join Join ACM Join SIGs Subscribe to Publications
    Institutions and Libraries Connect Contact Facebook Twitter Linkedin Feedback
    Bug Report The ACM Digital Library is published by the Association for Computing
    Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy Code of Ethics"'
  inline_citation: '>'
  journal: ACM Computing Surveys
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Survey on Data-driven Network Intrusion Detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
